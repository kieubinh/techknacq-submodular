{
  "info": {
    "authors": [
      "Zhengyu Niu",
      "Ji Donghong",
      "Chew Lim Tan"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P05-1049",
    "title": "Word Sense Disambiguation Using Label Propagation Based Semi-Supervised Learning",
    "url": "https://aclweb.org/anthology/P05-1049",
    "year": 2005
  },
  "references": [
    "acl-C92-2070",
    "acl-J04-1001",
    "acl-J94-4003",
    "acl-J98-1002",
    "acl-J98-1004",
    "acl-J98-1006",
    "acl-P00-1069",
    "acl-P02-1033",
    "acl-P03-1058",
    "acl-P04-1036",
    "acl-P91-1034",
    "acl-P95-1026",
    "acl-P97-1009",
    "acl-W02-1006",
    "acl-W04-0807",
    "acl-W04-2405"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Shortage of manually sense-tagged data is an obstacle to supervised word sense disambiguation methods.",
        "In this paper we investigate a label propagation based semi-supervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels.",
        "Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In this paper, we address the problem of word sense disambiguation (WSD), which is to assign an appropriate sense to an occurrence of a word in a given context.",
        "Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al., 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Schfitze, 1998).",
        "Supervised sense disambiguation has been very successful, but it requires a lot of manually sense-tagged data and can not utilize raw unannotated data that can be cheaply acquired.",
        "Fully unsupervised methods do not need the definition of senses and manually sense-tagged data, but their sense clustering results can not be directly used in many NLP tasks since there is no sense tag for each instance in clusters.",
        "Considering both the availability of a large amount of unlabelled data and direct use of word senses, semi-supervised learning methods have received great attention recently.",
        "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words.",
        "They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sense-tagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995).",
        "As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration.",
        "It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process.",
        "Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption underlying many supervised learning algorithms, such as kNN.",
        "Recently a promising family of semi-supervised learning algorithms are introduced, which can effectively combine unlabeled data with labeled data",
        "in learning process by exploiting cluster structure in data (Belkin and Niyogi, 2002; Blum et al., 2004; Chapelle et al., 1991; Szummer and Jaakkola, 2001; Zhu and Ghahramani, 2002; Zhu et al., 2003).",
        "Here we investigate a label propagation based semi-supervised learning algorithm (LP algorithm) (Zhu and Ghahramani, 2002) for WSD, which works by representing labeled and unlabeled examples as vertices in a connected graph, then iteratively propagating label information from any vertex to nearby vertices through weighted edges, finally inferring the labels of unlabeled examples after this propagation process converges.",
        "Compared with bootstrapping, LP algorithm is based on a global consistency assumption.",
        "Intuitively, if there is at least one labeled example in each cluster that consists of similar examples, then unlabeled examples will have the same labels as labeled examples in the same cluster by propagating the label information of any example to nearby examples according to their proximity.",
        "This paper is organized as follows.",
        "First, we will formulate WSD problem in the context of semi-supervised learning in section 2.",
        "Then in section 3 we will describe LP algorithm and discuss the difference between a supervised learning algorithm (SVM), bootstrapping algorithm and LP algorithm.",
        "Section 4 will provide experimental results of LP algorithm on widely used benchmark corpora.",
        "Finally we will conclude our work and suggest possible improvement in section 5."
      ]
    },
    {
      "heading": "2 Problem Setup",
      "text": [
        "Let X = {xi}Z i be a set of contexts of occurrences of an ambiguous word w, where xi represents the context of the i-th occurrence, and n is the total number of this word’s occurrences.",
        "Let 5 = {sj }�_1 denote the sense tag set of w. The first l examples xg(1 ≤ g ≤ l) are labeled as yg (yg ∈ 5) and other u (l +u = n) examples xh (l + 1 ≤ h ≤ n) are unlabeled.",
        "The goal is to predict the sense of w in context xh by the use of label information of xg and similarity information among examples in X.",
        "The cluster structure in X can be represented as a connected graph, where each vertex corresponds to an example, and the edge between any two examples xi and xj is weighted so that the closer the vertices in some distance measure, the larger the weight associated with this edge.",
        "The weights are defined as",
        "follows: WZj = exp(− da ) if i j and WZZ = 0 (1 ≤ i, j ≤ n), where dzj is the distance (ex.",
        "Euclidean distance) between xi and xj, and Q is used to control the weight WZj ."
      ]
    },
    {
      "heading": "3 Semi-supervised Learning Method",
      "text": []
    },
    {
      "heading": "3.1 Label Propagation Algorithm",
      "text": [
        "In LP algorithm (Zhu and Ghahramani, 2002), label information of any vertex in a graph is propagated to nearby vertices through weighted edges until a global stable stage is achieved.",
        "Larger edge weights allow labels to travel through easier.",
        "Thus the closer the examples, more likely they have similar labels (the global consistency assumption).",
        "In label propagation process, the soft label of each initial labeled example is clamped in each iteration to replenish label sources from these labeled data.",
        "Thus the labeled data act like sources to push out labels through unlabeled data.",
        "With this push from labeled examples, the class boundaries will be pushed through edges with large weights and settle in gaps along edges with small weights.",
        "If the data structure fits the classification goal, then LP algorithm can use these unlabeled data to help learning classification plane.",
        "Let YO ∈ Nn,' represent initial soft labels attached to vertices, where YQ �� – 1 if yz is sj and 0 otherwise.",
        "Let YL be the to l rows of YO and YU"
      ]
    },
    {
      "heading": "OP",
      "text": [
        "be the remaining u rows.",
        "Yr°, is consistent with the labeling in labeled data, and the initialization of Yu can be arbitrary.",
        "Optimally we expect that the value of WZj across different classes is as small as possible and the value of WZj within same class is as large as possible.",
        "This will make label propagation to stay within same class.",
        "In later experiments, we set Q as the average distance between labeled examples from different classes.",
        "to jump from example xj to example xi.",
        "Compute the row-normalized matrix T by TZj TZj/ En _1 TZk.",
        "This normalization is to maintain the class probability interpretation of Y.",
        "1.",
        "Initially set t=0, where t is iteration index; 2.",
        "Propagate the label by Yt+i = TV; 3.",
        "Clamp labeled data by replacing the top l row of Yt+i with YO .",
        "Repeat from step 2 until Yt converges; 4.",
        "Assign xh(l + 1 ≤ h ≤ n) with a label sj,",
        "where j = .",
        "This algorithmas been shown to converge to a uniqueolution, which is YU = limt,� YU = (I − T, ... ) – iT,,IY Ghahramani2002).",
        "Lo (Zhu and obtained ned with We can see that this solution can be - out iteration and the initialization of YU is not important, since YO does not affect the estimation of YU.",
        "I is u × u identity matrix.",
        "Tuu and Tul are acquiredy splitting matrix T after the l-throw and the l-th column into 4 sub-matrices.",
        "m ique red"
      ]
    },
    {
      "heading": "3.2 Comparison between SVM, Bootstrapping and LP",
      "text": [
        "For WSD, SVM is one of the state of the art supervised learning algorithmsMihalcea et al., 2004), while bootstrapping is one of the state of the art semi-supervised learning algorithmsLi and Li, 2004; Yarowsky, 1995).",
        "For comparing LP with SVM and bootstrapping, let us consider a dataset with two-moon pattern shown in Figure 1(a).",
        "The upper moon consists of 9 , while the lower moon consists of 13 points.",
        "There is only one la-beledpoint in each moon, and other 20 are unms ms poi poi",
        "labeled.",
        "The distance metric is Euclidian distance.",
        "We can see that the points in one moon should be more similar to each other than the points across the moons.",
        "Figure 1(b) shows the classification result of SVM.",
        "Vertical line denotes classification hyperplane, which has the maximum separating margin with respect to the labeled points in two classes.",
        "We can see that SVM does not work well when labeled data can not reveal the structure (two moon pattern) in each class.",
        "The reason is that the classification hyperplane was learned only from labeled data.",
        "In other words, the coherent structure (two-moon pattern) in unlabeled data was not explored when inferring class boundary.",
        "Figure 1(c) shows bootstrapping procedure using kNN (k=1) as base classifier with user-specified parameter b = 1 (the number of added examples from unlabeled data into classified data for each class in each iteration).",
        "Termination condition is that the dis-bootstrappi tance between labeled and unlabeled points is more points, than interclass distance (the distance between AO and Bo).",
        "Each arrow in Figure 1(c) represents points one classification operation in each iteration for each class.",
        "After eight iterations, Ai ∼ A8 were tagged",
        "as +1, and B1 ∼ B8 were tagged as −1, while Ag ∼ A10 and B9 ∼ B10 were still untagged.",
        "Then at the ninth iteration, A9 was tagged as +1 since the label of A9 was determined only by labeled points in kNN model: A9 is closer to any point in {Ao ∼ A8} than to any point in {Bo ∼ B8}, regardless of the intrinsic structure in data: A9 ∼ A10 and B9 ∼ B10 are closer to points in lower moon than to points in upper moon.",
        "In other words, bootstrapping method uses the unlabeled data under a local consistency based strategy.",
        "This is the reason that two points A9 and A10 are misclassified (shown in Figure 1(c)).",
        "From above analysis we see that both SVM and bootstrapping are based on a local consistency assumption.",
        "Finally we ran LP on a connected graph-minimum spanning tree generated for this dataset, shown in Figure 2(a).",
        "A, B, C represent three points, and the edge A − B connects the two moons.",
        "Figure 2(b)- 2(f) shows the convergence process of LP with t increasing from 1 to 100.",
        "When t = 1, label information of labeled data was pushed to only nearby points.",
        "After seven iteration steps (t = 7), point B in upper moon was misclassified as −1 since it first received label information from point A through the edge connecting two moons.",
        "After another three iteration steps (t=10), this misclassified point was re-tagged as +1.",
        "The reason of this self-correcting behavior is that with the push of label information from nearby points, the value of YB +1 became higher than YB�_i.",
        "In other words, the weight of edge B − C is larger than that of edge B − A, which makes it easier for +1 label of point C to travel to point B.",
        "Finally, when t ≥ 12 LP converged to a fixed point, which achieved the ideal classification result."
      ]
    },
    {
      "heading": "4 Experiments and Results",
      "text": []
    },
    {
      "heading": "4.1 Experiment Design",
      "text": [
        "For empirical comparison with SVM and bootstrapping, we evaluated LP on widely used benchmark corpora - “interest”, “line” 1 and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) 2.",
        "We used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations).",
        "For SVM, we did not perform feature selection on SENSEVAL-3 data since feature selection deteriorates its performance (Lee and Ng, 2002).",
        "When running LP on the three datasets, we removed the features with occurrence frequency (counted in both training set and test set) less than 3 times.",
        "We investigated two distance measures for LP: cosine similarity and Jensen-Shannon (JS) divergence (Lin, 1991).",
        "For the three datasets, we constructed connected graphs following (Zhu et al., 2003): two instances u, v will be connected by an edge if u is among v’s k nearest neighbors, or if v is among u’s k nearest neighbors as measured by cosine or JS distance measure.",
        "For “interest” and “line” corpora, k is 10 (following (Zhu et al., 2003)), while for SENSEVAL-3 data, k is 5 since the size of dataset for each word in SENSEVAL-3 is much less than that of “interest” and “line” datasets."
      ]
    },
    {
      "heading": "4.2 Experiment 1: LP vs. SVM",
      "text": [
        "In this experiment, we evaluated LP and SVM 3 on the data of English lexical sample task in SENSEVAL-3.",
        "We used l examples from training set as labeled data, and the remaining training examples and all the test examples as unlabeled data.",
        "For each labeled set size 1, we performed 20 trials.",
        "In each trial, we randomly sampled l labeled examples for each word from training set.",
        "If any sense was absent from the sampled labeled set, we redid the sampling.",
        "We conducted experiments with different values of 1, including 1% × Nw,train� 10% ×",
        "of examples in training set of word w).",
        "SVM and LP were evaluated using accuracy 4 (fine-grained score) on test set of SENSEVAL-3.",
        "We conducted paired t-test on the accuracy figures for each value of 1.",
        "Paired t-test is not run when percentage= 100%, since there is only one paired accuracy figure.",
        "Paired t-test is usually used to estimate the difference in means between normal populations based on a set of random paired observations.",
        "{≪, ≫}, {<, >}, and ∼ correspond to p-value ≤ 0.01, (0.01, 0.05], and > 0.05 respectively.",
        "≪(or ≫) means that the performance of LP is significantly better (or significantly worse) than SVM.",
        "< (or >) means that the performance of LP is better (or worse) than SVM.",
        "∼ means that the performance of LP is almost as same as SVM.",
        "Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data.",
        "It also lists the official results of baseline method and top 3 systems in ELS task of SENSEVAL-3.",
        "From Table 1, we see that with small labeled dataset (percentage of labeled data ≤ 10%), LP performs significantly better than SVM.",
        "When the percentage of labeled data increases from 50% to 75%, the performance of LPjS and SVM become almost same, while LP�osine performs significantly worse than SVM."
      ]
    },
    {
      "heading": "4.3 Experiment 2: LP vs. Bootstrapping",
      "text": [
        "Li and Li (2004) used “interest” and “line” corpora as test data.",
        "For the word “interest”, they used its four major senses.",
        "For comparison with their results, we took reduced “interest” corpus (constructed by retaining four major senses) and complete “line” corpus as evaluation data.",
        "In their algorithm, c is the number of senses of ambiguous word, and b (b = 15) is the number of examples added into classified data for each class in each iteration of bootstrapping.",
        "c × b can be considered as the size of initial labeled data in their bootstrapping algorithm.",
        "We ran LP with 20 trials on reduced “interest” corpus and complete “line” corpus.",
        "In each trial, we randomly sampled b labeled examples for each sense of “interest” or “line” as labeled data.",
        "The rest served as both unlabeled data and test data.",
        "Table 2 summarizes the average accuracies of LP on the two corpora.",
        "It also lists the accuracies of monolingual bootstrapping algorithm (MB), bilingual bootstrapping algorithm (BB) on “interest” and “line” corpora.",
        "We can see that LP performs much better than MB-D and MB-B on both “interest” and “line” corpora, while the performance of LP is comparable to BB on these two corpora."
      ]
    },
    {
      "heading": "4.4 An Example: Word “use”",
      "text": [
        "For investigating the reason for LP to outperform SVM and monolingual bootstrapping, we used the data of word “use” in English lexical sample task of SENSEVAL-3 as an example (totally 26 examples in training set and 14 examples in test set).",
        "For data",
        "test set by 1NN (accuracy= 14 = 42.9% ), (f) classification result on SENSEVAL-3 training set and test set by LP (accuracy= 14 = 71.4% ).",
        "visualization, we conducted unsupervised nonlinear dimensionality reduction5 on these 40 feature vectors with 210 dimensions.",
        "Figure 3 (a) shows the dimensionality reduced vectors in two-dimensional space.",
        "We randomly sampled only one labeled example for each sense of word “use” as labeled data.",
        "The remaining data in training set and test set served as unlabeled data for bootstrapping and LP.",
        "All of these three algorithms are evaluated using accuracy on test set.",
        "From Figure 3 (c) we can see that SVM misclassi",
        "fied many examples from class + into class × since using only features occurring in training set can not reveal the intrinsic structure in full dataset.",
        "For comparison, we implemented monolingual bootstrapping with kNN (k=1) as base classifier.",
        "The parameter b is set as 1.",
        "Only b unlabeled examples nearest to labeled examples and with the distance less than dieter-class (the minimum distance between labeled examples with different sense tags) will be added into classified data in each iteration till no such unlabeled examples can be found.",
        "Firstly we ran this monolingual bootstrapping on this dataset to augment initial labeled data.",
        "The resulting classified data is shown in Figure 3 (d).",
        "Then a 1NN model was learned on this classified data and we used this model to perform classification on the remaining unlabeled data.",
        "Figure 3 (e) reports the final classification result by this 1NN model.",
        "We can see that bootstrapping does not perform well since it is susceptible to small noise in dataset.",
        "For example, in Figure 3 (d), the unlabeled example B 6 happened to be closest to labeled example A, then 1NN model tagged example B with label ⋄.",
        "But the correct label of B should be + as shown in Figure 3 (b).",
        "This error caused misclassification of other unlabeled examples that should have label +.",
        "In LP, the label information of example C can travel to B through unlabeled data.",
        "Then example A will compete with C and other unlabeled examples around B when determining the label of B.",
        "In other words, the labels of unlabeled examples are determined not only by nearby labeled examples, but also by nearby unlabeled examples.",
        "Using this classification strategy achieves better performance than the local consistency based strategy adopted by SVM and bootstrapping."
      ]
    },
    {
      "heading": "4.5 Experiment 3: LPcosine vs. LPjs",
      "text": [
        "Table 3 summarizes the performance comparison between LPcosine and LPjs on three datasets.",
        "We can see that on SENSEVAL-3 corpus, LPjs per6 In the two-dimensional space, example B is not the closest example to A.",
        "The reason is that: (1) A is not close to most of nearby examples around B, and B is not close to most of nearby examples around A; (2) we used Isomap to maximally preserve the neighborhood information between any example and all other examples, which caused the loss of neighborhood information between a few example pairs for obtaining a globally optimal solution.",
        "LPjs and the results of three model selection criteria are reported in following two tables.",
        "In the lower table, < (or >) means that the average value of function H(Q.j,) is lower (or higher) than H(Q js), and it will result in selecting cosine (or JS) as distance measure.",
        "Qcosine (or Q js) represents a matrix using cosine similarity (or JS divergence).",
        "√ and × denote correct and wrong prediction results respectively, while ◦ means that any prediction is acceptable.",
        "forms significantly better than LPcoaine, but their performance is almost comparable on “interest” and “line” corpora.",
        "This observation motivates us to automatically select a distance measure that will boost the performance of LP on a given dataset.",
        "Cross-validation on labeled data is not feasible due to the setting of semi-supervised learning (l ≪ u).",
        "In (Zhu and Ghahramani, 2002; Zhu et al., 2003), they suggested a label entropy criterion H(YU) for model selection, where Y is the label matrix learned by their semi-supervised algorithms.",
        "The intuition behind their method is that good parameters should result in confident labeling.",
        "Entropy on matrix W (H(W)) is a commonly used measure for unsupervised feature selection (Dash and Liu, 2000), which can be considered here.",
        "Another possible criterion for model selection is to measure the entropy of c × c interclass distance matrix D calculated on labeled data (denoted as H(D)), where Di represents the average distance between the i-th class and the j-th class.",
        "We will investigate three criteria, H(D), H(W) and H(YU), for model selection.",
        "The distance measure can be automatically selected by minimizing the average value of function H(D), H(W) or H(YU) over 20 trials.",
        "Let Q be the M × N matrix.",
        "Function H(Q) can measure the entropy of matrix Q, which is defined as (Dash and Liu, 2000):",
        "where a is positive constant.",
        "The possible value of a is − In o.s , where I = MN EZ j QZj .",
        "5 is introduced for normalization of matrix Q.",
        "For SENSEVAL3 data, we calculated an overall average score of",
        "umber of examples in test set of word w. H(D), I(W) and H(YU) can be obtained by replacing Q with D, W and YU respectively.",
        "Table 3 reports the automatic prediction results of these three criteria.",
        "From Table 3, we can see that using H(W) an consistently select the optimal distance measure when the performance gap between LPcoaine and LPjs is very large (denoted by ≪or ≫).",
        "But H(D) and H(YU) fail to find the optimal distance measure when only very few labeled examples are available (percentage of labeled data ≤ 10%).",
        "H(W) measures the separability of matrix W. Higher value of H(W) means that distance measure decreases the separability of examples in full dataset.",
        "Then the boundary between clusters is obscured, which makes it difficult for LP to locate this boundary.",
        "Therefore higher value of H(W) results in worse performance of LP.",
        "When labeled dataset is small, the distances between classes can not be reliably estimated, which results in unreliable indication of the separability of examples in full dataset.",
        "This is the reason that H(D) performs poorly on SENSEVAL-3 corpus when the percentage of labeled data is less than 25%.",
        "For H(YU), small labeled dataset can not reveal intrinsic structure in data, which may bias the estimation of YU.",
        "Then labeling confidence (H(YU)) can not properly indicate the performance of LP.",
        "This may interpret the poor performance of H(YU) on SENSEVAL-3 data when percentage ≤ 25%."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper we have investigated a label propagation based semi-supervised learning algorithm for WSD, which fully realizes a global consistency assumption: similar examples should have similar labels.",
        "In learning process, the labels of unlabeled examples are determined not only by nearby labeled examples, but also by nearby unlabeled examples.",
        "Compared with semi-supervised WSD methods in the first and second categories, our corpus based method does not need external resources, including WordNet, bilingual lexicon, aligned parallel corpora.",
        "Our analysis and experimental results demonstrate the potential of this cluster assumption based algorithm.",
        "It achieves better performance than SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping and comparable to bilingual bootstrapping.",
        "Finally we suggest an entropy based method to automatically identify a distance measure that can boost the performance of LP algorithm on a given dataset.",
        "It has been shown that one sense per discourse property can improve the performance of bootstrapping algorithm (Li and Li, 2004; Yarowsky, 1995).",
        "This heuristics can be integrated into LP algorithm by setting weight WZj = 1 if the i-th and j-th instances are in the same discourse.",
        "In the future we may extend the evaluation of LP algorithm and related cluster assumption based algorithms using more benchmark data for WSD.",
        "Another direction is to use feature clustering technique to deal with data sparseness and noisy feature problem.",
        "Acknowledgements We would like to thank anonymous reviewers for their helpful comments.",
        "Z.Y.",
        "Niu is supported by A*STAR Graduate Scholarship."
      ]
    }
  ]
}
