{
  "info": {
    "authors": [
      "Trevor Cohn",
      "Philip Blunsom"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W05-0622",
    "title": "Semantic Role Labelling With Tree Conditional Random Fields",
    "url": "https://aclweb.org/anthology/W05-0622",
    "year": 2005
  },
  "references": [
    "acl-N03-1028",
    "acl-P05-1002",
    "acl-W03-0430",
    "acl-W04-2419",
    "acl-W04-3212",
    "acl-W05-0620"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper we apply conditional random fields (CRFs) to the semantic role labelling task.",
        "We define a random field over the structure of each sentence’s syntactic parse tree.",
        "For each node of the tree, the model must predict a semantic role label, which is interpreted as the labelling for the corresponding syntactic constituent.",
        "We show how modelling the task as a tree labelling problem allows for the use of efficient CRF inference algorithms, while also increasing generalisation performance when compared to the equivalent maximum entropy classifier.",
        "We have participated in the CoNLL-2005 shared task closed challenge with full syntactic information."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The semantic role labelling task (SRL) involves identifying which groups of words act as arguments to a given predicate.",
        "These arguments must be labelled with their role with respect to the predicate, indicating how the proposition should be semantically interpreted.",
        "We apply conditional random fields (CRFs) to the task of SRL proposed by the CoNLL shared task 2005 (Carreras and M`arquez, 2005).",
        "CRFs are undirected graphical models which define a conditional distribution over labellings given an observation (Lafferty et al., 2001).",
        "These models allow for the use of very large sets of arbitrary, overlapping and non-independent features.",
        "CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003; Cohn et al., 2005), part-of-speech (PoS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of table data (Pinto et al., 2003), among other tasks.",
        "While CRFs have not been used to date for SRL, their close cousin, the maximum entropy model has been, with strong generalisation performance (Xue and Palmer, 2004; Lim et al., 2004).",
        "Most CRF implementations have been specialised to work with chain structures, where the labels and observations form a linear sequence.",
        "Framing SRL as a linear tagging task is awkward, as there is no easy model of adjacency between the candidate constituent phrases.",
        "Our approach simultaneously performs both constituent selection and labelling, by defining an undirected random field over the parse tree.",
        "This allows the modelling of interactions between parent and child constituents, and the prediction of an optimal argument labelling for all constituents in one pass.",
        "The parse tree forms an acyclic graph, meaning that efficient exact inference in a CRF is possible using belief propagation."
      ]
    },
    {
      "heading": "2 Data",
      "text": [
        "The data used for this task was taken from the Propbank corpus, which supplements the Penn Treebank with semantic role annotation.",
        "Full details of the data set are provided in Carreras and M`arquez (2005)."
      ]
    },
    {
      "heading": "2.1 Data Representation",
      "text": [
        "nodes in the trees were relabelled with a semantic role label indicating how their corresponding syntactic constituent relates to each predicate, as shown in Figure 1.",
        "The role labels are shown as subscripts in the figure, and both the syntactic categories and the words at the leaves are shown for clarity only – these were not included in the tree.",
        "Additionally, the dashed lines show those edges which were pruned, following Xue and Palmer (2004) – only nodes which are siblings to a node on the path from the verb to the root are included in the tree.",
        "Child nodes of included prepositional phrase nodes are also included.",
        "This reduces the size of the resultant tree whilst only very occasionally excluding nodes which should be labelled as an argument.",
        "The tree nodes were labelled such that only argument constituents received the argument label while all argument children were labelled as outside, O.",
        "Where there were parse errors, such that no constituent exactly covered the token span of an argument, the smaller subsumed constituents were all given the argument label.",
        "We experimented with two alternative labelling strategies: labelling a constituent’s children with a new ‘inside’ label, and labelling the children with the parent’s argument label.",
        "In the figure, the IN and NP children of the PP would be affected by these changes, both receiving either the inside I label or AM-LOC label under the respective strategies.",
        "The inside strategy performed nearly identically to the standard (outside) strategy, indicating that either the model cannot reliably predict the inside argument, or that knowing that the children of a given node are inside an argument is not particularly useful in predicting its label.",
        "The second (duplication) strategy performed extremely poorly.",
        "While this allowed the internal argument nodes to influence their ancestor towards a particular labelling, it also dramatically increased the number of nodes given an argument label.",
        "This lead to spurious over-prediction of arguments.",
        "The model is used for decoding by predicting the maximum probability argument label assignment to each of the unlabelled trees.",
        "When these predictions were inconsistent, and one argument subsumed another, the node closest to the root of the tree was deemed to take precedence over its descendants."
      ]
    },
    {
      "heading": "3 Model",
      "text": [
        "We define a CRF over the labelling y given the observation tree x as: where C is the set of cliques in the observation tree, Ak are the model’s parameters and fk(�) is the feature function which maps a clique labelling to a vector of scalar values.",
        "The function Z(•) is the normalising function, which ensures that p is a valid probability distribution.",
        "This can be restated as:",
        "where C1 are the vertices in the graph and C2 are the maximal cliques in the graph, consisting of all (parent, child) pairs.",
        "The feature function has been split into g and h, each dealing with one and two node cliques respectively.",
        "Preliminary experimentation without any pairwise features (h), was used to mimic a simple maximum entropy classifier.",
        "This model performed considerably worse than the model with the pairwise features, indicating that the added complexity of modelling the parent-child interactions provides for more accurate modelling of the data.",
        "The log-likelihood of the training sample was optimised using limited memory variable metric (LMVM), a gradient based technique.",
        "This required the repeated calculation of the log-likelihood and its derivative, which in turn required the use of dynamic programming to calculate the marginal probability of each possible labelling of every clique using the sum-product algorithm (Pearl, 1988)."
      ]
    },
    {
      "heading": "4 Features",
      "text": [
        "As the conditional random field is conditioned on the observation, it allows feature functions to be defined over any part of the observation.",
        "The tree structure requires that features incorporate either a node labelling or the labelling of a parent and its",
        "child.",
        "We have defined node and pairwise clique features using data local to the corresponding syntactic node(s), as well as some features on the predicate itself.",
        "Each feature type has been made into binary feature functions g and h by combining (feature type, value) pairs with a label, or label pair, where this combination was seen at least once in the training data.",
        "The following feature types were employed, most of which were inspired by previous works: Basic features: {Head word, head PoS, phrase syntactic category, phrase path, position relative to the predicate, surface distance to the predicate, predicate lemma, predicate token, predicate voice, predicate sub-categorisation, syntactic frame}.",
        "These features are common to many SRL systems and are described in Xue and Palmer (2004).",
        "Context features {Head word offirst NP in preposition phrase, left and right sibling head words and syntactic categories, first and last word in phrase yield and their PoS, parent syntactic category and head word}.",
        "These features are described in Pradhan et al.",
        "(2005).",
        "Common ancestor of the verb The syntactic category of the deepest shared ancestor of both the verb and node.",
        "Feature conjunctions The following features were",
        "Default feature This feature is always on, which allows the classifier to model the prior probability distribution over the possible argument labels.",
        "Joint features These features were only defined over pairwise cliques: {whether the parent and child head words do not match, parent syntactic category + and child syntactic category, parent relative position + child relative position, parent relative position + child relative position + predicate PoS + predicate lemma}."
      ]
    },
    {
      "heading": "5 Experimental Results",
      "text": [
        "The model was trained on the full training set after removing unparsable sentences, yielding 90,388 predicates and 1,971,985 binary features.",
        "A Gaussian prior was used to regularise the model, with variance Q2 = 1.",
        "Training was performed on a 20 node PowerPC cluster, consuming a total of 62Gb of RAM and taking approximately 15 hours.",
        "Decoding required only 3Gb of RAM and about 5 minutes for the 3,228 predicates in the development set.",
        "Results are shown in Table 1."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "Conditional random fields proved useful in modelling the semantic structure of text when provided with a parse tree.",
        "Our novel use of a tree structure derived from the syntactic parse, allowed for parent-child interactions to be accurately modelled, which provided an improvement over a standard maximum entropy classifier.",
        "In addition, the parse constituent structure proved quite appropriate to the task, more so than modelling the data as a sequence of words or chunks, as has been done in previous approaches."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would both like to thank our research supervisor Steven Bird for his comments and feedback on this work.",
        "The research undertaken for this paper was supported by an Australian Postgraduate Award scholarship, a Melbourne Research Scholarship and a Melbourne University Postgraduate Overseas Research Experience Scholarship."
      ]
    }
  ]
}
