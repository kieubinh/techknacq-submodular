{
  "info": {
    "authors": [
      "Jun'ichi Kazama",
      "Kentaro Torisawa"
    ],
    "book": "Human Language Technology Conference and Empirical Methods in Natural Language Processing",
    "id": "acl-H05-1018",
    "title": "Speeding Up Training With Tree Kernels for Node Relation Labeling",
    "url": "https://aclweb.org/anthology/H05-1018",
    "year": 2005
  },
  "references": [
    "acl-J02-3001",
    "acl-P03-1004",
    "acl-P04-1016",
    "acl-P04-1043",
    "acl-P04-1054",
    "acl-W03-1012",
    "acl-W04-2416",
    "acl-W04-3239",
    "acl-W05-0620"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a method for speeding up the calculation of tree kernels during training.",
        "The calculation of tree kernels is still heavy even with efficient dynamic programming (DP) procedures.",
        "Our method maps trees into a small feature space where the inner product, which can be calculated much faster, yields the same value as the tree kernel for most tree pairs.",
        "The training is sped up by using the DP procedure only for the exceptional pairs.",
        "We describe an algorithm that detects such exceptional pairs and converts trees into vectors in a feature space.",
        "We propose tree kernels on marked labeled ordered trees and show that the training of SVMs for semantic role labeling using these kernels can be sped up by a factor of several tens."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many NLP tasks such as parse selection and tagging can be posed as the classification of labeled ordered trees.",
        "Several tree kernels have been proposed for building accurate kernel-based classifiers (Collins and Duffy, 2001; Kashima and Koyanagi, 2002).",
        "They have the following form in common.",
        "where Si is a possible subtree, #si (Tj) is the number of times Si is included in Tj, and W(Si) is the weight of Si.",
        "That is, tree kernels are inner products in a subtree feature space where a tree is mapped to vector V (Ti) = (,/W(Si):ff Si (m) i.",
        "With tree kernels we can take global structures into account, while alleviating overfitting with kernel-based learning methods such as support vector machines (SVMs) (Vapnik, 1995).",
        "Previous studies (Collins and Duffy, 2001; Kashima and Koyanagi, 2002) showed that although it is difficult to explicitly calculate the inner product in Eq.",
        "(1) because we need to consider an exponential number of possible subtrees, the tree kernels can be computed in O (I Ti I I T2 I) time by using dynamic programming (DP) procedures.",
        "However, these DP procedures are time-consuming in practice.",
        "In this paper, we present a method for speeding up the training with tree kernels.",
        "Our target application is node relation labeling, which includes NLP tasks such as semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Moschitti, 2004; Ha-cioglu et al., 2004).",
        "For this purpose, we designed kernels on marked labeled ordered trees and derived O (I Ti I I T2 I) procedures.",
        "However, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast.",
        "Our speed-up method is based on the observation that very few pairs in the training set have a great many common subtrees (we call such pairs malicious pairs) and most pairs have a very small number of common subtrees.",
        "This leads to a drastic variance in kernel values, e.g., when W(Si) = 1.",
        "We thus call this property of data unbalanced similarity.",
        "Fast calculation based on the inner product is possible for non-malicious pairs since we can convert the trees into vectors in a space of a small subset of all subtrees.",
        "We can speed up the training by using the DP procedure only for the rare malicious pairs.",
        "We developed the FREQTM algorithm, a modification of the FREQT algorithm (Asai et al., 2002), to detect the malicious pairs and efficiently convert trees into vectors by enumerating only the subtrees actually needed (feature subtrees).",
        "The experiments demonstrated that our method makes the training of SVMs for the SRL task faster by a factor of several tens, and that it enables the performance of the kernels to be assessed in detail."
      ]
    },
    {
      "heading": "2 Kernels for Labeled Ordered Trees",
      "text": [
        "The tree kernels proposed so far differ in how subtree inclusion is defined.",
        "For instance, Kashima and Koyanagi (2002) used the following definition.",
        "DEFINITION 2.1 5 is included in Tiff there exists a one-to-one function 0 from a node of 5 to a node of such that (i) pa(0(nz)) = 0 (pa (nz)) (pa(nz) returns the parent ofnode n), (ii)0(nz) r 0(nj)iff nz r nj (nz r nj means that nz is an elder sibling ofnj), and (iii) 1(0(nz)) = l(nz) (a(nz) returns the label of n).",
        "We refer to the tree kernel based on this definition as Kj.",
        "Collins and Duffy (2001) used a more restrictive definition where the preservation of CFG productions, i.e., nc(o(nz)) = nc(nz) if nc(nz) > 0 (nc(nz) is the number of children of nz), is required in addition to the requirements in Definition 2.1.",
        "We refer to the tree kernel based on this definition as K. It is pointed that extremely unbalanced kernel values cause overfitting.",
        "Therefore, Collins and Duffy (2001) used W(5z) = A(# of productions in 5z), and Kashima and Koyanagi (2002) used W(Si) _ alszl, where A (0 < A < 1) is a factor to alleviate the unbalance by penalizing large subtrees.",
        "To calculate the tree kernels efficiently, Collins and Duffy (2001) presented an O (1 Ti I IT2 1) DP procedure for K. Kashima and Koyanagi (2002) presented one for K1.",
        "The point of these procedures is that Eq.",
        "(1) can be transformed:",
        "where #s,(Tj A nk) is the number of times 5z is included in Tj with 0 (root (5z)) = nk.",
        "C(ni, n2) can then be calculated recursively from those of the children of ni and n2."
      ]
    },
    {
      "heading": "3 Kernels for Marked Labeled Ordered Trees for Node Relation Labeling",
      "text": []
    },
    {
      "heading": "3.1 Node Relation Labeling",
      "text": [
        "The node relation labeling finds relations among nodes in a tree.",
        "Figure 1 illustrates the concept of node relation labeling with the SRL task as an example.",
        "A0, A1, and AM-LOC are the semantic roles",
        "of the arguments of the verb “see (saw)”.",
        "We represent an argument by the node that is the highest in the parse tree among the nodes that exactly cover the words in the argument.",
        "The node for the verb is determined similarly.",
        "For example, the node labeled “PP” represents the AM-LOC argument “in the sky”, and the node labeled “VBD” represents the verb “see (saw)”.",
        "We assume that there is a two-node relation labeled with the semantic role (represented by the arrow in the figure) between the verb node and the argument node."
      ]
    },
    {
      "heading": "3.2 Kernels on Marked Labeled Ordered Trees",
      "text": [
        "We define a marked labeled ordered tree as a labeled ordered tree in which each node has a mark in addition to a label.",
        "We use m(nz) to denote the mark of node nz.",
        "If nz has no mark, m(nz) returns the special mark no-mark.",
        "We also use the function marked(nz), which returns true iff m(nz) is not no-mark.",
        "We can encode a k-node relation by using k distinct marks.",
        "Figure 2 shows how the semantic roles illustrated in Figure 1 can be encoded using marked labeled ordered trees.",
        "We used the mark *1 to represent the verb node and *2 to represent the argument node.",
        "The node relation labeling task can be posed as the classification of marked trees that returns +1 when the marks encode the correct relation and – 1",
        "otherwise.",
        "To enable such classification, we need tree kernels that take into account the node marks.",
        "We thus propose mark-aware tree kernels formulated as follows.",
        "where marked(Si) returns true iff marked(ni) _ true for at least one node in tree Si.",
        "In these kernels, we require m(O(nz)) = m(ni) in addition to 1(0(ni)) = l(ni) for subtree Si to be regarded as included in tree Tj.",
        "In other words, these kernels treat lm(ni) - (l(ni), m(ni)) as the new label of node ni and sum only over subtrees that have at least one marked node.",
        "We refer to the marked version of Klo as Kio and the marked version of Ke as Kr�.",
        "We can derive O (ITS I I T2 I) DP procedures for the above kernels as well.",
        "Algorithm 3.1 shows the DP procedure for Kio, which is derived by extending the DP procedure for Klo (Kashima and Koyanagi, 2002).",
        "The key is the use of Cr(n1, n2), which stores the sum over only marked subtrees, and its recursive calculation using C(ni, n2) and Cr(ni, n2) (B).",
        "An O(ITi I IT2I) procedure for Kr can also be derived by extending (Collins and Duffy, 2001).",
        "Table 1: Malicious and non-malicious pairs in the 1k data (3,136 trees) used in Sec. 5.2.",
        "We used K(Ti, Tj) = 104 with A = 1 as the threshold for maliciousness.",
        "(A): pairs (i, i).",
        "(B): pairs from the same sentence except (i, i).",
        "(C): pairs from different sentences.",
        "Some malicious pairs are from different but similar sentences, which are difficult to detect."
      ]
    },
    {
      "heading": "4.1 Basic Idea",
      "text": [
        "As mentioned, we define two types of tree pairs: malicious and non-malicious pairs.",
        "Table 1 shows how these two types of pairs are distributed in an actual training set.",
        "There is a clear distinction between malicious and non-malicious pairs, and we can exploit this property to speed up the training.",
        "We define subset F = {FZ} (feature subtrees), which includes only the subtrees that appear as a common included subtree in the non-malicious pairs.",
        "We convert a tree to feature vector V (Tj) (-,1W(Fj)#F. (Tj)) Z using only .F.",
        "Then we use a procedure that chooses the DP procedure or the inner product procedure depending on maliciousness:",
        "This procedure returns the same value as the original calculation.",
        "Naively, if I V (TZ) I (the number of feature subtrees such that #Fi (Ti) 7� 0) is small enough, we can expect a speed-up because the cost of calculating the inner product is O(I V (Ti) I + IV (Tj) 1).",
        "However, since IV (Ti) I might increase as the training set becomes larger, we need a way to scale the speed-up to large data.",
        "In most kernel-based methods, such as SVMs, we actually need to calculate the kernel values with all the training examples for a given example Ti: KS(Ti) = {K(TZ, Ti), ... , K(TZ, TL)}, where L is the number of training examples.",
        "Using occurrence pattern OP(FZ) = I (k, #F, (Tk)) I #Fi (Tk) 74 0} pre",
        "pared beforehand, we can calculate KS(Ti) efficiently (Algorithm 4.1).",
        "A similar technique was used in (Kudo and Matsumoto, 2003a) to speed up the calculation of inner products.",
        "We can show that the per-pair cost of Algorithm 4.1 is O (ciQ + rmc2 ITZ I ITj I), where Q is the average number of common feature subtrees in a tree pair, rm is the rate of malicious pairs, ci and c2 are the constant factors for vector operations and DP operations.",
        "This cost is independent of the number of training examples.",
        "We expect from our observations that both Q and rm are very small and that ci « c2."
      ]
    },
    {
      "heading": "4.2 Feature Subtree Enumeration with Malicious Pair Detection",
      "text": [
        "To detect malicious pairs and enumerate feature subtrees .F (and to convert each tree to a feature vector), we developed an algorithm based on the FREQT algorithm (Asai et al., 2002).",
        "The FREQT algorithm can efficiently enumerate subtrees that are included (Definition 2.1) in more than a pre-specified number of trees in the training examples by generating candidate subtrees using right most expansions (RMEs).",
        "FREQT-based algorithms have recently been used in methods that treat subtrees as features (Kudo and Matsumoto, 2004; Kudo and Matsumoto, 2003b).",
        "To develop the algorithm, we made the definition of maliciousness more search-oriented since it is costly to check for maliciousness based on the exact number of common subtrees or the kernel values (i.e., by using the DP procedure for all L2 pairs).",
        "Whatever definition we use, the correctness is preserved as long as we do not fail to enumerate the subtrees that appear in the pairs we consider non-malicious.",
        "First, we consider pairs (i, i) to always be malicious.",
        "Then, we use a FREQT search that enumerates the subtrees that are included in at least two trees as a basis.",
        "Next, we modify FREQT so that it stops the search if candidate subtree FZ is too large (larger than size D, e.g., 20), and we regard the pairs of the trees where Fi appears as malicious because having a large subtree in common implies having a",
        "• occ(Fi) returns occurrence list of Fi whose element (j, n) indicates that Fi appears in Tj and that n (of Tj ) is the node added to generated Fi in Tj by the RME (n works as the position of Fi in Tj ).",
        "• suP(Fi) returns the IDs of distinct trees in occ(Fi).",
        "• malicious(Fi) returns true iff all pairs in sup(Fi) are already registered in the set of malicious pairs, M. (Cur",
        "rently, this returns false if I sup(F;) I > M where M is the maximum support size of the malicious subtrees so far.",
        "We will remove this check since we found that it did not affect efficiency so much.)",
        "• RME(Fi, Tj, n) is a set of subtrees generated by RMEs of Fi in T,; (permitted when previously expanded node to generate Fi is n).",
        "possibly exponential number of subtrees of that subtree in common.",
        "Although this test is heuristic and conservative in that it ignores the shape and marks of a tree, it works fine empirically.",
        "Algorithm 4.2 is our algorithm, which we call FREQTM.",
        "The differences from FREQT are underlined.",
        "Table 2 summarizes the functions used.",
        "To make the search efficient, pruning is performed as follows (see also Figure 3).",
        "The basic idea behind is that if malicious(FZ) is true then malicious(Fk) is also true for Fk that is expanded from FZ by an",
        "RME since sup(Fk) C sup(FZ).",
        "This means we do not need to enumerate Fi nor any descendant of Fi.",
        "• (P) Once IFz I > D and the malicious pairs are registered, we stop searching further.",
        "• (P1) If the search from Fk (expanded from FZ) found a malicious subtree and if Isup(FZ)I _ I sup(Fk)1, we stop the search from any other subtree F,,t (expanded from FZ) since we can prove that malicious(Fm) = true without actually testing it (proof omitted).",
        "• (P2) If malicious(Fk) = true, we prune the search from Fk.",
        "To prune even when malicious(Fk) becomes true as a result of succeeding searches, we first run a search only for detecting malicious pairs (see (PC)).",
        "• (S) We stop searching when the occurrence list becomes too long (larger than threshold R) since it causes a severe search slowdown.",
        "Note that we use a depth-first version of FREQT as a basis to first find the largest subtrees and to detect malicious pairs at early points in the search.",
        "Enumeration of unnecessary subtrees is avoided because the registration of subtrees is performed at the postorder position (F).",
        "The conversion to vectors is performed by assigning an ID to subtree FZ when registering it at (F) and distributing the ID to all the examples in occ(FZ).",
        "Finally, D should be large enough to make rm sufficiently small but should not be so large that too many feature subtrees are enumerated.",
        "We expect that the cost of FREQTM is offset by the faster training, especially when training on the same data is repeatedly performed as in the tuning of hyperparameters.",
        "For Kr, we use a similar search procedure.",
        "However, the RME is modified so that all the children of a CFG production are expanded at once.",
        "Although the modification is not trivial, we omit the explanation due to space limitations."
      ]
    },
    {
      "heading": "4.3 Feature Compression",
      "text": [
        "Additionally, we use a simple but effective feature compression technique to boost speed-up.",
        "The idea is simple: feature subtrees FZ and Fj can be treated as one feature fk, with weight W (fk) = W (Fi) + W(Fj) if OP(Fi) = OP(Fj).",
        "This drastically reduces the number of features.",
        "Although this is sim",
        "ilar to finding closed and maximal subtrees (Chi et al., 2004), it is easy to implement since we need only the occurrence pattern, OP(FZ), which is easily obtained from occ(FZ) in the FREQTM search."
      ]
    },
    {
      "heading": "4.4 Alternative Methods",
      "text": [
        "Vishwanathan and Smola (2004) presented the O (IT1 + T2 1) procedure that exploits suffix trees to speed up the calculation of tree kernels.",
        "However, it can be applied to only a few types of subtrees that can be represented as a contiguous part in a string representation of a tree.",
        "Therefore, neither Klo nor K,' can be sped up by using this procedure.",
        "Another method is to change an inner loop, such as (B) in Algorithm 3.",
        "1, so that it iterates only over nodes in T2 that have l(nl).",
        "We use this as the baseline for comparison, since we found that this is about two times faster than the standard implementation.",
        "1"
      ]
    },
    {
      "heading": "4.5 Remaining Problem",
      "text": [
        "Note that the method described here cannot speed up classification, since the converted vectors are valid only for calculating the kernels between trees in the training set.",
        "However, when we classify the same trees repeatedly, we can convert the trees in the training set and the classified trees at the same time and use the obtained vectors for classification."
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": [
        "We first evaluated the speed-up by our method for the semantic role labeling (SRL) task.",
        "We then demonstrated that the speed-up method enables a detailed comparison of Klo and K,' for the SRL task."
      ]
    },
    {
      "heading": "5.1 Setting",
      "text": [
        "We used the data set provided for the CoNLL05 SRL shared task (Carreras and M`arquez, 2005).",
        "We used only the training part and divided it into our training, development, and testing sets (23,899, 7,966, and 7,967 sentences, respectively).",
        "As the tree structure, we used the output of Collins’ parser (with WSJ-style non-terminals) provided with the data set.",
        "We also used POS tags by inserting the nodes labeled by POS tags above the word nodes.",
        "The average number of nodes in a tree was about 82.",
        "We ignored any arguments (and verbs) that did not match any node in the tree (the rate of such cases was about 3.5%).",
        "2 The words were lowercased.",
        "We used TinySVM3 as the implementation of SVM and added our tree kernels, Kio and K,.",
        "We implemented FREQTM based on the implementation of FREQT by Kudo .4 We normalized the kernel values: K(TZ,Tj)/VK(TZ,TZ) x K(Tj,Tj).",
        "Note that this normalization barely affected the training time since we can calculate K(Ti, TZ) beforehand.",
        "We assumed two-step labeling where we first find the argument node and then we determine the label by using a binary classifier for each semantic role.",
        "In this experiment, we focused on the performance for the classifiers in the latter step.",
        "We used the marked labeled ordered tree that encoded the target role as a positive example and the trees that encoded other roles of the verb in the same sentence as negative examples.",
        "We trained and evaluated the classifiers using the examples generated as above.",
        "5 2 This was caused by parse errors, which can be solved by using more accurate parsers, and by bracketing inconsistencies between parser outputs and SRL annotations (e.g., phrasal verbs), many of which can be avoided by using heuristic transformers."
      ]
    },
    {
      "heading": "5.2 Training Speed-up",
      "text": [
        "We calculated the statistics for the conversion by FREQTM and measured the speed-up in SVM training for semantic role A2, for various numbers of training examples.",
        "For FREQTM, we used D = 20 and R = 20.",
        "For SVM training, we used convergence tolerance 0.001 (-e option in TinySVM), soft margin cost C = 1.0 x 103 (-c), maximum number of iterations 105, kernel cache size 512 MB (- m), and decay factor A = 0.2 for the weight of each subtree.",
        "We compared the time with our fast method (Algorithm 4.1) with that with the DP procedure with the node lookup described in Section 4.4.",
        "Note that these two methods yield almost identical SVM models (there are very slight differences due to the numerical computation).",
        "The time was measured using a computer with 2.4-GHz Opterons.",
        "Table 3 shows the results for Klo and K�,.",
        "The proposed method made the SVM training substantially faster for both Kio and K�,.",
        "As we expected, the speed-up factor did not decrease even though I V increased as the amount of data increased.",
        "Note that FREQTM actually detected non-trivial malicious pairs such as those from very similar sentences in addition to trivial ones, e.g., (i, i).",
        "FREQTM conversion was much faster and the converted feature vectors were much shorter for K,, presumably because K,' restricts the subtrees more.",
        "The compression technique described in Section 4.3 greatly reduced the number of features.",
        "Without this compression, the storage requirement would be impractical.",
        "It also boosted the speed-up.",
        "For example, the training time with Klo for the size 1,000 data in Table 3 was 86.32 seconds without compression.",
        "This means that the compression boosted the",
        "speed-up by a factor of more than 5.",
        "The cost of FREQTM is much smaller than that of SVM training with DP.",
        "Therefore, our method is beneficial even if we train the SVM only once.",
        "To see how our method scales to large amounts of data, we plotted the time for the conversion and the SVM training w.r.t.",
        "data size on a log-log scale.",
        "As shown in Figure 4, the scaling factor was about 1.7 for the conversion time, 2.1 for SVM training with DP, and 2.0 for the proposed SVM training for Kio.",
        "For Kr, the factors were about 1.3, 2.1, and 2.0, respectively.",
        "Regardless of the method, the cost of SVM training was about O(L2), as reported in the literature.",
        "Although FREQTM also has a super-linear cost, it is smaller than that of SVM training.",
        "Therefore, the cost of SVM training will become a problem before the cost of FREQTM does.",
        "As we mentioned, the choice of D is a trade-off.",
        "Figure 5 shows the relationships between D and the time of conversion by FREQTM, the time of SVM training using the converted vectors, and the rate of malicious pairs, rm.",
        "We can see that the choice of D is more important in the case of Klo and that D = 20 used in our evaluation is not a bad choice."
      ]
    },
    {
      "heading": "5.3 Semantic Role Labeling",
      "text": [
        "We assessed the performance of Klo and Kr for semantic roles A1, A2, AM-ADV, and AM-LOC using our fast training method.",
        "We tuned soft margin cost C and A by using the development set (we used the technique described in Section 4.5 to enable fast classification of the development set).",
        "We experimented with two training set sizes (4,000 and 8,000).",
        "For each A (0.1, 0.15, 0.2, 0.25, and 0.30), we tested 40 different values of C (C E [2 ...103] for size 4,000 and C E [0.5 ...103] for size 8,000), and we evaluated the accuracy of the best setting for the test set.6 Fast training is crucial since the performance differs substantially depending on the values of these hyperparameters.",
        "Table 4 shows the results.",
        "The accuracies are shown by Fl.",
        "We can see that Kr o outperformed Kr in all cases, presumably because Kr allows only too restrictive subtrees and therefore causes data sparseness.",
        "In addition, as one would expect, larger training sets are beneficial."
      ]
    },
    {
      "heading": "6 Discussion",
      "text": [
        "The proposed speed-up method can also be applied to labeled ordered trees (e.g., for parse selection).",
        "However, the speed-up might be smaller since without node marks the number of subtrees increases while the DP procedure becomes simpler.",
        "On the other hand, the FREQTM conversion for marked labeled ordered trees might be made faster by exploiting the mark information for pruning.",
        "Although our method is not a complete solution in a classification setting, it might be in a clustering setting (in a sense it is training only).",
        "However, it is an open question whether unbalanced similarity, which is the key to our speed-up, is ubiquitous in NLP tasks and under what conditions our method scales better than the SVMs or other kernel-based methods.",
        "Several studies claim that learning using tree kernels and other convolution kernels tends to overfit and propose selecting or restricting features (Cumby and Roth, 2003; Suzuki et al., 2004; Kudo and Matsumoto, 2004).",
        "Sometimes, the classification becomes faster as a result (Suzuki et al., 2004; Kudo and Matsumoto, 2004).",
        "We do not disagree with these studies.",
        "The fact that small A values resulted in the highest accuracy in our experiment implies that too large subtrees are not so useful.",
        "However, since this tendency depends on the task, we need to assess the performance of full tree kernels for comparison.",
        "In this sense, our method is of great importance.",
        "Node relation labeling is a generalization of node",
        "marking where we determine the mark (tag) of a node.",
        "Kashima and Koyanagi (2002) dealt with this task by inserting the node representing the mark above the node to be tagged and classifying the transformed tree using SVMs with tree kernels such as K1.",
        "For the SRL task, Moschitti (2004) applied the tree kernel (K,) to tree fragments that are heuristically extracted to reflect the role of interest.",
        "For relation extraction, Culotta and Sorensen (2004) proposed a tree kernel that operates on only the smallest tree fragment including two entities for which a relation is assigned.",
        "Our kernels on marked labeled ordered trees differ in what subtrees are permitted.",
        "Although comparisons are needed, we think our kernels are intuitive and general.",
        "There are many possible structures for which tree kernels can be defined.",
        "Shen et al.",
        "(2003) proposed a tree kernel for LTAG derivation trees to focus only on linguistically meaningful structures.",
        "Culotta and Sorensen (2004) proposed a tree kernel for dependency trees.",
        "An important future task is to find suitable structures for each task (the SRL task in our case).",
        "Our speed-up method will be beneficial as long as there is unbalanced similarity."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We have presented a method for speeding up the training with tree kernels.",
        "Using the SRL task, we demonstrated that our speed-up method made the training substantially faster."
      ]
    }
  ]
}
