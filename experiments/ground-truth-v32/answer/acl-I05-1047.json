{
  "info": {
    "authors": [
      "Zhou GuoDong"
    ],
    "book": "Second International Joint Conference on Natural Language Processing: Full Papers",
    "id": "acl-I05-1047",
    "title": "A Chunking Strategy Towards Unknown Word Detection in Chinese Word Segmentation",
    "url": "https://aclweb.org/anthology/I05-1047",
    "year": 2005
  },
  "references": [
    "acl-P02-1060",
    "acl-P96-1041",
    "acl-W03-1721",
    "acl-W03-1722",
    "acl-W03-1727",
    "acl-W03-1730",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "text": [
        "Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613 zhougd@i2r.a-star.edu.sg",
        "Abstract.",
        "This paper proposes a chunking strategy to detect unknown words in Chinese word segmentation.",
        "First, a raw sentence is pre-segmented into a sequence of word atoms using a maximum matching algorithm.",
        "Then a chunking model is applied to detect unknown words by chunking one or more word atoms together according to the word formation patterns of the word atoms.",
        "In this paper, a discriminative Markov model, named Mutual Information Independence Model (MIIM), is adopted in chunking.",
        "Besides, a maximum entropy model is applied to integrate various types of contexts and resolve the data sparseness problem in MIIM.",
        "Moreover, an error-driven learning approach is proposed to learn useful contexts in the maximum entropy model.",
        "In this way, the number of contexts in the maximum entropy model can be significantly reduced without performance decrease.",
        "This makes it possible for further improving the performance by considering more various types of contexts.",
        "Evaluation on the PK and CTB corpora in the First SIGHAN Chinese word segmentation bakeoff shows that our chunking approach successfully detects about 80% of unknown words on both of the corpora and outperforms the best-reported systems by 8.1% and 7.1% in unknown word detection on them respectively."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Prior to any linguistic analysis of Chinese text, Chinese word segmentation is the necessary first step and one of major bottlenecks in Chinese information processing since a Chinese sentence is written in a continuous string of characters without obvious separators (such as blanks) between the words.",
        "During the past two decades, this research has been a hot topic in Chinese information processing [1-10].",
        "There exist two major problems in Chinese word segmentation: ambiguity resolution and unknown word detection.",
        "While n-gram modeling and/or word cooccurrence has been successfully applied to deal with the ambiguity problems [3, 5, 10, 12, 13], unknown word detection has become the major bottleneck in Chinese word segmentation.",
        "Currently, almost all Chinese word segmentation systems rely on a word dictionary.",
        "The problem is that when the words stored in the dictionary are insufficient, the system's performance will be greatly deteriorated by the presence of words that are unknown to the system.",
        "Moreover, manual maintenance of a dictionary is very tedious and time consuming.",
        "It is therefore important for a Chinese word segmentation system to identify unknown words from the text automatically.",
        "In literature, two categories of competing approaches are widely used to detect unknown words: statistical approaches [5, 11, 12, 13, 14, 15] and rule-based approaches [5, 11, 14, 15].",
        "Although rule-based approaches have the advantage of being simple, the complexity and domain dependency of how the unknown words are produced greatly reduce the efficiency of these approaches.",
        "On the other hand, statistical approaches have the advantage of being domain-independent [16].",
        "It is interesting to note that many systems apply a hybrid approach [5, 11, 14, 15].",
        "Regardless of the choice of different approaches, finding a way to automatically detect unknown words has become a crucial issue in Chinese word segmentation and Chinese information processing in general.",
        "Input raw sentence: .",
        "MMA pre-segmentation: Unknown word detection:",
        "Zhang Jie graduate from",
        "Fig.",
        "1.",
        "MMA and unknown word detection by chunking: an example",
        "This paper proposes a chunking strategy to cope with unknown words in Chinese word segmentation.",
        "First, a raw sentence is pre-segmented into a sequence of word atoms (i.e. single-character words and multi-character words) using a maximum matching algorithm (MMA).",
        "Then a chunking model is applied to detect unknown words by chunking one or more word atoms together according to the word formation patterns of the word atoms.",
        "Figure 1 gives an example.",
        "Here, the problem of unknown word detection is recast as chunking one or more word atoms together to form a new word and a discriminative Markov model, named Mutual Information Independence Model (MIIM), is adopted in chunking.",
        "Besides, a maximum entropy model is applied to integrate various types of contexts and resolve the data sparseness problem in MIIM.",
        "Moreover, an error-driven learning approach is proposed to learn useful",
        "JiaoTong University.",
        "contexts in the maximum entropy model.",
        "In this way, the number of contexts in the maximum entropy model can be significantly reduced without performance decrease.",
        "This makes it possible for further improving the performance by considering more various types of contexts in the future.",
        "Evaluation on the PK and CTB corpora in the First SIGHAN Chinese word segmentation bakeoff shows that our chunking strategy performs best in unknown word detection on both of the corpora.",
        "The rest of the paper is as follows: In Section 2, we will discuss in details about our chunking strategy in unknown word detection.",
        "Experimental results are given in Section 3.",
        "Finally, some remarks and conclusions are made in Section 4."
      ]
    },
    {
      "heading": "2. Unknown Word Detection by Chunking",
      "text": [
        "In this section, we will first describe the chunking strategy in unknown word detection of Chinese word segmentation using a discriminative Markov model, called Mutual Information Independence Model (MIIM).",
        "Then a maximum entropy model is applied to integrate various types of contexts and resolve the data sparseness problem in MIIM.",
        "Finally, an error-driven learning approach is proposed to select useful contexts and reduce the context feature vector dimension.",
        "2.1 Mutual Information Independence Model and Unknown Word Detection Mutual Information Independence Model",
        "In this paper, we use a discriminative Markov model, called Mutual Information Independence Model (MIIM) proposed by Zhou et al. [17], in unknown word detection by chunking.",
        "MIIM is derived from a conditional probability model.",
        "Given an observation sequence O\" = o1o2 ■■■0n, the goal of a conditional probability model is to find a stochastic optimal state(tag) sequence S\" = sjs2 ■•• sn that maximizes:",
        "The second term in Equation (1) is the pairwise mutual information (PMI) between S\" and O\" .",
        "In order to simplify the computation of this term, we assume a pairwise mutual information independence (2):",
        "We have renamed the discriminative Markov model in [17] as the Mutual Information Independence Model according to the novel pairwise mutual information independence assumption in the model.",
        "Another reason is to distinguish it from the traditional Hidden Markov Model [18] and avoid misleading.",
        "That is, an individual state is only dependent on the observation sequence On and independent on other states in the state sequence Sn.",
        "This assumption is reasonable because the dependence among the states in the state sequence Sn has already been captured by the first term in Equation (1).",
        "Applying Equation (2) to Equation (1), we have Equation (3):",
        "We call the above model as shown in Equation (3) the Mutual Information Independence Model due to its pairwise mutual information assumption as shown in Equation (2).",
        "The above model consists of two sub-models: the state transition model",
        "J PMI(s;, S'f) as the first term in Equation (3) and the output model J log P(s; I Ojn ) as the second term in Equation (3).",
        "Here, a variant of the Viterbi",
        "algorithm [19] in decoding the standard Hidden Markov Model (HMM) [18] is implemented to find the most likely state sequence by replacing the state transition model and the output model of the standard HMM with the state transition model and the output model of the MIIM, respectively.",
        "Unknown Word Detection",
        "For unknown word detection by chunking, a word (known word or unknown word) is regarded as a chunk of one or more word atoms and we have:",
        "• oi =< Pi, wi > ; wi is the i - th word atom in the sequence of word atomsW\" = w1 w2 ••• wn ; pi is the word formation pattern of the word atom wi.",
        "Here Pi measures the word formation power of the word atom wi and consists of: o The percentage of wi occurring as a whole word (round to 10%)",
        "o The percentage of wi occurring at the beginning of other words (round to o The percentage of wi occurring at the end of other words (round to 10%) o The length of wi",
        "o The occurring frequency feature of wi , which is mapped to max(log(Frequency), 9 ).",
        "• si : the states are used to bracket and differentiate various types of words.",
        "In this way, Chinese unknown word detection can be regarded as a bracketing process while differentiation of different word types can help the bracketing process.",
        "si is structural and consists of three parts:",
        "o Boundary Category (B): it includes four values: {O, B, M, E}, where O means that current word atom is a whOle word and B/M/E means that current word atom is at the Beginning/in the Middle/at the End of a word.",
        "o Word Category (W): It is used to denote the class of the word.",
        "In our system, words are classified into two types: pure Chinese word type and mixed word type (i.e. including English characters and Chinese digits/numbers/symbols).",
        "o Word Atom Formation Pattern (P): Because of the limited number of boundary and word categories, the word atom formation pattern described above is added into the structural state to represent a more accurate state transition model in MIIM while keeping its output model.",
        "Problem with Unknown Word Detection Using MIIM",
        "From Equation (3), we can see that the state transition model of MIIM can be computed by using ngram modeling [20, 21, 22], where each tag is assumed to be dependent on the N-1 previous tags (e.g. 2).",
        "The problem with the above MIIM lies in the data sparseness problem raised by its output model: J log P(si I On ).",
        "Ideally, we would have sufficient training data for every event whose conditional probability we wish to calculate.",
        "Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data.",
        "Generally, two smoothing approaches [21, 22, 23] are applied to resolve this problem: linear interpolation and back-off.",
        "However, these two approaches only work well when the number of different information sources is very limited.",
        "When a few features and/or a long context are considered, the number of different information sources is exponential.",
        "This makes smoothing approaches inappropriate in our system.",
        "In this paper, the maximum entropy model [24] is proposed to integrate various context information sources and resolve the data sparseness problem in our system.",
        "The reason that we choose the maximum entropy model for this purpose is that it represents the state-of-the-art in the machine learning research community and there are good implementations of the algorithm available.",
        "Here, we use the open NLP maximum entropy package in our system.",
        "The maximum entropy model is a probability distribution estimation technique widely used in recent years for natural language processing tasks.",
        "The principle of the maximum entropy model in estimating probabilities is to include as much information as is known from the data while making no additional assumptions.",
        "The maximum entropy model returns the probability distribution that satisfies the above property with the highest entropy.",
        "Formally, the decision function of the maximum entropy model can be represented as:",
        "where o is the outcome, h is the history (context feature vector in this paper), Z(h) is a normalization function, {f1, f2, fk} are feature functions and {a1, a2, ak} are the model parameters.",
        "Each model parameter corresponds to exactly one feature and can be viewed as a \"weight\" for that feature.",
        "All features used in the maximum entropy model are binary, e.g.",
        "In order to reliably estimate P(si I O\" ) in the output model of MIIM using the maximum entropy model, various context information sources are included in the context feature vector:",
        "• pi : current word atom formation pattern",
        "• pi_1 pi : previous word atom formation pattern and current word atom formation pattern",
        "• PiPM : current word atom formation pattern and next word atom formation pattern",
        "• pi wi : current word atom formation pattern and current word atom",
        "• Pi_1wi_1 Pi : previous word atom formation pattern, previous word atom and current word atom formation pattern",
        "• PiPi+1wi+1 : current word atom formation pattern, next word atom formation pattern and next word atom",
        "• Pi_1 Piwi : previous word atom formation pattern, current word atom formation pattern and current word atom",
        "• PiwiPi+1 : current word atom formation pattern, current word atom and next word atom formation pattern",
        "• Pi_1wi_1 Ptw>i : previous word atom formation pattern, previous word atom, current word atom formation pattern and current word atom",
        "• Pi wi Pi+1 wi+1 : current word atom formation pattern, current word atom, next word atom formation pattern and next word atom",
        "However, there exists a problem when we include above various context information in the maximum entropy model: the context feature vector dimension easily becomes too large for the model to handle.",
        "One easy solution to this problem is to only keep those frequently occurring contexts in the model.",
        "Although this frequency filtering approach is simple, many useful contexts may not occur frequently and be filtered out while those kept may not be useful.",
        "To resolve this problem, we propose an alternative error-driven learning approach to only keep useful contexts in the model.",
        "Here, we propose an error-driven learning approach to examine the effectiveness of various contexts and select useful contexts to reduce the size of the context feature vector used in the maximum entropy model for estimating P(si I Oj\" ) in the output model of MIIM.",
        "This makes it possible to further improve the performance by incorporating more various types of contexts in the future.",
        "Assume O is the container for useful contexts.",
        "Given a set of existing useful contexts O and a set of new contexts AO , the effectiveness of a new context Ci 6 AO , E(O, Ci ), is measured by the Ci related reduction in errors which results from adding the new context set AO to the useful context set O :",
        "Here, # Error(O, Ci ) is the number of Ci related chunking errors before AO is added to O and # Error(O + AO, Ci ) is the number of Ci related chunking errors after AO is added to O .",
        "That is, E(O, Ci ) is the number of the chunking error corrections made on the context Ci 6 AO when AO is added to O .",
        "If E(O, Ci ) > 0, we declare that the new context Ci is a useful context and should be added to O .",
        "Otherwise, the new context Ci is considered useless and discarded.",
        "Given the above error-driven learning approach, we initialize O = {Pi} (i.e. we assume all the current word atom formation patterns are useful contexts) and choose one of the other context types as the new context set AO , e.g. O = {Piwi}.",
        "Then, we can train two MIIMs with different output models using O and O + AO respectively.",
        "Moreover, useful contexts are learnt on the training data in a twofold way.",
        "For each fold, two MIIMs are trained on 50% of the training data and for each new context Ci in AO , evaluate its effectiveness E(O, Ci ) on the remaining 50% of the training data according to the context effectiveness measure as shown in Equation (6).",
        "If E(O, Ci ) > 0, Ci is marked as a useful context and added to O .",
        "In this way, all the useful contexts in AO are incorporated into the useful context set O .",
        "Similarly, we can include useful contexts of other context types into the useful context set O one by one.",
        "In this paper, various types of contexts are learnt one by one in the exact same order as shown in Section 2.2.",
        "Finally, since different types of contexts may have cross-effects, the above process is iterated with the renewed useful context set O until very few useful contexts can be found at each loop.",
        "Our experiments show that iteration converges within four loops."
      ]
    },
    {
      "heading": "3. Experimental Results",
      "text": [
        "All of our experiments are evaluated on the PK and CTB benchmark corpora used in the First SIGHAN Chinese word segmentation bakeoff with the closed configuration.",
        "That is, only the training data from the particular corpus is used during training.",
        "For unknown word detection, the chunking training data is derived by using the same Maximum Matching Algorithm (MMA) to segment each word in the original training data as a chunk of word atoms.",
        "This is done in a twofold way.",
        "For each fold, the",
        "MMA is trained on 50% of the original training data and then used to segment the remaining 50% of the original training data.",
        "Then the MIIM is used to train a chunking model for unknown word detection on the chunking training data.",
        "Table 1 shows the details of the two corpora.",
        "Here, OOV is defined as the percentage of words in the test corpus not occurring in the training corpus and indicates the out-of-vocabulary rate in the test corpus.",
        "Table 1.",
        "Statistics of the corpora used in our evaluation",
        "Table 2 shows the detailed performance of our system in unknown word detection and Chinese word segmentation as a whole using the standard scoring script on the test data.",
        "In this and subsequent tables, various evaluation measures are provided: precision (P), recall (R), F-measure, recall on out-of-vocabulary words ( ROOV ) and recall on in-vocabulary words ( RN ).",
        "It shows that our system achieves precision/recall/F-measure of 93.5%/96.1%/94.8 and 90.5%/90.1%/90.3 on the PK and CTB corpora respectively.",
        "Especially, our chunking approach can successfully detect 80.5% and 77.6% of unknown words on the PK and CTB corpora respectively.",
        "Table 2.",
        "Detailed performance of our system on the 1st SIGHAN Chinese word segmentation benchmark data",
        "Table 3 and Table 4 compare our system with other best-reported systems on the PK and CTB corpora respectively.",
        "Table 3 shows that our chunking approach in unknown word detection outperforms others by more than 8% on the PK corpus.",
        "It also shows that our system performs comparably with the best reported systems on the PK corpus when the out-of-vocabulary rate is moderate(6.9%).",
        "Our performance in Chinese word segmentation as a whole is somewhat pulled down by the lower performance in recalling in-vocabulary words.",
        "This may be due to the preference of our chunking strategy in detecting unknown words by wrongly combining some of in-vocabulary words into unknown words.",
        "Such preference may cause negative effect in Chinese word segmentation as a whole when the gain in unknown word detection fails to compensate the loss in wrongly combining some of in-vocabulary words into unknown words.",
        "This happens when the out-of-vocabulary rate is not high, e.g. on the",
        "Corpus",
        "Abbreviation",
        "OOV",
        "Training Data",
        "Test Data",
        "Beijing University",
        "PK",
        "6.9%",
        "1100K words",
        "17K words",
        "UPENN Chinese Treebank",
        "CTB",
        "18.1%",
        "250K words",
        "40K words",
        "Corpus",
        "P",
        "R",
        "F",
        "ROOV",
        "RIV",
        "PK",
        "93.5",
        "96.1",
        "94.8",
        "80.5",
        "97.3",
        "CTB",
        "90.5",
        "90.1",
        "90.3",
        "77.6",
        "92.9",
        "PK corpus.",
        "Table 4 shows that our chunking approach in unknown word detection outperforms others by more than 7% on the CTB corpus.",
        "It also shows that our system outperforms the other best-reported systems by more than 2% in Chinese word segmentation as a whole on the CTB corpus.",
        "This is largely due to the huge gain in unknown word detection when the out-of-vocabulary rate is high (e.g. 18.1% in the CTB corpus), even though our system performs worse on recalling in-vocabulary words than others.",
        "Evaluation on both the PK and CTB corpora shows that our chunking approach can successfully detect about 80% of unknown words on corpora with a large range of the out-of-vocabulary rates.",
        "This suggests the powerfulness of using various word formation patterns of word atoms in detecting unknown words.",
        "This also demonstrates the effectiveness and robustness of our chunking approach in unknown word detection of Chinese word segmentation and its portability to different genres.",
        "Table 3.",
        "Comparison of our system with other best-reported systems on the PK corpus",
        "Finally, Table 5 and Table 6 compare our error-driven learning approach with the frequency filtering approach in learning useful contexts for the output model of MIIM on the PK and CTB corpora respectively.",
        "Due to memory limitation, at most 400K useful contexts are considered in the frequency filtering approach.",
        "First, they show that the error-driven learning approach is much more effective than the simple frequency filtering approach.",
        "With the same number of useful contexts, the error-driven learning approach outperforms the frequency filtering approach by 7.8%/0.6% and 5.5%/0.8% in ROOV (unknown word detection)/F-measure(Chinese word segmentation as a whole) on the PK and CTB corpora respectively.",
        "Moreover, the error-driven learning approach slightly outperforms the frequency filtering approach with the best configuration of 2.5 and 3.5 times of useful contexts.",
        "Second, they show that increasing the number of frequently occurring contexts using the frequency filtering approach may not increase the performance.",
        "This may be due to that some of frequently occurring contexts are noisy or useless and including them may have negative effect.",
        "Third, they show that the error-driven learning approach is effective in learning useful contexts by reducing 96-98% of possible contexts.",
        "Finally, the figures inside parentheses show the number of useful patterns shared between the error-driven learning approach and the frequency filtering approach.",
        "They show that about 40-50% of useful contexts selected using the error-driven learning approach do not occur frequently in the useful contexts selected using the frequency filtering approach.",
        "Corpus",
        "P",
        "R",
        "F",
        "ROOV RIV",
        "Ours",
        "93.5",
        "96.1",
        "94.8",
        "80.5 97.3",
        "Zhang et al. [25]",
        "94.0",
        "96.2",
        "95.1",
        "72.4 97.9",
        "Wu [26]",
        "93.8",
        "95.5",
        "94.7",
        "68.0 97.6",
        "Chen [27]",
        "93.8",
        "95.5",
        "94.6",
        "64.7 97.7",
        "Table 4.",
        "Comparison of our",
        "system with other best-reported systems on the CTB corpus",
        "Corpus",
        "P",
        "R",
        "F",
        "ROOV RIV",
        "Ours",
        "90.5",
        "90.1",
        "90.3",
        "77.6 92.9",
        "Zhang et al. [25]",
        "87.5",
        "88.6",
        "88.1",
        "70.5 92.7",
        "Duan et al. [28]",
        "85.6",
        "89.2",
        "87.4",
        "64.4 94.7",
        "Table 5.",
        "Comparison of the error-driven learning approach with the frequency filtering approach in learning useful contexts for the output model of MIIM on the PK corpus (Total number of possible contexts: 4836K)"
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "In this paper, a chunking strategy is presented to detect unknown words in Chinese word segmentation by chunking one or more word atoms together according to the various word formation patterns of the word atoms.",
        "Besides, a maximum entropy model is applied to integrate various types of contexts and resolve the data sparseness problem in our strategy.",
        "Finally, an error-driven learning approach is proposed to learn useful contexts in the maximum entropy model.",
        "In this way, the number of contexts in the maximum entropy model can be significantly reduced without performance decrease.",
        "This makes it possible for further improving the performance by considering more various types of contexts.",
        "Evaluation on the PK and CTB corpora in the First SIGHAN Chinese word segmentation bakeoff shows that our chunking strategy can detect about 80% of unknown words on both of the corpora and outperforms the best-reported systems by 8.1% and 7.1% in unknown word detection on them respectively.",
        "While our Chinese word segmentation system with chunking-based unknown word detection performs comparably with the best systems on the PK corpus when the out-of-vocabulary rate is moderate(6.9%), our system significantly outperforms others by more than 2% when the out-of-vocabulary rate is high(18.1%).",
        "This demonstrates the effectiveness and robustness of our chunking strategy in unknown word detection of Chinese word segmentation and its portability to different genres.",
        "Approach",
        "#useful contexts",
        "F",
        "ROOV",
        "RIV",
        "Error-Driven Learning",
        "98K",
        "94.8",
        "80.5",
        "97.3",
        "Frequency Filtering",
        "98K (63K)",
        "94.2",
        "72.7",
        "97.4",
        "Frequency Filtering (best performance)",
        "250K (90K)",
        "94.7",
        "80.2",
        "97.3",
        "Frequency Filtering",
        "400K (94K)",
        "94.6",
        "79.1",
        "97.1",
        "Table 6.",
        "Comparison of the error-driven learning approach with the frequency filtering approach in learning useful contexts for the output model of MIIM on the CTB corpus (Total number of possible contexts: 1038K)",
        "Approach",
        "#useful contexts",
        "F",
        "ROOV",
        "RIV",
        "Error-Driven Learning",
        "43K",
        "90.3",
        "77.6",
        "92.9",
        "Frequency Filtering",
        "43K (21K)",
        "89.5",
        "72.1",
        "92.8",
        "Frequency Filtering (best performance)",
        "150K",
        "90.1",
        "76.1",
        "93.0",
        "Frequency Filtering",
        "400K (40K)",
        "89.9",
        "75.8",
        "92.9"
      ]
    }
  ]
}
