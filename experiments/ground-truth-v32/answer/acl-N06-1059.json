{
  "info": {
    "authors": [
      "Chin-Yew Lin",
      "Guihong Cao",
      "Jianfeng Gao",
      "Jian-Yun Nie"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics",
    "id": "acl-N06-1059",
    "title": "An Information-Theoretic Approach to Automatic Evaluation of Summaries",
    "url": "https://aclweb.org/anthology/N06-1059",
    "year": 2006
  },
  "references": [
    "acl-N03-1020",
    "acl-N04-1019",
    "acl-P02-1040",
    "acl-P96-1041",
    "acl-W03-0508",
    "acl-W04-1013",
    "acl-W04-1014"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Until recently there are no common, convenient, and repeatable evaluation methods that could be easily applied to support fast turnaround development of automatic text summarization systems.",
        "In this paper, we introduce an information-theoretic approach to automatic evaluation of summaries based on the Jensen-Shannon divergence of distributions between an automatic summary and a set of reference summaries.",
        "Several variants of the approach are also considered and compared.",
        "The results indicate that JS divergence-based evaluation method achieves comparable performance with the common automatic evaluation method ROUGE in single documents summarization task; while achieves better performance than ROUGE in multiple document summarization task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Most previous automatic evaluation methods in summarization use co-occurrence statistics (Lin and Hovy 2003) to measure the content overlap between an automatic summary and a set of reference summaries.",
        "Among them, ROUGE (Lin 2004) has been used by the annual summarization evaluation conference, Document Understanding Conference1 (DUC), sponsored by NIST since 2001.",
        "Content and quality of a summary are the two main aspects of summarization measured in the past DUCs.",
        "Using a manual evaluation inter",
        "face called SEE2, NIST assessors compared the content overlap between a system summary and a reference summary and assigned a coverage score to indicate the extent of the overlap between system and reference summaries.",
        "The overall system content coverage score was then the average of coverage scores over a set of test topics.",
        "NIST assessors also judged the quality of a peer summary by answering a set of quality assessment questions related to grammaticality, coherence, and organization for each system summary.",
        "However, we only focus on automatic evaluation of content coverage in this paper and aim at establishing a statistical framework that can perform at least as good as the current state-of-the-art automatic summarization evaluation methods such as ROUGE.",
        "We start with a brief description of our statistical summary generation model and how to estimate its parameters in the next section.",
        "We then describe experimental setups and criterion of success in Section 3.",
        "The results of the experiments are shown and analyzed in Section 4.",
        "We discuss related work and recent advances in statistical language models for information retrieval in Section 5.",
        "Finally, we conclude and suggest future directions in Section 6."
      ]
    },
    {
      "heading": "2 Summarization Evaluation Using Information-Theoretic Measures",
      "text": [
        "Given a set of documents D = {d,, d2, ..., di13, i = 1 to n, we assume there exists a probabilistic distribution with parameters specified by BR that generates reference summaries from D. The task of summarization is to estimate R .",
        "Similarly, we as",
        "sume every system summary is generated from a probabilistic distribution with parameters specified by A.",
        "Therefore, a good summarizer should have its Averyclose to oR and the process of summary evaluation could be viewed as a task of estimating the distance between A and R .",
        "For example, if we use Kullback-Leibler (KL) divergence as the distance function, then the summary evaluation task could be viewed as finding the KL divergence between oA and oR .",
        "However, KL divergence is unspecified when a value is defined in oR but not in oA (Lin 1991, Dagan et al.",
        "1999).",
        "Usually smoothing has to be applied to address this missing data problem (unseen word in this case).",
        "Another problem is that KL divergence is not symmetric, i.e. KL(R LL A) # KL((A LLII R), except when oR = oA.",
        "This is counter-intuitive in our application scenario.",
        "We therefore use generalized Jensen-Shannon (JS) divergence proposed by Lin (1991).",
        "The JS divergence can be written as follows:",
        "where Pi is a probability distribution with weight 7ri, =1, and H() is Shannon entropy,",
        "The question now is how to estimate these distributions."
      ]
    },
    {
      "heading": "2.1 Estimation of Posterior and Prior System Summary Distributions",
      "text": [
        "By Bayes' rule, the posterior probability of",
        "distribution of the generation distribution for a prior .",
        "In our case, we assume a Dirichlet prior distribution (the conjugate distribution of the multinomial distribution) as follows: ((L",
        "where a",
        "Since the Jensen-Shannon divergence is a distance measure, we take its negative value to indicate the similarity between two distributions as follows: Equation (3) suggests that the problem of summary evaluation could be cast as ranking system summaries according to their negative Jensen-Shannon divergence with respect to the estimated posterior distribution of reference BA is estimated via maximum a Posterior (MAP)",
        "Substituting (5), (6), and (7) into (4), we have the posterior distribution P(A I SA) as below:",
        "We now turn to discuss different ways of estimating BAi and ai and their implications as described by Zaragoza et al.",
        "(2003).",
        "According to Equation (8), the posterior distribution of 6A given SA is also a Dirichlet distribution.",
        "Its maximum posterior estimation has the following form (Gelman et al.",
        "2003):",
        "and the posterior distribution (8) can be written as:",
        "If we set ai =1, then BA i does not depend on ai, i.e. all possible A�s have equal prior.",
        "In this case, equation (9) becomes the maximum likelihood estimation as follows:",
        "and the posterior distribution (8) can be written as:",
        "The problem with using maximum likelihood estimation is when ai equal to zero.",
        "If zero occurrence happens for word i, then its maximum likelihood estimation, 6� , would be zero and the whole posterior distribution would be zero.",
        "To tackle this problem, we need to redistribute some probability mass to zero occurrence events or unseen word events.",
        "The process of redistribution is called smoothing in the language modeling literatures.",
        "For an in-depth discussion of this topic, please see Chen and Goodman (1996).",
        "By choosing different value for i , we could derive different smoothing methods as discussed in Zaragoza et al.",
        "(2003).",
        "For example, we could estimate ai using topic collection frequency by setting ai = ,uP(wi I T) + 1, where p is a scaling factor and P(wi I T) is the probability of word i occurring in topic T. This is called Bayes-smoothing and has been used in language modeling for information retrieval (Zhai and Lafferty 2004).",
        "The Bayes-smoothing can be written as:",
        "We now turn to estimating the posterior distribution of 6R given a reference summary SR ."
      ]
    },
    {
      "heading": "2.2 Estimation of Reference Summary Distributions",
      "text": [
        "Given a reference summary SR, we could estimate posterior distribution 6R in the same way that we estimate posterior distribution BA as follows:",
        "where ai is the number of occurrence of word i in reference summary SR .",
        "Given another reference summary SRR, i.e., when multiple reference summaries are available, the posterior distribution can be updated using Bayesian inference as follows:",
        "equation (15), SR is independent of SR, and P(SRR) is computed using equation (7) but with the posterior distribution P(R I SR) as prior.",
        "In a more general case, given multiple (L) reference summaries, SR,1, \"',SR,L , the posterior distribution of 6R could",
        "be written as follows by repeat application of Bayesian inference with equation (16):",
        "where ai,; is the number of occurrence of word i in reference summary SR,;, and",
        "the topic collection could be computed as follows: Equation (17) indicates that estimation of posterior distribution given multiple summaries is the same as estimation of posterior distribution given a single summary that contains all the reference summaries.",
        "This is reasonable since we assume a bag-of-word unigram likelihood model for generating summaries4.",
        "It also bodes well with the consensus-oriented manual summarization evaluation approaches proposed by van Halteren and Teufel (2003) and Nenkova and Passonneau (2004).",
        "With equations (8) and (17), the summary score of system summary, SA, can be computed using Jensen-Shannon divergence from equation (3) as follows: ScOre aq,(SA SRL)= – IS1/2(P(OA I SA)[]P(R I SR1,L)), (20) where SR1,L is a shorthand for SR, 1,•••,SR,L ."
      ]
    },
    {
      "heading": "3 Experimental Setup",
      "text": [
        "We used data from DUC 2002 100-word single and multi-document tasks as our testing corpus.",
        "DUC 2002 data includes 59 topic sets.",
        "Each topic set contains about 10 news article pertaining to some news topic, for example, topic D061 is about \"Hurricane Gilbert\".",
        "Two human written summaries per topic are provided as reference summaries.",
        "14 sets of system summaries and 1 simple lead baseline summary are included for the single document summarization task (total 15 runs); while 8 sets of system summaries, 1 lead baseline, and 1 latest news baseline are included for the multi-document summarization task (total 12 runs).",
        "All summaries are about 100 words5.",
        "Manually evaluation results in average coverage6 scores are also included in the DUC 2002 data set.",
        "The commonly used criterion of success to evaluate an automatic evaluation method is to compute the correlation between the ranking of systems according to human assigned scores and the ranking according to automatic scores (Papineni et al.",
        "2002; Lin & Hovy 2003).",
        "We followed the same convention and computed Pearson's product moment correlation coefficient and Spearman's rank correlation coefficient as indicators of success.",
        "Besides evaluating the performance of the automatic evaluation measure based on Jensen-Shannon (IS) divergence as defined in equation (2), we also compared it with measures based on KL-divergence and simple log likelihood.",
        "The effect of smoothing and the difference of using single and multiple reference summaries were also investigated.",
        "To examine the effect of using longer n-grams (n > 1), we also used bag-of-bigram and bag-of-trigram models by simply replace unigrams in the model proposed in Section 2 with bigrams and trigrams and treat them as unigrams.",
        "Lemur toolkit version 4.07 was used to estimate models with modification to speedup computation of bigram and trigram models.",
        "We also ran standard ROUGE v1.5.5 with ROUGE1 to 4 as baselines.",
        "All experiments were run with common words excluded and Porter stemmer applied.",
        "We summarize these experiments in the following sections."
      ]
    },
    {
      "heading": "3.1 Jensen-Shannon Divergence (JSD)",
      "text": [
        "We use equation (22) to compute summary score and apply maximum likelihood estimation (�L ) of the parameters according to equation (11).",
        "Using a unigram model and single reference summary, we rewrite equation (22) as follows: 5 There were also 10-, 50-, and 200-word summary tasks in DUC 2002 multi-document summarization evaluation.",
        "However, we only used the data of 100-word summarization subtask for this experiment.",
        "Score=_, (SA I SR,') where P(O'iBS I SA) and P( ei I SR',') are estimated as follows:",
        "where P(B� I SA) and P(07 I SR',') are estimated as follows:",
        "C(wi,SA) and C(wi,SR') are the counts of word wi in system summary SA and reference summary SR,' respectively.",
        "When multiple reference summaries are used, P(OISR',L) is estimated as follows:",
        "C(wi,SA) and C(wi,SR') are the counts of word wi in system summary SA and reference summary SR' respectively.",
        "The Bayes-smoothing probability or Bayesian prior P(wi I C) is estimated from a general English corpus instead of the topic collection as we described in section 2.'.",
        "In our experiments, we used TREC AP88-90 collection that contained more than 200,000 news articles.",
        "When multiple reference summaries are used, P(BS I SR',L) is esti"
      ]
    },
    {
      "heading": "3.2 Jensen-Shannon Divergence with Smoothing (JSDS)",
      "text": [
        "To examine the effect of smoothing when we compute summary score using equation (22), we apply Bayes-smoothing as shown in equation ('5).",
        "Using a unigram model and single reference summary, we rewrite equation (22) as follows:",
        "The value of p could be determined empirically.",
        "In this experiment we set p to 2,000 following Zhai and Lafferty (2004)."
      ]
    },
    {
      "heading": "3.3 Kullback-Leibler Divergence with Smoothing (KLDS)",
      "text": [
        "To compare the performance of JSD and JSDS scoring methods with other alternative distance measure, we also compute summary scores using KL divergence with Bayes-smoothing as follows:",
        "The Bayes-smoothing factor y is also set to 2,000 and BS is estimated by the same way that we"
      ]
    },
    {
      "heading": "3.4 Log Likelihood with Smoothing (LLS)",
      "text": [
        "As a baseline measure, we also compute the log likelihood score of an automatic summary given a reference summary or a set of reference summaries as follows:",
        "where ISAI is the length of SA andp(BS LSR1,L) is es� timated as before."
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "Table 1 shows the results of all runs.",
        "According to Table 1, automatic evaluation measure based on Jensen-Shannon Divergence without Bayes-smoothing (JSD) performed the best among all measures.",
        "Among them, JSD over the bag-of-unigram model achieved the best results in the single document summarization task (P-SD-MR: 0.97, S-SD-MR: 0.91); while the bag-of-bigram model achieved the best results in the multiple document summarization task (P-MD-MR: 0.96, S-MD-MR: 0.94).",
        "Although the bag-of-bigram model did not perform as well as the bag-of-unigram model in the single document summarization task, its Pearson (SD-MR: 0.94) and Spearman",
        "(SD-MR: 0.90) correlation values were still over 90% regardless of single or multiple references were used.",
        "We also observed that using multiple references outperformed using only single reference.",
        "This is reasonable since we expect to estimate models better when more reference summaries are available.",
        "Smoothed measures did not perform well.",
        "This is not a surprise due to the nature of summarization evaluation.",
        "Intuitively, only information presented in system and reference summaries should be considered for evaluation.",
        "The JSD-based measure was also compared favorably to ROUGE in the multiple document summarization task as shown in Table 2.",
        "In particular, the JSD-based measure over bag-of-bigram model using multiple references achieved much better results in both Pearson's and Spearman's correlations than all versions of ROUGE.",
        "For single document summarization task, the JSD-based measure still achieved high correlations (90%+) though it was not as high as ROUGE2, 3, and 4."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "The approach described in this paper is most similar to the Bayesian extension in information retrieval (IR) work by Zaragoza et al.",
        "(2003).",
        "In their work, query likelihood model was presented as Bayesian inference.",
        "Other earlier language modeling (Rosenfeld 2002) work in information retrieval, especially the idea of modeling a document using bag-of-word unigram model, also inspire this work (Berger and Lafferty 1999, Lafferty and Zhai 2001 Statistical language models such as document language model (Ponte and Croft 1998, Zhai and Lafferty 2004), relevance-based language models (Lavrenko and Croft 2001), and dependency-based language models (Gao et al.",
        "2004) have been applied successfully in information retrieval.",
        "It has also been applied to topic detection and tracking (Lavrenko et al.",
        "2002, Larkey et al.",
        "2004).",
        "Ex",
        "tended models also have been developed to deal with vocabulary mismatch and query expansion problems (Berger and Lafferty 1999, Hofmann 1999, Lafferty and Zhai 2001).",
        "However, it has not been applied in automatic evaluation of summarization.",
        "Hori et al.",
        "(2004) also considered using \"posterior probability\" derived from consensus among human summaries as weighting factor to improve evaluations of speech summarization.",
        "But their notion of \"posterior probability\" was not true probability and was not presented as an integral part of the Bayesian inference framework as we have described in this paper."
      ]
    },
    {
      "heading": "6 Conclusions and Future Work",
      "text": [
        "The research proposed in this paper aims at providing a pilot study of applying information-theoretic measures in automatic evaluation of summaries.",
        "With the initial success of this study, we would like to: (1) verify the results with other set of data, for example, DUC 2003 data, (2) tune the Bayesian smoothing parameter p to further examine the effect of smoothing, (3) develop better content generation model and (4) add synonym and paraphrase matching capability in the future.",
        "To address (3), for example, we would like to explore mutual information-based dependency language modeling as proposed by Gao et al.",
        "(2004).",
        "For (4), manual evaluation methods recently proposed separately by van Halteren and Teufel (2003), the factoid method, and Nenkova and Pas-souneau (2004), the pyramid method, tried to take advantage of the availability of multiple references.",
        "Both methods assume that the more important a piece of information is, the more reference summaries it appears in.",
        "These manual evaluation methods can identify semantic equivalents.",
        "For example, a summary content unit (SCU) \"The diamonds were replaced by fake replicas 8\" created as defined in Nenkova and Passouneau (2004) from the following four contributing clauses (1a – d):",
        "the building dressed as cleaners when the thieves burst in with a bulldozer and sledgehammers.",
        "Contributors (1a – d) from 4 reference summaries to the SCU are underlined.",
        "The manual pyramid method can identify these semantic equivalents.",
        "It is obvious that automatic evaluation methods relying on strict n-gram or lexical matching would only find two out of four possible matches, i.e. \"switched the diamonds with fakes\" from (1 a) and (1b) while leave \"switched the diamonds with worthless glass\" (1c) and \"The diamonds had been swapped with glass replicas\" (1 d) unmatched.",
        "Allowing near synonyms such as fakes, worthless glass, and glass replicas to match might help, but how to acquire these equivalents and how to assign appropriate weights to reflect their subtle differences remain open questions.",
        "To find semantic equivalents automatically, we would like to try query expansion techniques (Hofmann 1999, Lafferty and Zhai 2001, Bai et al.",
        "2005, Cao et al.",
        "2005) commonly used in IR.",
        "Proper query expansion boosts IR system performance.",
        "We suspect that these techniques would help a little but we probably would need to develop much better paraphrase expansion and matching techniques to see significant boost in overall performance."
      ]
    },
    {
      "heading": "7 Acknowledgement",
      "text": [
        "Part of this work was conducted while the first author visited Microsoft Research Asia (MSRA) in the summer of 2005.",
        "He would like to thank Ming Zhou, Hsiao-Wuen Hon, and other staffs at MSRA for providing an excellent research environment and exciting intellectual exchanges during his visit."
      ]
    },
    {
      "heading": "Reference",
      "text": []
    }
  ]
}
