{
  "info": {
    "authors": [
      "Bing Zhao",
      "Niyu Ge",
      "Kishore Papineni"
    ],
    "book": "Human Language Technology Conference and Empirical Methods in Natural Language Processing",
    "id": "acl-H05-1023",
    "title": "Inner-Outer Bracket Models for Word Alignment Using Hidden Blocks",
    "url": "https://aclweb.org/anthology/H05-1023",
    "year": 2005
  },
  "references": [
    "acl-C96-2141",
    "acl-J03-1002",
    "acl-J03-1005",
    "acl-J93-2003",
    "acl-J97-3002",
    "acl-N03-1017",
    "acl-N04-1033",
    "acl-P01-1067",
    "acl-P02-1038",
    "acl-P02-1040",
    "acl-P03-1039",
    "acl-W02-1012",
    "acl-W02-1018",
    "acl-W03-1001"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Most statistical translation systems are based on phrase translation pairs, or “blocks”, which are obtained mainly from word alignment.",
        "We use blocks to infer better word alignment and improved word alignment which, in turn, leads to better inference of blocks.",
        "We propose two new probabilistic models based on the inner-outer segmentations and use EM algorithms for estimating the models’ parameters.",
        "The first model recovers IBM Model-1 as a special case.",
        "Both models outperform bidirectional IBM Model-4 in terms of word alignment accuracy by 10% absolute on the F-measure.",
        "Using blocks obtained from the models in actual translation systems yields statistically significant improvements in Chinese-English SMT evaluation."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Today’s statistical machine translation systems rely on high quality phrase translation pairs to acquire state-of-the-art performance, see (Koehn et al., 2003; Zens and Ney, 2004; Och and Ney, 2003).",
        "Here, phrase pairs, or “blocks” are obtained automatically from parallel sentence pairs via the underlying word alignments.",
        "Word alignments traditionally are based on IBM Models 1-5 (Brown et al., 1993) or on HMMs (Vogel et al., 1996).",
        "Automatic word alignment is challenging in that its accuracy is not yet close to inter-annotator agreement in some language pairs: for Chinese-English, inter-annotator agreement exceeds 90 on F-measure whereas IBM Model4 or HMM accuracy is typically below 80s.",
        "HMMs assume that words “close-in-source” are aligned to words “close-in-target”.",
        "While this locality assumption is generally sound, HMMs do have limitations: the self-transition probability of a state (word) is the only control on the duration in the state, the length of the phrase aligned to the word.",
        "Also there is no natural way to control repeated non-contiguous visits to a state.",
        "Despite these problems, HMMs remain attractive for their speed and reasonable accuracy.",
        "We propose a new method for localizing word alignments.",
        "We use blocks to achieve locality in the following manner: a block in a sentence pair is a source phrase aligned to a target phrase.",
        "We assume that words inside the source phrase cannot align to words outside the target phrase and that words outside the source phrase cannot align to words inside the target phrase.",
        "Furthermore, a block divides the sentence pair into two smaller regions: the inner part of the block, which corresponds to the source and target phrase in the block, and the outer part of the block, which corresponds to the remaining source and target words in the parallel sentence pair.",
        "The two regions are non-overlapping; and each of them is shorter than the original parallel sentence pair.",
        "The regions are thus easier to align than the original sentence pairs (e.g., using IBM Model-1).",
        "While the model uses a single block to split the sentence pair into two independent regions, it is not clear which block we should select for this purpose.",
        "Therefore, we treat the splitting block as a hidden variable.",
        "This proposed approach is far simpler than treating the entire sentence as a sequence of non-overlapping phrases (or chunks) and considering such sequential segmentation either explicitly or implicitly.",
        "For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al., 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and headword driven chunk reordering.",
        "Other approaches including (Watanabe et al., 2002) treat extracted phrase-pairs as new parallel data with limited success.",
        "Typically, they share a similar architecture of phrase level segmentation, reordering, translation as in (Och and Ney, 2002; Koehn and Knight, 2002; Yamada and Knight, 2001).",
        "The phrase level interaction has to be taken care of for the non-overlapping sequential segmentation in a complicated way.",
        "Our models model such interactions in a soft way.",
        "The hidden blocks are allowed to overlap with each other,",
        "Processing (HLT/EMNLP), pages 177–184, Vancouver, October 2005. c�2005 Association for Computational Linguistics while each block induced two non-overlapping regions, i.e. the model brackets the sentence pair into two independent parts which are generated synchronously.",
        "In this respect, it resembles bilingual bracketing (Wu, 1997), but our model has more lexical items in the blocks with many-to-many word alignment freedom in both inner and outer parts.",
        "We present our localization constraints using blocks for word alignment in Section 2; we detail our two new probabilistic models and their EM training algorithms in Section 3; our baseline system, a maximum-posterior inference for word alignment, is explained in Section 4; experimental results of alignments and translations are in Section 5; and Section",
        "2 Segmentation by a Block We use the following notation in the remainder of this paper: e and f denote the English and foreign sentences with sentence lengthes of I and J, respectively.",
        "ei is an English word at position i in e; fj is a foreign word at position j in f. a is the alignment vector with aj mapping the position of the English word ea� to which fj connects.",
        "Therefore, we have the standard limitation that one foreign word cannot be connected to more than one English word.",
        "A block S�� is defined as a pair of brackets as follows:",
        "where Se _ [il, iT] is a bracket in English sentence defined by a pair of indices: the left position il and the right position iT, corresponding to a English phrase e\".",
        "Similar notations are for Sf _ [jl, jT], which is one possible projection of Se in f. The subscript l and r are abbreviations of left and right, respectively.",
        "Se segments e into two parts: (Se, e) _ (Se�, Se��).",
        "The inner part Se � _ {ei, i E [il, iT] } and the outer part Se �� _ {ei, i E� [il, iT] }; Sf segments f similarly.",
        "Thus, the block S�� splits the parallel sentence pair into two non-overlapping regions: the Inner S0 � and OuterS ��parts (see Figure 1).",
        "With this segmentation, we assume the words in the inner part are aligned to inner part only: SE _ Se � H Sf � : {ei, i E [il, iT] } H { fj, j E [jl, jT] }; and words in the outer part are aligned to outer part only: S0 �� _ Se �� H Sf �� : {ei, i E� [il, iT]} H { fj, j E� [jl, jT]}.",
        "We do not allow alignments to cross block boundaries.",
        "Words inside a block SD can be aligned using a variety of models (IBM models 1-5, HMM, etc).",
        "We choose Model1 for simplicity.",
        "If the block boundaries are accurate, we can expect high quality word alignment.",
        "This is our proposed new localization method."
      ]
    },
    {
      "heading": "3 Inner-Outer Bracket Models",
      "text": [
        "We treat the constraining block as a hidden variable in a generative model shown in Eqn.",
        "2.",
        "where SD _ (Se, Sf) is the hidden block.",
        "In the generative process, the model first generates a bracket Se for e with a monolingual bracketing model of P(SeIe).",
        "It then uses the segmentation of the English (Se, e) to generate the projected bracket Sf of f using a generative translation model P(f, Sf ISe, e) _ P(Sfo, Sf� ISe��, Se�) – the key model to implement our proposed inner-outer constraints.",
        "With the hidden block S�� inferred, the model then generates word alignments within the inner and outer parts separately.",
        "We present two generating processes for the inner and outer parts induced by SD and corresponding two models of P(f, Sf ISe, e).",
        "These models are described in the following secions."
      ]
    },
    {
      "heading": "3.1 Inner-Outer Bracket Model-A",
      "text": [
        "The first model assumes that the inner part and the outer part are generated independently.",
        "By the formal equivalence of (f, Sf) with (Sf�, Sf,), Eqn.",
        "2 can be approximated as:",
        "where P(Sf� I Se�) and P(Sf��ISe��) are two independent generative models for inner and outer parts, respec",
        "tively and are futher decompsed into:",
        "where �a�1} is the word alignment vector.",
        "Given the block segmentation and word alignment, the generative process first randomly selects a ei according to either P(ei I SeE) or P(ei I Se �E); and then generates fd indexed by word alignmentad with i = ad according to a word level lexicon P(fdI ea, ).",
        "This generative process using the two models of P(SfE ISeE) and P(Sf IS;) must satisfy the constraints of segmentations inLuced by the hidden block S0 = (Se, Sf).",
        "The English words SeE inside the block can only generate the words in SfE and nothing else; likewise SeE� only generates Sf�E.",
        "Overall, the combination of P(SfEISeE)P(Sf�EISe�E) in Eqn.",
        "3 collaborates each other quite well in practice.",
        "For a particular observation Sf E, if SeE is too small (i.e., missing translations), P(SfEISeE) will suffer; and if SeE is too big (i.e., robbing useful words from Se�E), P(Sf �EISe�E) will suffer.",
        "Therefore, our proposed model in Eqn.",
        "3 combines the two costs and requires both inner and outer parts to be explained well at the same time.",
        "Because the model in Eqn.",
        "3 is essentially a two-level (SD and a) mixture model similar to IBM Models, the EM algorithm is quite straight forward as in IBM models.",
        "Shown in the following are several key E-step computations of the posteriors.",
        "The M-step (optimization) is simply the normalization of the fractional counts collected using the posteriors through the inference results from E-step:",
        "In principle, Se can be a bracket of any length not exceeding the sentence length.",
        "If we restrict the bracket length to that of the sentence length, we recover IBM Model-1.",
        "Figure 2 summarizes the generation process for Inner-Outer Bracket Model-A."
      ]
    },
    {
      "heading": "3.2 Inner-Outer Bracket Model-13",
      "text": [
        "A block S0 invokes both the inner and outer generations simultaneously in Bracket Model A (BM-A).",
        "However, the generative process is usually more effective in the inner part as S0 is generally small and accurate.",
        "We can build a model focusing on generating only the inner part with careful inferences to avoid errors from noisy blocks.",
        "To ensure that all f1 are generated, we need to propose enough blocks to cover each observation fd.",
        "This constraint can be met by treating the whole sentence pair as one block.",
        "The generative process is as follows: First the model generates an English bracket Se as before.",
        "The model then generates a projection Sf in f to localize all ad’s for the given Se according to P(Sf ISe, e).",
        "Se and Sf forms a hidden block S0.",
        "Given S0, the model then generates only the inner part fd E SfE via",
        "P(SfEISf,Se,e) is a bracket level emission probabilistic model which generates a bag of contiguous words fd E SfE under the constraints from the given hidden block S0 = (Sf,Se).",
        "The model is simplified in Eqn.",
        "7 with the assumption of bag-of-words’ independence within the bracket Sf:",
        "ther Pb[��(ad I SfE, SeE) when (fd, ea,) E S��E,or otherwise Pb[� (ad I Sf �E, Se�E) when (fd ,ea,) E S���E.",
        "Assuming P(SeIe) to be a uniform distribution, the posterior of selecting a hidden block given observations: P(S0 = (Se, Sf) Ie, f) is proportional to block level relative frequency P�ll(SfE ISeE) updated in each iteration; and can be smoothed with P(Sf I Se, f, e) = P(SfEISeE)P(Sf�EISe �E)/ P{b1f} P(S1fE I SeE)P(S1f I Se) assuming Model-1 alignment in the inner anS outer parts independently to reduce the risks of data sparseness in estimations.",
        "The P([jl, jT] ISe, e) in Eqn.",
        "6 is a localization probabilistic model, which has resemblances to an HMM’s transition probability, P(ajIaj_1), implementing the assumption “close-in-source” is aligned to “close-in-target”.",
        "However, instead of using the simple position variable aj, P([jl, jT] ISe, e) is more expressive with word identities to localize words {fj} aligned to Se�.",
        "To model P([jl,jT]I Se, e) reliably, Sf = [jl, jT] is equivalently defined as the center and width of the bracket Sf: (Obf,wbf).",
        "To simplify it further, we assume that wbf and Obf can be predicted independently.",
        "The width model, P(wbfISe,e), depends on the length of the English bracket and the fertilities of English words in it.",
        "To simplify M-step computations, we can compute the expected width as in Eqn.",
        "8.",
        "E{wbf ISe, e} N y • I iT – il + 1I, (8) where y is the expected bracket length ratio and is approximated by the average sentence length ratio computed using the whole parallel corpus.",
        "For Chinese-English, y = 1�1.3 = 0.77.",
        "In practice, this estimation is quite reliable.",
        "The center model P(Obf ISe, e) is harder.",
        "It is conditioned on the translational equivalence between the English bracket and its projection.",
        "We compute the expected Obf by averaging the weighted expected centers from all the English words in Se as in Eqn.",
        "9.",
        "The expectations of (Obf,wbf) from Eqn.",
        "8 and Eqn.",
        "9 give a reliable starting point for a local search for the optimal estimation of (^Obf, ^wbf) as in Eqn 10:",
        "where the score functions of P(Sf�ISe�)P(Sf��ISe��) are in Eqn.",
        "4 with the word alignment explicitly given from the previous iteration.",
        "For the very first iteration, full alignment is assumed; this means that every word pair is connected in the parallel sentences.",
        "During the local search in Eqn.",
        "10, one can choose the top-1 (Viterbi) (^Obf, ^wbf) or top-N candidates and normalize over these candidates to obtain the posteriors.",
        "Except for the local search of (^Obf,^wbf), the remainder EM steps are similar to Bracket Model-A, though with different interpretations.",
        "By performing local search in Eqn.",
        "10, Model-B localizes hidden blocks more accurately than the scheme of the smoothed relative frequency in Model-A’s EM iterations.",
        "The model is also more focused on the predictions in the inner part.",
        "Figure 3 summarizes the generative process of Model-B (BM-B)."
      ]
    },
    {
      "heading": "3.3 A Null Word Model",
      "text": [
        "The null word model allows words to be aligned to nothing.",
        "In the traditional IBM models, there is a universal null word which is attached to every sentence pair to compete with word generators.",
        "In our inner-outer bracket models, we use two context-specific null word models which use both the left and right context as competitors in the generative process for each observation fj: P(fjI fj_1, e) and P(fj I fj+1, e).",
        "This is similar to the approach in (Toutanova et al., 2002), in which the null word model is part of an extended HMM using left context only.",
        "With two null word models, we can associate fj with its left or right context (i.e., a null link) when the null word models are very strong, or when the word’s alignment is too far from the expected center ^Obf in Eqn.",
        "9."
      ]
    },
    {
      "heading": "4 A Max-Posterior for Word Alignment",
      "text": [
        "In the HMM framework, (Ge, 2004) proposed a maximum-posterior method which worked much better than Viterbi for Arabic to English translations.",
        "The difference between maximum-posterior and Viterbi, in a nutshell, is that while Viterbi computes the best state sequence given the observation, the maximum-posterior computes the best state one at a time.",
        "In the terminology of HMM, let the states be the words in the foreign sentence f�1 and observations be the words in the English sentence e�1.",
        "We use the subscript t to note the fact that et is observed (or emitted) at time step t. The posterior probabilities P(fj I et) (state given observation) are obtained after the forward-backward training.",
        "The maximum-posterior word alignments are obtained by first com",
        "that is, the point at which the posterior is maximum.",
        "The pair (j, t) defines a word pair (fj, et) which is then aligned.",
        "The procedure continues to find the next maximum in the posterior matrix.",
        "Contrast this with Viterbi alignment where one computes",
        "We observe, in parallel corpora, that when one word translates into multiple words in another language, it usually translates into a contiguous sequence of words.",
        "Therefore, we impose a contiguity constraint on word alignments.",
        "When one word fj aligns to multiple English words, the English words must be contiguous in e and vice versa.",
        "The algorithm to find word alignments using max-posterior with contiguity constraint is illustrated in Algorithm 1.",
        "Algorithm 1 A maximum-posterior algorithm with contiguity constraint 1: while (j, t) = (j, t) * (as computed in Eqn.",
        "11) do 2: if (fj, et) is not yet aligned then 3: align(fj, et); 4: else if (et is contiguous to what fj is aligned) or (fj is contiguous to what et is aligned) then 5: align(fj, et); 6: end if 7: end while",
        "The algorithm terminates when there isn’t any ’next’ posterior maximum to be found.",
        "By definition, there are at most JxT ’next’ maximums in the posterior matrix.",
        "And because of the contiguity constraint, not all ( fj, et) pairs are valid alignments.",
        "The algorithm is sure to terminate.",
        "The algorithm is, in a sense, directionless, for one fj can align to multiple et’s and vise versa as long as the multiple connections are contiguous.",
        "Viterbi, however, is directional in which one state can emit multiple observations but one observation can only come from one state."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We evaluate the performances of our proposed models in terms of word alignment accuracy and translation quality.",
        "For word alignment, we have 260 hand-aligned sentence pairs with a total of 4676 word pair links.",
        "The 260 sentence pairs are randomly selected from the CTTP1 corpus.",
        "They were then word aligned by eight bilingual speakers.",
        "In this set, we have one-to-one, one-to-many and many-to-many alignment links.",
        "If a link has one target functional word, it is considered to be a functional link (Examples of funbctional words are prepositions, determiners, etc.",
        "There are in total 87 such functional words in our experiments).",
        "We report the overall F-measures as well as F-measures for both content and functional word links.",
        "Our significance test shows an overall interval of ±1.56% F-measure at a 95% confidence level.",
        "For training data, the small training set has 5000 sentence pairs selected from XinHua news stories with a total of 131K English words and 125K Chinese words.",
        "The large training set has 181K sentence pairs (5k+176K); and the additional 176K sentence pairs are from FBIS and Sinorama, which has in total 6.7 million English words and 5.8 million Chinese words."
      ]
    },
    {
      "heading": "5.1 Baseline Systems",
      "text": [
        "The baseline is our implementation of HMM with the maximum-posterior algorithm introduced in section 4.",
        "The HMMs are trained unidirectionally.",
        "IBM Model-4 is trained with GIZA++ using the best reported settings in (Och and Ney, 2003).",
        "A few parameters, especially the maximum fertility, are tuned for GIZA++’s optimal performance.",
        "We collect bidirectional (bi) refined word alignment by growing the intersection of Chinese-to-English (CE) alignments and English-to-Chinese (EC) alignments with the neighboring unaligned word pairs which appear in the union similar to the “final-and” approaches (Koehn, 2003; Och and Ney, 2003; Tillmann, 2003).",
        "Table 1 summarizes our baseline with different settings.",
        "Table 1 shows that HMM EC-P gives the",
        "in the final word alignment (this means they are left unaligned).",
        "best baseline, better than bidirectional refined word alignments from GIZA M4 and the HMM Viterbi aligners."
      ]
    },
    {
      "heading": "5.2 Inner-Outer Bracket Models",
      "text": [
        "We trained HMM lexicon P(f �e) to initialize the inner-outer Bracket models.",
        "Afterwards, up to 15– 20 EM iterations are carried out.",
        "Iteration starts from the fully aligned sentence pairs, which give an F-measure of 9.28% at iteration one.",
        "the Top-1 projection from the inner parts of the block (top-1-inside) gives the best performance:pan F-measure of 72.29%, or a 7.5% absolute improvement",
        "Empirically we found that doing more than 5 iterations lead to overfitting.",
        "The peak performance in our model is usually achieved around iteration 4-5.",
        "At iteration 5, setting `BM-B Top-1” gives an F-measure of 73.93% which is better than BM-A’s best performance (72.29%).",
        "This is because Model B leverages a local search for less noisy blocks and",
        "hence the inner part isMmore accurately generated (which in turn83means the outer part is also more accurate).",
        "From this point on, all of our experi",
        "ments are using Model B.",
        "With smoothing, BM-B",
        "improves to 74.46%.",
        "After applying the null word model, we get 75.20%.",
        "By simply dropping links containing the715 English functional words, weogetd 76.24%, which is significantly better than our bestw baseline obtained from even the large training set th",
        "the performance of Inner-Outer Bracket Model-B (BM-B) over EM iterations.",
        "smoothing means when collecting the fractional counts, we reweigh the updated fractional count by 0.95 and give the remaining 0.05 weight to original fractional count from the links, which were aligned in the previous iteration.",
        "w/null means we applied the proposed Null word model in section 3.3 to infer null links.",
        "We also predefined a list of 15 English function words, for which there might be no corresponding Chinese words as translations.",
        "These 15 English words are \"a, an, the, of to for by up be been being does do did -”.",
        ", , , , , , , , , , , In the drop-null experiments, the links containing these predefined function words are simply dropped"
      ]
    },
    {
      "heading": "5.2.2 Large Data Track",
      "text": [
        "Figure 6 shows performance pictures of model BM-B on the large training set.",
        "Without dropping English functional words, the best performance is",
        "80.38% at iteration 4 using the Top-1 projection together with the null word models.",
        "By additionally dropping the links containing the 15 functional English words, we get 81.47%.",
        "These results are all significantly better than our strongest baseline system: 71.92% F-measure using HMM EC-P (70.24% using bidirectional Model-4 for comparisons).",
        "acc acy i On this data set, we experimented with different maximum bracket length limits, from one word (un-igram) to nine-gram.",
        "Results show that a maximum bracket length of four is already optimal (79.3% with %",
        "lead to better block selection.",
        "The experimental results above support the first claim.",
        "Now we consider the second claim that good word alignment leads to better block selection.",
        "Given reference human word alignment, we extract reference blocks up to five-gram phrases on Chinese.",
        "The block extraction procedure is based on the procedures in (Tillmann, 2003).",
        "During EM, we output all the hidden blocks actually inferred at each iteration, then we evaluate the precision, recall and F-measure of the hidden blocks according to the extracted reference blocks.",
        "The results are shown in Figure 7.",
        "Because we extract all",
        "possible n-grams at each position in e, the precision is low and the recall is relatively high as shown by Figure 7.",
        "It also shows that blocks do improve, presumably benefiting from better word alignments.",
        "Table 2 summarizes word alignment performances of Inner-Outer BM-B in different settings.",
        "Overall, without the handcrafted function word list, BM-B gives about 8% absolute improvement in F-measure on the large training set and 9% for the small set",
        "with a confidence interval of ±1.56%."
      ]
    },
    {
      "heading": "Re5.4 Translation Quality Evaluations",
      "text": [
        "We also carried out the translation experiments using the best settings for Inner-Outer BM-B (i.e. BM-Bdrop) on the TIDES Chinese-English 2003 test set.",
        "We trained our models on 354,252 test-specific sentence pairs drawn from LDC-supplied parallel corpora.",
        "On this training data, we ran 5 iterations of EM using BM-B to infer word alignments.",
        "A monotone decoder similar to (Tillmann and Ney, 2003) with a trigram language model3 is set up for translations.",
        "We report case sensitive Bleu (Papineni et al., 2002) score BleuC for all experiments.",
        "The baseline system (HMM) used phrase pairs built from the HMM-EC-P maximum posterior word alignment and the corresponding lexicons.",
        "The baseline BleuC score is 0.2276 ± 0.015.",
        "If we use the phrase pairs built from the bracket model instead (but keep the HMM trained lexicons), we get case sensitive BleuC score 0.2526.",
        "The improvement is statistically significant.",
        "If on the other hand, we use baseline phrase pairs with bracket model lexicons, we get a BleuC score 0.2325, which is only a marginal improvement.",
        "If we use both phrase pairs and lexicons from the bracket model, we get a case sensitive BleuC score 0.2750, which is a statistically significant improvement.",
        "The results are summarized in Table 3.",
        "Overall, using Model-B, we improve translation quality from 0.2276 to 0.2750 in case sensitive BleuC score."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "Our main contributions are two novel Inner-Outer Bracket models based on segmentations induced by hidden blocks.",
        "Modeling the Inner-Outer hidden segmentations, we get significantly improved word alignments for both the small training set and the large training set over the widely-practiced bidirectional IBM Model-4 alignment.",
        "We also show significant improvements in translation quality using our proposed bracket models.",
        "Robustness to noisy blocks merits further investigation."
      ]
    },
    {
      "heading": "7 Acknowledgement",
      "text": [
        "This work is supported by DARPA under contract number N66001-99-28916."
      ]
    }
  ]
}
