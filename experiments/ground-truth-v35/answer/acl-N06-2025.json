{
  "info": {
    "authors": [
      "Alessandro Moschitti"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics – Short Papers",
    "id": "acl-N06-2025",
    "title": "Syntactic Kernels for Natural Language Learning: The Semantic Role Labeling Case",
    "url": "https://aclweb.org/anthology/N06-2025",
    "year": 2006
  },
  "references": [
    "acl-E06-1015",
    "acl-J93-2004",
    "acl-P02-1034",
    "acl-P03-1004",
    "acl-P04-1016",
    "acl-P04-1043",
    "acl-P04-1054",
    "acl-P97-1003",
    "acl-W05-0620"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we use tree kernels to exploit deep syntactic parsing information for natural language applications.",
        "We study the properties of different kernels and we provide algorithms for their computation in linear average time.",
        "The experiments with SVMs on the task of predicate argument classification provide empirical data that validates our methods."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Recently, several tree kernels have been applied to natural language learning, e.g. (Collins and Duffy, 2002; Zelenko et al., 2003; Cumby and Roth, 2003; Culotta and Sorensen, 2004; Moschitti, 2004).",
        "Despite their promising results, three general objections against kernel methods are raised: (1) only a subset of the dual space features are relevant, thus, it may be possible to design features in the primal space that produce the same accuracy with a faster computation time; (2) in some cases the high number of features (substructures) of the dual space can produce overfitting with a consequent accuracy decrease (Cumby and Roth, 2003); and (3) the computation time of kernel functions may be too high and prevent their application in real scenarios.",
        "In this paper, we study the impact of the subtree (ST) (Vishwanathan and Smola, 2002), subset tree (SST) (Collins and Duffy, 2002) and partial tree (PT) kernels on Semantic Role Labeling (SRL).",
        "The PT kernel is a new function that we have designed to generate larger substructure spaces.",
        "Moreover,",
        "to solve the computation problems, we propose algorithms which evaluate the above kernels in linear average running time.",
        "We experimented such kernels with Support Vector Machines (SVMs) on the classification of semantic roles of PropBank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982) data sets.",
        "The results show that: (1) the kernel approach provides the same accuracy of the manually designed features.",
        "(2) The overfitting problem does not occur although the richer space of PTs does not provide better accuracy than the one based on SST.",
        "(3) The average running time of our tree kernel computation is linear.",
        "In the remainder of this paper, Section 2 introduces the different tree kernel spaces.",
        "Section 3 describes the kernel functions and our fast algorithms for their evaluation.",
        "Section 4 shows the comparative performance in terms of execution time and accuracy."
      ]
    },
    {
      "heading": "2 Tree kernel Spaces",
      "text": [
        "We consider three different tree kernel spaces: the subtrees (STs), the subset trees (SSTs) and the novel partial trees (PTs).",
        "An ST of a tree is rooted in any node and includes all its descendants.",
        "For example, Figure 1 shows the parse tree of the sentence \"Mary brought a cat\" together with its 6 STs.",
        "An SST is a more general structure since its leaves can be associated with nonterminal symbols.",
        "The SSTs satisfy the constraint that grammatical rules cannot be broken.",
        "For example, Figure 2 shows 10 SSTs out of 17 of the subtree of Figure 1 rooted in VP.",
        "If we relax the non-breaking rule constraint we obtain a more general form of substructures, i.e. the PTs.",
        "For example,"
      ]
    },
    {
      "heading": "3 Fast Tree Kernel Functions",
      "text": [
        "The main idea of tree kernels is to compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space.",
        "We designed a general function to compute the ST, SST and PT kernels.",
        "Our fast algorithm is inspired by the efficient evaluation of non-continuous subsequences (described in (Shawe-Taylor and Cristianini, 2004)).",
        "To further increase the computation speed, we also applied the pre-selection of node pairs which have non-null kernel."
      ]
    },
    {
      "heading": "3.1 Generalized Tree Kernel function",
      "text": [
        "Given a tree fragment space F = ff1, f2,.., fP}, we use the indicator function Ii(n) which is equal to 1 if the target fi is rooted at node n and 0 otherwise.",
        "We define the general kernel as:",
        "where NT1 and NT2 are the sets of nodes in T1 and T2, respectively and O(n1, n2) = LiP1 Ii(n1)Ii(n2), i.e. the number of common fragments rooted at the n1 and n2 nodes.",
        "We can compute it as follows: - if the node labels of n1 and n2 are different then",
        "where �J1 = (J11, J12, J13, ..) and �J2 = (J21, J22, J23, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of n2, respectively, �J1i and �J2i point to the i-th children in the two sequences, and l(•) returns the sequence length.",
        "We note that (1) Eq.",
        "2 is a convolution kernel according to the definition and the proof given in (Haussler, 1999).",
        "(2) Such kernel generates a feature space richer than those defined in (Vishwanathan and Smola, 2002; Collins and Duffy, 2002; Zelenko et al., 2003; Culotta and Sorensen, 2004; Shawe-Taylor and Cristianini, 2004).",
        "Additionally, we add the decay factor as follows: O(n1, n2) =",
        "where d(�J1) = �J1l(�J1) – �J11 and d(�J2) = �J2l(�J2) – �J21.",
        "In this way, we penalize subtrees built on child subsequences that contain gaps.",
        "Moreover, to have a similarity score between 0 and 1, we also apply the normalization in the kernel space, i.e.",
        "in Eq.",
        "3 can be distributed with respect to different types of sequences, e.g. those composed by p children, it follows that",
        "where Op evaluates the number of common subtrees rooted in subsequences of exactly p children (of n1 and n2) and lm = minf l(cn1), l(cn2)}.",
        "Note also that if we consider only the contribution of the longest sequence of node pairs that have the same children, we implement the SST kernel.",
        "For the STs computation we need also to remove the A2 term from Eq.",
        "4.",
        "Given the two child sequences c1a = cn1 and c2b = cn2 (a and b are the last children), Op(c1a, c2b) =",
        "where c1 [1 : i] and c2 [1 : r] are the child subsequences from 1 to i and from 1 to r of c1 and c2.",
        "If we name the double summation term as Dp, we can rewrite the relation as:",
        "By means of the above relation, we can compute the child subsequences of two sets c1 and c2 in O(p1c111c21).",
        "This means that the worst case complexity of the PT kernel is O(p�21 NT� 1 1 NT� 1), where p is the maximum branching factor of the two trees.",
        "Note that the average p in natural language parse trees is very small and the overall complexity can be reduced by avoiding the computation of node pairs with different labels.",
        "The next section shows our fast algorithm to find non-null node pairs."
      ]
    },
    {
      "heading": "3.2 Fast non-null node pair computation",
      "text": [
        "To compute the kernels defined in the previous section, we sum the A function for each pair (n1,n2)E NT, x NT, (Eq.",
        "1).",
        "When the labels associated with n1 and n2 are different, we can avoid evaluating A(n1, n2) since it is 0.",
        "Thus, we look for a node pair set Np ={(n1, n2)E NT, x NT, : label(n1) = label(n2)}.",
        "To efficiently build Np, we (i) extract the L1 and L2 lists of nodes from T1 and T2, (ii) sort them in alphanumeric order and (iii) scan them to find Np.",
        "Step (iii) may require only O (1 NT� 1 + 1 NT21) time, but, if label(n1) appears r1 times in T1 and label(n2) is repeated r2 times in T2, we need to consider r1 x r2 pairs.",
        "The formal can be found in (Moschitti, 2006)."
      ]
    },
    {
      "heading": "4 The Experiments",
      "text": [
        "In these experiments, we study tree kernel performance in terms of average running time and accuracy on the classification of predicate arguments.",
        "As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure.",
        "The experiments were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes the fast tree kernels in the SVM-light software (Joachims, 1999).",
        "The multiclassifiers were obtained by training an SVM for each class in the ONE-vs.-ALL fashion.",
        "In the testing phase, we selected the class associated with the maximum SVM score.",
        "For the ST, SST and PT kernels, we found that the best A values (see Section 3) on the development set were 1, 0.4 and 0.8, respectively, whereas the best p was 0.4."
      ]
    },
    {
      "heading": "4.1 Kernel running timeSexperiments S",
      "text": [
        "To study the FTK running time, we extracted from the Penn Treebank several samples of 500 trees containing exactly n nodes.",
        "Each point of Figure 4 shows the average computation time1 of the kernel function applied to the 250,000 pairs of trees of size n. It clearly appears that the FTK-SST and FTK-PT (i.e. FTK applied to the SST and PT kernels) average running time has linear behavior whereas, as expected, the naive SST algorithm shows a quadratic curve."
      ]
    },
    {
      "heading": "4.2 Experiments on SRL dataset",
      "text": [
        "We used two different corpora: PropBank (www.cis.upenn.edu/ – ace) along with Penn Treebank 2 (Marcus et al., 1993) and FrameNet.",
        "PropBank contains about 53,700 sentences and a fixed split between training and testing used in other researches.",
        "In this split, sections from 02 to 21 are used for training, section 23 for testing and section 22 as development set.",
        "We considered a total of 122,774 and 7,359 arguments (from Arg0 to Arg5, ArgA and ArgM) in training and testing, respectively.",
        "The tree structures were extracted from the Penn Treebank.",
        "From the FrameNet corpus (www.icsi.",
        "berkeley.edu/ – framenet) we extracted all",
        "manually designed features by 2/3 percent points, thus they can be seen as a useful tactic to boost system accuracy.",
        "24,558 sentences of the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 (www.",
        "senseval .",
        "org).",
        "We considered the 18 most frequent roles, for a total of 37,948 examples (30% of the sentences for testing and 70% for training/validation).",
        "The sentences were processed with the Collins’ parser (Collins, 1997) to generate automatic parse trees.",
        "We run ST, SST and PT kernels along with the linear kernel of standard features (Carreras and M`arquez, 2005) on PropBank training sets of different size.",
        "Figure 5 illustrates the learning curves associated with the above kernels for the SVM multiclassifiers.",
        "The tables 1 and 2 report the results, using all available training data, on PropBank and FrameNet test sets, respectively.",
        "We note that: (1) the accuracy of PTs is almost equal to the one produced by SSTs as the PT space is a hyperset of SSTs.",
        "The small difference is due to the poor relevance of the substructures in the PT - SST set, which degrade the PT space.",
        "(2) The high F1 measures of tree kernels on FrameNet suggest that they are robust with respect to automatic parse trees.",
        "Moreover, the learning time of SVMs using FTK for the classification of one large argument (Arg 0) is much lower than the one required by naive algorithm.",
        "With all the training data FTK terminated in 6 hours whereas the naive approach required more than 1 week.",
        "However, the complexity burden of working in the dual space can be alleviated with recent approaches proposed in (Kudo and Matsumoto, 2003; Suzuki et al., 2004).",
        "Finally, we carried out some experiments with the combination between linear and tree kernels and we found that tree kernels improve the models based on"
      ]
    }
  ]
}
