{
  "info": {
    "authors": [
      "Aria Haghighi",
      "Kristina Toutanova",
      "Christopher D. Manning"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W05-0623",
    "title": "A Joint Model for Semantic Role Labeling",
    "url": "https://aclweb.org/anthology/W05-0623",
    "year": 2005
  },
  "references": [
    "acl-A00-2018",
    "acl-J02-3001",
    "acl-N04-1030",
    "acl-P05-1073",
    "acl-W04-2412"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a semantic role labeling system submitted to the closed track of the CoNLL-2005 shared task.",
        "The system, introduced in (Toutanova et al., 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework.",
        "We also describe experiments aimed at increasing the robustness of the system in the presence of syntactic parse errors.",
        "Our final system achieves F1-Measures of 76.68 and 78.45 on the development and the WSJ portion of the test set, respectively."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "It is evident that there are strong statistical patterns in the syntactic realization and ordering of the arguments of verbs; for instance, if an active predicate has an A0 argument it is very likely to come before an A1 argument.",
        "Our model aims to capture such dependencies among the labels of nodes in a syntactic parse tree.",
        "However, building such a model is computationally expensive.",
        "Since the space of possible joint la-belings is exponential in the number of parse tree nodes, a model cannot exhaustively consider these labelings unless it makes strong independence assumptions.",
        "To overcome this problem, we adopt a discriminative re-ranking approach reminiscent of (Collins, 2000).",
        "We use a local model, which labels arguments independently, to generate a smaller number of likely joint labelings.",
        "These candidate labelings are in turn input to a joint model which can use global features and re-score the candidates.",
        "Both the local and global re-ranking models are log-linear (maximum entropy) models.",
        "In the following sections, we briefly describe our local and joint models and the system architecture for combining them.",
        "We list the features used by our models, with an emphasis on new features, and compare the performance of a local and a joint model on the CoNLL shared task.",
        "We also study an approach to increasing the robustness of the semantic role labeling system to syntactic parser errors, by considering multiple parse trees generated by a statistical parser."
      ]
    },
    {
      "heading": "2 Local Models",
      "text": [
        "Our local model labels nodes in a parse tree independently.",
        "We decompose the probability over labels (all argument labels plus NONE), into a product of the probability over ARG and NONE, and a probability over argument labels given that a node is an ARG.",
        "This can be seen as chaining an identification and a classification model.",
        "The identification model classifies each phrase as either an argument or non-argument and our classification model labels each potential argument with a specific argument label.",
        "The two models use the same features.",
        "Previous research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Carreras and M`arquez, 2004) has identified many useful features for local identification and classification.",
        "Below we list the features and hand-picked conjunctions of features used in our local models.",
        "The ones denoted with asterisks (*) were not present in (Toutanova et al., 2005).",
        "Although most of these features have been described in previous work, some features, described in the next section, are – to our knowledge – novel.",
        "• Phrase-Type Syntactic category of node • Predicate Lemma Stemmed target verb • Path Sequence of phrase types between the predicate and node, with 1,1 to indicate direction • Position Before or after predicate • Voice Voice of predicate • Head-Word of Phrase • Head-POS POS tag of head word • Sub-Cat CFG expansion of predicate’s parent • First/Last Word • Left/Right Sister Phrase-Type • Left/Right Sister Head-Word/Head-POS • Parent Phrase-Type • Parent POS/Head-Word • Ordinal Tree Distance Phrase-type concatenated with the length of the Path feature • Node-LCA Partial Path Path from the node to the lowest common ancestor of the predicate and the node • PP Parent Head-Word If the parent of the node is a PP, the parent’s headword • PP NP Head-Word/Head-POS For a PP, retrieve the headword /head-POS of its rightmost NP • Temporal Keywords* Is the head of the node a temporal word e.g ‘February’ or ‘afternoon’ • Missing subject* Is the predicate missing a subject in the“standard” location • Projected path* Path from the maximal extended projection of the predicate to the node • Predicate Lemma & Path • Predicate Lemma & Head-Word • Predicate Lemma & Phrase-Type • Voice & Position • Predicate Lemma & PP Parent Head-Word • Path & Missing subject* • Projected path & Missing subject*"
      ]
    },
    {
      "heading": "2.1 Additional Local Features",
      "text": [
        "We found that a large source of errors for A 0 and A 1 stemmed from cases such as those illustrated in Figure 1, where arguments were dislocated by raising or controlling verbs.",
        "Here, the predicate, expected, does not have a subject in the typical position – indicated by the empty NP – since the auxiliary is has raised the subject to its current position.",
        "In order to capture this class of examples, we use a binary feature, Missing Subject, indicating whether the predicate is “missing” its subject, and use this feature in conjunction with the Path feature, so that we learn typical paths to raised subjects conditioned on the absence of the subject in its typical position.",
        "In the particular case of Figure 1, there is another instance of an argument being quite far from",
        "its predicate.",
        "The predicate widen shares the trade gap with expect as a A1 argument.",
        "However, as expect is a raising verb, widen’s subject is not in its typical position either, and we should expect to find it in the same positions as expected’s subject.",
        "This indicates it may be useful to use the path relative to expected to find arguments for widen.",
        "In general, to identify certain arguments of predicates embedded in auxiliary and infinitival VPs we expect it to be helpful to take the path from the maximum extended projection of the predicate – the highest VP in the chain of VP’s dominating the predicate.",
        "We introduce a new path feature, Projected Path, which takes the path from the maximal extended projection to an argument node.",
        "This feature applies only when the argument is not dominated by the maximal projection, (e.g., direct objects).",
        "These features also handle other cases of discontinuous and non-local dependencies, such as those arising due to controller verbs.",
        "For a local model, these new features and their conjunctions improved F1-Measure from 73.80 to 74.52 on the development set.",
        "Notably, the F1-Measure of A 0 increased from 81.02 to 83.08."
      ]
    },
    {
      "heading": "3 Joint Model",
      "text": [
        "Our joint model, in contrast to the local model, collectively scores a labeling of all nodes in the parse tree.",
        "The model is trained to re-rank a set of N likely labelings according to the local model.",
        "We find the exact top N consistent' most likely local model labelings using a simple dynamic program described in (Toutanova et al., 2005).",
        "Most of the features we use are described in more detail in (Toutanova et al., 2005).",
        "Here we briefly describe these features and introduce several new joint features (denoted by *).",
        "A labeling L of all nodes in the parse tree specifies a candidate argument frame – the sequence of all nodes labeled with a non-NONE label according to L. The joint model features operate on candidate argument frames, and look at the labels and internal features of the candidate arguments.",
        "We introduce them in the context of the example in Figure 2.",
        "The candidate argument frame corresponding to the correct labeling for the",
        "tree is: [NP1-A1,VBD-V,PP1-A3,PP2-A4,NP2-AM-TMP].",
        "• Core arguments label sequence: The sequence of labels of core arguments concatenated with the predicate voice.",
        "Example: [voice:active: A1,V,A3,A4] Aback-off feature which substitutes specific argument labels with a generic argument (A) label is also included.",
        "• Flattened core arguments label sequence*: Same as the previous but merging consecutive equal labels.",
        "• Core arguments label and annotated phrase type sequence: The sequence of labels of core arguments together with annotated phrase types.",
        "Phrase types are annotated with the head word for PP nodes, and with the head POS tag for S and VP nodes.",
        "Example: [voice:active: NP-A1,V,PP-toA3,PP-from-A4].",
        "A back-off to generic A labels is also included.",
        "Also a variant that adds the predicate stem.",
        "• Repeated core argument labels with phrase types: Annotated phrase types for nodes with the same core argument label.",
        "This feature captures, for example, the tendency of WHNP referring phrases to occur as the second phrase having the same label as a preceding NP phrase.",
        "• Repeated core argument labels with phrase",
        "types and sister/adjacency information*: Similar to the previous feature, but also indicates whether all repeated arguments are sisters in the parse tree, or whether all repeated arguments are adjacent in terms of word spans.",
        "These features can provide robustness to parser errors, making it more likely to label adjacent phrases incorrectly split by the parser with the same label."
      ]
    },
    {
      "heading": "4 Combining Local and Joint Models",
      "text": [
        "It is useful to combine the joint model score with a local model score, because the local model has been trained using all negative examples, whereas the joint model has been trained only on likely argument frames .",
        "Our final score is given by a mixture of the local and joint model’s log-probabilities: scoreSRL(L�t) = a scoret(Llt) + scorei(Llt), where scoret(Llt) is the local score of L, scorei(Llt) is the corresponding joint score, and a is a tunable parameter.",
        "We search among the top N candidate labelings proposed by the local model, for the labeling that maximizes the final score."
      ]
    },
    {
      "heading": "5 Increasing Robustness to Parser Errors",
      "text": [
        "It is apparent that role labeling is very sensitive to the correctness of the given parse tree.",
        "If an argument does not correspond to a constituent in a parse tree, our model will not be able to consider the correct phrase.",
        "One way to address this problem is to utilize alternative parses.",
        "Recent releases of the Charniak parser (Charniak, 2000) have included an option to provide the top k parses of a given sentence according to the probability model of the parser.",
        "We use these alternative parses as follow: Suppose t1, ... , tk are trees for sentence s with given probabilities P(ti �s) by the parser.",
        "Then for a fixed predicate v, let Li",
        "denote the best joint labeling of tree ti, with score scoreSRL (Li �ti) according to our final joint model.",
        "Then we choose the labeling L which maximizes: arg max Q log P(tilS) + scoreSRL(Li�ti) (1) iEJ1,...,k1 Considering top k = 5 parse trees using this algorithm resulted in up to 0.4 absolute increase in F-Measure.",
        "In future work, we plan to experiment with better ways to combine information from multiple parse trees."
      ]
    },
    {
      "heading": "6 Experiments and Results",
      "text": [
        "For our final results we used a joint model with a = 1.5 (local model weight), Q = 1 (parse tree log-probability weight) , N = 15 (candidate labelings from the local model to consider) , and k = 5 (number of alternative parses).",
        "The whole training set for the CoNLL-2005 task was used to train the models.",
        "It takes about 2 hours to train a local identification model, 40 minutes to train a local classification model, and 7 hours to train a joint re-ranking model.2 In Table 1, we present our final development and test results using this model.",
        "The percentage of perfectly labeled propositions for the three sets is 55.11% (development), 56.52% (test), and 37.06% (Brown test).",
        "The improvement achieved by the joint model relative to the local model is about 2 points absolute in F-Measure, similar to the improvement when gold-standard syntactic parses are used (Toutanova et al., 2005).",
        "The relative error reduction is much lower for automatic parses, possibly due to a lower upper bound on performance.",
        "It is clear from the drop in performance from the WSJ to Brown test set that our learned model’s features do not generalize very well to related domains."
      ]
    }
  ]
}
