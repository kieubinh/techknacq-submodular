{
  "info": {
    "authors": [
      "Kevin Duh",
      "Katrin Kirchhoff"
    ],
    "book": "Workshop on Computational Approaches to Semitic Languages",
    "id": "acl-W05-0708",
    "title": "POS Tagging of Dialectal Arabic: A Minimally Supervised Approach",
    "url": "https://aclweb.org/anthology/W05-0708",
    "year": 2005
  },
  "references": [
    "acl-A00-1031",
    "acl-A92-1018",
    "acl-C04-1080",
    "acl-J92-4003",
    "acl-N01-1026",
    "acl-N03-1033",
    "acl-N03-2003",
    "acl-N04-4038",
    "acl-P00-1026",
    "acl-W00-0729",
    "acl-W00-0731",
    "acl-W04-3229",
    "acl-W95-0101",
    "acl-W96-0102",
    "acl-W98-1121"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Natural language processing technology for the dialects of Arabic is still in its infancy, due to the problem of obtaining large amounts of text data for spoken Arabic.",
        "In this paper we describe the development of a part-of-speech (POS) tagger for Egyptian Colloquial Arabic.",
        "We adopt a minimally supervised approach that only requires raw text data from several varieties of Arabic and a morphological analyzer for Modern Standard Arabic.",
        "No dialect-specific tools are used.",
        "We present several statistical modeling and cross-dialectal data sharing techniques to enhance the performance of the baseline tagger and compare the results to those obtained by a supervised tagger trained on hand-annotated data and, by a state-of-the-art Modern Standard Arabic tagger applied to Egyptian Arabic."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Part-of-speech (POS) tagging is a core natural language processing task that can benefit a wide range of downstream processing applications.",
        "Tagging is often the first step towards parsing or chunking (Osborne, 2000; Koeling, 2000), and knowledge of POS tags can benefit statistical language models for speech recognition or machine translation (Heeman, 1998; Vergyri et al., 2004).",
        "Many approaches for POS tagging have been developed in the past, including rule-based tagging (Brill, 1995),",
        "HMM taggers (Brants, 2000; Cutting and others, 1992), maximum-entropy models (Rathnaparki, 1996), cyclic dependency networks (Toutanova et al., 2003), memory-based learning (Daelemans et al., 1996), etc.",
        "All of these approaches require either a large amount of annotated training data (for supervised tagging) or a lexicon listing all possible tags for each word (for unsupervised tagging).",
        "Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al., 2004).",
        "Since large amount of text material as well as standard lexicons can be obtained in these cases, POS tagging is a straightforward task.",
        "The dialects of Arabic, by contrast, are spoken rather than written languages.",
        "Apart from small amounts of written dialectal material in e.g. plays, novels, chat rooms, etc., data can only be obtained by recording and manually transcribing actual conversations.",
        "Moreover, there is no universally agreed upon writing standard for dialects (though several standardization efforts are underway); any large-scale data collection and transcription effort therefore requires extensive training of annotators to ensure consistency.",
        "Due to this data acquisition bottleneck, the development of NLP technology for dialectal Arabic is still in its infancy.",
        "In addition to the problems of sparse training data and lack of writing standards, tagging of dialectal Arabic is difficult for the following reasons:",
        "• Resources such as lexicons, morphological analyzers, tokenizers, etc.",
        "are scarce or nonexistent for dialectal Arabic.",
        "• Dialectal Arabic is a spoken language.",
        "Tagging spoken language is typically harder than tag",
        "ging written language, due to the effect of dis-fluencies, incomplete sentences, varied word order, etc.",
        "• The rich morphology of Arabic leads to a large number of possible word forms, which increases the number of out-of-vocabulary (OOV) words.",
        "• The lack of short vowel information results in high lexical ambiguity.",
        "In this paper we present an attempt at developing a POS tagger for dialectal Arabic in a minimally supervised way.",
        "Our goal is to utilize existing resources and data for several varieties of Arabic in combination with unsupervised learning algorithms, rather than developing dialect-specific tools.",
        "The resources available to us are the CallHome Egyptian Colloquial Arabic (ECA) corpus, the LDC Lev-antine Arabic (LCA) corpus, the LDC MSA Treebank corpus, and a generally available morphological analysis tool (the LDC-distributed Buckwalter stemmer) for MSA.",
        "The target dialect is ECA, since this is the only dialectal corpus for which POS annotations are available.",
        "Our general approach is to bootstrap the tagger in an unsupervised way using POS information from the morphological analyzer, and to subsequently improve it by integrating additional data from other dialects and by general machine learning techniques.",
        "We compare the result against the performance of a tagger trained in a supervised way and an unsupervised tagger with a hand-developed ECA lexicon."
      ]
    },
    {
      "heading": "2 Data",
      "text": [
        "The ECA corpus consists of telephone conversations between family members or close friends, with one speaker being located in the U.S. and the other in Egypt.",
        "We use the combined train, eval96 and hub5 sections of the corpus for training, the dev set for development and the eval97 set for evaluation.",
        "The LCA data consists of telephone conversations on predefined topics between Levantine speakers previously unknown to each other; all of the available data was used.",
        "The Treebank corpus is a collection of MSA newswire text from Agence France Press, An Nahar News, and Unmah Press.",
        "We use parts 1 (v3.0), 2 (v2.0) and 3 (v1.0).",
        "The sizes of the various corpora are shown in Table 1.",
        "The ECA corpus was originally transcribed in a “ro-manized” form; a script representation was then derived automatically from the romanized transcripts.",
        "The script, therefore, does not entirely conform to the MSA standard: romanized forms may represent actual pronunciations and contain such MSA – * ECA changes as /B/ – * /s/ or /t/ and /ð/ – * /z/ or /d/.",
        "The resulting representation cannot be unambiguously mapped back to MSA script; the variants /s/ or /t/, for instance, are represented by or , rather than .",
        "This introduces additional noise into the data, but it also mimics the real-world situation of variable spelling standards that need to be handled by a robust NLP system.",
        "We use the script representation of this corpus for all our experiments.",
        "The ECA corpus is accompanied by a lexicon containing the morphological analysis of all words, i.e. an analysis in terms of stem and morphological characteristics such as person, number, gender, POS, etc.",
        "Since the analysis is based on the romanized form, a single tag can be assigned to the majority of words (75% of all tokens) in the corpus.",
        "We use this assignment as the reference annotation for our experiments to evaluate the output of our tagger.",
        "The remaining 25% tokens (ambiguous words) have 2 or more tags in the lexicon and are thus ignored during evaluation since the correct reference tag cannot be determined.",
        "Both the LCA and the MSA Treebank data sets were transcribed in standard MSA script.",
        "The LCA corpus only consists of raw orthographic transcriptions; no further annotations exist for this data set.",
        "Each word in the Treebank corpus is associated with all of its possible POS tags; the correct tag has been marked manually.",
        "We use the undecomposed word forms rather than the forms resulting from splitting off conjunctions, prepositions, and other clitics.",
        "Although improved tokenization can be expected to result in better tagging performance, tokenization tools for dialectal Arabic are currently not available, and our goal was to create comparable conditions for tagging across all of our data sets.",
        "Preprocessing of the data thus only included removing punctuation from the MSA data and removing word fragments from the spoken language corpora.",
        "Other disfluen-cies (fillers and repetitions) were retained since they are likely to have predictive value.",
        "Finally, singleton words (e.g. inconsistent spellings) were removed",
        "from the LCA data.",
        "The properties of the different data sets (number of words, n-grams, and ambiguous words) are displayed in Table 1.",
        "The only resource we utilize in addition to raw data is the LDC-distributed Buckwalter stemmer.",
        "The stemmer analyzes MSA script forms and outputs all possible morphological analyses (stems and POS tags, as well as diacritizations) for the word.",
        "The analysis is based on an internal stem lexicon combined with rules for affixation.",
        "Although the stemmer was developed primarily for MSA, it can accommodate a certain percentage of dialectal words.",
        "Table 2 shows the percentages of word types and tokens in the ECA and LCA corpora that received an analysis from the Buckwalter stemmer.",
        "Since both sets contain dialectal as well as standard MSA forms, it is not possible to determine precisely how many of the unanalyzable forms are dialectal forms vs. words that were rejected for other reasons, such as misspellings.",
        "The higher percentage of rejected word types in the ECA corpus is most likely due to the non-standard script forms described above.",
        "The POS tags used in the LDC ECA annotation and in the Buckwalter stemmer are rather fine-grained; furthermore, they are not identical.",
        "We therefore mapped both sets of tags to a unified, simpler tagset consisting only of the major POS categories listed in Table 2.",
        "The mapping from the original Buckwalter tag to the simplified set was based on the conversion scheme suggested in (Bies, 2003).",
        "The same underlying conversion rules were applicable to most of the LDC tags; those cases that could not be determined automatically were converted by hand.",
        "rence of each tag in the ECA corpus.",
        "Prior to the development of our tagger we computed the cross-corpus coverage of word n-grams in the ECA development set, in order to verify our assumption that utilizing data from other dialects might be helpful.",
        "Table 4 demonstrates that the n-gram coverage of the ECA development set increases slightly by adding LCA and MSA data."
      ]
    },
    {
      "heading": "3 Baseline Tagger",
      "text": [
        "We use a statistical trigram tagger in the form of a hidden Markov model (HMM).",
        "Let w0:M be a sequence of words (w0, w1, ... , wM) and t0:M be the corresponding sequence of tags.",
        "The trigram HMM computes the conditional probability of the word and tag sequence p(w0:M, t0:M) as:",
        "The lexical model p(wi l ti) characterizes the distribution of words for a specific tag; the contextual model p(ti l ti-1 , ti-2) is trigram model over the tag sequence.",
        "For notational simplicity, in subsequent sections we will denote p(tilti-1,ti-2) as p(ti l hi), where hi represents the tag history.",
        "The HMM is trained to maximize the likelihood of the training data.",
        "In supervised training, both tag and word sequences are observed, so the maximum likelihood estimate is obtained by relative frequency counting, and, possibly, smoothing.",
        "During unsupervised training, the tag sequences are hidden, and the Expectation-Maximization Algorithm is used to iteratively update probabilities based on expected counts.",
        "Unsupervised tagging requires a lexicon specifying the set of possible tags for each word.",
        "Given a test sentence w�0:M, the Viterbi algorithm is used to find the tag sequence maximizing the probability of tags given words: t�0:M = argmaxt0:�p(t0:Mlw�0:M).",
        "Our taggers are implemented using the Graphical Models Toolkit (GMTK) (Bilmes and Zweig, 2002), which allows training a wide range of probabilistic models with both hidden and observed variables.",
        "As a first step, we compare the performance of four different baseline systems on our ECA dev set.",
        "First, we trained a supervised tagger on the MSA treebank corpus (System I), in order to gauge how a standard system trained on written Arabic performs on dialectal speech.",
        "The second system (System II) is a supervised tagger trained on the manual ECA POS annotations.",
        "System III is an unsupervised tagger on the ECA training set.",
        "The lexicon for this system is derived from the reference annotations of the training set – thus, the correct tag is not known during training, but the lexicon is constrained by expert information.",
        "The difference in accuracy between Systems II and III indicates the loss due to the unsupervised training method.",
        "Finally, we trained a system using only the unannotated ECA data and a lexicon generated by applying the MSA analyzer to the training corpus and collecting all resulting tags for each word.",
        "In this case, the lexicon is much less constrained; moreover, many words do not receive an output from the stemmer at all.",
        "This is the training method with the least amount of supervision and therefore the method we are interested in most.",
        "Table 5 shows the accuracies of the four systems on the ECA development set.",
        "Also included is a breakdown of accuracy by analyzable (AW), unanalyzable (UW), and out-of-vocabulary (OOV) words.",
        "Analyzable words are those for which the stemmer outputs possible analyses; unanalyzable words cannot be processed by the stemmer.",
        "The percentage of unanalyzable word tokens in the dev set is 18.8%.",
        "The MSA-trained tagger (System I) achieves an accuracy of 97% on a held-out set (1 17k words) of MSA data, but performs poorly on ECA due to a high OOV rate (43%).",
        "By contrast, the OOV rate for taggers trained on ECA data is 9.5%.",
        "The minimally-supervised tagger (System IV) achieves a baseline accuracy of 62.76%.",
        "In the following sections, we present several methods to improve this system, in order to approximate as closely as possible the performance of the supervised systems.",
        "1"
      ]
    },
    {
      "heading": "4 System Improvements",
      "text": []
    },
    {
      "heading": "4.1 Adding Affix Features",
      "text": [
        "The low accuracy of unanalyzable and OOV words may significantly impact downstream applications.",
        "'The accuracy of a naive tagger which labels all words with the most likely tag (NN) achieves an accuracy of 20%.",
        "A tagger which labels words with the most likely tag amongst its possible tags achieves an accuracy of 52%.",
        "One common way to improve accuracy is to add word features.",
        "In particular, we are interested in features that can be derived automatically from the script form, such as affixes.",
        "Affix features are added in a Naive Bayes fashion to the basic HMM model defined in Eq.1.",
        "In addition to the lexical model p(wilti) we now have prefix and suffix models p(ail ti) and p(bilti), where a and b are the prefix and suffix variables, respectively.",
        "The affixes used",
        "- , - .",
        "Affixes are derived for each word by simple substring matching.",
        "More elaborate techniques are not used due to the philosophy of staying within a minimally supervised approach that does not require dialect-specific knowledge."
      ]
    },
    {
      "heading": "4.2 Constraining the Lexicon",
      "text": [
        "The quality of the lexicon has a major impact on unsupervised HMM training.",
        "Banko et.",
        "al.",
        "(2004) demonstrated that tagging accuracy improves when the number of possible tags per word in a “noisy lexicon” can be restricted based on corpus frequency.",
        "In the current setup, words that are not analyzable by the MSA stemmer are initally assigned all possible tags, with the exception of obvious restricted tags like the begin and end-of-sentence tags, NOTAG, etc.",
        "Our goal is to constrain the set of tags for these unanalyzable words.",
        "To this end, we cluster both analyzable and unanalyzable words, and reduce the set of possible tags for unanalyzable words based on its cluster membership.",
        "Several different clustering algorithms could in principle be used; here we utilize Brown’s clustering algorithm (Brown and others, 1992), which iteratively merges word clusters with high mutual information based on distributional criteria.",
        "The tagger lexicon is then derived as follows:",
        "1.",
        "Generate K clusters of words from data.",
        "2.",
        "For each cluster C, calculate P(tlC) = P w�A,C P(tlw)P(wlC) where t and w are the word and tag, and A is the set of analyzable words.",
        "3.",
        "The cluster’s tagset is determined by choosing all tags t' with P(t'lC) above a certain threshold -y.",
        "4.",
        "All unanalyzable words within this cluster are given these possible tags.",
        "The number of clusters K and the threshold y are variables that affect the final tagset for unanalyzable words.",
        "Using K = 200 and y = 0.05 for instance, the number of tags per unanalyzable word reduces to an average of four and ranges from one to eight tags.",
        "There is a tradeoff regarding the degree of tagset reduction: choosing fewer tags results in less confusability but may also involve the removal of the correct tag from a word’s lexicon entry.",
        "We did not optimize for K and y since an annotated development set for calculating accuracy is not available in a minimally supervised approach in practice.",
        "Nevertheless, we have observed that tagset reduction generally leads to improvements compared to the baseline system with an unconstrained lexicon.",
        "The improvements gained from adding affix features to System IV and constraining the lexicon are shown in Table 6.",
        "We notice that adding affix features yields improvements in OOV accuracy.",
        "The relationship between the constrained lexicon and unanalyzable word accuracy is less straighforward.",
        "In this case, the degradation of unanalyzable word accuracy is due to the fact that the constrained lexicon over-restricts the tagsets of some frequent unanalyzable words.",
        "However, the constrained lexicon generally improves the overall accuracy and is thus a viable technique.",
        "Finally, the combination of affix features and constrained lexicon results in a tagger with 69.83% accuracy, which is a 7% absolute improvement over System IV."
      ]
    },
    {
      "heading": "5 Cross-Dialectal Data Sharing",
      "text": [
        "Next we examine whether unannotated corpora in other dialects (LCA) can be used to further improve the ECA tagger.",
        "The problem of data sparseness for Arabic dialects would be less severe if we were able to exploit the commonalities between similar dialects.",
        "In natural language processing, Kim & Khu",
        "danpur (2004) have explored techniques for using parallel Chinese/English corpora for language modeling.",
        "Parallel corpora have also been used to infer morphological analyzers, POS taggers, and noun phrase bracketers by projections via word alignments (Yarowsky et al., 2001).",
        "In (Hana et al., 2004), Czech data is used to develop a morphological analyzer for Russian.",
        "In contrast to these works, we do not require parallel/comparable corpora or a bilingual dictionary, which may be difficult to obtain.",
        "Our goal is to develop general algorithms for utilizing the commonalities across dialects for developing a tool for a specific dialect.",
        "Although dialects can differ very strongly, they are similar in that they exhibit morphological simplifications and a different word order compared to MSA (e.g. SVO rather than VSO order), and close dialects share some vocabulary.",
        "Each of the tagger components (i.e. contextual model p(tilhi), lexical model p(wilti), and affix model p(ai l ti)p(bi l ti)) can be shared during training.",
        "In the following, we present two approaches for sharing data between dialects, each derived from following different assumptions about the underlying data generation process."
      ]
    },
    {
      "heading": "5.1 Contextual Model Interpolation",
      "text": [
        "Contextual model interpolation is a widely-used data-sharing technique which assumes that models trained on data from different sources can be “mixed” in order to provide the most appropriate probability distribution for the target data.",
        "In our case, we have LCA as an out-of-domain data source, and ECA as the in-domain data source, with the former being about 4 times larger than the latter.",
        "If properly combined, the larger amount of out-of-domain data might improve the robustness of the in-domain tagger.",
        "We therefore use a linear interpolation of in-domain and out-of-domain contextual models.",
        "The joint probability p(w0:M, t0:M) becomes:",
        "Here A defines the interpolation weights for the ECA contextual model pE (ti l hi) and the LCA contextual model pL (ti l hi).",
        "pE (wn l tn) is the ECA lexical model.",
        "The interpolation weight A is estimated by maximizing the likelihood of a held-out data set given the combined model.",
        "As an extension, we allow the interpolation weights to be a function of the current tag: A (ti ), since class-dependent interpolation has shown improvements over basic interpolation in applications such as language modeling (Bu-lyko et al., 2003)."
      ]
    },
    {
      "heading": "5.2 Joint Training of Contextual Model",
      "text": [
        "As an alternative to model interpolation, we consider training a single model jointly from the two different data sets.",
        "The underlying assumption of this technique is that tag sequences in LCA and ECA are generated by the same process, whereas the observations (the words) are generated from the tag by two different processes in the two different dialects.",
        "The HMM model for joint training is expressed as:",
        "A single conditional probability table is used for the contextual model, whereas the lexical model switches between two different parameter tables, one for LCA observations and another for ECA observations.",
        "During training, the contextual model is trained jointly from both ECA and LCA data; however, the data is divided into ECA and LCA portions when updating the lexical models.",
        "Both the contextual and lexical models are trained within the same training pass.",
        "A graphical model representation of this system is shown in Figure 1.",
        "This model can be implemented using the functionality of switching parents (Bilmes, 2000) provided by GMTK.",
        "During decoding, the tagger can in principle switch its lexical model to ECA or LCA, depending on the input; this system thus is essentially a multi-dialect tagger.",
        "In the experiments reported below, however, we exclusively test on ECA, and the LCA lexical model is not used.",
        "Due to the larger amount of data available for contextual model, joint training can be expected to improve the performance on the target dialect.",
        "The affix models can be trained jointly in a similar fashion."
      ]
    },
    {
      "heading": "5.3 Data sharing results",
      "text": [
        "The results for data sharing are shown in Table 7.",
        "The systems Interpolate-A and Interpolate-A(ti) are taggers built by interpolation and class-dependent interpolation, respectively.",
        "For joint training, we present results for two systems: JointTrain(1:4) is trained on the existing collection ECA and LCA data, which has a 1:4 ratio in terms of corpus size; JointTrain(2:1) weights the ECA data twice, in order to bias the training process more towards ECA’s distribution.",
        "We also provide results for two more taggers: the first (CombineData) is trained “naively” on the pooled data from both ECA and LCA, without any weighting, interpolation, or changes to the probabilistic model.",
        "The second (CombineLex) uses a contextual model trained on ECA and a lexical model estimated from both ECA and LCA data.",
        "The latter was trained in order to assess the potential for improvement due to the reduction in OOV rate on the dev set when adding the LCA data (cf. Table 4).",
        "All the above systems utilize the constrained lexicon, as it consistently gives improvements.",
        "Table 7 shows that, as expected, the brute-force combination of training data is not helpful and degrades performance.",
        "CombineLex results in higher accuracy but does not outperform models in Table 6.",
        "The same is true of the taggers using model interpolation.",
        "The best performance is obtained by the system using the joint contextual model with separate lexical models, with 2:1 weighting of ECA vs. LCA data.",
        "Finally, we added word affix information to the best shared-data system, which resulted in an accuracy of 70.88%.",
        "In contrast, adding affix to CombineData achieves 61.78%, suggesting that improvements in JointTrain comes from the joint training technique rather than simple addition of new data.",
        "This result is directly comparable to the best system in Section 4 (last row of Table 6)2.",
        "The analysis of tagging errors revealed that the most frequent confusions are between VBD/NNS, 2We also experimented with joint training of ECA+MSA.",
        "This gave good OOV accuracy, but overall it did not improve over the best system in Section 4.",
        "Also, note that all accuracies are calculated by ignoring the scoring of ambiguous words, which have several possible tags as the correct reference.",
        "If we score the ambiguous words as correct when the hypothesized tag is within this set, the accuracy of ECA+LCA+affi x JointTrain rises to 77.18%, which is an optimistic upper-bound on the total accuracy.",
        "VBP/VBD, and JJ/NN.",
        "Commonly mistagged words include cases like (“means”-3rd.sg), which is labeled as a particle in the reference but is most often tagged as a verb, which is also a reasonable tag."
      ]
    },
    {
      "heading": "6 Discussion and Future Work",
      "text": [
        "Table 8 highlights the performance of the various taggers on the ECA evaluation set.",
        "The accuracy of the unsupervised HMM tagger (System IV) improves from 58.47% to 66.61% via the affix features and constrained lexicon, and to a 68.48% by including joint training.",
        "These improvements are statistical significant at the 0.005 level according to a difference-of-proportions test.",
        "Several of the methods proposed here deserve further work: first, additional ways of constraining the lexicon can be explored, which may include imposing probability distributions on the possible tags for unanalyzable words.",
        "Other clustering algorithms (e.g. root-based clustering of Arabic (De Roeck and",
        "Al-Fares, 2000)), may be used instead of, or in addition to, distribution-based clustering.",
        "Cross-dialectal data sharing for tagging also deserves more research.",
        "For instance, the performance of the contextual model interpolation might be increased if one trains interpolation weights dependent on the classes based on previous two tags.",
        "Joint training of contextual model and data sharing for lexical models can be combined; other dialectal data may also be added into the same joint training framework.",
        "It would also be useful to extend these methods to create a more fine-grained part-of-speech tagger with case, person, number, etc.",
        "information.",
        "Stems, POS, and fine-grained POS can be combined into a factorial hidden Markov model, so that relationships between the stems and POS as well as data sharing between dialects can be simultaneously exploited to build a better system.",
        "In conclusion, we have presented the first steps towards developing a dialectal Arabic tagger with minimal supervision.",
        "We have shown that adding affix features and constraining the lexicon for unanalyzable words are simple resource-light methods to improve tagging accuracy.",
        "We also explore the possibility of improving an ECA tagger using LCA data and present two data sharing methods.",
        "The combination of these techniques yield a 10% improvement over the baseline."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This material is based on work funded by the NSF and the CIA under NSF Grant No.",
        "IIS-0326276.",
        "Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of these agencies."
      ]
    }
  ]
}
