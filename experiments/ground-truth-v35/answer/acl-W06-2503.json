{
  "info": {
    "authors": [
      "Diana McCarthy"
    ],
    "book": "Workshop on Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together",
    "id": "acl-W06-2503",
    "title": "Relating WordNet Senses for Word Sense Disambiguation",
    "url": "https://aclweb.org/anthology/W06-2503",
    "year": 2006
  },
  "references": [
    "acl-C94-2113",
    "acl-H05-1053",
    "acl-H93-1061",
    "acl-J98-1003",
    "acl-J98-1004",
    "acl-N01-1010",
    "acl-P04-1036",
    "acl-P05-1005",
    "acl-P98-2127",
    "acl-W04-0811"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The granularity of word senses in current general purpose sense inventories is often too fine-grained, with narrow sense distinctions that are irrelevant for many NLP applications.",
        "This has particularly been a problem with WordNet which is widely used for word sense disambiguation (WSD).",
        "There have been several attempts to group WordNet senses given a number of different information sources in order to reduce granularity.",
        "We propose relating senses as a matter of degree to permit a softer notion of relationships between senses compared to fixed groupings so that granularity can be varied according to the needs of the application.",
        "We compare two such approaches with a gold-standard produced by humans for this work.",
        "We also contrast this gold-standard and another used in previous research with the automatic methods for relating senses for use with back-off methods for WSD."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "It is likely that accurate word-level semantic disambiguation would benefit a number of different types of NLP application; however it is generally acknowledged by word sense disambiguation (WSD) researchers that current levels of accuracy need to be improved before WSD technology can usefully be integrated into applications (Ide and Wilks, in press).",
        "There are at least two major problems facing researchers in this area.",
        "One major problem is the lack of sufficient training data for supervised WSD systems.",
        "One response to this is",
        "to exploit the natural skew of the data and focus on finding the first (predominant) sense from a sample of text (McCarthy et al., 2004).",
        "Further contextual WSD may be required, but the technique provides a useful unsupervised back-off method.",
        "The other major problem for WSD is the granularity of the sense inventory since a preexisting lexical resource is often too fine-grained, with narrow sense distinctions that are irrelevant for the intended application.",
        "For example, WordNet (Fell-baum, 1998) which is widely used and publicly available, has a great many subtle distinctions that may in the end not be required.",
        "For example, in figure 1 we show the three senses (WNs#) for evidence from WordNet version 1.7.",
        "1 These are all clearly related.",
        "One promising approach for improving accuracy is to disambiguate to a coarser-grained inventory, which groups together the related senses of a word.",
        "This can be done either by defining the inventory specifically for the application, which might be most appropriate for machine translation, where correspondences across languages could 'We use WordNet 1.7 throughout this paper since the resources we use for evaluation were produced for this version.",
        "determine the inventory (Resnik and Yarowsky, 2000).",
        "There are however many systems using man-made resources, particularly WordNet, which have other purposes in mind, such as entailment for applications such as question-answering and information-extraction (Dagan et al., 2005).",
        "There have been several attempts to group WordNet senses using various different types of information sources.",
        "This paper describes work to automatically relate WordNet word senses using automatically acquired thesauruses (Lin, 1998) and WordNet similarity measures (Patwardhan and Pedersen, 2003).",
        "This work proposes using graded word sense relationships rather than fixed groupings (clusters).",
        "Previous research has focused on clustering WordNet senses into groups.",
        "One problem is that to do this a stopping condition is required such as the number of clusters required for each word.",
        "This has been done with the numbers determined by the gold-standard for the purposes of evaluation (Agirre and Lopez de Lacalle, 2003) but ultimately the right number of classes for each word cannot usually be predetermined even if one knows the application, unless only a sample of words are being handled.",
        "In cases where a gold-standard is provided by humans it is clear that further relationships could be drawn.",
        "For example, in the groups (hereafter referred to as SEGR) made publicly available for the SENSEVAL-2 English lexical sample (Kilgarriff, 2001) (hereafter referred to as SEVAL-2 ENG LEX) child is grouped as shown in table 1.",
        "Whilst it is perfectly reasonable the grouping decision was determined by the ‘youth’ vs ‘descendant’ distinction, the relationships between non-grouped senses, notably sense numbers 1 and 2 are apparent.",
        "It is quite possible that these senses will share contextual cues useful for WSD and distinction between the two might not be relevant in a given application, for example because they are translated in the same way (ni˜no/a in Spanish can mean both young boy/girl and son/daughter) or have common substitutions (boy/girl can be used as both offspring or young person).",
        "Instead of clustering senses into groups we evaluate 2 methods that produce ranked lists of related senses for each target word sense.",
        "We refer to these as RLISTs.",
        "Such listings resemble nearest neighbour approaches for automatically acquired thesauruses.",
        "They allow for a sense to be related to others which may not themselves be closely re",
        "lated.",
        "Since only a fixed number of senses are defined for each word, the RLISTs include all senses of the word.",
        "A cut-off can then be determined for any particular application.",
        "Previous research on clustering word senses has focused on comparison to the SEGR gold-standard.",
        "We evaluate the RLISTs against a new gold-standard produced by humans for this research since the SEGR does not have documentation with figures for inter-tagger agreement.",
        "As well as evaluating against a gold-standard, we also look at the effect of the RLISTs and the gold-standards themselves on WSD.",
        "Since the focus of this paper is not the WSD system, but the sense inventory, we use a simple WSD heuristic which uses the first sense of a word in all contexts, where the first sense of every word is specified by a resource.",
        "While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all-words task (Snyder and Palmer, 2004).",
        "We contrast the performance of first sense heuristics i) from SemCor (Miller et al., 1993) and ii) derived automatically from the sNC following (McCarthy et al., 2004) and also iii) an upper-bound first sense heuristic extracted from the test data.",
        "The paper is organised as follows.",
        "In the next section we describe some related work.",
        "In section 3 we describe the two methods we will use to relate senses.",
        "Our experiments are described in section 4.",
        "In 4.1 we describe the construction of a new gold-standard produced using the same sense inventory used for SEGR, and give inter-annotator agreement figures for the task.",
        "In section 4.2 we compare our methods to the new gold-standard and in section 4.3 we investigate how much effect coarser grained sense distinctions have on WSD using naive first sense heuristics.",
        "We follow this with a discussion and conclusion."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "There is a significant amount of previous work on grouping WordNet word senses using a number of different information sources, such as predicate argument structure (Palmer et al., forthcoming), information from WordNet (Mihalcea and Moldovan, 2001; Tomuro, 2001) 2 and other lexical resources (Peters and Peters, 1998) translations, system confusability, topic signature and contextual evidence (Agirre and Lopez de Lacalle, 2003).",
        "There is also work on grouping senses of other inventories using information in the inventory (Dolan, 1994) along with information retrieval techniques (Chen and Chang, 1998).",
        "One method presented here (referred to as DIST and described in section 3) relates most to that of Agirre and Lopez de Lacalle (2003).",
        "They use contexts of the senses gathered directly from either manually sense tagged corpora, or using instances of “monosemous relatives” which are monosemous words related to one of the target word senses in WordNet.",
        "We use contexts of occurrence indirectly.",
        "We obtain “nearest neighbours” which occur in similar contexts to the target word.",
        "A vector is created for each word sense with a WordNet similarity score between the sense and each nearest neighbour of the target word.",
        "3 While related senses may not have a lot of shared contexts directly, because of sparse data, they may have semantic associations with the same subset of words that share similar distributional contexts with the target word.",
        "This method avoids reliance on sense-tagged data or monosemous relatives because the distributional neighbours can be obtained automatically from raw text.",
        "Our other method relates to the findings of Kohomban and Lee (2005).",
        "We use the Jiang-Conrath score (JCN) in the WordNet Similarity Package.",
        "This is a distance measure between WordNet senses given corpus frequency counts and the structure of the WordNet hierarchy.",
        "It is described in more detail below.",
        "Kohomban and Lee (2005) get good results on disambiguation of the SENSEVAL all-words tasks using the 25 unique beginners from the WordNet hierarchy for training a coarse-grained WSD system and then using a first sense heuristic (provided using the frequency 2Mihalcea and Moldovan group WordNet synonym sets (synsets) rather than word senses.",
        "3 We have not tried using these vectors for relating senses of different words, but leave that for future research.",
        "data in SemCor) to determine the fine-grained output.",
        "This shows that the structure of WordNet is indeed helpful when selecting coarse senses for WSD.",
        "We use the JCN measure to contrast with our DIST measure which uses a combination of distributional neighbours and JCN.",
        "We have experimented only with nouns to date, although in principle our method can be extended for other POS."
      ]
    },
    {
      "heading": "3 Methods for producing RLISTs",
      "text": [
        "JCN This is a measure from the WordNet similarity package (Patwardhan and Pedersen, 2003) originally proposed as a distance measure (Jiang and Conrath, 1997).",
        "JCN uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.",
        "Each synset is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.",
        "The frequency data is used to calculate the “information content” (IC) of a class (IC(s) = – log(p(s))) and with this, Jiang and Conrath specify a distance measure: Djcn(s1, s2) = IC(s1)+IC(s2) – 2 × IC(s3) where the third class (s3) is the most informative, or most specific, superordinate synset of the two senses s1 and s2.",
        "This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:",
        "We use raw BNC data for calculating IC values.",
        "DIST We use a distributional similarity measure (Lin, 1998) to obtain a fixed number (50) of the top ranked nearest neighbours for the target nouns.",
        "For input we used grammatical relation data extracted using an automatic parser (Briscoe and Carroll, 2002).",
        "We used the 90 million words of written English from the British National Corpus (BNC) (Leech, 1992).",
        "For each noun we collect co-occurrence triples featuring the noun in a grammatical relationship with another word.",
        "The words and relationships considered are co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.",
        "Using this data, we compute the distributional similarity proposed by Lin between each pair of nouns, where the nouns have at least 10 triples.",
        "Each noun (w) is then listed with k (= 50) most similar nouns (the nearest neighbours).",
        "The nearest neighbours for a target noun (w) share distributional contexts and are typically se",
        "nearest neighbours mantically related to the various senses (5,,,) of w. The relationships between the various senses are brought out by the shared semantic relationships with the neighbours.",
        "For example the top nearest neighbours of chair include: stool, bench, chairman, furniture, staff, president.",
        "The senses of chair are 1 seat, 2 professorship, 3 chairperson and 4 electric chair.",
        "The seat and electric chair senses share semantic relationships with neighbours such as stool, bench, furniture whilst the professorship and chairperson senses are related via neighbours such as chairman, president, staff.",
        "The semantic similarity between a neighbour (n) e.g. stool and a word sense (si E 5,,,) e.g. electric chair is measured using the JCN measure described above.",
        "To relate the set of senses (5,,,) of a word (w) �Vsi = (f1... fk) with k features for each si E 5,,,.",
        "The jth feature in �Vsi is the highest JCN score between all senses of the jth neighbour and si.",
        "Figure 2 illustrates this process for chair.",
        "In contrast to using JCN between senses directly, the nearest neighbours permit senses in unrelated areas of WordNet to be related e.g. painting - activity and painting - object since both senses may have neighbours such as drawing in common.",
        "The vectors are used to produce RLISTs for each si.",
        "To produce the RLIST of a sense si of w we obtain a value for the Spear-man rank correlation coefficient (r) between the vector for si and that for each of the other senses of w (sl E 5,,,, where l =� i).",
        "r is calculated by obtaining rankings for the neighbours on �Vsi and �Vsl using the JCN values for ranking.",
        "We then list si with the other senses ordered according to the r value, for example the RLIST for sense 1 of chair is [4 (0.50), 3 (0.34), 2 (0.20)] where the sense number is indicated before the bracketed r score."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "For our experiments we use the same set of 20 nouns used by Agirre and Lopez de Lacalle (2003).",
        "The gold standard used in that work was SEGR.",
        "These groupings were released for SEN-SEVAL-2 but we cannot find any documentation on how they were produced or on inter-annotator agreement.",
        "4 We have therefore produced a new gold-standard (referred to as RS) for these nouns which we describe in section 4.1.",
        "We compare the results of our methods for relating senses and SEGR to RS.",
        "We then look at the performance of both the gold-standard groupings (SEGR and RS) compared to our automatic methods for coarser grained WSD of SEVAL-2 ENG LEX using some first sense heuristics."
      ]
    },
    {
      "heading": "4.1 Creating a Gold Standard",
      "text": [
        "To create the gold-standard we gave 3 native en-glish speakers a questionnaire with all possible pairings of WordNet 1.7 word senses for each of the 20 nouns in turn.",
        "The pairs were derived from all possible combinations of senses of the given noun and the judges were asked to indicate a “related”, “unrelated” or don’t know response for each pair.",
        "5 This task allows a sense to be related to others which are not themselves related.",
        "The ordering of the senses was randomised and fake IDs were generated instead of using the sense numbers provided with WordNet to avoid possible bias from indications of sense predominance.",
        "The words were presented one at a time and each combination of senses was presented along with the WordNet gloss.",
        "6 Table 2 provides the pairwise agreement (PWA) figures for each word along with the overall PWA figure.",
        "The number of word senses for each noun is given in brackets.",
        "Overall, more relationships were identified compared to the rather fine-grained classes in SEGR, although there was some variation.",
        "The proportion of related items for our three judges were 52.2%, 56.5% and 22.6% respectively.",
        "Given this variation, the last row gives the pairwise agreement for pairs where the more lenient judge has said the pair is unrelated.",
        "These figures are reasonable given that humans differ in their tendency to lump or split",
        "senses and the fact that figures for sense annotation with three judges (as opposed to two, with a third to break ties) are reported in this region (Koeling et al., 2005).",
        "Again, there are no details on annotation and agreement for SEGR."
      ]
    },
    {
      "heading": "4.2 Agreement of automatic methods with RS",
      "text": [
        "Figure 3 shows the PWA of the automatic methods JCN and DIST when calculated against the RS gold-standard at various threshold cut-offs.",
        "The difference of the best performance for these two methods (61.1% DIST and 62.2% for JCN) are not statistically significant (using the chi-squared test).",
        "The baseline which assumes that all pairs are unrelated is 54.1%.",
        "If we compare the SEGR to RS we get 68.9% accuracy.",
        "7 This shows that the SEGR accords with RS more than the automatic methods."
      ]
    },
    {
      "heading": "4.3 Application to SEVAL-2 ENG LEX",
      "text": [
        "We used the same words as in the experiment above and applied our methods as back-off to naive WSD heuristics on the SEVAL-2 ENG LEX 7Since these are groupings, there is only one possible answer and no thresholds are applied."
      ]
    },
    {
      "heading": "DIST",
      "text": [
        "test data.",
        "8 Using predominant senses is useful as a back-off method where local context is not sufficient.",
        "Disambiguation is performed using the first sense heuristic from i) SemCor (Semcor FS) ii) automatic rankings from the BNC produced using the method proposed by McCarthy et al.",
        "(2004) (Auto FS) and iii) an upper-bound first sense heuristic from the SEVAL-2 ENG LEX data itself (SEVAL-2 FS).",
        "This represents how well the method would perform if we knew the first sense.",
        "The results are shown in table 3.",
        "The accuracy figures are equivalent to both recall and precision as there were no words in this data without a first sense in either SemCor or the automatic rankings.",
        "The fourth row provides a random baseline which incorporates the number of related senses for each instance.",
        "Usually this is calculated as the sum of PwEtokens 1 sw 1 over all word tokens.",
        "Since we are evaluating RLISTs, as well as groups, the number of senses for a given word is not fixed, but depends in the token sense.",
        "We therefore calculate the random base-relatedsensestows line as PwsEtokens1Sw1 ,where ws is a word sense of word w. The columns show the results for different ways of relating senses; the senses are in the same group or above the threshold for RLISTs.",
        "The second column (fine-grained) are the results for these first sense heuristics with the raw WordNet synsets.",
        "The third and fourth columns are the results for the SEGR and RS gold standards.",
        "The final four columns give the results for RLISTs with JCN and DIST with the threshold indicated.",
        "8We performed the experiment on both the SENSEVAL-2 English lexical sample training and test data with very similar results, but just show the results on the test corpus due to lack of space.",
        "SemCor FS outperforms Auto FS, and is itself outperformed by the upper-bound, SEVAL-2 FS.",
        "All methods of relating WordNet synsets increase the accuracy at the expense of an increased baseline because the task is easier with less senses to discriminate between.",
        "Both JCN and DIST have threshold values which improve performance of the first sense heuristics more than the manually created SEGR given a comparable or a lower baseline (smaller classes, and a harder task) e.g. SE-VAL-2 FS and Auto FS for both types of RLISTs though SemCor FS only for JCN.",
        "RS should be compared to performance of JCN and DIST at a similar baseline so we show these in the 6th and 8th columns of the table.",
        "In this case the RS seems to outperform the automatic methods, but the results for JCN are close enough to be encouraging, especially considering the baseline 63.5 is lower than that for RS (65.3).",
        "The RLISTs permit a trade-off between accuracy and granularity.",
        "This can be seen by the graph in figure 5 which shows the accuracy obtained for the three first sense heuristics at a range of threshold values.",
        "The random baseline is also shown.",
        "The difference in performance compared to the baseline for a given heuristic is typically better on the fine-grained task, however the benefits of a coarse-grained inventory will depend not on this difference, but on the utility of the relationships and distinctions made between senses.",
        "We return to this point in the discussion and conclusions."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "The RLISTs show promising results when compared to the human produced gold-standards on a WSD task and even outperform the SEGR in most cases.",
        "There are other methods proposed in the literature which also make use of information in WordNet, particularly looking for senses with related words in common (Tomuro, 2001; Mihalcea and Moldovan, 2001).",
        "Tomuro does this to find systematic polysemy, by looking for overlap in words in different areas of WordNet.",
        "Evaluation is performed using WordNet cousins and inter-tagger agreement.",
        "Mihalcea and Moldovan look for related words in common between different senses of words to merge WordNet synsets.",
        "They also use the hand tagged data in SemCor to remove low frequency synsets.",
        "They demonstrate a large reduction in polysemy of the words in SemCor (up",
        "to 39%) with a small error rate (5.6%) measured on SemCor.",
        "Our DIST approach relates to Agirre and Lopez de Lacalle (2003) though they produced groups and evaluated against the SEGR.",
        "We use nearest neighbours and associate these with word senses, rather than finding occurrences of word senses in data directly.",
        "Nearest neighbours have been used previously to induce word senses from raw data (Pantel and Lin, 2002), but not for relating existing inventories of senses.",
        "Measures of distance in the WordNet hierarchy such as JCN have been widely used for WSD (Patwardhan et al., 2003) as well as the information contained in the structure of the hierarchy (Kohomban and Lee, 2005) which has been used for backing off when training a supervised system.",
        "Though coarser groupings can improve inter-tagger agreement and WSD there is also a need to examine which distinctions are useful since there are many ways that items can be grouped (Palmer et al., forthcoming).",
        "A major difference to previous work is our use of RLISTs, allowing for the level of granularity to be determined for a given application, and allowing for “soft relationships” so that a sense can be related to several others which are not themselves related.",
        "This might also be done with soft hierarchical clusters, but has not yet been tried.",
        "The idea of relating word sense as a matter of degree also relates to the methods of Sch¨utze (1998) although his work was evaluated using binary sense distinctions.",
        "The child example in table 1 demonstrate problems with hard, fixed groupings.",
        "Table 4 shows the RLISTs obtained with our methods, with the r scores in brackets.",
        "While many of the relationships in the SEGR are found, the relationships to the other senses are apparent.",
        "In SEGR no relationship is retained between the offspring sense (2) and the young person sense (1).",
        "According to the RS, all paired meanings of child are related.",
        "9 A distance measure, rather than a fixed grouping, seems appropriate to us because one might want the young person sense to be related to both human offspring and immature person, but not have the latter two senses directly related."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have investigated methods for relating WordNet word senses based on distributionally similar nearest neighbours and using the JCN measure.",
        "Whilst the senses for a given word can be clustered into sense groups, we propose the use of ranked lists to relate the senses of a word to each other.",
        "In this way, the granularity can be determined for a given application and the appropriate number of senses for a given word is not needed a priori.",
        "We have encouraging results for nouns when comparing RLISTs to manually created gold-standards.",
        "We have produced a new gold-standard for evaluation based on the words used in SEVAL-2 ENG LEx.",
        "We did this because there is no available documentation on inter-annotator agreement for the SEGR.",
        "In future, we hope to produce another gold-standard resource where the informants indicate a degree of relatedness, rather than a binary choice of related or unrelated for each pair.",
        "We would like to see the impact that coarser-grained WSD has on a task or application.",
        "Given the lack of a plug and play application for feeding disambiguated data, we hope to examine the benefits on some lexical acquisition tasks that might feed into an application, for example sense ranking (McCarthy et al., 2004) or selectional preference acquisition.",
        "At this stage we have only experimented with nouns, we hope to go on relating senses in other parts-of-speech, particularly verbs since they have very fine-grained distinctions in WordNet and many of the subtler distinctions are quite probably not important for some applications.",
        "(Palmer et al., forthcoming) has clearly demonstrated the necessity for using predicate-argument structure when grouping verb senses, so we want to exploit such information for verbs.",
        "We have focused on improving the first sense heuristic, but we plan to use our groupings with context-based WSD.",
        "To avoid a requirement for",
        "hand-tagged training data, we plan to exploit the collocates of nearest neighbours.",
        "Geoffrey Leech.",
        "1992.",
        "100 million words of English: the British National Corpus.",
        "Language Research, 28(1):1–13."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was funded by a Royal Society Dorothy Hodgkin Fellowship and a UK EPSRC project “ Ranking Word Senses for Disambiguation: Models and Applications.” We thank Siddharth Patwardhan and Ted Pedersen for making the WN Similarity package publicly available.",
        "We would also like to thank Adam Kilgarriff for suggesting the creation of the RS resource and Eneko Agirre, Oier Lopez de Lacalle, John Carroll and Rob Koeling for helpful discussions."
      ]
    }
  ]
}
