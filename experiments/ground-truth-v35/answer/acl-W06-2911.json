{
  "info": {
    "authors": [
      "Rie Kubota Ando"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W06-2911",
    "title": "Applying Alternating Structure Optimization to Word Sense Disambiguation",
    "url": "https://aclweb.org/anthology/W06-2911",
    "year": 2006
  },
  "references": [
    "acl-P04-1081",
    "acl-P05-1001",
    "acl-P05-1005",
    "acl-W02-1004",
    "acl-W02-1006",
    "acl-W03-0425",
    "acl-W04-0831",
    "acl-W04-0856"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a new application of the recently proposed machine learning method Alternating Structure Optimization (ASO), to word sense disambiguation (WSD).",
        "Given a set of WSD problems and their respective labeled examples, we seek to improve overall performance on that set by using all the labeled examples (irrespective of target words) for the entire set in learning a disambiguator for each individual problem.",
        "Thus, in effect, on each individual problem (e.g., disambiguation of “art”) we benefit from training examples for other problems (e.g., disambiguation of “bar”, “canal”, and so forth).",
        "We empirically study the effective use of ASO for this purpose in the multi-task and semi-supervised learning configurations.",
        "Our performance results rival or exceed those of the previous best systems on several Senseval lexical sample task data sets."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word sense disambiguation (WSD) is the task of assigning predefined senses to words occurring in some context.",
        "An example is to disambiguate an occurrence of “bank” between the “money bank” sense and the “river bank” sense.",
        "Previous studies e.g., (Lee and Ng, 2002; Florian and Yarowsky, 2002), have applied supervised learning techniques to WSD with success.",
        "A practical issue that arises in supervised WSD is the paucity of labeled examples (sense-annotated data) available for training.",
        "For example, the training set of the Senseval-21 English lexical sample",
        "task has only 10 labeled training examples per sense on average, which is in contrast to nearly 6K training examples per name class (on average) used for the CoNLL-2003 named entity chunking shared task2.",
        "One problem is that there are so many words and so many senses that it is hard to make available a sufficient number of labeled training examples for each of a large number of target words.",
        "On the other hand, this indicates that the total number of available labeled examples (irrespective of target words) can be relatively large.",
        "A natural question to ask is whether we can effectively use all the labeled examples (irrespective of target words) for learning on each individual WSD problem.",
        "Based on these observations, we study a new application of Alternating Structure Optimization (ASO) (Ando and Zhang, 2005a; Ando and Zhang, 2005b) to WSD.",
        "ASO is a recently proposed machine learning method for learning predictive structure (i.e., information useful for predictions) shared by multiple prediction problems via joint empirical risk minimization.",
        "It has been shown that on several tasks, performance can be significantly improved by a semi-supervised application of ASO, which obtains useful information from unlabeled data by learning automatically created prediction problems.",
        "In addition to such semi-supervised learning, this paper explores ASO multi-task learning, which learns a number of WSD problems simultaneously to exploit the inherent predictive structure shared by these WSD problems.",
        "Thus, in effect, each individual problem (e.g., disambiguation of “art”) benefits from labeled training examples for other problems (e.g., disambiguation of “bar”, disambiguation of “canal”, and so forth).",
        "The notion of benefiting from training data for other word senses is not new by itself.",
        "For instance,",
        "on the WSD task with respect to WordNet synsets, Kohomban and Lee (2005) trained classifiers for the top-level synsets of the WordNet semantic hierarchy, consolidating labeled examples associated with the WordNet sub-trees.",
        "To disambiguate test instances, these coarse-grained classifiers are first applied, and then fine-grained senses are determined using a heuristic mapping.",
        "By contrast, our approach does not require predefined relations among senses such as the WordNet hierarchy.",
        "Rather, we let the machine learning algorithm ASO automatically and implicitly find relations with respect to the disambiguation problems (i.e., finding shared predictive structure).",
        "Interestingly, in our experiments, seemingly unrelated or only loosely related word-sense pairs help to improve performance.",
        "This paper makes two contributions.",
        "First, we present a new application of ASO to WSD.",
        "We empirically study the effective use of ASO and show that labeled examples of all the words can be effectively exploited in learning each individual disambiguator.",
        "Second, we report performance results that rival or exceed the state-of-the-art systems on Senseval lexical sample tasks."
      ]
    },
    {
      "heading": "2 Alternating structure optimization",
      "text": [
        "This section gives a brief summary of ASO.",
        "We first introduce a standard linear prediction model for a single task and then extend it to a joint linear model used by ASO."
      ]
    },
    {
      "heading": "2.1 Standard linear prediction models",
      "text": [
        "In the standard formulation of supervised learning, we seek a predictor that maps an input vector (or feature vector) x E X to the corresponding output y E Y.",
        "For NLP tasks, binary features are often used",
        "– for example, if the word to the left is “money”, set the corresponding entry of x to 1; otherwise, set it to 0.",
        "A k-way classification problem can be cast ask binary classification problems, regarding output y = +1 and y = 1 as “in-class” and “out-of-class”, respectively.",
        "Predictors based on linear prediction models take the form: f (x) = wTx, where w is called a weight vector.",
        "A common method to obtain a predictor �f is regularized empirical risk minimization, which minimizes an empirical loss of the predictor (with",
        "A loss function L (•) quantifies the difference between the prediction f (Xi) and the true output �i, and r (•) is a regularization term to control the model complexity."
      ]
    },
    {
      "heading": "2.2 Joint linear models for ASO",
      "text": [
        "Consider m prediction problems indexed by f E {1 ... m}, each with nt samples (Xti, Yt) for i E {1 ... nt}, and assume that there exists a low-dimensional predictive structure shared by these m problems.",
        "Ando and Zhang (2005a) extend the above traditional linear model to a joint linear model so that a predictor for problem f is in the form:",
        "where I is the identity matrix.",
        "wt and vt are weight vectors specific to each problem `.",
        "Predictive structure is parameterized by the structure matrix O shared by all the m predictors.",
        "The goal of this model can also be regarded as learning a common good feature map Ox used for all the m problems."
      ]
    },
    {
      "heading": "2.3 ASO algorithm",
      "text": [
        "Analogous to (1), we compute O and predictors so that they minimize the empirical risk summed over all the problems:",
        "It has been shown in (Ando and Zhang, 2005a) that the optimizationproblem (3) has a simple solution using singular value decomposition (SVD) when we choose square regularization: r(ft) = A 1 1 wt 1 1 2 where A is a regularization parameter.",
        "Let ut = wt + OTvt .",
        "Then (3) becomes the minimization of the joint empirical risk written as: +�llue – T�ell2��(4) This minimization can be approximately solved by repeating the following alternating optimization procedure until a convergence criterion is met:",
        "1.",
        "Fix (O, {vt}), and find m predictors {ut} that minimizes the joint empirical risk (4).",
        "2.",
        "Fix m predictors {ut}, and find (O, {vt}) that minimizes the joint empirical risk (4).",
        "The first step is equivalent to training m predictors independently.",
        "The second step, which couples all the predictors, can be done by setting the rows of O to the most significant left singular vectors of the predictor (weight) matrix U = [u1,... , ur,,℄, and setting vt = Out.",
        "That is, the structure matrix O is computed so that the projection of the predictor matrix U onto the subspace spanned by O’s rows gives the best approximation (in the least squares sense) of U for the given row-dimension of O.",
        "Thus, intuitively, O captures the commonality of the m predictors.",
        "ASO has been shown to be useful in its semi-supervised learning configuration, where the above algorithm is applied to a number of auxiliary problems that are automatically created from the unlabeled data.",
        "By contrast, the focus of this paper is the multi-task learning configuration, where the ASO algorithm is applied to a number of real problems with the goal of improving overall performance on these problems."
      ]
    },
    {
      "heading": "3 Effective use of ASO on word sense disambiguation",
      "text": [
        "The essence of ASO is to learn information useful for prediction (predictive structure) shared by multiple tasks, assuming the existence of such shared structure.",
        "From this viewpoint, consider the target words of the Senseval-2 lexical sample task, shown in Figure 1.",
        "Here we have multiple disambiguation tasks; however, at a first glance, it is not entirely clear whether these tasks share predictive structure (or are related to each other).",
        "There is no direct semantic relationship (such as synonym or hyponym relations) among these words.",
        "Local word uni-grams in 5-word window, context word bi and trigrams of (w-2, w-1), (w+1,w+2),(w-1,w+1), (w-3, w-2, w-1), (w+1, w+2, w+3), (w-2, w-1, w+1), (w-1, w+1, w+2).",
        "Syntactic full parser output; see Section 3 for detail.",
        "Global all the words excluding stopwords.",
        "POS uni-, bi-, and trigrams in 5-word window.",
        "The goal of this section is to empirically study the effective use of ASO for improving overall performance on these seemingly unrelated disambiguation problems.",
        "Below we first describe the task setting, features, and algorithms used in our implementation, and then experiment with the Senseval2 English lexical sample data set (with the official training / test split) for the development of our methods.",
        "We will then evaluate the methods developed on the Senseval-2 data set by carrying out the Senseval-3 tasks, i.e., training on the Senseval-3 training data and then evaluating the results on the (unseen) Senseval-3 test sets in Section 4.",
        "Task setting In this work, we focus on the Senseval lexical sample task.",
        "We are given a set of target words, each of which is associated with several possible senses, and their labeled instances for training.",
        "Each instance contains an occurrence of one of the target words and its surrounding words, typically a few sentences.",
        "The task is to assign a sense to each test instance.",
        "Features We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stopwords) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syn",
        "tactic relations: syntactic information obtained from parser output.",
        "To generate syntactic relation features, we use the Slot Grammar-based full parser ESG (McCord, 1990).",
        "We use as features syntactic relation types (e.g., subject-of, object-of, and noun modifier), participants of syntactic relations, and bi-grams of syntactic relations / participants.",
        "Details of the other three types are shown in Figure 2.",
        "Implementation Our implementation follows Ando and Zhang (2005a).",
        "We use a modification of the Huber’s robust loss for regression: L(p, y) = (max(0,1 – py))2 if py > – 1; and – 4py otherwise; with square regularization (A = 10-4), and perform empirical risk minimization by stochastic gradient descent (SGD) (see e.g., Zhang (2004)).",
        "We perform one ASO iteration."
      ]
    },
    {
      "heading": "3.1 Exploring the multi-task learning configuration",
      "text": [
        "The goal is to effectively apply ASO to the set of word disambiguation problems so that overall performance is improved.",
        "We consider two factors: feature split and partitioning ofprediction problems."
      ]
    },
    {
      "heading": "3.1.1 Feature split and problem partitioning",
      "text": [
        "Our features described above inherently consist of four feature groups: local context (LC), global context (GC), syntactic relation (SR), and POS features.",
        "To exploit such a natural feature split, we explore the following extension of the joint linear model:",
        "where OjOTj = I for j E F, F is a set of disjoint feature groups, and x(j) (or v,(j)) is a portion of the feature vector x (or the weight vector vt) corresponding to the feature group j, respectively.",
        "This is a slight modification of the extension presented in (Ando and Zhang, 2005a).",
        "Using this model, ASO computes the structure matrix Oj for each feature group separately.",
        "That is, SVD is applied to the sub-matrix of the predictor (weight) matrix corresponding to each feature group j, which results in more focused dimension reduction of the predictor matrix.",
        "For example, suppose that F = {SR}.",
        "Then, we compute the structure matrix OSR from the corresponding sub-matrix of the predictor matrix U, which is the gray region of Figure 3 (a).",
        "The structure matrices O j for j �E F (associated with the white regions in the figure) should be regarded as being fixed to the zero matrices.",
        "Similarly, it is possible to compute a structure matrix from a subset of the predictors (such as noun disambiguators only), as in Figure 3 (b).",
        "In this example, we apply the extension of ASO with F = {SR} to three sets of problems (disambiguation of nouns, verbs, and adjectives, respectively) separately.",
        "To see why such partitioning may be useful for our WSD problems, consider the disambiguation of “bank” and the disambiguation of “save”.",
        "Since a “bank” as in “money bank” and a “save” as in “saving money” may occur in similar global contexts, certain global context features effective for recognizing the “money bank” sense may be also effective for disambiguating “save”, and vice versa.",
        "However, with respect to the position-sensitive local context features, these two disambiguation problems may not have much in common since, for instance, we sometimes say “the bank announced”, but we rarely say “the save announced”.",
        "That is, whether problems share predictive structure may depend on feature types, and in that case, seeking predictive structure for each feature group separately may be more effective.",
        "Hence, we experiment with the configurations with and without various feature splits using the extension of ASO.",
        "Our target words are nouns, verbs, and adjectives.",
        "As in the above example of “bank” (noun) and “save” (verb), the predictive structure of global context features may be shared by the problems irrespective of the parts of speech of the target words.",
        "However, the other types of features may be more dependent on the target word part of speech.",
        "There",
        "fore, we explore two types of configuration.",
        "One applies ASO to all the disambiguation problems at once.",
        "The other applies ASO separately to each of the three sets of disambiguation problems (noun disambiguation problems, verb disambiguation problems, and adjective disambiguation problems) and uses the structure matrix �z obtained from the noun disambiguation problems only for disambiguating nouns, and so forth.",
        "Thus, we explore combinations of two parameters.",
        "One is the set of feature groups F in the model (5).",
        "The other is the partitioning of disambiguation problems.",
        "In Figure 4, we compare performance on the Senseval-2 test set produced by training on the Senseval-2 training set using the various configurations discussed above.",
        "As the evaluation metric, we use the F-measure (micro-averaged)3 returned by the official Senseval scorer.",
        "Our baseline is the standard single-task configuration using the same loss function (modified Huber) and the same training algorithm (SGD).",
        "The results are in line with our expectation.",
        "To learn the shared predictive structure of local context (LC) and syntactic relations (SR), it is more advantageous to apply ASO to each of the three sets of problems (disambiguation of nouns, verbs, and adjectives, respectively), separately.",
        "By contrast, global context features (GC) can be more effectively exploited when ASO is applied to all the disambigua",
        "tion problems at once.",
        "It turned out that the configuration F = { P O S } does not improve the performance over the baseline.",
        "Therefore, we exclude POS from the feature group set F in the rest of our experiments.",
        "Comparison of F = {LC+SR+GC} (treating the features of these three types as one group) and F = {LC, SR, GC} indicates that use of this feature split indeed improves performance.",
        "Among the configurations shown in Figure 4, the best performance (67.8%) is obtained by applying ASO to the three sets of problems (corresponding to nouns, verbs, and adjectives) separately, with the feature split F = {LC, SR, GC}.",
        "ASO has one parameter, the dimensionality of the structure matrix Oz (i.e., the number of left singular vectors to compute).",
        "The performance shown in Figure 4 is the ceiling performance obtained at the best dimensionality (in {10, 25, 50,100,150, • • • }).",
        "In Figure 5, we show the performance dependency on Oz’s dimensionality when ASO is applied to all the problems at once (Figure 5 left), and when ASO is applied to the set of the noun disambiguation problems (Figure 5 right).",
        "In the left figure, the configuration F = { G C } (global context) produces better performance at a relatively low dimensionality.",
        "In the other configurations shown in these two figures, performance is relatively stable as long as the dimensionality is not too low."
      ]
    },
    {
      "heading": "3.2 Multi-task learning procedure for WSD",
      "text": [
        "Based on the above results on the Senseval-2 test set, we develop the following procedure using the feature split and problem partitioning shown in Figure 6.",
        "Let N, V, and A be sets of disambiguation problems whose target words are nouns, verbs, and adjectives, respectively.",
        "We write 0(z,,) for the struc",
        "ture matrix associated with the feature group j and computed from a problem sets.",
        "That is, we replace Oj in (5) with O(j,s).",
        "• Apply ASO to the three sets of disambiguation problems (corresponding to nouns, verbs, and adjectives), separately, using the extended model (5) with F = {LC, SR}.",
        "As a result, we obtain O(j s) for every (j, s) E {LC, SR} x {N, V, A}.",
        "• Apply ASO to all the disambiguation problems at once using the extended model (5) with F = {GC} to obtain O(GC,NUVUA).",
        "• For a problem f E P E {N, V, A}, our final predictor is based on the model:",
        "where T = {(LC, P), (SR, P), (GC, N U V U A) }.",
        "We obtain predictor ft by minimizing the regularized empirical risk with respect to wt and vt. We fix the dimension of the structure matrix corresponding to global context features to 50.",
        "The dimensions of the other structure matrices are set to 0.9 times the maximum possible rank to ensure relatively high dimensionality.",
        "This procedure produces 68.1% on the Senseval-2 English lexical sample test set."
      ]
    },
    {
      "heading": "3.3 Previous systems on Senseval-2 data set",
      "text": [
        "Figure 7 compares our performance with those of previous best systems on the Senseval-2 English lexical sample test set.",
        "Since we used this test set for the development of our method above, our performance should be understood as the potential performance.",
        "(In Section 4, we will present evaluation results on",
        "the unseen Senseval-3 test sets.)",
        "Nevertheless, it is worth noting that our potential performance (68.1%) exceeds those of the previous best systems.",
        "Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM.",
        "This is consistent with the fact that we adopted LN02’s feature design.",
        "FY02 (Florian and Yarowsky, 2002) combines classifiers by linear average stacking.",
        "The best system of the Senseval-2 competition was an early version of FY02.",
        "WSC04 used a polynomial kernel via the kernel Principal Component Analysis (KPCA) method (Sch¨olkopf et al., 1998) with nearest neighbor classifiers."
      ]
    },
    {
      "heading": "4 Evaluation on Senseval-3 tasks",
      "text": [
        "In this section, we evaluate the methods developed on the Senseval-2 data set above on the standard Senseval-3 lexical sample tasks."
      ]
    },
    {
      "heading": "4.1 Our methods in multi-task and semi-supervised configurations",
      "text": [
        "In addition to the multi-task configuration described in Section 3.2, we test the following semi-supervised application of ASO.",
        "We first create auxiliary problems following Ando and Zhang (2005a)’s partially-supervised strategy (Figure 8) with distinct feature maps �1 and �2 each of which uses one of {LC, GC, SR}.",
        "Then, we apply ASO to these auxiliary problems using the feature split and the problem partitioning described in Section 3.2.",
        "Note that the difference between the multi-task and semi-supervised configurations is the source of information.",
        "The multi-task configuration utilizes the label information of the training examples that are labeled for the rest of the multiple tasks, and the semi-supervised learning configuration exploits a large amount of unlabeled data.",
        "1.",
        "Train a classifier C, only using feature map IV, on the labeled data for the target task.",
        "2.",
        "Auxiliary problems are to predict the labels assigned by",
        "C, to the unlabeled data, using the other feature map IV 2.",
        "3.",
        "Apply ASO to the auxiliary problems to obtain O.",
        "4.",
        "Using the joint linear model (2), train the final",
        "predictor by minimizing the empirical risk for fixed O on the labeled data for the target task."
      ]
    },
    {
      "heading": "4.2 Data and evaluation metric",
      "text": [
        "We conduct evaluations on four Senseval-3 lexical sample tasks (English, Catalan, Italian, and Spanish) using the official training / test splits.",
        "Data statistics are shown in Figure 9.",
        "On the Spanish, Catalan, and Italian data sets, we use part-of-speech information (as features) and unlabeled examples (for semi-supervised learning) provided by the organizer.",
        "Since the English data set was not provided with these additional resources, we use an in-house POS tagger trained with the PennTree Bank corpus, and extract 100K unlabeled examples from the Reuters-RCV 1 corpus.",
        "On each language, the number of unlabeled examples is 5–15 times larger than that of the labeled training examples.",
        "We use syntactic relation features only for English data set.",
        "As in Section 3, we report micro-averaged F measure."
      ]
    },
    {
      "heading": "4.3 Baseline methods",
      "text": [
        "In addition to the standard single-task supervised configuration as in Section 3, we test the following method as an additional baseline.",
        "Output-based method The goal of our multi-task learning configuration is to benefit from having the labeled training examples of a number of words.",
        "An alternative to ASO for this purpose is to use directly as features the output values of classifiers trained for disambiguating the other words, which we call ‘output-based method’ (cf. Florian et al.",
        "(2003)).",
        "We explore several variations similarly to Section 3.1 and report the ceiling performance."
      ]
    },
    {
      "heading": "4.4 Evaluation results",
      "text": [
        "Figure 10 shows F-measure results on the four Senseval-3 data sets using the official training / test splits.",
        "Both ASO multi-task learning and semi-supervised learning improve performance over the single-task baseline on all the data sets.",
        "The best performance is achieved when we combine multi-task learning and semi-supervised learning by using all the corresponding structure matrices 0(j��) produced by both multi-task and semi-supervised learning, in the final predictors.",
        "This combined configuration outperforms the single-task supervised baseline by up to 5.7%.",
        "Performance improvements over the supervised baseline are relatively small on English and Spanish.",
        "We conjecture that this is because the supervised performance is already close to the highest performance that automatic methods could achieve.",
        "On these two languages, our (and previous) systems outperform inter-human agreement, which is unusual but can be regarded as an indication that these tasks are difficult.",
        "The performance of the output-based method (baseline) is relatively low.",
        "This indicates that output values or proposed labels are not expressive enough to integrate information from other predictors effectively on this task.",
        "We conjecture that for this method to be effective, the problems are required to be more closely related to each other as in Florian et al.",
        "(2003)’s named entity experiments.",
        "A practical advantage of ASO multi-task learning over ASO semi-supervised learning is that shorter computation time is required to produce similar performance.",
        "On this English data set, training for multi-task learning and semi-supervised learning takes 15 minutes and 92 minutes, respectively, using a Pentium-4 3.20GHz computer.",
        "The computation time mostly depends on the amount of the data on which auxiliary predictors are learned.",
        "Since our experiments use unlabeled data 5–15 times larger than labeled training data, semi-supervised learning takes longer, accordingly.",
        "GGS05 combined various kernels, which includes the LSA kernel that exploits unlabeled data with global context features.",
        "Our implementation of the LSA kernel with our classifier (and our other features) also produced performance similar to that of GGS05.",
        "While the LSA kernel is closely related to a special case of the semi-supervised application of ASO (see the discussion of PCA in Ando and Zhang (2005a)), our approach here is more general in that we exploit not only unlabeled data and global context features but also the labeled examples of other target words and other types of features.",
        "G04 achieved high performance on English using regularized least squares with compensation for skewed class distributions.",
        "SGG04 is an early version of GGS05.",
        "Our methods rival or exceed these state-of-the-art systems on all the data sets."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "With the goal of achieving higher WSD performance by exploiting all the currently available resources, our focus was the new application of the ASO algorithm in the multi-task learning configuration, which improves performance by learning a number of WSD problems simultaneously instead of training for each individual problem independently.",
        "A key finding is that using ASO with appropriate feature / problem partitioning, labeled examples of seemingly unrelated words can be effectively exploited.",
        "Combining ASO multi-task learning with ASO semi-supervised learning results in further improvements.",
        "The fact that performance improvements were obtained consistently across several languages / sense inventories demonstrates that our approach has broad applicability and hence practical significance."
      ]
    }
  ]
}
