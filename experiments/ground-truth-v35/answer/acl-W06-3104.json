{
  "info": {
    "authors": [
      "David A. Smith",
      "Jason M. Eisner"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W06-3104",
    "title": "Quasi-Synchronous Grammars: Alignment by Soft Projection of Syntactic Dependencies",
    "url": "https://aclweb.org/anthology/W06-3104",
    "year": 2006
  },
  "references": [
    "acl-C90-3045",
    "acl-J00-1004",
    "acl-J03-1002",
    "acl-J93-2003",
    "acl-J94-4004",
    "acl-J97-3002",
    "acl-P01-1067",
    "acl-P02-1050",
    "acl-P03-1011",
    "acl-P03-1054",
    "acl-P03-2041",
    "acl-P04-1061",
    "acl-P04-1083",
    "acl-P04-1084",
    "acl-P05-1034",
    "acl-P05-1067",
    "acl-W02-1039",
    "acl-W04-3228"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel.",
        "We present a new model of the translation process: quasi-synchronous grammar (QG).",
        "Given a source-language parse tree T1, a QG defines a monolingual grammar that generates translations of T1.",
        "The trees T2 allowed by this monolingual grammar are inspired by pieces of substructure in T1 and aligned to T1 at those points.",
        "We describe experiments learning quasi-synchronous context-free grammars from bitext.",
        "As with other monolingual language models, we evaluate the cross-entropy of QGs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence.",
        "When evaluated on a word alignment task, QG matches standard baselines."
      ]
    },
    {
      "heading": "1 Motivation and Related Work",
      "text": []
    },
    {
      "heading": "1.1 Sloppy Syntactic Alignment",
      "text": [
        "This paper proposes a new type of syntax-based model for machine translation and alignment.",
        "The goal is to make use of syntactic formalisms, such as context-free grammar or tree-substitution grammar, without being overly constrained by them.",
        "Let S1 and S2 denote the source and target sentences.",
        "We seek to model the conditional probability",
        "where T1 is a parse tree for S1, T2 is a parse tree for S2, and A is a node-to-node alignment between them.",
        "This model allows one to carry out a variety of alignment and decoding tasks.",
        "Given T1, one can translate it by finding the T2 and A that maximize (1).",
        "Given T1 and T2, one can align them by finding the A that maximizes (1) (equivalent to maximizing p(A I T2, T1)).",
        "Similarly, one can align S1 and S2 by finding the parses T1 and T2, and alignment A,",
        "usually accomplish such maximizations by dynamic programming.",
        "Equation (1) does not assume that T1 and T2 are isomorphic.",
        "For example, a model might judge T2 and A to be likely, given T1, provided that many – but not necessarily all – of the syntactic dependencies in T1 are aligned with corresponding dependencies in T2.",
        "Hwa et al.",
        "(2002) found that human translations from Chinese to English preserved only 39–42% of the unlabeled Chinese dependencies.",
        "They increased this figure to 67% by using more involved heuristics for aligning dependencies across these two languages.",
        "That suggests that (1) should be defined to consider more than one dependency at a time.",
        "This inspires the key novel feature of our models: A does not have to be a “well-behaved” syntactic alignment.",
        "Any portion of T2 can align to any portion of T1, or to NULL.",
        "Nodes that are syntactically related in T1 do not have to translate into nodes that are syntactically related in T2 – although (1) is usually higher if they do.",
        "This property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver",
        "gences observed between even closely related languages (Dorr, 1994; Fox, 2002).",
        "We can patch together an alignment without accounting for all the details of the translation process.",
        "For instance, perhaps a source NP (figure 1) or PP (figure 2) appears “out of place” in the target sentence.",
        "A linguist might account for the position of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysis-transfer-synthesis process in the translator’s head).",
        "But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort.",
        "Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer."
      ]
    },
    {
      "heading": "1.2 From Synchronous to Quasi-Synchronous Grammars",
      "text": [
        "Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993).",
        "It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003).",
        "The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1.1 When choosing how to expand a certain VP node in T2, a synchronous CFG process would observe that this node is aligned to a node VP' in T1, which had been expanded in T1 by VP' – * NP' V'.",
        "This might bias it toward choosing to expand the VP in T2 as VP – * V NP, with the new children V aligned to V' and NP aligned to NP'.",
        "The process then continues recursively by choosing moves to expand these children.",
        "One can regard this stochastic process as an instance of analysis-transfer-synthesis MT.",
        "Analysis chooses a parse T1 given 51.",
        "Transfer maps the context-free rules in T1 to rules of T2.",
        "Synthesis",
        "deterministically assembles the latter rules into an actual tree T2 and reads off its yield 52.",
        "What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1.",
        "It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3, or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005).",
        "However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation.",
        "In particular, a synchronous grammar cannot practically allow 52 to be any permutation of 51, as IBM Models 1–5 do.",
        "Our alternative is to define a “quasi-synchronous” stochastic process.",
        "It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human translator might be imagined to behave similarly.)",
        "When choosing how to expand nodes of T2, we are influenced both by the structure of T1 and by monolingual preferences about the structure of T2.",
        "Just as conditional Markov models can more easily incorporate global features than HMMs, we can look at the entire tree T1 at every stage in generating T2."
      ]
    },
    {
      "heading": "2 Quasi-Synchronous Grammar",
      "text": [
        "Given an input 51 or its parse T1, a quasi-synchronous grammar (QG) constructs a monolingual grammar for parsing, or generating, the possible translations 52 – that is, a grammar for finding appropriate trees T2.",
        "What ties this target-language grammar to the source-language input?",
        "The grammar provides for target-language words to take on",
        "multiple hidden “senses,” which correspond to (possibly empty sets of) word tokens in 51 or nodes in T1.",
        "To take a familiar example, when parsing the English side of a French-English bitext, the word bank might have the sense banque (financial) in one sentence and rive (littoral) in another.",
        "The QG4 considers the “sense” of the former bank token to be a pointer to the particular banque token to which it aligns.",
        "Thus, a particular assignment of 51 “senses” to word tokens in 52 encodes a word alignment.",
        "Now, selectional preferences in the monolingual grammar can be influenced by these T1-specific senses.",
        "So they can encode preferences for how T2 ought to copy the syntactic structure of T1.",
        "For example, if T1 contains the phrase banque nationale, then the QG for generating a corresponding T2 may encourage any T2 English noun whose sense is banque (more precisely, T1’s token of banque) to generate an adjectival English modifier with sense nationale.",
        "The exact probability of this, as well as the likely identity and position of that English modifier (e.g., national bank), may also be influenced by monolingual facts about English."
      ]
    },
    {
      "heading": "2.1 Definition",
      "text": [
        "A quasi-synchronous grammar is a monolingual grammar that generates translations of a source-language sentence.",
        "Each state of this monolingual grammar is annotated with a “sense” – a set of zero or more nodes from the source tree or forest.",
        "For example, consider a quasi-synchronous context-free grammar (QCFG) for generating translations of a source tree T1.",
        "The QCFG generates the target sentence using nonterminals from the cross product U x 2V1, where U is the set of monolingual target-language nonterminals such as NP, and V1 is the set of nodes in T1.",
        "Thus, a binarized QCFG has rules of the form",
        "where A, B, C E U are ordinary target-language nonterminals, a, Q, y E 2V1 are sets of source tree",
        "nodes to which A, B, C respectively align, and w is a target-language terminal.",
        "Similarly, a quasi-synchronous tree-substitution grammar (QTSG) annotates the root and frontier nodes of its elementary trees with sets of source nodes from 2 V1 ."
      ]
    },
    {
      "heading": "2.2 Taming Source Nodes",
      "text": [
        "This simple proposal, however, presents two main difficulties.",
        "First, the number of possible senses for each target node is exponential in the number of source nodes.",
        "Second, note that the senses are sets of source tree nodes, not word types or absolute sentence positions as in some other translation models.",
        "Except in the case of identical source trees, source tree nodes will not recur between training and test.",
        "To overcome the first problem, we want further restrictions on the set a in a QG state such as (A, a) .",
        "It should not be an arbitrary set of source nodes.",
        "In the experiments of this paper, we adopt the simplest option of requiring I a I < 1.",
        "Thus each node in the target tree is aligned to a single node in the source tree, or to 0 (the traditional NULL alignment).",
        "This allows one-to-many but not many-to-one alignments.",
        "To allow many-to-many alignments, one could limit I a I to at most 2 or 3 source nodes, perhaps further requiring the 2 or 3 source nodes to fall in a particular configuration within the source tree, such as child-parent or child-parent-grandparent.",
        "With that configurational requirement, the number of possible senses a remains small – at most three times the number of source nodes.",
        "We must also deal with the menagerie of different source tree nodes in different sentences.",
        "In other words, how can we tie the parameters of the different QGs that are used to generate translations of different source sentences?",
        "The answer is that the probability or weight of a rule such as (2) should depend on the specific nodes in a, Q, and y only through their properties – e.g., their nonterminal labels, their head words, and their grammatical relationship in the source tree.",
        "Such properties do recur between training and test.",
        "For example, suppose for simplicity that IaI = IQI = I-yI = 1.",
        "Then the rewrite probabilities of (2) and (3) could be log-linearly modeled using features that ask whether the single node in a has two children in the source tree; whether its children in the",
        "source are the nodes in Q and 'y; whether its nonterminal label in the source is A; whether its fringe in the source translates as w; and so on.",
        "The model should also consider monolingual features of (2) and (3), evaluating in particular whether A – * BC is likely in the target language.",
        "Whether rule weights are given by factored generative models or by naive Bayes or log-linear models, we want to score QG productions with a small set of monolingual and bilingual features."
      ]
    },
    {
      "heading": "2.3 Synchronous Grammars Again",
      "text": [
        "Finally, note that synchronous grammar is a special case of quasi-synchronous grammar.",
        "In the context-free case, a synchronous grammar restricts senses to single nodes in the source tree and the NULL node.",
        "Further, for any k-ary production",
        "a synchronous context-free grammar requires that",
        "1.",
        "(Vi =� j) ai =� aj unless ai = NULL, 2.",
        "(Vi > 0) ai is a child of �0 in the source tree, unless ai = NULL.",
        "Since NULL has no children in the source tree, these rules imply that the children of any node aligned to NULL are themselves aligned to NULL.",
        "The construction for synchronous tree-substitution and tree-adjoining grammars goes through similarly but operates on the derivation trees."
      ]
    },
    {
      "heading": "3 Parameterizing a QCFG",
      "text": [
        "Recall that our goal is a conditional model of p(T2, A I T1).",
        "For the remainder of this paper, we adopt a dependency-tree representation of T1 and T2.",
        "Each tree node represents a word of the sentence together with a part-of-speech tag.",
        "Syntactic dependencies in each tree are represented directly by the parent-child relationships.",
        "Why this representation?",
        "First, it helps us concisely formulate a QG translation model where the source dependencies influence the generation of target dependencies (see figure 3).",
        "Second, for evaluation, it is trivial to obtain the word-to-word alignments from the node-to-node alignments.",
        "Third, the part-of-speech tags are useful backoff features, and in fact play a special role in our model below.",
        "When stochastically generating a translation T2, our quasi-synchronous generative process will be influenced by both fluency and adequacy.",
        "That is, it considers both the local well-formedness of T2 (a monolingual criterion) and T2’s local faithfulness to T1 (a bilingual criterion).",
        "We combine these in a simple generative model rather than a log-linear model.",
        "When generating the children of a node in T2, the process first generates their tags using monolingual parameters (fluency), and then fills in in the words using bilingual parameters (adequacy) that select and translate words from T1.5 Concretely, each node in T2 is labeled by a triple (tag, word, aligned word).",
        "Given a parent node (p, h, h') in T2, we wish to generate sequences of left and right child nodes, of the form (c, a, a').",
        "Our monolingual parameters come from a simple generative model of syntax used for grammar induction: the Dependency Model with Valence (DMV) of Klein and Manning (2004).",
        "In scoring dependency attachments, DMV uses tags rather than words.",
        "The parameters of the model are: 1 .",
        "pehoose(c I p, dir): the probability of generating c as the next child tag in the sequence of dir children, where dir E {left, right}.",
        "2. pstop(s I h, dir, adj): the probability of generating no more child tags in the sequence of dir children.",
        "This is conditioned in part on the “adjacency” adj E {true, false}, which indicates whether the sequence of dir children is empty so far.",
        "Our bilingual parameters score word-to-word translation and aligned dependency configurations.",
        "We thus use the conditional probability ptrans (a I a') that source word a', which may be NULL, translates as target word a.",
        "Finally, when a parent word h aligned to h' generates a child, we stochastically decide to align the child to a node a' in T1 with one several possible relations to h'.",
        "A “monotonic” dependency alignment, for example, would have h' and a' in a parent-child relationship like their target-tree analogues.",
        "In different versions of the model, we allowed various dependency alignment configurations (figure 3).",
        "These configurations rep",
        "resent cases where the parent-child dependency being generated by the QG in the target language maps onto source-language child-parent, for head swapping; the same source node, for two-to-one alignment; nodes that are siblings or in a c-command relationship, for scrambling and extraposition; or in a grandparent-grandchild relationship, e.g. when a preposition is inserted in the source language.",
        "We also allowed a “none-of-the-above” configuration, to account for extremely mismatched sentences.",
        "The probability of the target-language dependency treelet rooted at h is thus:"
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We claim that for modeling human-translated bitext, it is better to project syntax only loosely.",
        "To evaluate this claim, we train quasi-synchronous dependency grammars that allow progressively more divergence from monotonic tree alignment.",
        "We evaluate these models on cross-entropy over held-out data and on error rate in a word-alignment task.",
        "One might doubt the use of dependency trees for alignment, since Gildea (2004) found that constituency trees aligned better.",
        "That experiment, however, aligned only the 1-best parse trees.",
        "We too will consider only the 1-best source tree T1, but in con-strast to Gildea, we will search for the target tree T2 that aligns best with T1.",
        "Finding T2 and the alignment is simply a matter of parsing 52 with the QG derived from T1."
      ]
    },
    {
      "heading": "4.1 Data and Training",
      "text": [
        "We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002).",
        "We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003).",
        "Initial estimates of lexical translation probabilities came from the IBM Model 4 translation tables produced by GIZA++ (Brown et al., 1993; Och and Ney, 2003).",
        "All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs.",
        "The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs.",
        "We held out one thousand sentences for evaluating the cross-entropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER).",
        "We trained the model parameters on bitext using the Expectation-Maximization (EM) algorithm.",
        "The T1 tree is fully observed, but we parse the target language.",
        "As noted, the initial lexical translation probabilities came from IBM Model 4.",
        "We initialized the monolingual DMV parameters in one of two ways: using either simple tag co-occurrences as in (Klein and Manning, 2004) or “supervised” counts from the monolingual target-language parser.",
        "This latter initialization simulates the condition when one has a small amount of bitext but a larger amount of target data for language modeling.",
        "As with any monolingual grammar, we perform EM training with the Inside-Outside algorithm, computing inside probabilities with dynamic programming and outside probabilities through backpropagation.",
        "Searching the full space of target-language dependency trees and alignments to the source tree consumed several seconds per sentence.",
        "During training, therefore, we constrained alignments to come from the union of GIZA++ Model 4 alignments.",
        "These constraints were applied only during training and not during evaluation of cross-entropy or AER."
      ]
    },
    {
      "heading": "4.2 Conditional Cross-Entropy of the Model",
      "text": [
        "To test the explanatory power of our QCFG, we evaluated its conditional cross-entropy on held-out data (table 1).",
        "In other words, we measured how well a trained QCFG could predict the true translation of novel source sentences by summing over all parses of the target given the source.",
        "We trained QCFG models under different conditions of bitext size and parameter initialization.",
        "However, the principal independent variable was the set of dependency alignment configurations allowed.",
        "From these cross-entropy results, it is clear that strictly synchronous grammar is unwise.",
        "We ob",
        "tain comparatively poor performance if we require parent-child pairs in the target tree to align to parent-child pairs in the source (or to parent-NULL or NULL-NULL).",
        "Performance improves as we allow and distinguish more alignment configurations."
      ]
    },
    {
      "heading": "4.3 Word Alignment",
      "text": [
        "We computed standard measures of alignment precision, recall, and error rate on a test set of 100 hand-aligned German sentence pairs with 1300 alignment links.",
        "As with many word-alignment evaluations, we do not score links to NULL.",
        "Just as for cross-entropy, we see that more permissive alignments lead to better performance (table 2).",
        "Having selected the best system using the cross-entropy measurement, we compare its alignment error rate against the standard GIZA•• Model 4 baselines.",
        "As Figure 4 shows, our QCFG for German – * English consistently produces better alignments than the Model 4 channel model for the same direction, German – * English.",
        "This comparison is the appropriate one because both of these models are forced to align each English word to at most one German word.",
        "6"
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "With quasi-synchronous grammars, we have presented a new approach to syntactic MT: constructing a monolingual target-language grammar that describes the aligned translations of a source-language sentence.",
        "We described a simple parameterization",
        "with gradually increasing syntactic domains of locality, and estimated those parameters on German-English bitext.",
        "The QG formalism admits many more nuanced options for features than we have exploited.",
        "In particular, we now are exploring log-linear QGs that score overlapping elementary trees of T2 while considering the syntactic configuration and lexical content of the T1 nodes to which each elementary tree aligns.",
        "Even simple QGs, however, turned out to do quite well.",
        "Our evaluation on a German-English word-alignment task showed them to be competitive with IBM model 4 – consistently beating the German-English direction by several percentage points of alignment error rate and within 1% AER of the English-German direction.",
        "In particular, alignment accuracy benefited from allowing syntactic breakages between the two dependency structures.",
        "We are also working on a translation decoding using QG.",
        "Our first system uses the QG to find optimal T2 aligned to T1 and then extracts a synchronous tree-substitution grammar from the aligned trees.",
        "Our second system searches a target-language vocabulary for the optimal T2 given the input T1."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was supported by a National Science Foundation Graduate Research Fellowship for the first author and by NSF Grant No.",
        "0313193."
      ]
    }
  ]
}
