{
  "info": {
    "authors": [
      "Taro Watanabe",
      "Hajime Tsukada",
      "Hideki Isozaki"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W06-3115",
    "title": "NTT System Description for the WMT2006 Shared Task",
    "url": "https://aclweb.org/anthology/W06-3115",
    "year": 2006
  },
  "references": [
    "acl-J03-1002",
    "acl-J04-4002",
    "acl-N03-1017",
    "acl-N04-1021",
    "acl-P02-1038",
    "acl-P03-1021",
    "acl-P05-1033",
    "acl-P06-1098"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present two translation systems experimented for the shared-task of “Workshop on Statistical Machine Translation,” a phrase-based model and a hierarchical phrase-based model.",
        "The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals.",
        "Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model.",
        "We also report a phrase/rule extraction technique differentiating tokenization of corpora."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task.",
        "One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003).",
        "The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005).",
        "Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions.",
        "We also explored three types of corpus preprocessing in Section 3.",
        "As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size.",
        "In our method, phrase/rule translation pairs extracted from three distinctly word-aligned corpora are aggregated into one large phrase/rule translation table.",
        "The experiments and the final translation results are presented in Section 4."
      ]
    },
    {
      "heading": "2 Translation Models",
      "text": [
        "We used a log-linear approach (Och and Ney, 2002) in which a foreign language sentence f1J = f1, f2, ...fJ is translated into another language, i.e. English, eI1 = e1, e2,..., eI by seeking a maximum likelihood solution of",
        "In this framework, the posterior probability Pr(eI1 |f1J) is directly maximized using a log-linear combination of feature functions hm(eI1 , f1J), such as a ngram language model or a translation model.",
        "When decoding, the denominator is dropped since it depends only on f1J.",
        "Feature function scaling factors Am are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003).",
        "This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted.",
        "In a phrase-based statistical translation (Koehn et al., 2003), a bilingual text is decomposed as K phrase translation pairs (e1, fa1), (e2, fa2), ...: The input foreign sentence is segmented into phrases fK1"
      ]
    },
    {
      "heading": "Proceedings of the Workshop on Statistical Machine Translation, pages 122–125, New York City, June 2006. c�2006 Association for Computational Linguistics",
      "text": [
        "mapped into corresponding EnglisheK1, then, reordered to form the output English sentence according to a phrase alignment index mapping a.",
        "In a hierarchical phrase-based translation (Chiang, 2005), translation is modeled after a weighted synchronous-CFG consisting of production rules whose right-hand side is paired (Aho and Ullman, 1969):",
        "where X is a non-terminal, y and a are strings of terminals and non-terminals.",
        "∼ is a one-to-one correspondence for the non-terminals appeared in y and a.",
        "Starting from an initial non-terminal, each rule rewrites non-terminals in y and a that are associated with ∼."
      ]
    },
    {
      "heading": "2.1 Phrase/Rule Extraction",
      "text": [
        "The phrase extraction algorithm is based on those presented by Koehn et al.",
        "(2003).",
        "First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004).",
        "Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003).",
        "The method exhaustively extracts phrase pairs Vj+m, e,.+n) from a sentence pair (f1J , eI1) that do not violate the word alignment constraints a.",
        "In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases (Chiang, 2005):",
        "1.",
        "A phrase pair (f, e) constitutes a rule: X→ (f,0 2.",
        "A rule X → (y, a) and a phrase pair (f, e) s.t.",
        "y = y′fy′ ′ and a = a′ea′′ constitutes a rule: X → (y′ Xk y′′, a′ Xk a"
      ]
    },
    {
      "heading": "2.2 Decoding",
      "text": [
        "The decoder for the phrase-based model is a left-to-right generation decoder with a beam search strategy synchronized with the cardinality of already translated foreign words.",
        "The decoding process is very similar to those described in (Koehn et al., 2003): It starts from an initial empty hypothesis.",
        "From an existing hypothesis, new hypothesis is generated by consuming a phrase translation pair that covers untranslated foreign word positions.",
        "The score for the newly generated hypothesis is updated by combining the scores of feature functions described in Section 2.3.",
        "The English side of the phrase is simply concatenated to form a new prefix of English sentence.",
        "In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006).",
        "The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule."
      ]
    },
    {
      "heading": "2.3 Feature Functions",
      "text": [
        "Our phrase-based model uses a standard pharaoh feature functions listed as follows (Koehn et al., 2003):",
        "• Relative-count based phrase translation probabilities in both directions.",
        "• Lexically weighted feature functions in both directions.",
        "• The supplied trigram language model.",
        "• Distortion model that counts the number of words skipped.",
        "• The number of words in English-side and the number of phrases that constitute translation.",
        "For details, please refer to Koehn et al.",
        "(2003).",
        "In addition, we added three feature functions to restrict reorderings and to represent globalized insertion/deletion of words:",
        "• Lexicalized reordering feature function scores whether a phrase translation pair is monotonically translated or not (Och et al., 2004):",
        "where Sk = 1 iff ak − ak−1 = 1 otherwise Sk = 0.",
        "• Deletion feature function penalizes words that do not constitute a translation according to a",
        "de-en es-en fr-en en-de en-es en-fr lower 37,711,217 61,161,868 56,025,918 38,142,663 60,619,435 55,198,497 stem 46,550,101 75,610,696 68,210,968 46,749,195 75,473,313 67,733,045 prefix4 53,429,522 78,193,818 70,514,377 53,647,033 78,223,236 70,378,947 merged 80,260,191 111,153,303 103,523,206 80,666,414 110,787,982 102,940, 840 lexicon model t(f|e) (Bender et al., 2004):",
        "The deletion model simply counts the number of words whose lexicon model probability is lower than a threshold Tdel .",
        "Likewise, we also added an insertion model hins(eI1� f1J) that penalizes the spuriously inserted English words using a lexicon model t(elf).",
        "For the hierarchical phrase-based model, we employed the same feature set except for the distortion model and the lexicalized reordering model."
      ]
    },
    {
      "heading": "3 Phrase Extraction from Different Word Alignment",
      "text": [
        "We prepared three kinds of corpora differentiated by tokenization methods.",
        "First, the simplest preprocessing is lower-casing (lower).",
        "Second, corpora were transformed by a Porter’s algorithm based multilingual stemmer (stem) 1.",
        "Third, mixed-cased corpora were truncated to the prefix of four letters of each word (prefix4).",
        "For each differently tokenized corpus, we computed word alignments by a HMM translation model (Och and Ney, 2003) and by a word alignment refinement heuristic of “grow-diagfinal” (Koehn et al., 2003).",
        "Different preprocessing yields quite divergent alignment points as illustrated in Table 1.",
        "The table also shows the numbers for the intersection and union of three alignment annotations.",
        "The (hierarchical) phrase translation pairs are extracted from three distinctly word aligned corpora.",
        "In this process, each word is recovered into its lowercased form.",
        "The associated counts are aggregated to constitute relative count-based feature functions.",
        "Table 2 summarizes the size of phrase tables induced from the corpora.",
        "The number of rules extracted for the hierarchical phrase-based model was roughly twice as large as those for the phrase-based model.",
        "Fewer word alignments resulted in larger phrase translation table size as observed in the “prefix4” corpus.",
        "The size is further increased by our aggregation step (merged).",
        "Different induction/refinement algorithms or preprocessings of a corpus bias word alignment.",
        "We found that some word alignments were consistent even with different preprocessings, though we could not justify whether such alignments would match against human intuition.",
        "If we could trust such consistently aligned words, reliable (hierarchical) phrase translation pairs would be extracted, which, in turn, would result in better estimates for relative count-based feature functions.",
        "At the same time, differently biased word alignment annotations suggest alternative phrase translation pairs that is useful for increasing the coverage of translations."
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "Table 3 shows the open test translation results on 2005 and 2006 test set (the development-test set and the final test set) 2.",
        "We used the merged (hierarchical) phrase tables for decoding.",
        "Feature function scaling factors were optimized on BLEU score using the supplied development set that is identical to the 2005’s development set.",
        "We observed that our",
        "results are very comparable to the last year’s best results in test2005.",
        "Also found that our hierarchical phrase-based translation (Rule) performed slightly inferior to the phrase-based translation (Phrase) in both test sets.",
        "The hierarchically combined phrases seem to be too flexible to represent the relationship of similar language pairs.",
        "Note that our hierarchical phrase-based model performed better in the English-to-German translation task.",
        "Those language pair requires rather distorted reordering, which could be represented by hierarchically combined phrases.",
        "We also conducted additional studies on how differently aligned corpora might affect the translation quality on Spanish-to-English task for the 2005 test set.",
        "Using our phrase-based model, the BLEU scores for lower/stem/prefix4 were 30.90/30.89/30.76, respectively.",
        "The differences of translation qualities were statistically significant at the 95% confidence level.",
        "Our phrase translation pairs aggregated from all the differently preprocessed corpora improved the translation quality."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We presented two translation models, a phrase-based model and a hierarchical phrase-based model.",
        "The former performed as well as the last year’s best system, whereas the latter performed comparable to our phrase-based model.",
        "We are going to experiment new feature functions to restrict the too flexible reordering represented by our hierarchical phrase-based model.",
        "We also investigated different word alignment annotations, first using lower-cased corpus, second performing stemming, and third retaining only 4 letter prefix.",
        "Differently preprocessed corpora resulted in quite divergent word alignment.",
        "Large phrase/rule translation tables were accumulated from three distinctly aligned corpora, which in turn, increased the translation quality."
      ]
    }
  ]
}
