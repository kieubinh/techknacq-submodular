{
  "info": {
    "authors": [
      "Bing Zhao",
      "Alex Waibel"
    ],
    "book": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-I05-3011",
    "title": "Learning a Log-Linear Model with Bilingual Phrase-Pair Features for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/I05-3011",
    "year": 2005
  },
  "references": [
    "acl-J03-1002",
    "acl-J04-4002",
    "acl-J93-2003",
    "acl-J97-3002",
    "acl-N03-1017",
    "acl-N04-1033",
    "acl-P02-1038",
    "acl-P02-1040",
    "acl-W02-1039",
    "acl-W03-1001",
    "acl-W05-0825"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We propose a set of informative feature functions together with a log-linear model framework for bilingual phrase-pair extraction to improve phrase-based statistical machine translation.",
        "The base feature functions investigated are phrase length model, phrase-level centers' distortion, lexicon translation equivalence, bracketing constraints and word alignment links.",
        "Two generative models show strong baselines with these base features, illustrating the effectiveness of the proposed feature functions.",
        "Strategies of extending the features and a log-linear model of learning the weighted combination of them are proposed to effectively extract phrase-pairs from parallel data.",
        "Experimental results of TIDES'03 Chinese-English small data track show improved translation qualities."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Bilingual phrase-pair extraction from parallel data becomes a key component in today's state of the art phrase-based statistical machine translation systems.",
        "The significant advantages of using phrase-pairs over word level mixture models (Brown et al., 1993) are that both fertilities and distortions above phrase level are simpler to model and phrase-based approaches have flexibilities of modeling local word-reordering and are less sensitive to the preprocessing errors such as word segmentations (e.g., for Chinese and Japanese) and tokenization.",
        "These advantages are observed and supported by the positive evidences from many previous works such as (Wu, 1997; Och and Ney, 2004; Koehn et al., 2003; Zens and Ney, 2004; Vogel et al., 2003).",
        "In practice, a phrase-pair extraction based on word alignment (Koehn, 2004; Tillmann, 2003; Och and Ney, 2004) works quite well given simple heuristics and millions of parallel sentences to cover enough instances needed for translation.",
        "However, it is more or less difficult to extend the heuristics to include more informative clues for better phrase-pair extractions.",
        "We propose a principled framework of combining a set of informative feature functions via a log-linear model for bilingual phrase-pair extraction.",
        "In particular, we introduce a few informative feature functions in keeping the philosophy of phrase translations.",
        "The base feature functions are designed to model three diverse aspects of a block (a phrase pair): a phrase-level length model to approximate phrasal length relevance, a center distortion model to model relative positions' differences for phrases in a given sentence pair and a generative phrase level lexical model to model translational equivalence.",
        "Furthermore, we extend the base feature functions by symmetrizing with noisy-channel models in two directions: source-to-target and target-to-source.",
        "The utilities of the proposed features are demonstrated using two generative models establishing strong baselines; together with constraints from word alignments, these feature functions are combined in a log-linear model to extract phrase pairs effectively; improved translation qualities are achieved over a state of the art system on TIDES'03 Chinese-English small data tack.",
        "The remainder paper is structured as follows: in Section 2, statistical machine translation is briefly reviewed with notations for Blocks; in Section 3, base feature functions are explained; Section 4 contains two novel generative models as our baselines; our proposed log-linear model with extended feature functions are in Section 5; in Section 6, our experimental results; conclusions and discussions are given in Section 7."
      ]
    },
    {
      "heading": "2 Statistical Machine Translation",
      "text": [
        "Generally speaking, the task of statistical machine translation (SMT) is to translate one sentence in a source language F into a target language E. For example, given a French sentence f with J words denoted as f{ = /1/2.../J, an SMT system automatically translates it into an English sentence e with / words denoted by e[ = eie2...ej.",
        "The SMT system first proposes English hypotheses in its model space.",
        "Among all the hypotheses, the system selects the one with the highest conditional probability according to Bayes 's decision rule as follows: where P(f{\\e[) is called translation model and P(e[) is the language model.",
        "For most of the phrase based machine translation systems, the translation model are essentially a collection of bilingual phrase-pairs extracted from parallel sentence pairs.",
        "This paper focus on how to extract high quality phrase-pairs from parallel data.",
        "Each phrase pair is represented as a Block: X in a given parallel sentence pair: self-contained.",
        "Each of the feature functions corresponds to one special aspect of the block embedded in the context of a given sentence pair.",
        "These aspects are then quantified by the proposed sub-models or our designed feature functions.",
        "Given the word fertility defined as in (Brown et al., 1993), we can compute a probability to predict phrase length relevance between a pair of phrases: given the candidate target phrase (English) e[ and a source phrase (French) f( of length J, the model gives the probabilistic estimation of P(J\\e[) via a dynamic programming algorithm using the English word fertilities models P((j)\\ei).",
        "Figure 1 shows an example fertility where f^+l is the source phrase with (I + 1) French words; its projection is el+k in the target sentence with left boundary at the position of i and right boundary at (i + k).",
        "We view the phrase-pair extraction as a local search algorithm: given a source phrase fj+\\ search for the projected boundaries of candidate target phrase e\\+k according to a weighted combination of diverse feature functions in a log-linear model.",
        "The log linear model then servers as a performance measure to guide a local search (i.e., a stochastic hill-climbing) to extract bilingual phrase pairs from the parallel data."
      ]
    },
    {
      "heading": "3 Base Feature Functions",
      "text": [
        "We introduce several informative base feature functions, some of which are shown to be helpful in our previous works (Zhao and Vogel, 2005).",
        "They'll be briefly summarized here to be more trellis of an English trigram, where each edge between two nodes represents one English word e^.",
        "The arc between two nodes represents one candidate non-zero fertility for word e^.",
        "The fertility of zero (i.e., generating a Null word) corresponds to the direct edge between two nodes and thus, the Null word is incorporated into this model's representation.",
        "Each arc is associated with a English word fertility probability P(^|e^).",
        "A path (/)[ through the trellis represents the number of French words fa generated by the English trigram.",
        "Thus, the probability of generating J words from the English phrase along the Viterbi path is: The Viterbi path is inferred via dynamic programming as follows: As introduced in Section 1, the distortion model above phrase level is usually easier to model.",
        "Empirical observations show that most high quality blocks are located close to the diagonal or the inverse diagonal in the alignment matrix of a sentence pair.",
        "A simple distortion model is designed to estimate how far away the phrase pairs are from each other.",
        "The center 0 fj+i of the phrase f^ is a normalized relative position in the source sentence defined as follows:",
        "dex, which is weighted by the word level translation probabilities; the term of J2i=i P{fj'\\ei) provides a normalization so that the expected center is within the range of target sentence length.",
        "After this, the expected center of ê^k is simply a average of © *+* {j.",
        "The center of the English phrase is computed accordingly.",
        "Figure 2 shows histograms of the differences between the centers: (0 fj+i – 0 i+k) of 30.8K oracle phrase pairs extracted from 627 human word-aligned sentence pairs: for each source phrase, find the leftmost and rightmost projected positions in the target sentence according to both word alignment and the coherence constraint (Fox, 2002).",
        "For phrase-pair extraction, the expected center of the phrase ê^k is estimated for a given French phrase's center and then a local search starting around it is carried out to get candidate target phrases.",
        "The expected relative center for every French word is first computed as follows: Given the estimated centers of (Dfj+i and 0 i+k, we can compute how close they are by the probability of P(0fj+z|0 i+k).",
        "To estimate P(0fj+z|0 i+k), one can start with a flat gaussian model to enforce the point nal and build an initial list of phrase pairs and then compute the histogram to approximate in Figure 2, this probability can be approximated as a gaussian distribution.",
        "Similar to IBM Model-1 (Brown et al., 1993), we use a bag-of-word generative model within the block.",
        "According to Bayes rule: where P(fjt\\ei) is the word translation lexicon estimated in IBM Models, i is the position inwhere P(e^/|e^+ ) ~ l/(k + 1) is approximated by a unigram bag-of-word language model.",
        "Because phrase-pairs are usually very short, this assumption works very well in practice (Koehn et al., 2003; Brown et al., 1993).",
        "The parameters used by the three feature functions are estimated using IBM Model-4.",
        "We here present two generative models of Pr(fj+l\\el^k) f°r phrase-pair extraction.",
        "The first one models three aspects of a phrase pair, i.e., the phrase level lexical translation equivalence, positions' distortion and length relevance.",
        "The second one takes into consideration of bracketing a sentence pair at a block level and generate sub-blocks synchronously.",
        "To model Pr(/j+/|e*+fc), Model-l first proposes how many words of fj to generate according to P{1 + l|e^+fc); then it proposes the location of the source phrases f?+l by predicting the center Qrj+i of the phrase; and then the model generates the words fj according to a lexicon model P(f\\e).",
        "The model is summarized in Eqn.",
        "8. where the three components P{l+l\\el^k), P(Of*+i|®P<+*) and P(f]+l\\^k) control three different aspects of a bilingual phrase pair: phrase level fertility, center distortion and translation equivalence as explained in the previous section.",
        "A variation of this model is applied in a ACL05 shared task for phrase-based statistical machine translation (Zhao and Vogel, 2005).",
        "Instead of modeling the three aspects individually as in Model-l, we propose to model the brackets induced by the segmentation of the parallel sentence pair given a block.",
        "Shown in Figure 3, a phrase pair (block A) split the sentence pairs into five shaded parts A, P, C, P>, i?, which are the valid parts to be generated in our Model-2.",
        "We enforce the following bracketing constraints so that one can only bracket the sentence pair in the following two ways: where 5^ indicates the bracketing along the diagonal and S<:> indicates the inverse bracketing.",
        "Each bracketing direction is associated with a probability under the same assumption of \"bag-of-words\" generation as in Eqn.",
        "7.",
        "In a way, this model relates to the bilingual bracketing (Wu, 1997) as it requires the other two brackets (either (P, C) or (P, P)) to be generated synchronously.",
        "However, the model is a flat one because it requires only one level bracketing for any given block A.",
        "The model is summarized as follows: where P(5D|e,f) ~ P(A)P(B)P(C)\\ and P(A), P{B) and P(C) are defined similarly as in Eqn.",
        "7 using the lexicon of P(/|e).",
        "The parameters P(f\\e) are from IBM Model-4.",
        "Using the constraints from the bracketing in Eqn.",
        "9, this model gives quite good performance shown in our experiments.",
        "Both Eqn.",
        "8 and Eqn.",
        "10 involve some local search within a sentence pair: given a source phrase fj+l, search for the candidate phrase translation ê^k in the target sentence according to the score computed in Eqn.",
        "8 or Eqn.",
        "10.",
        "In practice, only the phrase pairs to be used in decoding are searched, i.e., we only search phrase pairs in which the source phrase f^+l is seen in a predefined list to save computations and disk",
        "space.",
        "With Eqn.",
        "8 or Eqn.",
        "10 as the performance measures, we employ a twisted stochastic hill-climbing, in which several downhill moves are accepted to allow one or two left or right functional words to be attached to e^.",
        "To make the local search more effective, we normalize the lexicon model P(f\\e) within the parallel sentence pair in Eqn.",
        "11 In this way, the distribution of P(f\\e) is sharper and more focused in the context of a sentence pair.",
        "We'll use the aforementioned two generative models as our baselines.",
        "The sub models are generalized and extended as feature functions in our proposed log-linear model for phrase-pair extraction in Section 5."
      ]
    },
    {
      "heading": "5 A Log-Linear Model",
      "text": [
        "The phrase level fertility model (3.1), distortion model(3.2), lexicon model(3.3) and the bracketing model in 4.2 are all real-valued and bounded (G [0,1]).",
        "We define a log-linear model to combine these sub-models in Eqn.",
        "12: where 0m(X, e, f) is a feature function corresponding to the log probabilities (i.e. raw scores) from the models listed above.",
        "The parameters are the feature functions' weights {Am}.",
        "We define three base feature functions E2FFScoreIn: P(l + l\\e^k); E2FIBMScoreIN: P(fj+l\\etk)> and E2FIBMBracket: Pr(X\\e,f) as refereed in Section 4.",
        "We then extend the base feature functions by considering the remaining part of the sentence pair excluding the block.",
        "This means, the region exclude block A in Figure 3.",
        "The motivation is if the block is of high quality, the remaining part should also be explained well by the model.",
        "Therefore, we add the following three extended feature functions:",
        "• E2FFScoreOut: P(J – l – l\\ei'g[i,i+k]) which estimates how well the remaining English words eif^^i+k] can generate the remaining sentence length of (J – I – 1).",
        "This model can be computed similarly via dynamic programming as in 3.",
        "• E2FIBMScoreOut: Generating the remaining French words in the sentence pair: This estimates how well the translational equivalence are kept in accordance with the philosophy of the phrase extraction from a parallel sentence pair.",
        "• AlignmentLinks: Averaged word alignment links per source word.",
        "We count how many alignment links within the block and normalize this number by the length of the source phrase.",
        "As introduced in Eqn.",
        "1, so far all our models' parameters are using one direction noisy-channel model.",
        "In practice, we train both directions of IBM Model-4 – source-to-target and target-to-source to further extend our base feature functions.",
        "In this way, we obtain the lexicon models P(f\\e) and P(e\\f), the fertility models P(<fi\\e) and P(<fi\\f) to compute the feature functions defined in Section 5.1.",
        "Therefore, we have additional five more feature functions of F2EFScoreIn, F2EFScoreOut, F2EIBMScoreIN, F2EIBMScoreOut and F2EIBMBracket defined accordingly as in Section 5.1.",
        "Therefore, we have in total 11 real-valued feature functions for bilingual phrase-pair extraction.",
        "Except the feature function of AlignmentLinks, the other 10 feature functions are all bounded in the range of [0,1].",
        "Using direct maximum entropy model for statistical machine translation was explored in (Pap-ineni et al., 1998).",
        "To learn the log-linear model in Eqn.",
        "12, a maximum bleu score optimizer implemented in (Koehn, 2004) is modified with a sampling of N-Best list phrase pairs generated by an initial assignment of weights.",
        "To optimize the weights, we view each extracted phrase-pair as a hypothesis block and the reference blocks are extracted from the human word-aligned sentence pairs as described in Section 3.2.",
        "We compute word-level F-measure for each extracted block according to all the reference blocks, which contain the same extracted source phrase.",
        "Therefore, the data point for optimization is M raw scores of feature functions together with a performance indicator of word-level F-measure.",
        "Finally, a modified optimizer similar to (Och and Ney, 2002) is utilized to obtain the optimized weights for the proposed feature functions.",
        "The inference is similar to the one in Section 4.3: a hill-climbing with a performance measure to score the phrase pairs (fj+\\ el^k) according to the log-linear model as in Eqn 13:",
        "the fertility tables, lexicons and word alignments.",
        "We then refine the word alignment by growing the intersections from two directions with new unaligned word pairs which occur in the union (Koehn et al., 2003).",
        "We report NIST and Bleu (Papineni et al., 2002) scores as the translation performance measures using a decoder in (Vogel et al., 2003).",
        "The trigram language model is trained using Gigaword Xinhua news part.",
        "The two generative models Eqn.",
        "8 and Eqn.",
        "10 give strong baselines.",
        "In the local search of phrase pairs in Eqn.",
        "8 and 10, the Top-N scored target phrase candidates for each source phrase within the sentence pair are collected for decoding.",
        "There is no significant difference observed between the two generative models.",
        "Figure 4 shows the performance of Model-2 over Top-N configurations, in which the Top-7 gives the best performance.",
        "However, Top-4 configuration already gives performance close to the optimal and we can avoid extracting too many noisy phrase-pairs which significantly slow down the decoding process.",
        "In the experiments so forth, we'll use up to Top-4 candidates for phrase-pair extraction.",
        "Using GIZA++(Och and Ney, 2003), we trained the IBM-Model-4 in both directions for Table 2 summarizes the baselines from the two generative models.",
        "The best baseline model is Model-l using the Top-2 configuration for phrase-pair extraction.",
        "The pairwise correlations among the 11 (M=ll) real-valued feature functions are investigated.",
        "The MxM correlation matrix is obtained by computing the pairwise linear correlation coefficient between the feature functions using the phrase-pairs extracted from Model-2.",
        "The feature functions which are highly correlated are regrouped close to each other via standard K-means.",
        "The result is shown in Figure 5 and the clusters are shown in Table 3.",
        "26232 4947 clustered together related to the Chinese-to-English direction.",
        "The feature functions for the outer brackets like F2EIBMScoreOut (FID 6) and E2FIBMScoreOut (FID 8) are somewhat misplaced and the the fertility models for outside parts F2EFScoreOut and E2FFScoreOut are singled out in the clustering process.",
        "These evidences illustrate that feature functions from the outer part of the block have little overlap with the feature functions from the inner part of the block.",
        "Two directions of the noisy channel model are weakly correlated.",
        "All these observations confirm our intuitions.",
        "We hold 627 sentence pairs with one reference each from the 2001 Dry-Run test as development dataset, which were word-aligned by bilingual speakers.",
        "From the word alignment, we can extract the gold-standard blocks (phrase pairs).",
        "On the same held-out 627 sentence pairs, we run our baseline models with up to Top-20 target phrase candidates for each source phrase to collect the raw scores from all feature functions and we compute word-level F-measure for each extracted block.",
        "The more correlated the two feature functions are, the more overlapping information the two share with.",
        "In Figure 5, intuitively, features of FID 2,9,7,11,6 are grouped together with aspects using the noisy channel model in English-to-Chinese direction and FID of 8, 5,10,1 are The final optimized weights learnt are (0.0480, 0.0660, 0.0048, 0.0032, 0.1376, 0.1332, 0.1662, 0.1113, 0.2495, 0.4621, 0.0753), in the order of the FIDs assigned in Table 3.",
        "Table 5 summarizes the log-linear model's performances at different configurations.",
        "The best Bleu score for the log-linear model is 0.1834, an improvement over the best generative models' performance of 0.1726.",
        "The log-linear model based on maximum entropy principle has several advantages over the generative models.",
        "It introduces less data fragmentation, requiring fewer independence assumptions and exploiting a principled technique for automatic feature weighting.",
        "However, a drawback of our approach is we have to simulate the phrase-pair extraction performance measure from the hand-aligned data set to compute the word-level F-measure.",
        "This potentially introduces some errors before the optimizations."
      ]
    },
    {
      "heading": "7 Conclusions and Discussions",
      "text": [
        "We presented a set of informative feature functions for bilingual phrase-pair extractions.",
        "A log-linear model is proposed to combine these feature functions and improvements are demonstrated in Chinese-English TIDES'03 small data track evaluations.",
        "The log-linear model is a promising framework which has the advantage of leveraging the overlapping features and it has flexibilities of exploiting more informative feature functions.",
        "However, the optimization took long to converge and run random restarts were needed to avoid local optima.",
        "Different optimization criteria, efficient algorithms and better feature functions can potentially bring more improvements."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We thank Jaime Carbonell and Chiori Hori for their valuable comments and Philipp Koehn for his help with the Pharaoh software package; in particular, the optimization toolkit."
      ]
    }
  ]
}
