{
  "info": {
    "authors": [
      "Bing Zhao",
      "Eric P. Xing",
      "Alex Waibel"
    ],
    "book": "Workshop on Building and Using Parallel Texts",
    "id": "acl-W05-0804",
    "title": "Bilingual Word Spectral Clustering for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/W05-0804",
    "year": 2005
  },
  "references": [
    "acl-C00-2163",
    "acl-C96-2141",
    "acl-E99-1010",
    "acl-J03-1002",
    "acl-J93-2003",
    "acl-W02-1012",
    "acl-W05-0825"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, a variant of a spectral clustering algorithm is proposed for bilingual word clustering.",
        "The proposed algorithm generates the two sets of clusters for both languages efficiently with high semantic correlation within monolingual clusters, and high translation quality across the clusters between two languages.",
        "Each cluster level translation is considered as a bilingual concept, which generalizes words in bilingual clusters.",
        "This scheme improves the robustness for statistical machine translation models.",
        "Two HMMbased translation models are tested to use these bilingual clusters.",
        "Improved perplexity, word alignment accuracy, and translation quality are observed in our experiments."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Statistical natural language processing usually suffers from the sparse data problem.",
        "Comparing to the available monolingual data, we have much less training data especially for statistical machine translation (SMT).",
        "For example, in language modelling, there are more than 1.7 billion words corpora available: English Gigaword by (Graff, 2003).",
        "However, for machine translation tasks, there are typically less than 10 million words of training data.",
        "Bilingual word clustering is a process of forming corresponding word clusters suitable for machine translation.",
        "Previous work from (Wang et al., 1996) showed improvements in perplexity-oriented measures using mixture-based translation lexicon (Brown et al., 1993).",
        "A later study by (Och,",
        "1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model.",
        "Both approaches are optimizing the maximum likelihood of parallel corpus, in which a data point is a sentence pair: an English sentence and its translation in another language such as French.",
        "These algorithms are essentially the same as monolingual word clusterings (Kneser and Ney, 1993) – an iterative local search.",
        "In each iteration, a two-level loop over every possible word-cluster assignment is tested for better likelihood change.",
        "This kind of approach has two drawbacks: first it is easily to get stuck in local optima; second, the clustering of English and the other language are basically two separated optimization processes, and cluster-level translation is modelled loosely.",
        "These drawbacks make their approaches generally not very effective in improving translation models.",
        "In this paper, we propose a variant of the spectral clustering algorithm (Ng et al., 2001) for bilingual word clustering.",
        "Given parallel corpus, first, the word’s bilingual context is used directly as features - for instance, each English word is represented by its bilingual word translation candidates.",
        "Second, latent eigenstructure analysis is carried out in this bilingual feature space, which leads to clusters of words with similar translations.",
        "Essentially an affinity matrix is computed using these cross-lingual features.",
        "It is then decomposed into two sub-spaces, which are meaningful for translation tasks: the left subspace corresponds to the representation of words in English vocabulary, and the right subspace corresponds to words in French.",
        "Each eigenvector is considered as one bilingual concept, and the bilingual clusters are considered to be its realizations in two languages.",
        "Finally, a general K-means cluster",
        "ing algorithm is used to find out word clusters in the two subspaces.",
        "The remainder of the paper is structured as follows: in section 2, concepts of translation models are introduced together with two extended HMMs; in section 3, our proposed bilingual word clustering algorithm is explained in detail, and the related works are analyzed; in section 4, evaluation metrics are defined and the experimental results are given; in section 5, the discussions and conclusions."
      ]
    },
    {
      "heading": "2 Statistical Machine Translation",
      "text": [
        "The task of translation is to translate one sentence in some source language F into a target language E. For example, given a French sentence with J words denoted as f J1 = f1 f2... fJ, an SMT system automatically translates it into an English sentence with I words denoted by e�1 = e1e2 ... e�.",
        "The SMT system first proposes multiple English hypotheses in its model space.",
        "Among all the hypotheses, the system selects the one with the highest conditional probability according to Bayes’s decision rule:",
        "where P(fJ1 I e�1) is called translation model, and P(e�1) is called language model.",
        "The translation model is the key component, which is the focus in this paper."
      ]
    },
    {
      "heading": "2.1 HMM-based Translation Model",
      "text": [
        "HMM is one of the effective translation models (Vogel et al., 1996), which is easily scalable to very large training corpus.",
        "To model word-to-word translation, we introduce the mapping j – * aj, which assigns a French word fj in position j to a English word ez in position i = aj denoted as eaj.",
        "Each French word fj is an observation, and it is generated by a HMM state defined as [eaj, aj], where the alignment aj for position j is considered to have a dependency on the previous alignment aj-1.",
        "Thus the first-order HMM is defined as follows:",
        "where P(aj I aj-1) is the transition probability.",
        "This model captures the assumption that words close in the source sentence are aligned to words close in the target sentence.",
        "An additional pseudo word of “NULL” is used as the beginning of English sentence for HMM to start with.",
        "The (Och and Ney, 2003) model includes other refinements such as special treatment of a jump to a Null word, and a uniform smoothing prior.",
        "The HMM with these refinements is used as our baseline.",
        "Motivated by the work in both (Och and Ney, 2000) and (Toutanova et al., 2002), we propose the two following simplest versions of extended HMMs to utilize bilingual word clusters."
      ]
    },
    {
      "heading": "2.2 Extensions to HMM with word clusters",
      "text": [
        "Let F denote the cluster mapping fj – * F (fj), which assigns French word fj to its cluster ID Fj = F (f j) .",
        "Similarly E maps English word ez to its cluster ID of Ez = E(ez).",
        "In this paper, we assume each word belongs to one cluster only.",
        "With bilingual word clusters, we can extend the HMM model in Eqn.",
        "1 in the following two ways:",
        "where E(eaj_1) and F(fj-1) are non overlapping word clusters (Eaj_1, Fj-1)for English and French respectively.",
        "Another explicit way of utilizing bilingual word clusters can be considered as a two-stream HMM as follows:",
        "This model introduces the translation of bilingual word clusters directly as an extra factor to Eqn.",
        "2.",
        "Intuitively, the role of this factor is to boost the translation probabilities for words sharing the same concept.",
        "This is a more expressive model because it models both word and the cluster level translation equivalence.",
        "Also, compared with the model in Eqn.",
        "3, this model is easier to train, as it uses a two-dimension table instead of a four-dimension table.",
        "However, we do not want this P(Fj I Eaj) to dominate the HMM transition structure, and the obser",
        "vation probability of P(fj I ear) during the EM iterations.",
        "Thus a uniform prior P(Fj) = 1/IFI is introduced as a smoothing factor for P(Fj I Ear):",
        "where IFI is the total number of word clusters in French (we use the same number of clusters for both languages).",
        "A can be chosen to get optimal performance on a development set.",
        "In our case, we fix it to be 0.5 in all our experiments."
      ]
    },
    {
      "heading": "3 Bilingual Word Clustering",
      "text": [
        "In bilingual word clustering, the task is to build word clusters F and E to form partitions of the vocabularies of the two languages respectively.",
        "The two partitions for the vocabularies of F and E are aimed to be suitable for machine translation in the sense that the cluster/partition level translation equivalence is reliable and focused to handle data sparseness; the translation model using these clusters explains the parallel corpus { (f J1 , e I1) } better in terms of perplexity or joint likelihood."
      ]
    },
    {
      "heading": "3.1 From Monolingual to Bilingual",
      "text": [
        "To infer bilingual word clusters of (F, E), one can optimize the joint probability of the parallel corpus { (f J1 , eI1) } using the clusters as follows:",
        "Eqn.",
        "6 separates the optimization process into two parts: the monolingual part for E, and the bilingual part for F given fixed E. The monolingual part is considered as a prior probability:P(eI1 I E), and E can be inferred using corpus bigram statistics in the following equation:",
        "We need to fix the number of clusters beforehand, otherwise the optimum is reached when each word is a class of its own.",
        "There exists efficient leave-one-out style algorithm (Kneser and Ney, 1993), which can automatically determine the number of clusters.",
        "For the bilingual part P(fJ1 I eI1, F, E), we can slightly modify the same algorithm as in (Kneser and Ney, 1993).",
        "Given the word alignment {�J1} between fJ1 and eI1 collected from the Viterbi path in HMM-based translation model, we can infer F� as follows:",
        "Overall, this bilingual word clustering algorithm is essentially a two-step approach.",
        "In the first step, E is inferred by optimizing the monolingual likelihood of English data, and secondly F is inferred by optimizing the bilingual part without changing E. In this way, the algorithm is easy to implement without much change from the monolingual correspondent.",
        "This approach was shown to give the best results in (Och, 1999).",
        "We use it as our baseline to compare with."
      ]
    },
    {
      "heading": "3.2 Bilingual Word Spectral Clustering",
      "text": [
        "Instead of using word alignment to bridge the parallel sentence pair, and optimize the likelihood in two separate steps, we develop an alignment-free algorithm using a variant of spectral clustering algorithm.",
        "The goal is to build high cluster-level translation quality suitable for translation modelling, and at the same time maintain high intra-cluster similarity , and low inter-cluster similarity for monolingual clusters."
      ]
    },
    {
      "heading": "3.2.1 Notations",
      "text": [
        "We define the vocabulary VF as the French vocabulary with a size of I VF I; VE as the English vocabulary with size of I VE I.",
        "A co-occurrence matrix C{F,E} is built with I VF I rows and I VE I columns; each element represents the co-occurrence counts of the corresponding French word fj and English word ei.",
        "In this way, each French word forms a row vector with a dimension of I VE I, and each dimensionality is a co-occurring English word.",
        "The elements in the vector are the co-occurrence counts.",
        "We can also",
        "view each column as a vector for English word, and we’ll have similar interpretations as above."
      ]
    },
    {
      "heading": "3.2.2 Algorithm",
      "text": [
        "With C{F,E}, we can infer two affinity matrixes as follows:",
        "where AE is an I VE I x I VE I affinity matrix for English words, with rows and columns representing English words and each element the inner product between two English words column vectors.",
        "Correspondingly, AF is an affinity matrix of size IVFI x IVFI for French words with similar definitions.",
        "Both AE and AF are symmetric and non-negative.",
        "Now we can compute the eigenstructure for both AE and AF.",
        "In fact, the eigenvectors of the two are correspondingly the right and left subspaces of the original co-occurrence matrix of C{F,E} respectively.",
        "This can be computed using singular value decomposition (SVD): C{F,E} = USVT, AE = VS2VT, and AF = US2UT, where U is the left sub-space, and V the right subspace of the co-occurrence matrix C{F,E} .",
        "S is a diagonal matrix, with the singular values ranked from large to small along the diagonal.",
        "Obviously, the left subspace U is the eigenstructure for AF; the right subspace V is the eigenstructure for AE.",
        "By choosing the top K singular values (the square root of the eigenvalues for both AE and AF), the subspaces will be reduced to: UIVF I x K and VI VE I x K respectively.",
        "Based on these subspaces, we can carry out K-means or other clustering algorithms to infer word clusters for both languages.",
        "Our algorithm goes as follows:",
        "• Initialize bilingual co-occurrence matrix C{F,E} with rows representing French words, and columns English words.",
        "CjZ is the co-occurrence raw counts of French word fj and English word eZ; • Form the affinity matrix AE = CT{F,E}C{F,E} and AF = CT{F,E}C{F,E}.",
        "Kernels can also be",
        "applied here such as AE = exp( 0r2 for English words.",
        "Set AEZZ = 0 and AFZZ = 0, and normalize each row to be unit length;",
        "• Compute the eigen structure of the normalized matrix AE, and find the k largest eigen vectors: v1, v2, ..., vk; Similarly, find the k largest eigenvectors of AF: u1, u2, ..., uk; • Stack the k eigenvectors of v1, v2, ..., vk in the columns of YE, and stack the eigenvectors u1, u2, ..., uk in the columns for YF; Normalize rows of both YE and YF to have unit length.",
        "YE is size of IVEI x k and YF is size of IVFI x k; • Treat each row of YE as a point in RIVE I xk, and cluster them into K English word clusters using K-means.",
        "Treat each row of YF as a point in RIVF I Ix k, and cluster them into K French word clusters.",
        "• Finally, assign original word eZ to cluster Ek if row i of the matrix YE is clustered as Ek; similar assignments are for French words.",
        "Here AE and AF are affinity matrixes of pairwise inner products between the monolingual words.",
        "The more similar the two words, the larger the value.",
        "In our implementations, we did not apply a kernel function like the algorithm in (Ng et al., 2001).",
        "But the kernel function such as the exponential function mentioned above can be applied here to control how rapidly the similarity falls, using some carefully chosen scaling parameter."
      ]
    },
    {
      "heading": "3.2.3 Related Clustering Algorithms",
      "text": [
        "The above algorithm is very close to the variants of a big family of the spectral clustering algorithms introduced in (Meila and Shi, 2000) and studied in (Ng et al., 2001).",
        "Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with high intra-cluster similarity and low inter-cluster similarity.",
        "It’s shown to be computing the k-way normalized cut: K – trYT D – 12 AD – 12 Y for any matrix Y ERM x �.",
        "A is the affinity matrix, and Y in our algorithm corresponds to the subspaces of U and V. Experimentally, it has been observed that using more eigenvectors and directly computing a k-way partitioning usually gives better performance.",
        "In our implementations, we used the top 500 eigenvectors to construct the subspaces of U and V for K-means clustering."
      ]
    },
    {
      "heading": "3.2.4 K-means",
      "text": [
        "The K-means here can be considered as a post-processing step in our proposed bilingual word clustering.",
        "For initial centroids, we first compute the center of the whole data set.",
        "The farthest centroid from the center is then chosen to be the first initial centroid; and after that, the other K-1 centroids are chosen one by one to well separate all the previous chosen centroids.",
        "The stopping criterion is: if the maximal change of the clusters’ centroids is less than the threshold of 1 e-3 between two iterations, the clustering algorithm then stops."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "To test our algorithm, we applied it to the TIDES Chinese-English small data track evaluation test set.",
        "After preprocessing, such as English tokenization, Chinese word segmentation, and parallel sentence splitting, there are in total 4172 parallel sentence pairs for training.",
        "We manually labeled word alignments for 627 test sentence pairs randomly sampled from the dry-run test data in 2001, which has four human translations for each Chinese sentence.",
        "The preprocessing for the test data is different from the above, as it is designed for humans to label word alignments correctly by removing ambiguities from tokenization and word segmentation as much as possible.",
        "The data statistics are shown in Table 1."
      ]
    },
    {
      "heading": "4.1 Building Co-occurrence Matrix",
      "text": [
        "Bilingual word co-occurrence counts are collected from the training data for constructing the matrix of C{F,E}.",
        "Raw counts are collected without word alignment between the parallel sentences.",
        "Practically, we can use word alignment as used in (Och, 1999).",
        "Given an initial word alignment inferred by HMM, the counts are collected from the aligned word pair.",
        "If the counts are L-1 normalized, then the co-occurrence matrix is essentially the bilingual word-to-word translation lexicon such as P ( fj I ea .)",
        ".",
        "We can remove very small entries (P (f Ie) G 1e – �), so that the matrix of C{F,E} is more sparse for eigenstructure computation.",
        "The proposed algorithm is from init wo alig then carried out to generate the bilingual word clusters for both English and Chinese.",
        "It is clear, that using the initial HMM word alignment for co-occurrence matrix makes a difference.",
        "The top Eigenvalue using word alignment in plot a.",
        "(the deep blue curve) is 3.1946.",
        "The two plateaus indicate how many top K eigenvectors to choose to reduce the feature space.",
        "The first one indicates that K is in the range of 50 to 120, and the second plateau indicates K is in the range of 500 to 800.",
        "Plot b. is inferred from the raw co-occurrence counts with the top eigenvalue of 2.7148.",
        "There is no clear plateau, which indicates that the feature space is less structured than the one built with initial word alignment.",
        "We find 500 top eigenvectors are good enough for bilingual clustering in terms of efficiency and effectiveness."
      ]
    },
    {
      "heading": "4.2 Clustering Results",
      "text": [
        "Clusters built via the two described methods are compared.",
        "The first method bil1 is the two-step optimization approach: first optimizing the monolingual clusters for target language (English), and afterwards optimizing clusters for the source language (Chinese).",
        "The second method bil2 is our proposed algorithm to compute the eigenstructure of the co-occurrence matrix, which builds the left and right subspaces, and finds clusters in such spaces.",
        "Top 500 eigenvectors are used to construct these subspaces.",
        "For both methods, 1000 clusters are inferred for English and Chinese respectively.",
        "The number of clusters is chosen in a way that the final word alignment accuracy was optimal.",
        "Table 2 provides the clustering examples using the two algorithms.",
        "The monolingual word clusters often contain words with similar syntax functions.",
        "This happens with esp.",
        "frequent words (eg.",
        "mono-E1 and mono-E2).",
        "The algorithm tends to put rare words such as “carota, anglophobia” into a very big cluster (eg.",
        "mono-E3).",
        "In addition, the words within these monolingual clusters rarely share similar translations such as the typical cluster of “week, month, year”.",
        "This indicates that the corresponding Chinese clusters inferred by optimizing Eqn.",
        "7 are not close in terms of translational similarity.",
        "Overall, the method of bil 1 does not give us a good translational correspondence between clusters of two languages.",
        "The English cluster of mono-E3 and its best aligned candidate of bil1-C3 are not well correlated either.",
        "Our proposed bilingual cluster algorithm bil2 generates the clusters with stronger semantic meaning within a cluster.",
        "The cluster of bil2-E1 relates to the concept of “wine” in English.",
        "The monolingual word clustering tends to scatter those words into several big noisy clusters.",
        "This cluster also has a good translational correspondent in bil2-C1 in Chinese.",
        "The clusters of bil2-E2 and bil2-C2 are also correlated very well.",
        "We noticed that the Chinese clusters are slightly more noisy than their English corresponding ones.",
        "This comes from the noise in the parallel corpus, and sometimes from ambiguities of the word segmentation in the preprocessing steps.",
        "To measure the quality of the bilingual clusters, we can use the following two kind of metrics:",
        "• Average c-mirror (Wang et al., 1996): The c-mirror of a class EZ is the set of clusters in Chinese which have a translation probability greater than c. In our case, c is 0.05, the same value used in (Och, 1999).",
        "• Perplexity: The perplexity is defined as proportional to the negative log likelihood of the HMM model Viterbi alignment path for each sentence pair.",
        "We use the bilingual word clus",
        "ters in two extended HMM models, and measure the perplexities of the unseen test data after seven forward-backward training iterations.",
        "The two perplexities are defined as PP1 =",
        "two extended HMM models in Eqn 3 and 4.",
        "Both metrics measure the extent to which the translation probability is spread out.",
        "The smaller the better.",
        "The following table summarizes the results on c-mirror and perplexity using different methods on the unseen test data.",
        "algorithms e-mirror HMM-1 Perp HMM-2 Perp baseline - 1717.82 bil1 3.97 1810.55 352.28 bil2 2.54 1610.86 343.64 The baseline uses no word clusters.",
        "bil1 and bil2 are defined as above.",
        "It is clear that our proposed method gives overall lower perplexity: 1611 from the baseline of 1717 using the extended HMM-1.",
        "If we use HMM-2, the perplexity goes down even more using bilingual clusters: 352.28 using bil 1, and 343.64 using bil2.",
        "As stated, the four-dimensional",
        "subject to overfitting, and usually gives worse perplexities.",
        "Average E-mirror for the two-step bilingual clustering algorithm is 3.97, and for spectral clustering algorithm is 2.54.",
        "This means our proposed algorithm generates more focused clusters of translational equivalence.",
        "Figure 2 shows the histogram for the cluster pairs (Fj , EZ ), of which the cluster level translation probabilities P(Fj IEZ) E [0.05, 1].",
        "The interval [0.05, 1] is divided into 10 bins, with first bin [0.05, 0.",
        "1], and 9 bins divides [0.",
        "1, 1] equally.",
        "The percentage for clusters pairs with P(Fj IEZ) falling in each bin is drawn.",
        "Our algorithm generates much better aligned cluster pairs than the two-step optimization algorithm.",
        "There are 120 cluster pairs aligned with P(Fj IEZ) > 0.9 using clusters from our algorithm, while there are only 8 such cluster pairs using the two-step approach.",
        "Figure 3 compares the E-mirror at different numbers of clusters using the two approaches.",
        "Our algorithm has a much better E-mirror than the two-step approach over different number of clusters.",
        "Overall, the extended HMM-2 is better than HMM-1 in terms of perplexity, and is easier to train."
      ]
    },
    {
      "heading": "4.3 Applications in Word Alignment",
      "text": [
        "We also applied our bilingual word clustering in a word alignment setting.",
        "The training data is the TIDES small data track.",
        "The word alignments are manually labeled for 627 sentences sampled from the dryrun test data in 2001.",
        "In this manually aligned data, we include one-to-one, one-to-many, and many-to-many word alignments.",
        "Figure 4 summarizes the word alignment accuracy for different",
        "methods.",
        "The baseline is the standard HMM translation model defined in Eqn.",
        "2; the HMM1 is defined in Eqn 3, and HMM2 is defined in Eqn 4.",
        "The algorithm is applying our proposed bilingual word clustering algorithm to infer 1000 clusters for both languages.",
        "As expected, Figure 4 shows that using",
        "word clusters is helpful for word alignment.",
        "HMM2 gives the best performance in terms of F-measure of word alignment.",
        "One quarter of the words in the test vocabulary are unseen as shown in Table 1.",
        "These unseen words related alignment links (4778 out of 14769) will be left unaligned by translation models.",
        "Thus the oracle (best possible) recall we could get is 67.65%.",
        "Our standard t-test showed that significant interval is 0.82% at the 95% confidence level.",
        "The improvement at the last iteration of HMM is marginally significant."
      ]
    },
    {
      "heading": "4.4 Applications in Phrase-based Translations",
      "text": [
        "Our pilot word alignment on unseen data showed improvements.",
        "However, we find it more effective in our phrase extraction, in which three key scores",
        "are computed: phrase level fertilities, distortions, and lexicon scores.",
        "These scores are used in a local greedy search to extract phrase pairs (Zhao and Vogel, 2005).",
        "This phrase extraction is more sensitive to the differences in P(fj �ez) than the HMM Viterbi word aligner.",
        "The evaluation conditions are defined in NIST 2003 Small track.",
        "Around 247K test set (919 Chinese sentences) specific phrase pairs are extracted with up to 7-gram in source phrase.",
        "A trigram language model is trained using Gigaword XinHua news part.",
        "With a monotone phrase-based decoder, the translation results are reported in Table 3.",
        "The",
        "baseline is using the lexicon P(fj �ez) trained from standard HMM in Eqn.",
        "2, which gives a BLEU score of 0.1558 +/- 0.0113.",
        "Bil1 and Bil2 are using P (fj � ez) from HMM in Eqn.",
        "4 with 1000 bilingual word clusters inferred from the two-step algorithm and the proposed one respectively.",
        "Using the clusters from the two-step algorithm gives a BLEU score of 0.",
        "1575, which is close to the baseline.",
        "Using clusters from our algorithm, we observe more improvements with BLEU score of 0.1644 and a NIST score of 6.582."
      ]
    },
    {
      "heading": "5 Discussions and Conclusions",
      "text": [
        "In this paper, a new approach for bilingual word clustering using eigenstructure in bilingual feature space is proposed.",
        "Eigenvectors from this feature space are considered as bilingual concepts.",
        "Bilingual clusters from the subspaces expanded by these concepts are inferred with high semantic correlations within each cluster, and high translation qualities across clusters from the two languages.",
        "Our empirical study also showed effectiveness of using bilingual word clusters in extended HMMs for statistical machine translation.",
        "The K-means based clustering algorithm can be easily extended to do hierarchical clustering.",
        "However, extensions of translation models are needed to leverage the hierarchical clusters appropriately."
      ]
    }
  ]
}
