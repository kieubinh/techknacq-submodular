{
  "info": {
    "authors": [
      "Ying Zhang",
      "Stephan Vogel"
    ],
    "book": "Workshop on Building and Using Parallel Texts",
    "id": "acl-W05-0829",
    "title": "Competitive Grouping in Integrated Phrase Segmentation and Alignment Model",
    "url": "https://aclweb.org/anthology/W05-0829",
    "year": 2005
  },
  "references": [
    "acl-J90-1003",
    "acl-J93-2003",
    "acl-N03-1017",
    "acl-P97-1063",
    "acl-W02-1018",
    "acl-W99-0604"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model.",
        "ISA extracts phrase pairs from a bilingual corpus without requiring the pre-calculated word alignment as many other phrase alignment models do.",
        "Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993).",
        "Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path.",
        "The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment.",
        "ISA segments the sentence into phrases and finds their alignment simultaneously.",
        "ISA is simple and fast.",
        "Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training.",
        "In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment."
      ]
    },
    {
      "heading": "2 Translation Likelihood as a Statistical Test",
      "text": [
        "Given a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C (f, e) and the marginal frequency C(f ) _ Ee C(f, e) and C(e) _ E f C(f, e).",
        "We can apply various statistical tests (Manning and Sch¨utze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations.",
        "In the following sections, we will use x2 statistics to measure the the mutual translation likelihood (Church and Hanks, 1990)."
      ]
    },
    {
      "heading": "3 The Core of the Integrated Phrase Segmentation and Alignment",
      "text": [
        "The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm.",
        "It was designed to overcome the problem of indirect associations using a simple heuristic: whenever several word tokens fi in one half of the bilingual corpus co-occur with a particular word token e in the other half of the corpus, the word that is most likely to be e’s translation is the one for which the likelihood L(f, e) of translational equivalence is highest.",
        "The simplicity of this algorithm depends on a one-to-one alignment assumption.",
        "Each word translates to at most one other word.",
        "Thus when one pair If, e} is “linked”, neither f nor e can be aligned with any other words.",
        "This assumption renders CLA unusable in phrase level alignment.",
        "We propose an extension, the competitive grouping, as the core component in the ISA model."
      ]
    },
    {
      "heading": "3.1 Competitive Grouping Algorithm (CGA)",
      "text": [
        "The key modification to the competitive linking algorithm is to make it less greedy.",
        "When a word pair is found to be the winner of the competition, we allow it to invite its neighbors to join the “winner’s club” and group them together as an aligned phrase pair.",
        "The one-to-one assumption is thus discarded in CGA.",
        "In addition, we introduce the locality assumption for phrase alignment.",
        "Locality states that a source phrase of adjacent words can only be aligned to a target phrase composed of adjacent words.",
        "This is not true of most language pairs in cases such as the relative clause, passive tense, and prepositional clause, etc.",
        "; however this assumption renders the problem tractable.",
        "Here is a description of CGA: For a sentence pair {f, e}, represent the word pair statistics for each word pair {f, e} in a two dimensional matrix LI,J, where L(i, j) = �2 (fi, ej) in our implementation.",
        "1",
        "Denote an aligned phrase pair {�f, �e} as a tuple [istart, iend, jstart, jend] where f� is fistart , fistart+1 , ... , fiend and similarly for �e.",
        "1.",
        "Find i* and j* such that L(i*, j*) is the highest.",
        "Create aseedphrase pair [i*, i*, j*, j*] which is simply the word pair { fi*, ej* } itself.",
        "2.",
        "Expand the current phrase pair",
        "[istart, iend, jstart, jend] to the neighboring territory to include adjacent source and target words in the phrase alignment group.",
        "There 1x2 statistics were found to be more discriminative in our experiments than other symmetric word association measures, such as the averaged mutual information, 02 statistics and Dice-coefficient.",
        "are 8 ways to group new words into the phrase pair.",
        "For example, one can expand to the north by including an additional source word fistart_1 to be aligned with all the target words in the current group; or one can expand to the northeast by including fistart_1 and ejend+1 (Figure 1).",
        "Two criteria have to be satisfied for each expansion: (a) If a new source word fig is to be grouped, maxjstart<j<jend L(i', j) should be no smaller than max1<j<J L(i', j).",
        "Since CGA is a greedy algorithm as described below, this is to guarantee that fig will not “regret” the decision of joining the phrase pair because it does not have other “better” target words to be aligned with.",
        "Similar constraint is applied if a new target word ej, is to be grouped.",
        "(b) The highest value in the newly-expanded area needs to be “similar” to the seed value L(i*,j*).",
        "Expand the current phrase pair to the largest extend possible as long as both criteria are satisfied.",
        "3.",
        "The locality assumption means that the aligned phrase cannot be aligned again.",
        "Therefore, all the source and target words in the phrase pair are marked as “invalid” and will be skipped in the following steps.",
        "4.",
        "If there is another valid pair { fi , ej }, then repeat from Step 1."
      ]
    },
    {
      "heading": "3.2 Exploring all possible groupings",
      "text": [
        "The similarity criterion 2-(b) described previously is used to control the granularity of phrase pairs.",
        "In cases where the pairs t f1 f2, e1 e2 }, tf1, e1 } and",
        "The granularity of the phrase pairs is hard to optimize especially when the test data is unknown.",
        "On the one hand, we prefer long phrases since interaction among the words in the phrase, for example word sense, morphology and local reordering could be encapsulated.",
        "On the other hand, long phrase pairs are less likely to occur in the test data than the shorter ones and may lead to low coverage.",
        "To have both long and short phrases in the alignment, we apply a range of similarity thresholds for each of the expansion operations.",
        "By applying a low similarity threshold, the expanded phrase pairs tend to be large, while a higher similarity threshold results in shorter phrase pairs.",
        "As described above, CGA is a greedy algorithm and the expansion of the seed pair restricts the possible alignments for the rest of the sentence.",
        "Figure 4 shows an example as we explore all the possible grouping choices in a depth-first search.",
        "In the end, all unique phrase pairs along the path traveled are output as phrase translation candidates for the current sentence pair."
      ]
    },
    {
      "heading": "3.3 Phrase translation probabilities",
      "text": [
        "Each aligned phrase pair t�f, �e} is assigned a likelihood score L(�f, �e), defined as: EZ maxj log L(fZ, ej) + Ej maxZ log L(fZ, ej ) 1�f1+1�e1 where i ranges over all words in f� and similarly j in �e.",
        "Given the collected phrase pairs and their likelihood, we estimate the phrase translation probability",
        "by their weighted frequency:"
      ]
    },
    {
      "heading": "4 Learning co-occurrence information",
      "text": [
        "In most cases, word alignment information is not given and is treated as a hidden parameter in the training process.",
        "We initialize a word pair co-occurrence frequency by assuming uniform alignment for each sentence pair, i.e. for sentence pair (f, e) where f has I words and e has J words, each word pair tf, e} is considered to be aligned with frequency I X J .",
        "These co-occurrence frequencies will be accumulated over the whole corpus to calculate the initial L(f, e).",
        "Then we iterate the ISA model:",
        "1.",
        "Apply the competitive grouping algorithm to each sentence pair to find all possible phrase pairs.",
        "2.",
        "For each identified phrase pair t�f, �e}, increase the co-occurrence counts for all word pairs inside t�f, �e} with weight 111161 3.",
        "Calculate L(f, e) again and goto Step 1 for several iterations."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We participated the shared task in the WPT05 workshop2 and applied ISA to all four language pairs",
        "(French-English, Finnish-English, German-English and Spanish-English).",
        "Table 1 shows the n-gram coverage of the dev-test set.",
        "French and Spanish data are better covered by the training data compared to the German and Finnish sets.",
        "Since our phrase alignment is constrained by the locality assumption and we can only extract phrase pairs of adjacent words, lower n-gram coverage will result in lower translation scores.",
        "We used the training data",
        "the training data.",
        "Numbers in parenthesis are the actual counts of n-gram tokens in the dev-test data.",
        "and the language model as provided and manually tuned the parameters of the Pharaoh decoder3 to optimize BLEU scores.",
        "Table 2 shows the translation results on the dev-test and the test set of WPT05.",
        "The BLEU scores appear comparable to those of other state-of-the-art phrase alignment systems, in spite of the simplicity of the ISA model and ease of training."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this paper, we introduced the competitive grouping algorithm which is at the core of the ISA phrase alignment model.",
        "As an extension to the competitive linking algorithm which is used for word-to-word alignment, CGA overcomes the assumption of one-to-one mapping and makes it possible to align phrase",
        "pairs.",
        "Despite its simplicity, the ISA model has achieved competitive translation results.",
        "We plan to release ISA toolkit4 to the community in the near future."
      ]
    }
  ]
}
