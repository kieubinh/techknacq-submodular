{
  "info": {
    "authors": [
      "Chi-San Althon Lin",
      "Tony C. Smith"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W06-1622",
    "title": "Semantic Role Labeling Via Instance-Based Learning",
    "url": "https://aclweb.org/anthology/W06-1622",
    "year": 2006
  },
  "references": [
    "acl-A00-2018",
    "acl-C04-1197",
    "acl-J02-3001",
    "acl-J05-1004",
    "acl-N04-1030",
    "acl-P03-1002",
    "acl-W03-1008",
    "acl-W04-0817",
    "acl-W04-2414",
    "acl-W04-2415",
    "acl-W04-2417",
    "acl-W04-2418",
    "acl-W05-0625",
    "acl-W05-0626",
    "acl-W05-0629"
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": [
        "The proceedings from CoNLL2004 and CoNLL2005 detail a wide variety of approaches to Semantic Role Labeling (SRL).",
        "Many research efforts utilize machine learning (ML) approaches; such as support vector machines (Mo-schitti et al., 2004; Pradhan et al., 2004), perceptrons (Carreras et al., 2004), the SNoW learning architecture (Punyakanok et al., 2004), EM-based clustering (Baldewein et al., 2004), transformation-based learning (Higgins, 2004), memory-based learning (Kouchnir, 2004), and inductive learning (Surdeanu et al., 2003).",
        "This paper compares two instance-based learning approaches, kNN and PML.",
        "The PML method used here utilizes a modification of the backoff lattice method used by Gildea & Jurafsky (2002) to use a set of basic features – specifically, the features employed for learning in this paper are Predicate (pr), Voice (vo), Phrase Type (pt), Distance (di), Head Word (hw), Path (pa), Preposition in a PP (pp), and an “Actor” heuristic.",
        "The general approach presented here is an example of memory-based learning.",
        "Many existing SRL systems are also memory-based (Bosch et al., 2004;Kouchnir, 2004), implemented using TilMBL software (http://ilk.kub.nl/software.html) with advanced methods such as Feature Weighting, and so forth.",
        "This paper measures the performance of kNN and PML for comparison in terms of accuracy and processing speed, both against each other and against previously published results."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": []
    },
    {
      "heading": "Features",
      "text": [
        "Most of the systems outlined in CoNLL2004 and CoNLL2005 utilize as many as 30 features for learning approaches to SRL.",
        "The research presented here uses only seven of these:"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper demonstrates two methods to improve the performance of instance-based learning (IBL) algorithms for the problem of Semantic Role Labeling (SRL).",
        "Two IBL algorithms are utilized: k-Nearest Neighbor (kNN), and Priority Maximum Likelihood (PML) with a modified back-off combination method.",
        "The experimental data are the WSJ23 and Brown Corpus test sets from the CoNLL2005 Shared Task.",
        "It is shown that applying the Tree-Based Predicate-Argument Recognition Algorithm (PARA) to the data as a preprocessing stage allows kNN and PML to deliver F1: 68.61 and 71.02 respectively on the WSJ23, and F1: 56.96 and 60.55 on the Brown Corpus; an increase of 8.28 in F1 measurement over the most recent published PML results for this problem (Palmer et al., 2005).",
        "Training times for IBL algorithms are very much faster than for other widely used techniques for SRL (e.g. parsing, support vector machines, perceptrons, etc); and the feature reduction effects of PARA yield testing and processing speeds of around 1.0 second per sentence for kNN and 0.9 second per sentence for PML respectively, suggesting that IBL could be a more practical way to perform SRL for NLP applications where it is employed; such as real-time Machine Translation or Automatic Speech Recognition.",
        ".",
        "Predicate – the given predicate lemma Voice – whether the predicate is realized as an active or passive construction (Pradhan et al., 2004, claim approximately 11% of the sentences in PropBank use a passive instantiation) Phrase Type – the syntactic category (NP, PP, S, etc.)",
        "of the phrase corresponding to the semantic argument Distance – the relative displacement from the predicate, measured in intervening constituents (negative if the constituent appears prior to the predicate, positive if it appears after it) Head Word – the syntactic head of the phrase, calculated by finding the last noun of a Noun Phrase Path – the syntactic path through the parse tree, from the parse constituent to the predicate being classified (for example, in Figure 1, the path from Arg0 – “The officer“ to the predicate “came“, is represented with the string NP T S y VP y VBD” represent upward and downward movements in the tree respectively) Preposition – the preposition of an argument in a PP, such as “during”, “at”, “with”, etc (for example, in Figure 1, the preposition for the PP with Argm-Loc label is “to”).",
        "In addition, an actor heuristic is adopted: where an instance can be labeled as A0 (actor) only if the argument is a subject before the predicate in active voice, or if the preposition “by” appears prior to this argument but after the predicate in a passive voice sentence.",
        "For example, if there is a set of labels, A0 (subject or actor) V (active) A0 (non actor), then the latter “A0” after V is skipped and labeled to another suitable role by this heuristic; such as the label with the second highest probability for this argument according to the PML estimate, or with the second shortest distance estimate by kNN."
      ]
    },
    {
      "heading": "2.1 k Nearest Neighbour (kNN) Algorithm",
      "text": [
        "One instance-based learning algorithm is k-Nearest Neighbour (kNN), which is suitable when 1) instances can be mapped to points/classifications in n-dimensional feature dimension, 2) fewer than 20 features are utilized, and 3) training data is sufficiently abundant.",
        "One advantage of kNN is that training is very fast; one disadvantage is it is generally slow at testing.",
        "The implementation of kNN is described as following"
      ]
    },
    {
      "heading": "1. Instance base:",
      "text": [
        "All the training data is stored in a format similar to Bosch et al., (2004) – specifically, “Role, Predicate, Voice, Phrase type, Distance, Head Word, Path”.",
        "As an example instance, the second argument of a predicate “take” in the training data is stored as: A0 take active NP –1 classics NP T S y VP y VBD This format maps each argument to six feature dimensions + one classification.",
        "2.",
        "Distance metric (Euclidean distance) is defined as:",
        "D(xi, xj) = 'f E (ar(xi))-ar(xj))2 where r=1 to n (n = number of different classifications), and ar(x) is the r-th feature of instance x.",
        "If instances xi and xj are identical, then D(xi , xj )=0 otherwise D(xi , xj ) represents the vector distance between xi and xj ."
      ]
    },
    {
      "heading": "3. Classification function",
      "text": [
        "Given a query/test instance xq to be classified, let x1, ... xk denote the k instances from the training data that are nearest to xq.",
        "The classification function is",
        "where i =1 to k, v =1 to m (m = size of training data), & (a,b)=1 if a=b, 0 otherwise; and v denotes a semantic role for each instance of training data.",
        "Computational complexity for kNN is linear, such that TkNN -> O( m * n ), which is proportional to the product of the number of features (m) and the number of training instances (n)."
      ]
    },
    {
      "heading": "2.2 Priority Maximum Likelihood (PML) Estimation",
      "text": [
        "Gildea & Jurafsky (2002), Gildea & Hocken-maier (2003) and Palmer et al., (2005) use a statistical approach based on Maximum Likelihood method for SRL, with different backoff combina"
      ]
    },
    {
      "heading": "Arg0 Predicate",
      "text": [
        "tion methods in which selected probabilities are combined with linear interpolation.",
        "The probability estimation or Maximum Likelihood is based on the number of known features available.",
        "If the full feature set is selected the probability is calculated by",
        "Gildea & Jurafsky (2002) claims “there is a trade-off between more-specific distributions, which have higher accuracy but lower coverage, and less-specific distributions, which have lower accuracy but higher coverage” and that the selection of feature subsets is exponential; and that selection of combinations of different feature subsets is doubly exponential, which is NP-complete.",
        "Gildea & Jurafsky (2002) propose the backoff combination in a linear interpolation for both coverage and precision.",
        "Following their lead, the research presented here uses Priority Maximum Likelihood Estimation modified from the backoff combination as follows:",
        "where Σ i λ i = 1.",
        "Figure 2 depicts a graphic organization of the priority combination with more-specific distribution toward the top, similar to Palmer et al.",
        "(2005) but adding another preposition feature.",
        "The backoff lattice is consulted to calculate probabilities for whichever subset of features is available to combine.",
        "As Gildea & Jurasksy (2002) state, “the less-specific distributions were used only when no data were present for any more-specific distribution.",
        "Thus, the distributions selected are arranged in a cut across the lattice representing the most-specific distributions for which data are available.”",
        "The classification decision is made by the following calculation for each argument in a sentence: argmax r1 .. n P(r1 ... n |f1,..n) This approach is described in more detail in Gildea and Jurasky (2002).",
        "The computational complexity of PML is hard to calculate due to the many different distributions at each priority level.",
        "In Figure 2, the two calculations P(r I hw, pp), and P(r I pt, di, vo, pp) belong to the global search, while the rest belong to a local search which can reduce the computational complexity.",
        "Examination of the details of execution time (described in the results section of this paper) show that a plot of the execution time exhibits logarithmic characteristics, implying that the computational complexity for PML is log-linear, such that TPML -> O( m * log n ) where m denotes the size of features and n denotes the size of training data."
      ]
    },
    {
      "heading": "2.3 Predicate-Argument Recognition Algorithm (PARA)",
      "text": [
        "Lin & Smith (2005; 2006) describe a tree-based predicate-argument recognition algorithm (PARA).",
        "PARA simply finds all boundaries for given predicates by browsing input parse-trees, such as given by Charniak’s parser or hand-corrected parses.",
        "There are three major types of phrases including given predicates, which are VP, NP, and PP.",
        "Boundaries can be recognized within boundary areas or from the top levels of clauses (as in Xue & Palmer, 2004).",
        "Figure 3 shows the basic algorithm of PARA, and more details can be found in Lin & Smith (2006).",
        "The best state-of-the-art ML technique using the same syntactic information (Moschitti, 2005) only just outperforms a preliminary version of PARA in F1 from 80.72 to 81.52 for boundary recognition tasks.",
        "But PARA is much faster than all other existing techniques, and is therefore used for preprocessing in this study to minimize query time when applying instance-based learning to SRL.",
        "The computational complexity of PARA is constant."
      ]
    },
    {
      "heading": "3 System Architecture",
      "text": [
        "There are two stages to this system: the building stage (comparable to training for inductive systems) and testing (or classification).",
        "The building stage shown in Figure 4 just stores all feature representations of training instances in memory without any calculations.",
        "All instances are stored in memory in the format described earlier, denoting {Role (r), Predicate (pr), Voice (vo),",
        "Phrase Type (pt), Path (pa), Distance (di), Head Word (hw), Preposition in a PP (pp) }.",
        "Figure 5 characterizes the testing stage, where new instances are classified by matching their feature representation to all instances in memory in order to find the most similar instances.",
        "There are two tasks during the testing stage: Argument Identification (or Boundary recognition) performed by PARA, and Argument Classification (or Role Labeling) performed using either kNN or PML.",
        "This approach is thus a “lazy learning” strategy applied to SRL because no calculations occur during the building stage."
      ]
    },
    {
      "heading": "4 Data, Evaluation, and Parsers",
      "text": [
        "The research outlined here uses the dataset released by the CoNLL-05 Shared Task (http://www.lsi.upc.edu/ – srlconll/soft.html).",
        "It includes several Wall Street Journal sections with parse-trees from both Charniak’s (2000) parser and Collins’ (1999) parser.",
        "These sections are also part of the PropBank corpus (http://www.cis.upenn.edu/ – treebank).",
        "WSJ sections 20 and 21 (with Charniak’s parses) were used as test data.",
        "PARA operates directly on the parse tree.",
        "Evaluation is carried out using precision, recall and F1 measures of assignment-accuracy of predicated arguments.",
        "Precision (p) is the proportion of arguments predicated by the system that are correct.",
        "Recall (r) is the proportion of correct arguments in the dataset that are predicated by the system.",
        "Finally, the F1 measure computes the harmonic mean of precision and recall, such that F1 =2*p*r / (p+r), and is the most commonly used primary measure when comparing different SRL systems.",
        "For consistency, the performance of PARA for boundary recognition is tested using the official evaluation script from CoNLL 2005, srl-eval.pl (http://www.lsi.upc.edu/ – srlconll/soft.html) in all experiments presented in this paper.",
        "Related statistics of training data and testing data are outlined in Table 1.",
        "The average number of predicates in a sentence for WSJ02-21 is 2.27, and each predicate comes with an average of 2.64 arguments."
      ]
    },
    {
      "heading": "Create_Boundary(predicate, tree)",
      "text": [
        "If the phrase type of the predicate == VP - find the boundary area ( the closest S clause) - find NP before predicate - If there is no NP, then find the closest NP from Ancestors.",
        "- find if WHNP in it’s siblings of the boundary area, if found // for what, which, that , who,... - if the word of the first WP’s family is “what” then - add WHNP to boundary list else // not what, such as who which,... - find the closest NP from Ancestors - add the NP to the boundary list and add this WHNP to boundary list as reference of NP - add valid boundaries of the rest of constituents to boundary list.",
        "If phrase type of the predicate ==NP - find the boundary area ( the NP clause) - find RB(POS) before predicate and add to boundary list.",
        "- Add this predicate to boundary list.",
        "- Add the rest of word group after the predicate and before the end of the NP clause as a whole boundary to boundary list.",
        "If phrase type of the predicate ==PP - find the boundary area ( the PP clause) - find the closet NP from Ancestors if the lemma of the predicate is “include”, and add this NP to boundary list.",
        "(special for PropBank) - Add this predicate to boundary list.",
        "- Add the rest of children of this predicate to boundary list or add one closest NP outside the boundary area to boundary list if there is no child after this predicate."
      ]
    },
    {
      "heading": "5 Experiments and Results",
      "text": [
        "Experimental results were obtained for part of the Brown corpus (the part provided by CoNLL2005) and for Wall Street Journal (WSJ) Sections 21, 23, and 24 using different training data sets (WSJ 21, WSJ 15 to 18, and WSJ 02 to 21) shown in Table 1.",
        "There are two tasks, Role classification with known arguments as input, and Boundary recognition & Role classification with gold (hand-corrected) parses or auto (Charniak’s) parses.",
        "In addition, execution speed, the learning curve, and some further results for exploration of kNN and PML are also included below."
      ]
    },
    {
      "heading": "5.1 WSJ 24 with known arguments",
      "text": [
        "Table 2 shows the results from kNN and PML with known boundaries/arguments (i.e. the systems are given the correct arguments for role classification).",
        "All training datasets (WSJ02-21) include Charniak’s parse trees.",
        "The table shows that PML achieves F1: 2.69 better than kNN."
      ]
    },
    {
      "heading": "5.2 Features & Heuristic on WSJ 24 with known arguments",
      "text": [
        "Table 3 shows the contribution of each feature and the actor heuristic by excluding one feature or heuristic.",
        "It indicates that Head Word, Preposition, and Distance are the three features that contribute most to system accuracy, and the additional Actor heuristic is fourth.",
        "Path, Phrase type and Voice are the three features contibuting the least for both classification algorithms."
      ]
    },
    {
      "heading": "5.3 Learning Curve",
      "text": [
        "Table 4 shows that performance improves as more training data is provided; and that PML outperforms kNN by about F1:2.8 on average for WSJ 24 for the three different training sets, mainly because the backoff lattice improves both recall and precision.",
        "The table shows that it is not always beneficial to include all features for labeling all roles.",
        "While P(r I hw, pt, pre, pp) is mainly for adjunctive roles (e.g. AM-TMP), P(r I pt, di, vo, pr, pp) is mainly for core roles (e.g. A0)."
      ]
    },
    {
      "heading": "5.4 Performance of Execution Time",
      "text": [
        "Building (or training) time is about 2.5 minutes for both PML and kNN, whereas it takes anywhere from about 10 hours to 60 hours for other ML-based architectures (according to the data presented by McCracken http://www.lsi.upc.es/ ~srlconll/st05/slides/mccracken.pdf).",
        "Table 5 shows average execution time (in seconds) per sentence for the two algorithms.",
        "PML runs faster than kNN when all 20 training datasets are used (i.e. WSJ 02 to 21).",
        "A graphic illustration of execution speed is shown in Figure 6.",
        "The simulation formulas for PML and kNN are “y = 0.1734Ln(x) - 0.9046” and “y = 2.441* 10-5 x + 0.0129” respectively.",
        "“x” denotes numbers of training sentences, and “y” denotes second per sentence related to “x” training sentences.",
        "The execution time for PML is about 8 times longer than kNN for 1.7k training sentences, but PML ultimately runs faster than kNN on all 39.8K training sentences (and, extrapolating from the graph in Figure 6, on any larger datasets).",
        "Thus PML seems generally more suitable for large training data."
      ]
    },
    {
      "heading": "5.5 WSJ 24 with Gold parses and PARA",
      "text": [
        "Table 6 shows performance for both systems when gold (hand-corrected) parses are supplied and PARA preprocessing is employed.",
        "Compared to the results in Table 4, the performance on the combined training sets (WSJ 02 to 21) drops F1:9.24 and Lacc (label accuracy):2.4 for kNN; and drops F1:8.02 and Lacc:0.66 for PML respectively.",
        "This may indicate that PML is more error tolerant in labeling accuracy.",
        "However, both systems perform worse due largely to an idiosyncratic problem in the PARA-preprocessor when dealing with hand-corrected parses – ultimately due to a particular parsing error."
      ]
    },
    {
      "heading": "5.6 WSJ 24 with Charniak’s parses and PARA",
      "text": [
        "Table 7 shows the performance of both systems using auto-parsing (i.e. Charniak’s parser) and PARA argument recognition.",
        "Compared to the results in Table 4, the performance on all training sets (WSJ 02 to 21) drops F1:17.25 and Lacc:0.65 for kNN, and F1:16.78 and Lacc:-0.78 (i.e. increasing Lacc) for PML respectively.",
        "Both systems drop a lot in F1 due to errors caused by the auto-parser (in particular errors relating to punctuation), whose effects are subsequently exacerbated by PARA.",
        "Even so, the label accuracy (Lacc) is more or less similar because the training dataset are parsed by Charniak’s parser instead of gold parses.",
        "Table 8 shows the results for WSJ 23, where the performance of PML exceeds kNN by about F1:3.8.",
        "WSJ 23 is used as a comparison dataset in SRL.",
        "More comparisons with other systems are shown in Table 12."
      ]
    },
    {
      "heading": "5.8 Brown corpus with Charniak’s parses and PARA",
      "text": [
        "Table 9 shows the results when moving to a different language domain – the Brown corpus.",
        "Both systems drop a lot in F1 .",
        "Compared to WSJ 23, MPL drops 10.47 in F1 and kNN, 11.65 in F1.",
        "These drops are caused partially by PARA, and partially by classifiers.",
        "PARA in Lin & Smith (2006) drops about 3.1 in F1 when moving to the Brown Corpus; but more research is required to uncover the cause."
      ]
    },
    {
      "heading": "5.9 Further results on kNN with all training data",
      "text": [
        "Table 10 shows different results for various values of k in kNN.",
        "Both systems, GP (gold-parse) & PARA and CP (Charniak’s parse) & PARA, perform best (as measured by F1) when K is set as one.",
        "But when the system is labeling a known argument, selection of k=5 is better in terms of both F1 and Label accuracy (Lacc)."
      ]
    },
    {
      "heading": "5.10 Further results on PML with all training data",
      "text": [
        "Table 11 shows results for PML with different methods of calculating probabilities.",
        "“L+G” means the basic probability distribution (from Figure 2).",
        "“L only” and “G only” mean all probability is calculated only as either “local” or “global”, respectively.",
        "“L>>G” means that probabilities are calculated globally only when the local probability is zero.",
        "“L only” is the fastest approach, and “G only” the slowest (about five seconds per sentence).",
        "Both are poor in performance.",
        "“L+G” has the best result and “L>>G” is rated as intermediate in performance and execution time."
      ]
    },
    {
      "heading": "5.11 Comparison with other systems",
      "text": [
        "Table 12 shows results from other existing systems.",
        "In the second row (PARA+PML) is trained on all datasets (WSJ 02 to 21) for the “BR+RL” task (to recognize argument boundaries and label arguments) on the test data WSJ 23, with an improvement of F1:8.28 in comparison to the result of Palmer et al., (2005) given in the",
        "first row.",
        "The basic kNN in the fourth row, trained by four datasets (WSJ 15 to 18 in CoNLL 2004) for the RL” task (to label arguments by giving the known arguments) on the test data WSJ 21, increases F1:6.68 compared to the result of Kouchnir (2004) in the third row.",
        "Execution time for our own reimplementation of Palmer (2005) is about 3.785 sec per sentence.",
        "Instead of calculating each node in a parse tree like the Palmer (2005) model, PARA+PML can only focus on essential nodes from the output of PARA, which helps to reduce the execution time as 0.941 second per sentence.",
        "Execution time by Palmer (2005) is about 4 times longer than PARA+PML on the same machine (n.b.",
        "execution times are for a computer running Linux on a P4 2.6GHz CPU with 1G MBRAM).",
        "More details from different systems and combinations of systems are described in the proceedings of CoNLL-2005."
      ]
    },
    {
      "heading": "6 Summary and Remarks",
      "text": [
        "This paper has shown that basic syntactic information is useful for Semantic role labeling using instance-based learning techniques.",
        "Specifically, the following have been demonstrated: 1.",
        "It is possible to achieve acceptable F1 scores with considerably faster execution times (compared to Gildea & Jurasky, 2002) for the Semantic role labeling problem using the Priority Maximum Likelihood instance-based learning algorithm and the Tree-based Predicate-Argument Algorithm (PARA) as a preprocessing step, without any training given a state-of-the-art parser such as Charniak’s parser.",
        "The overall performance on WSJ 23 dataset is 71.02 in F1 score.",
        "Performance drops to 60.55 for the Brown corpus, but this appears to be similar to performance drops experienced by other systems reported in CoNLL-2005.",
        "2.",
        "F1 performance is better for PML than for kNN, where the computational complexity for PML is O( m * log n ) as opposed to O( m * n ) for kNN, where m denotes the number of features and n denotes the number of training instances.",
        "3.",
        "Execution time for the instance-based learning presented here is about four times faster for SRL than the comparable approach used by Palmer, (2005).",
        "That is, PARA plays an important role reducing the overhead during classification when using instance-based learning.",
        "4.",
        "Using PARA, and other modifications such as the preposition feature and Actor heuristic, improves the accuracy of both kNN and PML in comparison to similar approaches.",
        "5.",
        "The best system developed for this paper (PML & PARA) is still outperformed by some of the best systems from CoNLL2005 when it comes to accuracy, but it is much simpler and is many orders-of-magnitude faster at delivering acceptable performance.",
        "With the latest revised and optimized PML, the performance on WSJ 23 is 71.22 in F1, and the speed is 0.623 second per sentence with 3.0G CPU and 1 G RAM.",
        "Koomen et al.",
        "(2006), with more than 25 features, achieved the best results reported in CoNLL2005 on WSJ 24; but PML’s performance (using PARA as a preprocessor, and seven features) achieves an F1 measure 5.10 less than Kooman’s system (74.76) on WSJ 24 utilising Charniak-1 parses, and 4.07 less when using Kooman’s test result (WSJ 23) as known-boundary input.",
        "In this experiment, with the Actor heuristic, PML delivers better accuracy for A0 (89.96%) than Kooman’s (88.22%), but the recall (83.53%) is 4.35 % lower than Kooman’s (87.88%).",
        "There are some spaces to improve PML such as low accuracy on AM-MOD, and AM-NEG, and duplicate core roles, and forth.",
        "Future work will investigate using more features, new heuristics and/or other ML approaches to improve the performance of instance-based learning algorithms at the SRL task."
      ]
    }
  ]
}
