{
  "info": {
    "authors": [
      "Yejin Choi",
      "Eric Breck",
      "Claire Cardie"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W06-1651",
    "title": "Joint Extraction of Entities and Relations for Opinion Recognition",
    "url": "https://aclweb.org/anthology/W06-1651",
    "year": 2006
  },
  "references": [
    "acl-C02-1151",
    "acl-C04-1197",
    "acl-H01-1054",
    "acl-H05-1044",
    "acl-H05-1045",
    "acl-H05-1068",
    "acl-H05-1116",
    "acl-I05-2011",
    "acl-P04-1056",
    "acl-W04-2401",
    "acl-W05-0625"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis.",
        "We identify two types of opinion-related entities – expressions of opinions and sources of opinions – along with the linking relation that exists between them.",
        "Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities.",
        "Performance further improves when a semantic role labeling system is incorporated.",
        "The resulting system achieves F-measures of 79 and 69 for entity and relation extraction, respectively, improving substantially over prior results in the area."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Information extraction tasks such as recognizing entities and relations have long been considered critical to many domain-specific NLP tasks (e.g. Mooney and Bunescu (2005), Prager et al.",
        "(2000), White et al.",
        "(2001)).",
        "Researchers have further shown that opinion-oriented information extraction can provide analogous benefits to a variety of practical applications including product reputation tracking (Morinaga et al., 2002), opinion-oriented question answering (Stoyanov et al., 2005), and opinion-oriented summarization (e.g. Cardie et al.",
        "(2004), Liu et al.",
        "(2005)).",
        "Moreover, much progress has been made in the area of opinion extraction: it is possible to identify sources of opinions (i.e. the opinion holders) (e.g. Choi et al.",
        "(2005) and Kim and Hovy (2005b)), to determine the polarity and strength of opinion expressions (e.g. Wilson et al.",
        "(2005)), and to recognize propositional opinions and their sources (e.g. Bethard et al.",
        "(2004)) with reasonable accuracy.",
        "To date, however, there has been no effort to simultaneously identify arbitrary opinion expressions, their sources, and the relations between them.",
        "Without progress on the joint extraction of opinion entities and their relations, the capabilities of opinion-based applications will remain limited.",
        "Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)).",
        "Moreover, it has been shown that exploiting dependencies among entities and/or relations via global inference not only solves the joint extraction task, but often boosts performance on the individual tasks when compared to classifiers that handle the tasks independently – for semantic role labeling (e.g. Punyakanok et al.",
        "(2004)), information extraction (e.g. Roth and Yih (2004)), and sequence tagging (e.g. Sutton et al.",
        "(2004)).",
        "In this paper, we present a global inference approach (Roth and Yih, 2004) to the extraction of opinion-related entities and relations.",
        "In particular, we aim to identify two types of entities (i.e. spans of text): entities that express opinions and entities that denote sources of opinions.",
        "More specifically, we use the term opinion expression to denote all direct expressions of subjectivity including opinions, emotions, beliefs, sentiment, etc., as well as all speech expressions that introduce subjective propositions; and use the term source to denote the person or entity (e.g. a re",
        "Sydney, July 2006. c�2006 Association for Computational Linguistics port) that holds the opinion.1 In addition, we aim to identify the relations between opinion expression entities and source entities.",
        "That is, for a given opinion expression OZ and source entity Sj, we determine whether the relation LZj def (Sj expresses OZ) obtains, i.e. whether Sj is the source of opinion expression OZ.",
        "We refer to this particular relation as the link relation in the rest of the paper.",
        "Consider, for example, the following sentences: S1.",
        "[Bush](') intends(') to curb the increase in harmful gas emissions and is counting on(') the good will(2) of [US industrialists](2) .",
        "S2.",
        "By questioning(3) [the Imam] (4)’s edict(4) [the Islamic Republic ofIran] (3) made [the people of the world] (5) understand (5) ...",
        "The underlined phrases above are opinion expressions and phrases marked with square brackets are source entities.",
        "The numeric superscripts on entities indicate link relations: a source entity and an opinion expression with the same number satisfy the link relation.",
        "For instance, the source entity “Bush” and the opinion expression “intends” satisfy the link relation, and so do “Bush” and “counting on.” Notice that a sentence may contain more than one link relation, and link relations are not one-to-one mappings between sources and opinions.",
        "Also, the pair of entities in a link relation may not be the closest entities to each other, as is the case in the second sentence, between “questioning” and “the Islamic Republic ofIran.” We expect the extraction of opinion relations to be critical for many opinion-oriented NLP applications.",
        "For instance, consider the following question that might be given to a question-answering system:",
        "• What is the Imam’s opinion toward the Islamic"
      ]
    },
    {
      "heading": "Republic ofIran?",
      "text": [
        "Without in-depth opinion analysis, the question-answering system might mistake example S2 as relevant to the query, even though S2 exhibits the opinion of the Islamic Republic of Iran toward Imam, not the other way around.",
        "Inspired by Roth and Yih (2004), we model our task as global, constraint-based inference over separately trained entity and relation classifiers.",
        "In particular, we develop three base classifiers: two sequence-tagging classifiers for the extraction",
        "of opinion expressions and sources, and a binary classifier to identify the link relation.",
        "The global inference procedure is implemented via integer linear programming (ILP) to produce an optimal and coherent extraction of entities and relations.",
        "Because many (60%) opinion-source relations appear as predicate-argument relations, where the predicate is a verb, we also hypothesize that semantic role labeling (SRL) will be very useful for our task.",
        "We present two baseline methods for the joint opinion-source recognition task that use a state-of-the-art SRL system (Punyakanok et al., 2005), and describe two additional methods for incorporating SRL into our ILP-based system.",
        "Our experiments show that the global inference approach not only improves relation extraction over the base classifier, but does the same for individual entity extractions.",
        "For source extraction in particular, our system achieves an F-measure of 78.1, significantly outperforming previous results in this area (Choi et al., 2005), which obtained an F-measure of 69.4 on the same corpus.",
        "In addition, we achieve an F-measure of 68.9 for link relation identification and 82.0 for opinion expression extraction; for the latter task, our system achieves human-level performance.2"
      ]
    },
    {
      "heading": "2 High-Level Approach and Related Work",
      "text": [
        "Our system operates in three phases.",
        "Opinion and Source Entity Extraction We begin by developing two separate token-level sequence-tagging classifiers for opinion expression extraction and source extraction, using linear-chain Conditional Random Fields (CRFs) (Lafferty et al., 2001).",
        "The sequence-tagging classifiers are trained using only local syntactic and lexical information to extract each type of entity without knowledge of any nearby or neighboring entities or relations.",
        "We collect n-best sequences from each sequence tagger in order to boost the recall of the final system.",
        "Link Relation Classification We also develop a relation classifier that is trained and tested on all pairs of opinion and source entities extracted from the aforementioned n-best opinion expression and source sequences.",
        "The relation classifier is modeled using Markov order-0 CRFs(Lafferty",
        "et al., 2001), which are equivalent to maximum entropy models.",
        "It is trained using only local syntactic information potentially useful for connecting a pair of entities, but has no knowledge of nearby or neighboring extracted entities and link relations.",
        "Integer Linear Programming Finally, we formulate an integer linear programming problem for each sentence using the results from the previous two phases.",
        "In particular, we specify a number of soft and hard constraints among relations and entities that take into account the confidence values provided by the supporting entity and relation classifiers, and that encode a number of heuristics to ensure coherent output.",
        "Given these constraints, global inference via ILP finds the optimal, coherent set of opinion-source pairs by exploiting mutual dependencies among the entities and relations.",
        "While good performance in entity or relation extraction can contribute to better performance of the final system, this is not always the case.",
        "Punyakanok et al.",
        "(2004) notes that, in general, it is better to have high recall from the classifiers included in the ILP formulation.",
        "For this reason, it is not our goal to directly optimize the performance of our opinion and source entity extraction models or our relation classifier.",
        "The rest of the paper is organized as follows.",
        "Related work is outlined below.",
        "Section 3 describes the components of the first phase of our system, the opinion and source extraction classifiers.",
        "Section 4 describes the construction of the link relation classifier for phase two.",
        "Section 5 describes the ILP formulation to perform global inference over the results from the previous two phases.",
        "Experimental results that compare our ILP approach to a number of baselines are presented in Section 6.",
        "Section 7 describes how SRL can be incorporated into our global inference system to further improve the performance.",
        "Final experimental results and discussion comprise Section 8.",
        "Related Work The definition of our source-expresses-opinion task is similar to that of Bethard et al.",
        "(2004); however, our definition of opinion and source entities are much more extensive, going beyond single sentences and propositional opinion expressions.",
        "In particular, we evaluate our approach with respect to (1) a wide variety of opinion expressions, (2) explicit and implicit3 sources, (3) multiple opinion-source link relations 3Implicit sources are those that are not explicitly mentioned.",
        "See Section 8 for more details.",
        "per sentence, and (4) link relations that span more than one sentence.",
        "In addition, the link relation model explicitly exploits mutual dependencies among entities and relations, while Bethard et al.",
        "(2004) does not directly capture the potential influence among entities.",
        "Kim and Hovy (2005b) and Choi et al.",
        "(2005) focus only on the extraction of sources of opinions, without extracting opinion expressions.",
        "Specifically, Kim and Hovy (2005b) assume a priori existence of the opinion expressions and extract a single source for each, while Choi et al.",
        "(2005) do not explicitly extract opinion expressions nor link an opinion expression to a source even though their model implicitly learns approximations of opinion expressions in order to identify opinion sources.",
        "Other previous research focuses only on the extraction of opinion expressions (e.g. Kim and Hovy (2005a), Munson et al.",
        "(2005) and Wilson et al.",
        "(2005)), omitting source identification altogether.",
        "There have also been previous efforts to simultaneously extract entities and relations by exploiting their mutual dependencies.",
        "Roth and Yih (2002) formulated global inference using a Bayesian network, where they captured the influence between a relation and a pair of entities via the conditional probability of a relation, given a pair of entities.",
        "This approach however, could not exploit dependencies between relations.",
        "Roth and Yih (2004) later formulated global inference using integer linear programming, which is the approach that we apply here.",
        "In contrast to our work, Roth and Yih (2004) operated in the domain of factual information extraction rather than opinion extraction, and assumed that the exact boundaries of entities from the gold standard are known a priori, which may not be available in practice."
      ]
    },
    {
      "heading": "3 Extraction of Opinion and Source Entities",
      "text": [
        "We develop two separate sequence tagging classifiers for opinion extraction and source extraction, using linear-chain Conditional Random Fields (CRFs) (Lafferty et al., 2001).",
        "The sequence tagging is encoded as the typical ‘BIO’ scheme.4 Each training or test instance represents a sentence, encoded as a linear chain of tokens and their",
        "associated features.",
        "Our feature set is based on that of Choi et al.",
        "(2005) for source extraction5, but we include additional lexical and WordNet-based features.",
        "For simplicity, we use the same features for opinion entity extraction and source extraction, and let the CRFs learn appropriate feature weights for each task."
      ]
    },
    {
      "heading": "3.1 Entity extraction features",
      "text": [
        "For each token xi, we include the following features.",
        "For details, see Choi et al.",
        "(2005).",
        "word: words in a [-4, +4] window centered on xi.",
        "part-of-speech: POS tags in a [-2, +2] window.6 grammatical role: grammatical role (subject, object, prepositional phrase types) of xi derived from a dependency parse.7 dictionary: whether xi is in the opinion expression dictionary culled from the training data and augmented by approximately 500 opinion words from the MPQA Final Report8.",
        "Also computed for tokens in a [-1, +1] window and for xi’s parent “chunk” in the dependency parse.",
        "semantic class: xi’s semantic class.9 WordNet: the WordNet hypernym of xi.10"
      ]
    },
    {
      "heading": "4 Relation Classification",
      "text": [
        "We also develop a maximum entropy binary classifier for opinion-source link relation classification.",
        "Given an opinion-source pair, Oi-5j, the relation classifier decides whether the pair exhibits a valid link relation, Li j.",
        "The relation classifier focuses only on the syntactic structure and lexical properties between the two entities of a given pair, without knowing whether the proposed entities are correct.",
        "Opinion and source entities are taken from the n-best sequences of the entity extraction models; therefore, some are invariably incorrect.",
        "From each sentence, we create training and test instances for all possible opinion-source pairings that do not overlap: we create an instance for Li,j only if the span of Oi and 5j do not overlap.",
        "For training, we also filter out instances for which neither the proposed opinion nor source en",
        "tity overlaps with a correct opinion or source entity per the gold standard.",
        "This training instance filtering helps to avoid confusion between examples like the following (where entities marked in bold are the gold standard entities, and entities in square brackets represent the n-best output sequences from the entity extraction classifiers):",
        "(1) [The president] s1 walked away from [the meeting] o1, [ [revealing] o2 his disappointment] o3 with the deal.",
        "(2) [The monster] s2 walked away, [revealing] o4 a little box hidden underneath.",
        "For these sentences, we construct training instances for L1 ,1, L1,2, and L1,3, but not L2,4, which in fact has very similar sentential structure as L1,2, and hence could confuse the learning algorithm."
      ]
    },
    {
      "heading": "4.1 Relation extraction features",
      "text": [
        "The training and test instances for each (potential) link Li,j (with opinion candidate entity Oi and source candidate entity 5j) include the following features.",
        "opinion entity word: the words contained in Oi.",
        "phrase type: the syntactic category of the constituent in which the entity is embedded, e.g. NP or VP.",
        "We encode separate features for Oi and 5j .",
        "grammatical role: the grammatical role of the constituent in which the entity is embedded.",
        "Grammatical roles are derived from dependency parse trees, as done for the entity extraction classifiers.",
        "We encode separate features for Oi and 5j.",
        "position: a boolean value indicating whether 5j precedes Oi.",
        "distance: the distance between Oi and 5j in numbers of tokens.",
        "We use four coarse categories: adjacent, very near, near, far.",
        "dependency path: the path through the dependency tree from the head of 5j to the head of Oi.",
        "For instance, ‘subj↑verb’ or ‘subj↑verb↓obj’.",
        "voice: whether the voice of Oi is passive or active.",
        "syntactic frame: key intra-sentential relations between Oi and 5j.",
        "The syntactic frames that we use are: ◦ [E1:role] [distance] [E2:role], where distance ∈ {adjacent, very near, near, far}, and Ei:role is the grammatical role of Ei.",
        "Either E1 is an opinion entity and E2 is a source, or vice versa.",
        "◦ [E1:phrase] [distance] [E2:phrase], where Ei:phrase is the phrasal type of entity Ei.",
        "a negative conditional probability of the span of an entity to be extracted (or suppressed) given the labelings of the adjacent variables of the CRFs: def woi = −P (xk, xk+1, ..., xl |xk-1, xl+1) where xk = ‘B’ & xm = ‘I’ for m ∈ [k + 1, l] 5 Integer Linear Programming def Approach �woi = −P (xk, xk+1, ..., xl |xk-1, xl+1) As noted in the introduction, we model our task where xm = ‘O’ for m ∈ [k, l] as global, constraint-based inference over the separately trained entity and relation classifiers, and implement the inference procedure as binary integer linear programming (ILP) ((Roth and Yih, 2004), (Punyakanok et al., 2004)).",
        "ILP consists of an objective function which is a dot product between a vector of variables and a vector of weights, and a set of equality and inequality constraints among variables.",
        "Given an objective function and a set of constraints, LP finds the optimal assignment of values to variables, i.e. one that minimizes the objective function.",
        "In binary ILP, the assignments to variables must be either 0 or 1.",
        "The variables and constraints defined for the opinion recognition task are summarized in Table 1 and explained below.",
        "Entity variables and weights For each opinion entity, we add two variables, Oi and �Oi, where Oi = 1 means to extract the opinion entity, and 435 where xi is the value assigned to the random variable of the CRF corresponding to an entity Oi.",
        "Likewise, for each source entity, we add two variables Sj and �Sj and a constraint Sj + �Sj = 1.",
        "The weights for source variables are computed in the same way as opinion entities.",
        "Relation variables and weights For each link relation, we add two variables Li,j and �Li ,j, and a constraint Li,j + �Li,j = 1.",
        "By the definition of a link, if Li,j = 1, then it is implied that Oi = 1 and Sj = 1.",
        "That is, if a link is extracted, then the pair of entities for the link must be also extracted.",
        "Constraints to ensure this coherency are explained in the following subsection.",
        "The weights for link variables are based on probabilities from the binary link classifier.",
        "Constraints for link coherency In our corpus, a source entity can be linked to more than one opinion entity, but an opinion entity is linked to only one source.",
        "Nonetheless, the majority of opinion-source pairs involve one-to-one mappings, which we encode as hard and soft constraints as follows: For each opinion entity, we add an equality constraint Oi = Ej Lij to enforce that only one link can emanate from an opinion entity.",
        "For each source entity, we add an equality constraint and an inequality constraint that together allow a source to link to at most two opinions: Sj +Aj = Ei Li j and Aj − Sj ≤ 0, where Aj is an auxiliary variable, such that its weight is some positive constant value that suppresses Aj from being assigned to 1.",
        "And Aj can be assigned to 1 only if Sj is already assigned to 1.",
        "It is possible to add more auxiliary variables to allow more than two opinions to link to a source, but for our experiments two seemed to be a reasonable limit.",
        "Constraints for entity coherency When we use n-best sequences where n > 1, proposed entities can overlap.",
        "Because this should not be the case in the final result, we add an equality constraint Xi + Xj = 1, X ∈ {S, O} for all pairs of entities with overlapping spans.",
        "Adjustments to weights To balance the precision and recall, and to take into account the performance of different base classifiers, we apply adjustments to weights as follows.",
        "1) We define six coefficients cx and �cx, where x ∈ {O, S, L} to modify a group of weights as follows.",
        "∀i, x, wxz := wxz ∗ cx; ∀i, x, �wxz := �wxz ∗ �cx; In general, increasing cx will promote recall, while increasing �cx will promote precision.",
        "Also, setting co > cs will put higher confidence on the opinion extraction classifier than the source extraction classifier.",
        "2) We also define one constant cA to set the weights for auxiliary variable Ai.",
        "That is, ∀i, wAz := cA.",
        "3) Finally, we adjust the confidence of the link variable based on n-th-best sequences of the entity extraction classifiers as follows.",
        "∀i, wLz,� := wLz,� ∗ d where d def = 4/(3 + min (m, n)), when Oi is from an m-th sequence and Sj is from a nth sequence.",
        "11 11 This will smoothly degrade the confidence of a link based on the entities from higher nth sequences.",
        "Values of d decrease as 4/4, 4/5, 4/6, 4/7...."
      ]
    },
    {
      "heading": "6 Experiments–I",
      "text": [
        "We evaluate our system using the NRRC Multi-Perspective Question Answering (MPQA) corpus that contains 535 newswire articles that are manually annotated for opinion-related information.",
        "In particular, our gold standard opinion entities correspond to direct subjective expression annotations and subjective speech event annotations (i.e. speech events that introduce opinions) in the MPQA corpus (Wiebe et al., 2005).",
        "Gold standard source entities and link relations can be extracted from the agent attribute associated with each opinion entity.",
        "We use 135 documents as a development set and report 10-fold cross validation results on the remaining 400 documents in all experiments below.",
        "We evaluate entity and link extraction using both an overlap and exact matching scheme.",
        "12 Because the exact start and endpoints of the manual annotations are somewhat arbitrary, the overlap scheme is more reasonable for our task (Wiebe et al., 2005).",
        "We report results according to both matching schemes, but focus our discussion on results obtained using overlap matching.",
        "13 We use the Mallet14 implementation of CRFs.",
        "For brevity, we will refer to the opinion extraction classifier as CRF-OP, the source extraction classifier as CRF-SRC, and the link relation classifier as CRF-LINK.",
        "For ILP, we use Matlab, which produced the optimal assignment in a matter of few seconds for each sentence.",
        "The weight adjustment constants defined for ILP are based on the development data.",
        "15 The link-nearest baselines For baselines, we first consider a link-nearest heuristic: for each opinion entity extracted by CRF-OP, the link-nearest heuristic creates a link relation with the closest source entity extracted by CRF-SRC.",
        "Recall that CRF-SRC and CRF-OP extract entities from n-best sequences.",
        "We test the link-nearest heuristic with n = {1, 2,10} where larger n will boost recall at the cost of precision.",
        "Results for the"
      ]
    },
    {
      "heading": "12 Given two links L1,1 = (O1, S1) and L2,2 = (O2, S2),",
      "text": [
        "exact matching requires the spans of O1 and O2, and the spans of S1 and S2, to match exactly, while overlap matching requires the spans to overlap.",
        "link-nearest heuristic on the full source-expresses-opinion relation extraction task are shown in the first three rows of table 2.",
        "NEAREST-1 performs the best in overlap-match F-measure, reaching 59.9.",
        "NEAREST-10 has higher recall (66.3%), but the precision is really low (20.9%).",
        "Performance of the opinion and source entity classifiers will be discussed in Section 8.",
        "SRL baselines Next, we consider two baselines that use a state-of-the-art SRL system (Punyakanok et al., 2005).",
        "In many link relations, the opinion expression entity is a verb phrase and the source entity is in an agent argument position.",
        "Hence our second baseline, SRL, extracts all verb(V)-agent(A0) frames from the output of the SRL system and provides an upper bound on recall (59.7%) for systems that use SRL in isolation for our task.",
        "A more sophisticated baseline, SRL+CRF-OP, extracts only those V-A0 frames whose verb overlaps with entities extracted by the opinion expression extractor, CRF-OP.",
        "As shown in table 2, filtering out V-A0 frames that are incompatible with the opinion extractor boosts precision to 83.2%, but the F-measure (58.9) is lower than that of NEAREST-1.",
        "ILP results The ILP-n system in table 2 denotes the results of the ILP approach applied to the n-best sequences.",
        "ILP-10 reaches an F-measure of 68.0, a significant improvement over the highest performing baseline16, and also a substantial improvement over ILP-1.",
        "Note that the performance of NEAREST-10 was much worse than that",
        "and SRL constraints, n-best of NEAREST-1, because the 10-best sequences include many incorrect entities whereas the corresponding ILP formulation can discard the bad entities by considering dependencies among entities and relations.",
        "17"
      ]
    },
    {
      "heading": "7 Additional SRL Incorporation",
      "text": [
        "We next explore two approaches for more directly incorporating SRL into our system.",
        "Extra SRL Features for the Link classifier We incorporate SRL into the link classifier by adding extra features based on SRL.",
        "We add boolean features to check whether the span of an SRL argument and an entity matches exactly.",
        "In addition, we include syntactic frame features as follows:",
        "• [E1:srl-arg] [E2:srl-arg], where EZ:srl-arg indicates the SRL argument type of entity EZ.",
        "• [E1.srl-arg] [E1:headword] [E2:srl-arg], where E1 must be an opinion entity, and E2 must be a source entity.",
        "Extra SRL Constraints for the ILP phase We also incorporate SRL into the ILP phase of our system by adding extra constraints based on SRL.",
        "In particular, we assign very high weights for links that match V-A0 frames generated by SRL, in order to force the extraction of V-A0 frames.",
        "17A potential issue with overlap precision and recall is that the measures may drastically overestimate the system’s performance as follows: a system predicting a single link relation whose source and opinion expression both overlap with every token of a document would achieve 100% overlap precision and recall.",
        "We can ensure this does not happen by measuring the average number of (source, opinion) pairs to which each correct or predicted pair is aligned (excluding pairs not aligned at all).",
        "In our data, this does not exceed 1.08, (except for baselines), so we can conclude these evaluation measures are behaving reasonably."
      ]
    },
    {
      "heading": "8 Experiments–II",
      "text": [
        "Results using SRL are shown in Table 3 (on the previous page).",
        "In the table, ILP+SRL- f denotes the ILP approach using the link classifier with the extra SRL ‘f’eatures, and ILP+SRL- f c denotes the ILP approach using both the extra SRL ‘f’eatures and the SRL ‘c’onstraints.",
        "For comparison, the ILP-1 and ILP-10 results from Table 2 are shown in rows 1 and 2.",
        "The F-measure score of ILP+SRL- f -10 is 68.9, about a 1 point increase from that of ILP-10, which shows that extra SRL features for the link classifier further improve the performance over our previous best results.",
        "18 ILP+SRL- f c-10 also performs better than ILP-10 in F-measure, although it is slightly worse than ILP+SRL- f -10.",
        "This indicates that the link classifier with extra SRL features already makes good use of the V-A0 frames from the SRL system, so that forcing the extraction of such frames via extra ILP constraints only hurts performance by not allowing the extraction of non-V-A0 pairs in the neighborhood that could have been better choices.",
        "Contribution of the ILP phase In order to highlight the contribution of the ILP phase for our task, we present ‘before’ and ‘after’ performance in Table 4.",
        "The first row shows the performance of the individual CRF-OP, CRF-SRC, and CRF-LINK classifiers before the ILP phase.",
        "Without the ILP phase, the 1-best sequence generates the best scores.",
        "However, we also present the performance with merged 10-best entity sequences19 in order to demonstrate that using 10-best sequences without ILP will only hurt performance.",
        "The precision of the merged 10-best sequences system is very low, however the recall level is above 95% for both 18Statistically significant by paired-t test, where p < 0.001.",
        "19If an entity Ei extracted by the ith-best sequence overlaps with an entity Ej extracted by the jth-best sequence, where i < j, then we discard Ej.",
        "If Ei and Ej do not overlap, then we extract both entities.",
        "CRF-OP and CRF-SRC, giving an upper bound for recall for our approach.",
        "The third row presents results after the ILP phase is applied for the 10- best sequences, and we see that, in addition to the improved link extraction described in Section 7, the performance on source extraction is substantially improved, from F-measure of 73.9 to 78.1.",
        "Performance on opinion expression extraction decreases from F-measure of 81.9 to 78.8.",
        "This decrease is largely due to implicit links, which we will explain below.",
        "The fourth row takes the union of the entities from ILP-SRL- f -10 and the entities from the best sequences from CRF-OP and CRF-SRC.",
        "This process brings the F-measure of CRF-OP up to 82.0, with a different precision-recall break down from those of 1-best sequences without ILP phase.",
        "In particular, the recall on opinion expressions now reaches 82.3%, while maintaining a high precision of 81.7%.",
        "Effects of ILP weight adjustment Finally, we show the effect of weight adjustment in the ILP formulation in Table 5.",
        "The DEV.CONF row shows relation extraction performance using a weight configuration based from the development data.",
        "In order to see the effect of weight adjustment, we ran an experiment, NO.CONF, using fixed default weights .20 Not surprisingly, our weight adjustment tuned from the development set is not the optimal choice for cross-validation set.",
        "Nevertheless, the weight adjustment helps to balance the precision and recall, i.e. it improves recall at the 20To be precise, c. = 1.0, �c.",
        "= 1.0 for x E {O, S, L}, but ca = 0.2 is the same as before.",
        "cost of precision.",
        "The weight adjustment is more effective when the gap between precision and recall is large, as was the case with the development data.",
        "Implicit links A good portion of errors stem from the implicit link relation, which our system did not model directly.",
        "An implicit link relation holds for an opinion entity without an associated source entity.",
        "In this case, the opinion entity is linked to an implicit source.",
        "Consider the following example.",
        "• Anti-Soviet hysteria was firmly oppressed.",
        "Notice that opinion expressions such as “Anti-Soviet hysteria” and “firmly oppressed” do not have associated source entities, because sources of these opinion expressions are not explicitly mentioned in the text.",
        "Because our system forces each opinion to be linked with an explicit source entity, opinion expressions that do not have explicit source entities will be dropped during the global inference phase of our system.",
        "Implicit links amount to 7% of the link relations in our corpus, so the upper bound for recall for our ILP system is 93%.",
        "In the future we will extend our system to handle implicit links as well.",
        "Note that we report results against a gold standard that includes implicit links.",
        "Excluding them from the gold standard, the performance of our final system ILP+SRL- f -10 is 72.6% in recall, 72.4% in precision, and 72.5 in F-measure."
      ]
    },
    {
      "heading": "9 Conclusion",
      "text": [
        "This paper presented a global inference approach to jointly extract entities and relations in the context of opinion oriented information extraction.",
        "The final system achieves performance levels that are potentially good enough for many practical NLP applications.",
        "Acknowledgments We thank the reviewers for their many helpful comments and Vasin Punyakanok for running our data through his SRL system.",
        "This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS-0535099 and IIS-0208028, and by gifts from Google and the Xerox Foundation."
      ]
    }
  ]
}
