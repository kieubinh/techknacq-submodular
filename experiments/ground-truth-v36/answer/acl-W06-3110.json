{
  "info": {
    "authors": [
      "Richard Zens",
      "Hermann Ney"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W06-3110",
    "title": "N-Gram Posterior Probabilities for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/W06-3110",
    "year": 2006
  },
  "references": [
    "acl-C04-1046",
    "acl-J90-2002",
    "acl-N04-1021",
    "acl-N04-1033",
    "acl-P02-1038",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-W03-0413",
    "acl-W99-0604"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Word posterior probabilities are a common approach for confidence estimation in automatic speech recognition and machine translation.",
        "We will generalize this idea and introduce n-gram posterior probabilities and show how these can be used to improve translation quality.",
        "Additionally, we will introduce a sentence length model based on posterior probabilities.",
        "We will show significant improvements on the Chinese-English NIST task.",
        "The absolute improvements of the BLEU score is between 1.1% and 1.6%."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The use of word posterior probabilities is a common approach for confidence estimation in automatic speech recognition, e.g. see (Wessel, 2002).",
        "This idea has been adopted to estimate confidences for machine translation, e.g. see (Blatz et al., 2003; Ueffing et al., 2003; Blatz et al., 2004).",
        "These confidence measures were used in the computer assisted translation (CAT) framework, e.g. (Gandrabur and Foster, 2003).",
        "The (simplified) idea is that the confidence measure is used to decide if the machine-generated prediction should be suggested to the human translator or not.",
        "There is only few work on how to improve machine translation performance using confidence measures.",
        "The only work, we are aware of, is (Blatz et al., 2003).",
        "The outcome was that the confidence measures did not result in improvements of the translation quality measured with the BLEU and NIST scores.",
        "Here, we focus on how the ideas and methods commonly used for confidence estimation can be adapted and/or extended to improve translation quality.",
        "So far, always word-level posterior probabilities were used.",
        "Here, we will generalize this idea to n-grams.",
        "In addition to the n-gram posterior probabilities, we introduce a sentence-length model based on posterior probabilities.",
        "The common phrase-based translation systems, such as (Och et al., 1999; Koehn, 2004), do not use an explicit sentence length model.",
        "Only the simple word penalty goes into that direction.",
        "It can be adjusted to prefer longer or shorter translations.",
        "Here, we will explicitly model the sentence length.",
        "The novel contributions of this work are to introduce n-gram posterior probabilities and sentence length posterior probabilities.",
        "Using these methods, we achieve significant improvements of translation quality.",
        "The remaining part of this paper is structured as follows: first, we will briefly describe the baseline system, which is a state-of-the-art phrase-based statistical machine translation system.",
        "Then, in Section 3, we will introduce the n-gram posterior probabilities.",
        "In Section 4, we will define the sentence length model.",
        "Afterwards, in Section 5, we will describe how these novel models can be used for rescoring/reranking.",
        "The experimental results will be presented in Section 6.",
        "Future applications will be described in Section 7.",
        "Finally, we will conclude in Section 8."
      ]
    },
    {
      "heading": "2 Baseline System",
      "text": [
        "In statistical machine translation, we are given a source language sentence f J1 = f1 ...f� ... fJ, which is to be translated into a target language sentence eI1 = e1 ... ei ... eI.",
        "Among all possible target language sentences, we will choose the sentence with the highest probability:",
        "The posterior probability Pr(eI1 I fJ1) is modeled directly using a log-linear combination of several models (Och and Ney, 2002):",
        "(2) The denominator is a normalization factor that depends only on the source sentence fJ1.",
        "Therefore, we can omit it during the search process.",
        "As a decision rule, we obtain:",
        "This approach is a generalization of the source-channel approach (Brown et al., 1990).",
        "It has the advantage that additional models h(•) can be easily integrated into the overall system.",
        "The model scaling factors AM1 are trained with respect to the final translation quality measured by an error criterion (Och, 2003).",
        "We use a state-of-the-art phrase-based translation system as described in (Zens and Ney, 2004; Zens et al., 2005).",
        "The baseline system includes the following models: an n-gram language model, a phrase translation model and a word-based lexicon model.",
        "The latter two models are used for both directions: p(f Ie) and p(eIf).",
        "Additionally, we use a word penalty and a phrase penalty."
      ]
    },
    {
      "heading": "3 N-Gram Posterior Probabilities",
      "text": [
        "The idea is similar to the word posterior probabilities: we sum the sentence posterior probabilities for each occurrence of an n-gram.",
        "Let S(•, •) denote the Kronecker function.",
        "Then, we define the fractional count C(en1, fJ1) of an n-gram en1 for a source sentence fJ1 as:",
        "(4) The sums over the target language sentences are limited to an N-best list, i.e. the N best translation candidates according to the baseline model.",
        "In this equation, the term S(e′Z+n-1, en1) is one if and only if the n-gram en 1 occurs in the target sentence e′I1 starting at position i.",
        "Then, the posterior probability of an n-gram is obtained as:",
        "Note that the widely used word posterior probability is obtained as a special case, namely if n is set to one."
      ]
    },
    {
      "heading": "4 Sentence Length Posterior Probability",
      "text": [
        "The common phrase-based translation systems, such as (Och et al., 1999; Koehn, 2004), do not use an explicit sentence length model.",
        "Only the simple word penalty goes into that direction.",
        "It can be adjusted to prefer longer or shorter translations.",
        "Here, we will use the posterior probability of a specific target sentence length I as length model:",
        "Note that the sum is carried out only over target sentences eI1 with the a specific length I.",
        "Again, the candidate target language sentences are limited to an N-best list."
      ]
    },
    {
      "heading": "5 Rescoring/Reranking",
      "text": [
        "A straightforward application of the posterior probabilities is to use them as additional features in a rescoring/reranking approach (Och et al., 2004).",
        "The use of N-best lists in machine translation has several advantages.",
        "It alleviates the effects of the huge search space which is represented in word",
        "graphs by using a compact excerpt of the N best hypotheses generated by the system.",
        "N-best lists are suitable for easily applying several rescoring techniques since the hypotheses are already fully generated.",
        "In comparison, word graph rescoring techniques need specialized tools which can traverse the graph accordingly.",
        "The n-gram posterior probabilities can be used similar to an n-gram language model:",
        "Note that the models do not require smoothing as long as they are applied to the same N-best list they are trained on.",
        "If the models are used for unseen sentences, smoothing is important to avoid zero probabilities.",
        "We use a linear interpolation with weights an and the smoothed (n – 1)-gram model as generalized distribution.",
        "Note that absolute discounting techniques that are often used in language modeling cannot be applied in a straightforward way, because here we have fractional counts.",
        "The usage of the sentence length posterior probability for rescoring is even simpler.",
        "The resulting feature is:",
        "Again, the model does not require smoothing as long as it is applied to the same N-best list it is trained on.",
        "If it is applied to other sentences, smoothing becomes important.",
        "We propose to smooth the sentence length model with a Poisson distribution.",
        "We use a linear interpolation with weight Q.",
        "The mean A of the Poisson distribution is chosen to be identical to the mean of the unsmoothed length model:"
      ]
    },
    {
      "heading": "6 Experimental Results",
      "text": []
    },
    {
      "heading": "6.1 Corpus Statistics",
      "text": [
        "The experiments were carried out on the large data track of the Chinese-English NIST task.",
        "The corpus statistics of the bilingual training corpus are shown in Table 1.",
        "The language model was trained on the English part of the bilingual training corpus and additional monolingual English data from the GigaWord corpus.",
        "The total amount of language model training data was about 600M running words.",
        "We use a fourgram language model with modified Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).",
        "To measure the translation quality, we use the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002).",
        "The BLEU score is the geometric mean of the n-gram precision in combination with a brevity penalty for too short sentences.",
        "The NIST score is the arithmetic mean of a weighted n-gram precision in combination with a brevity penalty for too short sentences.",
        "Both scores are computed case-sensitive with respect to four reference translations using the mteval-v11b tool1.",
        "As the BLEU and NIST scores measure accuracy higher scores are better.",
        "We use the BLEU score as primary criterion which is optimized on the development set using the Downhill Simplex algorithm (Press et al., 2002).",
        "As development set, we use the NIST 2002 evaluation set.",
        "Note that the baseline system is already well-tuned and would have obtained a high rank in the last NIST evaluation (NIST, 2005)."
      ]
    },
    {
      "heading": "6.2 Translation Results",
      "text": [
        "The translation results for the Chinese-English NIST task are presented in Table 2.",
        "We carried out experiments for evaluation sets of several years.",
        "For these rescoring experiments, we use the 10 000 best translation candidates, i.e. N-best lists of size N=10 000.",
        "Using the 1-gram posterior probabilities, i.e. the conventional word posterior probabilities, there is only a very small improvement, or no improvement at all.",
        "This is consistent with the findings of the JHU workshop on confidence estimation for statistical machine translation 2003 (Blatz et al., 2003), where the word-level confidence measures also did not help to improve the BLEU or NIST scores.",
        "Successively adding higher order n-gram posterior probabilities, the translation quality improves consistently across all evaluation sets.",
        "We also performed experiments with n-gram orders beyond four, but these did not result in further improvements.",
        "Adding the sentence length posterior probability feature is also helpful for all evaluation sets.",
        "For the development set, the overall improvement is 1.5% for the BLEU score.",
        "On the blind evaluation sets, the overall improvement of the translation quality ranges between 1.1% and 1.6% BLEU.",
        "Some translation examples are shown in Table 3."
      ]
    },
    {
      "heading": "7 Future Applications",
      "text": [
        "We have shown that the n-gram posterior probabilities are very useful in a rescoring/reranking framework.",
        "In addition, there are several other potential applications.",
        "In this section, we will describe two of them."
      ]
    },
    {
      "heading": "7.1 Iterative Search",
      "text": [
        "The n-gram posterior probability can be used for rescoring as described in Section 5.",
        "An alternative is to use them directly during the search.",
        "In this second search pass, we use the models from the first pass, i.e. the baseline system, and additionally the n-gram and sentence length posterior probabilities.",
        "As the n-gram posterior probabilities are basically a kind of sentence-specific language model, it is straightforward to integrate them.",
        "This process can also be iterated.",
        "Thus, using the N-best list of the second pass to recompute the n-gram and sentence length posterior probabilities and do a third search pass, etc.."
      ]
    },
    {
      "heading": "7.2 Computer Assisted Translation",
      "text": [
        "In the computer assisted translation (CAT) framework, the goal is to improve the productivity of human translators.",
        "The machine translation system takes not only the current source language sentence but also the already typed partial translation into account.",
        "Based on this information, the system suggest completions of the sentence.",
        "Word-level posterior probabilities have been used to select the most appropriate completion of the system, for more details see e.g. (Gandrabur and Foster, 2003; Ueffing and Ney, 2005).",
        "The n-gram based posterior probabilities as described in this work, might be better suited for this task as they explicitly model the dependency on the previous words, i.e. the given prefix."
      ]
    },
    {
      "heading": "8 Conclusions",
      "text": [
        "We introduced n-gram and sentence length posterior probabilities and demonstrated their usefulness for rescoring purposes.",
        "We performed systematic experiments on the Chinese-English NIST task and showed significant improvements of the translation quality.",
        "The improvements were consistent among several evaluation sets.",
        "An interesting property of the introduced methods is that they do not require additional knowledge sources.",
        "Thus the given knowledge sources are better exploited.",
        "Our intuition is that the posterior models prefer hypotheses with n-grams that are common in the N-best list.",
        "The achieved results are promising.",
        "Despite that, there are several ways to improve the approach.",
        "For the decision rule in Equation 3, the model scaling factors aM can be multiplied with a constant factor without changing the result.",
        "This global factor would affect the proposed posterior probabilities.",
        "So far, we have not tuned this parameter, but a proper adjustment might result in further improvements.",
        "Currently, the posterior probabilities are computed on an N-best list.",
        "Using word graphs instead should result in more reliable estimates, as the number of hypotheses in a word graph is some orders of a magnitude larger than in an N-best list."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
        "HR0011-06-C-0023, and was partly funded by the European Union under the integrated project TC-STAR (Technology and Corpora for Speech to Speech Translation, IST-2002-FP6-506738, http://www.tc-star.org)."
      ]
    }
  ]
}
