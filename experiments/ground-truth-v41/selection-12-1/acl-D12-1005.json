{
  "info": {
    "authors": [
      "Benjamin Van Durme"
    ],
    "book": "EMNLP",
    "id": "acl-D12-1005",
    "title": "Streaming Analysis of Discourse Participants",
    "url": "https://aclweb.org/anthology/D12-1005",
    "year": 2012
  },
  "references": [
    "acl-D07-1049",
    "acl-D09-1079",
    "acl-D11-1023",
    "acl-D11-1120",
    "acl-N09-1058",
    "acl-N10-1021",
    "acl-P05-1054",
    "acl-P05-1077",
    "acl-P07-1065",
    "acl-P08-1058",
    "acl-P08-1077",
    "acl-P09-1080",
    "acl-P10-2043",
    "acl-P11-1032",
    "acl-P11-2004"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Inferring attributes of discourse participants has been treated as a batch-processing task: data such as all tweets from a given author are gathered in bulk, processed, analyzed for a particular feature, then reported as a result of academic interest.",
        "Given the sources and scale of material used in these efforts, along with potential use cases of such analytic tools, discourse analysis should be reconsidered as a streaming challenge.",
        "We show that under certain common formulations, the batch-processing analytic framework can be decomposed into a sequential series of updates, using as an example the task of gender classification.",
        "Once in a streaming framework, and motivated by large data sets generated by social media services, we present novel results in approximate counting, showing its applicability to space efficient streaming classification."
      ]
    },
    {
      "text": [
        "Twitter deal with a very large number of individuals, each with a variety of implicit attributes (such as gender).",
        "This motivates a desire for online space efficiency.",
        "explicitly amenable to keeping an online average."
      ]
    },
    {
      "heading": "3.1 Reservoir Counting",
      "text": [
        "Reservoir Counting plays on the folklore algorithm of reservoir sampling, first described in the literature by Vitter (1985).",
        "As applied to a stream of arbitrary elements, reservoir sampling maintains a list (reservoir) of length k, where the contents of the reservoir represents a uniform random sample over all elements 1...t observed thus far in the stream.",
        "When the stream is a sequence of positive and negative integers, reservoir counting implicitly views each value as being unrolled into a sequence made up of either 1 or -1.",
        "For instance, the sequence: (3, -2, 1) would be viewed as: (1, 1, 1, -1, -1, 1) Since there are only two distinct values in this stream, the contents of the reservoir can be characterized by knowing the fixed value k, and then s: how many elements in the reservoir are 1.4 This led to Van Durme and Lall defining a method, ReservoirUpdate, here abbreviated to ResUp, that allows for maintaining an approximate sum, defined as t(2sk ?",
        "1), through updating these two parameters t and s with each newly observed element.",
        "Expected accuracy of the approximation varies with the size of the sample, k. Reservoir Counting exploits the fact that the reservoir need only be considered implicitly, where s represented as a b-bit unsigned integer can be used to characterize a reservoir of size k = 2b ?",
        "1.",
        "This allowed those authors to show a 50% space reduction in the task of online",
        "Locality Sensitive Hashing, at similar levels of accuracy, by replacing explicit 32-bit counting variables with approximate counters of smaller size.",
        "See (Van Durme and Lall, 2011) for further details."
      ]
    },
    {
      "heading": "3.2 Reservoir Averaging",
      "text": [
        "For a given integer x, let m = |x |be the magnitude of x, and ?",
        "= sign(x).",
        "For a given sequence, let m?",
        "be the largest such value of m. Modifying the earlier implicit construction, consider the sequence (3, -2, 1), with m?",
        "= 3, mapped to the sequence: (1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1) where each value x is replaced with m?",
        "+ m elements of ?, andm?",
        "?m elements of??.",
        "This views x as a sequence of length 2m?, made up of 1s and -1s, where each x in the discrete range [?m?,m?]",
        "has a unique number of 1s.",
        "Now recognize that the average over the original sequence, here 3?2+13 =",
        "Generally for a sequence (x1, ..., xn), with m?",
        "as defined, the average times 1m?",
        "is equal to:",
        "where n2m?",
        "is the total number of 1s and -1s observed in the implicit stream, up to and including the mapping of element xn.",
        "If applying Reservoir Counting, s would then record the sampled number of 1s, as per norm, where t maintained as the implicit stream length can also be viewed as storing t = n2m?.",
        "At any point in the stream, the average over the original value sequence can then be approximated as: (1) the approximate sum of the implicit stream; divided by (2) the implicit stream length; times (3) m?",
        "to cancel the 1m?",
        "term:",
        "Granularity As defined this scheme operates on streams of integers.",
        "We extend the definition to work with a stream of fixed precision floating point variables.",
        "Let g be a positive integer that we refer to as the granularity.",
        "Modify the mapping of value x from a sequence of length 2m?, to a sequence of length g, comprised of m?+m2m?",
        "g instances of ?, and",
        "2m?",
        ")g instances of -?.",
        "As seen in line 4 of Algorithm 1, a random coin flip determines placement of the remainder.",
        "For example, the value 1.3, with m?",
        "= 3, and g = 10, would now be represented as a sequence of 3+1.36 g = 7.16 ?",
        "(7, 8) instances of 1, followed by however many instances of 1 that lead to a sequence of length g, after probabilistic rounding.",
        "The possible sequences are thus: (1, 1, 1, 1, 1, 1, 1, -1, -1, -1) (1, 1, 1, 1, 1, 1, 1, 1, -1, -1) with the former more likely.",
        "At this point we have described the framework captured by Algorithm 1, where Van Durme and Lall (2011) defined ResUp.",
        "Algorithm 1 UPDATEAVERAGE(n, k,m,m?, ?, g, s) Parameters:",
        "n : size of stream k : size of reservoir, also maximum value of s m : magnitude of update m?",
        ": maximum magnitude of all updates ?",
        ": sign of update g : granularity s : current value of reservoir 1: if m = 0 or ?",
        "= 0 then 2: Return without doing anything 3: v := m+m?2m?",
        "g 4: v := dve with probability v ?",
        "bvc, bvc otherwise 5: s?",
        ":= ResUp(ng, k, v, ?, s) 6: s?",
        ":= ResUp((ng + v), k, g ?",
        "v,?",
        "?, s?)",
        "7: Return s?",
        "Log-scale Counting For additional space savings",
        "we might approximate the length parameter t with a small bit representation, using the approximate counting scheme of Morris (1978).",
        "The method enables counting in log-scale by probabilistically incrementing a counter, where it becomes less and less likely to update the counter after each increment.",
        "This scheme is popularly known and used in a variety of contexts, recently in the community by Talbot (2009) and Van Durme and Lall (2009)",
        "quences, with m?",
        "= 100, g = 100, and using an 8 bit Morris-style counter of base 2.",
        "Larger reservoir sizes lead to better approximation, at higher cost in bits.",
        "to provide a streaming extension to the Bloom-filter based count-storage mechanism of Talbot and Osborne (2007a) and Talbot and Osborne (2007b).",
        "See (Flajolet, 1985) for a detailed analysis of Morris-style counting."
      ]
    },
    {
      "heading": "3.3 Experiment",
      "text": [
        "We show through experimentation on synthetic data that this approach gives reasonable levels of accuracy at space efficient sizes of the length and sum parameter.",
        "Random sequences of 1,000 values were generated by: (1) fix a value for m?",
        "; (2) draw a polarity bias term ?",
        "uniformly from the range [0,1]; then (3) for each value, x: (a) ?",
        "was positive with probability ?",
        "; (b) m was drawn from [0, m?].",
        "Figure 5 shows results for varying reservoir sizes (using 4, 8 or 12 bits) when g = 100, m?",
        "= 100, and the length parameter was represented with an 8 bit Morris-style counter of base 2."
      ]
    },
    {
      "heading": "3.4 Justification",
      "text": [
        "Before we close this section, one might ask why this extension is needed in the first place.",
        "As Reservoir Counting already allows for keeping an online sum, and pairs it with a length parameter, then this would presumably be what is needed to get the average we are focussed on.",
        "Unfortunately that is not the case: the parameter recording the current stream length, here called t, tracks the length of the implicit stream of 1s and -1s, it does not track the length of the original stream of values that gave rise to the mapped version.",
        "As an example, consider again the sequence: (3, -2, 1), as compared to: (2,1,-1,-1,1).",
        "Both have the same sum, and would therefore be viewed the same under the pre-existing Reservoir Counting algorithm, giving rise to implicit streams of the same length.",
        "But critically the sequences have different averages: 23 6=",
        "5 , which we cannot detect based on the original counting algorithm.",
        "Finally, we restate the constraint: for the sequence to averaged, one must know m?",
        "ahead of time."
      ]
    },
    {
      "heading": "4 Application to Classification",
      "text": [
        "Going back to our streaming analysis model, we have a situation that can be viewed as a sequence of values, such that we do know m?.",
        "First reinterpret the fraction stzt equivalently as the normalized sum of a stream of elements sampled from w:",
        "The value m?",
        "is then: m?",
        "= maxj |wj |, over a sequence of length zt.",
        "Rather than updating stand zt through basic addition, we can now use a smaller bitwise representation for each variable, and update via Reservoir Averaging."
      ]
    },
    {
      "heading": "4.1 Problems in Practice",
      "text": [
        "Reconsidering the earlier classification experiment, we found this approximation method led to terrible results: while our experiments on synthetic data worked well, those sequences were sampled somewhat uniformly over the range of possible values.",
        "As seen in Figure 6, sequences arising from observed feature weights in a practical setting may not be so broadly distributed.",
        "In brief: the more the maximum possible update, m?, can be viewed as an outlier, then the more the resulting implicit encoding of g elements per observed weight becomes dominated by ?filler?.",
        "As few observed elements will in that case require the provided range, then the implicit representation will be a mostly balanced set of",
        "served over a full set of communications by a single example speaker.",
        "Most observed features have relatively small magnitude weight.",
        "The mean value is 1.3, with",
        "speaker as MALE.",
        "1 and 1 values.",
        "These mostly balanced encodings make it difficult to maintain an adequate approximation of the true average, when reliant on a small, implicit uniform sample.",
        "Here we leave further analysis aside, focusing instead on a modified solution for the classification model under consideration."
      ]
    },
    {
      "heading": "4.2 Rewriting History",
      "text": [
        "Practically we would like to restrict our range to the dense region of weight updates, while at the same time not throwing away or truncating larger weights that appear outside a reduced window.",
        "We do this by fitting a replacement to m?",
        ": m?",
        "?",
        "m?, based on the classifier's training data, such that too-large elements will be accommodated into the stream by implicitly assuming that the portion of a value that falls outside the restricted window is ?spread out?",
        "over the previously observed values.",
        "That is, we modify the contents of the implicit reservoir by rewriting history: pretending that earlier elements were larger than they were, but still within the reduced window.",
        "As long as we don't see too many values that are overly large, then there will be room to accommodate the overflow without any theoretical damage to the implicit stream: all count mass may still be accounted for.",
        "If a moderately high number of overly large elements are observed, then we expect in practice for this to have a negligible impact on downstream performance.",
        "If an exceptional number of elements are overly large, then the training data was not representative of the test set.",
        "The newly introduced parameter m?",
        "is used in MODIFIEDUPDATEAVERAGE (MUA), which relies on REWRITEHISTORY.",
        "Note that MUA uses the same value of n when calling REWRITEHISTORY as it does in the subsequent line calling UPDATEAVERAGE: we modify the state of the reservoir without incrementing the stream length, taking the current overflow and pretending we saw it earlier, spread out across previous elements.",
        "This happens by first estimating the number of 1 values seen thus far in the stream: skn, then adding in twice the overflow value, which represents removing o instances of ??",
        "from the stream, and then adding o instances of ?.",
        "We probabilistically round the resultant fraction to achieve a modified version of s, which is returned.",
        "Algorithm 2 MUA(n, k,m,m?, ?, g, s)",
        "1: if m < m?",
        "then 2: Return UPDATEAVERAGE(n, k,m,m?, ?, g, s) 3: s?",
        ":= REWRITEHISTORY(n, k,m,m?, ?, g, s) 4: Return UPDATEAVERAGE(n, k,m?,m?, ?, g, s?)",
        "Algorithm 3 REWRITEHISTORY(n, k,m,m?, ?, g, s) Parameters: o : overflow to be accommodated 1: o := m?m?2m?",
        "g 2: if ?",
        "> 0 then 3: if s = k then 4: Return s 5: p := min(1.0, sk + 2on ) 6: else 7: if s = 0 then 8: Return s 9: p := max(0.0, sk ?",
        "2on ) 10: Return dpke with prob.",
        "pk ?",
        "bpkc, bpkc otherwise"
      ]
    },
    {
      "heading": "4.3 Experiment",
      "text": [
        "a version of the experiment when using approximation.",
        "Parameters were: g = 100; k = 255; and a Morris-style counter for stream length using 8 bits",
        "bands reflecting 95% confidence.",
        "and a base of 1.3.",
        "The value m?",
        "was fit independently for each split of 10-fold cross validation, by finding through simple line search that which minimized the number of prediction errors on the original training data (see Figure 8).",
        "This result shows our ability to replace 2 variables of 32 bits (sum and length) with 2 approximation variables of 8 bits (reservoir status s, and stream length n), leading to a 75% reduction in the cost of maintaining online classifier state, with no significant cost in accuracy."
      ]
    },
    {
      "heading": "5 Real World Stream: Twitter",
      "text": []
    },
    {
      "heading": "5.1 Setup",
      "text": [
        "Based on the tweet IDs from the data used by Burger et al2011), we recovered 2,958,107 of their roughly 4 million original tweets.",
        "These tweets were then matched against the gender labels established in that prior work.",
        "As reported by Burger et althe dominant language in the collection is English (66.7% reported), followed by Portuguese (14.4%) then Spanish (6.0%), with a large variety of other languages with small numbers of examples.",
        "5Standard practice in Twitter data exchanges is to share only the unique tweet identifications and then requery the content from Twitter, thus allowing, e.g., the individual authors the ability to delete previous posts and have that reflected in future data collects.",
        "While respectful of author privacy, it does pose a challenge for scientific reproducibility.",
        "speaker in the Switchboard training set, across 10 splits.",
        "A value of m?",
        "= 5 was on average that which minimized the number of mistakes made.",
        "Content was lowercased, then processed by regular expression to collapse the following into respective single symbols: emoticons; URLs; usernames (@mentions); and hashtags.",
        "Any content deemed to be a retweet (following the characters RT) was removed.",
        "Text was then tokenized according to a modified version of the Penn TreeBank tokenization standard6 that was less English-centric."
      ]
    },
    {
      "heading": "5.2 Experiment",
      "text": [
        "A log-linear classifier was built using all those authors in the training set7 with at least 10 tweets.",
        "Similar to the previous experiment, unigrams and bigrams features were used exclusively, with the parameter m?",
        "fit on the training data.",
        "As seen in Figure 9, results were as in Switchboard: accuracy improves as more data streams in per author, and our approximate model sacrifices perhaps a point of accuracy in return for a 75% reduction in memory requirements per author.",
        "Table 2 gives the top features per gender.",
        "We see similarities to Switchboard in terms such as my",
        "and approximation, on the Twitter dataset, with bands reflecting 95% confidence.",
        "wife, along with terms suggesting a more youthful population.",
        "Foreign terms are recognized by their parenthetical translation and 1st- or 2nd-person + Male/Female gender marking.",
        "For example, the Portuguese obrigado can be taken to be literally saying: I'm obliged (thank you), and I'm male."
      ]
    },
    {
      "heading": "6 Related Work",
      "text": [
        "Streaming algorithms have been developed within the applied communities of networking, and (very large) databases, as well as being a popular topic in the theoretical computer science literature.",
        "A sum",
        "bro, cansado (tired [1M]), gay, mate, dude, [@username] why, buddy, windows, album, dope, beer, [@username] yo, sir, ps3, comics, galera (folks/people), amigo (friend [2M]), man !, fuckin, omg omg, cheers, ai n't Female obrigada (thank you [1F]), hubby, husband, cute, my husband, ?, cansada (tired [1F]), hair, dress, soooo, lovely, etsy, boyfriend, jonas, loved, book, sooo, girl, se?",
        "(I), lindo (cute), shopping, amiga (friend [2F]), yummy, ppl, cupcakes mary of the streaming algorithms community is beyond the scope of this work: interested readers are directed to Muthukrishnan (2005) as a starting point.",
        "Within computational linguistics interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections.",
        "Leven-berg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode.",
        "Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al2005; Bhagat and Ravichandran, 2008).",
        "Finally, Goyal et al2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling.",
        "For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (non-streaming) methodology."
      ]
    },
    {
      "heading": "7 Conclusions and Future Work",
      "text": [
        "We have taken the predominately batch-oriented process of analyzing communication data and shown it to be fertile territory for research in large-scale streaming algorithms.",
        "Using the example task of automatic gender detection, on both spoken transcripts and microblogs, we showed that classification can be thought of as a continuously running process, becoming more robust as further communications become available.",
        "Once positioned within a streaming framework, we presented a novel approximation technique for compressing the streaming memory requirements of the classifier (per author) by 75%.",
        "There are a number of avenues to explore based on this framework.",
        "For instance, while here we assumed a static, pre-built classifier which was then applied to streaming data, future work may consider the interplay with online learning, based on methods such as by Crammer et al2006).",
        "In the appli8See the survey by Cormode and Hadjieleftheriou (2009) for approaches to the frequent items problem, otherwise known as finding heavy hitters.",
        "cations arena, one might take the savings provided here to run multiple models in parallel, either for more robust predictions (perhaps ?triangulating?",
        "on language ID and/or domain over the stream), or predicting additional properties, such as age, nationality, political orientation, and so forth.",
        "Finally, we assumed here strictly count-based features; streaming log-counting methods, tailored Bloom-filters for binary feature storage, and other related topics are assuredly applicable, and should give rise to many interesting new results.",
        "Acknowledgments I thank the reviewers and my colleagues at Johns Hopkins University for helpful feedback, in particular Matt Post, Mark Dredze,"
      ]
    }
  ]
}
