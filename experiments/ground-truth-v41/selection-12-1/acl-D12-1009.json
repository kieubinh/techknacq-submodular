{
  "info": {
    "authors": [
      "Michael J. Paul"
    ],
    "book": "EMNLP",
    "id": "acl-D12-1009",
    "title": "Mixed Membership Markov Models for Unsupervised Conversation Modeling",
    "url": "https://aclweb.org/anthology/D12-1009",
    "year": 2012
  },
  "references": [
    "acl-D11-1002",
    "acl-D11-1069",
    "acl-J00-3003",
    "acl-N04-1015",
    "acl-N09-1042",
    "acl-N10-1020",
    "acl-P05-1045",
    "acl-P06-1026",
    "acl-P07-1094",
    "acl-P11-1153",
    "acl-P95-1005",
    "acl-W04-3240",
    "acl-W10-2923"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Recent work has explored the use of hidden Markov models for unsupervised discourse and conversation modeling, where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence.",
        "We extend this approach to allow each block of text to be a mixture of multiple classes.",
        "Under our model, the probability of a class in a text block is a log-linear function of the classes in the previous block.",
        "We show that this model performs well at predictive tasks on two conversation data sets, improving thread reconstruction accuracy by up to 15 percentage points over a standard HMM.",
        "Additionally, we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The proliferation of social media in recent years has lead to an increased use of informal Web data in the language processing community.",
        "With this rising interest in social domains, it is natural to consider models which explicitly incorporate the conversational patterns of social text.",
        "Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search (Elsas and Carbonell, 2009; Seo et al. 2009), question answering and expert finding (Xu et al., 2008; Wang et al2011a), and interpersonal relationship identification (Diehl et al2007).",
        "While conversational features may be important, Web-derived corpora are not always annotated with this information, and the nature of conversations on the Web can vary wildly across domains and venues.",
        "Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models (Ritter et al. 2010), where each state corresponds to a conversational class or ?act.?",
        "Unlike more traditional uses of HMMs in which a single token is emitted per time step, HMM emissions in conversations correspond to entire blocks of text, such that an entire message is generated at each step.",
        "Because each time step is associated with a block of variables, we refer to this type of HMM as a block HMM (Fig.",
        "1a).",
        "While block HMMs offer a concise model of inter-message structure, they have the limitation that each text block (message) belongs to exactly one class.",
        "Many modern generative models of text, in contrast, allow documents to contain many latent classes.",
        "For example, topic models such as Latent Dirichlet Allocation (LDA) (Blei et al2003) assume each document has its own distribution over multiple classes (often called ?topics?).",
        "For many predictive tasks, topic models outperform single-class generative models such as Naive Bayes.",
        "These properties could similarly be desirable in conversation modeling.",
        "An email might contain a request, a question, and an answer to a previous question ?",
        "three distinct dialog acts within a single message.",
        "This motivates the desire to allow a message to be a mixture of classes.",
        "In this paper, we introduce a new type of model which combines the functionality of topic models, which posit latent class assignments to each individual token, with Markovian sequence models, which",
        "butions are dependent across blocks.",
        "Some parameters are omitted for simplicity.",
        "This figure depicts the Bayesian variant of the block HMM (Ritter et al2010) where the transition distributions pi depend on a Dirichlet(?)",
        "prior.",
        "govern the transitions between text blocks in a sequence.",
        "We generalize the block HMM approach so that there is no longer a one-to-one correspondence between states in the Markov chain and latent discourse classes.",
        "Instead, we allow a state in the HMM to correspond to a mixture of many classes: we refer to this family of models as mixed membership Markov models (M4).",
        "Instead of defining explicit transition probabilities from one class to another as in a traditional HMM, we define the distribution over classes as a function of the entire histogram of class assignments of the previous text segment.",
        "We define our model using the same number of parameters as a standard HMM (?2), and we present a straightforward approximate inference algorithm (?3).",
        "While we introduce a general model, we will focus on the task of unsupervised conversation modeling.",
        "Specifically, we build off the Bayesian block HMMs used by Ritter et al2010) for modeling Twitter conversations, which will be our primary baseline.",
        "After discussing related work (?4), we present experimental results on a set of Twitter conversations as well as a set of threads from CNET discussion forums (?5).",
        "We show that M4 increases thread reconstruction accuracy by up to 15% compared to the HMM of Ritter et al2010), and we reduce variation of information against speech act annotations by an average of 18% from HMM and LDA baselines.",
        "To the best of our knowledge, this work is the first attempt to quantitatively compare unsupervised models against gold standard speech act annotations."
      ]
    },
    {
      "heading": "2 M4: Mixed Membership Markov Models",
      "text": [
        "In this section, we extend the block HMM by introducing mixed membership Markov models (M4).",
        "Under the block HMM, as utilized by Ritter et al. (2010), messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models associated with a state in a hidden Markov model.",
        "The intuition is that HMM states should correspond to some notion of a conversation ?act?",
        "such as QUESTION or ANSWER.",
        "The intuition is the same under M4, but now each token in a message is given its own class assignment, according to a class distribution for that particular message.",
        "A message's class distribution depends on the class assignments of the previous message, yielding a model that retains sequential dependencies between messages, while allowing for finer grained class allocation than the block HMM.",
        "Modeling messages (or more generally, text blocks) as a mixture of multiple classes rather than a single class gives rise to the ?mixed membership?",
        "property.",
        "In the subsections below, we formalize and ana-lyze this new model."
      ]
    },
    {
      "heading": "2.1 Structure Assumptions",
      "text": [
        "We first define the discourse structure and terminology we will be assuming.",
        "The discourse structure is a directed graph, where nodes correspond to segments of a document (which we will refer to as ?blocks?",
        "of text), and the edges define the dependencies between them.",
        "Thus, a text block is a set of tokens, while a document consists of the discourse graph and all blocks associated with it.",
        "In the context of modeling conversation threads, which will be the focus of our experiments later, we will assume a block corresponds to a single message in a thread.",
        "The parent of a message m is the message to which it is a response; if a message is not in response to anything in particular, then it has no parent.",
        "Any replies to the message m are the children of m. The thread as a whole is called a document.",
        "The discourse graph should be acyclic.",
        "A directed acyclic graph (DAG) offers a flexible representation of discourse (Rose?",
        "et al1995), but for simplicity, we will restrict this and assume that each sub-graph is a tree; i.e. no message has multiple parents.",
        "The graph as a whole may be a forest: for example, someone could write a new message in a conversation that is not directly in reply to any previous message, so this message would not have any parents, and would form the root of a new tree in the forest."
      ]
    },
    {
      "heading": "2.2 Generative Story",
      "text": [
        "Extending the block HMM, latent classes in M4 are now associated with each individual token, rather than one class for an entire block.",
        "The key difference between the generative process behind M4 and the block HMM is that the transition distributions are defined with a log-linear model, which uses class assignments in a block as features to define the distribution over classes for the children of that block.",
        "Put another way, a state in M4 corresponds to a class histogram, and transitions between states are functions of the log-linear parameters.",
        "Given a block b, we will use the notation b to denote the block's feature vector, which consists of the histogram of latent class assignments for the tokens of b.",
        "There are K classes.",
        "Additionally, we assume each feature vector has an extra cell containing an indicator denoting whether the block has no parent ?",
        "this allows us to learn transitions from a ?start?",
        "state.",
        "We also include a bias feature that is always 1, to learn a default weight for each class.",
        "There",
        "rather than the raw counts themselves.",
        "For example, we experimented with binary indicator features (i.e. ?does class k appear anywhere in block b??",
        "), but this performed consistently worse in early experiments, and we do not consider this further.",
        "are thus K + 2 features which are used to predict the probability of each of the K classes.",
        "The features are weighted by transition parameters, denoted ?.",
        "The random variable z denotes a latent class, and ?z is a discrete distribution over word types ?",
        "that is, each class is associated with a unigram language model.",
        "The transition distribution over classes is denoted pi, which is given in terms of ?",
        "and the feature vector of the parent block.",
        "Under this model, a corpus D is generated by:",
        "1.",
        "For each (j, k) in the transition matrix ?K?K+2: (a) Draw transition weight ?jk ?",
        "N (0, ?2).",
        "2.",
        "For each class j: (a) Draw word distribution ?j ?Dirichlet(?).",
        "3.",
        "For each block b of each document d in D: (a) Set class probability pibj = exp(?Tj a)?",
        "j?",
        "exp(?",
        "for all classes j, where a is the feature vector for block a, the parent of b.",
        "(b) For each token n in block b: i.",
        "Sample class z(b,n) ?",
        "pib.",
        "ii.",
        "Sample word w(b,n) ?",
        "?z .",
        "For each block of text in a document (e.g. each message in a conversation), the distribution over classes pi is computed as a function of the feature vector of the block's parent and the transition parameters (feature weights) ?.",
        "Each ?jk has an intuitive interpretation: a positive value means that the occurrence of class k in a parent block increases the probability that j will appear in the next block, while a negative value reduces this probability.",
        "The observed words of each block are generated by repeatedly sampling classes from the block's distribution pi, and for each sampled class z, a single word is sampled from the class-specific distribution over words ?z .",
        "In contrast, under the block HMM, a class z is sampled once from the transition distribution, and words are repeatedly sampled from ?z .",
        "We place a symmetric Dirichlet prior on each ?",
        "with concentration parameter ?, which smoothes the word distributions, and we place a 0-mean Gaussian prior on each ?",
        "parameter, which acts as a regular-izer.",
        "The graphical diagram is shown in Figure 1 along with the block HMM and LDA.",
        "This figure",
        "shows how M4 combines the sequential dependencies of the block HMM with the token-specific class assignments of LDA."
      ]
    },
    {
      "heading": "2.3 Discussion",
      "text": [
        "Like the block HMM, M4 is a type of HMM.",
        "A latent sequence under M4 forms a Markov chain in which a state corresponds to a histogram of classes.",
        "(For simplicity, we are ignoring the extra features of the start state indicator and bias in this discussion.)",
        "If we assume a priori that the length of a block is unbounded, then this state space is NK where 0 ?",
        "N. The probability of transitioning from a state b to another state b?",
        "?",
        "NK is:",
        "k b?k, ?N is the probability that a block has N tokens,2 and pi(b) is the transition distribution given a vector b.",
        "This follows from the generative story defined above, with an additional step of generating the number of tokens N from the distribution ?.",
        "We currently define a block b's distribution pib in terms of the discrete feature vector a given by its parent a.",
        "We could have instead made pib a function of the parent's distribution pia ?",
        "this would lead to a model that assumes a dynamical system over a continuous space rather than a Markov chain.",
        "However, as a generative story we believe it makes more sense for a block's distribution to depend on the actual class values which are emitted by the parent.",
        "Similar arguments are made by Blei and Mcauliffe (2007) when designing supervised topic models.",
        "Under a block HMM with one class per block, there are K states corresponding to the K classes, requiring K?K parameters to define the transition matrix.",
        "Under M4, there is a countably infinite number of states, but the transitions are still defined by K?K parameters (ignoring extra features).",
        "M4 thus utilizes a larger state space without increasing the number of free parameters."
      ]
    },
    {
      "heading": "3 Inference and Parameter Estimation",
      "text": [
        "We must infer the values of the hidden variables z as well as the parameters for the word distributions 2The distribution over the number of tokens can be arbitrary, as this is observed and does not affect inference.",
        "In topic models, this is sometimes assumed to be Poisson (Blei et al2003).",
        "?",
        "and transition weights ?.",
        "Standard HMM dynamic programming algorithms cannot straightforwardly be used for M4 because of the unboundedly large state space.",
        "We instead turn to Markov chain Monte Carlo (MCMC) methods as a tool for approximate inference.",
        "We derive a stochastic EM algorithm in which we alternate between sampling class assignments for the word tokens and optimizing the transition parameters, outlined in the following two subsections."
      ]
    },
    {
      "heading": "3.1 Latent Class Sampling",
      "text": [
        "To explore the posterior distribution over latent classes, we use a collapsed Gibbs sampler such that we marginalize out each word multinomial ?",
        "and only need to sample the token assignments z conditioned on each other.",
        "Given the current state of the sampler, we sample a token's class according to:",
        "The notation nwk indicates the number of tokens with word type w that have been assigned to topic k. W is the vocabulary size.",
        "a is the parent block of b, and C is the set of b's children.",
        "b is the feature vector corresponding to block b (i.e. the class histogram plus the bias feature), where the histogram includes the incremented count of the candidate class k. This sampling distribution is very similar to that of LDA (Griffiths and Steyvers, 2004), but the distribution over ?topics?",
        "is now a function of the previous block, which gives the leftmost term.",
        "The rightmost term is a result of the dependency of the child blocks (C) on the class assignments of b.",
        "Due to the rightmost term, the complexity of computing the sampling distribution is quadratic in the number of classes, rather than the linear complexity of a single-class HMM.",
        "Our assumption is that the number of sequence-dependent classes (e.g. speech acts or discourse states) will be reasonably small.",
        "If it is desired to have a large number of latent topics as is common in LDA, this model could be combined with a standard topic model without sequential dependencies, as explored by Ritter et al2010)."
      ]
    },
    {
      "heading": "3.2 Transition Parameter Optimization",
      "text": [
        "Differentiating the corpus likelihood with respect to ?",
        "yields the standard equation for log-linear models:",
        "where a is the parent of block b, a is the feature vector associated with a, nzb is the number of times class z occurs in block b and nb is the total number of tokens in block b.",
        "Standard optimization methods can be used to learn these parameters.",
        "In our experiments, we find that we obtain good results by simply performing a single iteration of gradient ascent after each sampling iteration t,3 with the following update:",
        "where ?",
        "is a step size function."
      ]
    },
    {
      "heading": "4 Related Work",
      "text": [
        "Hidden Markov models have a recent history as simple models of document structure.",
        "Stolcke et al. (2000) used HMMs as a general model of discourse with an application to speech acts (or dialog acts) in conversations.",
        "Barzilay and Lee (2004) applied HMMs as an unsupervised model of discourse.",
        "This work used HMMs to model the progression of sentences in articles, and was shown to be useful for ordering sentences and generating summaries of news articles.",
        "More recently, Wang et al2011b) experimented with similar tasks using a related HMM-based model called the Structural Topic Model.",
        "Unsupervised HMMs were applied to conversational data by Ritter et al2010) who experimented with Twitter conversations.",
        "The authors also experimented with incorporating a topic model on top of the HMM to distinguish speech acts from topical clusters, with mixed results.",
        "Joty et al2011) extended this work by enriching the emission distributions and using additional features such as speaker and position information.",
        "An approach to unsupervised discourse modeling that does not use HMMs is 3Incremental updates are justified under the generalized EM algorithm (Dempster et al1977).",
        "Each gradient step with respect to ?",
        "corresponds to a generalized M-step, while each sampling iteration corresponds to a stochastic E-step.",
        "the latent permutation model of Chen et al2009).",
        "This model assumes each segment (e.g. paragraph) in a document is associated with a latent class or topic, and the ordering of topics within a document is modeled as a deviation from some canonical ordering.",
        "Extensions to the block HMM have incorporated mixed membership properties within blocks, notably the Markov Clustering Topic Model (Hospedales et al2009), which allows each HMM state to be associated with its own distribution over topics in a topic model.",
        "Like the block HMM, this still assumes a relatively small number of HMM states, but with an extra layer of latent variables before the observations are emitted.",
        "This is more restrictive than the unbounded state space of M4.",
        "Decoupling HMM states from latent classes was considered by Beal et al1997) with the Factorial HMM, which uses factorized state representations.",
        "The Factorial HMM is most often used to model independent Markov chains, whereas M4 has a dense graphical model topology: the probability of each of the latent classes depends on the counts of all of the classes in the previous block.",
        "The trick in M4 is to define the transition matrix via a function of a limited number of parameters, allowing tractable inference in a model with arbitrarily many states.",
        "In topic models, log-linear formulations of latent class distributions4 are utilized in correlated topic models (Blei and Lafferty, 2007) as a means of incorporating covariance structure among topic probabilities.",
        "Applying log-linear regression to potentially many features was combined with LDA by Mimno and McCallum (2008), who model the Dirichlet prior over topics as a function of document features.",
        "In M4, such features would correspond to the class histograms of previous blocks, introducing additional dependencies between documents.",
        "One topic model that imposes sequential dependencies between documents is Sequential LDA (Du et al2010), which models a document as a sequence of segments (such as paragraphs) governed by a Pitman-Yor process, in which the latent topic distribution of one segment serves as the base distribution for the next segment.",
        "This is in the spirit",
        "of our work, where the latent classes in a segment depend on the class distribution of the previous segment.",
        "By using the Pitman-Yor process, however, this work assumes topics are positively correlated, i.e. the occurrence of a topic in one segment makes it likely to appear in the next.",
        "In contrast, we wish to learn arbitrary transitions, both positive and negative, between the latent classes."
      ]
    },
    {
      "heading": "5 Experiments with Conversation Data",
      "text": [
        "We experiment with two corpora of text-based asynchronous conversations on the Web.",
        "One of these is annotated with speech act labels, against which we compare our unsupervised clusters.",
        "We measure the predictive capabilities of the model via perplexity experiments and the task of thread reconstruction."
      ]
    },
    {
      "heading": "5.1 Data Sets",
      "text": [
        "First, we use a corpus of discussion threads from CNET forums (Kim et al2010), which are mostly technical discussion and support.",
        "This corpus includes 321 threads and a total of 1309 messages, with an average message length of 78 tokens after preprocessing.5 Second, we use the Twitter data set created by Ritter et al2010).",
        "We consider 36K conversation threads for a total of 100K messages with average length 13.4 tokens.",
        "Both data sets are already annotated with the reply structure, so the discourse graph is given.",
        "We preprocess the data by treating contiguous blocks of punctuation as tokens, and we remove infrequent words.",
        "The Twitter corpus has some additional preprocessing, such as converting URLs to a single word type."
      ]
    },
    {
      "heading": "5.2 Baseline Models",
      "text": [
        "Our work is motivated by the Bayesian HMM approach of Ritter et al2010) ?",
        "the model we refer to as the block HMM (BHMM) ?",
        "and we consider this our primary baseline.",
        "(See also (Goldwater and Griffiths, 2007) for more details on Bayesian HMMs with Dirichlet priors.)",
        "We also compare against LDA, which makes latent assignments at the token-level, but blocks of text are independent of 5Three messages in this corpus have multiple parents.",
        "For the sake of conciseness, we simply remove these threads rather than introducing a method to model multiple parents.",
        "each other.",
        "In other words, BHMM models sequential dependencies but allows only single-class membership, whereas LDA uses no sequence information but has a mixed membership property.",
        "M4 combines these two properties.",
        "We use standard Gibbs samplers for both baseline models, and we optimize the Dirichlet hyperparam-eters (for the transition and topic distributions) using Minka's fixed-point iterations (2003)."
      ]
    },
    {
      "heading": "5.3 Incorporating Background Distributions",
      "text": [
        "In our experiments, we find that the intrusion of common stop words can make the results difficult to interpret, but we do not want to perform simple stop word removal because common function words often play important roles in the latent classes (i.e. speech acts) of the conversation data we consider here.",
        "We instead handle this by extending our model to include a ?background?",
        "distribution over words which is independent of the latent classes in a document; this was also done by Wang et al2011b).",
        "The idea is to introduce a binary switching variable x into the model which determines whether a word is generated from the general background distribution or from the distribution specific to a latent class z.",
        "Loosely, if the marginal probability of a word was given by p(w) = ?",
        "z p(w|z)p(z), the introduction of a background distribution gives the marginal probability p(w) = p(x = 0)p(w|B) +",
        "z p(w|z).",
        "This is common practice and we will not go into detail; see (Chemudugunta et al. 2006) for a general example on sampling switching variables.",
        "We augment all three models with a background distribution in exactly the same way, so that the comparison is fair.",
        "We use a Beta(10.0, 10.0) prior over the switching distribution."
      ]
    },
    {
      "heading": "5.4 Experimental Setup",
      "text": [
        "All of our results are averaged across four randomly initialized chains which are run for 5000 iterations, with five samples collected during the final 500 iterations.",
        "We take small gradient steps of decreasing size with ?",
        "(t) = 0.1/(1000 + t).",
        "We set ?2 = 10.0 as the variance of the ?",
        "weights.",
        "We use optimized asymmetric priors as described in ?5.2, and we use a symmetric Dirichlet for the word distributions, following Wallach et al. (2009).",
        "We sample the scaling hyperparameter ?",
        "via",
        "numbers of latent classes.",
        "Metropolis-Hastings proposals: we add Gaussian-distributed noise to the log of the current ?, then exponentiate this to yield the proposed ?(new).",
        "This log-space proposal ensures that ?",
        "is always positive.",
        "When computing the transition distributions for M4, we normalize the class histograms so that the counts to sum to 1.",
        "This helps with numeric stability because the input vectors stay within a small bounded range.6"
      ]
    },
    {
      "heading": "5.5 Experimental Results 5.5.1 Perplexity",
      "text": [
        "We begin with standard measures of the perplexity of held-out data.",
        "For these experiments, we train on 75% of the data, and test on the remaining 25%.",
        "We run the sampler for 500 iterations using the word distributions and transition parameters learned during training; we compute the average perplexity from the final ten sampling iterations.",
        "Results for different numbers of classes are shown in Table 1.",
        "These results demonstrate the advantage of models with the mixed membership property.",
        "Although LDA outperforms both sequence models, this is be expected.",
        "Each block's topic distribution is stochastically generated with LDA, whereas in the two sequence models, the distribution over classes is simply a deterministic function of the previous block.",
        "This allows LDA to infer parameters that fit the data more tightly.",
        "Comparing only the two sequence models, we find that M4 does significantly better than BHMM in all cases with p < 0.05.",
        "The horizontal bar indicates a random baseline.",
        "If capturing sequence information is not important, then LDA may provide a better fit to a corpus than sequence models.",
        "In the next two subsections, we will consider tasks where the sequential structure is important, thus LDA is not an appropriate choice.",
        "A natural predictive task of the sequence models is to reconstruct the discourse graph of a document where the structure is unknown.",
        "In the conversation domain, this corresponds to the task of thread reconstruction (Yeh and Harnly, 2006; Wang et al. 2011c).",
        "Given only a flat structure, can we recover the reply structure of messages in the conversation?",
        "Previous work with BHMM found the optimal structure by computing the likelihood of all permutations of a thread or sequence (Ritter et al2010; Wang et al2011b).",
        "We take a more practical approach and find the optimal structure as part of our inference procedure.",
        "We do this by treating the parent of each block as a hidden variable to be inferred.",
        "The parent of block b is the random variable rb, and we alternate between sampling values of the latent classes z and the parents r. The sampling distributions are annealed, as a search technique to find the best configuration of assignments (Finkel et al. 2005).",
        "At temperature ?",
        ", we sample a block's parent according to:",
        "For each conversation thread, any message is a candidate for the parent of block b (except b itself) including the dummy ?start?",
        "block.",
        "As before, we train on 75% of the data, and run this experiment on the remaining 25%.",
        "We run the sampler for 500 iterations, cooling ?",
        "by 1% after each iteration, where ?",
        "(0) = 1.",
        "We measure accuracy as the percentage of blocks whose assignment for rb matches the true parent.",
        "For each fold, we run this estimation procedure from five random initializations and average the results.",
        "Like Ritter et al. (2010), we do not enforce temporal constraints in the thread structure for this experiment.",
        "We are purely evaluating the predictive abilities of the model rather than its performance in a full-fledged reconstruction setup, which would require richer features beyond the scope of this paper.",
        "Figure 2 shows results comparing M4 against BHMM.",
        "Because all blocks are independent under LDA, it cannot be used in this experiment; using LDA would amount to a random baseline.",
        "We plot the distribution of results from various samples and various numbers of classes in {5, .",
        ".",
        ".",
        ", 25}.",
        "Most of the variance is across folds and samples; we find that there is not a strong trend in accuracy as a function of the number of classes.",
        "This suggests that most of the sequence predictions are carried by a small subset of the classes.",
        "On average, M4 outperforms BHMM by more than 15 points on the CNET corpus.",
        "M4 is also better on the Twitter corpus, but the difference is not so stark.",
        "This seems to confirm our intuition that the advantage of M4 over BHMM is greater when the blocks are longer; tweets may be short enough that the single-class assumption is not as limiting.",
        "Thus far, we have investigated the predictive power of the model, but we would also like to determine if the inferred clusters correspond to human-interpretible classes.",
        "In the case of conversation data, our hope is that some of the latent classes represent speech acts or dialog acts (Searle, 1975).",
        "While there is a body of work in supervised speech act classification (Cohen et al2004; Bangalore et al., 2006; Surendran and Levow, 2006; Qadir and Riloff, 2011), the variety of conversation domains on the Web motivates the use of unsupervised ap",
        "human-created speech act annotations of the CNET corpus and the latent class assignments by various models.",
        "proaches.",
        "The CNET corpus is annotated with twelve speech act classes: QUESTION and ANSWER, which are both broken down into multiple sub-classes, as well as RESOLUTION, REPRODUCTION, and OTHER (Kim et al2010).",
        "We would like to quantitatively measure how closely the latent states induced by our model match these annotations.7 We can measure this with variation of information (Meila, 2003), which has been used in recent years for unsupervised evaluation, e.g. in part-of-speech clustering (Goldwater and Griffiths, 2007).",
        "Given two sets of variable assignments z and z?, the variation of information is defined as H(Z|Z ?)",
        "+ H(Z ?|Z).",
        "In other words, given one clustering, how much uncertainty do we have about the other?",
        "Results are shown in Figure 3: a lower value corresponds to higher similarity.",
        "On the CNET corpus, M4 outperforms both baselines in all cases by a very significant margin.",
        "Qualitatively, we see clusters and transition parameters that make sense.",
        "For example, the class with top words {i,my, have, computer, am, ?, tried, help} is most likely to begin a thread (with ?",
        "= +1.94) and appears to describe questions or requests for 7Some messages have multiple labels.",
        "Since messages are not annotated at finer granularities, we handle this by simply duplicating such messages, once per label, and measuring clustering performance on this expanded set of labeled data which now has one label per token.",
        "!",
        "you ?",
        ":) u your good !!",
        "thanks .",
        "i , it you but that im lol its to in !",
        ".",
        "im ?",
        "the at be going !",
        "* :d lol haha :p ?",
        ".. me !!",
        ":o ?",
        "he .",
        "is the him his that was like .",
        "the of , ?",
        "sponds to a class learned by the model, and the most probable words are shown for each class.",
        "The symbols + and ?",
        "on the directed edges denote the sign of the ?",
        "associated with transitioning from one class to another, and the size of the symbols is scaled by the magnitude of ?.",
        "Non-edge arrows going into a node represent the weight of starting a conversation with that class.",
        "Low-magnitude weights are not shown, and some edges are omitted to avoid clutter.",
        "help.",
        "The class is not likely to be followed by itself (?",
        "= ?0.32) but is likely to be followed by the class with words {you, your, /, com, ., http, windows} (with ?",
        "= +1.38).",
        "The Twitter corpus does not have speech act annotations, so we offer example output in Figure 4.",
        "We again see patterns that we might expect to find in social media conversations, and some classes appear to correspond to speech acts such a declarations, personal questions, and replies.",
        "For example, the class in the center of the figure has words like you and but which suggests it is used in reply to other messages, and indeed we see that it has a positive weight of following almost every class, but a negative weight for actually starting a thread.",
        "Conversely, the class containing URLs (which corresponds to the act of sharing news or media) is likely to begin a thread, but is not likely to follow other classes except itself.",
        "How well unsupervised models can truly capture speech acts is an open question.",
        "Much as LDA ?topics?",
        "do not always correspond to what humans would judge to be semantic classes (Chang et al. 2009), the conversation classes inferred by unsupervised sequence models are similarly unlikely to be a perfect fit to human-assigned classes.",
        "Nevertheless, these results suggest M4 is a step forward.",
        "Our model provides a framework for defining inter-message transitions as functions of multiple classes, which will be a desirable property for many corpora."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have presented mixed membership Markov models (M4), which extend the simple HMM approach to discourse modeling by positing class assignments at the level of individual tokens.",
        "This allows blocks of text to belong to potentially multiple classes, a property that relates M4 to topic models.",
        "This type of model can be viewed as an HMM with an expanded state space, but because the transition probabilities are a function of a small number of parameters, the output remains human-interpretible.",
        "M4 can be taken as a general family of models and can be readily extended.",
        "In this work, we focused on introducing a model of inter-message structure, but certainly more sophisticated models of intra-message structure beyond unigram language models could be incorporated into M4.",
        "Standard topic model extensions such as n-gram models (Wallach, 2006) can straightforwardly be applied here, and indeed we already applied such an extension by incorporating background distributions in ?5.3.",
        "For conversational data, it could make sense to segment",
        "messages (e.g. into sentences) and constraint each segment to belong to one class or speech act; modifications along these lines have been applied to topic models as well (Gruber et al2007).",
        "While we have focused on conversation modeling, M4 is a general probabilistic model that could be applied to other discourse applications, for example modeling sentences or paragraphs in articles rather than messages in conversations; it could also be applied to data beyond text.",
        "Compared to a Bayesian block HMM, M4 performs much better at a variety of tasks.",
        "A drawback is that the time complexity of inference as presented here is quadratic in the number of classes rather than linear.",
        "Improving this may be the subject of future research.",
        "Another potential avenue of future work is to model transitions such that a Dirichlet prior for the class distribution of a block, rather than the class distribution itself, depends on the previous class assignments.",
        "This would yield a model that more closely resembles LDA, but with topic priors that encode sequence information."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "Thanks to Matt Gormley, Mark Dredze, Jason Eis-ner, the members of my lab and the anonymous reviewers for helpful feedback and discussions.",
        "This material is based upon work supported by a National Science Foundation Graduate Research Fellowship under Grant No.",
        "DGE-0707427 and a Dean's Fellowship from the Johns Hopkins University Whiting School of Engineering."
      ]
    }
  ]
}
