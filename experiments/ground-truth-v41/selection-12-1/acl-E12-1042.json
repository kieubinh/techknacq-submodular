{
  "info": {
    "authors": [
      "Franco M. Luque",
      "Ariadna Quattoni",
      "Borja Balle",
      "Xavier Carreras"
    ],
    "book": "EACL",
    "id": "acl-E12-1042",
    "title": "Spectral Learning for Non-Deterministic Dependency Parsing",
    "url": "https://aclweb.org/anthology/E12-1042",
    "year": 2012
  },
  "references": [
    "acl-D07-1101",
    "acl-E06-1011",
    "acl-H05-1066",
    "acl-J93-2004",
    "acl-N07-1051",
    "acl-P04-1014",
    "acl-P04-1058",
    "acl-P05-1010",
    "acl-P06-1055",
    "acl-P08-2054",
    "acl-P09-1039",
    "acl-P10-1001",
    "acl-P96-1024",
    "acl-P99-1059",
    "acl-W05-1504",
    "acl-W06-1666",
    "acl-W07-2218"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper we study spectral learning methods for non-deterministic split head-automata grammars, a powerful hidden-state formalism for dependency parsing.",
        "We present a learning algorithm that, like other spectral methods, is efficient and non-susceptible to local minima.",
        "We show how this algorithm can be formulated as a technique for inducing hidden structure from distributions computed by forward-backward recursions.",
        "Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena.",
        "Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al. 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al. 2005; Petrov et al. 2006; Musillo and Merlo, 2008).",
        "Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization.",
        "However, they have two main drawbacks.",
        "(1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems.",
        "A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered.",
        "This can be achieved by means of feature engineering, but compressing such information into a state of bounded size will typically be labor intensive, and will not generalize across languages.",
        "(2) Increasing the size of the factors generally results in polynomial increases in the parsing cost.",
        "In principle, hidden variable models could solve some of the problems of feature engineering in higher-order factorizations, since they could automatically induce the information in a derivation history that should be passed across factors.",
        "Potentially, they would require less feature engineering since they can learn from an annotated corpus an optimal way to compress derivations into hidden states.",
        "For example, one line of work has added hidden annotations to the non-terminals of a phrase-structure grammar (Matsuzaki et al. 2005; Petrov et al. 2006; Musillo and Merlo, 2008), resulting in compact grammars that obtain parsing accuracies comparable to lexicalized grammars.",
        "A second line of work has modeled hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de Rijke, 2004).",
        "Finally, a third line of work has induced hidden structure from the history of actions of a parser (Titov and Henderson, 2007).",
        "However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem.",
        "As a result, training a hidden-variable model is both expensive and prone to local minima issues.",
        "In this paper we present a learning algorithm for hidden-state split head-automata grammars (SHAG) (Eisner and Satta, 1999).",
        "In this for",
        "malism, head-modifier sequences are generated by a collection of finite-state automata.",
        "In our case, the underlying machines are probabilistic non-deterministic finite state automata (PNFA), which we parameterize using the operator model representation.",
        "This representation allows the use of simple spectral algorithms for estimating the model parameters from data (Hsu et al. 2009; Bailly, 2011; Balle et al. 2012).",
        "In all previous work, the algorithms used to induce hidden structure require running repeated inference on training data?e.g.",
        "Expectation-Maximization (Dempster et al. 1977), or split-merge algorithms.",
        "In contrast, spectral methods are simple and very efficient ?parameter estimation is reduced to computing some data statistics, performing SVD, and inverting matrices.",
        "The main contributions of this paper are: ?",
        "We present a spectral learning algorithm for inducing PNFA with applications to head-automata dependency grammars.",
        "Our formulation is based on thinking about the distribution generated by a PNFA in terms of the forward-backward recursions.",
        "?",
        "Spectral learning algorithms in previous work only use statistics of prefixes of sequences.",
        "In contrast, our algorithm is able to learn from substring statistics.",
        "?",
        "We derive an inside-outside algorithm for non-deterministic SHAG that runs in cubic time, keeping the costs of CFG parsing.",
        "?",
        "In experiments we show that adding non-determinism improves the accuracy of several baselines.",
        "When we compare our algorithm to EM we observe a reduction of two orders of magnitude in training time.",
        "The paper is organized as follows.",
        "Next section describes the necessary background on SHAG and operator models.",
        "Section 3 introduces Operator SHAG for parsing, and presents a spectral learning algorithm.",
        "Section 4 presents a parsing algorithm.",
        "Section 5 presents experiments and analysis of results, and section 6 concludes."
      ]
    },
    {
      "heading": "2 Preliminaries",
      "text": []
    },
    {
      "heading": "2.1 Head-Automata Dependency Grammars",
      "text": [
        "In this work we use split head-automata grammars (SHAG) (Eisner and Satta, 1999; Eisner, 2000), a context-free grammatical formalism whose derivations are projective dependency trees.",
        "We will use xi:j = xixi+1 ?",
        "?",
        "?xj to denote a sequence of symbols xt with i ?",
        "t ?",
        "j.",
        "A SHAG generates sentences s0:N , where symbols st ?",
        "X with 1 ?",
        "t ?",
        "N are regular words and s0 = ?",
        "6?",
        "X is a special root symbol.",
        "Let X?",
        "= X ?",
        "{?}.",
        "A derivation y, i.e. a dependency tree, is a collection of head-modifier sequences ?h, d, x1:T ?, where h ?",
        "X?",
        "is a word, d ?",
        "{LEFT, RIGHT} is a direction, and x1:T is a sequence of T words, where each xt ?",
        "X is a modifier of h in direction d. We say that h is the head of each xt.",
        "Modifier sequences x1:T are ordered head-outwards, i.e. among x1:T , x1 is the word closest to h in the derived sentence, and xT is the furthest.",
        "A derivation y of a sentence s0:N consists of a LEFT and a RIGHT head-modifier sequence for each st. As special cases, the LEFT sequence of the root symbol is always empty, while the RIGHT one consists of a single word corresponding to the head of the sentence.",
        "We denote by Y the set of all valid derivations.",
        "Assume a derivation y contains ?h, LEFT, x1:T ?",
        "and ?h, RIGHT, x?1:T ??.",
        "Let L(y, h) be the derived sentence headed by h, which can be expressed as",
        "The language generated by a SHAG are the strings L(y, ?)",
        "for any y ?",
        "Y .",
        "In this paper we use probabilistic versions of SHAG where probabilities of head-modifier sequences in a derivation are independent of each other:",
        "In the literature, standard arc-factored models further assume that",
        "where xT+1 is always a special STOP word, and ?t is the state of a deterministic automaton generating x1:T+1.",
        "For example, setting ?1 = FIRST and ?t>1 = REST corresponds to first-order models, while setting ?1 = NULL and ?t>1 = xt?1 corresponds to sibling models (Eisner, 2000; McDonald et al. 2005; McDonald and Pereira, 2006)."
      ]
    },
    {
      "heading": "2.2 Operator Models",
      "text": [
        "An operator model A with n states is a tuple ?",
        "?1, ?>?, {Aa}a?X ?, where Aa ?",
        "R n?n is an operator matrix and ?1, ??",
        "?",
        "Rn are vectors.",
        "A computes a function f : X ?",
        "?",
        "R as follows:",
        "One intuitive way of understanding operator models is to consider the case where f computes a probability distribution over strings.",
        "Such a distribution can be described in two equivalent ways: by making some independence assumptions and providing the corresponding parameters, or by explaining the process used to compute f .",
        "This is akin to describing the distribution defined by an HMM in terms of a factorization and its corresponding transition and emission parameters, or using the inductive equations of the forward algorithm.",
        "The operator model representation takes the latter approach.",
        "Operator models have had numerous applications.",
        "For example, they can be used as an alternative parameterization of the function computed by an HMM (Hsu et al. 2009).",
        "Consider an HMM with n hidden states and initial-state probabilities pi ?",
        "Rn, transition probabilities T ?",
        "Rn?n, and observation probabilities Oa ?",
        "Rn?n for each a ?",
        "X , with the following meaning: ?",
        "pi(i) is the probability of starting at state i, ?",
        "T (i, j) is the probability of transitioning from state j to state i, ?",
        "Oa is a diagonal matrix, such that Oa(i, i) is the probability of generating symbol a from state i.",
        "Given an HMM, an equivalent operator model can be defined by setting ?1 = pi, Aa = TOa and ??",
        "= ~1.",
        "To see this, let us show that the forward algorithm computes the expression in equation (3).",
        "Let ?t denote the state of the HMM at time t. Consider a state-distribution vector ?t ?",
        "Rn, where ?t(i) = P(x1:t?1, ?t = i).",
        "Initially ?1 = pi.",
        "At each step in the chain of products (3), ?t+1 = Axt ?t updates the state distribution from positions t to t + 1 by applying the appropriate operator, i.e. by emitting symbol xt and transitioning to the new state distribution.",
        "The probability of x1:T is given by ?",
        "i ?T+1(i).",
        "Hence, Aa(i, j) is the probability of generating symbol a and moving to state i given that we are at state j. HMM are only one example of distributions that can be parameterized by operator models.",
        "In general, operator models can parameterize any PNFA, where the parameters of the model correspond to probabilities of emitting a symbol from a state and moving to the next state.",
        "The advantage of working with operator models is that, under certain mild assumptions on the operator parameters, there exist algorithms that can estimate the operators from observable statistics of the input sequences.",
        "These algorithms are extremely efficient and are not susceptible to local minima issues.",
        "See (Hsu et al. 2009) for theoretical proofs of the learnability of HMM under the operator model representation.",
        "In the following, we write x = xi:j ?",
        "X ?",
        "to denote sequences of symbols, and use Axi:j as a shorthand for Axj ?",
        "?",
        "?Axi .",
        "Also, for convenience we assume X = {1, .",
        ".",
        ".",
        ", l}, so that we can index vectors and matrices by symbols in X ."
      ]
    },
    {
      "heading": "3 Learning Operator SHAG",
      "text": [
        "We will define a SHAG using a collection of operator models to compute probabilities.",
        "Assume that for each possible head h in the vocabulary X?",
        "and each direction d ?",
        "{LEFT, RIGHT} we have an operator model that computes probabilities of modifier sequences as follows:",
        "Then, this collection of operator models defines an operator SHAG that assigns a probability to each y ?",
        "Y according to (1).",
        "To learn the model parameters, namely ?",
        "?h,d1 , ?",
        "a }a?X ?",
        "for h ?",
        "X?",
        "and d ?",
        "{LEFT, RIGHT}, we use spectral learning methods based on the works of Hsu et al(2009), Bailly (2011) and Balle et al(2012).",
        "The main challenge of learning an operator model is to infer a hidden-state space from observable quantities, i.e. quantities that can be computed from the distribution of sequences that we observe.",
        "As it turns out, we cannot recover the actual hidden-state space used by the operators we wish to learn.",
        "The key insight of the spectral learning method is that we can recover a hidden-state space that corresponds to a projection of the original hidden space.",
        "Such projected space is equivalent to the original one in the sense that we",
        "can find operators in the projected space that parameterize the same probability distribution over sequences.",
        "In the rest of this section we describe an algorithm for learning an operator model.",
        "We will assume a fixed head word and direction, and drop h and d from all terms.",
        "Hence, our goal is to learn the following distribution, parameterized by oper",
        "Our algorithm shares many features with the previous spectral algorithms of Hsu et al(2009) and Bailly (2011), though the derivation given here is based upon the general formulation of Balle et al(2012).",
        "The main difference is that our algorithm is able to learn operator models from substring statistics, while algorithms in previous works were restricted to statistics on prefixes.",
        "In principle, our algorithm should extract much more information from a sample."
      ]
    },
    {
      "heading": "3.1 Preliminary Definitions",
      "text": [
        "The spectral learning algorithm will use statistics estimated from samples of the target distribution.",
        "More specifically, consider the function that computes the expected number of occurrences of a substring x in a random string x?",
        "drawn from P:",
        "where x v] x?",
        "denotes the number of times x appears in x?.",
        "Here we assume that the true values of f(x) for bigrams are known, though in practice the algorithm will work with empirical estimates of these.",
        "The information about f known by the algorithm is organized in matrix form as follows.",
        "Let P ?",
        "Rl?l be a matrix containing the value of f(x) for all strings of length two, i.e. bigrams.2.",
        "That is, each entry in P ?",
        "Rl?l contains the expected number of occurrences of a given bigram:",
        "2In fact, while we restrict ourselves to strings of length two, an analogous algorithm can be derived that considers longer strings to define P .",
        "See (Balle et al. 2012) for details.",
        "Furthermore, for each b ?",
        "X let Pb ?",
        "Rl?l denote the matrix whose entries are given by",
        "the expected number of occurrences of trigrams.",
        "Finally, we define vectors p1 ?",
        "Rl and p?",
        "?",
        "Rl as follows: p1(a) = ?",
        "s?X ?",
        "P(as), the probability that a string begins with a particular symbol; and p?",
        "(a) = ?",
        "p?X ?",
        "P(pa), the probability that a string ends with a particular symbol.",
        "Now we show a particularly useful way to express the quantities defined above in terms of the operators ?",
        "?1, ?>?, {Aa}a?X ?",
        "of P. First, note that each entry of P can be written in this form:",
        "It is not hard to see that, since P is a probability distribution over X ?, actually ?>?",
        "?1?1.",
        "From (8) it is natural to define a forward matrix F ?",
        "Rn?l whose ath column contains the sum of all hidden-state vectors obtained after generating all prefixes ended in a:",
        "Conversely, we also define a backward matrix B ?",
        "Rl?n whose ath row contains the probability of generating a from any possible state:",
        "By plugging the forward and backward matrices into (8) one obtains the factorization P = BF .",
        "With similar arguments it is easy to see that one also has Pb = BAbF , p1 = B ?1, and",
        "?",
        "F .",
        "Hence, ifB and F were known, one could in principle invert these expressions in order to recover the operators of the model from empirical estimations computed from a sample.",
        "In the next section we show that in fact one does not need to know B and F to learn an operator model for P, but rather that having a ?good?",
        "factorization of P is enough."
      ]
    },
    {
      "heading": "3.2 Inducing a Hidden-State Space",
      "text": [
        "We have shown that an operator model A computing P induces a factorization of the matrix P , namely P = BF .",
        "More generally, it turns out that when the rank of P equals the minimal number of states of an operator model that computes P, then one can prove a duality relation between operators and factorizations of P .",
        "In particular, one can show that, for any rank factorization P = QR, the operators given by ?",
        "?1 = Q+p1, ??>?",
        "= p",
        "and A?a = Q+PaR+, yield an operator model for P. A key fact in proving this result is that the function P is invariant to the basis chosen to represent operator matrices.",
        "See (Balle et al. 2012) for further details.",
        "Thus, we can recover an operator model for P from any rank factorization of P , provided a rank assumption on P holds (which hereafter we assume to be the case).",
        "Since we only have access to an approximation of P , it seems reasonable to choose a factorization which is robust to estimation errors.",
        "A natural such choice is the thin SVD decomposition of P (i.e. using top n singular vectors), given by: P = U(?V >) = U(U>P ).",
        "Intuitively, we can think of U and U>P as projected backward and forward matrices.",
        "Now that we have a factorization of P we can construct an operator model for P as follows: 3",
        "Algorithm 1 presents pseudo-code for an algorithm learning operators of a SHAG from training head-modifier sequences using this spectral method.",
        "Note that each operator model in the 3To see that equations (11-13) define a model for P, one must first see that the matrix M = F (?V >)+ is invertible with inverse M?1 = U>B.",
        "Using this and recalling that",
        "?",
        "The number of hidden states n 1: for each h ?",
        "X?",
        "and d ?",
        "{LEFT, RIGHT} do 2: Compute an empirical estimate from TRAIN of statistics matrices p?1, p?",
        "?, P?",
        ", and {P?a}a?X 3: Compute the SVD of P?",
        "and let U?",
        "be the matrix of top n left singular vectors of P?",
        "4: Compute the observable operators for h and d: 5: ?",
        "?h,d1 = U?",
        "for each h ?",
        "X?",
        ", d ?",
        "{LEFT, RIGHT}, a ?",
        "X SHAG is learned separately.",
        "The running time of the algorithm is dominated by two computations.",
        "First, a pass over the training sequences to compute statistics over unigrams, bigrams and trigrams.",
        "Second, SVD and matrix operations for computing the operators, which run in time cubic in the number of symbols l. However, note that when dealing with sparse matrices many of these operations can be performed more efficiently."
      ]
    },
    {
      "heading": "4 Parsing Algorithms",
      "text": [
        "Given a sentence s0:N we would like to find its most likely derivation, y?",
        "= argmaxy?Y(s0:N ) P(y).",
        "This problem, known as MAP inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states.",
        "We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations).",
        "For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007).",
        "In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies.",
        "We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals.",
        "Let (si, sj) denote a dependency between head word i and modifier word j.",
        "The posterior or marginal probability of a dependency (si, sj) given a sentence s0:N is defined as",
        "To compute marginals, the sum over derivations can be decomposed into a product of inside and outside quantities (Baker, 1979).",
        "Below we describe an inside-outside algorithm for our grammars.",
        "Given a sentence s0:N and marginal scores ?i,j , we compute the parse tree for s0:N as",
        "using the standard projective parsing algorithm for arc-factored models (Eisner, 2000).",
        "Overall we use a two-pass parsing process, first to compute marginals and then to compute the best tree."
      ]
    },
    {
      "heading": "4.1 An Inside-Outside Algorithm",
      "text": [
        "In this section we sketch an algorithm to compute marginal probabilities of dependencies.",
        "Our algorithm is an adaptation of the parsing algorithm for SHAG by Eisner and Satta (1999) to the case of non-deterministic head-automata, and has a runtime cost of O(n2N3), where n is the number of states of the model, and N is the length of the input sentence.",
        "Hence the algorithm maintains the standard cubic cost on the sentence length, while the quadratic cost on n is inherent to the computations defined by our model in Eq.",
        "(3).",
        "The main insight behind our extension is that, because the computations of our model involve state-distribution vectors, we need to extend the standard inside/outside quantities to be in the form of such state-distribution quantities.4 Throughout this section we assume a fixed sentence s0:N .",
        "Let Y(xi:j) be the set of derivations that yield a subsequence xi:j .",
        "For a derivation y, we use root(y) to indicate the root word of it, and use (xi, xj) ?",
        "y to refer a dependency in y from head xi to modifier xj .",
        "Following Eisner 4Technically, when working with the projected operators the state-distribution vectors will not be distributions in the formal sense.",
        "However, they correspond to a projection of a state distribution, for some projection that we do not recover from data (namely M?1 in footnote 3).",
        "This projection has no effect on the computations because it cancels out.",
        "and Satta (1999), we use decoding structures related to complete half-constituents (or ?triangles?, denoted C) and incomplete half-constituents (or ?trapezoids?, denoted I), each decorated with a direction (denoted L and R).",
        "We assume familiarity with their algorithm.",
        "We define ?I,Ri,j ?",
        "R n as the inside score-vector of a right trapezoid dominated by dependency (si, sj),",
        "sequences in the range si:j that do not involve si.",
        "The term ?si,R(x1:t) is a forward state-distribution vector ?the qth coordinate of the vector is the probability that si generates right modifiers x1:t and remains at state q.",
        "Similarly, we define ?I,Ri,j ?",
        "R n as the outside score-vector of a right trapezoid, as",
        "where ?si,R(xt+1:T ) ?",
        "Rn is a backward state-distribution vector ?the qth coordinate is the probability of being at state q of the right automaton of si and generating xt+1:T .",
        "Analogous inside-outside expressions can be defined for the rest of structures (left/right triangles and trapezoids).",
        "With these quantities, we can compute marginals as",
        "Finally, we sketch the equations for computing inside scores in O(N3) time.",
        "The outside equations can be derived analogously (see (Paskin, 2001)).",
        "For 0 ?",
        "i < j ?",
        "N :"
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "The goal of our experiments is to show that incorporating hidden states in a SHAG using operator models can consistently improve parsing accuracy.",
        "A second goal is to compare the spectral learning algorithm to EM, a standard learning method that also induces hidden states.",
        "The first set of experiments involve fully unlex-icalized models, i.e. parsing part-of-speech tag sequences.",
        "While this setting falls behind the state-of-the-art, it is nonetheless valid to analyze empirically the effect of incorporating hidden states via operator models, which results in large improvements.",
        "In a second set of experiments, we combine the unlexicalized hidden-state models with simple lexicalized models.",
        "Finally, we present some analysis of the automaton learned by the spectral algorithm to see the information that is captured in the hidden state space."
      ]
    },
    {
      "heading": "5.1 Fully Unlexicalized Grammars",
      "text": [
        "We trained fully unlexicalized dependency grammars from dependency treebanks, that is, X are PoS tags and we parse PoS tag sequences.",
        "In all cases, our modifier sequences include special START and STOP symbols at the boundaries.",
        "5 6 We compare the following SHAG models: ?",
        "DET: a baseline deterministic grammar with a single state.",
        "?",
        "DET+F: a deterministic grammar with two states, one emitting the first modifier of a sequence, and another emitting the rest (see (Eisner and Smith, 2010) for a similar deterministic baseline).",
        "?",
        "SPECTRAL: a non-deterministic grammar with n hidden states trained with the spectral algorithm.",
        "n is a parameter of the model.",
        "?",
        "EM: a non-deterministic grammar with n states trained with EM.",
        "Here, we estimate operators ??",
        "?1, ??",
        "?, A?",
        "h,d a ?",
        "using forward-backward for the E step.",
        "To initialize, we mimicked an HMM initialization: (1) we set ?",
        "?1 and ???",
        "randomly; (2) we created a random transition matrix T ?",
        "Rn?n; (3) we",
        "STOP symbols can be packed into ?1 and ??",
        "respectively.",
        "One just defines ?",
        "?1 = ASTART ?1 and ?",
        "for fully unlexicalized models.",
        "created a diagonal matrix Oh,da ?",
        "Rn?n, where Oh,da (i, i) is the probability of generating symbol a from h and d (estimated from training); (4) we set A?h,da = TO h,d a .",
        "We trained SHAG models using the standard WSJ sections of the English Penn Treebank (Marcus et al. 1994).",
        "Figure 1 shows the Unlabeled Attachment Score (UAS) curve on the development set, in terms of the number of hidden states for the spectral and EM models.",
        "We can see that DET+F largely outperforms DET7, while the hidden-state models obtain much larger improvements.",
        "For the EM model, we show the accuracy curve after 5, 10, 25 and 100 iterations.8 In terms of peak accuracies, EM gives a slightly better result than the spectral method (80.51% for EM with 15 states versus 79.75% for the spectral method with 9 states).",
        "However, the spectral algorithm is much faster to train.",
        "With our Matlab implementation, it took about 30 seconds, while each iteration of EM took from 2 to 3 minutes, depending on the number of states.",
        "To give a concrete example, to reach an accuracy close to 80%, there is a factor of 150 between the training times of the spectral method and EM (where we compare the peak performance of the spectral method versus EM at 25 iterations with 13 states).",
        "7For parsing with deterministic SHAG we employ MBR inference, even though Viterbi inference can be performed exactly.",
        "In experiments on development data DET improved from 62.65% using Viterbi to 68.52% using MBR, and DET+F improved from 72.72% to 74.80%.",
        "8We ran EM 10 times under different initial conditions and selected the run that gave the best absolute accuracy after 100 iterations.",
        "We did not observe significant differences between the runs.",
        "calized models on the WSJ test set.",
        "Table 1 shows results on WSJ test data, selecting the models that obtain peak performances in development.",
        "We observe the same behavior: hidden-states largely improve over deterministic baselines, and EM obtains a slight improvement over the spectral algorithm.",
        "Comparing to previous work on parsing WSJ PoS sequences, Eisner and Smith (2010) obtained an accuracy of 75.6% using a deterministic SHAG that uses information about dependency lengths.",
        "However, they used Viterbi inference, which we found to perform worse than MBR inference (see footnote 7)."
      ]
    },
    {
      "heading": "5.2 Experiments with Lexicalized Grammars",
      "text": [
        "We now turn to combining lexicalized deterministic grammars with the unlexicalized grammars obtained in the previous experiment using the spectral algorithm.",
        "The goal behind this experiment is to show that the information captured in hidden states is complimentary to head-modifier lexical preferences.",
        "In this case X consists of lexical items, and we assume access to the PoS tag of each lexical item.",
        "We will denote as ta and wa the PoS tag and word of a symbol a ?",
        "X?",
        ".",
        "We will estimate conditional distributions P(a |h, d, ?",
        "), where a ?",
        "X is a modifier, h ?",
        "X?",
        "is a head, d is a direction, and ?",
        "is a deterministic state.",
        "Following Collins (1999), we use three configurations of determin",
        "istic states: ?",
        "LEX: a single state.",
        "?",
        "LEX+F: two distinct states for first modifier and rest of modifiers.",
        "?",
        "LEX+FCP: four distinct states, encoding:",
        "first modifier, previous modifier was a coordination, previous modifier was punctuation, and previous modifier was some other word.",
        "To estimate P we use a back-off strategy: P(a|h, d, ?)",
        "= PA(ta|h, d, ?",
        ")PB(wa|ta, h, d, ?)",
        "To estimate PA we use two back-off levels, the fine level conditions on {wh, d, ?}",
        "and the",
        "for lexicalized models.",
        "coarse level conditions on {th, d, ?}.",
        "For PB we use three levels, which from fine to coarse are {ta, wh, d, ?",
        "}, {ta, th, d, ?}",
        "and {ta}.",
        "We follow Collins (1999) to estimate PA and PB from a treebank using a back-off strategy.",
        "We use a simple approach to combine lexical models with the unlexical hidden-state models we obtained in the previous experiment.",
        "Namely, we use a log-linear model that computes scores for head-modifier sequences as",
        "where Psp and Pdet are respectively spectral and deterministic probabilistic models.",
        "We tested combinations of each deterministic model with the spectral unlexicalized model using different number of states.",
        "Figure 2 shows the accuracies of single deterministic models, together with combinations using different number of states.",
        "In all cases, the combinations largely improve over the purely deterministic lexical counterparts, suggesting that the information encoded in hidden states is complementary to lexical preferences."
      ]
    },
    {
      "heading": "5.3 Results Analysis",
      "text": [
        "We conclude the experiments by analyzing the state space learned by the spectral algorithm.",
        "Consider the space Rn where the forward-state vectors lie.",
        "Generating a modifier sequence corresponds to a path through the n-dimensional state space.",
        "We clustered sets of forward-state vectors in order to create a DFA that we can use to visualize the phenomena captured by the state space.",
        "left modifier sequences.",
        "To build a DFA, we computed the forward vectors corresponding to frequent prefixes of modifier sequences of the development set.",
        "Then, we clustered these vectors using a Group Average Agglomerative algorithm using the cosine similarity measure (Manning et al. 2008).",
        "This similarity measure is appropriate because it compares the angle between vectors, and is not affected by their magnitude (the magnitude of forward vectors decreases with the number of modifiers generated).",
        "Each cluster i defines a state in the DFA, and we say that a sequence x1:t is in state i if its corresponding forward vector at time t is in cluster i.",
        "Then, transitions in the DFA are defined using a procedure that looks at how sequences traverse the states.",
        "If a sequence x1:t is at state i at time t ?",
        "1, and goes to state j at time t, then we define a transition from state i to state j with label xt.",
        "This procedure may require merging states to give a consistent DFA, because different sequences may define different transitions for the same states and modifiers.",
        "After doing a merge, new merges may be required, so the procedure must be repeated until a DFA is obtained.",
        "For this analysis, we took the spectral model with 9 states, and built DFA from the non-deterministic automata corresponding to heads and directions where we saw largest improvements in accuracy with respect to the baselines.",
        "A DFA for the automaton (NN, LEFT) is shown in Figure 3.",
        "The vectors were originally divided in ten clusters, but the DFA construction required two state mergings, leading to a eight state automaton.",
        "The state named I is the initial state.",
        "Clearly, we can see that there are special states for punctuation (state 9) and coordination (states 1 and 5).",
        "States 0 and 2 are harder to interpret.",
        "To understand them better, we computed an estimation of the probabilities of the transitions, by counting the number of times each of them is used.",
        "We found that our estimation of generating STOP from state 0 is 0.67, and from state 2 it is 0.15.",
        "Interestingly, state 2 can transition to state 0 generating prp$, POS or DT, that are usual endings of modifier sequences for nouns (recall that modifiers are generated head-outwards, so for a left automaton the final modifier is the leftmost modifier in the sentence)."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "Our main contribution is a basic tool for inducing sequential hidden structure in dependency grammars.",
        "Most of the recent work in dependency parsing has explored explicit feature engineering.",
        "In part, this may be attributed to the high cost of using tools such as EM to induce representations.",
        "Our experiments have shown that adding hidden-structure improves parsing accuracy, and that our spectral algorithm is highly scalable.",
        "Our methods may be used to enrich the representational power of more sophisticated dependency models.",
        "For example, future work should consider enhancing lexicalized dependency grammars with hidden states that summarize lexical dependencies.",
        "Another line for future research should extend the learning algorithm to be able to capture vertical hidden relations in the dependency tree, in addition to sequential relations.",
        "Acknowledgements We are grateful to Gabriele Musillo and the anonymous reviewers for providing us with helpful comments.",
        "This work was supported by a Google Research Award and by the European Commission (PASCAL2 NoE FP7-216886, XLike STREP FP7-288342).",
        "Borja Balle was supported by an FPU fellowship (AP2008-02064) of the Spanish Ministry"
      ]
    }
  ]
}
