{
  "info": {
    "authors": [
      "Jagadeesh Jagarlamudi",
      "Hal Daum√© III"
    ],
    "book": "NAACL",
    "id": "acl-N12-1088",
    "title": "Low-Dimensional Discriminative Reranking",
    "url": "https://aclweb.org/anthology/N12-1088",
    "year": 2012
  },
  "references": [
    "acl-D07-1080",
    "acl-D07-1117",
    "acl-D11-1086",
    "acl-E09-1033",
    "acl-J05-1003",
    "acl-J93-2003",
    "acl-N04-1023",
    "acl-N07-2047",
    "acl-N09-1025",
    "acl-N10-1095",
    "acl-P02-1062",
    "acl-P05-1012",
    "acl-P05-1022",
    "acl-P06-1096",
    "acl-P99-1023",
    "acl-W03-0402",
    "acl-W06-2920"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The accuracy of many natural language processing tasks can be improved by a reranking step, which involves selecting a single output from a list of candidate outputs generated by a baseline system.",
        "We propose a novel family of reranking algorithms based on learning separate low-dimensional embeddings of the task's input and output spaces.",
        "This embedding is learned in such a way that prediction becomes a low-dimensional nearest-neighbor search, which can be done computationally efficiently.",
        "A key quality of our approach is that feature engineering can be done separately on the input and output spaces; the relationship between inputs and outputs is learned automatically.",
        "Experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Mapping inputs to outputs lies at the heart of many Natural Language Processing applications.",
        "For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993).",
        "The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007).",
        "For the POS tagging, reranking is relative less explored due to the already higher accuracies in English (Collins, 2002), but it is shown to improve accuracies in other languages such as Chinese (Huang et al., 2007).",
        "In this paper, we propose a novel approach to discriminative reranking and show its effectiveness in POS tagging.",
        "Reranking allows us to use arbitrary features defined jointly on input and output spaces that are often difficult to incorporate into the baseline decoder due to the computational tractability issues.",
        "The effectiveness of reranking depends on the joint features defined over both input and output spaces.",
        "This has led the community to spend substantial efforts in defining joint features for reranking (Fraser et al., 2009; Chiang et al., 2009).",
        "Unfortunately, developing joint features over the input and output space can be challenging, especially in problems for which the exact mapping between the input and the output is unclear (for instance, in automatic caption generation for images, semantic parsing or non-literal translation).",
        "In contrast to prior work, our approach uses features defined separately within the input and output spaces, and learns a mapping function that can map an object from one space into the other.",
        "Since our approach requires within-space features, it makes the feature engineering relatively easy.",
        "For clarity, we will discuss our approach in the context of POS tagging, though of course it generalizes to any reranking problem.",
        "At test time, in POS tagging, we receive a sentence and a list of candidate output POS sequences as input.",
        "We run a feature extractor on the input sentence to obtain a representation x ?",
        "Rd1 ; we run an independent",
        "feature extractor on each of the m-many outputs to obtain representations y?1, .",
        ".",
        ".",
        ", y?m ?",
        "Rd2 .",
        "We will project all of these points down to a low k-dimensional space by means of matrices A ?",
        "Rd1?k (for x as ATx) and B ?",
        "Rd2?k (for y?",
        "as BT y?).",
        "We then select as the output the y?j that maximizes cosine similar to x in the lower-dimensional space: maxj cos(ATx, BT y?j).",
        "The goal is to learn the projection matrices A and B so that the result of this operation is a low-loss output.",
        "Given training data of sentences and their reference tag sequences, our approach implicitly uses all possible pairwise feature combinations across the views and learns the matrices A and B that can map a given sentence (as its feature vector) to its corresponding tag sequence.",
        "Considering all possible pairwise combinations enables our model to automatically handle long range dependencies such as a word at a position effecting the tag choice at any other position.",
        "Experiments performed on four languages (English, Chinese, French and Swedish) show the effectiveness of our approach in comparison to the baseline decoder and to the existing reranking approaches (Sec.",
        "4).",
        "Using only the within-space features, our models are able to beat reranking approaches that use more informative joint features.",
        "While it is possible to include joint features into our models, we leave this for future work."
      ]
    },
    {
      "heading": "2 Models for Low-Dimensional Reranking",
      "text": [
        "In this section, we describe our approach to learning low-dimensional representations for reranking.",
        "We first fix some notation, then discuss the intuition behind the problem we wish to solve.",
        "We propose both generative-style and discriminative-style approaches to formalizing this intuition, as well as a softened variant of the discriminative model.",
        "In the subsequent section, we discuss computational issues related to these models."
      ]
    },
    {
      "heading": "2.1 Notation",
      "text": [
        "Let xi ?",
        "Rd1 and yi ?",
        "Rd2 be the feature vectors representing the ith(1 ?",
        "?",
        "?n) sentence and its reference tag sequence from the training data.",
        "Each sentence is also associated with mi number of candidate tag sequences, output by the baseline decoder, and are represented as y?ij ?",
        "Rd2 j = 1 ?",
        "?",
        "?mi.",
        "Each candidate tag sequence (y?ij) is also associated with a non-negative loss Lij .",
        "Note that we place absolutely no constraints on the loss function.",
        "Moreover, letX (d1?n) and Y (d2?n) denote the data matrices with xi and yi as columns respectively.",
        "Finally, let ?u,v?",
        "denote the dot product of the two vectors u and v."
      ]
    },
    {
      "heading": "2.2 Intuition",
      "text": [
        "As stated in the introduction, our goal is to learn projections A ?",
        "Rd1?k and B ?",
        "Rd2?k in such a way that test-time predictions are made with high accuracy (or low loss).",
        "At test time, the output will be chosen by maximizing cosine similarity between the input and the output, after projecting these vectors into a low-dimensional space using A and B, respectively.",
        "The cosine similarity in our context is:",
        "Our goal is to learn A and B in such a way that the y?j with maximum cosine similarity to an x is actually the correct output.",
        "In what follows, we will describe our models to find one-dimensional projection vectors a ?",
        "Rd1 and b ?",
        "Rd2 , but the generalization to matrices A and B is very trivial."
      ]
    },
    {
      "heading": "2.3 A Generative-Style Model",
      "text": [
        "The first model we propose is akin to a generative probabilistic model, in the sense that it attempts to model the relationship between an input and its desired output, without taking alternate possible outputs into account.",
        "In the context of the intuition sketched in the previous section, the idea is to choose A and B so as to maximize the cosine similarities on the training data between each input and it's correct (or minimal-loss) output.",
        "This model intentionally ignores the information present in the alternative, incorrect outputs.",
        "The hope is that by making the cosine similarities with the best output as high as possible, all the alternate outputs will look bad in comparison.",
        "Given a training data of sentences and their reference tag sequences represented as X and Y (Sec.",
        "2.1), our generative model finds projection directions, in word and tag spaces, along which the",
        "aligned sentence and tag sequence pairs have maximum cosine similarity.",
        "In the one-dimensional setting, it finds directions a ?",
        "Rd1 and b ?",
        "Rd2 such that the correlation as defined in Eq.",
        "2 is maximized.",
        "(2) Since the objective is invariant to the scaling of vectors a and b, it can be rewritten as:",
        "s.t.",
        "aTXXTa = 1 and bTY Y Tb = 1(4) We refer to the constraints in Eq.",
        "4 as length constraints in the rest of this paper.",
        "To understand why maximizing this objective function learns a good mapping function between the sentence and the tag sequence, consider decomposing the objective function as follows:",
        "where we replaced the scalars xliymi and albm with ?lmi and wlm respectively.",
        "So finally, the objective can be expressed as aTXY Tb = ?",
        "i?w, ?(xi,yi)?",
        "where w is the weight vector and ?",
        "(xi,yi) is a vector of size (d1 ?",
        "d2) and is given by the Kronecker product of the two feature vectors xi and yi.",
        "In this form, the generative objective function bears similarity to the linear boundary surface widely used in machine learning, except that the weights are restricted to be the outer product of two vectors.",
        "From the reduced expressions, it is clear that our generative model considers all possible pairwise combinations of the input features (d1?d2) and learns which of them are more important than others.",
        "Intuitively, it puts higher weight on a word and tag pair that co-occur frequently in the training data, at the same time each of these are infrequent in their own views."
      ]
    },
    {
      "heading": "2.4 A Discriminative-Style Model",
      "text": [
        "The primary disadvantage of our generative model is that it only uses input sentences and their reference tag sequences and does not use the incorrect candidate tag sequences of a given sentence at all.",
        "In what follows, we describe a model that utilize the incorrect candidate tag sequences as negative examples to improve the projection directions (a and b).",
        "Our goal is to address this by adding constraints to our model that explicitly penalize ranking high-loss outputs higher than low-loss outputs, as is often done in the context of maximum-margin structure prediction techniques (Taskar et al., 2004).",
        "In this section, we describe a discriminative model that keeps track of the margin deviations and finds the projection directions iteratively.",
        "Intuitively, after the projection into the lower dimensional subspace, the cosine similarity of a sentence to its reference tag sequence must be greater than that of its incorrect candidate tag sequences.",
        "Moreover, the margin between these similarities should be proportional to the loss of the candidate translation, i.e. the more dissimilar a candidate tag sequence to its reference is, the farther it should be from the reference in the projected space.",
        "From the decomposition shown in Eq.",
        "5, for a given pair of source sentence xi and a tag sequence yj , the generative model assigns a score of : ?a,xi??b,yj?",
        "= aTxiyTj b Each input sentence is also associated with a list of candidate tag sequences and since each of these candidate sequences are incorrect they should be assigned a score less than that of the reference tag sequence.",
        "Drawing ideas from structure prediction literature (Bakir et al., 2007), we modify the objective function in order to include these terms.",
        "This idea can be captured using a loss augmented margin constraint for each sentence, tag sequence pair (Tsochantaridis et al., 2004).",
        "Let ?i denote a non-negative slack variable, then we define our new optimization problem as: arg max",
        "where 0 ?",
        "?",
        "?",
        "1 is a weight parameter.",
        "This objective function is ensuring that the margin between the reference and the candidate tag sequences in the projected space (as given by aTxiyTi b?aTxiy?Tijb) is proportional to its loss (Lij).",
        "Notice that the slack is defined for each sentence and it remains the same for all of its candidate tag sequences."
      ]
    },
    {
      "heading": "2.5 A Softened Discriminative Model",
      "text": [
        "One disadvantage of the discriminative model described in the previous section is that it cannot be optimized in closed form (as discussed in the next section).",
        "In this section, we consider a model that lies between the generative model and the (fully) discriminative model.",
        "This softened model has attractive computational properties (it is easy to compute) and will also form a building block for the optimization of the full discriminative model.",
        "For each sentence xi, its reference tag sequence yi should be assigned a higher score than any of its candidate tag sequences y?ij i.e. we want to maximize aTxiyTi b?aTxiy?Tijb.",
        "In the fully discriminative model, we enforce that this is at least one (modulo slack).",
        "In the relaxed version, we instead require that this hold on average.",
        "In order to achieve this we add the following terms to the objective function:",
        "where rij = yi ?",
        "y?ij is the residual vector between the reference and the candidate sequences.",
        "Now, we simply sum all these terms for a given sentence weighted by their loss and encourage it to be as high as possible, i.e. we maximize",
        "The normalization bymi takes care of unequal numbers of candidate tag sequences that often arises because of the difference in the lengths of the input sentences.",
        "Now let R denote a matrix of the same size as that of Y (i.e. d2 ?",
        "n) with its ith column as given by 1mi ?mi j=1 Lijrij , then we add the following term to the generative objective function:",
        "Finally, the projection directions are obtained by solving the following optimization problem :",
        "s.t.",
        "aTXXTa = 1 and bTY Y Tb = 1 where 0 ?",
        "?",
        "?",
        "1 is the weight parameter to be tuned on the development set."
      ]
    },
    {
      "heading": "3 Optimization",
      "text": [
        "In this section, we describe how we solve the optimization problems associated with our models.",
        "First we discuss the solution of the generative model.",
        "Next, we discuss the softened discriminative model, since its solution will be used as a subroutine in our final discussion of the fully discriminative model."
      ]
    },
    {
      "heading": "3.1 Optimizing the Generative Model",
      "text": [
        "The optimization problem corresponding to the generative model turns out to be identical to that of canonical correlation analysis (CCA) (Hotelling, 1936; Hardoon et al., 2004), which immediately suggests a solution by solving an eigensystem.",
        "In particular, the projection directions are obtained by solving the following generalized eigensystem:",
        "?",
        "is a regularization parameter and I is the identity matrix of appropriate size.",
        "Using these eigenvectors as columns, we form projection matrices A and B.",
        "These projection matrices are used to project sentences and tag sequences into a common lower dimensional subspace.",
        "In general, using all the eigenvectors is suboptimal from the generalization perspective so we retain only top k eigenvectors."
      ]
    },
    {
      "heading": "3.2 Optimizing the Softened Model",
      "text": [
        "In the softened discriminative version, the summation of all the difference terms over all candidate tag sequences and sentences (Eq.",
        "9), enables a simpler objective function whose optimum can be derived by following a procedure very similar to that of the",
        "generative model.",
        "In particular, the projection directions are obtained by solving Eq.",
        "11 except that Cxy is replaced with X((1?",
        "?",
        ")Y T + ?RT )."
      ]
    },
    {
      "heading": "3.3 Optimizing the Discriminative Model",
      "text": [
        "To solve the discriminative model, we begin by constructing the Lagrange dual.",
        "Let ?1, ?2 and ?ij be the Lagrangian multipliers corresponding to the length and the margin constraints respectively, then the Lagrangian of Eq.",
        "6 is given by:",
        "Differentiating the Lagrangian with respect to the parameters a,b and setting them to zero yields the solution for the parameters in terms of the Lagrangian multipliers ?ij as follows:",
        "covariance matrix to indicate that it is dependent on the Lagrangian multipliers ?ij .",
        "In other words, the solution is similar to that of the previous formulation except that the residual vectors are weighted by the Lagrangian multipliers instead of the loss function.",
        "Unlike the max margin formulations of SVM, it is not easy to rewrite the parameters a,b in terms of the Lagrangian multipliers ?ij as C?xy itself depends on ?ij?s.",
        "Hence, rewriting the parameters in terms of the Lagrangian multipliers and then solving the dual is not amenable in this case.",
        "In order to solve this optimization problem, we resort to an alternate optimization technique in the primal space.",
        "It proceeds in two stages.",
        "In the first stage, we keep the Lagrangian multipliers ?ij fixed and then solve for the parameters a,b, ?1, ?2 and ?i.",
        "Projection directions a,b and their Lagrangian multipliers ?1, ?2 are obtained by solving the generalized eigenvalue problem given in Eq.",
        "12.",
        "Using Algorithm 1 Alternate optimization algorithm for solving the parameters of Discriminative Model.",
        "Input: X,Y, Y?",
        ", L, ?, ?",
        "6: Solve for the eigenvectors of Eq.",
        "12. .",
        "7: Form matrices A,B with top k eigenvectors as columns; k is determined using dev.",
        "set.",
        "8: Let An & Bn be normalized versions of A and B s.t.",
        "they follow the length constraints.",
        "9: for each sentence i = 1 ?",
        "?",
        "?n do 10: j = 1?",
        "?",
        "?mi, ?ij =",
        "these projection directions, we determine the slack variable ?i for each sentence.",
        "In the second stage of the alternate optimization, we fix a,b and ?i and take a gradient descent step along ?ij's to minimize the function.",
        "We repeat this process until convergence.",
        "In our experiments, we noticed that this algorithm converges within five iterations, so we only run it for five iterations.",
        "The pseudocode of our approach is shown in Alg.",
        "1.",
        "First we initialize the Lagrangian multipliers proportional to the loss of the candidate tag sequences (step 1).",
        "This ensures that the eigenvectors solved in step 6 are same as the output given by the softened model (Sec.",
        "2.5).",
        "In general, in our experiments, we observed that this is a good starting point.",
        "After solving the generalized eigenvalue problem in step.",
        "6, we consider the top k eigenvectors, as determined by the error on the development set and normalize them so that they follow the length constraints (steps 7 and 8).",
        "In the rest of the algorithm,",
        "we use these normalized projection directions to find the slack values which are in turn used to find the update direction for the Lagrangian variables.",
        "In step 10, we compute the potential slack value (?ij) for each constraint so that it is satisfied and then choose the minimum of the positive ?ij values as the slack for this sentence (step 11).",
        "If the chosen slack value is equal to zero, it implies that ?ij ?",
        "0 ?j = 1 ?",
        "?",
        "?mi which in turn implies that all the constraints of a given input sentence are satisfied by the current projection directions and hence there is no need to update the Lagrangian multipliers.",
        "Otherwise, some of the constraints are still not satisfied and hence we will update their corresponding Lagrangian multipliers in steps 13 and 14.",
        "In specific, step 13 computes the deviation of the margin constraints with the new slack value and step 14 updates the Lagrangian multipliers along the gradient direction.",
        "In principle, our approach is similar to the cutting plane algorithm used to optimize slack rescaling version of Structured SVM (Tsochantaridis et al., 2004), but it differs in selecting the slack variable (step 11).",
        "The cutting plane method chooses ?i as the maximum of {0, ?ij} where as we choose the minimum of the positive ?ij values as the slack.",
        "Intuitively, this means that the cutting plane algorithm chooses a constraint that is most violated which results in fewer constraints.",
        "This is crucial in structured SVM, because solving the dual problem is cubic in terms of the number of examples and constraints.",
        "In contrast, our approach selects the slack such that at least one of the constraints is satisfied and adds all the remaining constraints to the active set.",
        "Since step 6 considers a weighted average of all these constraints the complexity depends only on the number of training examples and not the constraints."
      ]
    },
    {
      "heading": "3.4 Combining with Viterbi Decoding Score",
      "text": [
        "All the three formulations discussed until now do not consider the Viterbi decoding score assigned to each candidate tag sequence.",
        "As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences.",
        "Here, we describe a simple linear combination of the Viterbi decoding score and the score obtained by projecting into the low-dimensional subspace, using projection directions obtained by any of the above models.",
        "For a given sentence xi and candidate tag sequence pair y?ij , let sij and pij (Eq.",
        "1) be the scores assigned by Viterbi decoding and the lower dimensional projections respectively.",
        "Then we define the final score for this pair as a simple linear combination of these two scores as: Score(xi, y?ij) = sij + w pij (13) The weight w is optimized using a grid search on the development data set, we search for w from 0 to 100 with an increment of 1 and choose the value for which the error is minimum on the development set."
      ]
    },
    {
      "heading": "3.5 Reranking for POS Tagging",
      "text": [
        "To summarize our approach, we convert the training data into feature vectors and use any of the three methods discussed above to find the lower dimensional projection directions (a and b).",
        "Each of those approaches involve solving a similar generalized eigenvalue problem (Eq.",
        "11) with the cross covariance matrix Cxy defined differently in the three approaches.",
        "This problem can be solved in different ways, but we use the following approach since it reduces the size of the eigenvalue problem.",
        "where ?",
        "is the eigenvalue.",
        "Assuming that d2 ?",
        "d1, which is usually true in POS tagging because of the smaller tag vocabulary, these equations solve a smaller eigenvalue problem.",
        "After solving the eigenvalue problem, we form matricesA andB with columns as the top k eigenvectors a and b respectively.",
        "Given a new sentence and candidate tag sequence pair (xi, y?ij), their similarity is obtained using Eq.",
        "1.",
        "Now, based on the development data set we find the weight (w) for the linear combination of the projection and Viterbi decoding scores (Eq.",
        "13).",
        "During the reranking stage, we first use Eq.",
        "1 to compute the projection score for all the candidate tag sequences and then use Eq.",
        "13 to combine this scores with the decoding score.",
        "The candidate tag sequences are reranked based on this final score."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "In this section, we report POS tagging experiments on four languages: English, Chinese, French and",
        "Swedish.",
        "The data in all these languages is obtained from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006).",
        "We only consider the word and its fine grained POS tag (columns 2 and 5 respectively) and ignore the dependency links in the data.",
        "Table 1 shows the data statistics in each of these languages.",
        "We use a second order Hidden Markov Model (Thede and Harper, 1999) based tagger as a baseline tagger in our experiments.",
        "This model uses trigram transition and emission probabilities and is shown to achieve good accuracies in English and other languages (Huang et al., 2007).",
        "We refer to this as the baseline tagger in the rest of this paper and is used to produce n-best list for each candidate sentence.",
        "The n-best list for training data is produced using multi-fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005).",
        "The first block of Table 2 shows the accuracies of the top-ranked tag sequence (according to the Viterbi decoding score) and the oracle accuracies on the 10-best list.",
        "As expected the accuracies on English and French are high and are on par with the state-of-the-art systems.",
        "From the oracle scores, it is clear that though there is a chance for improvement using reranking, the scope for improvement in English is less compared to the 5 point improvement reported for parsing (Charniak and Johnson, 2005).",
        "This indicates the difficulty of the reranking problem for POS tagging in well-resourced languages."
      ]
    },
    {
      "heading": "4.1 Reranking Features and Baselines",
      "text": [
        "In this paper, except for Chinese, we use suffixes of length two to four as features in the word view and unigram and bigram tag sequences as features in the tag view.",
        "That is, we convert each word of the sentence into suffixes of length two to four and then treat each sentence as a bag of suffixes.",
        "Similarly, we treat a candidate POS tag sequence as a bag of unigram and bigram tag features.",
        "For Chinese, we use character sequences of length one and two as features for the sentences and use unigram and bigram POS tag sequences on the tag view.",
        "We did not include any alignment based features, i.e. features that depend on the position.",
        "We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al., 2007).",
        "In order to enable a fair comparison, we use suffix and tag pairs as features for both these models.",
        "For example, we would generate the following features for the word ?selling?",
        "in the phrase ?the/DT selling/NN pressure/NN?",
        ": (ng, NN), (ng, DT NN), (ing,NN), (ing,DT NN), (ling,NN), (ling,DT NN).",
        "For comparison purposes, we also show results by running the baseline rerankers with n-gram features."
      ]
    },
    {
      "heading": "4.2 Results",
      "text": [
        "There are following hyper parameters in each of our models, regularization parameter ?",
        ", weight parameter ?",
        "in the discriminative and softened discriminative models, the linear combination weight w with the Viterbi decoding score, and finally, the size of the lower dimensional subspace (k).",
        "We use grid search to tune these parameters based on the development data set.",
        "The optimal hyperparameter values differ based on the model and the language, but the tagging accuracy is relatively robust with respect to these parameter values.",
        "For English, the best values for the discriminative model are ?",
        "= 0.95, ?",
        "= 0.3 and k = 75.",
        "For the same language, Fig. 1 shows the performance with respect to ?",
        "and ?",
        "parameters, respectively, with other parameters fixed to their optimal values.",
        "Notice that, although the performance varies it is always more than the accuracy of the baseline tagger (96.74%).",
        "Table 2 shows the results of different models on the development and test data sets.",
        "On the test data set, the baseline reranking approaches perform better than the HMM decoder in Chinese and Swedish languages, but they underperform in English and French languages.",
        "This is justifiable because the individual characters are good indicators of POS tag",
        "showed the results of Collins and Koo (2005) its regularized versions with n-gram features.",
        "The improvements of our discriminative models are statistically significant at p = 0.01 and p = 0.05 levels on Chinese and English respectively.",
        "information for Chinese and this additional information is being exploited by the reranking approaches.",
        "Swedish, on the other hand, is a Germanic language with compound word phenomenon which makes the baseline HMM decoder weaker compared to English and French.",
        "The fourth block shows the performance of our models.",
        "Except in Swedish, one of our models outperform the baseline decoder and the other reranking approaches.",
        "The fact that our models outperform the baseline system and other reranking approaches indicate that, by considering all the pairwise combinations of the input features our models capture dependencies that are left by other models.",
        "Among the different formulations of our approach, maximizing the margin between the correct and incorrect candidates performed better than generative, and ensuring that the margin is proportional to the loss of the candidate sequence (discriminative) led to even more improved results.",
        "Except in Chinese, our discriminative version performed at least as well as the other variants.",
        "Compared to the baseline decoder, the discriminative version achieves a maximum improvement of 0.6 points in Chinese while achieving 0.15, 0.12 and 0.13 points of improvement in English, French and Swedish languages respectively.",
        "We also reported the results of the baseline rerankers with n-gram features in the fifth block of",
        "coding score.",
        "reader should compare our results with the baseline rerankers run with the suffix features.",
        "The performance of these baseline rankers improved when we include the n-gram features but it is still less than the discriminative model in most cases.",
        "Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score.",
        "As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005)."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "In this section, we discuss approaches that are most relevant to our problem and the approach.",
        "In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006).",
        "Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task.",
        "Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese.",
        "In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship between them automatically.",
        "This is the primary difference between our approach and the existing rerankers.",
        "In principle, our margin formulations are similar to the max margin formulations of CCA (Szedmak et al., 2007) and maximum margin regression (Szedmak et al., 2006; Wang et al., 2007).",
        "These approaches solve the following optimization problem:",
        "s.t.",
        "?yi,W?(x)i?",
        "?",
        "1?",
        "?i ?i = 1 ?",
        "?",
        "?n Our approach differs from these formulations in two main ways: the score assigned by our generative model (equivalent to CCA) for an input-output pair (xTi abTyi) can be converted into this format by substituting W ?",
        "baT but in doing so we are ignoring the rank constraint.",
        "It is often observed that, dimensionality reduction leads to an improved performance and thus the rank constraint becomes crucial.",
        "Another major difference is that, the constraints in Eq.",
        "16 represent that any input and output pair should have at least a margin of 1 (modulo slack), whereas in our approach, the constraints include incorrect outputs along with their loss value.",
        "In other words, our formulation is more suitable for the reranking problem while Eq.",
        "16 is more suitable for regression or classification tasks.",
        "Our generative model is very similar to the supervised semantic hashing work (Bai et al., 2010) but the way we optimize is completely different from theirs."
      ]
    },
    {
      "heading": "6 Discussion",
      "text": [
        "In this paper, we proposed a novel family of models for discriminative reranking problem and showed improvements for the POS tagging task in four different languages.",
        "Here, we restricted our scope to showing the utility of our technique and, hence, did not experiment with different features, though it is an important direction.",
        "By using only within space features, our models are able to beat the reranking approaches that use potentially more informative alignment-based features.",
        "It is also possible to include alignment-based features into our models by posing the problem as a feature selection problem on the covariance matrices (Jagarlamudi et al., 2011).",
        "Our approach involves an inverse computation and an eigenvalue problem.",
        "Although our models scale to medium size data sets (our Chinese data set has 50K examples and 33K features), these operations can be expensive.",
        "But there are alternative approximation techniques that scale well to large data sets (Halko et al., 2009).",
        "We leave this for future work."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank Zhongqiang Huang for providing the code for the baseline systems, Raghavendra Udupa and the anonymous reviewers for their insightful comments.",
        "This work is partially funded by NSF grants IIS-1153487 and IIS-1139909."
      ]
    }
  ]
}
