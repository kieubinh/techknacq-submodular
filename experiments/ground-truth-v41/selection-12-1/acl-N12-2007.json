{
  "info": {
    "authors": [
      "Beibei Yang",
      "Jesse M. Heines"
    ],
    "book": "NAACL",
    "id": "acl-N12-2007",
    "title": "Domain-Specific Semantic Relatedness from Wikipedia: Can a Course Be Transferred?",
    "url": "https://aclweb.org/anthology/N12-2007",
    "year": 2012
  },
  "references": [
    "acl-J06-1003",
    "acl-J98-1006",
    "acl-W11-1418"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Semantic relatedness, or its inverse, semantic distance, measures the degree of closeness between two pieces of text determined by their meaning.",
        "Related work typically measures semantics based on a sparse knowledge base such as WordNet1 or CYC that requires intensive manual efforts to build and maintain.",
        "Other work is based on the Brown corpus, or more recently, Wikipedia.",
        "Wikipedia-based measures, however, typically do not take into account the rapid growth of that resource, which exponentially increases the time to prepare and query the knowledge base.",
        "Furthermore, the generalized knowledge domain may be difficult to adapt to a specific domain.",
        "To address these problems, this paper proposes a domain-specific semantic relatedness measure based on part of Wikipedia that ana-lyzes course descriptions to suggest whether a course can be transferred from one institution to another.",
        "We show that our results perform well when compared to previous work."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many NLP techniques have been adapted to the education field for building systems such as automated scoring, intelligent tutoring, and learner cognition.",
        "Few, however, address the identification of transfer course equivalencies.",
        "A recent study by the Na",
        "fer to another institution.",
        "Correspondingly, University of Massachusetts Lowell (UML) accepts hundreds of transfer students every year.",
        "Each transfer course must be evaluated for credits by manually comparing its course description to courses offered at UML.",
        "This process is labor-intensive and highly inefficient.",
        "There is a publicly available course transfer dictionary which lists course numbers from hundreds of institutions and their equivalent courses at UML, but the data set is sparse, non-uniform, and always out of date.",
        "External institutions cancel courses, change course numbers, etc., and such information is virtually impossible to keep up to date in the transfer dictionary.",
        "Furthermore, the transfer dictionary does not list course descriptions.",
        "From our experience, course descriptions change over the years even when course numbers do not, and this of course affect equivalencies.",
        "This work proposes a domain-specific semantic relatedness measure using Wikipedia that automatically suggests whether two courses from different institutions are equivalent by analyzing their course descriptions.",
        "The goal is to assist transfer coordinators by suggesting equivalent courses within a reasonable amount of time on a standard laptop system.",
        "Our model is a mapping function: f : (C1, C2) ?",
        "n, n ?",
        "[0, 1], where C1 is a Computer Science (CS) course from an external institution, and C2 is a CS course offered at UML.",
        "Output n is the semantic relatedness score, where a bigger value indicates C1 and C2 are more related.",
        "Each course description is a short text passage: ?",
        "C1: [Analysis of Algorithms] Discusses basic methods for designing and analyzing efficient algorithms empha",
        "sizing methods used in practice.",
        "Topics include sorting, searching, dynamic programming, greedy algorithms, advanced data structures, graph algorithms (shortest path, spanning trees, tree traversals), matrix operations, string matching, NP completeness.",
        "?",
        "C2: [Computing III] Object-oriented programming.",
        "Classes, methods, polymorphism, inheritance.",
        "Object-oriented design.",
        "C++.",
        "UNIX.",
        "Ethical and social issues.",
        "We choose Wikipedia as the knowledge base due to its rich contents (Figure 1) and continuously coalescent growth (Bounova, 2011).",
        "Although Wikipedia was launched 10 years later, it grew much faster than WordNet over the last decade (Figure 2).",
        "The contributions of this paper are twofold.",
        "First, we address the problem of domain-specific semantic relatedness using Wikipedia.",
        "We propose a method to suggest course equivalencies by computing semantic relatedness among Computer Science course descriptions.",
        "Our approach can be easily modified for other majors and even other languages.",
        "Second, we evaluate the correlation of our approach and a human judgment data set we built.",
        "Both accuracy and correlation indicate that our approach outperforms previous work."
      ]
    },
    {
      "heading": "2 Related Research",
      "text": [
        "Semantic relatedness has been used in applications such as word sense disambiguation, named entity disambiguation, text summarization and annotation, lexical selection, automatic spelling correction, and text structure evaluation.",
        "WordNet is commonly used as a lexicographic resource to calculate semantic relatedness (Budanitsky and Hirst, 2006).",
        "A WordNet-based method uses one or more edge-counting techniques in theWordNet taxonomy (Lea-cock and Chodorow, 1998; Hirst and St-Onge, 1998).",
        "The relatedness of two concept nodes is a function of the minimum number of hops between them.",
        "Some related work calculates co-occurrence on one or more large corpora to deduce semantic relatedness (Sahami and Heilman, 2006; Cilibrasi and Vitanyi, 2007).",
        "Two words are likely to be related if they co-occur within similar contexts (Lin, 1998).",
        "Others combine lexicographic resources with corpus statistics (Jiang and Conrath, 1997).",
        "It has been shown that these composite methods generally outperform lexicographic resource-and corpus-based methods (Budanitsky and Hirst, 2006; Curran, 2004; Mohammad, 2008).",
        "Li et al. (2006) propose a hybrid method based on WordNet and the Brown corpus to incorporate semantic similarity between words, semantic similarity between sentences, and word order similarity to measure the overall sentence similarity.",
        "Yang and Heines (2011) modify this work to suggest transfer course equivalencies, but the experiment is based on non-technical courses.",
        "Due to theWordNet sparsity on technical terms, the experiment does not perform well on Computer Science courses.",
        "In recent years, there has been increasing interest in applying Wikipedia and related resources to question answering (Buscaldi and Rosso, 2006), word sense disambiguation (WSD) (Mihalcea and Cso-mai, 2007), name entity disambiguation (Ni et al., 2010), ontology evaluation (Yu et al., 2007), semantic web (Wu, 2010), and computing semantic relatedness (Ponzetto and Strube, 2007).",
        "Ponzetto and Strube (2007) deduce semantic relatedness of words by modeling relations on the Wikipedia category graph.",
        "Gabrilovich and Markovitch (2009) introduce the Explicit Semantic Analysis (ESA) model which calculates TF-IDF (Manning et al., 2008) values for every word in Wikipedia and further uses local linkage information to build a second-level semantic interpreter.",
        "Our approach is different from prior work on Wikipedia.",
        "While Mihalcea and Csomai (2007) use the annotation in the page title of a concept to perform WSD, our approach uses a page's parent category as a cue to the correct sense.",
        "Ponzetto and Strube (2007) limit their measurement to word pairs, while our work focuses on text of any length.",
        "Gabrilovich and Markovitch (2009) computes TF-IDF statistics for every word and every document of Wikipedia which is highly inefficient.",
        "They also remove category pages and disambiguation pages.",
        "In contrast, our model is mainly based on the category taxonomy and the corpus statistics are limited to metadata that are mostly available in Wikipedia.",
        "Furthermore, we compute concept relatedness on a domain-specific hierarchy that weighs both path lengths and diversions from the topic.",
        "The domain-specific hierarchy is much smaller than the entire Wikipedia corpus.",
        "As a result, our algorithm is more efficient3 than previous work.",
        "3In our experiment, the average time needed to compare one pair of course descriptions ranged from 0.16 second (with partial caching) to 1 minute (without caching) on a 2.6Ghz Quad-Core PC.",
        "The most time-consuming part before comparing courses was to index all the Wikipedia tables in a MySQL database, which took overnight (same for ESA).",
        "It only took us 15 minutes to go through 19K pages to build a hierarchy of D = 4.",
        "In contrast, ESA's first level semantic interpreter (which tokenizes every Wikipedia page to compute TF-IDF) took 7 days to build over the same 19K pages.",
        "Both implementations were single-threaded, coded in Python, and tested over the English Wikipedia of July 2011."
      ]
    },
    {
      "heading": "3 Proposed Method",
      "text": [
        "Our method contains four modules.",
        "Section 3.1 explains how to construct a domain-specific hierarchy fromWikipedia.",
        "Section 3.2 presents semantic relatedness between concepts.",
        "Section 3.3 describes the steps to generate features from course descriptions.",
        "And section 3.4 evaluates course relatedness."
      ]
    },
    {
      "heading": "3.1 Extract a Lexicographical Hierarchy",
      "text": [
        "When a domain is specified (e.g., CS courses), we start from a generic Wikipedia category in this domain, choose its parent as the root, and use a depth-limited search to recursively traverse each subcategory (including subpages) to build a lexicographical hierarchy with depth D. For example, to find CS course equivalencies, we built a hierarchy using the parent of ?Category:Computer science,?",
        "i.e., ?Cat-egory:Applied sciences,?",
        "as the root.",
        "The generic category's parent is chosen as the root to make sure the hierarchy not only covers the terms in this domain, but also those in neighbor domains.",
        "The hierarchy of ?Category:Applied sciences?",
        "not only covers Computer Science, but also related fields such as Computational Linguistics and Mathematics.",
        "Both the number of nodes and number of edges in the hierarchy grow exponentially4 as the depth increases.",
        "Therefore, D need not be a big number to cover most terms in the domain.",
        "We have found the hierarchy speeds up the semantic measurement dramatically and covers almost all the words in the specific domain.",
        "In our experiment on CS courses (D=6), we eliminated over 71% of Wikipedia articles,5 yet the hierarchy covered over 90% of CS terminologies mentioned in the course descriptions."
      ]
    },
    {
      "heading": "3.2 Semantic Relatedness Between Concepts",
      "text": [
        "Similar to the work of Li et al. (2006), the semantic relatedness between two Wikipedia concepts,6 t1 and t2 in the hierarchy is defined as:",
        "where p is the shortest path between t1 and t2, and d is the depth of the lowest common hypernym of t1",
        "and t2 in the hierarchy (Section 3.1).",
        "This is different from related work on semantic relatedness from Wikipedia (Ponzetto and Strube, 2007) in that we not only consider the shortest path (p) between two concepts but also their common distance (d) from the topic, which in turn emphasizes domain awareness."
      ]
    },
    {
      "heading": "3.3 Generate Course Description Features",
      "text": [
        "The built-in redirection in Wikipedia is useful for spelling corrections because variations of a term redirect to the same page.",
        "To generate features from a course description C, we start by generating n-grams (n ?",
        "[1, 3]) from C. We then query the redirection data to fetch all pages that match any of the n-grams.",
        "The identified pages are still sparse.",
        "We therefore query the title data to fetch those that match any of the n-grams.",
        "Page topics are not discriminated in this step.",
        "For example, unigram ?Java?",
        "returns both ?Java (software platform)?",
        "and ?Java (dance).?",
        "Wikipedia contains a collection of disambiguation pages.",
        "Each disambiguation page includes a list of alternative uses of a term.",
        "Note that there are two different Wikipedia disambiguation pages: explicit and implicit.",
        "A page is explicit when the page title is annotated by Wikipedia as ?disambiguation,?",
        "such as ?Oil (disambiguation).?",
        "A page is implicit when it is not so annotated, but points to a category such as ?Category:Disambiguation pages,?",
        "or ?Cat-egory:All disambiguation pages.?",
        "We iterate over the pages fetched from the last step, using disambiguation pages to enrich and refine the features of a course description.",
        "Unlike the work of Mihalcea and Csomai (2007) which uses the annotation in the page title of a concept to perform WSD, our approach uses a page's parent category as a cue to the correct sense.",
        "Typically, the sense of a concept depends on the senses of other concepts in the context.",
        "For example, a paragraph on programming languages and data types ensures that ?data?",
        "more likely corresponds to a page under ?Category:Computer data?",
        "than one under ?Category:Star Trek.?",
        "Algorithm 1 explains the steps to generate features for a course C. Given the C1 and C2 in section 1, their generated features F1 and F2 are:",
        "F1: Shortest path problem, Tree traversal, Spanning tree, Tree, Analysis, List of algorithms, Completeness, Algorithm, Sorting, Data structure, Structure, Design, Data.",
        "F2: Unix, Social, Ethics, Object-oriented design, Computer programming, C++, Object-oriented programming, Design.",
        "Algorithm 1 Feature Generation (F ) for Course C 1.",
        "Tc ?",
        "?",
        "(clear terms), Ta ?",
        "?",
        "(ambiguous terms).",
        "2.",
        "Generate all possible n-grams (n ?",
        "[1, 3]) G from C. 3.",
        "Fetch the pages whose titles match any of g ?",
        "G from Wikipedia redirection data.",
        "For each page pid of term t, Tc ?",
        "Tc ?",
        "{t : pid}.",
        "4.",
        "Fetch the pages whose titles match any of g ?",
        "G from Wikipedia page title data.",
        "If a disambiguation page, include all the terms this page refers to.",
        "If a page pid corresponds to a term t that is not ambiguous, Tc ?",
        "Tc ?",
        "{t : pid}, else Ta ?",
        "Ta ?",
        "{t : pid}.",
        "5.",
        "For each term ta ?",
        "Ta, find the disambiguation that is on average most related (Equation 1) to the set of clear terms.",
        "If a page pid of ta is on average the most related to the terms in Tc, and the relatedness score is above a threshold ?",
        "(?",
        "?",
        "[0, 1]), set Tc ?",
        "Tc ?",
        "{ta : pid}.",
        "If ta and a clear term are different senses of the same term, keep the one that is more related to all the other clear terms.",
        "6.",
        "Return clear terms as features.",
        "Algorithm 2 Semantic Vector SV1 for F1 and J 1. for all words ti ?",
        "J do 2. if ti ?",
        "F1, set SV1i = 1 where SV1i ?",
        "SV1.",
        "3. if ti /?",
        "F1, the semantic relatedness between ti and each term t1j ?",
        "F1 is calculated (Equation 1).",
        "Set SV1i to the highest score if the score exceeds the preset threshold ?, otherwise SV1i = 0.",
        "4. end for"
      ]
    },
    {
      "heading": "3.4 Determine Course Relatedness",
      "text": [
        "Given two course descriptions C1 and C2, we use Algorithm 1 to generate features F1 for C1, and F2 forC2.",
        "Next, the two feature lists are joined together into a unique set of terms, namely J .",
        "Similar to previous work (Li et al., 2006), semantic vectors SV1 (Algorithm 2) and SV2 are computed for F1 and F2.",
        "Each value of an entry of SV1 for features F1 is reweighed as:",
        "where SV1i is the semantic relatedness between ti ?",
        "F1 and tj ?",
        "J .",
        "I(ti) is the information content of ti, and I(tj) is the information content of tj .",
        "Similarly, we reweigh each value for the semantic vector SV2 of F2.",
        "The information content I(t) of a term t is a weighed sum of the category information content Ic(t) and the linkage information content Il(t):",
        "Inspired by related work (Seco et al., 2004), the category information content of term t is redefined as a function of its siblings:",
        "where siblings(t) is the number of siblings for t on average, and N is the total number of terms in the hierarchy (Section 3.1).",
        "The linkage information content is a function of outlinks and inlinks of the page pid that t corresponds to:",
        "where inlinks(pid) and outlinks(pid) are the numbers of inlinks and outlinks of a page pid.",
        "MAXIN and MAXOUT are the maximum numbers of inlinks and outlinks that a page has in Wikipedia.7 Finally, the relatedness of two courses is a cosine coefficient of the two semantic vectors:"
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": [
        "Wikipedia offers its content as database backup dumps (wikidumps) freely available to download.",
        "Our application is based on the English wikidump8 of July 2011.",
        "We have extracted redirections, titles, categories, and links from the wikidump into separate tables in MySQL.",
        "Using the steps outlined in Section 3.1, we built a table for the hierarchy with ?Category:Applied sciences?",
        "as the root.",
        "The attributes of each table were indexed to speed up queries.",
        "Our experiment used ?",
        "= 0.2, ?",
        "= 0.5, ?",
        "= 0.2, and ?",
        "= 0.6.",
        "These values were found 7The computation of MAXIN and MAXOUT could be time-consuming.",
        "They are therefore based on the entire Wikipedia instead of the constructed hierarchy to avoid the recalculation when the domain changes.",
        "This also ensures the maximum linkage information is unbiased for every domain.",
        "For the July 2011 wikidump, page ?Geographic coordinate system?",
        "has the most in-links, a total of 575,277.",
        "Page ?List of Italian communes (2009)?",
        "has the most out-links, a total of 8,103.",
        "empirically to perform well over randomly selected samples.",
        "We randomly selected 25 CS courses from 19 universities that can be transferred to University of Massachusetts Lowell (UML) according to the transfer dictionary.",
        "Each transfer course was compared to all 44 CS courses offered at UML, a total of 1,100 comparisons.",
        "The result was considered correct for each course if the real equivalent course in UML appears among the top 3 in the list of highest scores.",
        "We excluded all Wikipedia pages whose titles contained specific dates or were annotated as ?magazine?, ?journal?, or ?album.?",
        "We removed both general and domain stop words (such as ?course,?",
        "?book,?",
        "and ?student?)",
        "from course descriptions.",
        "If a course description contains the keywords ?not?",
        "or ?no,?",
        "e.g., ?This course requires no computer programming skills,?",
        "the segment after such keyword is ignored.",
        "We tested our approach against the work by Li et al. (2006) and TF-IDF on the same data set of course descriptions.",
        "The accuracy of our proposed approach is 72%, compared to 52% using Li et al. (2006), and 32% using TF-IDF.",
        "scores with human judgments.",
        "Since the transfer dictionary is always out of date, we found a few equivalent course pairs that were unintuitive.",
        "To make a more meaningful evaluation, we set up a human judgment data set.",
        "We gave 6 annotators (CS students and professors) a list of 32 pairs of courses, with only course titles and descriptions.",
        "They independently evaluated whether each pair is equivalent on a scale from 1 to 5.",
        "We averaged their evaluations for each pair and converted the scale from [1,5] to [0,1].",
        "Next, we ran our approach, the work by Li et al. (2006), and TF-IDF on the same 32 course pairs.",
        "Table 1 reports the Pearson's correlation coefficient of course relatedness scores with human judgment, and statistical significances.",
        "Our approach has a higher correlation to the human judgment data set compared to previ",
        "ous work.",
        "Furthermore, a smaller p-value indicates our approach is more likely to correlate with human judgment.",
        "During the experiment, we have found some misclassified categories in the wikidump.9 For example, ?Category:Software?",
        "has over 350 subcategories with names similar to ?Category:A-Class Britney Spears articles,?",
        "or ?Category:FA-Class Coca-Cola articles.?",
        "None of these appears in the Wikipedia website or the Wikipedia API10 as a subcategory of ?Category:Software.?",
        "More study is required on how they are formed."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "This paper presents a domain-specific algorithm to suggest equivalent courses based on analyzing their semantic relatedness using Wikipedia.",
        "Both accuracy and correlation suggest our approach outperforms previous work.",
        "Future work includes comparing our approach with ESA, experimenting on more courses from more universities, and adapting our work to courses in other languages."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors thank Dr. Karen M. Daniels for reviewing drafts of this paper.",
        "We also appreciate the insightful suggestions from Dr. Saif Mohammad at the early stage of our work.",
        "Last, but not least, we thank the reviewers for their comments that guided improvement of the contents of this paper."
      ]
    }
  ]
}
