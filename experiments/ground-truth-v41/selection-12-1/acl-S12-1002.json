{
  "info": {
    "authors": [
      "Razvan Bunescu"
    ],
    "book": "SemEval",
    "id": "acl-S12-1002",
    "title": "Adaptive Clustering for Coreference Resolution with Deterministic Rules and Web-Based Language Models",
    "url": "https://aclweb.org/anthology/S12-1002",
    "year": 2012
  },
  "references": [
    "acl-C90-3063",
    "acl-D08-1031",
    "acl-D08-1068",
    "acl-D09-1101",
    "acl-D09-1120",
    "acl-D10-1048",
    "acl-N04-1037",
    "acl-N06-1025",
    "acl-N07-1011",
    "acl-N10-2012",
    "acl-P05-1021",
    "acl-P11-1082",
    "acl-W11-1901",
    "acl-W11-1902"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a novel adaptive clustering model for coreference resolution in which the expert rules of a state of the art deterministic system are used as features over pairs of clusters.",
        "A significant advantage of the new approach is that the expert rules can be easily augmented with new semantic features.",
        "We demonstrate this advantage by incorporating semantic compatibility features for neutral pronouns computed from web n-gram statistics.",
        "Experimental results show that the combination of the new features with the expert rules in the adaptive clustering approach results in an overall performance improvement, and over 5% improvement in F1 measure for the target pronouns when evaluated on the ACE 2004 newswire corpus."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Coreference resolution is the task of clustering a sequence of textual entity mentions into a set of maximal non-overlapping clusters, such that mentions in a cluster refer to the same discourse entity.",
        "Coreference resolution is an important subtask in a wide array of natural language processing problems, among them information extraction, question answering, and machine translation.",
        "The availability of corpora annotated with coreference relations has led to the development of a diverse set of supervised learning approaches for coreference.",
        "While learning models enjoy a largely undisputed role in many NLP applications, deterministic models based on rich sets of expert rules for coreference have been shown recently to achieve performance rivaling, if not exceeding, the performance of state of the art machine learning approaches (Haghighi and Klein, 2009; Raghunathan et al., 2010).",
        "In particular, the top performing system in the CoNLL 2011 shared task (Pradhan et al., 2011) is a multi-pass system that applies tiers of deterministic coreference sieves from highest to lowest precision (Lee et al., 2011).",
        "The PRECISECONSTRUCTS sieve, for example, creates coreference links between mentions that are found to match patterns of apposition, predicate nominatives, acronyms, demonyms, or relative pronouns.",
        "This is a high precision sieve, correspondingly it is among the first sieves to be applied.",
        "The PRONOUN-MATCH sieve links an anaphoric pronoun with the first antecedent mention that agrees in number and gender with the pronoun, based on an ordering of the antecedents that uses syntactic rules to model discourse salience.",
        "This is the last sieve to be applied, due to its lower overall precision, as estimated on development data.",
        "While very successful, this deterministic multi-pass sieve approach to coreference can nevertheless be quite unwieldy when one seeks to integrate new sources of knowledge in order to improve the resolution performance.",
        "Pronoun resolution, for example, was shown by Yang et al. (2005) to benefit from semantic compatibility information extracted from search engine statistics.",
        "The semantic compatibility between candidate antecedents and the pronoun context induces a new ordering between the antecedents.",
        "One possibility for using compatibility scores in the deterministic system is to ignore the salience-based ordering and replace it with the new compatibility-based ordering.",
        "The draw",
        "back of this simple approach is that now discourse salience, an important signal in pronoun resolution, is completely ignored.",
        "Ideally, we would want to use both discourse salience and semantic compatibility when ranking the candidate antecedents of the pronoun, something that can be achieved naturally in a discriminative learning approach that uses the two rankings as different, but overlapping, features.",
        "Consequently, we propose an adaptive clustering model for coreference in which the expert rules are successfully supplemented by semantic compatibility features obtained from limited history web n-gram statistics."
      ]
    },
    {
      "heading": "2 A Coreference Resolution Algorithm",
      "text": [
        "From a machine learning perspective, the deterministic system of Lee et al. (2011) represents a trove of coreference resolution features.",
        "Since the deterministic sieves use not only information about a pair of mentions, but also the clusters to which they have been assigned so far, a learning model that utilized the sieves as features would need to be able to work with features defined on pairs of clusters.",
        "We therefore chose to model coreference resolution as the greedy clustering process shown in Algorithm 1.",
        "The algorithm starts by initializing the clustering C with a set of singleton clusters.",
        "Then, as long as the clustering contains more than one cluster, it repeatedly finds the highest scoring pair of",
        "clusters ?Ci, Cj?.",
        "If the score passes the threshold ?",
        "= f(?, ?",
        "), the clusters Ci, Cj are joined into one cluster and the process continues with another highest scoring pair of clusters.",
        "1: for i = 1 to n do 2: Ci ?",
        "{xi} 3: C ?",
        "{Ci}1?i?n 4: ?Ci, Cj?",
        "?",
        "argmax p?P(C) f(p) 5: while |C |> 1 and f(Ci, Cj) > ?",
        "do 6: replace Ci, Cj in C with Ci ?",
        "Cj 7: ?Ci, Cj?",
        "?",
        "argmax",
        "The scoring function f(Ci, Cj) is a linearly weighted combination of features ?",
        "(Ci, Cj) extracted from the cluster pair, parametrized by a weight vector w. The function P takes a clustering C as argument and returns a set of cluster pairs ?Ci, Cj?",
        "as follows: P(C)={?Ci, Cj?",
        "|Ci, Cj?C, Ci 6=Cj}?{?",
        "?, ??}",
        "P(C) contains a special cluster pair ?",
        "?, ?",
        "?, where ?",
        "(?, ?)",
        "is defined to contain a binary feature uniquely associated with this empty pair.",
        "Its corresponding weight is learned together with all other weights and will effectively function as a clustering",
        "Input: A dataset of training clusterings C; The number of training epochs T .",
        "Output: The averaged parameters w.",
        "1: w?",
        "0 2: for t = 1 to T do 3: for all C ?",
        "C do 4: w?",
        "UPDATE(C,w) 5: return w",
        "Algorithm 3 UPDATE(C,w) Input: A gold clustering C = {C1, C2, ..., Cm}; The current parameters w. Output: The updated parameters w.",
        "1: X ?",
        "C1 ?",
        "C2 ?",
        "... ?",
        "Cm = {x1, x2, ..., xn} 2: for i = 1 to n do 3: C?i ?",
        "{xi} 4: C?",
        "?",
        "{C?i}1?i?n 5: while |C?",
        "|> 1 do 6: ?C?i, C?j?",
        "= argmax",
        "10: w?",
        "w + ?",
        "(C?k, C?l)?",
        "?",
        "(Ci, Cj) 11: if ?C?i, C?j?",
        "= ?",
        "?, ??",
        "then 12: return w 13: replace C?i, C?j in C?",
        "with C?i ?",
        "C?j 14: return w",
        "Algorithms 2 and 3 show an incremental learning model for the weight vector w that is parametrized with the number of training epochs T and a set of training clusterings C in which each clustering contains the true coreference clusters from one document.",
        "Algorithm 2 repeatedly uses all true clusterings to update the current weight vector and instead of the last computed weights it returns an averaged weight vector to control for overfitting, as originally proposed by Freund and Schapire (1999).",
        "The core of the learning model is in the update procedure shown in Algorithm 3.",
        "Like the greedy clustering of Algorithm 1, it starts with an initial system clustering C?",
        "that contains all singleton clusters.",
        "At every step in the iteration (lines 5?13), it joins the highest scoring pair of clusters ?C?i, C?j?, computed according to the current parameters.",
        "The iteration ends when either the empty pair obtains the highest score or everything has been joined into only one cluster.",
        "The weight update logic is implemented in lines 7?",
        "10: if a more accurate pair ?C?k, C?l?",
        "can be found, the highest scoring such pair is used in the percep-tron update in line 10.",
        "If multiple cluster pairs obtain the maximum score in lines 6 and 9, the algorithm selects one of them at random.",
        "This is useful especially in the beginning, when the weight vector is zero and consequently all cluster pairs have the same score of 0.",
        "We define the goodness g(C?k, C?l|C) of a proposed pair ?C?k, C?l?",
        "with respect to the true clus-teringC as the accuracy of the coreference pairs that would be created if C?k and C?l were joined:",
        "It can be shown that this definition of the goodness function selects a cluster pair (lines 7?9) that, when joined, results in a clustering with a better pairwise accuracy.",
        "Therefore, the algorithm can be seen as trying to fit the training data by searching for parameters that greedily maximize the clustering accuracy, while overfitting is kept under control by computing an averaged version of the parameters.",
        "We have chosen to use a perceptron update for simplicity, but the algorithm can be easily instantiated to accommodate other types of incremental updates, e.g. MIRA (Crammer and Singer, 2003)."
      ]
    },
    {
      "heading": "3 Expert Rules as Features",
      "text": [
        "With the exception of mention detection which is run separately, all the remaining 12 sieves mentioned in (Lee et al., 2011) are used as Boolean features defined on cluster pairs, i.e. if any of the mention pairs in the cluster pair ?C?i, C?j?",
        "were linked by sieve k, then the corresponding sieve feature ?k(C?i, C?j) = 1.",
        "We used the implementation from the Stanford CoreNLP package1 for all sieves, with a modification for the PRONOUNMATCH sieve which was split into 3 different sieves as follows: ?",
        "ITPRONOUNMATCH: this sieve finds antecedents only for neutral pronouns it.",
        "?",
        "ITSPRONOUNMATCH: this sieve finds antecedents only for neutral possessive pronouns its.",
        "?",
        "OTHERPRONOUNMATCH: this is a catch-all sieve for the remaining pronouns.",
        "This 3-way split was performed in order to enable the combination of the discourse salience features captured by the pronoun sieves with the semantic compatibility features for neutral pronouns that will be introduced in the next section.",
        "The OTHERPRONOUNMATCH sieve works exactly as the original PRONOUNMATCH: for a given non-neutral pronoun, it searches in the current sentence and the previous 3 sentences for the first mention that agrees in gender and number with the pronoun.",
        "The candidate antecedents for the pronoun are ordered based on a notion of discourse salience that favors syntactic salience and document proximity (Raghunathan et al., 2010)."
      ]
    },
    {
      "heading": "4 Discourse Salience Features",
      "text": [
        "The IT/SPRONOUNMATCH sieves use the same implementation for finding the first matching candidate antecedent as the original PRONOUNMATCH.",
        "However, unlike OTHERPRONOUNMATCH and the other sieves that generate Boolean features, the neutral pronoun sieves are used to generate real valued features.",
        "If the neutral pronoun is the leftmost mention in the cluster C?j from a cluster pair ?C?i, C?j?, the corresponding normalized feature is computed as follows:",
        "of candidate mentions that precede the neutral pronoun and agree in gender and number with it, ordered from most salient to least salient.",
        "2.",
        "Let Ai ?",
        "C?i be the set of mentions in the cluster C?i that appear before the pronoun and agree with it.",
        "3.",
        "For each mention m ?",
        "Ai, find its rank in the",
        "4.",
        "Find the minimum rank across all the mentions in Ai and compute the feature as follows:",
        "If Ai is empty, set ?it/s(C?i, C?j) = 0.",
        "The discourse salience feature described above is by definition normalized in the interval [0, 1].",
        "It takes the maximum value of 1 when the most salient mention in the discourse at the current position agrees with the pronoun and also belongs to the candidate cluster.",
        "The feature is 0 when the candidate cluster does not contain any mention that agrees in gender and number with the pronoun."
      ]
    },
    {
      "heading": "5 Semantic Compatibility Features",
      "text": [
        "Each of the two types of neutral pronouns is associated with a new feature that computes the semantic compatibility between the syntactic head of a candidate antecedent and the context of the neutral pronoun.",
        "If the neutral pronoun is the leftmost mention in the cluster C?j from a cluster pair ?C?i, C?j?",
        "and cj is the pronoun context, then the new normalized features ?it/s(C?i, C?j) are computed as follows:",
        "1.",
        "Compute the maximum semantic similarity between the pronoun context and any mention in C?i that precedes the pronoun and is in agree",
        "2.",
        "Compute the maximum and minimum semantic similarity between the pronoun context and any mention that precedes the pronoun and is in agreement with it:",
        "To avoid numerical instability, if the overall maximum and minimum similarities are very close (Mall ?",
        "mall < 1e?4) we set ?it/s(C?i, C?j) = 1.",
        "Like the salience feature ?it/s, the semantic compatibility feature ?it/s is normalized in the interval [0, 1].",
        "Its definition assumes that we can compute comp(m, cj), the semantic compatibility between a candidate antecedent mention m and the pronoun context cj .",
        "For the possessive pronoun its, we extract the syntactic head h of the mention m and replace the pronoun with the mention head h in the possessive context.",
        "We use the resulting possessive pronoun context pcj(h) to define the semantic compatibility as the following conditional probability: comp(m, cj) = logP (pcj(h)|h) (5) = logP (pcj(h))?",
        "logP (h) To compute the n-gram probabilities P (pcj(h)) and",
        "els provided by the Microsoft Web N-Gram Corpus (Wang et al., 2010), as described in the next section.",
        "Figure 1 shows an example of a possessive neutral pronoun context, together with the set of candidate antecedents that agree in number and gender with the pronoun, from the current and previous 3 sentences.",
        "Each candidate antecedent is given an index that reflects its ranking in the discourse salience based ordering.",
        "We see that discourse salience does not help here, as the most salient mention is not the correct antecedent.",
        "The figure also shows the",
        "In 1946, the nine justices dismissed a case[7] involving the apportionment[8] of congressional districts.",
        "That view[6] would slowly change.",
        "In 1962, the court[3] abandoned its[5] caution[4].",
        "Finding remedies to the unequal distribution[1] of political power[2] was indeed within its constitutional authority.",
        "compatibility score computed for each candidate antecedent, using the formula described above.",
        "In this example, when ranking the candidate antecedents based on their compatibility scores, the top ranked mention is the correct antecedent, whereas the most salient mention is down in the list.",
        "When the set of candidate mentions contains pronouns, we require that they are resolved to a nominal or named mention, and use the head of this mention to instantiate the possessive context.",
        "This is the case of the pronominal mention [5] in Figure 1, which we assumed was already resolved to the noun court (even if the pronoun [5] were resolved to an incorrect mention, the noun court would still be ranked first due to mention [3]).",
        "This partial ordering between coreference decisions is satisfied automatically by setting the semantic compatibility feature",
        "C?i contains only pronouns.",
        "A similar feature is introduced for all neutral pronouns it appearing in subject-verb-object triples.",
        "The letter[5] appears to be an attempt[6] to calm the concerns of the current American administration[7].",
        "?I confirm my commitment[1] to the points made therein,?",
        "Aristide said in the letter[2], ?confident that they will help strengthen the ties between our two nations where",
        "The new pronoun context pcj(h) is obtained by replacing the pronoun it in the subject-verb-object context cj with the head h of the candidate antecedent mention.",
        "Figure 2 shows a neutral pronoun context, together with the set of candidate antecedents that agree in number and gender with the pronoun, from an abridged version of the original current and previous 3 sentences.",
        "Each candidate antecedent is given an index that reflects its ranking in the discourse salience based ordering.",
        "Discourse salience does not help here, as the most salient mention is not the correct antecedent.",
        "The figure shows the compatibility score computed for each candidate antecedent, using Equation 6.",
        "In this example, the top ranked mention in the compatibility based ordering is the correct antecedent, whereas the most most salient mention is at the bottom of the list.",
        "To summarize, in the last two sections we described two special features for neutral pronouns: the discourse salience feature ?it/s and the semantic compatibility feature ?it/s.",
        "The two real-valued",
        "Candidate mentions Original context N-gram context capital, store, GE, side, offer with its corporate tentacles reaching GE's corporate tentacles AOL, Microsoft, Yahoo, product its substantial customer base AOL's customer base regime, Serbia, state, EU, embargo meets its international obligations Serbia's international obligations company, secret, internet, FBI it was investigating the incident FBI was investigating the incident goal, team, realm, NHL, victory something it has not experienced since NHL has experienced Onvia, line, Nasdaq, rating said Tuesday it will cut jobs Onvia will cut jobs coalition, government, Italy but it has had more direct exposure Italy has had direct exposure Pinochet, arrest, Chile, court while it studied a judge 's explanation court studied the explanation",
        "features are computed at the level of cluster pairs as described in Equations 3 and 4.",
        "Their computation relies on the mention level rank (Equation 2) and semantic compatibility (Equation 6) respectively."
      ]
    },
    {
      "heading": "6 Web-based Language Models",
      "text": [
        "We used the Microsoft Web N-Gram Corpus2 to compute the pronoun context probability P (pcj(h)) and the candidate head probability P (h).",
        "This corpus provides smoothed back-off language models that are computed dynamically from N-gram statistics using the CALM algorithm (Wang and Li, 2009).",
        "The N-grams are collected from the tokenized versions of the billions of web pages indexed by the Bing search engine.",
        "Separate models have been created for the document body, the document title and the anchor text.",
        "In our experiments, we used the April 2010 version of the document body language models.",
        "The number of words in the pronoun context and the antecedent head determine the order of the language models used for estimating the conditional probabilities.",
        "For example, to estimate P (administration sent troops |administration), we used a trigram model for the context probability P (administration sent troops) and a unigram model for the head probability P (administration).",
        "Since the maximum order of the N-grams available in the Microsoft corpus is 5, we designed the context and head extraction rules to return N-grams with size at most 5.",
        "Table 1 shows a number of examples of N-grams generated from the original contexts, in which the pronoun was replaced with the correct antecedent.",
        "To get a sense of the utility of each context in matching the right antecedent, the table also",
        "shows a sample of candidate antecedents.",
        "For possessive contexts, the N-gram extraction rules use the head of the NP context and its closest premodifier whenever available.",
        "Using the pre-modifier was meant to increase the discriminative power of the context.",
        "For the subject-verb-object N-grams, we used the verb at the same tense as in the original context, which made it necessary to also include the auxiliary verbs, as shown in lines 4?7 in the table.",
        "Furthermore, in order to keep the generated N-grams within the maximum size of 5, we did not include modifiers for the subject or object nouns, as illustrated in the last line of the table.",
        "Some of the examples in the table also illustrate the limits of the context-based semantic compatibility feature.",
        "In the second example, all three company names are equally good matches for the possessive context.",
        "In these situations, we expect the discourse salience feature to provide the additional information necessary for extracting the correct antecedent.",
        "This combination of discourse salience with semantic compatibility features is done in the adaptive clustering algorithm introduced in Section 2."
      ]
    },
    {
      "heading": "7 Experimental Results",
      "text": [
        "We compare our adaptive clustering (AC) approach with the state of the art deterministic sieves (DT) system of Lee et al. (2011) on the newswire portion of the ACE-2004 dataset.",
        "The newswire section of the corpus contains 128 documents annotated with gold mentions and coreference information, where coreference is marked only between mentions that belong to one of seven semantic classes: person, organization, location, geopolitical entity, facility, vehicle, and weapon.",
        "This set of documents has been used before to evaluate coreference resolution sys",
        "Klein, 2009; Raghunathan et al., 2010), with the best results so far obtained by the deterministic sieve system of Lee at al.",
        "(2011).",
        "There are 11,398 annotated gold mentions, out of which 135 are possessive neutral pronouns its and 88 are neutral pronouns it in a subject-verb-object triple.",
        "Given the very small number of neutral pronouns, in order to obtain reliable estimates for the model parameters we tested the adaptive clustering algorithm in a 16 fold cross-validation scenario.",
        "Thus, the set of 128 documents was split into 16 folds, where each fold contains 120 documents for training and 8 documents for testing.",
        "The final results were pooled together from the 16 disjoint test sets.",
        "During training, the AC's update procedure was run for 10 epochs.",
        "Since the AC algorithm does not need to tune any hyper parameters, there was no need for development data.",
        "Table 2 shows the results obtained by the two systems on the newswire corpus under three evaluation scenarios.",
        "We use the B3 version of the precision (P), recall (R), and F1 measure, computed either on all mention pairs (all) or only on links that contain at least one neutral pronoun (neutral) marked as a mention in ACE.",
        "Furthermore, we report results on gold mentions (Gold) as well as on mentions extracted automatically (Auto).",
        "Since the number of neutral pronouns marked as gold mentions is small compared to the total number of mentions, the impact on the overall performance shown in the first two rows is small.",
        "However, when looking at coreference links that contain at least one neutral pronoun, the improvement becomes substantial.",
        "AC increases F1 with 5.3% when the mentions are extracted automatically during testing, a setting that reflects a more realistic use of the system.",
        "We have also evaluated the AC approach in the Gold setting using only the original DT sieves as features, obtaining an F1 of 80.3% for all mentions and 63.4% ?",
        "same as DT ?",
        "for neutral pronouns.",
        "By matching the performance of the DT system in the first two rows of the table, the AC system proves that it can successfully learn the relative importance of the deterministic sieves, which in (Raghunathan et al., 2010) and (Lee et al., 2011) have been manually ordered using a separate development dataset.",
        "Furthermore, in the DT system the sieves are applied on mentions in their textual order, whereas the adaptive clustering algorithm AC does not assume a predefined ordering among coreference resolution decisions.",
        "Thus, the algorithm has the capability to make the first clustering decisions in any section of the document in which the coreference decisions are potentially easier to make.",
        "We have run experiments in which the AC system was augmented with a feature that computed the normalized distance between a cluster and the beginning of the document, but this did not lead to an improvement in the results, lending further credence to the hypothesis that a strictly left to right ordering of the coreference decisions is not necessary, at least with the current features.",
        "The same behavior, albeit with smaller increases in performance, was observed when the DT and AC approaches were compared on the newswire section of the development dataset used in the CoNLL 2011 shared task (Pradhan et al., 2011).",
        "For these experiments, the AC system was trained on all 128 documents from the newswire portion of ACE 2004.",
        "On gold mentions, the DT and AC systems obtained a very similar performance.",
        "When evaluated only on links that contain at least one neutral pronoun, in a setting where the mentions were automatically detected, the AC approach improved the F1 measure over the DT system from 58.6% to 59.1%.",
        "One reason for the smaller increase in performance in the CoNLL experiments could be given by the different annotation schemes used in the two datasets.",
        "Compared to ACE, the CoNLL dataset does not include coreference links for appositives, predicate nominals or relative pronouns.",
        "The different annotation schemes may have led to mismatches in the training and test data for the AC system, which was trained on ACE and tested on CoNLL.",
        "While we tried to control for these conditions during the evaluation of the AC system, it is conceivable that the differ",
        "ences in annotation still had some effect on the performance of the AC approach.",
        "Another cause for the smaller increase in performance was that the pronominal contexts were less discriminative in the CoNLL data, especially for the neutral pronoun it.",
        "When evaluated only on links that contained at least one possessive neutral pronoun its, the improvement in F1 increased at 1.9%, as shown in Table 3."
      ]
    },
    {
      "heading": "8 Related Work",
      "text": [
        "Closest to our clustering approach from Section 2 is the error-driven first-order probabilistic model of Culotta et al. (2007).",
        "Among significant differences we mention that our model is non-probabilistic, simpler and easier to understand and implement.",
        "Furthermore, the update step does not stop after the first clustering error, instead the algorithm learns and uses a clustering threshold ?",
        "to determine when to stop during training and testing.",
        "This required the design of a method to order cluster pairs in which the clusters may not be consistent with the true coreference chains, which led to the introduction of the goodness function in Equation 1 as a new scoring measure for cluster pairs.",
        "The strategy of continuing the clustering during training as long as a an adaptive threshold is met better matches the training with the testing, and was observed to lead to better performance.",
        "The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.",
        "Compared to it, our adaptive clustering approach is less constrained: it uses only a weak, partial ordering between coreference decisions, and does not require a singleton cluster at every clustering step.",
        "This allows clustering to start in any section of the document where coreference decisions are easier to make, and thus create accurate clusters earlier in the process.",
        "The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al., 2011), and (Rahman and Ng, 2011).",
        "The focus in these studies has been on the semantic similarity between a mention and a candidate antecedent, or the parallelism between the semantic role structures in which the two appear.",
        "One of the earliest methods for using predicate-argument frequencies in pronoun resolution is that of Dagan and Itai (1990).",
        "Closer to our use of semantic compatibility features for pronouns are the approaches of Kehler et al. (2004) and Yang et al. (2005).",
        "The last work showed that pronoun resolution can be improved by incorporating semantic compatibility features derived from search engine statistics in the twin-candidate model.",
        "In our approach, we use web-based language models to compute semantic compatibility features for neutral pronouns and show that they can improve performance over a state-of-the-art coreference resolution system.",
        "The use of language models instead of search engine statistics is more practical, as they eliminate the latency involved in using search engine queries.",
        "Web-based language models can be built on readily avail"
      ]
    },
    {
      "heading": "9 Conclusion",
      "text": [
        "We described a novel adaptive clustering method for coreference resolution and showed that it can not only learn the relative importance of the original expert rules of Lee et al. (2011), but also extend them effectively with new semantic compatibility features.",
        "Experimental results show that the new method improves the performance of the state of the art deterministic system and obtains a substantial improvement for neutral pronouns when the mentions are extracted automatically."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the anonymous reviewers for their helpful suggestions.",
        "This work was supported by grant IIS-1018590 from the NSF.",
        "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the NSF."
      ]
    }
  ]
}
