{
  "info": {
    "authors": [
      "NIkos Malandrakis",
      "Elias Iosif",
      "Alexandros Potamianos"
    ],
    "book": "SemEval",
    "id": "acl-S12-1082",
    "title": "DeepPurple: Estimating Sentence Semantic Similarity using N-gram Regression Models and Web Snippets",
    "url": "https://aclweb.org/anthology/S12-1082",
    "year": 2012
  },
  "references": [
    "acl-C08-1107",
    "acl-H05-1079",
    "acl-I05-5003",
    "acl-J06-1003",
    "acl-J10-3003",
    "acl-J10-4006",
    "acl-J90-1003",
    "acl-N03-1033",
    "acl-P02-1040",
    "acl-P05-1045",
    "acl-P06-1114",
    "acl-P09-1089",
    "acl-P09-3004",
    "acl-W03-1604",
    "acl-W06-2501",
    "acl-W07-1401",
    "acl-W07-1407"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, and 3) sentence length.",
        "Lexical semantic similarity is computed via co-occurrence counts on a corpus harvested from the web using a modified mutual information metric.",
        "State-of-the-art results are obtained for semantic similarity computation at the word level, however, the fusion of this information at the sentence level provides only moderate improvement on Task 6 of SemEval?12.",
        "Despite the simple features used, regression models provide good performance, especially for shorter sentences, reaching correlation of 0.62 on the SemEval test set."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Recently, there has been significant research activity on the area of semantic similarity estimation motivated both by abundance of relevant web data and linguistic resources for this task.",
        "Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009).",
        "Word-or term-level STS (a special case of sentence level STS) has also been successfully applied to the problem of grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011).",
        "In this work, we built on previous research on word-level semantic similarity estimation to design and implement a system for sentence-level STS for Task6 of the SemEval?12 campaign.",
        "Semantic similarity between words can be regarded as the graded semantic equivalence at the lexeme level and is tightly related with the tasks of word sense discovery and disambiguation (Agirre and Edmonds, 2007).",
        "Metrics of word semantic similarity can be divided into: (i) knowledge-based metrics (Miller, 1990; Budanitsky and Hirst, 2006) and (ii) corpus-based metrics (Baroni and Lenci, 2010; Iosif and Potamianos, 2010).",
        "When more complex structures, such as phrases and sentences, are considered, it is much harder to estimate semantic equivalence due to the non-compositional nature of sentence-level semantics and the exponential explosion of possible interpretations.",
        "STS is closely related to the problems of paraphrasing, which is bidirectional and based on semantic equivalence (Madnani and Dorr, 2010) and textual entailment, which is directional and based on relations between semantics (Dagan et al., 2006).",
        "Related methods incorporate measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005).",
        "Measures from machine translation evaluation are often used to evaluate lexical level approaches (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric based on word n-gram hit rates.",
        "Motivated by BLEU, we use n-gram hit rates and word-level semantic similarity scores as features in",
        "a linear regression model to estimate sentence level semantic similarity.",
        "We also propose sigmoid scaling of similarity scores and sentence-length dependent modeling.",
        "The models are evaluated on the SemEval?12 sentence similarity task."
      ]
    },
    {
      "heading": "2 Semantic similarity between words",
      "text": [
        "In this section, two different metrics of word similarity are presented.",
        "The first is a language-agnostic, corpus-based metric requiring no knowledge resources, while the second metric relies on WordNet.",
        "Corpus-based metric: Given a corpus, the semantic similarity between two words, wi and wj , is estimated as their pointwise mutual information (Church and Hanks, 1990): I(i, j) = log p?(i,j)p?(i)p?",
        "(j) , where p?",
        "(i) and p?",
        "(j) are the occurrence probabilities of wi and wj , respectively, while the probability of their co-occurrence is denoted by p?",
        "(i, j).",
        "These probabilities are computed according to maximum likelihood estimation.",
        "The assumption of this metric is that co-occurrence implies semantic similarity.",
        "During the past decade the web has been used for estimating the required probabilities (Turney, 2001; Bollegala et al., 2007), by querying web search engines and retrieving the number of hits required to estimate the frequency of individual words and their co-occurrence.",
        "However, these approaches have failed to obtain state-of-the-art results (Bollegala et al., 2007), unless ?expensive?",
        "conjunctive AND queries are used for harvesting a corpus and then using this corpus to estimate similarity scores (Iosif and Potamianos, 2010).",
        "Recently, a scalable approach1 for harvesting a corpus has been proposed where web snippets are downloaded using individual queries for each word (Iosif and Potamianos, 2012b).",
        "Semantic similarity can then be estimated using the I(i, j) metric and within-snippet word co-occurrence frequencies.",
        "Under the maximum sense similarity assumption (Resnik, 1995), it is relatively easy to show that a (more) lexically-balanced corpus2 (as the one cre",
        "words can be estimated as the minimum pairwise similarity of their senses.",
        "The gist of the argument is that although words often co-occur with their closest senses, word occurrences cor-ated above) can significantly reduce the semantic similarity estimation error of the mutual information metric I(i, j).",
        "This is also experimentally verified in (Iosif and Potamianos, 2012c).",
        "In addition, one can modify the mutual information metric to further reduce estimation error (for the theoretical foundation behind this see (Iosif and Potamianos, 2012a)).",
        "Specifically, one may introduce exponential weights ?",
        "in order to reduce the contribution of p(i) and p(j) in the similarity metric.",
        "The modified metric Ia(i, j), is defined as:",
        "The weight ?",
        "was estimated on the corpus of (Iosif and Potamianos, 2012b) in order to maximize word sense coverage in the semantic neighborhood of each word.",
        "The Ia(i, j) metric using the estimated value of ?",
        "= 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002).",
        "For more details see (Iosif and Potamianos, 2012a).",
        "WordNet-based metrics: For comparison purposes, we evaluated various similarity metrics on the task of word similarity computation on three standard datasets (same as above).",
        "The best results were obtained by the Vector metric (Patwardhan and Pedersen, 2006), which exploits the lexical information that is included in the WordNet glosses.",
        "This metric was incorporated to our proposed approach.",
        "All metrics were computed using the Word-Net::Similarity module (Pedersen, 2005)."
      ]
    },
    {
      "heading": "3 N-gram Regression Models",
      "text": [
        "Inspired by BLEU (Papineni et al., 2002), we propose a simple regression model that combines evidence from two sources: number of n-gram matches and degree of similarity between non-matching words between two sentences.",
        "In order to incorporate a word semantic similarity metric into BLEU, we apply the following two-pass process: first lexical hits are identified and counted, and then the semantic similarity between n-grams not matched durrespond to all senses, i.e., the denominator of I(i, j) is overestimated causing large underestimation error for similarities between polysemous words.",
        "ing the first pass is estimated.",
        "All word similarity metrics used are peak-to-peak normalized in the [0,1] range, so they serve as a ?degree-of-match?.",
        "The semantic similarity scores from word pairs are summed together (just like n-gram hits) to obtain a BLEU-like semantic similarity score.",
        "The main problem here is one of alignment, since we need to compare each non-matched n-gram from the hypothesis with an n-gram from the reference.",
        "We use a simple approach: we iterate on the hypothesis n-grams, left-to-right, and compare each with the most similar non-matched n-gram in the reference.",
        "This modification to BLEU is only applied to 1-grams, since semantic similarity scores for bi-grams (or higher) were not available.",
        "Thus, our list of features are the hit rates obtained by BLEU (for 1-, 2-, 3-, 4-grams) and the total semantic similarity (SS) score for 1-grams3.",
        "These features are then combined using a multiple linear regression model:",
        "where D?L is the estimated similarity, Bn is the BLEU hit rate for n-grams, M1 is the total semantic similarity score (SS) for non-matching 1-grams and an are the trainable parameters of the model.",
        "Motivated by evidence of cognitive scaling of semantic similarity scores (Iosif and Potamianos, 2010), we propose the use of a sigmoid function to scale DL sentence similarities.",
        "We have also observed in the SemEval data that the way humans rate sentence similarity is very much dependent on sentence length4.",
        "To capture the effect of length and cognitive scaling we propose next two modifications to the linear regression model.",
        "The sigmoid fusion scheme is described by the following equation:",
        "where we assume that sentence length l (average 3Note that the features are computed twice on each sentence in a forward and backward fashion (where the word order is reversed), and then averaged between the two runs.",
        "4We speculate that shorter sentences are mostly compared at the lexical level using the short-term memory language buffers, while longer sentences tend to be compared at a higher cognitive level, where the non-compositional nature of sentence semantics dominate.",
        "length for each sentence pair, in words) acts as a scaling factor for the linearly estimated similarity.",
        "The hierarchical fusion scheme is actually a collection of (overlapping) linear regression models, each matching a range of sentence lengths.",
        "For example, the first model DL1 is trained with sentences with length up to l1, i.e., l ?",
        "l1, the second model DL2 up to length l2 etc.",
        "During testing, sentences with length l ?",
        "[1, l1] are decoded with DL1, sentences with length l ?",
        "(l1, l2] with model DL2 etc.",
        "Each of these partial models is a linear fusion model as shown in (2).",
        "In this work, we use four models with l1 = 10, l2 = 20, l3 = 30, l4 =?."
      ]
    },
    {
      "heading": "4 Experimental Procedure and Results",
      "text": [
        "Initially all sentences are preprocessed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.",
        "The exact type of preprocessing used depends on the metric used.",
        "For the plain lexical BLEU, we use lemmatization, stemming (of lemmas) and remove all non-content words, keeping only nouns, adjectives, verbs and adverbs.",
        "For computing semantic similarity scores, we don't use stemming and keep only noun words, since we only have similarities between non-noun words.",
        "For the computation of semantic similarity we have created a dictionary containing all the single-word nouns included in WordNet (approx.",
        "60K) and then downloaded snippets of the 500 top-ranked documents for each word by formulating single-word queries and submitting them to the Yahoo!",
        "search engine.",
        "Next, results are reported in terms of correlation between the automatically computed scores and the ground truth, for each of the corpora in Task 6 of SemEval?12 (paraphrase, video, europarl, WordNet, news).",
        "Overall correlation (?Ovrl?)",
        "computed on the join of the dataset, as well as, average (?Mean?)",
        "correlation across all task is also reported.",
        "Training is performed on a subset of the first three corpora and testing on all five corpora.",
        "Baseline BLEU: The first set of results in Table 1, shows the correlation performance of the plain BLEU hit rates (per training data set and overall/average).",
        "The best performing hit rate is the one",
        "mance of the modified version of BLEU that incorporates various word-level similarity metrics is shown in Table 2.",
        "Here the BLEU hits (exact matches) are summed together with the normalized similarity scores (approximate matches) to obtain a single B1+M1 (Purple) score5.",
        "As we can see, there are definite benefits to using the modified version, particularly with regards to mean correlation.",
        "Overall the best performers, when taking into account both mean and overall correlation, are the WordNet-based and Ia metrics, with the Ia metric winning by a slight margin, earning a place in the final models.",
        "formance of the various regression models (fusion schemes) is investigated.",
        "Each regression model is evaluated by performing 10-fold cross-validation on the SemEval training set.",
        "Correlation performance is shown in Table 3 both with and without semantic similarity.",
        "The baseline in this case is the Purple metric (corresponding to no fusion).",
        "Clearly the use of regression models significantly improves performance compared to the 1-gram BLEU and Purple baselines for almost all datasets, and especially for the combined dataset (overall).",
        "Among the fusion schemes, the hierarchical models perform the best.",
        "Following fusion, the performance gain from incorporating semantic similarity (SS) is much smaller.",
        "Finally, in Table 4, correlation performance of our submissions on the official SemEval test set is 5It should be stressed that the plain BLEU unigram scores shown in this table are not comparable to those in Table 1, since here scores are calculated over only the nouns of each sentence.",
        "shown.",
        "The overall correlation performance of the Hierarchical model ranks somewhere in the middle (43rd out of 89 systems), while the mean correlation (weighted by number of samples per set) is notably better: 23rd out of 89.",
        "Comparing the individual dataset results, our systems underperform for the two datasets that originate from the machine translation (MT) literature (and contain longer sentences), while we achieve good results for the rest (19th for paraphrase, 37th for video and 29th for WN)."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "We have shown that: 1) a regression model that combines counts of exact and approximate n-gram matches provides good performance for sentence similarity computation (especially for short and medium length sentences), 2) the non-linear scaling of hit-rates with respect to sentence length improves performance, 3) incorporating word semantic similarity scores (soft-match) into the model can improve performance, and 4) web snippet corpus creation and the modified mutual information metric is a language agnostic approach that can (at least) match semantic similarity performance of the best resource-based metrics for this task.",
        "Future work, should involve the extension of this approach to model larger lexical chunks, the incorporation of compositional models of meaning, and in general the phrase-level modeling of semantic similarity, in order to compete with MT-based systems trained on massive external parallel corpora."
      ]
    }
  ]
}
