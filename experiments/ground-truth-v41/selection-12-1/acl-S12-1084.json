{
  "info": {
    "authors": [
      "Zhu Tiantian",
      "Lan Man"
    ],
    "book": "SemEval",
    "id": "acl-S12-1084",
    "title": "Tiantianzhu7:System Description of Semantic Textual Similarity (STS) in the SemEval-2012 (Task 6)",
    "url": "https://aclweb.org/anthology/S12-1084",
    "year": 2012
  },
  "references": [
    "acl-C10-2048",
    "acl-P94-1019"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper briefly reports our submissions to the Semantic Textual Similarity (STS) task in the SemEval 2012 (Task 6).",
        "We first use knowledge-based methods to compute word semantic similarity as well as Word Sense Disambiguation (WSD).",
        "We also consider word order similarity from the structure of the sentence.",
        "Finally we sum up several aspects of similarity with different coefficients and get the sentence similarity score."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The task of semantic textual similarity (STS) is to measure the degree of semantic equivalence between two sentences.",
        "It plays an increasingly important role in several text-related research and applications, such as text mining, Web page retrieval, automatic question-answering, text summarization, and machine translation.",
        "The goal of the Semeval 2012 STS task (task 6) is to build a unified framework for the evaluation of semantic textual similarity modules for different systems and to characterize their impact on NLP applications.",
        "Generally, there are two ways to measure similarity of two sentences, i.e, corpus-based methods and knowledge-based methods.",
        "The corpus-based method typically computes sentence similarity based on the frequency of word occurrence or the co-occurrence between collocated words.",
        "For example, in (Islam and Inkpen, 2008) they proposed a corpus-based sentence similarity measure as a function of string similarity, word similarity and common word order similarity (CWO).",
        "The knowledge-based method computes sentence similarity based on the semantic information collected from knowledge bases.",
        "With the aid of a number of successful computational linguistic projects, many semantic knowledge bases are readily available, for example, WordNet, Spatial Date Transfer Standard, Gene Ontology, etc.",
        "Among them, the most widely used one is WordNet, which is organized by meanings and developed at Princeton University.",
        "Several methods computed word similarity by using WordNet, such as the Lesk method in (Banerjee and Pedersen, 2003), the lch method in (Leacock and Chodorow, 1998)and the wup method in (Wu and Palmer, 1994).",
        "Generally, although the knowledge-based methods heavily depend on the knowledge bases, they performed much better than the corpus-based methods in most cases.",
        "Therefore, in our STS system, we use a knowledge-based method to compute word similarity.",
        "The rest of this paper is organized as follows.",
        "Section 2 describes our system.",
        "Section 3 presents the results of our system."
      ]
    },
    {
      "heading": "2 System Description",
      "text": [
        "Usually, a sentence is composed of some nouns, verbs, adjectives, adverbs and/or some stop words.",
        "We found that these words carry a lot of information, especially the nouns and verbs.",
        "Although the adjectives and adverbs also make contribution to the semantic meaning of the sentence, they are much weaker than the nouns and verbs.",
        "So we consider to measure the sentence semantic similarities from three aspects.",
        "We define the following three types of similarity from two compared sentences to measure",
        "the semantic similarity: (1) Noun Similarity to measure the similarity between the nouns from the two compared sentences, (2) Verb Similarity to measure the similarity between Verbs, (3) ADJ-ADV Similarity to measure the similarity between the adjectives and adverbs from each sentence.",
        "Besides the semantic information similarity, we also found that the structure of the sentences carry some information which cannot be ignored.",
        "Therefore, we define the last aspect of the sentence similarity as Word Order Similarity.",
        "In the following we will introduce the different components of our system."
      ]
    },
    {
      "heading": "2.1 POS",
      "text": [
        "As a basic natural language processing technique, part of speech tagging is to identify the part of speech of individual words in the sentence.",
        "In order to compute the three above semantic similarities, we first identify the nouns, verbs, adjectives, and adverbs in the sentence.",
        "Then we can calculate the Noun Similarity, Verb Similarity and ADJ-ADV Similarity from two sentences."
      ]
    },
    {
      "heading": "2.2 Semantic similarity between words",
      "text": [
        "The word similarity measurement have important impact on the performance of sentence similarity.",
        "Currently, many lexical resources based approaches perform comparatively well to compute semantic word similarities.",
        "However, the exact resources they are based are quite different.",
        "For example, some are based on dictionary and/or thesaurus, and others are based on WordNet.",
        "WordNet is a machine-readable lexical database.",
        "The words in Wordnet are classified into four categories, i.e., nouns, verbs, adjectives and adverbs.",
        "WordNet groups these words into sets of synonyms called synsets, provides short definitions, and records the various semantic relations between these synsets.",
        "The synsets are interlinked by means of conceptual-semantic and lexical relations.",
        "WordNet alo provides the most common relationships include Hyponym/Hypernym (i.e., is-a relationships) and Meronym/Holonym (i.e., part-of relationships).",
        "Nouns and verbs are organized into hierarchies based on the hyponymy/hypernym relation between synsets while adjectives and adverbs are not.",
        "In this paper, we adopt the wup method in (Wu and Palmer, 1994) to estimate the semantic similarity between two words, which estimates the semantic similarity between two words based on the depth of the two words in WordNet and the depth of their least common subsumer (LCS), where LCS is defined as the common ancestor deepest in the taxonomy.",
        "For example, given two words, w1 and w2, the semantic similarity s(w1,w2) is the function of their depth in the taxonomy and the depth of their least common subsumer.",
        "If d1 and d2 are the depth of w1 and w2 in WordNet, and h is the depth of their least common subsumer in WordNet, the semantic similarity can be written as:"
      ]
    },
    {
      "heading": "2.3 Word Sense Disambiguation Word Sense Disambiguation (WSD) is to identify",
      "text": [
        "the actual meaning of a word according to the context.",
        "In our word similarity method, we take the nearest meaning of two words into consideration rather than their actual meaning.",
        "More importantly, the nearest meaning does not always represent the actual meaning.",
        "In our system, we used a WSD algorithm proposed by (Ted Pedersen et al.,2005), which computes semantic relatedness of word senses using extended gloss overlaps of their dictionary definitions.",
        "We utilize this WSD algorithm for each sentence to get the actual meaning of each word before computing the word semantic similarity."
      ]
    },
    {
      "heading": "2.4 Semantic Similarity",
      "text": [
        "We adopt a similar way to compute the three types of semantic similarities.",
        "Here we take Noun Similarity as an example.",
        "Suppose sentence s1 and s2 are the two sentences to be compared, s1 has a nouns while s2 has b nouns.",
        "Then we get a ?",
        "b noun pairs and use the word similarity method mentioned in section 2.2 to compute the Noun Similarity of each noun pair.",
        "After that, for each noun, we choose its highest score in noun pairs as its similarity score.",
        "Then we use the formula below to compute the Noun Similarity.",
        "where c represents the number of noun words in sequence a and sequence b, c = min(a, b); ni represents the highest matching similarity score of i-th word in the shorter sequence with respect to one of the words in the longer sequence; and",
        "resents the sum of the highest matching similarity score between the words in sequence a and sequence b.",
        "Similarly, we can get SimV erb.",
        "Since there is no Hyponym/Hypernym relation for adjectives and adverbs in WordNet, we just compute ADJ-ADV Similarity based on the frequency of overlap of simple words."
      ]
    },
    {
      "heading": "2.5 Word Order Similarity",
      "text": [
        "We believe that word order information also make contributions to sentence similarity.",
        "In most cases, the longer common sequence (LCS) the two sentences have, the higher similarity score the sentences get.",
        "For example the pair of sentences s1 and s2, we remove all the punctuation from the sentences: ?",
        "s1: But other sources close to the sale said Vivendi was keeping the door open to further bids and hoped to see bidders interested in individual assets team up ?",
        "s2: But other sources close to the sale said Vivendi was keeping the door open for further bids in the next day or two Since the length of the longest common sequence is 14, we use the following formula to compute the word order similarity.",
        "where the shorter length means the length of the shorter sentence."
      ]
    },
    {
      "heading": "2.6 Overall Similarity",
      "text": [
        "After we have the Noun Similarity, Verb Similarity, ADJ-ADV Similarity and Word Order Similarity, we calculate the Overall Similarity of two compared sentences based on these four scores of similarity.",
        "We combine them in the following way:",
        "Where a, b, c and d are the coefficients which denote the contribution of each aspect to the overall sentence similarity, For different data collections, we empirically set different coefficients, for example, for the MSR Paraphrase data, the four coefficients are set as 0.5, 0.3, 0.1, 0.1, because it is hard to get the highest score 5 even when the two sentences are almost the same meaning, We empirically set a threshold, if the score exceeds the threshold we set the score 5."
      ]
    },
    {
      "heading": "3 Experiment and Results on STS",
      "text": [
        "Firstly, Stanford parser1 is used to parse each sentence and to tag each word with a part of speech(POS).",
        "Secondly, WordNet SenseRelate All-Words2, a WSD tool from CPAN is used to disambiguate and to assign a sense for each word based on the assigned POS.",
        "We submitted three runs: run 1 with WSD, run 2 without WSD, run 3 removing stop words and without WSD.",
        "The stoplist is available online3.",
        "Table 1 lists the performance of these three systems as well as the baseline and the rank 1 results on STS task in SemEval 2012.",
        "We can see that run1 gets the best result, which means WSD has improved the accuracy of sentence similarity.",
        "Run3 gets better result than run2, which proves that stop words do disturb the computation of sentence similarity, removing them is a better choice in our system."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "In our work, we adopt a knowledge-based word similarity method with WSD to measure the semantic similarity between two sentences from four aspects: Noun Similarity, Verb Similarity, ADJ-ADV Similarity and Word Order Similarity.",
        "The results show that WSD improves the pearson coefficient at some degree.",
        "However, our system did not get a good rank.",
        "It indicates there still exists many problems such as wrong POS tag and wrong WSD which might lead to wrong meaning of one word in a sentence."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors would like to thank the organizers for their invaluable support making STS a first-rank and interesting international event."
      ]
    }
  ]
}
