{
  "info": {
    "authors": [
      "Torsten Zesch",
      "Jens Haase"
    ],
    "book": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP",
    "id": "acl-W12-2036",
    "title": "HOO 2012 Shared Task: UKP Lab System Description",
    "url": "https://aclweb.org/anthology/W12-2036",
    "year": 2012
  },
  "references": [
    "acl-A97-1025",
    "acl-C04-1024",
    "acl-E06-1015",
    "acl-P96-1010"
  ],
  "sections": [
    {
      "text": [
        "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 302?306, Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics HOO 2012 Shared Task: UKP Lab System Description Torsten Zesch??",
        "and Jens Haase?"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe the UKP Lab system participating in the HOO 2012 Shared Task on preposition and determiner error correction.",
        "Our focus was to implement a highly flexible and modular system which can be easily augmented by other researchers.",
        "The system might be used to provide a level playground for subsequent shared tasks and enable further progress in this important research field on top of the state of the art identified by the shared task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "UKP Lab already participated in the previous HOO Shared Task in 2011.",
        "Our knowledge-based system (Zesch, 2011) was targeted towards detecting real-word spelling errors, but performed also well on a number of other error classes.1 However, it was not competitive for article and preposition errors where supervised systems based on confusion sets constitute the state of the art.",
        "Thus, we tailor the HOO 2011 system towards correcting article and preposition errors, but also implement a supervised approach based on confusion sets (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001).",
        "We decided to implement a basic system that should be as flexible as possible and might serve as a basis for experiments in future rounds of the shared task.",
        "We also plan to model the most successful",
        "systems in our framework as soon as the system descriptions are made available.2 This might provide a level playground for subsequent shared tasks and enable real progress in this important field on top of the state of the art identified by the HOO shared task."
      ]
    },
    {
      "heading": "2 Supervised Error Detection",
      "text": [
        "We implement a generic framework for article and preposition error detection based on the open-source DKPro framework.3 DKPro is a collection of software components for natural language processing based on the Apache UIMA framework (Ferrucci and Lally, 2004).",
        "It comes with a collection of ready-made modules which can be combined to form more complex applications.",
        "Our goal is to develop a system which is as flexible as possible with respect to (i) linguistic preprocessing, (ii) the extraction of features, and (iii) the applied classification method.",
        "We will make the source code publicly available as part of the DKPro infrastructure and hope that this will lower the obstacles for participating in future rounds of the HOO Shared Task.",
        "We also provide a reference implementation of the HOO 2012 experiments based on the DKPro Lab framework (Eckart de Castilho and Gurevych, 2011) which enables (i) parameter sweeping, (ii) modeling of interdependent tasks (like e.g. training and test cycles), (iii) generating performance reports, and (iv) storing all experimental results in a convenient manner."
      ]
    },
    {
      "heading": "2.1 Linguistic Preprocessing",
      "text": [
        "For our basic implementation, we only use a few preprocessing steps.",
        "We tokenize and sentence split the data with the default DKPro segmenter, and then use TreeTagger (Schmid, 2004) to POS-tag and chunk the sentences.",
        "However, the framework allows the effortless addition of other preprocessing components, e.g. parsing or named-entity recognition."
      ]
    },
    {
      "heading": "2.2 Feature Extraction",
      "text": [
        "We implement a generic feature extraction process based on the ClearTK project (Ogren et al., 2008).",
        "ClearTK provides a set of highly flexible feature extractors that access the annotations (e.g. POS tags, chunks, etc.)",
        "created by the linguistic preprocessing.",
        "One important decision during training is to decide which instances should be used for feature extraction.",
        "In the simplest setting, each token is used to generate an instance, but this would result in a very high number of negative instances for every positive instance.",
        "For the error classes RT/UT and RD/UD, a more balanced distribution of instances can be easily enforced by only creating a positive instance if the token equals an element in the corresponding confusion set.",
        "We create a negative instance by removing or changing the article/preposition.",
        "For articles, we use the confusion set: {a, an, the, this}4 For prepositions, we use the confusion set: {as, at, but, by, for, from, in, of, on, out, over, since, than, to, up, with} The confusion set is a parameter to the feature extraction method and can be changed easily.",
        "This also makes it possible to apply the framework to other error classes, e.g. for correcting frequently confused words like (accept, except) or (than, then).",
        "Table 1 lists the set of basic features implemented in the reference system.",
        "As our goal was to implement a highly flexible system, we put more effort in the overall architecture than in the feature engineering.",
        "N-gram features are computed based on the 4In the official runs, an was not part of the confusion set, but was specially handled in a post-processing step.",
        "In the current version of the framework, we removed this heuristic and now treat an as a normal part of the confusion set.",
        "Google Web1T n-gram corpus (Brants and Franz, 2006) which is accessed using jWeb1T.5 The listed features can be improved in many ways, e.g. the chunk feature could also encode the type of the chunk.",
        "As the framework allows to easily add new feature extractors, we are going to integrate the most successful features from the shared task.",
        "Due to the modular architecture of ClearTK, the implemented feature extractors could even be reused for other classification tasks unrelated to spelling correction."
      ]
    },
    {
      "heading": "2.3 Classification",
      "text": [
        "ClearTK provides a wide range of adapters to well known machine learning frameworks and classification tools.",
        "As of April 2012, the following adapters are supported:",
        "?",
        "Weka11 (Hall et al., 2009) As we can easily switch the classifier, we tried a wide range of classifiers, but SVM worked generally best.",
        "For the official runs, we used SVM as implemented in the Weka toolkit with the parameter ?BuildLogisticModels?",
        "which allows to base a detection decision on the confidence of the classifier in order to improve precision."
      ]
    },
    {
      "heading": "3 Knowledge-based Error Detection",
      "text": [
        "Besides the supervised system described above, we also apply our knowledge-based system from the HOO 2011 Pilot Round (Zesch, 2011).",
        "We reimplemented two state-of-the-art approaches: the",
        "2005) and the statistical approach (Mays et al., 1991; Wilcox-OHearn et al., 2008).",
        "Both approaches measure the contextual fitness of a word and the surrounding context.",
        "For that purpose, the knowledge-based approach computes the semantic relatedness of a target word with all other words in a certain context window.",
        "This approach is not suitable for correcting article or preposition errors, as these word classes are not linked to the context via lexical-semantic relations.",
        "Thus, we only use the statistical approach that computes the probability of a sentence based on a n-gram language model.",
        "We use the Google Web1T n-gram data (Brants and Franz, 2006).",
        "Although being generally applicable to article and preposition errors, the statistical approach needs some adaptations in order to achieve acceptable performance.",
        "In the original definition, the approach computes the probability of all alternative sentences where the target word is replaced with a word from the vocabulary that has low edit distance to the target word.",
        "This results in a very high false detection rate.",
        "Thus, we (i) limit detections to positions where an article or preposition is already present, and (ii) select the substitution candidate not from all tokens with low edit distance to the original token, but only from the appropriate confusion set.",
        "As the statistical approach is purely based on n-gram frequencies, while this is only one feature of the supervised approach, we expect the supervised approach to outperform our adapted knowledge-based system by a wide margin."
      ]
    },
    {
      "heading": "4 Experimental Setup",
      "text": [
        "We model all experiment pipelines in the previously described framework.",
        "As training data, we use the publicly available Brown corpus (Francis W. Nelson and Kuc?era, 1964), but limit training to 3,700 randomly selected sentences in order to speed up the training process."
      ]
    },
    {
      "heading": "4.1 Unofficial Runs",
      "text": [
        "Due to technical problems, we were not able to submit all runs in time.",
        "We therefore report also unofficial runs which we evaluated on the test data that was available for participants for a limited amount of time.12 Although we did not tailor the unofficial runs in any way towards the test data, they have certainly a different status than the official runs.",
        "We do not consider this as a major problem, as our basic 12HOO 2012 test data was subject to a strict license and needed to be deleted after the evaluation period.",
        "feature set is not competitive with the best performing systems anyway.",
        "We implemented two baseline systems, one for articles and one for prepositions.",
        "The baselines replace every occurrence of an article/preposition with the most frequent article/preposition from the confusion set (the for articles, of for prepositions).",
        "We also apply the adapted HOO 2011 statistical approach in two versions as described above: one adapted towards articles, and one adapted towards prepositions.",
        "Finally, we use the new framework for supervised error correction based on the basic feature set described above with two classifiers: Naive Bayes and SVM as implemented in the Weka toolkit version 3.7.5.",
        "We treat the correction task as a multi-class problem and only target the error classes RD, RT, UD, UT.",
        "The remaining error classes MD and MT (missing articles and prepositions) are more challenging, as it is less obvious how to create good training data from a non-error annotated corpus."
      ]
    },
    {
      "heading": "4.2 Official Runs",
      "text": [
        "The three runs that were officially submitted are also based on the SVM implementation in Weka, but we applied the parameter ?BuildLogisticModels?",
        "which allows to base a detection decision on the confidence of the classifier in order to improve precision.",
        "We tuned parameters on the training data and report three runs for the threshold combinations (?RD, ?RT ) = (0.95, 0.8), (0.8, 0.7), and (0.5, 0.3)."
      ]
    },
    {
      "heading": "5 Results",
      "text": [
        "Table 2 summarizes the results of all runs.",
        "As expected, the basic feature set used in our experiments is not competitive with the top-performing systems in the shared task.13 However, some observations can be made from the relative differences between the scores.",
        "The thresholds applied in the official runs are not working as expected, as precision is not influenced, while recall drops a lot.",
        "The HOO 2011 system based on the statistical approach performs quite well for prepositions, but not for articles.",
        "Its performance is comparable to the supervised runs, but this is only due to the limited feature set used in our experiment.",
        "As mentioned above, our focus was to implement a highly flexible and modular system for supervised error correction which can be easily augmented by other researchers.",
        "We plan to model the most successful systems in our framework as soon as the system descriptions are made available, and we invite other participating teams to help with this effort.",
        "The system might provide a level playground for subsequent shared tasks and enable further progress in this important field of research."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "for detection, 35% for recognition, and 28% for correction.",
        "See (Dale et al., 2012) for an overview of the results."
      ]
    }
  ]
}
