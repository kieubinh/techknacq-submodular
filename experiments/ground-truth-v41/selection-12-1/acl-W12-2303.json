{
  "info": {
    "authors": [
      "Jianqiang Ma",
      "Dale Gerdemann"
    ],
    "book": "Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology",
    "id": "acl-W12-2303",
    "title": "Phrase-Based Approach for Adaptive Tokenization",
    "url": "https://aclweb.org/anthology/W12-2303",
    "year": 2012
  },
  "references": [
    "acl-C04-1081",
    "acl-C08-5001",
    "acl-I05-3017",
    "acl-J01-3002",
    "acl-J05-1003",
    "acl-J96-1002",
    "acl-J96-3004",
    "acl-J97-4004",
    "acl-P07-2018",
    "acl-P08-1102",
    "acl-W03-1705",
    "acl-W03-1728",
    "acl-W05-1506",
    "acl-W06-0115",
    "acl-W10-4126"
  ],
  "sections": [
    {
      "text": [
        "segmentation bakeoffs (Emerson, 2005; Levow, 2006; Zhao and Liu, 2010).",
        "The tokenization model has advantages in simplicity and efficiency, as the basic operation in segmentation is string matching with linear time complexity to the sentence length and it only needs a dictionary thus requires no training as in the character classification model, which can easily have millions of features and require hundreds of iterations in the training phase.",
        "On the other hand, it has inferior performance, caused by its poor OOV induction ability.",
        "This work proposes a framework called phrase-based tokenization as a generalization of the tokenization model to cope with its deficiencies in OOV recognition, while preserving its advantages of simplicity and efficiency, which are important for adaptive word segmentation.",
        "The segmentation hypothesis unit is extended from a word to a phrase, which is a character string of arbitrary length, i.e. combinations of partial and/or complete words.",
        "And the statistics of different tokenizations of the same phrase are collected and used for parameters estimation, which leads to a linear time model construction procedure.",
        "This extension makes hypothesis units capable of capturing richer context and describing morphological behavior of characters, which improves OOV recognition.",
        "Moreover, overlapping hypothesis units can be combined once certain consistency conditions are satisfied, which avoids the unrealistic assumption of independence among the tokenizations of neighboring phrases.",
        "Phrase-based tokenization decomposes the sentence tokenization into phrase tokenizations.",
        "We use a graph called phrase tokenization lattice to represent all the hypotheses of phrase tokenization in a given sentence.",
        "Under such a formulation, tokenizing a sentence is transformed to the shortest path search problem on the graph, which can be efficiently solved by dynamic programming techniques similar to the Viterbi (1967) algorithm.",
        "2 Phrase-Based Model The hypothesis unit of the tokenization model is the word, i.e. it selects the best word sequence from all the words that can be matched by substrings of the sentence (usually in a greedy manner).",
        "Once a word is chosen, the corresponding boundaries are determined.",
        "This implies that as the characters in a word are always considered as a whole, the morphological behavior of an individual character, e.g. the distribution of its positions in words, is ignored thus makes it impossible to model the word formation process and recognize OOV.",
        "Critical tokenization (Guo, 1997) suggests a method of discovering all and only unambiguous token boundaries (critical points) and generating longest substrings with all inner positions ambiguous (critical fragments) under the assumption of complete dictionary.",
        "Then an example-based method using the context can be adopted to disambiguate the tokenization of critical fragments (Hu et al. 2004).",
        "However, the complete dictionary assumption is not realistic in practice, as the word formation is so dynamic and productive that there is no dictionary that is even close to the complete lexicon.",
        "Given the presence of OOV, a word, including a monosyllabic word, in the original dictionary may be a substring, i.e. a partial word, of an OOV.",
        "In this case, the critical points found by the dictionary are not guaranteed to be unambiguous.",
        "As the complete dictionary does not exist as a static object, a possible solution is to make a dynamic dictionary, which induces words on the fly.",
        "But this will not be discussed in this paper.",
        "Instead, we attempt to generalize the tokenization model to work without the complete dictionary.",
        "Different from making distinctions of critical fragments and ?non-critical?",
        "fragments in critical tokenization, we suggest using phrases to represent potentially ambiguous fragments of sentences in a unified way.",
        "We define a phrase as a substring of a sentence, the boundaries of which, depending on the tokenization, may or may not necessarily match word boundaries.",
        "The fact that partial words, including single characters, may appear on both ends of a phrase makes it possible to describe ?morphemes in the context?",
        "for OOV induction.",
        "A consequence of introducing phrase in tokenization is that a manually segmented corpus is needed in order to collect phrases.",
        "2.1 Tokenization Tokenization is the process of separating words or word-like units from sentences or character strings.",
        "We can consider sentence tokenization as a mapping from each position in the sentence to a",
        "binary value, which indicates the presence (denoted as #) or the absence of word boundary (denoted as $) at that position.",
        "A specific tokenization realization of a sentence can be represented by a list of binary values, which can be generated by the concatenations of its sub-lists.",
        "In other words, a tokenization of a given sentence can be represented as the concatenation of the tokenizations of its component phrases.",
        "If we assume that the tokenization of a phrase is independent of other phrases in the same sentence, the sentence tokenization problem is decomposed to smaller phrase tokenization problems, which are unrelated to each other.",
        "The independency assumption is not necessarily true but in general is a good approximation.",
        "We take this assumption by default, unless there exists evidence that suggests otherwise.",
        "In that case, we introduce a method called backward dependency match to fix the problem, which will be discussed in Section 3.3.",
        "2.2 Phrase Tokenization Lattice Informally a phrase tokenization lattice, or lattice in short, is a set of hypothesized tokenization of phrases in the given sentence, which is a compact representation of all the possible tokenization for that sentence.",
        "Using the notations in Mohri (2002), we formally define a lattice as a weighted directed graph < V , E > with a mapping",
        "are connected with a node.",
        "The weight for the path !",
        "can be defined as: w ( ! )",
        "= !i=1k w ( ei ) (1) which is the product of the weights of its component edges.",
        "A path from the source node to the sink node, represents a tokenization of the sentence being factored as the concatenation of tokenizations of phrases represented by those edges of on that path.",
        "For example, with some edges being pruned, the lattice for the sentence ?????",
        "?Someone questions him?",
        "is shown in Figure 1.",
        "(2) where D is the set of all paths from the source node to the sink node, and T !",
        "is the path with the highest weight, which represents the best tokenization of the sentence.",
        "Intuitively, we consider the product of phrase tokenization probabilities as the probability of the sentence tokenization that is generated from the concatenation of these phrase tokenizations.",
        "Note that every edge in the lattice is from a node represents an earlier sentence position to a node that represents a later one.",
        "In other words, the lattice is acyclic and has a clear topological order.",
        "In this case, the best path can be found using the Viterbi (1967) algorithm efficiently2.",
        "3 Training and Inference Algorithms"
      ]
    },
    {
      "heading": "3.1 Model Training In order to use the lattice to tokenize unseen sentences, we first have to build a model that can generate the edges and their associated weight, i.e. the tokenization of all the possible phrases and their corresponding phrase tokenization probability. We do it by collecting all the phrases that have occurred in a training corpus and use maximum likelihood estimation (MLE) to estimate the phrase tokenization probabilities. The estimation of the probability that a particular phrase A = a",
      "text": [
        "The result of the MLE estimation is stored in a data structure called phase tokenization table, from which one can retrieval all the possible tokenizations with their corresponding probabilities for the every phrase that has occurred in the training corpus.",
        "With this model, we can construct the lattice, i.e. determine the set of edges E and the mapping function W (defining nodes is trivial) for a given sentence in a simple string matching and table retrieval manner: when a substring of sentence is matched to a stored phrase, an edge is built from the its starting and ending node to represent a tokenization of that phrase, with the weight of the edge equals to the MLE estimation of the stored phrase-tokenization pair.",
        "3.2 Simple Dynamic Programming Once the model is built, we can tokenize a given sentence by the inference on the lattice which represents that sentence.",
        "The proposed simple dynamic programming algorithm (Algorithm 1, as 2 More rigid mathematical descriptions of this family of problems and generic algorithms based on semirings are discussed in Mohri (2002) and Huang (2008).",
        "shown in Figure 2) can be considered as the phrase tokenization lattice version of the evalUtterance algorithm in Venkataraman (2001).",
        "The best tokenization of the partial sentence up to a certain position is yielded by the best combination of one previous best tokenization and one of the phrase tokenizations under consideration at the current step.",
        "The upper bound of the time complexity of Algorithm 1 is O ( k n 2 ) , where n is the sentence length and k is the maximum number of the possible tokenization for a phrase.",
        "But in practice, it is neither necessary nor possible (due to data sparseness) to consider phrases of arbitrary length, so we set a constraint of maximum phrase length of about 10, which makes the time complexity de-facto linear.",
        "and path p 2 = e2 e5!e7 .",
        "Note that p 2 recognizes the word??",
        "?question?",
        "by combining two partial words, even though the word itself has not seen before.",
        "Of course, this OOV is finally recognized only if a path that can yield it is the best path found by the decoding algorithm.",
        "Once the best path is found, the procedure of mapping it back to segmented words is as follows.",
        "The phrase tokenizations represented by the edges of the best path are concatenated, before substituting meta symbols # and $ into white space and empty string, respectively.",
        "For example, if p 2 = e2 e5!e7 is the best path, the concatenation of the phrase tokenizations of the three edges on the path will be #?#?##?$$?#?#, and removal of $ and substitution of # into the white space will further transform it into ?",
        "?",
        "??",
        "?",
        "?Somebody questions him?, which is the final result of the algorithm.",
        "3.3 Compatibility and Backward Dependency Match As mentioned in Section 2, the independency assumption of phrase tokenization is not always true.",
        "Considering the example in Figure 1, e",
        "and and e",
        "are not really compatible, as e",
        "represents a word while e",
        "represents a partial word that expects the suffix of its preceding phrase to form a word with its prefix.",
        "To solve this problem, we require that the last (meta) symbol of the preceding tokenization must equal to the first (meta) symbol of the following tokenization in order to concatenate the two.",
        "This, however, has the consequence that there may be no valid tokenization at all for some positions.",
        "As a result, we have to maintain the top k hypotheses and use the k-best path search algorithms instead of 1-best (Mohri, 2002).",
        "We adopt the naive k-best path search, but it is possible to use more advanced techniques (Huang and Chiang, 2005).",
        "The compatibility problem is just the most salient example of the general problem of variable independency assumptions, which is the \"unigram model\" of phrase tokenization.",
        "A natural extension is a higher order Markov model.",
        "But that is inflexible, as it assumes a fixed variable dependency structure (the current variable is always dependent on previous n variables).",
        "So we propose a method called backward dependency match, in which we start from the independency assumption, then try to explore the longest sequence of adjacent dependencies that we can reach via string match for a given phrase and its precedent.",
        "To simplify the discussion, we use sequence labeling, or conditional probability notation of the tokenization.",
        "A tokenization of the given character sequence (sentence) is represented as a specific label sequence of same length.",
        "The label can be those in the standard 4-tag set of word segmentation (Huang and Zhao, 2007) or the #/$ labels indicating the presence or absence of a word boundary after a specific character.",
        "The possible tokenizations of character sequence",
        "In this case, the factorization of the tokenization",
        "greatly, which leads to the differences in F-score.",
        "In general both Algorithm 1 and Algorithm 2 improves OOV Recall significantly, compared with the baseline algorithm, maximum matching, which has barely any OOV recognition capacity.",
        "This confirms the effectiveness of the proposed phrase-based model in modeling morphological behaviors of characters.",
        "Moreover, Algorithm 2 works consistently better than Algorithm 1, which suggests the usefulness of its strategy of dealing with dependencies among phrase tokenizations.",
        "Besides, the proposed method has the linear training and testing (when setting a maximum phrase length) time complexity, while the training complexity of CRF is the proportional to the feature numbers, which are often over millions.",
        "Even with current prototype, our method takes only minutes to build the model, in contrast with several hours that CRF segmenter needs to train the model for the same corpus on the same machine.",
        "Admittedly, our model still underperforms the best systems in the bakeoff.",
        "This may be resulted from that 1) our system is still a prototype that ignores many minor issues and lack optimization and 2) as a generative model, our model may suffer more from the data sparseness problem, compared with discriminative models, such as CRF.",
        "As mentioned earlier, the OOV recognition is the dominant factor that influences the overall accuracy.",
        "Different from the mechanism of tokenization combination in our approach, state-of-art systems such as those based on MaxEnt or CRF, achieve OOV recognition basically in the same way as in-dictionary word recognition.",
        "The segmentation is modeled as assigning labels to characters.",
        "And the probability of the label assignment for a character token is mostly determined by its features, which are usually local contexts in the form of character co-occurrences.",
        "There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.",
        "For example, the Sproat et al. (1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.",
        "Another typical example is Ma and Chen (2003), which proposed context free grammar like rules together with a recursive bottom-up merge algorithm that merges possible morphemes after an initial segmentation using maximum matching.",
        "It would be fairer to compare the OOV recognition performance of our approach with these methods, rather than maximum matching.",
        "But most earlier works are not evaluated on standard bake-off corpora and the implementations are not openly available, so it is difficult to make direct comparisons.",
        "F-score As CityU MSR PKU",
        "tokenization model by considering the phrase instead of the word as the segmentation hypothesis unit, which is capable of describing ?morphemes in the context?",
        "and improves the OOV recognition performance significantly.",
        "Our approach decomposes sentence tokenization into phrase tokenizations.",
        "The final tokenization of the sentence is determined by finding the best combination of the tokenizations of phrases that cover the whole sentence.",
        "The tokenization hypotheses of a sentence are represented by a weighed directed acyclic graph called phrase tokenization lattice.",
        "Using this formalism, the sentence tokenization problem becomes a shortest path search problem on the graph.",
        "In our model, one only needs to estimate the phrase tokenization probabilities in order to segment new sentences.",
        "The training is thus a linear time phrase extraction and maximum likelihood estimation procedure.",
        "We adopted a Viterbi-style dynamic programming algorithm to segment unseen sentences using the lattice.",
        "We also proposed a method called backward dependency match to model the dependencies of adjacent phrases to overcome the limitations of the assumption that tokenizations of neighboring phrases is independent.",
        "The experiment showed the effectiveness of the proposed phrase-based model in recognizing out-of-vocabulary words and its superior overall performance compared with the traditional tokenization model.",
        "It has both the efficiency of the tokenization model and the high performance of the character classification model.",
        "One possible extension of the proposed model is to apply re-ranking techniques (Collins and Koo, 2005) to the k-best list generated by Algorithm 2.",
        "A second improvement would be to combine our model with other models in a log linear way as in Jiang et al. (2008).",
        "Since phrase-based tokenization is a model that can be accompanied by different training algorithms, it is also interesting to see whether discriminative training can lead to better performance.",
        "Acknowledgments The research leading to these results has received funding from the European Commission's 7th Framework Program under grant agreement n?",
        "238405 (CLARA)."
      ]
    },
    {
      "heading": "References Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1992. A Maximum Entropy Approach to Natural Language Processing. 1996. Computational Linguistics, 22(1): 39-71 Michael Collins and Terry Koo. 2005. Discriminative Reranking for Natural Language Parsing. Computational Linguistics, 31(1):25-69. Thomas Emerson. 2005. The second international Chi nese word segmentation bakeoff. In Proceedings of Forth SIGHAN Workshop on Chinese Language Processing. Jeju Island, Korea. Jin Guo. 1997. Critical tokenization and its properties. Computational Linguistics, 23(4): 569-596 Qinan Hu, Haihua Pan, and Chunyu Kit. 2004. An example-based study on Chinese word segmentation using critical fragments. In Proceedings of IJCNLP-2004. Hainan Island, China Changning Huang and Hai Zhao. 2007. Chinese Word Segmentation: a Decade Review. Journal of Chinese Information Processing, 21(3): 8-20 Chu-Ren Huang, Petr Simon, Shu-Kai Hsieh, and Laurent Pr?vot. Rethinking Chinese word segmentation: tokenization, character classification, or wordbreak identification. In Proceedings of ACL-2007. Prague, Czech Liang Huang. 2008. Advanced dynamic programming in semiring and hypergraph frameworks. In Proceedings of COLING 2008. Manchester, UK. Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technology. Vancouver, Canada Wenbin Jiang, Liang Huang, Qun Liu, Yajuan Lu. 2008. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. In Proceedings of ACL 2008: HLT. Columbus, USA John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001. Williamstown, MA, USA Gina-Anne Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing. Sydney, Australia",
      "text": []
    }
  ]
}
