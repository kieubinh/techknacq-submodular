{
  "info": {
    "authors": [
      "Ariya Rastrow",
      "Sanjeev Khudanpur",
      "Mark Dredze"
    ],
    "book": "Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? on the Future of Language Modeling for HLT",
    "id": "acl-W12-2707",
    "title": "Revisiting the Case for Explicit Syntactic Information in Language Models",
    "url": "https://aclweb.org/anthology/W12-2707",
    "year": 2012
  },
  "references": [
    "acl-D07-1111",
    "acl-D09-1087",
    "acl-D09-1116",
    "acl-J93-2004",
    "acl-P05-1063",
    "acl-W11-0328"
  ],
  "sections": [
    {
      "text": [
        "texts used for Jelinek-Mercer smoothing in the SLM.",
        "The coefficients ?fm(pi?i) are estimated on a held-out set using the bucketing algorithm suggested by Bahl (1983), which ties ?fm(pi?i)'s based on the count of fm(pi?i)'s in the training data.",
        "We use the expected count of the features from the last iteration of EM training, since the pi?i are latent states.",
        "We perform the bucketing algorithm for each level f1, f2, ?",
        "?",
        "?",
        ", fM of equivalence classification separately, and estimate the bucketed ?c(fm) using the Baum-Welch algorithm (Baum, 1972) to maximize the likelihood of held out data, where the word probability assignment in Eq.",
        "1 is replaced with:",
        "pg(pij?i|W?i).",
        "The hierarchy shown in Figure 4 is used1 for obtaining a smooth estimate pJM(?|?)",
        "at each level."
      ]
    },
    {
      "heading": "5 SLM Experiments",
      "text": [
        "We train a dependency SLM for two different tasks, namely Broadcast News (BN) and Wall Street Journal (WSJ).",
        "Unlike Section 3.2, where we swept through multiple training sets of multiple sizes,",
        "gressive in that it drops both the tag and headword from the history.",
        "However, in many cases the headword's tag alone is sufficient, suggesting a more gradual interpolation.",
        "Keeping the headtag adds more specific information and at the same time is less sparse.",
        "A similar idea is found, e.g., in the back-off hierarchical class n-gram language model (Zitouni, 2007) where instead of backing off from the n-gram right to the (n ?",
        "1)- gram a more gradual backoff ?",
        "by considering a hierarchy of fine-to-coarse classes for the last word in the history?",
        "is used.",
        "training the SLM is computationally intensive.",
        "Yet, useful insights may be gained from the 40M word case.",
        "So we choose the source of text most suitable for each task, and proceed as follows."
      ]
    },
    {
      "heading": "5.1 Setup",
      "text": [
        "The following summarizes the setup for each task: ?",
        "BN setup : EARS BN03 corpus, which has about 42M words serves as our training text.",
        "We also use rt04 (45K words) as our evaluation data.",
        "Finally, to interpolate our structured language models with the baseline 4-gram model, we use rt03+dev04f (about 40K words) data sets to serve as our development set.",
        "The vocabulary we use in BN experiments has about 84K words.",
        "?",
        "WSJ setup : The training text consists of about 37M words.",
        "We use eval92+eval93 (10K words) as our evaluation set and dev93 (9K words) serves as our development set for interpolating SLMs with the baseline 4-gram model.",
        "In both cases, we sample about 20K sentences from the training text (we exclude them from training data) to serve as our heldout data for applying the bucketing algorithm and estimating ??s.",
        "To apply the dependency parser, all the data sets are first converted to Treebank-style tokenization and POS-tagged using the tagger of (Tsuruoka et al., 2011)2.",
        "Both the POS-tagger and the shift-reduce dependency parser are trained on the Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and the WSJ Penn Treebank (after converting them to dependency trees) which consists of about 1.2M tokens.",
        "Finally, we train a modified kneser-ney 4-gram LM on the tokenized training text to serve as our baseline LM, for both experiments."
      ]
    },
    {
      "heading": "5.2 Results and Analysis",
      "text": [
        "Table 2 shows the perplexity results for BN and WSJ experiments, respectively.",
        "It is evident that the 4 gram baseline for BN is stronger than the 40M case of Table 1.",
        "Yet, the interpolated SLM significantly improves over the 4-gram LM, as it does for WSJ.",
        "and WSJ tasks.",
        "Also, to show that, in fact, the syntactic dependencies modeled through the SLM parameterization is enhancing predictive power of the LM in the problematic regions, i.e. syntactically-distant positions, we calculate the following (log) probability ratio for each position in the test data,",
        "where pKN+SLM is the word probability assignment of the interpolated SLM at each position, and pKN(wi) is the probability assigned by the baseline 4-gram model.",
        "The quantity above measures the improvement (or degradation) gained as a result of using the SLM parameterization3.",
        "Figures 5(a) and 5(b) illustrate the histogram of the above probability ratio for all the word positions in evaluation data of BN and WSJ tasks, respectively.",
        "In these figures the histograms for syntactically-distant and syntactically-local are shown separately to measure the effect of the SLM for either of the position types.",
        "It can be observed in the figures that for both tasks the percentage of positions with log pKN+SLM(wi|W?i)pKN(wi|W?i) around zero is much higher for syntactically-local (blue bars) than the syntactically-distant (red bars).",
        "To confirm this, we calculate the average log pKN+SLM(wi|W?i)pKN(wi|W?i) ?this is the average log-likelihood improvement, which is directly 2007) are excluded.",
        "3If log pKN+SLM(wi|W?i)pKN(wi|W?i) is greater than zero, then the SLM has a better predictive power for word position wi.",
        "This is a meaningful comparison due to the fact that the probability assignment using both SLM and n-gram is a proper probability (which sums to one over all words at each position).",
        "model for (a) BN task (b) WSJ task.",
        "related to perplexity improvement?",
        "for each position type in the figures.",
        "Table 3, reports the perplexity performance of each LM (baseline 4-gram, SLM and interpolated SLM) on different positions of the evaluation data for BN and WSJ tasks.",
        "As it can be observed from this table, the use of long-span dependencies in the SLM partially fills the gap between the performance of the baseline 4-gram LM on syntactically-distant positionsN versus syntactically-local positionsM.",
        "In addition, it can be seen that the SLM by itself fills the gap substantially, however, due to its underlying parameterization which is based on Jelinek-Mercer smoothing it has a worse performance on regular syntactically-local positions (which account for the majority of the positions) compared to the Kneser-Ney smoothed LM4.",
        "Therefore, to improve the overall performance, the interpolated SLM takes advantage of both the better modeling performance",
        "the 4-gram LM, SLM and their interpolation.",
        "The SLM has lower perplexity than the 4-gram in syntactically distant positions N , and has a smaller discrepancy PPLNPPLM between preplexity on the distant and local predictions, complementing the 4-gram model.",
        "the better features included in the SLM for improving predictive power on syntactically-distant positions."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "The results of Table 1 and Figure 2 suggest that predicting the next word is about 50% more difficult when its syntactic dependence on the history reaches beyond n-gram range.",
        "They also suggest that this difficulty does not diminish with increasing training data size.",
        "If anything, the relative difficulty of word positions with nonlocal dependencies relative to those with local dependencies appears to increase with increasing training data and n-gram order.",
        "Finally, it appears that language models that exploit long-distance syntactic dependencies explicitly at positions where the n-gram is least effective are beneficial as complementary models.",
        "Tables 2 and 3 demonstrates that a particular, recently-proposed SLM with such properties improves a 4-gram LM trained on a large corpus."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Thanks to Kenji Sagae for sharing his shift-reduce dependency parser and the anonymous reviewers for helpful comments."
      ]
    }
  ]
}
