{
  "info": {
    "authors": [
      "Eraldo Fernandes",
      "Cìcero dos Santos",
      "Ruy Milidiú"
    ],
    "book": "Joint Conference on EMNLP and CoNLL – Shared Task",
    "id": "acl-W12-4502",
    "title": "Latent Structure Perceptron with Feature Induction for Unrestricted Coreference Resolution",
    "url": "https://aclweb.org/anthology/W12-4502",
    "year": 2012
  },
  "references": [
    "acl-P05-1012",
    "acl-P08-1074",
    "acl-W02-1001",
    "acl-W06-2932",
    "acl-W11-1902",
    "acl-W11-1906",
    "acl-W12-4501"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe a machine learning system based on large margin structure perceptron for unrestricted coreference resolution that introduces two key modeling techniques: latent corefer-ence trees and entropy guided feature induction.",
        "The proposed latent tree modeling turns the learning problem computationally feasible.",
        "Additionally, using an automatic feature induction method, we are able to efficiently build nonlinear models and, hence, achieve high performances with a linear learning algorithm.",
        "Our system is evaluated on the CoNLL2012 Shared Task closed track, which comprises three languages: Arabic, Chinese and English.",
        "We apply the same system to all languages, except for minor adaptations on some language dependent features, like static lists of pronouns.",
        "Our system achieves an official score of 58.69, the best one among all the competitors."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The CoNLL-2012 Shared Task (Pradhan et al., 2012) is dedicated to the modeling of coreference resolution for multiple languages.",
        "The participants are provided with corpora for three languages: Arabic, Chinese and English.",
        "These corpora are provided by the OntoNotes project and, besides accurate anaphoric coreference information, contain various annotation layers such as part-of-speech (POS) tagging, syntax parsing, named entities (NE) and semantic role labeling (SRL).",
        "The shared task consists in the automatic identification of coreferring mentions of entities and events, given predicted information on other OntoNotes layers.",
        "We propose a machine learning system for coreference resolution that is based on the large margin structure perceptron algorithm (Collins, 2002; Fernandes and Milidiu?, 2012).",
        "Our system learns a predictor that takes as input a set of candidate mentions in a document and directly outputs the clusters of coreferring mentions.",
        "This predictor comprises an optimization problem whose objective is a function of the clustering features.",
        "To embed classic cluster metrics in this objective function is practically infeasible since most of such metrics lead to NP-hard optimization problems.",
        "Thus, we introduce coreference trees in order to represent a cluster by a directed tree over its mentions.",
        "In that way, the prediction problem optimizes over trees instead of clusters, which makes our approach computationally feasible.",
        "Since coreference trees are not given in the training data, we assume that these structures are latent and use the latent structure perceptron (Fernandes and Brefeld, 2011; Yu and Joachims, 2009) as the learning algorithm.",
        "To provide high predicting power features to our model, we use entropy guided feature induction (Fernandes and Milidiu?, 2012).",
        "By using this technique, we automatically generate several feature templates that capture coreference specific local context knowledge.",
        "Furthermore, this feature induction technique extends the structure perceptron framework by providing an efficient general method to build strong nonlinear classifiers.",
        "Our system is evaluated on the CoNLL-2012 Shared Task closed track and achieves the scores",
        "54.22, 58.49 and 63.37 on Arabic, Chinese and English test sets, respectively.",
        "The official score ?",
        "the mean over the three languages ?",
        "is 58.69, which is the best score achieved in the shared task.",
        "The remainder of this paper is organized as follows.",
        "In Section 2, we present our machine learning modeling for the unrestricted coreference resolution task.",
        "In Section 3, we present the corpus preprocessing steps.",
        "The experimental findings are depicted in Section 4 and, in Section 5, we present our final remarks."
      ]
    },
    {
      "heading": "2 Task Modeling",
      "text": [
        "Coreference resolution consists in identifying mention clusters in a document.",
        "We split this task into two subtasks: mention detection and mention clustering.",
        "For the first subtask, we apply the strategy proposed in (dos Santos and Carvalho, 2011).",
        "The second subtask requires a complex output.",
        "Hence, we use a structure learning approach that has been successfully applied to many similar structure finding NLP tasks (Collins, 2002; Tsochantaridis et al., 2005; McDonald et al., 2006; Fernandes and Brefeld, 2011; Fernandes and Milidiu?, 2012)."
      ]
    },
    {
      "heading": "2.1 Mention Detection",
      "text": [
        "For each text document, we generate a list of candidate mentions using the strategy of (dos Santos and Carvalho, 2011).",
        "The basic idea is to use all noun phrases, and, additionally, pronouns and named entities, even if they are inside larger noun phrases.",
        "We do not include verbs as mentions."
      ]
    },
    {
      "heading": "2.2 Mention Clustering",
      "text": [
        "In the mention clustering subtask, a training instance (x,y) consists of a set of mentions x from a document and the correct coreferring clusters y.",
        "The structure perceptron algorithm learns a predictor from a given training set D = {(x,y)} of correct input-output pairs.",
        "More specifically, it learns the weight vector w of the parameterized predictor given by",
        "s(y?",
        ";w), where Y(x) is the set of clusterings over mentions x and s is a w-parameterized scoring function over clusterings.",
        "We use the large margin structure perceptron (Fernandes and Milidiu?, 2012) that, during training, embeds a loss function in the prediction problem.",
        "Hence, it uses a loss-augmented predictor given by",
        "where ` is a non-negative loss function that measures how a candidate clustering y?",
        "differs from the ground truth y.",
        "The training algorithm makes intense use of the predictor, hence the prediction problem must be efficiently solved.",
        "Letting s be a classic clustering metric is infeasible, since most of such metrics lead to NP-hard optimization problems.",
        "In order to reduce the complexity of the prediction problem, we introduce coreference trees to represent clusters of coreferring mentions.",
        "A coreference tree is a directed tree whose nodes are the coreferring mentions and arcs represent some coreference relation between mentions.",
        "In Figure 1, we present a document with seven highlighted mentions comprising two clusters.",
        "One plausible coreference tree for the cluster {a1,a2,a3,a4} is presented in Figure 2.",
        "North Koreaa1 opened itsa2 doors to the U.S. today, welcoming Secretary of State Madeleine Albrightb1 .",
        "Sheb2 says herb3 visit is a good start.",
        "The U.S. remains concerned about North Korea?sa3 missile development program and itsa4 exports of missiles to Iran.",
        "mentions comprising two clusters: {a1,a2,a3,a4} and {b1,b2,b3}.",
        "The letter in the mention subscript indicates its cluster and the number uniquely identifies the mention within the cluster.",
        "We are not concerned about the semantics underlying coreference trees, since they are just auxiliary",
        "structures for the clustering task.",
        "However, we argue that this concept is linguistically plausible, since there is a dependency relation between coreferring mentions.",
        "Observing the aforementioned example, one may agree that mention a3 (North Korea?s) is indeed more likely to be associated with mention a1 (North Korea) than with mention a2 (its), even considering that a2 is closer than a1 in the text.",
        "For a given document, we have a forest of coreference trees, one tree for each coreferring cluster.",
        "However, for the sake of simplicity, we link the root node of every coreference tree to an artificial root node, obtaining the document tree.",
        "In Figure 3, we depict a document tree for the text in Figure 1.",
        "the text in Figure 1.",
        "Dashed lines indicate artificial arcs.",
        "Coreference trees are not given in the training data.",
        "Thus, we assume that these structures are latent and make use of the latent structure perceptron (Fernandes and Brefeld, 2011; Yu and Joachims, 2009) to train our models.",
        "We decompose the original predictor into two predictors, that is",
        "where the latent predictor Fh(x) is defined as argmaxh?H(x)?w,?",
        "(x,h)?,H(x) is the set of feasible document trees for x and ?",
        "(x,h) is the joint feature vector representation of mentions x and document tree h. Hence, the latent predictor finds a maximum scoring rooted tree over the given mentions x, where a tree score is given by a linear function over its features.",
        "Fy(h) is a straightforward procedure that creates a cluster for each subtree connected to the artificial root node in the document tree h. In Figure 4, we depict the proposed latent structure perceptron algorithm for the mention clustering task.",
        "Like its univariate counterpart (Rosenblatt,",
        "1957), the structure perceptron is an online algorithm that iterates through the training set.",
        "For each training instance, it performs two major steps: (i) a prediction for the given input using the current model; and (ii) a model update based on the difference between the predicted and the ground truth outputs.",
        "The latent structure perceptron performs an additional step to predict the latent ground truth h?",
        "using a specialization of the latent predictor and the current model.",
        "This algorithm learns to predict document trees that help to solve the clustering task.",
        "Thereafter, for an unseen document x, the predictor Fh(x) and the learned model w are employed to produce a predicted document tree h which, in turn, is fed to Fy(h) to give the predicted clusters.",
        "Golden coreference trees are not available.",
        "However, during training, for a given input x, we have the golden clustering y.",
        "Thus, we predict the constrained document tree h?",
        "for the training instance",
        "(x,y) using a specialization of the latent predictor ?",
        "the constrained latent predictor ?",
        "that makes use of y.",
        "The constrained predictor finds the maximum scoring document tree among all rooted trees of x that follow the correct clustering y, that is, rooted",
        "trees that only include arcs between mentions that are coreferent according to y, plus one arc from the artificial node to each cluster.",
        "In that way, the constrained predictor optimizes over a subset H(x,y) contained in H(x) and, moreover, it guarantees that",
        "Fy(h?)",
        "= y, for any w. The constrained tree is used as the ground truth on each iteration.",
        "Therefore, the model update is determined by the difference between the constrained document tree and the document tree predicted by the ordinary predictor.",
        "The loss function measures the impurity in the predicted document tree.",
        "In our modeling, we use a simple loss function that just counts how many predicted edges are not present in the constrained document tree.",
        "For the arcs from the artificial root node, we use a different loss value.",
        "We set that through the parameter r, which we call the root loss value.",
        "We decompose the joint feature vector ?",
        "(x,h) along tree edges, that is, pairs of candidate coreferring mentions.",
        "This approach is similar to previous structure learning modelings for dependency parsing (McDonald et al., 2005; Fernandes and Milidiu?, 2012).",
        "Thus, the prediction problem reduces to a maximum branching problem, which is efficiently solved by the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967).",
        "We also use the averaged structure perceptron as suggested by (Collins, 2002), since it provides a more robust model."
      ]
    },
    {
      "heading": "3 Data Preparation",
      "text": [
        "It is necessary to perform some corpus processing steps in order to prepare training and test data.",
        "In this section, we detail the methodology we use to generate coreference arcs and the features that describe them."
      ]
    },
    {
      "heading": "3.1 Coreference Arcs Generation",
      "text": [
        "The input for the prediction problem is a graph whose nodes are the mentions in a document.",
        "Ideally, we could consider the complete graph for each document, thus every mention pair would be an option for building the document tree.",
        "However, since the total number of mentions is huge and a big portion of arcs can be easily identified as incorrect, we filter the arcs and, thus, include only candidate mention pairs that are more likely to be coreferent.",
        "We filter arcs by simply adapting the sieves method proposed in (Lee et al., 2011).",
        "However, in our filtering strategy, precision is not a concern and the application order of filters is not important.",
        "The objective here is to build a small set of candidate arcs that shows good recall.",
        "Given a mention pair (mi,mj), where mi appears before mj in the text, we create a directed arc from mi to mj if at least one of the following conditions holds: (1) the number of mentions between mi and mj is not greater than a given parameter; (2) mj is an alias of mi; (3) there is a match of both mentions strings up to their head words; (4) the head word of mi matches the head word of mj ; (5) test shallow discourse attributes match for both mentions; (6) mj is a pronoun and mi has the same gender, number, speaker and animacy of mj ; (7) mj is a pronoun and mi is a compatible pronoun or proper name.",
        "Sieves 2 to 7 are obtained from (Lee et al., 2011).",
        "We only introduce sieve 1 to lift recall without using other strongly language-dependent sieves."
      ]
    },
    {
      "heading": "3.2 Basic Features",
      "text": [
        "We use a set of 70 basic features to describe each pair of mentions (mi, mj).",
        "The feature set includes lexical, syntactic, semantic, and positional information.",
        "Our feature set is very similar to the one used by (dos Santos and Carvalho, 2011).",
        "However, here we do not use the semantic features derived from WordNet.",
        "In the following, we briefly describe some of these basic features.",
        "Lexical: head word of mi/j ; String matching of (head word of) mi and mj (y/n); Both are pronouns and their strings match (y/n); Previous/Next two words of mi/j ; Length of mi/j ; Edit distance of head words; mi/j is a definitive NP (y/n); mi/j is a demonstrative NP (y/n); Both are proper names and their strings match (y/n).",
        "Syntactic: POS tag of the mi/j head word; Previous/Next two POS tags of mi/j ; mi and mj are both pronouns / proper names (y/n); Previous/Next predicate of mi/j ; Compatible pronouns, which checks whether two pronouns agree in number, gender and person (y/n); NP embedding level; Number of embedded NPs in mi/j .",
        "Semantic: the result of a baseline system; sense of the mi/j head word; Named entity type of mi/j ; mi and mj have the same named entity; Semantic role of mi/j for the prev/next predicate; Concatenation of semantic roles of mi and mj for the same predicate (if they are in the same sentence); Same speaker (y/n); mj is an alias of mi (y/n).",
        "Distance and Position: Distance between mi and mj in sentences; Distance in number of mentions;",
        "Distance in number of person names (applies only for the cases where mi and mj are both pronouns or one of them is a person name); One mention is in apposition to the other (y/n)."
      ]
    },
    {
      "heading": "3.3 Language Specifics",
      "text": [
        "Our system can be easily adapted to different languages.",
        "In our experiments, only small changes are needed in order to train and apply the system to three different languages.",
        "The adaptations are due to: lack of input features for some languages; different POS tagsets are used in the corpora; and creation of static list of language specific pronouns.",
        "Some input features, that are available for the English corpus, are not available in Arabic and Chinese corpora.",
        "Namely, the Arabic corpus does not contain NE, SRL and speaker features.",
        "Therefore, for this language we do not derive basic features that make use of these input features.",
        "For Chinese, we do not use features derived from NE data, since this data is not provided.",
        "Additionally, the Chinese corpus uses a different POS tagset.",
        "Hence, some few mappings are needed during the basic feature derivation stage.",
        "The lack of input features for Arabic and Chinese also impact the sieve-based arcs generation.",
        "For Chinese, we do not use sieve 6, and, for Arabic, we only use sieves 1, 3, 4 and 7.",
        "Sieve 7 is not used for the English corpus, since it is a specialization of sieve 6.",
        "The first sieve parameter is 4 for Arabic and Chinese, and 8 for English.",
        "In the arcs generation and basic feature derivation steps, our system makes use of static lists of language specific pronouns.",
        "In our experiments, we use the POS tagging information and the golden coreference chains to automatically extract these pronoun lists from training corpora."
      ]
    },
    {
      "heading": "3.4 Entropy Guided Feature Induction",
      "text": [
        "In order to improve the predictive power of our system, we add complex features that are combinations of the basic features described in the previous section.",
        "We use feature templates to generate such complex features.",
        "However, we automatically generate templates using the entropy guided feature induction approach (Fernandes and Milidiu?, 2012; Milidiu?",
        "et al., 2008).",
        "These automatically generated templates capture complex contextual information and are difficult to be handcrafted by humans.",
        "Furthermore, this feature induction mechanism extends the structure perceptron framework by providing an efficient general method to build strong nonlinear predictors.",
        "We experiment with different template sets for each language.",
        "The main difference between these sets is basically the training data used to induce them.",
        "We obtain better results when merging different template sets.",
        "For the English language, it is better to use a template set of 196 templates, which merges two different sets: (a) a set induced using training data that contains mention pairs produced by filters 2 to 6; and (b) another set induced using training data that contains mention pairs produced by all filters.",
        "For Chinese and Arabic, it is better to use template sets induced specifically for these languages merged with the template set (a) generated for the English language.",
        "The final set for the Chinese language has 197 templates, while the final set for Arabic has 223."
      ]
    },
    {
      "heading": "4 Empirical Results",
      "text": [
        "We train our system on the corpora provided in the CoNLL-2012 Shared Task.",
        "There are corpora available on three languages: Arabic, Chinese and English.",
        "For each language, results are reported using three metrics: MUC, B3 and CEAFe.",
        "We also report the mean of the F-scores on these three metrics, which gives a unique score for each language.",
        "Additionally, the official score on the CoNLL-2012 shared task is reported, that is the mean of the scores obtained on the three languages.",
        "We report our system results on development and test sets.",
        "The development results are obtained with systems trained only on the training sets.",
        "However, test set results are obtained by training on a larger dataset ?",
        "the one obtained by concatenating training and development sets.",
        "During training, we use the gold standard input features, which produce better performing models than using the provided automatic values.",
        "That is usually the case on NLP tasks, since golden values eliminate the additional noise introduced by automatic features.",
        "On the other hand, during evaluation, we use the automatic values provided in the CoNLL shared task corpora.",
        "In Table 1, we present our system performances on the CoNLL-2012 development sets for the three languages.",
        "Given the size of the Arabic training cor",
        "automatic or golden; and mention candidates can be automatically identified (Auto), golden mention boundaries (GB) or golden mentions (GM).",
        "pus and the feature limitations for Arabic and Chinese, the performance variations among the three languages are no more than expected.",
        "One important parameter that we introduce in this work is the root loss value, a different loss function value on arcs from the artificial root node.",
        "The effect of this parameter is to diminish the creation of clusters, thus stimulating bigger clusters and adjusting the balance between precision and recall.",
        "Using the development sets for tuning, we set the value of the root loss value parameter to 6, 2 and 1.5 for Arabic, Chinese and English, respectively.",
        "In Table 2, we present our system performances on the development sets when we set this parameter to 1 for all languages, that is",
        "equivalent to not use this parameter at all.",
        "We can observe, by comparing these results with the ones in Table 1, that this parameter really causes a better balancing between precision and recall, and consequently increases the F1 scores.",
        "Its effect is accentuated on Arabic and Chinese, since the unbalancing issue is worse on these languages.",
        "The official results on the test sets are depicted in Table 3.",
        "For Chinese and English, these performances are virtually identical to the performances on the development sets.",
        "On the other hand, the official performance for the Arabic language is significantly higher than the development set performance.",
        "This difference is likely due to the fact that the Arabic training set is much smaller than the Chinese and English counterparts.",
        "Thus, by including the development set in the training of the final Arabic system, we significantly improve the official performance.",
        "We report in Table 4 the supplementary results provided by the shared task organizers on the test sets.",
        "These additional experiments investigate two key aspects of any coreference resolution system: the parse feature and the mention candidates that are given to the clustering procedure.",
        "We alternate the parse feature between the official automatic parse and the golden parse from OntoNotes.",
        "Regarding mention candidates, we use three different strategies: automatic mentions (Auto, in Table 4), golden mention boundaries (GB) and golden mentions (GM).",
        "Automatic mentions are completely detected by our system, as described in Section 2.1.",
        "Golden mention boundaries comprise all noun phrases in the golden parse tree, even when the automatic parse is used as input feature.",
        "Golden mentions are all non-singleton mentions, i.e., all mentions that take part in some entity cluster.",
        "It is important to notice that golden mention information is much stronger than golden boundaries.",
        "By observing Table 4, it is clear that the most beneficial information is golden mentions (compare the Auto/GM results in Table 4 with the results in Table 3).",
        "The mean F-score over all languages when using golden mentions is almost 8 points higher than the official score.",
        "These results are not surprising since to identify non-singleton mentions greatly reduces the final task complexity.",
        "Golden mention boundaries (Auto/GB) increase the mean F-score for Chinese by almost 3 points.",
        "Conversely, for the other two languages, the results are decreased when this information is given.",
        "This is probably due to parameter tuning, since any additional information potentially changes the learning problem and, nevertheless, we use exactly the same three models ?",
        "one per language ?",
        "to produce all the results on Tables 3 and 4.",
        "One can observe, for instance, that the recall/precision balance greatly varies among the different configurations in these experiments.",
        "The golden parse feature (Golden/Auto) causes big improvements on the mean F-scores for all languages, specially for Chinese."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we describe a machine learning system based on large margin latent structure perceptron for unrestricted coreference resolution.",
        "We introduce two modeling approaches that have direct impact on the final system performance: latent coreference trees and entropy guided feature induction.",
        "According to our experiments, latent coreference trees are powerful enough to model the complexity of coreference structures in a document, while turning the learning problem computationally feasible.",
        "Our empirical findings also show that entropy guided feature induction enables learning of effective nonlinear classifiers.",
        "Our system is evaluated on the CoNLL-2012 Shared Task closed track, which consists on modeling coreference resolution for three languages: Arabic, Chinese and English.",
        "In order to cope with this multi-language task, our system needs only minor adaptations on some language dependent features.",
        "As future work, we plan to include second order features and cluster sensitive features."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": []
    }
  ]
}
