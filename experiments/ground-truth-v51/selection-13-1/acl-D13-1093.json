{
  "info": {
    "authors": [
      "Hao Zhang",
      "Liang Huang",
      "Kai Zhao",
      "Ryan McDonald"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1093",
    "title": "Online Learning for Inexact Hypergraph Search",
    "url": "https://aclweb.org/anthology/D13-1093",
    "year": 2013
  },
  "references": [
    "acl-C96-1058",
    "acl-D07-1096",
    "acl-D08-1024",
    "acl-D08-1059",
    "acl-D10-1004",
    "acl-D12-1030",
    "acl-E12-1009",
    "acl-J07-2003",
    "acl-N12-1015",
    "acl-P04-1015",
    "acl-P08-1067",
    "acl-P10-1001",
    "acl-P11-2033",
    "acl-P13-2109",
    "acl-Q13-1004",
    "acl-W02-1001",
    "acl-W06-2920"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Online learning algorithms like the percep-tron are widely used for structured prediction tasks.",
        "For sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors (Collins and Roark, 2004; Huang et al., 2012).",
        "However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs.",
        "In this paper, we generalize the violation-fixing perceptron of Huang et al.",
        "(2012) to hypergraphs and apply it to the cube-pruning parser of Zhang and McDonald (2012).",
        "This results in the highest reported scores on WSJ evaluation set (UAS 93.50% and LAS 92.41% respectively) without the aid of additional resources."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Structured prediction problems generally deal with exponentially many outputs, often making exact search infeasible.",
        "For sequential search problems, such as tagging and incremental parsing, beam search coupled with perceptron algorithms that account for potential search errors have been shown to be a powerful combination (Collins and Roark, 2004; Daume?",
        "and Marcu, 2005; Zhang and Clark, 2008; Huang et al., 2012).",
        "However, sequential search algorithms, and in particular left-to-right beam search (Collins and Roark, 2004; Zhang and Clark, 2008), squeeze inference into a very narrow space.",
        "To address this, Huang (2008) formulated constituency parsing as approximate bottom-up inference in order to compactly represent an exponential number of outputs while scoring features of arbitrary scope.",
        "This idea was adapted to graph-based dependency parsers by Zhang and McDonald (2012) and shown to outperform left-to-right beam search.",
        "Both these examples, bottom-up approximate dependency and constituency parsing, can be viewed as specific instances of inexact hypergraph search.",
        "Typically, the approximation is accomplished by cube-pruning throughout the hypergraph (Chiang, 2007).",
        "Unfortunately, as the scope of features at each node increases, the inexactness of search and its negative impact on learning can potentially be exacerbated.",
        "Unlike sequential search, the impact on learning of approximate hypergraph search ?",
        "as well as methods to mitigate any ill effects ?",
        "has not been studied.",
        "Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of Huang et al.",
        "(2012).",
        "We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012)."
      ]
    },
    {
      "heading": "2 Structured Perceptron for Inexact",
      "text": []
    },
    {
      "heading": "Hypergraph Search",
      "text": [
        "The structured perceptron algorithm (Collins, 2002) is a general learning algorithm.",
        "Given training instances (x, y?",
        "), the algorithm first solves the decoding problem y?",
        "= argmaxy?Y(x)w ?",
        "f(x, y) given the weight vector w for the high-dimensional feature representation f of the mapping (x, y), where y?",
        "is the prediction under the current model, y?",
        "is the gold output and Y(x) is the space of all valid outputs for input x.",
        "The perceptron update rule is simply:",
        "The convergence of original perceptron algorithm relies on the argmax function being exact so that the conditionw ?f(x, y?)",
        "> w ?f(x, y?)",
        "(modulo ties) always holds.",
        "This condition is called a violation because the prediction y?",
        "scores higher than the correct label y?.",
        "Each perceptron update moves weights",
        "A B C D E F",
        "are from the gold and Viterbi trees, respectively.",
        "away from y?",
        "and towards y?",
        "to fix such violations.",
        "But when search is inexact, y?",
        "could be suboptimal so that sometimes w ?",
        "f(x, y?)",
        "< w ?",
        "f(x, y?).",
        "Huang et al. (2012) named such instances non-violations and showed that perceptron model updates for non-violations nullify guarantees of convergence.",
        "To account for this, they generalized the original update rule to select an output y?",
        "within the pruned search space that scores higher than y?, but is not necessarily the highest among all possibilities, which represents a true violation of the model on that training instance.",
        "This violation fixing perceptron thus relaxes the argmax function to accommodate inexact search and becomes provably convergent as a result.",
        "In the sequential cases where y?",
        "has a linear structure such as tagging and incremental parsing, the violation fixing perceptron boils down to finding and updating along a certain prefix of y?.",
        "Collins and Roark (2004) locate the earliest position in a chain structure where y?pref is worse than y?pref by a margin large enough to cause y?",
        "to be dropped from the beam.",
        "Huang et al. (2012) locate the position where the violation is largest among all prefixes of y?, where size of a violation is defined as w ?",
        "f(x, y?pref) ?",
        "w ?",
        "f(x, y?pref).",
        "For hypergraphs, the notion of prefix must be generalized to subtrees.",
        "Figure 1 shows the packed-forest representation of the union of gold subtrees and highest-scoring (Viterbi) subtrees at every gold node for an input.",
        "At each gold node, there are two incoming hyperedges: one for the gold subtree and the other for the Viterbi subtree.",
        "After bottom-up parsing, we can compute the scores for the gold subtrees as well as extract the corresponding Viterbi subtrees by following backpointers.",
        "These Viterbi subtrees need not necessarily to belong to the full Viterbi path (i.e., the Viterbi tree rooted at node N ).",
        "An update strategy must choose a subtree or a set of subtrees at gold nodes.",
        "This is to ensure that the model is updating its weights relative to the intersection of the search space and the gold path.",
        "Our first update strategy is called single-node max-violation (s-max).",
        "Given a gold tree y?, it traverses the gold tree and finds the node n on which the violation between the Viterbi subtree and the gold subtree is the largest over all gold nodes.",
        "The violation is guaranteed to be greater than or equal to zero because the lower bound for the max-violation on any hypergraph is 0 which happens at the leaf nodes.",
        "Then we choose the subtree pair (y?n, y?n) and do the update similar to the prefix update for the sequential case.",
        "For example, in Figure 1, suppose the max-violation happens at node K , which covers the left half of the input x, then the perceptron update would move parameters to the subtree represented by nodes B , C , H and K and away from A , B , G and K .",
        "Our second update strategy is called parallel max-violation (p-max).",
        "It is based on the observation that violations on non-overlapping nodes can be fixed in parallel.",
        "We define a set of frontiers as a set of nodes that are non-overlapping and the union of which covers the entire input string x.",
        "The frontier set can include up to |x |nodes, in the case where the frontier is equivalent to the set of leaves.",
        "We traverse y?",
        "bottom-up to compute the set of frontiers such that each has the max-violation in the span it covers.",
        "Concretely, for each node n, the max-violation frontier set can be defined recursively,",
        "where maxv(n) is the function that returns the node with the absolute maximum violation in the subtree rooted at n and can easily be computed recursively over the hypergraph.",
        "To make a perceptron update, we generate the max-violation frontier set for the entire hypergraph and use it to choose subtree pairs",
        "n), where root(x) is the root of the hypergraph for input x.",
        "For example, in Figure 1, if the union of K and L satisfies the definition of ft, then the perceptron update would move feature",
        "weights away from the union of the two Viterbi subtrees and towards their gold counterparts.",
        "In our experiments, we compare the performance of the two violation-fixing update strategies against two baselines.",
        "The first baseline is the standard update, where updates always happen at the root node of a gold tree, even if the Viterbi tree at the root node leads to a non-violation update.",
        "The second baseline is the skip update, which also always updates at the root nodes but skips any non-violations.",
        "This is the strategy used by Zhang and McDonald (2012)."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": [
        "We ran a number of experiments on the cube-pruning dependency parser of Zhang and McDonald (2012), whose search space can be represented as a hypergraph in which the nodes are the complete and incomplete states and the hyperedges are the instantiations of the two parsing rules in the Eisner algorithm (Eisner, 1996).",
        "The feature templates we used are a superset of Zhang and McDonald (2012).",
        "These features include first-, second-, and third-order features and their labeled counterparts, as well as valency features.",
        "In addition, we also included a feature template from Bohnet and Kuhn (2012).",
        "This template examines the leftmost child and the rightmost child of a modifier simultaneously.",
        "All other high-order features of Zhang and McDonald (2012) only look at arcs on the same side of their head.",
        "We trained the parser with hamming-loss-augmented MIRA (Crammer et al., 2006), following Martins et al.",
        "(2010).",
        "Based on results on the English validation data, in all the experiments, we trained MIRA with 8 epochs and used a beam of size 6 per node.",
        "To speed up the parser, we used an unlabeled first-order model to prune unlikely dependency arcs at both training and testing time (Koo and Collins, 2010; Martins et al., 2013).",
        "We followed Rush and Petrov (2012) to train the first-order model to minimize filter loss with respect to max-marginal filtering.",
        "On the English validation corpus, the filtering model pruned 80% of arcs while keeping the oracle unlabeled attachment score above 99.50%.",
        "During training only, we insert the gold tree into the hypergraph if it was mistakenly pruned.",
        "This ensures that the gold nodes are always available, which is required for model updates."
      ]
    },
    {
      "heading": "3.1 English and Chinese Results",
      "text": [
        "We report dependency parsing results on the Penn WSJ Treebank and the Chinese CTB-5 Treebank.",
        "Both treebanks are constituency treebanks.",
        "We generated two versions of dependency treebanks by applying commonly-used conversion procedures.",
        "For the first English version (PTB-YM), we used the Penn2Malt1 software to apply the head rules of Ya-mada and Matsumoto and the Malt label set.",
        "For the second English version (PTB-S), we used the Stanford dependency framework (De Marneffe et al., 2006) by applying version 2.0.5 of the Stanford parser.",
        "We split the data in the standard way: sections 2-21 for training; section 22 for validation; and section 23 for evaluation.",
        "We utilized a linear chain CRF tagger which has an accuracy of 96.9% on the validation data and 97.3% on the evaluation data2.",
        "For Chinese, we use the Chinese Penn Treebank converted to dependencies and split into train/-validation/evaluation according to Zhang and Nivre (2011).",
        "We report both unlabeled attachment scores (UAS) and labeled attachment scores (LAS), ignoring punctuations (Buchholz and Marsi, 2006).",
        "Table 1 displays the results.",
        "Our improved cube-pruned parser represents a significant improvement over the feature-rich transition-based parser of Zhang and Nivre (2011) with a large beam size.",
        "It also improves over the baseline cube-pruning parser without max-violation update strategies (Zhang and McDonald, 2012), showing the importance of update strategies in inexact hypergraph search.",
        "The UAS score on Penn-YM is slightly higher than the best result known in the literature which was reported by the fourth-order unlabeled dependency parser of Ma and Zhao (2012), although we did not utilize fourth-order features.",
        "The LAS score on Penn-YM is on par with the best reported by Bohnet and Kuhn (2012).",
        "On Penn-S, there are not many existing results to compare with, due to the tradition of reporting results on Penn-YM in the past.",
        "Nevertheless, our result is higher than the second best by a large margin.",
        "Our Chinese parsing scores are the highest reported results.",
        "punctuations.",
        "We also include the tokens per second numbers for different parsers whenever available, although the numbers from other papers were obtained on different machines.",
        "Speed numbers marked with ?",
        "were converted from sentences per second.",
        "The speed of our parser is around 200-300 tokens per second for English.",
        "This is faster than the parser of Bohnet and Kuhn (2012) which has roughly the same level of accuracy, but is slower than the parser of Martins et al. (2013) and Rush and Petrov (2012), both of which only do unlabeled dependency parsing and are less accurate.",
        "Given that predicting labels on arcs can slow down a parser by a constant factor proportional to the size of the label set, the speed of our parser is competitive.",
        "We also tried to prune away arc labels based on observed labels for each POS tag pair in the training data.",
        "By doing so, we could speed up our parser to 500-600 tokens per second with less than a 0.2% drop in both UAS and LAS."
      ]
    },
    {
      "heading": "3.2 Importance of Update Strategies",
      "text": [
        "The lower portion of Table 1 compares cube-pruning parsing with different online update strategies in order to show the importance of choosing an update strategy that accommodates search errors.",
        "The max-violation update strategies (s-max and p-max) improved results on both versions of the Penn Treebank as well as the CTB-5 Chinese treebank.",
        "It made a larger difference on Penn-S relative to Penn-YM, improving as much as 0.93% in LAS against the skip update strategy.",
        "Additionally, we measured the percentage of non-violation updates at root nodes.",
        "In the last epoch of training, on Penn-YM, there was 24% non-violations if we used the skip update strategy; on Penn-S, there was 36% non-violations.",
        "The portion of non-violations indicates the inexactness",
        "validation data set of Penn-YM.",
        "The x-axis is the number of training epochs.",
        "The y-axis is the UAS score.",
        "s-max stands for single-node max-violation.",
        "p-max stands for parallel max-violation.",
        "of the underlying search.",
        "Search is harder on Penn-S due to the larger label set.",
        "Thus, as expected, max-violation update strategies improve most where the search is the hardest and least exact.",
        "Figure 2 shows accuracy per training epoch on the validation data.",
        "It can be seen that bad update strategies are not simply slow learners.",
        "More iterations of training cannot close the gap between strategies.",
        "Forcing invalid updates on non-violations (standard update) or simply ignoring them (skip update) produces less accurate models overall.",
        "we use the 2006 data set.",
        "The best results with ?",
        "are the maximum in the following papers: Buchholz and Marsi (2006), Nivre et al. (2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al. (2013), For consistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script.",
        "ZN 2011 (reimpl.)",
        "is our reimplementation of Zhang and Nivre (2011), with a beam of 64.",
        "Results in bold are the best among ZN 2011 reimplementation and different update strategies from this paper."
      ]
    },
    {
      "heading": "3.3 CoNLL Results",
      "text": [
        "We also report parsing results for 17 languages from the CoNLL 2006/2007 shared-task (Buchholz and Marsi, 2006; Nivre et al., 2007).",
        "The parser in our experiments can only produce projective dependency trees as it uses an Eisner algorithm backbone to generate the hypergraph (Eisner, 1996).",
        "So, at training time, we convert non-projective trees ?",
        "of which there are many in the CoNLL data ?",
        "to projective ones through flattening, i.e., attaching words to the lowest ancestor that results in projective trees.",
        "At testing time, our parser can only predict projective trees, though we evaluate on the true non-projective trees.",
        "Table 2 shows the full results.",
        "We sort the languages according to the percentage of non-projective trees in increasing order.",
        "The Spanish treebank is 98% projective, while the Dutch treebank is only 64% projective.",
        "With respect to the Zhang and Nivre (2011) baseline, we improved UAS in 16 languages and LAS in 15 languages.",
        "The improvements are stronger for the projective languages in the top rows.",
        "We achieved the best published UAS results for 7 languages: Spanish, Catalan, Bul-garain, Italian, Swedish, Danish, and Greek.",
        "As these languages are typically from the more projective data sets, we speculate that extending the parser used in this study to handle non-projectivity will lead to state-of-the-art models for the majority of languages."
      ]
    },
    {
      "heading": "4 Conclusions",
      "text": [
        "We proposed perceptron update strategies for inexact hypergraph search and experimented with a cube-pruning dependency parser.",
        "Both single-node max-violation and parallel max-violation update strategies signficantly improved parsing results over the strategy that ignores any invalid udpates caused by inexactness of search.",
        "The update strategies are applicable to any bottom-up parsing problems such as constituent parsing (Huang, 2008) and syntax-based machine translation with online learning (Chiang et al., 2008).",
        "Acknowledgments: We thank Andre?",
        "F. T. Martins for the dependency converted Penn Treebank with automatic POS tags from his experiments; the reviewers for their useful suggestions; the NLP team at Google for numerous discussions and comments; Liang Huang and Kai Zhao are supported in part by DARPA FA8750-13-2-0041 (DEFT), PSC-CUNY, and a Google Faculty Research Award."
      ]
    }
  ]
}
