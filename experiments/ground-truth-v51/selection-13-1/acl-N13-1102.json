{
  "info": {
    "authors": [
      "Xian Qian",
      "Yang Liu"
    ],
    "book": "NAACL",
    "id": "acl-N13-1102",
    "title": "Disfluency Detection Using Multi-step Stacked Learning",
    "url": "https://aclweb.org/anthology/N13-1102",
    "year": 2013
  },
  "references": [
    "acl-H05-1030",
    "acl-N01-1016",
    "acl-N09-2028",
    "acl-P04-1005",
    "acl-P06-1071",
    "acl-P11-1071",
    "acl-W02-1001"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we propose a multi-step stacked learning model for disfluency detection.",
        "Our method incorporates refined n-gram features step by step from different word sequences.",
        "First, we detect filler words.",
        "Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text.",
        "In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection.",
        "We useMax-MarginMarkov Networks (M3Ns) as the classifier with the weighted hamming loss to balance precision and recall.",
        "Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and M3Ns with weighted hamming loss can significantly improve the performance.",
        "Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1"
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Detecting disfluencies in spontaneous speech can be used to clean up speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules.",
        "There are two types of disfluencies: filler words including filled pauses (e.g., ?uh?, ?um?)",
        "and discourse markers (e.g., ?I mean?, ?you know?",
        "), and edited words that are repeated, discarded, or corrected by",
        "the following words.",
        "An example is shown below that includes edited words and filler words.",
        "I want a flight to Boston",
        "curate than edit detection as they are often fixed phrases (e.g., ?uh?, ?you know?, ?I mean?",
        "), hence our work focuses on edited word detection.",
        "Many models have been evaluated for this task.",
        "Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection.",
        "They showed that CRFs significantly outperformed Maximum Entropy models and HMM-s. Johnson and Charniak (2004) proposed a TAG-based noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001).",
        "Zwarts and Johnson (2011) extended this model using minimal expected Floss oriented n-best reranking.",
        "They obtained the best reported F-score of 83.8% on the Switchboard corpus.",
        "Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints.",
        "From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006).",
        "Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage.",
        "They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset.",
        "In this paper, we propose a multi-step stacked learning approach for disfluency detection.",
        "In our method, we first perform filler word detection, then edited word detection.",
        "In every step, we generate new refined n-gram features based on the processed text (remove the detected filler or edited words from the previous step), and use these in the next step.",
        "We also include a new type of features, called in-between features, and incorporate them into the last step.",
        "For edited word detection, we use Max-Margin Markov Networks (M3Ns) with weighted hamming loss as the classifier, as it can well balance the precision and recall to achieve high performance.",
        "On the commonly used Switchboard corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection."
      ]
    },
    {
      "heading": "2 Balancing Precision and Recall Using",
      "text": [
        "Weighted M3Ns We use a sequence labeling model for edit detection.",
        "Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other).",
        "For example, the previous sentence is represented as:",
        "We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words:",
        "There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006).",
        "Previous work has shown that minimizing Floss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words.",
        "In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward).",
        "The learning task for M3Ns can be represented as follows:",
        "?x,y ?",
        "0, ?x, y The above shows the dual form for trainingM3Ns, where x is the observation of a training sample, y ?",
        "Y is a label.",
        "?",
        "is the parameter needed to be optimized, C > 0 is the regularization parameter.",
        "?f(x, y) is the residual feature vector: f(x, y?)",
        "?",
        "f(x, y), where y?",
        "is the true label of x. L(x, y) is the loss function.",
        "Taskar et al. (2004) used un-weighted hamming loss, which is the number of incorrect components: L(x, y) =",
        "where ?",
        "(a, b) is the binary indicator function (it is 0 if a = b).",
        "In our work, we use the weighted hamming loss:",
        "where v(yt, y?t) is the weighted loss for the error when y?t is mislabeled as yt.",
        "Such a weighted loss function allows us to balance the model's precision and recall rates.",
        "For example, if we assign a large value to v(O, ?E) (?E denotes SE, BE, IE, EE), then the classifier is more sensitive to false negative errors (edited word misclassified as non-edited word), thus we can improve the recall rate.",
        "In our work, we tune the weight matrix v using the development dataset."
      ]
    },
    {
      "heading": "3 Multi-step Stacked Learning for Edit",
      "text": []
    },
    {
      "heading": "Disfluency Detection",
      "text": [
        "Rather than just using the above M3Ns with some features, in this paper we propose to use stacked learning to incorporate gradually refined n-gram features.",
        "Stacked learning is a meta-learning approach (Cohen and de Carvalho, 2005).",
        "Its idea is to use two",
        "(or more) levels of predictors, where the outputs of the low level predictors are incorporated as features into the next level predictors.",
        "It has the advantage of incorporating non-local features as well as non-linear classifiers.",
        "In our task, we do not just use the classifier's output (a word is an edited word or not) as a feature, rather we use such output to remove the disfluencies and extract new n-gram features for the subsequent stacked classifiers.",
        "We use 10 fold cross validation to train the low level predictors.",
        "The following describes the three steps in our approach."
      ]
    },
    {
      "heading": "3.1 Step 1: Filler Word Detection",
      "text": [
        "In the first step, we automatically detect filler words.",
        "Since filler words often occur immediately after edited words (before the corrected words), we expect that removing them will make rough copy detection easy.",
        "For example, in the previous example shown in Section 1, if ?uh I mean?",
        "is removed, then the reparandum ?to Boston?",
        "and repair ?to Denver?",
        "will be adjacent and we can use word/POS based n-gram features to detect that disfluency.",
        "Otherwise, the classifier needs to skip possible filler words to find the rough copy of the reparandum.",
        "For filler word detection, similar to edited word detection, we define 5 labels: BP , IP , EP , SP , O.",
        "We use un-weighted hamming loss to learn M3Ns for this task.",
        "Since for filler word detection, our performance metric is not F-measure, but just the overall accuracy in order to generate cleaned text for subsequent n-gram features, we did not use the weighted hamming hoss for this.",
        "The features we used are listed in Table 1.",
        "All n-grams are extracted from the original text."
      ]
    },
    {
      "heading": "3.2 Step 2: Edited Word Detection",
      "text": [
        "In the second step, edited words are detected using M3Ns with the weighted-hamming loss.",
        "The features we used are listed in Table 2.",
        "All n-grams in the first step are also used here.",
        "Besides that, word n-grams, POS n-grams and logic n-grams extracted from filler word removed text are included.",
        "Feature templates I(w0, w?i) is to generate features detecting rough copies separated by filler words."
      ]
    },
    {
      "heading": "3.3 Step 3: Refined Edited Word Detection",
      "text": [
        "In this step, we use n-gram features extracted from the text after removing edit disfluencies based on",
        "w0, p0 denote the current word and POS tag respectively.",
        "w?i denotes the ith word to the left, wi denotes the ith word to the right.",
        "The logic function I(a, b) indicates whether a and b are identical (eigher unigrams or bigrams).",
        "All templates in Table 1 unigrams w?1, w?2, w?3, w?4 bigrams p0p?1, p0p?2, p0p?3, p0p?4",
        "w?i, p?i denote the ith word/POS tag to the right in the filler words removed text.",
        "If current word w0 is removed in step 1, we use its original n-gram features rather than the refined n-gram features.",
        "the previous step.",
        "According to our analysis of the errors produced by step 2, we observed that many errors occurred at the boundaries of the disfluencies, and the word bigrams after removing the edited words are unnatural.",
        "The following is an example:",
        "?",
        "Ref: The new type is prettier than what their/SE they used to look like.",
        "?",
        "Sys: The new type is prettier than what/BE",
        "their/EE they used to look like.",
        "Using the system's prediction, we would have bigram than they, which is odd.",
        "Usually, the pronoun following than is accusative case.",
        "We expect adding n-gram features derived from the cleaned-up sentences would allow the new classifier to fix such hypothesis.",
        "This kind of n-gram features is similar to the language models used in (Zwarts and Johnson,",
        "2011).",
        "They have the benefit of measuring the fluency of the cleaned text.",
        "Another common error we noticed is caused by the ambiguities of coordinates, because the coordinates have similar patterns as rough copies.",
        "For example,",
        "?",
        "Coordinates: they ca n't decide which are the good aspects and which are the bad aspects ?",
        "Rough Copies: it/BE ?s/IE a/IE pleasure/IE to/EE it s good to get outside",
        "To distinguish the rough copies and the coordinate examples shown above, we analyze the training data statistically.",
        "We extract all the pieces lying between identical word bigrams AB .",
        ".",
        ".",
        "AB.",
        "The observation is that coordinates are often longer than edited sequences.",
        "Hence we introduce the in-between features for each word.",
        "If a word lies between identical word bigrams, then its in-between feature is the log length of the subsequence lying between the two bigrams; otherwise, it is zero (we use log length to avoid sparsity).",
        "We also used other patterns such as A .",
        ".",
        ".",
        "A and ABC .",
        ".",
        ".",
        "ABC, but they are too noisy or infrequent and do not yield much performance gain.",
        "Table 3 lists the feature templates used in this last step.",
        "3).",
        "w?",
        "?i denotes the ith word tag to the right in the edited word removed text.",
        "LAB denotes the log length of the subsequence in the pattern AB.",
        ".",
        ".",
        "AB, bAB indicates whether the current word lies between two identical bigrams."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Experimental Setup",
      "text": [
        "We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004).",
        "We also remove the partial words and punctuation from the training and test data for the reason to simulate the situation when speech recognizers are used and such kind of information is not available (Johnson and Charniak, 2004).",
        "We tuned the weight matrix for hamming loss on the development dataset using simple grid search.",
        "The diagonal elements are fixed at 0; for false positive errors, O ?",
        "?E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, ?E ?",
        "O, we tried the weight from 1 to 3, and increased the weight 0.5 each time.",
        "The optimal weight matrix is shown in Table 4.",
        "Note that we use five labels in the sequence labeling task; however, for edited word detection evaluation, it is only a binary task, that is, all of the words labeled with ?E will be mapped to the class of edited words."
      ]
    },
    {
      "heading": "4.2 Results",
      "text": [
        "We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passive-aggressive (PA) learning.",
        "For each model, we tuned the parameters on the development data: Gaussian prior for CRFs is 1.0, iteration number for AP is 10, iteration number and regularization penalty for PA are 10 and 1.",
        "For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training.",
        "Regularization penalty is C = 0.1 and iteration number is 30.",
        "Table 5 shows the results using different models and features.",
        "The baseline models use only the n-grams features extracted from the original text.",
        "We can see that M3Ns with the weighted hamming loss achieve the best performance, outperforming all the other models.",
        "Regarding the features, the gradually added n-gram features have consistent improvement for all models.",
        "Using the weighted hamming loss in M3Ns, we observe a gain of 2.2% after deleting filler words, and 1.8% after deleting edited words.",
        "In our analysis, we also noticed that the in-between fea",
        "for stacked learning.",
        "F scores are reported.",
        "AP = Averaged Perceptron, PA = online Passive Aggresive, M3N = un-weighted M3Ns, w. M3N = weighted M3Ns.",
        "tures yield about 1% improvement in F-score for all models (the gain of step 3 over step 2 is because of the in-between features and the new n-gram features extracted from the text after removing previously detected edited words).",
        "We performed McNemar's test to evaluate the significance of the difference among various methods, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value < 0.001).",
        "There are no significant differences among CRFs, AP and PA.",
        "Using recovered n-gram features and in-between features significantly improves all sequence labeling models (p value < 0.001).",
        "We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6.",
        "We achieved the best F-score.",
        "The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models.",
        "the re-segmented Switchboard corpus, which is not exactly the same as ours.",
        "?",
        "they reported the F-score of BE tag (beginning of the edited sequences).",
        "+ they used language model learned from 3 additional corpora."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we proposed multi-step stacked learning to extract n-gram features step by step.",
        "The first level removes the filler words providing new ngram-s for the second level to remove edited words.",
        "The third level uses the n-grams from the original text and the cleaned text generated by the previous two steps for accurate edit detection.",
        "To minimize the Floss approximately, we modified the hamming loss in M3Ns.",
        "Experimental results show that our method is effective, and achieved the best reported performance on the Switchboard corpus without the use of any additional resources."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank three anonymous reviewers for their valuable comments.",
        "This work is partly supported by"
      ]
    },
    {
      "heading": "DARPA under Contract No. HR0011-12-C-0016",
      "text": [
        "and FA8750-13-2-0041.",
        "Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA."
      ]
    }
  ]
}
