{
  "info": {
    "authors": [
      "Victor Chahuneau",
      "Noah A. Smith",
      "Chris Dyer"
    ],
    "book": "NAACL",
    "id": "acl-N13-1140",
    "title": "Knowledge-Rich Morphological Priors for Bayesian Language Models",
    "url": "https://aclweb.org/anthology/N13-1140",
    "year": 2013
  },
  "references": [
    "acl-C00-1042",
    "acl-D11-1080",
    "acl-E09-2008",
    "acl-E93-1066",
    "acl-H05-1060",
    "acl-H05-1085",
    "acl-J01-2001",
    "acl-J92-1002",
    "acl-J93-2003",
    "acl-J94-3001",
    "acl-N03-2002",
    "acl-N06-2013",
    "acl-N13-1073",
    "acl-P01-1035",
    "acl-P01-1063",
    "acl-P05-1071",
    "acl-P06-1124",
    "acl-P07-1017",
    "acl-P08-1084",
    "acl-P11-1072",
    "acl-P84-1038",
    "acl-W05-1107",
    "acl-W07-0704",
    "acl-W10-1401"
  ],
  "sections": [
    {
      "heading": "Experiments",
      "text": [
        "We evaluate the alignment error rate of our models for two language pairs with rich morphology on the target side.",
        "We compare to alignments inferred using IBM Model 4 trained with EM (Brown et al., 1993),10 a version of our baseline model (described above) without PY priors (learned using EM), and the PY-based baseline.",
        "We consider two language pairs.",
        "English-Turkish We use a 2.8M word cleaned version of the South-East European Times corpus (Tyers and Alperen, 2010) and gold-standard alignments from ?akmak et al. (2012).",
        "Our morphological analyzer is identical to the one used in the previous sections.",
        "English-Czech We use the 1.3M word News Commentary corpus and gold-standard alignments 10We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. from Bojar and Prokopov?",
        "(2006).",
        "The morphological analyzer is provided by Xerox.",
        "Results Results are shown in Table 5.",
        "Our lightly parameterized model performs much better than IBM Model 4 in these small-data conditions.",
        "With an identical model, we find PY priors outperform traditional multinomial distributions.",
        "Adding morphology further reduced the alignment error rate, for both languages.",
        "As an example of how our model generalizes better, consider the sentence pair in Fig. 3, taken from the evaluation data.",
        "The two words composing the Turkish sentence are not found elsewhere in the corpus, but several related inflections occur.11 It is therefore trivial for the stem-base model to find the correct alignment (marked in black), while all the other models have no evidence for it and choose an arbitrary alignment (gray points)."
      ]
    },
    {
      "heading": "6 Related Work",
      "text": [
        "Computational morphology has received considerable attention in NLP since the early work on two-level morphology (Koskenniemi, 1984; Kaplan and 11?devinin, ?devini, ?devleri; bitmez, bitirileceg?inden, bitmesiyle, ...",
        "Kay, 1994).",
        "It is now widely accepted that finite-state transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach.",
        "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.",
        "Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-T?r et al., 2000; Hajic?",
        "et al, 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009).",
        "Our disambiguation model is closely related to generative models used for this purpose (Hakkani-T?r et al., 2000).",
        "Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and La-gus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001).",
        "In ?3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds.",
        "Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; Al-Haj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010).",
        "Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Ar?soy et al., 2008), and more heuristic approaches (Monz, 2011).",
        "Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly lit-12Developing a high-coverage analyzer can be a time-consuming process even with the simplicity of modern toolkits, and unsupervised morphology learning is an attractive problem for computational cognitive science.",
        "tle attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption).",
        "The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word-and character-based models.",
        "Character-based language models are reviewed by Carpenter (2005).",
        "So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002).",
        "Open-vocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009).",
        "Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators.",
        "It has been shown that the widely used modified Kneser-Ney estimator (Chen and Goodman, 1998) for n-gram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006)."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We described a generative model which makes use of morphological analyzers to produce richer word distributions through sharing of statistical strength between stems.",
        "We have shown how it can be integrated into several models central to NLP applications and have empirically validated the effectiveness of these changes.",
        "Although this paper mostly focused on languages that are well studied and for which high-quality analyzers are available, our models are especially relevant in low-resource scenarios because they do not require disambiguated analyses.",
        "In future work, we plan to apply these techniques to languages such as Kinyarwanda, a resource-poor but morphologically rich language spoken in Rwanda.",
        "It is our belief that knowledge-rich models can help bridge the gap between low-and high-resource languages."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank Kemal Oflazer for making his Turkish language morphological analyzer available to us and Brendan O?Connor for gathering the Turkish tweets used in",
        "the predictive text experiments.",
        "This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533."
      ]
    }
  ]
}
