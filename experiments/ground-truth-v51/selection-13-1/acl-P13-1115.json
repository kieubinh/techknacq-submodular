{
  "info": {
    "authors": [
      "Zhenhua Tian",
      "Hengheng Xiang",
      "Ziqi Liu",
      "Qinghua Zheng"
    ],
    "book": "ACL",
    "id": "acl-P13-1115",
    "title": "A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation",
    "url": "https://aclweb.org/anthology/P13-1115",
    "year": 2013
  },
  "references": [
    "acl-C00-1028",
    "acl-D07-1039",
    "acl-D07-1042",
    "acl-D08-1007",
    "acl-E03-1034",
    "acl-E99-1005",
    "acl-J02-2003",
    "acl-J02-3001",
    "acl-J03-3005",
    "acl-J10-4007",
    "acl-J98-2002",
    "acl-N07-1071",
    "acl-P01-1046",
    "acl-P07-1028",
    "acl-P10-1044",
    "acl-P10-1045",
    "acl-P10-1046",
    "acl-P99-1004",
    "acl-P99-1014",
    "acl-W01-0703",
    "acl-W97-0209"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences.",
        "Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate's smoothed preferences.",
        "Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Selectional preferences (SP) or selectional restrictions capture the plausibility of predicates and their arguments for a given relation.",
        "Kaze and Fodor (1963) describe that predicates and their arguments have strict boolean restrictions, either satisfied or violated.",
        "Sentences are semantically anomalous and not consistent in reading if they violated the restrictions.",
        "Wilks (1973) argues that ?rejecting utterances is just what humans do not.",
        "They try to understand them.?",
        "He further states selectional restrictions as preferences between the predicates and arguments, where the violation can be less preferred, but not fatal.",
        "For instance, given the predicate word eat, word food is likely to be its object, iPhone is likely to be implausible for it, and tiger is less preferred but not curious.",
        "SP have been proven to help many natural language processing tasks that involve attachment de-?Partial of this work was done when the first author visiting at Language Technologies Institute of Carnegie Mellon University sponsored by the China Scholarship Council.",
        "cisions, such as semantic role labeling (Resnik, 1993; Gildea and Jurafsky, 2002), word sense disambiguation (Resnik, 1997), human plausibility judgements (Spasic?",
        "and Ananiadou, 2004), syntactic disambiguation (Toutanova et al., 2005), word compositionality (McCarthy et al., 2007), textual entailment (Pantel et al., 2007) and pronoun resolution (Bergsma et al., 2008) etc.",
        "A direct approach to acquire SP is to extract triples (q, r, a) of predicates, relations, and arguments from a syntactically analyzed corpus, and then conduct maximum likelihood estimation (M-LE) on the data.",
        "However, this strategy is infeasible for many plausible triples due to data sparsity.",
        "For example, given the relation <verb-dobjnoun> in a corpus, we may see plausible triples: eat - {food, cake, apple, banana, candy...} But we may not see plausible and implausible triples such as: eat - {watermelon, ziti, escarole, iPhone...} Then how to use a smooth model to alleviate data sparsity for SP?",
        "Random walk models have been successfully applied to alleviate the data sparsity issue on collaborative filtering in recommender systems.",
        "Many online businesses, such as Netflix, Ama-zon.com, and Facebook, have used recommender systems to provide personalized suggestions on the movies, books, or friends that the users may prefer and interested in (Liben-Nowell and Klein-berg, 2007; Yildirim and Krishnamoorthy, 2008).",
        "In this paper, we present an extension of using the random walk model to alleviate data sparsity for SP.",
        "The main intuition is to aggregate all the transitions from a given predicate to its nearby predicates, and propagate their preferences on arguments as the given predicate's smoothed argu",
        "ment preferences.",
        "Our work and contributions are summarized as follows: ?",
        "We present a framework of random walk approach to SP.",
        "It contains four components with flexible configurations.",
        "Each component is corresponding to a specific functional operation on the bipartite and monopartite graphs which representing the SP data; ?",
        "We propose an adjusted preference ranking method to measure SP based on the popularity and association of predicate-argument pairs.",
        "It better correlates with human plausibility judgements.",
        "It also helps to discover similar predicates more precisely; ?",
        "We introduce a probability function for random walk based on the predicate distances.",
        "It controls the influence of nearby and distant predicates to achieve more accurate results; ?",
        "We find out that propagate the measured preferences of predicate-argument pairs is more proper and natural for SP smooth.",
        "It helps to improve the final performance significantly.",
        "We conduct experiments using two sections of the LDC English gigaword corpora as the generalization data.",
        "For the pseudo-disambiguation task, we evaluate it on the Penn TreeBank-3 data.",
        "Results show that our model outperforms several previous methods.",
        "We further investigate the correlations of smoothed scores with human plausibility judgements.",
        "Again our method achieves better correlations on two third party data.",
        "The remainder of the paper is organized as follows: Section 2 introduces related work.",
        "Section 3 briefly formulates the overall framework of our method.",
        "Section 4 describes the detailed model configurations, with discussions on their roles and implications.",
        "Section 5 provides experiments on both the pseudo-disambiguation task and human plausibility judgements.",
        "Finally, Section 6 summarizes the conclusions and future work."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": []
    },
    {
      "heading": "2.1 WordNet-based Approach",
      "text": [
        "Resnik (1996) conducts the pioneer work on corpus-driven SP induction.",
        "For a given predicate q, the system firstly computes its distribution of argument semantic classes based on WordNet.",
        "Then for a given argument a, the system collects the set of candidate semantic classes which contain the argument a, and ensures they are seen in q.",
        "Finally the system picks a semantic class from the candidates with the maximal selectional association score, and defines the score as smoothed score of (q, a).",
        "Many researchers have followed the so-called WordNet-based approach to SP.",
        "One of the key issues is to induce the set of argument semantic classes that are acceptable by the given predicate.",
        "Li and Abe (1998) propose a tree cut model based on minimal description length (MDL) principle for the induction of semantic classes.",
        "Clark and Weir (2002) suggest a hypothesis testing method by ascending the noun hierarchy of WordNet.",
        "Cia-ramita and Johnson (2000) model WordNet as a Bayesian network to solve the ?explain away?",
        "ambiguity.",
        "Beyond induction on argument classes only, Agirre and Martinez (2001) propose a class-to-class model that simultaneously learns SP on both the predicate and argument classes.",
        "WordNet-based approach produces human interpretable output, but suffers the poor lexical coverage problem.",
        "Gildea and Jurafsky (2002) show that clustering-based approach has better coverage than WordNet-based approach.",
        "Brockmann and Lapata (2003) find out that sophisticated WordNet-based methods do not always outperform simple frequency-based methods."
      ]
    },
    {
      "heading": "2.2 Distributional Models without WordNet",
      "text": [
        "Alternatively, Rooth et al. (1999) propose an EM-based clustering smooth for SP.",
        "The key idea is to use the latent clusterings to take the place ofWord-Net semantic classes.",
        "Where the latent clusterings are automatically derived from distributional data based on EM algorithm.",
        "Recently, more sophisticated methods are innovated for SP based on topic models, where the latent variables (topics) take the place of semantic classes and distributional clusterings (Se?aghdha, 2010; Ritter et al., 2010).",
        "Without introducing semantic classes and latent variables, Keller and Lapata (2003) use the web to obtain frequencies for unseen bigrams smooth.",
        "Pantel et al. (2007) apply a collection of rules to filter out incorrect inferences for SP.",
        "Specifically, Dagan et al. (1999) introduce a general similarity-based model for word co-occurrence probabilities, which can be interpreted for SP.",
        "Similarly, Erk et al.",
        "propose an argument-oriented similarity model based on semantic or syntactic vector spaces (Erk,",
        "2007; Erk et al., 2010).",
        "They compare several similarity functions and weighting functions in their model.",
        "Furthermore, instead of employing various similarity functions, Bergsma et al. (2008) propose a discriminative approach to learn the weights between the predicates, based on the verb-noun co-occurrences and other kinds of features.",
        "Random walk model falls into the non-class based distributional approach.",
        "Previous literatures have fully studied the selection of distance or similarity functions to find out similar predicates and arguments (Dagan et al., 1999; Erk et al., 2010), or learn the weights between the predicates (Bergsma et al., 2008).",
        "Instead, we put effort in following issues: 1) how to measure SP; 2) how to transfer between predicates using random walk; 3) how to propagate the preferences for smooth.",
        "Experiments show these issues are important for SP and they should be addressed properly to achieve better results."
      ]
    },
    {
      "heading": "3 RSP: A Random Walk Model for SP",
      "text": [
        "In this section, we briefly introduce how to address SP using random walk.",
        "We propose a framework of RSP with four components (functions).",
        "Each of them are flexible to be configured.",
        "In summary, Algorithm 1 describes the overall process.",
        "Algorithm 1 RSP: Random walk model for SP",
        "Require: Init bipartite graph G with raw counts 1: // Ranking on the bipartite graph G; 2: R = ?",
        "(G); // ranking function 3: // Project R to monopartite graph D 4: D = ?",
        "(R); // distance function 5: // Transform D to stochastic matrix P 6: P = ?",
        "(D); // probability function 7: // Get the convergence P?",
        "8: P?",
        "=?",
        "?t=1 (dP )t |(dP )t |= dP (I ?",
        "dP )?1; 9: return Smoothed bipartite graph R?",
        "10: R?",
        "= P?",
        "?R; // propagation function",
        "Bipartite Graph Construction: For a given relation r, the observed predicate-argument pairs can be represented by a bipartite graph G=(X,Y,E).",
        "Where X={q1 , q2 , ..., qm} are the m predicates, and Y ={a1 , a2 , ..., an} are the n arguments.",
        "We initiate the links E with the raw co-occurrence counts of seen predicate-argument pairs in a given generalization data.",
        "We represent the graph by an adjacency matrix with rows representing predicates and columns as arguments.",
        "For convenience, we use indices i, j to represent predicates qi , qj , and k, l for arguments ak , al .",
        "We employ a preference ranking function ?",
        "to measure the SP between the predicates and arguments.",
        "It transforms G to a corresponding bipartite graph R, with links representing the strength of SP.",
        "Each row of the adjacency matrix R denotes the predicate vector q?i or q?j .",
        "We discuss the selection of ?",
        "in section 4.1.",
        "graph of the verb-dobj-noun relation, (Q) the predicate-projection monopartite graph, and (A) the argument-projection monopartite graph.",
        "Monopartite Graph Projection: In order to conduct random walk on the graph, we project the bipartite graph R onto a monopartite graph Q=(X,E) between the predicates, or A=(Y,E) between the arguments (Zhou et al., 2007).",
        "Figure 1 illustrates the intuition of the projection.",
        "The links in Q represent the indirect connects between the predicates in R. Two predicates are connected in Q if they share at least one common neighbor argument in R. The weight of the links in Q could be set by arbitrary distance measures.",
        "We refer D as an instance of the projection Q by a given distance function ?.",
        "Stochastic Walking Strategy: We introduce a probability function ?",
        "to transform the predicate distances D into transition probabilities P .",
        "Where P is a stochastic matrix, with each element pij represents the transition probability from predicate qi to qj .",
        "Generally speaking, nearby predicates gain higher probabilities to be visited, while distant predicates will be penalized.",
        "Follow Equation 4, we aggregate over all orders of the transition probabilities P as the final stationary probabilities P?",
        ".",
        "According to the PerronFrobenius theory, one can verify that it converges to dP (I ?",
        "dP )?1 when P is non-negative and regular matrix (Li et al., 2009).",
        "Where t represents the orders: the length of the path between two nodes in terms of edges.",
        "The damp factor d ?",
        "(0, 1), and its value mainly depends on the data sparsity level.",
        "Typically d prefers small values such as 0.005.",
        "It means higher order transitions are much less reliable than lower orders (Liben-Nowell and Kleinberg, 2007).",
        "combine the converged transition probabilities P?",
        "with the measured preferences R as the propagation function: 1) for a given predicate, firstly it transfers to all nearby predicates with designed probabilities; 2) then it sums over the arguments preferred by these predicates with quantified scores to get smoothed R?.",
        "We further describe its configuration details in Section 4.4 and Equation 12 with two propagation modes."
      ]
    },
    {
      "heading": "4 Model Configurations",
      "text": []
    },
    {
      "heading": "4.1 Preference Ranking: Measure the Selectional Preferences",
      "text": [
        "In collaborative filtering, usually there are explicit and scaled user ratings on their item preferences.",
        "For instance, a user ratings a movie with a score?",
        "[0,10] on IMDB site.",
        "But in SP, the preferences between the predicates and arguments are implicit: their co-occurrence counts follow the power law distribution and vary greatly.",
        "Therefore, we employ a ranking function ?",
        "to measure the SP of the seen predicate-argument pairs.",
        "We suppose this could bring at least two benefits: 1) a proper measure on the preferences can make the discovering of nearby predicates with similar preferences to be more accurate; 2) while propagation, we propagate the scored preferences, rather than the raw counts or conditional probabilities, which could be more proper and agree with the nature of SP smooth.",
        "We denote SelPref(q, a) as Pr(q, a) for short.",
        "SelPref(q, a) = ?",
        "(q, a) (6) Previous literatures have well studied on various smooth models for SP.",
        "However, they vary greatly on the measure of preferences.",
        "It is still not clear how to do this best.",
        "Lapata et al. investigate the correlations between the co-occurrence counts (CT) c(q, a), or smoothed counts with the human plausibility judgements (Lapata et al., 1999; Lapata et al., 2001).",
        "Some introduce conditional probability (CP) p(a|q) for the decision of preference judgements (Chambers and Jurafsky, 2010; Erk et al., 2010; Se?aghdha, 2010).",
        "Meanwhile, the point-wise mutual information (MI) is also employed by many researchers to filter out incorrect inferences (Pantel et al., 2007; Bergsma et al., 2008).",
        "In this paper, we present an adjusted ranking function (AR) in Equation 8 to measure the SP of seen predicate-argument pairs.",
        "Intuitively, it measures the preferences by combining both the popularity and association, with parameters control the uncertainty of the trade-off between the two.",
        "We define the popularity as the joint probability p(q, a) based on MLE, and the association as MI.",
        "This is potentially similar to the process of human plausibility judgements.",
        "One may judge the plausibility of a predicate-argument collocation from two sides: 1) if it has enough evidences and commonly to be seen; 2) if it has strong association according to the cognition based on kinds of background knowledge.",
        "This metric is also similar to the TF-IDF (TD) used in information retrieval.",
        "We verify if a metric is better by two tasks: 1) how well it correlates with human plausibility judgements; 2) how well it helps with the smooth inference to disambiguate plausible and implausible instances.",
        "We conduct empirical experiments on these issues in Section 5.3 and Section 5.4."
      ]
    },
    {
      "heading": "4.2 Distance Function: Projection of the Monopartite Graph",
      "text": [
        "In Equation 9, the distance function ?",
        "is used to discover nearby predicates with distance dij .",
        "It weights the links on the monopartite graph Q.",
        "It",
        "guides the walker to transfer between predicates.",
        "We calculate ?",
        "based on the vectors q?i, q?j represented by the measured preferences in R.",
        "Where ?",
        "can be distance functions such as Euclidean (norm) distance or Kullback-Leibler divergence (KL) etc., or one minus the similarity functions such as Jaccard and Cosine etc.",
        "The selection of distributional functions has been fully studied by previous work (Lee, 1999; Erk et al., 2010).",
        "In this paper, we do not focus on this issue due to page limits.",
        "We simply use the Cosine function:"
      ]
    },
    {
      "heading": "4.3 Probability Function: the Walk Strategy",
      "text": [
        "We define the probability function ?",
        "as Equation 11.",
        "Where the transition probability p(qj |qi) in P is defined as a function of the distance dij with a parameter ?.",
        "Intuitively, it means in a given walk step, a predicate qj which is far away from qi will get much less probability to be visited, and qi has high probabilities to start walk from itself and its nearby predicates to pursue good precision.",
        "Once we get the transition matrix P , we can compute P?",
        "according to Equation 4.",
        "Where the parameter ?",
        "is used to control the balance of nearby and distant predicates.",
        "Z(qi) is the normalize factor.",
        "Typically, ?",
        "around 2 can produce good enough results in most cases.",
        "We verify the settings of ?",
        "in section 5.3.2."
      ]
    },
    {
      "heading": "4.4 Propagation Function",
      "text": [
        "The propagation function in Equation 5 is represented by the matrix form.",
        "It can be expanded and rewritten as Equation 12.",
        "Where p?",
        "(qj |qi) is the converged transition probability from predicate qi to qj .",
        "Pr(ak, qj) is the measured preference of predicate qj with argument ak.",
        "We employ two propagation modes (PropMode) for the preference propagation function.",
        "One is ?CP?",
        "mode.",
        "In this mode, we always set Pr(q, a) as the conditional probability p(a|q) for the propagation function, despite what ?",
        "is used for the distance function.",
        "This mode is similar to previous methods (Dagan et al., 1999; Keller and Lapata, 2003; Bergsma et al., 2008).",
        "The other is ?PP?",
        "mode.",
        "We set ranking function ?=Pr(q, a) always to be the same in both the distance function and the propagation function.",
        "That means what we propagated is the designed and scored preferences.",
        "This could be more proper and agree with the nature of SP smooth.",
        "We show the improvement of this extension in section 5.3.1."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Data Set",
      "text": [
        "Generalization Data: We parsed the Agence France-Presse (AFP) and New York Times (NYT) sections of the LDC English Gigaword corpora (Parker et al., 2011), each from year 2001-2010.",
        "The parser is provided by the Stanford CoreNLP package1.",
        "We filter out all tokens containing non-alphabetic characters, collect the <verb-dobjnoun > triples from the syntactically analyzed data.",
        "Predicates (verbs) whose frequency lower than 30 and arguments (noun headwords) whose frequency less than 5 are excluded out.",
        "No other filters have been done.",
        "The resulting data consist of: ?",
        "AFP: 26, 118, 892 verb-dobj-noun observations with 1, 918, 275 distinct triples, totally 4, 771 predicates and 44, 777 arguments.",
        "?",
        "NYT: 29, 149, 574 verb-dobj-noun observations with 3, 281, 391 distinct triples, totally 5, 782 predicates and 57, 480 arguments.",
        "Test Data: For pseudo-disambiguation, we employ Penn TreeBank-3 (PTB) as the test data (Marcus et al., 1999)2.",
        "We collect the 36, 400 manually annotated verb-dobj-noun dependencies (with 23, 553 distinct ones) from PTB.",
        "We keep dependencies whose predicates and arguments are seen in the generalization data.",
        "We randomly select 20% of these dependencies as the test set.",
        "We split the test set equally into two parts: one as the development set and the other as the final test set.",
        "for the correlation evaluation.",
        "In each they collect a set of predicate-argument pairs, and annotate with two kinds of human ratings: one for an argument takes the role as the patient of a predicate, and the other for the argument as the agent.",
        "The rating values are between 1 and 7: e.g. they assign hunter-subj-shoot with a rating 6.9 but 2.8 for shoot-dobj-hunter.",
        "?",
        "PBP: Pado?",
        "et al (2007) develop a set of human plausibility ratings on the basis of the Penn TreeBank and FrameNet respectively.",
        "We refer PBP as their 212 patient ratings from the Penn TreeBank.",
        "?",
        "MRP: This data are originally contributed by McRae et al. (1998).",
        "We use all their 723 patient-nn ratings.",
        "Without explicit explanation, we remove all the selected PTB tests and human plausibility pairs from AFP and NYT to treat them unseen."
      ]
    },
    {
      "heading": "5.2 Comparison Methods",
      "text": [
        "Since RSP falls into the unsupervised distributional approach, we compare it with previous similarity-based methods and unsupervised generative topic model 3.",
        "Erk et al. (Erk, 2007; Erk et al., 2010) are the pioneers to address SP using similarity-based method.",
        "For a given (q, a) in relation r, the model sums over the similarities between a and the seen headwords a?",
        "?",
        "Seen(q, r).",
        "They investigated several similarity functions sim(a, a?)",
        "such as Jaccard, Cosine, Lin, and nGCM etc., and different weighting functions wtq,r(a?",
        ").",
        "For comparison, we suppose the primary corpus and generalization corpus in their model to be the same.",
        "We set the similarity function of their model as nGCM, use both the FREQ and DISCR weighting functions.",
        "The vector space is in SYN-PRIMARY setting with 2, 000 basis elements.",
        "Dagan et al. (1999) propose state-of-the-art similarity based model for word co-occurrence probabilities.",
        "Though it is not intended for SP, but it can be interpreted and rewritten for SP as:",
        "They use the k-closest nearbys as Simset(q), with a parameter ?",
        "to revise the similarity function.",
        "For comparison, we use the Jensen-Shannon divergence (Lin, 1991) which shows the best performance in their work as sim(q, q?",
        "), and optimize the settings of k and ?",
        "in our experiments.",
        "LDA-SP: Another kind of sophisticated unsupervised approaches for SP are latent variable models based on Latent Dirichlet Allocation (LDA).",
        "O?",
        "Se?aghdha (2010) applies topic models for the SP induction with three variations: LDA, Rooth-LDA, and Dual-LDA; Ritter et al. (2010) focus on inferring latent topics and their distributions over multiple arguments and relations (e.g., the subject and direct object of a verb).",
        "In this work, we compare with O?",
        "Se?aghdha's original LDA approach to SP.",
        "We use the Matlab Topic Modeling Toolbox4 for the inference of latent topics.",
        "The hyper parameters are set as suggested ?=50/T and ?=200/n, where T is the number of topics and n is the number of arguments.",
        "We test T=100, 200, 300, each with 1, 000 iterations of Gibbs sampling."
      ]
    },
    {
      "heading": "5.3 Pseudo-Disambiguation",
      "text": [
        "Pseudo-disambiguation has been used for SP evaluation by many researchers (Rooth et al., 1999; Erk, 2007; Bergsma et al., 2008; Chambers and Jurafsky, 2010; Ritter et al., 2010).",
        "First the system removes a portion of seen predicate-argument pairs from the generalization data to treat them as unseen positive tests (q, a+).",
        "Then it introduces confounder selection to create a pseudo negative test (q, a?)",
        "for each positive (q, a+).",
        "Finally it evaluates a SP model by how well the model disambiguates these positive and negative tests.",
        "Confounder Selection: for a given (q, a+), the system selects an argument a?",
        "from the argument vocabulary.",
        "Then by ensure (q, a?)",
        "is unseen in the generalization data, it treats a?",
        "as pseudo a?.",
        "This process guarantees that (q, a?)",
        "to be negative in real case with very high probability.",
        "Previous work have made advances on confounder selection with random, bucket and nearest confounder-s. Random confounder (RND) most closes to the realistic case; While nearest confounder (NER) is reproducible and it avoids frequency bias (Chambers and Jurafsky, 2010).",
        "In this work, we employ both RND and NER confounders: 1) for RND, we randomly select",
        "confounders according to the occurrence probability of arguments.",
        "We sample confounders on both the development and final test data with 100 iterations.",
        "2) for NER, firstly we sort the arguments by their frequency.",
        "Then we select the nearest confounders with two iterations.",
        "One iteration selects the confounder whose frequency is more than or equal to a+, and the other iteration with frequency lower than or equal to a+.",
        "Evaluation Metric: we evaluate performance",
        "on both the pairwise and pointwise settings: 1) On pairwise setting, we combine corresponding (q, a+, a?)",
        "together as test instances.",
        "The performance is evaluated based on the accuracy (AC-C) metric.",
        "It computes the portion of test instances (q, a+, a?)",
        "which correctly predicted by the s",
        "mooth model with score(q, a+) > score(q, a?).",
        "We weight each instance equally for macroACC, and weight each by the frequency of the positive pair (q, a+) for microACC.",
        "2) On pointwise setting, we use each positive test (q, a+) or negative test (q, a?)",
        "as test instances independently.",
        "We treat it as a binary classification task, and evaluate using the standard area-under-the-curve (AUC) metric.",
        "This metric is firstly employed for the SP evaluation by Ritter et al(2010).",
        "For macroAUC, we weight each instance equally; for microAUC, we weight each by its argument frequency (Bergsma et al., 2008).",
        "Parameters Tuning: The parameters are tuned on the PTB development set, using AFP as the generalization data.",
        "We report the overall performance on the final test set.",
        "While using NYT as the generalization data, we hold the same parameter settings as AFP to ensure the results are robust.",
        "Note that indeed the parameter settings would vary among different generalization and test data."
      ]
    },
    {
      "heading": "5.3.1 Verify Ranking Function and Propagation Method",
      "text": [
        "This experiment is conducted on the PTB development set with RND confounders.",
        "We use AFP and NYT as the generalization data.",
        "For comparison, we set the distance function ?",
        "as Cosine, with default d=0.005, and ?=1.",
        "In Table 1, the evaluation metric is Accuracy.",
        "The first 4 rows are the results of ?CP?",
        "PropMode, and the latter 3 rows are the ?PP?",
        "PropMode.",
        "With respect to the ranking function ?, CP performs the worst as it considers only the popularity rather than association.",
        "The heavy bias on frequent predicates and arguments has two major drawbacks: a) The computation of predicate distances would rely much more on frequent arguments, rather than those arguments they preferred; b) While propagation, it may bias more on frequent arguments, too.",
        "Even these frequent arguments are less preferred and not proper to be propagated.",
        "Crit.",
        "AFP NYTmacro micro macro micro",
        "For MI, it biases infrequent arguments with strong association, without regarding to the popular arguments with more evidences.",
        "Furthermore, the generalization data is automatically parsed and kind of noisy, especially on infrequent predicates and arguments.",
        "The noises could yield unreliable estimations and decrease the performance.",
        "For TD, it outperforms MI method on ?CP?",
        "PropMode, but it not always outperforms MI on ?PP?",
        "PropMode.",
        "It is no surprise to find out the adjusted ranking AR achieves better results on both AFP and NYT data, with ?1=0.2 and ?2=0.6.",
        "Finally, it shows the ?PP?",
        "mode, which propagating the designed preference scores, gains significantly better performance as discussed in Section 4.4.",
        "5.3.2 Verify ?",
        "of the Probability Function This experiment is conducted on the PTB development tests with both RND and NER confounders.",
        "The generalization data is AFP.",
        "We set the ranking function ?",
        "as AR (with tuned ?1=0.2 and ?2=0.6), the distance function ?",
        "as Cosine, default d=0.005, and we restrict ?",
        "?",
        "[0.5, 4].",
        "Figure 2 shows ?",
        "has significant impact on the performance.",
        "Starting from ?=0.5, the system gains better performance while ?",
        "increasing.",
        "It achieves good results around ?=2.",
        "This means for a given predicate, the penalty on its distant predicates helps to get more accurate smooth.",
        "The performance will drop if ?",
        "becomes too big.",
        "This means closest predicates are useful for smooth.",
        "It it not better to penalize them heavily.",
        "Finally we compare the overall performance of different models.",
        "We report the results on the PTB final test set, with RND and NER confounders.",
        "Table 2 shows the overall performance on Accuracy metric.",
        "Among previous methods in the first 4 rows, LDA-SP performs the best in most cases.",
        "In the last 4 rows, RSPnaive means both the ranking function and PropMode are set as ?CP?",
        "and ?=1.",
        "This configuration yields poor performance.",
        "Iteratively, by employing the adjusted ranking function, smoothing with preference propagation method, and revising the probability function with the parameter ?, RSP outperforms all previous methods.",
        "The parameter settings of RSP-All are ?1=0.2, ?2=0.6, ?=1.75 and d=0.005.",
        "Figure 3 show the macro (left) and micro (right) receiver-operating-characteristic (ROC) curves of different models, using AFP as the generalization data and RND confounders.",
        "For each kind of previous methods, we show the best AUC they achieved.",
        "RASP-All still performs the best on the terms of AUC metric, achieving macroAUC at 84% and microAUC at 89%.",
        "We also verified the AUC metric using NYT as the generalization data.",
        "The results are similar to the AFP data.",
        "It is also interesting to find out that the ACC metric is not always bring into correspondence with the AUC metric.",
        "The difference mainly raise on the pointwise and pairwise test settings of pseudo-disambiguation."
      ]
    },
    {
      "heading": "5.4 Human Plausibility Judgements",
      "text": [
        "We conduct empirical studies on the correlations between different preference ranking func",
        "tions and human ratings.",
        "Follow Lapata et al.",
        "(2001), we first collect the co-occurrence counts of predicate-argument pairs in the human plausibility data from AFP and NYT (before removing them as unseen pairs).",
        "Then we score them with different ranking functions (described in Section 4.1) based on MLE.",
        "Inspired by Erk et al.",
        "(2010), we do not suppose linear correlations between the estimated scores and human ratings.",
        "We use the Spearman's ?",
        "and Kendal's ?",
        "rank correlation coefficient.",
        "We also compare the correlations between the smoothed scores of different models with human ratings.",
        "With respect to upper bounds, Pado?",
        "et al. (2007) suggest that the typical agreement of human participants is around a correlation of 0.7 on their plausibility data.",
        "We hold that automatic models of plausibility can not be expected to surpass this upper bound.",
        "In Table 3, all coefficients are verified at significant level p<0.01.",
        "The first 5 rows are the correlations between the preference ranking functions and human ratings based on MLE.",
        "On both the PBP and MRP data, the proposed AR metric better correlates with human ratings than others, with ?2 >0.5 and ?1 around [0.2, 0.35].",
        "The latter 6 rows are the results of smooth models.",
        "It shows LDA-SP performs good correlation with human ratings, where LDA-SP+Bayes refers to the Bayes prediction method of Ritter et al. (2010).",
        "RSP model gains the best correlation on the two plausibility data in most cases, where the parameter settings are the same as pseudo-disambiguation."
      ]
    },
    {
      "heading": "6 Conclusions and Future Work",
      "text": [
        "In this work we present an random walk approach to SP.",
        "Experiments show it is efficient and effective to address data sparsity for SP.",
        "It is also flexible to be applied to new data.",
        "We find out that a proper measure on SP between the predicates and arguments is important for SP.",
        "It helps with the discovering of nearby predicates and it makes the preference propagation to be more accurate.",
        "Another issue is that it is not good enough to directly applies the similarity or distance functions for smooth.",
        "Potential future work including but not limited to follows: investigate argument-oriented and personalized random walk, extend the model in heterogenous network with multiple link types, discover soft clusters using random walk for semantic induction, and combine it with discriminative learning approach etc."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The research is supported in part by the Na",
        "sored by the China Scholarship Council (CSC).",
        "We also gratefully acknowledge the anonymous reviewers for their helpful comments."
      ]
    }
  ]
}
