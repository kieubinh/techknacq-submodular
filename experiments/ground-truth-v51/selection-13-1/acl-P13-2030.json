{
  "info": {
    "authors": [
      "Ruey-Cheng Chen"
    ],
    "book": "ACL",
    "id": "acl-P13-2030",
    "title": "An improved MDL-based compression algorithm for unsupervised word segmentation",
    "url": "https://aclweb.org/anthology/P13-2030",
    "year": 2013
  },
  "references": [
    "acl-D10-1081",
    "acl-I05-1009",
    "acl-I05-3017",
    "acl-N09-1036",
    "acl-P09-1012",
    "acl-P11-2095",
    "acl-P12-2017",
    "acl-W12-2304"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We study the mathematical properties of a recently proposed MDL-based unsupervised word segmentation algorithm, called regularized compression.",
        "Our analysis shows that its objective function can be efficiently approximated using the negative empirical pointwise mutual information.",
        "The proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Gold-water et al., 2009) and adaptors grammar (Johnson and Goldwater, 2009).",
        "Despite this wide recognition, they are also notoriously computational prohibitive and have limited adoption on larger corpora.",
        "While much effort has been directed to mitigating this issue within the Bayes framework (Borschinger and Johnson, 2011), many have found minimum description length (MDL) based methods more promising in addressing the scalability problem.",
        "MDL-based methods (Rissanen, 1978) rely on underlying search algorithms to segment the text in as many possible ways and use description length to decide which to output.",
        "As different algorithms explore different trajectories in the search space, segmentation accuracy depends largely on the search coverage.",
        "Early work in this line focused more on existing segmentation algorithm, such as branching entropy (Tanaka-Ishii, 2005; Zhikov et al., 2010) and bootstrap voting experts (Hewlett and Cohen, 2009; Hewlett and Cohen, 2011).",
        "A recent study (Chen et al., 2012) on a compression-based algorithm, regularized compression, has achieved comparable performance result to hierarchical Bayes methods.",
        "Along this line, in this paper we present a novel extension to the regularized compressor algorithm.",
        "We propose a lower-bound approximate to the original objective and show that, through analysis and experimentation, this amendment improves segmentation performance and runtime efficiency."
      ]
    },
    {
      "heading": "2 Regularized Compression",
      "text": [
        "The dynamics behind regularized compression is similar to digram coding (Witten et al., 1999).",
        "One first breaks the text down to a sequence of characters (W0) and then works from that representation up in an agglomerative fashion, iteratively removing word boundaries between the two selected word types.",
        "Hence, a new sequence Wi is created in the i-th iteration by merging all the occurrences of some selected bigram (x, y) in the original sequence Wi?1.",
        "Unlike in digram coding, where the most frequent pair of word types is always selected, in regularized compression a specialized decision criterion is used to balance compression rate and vocabulary complexity: min.",
        "?",
        "?f(x, y) + |Wi?1|?H?",
        "(Wi?1,Wi) s.t.",
        "either x or y is a character f(x, y) > nms.",
        "Here, the criterion is written slightly differently.",
        "Note that f(x, y) is the bigram frequency, |Wi?1 |the sequence length of Wi?1, and",
        "ference between the empirical Shannon entropy measured on Wi and Wi?1, using maximum likelihood estimates.",
        "Specifically, this empirical estimate H?",
        "(W ) for a sequence W corresponds to:",
        "For this equation to work, one needs to estimate other model parameters.",
        "See Chen et al. (2012) for a comprehensive treatment.",
        "frequency and sequence length in regularized compression.",
        "In the new sequence Wi, each occurrence of the x-y bigram is replaced with a new (conceptually unseen) word z.",
        "This has an effect of reducing the number of words in the sequence."
      ]
    },
    {
      "heading": "3 Change in Description Length",
      "text": [
        "The second term of the aforementioned objective is in fact an approximate to the change in description length.",
        "This is made obvious by coding up a sequence W using the Shannon code, with which the description length ofW is equal to |W |H?",
        "(W ).",
        "Here, the change in description length between sequences Wi?1 and Wi is written as:",
        "Let us focus on this equation.",
        "Suppose that the original sequence Wi?1 is N word long, the selected word type pair x and y each occurs k and l times, respectively, and altogether x-y bigram occurs m times in Wi?1.",
        "In the new sequence Wi, each of the m bigrams is replaced with an unseen word z = xy.",
        "These altogether have reduced the sequence length by m. The end result is that compression moves probability masses from one place to the other, causing a change in description length.",
        "See Table 1 for a summary to this exchange.",
        "Now, as we expand Equation (1) and reorganize the remaining, we find that:",
        "(2) Note that each line in Equation (2) is of the form x1 log x1 ?",
        "x2 log x2 for some x1, x2 ?",
        "0.",
        "We exploit this pattern and derive a bound for ?L through analysis.",
        "Consider g(x) = x log x.",
        "Since g??",
        "(x) > 0 for x ?",
        "0, by the Taylor series we have the following relations for any x1, x2 ?",
        "0:",
        "Plugging these into Equation (2), we have: m log (k ?m)(l ?m)Nm ?",
        "?L ?",
        "?.",
        "(3) The lower bound1 at the left-hand side is a best-case estimate.",
        "As our aim is to minimize ?L, we use this quantity to serve as an approximate."
      ]
    },
    {
      "heading": "4 Proposed Method",
      "text": [
        "Based on this finding, we propose the following two variations (see Figure 1) for the regularized compression framework: ?",
        "G1: Replacing the second term in the original objective with the lower bound in Equation (3).",
        "The new objective function is written out as Equation (4).",
        "?",
        "G2: Same as G1 except that the lower bound is divided by f(x, y) beforehand.",
        "The normalized lower bound approximates the per-word change in description length, as shown in Equation (5).",
        "With this variation, the function remains in a scalarized form as the original does.",
        "We use the following procedure to compute description length.",
        "Given a word sequence W , we write out all the induced word types (say, M types in total) entry by entry as a character sequence, denoted as C. Then the overall description length is:",
        "Three free parameters, ?, ?, and nms remain to be estimated.",
        "A detailed treatment on parameter estimation is given in the following paragraphs.",
        "Trade-off ?",
        "This parameter controls the balance between compression rate and vocabulary complexity.",
        "Throughout this experiment, we estimated this parameter using MDL-based grid search.",
        "Multiple search runs at different granularity levels were employed as necessary.",
        "Compression rate ?",
        "This is the minimum threshold value for compression rate.",
        "The compressor algorithm would go on as many iteration as possible until the overall compression rate (i.e., 1Sharp-eyed readers may have noticed the similarity between the lower bound and the negative (empirical) point-wise mutual information.",
        "In fact, when f(z) > 0 in Wi?1, it can be shown that limm?0 ?L/m converges to the empirical pointwise mutual information (proof omitted here).",
        "word/character ratio) is lower than ?.",
        "Setting this value to 0 forces the compressor to go on until no more can be done.",
        "In this paper, we experimented with predetermined ?",
        "values as well as those learned from MDL-based grid search.",
        "Minimum support nms We simply followed the suggested setting nms = 3 (Chen et al., 2012)."
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": []
    },
    {
      "heading": "5.1 Setup",
      "text": [
        "In the experiment, we tested our methods on Brent's derivation of the Bernstein-Ratner corpus (Brent and Cartwright, 1996; Bernstein-Ratner, 1987).",
        "This dataset is distributed via the CHILDES project (MacWhinney and Snow, 1990) and has been commonly used as a standard benchmark for phonetic segmentation.",
        "Our baseline method is the original regularized compressor algorithm (Chen et al., 2012).",
        "In our experiment, we considered the following three search settings for finding the model parameters:",
        "(a) Fix ?",
        "to 0 and vary ?",
        "to find the best value (in the sense of description length); (b) Fix ?",
        "to the best value found in setting (a) and vary ?",
        "; (c) Set ?",
        "to a heuristic value 0.37 (Chen et al., 2012) and vary ?.",
        "Settings (a) and (b) can be seen as running a stochastic grid searcher one round for each parameter2.",
        "Note that we tested (c) here only to compare with the best baseline setting."
      ]
    },
    {
      "heading": "5.2 Result",
      "text": [
        "Table 2 summarizes the result for each objective and each search setting.",
        "The best (?, ?)",
        "pair for 2A more formal way to estimate both ?",
        "and ?",
        "is to run a stochastic searcher that varies between settings (a) and (b), fixing the best value found in the previous run.",
        "Here, for simplicity, we leave this to future work.",
        "sured using word-level precision (P), recall (R), and F-measure (F).",
        "G1 is (0.03, 0.38) and the best for G2 is (0.002, 0.36).",
        "On one hand, the performance ofG1 is consistently inferior to the baseline across all settings.",
        "Although approximation error was one possible cause, we noticed that the compression process was no longer properly regularized, since f(x, y) and the ?L estimate in the objective are intermingled.",
        "In this case, adjusting ?",
        "has little effect in balancing compression rate and complexity.",
        "The second objective G2, on the other hand, did not suffer as much from the aforementioned lack of regularization.",
        "We found that, in all three settings, G2 outperforms the baseline by 1 to 2 percentage points in F-measure.",
        "The best performance result achieved by G2 in our experiment is 81.7 in word-level F-measure, although this was obtained from search setting (c), using a heuristic ?",
        "value 0.37.",
        "It is interesting to note that G1 (b) and G2 (b) also gave very close estimates to this heuristic value.",
        "Nevertheless, it remains an open issue whether there is a connection between the optimal ?",
        "value and the true word/token ratio (?",
        "0.35 for Bernstein-Ratner corpus).",
        "The result has led us to conclude that MDL-based grid search is efficient in optimizing segmentation accuracy.",
        "Minimization of description length is in general aligned with performance improvement, although under finer granularity MDL-based search may not be as effec",
        "measure.",
        "We deliberately reproduced the results for adaptors grammar and regularized compression.",
        "The other measurements came directly from the literature.",
        "tive.",
        "In our experiment, search setting (b) won out on description length for both objectives, while the best performance was in fact achieved by the others.",
        "It would be interesting to confirm this by studying the correlation between description length and word-level F-measure.",
        "In Table 3, we summarize many published results for segmentation methods ever tested on the Bernstein-Ratner corpus.",
        "Of the proposed methods, we include only setting (b) since it is more general than the others.",
        "From Table 3, we find that the performance of G2 (b) is competitive to other state-of-the-art hierarchical Bayesian models and MDL methods, though it still lags 7 percentage points behind the best result achieved by adaptors grammar with colloc3-syllable.",
        "We also compare adaptors grammar to regularized compressor on average running time, which is shown in Table 4.",
        "On our test machine, it took roughly 15 hours for one instance of adaptors grammar with colloc3-syllable to run to the finish.",
        "Yet an improved regularized compressor could deliver the result in merely 1.25 second.",
        "In other words, even in an 100 ?",
        "100 grid search, the regularized compressor algorithm can still finish 4 to 5 times earlier than one single adaptors grammar instance."
      ]
    },
    {
      "heading": "6 Concluding Remarks",
      "text": [
        "In this paper, we derive a new lower-bound approximate to the objective function used in the regularized compression algorithm.",
        "As computing the approximate no longer relies on the change in lexicon entropy, the new compressor algorithm is made more efficient than the original.",
        "Besides run",
        "the Bernstein-Ratner corpus for adaptors grammar (per fold, based on trace output) and regularized compressors, tested on an Intel Xeon 2.5GHz 8 core machine with 8GB RAM.",
        "time efficiency, our experiment result also shows improved performance.",
        "Using MDL alone, one proposed method outperforms the original regularized compressor (Chen et al., 2012) in precision by 2 percentage points and in F-measure by 1.",
        "Its performance is only second to the state of the art, achieved by adaptors grammar with colloc3-syllable (Johnson and Goldwater, 2009).",
        "A natural extension of this work is to reproduce this result on some other word segmentation benchmarks, specifically those in other Asian languages (Emerson, 2005; Zhikov et al., 2010).",
        "Furthermore, it would be interesting to investigate stochastic optimization techniques for regularized compression that simultaneously fit both ?",
        "and ?.",
        "We believe this would be the key to adapt the algorithm to larger datasets."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank the anonymous reviewers for their valuable feedback."
      ]
    }
  ]
}
