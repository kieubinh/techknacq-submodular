{
  "info": {
    "authors": [
      "Zede Zhu",
      "Miao Li",
      "Lei Chen",
      "Zhenxin Yang"
    ],
    "book": "ACL",
    "id": "acl-P13-2050",
    "title": "Building Comparable Corpora Based on Bilingual LDA Model",
    "url": "https://aclweb.org/anthology/P13-2050",
    "year": 2013
  },
  "references": [
    "acl-C10-1073",
    "acl-D09-1092",
    "acl-E09-1003",
    "acl-E09-1096",
    "acl-N12-1065",
    "acl-P11-2084",
    "acl-W09-3107"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Comparable corpora are important basic resources in cross-language information processing.",
        "However, the existing methods of building comparable corpora, which use inter-translate words and relative features, cannot evaluate the topical relation between document pairs.",
        "This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages.",
        "Experiments show that the novel method can obtain similar documents with consistent topics own better adaptability and stability performance."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Comparable corpora can be mined fine-grained translation equivalents, such as bilingual terminologies, named entities and parallel sentences, to support the bilingual lexicography, statistical machine translation and cross-language information retrieval (AbduI-Rauf et al., 2009).",
        "Comparable corpora are defined as pairs of monolingual corpora selected according to the criteria of content similarity but non-direct translation in different languages, which reduces limitation of matching source language and target language documents.",
        "Thus comparable corpora have the advantage over parallel corpora in which they are more up-to-date, abundant and accessible (Ji, 2009).",
        "Many works, which focused on the exploitation of building comparable corpora, were proposed in the past years.",
        "Tao et al. (2005) acquired comparable corpora based on the truth that terms are inter-translation in different languages if they have similar frequency correlation at the same time periods.",
        "Talvensaari et al. (2007) extracted appropriate keywords from the source language documents and translated them into the target language, which were regarded as the query words to retrieve similar target documents.",
        "Thuy et al. (2009) analyzed document similarity based on the publication dates, linguistic independent units, bilingual dictionaries and word frequency distributions.",
        "Otero et al. (2010) took advantage of the translation equivalents inserted in Wikipedia by means of interlanguage links to extract similar articles.",
        "Bo et al. (2010) proposed a comparability measure based on the expectation of finding the translation for each word.",
        "The above studies rely on the high coverage of the original bilingual knowledge and a specific data source together with the translation vocabularies, co-occurrence information and language links.",
        "However, the severest problem is that they cannot understand semantic information.",
        "The new studies seek to match similar documents on topic level to solve the traditional problems.",
        "Pre-iss (2012) transformed the source language topical model to the target language and classified probability distribution of topics in the same language, whose shortcoming is that the effect of model translation seriously hampers the comparable corpora quality.",
        "Ni et al. (2009) adapted monolingual topic model to bilingual topic model in which the documents of a concept unit in different languages were assumed to share identical topic distribution.",
        "Bilingual topic model is widely adopted to mine translation equivalents from multi-language documents (Mimno et al., 2009; Ivan et al., 2011).",
        "Based on the bilingual topic model, this paper predicts the topical structure of documents in different languages and calculates the similarity of topics over documents to build comparable corpora.",
        "The paper concretely includes: 1) Introduce the Bilingual LDA (Latent Dirichlet Allocation) model which builds comparable corpora and improves the efficiency of matching similar documents; 2) Design a novel method of TFIDF",
        "to enhance the distinguishing ability of topics from different documents; 3) Propose a tailored",
        "method of conditional probability to calculate document similarity; 4) Address a language-independent study which isn't limited to a particular data source in any language."
      ]
    },
    {
      "heading": "2 Bilingual LDA Model",
      "text": []
    },
    {
      "heading": "2.1 Standard LDA",
      "text": [
        "LDA model (Blei et al., 2003) represents the latent topic of the document distribution by Dirichlet distribution with a K-dimensional implicit random variable, which is transformed into a complete generative model when ?",
        "is exerted to Dirichlet distribution (Griffiths et al., 2004) (Shown in Fig. 1), ?",
        "m?",
        ",m n?",
        ",m n?",
        "?",
        "where ?",
        "and ?",
        "denote the parameters distributed by Dirichlet; K denotes the topic numbers; k?",
        "denotes the vocabulary probability distribution in the topic k; M denotes the document number; m?",
        "denotes the topic probability distribution in the document m; Nm denotes the length of m; ,m n?",
        "and ?m,n denote the topic and the word in m re-spectively."
      ]
    },
    {
      "heading": "2.2 Bilingual LDA",
      "text": [
        "Bilingual LDA is a bilingual extension of a standard LDA model.",
        "It takes advantage of the document alignment which shares the same topic distribution m?",
        "and uses different word distributions for each topic (Shown in Fig. 2), where S and T denote source language and target language respectively.",
        "m n?",
        "are drawn using , ( |)l lm n n mP ??",
        "??",
        "and , ,( |, )l l l lm n n m nP?",
        "?",
        "???",
        ".",
        "Giving the comparable corpora M, the distribution ,k v?",
        "can be obtained by sampling a new token as word v from a topic k. For new collection of documents M?",
        ", keeping ,k v?",
        ", the distribution ,lm k?",
        "?",
        "of sampling a topic k from document m?",
        "can be obtained as follows:",
        "where ( )lkmn?",
        "denotes the total number of times that the document m?",
        "is assigned to the topic k. 3 Building comparable corpora Based on the bilingual LDA model, building comparable corpora includes several steps to generate the bilingual topic model ,k v?",
        "from the given bilingual corpora, predict the topic distribution ,lm k?",
        "?",
        "of the new documents, calculate the similarity of documents and select the largest similar document pairs.",
        "The key step is that the document similarity is calculated to align the source language document Sm?",
        "with relevant target language document Tm?",
        ".",
        "As one general way of expressing similarity, the Kullback-Leibler (KL) Divergence is adopted to measure the document similarity by topic dis",
        "The remainder section focuses on other two methods of calculating document similarity."
      ]
    },
    {
      "heading": "3.1 Cosine Similarity",
      "text": [
        "The similarity between Sm?",
        "and Tm?",
        "can be measured by Topic Frequency-Inverse Document Frequency.",
        "It gives high weights to the topic which appears frequently in a specific document and rarely appears in other documents.",
        "Then the relation between ,SmTFIDF ??",
        "and ,TmTFIDF ??",
        "is measured by Cosine Similarity (CS).",
        "Similar to Term Frequency-Inverse Document Frequency (Manning et al.,1999), Topic Frequency (TF) denoting frequency of topic ?",
        "for the document lm?",
        "is denoted by ( |)lP m?",
        "?",
        ".",
        "Given a constant value?",
        ", Inverse Document Frequency (IDF) is defined as the total number of documents M?",
        "divided by the number of documents",
        "Thus, the TFIDF score of the topic k over document lm?",
        "is given by:"
      ]
    },
    {
      "heading": "3.2 Conditional Probability",
      "text": [
        "The similarity between Sm?",
        "and Tm?",
        "is defined as the Conditional Probability (CP) of documents",
        "sponse to the cue Sm?",
        ".",
        "( )P ?",
        "as prior topic distribution is assumed a uniform distribution and satisfied the condition ( ) ( )kP P?",
        "?",
        "?",
        ".",
        "According to the total probability formula, the document Tm?",
        "is given as:",
        "Based on the Bayesian formula, the probability that a given topic ?",
        "is assigned to a particular target language document Tm?",
        "is expressed:",
        "that all topics ?",
        "are assigned to a particular document Tm?",
        "is a constant?",
        ", thus equation (8) is converted as follows:",
        "According to the total probability formula, the similarity between Sm?",
        "and Tm?",
        "is given by:"
      ]
    },
    {
      "heading": "4 Experiments and analysis",
      "text": []
    },
    {
      "heading": "4.1 Datasets and Evaluation",
      "text": [
        "The experiments are conducted on two sets of Chinese-English comparable corpora.",
        "The first dataset is news corpora with 3254 comparable document pairs, from which 200 pairs are randomly selected as the test dataset News-Test and the remainder is the training dataset News-Train.",
        "The second dataset contains 8317 bilingual Wikipedia entry pairs, from which 200 pairs are randomly selected as the test dataset Wiki-Test and the remainder is the training dataset Wiki-Train.",
        "Then News-Train and Wiki-Train are merged into the training dataset NW-Train.",
        "And the hand-labeled gold standard namely NW-Test is composed of News-Test and Wiki-Test.",
        "Braschler et al. (1998) used five levels of relevance to assess the alignments as follows: Same Story, Related Story, Shared Aspect, Common Terminology and Unrelated.",
        "The paper selects the documents with Same Story and Related Story as comparable corpora.",
        "Let Cp be the comparable corpora in the building result and Cl be the comparable corpora in the labeled result.",
        "The"
      ]
    },
    {
      "heading": "4.2 Results and analysis",
      "text": [
        "Two groups of validation experiments are set with sampling frequency of 1000, parameter ?",
        "of 50/K, parameter ?",
        "of 0.01 and topic number K of 600.",
        "Group 1: Different data source We learn bilingual LDA models by taking different training datasets.",
        "The performance of three approaches (KL, CS and CP) is examined on different test datasets.",
        "Tab.",
        "1 demonstrates these results with the winners for each algorithm in bold.",
        "The results indicate the robustness and effectiveness of these algorithms.",
        "The performance of algorithms on Wiki-Train is much better than News-Train.",
        "The main reason is that Wiki-Train is an extensive snapshot of human knowledge which can cover most topics talked in News-Train.",
        "The probability of vocabularies among the test dataset which have not appeared in the training data is very low.",
        "And then the document topic can effectively concentrate all the vocabularies?",
        "expressions.",
        "The topic model slightly faces with the problem of knowledge migration issue, so the performance of the topic model trained by Wiki-Train shows a slight decline in the experiments on News-Test.",
        "CS shows the strongest performance among the three algorithms to recognize the document pairs with similar topics.",
        "CP has almost equivalent performance with CS.",
        "Comparing the equation (5) and (6) with (10), we can find out that CP is similar to a simplified CS.",
        "CP can improve the operating efficiency and decrease the performance.",
        "The performance achieved by KL is the weakest and there is a large gap between KL and others.",
        "In addition, the shortage of KL is that when the exchange between the source language and the target language documents takes place, different evaluations will occur in the same document pairs.",
        "We adopt the NW-Train and NW-Test as training set and test set respectively, and utilize the CS algorithm to calculate the document similarity to verify the excellence of methods in the study.",
        "Then we compare its performance with the existing representative approaches proposed by Thuy et al. (2009) and Preiss (2012) (Shown in Tab.",
        "2).",
        "The table shows CS outperforms other algorithms, which indicates that bilingual LDA is valid to construct comparable corpora.",
        "Thuy et al. (2009) matches similar documents in the view of inter-translated vocabulary and co-occurrence information features, which cannot understand the content effectively.",
        "Preiss (2012) uses monolingual training dataset to generate topic model and translates source language topic model into target language topic model respectively.",
        "Yet the translation accuracy constrains the matching effectiveness of similar documents, and the cosine similarity is directly used to calculate document-topic similarity failing to highlight the topic contributions of different documents."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "This study proposes a new method of using bilingual topic to match similar documents.",
        "When CS is used to match the documents, TFIDF is proposed to enhance the topic discrepancies among different documents.",
        "The method of CP is also addressed to measure document similarity.",
        "Experimental results show that the matching algorithm is superior to the existing algorithms.",
        "It can utilize comprehensively large scales of document information in training set to avoid the information deficiency of the document itself and over-reliance on bilingual knowledge.",
        "The algorithm makes the document match on the basis of understanding the document.",
        "This study does not calculate similar contents existed in the monolingual documents.",
        "However, a large number of documents in the same language describe the same event.",
        "We intend to incorporate monolingual document similarity into bilingual topics analysis to match multi-documents in different languages perfectly."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The work is supported by the National Natural Science Foundation of China under No.",
        "61070099 and the project of MSR-CNIC Windows Azure Theme."
      ]
    }
  ]
}
