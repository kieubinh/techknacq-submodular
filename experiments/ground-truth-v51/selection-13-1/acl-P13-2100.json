{
  "info": {
    "authors": [
      "Gerasimos Lampouras",
      "Ion Androutsopoulos"
    ],
    "book": "ACL",
    "id": "acl-P13-2100",
    "title": "Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts",
    "url": "https://aclweb.org/anthology/P13-2100",
    "year": 2013
  },
  "references": [
    "acl-C10-2128",
    "acl-D12-1022",
    "acl-E09-2005",
    "acl-H05-1042",
    "acl-N06-1046",
    "acl-N12-1093",
    "acl-P04-1051",
    "acl-P09-1011",
    "acl-P11-1049",
    "acl-P12-1039",
    "acl-P84-1107",
    "acl-W05-0618"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present an ILP model of concept-to-text generation.",
        "Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicaliza-tion, and aggregation to avoid greedy decisions and produce more compact texts."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000).",
        "With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al., 2008) in natural language (Bontcheva, 2005; Mellish and Sun, 2006; Gala-nis and Androutsopoulos, 2007; Mellish and Pan, 2008; Schwitter et al., 2008; Schwitter, 2010; Liang et al., 2011; Williams et al., 2011).",
        "NLG systems typically employ a pipeline architecture.",
        "They usually start by selecting the logical facts to express.",
        "The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text.",
        "Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence.",
        "Sentence aggregation then combines sentences into longer ones.",
        "Another component generates appropriate referring expressions, and surface realization produces the final text.",
        "Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified.",
        "This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be codependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008).",
        "For example, content selection and lexicalization may lead to more or fewer sentence aggregation opportunities.",
        "We present an Integer Linear Programming (ILP) model that combines content selection, lexicalization, and sentence aggregation.",
        "Our model does not consider text planning, nor referring expression generation, which we hope to include in future work, but it is combined with an external simple text planner and a referring expression generation component; we also do not discuss surface realization.",
        "Unlike pipeline architectures, our model jointly examines the possible choices in the three NLG stages it considers, to avoid greedy local decisions.",
        "Given an individual (entity) or class of an OWL ontology and a set of facts (OWL axioms) about the individual or class, we aim to produce a text that expresses as many of the facts in as few words as possible.",
        "This is important when space is limited or expensive (e.g., product descriptions on smartphones, advertisements in search engines).",
        "Although the search space of our model is very large and ILP problems are in general NP-hard, ILP solvers can be used, they are very fast in practice, and they guarantee finding a global optimum.",
        "Experiments show that our ILP model outperforms, in terms of compression, an NLG system that uses the same components, but connected in a pipeline, with no deterioration in fluency and clarity."
      ]
    },
    {
      "heading": "2 Related work",
      "text": [
        "Marciniak and Strube (2005) propose a general ILP approach for language processing applications where the decisions of classifiers that consider particular, but co-dependent, subtasks need to be combined.",
        "They also show how their approach can be used to generate multi-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider.",
        "Barzilay and Lapata (2005) treat content selection as an optimization problem.",
        "Given a pool of facts and scores indicating the importance of each",
        "fact or pair of facts, they select the facts to express by formulating an optimization problem similar to energy minimization.",
        "In other work, Barzilay and Lapata (2006) consider sentence aggregation.",
        "Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets.",
        "Sentences expressing facts that are placed in the same subset are aggregated to form a longer sentence.",
        "An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset.",
        "Althaus et al. (2004) show that ordering a set of sentences to maximize sentence-to-sentence coherence is equivalent to the traveling salesman problem and, hence, NP-complete.",
        "They also show how an ILP solver can be used in practice.",
        "Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al., 2011; Galanis et al., 2012; Woodsend and Lapata, 2012), where the input is text, not formal knowledge rep-resetations.",
        "Statistical methods to jointly perform content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al., 2009; Konstas and Lapata, 2012a; Konstas and Lapata, 2012b), but they are currently limited to generating single sentences from flat records.",
        "To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation.",
        "It is also the first article to consider ILP in NLG from OWL ontologies."
      ]
    },
    {
      "heading": "3 Our ILP model of NLG",
      "text": [
        "Let F = {f1, .",
        ".",
        ".",
        ", fn} be the set of all the facts fi (OWL axioms) about the individual or class to be described.",
        "OWL axioms can be represented as sets of RDF triples of the form ?S,R,O?, where S is an individual or class, O is another individual, class, or datatype value, and R is a relation (property) that connects S to O.",
        "Hence, we can assume that each fact fi is a triple ?Si, Ri, Oi?.1 For each fact fi, a set Pi = {pi1, pi2, .",
        ".",
        ". }",
        "of alternative sentence plans is available.",
        "Each 1We actually convert the RDF triples to simpler message triples, so that each message triple can be easily expressed by a simple sentence, but we do not discuss this conversion here.",
        "sentence plan pik specifies how to express fi = ?Si, Ri, Oi?",
        "as an alternative single sentence.",
        "In our work, a sentence plan is a sequence of slots, along with instructions specifying how to fill the slots in; and each sentence plan is associated with the relations it can express.",
        "For example, ?exhibit12,foundIn,athens?",
        "could be expressed using a sentence plan like ?",
        "[ref (S)] [findpast] [in] [ref (O)]?, where square brackets denote slots, ref (S) and ref (O) are instructions requiring referring expressions for S and O in the corresponding slots, and ?findpast?",
        "requires the simple past form of ?find?.",
        "In our example, the sentence plan would lead to a sentence like ?Exhibit 12 was found in Athens?.",
        "We call elements the slots with their instructions, but with ?S?",
        "and ?O?",
        "accompanied by the individuals, classes, or datatype values they refer to; in our example, the elements are ?",
        "[ref (S: exhibit12)]?, ?",
        "[findpast]?, ?",
        "[in]?, ?",
        "[ref (O: athens)]?.",
        "Different sentence plans may lead to more or fewer aggregation opportunities; for example, sentences with the same verb are easier to aggregate.",
        "We use aggregation rules (Dalianis, 1999) that operate on sentence plans and usually lead to shorter texts.",
        "Let s1, .",
        ".",
        ".",
        ", sm be disjoint subsets of F , each containing 0 to n facts, with m < n. A single sentence is generated for each subset sj by aggregating the sentences (more precisely, the sentence plans) expressing the facts of sj .2 An empty sj generates no sentence, i.e., the resulting text can be at most m sentences long.",
        "Let us also define:",
        "1, if sentence plan pik is used to express fact fi, and fi is in subset sj 0, otherwise (2)",
        "and let B be the set of all the distinct elements (no duplicates) from all the available sentence plans that can express the facts of F .",
        "The length of an aggregated sentence resulting from a subset sj can be roughly estimated by counting the distinct elements of the sentence plans that have been chosen to express the facts of sj ; elements that occur more than once in the chosen sentence plans of sj 2All the sentences of every possible subset sj can be aggregated, because all the sentences share the same subject, the class or individual being described.",
        "If multiple aggregation rules apply, we use the one that leads to a shorter text.",
        "are counted only once, because they will probably be expressed only once, due to aggregation.",
        "Our objective function (4) maximizes the number of selected facts fi and minimizes the number of distinct elements in each subset sj , i.e., the approximate length of the corresponding aggregated sentence; an alternative explanation is that by minimizing the number of distinct elements in each sj , we favor subsets that aggregate well.",
        "By a and b we jointly denote all the ai and btj variables.",
        "The two parts (sums) of the objective function are normalized to [0, 1] by dividing by the total number of available facts |F |and the number of subsets m times the total number of distinct elements |B|.",
        "In the first part of the objective, we treat all the facts as equally important; if importance scores are also available for the facts, they can be added as multipliers of ?i.",
        "The parameters ?1 and ?2 are used to tune the priority given to expressing many facts vs. generating shorter texts; we set ?1 + ?2 = 1.",
        "Constraint 5 ensures that for each selected fact, only one sentence plan in only one subset is selected; if a fact is not selected, no sentence plan for the fact is selected either.",
        "|?",
        "|denotes the cardinality of a set ?.",
        "In constraint 6, Bik is the set of distinct elements et of the sentence plan pik.",
        "This constraint ensures that if pik is selected in a subset sj , then all the elements of pik are also present in sj .",
        "If pik is not selected in sj , then some of its elements may still be present in sj , if they appear in another selected sentence plan of sj .",
        "In constraint 7, P (et) is the set of sentence plans that contain element et.",
        "If et is used in a subset sj , then at least one of the sentence plans of P (et) must also be selected in sj .",
        "If et is not used in sj , then no sentence plan of P (et) may be selected in sj .",
        "Lastly, constraint 8 limits the number of elements that a subset sj can contain to a maximum allowed number Bmax, in effect limiting the maximum length of an aggregated sentence.",
        "We assume that each relation R has been manually mapped to a single topical section; e.g., relations expressing the color, body, and flavor of a wine may be grouped in one section, and relations about the wine's producer in another.",
        "The section of a fact fi = ?Si, Ri, Oi?",
        "is the section of its relation Ri.",
        "Constraint 9 ensures that facts from different sections will not be placed in the same subset sj , to avoid unnatural aggregations."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We used NaturalOWL (Galanis and Androutsopoulos, 2007; Galanis et al., 2009; Androutsopoulos et al., 2013), an NLG system for OWL ontologies that relies on a pipeline of content selection, text planning, lexicalization, aggregation, referring expression generation, and surface realization.3 We modified content selection, lexicalization, and aggregation to use our ILP model, maintaining the aggregation rules of the original system.4 For referring expression generation and surface realization, the new system, called ILPNLG, invokes the corresponding components of NaturalOWL.",
        "The original system, called PIPELINE, assumes that each relation has been mapped to a topical section, as in ILPNLG.",
        "It also assumes that a manually specified order of the sections and the relations of each section is available, which is used by the text planner to order the selected facts (by their relations).",
        "The subsequent components of the pipeline are not allowed to change the order of the facts, and aggregation operates only on sentence plans of adjacent facts from the same section.",
        "In ILPNLG, the manually specified order of sections and relations is used to order the sentences of each subset sj (before aggregating them), the aggregated sentences in each section (each aggregated sentence inherits the minimum order of its constituents), and the sections (with their sentences).",
        "We used the Wine Ontology, which had been",
        "used in previous experiments with PIPELINE.5 We kept the 2 topical sections, the ordering of sections and relations, and the sentence plans that had been used in the previous experiments, but we added more sentence plans to ensure that 3 sentence plans were available per fact.",
        "We generated texts for the 52 wine individuals of the ontology; we did not experiment with texts describing classes of wines, because we could not think of multiple alternative sentence plans for many of their axioms.",
        "For each individual, there were 5 facts on average and a maximum of 6 facts.",
        "PIPELINE has a parameter M specifying the maximum number of facts it is allowed to report per text.",
        "When M is smaller than the number of available facts |F |and all the facts are treated as equally important, as in our experiments, it selects randomly M of the available facts.",
        "We repeated the generation of PIPELINE's texts for the 52 individuals for M = 2, 3, 4, 5, 6.",
        "For each M , the texts of PIPELINE for the 52 individuals were generated three times, each time using one of the different alternative sentence plans of each relation.",
        "We also generated the texts using a variant of PIPELINE, dubbed PIPELINESHORT, which always selects the shortest (in elements) sentence plan among the available ones.",
        "In all cases, PIPELINE and PIPELINESHORT were allowed to form aggregated sentences containing up to Bmax = 22 distinct elements, which was the number of distinct elements of the longest aggregated sentence in the previous experiments, where PIPELINE was allowed to aggregate up to 3 original sentences.",
        "With ILPNLG, we repeated the generation of the texts of the 52 individuals using different values of ?1 (?2 = 1 ?",
        "?1), which led to texts expressing from zero to all of the available facts.",
        "We set the maximum number of fact subsets to m = 3, which was the maximum number of aggregated sentences observed in the texts of PIPELINE and PIPELINESHORT.",
        "Again, we set Bmax = 22.",
        "We compared ILPNLG to PIPELINE and PIPELINESHORT by measuring the average number of facts they reported divided by the average text length (in words).",
        "Figure 1 shows this ratio as a function of the average number of reported facts, along with 95% confidence intervals (of sample means).",
        "PIPELINESHORT achieved better results than PIPELINE, but the differences were small.",
        "For ?1 < 0.2, ILPNLG produces empty texts,",
        "since it focuses on minimizing the number of distinct elements of each text.",
        "For ?1 ?",
        "0.225, it performs better than the other systems.",
        "For ?1 ?",
        "0.3, it obtains the highest fact/words ratio by selecting the facts and sentence plans that lead to the most compressive aggregations.",
        "For greater values of ?1, it selects additional facts whose sentence plans do not aggregate that well, which is why the ratio declines.",
        "For small numbers of facts, the two pipeline systems select facts and sentence plans that offer very few aggregation opportunities; as the number of selected facts increases, some more aggregation opportunities arise, which is why the facts/words ratio of the two systems improves.",
        "In all the experiments, the ILP solver was very fast (average: 0.08 sec, worst: 0.14 sec).",
        "Experiments with human judges also showed that the texts of ILPNLG cannot be distinguished from those of PIPELINESHORT in terms of fluency and text clarity.",
        "Hence, the highest compactness of the texts of ILPNLG does not come at the expense of lower text quality.",
        "Space does not permit a more detailed description of these experiments.",
        "We show below texts produced by PIPELINE (M = 4) and ILPNLG (?1 = 0.3).",
        "PIPELINE: This is a strong Sauternes.",
        "It is made from Semil-lon grapes and it is produced by Chateau D?ychem.",
        "ILPNLG: This is a strong Sauternes.",
        "It is made from Semillon grapes by Chateau D?ychem.",
        "PIPELINE: This is a full Riesling and it has moderate flavor.",
        "It is produced by Volrad.",
        "ILPNLG: This is a full sweet moderate Riesling.",
        "In the first pair, PIPELINE uses different verbs for the grapes and producer, whereas ILPNLG uses the same verb, which leads to a more compressive aggregation; both texts describe the same wine and report 4 facts.",
        "In the second pair, ILPNLG has chosen to express the sweetness instead of the producer, and uses the same verb (?be?)",
        "for all the facts, leading to a shorter sentence; again both texts describe the same wine and report 4 facts.",
        "In both examples, some facts are not aggregated because they belong in different sections."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "We presented an ILP model for NLG that jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy local decisions and produce more compact texts.",
        "Experiments verified that our model can express more facts per word, compared to a pipeline, which is important when space is scarce.",
        "An off-the-shelf ILP solver took approximately 0.1 sec for each text.",
        "We plan to extend our model to include text planning and referring expressions generation."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": []
    }
  ]
}
