{
  "info": {
    "authors": [
      "Mikhail Kozhevnikov",
      "Ivan Titov"
    ],
    "book": "*SEM",
    "id": "acl-S13-1044",
    "title": "Bootstrapping Semantic Role Labelers from Parallel Data",
    "url": "https://aclweb.org/anthology/S13-1044",
    "year": 2013
  },
  "references": [
    "acl-C10-1011",
    "acl-C10-1064",
    "acl-D09-1002",
    "acl-D10-1030",
    "acl-D11-1006",
    "acl-D12-1001",
    "acl-E12-1003",
    "acl-H05-1108",
    "acl-I08-3008",
    "acl-J03-1002",
    "acl-J05-1004",
    "acl-J08-2006",
    "acl-N10-1137",
    "acl-P11-1112",
    "acl-P11-1149",
    "acl-P11-2052",
    "acl-P11-2120",
    "acl-P12-1066",
    "acl-P12-1068",
    "acl-P12-2025",
    "acl-P98-1013",
    "acl-W01-0715",
    "acl-W05-0309",
    "acl-W08-0336",
    "acl-W09-1201",
    "acl-W09-1206",
    "acl-W10-0801",
    "acl-W12-3404"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present an approach which uses the similarity in semantic structure of bilingual parallel sentences to bootstrap a pair of semantic role labeling (SRL) models.",
        "The setting is similar to co-training, except for the intermediate model required to convert the SRL structure between the two annotation schemes used for different languages.",
        "Our approach can facilitate the construction of SRL models for resource-poor languages, while preserving the annotation schemes designed for the target language and making use of the limited resources available for it.",
        "We evaluate the model on four language pairs, English vs German, Spanish, Czech and Chinese.",
        "Consistent improvements are observed over the self-training baseline."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The success of statistical modeling methods in a variety of natural language processing (NLP) tasks in the last decade depended crucially on the availability of annotated resources for their training.",
        "And while sizable resources for most standard tasks are only available for a few languages, the human effort required to achieve reasonable performance on such tasks for other languages may be significantly reduced by leveraging existing resources and the similarities between languages.",
        "This idea has lead to the development of cross-lingual annotation projection approaches, which make use of parallel corpora (Pado?",
        "and Lapata, 2009), as well as attempts to adapt models directly to other languages (McDonald et al., 2011).",
        "In this paper we consider correspondences between SRL structures in translated sentences from a different perspective.",
        "Most cross-lingual annotation projection approaches transfer the source language annotation scheme to the target language without modification, which makes it hard to combine their output with existing target language resources, as annotation schemes may vary significantly.",
        "We instead address the problem of information transfer between two existing annotation schemes (figure 1) for a pair of languages using an intermediate model of role correspondence (RCM).",
        "An RCM models the probability of a pair of corresponding arguments being assigned a certain pair of roles.",
        "We then use it to guide a pair of monolingual models toward compatible predictions on parallel data in order to extend the coverage and/or accuracy of one or both models.",
        "example.",
        "The notion of compatibility here is highly non-trivial, even for sentences translated as close to the original as possible.",
        "Zhuang and Zong (2010), for example, observe that in the English-Chinese parallel PropBank (Palmer et al., 2005b) corresponding arguments often bear different labels, even though the same inventory of semantic roles is used for both",
        "languages and the annotation guidelines are similar.",
        "When different annotation schemes are considered, the problem is further complicated by the difference in the granularity of semantic roles used and varying notions of what is an argument and what is not.",
        "Manually annotated training data for such a model is hard to come by.",
        "Instead, we propose an iterative procedure similar to bootstrapping, where the parameters of the RCM are initially estimated from a parallel corpus automatically annotated with semantic roles using the monolingual models independently, and then the RCM is used to refine these annotations via a joint inference procedure, serving to enforce consistency on the predictions of monolingual models on parallel sentences.",
        "The obtained annotations on the parallel corpus are expected to be of higher quality than the independent predictions of the models, so they can be used to improve the SRL models?",
        "performance and/or coverage.",
        "We evaluate this approach by augmenting the original training data with the annotations obtained on parallel data and observing the change in the model's performance.",
        "This is especially useful if one of the languages is relatively poor in resources, in which case the proposed procedure will help propagate information from the stronger model to the weaker one.",
        "Even if the two models are comparable in their predictive power, we may be able to benefit from the fact that certain semantic roles are realized less ambiguously in one language than in another.",
        "We will henceforth refer to these two alternatives as the projection and symmetric setups.",
        "The paper is structured as follows.",
        "In the next section we present our approach and discuss the issues of role correspondence modeling, then describe the implementation and datasets used in evaluation in section 3, present the evaluation and results in section 4 and conclude with the discussion of related work in section 5."
      ]
    },
    {
      "heading": "2 Approach",
      "text": [
        "We consider bootstrapping a pair of SRL models on a parallel corpus, using the correspondence between their predictions on parallel sentences to guide the learning.",
        "The models are forced toward compatible predictions, where the notion of compatibility is defined by a (statistical) role correspondence model.",
        "Let us consider a pair of languages, ?",
        "and ?, and their corresponding datasets T 0?",
        "and T",
        "?",
        ", annotated with semantic roles (the upper indices here denote the iteration number).",
        "We will refer to these as the initial training sets.",
        "We also assume that a word-aligned parallel corpus is available for the pair of languages, which we denote P , with the predicates and their respective arguments identified on both sides.",
        "The procedure is then as follows: we train monolingual models M0?",
        "and M",
        "?",
        ", respectively, apply them to the two sides of the parallel corpus, resulting in a labeling P 0.",
        "We collect the semantic role co-occurrence information and train the role correspondence model C0 on it, then proceed to the joint inference step involving M0?, M",
        "resulting in a refined labeling P 1 of the parallel corpus.",
        "The two sides of the P 1 are then used to augment the initial training sets, yielding T 1?",
        "and T",
        "?",
        ", and new models M1?",
        "and M",
        "?",
        "are trained on these.",
        "The process can then be repeated using M1?",
        "and M",
        "?",
        "instead of the initial models.",
        "We report the model's performance on a held-out test set, drawn from the same corpus as the corresponding initial training set.",
        "The procedure can be seen as a form of co-training (Blum and Mitchell, 1998) of a pair of monolingual SRL models.",
        "In our case, however, the question of the models?",
        "agreement is not as trivial as in most applications of co-training, requiring a statistical model of its own (Ci).",
        "In the low-resource (projection) setup our approach is also similar to self-training with weak supervision coming from the stronger model.",
        "Note that although the approach is iterative, we have observed no significant improvements from repeating the procedure, possibly owing to the noise introduced by the errors in preprocessing.",
        "In the evaluation we run only one iteration.",
        "In the notation introduced above, the self-training baseline model (SELF) is trained on P 0?",
        ", the joint model (JOINT) ?",
        "on P 1?",
        "and the combined model (COMB) ?",
        "on T"
      ]
    },
    {
      "heading": "2.1 Modeling Role Correspondence",
      "text": [
        "It is necessary to distinguish between semantic roles and their interpretation in a particular context.",
        "The former can be defined in a variety of",
        "ways, depending on the formalism used.",
        "In case of FrameNet (Baker et al., 1998), for example, the interpretation of a semantic role (frame element) is explicitly provided for each separate frame, so a frame and a frame element label together describe the semantics of an argument.",
        "PropBank (Palmer et al., 2005a) follows a mixed strategy ?",
        "the labels for a relatively small set of core roles are numbered and their interpretations are provided separately for each predicate (although those of the first two roles, A0 and A1, consistently denote what is known as Proto-Agent and Proto-Patient), while modifiers (Merlo and Leybold, 2001) bear labels that are interpreted consistently across all predicates.",
        "Other resources, such as Prague Dependency Treebank (Hajic?",
        "et al, 2006), use a single set of semantic roles (functors), which are interpretable across different predicates.",
        "From the standpoint of defining the semantic similarity of parallel sentences, the important implication is that we cannot assume that the corresponding arguments should bear the same label, even if the annotation schemes used are compatible (Zhuang and Zong, 2010).",
        "Nor can we write down a single mapping between the roles that will be valid across different predicates (figure 2), which motivates the need for a statistical model of semantic role correspondence.",
        "I do not have these concerns Yo no tengo tales preocupaciones",
        "corresponds to art0-agt, art1-tem or art2-ben, depending on the predicate.",
        "We assume the existence of a one-to-one mapping between semantic roles for a given predicate pair.",
        "As the mappings are not completely independent ?",
        "at least some roles have the same interpretation across different predicate pairs, ?",
        "we choose to build a single model, which relies on features derived from the pair of predicates in question, rather than create a separate model for each predicate pair.",
        "The model can then make decisions specific to particular predicates or predicate pairs, where sufficient data has been observed or back off to a generic mapping where there is not enough data.",
        "For the purpose of this study, we choose to separately model the probability of a target role, given the source one and the necessary contextual information and vice versa.",
        "These two components are referred to as projection models and realized as a pair of linear classifiers.",
        "Training such a model in a conventional fashion would require a rather specific kind of dataset, namely a parallel corpus annotated with semantic roles, and assuming the availability of such data would severely limit the applicability of the approach proposed, as, to our knowledge, it is currently only available for two language pairs, namely English-Chinese (Palmer et al., 2005b) and English-Czech (Hajic?",
        "et al, 2012).",
        "We instead use the automatically produced annotations on a parallel corpus, effectively enforcing consistency on the role correspondence in the monolingual models?",
        "predictions."
      ]
    },
    {
      "heading": "2.2 Joint Inference",
      "text": [
        "The joint inference would have been simplest if the arguments were classified independently.",
        "This assumption is too restrictive, though, since the interdependencies between the arguments can be used to improve the accuracy of semantic role labeling (Roth and Yih, 2005).",
        "In the projection setup we assume that the model for one of the languages, which we will henceforth refer to as source, is much better informed than the one for the other language, referred to as target, so we only have to propagate the information one way.",
        "The scoring functions of these two models will be denoted fs and ft, respectively, and that of the projection model from source to target ?",
        "fst.",
        "Source and target sentences are denoted Ss and St,",
        "and aligned predicates in these sentences ?",
        "ps and pt.",
        "The task is then to identify the target language role assignment rt that would maximize the objective L(rt) = ?tft(rt, St, pt) + ?stfst(rt, rs, ps, pt), where rs = argmaxrfs(rs, Ss, ps) is the role assignment of the source-side arguments as predicted by the monolingual model and ?",
        "are the weights associated with the models.",
        "The exact maximization of this objective is computationally expensive, so we resort to an approximation.",
        "We chose to use the dual decomposition method primarily because it fits the structure of the objective particularly well (in that it is a sum of the objectives of two independent models) and since it allows a wide range of monolingual models to be used in this setup.",
        "The only requirement here is that the monolingual model must be able to incorporate a bias toward or away from a certain prediction.",
        "To apply this approximation, we decouple the rt variables into rt and rst and get L1(rt, rst) = ?tft(rt, St, pt) + ?stfst(rst, rs, ps, pt) under the condition that rt = rst.",
        "Applying the Lagrangian relaxation, we replace the hard equality constraint on rt and rst with a soft one, using slack variables ?, which results in the following objective:",
        "where i indexes aligned argument pairs and I is an indicator function.",
        "This is equivalent to",
        "are the augmented objectives of the two component models, incorporating bias factors on various possible predictions.",
        "The minimization with respect to ?",
        "is performed using a subgradient descent algorithm following Sontag et al. (2011).",
        "Whenever the method converges, it converges to the global maximum of the sum of the objectives.",
        "We found that in our case it reaches a solution within the first 1000 iterations over 99% of the time.",
        "If the models have comparable accuracy, the above inference procedure can be extended to perform projection both ways.",
        "Formulating this as a dual decomposition problem would require using three separate components, two for the monolingual models and one for the RCM, which would have to make its own predictions for the semantic roles on both sides without conditioning on the predictions of the monolingual models.",
        "This calls for a different kind of model than the one we use ?",
        "a model that will rely on a (possibly simplified) feature representation of the source and target arguments to jointly predict their labels.",
        "Instead, we perform the projection setup inference procedure in both directions simultaneously, interleaving gradient descent steps and allowing the projection models to access the updated predictions of the monolingual models.",
        "This results in a block gradient descent algorithm with the following updates:",
        "n+1 is the update rate function for step n, and gs and gts are defined as in (3), but with the direction reversed.",
        "This procedure allows us to use the same RCM implementation as in the projection setup.",
        "Moreover, the inference procedure for projection setup is",
        "a special case of this one with ?s(n) set to 0.",
        "The algorithm also demonstrates convergence similar to that of the projection version, although it lacks the optimality guarantees."
      ]
    },
    {
      "heading": "3 Experimental Setup",
      "text": [
        "We evaluate our approach on four language pairs, namely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively."
      ]
    },
    {
      "heading": "3.1 Parallel Data",
      "text": [
        "The parallel data for the first three language pairs is drawn from Europarl v6 (Koehn, 2005) and from MultiUN (Eisele and Chen, 2010) for English-Chinese.",
        "We applied Stanford Tokenizer for English, tokenizer scripts (Koehn, 2005) provided with the Europarl corpus to German, Spanish and Czech, and Stanford Chinese Segmenter (Chang et al., 2008) to Chinese, then performed POS-tagging, morphology tagging (where applicable) and dependency parsing using MATE-tools (Bohnet, 2010).",
        "Word alignments were acquired using GIZA++ (Och and Ney, 2003) with its standard settings.",
        "Predicate identification on the parallel data was done using the supervised classifiers of the monolingual SRL systems, except for German, where a simple heuristic had to be used instead, as only some of the predicates are marked in the training data, which makes it hard to train a supervised classifier.",
        "Following van der Plas et al. (2011), we then retain only those sentences where all identified predicates were aligned.",
        "In the experiments we used 50 thousand predicate pairs in each case, as increasing the amount further did not yield noticeable benefits, while increasing the running time."
      ]
    },
    {
      "heading": "3.2 Annotated Data",
      "text": [
        "The CoNLL?09 (Hajic?",
        "et al, 2009) datasets were used as a source of annotated data for all languages.",
        "Only verbal predicates were considered and predicted syntax was used in evaluation.",
        "We consider subsets of the training data in order to emulate the scenario with a resource-poor language.",
        "Due to the different sources the datasets were derived from, sentences contain different proportions of annotated predicates depending on the language.",
        "The German corpus, for example, contains about 6 times fewer argument labels per sentence than the English one.",
        "We will therefore indicate the sizes of the datasets used in the number of argument labels they contain, referred to as instances, rather than the number of predicates or sentences.",
        "The corpus for English, for example, contains 6.2 such instances per sentence on average.",
        "We use the 20 thousand instances of the available data as the training corpus for each language and split the rest equally between the development and the test set.",
        "The secondary (?out-of-domain?)",
        "test sets are preserved as they are.",
        "In dependency-based SRL, only heads of syntactic constituents are marked with semantic roles.",
        "The heads of corresponding arguments may or may not align, however, even if the arguments are lexically very similar, because their syntactic structure may differ.",
        "In general, one would have to identify the whole phrase for each argument and take into account the links between constituents, rather than single words (Pado?",
        "and Lapata, 2005).",
        "As reconstructing the constituents from the dependency tree is non-trivial (Hwang et al., 2010), we are using a heuristic to address the most common version of this problem, i.e. a preposition or an auxiliary verb being an argument head.",
        "In such a case we also take into account any alignment links involving the head's immediate descendants."
      ]
    },
    {
      "heading": "3.3 Implementation",
      "text": [
        "Our system is based on that of Bjo?rkelund et al. (2009).",
        "It is a pipeline system comprised of a set of binary or multiclass linear classifiers.",
        "Both here and in the projection model, the classifiers are trained using Liblinear (Fan et al., 2008).",
        "We employed a uniqueness constraint on role labels (Chang et al., 2007), preventing some of them from being assigned to more than one argument in the same predicate, which appears to be more reliable in a low-resource setting we consider than the reranker the original system employed.",
        "The constraint is enforced in the monolingual model inference using a beam-search approximation with the beam size of 10.",
        "The label uniqueness information was derived from the training sets."
      ]
    },
    {
      "heading": "3.4 The Projection Model",
      "text": [
        "Each projection model is realized by a single linear classifier applied to each argument pair independently.",
        "It relies on features derived from the source semantic role and source and target predicates, and predicts the semantic role for the argument in the target sentence.",
        "The features include the source semantic role and its conjunctions with (lowercased) forms and lem-mata of the source and target predicates.",
        "For example, assuming the source semantic role is A3 and the source and target predicates are went and ging (past tense of ?gehen?, German), the features would be as shown in figure 3."
      ]
    },
    {
      "heading": "3.5 Parameters",
      "text": [
        "In case of projection there are two parameters, ?st and ?t, ?",
        "the weights of the component models in the objective.",
        "Only their relative values matter (except in the choice of ?0), so we set ?t to 1 and only tune the weight of the role correspondence model.",
        "In the symmetric setup, the objective takes the form L(rt, rs) = ?tft(rt, St, pt) + ?stfst(rt, rs, ps, pt) + ?sfs(rs, Ss, ps) + ?tsfts(rs, rt, pt, ps).",
        "Since we assume that the two monolingual models here have comparable performance, we do not tune their relative weights, setting both 's and ?t to 1.",
        "We also use the same weight for both projection models, ?st = ?ts, and this value plays an important role ?",
        "it basically indicates how strongly we insist on the role correspondence models?",
        "correctness.",
        "If this weight is set to 0, the RCM will accept the initial predictions the monolingual models make, and if it is set to a sufficiently large value, the predictions of the monolingual models will be biased until they match the mapping suggested by the RCM.",
        "The optimal weight will therefore depend on the language pair, the sizes of the initial training sets and the RCM used.",
        "We use the value of 0.7 in all projection experiments and 0.5 in the symmetric setup, however, as excessive tuning may be undesirable in the low-resource setting."
      ]
    },
    {
      "heading": "3.6 Domains",
      "text": [
        "One important factor in the understanding of the evaluation figures presented is the fact that sources of annotated and parallel data belong to different domains.",
        "The former usually contains some sort of newswire text ?",
        "Wall Street Journal in case of English, Xinhua newswire, Hong Kong news and Sinorama news magazine for Chinese, etc.",
        "Parallel data, on the other hand, comes from the proceedings of European Parliament and United Nations, which are quite different.",
        "For example, the sentences in the latter domain often start with someone being addressed, either by name or by title, which can hardly be expected to occur as often in a newspaper or a magazine article.",
        "As is well-known, the performance of many statistical tools drops significantly outside the domain they were trained on (Pradhan et al., 2008), and the preprocessing and SRL models used here are no exception, which results in relatively low quality of the initial predictions on the parallel text.",
        "The low argument identification performance, in particular, is presumably due to inaccurate dependency parses, on which it heavily relies.",
        "Several approached have been proposed to improve the accuracy of dependency parsers and other tools on out-of-domain data, but this is beyond the scope of this paper.",
        "In some cases (though seldom), sources of parallel data belonging to the same domain as the annotated training data can be obtained.",
        "Another concern is that the performance of a model trained on automatically labeled parallel data as measured on a test set we use may not reflect the quality of these annotations.",
        "To assess the resulting model's coverage, it would be interesting to evaluate it on data outside the original domain, so we consider the out-of-domain (OOD) test sets as provided for the CoNLL Shared Task 2009 where available.",
        "Perhaps the most interesting one of these is the German OOD test set, which is drawn from Europarl (as is the parallel data we use).",
        "It was originally annotated with syntactic dependency trees and se",
        "mantic structure in the SALSA format (Burchardt et al., 2006) for Pado?",
        "and Lapata (2005), and then converted into a PropBank-like form for the CoNLL Shared Task 2009 (Hajic?",
        "et al, 2009).",
        "The OOD test set for English is drawn from the Brown corpus (Francis and Kucera, 1967) and the one for Czech ?",
        "from a Czech translation of Wall Street Journal articles (Hajic?",
        "et al, 2012)."
      ]
    },
    {
      "heading": "4 Evaluation",
      "text": [
        "The first question we are interested in is how the joint inference affects the quality of the automatically obtained annotations on the parallel data.",
        "To answer this, we will run the monolingual models independently and jointly, then train models on the output of these two procedures and compare their performance on a test set.",
        "Note that we do not add the initial training data at this point, so the initial model scores are provided for reference, rather than as a baseline."
      ]
    },
    {
      "heading": "4.1 Projection Setup",
      "text": [
        "A small initial training set of 600 instances was used here for the target language here and the full training set (20000 instances) for the source one.",
        "?st was set to 0.7 in all experiments in this section.",
        "refined model and the difference in their performance.",
        "Asterisk indicates out-of-domain test set, statistically significant improvements are highlighted in bold.",
        "In table 1, we present the accuracy of the model trained on the output of the joint inference (JOINT) against that of the self-training baseline (SELF).",
        "The ?SELF column contains the difference between the two.",
        "Note that the SELF model is trained on the parallel data automatically annotated using monolingual SRL models (not mixed with the initial training set), since we are interested in the effect of joint inference on the quality of the annotation obtained.",
        "Where the improvement is positive and statistically significant with p < 0.005 according to the permutation test (Good, 2000), they are highlighted in bold.",
        "We can see that the refined model (JOINT) outperforms the self-training baseline in most cases by a moderate, but statistically significant margin, which indicates that the joint inference does improve the quality of annotations on the parallel corpus.",
        "The slightly higher improvement on the German OOD test set supports our hypothesis that the procedure enhances the performance of the model on parallel data, as the data for this test set is also drawn from the Europarl corpus.",
        "The improvement over the initial model (?INIT) in this case is statistically significant with p < 0.05.",
        "Higher p-value may be attributed to the smaller test set size.",
        "Figure 4 shows how the performance of the JOINT model changes with the size of the initial training set.",
        "The improvements are smaller for en-cz, en-de and en-zh, but they are also statistically significant for initial training sets of up to 2000 instances.",
        "Projection to English from other languages performs"
      ]
    },
    {
      "heading": "4.2 Combining",
      "text": [
        "In practice, automatically obtained annotations are usually combined with the existing labeled data.",
        "For this purpose, the initial training set is replicated so as to constitute 0.3 (an empirically chosen value that appears to work well in most experiments) of the size of the automatically labeled dataset.",
        "We compare the performance of the model trained on the resulting dataset (COMB) with that of the JOINT model and the initial models.",
        "The results are presented in table 2.",
        "We omit projection from other languages to English, since the JOINT model there fails to outperform the initial model and we do not expect to benefit from adding the automatically annotated data to the initial training set in this case.",
        "notation to the initial training set.",
        "Asterisk indicates out-of-domain test set, statistically significant improvements are highlighted in bold."
      ]
    },
    {
      "heading": "4.3 Symmetric Setup",
      "text": [
        "In the symmetric setup evaluation, we use a slightly larger initial training set of 1400 instances for both source and target language.",
        "The projection model weight is set to 0.5.",
        "Table 3 shows the accuracy of the JOINT model and the SELF baseline.",
        "Note that here, unlike section 4.1, the joint inference is run once and then a model is trained for each language and evaluated on the corresponding test set(s).",
        "The results support our intuition that joint inference helps improve the quality of the resulting annotations, at least in some cases."
      ]
    },
    {
      "heading": "4.4 Oracle RCM",
      "text": [
        "It would be useful to know to what extent the performance of the role correspondence model affects the quality of the output (and thus the performance of the resulting model).",
        "The RCM we use is rather",
        "out-of-domain test set, statistically significant improvements are highlighted in bold.",
        "simplistic, and we believe it can be substantially improved for any given language pair by incorporating prior knowledge and/or using external sources of information.",
        "In order to estimate the potential impact of such improvements, we simulate a better informed projection model, giving it access to the predictions of more accurate monolingual models on the parallel data ?",
        "those trained on the full training set, rather than the initial training set used in this particular experiment.",
        "We refer to the resulting RCM as oracle and assess the difference it makes, compared to a regular one (table 4)."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006), such as measures of syntactic and semantic similarity (Fu?rstenau and Lapata, 2009) or external confidence measures (Gold-wasser et al., 2011).",
        "The alternative we propose is primarily motivated by the research on annotation projection (Pado?",
        "and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; S?gaard, 2011; Lopez et al., 2008; McDonald et al., 2011).",
        "The key difference of the present approach compared to annotation projection is that we assume",
        "tial model, self-training baseline, refined model and its improvement over the other two.",
        "Asterisk indicates out-of-domain test set, statistically significant improvements are highlighted in bold.",
        "the availability of some amount of training data for the target language, possibly using a different inventory of semantic roles.",
        "As mentioned previously, from the training point of view this approach can be seen as similar to co-training (Blum and Mitchell, 1998), other applications of which to NLP are too numerous to list here.",
        "Most closely related is the joint inference in Zhuang and Zong (2010), the main difference being that it relies on a manually annotated parallel corpus, aligned on the argument level, and evaluates only the inference procedure and only on in-domain data.",
        "Other related approaches include Kim et al. (2010), where a cross-lingual transfer of relations is performed (which basically represent parts of the predicate-argument structure considered by SRL methods), and Frermann and Bond (2012), where semantic structure matching is used to rank HPSG parses for parallel sentences.",
        "Unsupervised semantic role labeling methods (Lang and Lapata, 2010; Lang and Lapata, 2011; Titov and Klementiev, 2012a; Lorenzo and Cerisara, 2012) present an alternative to the cross-lingual information propagation approaches such as ours, and at least one the methods in this area also makes use of parallel data (Titov and Klementiev, 2012b).",
        "Conclusions We have presented an approach to information transfer between SRL systems for different language pairs using parallel data.",
        "The task proves challenging due to non-trivial mapping between the role labels used in different SRL annotation schemes and the nature of parallel data ?",
        "the difference in domains and the limited accuracy of the preprocessing tools.",
        "We observe consistent improvements over self-training baseline from using joint inference and the experiments suggest that improving the role correspondence model, for example using language-specific prior knowledge or external data sources, may dramatically increase the performance of the resulting system."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors acknowledge the support of the MMCI Cluster of Excellence and thank Alexandre Klementiev and Manfred Pinkal for valuable suggestions."
      ]
    }
  ]
}
