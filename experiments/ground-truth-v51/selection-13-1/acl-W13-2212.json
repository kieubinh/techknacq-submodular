{
  "info": {
    "authors": [
      "Nadir Durrani",
      "Barry Haddow",
      "Kenneth Heafield",
      "Philipp Koehn"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W13-2212",
    "title": "Edinburghâ€™s Machine Translation Systems for European Language Pairs",
    "url": "https://aclweb.org/anthology/W13-2212",
    "year": 2013
  },
  "references": [
    "acl-D07-1090",
    "acl-D07-1091",
    "acl-D07-1103",
    "acl-D10-1044",
    "acl-D11-1033",
    "acl-E03-1076",
    "acl-E12-1055",
    "acl-N09-1025",
    "acl-N12-1047",
    "acl-N13-1001",
    "acl-N13-1035",
    "acl-P05-1066",
    "acl-P10-2041",
    "acl-P11-1105",
    "acl-P13-2071",
    "acl-P13-2121",
    "acl-W11-2123",
    "acl-W12-3102",
    "acl-W12-3139",
    "acl-W12-3154"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We validated various novel and recently proposed methods for statistical machine translation on 10 language pairs, using large data resources.",
        "We saw gains from optimizing parameters, training with sparse features, the operation sequence model, and domain adaptation techniques.",
        "We also report on utilizing a huge language model trained on 126 billion tokens.",
        "The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs.",
        "Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications, led to a strong showing in the evaluation campaign.",
        "We would like to stress especially two contributions: the use of the new operation sequence model (Section 3) within Moses, and ?",
        "in a separate unconstraint track submission ?",
        "the use of a huge language model trained on 126 billion tokens with a new training tool (Section 4)."
      ]
    },
    {
      "heading": "1 Initial System Development",
      "text": [
        "We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012).",
        "The notable features of these systems are: ?",
        "Moses phrase-based models with mostly default settings ?",
        "training on all available parallel data, including the large UN parallel data, the French-English 109 parallel data and the LDC Gigaword data ?",
        "very large tuning set consisting of the test sets from 2008-2010, with a total of 7,567 sentences per language ?",
        "German?English with syntactic pre-reordering (Collins et al., 2005), compound splitting (Koehn and Knight, 2003) and use of factored representation for a POS target sequence model (Koehn and Hoang, 2007) ?",
        "English?German with morphological target sequence model Note that while our final 2012 systems included subsampling of training data with modified Moore-Lewis filtering (Axelrod et al., 2011), we did not use such filtering at the starting point of our development.",
        "We will report on such filtering in Section 2.",
        "Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 training data at a later stage.",
        "In this section, we report cased BLEU scores (Papineni et al., 2001) on newstest2011."
      ]
    },
    {
      "heading": "1.1 Factored Backoff (German?English)",
      "text": [
        "We have consistently used factored models in past WMT systems for the German?English language pairs to include POS and morphological target sequence models.",
        "But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results.",
        "A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a).",
        "Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German?English."
      ]
    },
    {
      "heading": "1.2 Tuning with k-best MIRA",
      "text": [
        "In preparation for training with sparse features, we moved away from MERT which is known to fall",
        "apart with many more than a couple of dozen features.",
        "Instead, we used k-best MIRA (Cherry and Foster, 2012).",
        "For the different language pairs, we saw improvements in BLEU of -.05 to +.39, with an average of +.09.",
        "There was only a minimal change in the length ratio (Table 1)",
        "(cased BLEU scores with length ratio)"
      ]
    },
    {
      "heading": "1.3 Translation Table Smoothing with Kneser-Ney Discounting",
      "text": [
        "Previously, we smoothed counts for the phrasal conditional probability distributions in the translation model with Good Turing discounting.",
        "We explored the use of Kneser-Ney discounting, but results are mixed (no difference on average, see"
      ]
    },
    {
      "heading": "1.4 Sparse Features",
      "text": [
        "A significant extension of the Moses system over the last couple of years was the support for large numbers of sparse features.",
        "This year, we tested this capability on our big WMT systems.",
        "First, we used features proposed by Chiang et al. (2009): ?",
        "phrase pair count bin features (bins 1, 2, 3, 4?5, 6?9, 10+) ?",
        "target word insertion features ?",
        "source word deletion features ?",
        "word translation features ?",
        "phrase length feature (source, target, both) The lexical features were restricted to the 50 most frequent words.",
        "All these features together only gave minor improvements (Table 3).",
        "We also explored domain features in the sparse feature framework, in three different variations.",
        "Assume that we have three domains, and a phrase pair occurs in domain A 15 times, in domain B 5 times, and in domain C never.",
        "We compute three types of domain features: ?",
        "binary indicator, if phrase-pairs occurs in domain (example: indA = 1, indB = 1, indC = 0) ?",
        "ratio how frequent the phrase pairs occurs in domain (example: ratioA = 1515+5 = .75, ratioB =",
        "15+5 = .25, ratioC = 0) ?",
        "subset of domains in which phrase pair occurs (example: subsetAB = 1, other subsets 0) We tested all three feature types, and found the biggest gain with the domain indicator feature (+.11, Table 4).",
        "Note that we define as domain the different corpora (Europarl, etc.).",
        "The number of domains ranges from 2 to 9 (see column #d).1 #d base.",
        "indicator ratio subset",
        "When combining the domain features and the other sparse features, we see roughly additive gains (Table 5).",
        "We use the domain indicator feature and the other sparse features in subsequent experiments."
      ]
    },
    {
      "heading": "1.5 Tuning Settings",
      "text": [
        "Given the opportunity to explore the parameter tuning of models with sparse features across many language pairs, we investigated a number of settings.",
        "We expect tuning to work better with more iterations, longer n-best lists and bigger cube pruning pop limits.",
        "Our baseline settings are 10 iterations with 100-best lists (accumulating) and a pop limit of 1000 for tuning and 5000 for testing.",
        "base 25 it.",
        "25it+1k-best 25it+pop5k",
        "list, and cube pruning pop limit) Results support running tuning for 25 iterations but we see no gains for 5000 pops.",
        "There is evidence that an n-best list size of 1000 is better in tuning but we did not adopt this since these large lists take up a lot of disk space and slow down the MIRA optimization step (Table 6)."
      ]
    },
    {
      "heading": "1.6 Smaller Phrases",
      "text": [
        "Given the very large corpus sizes (up to a billion words of parallel data for French?English), the size of translation model and lexicalized reordering model becomes a challenge.",
        "Hence, we want to examine if restriction to smaller phrases is feasible without loss in translation quality.",
        "Results in Table 7 suggest that a maximum phrase length of 5 gives almost identical results, and only with a phrase length limit of 4 significant losses occur.",
        "We adopted the limit of 5."
      ]
    },
    {
      "heading": "1.7 Unpruned Language Models",
      "text": [
        "Previously, we trained 5-gram language models using the default settings of the SRILM toolkit in terms of singleton pruning.",
        "Thus, training throws out all singletons n-grams of order 3 and higher.",
        "We explored whether unpruned language models could give better performance, even if we are only able to train 4-gram models due to memory constraints.",
        "At the time, we were not able to build unpruned 4-gram language models for English, but for the other language pairs we did see improvements of -.07 to +.13 (Table 8).",
        "We adopted such models for these language pairs."
      ]
    },
    {
      "heading": "1.8 Translations per Input Phrase",
      "text": [
        "Finally, we explored one more parameter: the limit on how many translation options are considered per input phrase.",
        "The default for this setting is 20.",
        "However, our experiments (Table 9) show that we can get better results with a translation table limit of 100, so we adopted this.",
        "ttl 20 ttl 30 ttl 50 ttl 100"
      ]
    },
    {
      "heading": "1.9 Other Experiments",
      "text": [
        "We explored a number of other settings and features, but did not observe any gains.",
        "?",
        "Using HMM alignment instead of IBM Model 4 leads to losses of ?.01 to ?.27.",
        "?",
        "An earlier check of modified Moore?Lewis filtering (see also below in Section 3) gave very inconsistent results.",
        "?",
        "Filtering the phrase table with significance filtering (Johnson et al., 2007) leads to losses of ?.19 to ?.63.",
        "?",
        "Throwing out phrase pairs with direct translation probability ?(e?|f?)",
        "of less than 10?5 has almost no effect.",
        "?",
        "Double-checking the contribution of the sparse lexical features in the final setup, we observe an average losses of ?.07 when dropping these features.",
        "?",
        "For the German?English language pairs we saw some benefits to using sparse lexical features over POS tags instead of words, so we used this in the final system."
      ]
    },
    {
      "heading": "1.10 Summary",
      "text": [
        "We adopted a number of changes that improved our baseline system by an average of +.30, see Table 10 for a breakdown.",
        "avg.",
        "method",
        "Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and tuning with 1000-best lists (+.02).",
        "The improvements differed significantly by language pair, as detailed in Table 11, with the biggest gains for English?French (+.70), no gain for English?German and no gain for English?",
        "German.",
        "The final experiment of the initial system development phase was to train the systems on the new data, adding newstest2011 to the tuning set (now 10,068 sentences).",
        "Table 12 reports the gains on newstest2012 due to added data, indicating very clearly that valuable new data resources became available this year."
      ]
    },
    {
      "heading": "2 Domain Adaptation Techniques",
      "text": [
        "We explored two additional domain adaptation techniques: phrase table interpolation and modified Moore-Lewis filtering."
      ]
    },
    {
      "heading": "2.1 Phrase Table Interpolation",
      "text": [
        "We experimented with phrase-table interpolation using perplexity minimisation (Foster et al., 2010; Sennrich, 2012).",
        "In particular, we used the implementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper.",
        "For each language pair, we took the alignments created from all the data concatenated, built separate phrase tables from each of the individual corpora, and interpolated using each method.",
        "The results are shown in Table 13",
        "methods) with baseline (on newstest2012).",
        "The baselines are as Table 12 except for the starred rows where tuning with PRO was found to be better.",
        "The modified interpolation was not possible in fr?en as it uses to much RAM.",
        "The results from the phrase-table interpolation are quite mixed, and we only used the technique",
        "for the final system in en-es.",
        "An interpolation based on PRO has recently been shown (Haddow, 2013) to improve on perplexity minimisation is some cases, but the current implementation of this method is limited to 2 phrase-tables, so we did not use it in this evaluation."
      ]
    },
    {
      "heading": "2.2 Modified Moore-Lewis Filtering",
      "text": [
        "In last year's evaluation (Koehn and Haddow, 2012b) we had some success with modified Moore-Lewis filtering (Moore and Lewis, 2010; Axelrod et al., 2011) of the training data.",
        "This year we conducted experiments in most of the language pairs using MML filtering, and also experimented using instance weighting (Mansour and Ney, 2012) using the (exponential of) the MML weights.",
        "The results are show in Table 14 base MML Inst.",
        "Wt Inst.",
        "Wt",
        "baseline.",
        "The MML uses monolingual news as in-domain, and selects from all training data after alignment.The weighting uses the MML weights, optionally downscaled by 10, then exponentiated.",
        "Baselines are as Table 13.",
        "As with phrase-table interpolation, MML filtering and weighting shows a very mixed picture, and not the consistent improvements these techniques offer on IWSLT data.",
        "In the final systems, we used MML filtering only for es-en."
      ]
    },
    {
      "heading": "3 Operation Sequence Model (OSM)",
      "text": [
        "We enhanced the phrase segmentation and reordering mechanism by integrating OSM: an operation sequence N-gram-based translation and reordering model (Durrani et al., 2011) into the Moses phrase-based decoder.",
        "The model is based on minimal translation units (MTUs) and Markov chains over sequences of operations.",
        "An operation can be (a) to jointly generate a bi-language MTU, composed from source and target words, or",
        "By coupling reordering with lexical generation, each (translation or reordering) decision conditions on n ?",
        "1 previous (translation and reordering) decisions spanning across phrasal boundaries thus overcoming the problematic phrasal independence assumption in the phrase-based model.",
        "In the OSM model, the reordering decisions influence lexical selection and vice versa.",
        "Lexical generation is strongly coupled with reordering thus improving the overall reordering mechanism.",
        "We used the modified version of the OSM model (Durrani et al., 2013b) that additionally handles discontinuous and unaligned target MTUs3.",
        "We borrow 4 count-based supportive features, the Gap, Open Gap, Gap-width and Deletion penalties from Durrani et al. (2011).",
        "Training: During training, each bilingual sentence pair is deterministically converted to a unique sequence of operations.",
        "Please refer to Durrani et al. (2011) for a list of operations and the conversion algorithm and see Figure 1 and Table 15 for a sample bilingual sentence pair and its stepwise conversion into a sequence of operation.",
        "A 9-gram Kneser-Ney smoothed operation sequence model is trained with SRILM.",
        "Search: Although the OSM model is based on minimal units, phrase-based search on top of OSM model was found to be superior to the MTU-based decoding in Durrani et al. (2013a).",
        "Following this framework allows us to use OSM model in tandem with phrase-based models.",
        "We integrated the generative story of the OSM model into the hypothesis extension of the phrase-based Moses decoder.",
        "Please refer to (Durrani et al., 2013b) for details.",
        "Results: Table 16 shows case-sensitive BLEU scores on newstest2012 and newstest2013 for fi-3In the original OSM model these are removed from the alignments through a post-processing heuristic which hurts in some language pairs.",
        "See Durrani et al. (2013b) for detailed experiments.",
        "nal systems from Section 1 and these systems augmented with the operation sequence model.",
        "The model gives gains for all language pairs (BLEU +.09 to +.90, average +.37, on newstest2013)."
      ]
    },
    {
      "heading": "4 Huge Language Models",
      "text": [
        "To overcome the memory limitations of SRILM, we implemented modified Kneser-Ney (Kneser and Ney, 1995; Chen and Goodman, 1998) smoothing from scratch using disk-based streaming algorithms.",
        "This open-source4 tool is described fully by Heafield et al. (2013).",
        "We used it to estimate an unpruned 5?gram language model on web pages from ClueWeb09.5 The corpus was preprocessed by removing spam (Cormack et al., 2011), selecting English documents, splitting sentences, deduplicating, tokenizing, and truecasing.",
        "Estimation on the remaining 126 billion tokens took 2.8 days on a single machine with 140 GB RAM (of which 123 GB was used at peak) and six hard drives in a RAID5 configuration.",
        "Statistics about the resulting model are shown in Table 17.",
        "The large language model was then quantized to 10 bits and compressed to 643 GB with KenLM (Heafield, 2011), loaded onto a machine with 1 TB RAM, and used as an additional feature in unconstrained French?English, Spanish?English, and Czech?English submissions.",
        "This additional language model is the only difference between our final constrained and unconstrained submissions; no additional parallel data was used.",
        "Results are shown in Table 18.",
        "Improvement from large language models is not a new result (Brants et al., 2007); the primary contribution is estimating on a single machine.",
        "German?English was not ready in time."
      ]
    },
    {
      "heading": "5 Summary",
      "text": [
        "Table 19 breaks down the gains over the final system from Section 1 from using the operation sequence models (OSM), modified Moore-Lewis filtering (MML), fixing a bug with the sparse lexical features (Sparse-Lex Bugfix), and instance weighting (Instance Wt.",
        "), translation model combination (TM-Combine), and use of the huge language model (ClueWeb09 LM)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Thanks to Miles Osborne for preprocessing the ClueWeb09 corpus.",
        "The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement 287658 (EU BRIDGE) and grant agreement 288487(MosesCore).This work made use of the resources provided by the Edinburgh Compute and Data Facility6.",
        "The ECDF is partially supported by the eDIKT initia",
        "3.",
        "1+TM-Combine 34.31 +44 29.76 +.10 4.",
        "1+Instance Wt.",
        "34.27 +.40 29.63 ?.03 5.",
        "1+Sparse-Lex Bugfix 34.20 +.33 29.86 +.20 6.",
        "1+2+3: OSM+TM-Cmb.",
        "34.63 +.76 30.21 +.55 7.",
        "1+2+4: OSM+Inst.",
        "Wt.",
        "34.58 +.71 30.11 +.45"
      ]
    }
  ]
}
