{
  "info": {
    "authors": [
      "Xiaofeng Wu",
      "Hui Yu",
      "Qun Liu"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W13-2256",
    "title": "DCU Participation in WMT2013 Metrics Task",
    "url": "https://aclweb.org/anthology/W13-2256",
    "year": 2013
  },
  "references": [
    "acl-C10-2175",
    "acl-P02-1040",
    "acl-P08-1007",
    "acl-W05-0904",
    "acl-W07-0411",
    "acl-W07-0714",
    "acl-W07-0734",
    "acl-W11-2105",
    "acl-W12-3104"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we propose a novel syntactic based MT evaluation metric which only employs the dependency information in the source side.",
        "Experimental results show that our method achieves higher correlation with human judgments than BLEU, TER, HWCM and METEOR at both sentence and system level for all of the four language pairs in WMT 2010."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Automatic evaluation plays a more important role in the evolution of machine translation.",
        "At the earliest stage, the automatic evaluation metrics only use the lexical information, in which, BLEU (Pap-ineni et al., 2002) is the most popular one.",
        "BLEU is simple and effective.",
        "Most of the researchers regard BLEU as their primary evaluation metric to develop and compare MT systems.",
        "However, BLEU only employs the lexical information and cannot adequately reflect the structural level similarity.",
        "Translation Error Rate (TER) (Snover et al., 2006) measures the number of edits required to change the hypothesis into one of the references.",
        "METEOR (Lavie and Agarwal, 2007), which defines loose unigram matching between the hypothesis and the references with the help of stemming and Wordnet-looking-up, is also a lexical based method and achieves the first-class human-evaluation-correlation score.",
        "AMBER (Chen and Kuhn, 2011; Chen et al., 2012) incorporates recall, extra penalties and some text processing variants on the basis of BLEU.",
        "The main weakness of all the above lexical based methods is that they cannot adequately reflect the structural level similarity.",
        "To overcome the weakness of the lexical based methods, many syntactic based metrics were proposed.",
        "Liu and Gildea (2005) proposed STM, a constituent tree based approach, and HWCM, a dependency tree based approach.",
        "Both of the two methods compute the similarity between the sub-trees of the hypothesis and the reference.",
        "Owczarzak et al(2007a; 2007b; 2007c) presented a method using the Lexical-Functional Grammar (LFG) dependency tree.",
        "MAXSIM (Chan and Ng, 2008) and the method proposed by Zhu et al(2010) also employed the syntactic information in association with lexical information.With the syntactic information which can reflect structural information, the correlation with the human judgments can be improved to a certain extent.",
        "As we know that the hypothesis is potentially noisy, and these errors expand through the parsing process.",
        "Thus the power of syntactic information could be considerably weakened.",
        "In this paper, we attempt to overcome the shortcoming of the syntactic based methods and propose a novel dependency based MT evaluation metric.",
        "The proposed metric only employs the reference dependency tree which contains both the lexical and syntactic information, leaving the hypothesis side unparsed to avoid the error propagation.",
        "In our metric, F-score is calculated using the string of hypothesis and the dependency based n-grams which are extracted from the reference dependency tree.",
        "Experimental results show that our method achieves higher correlation with human judgments than BLEU, HWCM, TER and METEOR at both sentence level and system level for all of the four language pairs in WMT 2010."
      ]
    },
    {
      "heading": "2 Background: HWCM",
      "text": [
        "HWCM is a dependency based metric which extracts the headword chains, a sequence of words which corresponds to a path in the dependency tree, from both the hypothesis and the reference dependency tree.",
        "The score of HWCM is obtained",
        "In formula (1), D is the maximum length of the headword chain.",
        "chainn(hyp) denotes the set of the headword chains with length of n in the tree of hypothesis.",
        "count(g) denotes the number of times g appears in the headword chain of the hypothesis dependency tree and countclip(g) denotes the clipped number of times when g appears in the the headword chain of the reference dependency trees.",
        "Clipped means that the count computed from the headword chain of the hypothesis tree should not exceed the maximum number of times when g occurs in headword chain of any single reference tree.",
        "The following are two sentences representing as reference and hypothesis, and Figure 1 and Figure 2 are the dependency trees respectively.",
        "reference: It is not for want of trying .",
        "hypothesis: This is not for lack of trying .",
        "In the example above, there are 8 1-word, 7 2 word and 3 3-word headword chains in the hypothesis dependency tree.",
        "The number of 1-word and 2-word headword chains in the hypothesis tree which can match their counterparts in the reference tree is 5 and 4 respectively.",
        "The 3-word headword chains in the hypothesis dependency tree are is for lack, for lack of and lack of trying.",
        "Due to the difference in the dependency structures, they have no matches in the reference side."
      ]
    },
    {
      "heading": "3 A Novel Dependency Based MT",
      "text": []
    },
    {
      "heading": "Evaluation Method",
      "text": [
        "In this new method, we calculate F-score using the string of hypothesis and the dep-n-grams which are extracted from the reference dependency tree.",
        "The new method is named DEPREF since it is a DEPendency based method only using dependency tree of REference to calculate the F-score.",
        "In DEPREF, after the parsing of the reference sentences, there are three steps below being carried out.",
        "1) Extracting the dependency based n-gram (dep-n-gram) in the dependency tree of the reference.",
        "2) Matching the dep-n-gram with the string of hypothesis.",
        "3) Obtaining the final score of a hypothesis.",
        "The detail description of our method will be found in paper (Liu et al., 2013) .",
        "We only give the experiment results in this paper."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "Both the sentence level evaluation and the system level evaluation are conducted to assess the performance of our automatic metric.",
        "At the sentence level evaluation, Kendall's rank correlation coefficient ?",
        "is used.",
        "At the system level evaluation, the Spearman's rank correlation coefficient ?",
        "is used."
      ]
    },
    {
      "heading": "4.1 Data",
      "text": [
        "There are four language pairs in our experiments including German-to-English, Czech-to-English, French-to-English and Spanish-to-English, which are all derived from WMT2010.",
        "Each of the four language pairs consists of 2034 sentences and the references of the four language pairs are the same.",
        "There are 24 translation systems for French-to-English, 25 for German-to-English, 12 for Czech-to-English and 15 for Spanish-to-English.",
        "We parsed the reference into constituent tree by Berkeley parser and then converted the constituent tree into dependency tree by Penn2Malt 1.",
        "Presumably, we believe that the performance will be even better if the dependency trees are manually revised.",
        "In the experiments, we compare the performance of our metric with the widely used lexical based metrics BLEU, TER, METEOR and a dependency based metric HWCM.",
        "In order to make a fair comparison with METEOR which is known to perform best when external resources like stem and synonym are provided, we also provide results of DEPREF with external resources.",
        "English, Spanish-to-English and French-to-English.",
        "The number in bold is the maximum value in each column.",
        "N stands for the max length of the headword chains in HWCM in Table 1.A."
      ]
    },
    {
      "heading": "4.2 Sentence-level Evaluation",
      "text": [
        "Kendall's rank correlation coefficient ?",
        "is employed to evaluate the correlation of all the MT evaluation metrics and human judgements at the sentence level.",
        "A higher value of ?",
        "means a better ranking similarity with the human judges.",
        "The correlation scores of the four language pairs and the average scores are shown in Table 1.A (without external resources) and Table 1.B (with stemming and synonym), Our method performs best when maximum length of dep-n-gram is set to 3, so we present only the results when the maximum length of dep-n-gram equals 3.",
        "From Table 1.A, we can see that all our methods are far more better than BLEU, TER and HWCM when there is no external resources applied on all of the four language pairs.",
        "In Table 1.B, external resources is considered.",
        "DEPREF is also better than METEOR on the four language pairs.",
        "From the comparison between Table 1.A and Table 1.B, we can conclude that external resources is helpful for DEPREF on most of the language pairs.",
        "When comparing DEPREF without external resources with METEOR, we find that DEPREF obtains better results on Czech-English and German-English."
      ]
    },
    {
      "heading": "4.3 System-level Evaluation",
      "text": [
        "We also evaluated the metrics with the human rankings at the system level to further investigate the effectiveness of our metrics.",
        "The matching of the words in DEPREF is correlated with the position of the words, so the traditional way of computing system level score, like what BLEU does, is not feasible for DEPREF.",
        "Therefore, we resort to the way of adding the sentence level scores together to obtain the system level score.",
        "At system level evaluation, we employ Spearman's rank correlation coefficient ?.",
        "The correlations of the four language pairs and the average scores are shown in Table 2.A (without external resources) and Table 2.B (with stem and synonym).",
        "From Table 2.A, we can see that the correlation of DEPREF is better than BLEU, TER and HWCM on German-English, Spanish-English and French-English.",
        "On Czech-English, our metric DEPREF is better than BLEU and TER.",
        "In Table 2.B (with stem and synonym), DEPREF obtains better results than METEOR on all of the language pairs except one case that DEPREF gets the same result as METEOR on Czech-English.",
        "When comparing DEPREF without external resources with METEOR, we can find that DEPREF gets better result than METEOR on Spanish-English and French-English.",
        "From Table 1 and Table 2, we can conclude that, DEPREF without external resources can obtain comparable result with METEOR, and DEPREF with external resources can obtain better results than METEOR.",
        "The only exception is that at the system level evaluation, Czech-English's best score is abtained by HWCM.",
        "Notice that there are only 12 systems in Czech-English, which means there are only 12 numbers to be sorted, we believe",
        "English, Spanish-to-English and French-to-English.",
        "The number in bold is the maximum value in each column.",
        "N stands for the max length of the headword chains in HWCM in Table 2.A.",
        "the spareness issure is more serious in this case."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we propose a new automatic MT evaluation method DEPREF.",
        "The experiments are carried out at both sentence-level and system-level using four language pairs from WMT 2010.",
        "The experiment results indicate that DEPREF achieves better correlation than BLEU, HWCM, TER and METEOR at both sentence level and system level."
      ]
    }
  ]
}
