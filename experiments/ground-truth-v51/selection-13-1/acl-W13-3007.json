{
  "info": {
    "authors": [
      "Jeffrey Heinz",
      "Jim Rogers"
    ],
    "book": "MoL",
    "id": "acl-W13-3007",
    "title": "Learning Subregular Classes of Languages with Factored Deterministic Automata",
    "url": "https://aclweb.org/anthology/W13-3007",
    "year": 2013
  },
  "references": [
    "acl-P11-2011"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper shows how factored finite-state representations of subregular language classes are identifiable in the limit from positive data by learners which are polytime iterative and optimal.",
        "These representations are motivated in two ways.",
        "First, the size of this representation for a given regular language can be exponentially smaller than the size of the minimal deterministic acceptor recognizing the language.",
        "Second, these representations (including the exponentially smaller ones) describe actual formal languages which successfully model natural language phenomenon, notably in the sub-field of phonology."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In this paper we show how to define certain sub-regular classes of languages which are identifiable in the limit from positive data (ILPD) by efficient, well-behaved learners with a lattice-structured hypothesis space (Heinz et al., 2012).",
        "It is shown that every finite set of DFAs defines such an ILPD class.",
        "In this case, each DFA can be viewed as one factor in the description of every language in the class.",
        "This factoring of language classes into multiple DFA can provide a compact, canonical representation of the grammars for every language in the class.",
        "Additionally, many subregular classes of languages can be learned by the above methods including the Locally k-Testable, Strictly k-Local, Piecewise k-Testable, and Strictly k-Piecewise languages (McNaughton and Papert, 1971; Rogers and Pullum, 2011; Rogers et al., 2010).",
        "From a linguistic (and cognitive) perspective, these subregular classes are interesting because they appear to be sufficient for modeling phonotactic patterns in human language (Heinz, 2010; Heinz et al., 2011; Rogers et al., to appear)."
      ]
    },
    {
      "heading": "2 Preliminaries",
      "text": [
        "For any function f and element a in the domain of f , we write f(a)?",
        "if f(a) is defined, f(a)?= x if it is defined for a and its value is x, and f(a) ?",
        "otherwise.",
        "The range of f , the set of values f takes at elements for which it is defined, is denoted range(f).",
        "??",
        "and ?k denote all sequences of any finite length, and of length k, over a finite alphabet ?.",
        "The empty string is denoted ?.",
        "A language L is a subset of ??.",
        "For all x, y belonging to a partially-ordered set (S,?",
        "), if x ?",
        "z and y ?",
        "z then z is an upper bound of x and y.",
        "For all x, y ?",
        "S, the least upper bound (lub) x?",
        "y = z iff x ?",
        "z, y ?",
        "z, and for all z?",
        "which upper bound x and y, it is the case that z ?",
        "z?.",
        "An upper semi-lattice is a partially ordered set (S,?)",
        "such that every subset of S has a lub.",
        "If S is finite, this is equivalent to the existence of x ?",
        "y for all x, y ?",
        "S. A deterministic finite-state automaton (DFA) is a tuple (Q,?, Q0, F, ?).",
        "The states of the DFA are Q; the input alphabet is ?",
        "; the set of initial states is Q0; the final states are F ; and ?",
        ": Q?",
        "?",
        "?",
        "Q is the transition function.",
        "We admit a set of initial states solely to accommodate the empty DFA, which has none.",
        "Deterministic automata never have more than one initial state.",
        "We will assume that, if the automaton is non-empty, then Q0 = {q0}; The transition function's domain is extended to Q?",
        "??",
        "in the usual way.",
        "The language of a DFA A is L(A) def= {w ?",
        "??",
        "|?",
        "(q0, w)??",
        "F}.",
        "A DFA is trim iff it has no useless states: (?q ?",
        "Q)[ ?w, v ?",
        "??",
        "| ?",
        "(q0, w)?= q and ?",
        "(q, v)??",
        "F ].",
        "Every DFA can be trimmed by eliminating useless states from Q and restricting the remaining components accordingly.",
        "The empty DFA isA?",
        "= (?,?,?,?,?).",
        "This is the minimal trim DFA such that L(A?)",
        "= ?.",
        "The DFA product of A1 = (Q1,?, Q01, F1, ?1)",
        "The DFA product of two DFA is also a DFA.",
        "It is not necessarily trim, but we will generally assume that in taking the product the result has been trimmed, as well.",
        "The product operation is associative and commutative (up to isomorphism), and so it can be applied to a finite set S of DFA, in which case we",
        "tion for the product of a finite sequence of DFAs:",
        "quences are used instead of sets in order to match factors in two grammars.",
        "Let DFA denote the collection of finite sequences of DFAs.",
        "Theorem 1 is well-known.",
        "Theorem 1 Consider a finite set S of DFA.",
        "Then",
        "= ?A?S L(A).",
        "An important consequence of Theorem 1 is that some languages are exponentially more compactly represented by their factors.",
        "The grammar",
        "states.",
        "An example of such a language is given in Section 4, Figures 1 and 2."
      ]
    },
    {
      "heading": "2.1 Identification in the limit",
      "text": [
        "A positive text T for a language L is a total function T : N ?",
        "L ?",
        "{#} (# is a ?pause?)",
        "such that range(T ) = L (i.e., for every w ?",
        "L there is at least one n ?",
        "N for which w =",
        "of all finite initial portions of all positive texts for all possible languages.",
        "The content of an element T [i] of SEQ is content(T [i]) def= {w ?",
        "??",
        "|(?j ?",
        "i?",
        "1)[T (j) = w]}.",
        "In this paper, learning algorithms are programs: ?",
        ": SEQ ?",
        "DFA.",
        "A learner ?",
        "identifies in the limit from positive texts a collection of languages L if and only if for all L ?",
        "L, for all positive texts T for L, there exists an n ?",
        "N such that (?m ?",
        "n)[?",
        "(T [m]) = ?",
        "(T [n])] and L(T [n]) = L (see Gold (1967) and Jain et al. (1999)).",
        "A class of languages is ILPD iff it is identifiable in the limit by such a learner."
      ]
    },
    {
      "heading": "3 Classes of factorable-DFA languages",
      "text": [
        "In this section, classes of factorable-DFA languages are introduced.",
        "The notion of sub-DFA is central to this concept.",
        "Pictorially, a sub-DFA is obtained from a DFA by removing zero or more states, transitions, and/or revoking the final status of zero or more final states.",
        "Every grammar ~A determines a class of languages: those recognized by a sub-grammar of ~A.",
        "Our interest is not in L( ~A), itself.",
        "Indeed, this will generally be ??.",
        "Rather, our interest is in identifying languages relative to the class of languages recognizable by sub-grammars of ~A.",
        "Definition 2 Let G( ~A) def= {~B |~B ?",
        "~A}, the class of grammars that are sub-grammars of ~A.",
        "Let L( ~A) def= {L( ~B) |~B ?",
        "~A}, the class of languages recognized by sub-grammars of ~A.",
        "A class of languages is a factorable-DFA class iff it is L( ~A) for some ~A.",
        "The set G( ~A) is necessarily finite, since ~A is, so every class L( ~A) is trivially ILPD by a learning algorithm that systematically rules out grammars that are incompatible with the text, but this na?",
        "?ve algorithm is prohibitively inefficient.",
        "Our goal is",
        "to establish that the efficient general learning algorithm given by Heinz et al. (2012) can be applied to every class of factorable-DFA languages, and that this class includes many of the well-known sub-regular language classes as well as classes that are, in a particular sense, mixtures of these."
      ]
    },
    {
      "heading": "4 A motivating example",
      "text": [
        "This section describes the Strictly 2-Piecewise languages, which motivate the factorization that is at the heart of this analysis.",
        "Strictly Piecewise (SP) languages are characterized in Rogers et al. (2010) and are a special subclass of the Piecewise Testable languages (Simon, 1975).",
        "Every SP language is the intersection of a finite set of complements of principal shuffle ideals:",
        "So v ?",
        "SI(w) iff w occurs as a subsequence of v and L ?",
        "SP iff there is a finite set of strings for which L includes all and only those strings that do not include those strings as subsequences.",
        "We say that L is generated by S. It turns out that SP is exactly the class of languages that are closed under subsequence.",
        "A language is SPk iff it is generated by a set of strings each of which is of length less than or equal to k. Clearly, every SP language is SPk for some k and SP = ?1?k?N[SPk].",
        "If w ?",
        "??",
        "and |w |= k, then SI(w) = L(Aw) for a DFA Aw with no more than k states.",
        "For example, if k = 2 and ?",
        "= {a, b, c} and, hence, w ?",
        "{a, b, c}2, then the minimal trim DFA recognizing SI(w) will be a sub-DFA (in which one of the transitions from the ?1 state has been removed) of one of the three DFA of Figure 1.",
        "Figure 1 shows ~A = ?Aa, Ab, Ac?, where ?",
        "= {a, b, c} and each A?",
        "is a DFA accepting ??",
        "whose states distinguish whether ?",
        "has yet occurred.",
        "Figure 2 shows ?",
        "~A.",
        "Note that every SP2 language over {a, b, c} is L( ~B) for some ~B ?",
        "~A.",
        "The class of grammars of G( ~A) recognize a slight extension of SP2 over {a, b, c} (which includes 1-Reverse Definite languages as well).",
        "Observe that 6 states are required to describe ~A but 8 states are required to describe",
        "?",
        "~A.",
        "Let ~A?",
        "be the sequence of DFA with one DFA for each letter in ?, as in Figure 1.",
        "As card(?)",
        "increases the number of states of ~A?",
        "is 2 ?",
        "card(?)",
        "but the number of states in ?",
        "~A?",
        "is 2card(?).",
        "The",
        "number of states in the product, in this case, is exponential in the number of its factors.",
        "The Strictly 2-Piecewise languages are currently the strongest computational characterization1 of long-distance phonotactic patterns in human languages (Heinz, 2010).",
        "The size of the phonemic inventories2 in the world's languages ranges from 11 to 140 (Maddieson, 1984).",
        "English has about 40, depending on the dialect.",
        "With an alphabet of that size ~A?",
        "would have 80 states, while ?",
        "~A?",
        "would have 240 ?",
        "1 ?",
        "1012 states.",
        "The fact that there are about 1011 neurons in human brains (Williams and Herrup, 1988) helps motivate interest in the more compact, parallel representation given by ~A?",
        "as opposed to the singular representation of the DFA ?",
        "~A?."
      ]
    },
    {
      "heading": "5 Learning factorable classes of languages",
      "text": [
        "In this section, classes of factorable-DFA languages are shown to be analyzable as finite lattice spaces.",
        "By Theorem 6 of Heinz et al. (2012), every such class of languages can be identified in the limit from positive texts.",
        "of and ~B and ~C is ~B ?",
        "~C def= ?B1 ?",
        "C1 ?",
        "?",
        "?",
        "Bn ?",
        "Cn?.",
        "Note that the join of two sub-DFA of A is also a sub-DFA of A.",
        "Since G( ~A) is finite, binary join suffices to define join of any set of sub-DFA of a given DFA (as iterated binary joins).",
        "Let ?",
        "[S] be the join of S, a set of sub-DFAs of some A (or ~A).",
        "phonemes, and the phonemic inventory is the set of these representations (Hayes, 2009).",
        "Similarly the set of sub-grammars of a grammar ~A, ordered again by?, ({~B ?",
        "~A},?",
        "), is an upper semi-lattice with the least upper bound of a set of sub-grammars of ~A being their join.3 This follows from the fact that Q1 ?Q2 (similarly F1 ?F2 and ?1 ?",
        "?2) is the lub of Q1 and Q2 (etc.)",
        "in the lattice of sets ordered by subset."
      ]
    },
    {
      "heading": "5.1 Paths and Chisels",
      "text": [
        "Definition 4 LetA = (Q,?, {q0}, F, ?)",
        "be a non-empty DFA and w = ?0?1 ?",
        "?",
        "?",
        "?n ?",
        "??.",
        "If ?",
        "(q0, w)?, the path of w in A is the sequence",
        "where (?0 ?",
        "i ?",
        "n)[qi+1 = ?",
        "(qi, ?i)].",
        "If ?",
        "(q0, w)?",
        "then ?",
        "(A, w)?.",
        "If ?",
        "(A, w)?, let Q?",
        "(A,w) denote set of states it traverses, ??",
        "(A,w) denote the the transitions it traverses, and let F?",
        "(A,w) = {qn+1}.",
        "Next, for any DFA A, and any w ?",
        "L(A), we define the chisel of w given A to be the sub-DFA of A that exactly encompasses the path etched out in A by w. Definition 5 For any non-empty DFA A = (Q,?, {q0}, F, ?)",
        "and all w ?",
        "?",
        "?, if w ?",
        "L(A), then the chisel of w given A is the sub-DFA",
        "Observe that CA(w) ?",
        "A for all words w and all A, and that CA(w) is trim.",
        "Using the join, the domain of the chisel is extended to sets of words: C ~A(S) =",
        "per semi-lattice with the lub of two elements given by the join ?.",
        "Henceforth consider only nonempty A.",
        "For the first statement, let S be the set of u?v where, for each q ?",
        "Q and for each ?",
        "?",
        "?, ?",
        "(q0, u) ?= q and ?(?",
        "(q, ?",
        "), v) ??",
        "F such that u?v has minimal length.",
        "By construction, S is finite.",
        "Furthermore, for every state and every transition in A, there is a word in S whose path touches that state and transition.",
        "By definition of ?",
        "it follows that CA(S) = A.",
        "For proof of the second statement, for each Ai in ~A, construct Si as stated and take their union.",
        "Heinz et al. (2012) define lattice spaces.",
        "For an upper semi-lattice V and a function f : ??",
        "?",
        "V such that f and ?",
        "are (total) computable, (V, f) is called a Lattice Space (LS) iff, for each v ?",
        "V , there exists a finite D ?",
        "range(f) with?D = v. Theorem 3 For all grammars ~A = ?A1 ?",
        "?",
        "?",
        "An?,",
        "For Heinz et al. (2012), elements of the lattice are grammars.",
        "Likewise, here, each grammar ~A = ?A1 ?",
        "?",
        "?",
        "An?",
        "defines a lattice whose elements are its sub-grammars.",
        "Heinz et al. (2012) associate the languages of a grammar v in a lattice space (V, f) with {w ?",
        "??",
        "|f(w) ?",
        "v}.",
        "This definition coincides with ours: for any element ~A?",
        "of C( ~A) (note ~A?",
        "?",
        "~A), a word w belongs to L( ~A?)",
        "if and only if C ~A(w) is a sub-DFA of ~A?.",
        "The class of languages of a LS is the collection of languages obtained by every element in the lattice.",
        "For every LS (C( ~A), C ~A), we now define a learner ?",
        "according to the construction in Heinz et al. (2012): ?T ?",
        "SEQ, ?",
        "(T ) = ?w?content(T ) C ~A(w).",
        "Let L(C( ~A),C ~A) denote the class of languages associated with the LS in Theorem 3.",
        "According to Heinz et al. (2012, Theorem 6), the learner ?",
        "identifies L(C( ~A),CvA) in the limit from positive data.",
        "Furthermore, ?",
        "is polytime iterative,",
        "i.e can compute the next hypothesis in polytime from the previous hypothesis alone, and optimal in the sense that no other learner converges more quickly on languages in L(C( ~A),CG).",
        "In addition, this learner is globally-consistent (every hypothesis covers the data seen so far), locally-conservative (the hypothesis never changes unless the current datum is not consistent with the current hypothesis), strongly-monotone (the current hypothesis is a superset of all prior hypotheses), and prudent (it never hypothesizes a language that is not in the target class).",
        "Formal definitions of these terms are given in Heinz et al. (2012) and can also be found elsewhere, e.g. Jain et al. (1999)."
      ]
    },
    {
      "heading": "6 Complexity considerations",
      "text": [
        "The space of sub-grammars of a given sequence of DFAs is necessarily finite and, thus, identifiable in the limit from positive data by a na?",
        "?ve learner that simply enumerates the space of grammars.",
        "The lattice learning algorithm has better efficiency because it works bottom-up, extending the grammar minimally, at each step, with the chisel of the current string of the text.",
        "The lattice learner never explores any part of the space of grammars that is not a sub-grammar of the correct one and, as it never moves down in the lattice, it will skip much of the space of grammars that are sub-grammars of the correct one.",
        "The space it explores will be minimal, given the text it is running on.",
        "Generalization is a result of the fact that in extending the grammar for a string the learner adds its entire Nerode equivalence class to the language.",
        "The time complexity of either learning or recognition with the factored automata may actually be somewhat worse than the complexity of doing so with its product.",
        "Computing the chisel of a string w in the product machine of Figure 2 is ?",
        "(|w|), while in the factored machine of Figure 1 one must compute the chisel in each factor and its complexity is, thus, ?",
        "(|w |card(?)k?1).",
        "But ?",
        "and k are fixed for a given factorization, so this works out to be a constant factor.",
        "Where the factorization makes a substantial difference is in the number of features that must be learned.",
        "In the factored grammar of the example, the total number of states plus edges is ?(kcard(?",
        ")k?1), while in its product it is ?(2(card(?)k?1)).",
        "This represents an exponential improvement in the space complexity of the factored grammar.",
        "Every DFA can be factored in many ways, but the factorizations do not necessarily provide an asymptotically significant improvement in space complexity.",
        "The canonical contrast is between sequences of automata ?A1, .",
        ".",
        ".",
        ",An?",
        "that count modulo some sequence of mi ?",
        "N. If the mi are pairwise prime, the product will require",
        "other hand, they are all multiples of each other it will require just ?",
        "(maxi[mi])."
      ]
    },
    {
      "heading": "7 Examples",
      "text": [
        "The fact that the class of SP2 languages is efficiently identifiable in the limit from positive data is neither surprising or new.",
        "The obvious approach to learning these languages simply accumulates the set of pairs of symbols that occur as subsequences of the strings in the text and builds a machine that accepts all and only those strings in which no other such pairs occur.",
        "This, in fact, is essentially what the lattice learner is doing.",
        "What is significant is that the lattice learner provides a general approach to learning any language class that can be captured by a factored grammar and, more importantly, any class of languages that are intersections of languages that are in classes that can be captured this way.",
        "Factored grammars in which each factor recognizes ?",
        "?, as in the case of Figure 1, are of particular interest.",
        "Every sub-Star-Free class of languages in which the parameters of the class (k, for example) are fixed can be factored in this way.4 If the parameters are not fixed and the class of languages is not finite, none of these classes can be identified in the limit from positive data at all.5 So this approach is potentially useful at least for all sub-Star-Free classes.",
        "The learners for non-strict classes are practical, however, only for small values of the parameters.",
        "So that leaves the Strictly Local SLk and Strictly Piecewise SPk languages as the obvious targets.",
        "The SLk languages are those that are determined by the substrings of length no greater than k that occur within the string (including endmark-4We conjecture that there is a parameterized class of languages that is equivalent to the Star-Free languages, which would make that class learnable in this way as well.",
        "5For most of these classes, including the Definite, Reverse-Definite and Strictly Local classes and their super classes, this is immediate from the fact that they are super-finite.",
        "SP, on the other hand, is not super-finite (since it does not include all finite languages) but nevertheless, it is not IPLD.",
        "ers).",
        "These can be factored on the basis of those substrings, just as the SPk languages can, although the construction is somewhat more complex.",
        "(See the Knuth-Morris-Pratt algorithm (Knuth et al., 1977) for a way of doing this.)",
        "But SLk is a case in which there is no complexity advantage in factoring the DFA.",
        "This is because every SLk language is recognized by a DFA that is a Myhill graph: with a state for each string of ?<k (i.e., of length less than k).",
        "Such a graph has ?(card(?",
        ")k?1) states, asymptotically the same as the number of states in the factored grammar, which is actually marginally worse.",
        "Therefore, factored SLk grammars are not, in themselves, interesting.",
        "But they are interesting as factors of other grammars.",
        "Let (SL+ SP)k,l (resp.",
        "(LT + SP)k,l, (SL + PT)k,l) be the class of languages that are intersections of SLk and SPl (resp.",
        "LTk and SPl, SLk and PTl) languages.",
        "Where LT (PT) languages are determined by the set of substrings (subsequences) that occur in the string (see Rogers and Pullum (2011) and Rogers et al. (2010)).",
        "These classes capture co-occurrence of local constraints (based on adjacency) and long-distance constraints (based on precedence).",
        "These are of particular interest in phonotactics, as they are linguistically well-motivated approaches to modeling phonotactics and they are sufficiently powerful to model most phonotactic patterns.",
        "The results of Heinz (2007) and Heinz (2010) strongly suggest that nearly all segmental patterns are (SL+ SP)k,l for small k and l. Moreover, roughly 72% of the stress patterns that are included in Heinz's database (Heinz, 2009; Phonology Lab, 2012) of patterns that have been attested in natural language can be modeled with SLk grammars with k ?",
        "6.",
        "Of the rest, all but four are LT1 + SP4 and all but two are LT2 + SP4.",
        "Both of these last two are properly regular (Wibel et al., in prep)."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We have shown how subregular classes of languages can be learned over factored representations, which can be exponentially more compact than representations with a single DFA.",
        "Essentially, words in the data presentation are passed through each factor, ?activating?",
        "the parts touched.",
        "This approach immediately allows one to naturally ?mix?",
        "well-characterized learnable subregular classes in such a way that the resulting language class is also learnable.",
        "While this mixing is partly motivated by the different kinds of phonotactic patterns in natural language, it also suggests a very interesting theoretical possibility.",
        "Specifically, we anticipate that the right parameterization of these well-studied subregular classes will cover the class of star-free languages.",
        "Future work could also include extending the current analysis to factoring stochastic languages, perhaps in a way that connects with earlier research on factored HMMs (Ghahramani and Jordan, 1997)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This paper has benefited from the insightful comments of three anonymous reviewers, for which the authors are grateful.",
        "The authors also thank Jie Fu and Herbert G. Tanner for useful discussion.",
        "This research was supported by NSF grant 1035577 to the first author, and the work was completed while the second author was on sabbatical at the Department of Linguistics and Cognitive Science at the University of Delaware."
      ]
    }
  ]
}
