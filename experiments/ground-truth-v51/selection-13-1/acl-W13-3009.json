{
  "info": {
    "authors": [
      "Eric Corlett",
      "Gerald Penn"
    ],
    "book": "MoL",
    "id": "acl-W13-3009",
    "title": "Why Letter Substitution Puzzles are Not Hard to Solve: A Case Study in Entropy and Probabilistic Search-Complexity",
    "url": "https://aclweb.org/anthology/W13-3009",
    "year": 2013
  },
  "references": [
    "acl-P10-1106"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper we investigate the theoretical causes of the disparity between the theoretical and practical running times for the A?",
        "algorithm proposed in Corlett and Penn (2010) for deciphering letter-substitution ciphers.",
        "We argue that the difference seen is due to the relatively low entropies of the probability distributions of character transitions seen in natural language, and we develop a principled way of incorporating entropy into our complexity analysis.",
        "Specifically, we find that the low entropy of natural languages can allow us, with high probability, to bound the depth of the heuristic values expanded in the search.",
        "This leads to a novel probabilistic bound on search depth in these tasks."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "When working in NLP, we can find ourselves using algorithms whose worst-case running time bounds do not accurately describe their empirically determined running times.",
        "Specifically, we can often find that the algorithms that we are using can be made to run efficiently on real-world instances of their problems despite having theoretically high running times.",
        "Thus, we have an apparent disparity between the theoretical and practical running times of these algorithms, and so we must ask why these algorithms can provide results in a reasonable time frame.",
        "We must also ask to what extent we can expect our algorithms to remain practical as we change the downstream domains from which we draw problem instances.",
        "At a high level, the reason such algorithms can work well in the real world is that the real world applications from which we draw our inputs do not tend to include the high complexity inputs.",
        "In other words, our problem space either does not cover all possible inputs to the algorithm, or it does, but with a probability distribution that gives a vanishingly small likelihood to the ?hard?",
        "inputs.",
        "Thus, it would be beneficial to incorporate into our running time analysis the fact that our possible inputs are restricted, even if only restricted in relative frequency rather than in absolute terms.",
        "This means that any running time that we observe must be considered to be dependent on the distribution of inputs that we expect to sample from.",
        "It probably does not come as a surprise that any empirical analysis of running time carries with it the assumption that the data on which the tests were run are typical of the data which we expect to see in practice.",
        "Yet the received wisdom on the asymptotic complexity of algorithms in computational linguistics (generally what one might see in an advanced undergraduate algorithms curriculum) has been content to consider input only in terms of its size or length, and not the distribution from which it was sampled.",
        "Indeed, many algorithms in NLP actually take entire distributions as input, such as language models.",
        "Without a more mature theoretical understanding of time complexity, it is not clear exactly what any empirical running time results would mean.",
        "A worst-case complexity result gives a guarantee that an algorithm will take no more than a certain number of steps to complete.",
        "An average-case result gives the expected number of steps to complete.",
        "But an empirical running time found by sampling from a distribution that is potentially different from what the algorithm was designed for is only a lesson in how truly different the distribution is.",
        "It is also common for the theoretical study of asymptotic time complexity in NLP to focus on the worst-case complexity of a problem or algorithm rather than an expected complexity, in spite of the existence for now over 20 years of methods for average-case analysis of an algorithm.",
        "Even these, however, often assume a uniform distribu",
        "tion over input, when in fact the true expectation must consider the probability distribution that we will draw the inputs from.",
        "Uniform distributions are only common because we may not know what the distribution is beforehand.",
        "Ideally, we should want to characterize the running time of an algorithm using some known properties of its input distribution, even if the precise distribution is not known.",
        "Previous work that attempts this does exist.",
        "In particular, there is a variant of analysis referred to as smoothed analysis which gives a bound on the average-case running time of an algorithm under the assumption that all inputs are sampled with Gaussian measurement error.",
        "As we will argue in Section 2, however, this approach is of limited use to us.",
        "We instead approach the disparity of theoretical and practical running time by making use of statistics such as entropy, which are taken from the input probability distributions, as eligible factors in our analysis of the running time complexity.",
        "This is a reasonable approach to the problem, in view of the numerous entropic studies of word and character distributions dating back to Shannon.",
        "Specifically, we analyze the running time of the A?",
        "search algorithm described in Corlett and Penn (2010).",
        "This algorithm deciphers text that has been enciphered using a consistent letter substitution, and its running time is linear in the length of the text being deciphered, but theoretically exponential in the size of the input and output alphabets.",
        "This na?",
        "?ve theoretical analysis assumes that characters are uniformly distributed, however.",
        "A far more informative bound is attainable by making reference to the entropy of the input.",
        "Because the algorithm takes a language model as one of its inputs (the algorithm is guaranteed to find the model-optimal letter substitution over a given text), there are actually two input distributions: the distribution assumed by the input language model, and the distribution from which the text to be deciphered was sampled.",
        "Another way to view this problem is as a search for a permutation of letters as the outcomes of one distribution such that the two distributions are maximally similar.",
        "So our informative bound is attained through reference to the cross-entropy of these two distributions.",
        "We first formalize our innate assumption that these two distributions are similar, and build an upper bound for the algorithm's complexity that incorporates the cross-entropy between the two distributions.",
        "The analysis concludes that, rather than being exponential in the length of the input or in the size of the alphabets, it is merely exponential in the cross-entropy of these two distributions, thus exposing the importance of their similarity.",
        "Essentially, our bound acts as a probability distribution over the necessary search depth."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "The closest previous work to the analysis presented here is the use of smoothed analysis to explain the tractable real-world running time of a number of algorithms with an exponential worst-case complexity.",
        "These algorithms include the simplex algorithm, as described by Spielman and Teng (2004), the k-means clustering algorithm, as described by Arthur et al. (2009) and others.",
        "As in our current approach, smoothed analysis works by running a general average-case analysis of the algorithms without direct knowledge of the distribution from which the problem inputs have been drawn.",
        "The assumption made in smoothed analysis is that every input has been read with some Gaussian measurement error.",
        "That is, in a typical worst-case analysis, we may have an adversary choose any input for our algorithm, after which we must calculate how bad the resulting running time might be, but in a smoothed analysis, the adversary gives us input by placing it into the real world so that we may measure it, and this measurement adds a small error drawn from a Gaussian distribution to the problem instance.",
        "The point of smoothed analysis is to find the worst average-case running time, under these conditions, that the adversary can subject us to.",
        "Thus the analysis is an average case, subject to this error, of worst cases.",
        "In the papers cited above, this method of analysis was able to drop running times from exponential to polynomial.",
        "It is unfortunate that this approach does not readily apply to many of the algorithms that we use in NLP.",
        "To see why this is, simply note that we can only add a small Gaussian error to our inputs if our inputs themselves are numerical.",
        "If the inputs to our algorithms are discrete, say, in the form of strings, then Gaussian errors are not meaningful.",
        "Rather, we must ask what sort of error we can expect to see in our inputs, and to what extent these errors contribute to the running time of our algorithms.",
        "In the case of decipherment, ?error?",
        "is committed by substituting one character for an",
        "other consistently.",
        "The strongest known result on the search complexity of A?",
        "is given in Pearl (1984).",
        "This work found that, under certain assumptions, a bound on the absolute error between the heuristic used and the true best cost to reach the goal yields a polynomial worst-case depth for the search.",
        "This happens when the bound is constant across search instances of different sizes.",
        "On the other hand, if the relative error does not have this constant bound, the search complexity can still be exponential.",
        "This analysis assumes that the relative errors in the heuristic are independent between nodes of the search tree.",
        "It is also often very difficult even to calculate the value of a heuristic that possesses such a bound, as it might involve calculating the true best cost, which can be as difficult as completely solving a search problem instance (Korf et al., 2001).",
        "Thus, most practical heuristics still give rise to theoretically exponential search complexities in this view.",
        "In Korf and Reid (1998) and Korf et al. (2001), on the other hand, several practical problems are treated, such as random k-SAT, Rubik's cubes, or sliding tile puzzles, which are not wholly unlike deciphering letter substitution puzzles in that they calculate permutations, and therefore can assume, as we do, that overall time complexity directly corresponds to the number of nodes visited at different depths in the search tree that have a heuristic low enough to guarantee node expansion.",
        "But their analysis assumes that it is possible to both estimate and use a probability distribution of heuristic values on different nodes of the search graph, whereas in our task, this distribution is very difficult to sample because almost every node in the search graph has a worse heuristic score than the goal does, and would therefore never be expanded.",
        "Without an accurate idea of what the distribution of the heuristic is, we cannot accurately estimate the complexity of the algorithm.",
        "On the other hand, their analysis makes no use of any estimates of the cost of reaching the goal, because the practical problems that they consider do not allow for particularly accurate estimates.",
        "In our treatment, we find that the cost to reach the goal can be estimated with high probability, and that this estimate is much less than the cost of most nodes in the search graph.",
        "These different characteristics allow us to formulate a different sort of bound on the search complexity for the decipherment problem."
      ]
    },
    {
      "heading": "3 The Algorithm",
      "text": [
        "We now turn to the algorithm given in Corlett and Penn (2010) which we will investigate, and we explain the model we use to find our bound.",
        "The purpose of the algorithm is to allow us to read a given ciphertext C which is assumed to be generated by putting an unknown plaintext P through an unknown monoalphabetic cipher.",
        "We will denote the ciphertext alphabet as ?c and the plaintext alphabet as ?p.",
        "Given any string T , we will denote n(T ) as the length of T .",
        "Furthermore, we assume that the plaintext P is drawn from some string distribution q.",
        "We do not assume q to be a trigram distribution, but we do require it to be a distribution from which trigrams can be calculated (e.g, a 5-gram corpus will in general have probabilities that cannot be predicted using the associated trigrams, but the associated trigram corpus can be recovered from the 5-grams).",
        "It is important to realize in the algorithm description and analysis that q may also not be known exactly, but we only assume that it exists, and that we can approximate it with a known trigram distribution p. In Corlett and Penn (2010), for example, p is the trigram distribution found using the Penn treebank.",
        "It is assumed that this is a good approximation for the distribution q, which in Corlett and Penn (2010) is the text in Wikipedia from which ciphers are drawn.",
        "As is common when dealing with probability distributions over natural languages, we assume that both p and q are stationary and ergodic, and we furthermore assume that p is smooth enough that any trigram that can be found in any string generated by q occurs in p (i.e., we assume that the cross entropyH(p, q) is finite).",
        "The algorithm works in a model in which, for any run of the algorithm, the plaintext string P is drawn according to the distribution q.",
        "We do not directly observe P , but instead its encoding using the cipher key, which we will call piT .",
        "We observe the ciphertext C = pi?1T (P ).",
        "We note that piT is unknown, but that it does not change as new ciphertexts are drawn.",
        "Now, the way that the algorithm in Corlett and Penn (2010) works is by searching over the possible keys to the cipher to find the one that maximizes the probability of the plaintext according to the distribution p. It does so as follows.",
        "In addition to the possible keys to the cipher,",
        "weakened cipher keys called partial solutions are added to the search space.",
        "A partial solution of size k (denoted as pik) is a section of a possible full cipher key which is only defined on k character types in the cipher.",
        "We consider the character types to be fixed according to some preset order, and so the k fixed letters in pik do not change between different partial solutions of size k. Given a partial solution pik, a string pi",
        "is defined whose probability we use as an upper bound for the probability of the plaintext whenever the true solution to the cipher contains pik as a subset.",
        "The string pin(C)k (C) is the most likely string that we can find that is consistent with C on the letters fixed by pik.",
        "That is, we define the set ?k so that S ?",
        "?k iff whenever si and ci are the characters at index i in S and C, then si = pik(ci) if ci is fixed in pik.",
        "Note that if ck is not fixed in pik, we let si take any value.",
        "We extend the partial character function to the full string function pin(C)k on ?",
        "In Corlett and Penn (2010), the value pin(C)k (C) is efficiently computed by running it through the Viterbi algorithm.",
        "That is, given C, p and pik, a run of the Viterbi algorithm is set up in which the letter transition probabilities are those that are given in p. In order to describe the emission probabilities, suppose that we partition the ciphertext alphabet ?c into two sets ?1 and ?2, where ?1 is the set of ciphertext letters fixed by pik.",
        "For any plaintext letter y ?",
        "?p, if there is a ciphertext letter x ?",
        "?1 such that y ?",
        "x is a rule in pik, then the emission probability that y will be seen as x is set to 1, and the probability that y will be seen as any other letter is set to 0.",
        "On the other hand, if there is no rule y ?",
        "x in pik for any ciphertext letter x, then the emission probability associated with y is uniform over the letters x ?",
        "?2 and 0 for the letters x ?",
        "?1.",
        "The search algorithm described in Corlett and Penn (2010) uses the probability of the string pin(C)k (C), or more precisely, the log probability ?logprobp(pi n(C) k (C)), as an A ?",
        "heuristic over the partial solutions pik.",
        "In this search, an edge is added from a size k partial solution pik to a size k + 1 partial solution pik+1 if pik agrees with pik+1 wherever it is defined.",
        "The score of a node pik is the log probability of its associated string: ?logprobp(pi n(C) k (C)).",
        "We can see that if pik has an edge leading to pik+1, then ?k+1 ?",
        "?k, so that",
        "Thus, the heuristic is nondecreasing.",
        "Moreover, by applying the same statement inductively we can see that any full solution to the cipher that has pik as a subset must have a score at least as great as that of pik.",
        "This means that the score never overestimates the cost of completing a solution, and therefore that the heuristic is admissible."
      ]
    },
    {
      "heading": "4 Analysis",
      "text": [
        "The bound that we will prove is that for any k > 0 and for any ?, ?",
        "> 0, there exists an n ?",
        "N such that if the length n(C) of the cipher C is at least n, then with probability at least 1 ?",
        "?, the search for the key to the cipher C requires no more than 2n?(H(p,q)+?)",
        "expansions of any partial solution of size k to complete.",
        "Applying the same bound over every size k of partial solution will then give us that for any ?, ?",
        "> 0, there exists a n0 > 0 such that if the length n(C) of the cipher C is at least n, then with probability at least 1 ?",
        "?, the search for the key to the cipher C requires no more than 2n(H(p,q)+?)",
        "expansions of any partial solution of size greater than 0 to complete (note that there is only one partial solution of size 0).",
        "Let pi?",
        "be the solution that is found by the search.",
        "This solution has the property that it is the full solution that induces the most probable plaintext from the cipher, and so it produces a plaintext that is at least as likely as that of the true solution",
        "We find our bound by making use of the fact that an A?",
        "search never expands a node whose score is greater than that of the goal node pi?.",
        "Thus, a partial solution pik is expanded only if",
        "we have that pik is expanded only if",
        "So we would like to count the number of solutions satisfying this inequality.",
        "We would first like to approximate the value of ?logprobp(P ), then.",
        "But, since P is drawn from an ergodic stationary distribution q, this value will approach the cross entropy H(p, q) with high probability: for any ?1, ?1 > 0, there exists an",
        "with probability at least 1 ?",
        "?1.",
        "In this case, we have that ?logprobp(P ) < n(C)(H(p, q) + ?1).",
        "Now, if k is fixed, and if pik and pi?k are two different size k partial solutions, then pik and pi?k must disagree on at least one letter assignment.",
        "Thus, the sets ?k and ?",
        "?k must be disjoint.",
        "But then we also have that pin(C)k (C) 6= pi n(C)?",
        "k (C).",
        "Therefore, if we can find an upper bound for the size of the",
        "we will have an upper bound on the number of times the search will expand any partial solution of size k. We note that under the previous assumptions, and with probability at least 1?",
        "?1, none of these strings can have a log probability larger than n(C)(H(p, q) + ?1).",
        "For any plaintext string C drawn from q, we let aPb be the substring of P between the indices a and b.",
        "Similarly, we let aSb be the substring of S = pin(C)k (C) between the indices a and b.",
        "We now turn to the proof of our bound: Let",
        "?, ?",
        "> 0 be given.",
        "We give the following three bounds on n: (a) As stated above, we can choose n1 so that for any string P drawn from q with length at least n1, |?",
        "logprobp(P )/n(P )?H(p, q) |< ?1/2 with probability at least 1?",
        "?/3.",
        "(b) We have noted that if k is fixed then any two size k partial solutions must disagree on at least one of the letters that they fix.",
        "So if we have a substring aPb of P with an instance of every letter type fixed by the partial solutions of size k, then the substrings aSb of S must be distinct for every S ?",
        "{S ?",
        "?n(C)p |S = pin(C)k (C) for some pik}.",
        "Since q is ergodic, we can find an n2 such that for any string P drawn from q with length at least n2, every letter fixed in pik can be found in some length n2 substring P2 of P , with probability at least 1?",
        "?/3.",
        "(c) By the Lemma below, there exists an n?",
        "> 0 such that for all partial solutions pik, there exists a trigram distribution rk on the alphabet ?p such that if S = pi",
        "with a probability of at least 1?",
        "?/3.",
        "Let n = max(n1, n2, n?).",
        "Then, the probability of any single one of the properties in (a), (b) or (c) failing in a string of length at least n is at most ?/3, and so the probability of any of them failing is at most ?.",
        "Thus, with a probability of at least 1?",
        "?, all three of the properties hold for any string P drawn from q with length at least n. Let P be drawn from q, and suppose n(P ) > n. Let aPb be a length n substring of P containing a token of every letter type fixed by the size k partial solutions.",
        "Suppose that pik is a partial solution such that",
        "So, for our bound we will simply need to find the number of substrings aSb such that",
        "strings we need becomes",
        "Thus, we have a bound of 2n?(H(p,q)+?)",
        "on the number of substrings of length n satisfying ?",
        "log probp(aSb) < n(H(p, q) + ?).",
        "Since we know that with probability at least 1?",
        "?, these are the only strings that need be considered, we have proven our bound."
      ]
    },
    {
      "heading": "4.1 Lemma:",
      "text": [
        "We now show that for any fixed k > 0 and ?",
        "?, ??",
        "> 0, there exists some n?",
        "> 0 such that for all partial solutions pik, there exists a trigram distribution rk on the alphabet ?p such that if S = pi",
        "with a probability of at least 1?",
        "??.",
        "Proof of Lemma: Given any partial solution pik, it will be useful in this section to consider the strings S = pin(C)k (C) as functions of the plaintext P rather than the ciphertext C. Since C =",
        "piT is derived from a character bijection between ?c and ?p, and since pi n(C) k fixes the k character types in ?c that are defined in pik, we have that pin(C)?k fixes k character types in ?p.",
        "Let ?P1 be the set of k character types in ?p that are fixed by pin(C)?k , and let ?P2 = ?p \\?P1 .",
        "We note that ?P1 and ?P2 do not depend on which pik we use, but only on k. Now, any string P which is drawn from q can be decomposed into overlapping substrings by splitting it whenever it has see two adjacent characters from ?P1 .",
        "When we see a bigram in P of this form, say, y1y2, we split P so that both the end of the initial string and the beginning of the new string are y1y2.",
        "Note that when we have more than two adjacent characters from ?P1 we will split the string more than once, so that we get a series of three-character substrings of P in our decomposition.",
        "As a matter of bookkeeping we will consider the initial segment to begin with two start characters s with indices corresponding to 0 and ?1 in P .",
        "As an example, consider the string P = friends, romans, countrymen, lend me your ears Where ?P1 = {?",
        "?, ?, ?, ?a?, ?y?}.",
        "In this case, we would decompose P into the strings ?ssfriends, ?, ?, romans, ?, ?, countrymen, ?, ?, lend me ?, ?e y?, ?",
        "your e?",
        "and ?",
        "ears?.",
        "Let M be the set of all substrings that can be generated in this way by decomposing strings P which are drawn from q.",
        "Since the end of any string m ?M contains two adjacent characters in ?P1 and since the presence of two adjacent characters in ?P1 signals a position at which a string will be decomposed into segments, we have that the set M is prefix-free.",
        "Every string m ?",
        "M is a string in ?p, and so they will have probabilities probq(m) in q.",
        "It should be noted that for any m ?",
        "M the probability probq(m) may be different from the trigram probabilities predicted by q, but will instead be the overall probability in q of seeing the string m. For any pair T, P of strings, let #(T, P ) be the number of times T occurs in P .",
        "Since we assume that the strings drawn from q converge to the distribution q, we have that for any ?3, ?3 > 0 and any n4 > 0, there exists an n3 > 0 such that for any substring P3 of P of length at least n3, where P is drawn from q, and for any m ?",
        "M of length at most n4, the number |#(m,P )/len(P3) ?",
        "probq(m) |< ?3 with probability greater than 1?",
        "?3.",
        "Now suppose that for some P drawn from q we have a substring aPb of P such that aPb = m,m ?",
        "M .",
        "If S = pin(C)?k (P ), consider the substring aSb of S. Recall that the string function pin(C)?k can map the characters in P to S in one of two ways: if a character xi ?",
        "?P1 is found at index i in P , then the corresponding character in S",
        "is pik(xi).",
        "Otherwise, xi is mapped to whichever character yi in ?P maximizes the probability in p of S given pin(C)?k (xi?2)pi n(C)?",
        "k (xi?1)yi.",
        "Since the values of pin(C)?k (xi?2), pi n(C)?",
        "k (xi?1) and yi are interdependent, and since pin(C)?k (xi?2) is dependent on its previous two neighbors, the value that yi takes may be dependent on the values taken by pin(C)?k (xj) for indices j quite far from i.",
        "However, we see that no dependencies can cross over a substring in P containing two adjacent characters in ?P1 , since these characters are not transformed by pin(C)?k in a way that depends on their neighbors.",
        "Thus, if aPb = m ?",
        "M , the endpoints of aPb are made up of two adjacent characters in ?P1 , and so the substring aSb of S depends only on the substring aPb of P .",
        "Specifically, we see that",
        "Since we can decompose any P into overlapping substrings m1,m2, .",
        ".",
        ".",
        ",mt in M , then, we can carry over this decomposition into S to break",
        "k (mt).",
        "Note that the score generated by S in the A?",
        "search algorithm is the sum ?",
        "1?i?",
        "logprobp(yi?2yi?1yi), where yi is the ith character in S. Also note that every three-character sequence yi?2yi?1yi occurs exactly once in the decomposition",
        "k (mt).",
        "Since for anym the number of occurrences of pin(C)?k (m) in S under this decomposition will be equal to the number of occurrences of m in P , we have that",
        "Having finished these definitions, we can now define the distribution rk.",
        "In principle, this distribution should be the limit of the frequency of trigram counts of the strings S = pin(C)?k (P ), where n(P ) approaches infinity.",
        "Given a string S = pin(C)?k (P ), where P is drawn from q, and given any trigram y1y2y3 of characters in ?p, this frequency count is",
        "as we have done above, we see that any instance of the trigram y1y2y3 in S occurs in exactly one of the substrings pin(C)?k (mi), 1 ?",
        "i ?",
        "t. Grouping together similar mis, we find",
        "As n(P ) approaches infinity, we find that #(m,P )n(P ) approaches probq(m), and so we can write",
        "so we have that probrk is a valid probability distribution.",
        "In the above calculation we can rearrange the terms, so convergence implies absolute convergence.",
        "The sum",
        "gives (n(pin(C)?k (m)) ?",
        "2) because there is one trigram for every character in pin(C)?k (m), less two to compensate for the endpoints.",
        "However, since the different m overlap by two in a decomposition from P , the sum (n(m) ?",
        "2)#(m,P ) just gives back the length n(P ), allowing for the fact that the initial m has two extra start characters.",
        "Having defined rk, we can now find the value of H(p, rk).",
        "By definition, this term will be",
        "?logprobp(m)probq(m).",
        "Now, we can finish the proof of the Lemma.",
        "Holding k fixed, let ?",
        "?, ??",
        "> 0 be given.",
        "Since we have assumed that p does not assign a zero probability to any trigram generated by q, we can find a trigram x1x2x3 generated by q whose probability in p is minimal.",
        "Let X = ?logprobp(x1x2x3), and note that probp(x1x2x3) > 0 implies X < ?.",
        "Since we know by the argument above that when P is sampled from q,",
        "Thus, we can choose n4 so that",
        "n?",
        "such that if P is sampled from q and aPb is a substring of P with length greater than n?, then with probability at least 1 ?",
        "?",
        "?, for every m ?",
        "M we will have that",
        "Let pik be any partial solution of length k, and let rk be the trigram probability distribution described above.",
        "Then let P be sampled from q, and let S = pin(C)k (C) = pi n(C)?",
        "k (P ), and let a, b be indices of S such that b ?",
        "a = n > n?.",
        "Finally, we will partition the set M as follows: we let M ?",
        "be the set {m ?M |n(n) ?",
        "n4} andM ??",
        "be the set {m ?",
        "M |n(m) > n4}.",
        "Thus, we have that",
        "is the character at the index i in pin(C)?k (m).",
        "Taking the maximum possible values for ?logprobp(yiyi+1yi+2), we find that this sum is at most (n(m)?",
        "2)X .",
        "Applying this bound to the previous formula, we find that it is at most"
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we discussed a discrepancy between the theoretical and practical running times of certain algorithms that are sensitive to the entropies of their input, or the entropies of the distributions from which their inputs are sampled.",
        "We then used the algorithm from Corlett and Penn (2010) as a subject to allow us to investigate ways to talk about average-case complexity in light of this discrepancy.",
        "Our analysis was sufficient to give us a bound on the search complexity of this algorithm which is exponential in the cross-entropy between the training distribution and the input distribution.",
        "Our method in effect yields a probabilistic bound on the depth of the search heuristic used.",
        "This leads to an exponentially smaller search space for the overall problem.",
        "We must note, however, that our analysis does not fully reconcile the discrepancy between the",
        "theoretical and practical running time for this algorithm.",
        "In particular, our bound still does not explain why the number of search nodes expanded by this algorithm tends to converge on one per partial solution size as the length of the string grows very large.",
        "As such, we are interested in further studies as to how to explain the running time of this algorithm.",
        "It is our opinion that this can be done by refining our description of the sets ?k to exclude strings which cannot be considered by the algorithm.",
        "Not only would this allow us to reduce the overall number of strings we would have to count when determining the bound, but we would also have to consider fewer strings when determining the value of n?.",
        "Both changes would reduce the overall complexity of our bound.",
        "This general strategy may have the potential to illuminate the practical time complexities of approximate search algorithms as well."
      ]
    }
  ]
}
