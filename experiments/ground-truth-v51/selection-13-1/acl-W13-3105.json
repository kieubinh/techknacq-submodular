{
  "info": {
    "authors": [
      "Lei Li",
      "Wei Heng",
      "Jia Yu",
      "Yu Liu",
      "Shuhong Wan"
    ],
    "book": "MultiLing",
    "id": "acl-W13-3105",
    "title": "CIST System Report for ACL MultiLing 2013 â€“ Track 1: Multilingual Multi-document Summarization",
    "url": "https://aclweb.org/anthology/W13-3105",
    "year": 2013
  },
  "references": [
    "acl-P06-2020",
    "acl-P10-1084"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This report provides a description of the methods applied in CIST system participating ACL MultiLing 2013.",
        "Summarization is based on sentence extraction.",
        "hLDA topic model is adopted for multilingual multi-document mod-eling.",
        "Various features are combined to evaluate and extract candidate summary sentences."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "CIST system has participated Track 1: Multilingual Multi-document Summarization in ACL MultiLing 2013 workshop.",
        "It could deal with all ten languages: Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian and Spanish.",
        "It summarizes every topic containing 10 texts and generates a summary in plain text, UTF8 encoding, less than 250 words."
      ]
    },
    {
      "heading": "2 System Design",
      "text": [
        "There have been many researches about multi-document summarization, (Wan et al., 2006; He et al., 2008; Flore et al., 2008; Bellemare et al., 2008; Conroy and Schlesinger, 2008; Zheng and Takenobu, 2009; Louis and Nenkova, 2009; Long et al., 2009; Lin and Chen, 2009; Gong et al., 2010; Darling, 2010; Kumar et al., 2010; Genest and Lapalme, 2010; Jin et al., 2010; Kennedy et al., 2010; Zhang et al., 2011), but less about multilingual multi-document summarization (Leuski et al., 2003; Liu et al., 2011; Conroy et al., 2011; Hmida and Favre, 2011; Das and Srihari, 2011; Steinberger et al., 2011; Saggion, 2011; El-Haj et al., 2011).",
        "This system must be applicable for unlimited topics, we couldn't use topic knowledge.",
        "Different topic has different language styles, so we use sentence as the processing unit and summarization method based on sentence extraction.",
        "It must also be available for different languages, we couldn't use much specific knowledge for all languages except one or two we understand.",
        "We refer to a statistical method, hLDA (hierarchical Latent Dirichlet Allocation (LDA)).",
        "LDA has been widely applied.",
        "(Arora and Balaraman, 2008; Krestel et al., 2009).",
        "Some improvements have been made.",
        "(Griffiths et al., 2005; Blei and Lafferty, 2006; Wang and Blei, 2009).",
        "One is to relax its assumption that topic number is known and fixed.",
        "Teh et al. (2006) provided an elegant solution.",
        "Blei et al. (2010) extended it to exploit the hierarchical tree structure of topics, hDLA, which is unsupervised method in which topic number could grow with the data set automatically.",
        "There's no relations between topics in LDA (Blei, 2003), but hLDA could organize topics into a hierarchy, in which higher level topics are more abstractive.",
        "This could achieve a deeper semantic model similar with human mind and is especially helpful for summarization.",
        "Celikyilmaz (2010) provided a multi-document summarization method based on hLDA with competitive results.",
        "However, it has the disadvantage of relying on ideal summaries.",
        "To avoid this, the innovation of our work is completely dependent on data and hierarchy to extract candidate summary sentences.",
        "Figure 1 and 2 show the framework for ten languages.",
        "Since Chinese Hanzi is different from other languages, we treat it with special processing.",
        "But the main modules are the same.",
        "The kernel one is constructing an hLDA model1.",
        "It?s"
      ]
    },
    {
      "heading": "3 Text Pre-processing",
      "text": [
        "There are some unified preprocessing steps for all languages and a special step for Chinese."
      ]
    },
    {
      "heading": "3.1 Merging Documents",
      "text": [
        "We treat multi-document together, so we firstly combine them into a big text.",
        "As to Chinese, we combine and delete empty lines.",
        "As to other nine languages, we do this when we split sentences."
      ]
    },
    {
      "heading": "3.2 Splitting Sentences",
      "text": [
        "We split sentences to get the processing unit.",
        "There are two lines of title and date ending with no punctuation mark.",
        "We add a full stop ourselves to avoid them being connected with the first sentence.",
        "For Chinese, we split sentences according to ending punctuation marks, while for other nine languages, the full stop ?.?",
        "could have other functions.",
        "We adopt machine learning method 2 .",
        "After some experiments, we choose Support Vector Machine model for English and French, Na?ve Bayes model for other 7 languages."
      ]
    },
    {
      "heading": "3.3 Removing Stop Words",
      "text": [
        "We add ICTCLAS3 word segmentation to Chinese to make all languages have the same word separator.",
        "Then we could obtain words easily, among which are some stop words.",
        "We construct stop lists.",
        "For English and Chinese, the stop list contains punctuation marks and some functional words, while for other languages, it contains punctuation marks, which could unified the whole process easily although generally we do not treat punctuation marks as words.",
        "At the same time, all capitalized characters are changed to lower case."
      ]
    },
    {
      "heading": "3.4 Generating Input File for hLDA",
      "text": [
        "We build a dictionary for remaining words, which are sorted according to frequency.",
        "The more frequent words are located before the less frequent ones.",
        "This is a mapping from word to a number varying from 1 to dictionary size.",
        "Finally we generate an input file for hLDA, in which each line represents a sentence, in the following form: [number of words in the sentence] [word-NumberA]:[local frequencyA] [word-NumberB]:[local frequencyB]...",
        "Figure 3 shows an example.",
        "As we can see that now it's language independent."
      ]
    },
    {
      "heading": "4 hLDA Topic Modeling",
      "text": [
        "Given a collection of sentences in the input file, we wish to discover common usage patterns or topics and organize them into a hierarchy.",
        "Each node is associated with a topic, which is a distribution across words.",
        "A sentence is generated by choosing a path from the root to a leaf, repeatedly sampling topics along that path, and sampling the words from the selected topics.",
        "Sentences sharing the same path should be similar to each other because they share the same sub-topics.",
        "All sentences share the topic distribution associated with the root node.",
        "As to this system, we set hierarchy depth to 3, because we have found out in former experiments that 2 is too simple, and 4 or bigger is too complex for the unit of sentence."
      ]
    },
    {
      "heading": "4.1 Hierarchy Evaluation",
      "text": [
        "In order to make sure that a hierarchy is good, we need to evaluate its performance.",
        "The best method is human reading, but it's too laborious to browse all topics and all languages.",
        "In fact, we could not understand all ten languages at all.",
        "So we build another simpler and faster evaluation method based on numbers.",
        "According to former empirical analysis, if a hierarchy has more than 4 paths and the sentence numbers for all paths appear in balanced order from bigger to smaller, and the sentences in bigger paths could occupy 70-85% in all sentences, then we could possibly infer that this hierarchy is good."
      ]
    },
    {
      "heading": "4.2 Parameter Setting",
      "text": [
        "When facing a new corpus, we could hardly set the parameters automatically either by human or machine.",
        "There is a choice of sampling.",
        "We tried it for all languages with 100000 iterations.",
        "But the results are poor, even in the worst case each sentence is set to a single path.",
        "Thus we give up sampling and try to set the parameters by human.",
        "We begin with Chinese because it seems to be the most difficult case.",
        "We randomly choose two topics for original testing and set some parameters according to former experience.",
        "Then we evaluate the result using method in 4.1.",
        "If it's not good, we go on to adjust the settings until we obtain a satisfactory result.",
        "The satisfied settings are then used originally for the whole corpus.",
        "Table 1 shows the details.",
        "After running the whole corpus, we evaluate the results again.",
        "We found out that for most cases, the hierarchy is good, but there are some cases not so good, as shown in Table 2.",
        "So one set of parameter settings could not deal with all languages and topics successfully.",
        "The reason may be that different language and different topic must have different inherent features."
      ]
    },
    {
      "heading": "4.3 Parameter Adjustment",
      "text": [
        "We analyze the bad results and try to adjust the settings.",
        "For instance, in English M006, there are only two paths indicating that the tree is too clustered.",
        "Parameter ETA should be reduced to separate more sub-topics.",
        "But too small ETA may lead to hLDA failure without level assignment result in limited iterations.",
        "So we also adjust GEM to get closer to the prior explanation of corpus.",
        "In some case, the numbers are assigned too much to the former big paths, then we should adjust SCALING parameters to separate some numbers to the smaller paths.",
        "For the bad cases in Table 2, we finally use the settings in Table 3."
      ]
    },
    {
      "heading": "5 Summary Generation",
      "text": []
    },
    {
      "heading": "5.1 Sentence Evaluation",
      "text": [
        "In the hLDA result, sentences are clustered into subtopics in a hierarchical tree.",
        "A subtopic is more important if it contains more sentences.",
        "Trivial subtopics containing only one or two sentences could be neglected.",
        "Final summary",
        "should cover those most important subtopics with their most representative sentences.",
        "We evaluate the sentence importance in a sub-topic",
        "considering three features.",
        "1) Sentence coverage, which means that how",
        "much a sentence could contain words appearing in more sentences for a sub-topic.",
        "We consider sentence coverage of each word in one sentence.",
        "The sentence weight is calculated as eq.",
        "(1).",
        "Where wi is the ith word in sentence s, nums(wi)",
        "is the number of sentences that wi covers, |s |is the number of words in the sentence, and n is the total number of all sentences.",
        "2) Word Abstractive level.",
        "hLDA constructs a hierarchy by positioning all sentences on a three",
        "level tree.",
        "Level 0 is the most abstractive one, level 2 is the most specific one, and level 1 is between them.",
        "We evaluate the sentence abstractive feature as eq.(2).",
        "(2) Where num(W0), num(W1), num(W2) are numbers of level 0, 1 and 2 words respectively in the sentence.",
        "There are three parameters: a, b and c, which are used to control the weights for words in different levels.",
        "Although we hope the summary to be as abstractive as possible, there is really some specific information we also want.",
        "For instance, earthquake news needs specific information about death toll and money lost.",
        "3) Named entity.",
        "We consider the number of named entities in one sentence.",
        "This time we only have time to use Stanford's named entity recognition toolkit4, which could identify English person, address and institutional names.",
        "If one sentence contains more entities, then it has a high priority to be chosen as candidate summary sentence.",
        "Let Sn be the number of named entity categories in one sentence.",
        "For example, if one sentence has only person names, then Sn is 1; else if it also has address information, then Sn is 2; else if it contains all three categories, then Sn is 3.",
        "At last, we calculate sentence score S as eq.",
        "(3, 4), where d, e and f are feature weights:",
        "After experiments, we set {a, b, c, d, e, f} to {0.3, 1, 0.3, 2, 1, 0.05} for English, {a, b, c, d, e}"
      ]
    },
    {
      "heading": "4 http://nlp.stanford.edu/software/CRF-NER.shtml",
      "text": [
        "to {1, 0.75, 0.25, 2, 1} for Chinese without M004 and M006, and {0.3, 1, 0.3, 2, 1} for others."
      ]
    },
    {
      "heading": "5.2 Summary Generation",
      "text": [
        "We extract 30 candidate sentences with high S ordered by S from bigger to smaller and check them one by one.",
        "We use 30 sentences to make sure that when a candidate sentence is not good to be in a final summary, we could have enough other alternative sentences with less S. Then we generate the final summary as Figure 5."
      ]
    },
    {
      "heading": "6 Evaluations",
      "text": [
        "We?ve got only the automatic evaluation result.",
        "CIST could get best performance in some language, such as Hindi in ROUGE, and in some topics, such as Arabic M104, English and Romania M005, Czech M007, Spanish M103 etc.",
        "in N-gram graph methods: AutoSummENG, MeMoG and NPowER.",
        "CIST could also get nearly worst performance in some cases, such as French and Hebrew.",
        "In other cases it gets middle performance.",
        "But Chinese result looks very strange to us; we think that it needs more special discussion."
      ]
    },
    {
      "heading": "7 Conclusion and Future Work",
      "text": [
        "hLDA is a language independent model.",
        "It could work well sometimes, but not stable enough.",
        "Future work will focus on parameter adjustment, modeling result evaluation, sentence evaluation and good summary generation."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We get support from NSFC 61202247, 71231002, Fundamental Research Funds for Central Universities 2013RC0304 and Beijing Science and Technology Information Institute.",
        "2(3):245?269.",
        "Arora Rachit, and Balaraman Ravindran, 2008.",
        "Latent dirichlet alocation based multi-document summarization.",
        "Proceedings of the second workshop on Analytics for noisy unstructured text data.",
        "ACM, 2008.",
        "Asli Celikyilmaz and Dilek Hakkani-Tur.",
        "2010.",
        "A hybrid hierarchical model for multi-document summarization.",
        "Proceedings of the 48th Annual Meeting of the Association for Computa"
      ]
    }
  ]
}
