{
  "info": {
    "authors": [
      "Hang Ren",
      "Weiqun Xu",
      "Yan Zhang",
      "Yonghong Yan"
    ],
    "book": "SIGDIAL",
    "id": "acl-W13-4071",
    "title": "Dialog State Tracking using Conditional Random Fields",
    "url": "https://aclweb.org/anthology/W13-4071",
    "year": 2013
  },
  "references": [
    "acl-W13-4065"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents our approach to dialog state tracking for the Dialog State Tracking Challenge task.",
        "In our approach we use discriminative general structured conditional random fields, instead of traditional generative directed graphic models, to incorporate arbitrary overlapping features.",
        "Our approach outperforms the simple 1-best tracking approach."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Spoken dialog systems have been widely developed in recent years.",
        "However, when dialogs are conducted in noisy environments or the utterance itself is noisy, it is difficult for machines to correctly recognize or understand user utterances.",
        "In this paper we present a novel dialog state tracking method, which directly models the joint probability of hypotheses onN best lists.",
        "Experiments are then conducted on the DSTC shared corpus, which provides a common dataset and an evaluation framework The remainder of this paper is organized as follows.",
        "Section 2 reviews relevant studies in dialog state tracking.",
        "Section 3 introduces our new approach and presents the model and features we used in detail.",
        "Section 4 describes experiment settings and gives the result.",
        "Section 5 concludes this paper with a discussion for possible future directions."
      ]
    },
    {
      "heading": "2 Previous Work",
      "text": [
        "For the task of dialog state tracking, previous research focused on dynamic Bayesian models (DBN)(Young et al., 2013).",
        "User goal, dialog history and other variables are modeled in a graphical model.",
        "Usually, Markov assumptions are made and in each turn the dialog state is dependent on the ASR outputs and the dialog state of the previous turn.",
        "Dependency on other features, such as system action, dialog history could be assumed as long as their likelihood is modeled.",
        "For a POMDP-based dialog model, the state update rule is as follows:",
        "where bt(st) is the belief state at time t, ot+1 is the observation at time t+ 1, at is the machine action.",
        "Thus the dialog states are estimated incrementally turn by turn.",
        "Since each node has hundreds, or even thousands, of possible assignments, approximation is necessary to make efficient computation possible.",
        "In POMDP-based dialog systems, two common approaches are adopted (Young et al., 2013), i.e., N best approximation and domain factorization.",
        "In theN best approach, the probability distribution of user goals are approximated using N best list.",
        "The hidden information state (HIS) model (Young et al., 2010) makes a further simplification that similar user goals are grouped into a single entity called partition, inside which all user goals are assigned the same probabilities.",
        "The Bayesian update of dialog state (BUDS) model (Thomson and Young, 2010) is a representative of the second approach and adopts a different approximation strategy, where each node is further divided into sub-nodes for different domain concepts and independence assumptions of sub-nodes across concepts are made.",
        "Recent studies have suggested that a discriminative model may yield better performance than a generative one (Bohus and Rudnicky, 2006).",
        "In a discriminative model, the emission part of the state update rule is modeled dis-criminatively.",
        "Possible flawed assumptions in a completely generative models could be mitigated",
        "in this way, such as the approximation of observation probability using SLU scores (Williams, 2012a; Williams, 2012b)."
      ]
    },
    {
      "heading": "3 Proposed Method",
      "text": []
    },
    {
      "heading": "3.1 Discriminative State Tracking Model",
      "text": [
        "Most previous methods model the distribution of user goals for each turn explicitly, which can lead to high computation cost.",
        "In our work, the joint probability of all items on the N best lists from SLU is modeled directly and the state tracking result is generated at a post-processing stage.",
        "Thus the state tracking problem is converted into a la-beling task as is shown in equation 2, which involves modeling the joint probability of the N - best hypotheses.",
        "where Ht,m is a binary variable indicating the truthfulness of the m-th hypothesis at turn t. For each turn, the model takes into account all the slots on theN best lists from the first turn up to the current one, and those slots predicted to be true are added to the dialog state.",
        "The graphical model is illustrated in figure 1.",
        "To predict dialog state at turn t, the N best items from turn 1 to t are all considered.",
        "Hypotheses assigned true labels are included in the dialog state.",
        "Compared to the DBN approach, the dialog states are built ?jointly?.",
        "This approach is reasonable because what the tracker generates is just some combinations of all N best lists in a session, and there is no point guessing beyond SLU outputs.",
        "We leverage general structured Conditional Random Fields (CRFs) to model the probabilities of the N best items, where factors are used to strengthen local dependency.",
        "Since CRF is a discriminative model, arbitrary overlapping features can be added, which is commonly considered as an advantage over generative models."
      ]
    },
    {
      "heading": "3.2 Conditional Random Fields",
      "text": [
        "CRF is first introduced to address the problem of label bias in sequence prediction (Lafferty et al., 2001).",
        "Linear-chain CRFs are widely used to solve common sequence labeling problem in natural language processing.",
        "General structured CRF has also been reported to be successful in various tasks (Sutton and McCallum, 2012).",
        "In general structured CRF, factor templates are utilized to specify both model structure and pa",
        "the 8 rectangles above denote N best hypotheses for each turn, and the box below represents the dialog state up to the current turn.",
        "Connections between rectangles denote ?Label-Label?",
        "factors.",
        "?Label-Observation?",
        "factors are not shown for simplicity.",
        "rameter tying (Sutton and McCallum, 2012).",
        "Factors are partitioned into a series of templates, and factors inside each template share the same parameters.",
        "where C is the set of factor templates and x,y are inputs and labels respectively.",
        "Template factors are written as",
        "In the experiment we use Factorie1 to define and train the model."
      ]
    },
    {
      "heading": "3.3 Model Structure and Features",
      "text": [
        "In the model, slots in every N best item up to the current turn are represented as binary variables.",
        "For simplification of data structure, each slot in a single N best item is extracted and represented using different label variables, with the same rank indicating their",
        "original places in the N best list.",
        "For example, the item slots: [from: Pittsburgh, data: Tuesday], score: 0.85, rank: 2, is converted to two slots: slots: [from: Pittsburgh], score: 0.85, rank: 2 and slots: [date: Tuesday], score: 0.85, rank: 2.",
        "Label-label connections are specified using factor templates between slot pairs, and Label-observation templates are used to add slot-wise features.",
        "Without label-label connection the model is reduced to a maximum entropy model, and with more connections added, the graph tends to have loopy structures.",
        "Two classes of feature sets (templates) in the experiment are defined as follows.",
        "(1) Label-Label factor templates are used to strengthen the bond between certain slots.",
        "Pairwise-slots of the same rank This template is built for pairs of slots in a turn with the same rank to bind their boolean assignment.",
        "To avoid creating too many loops and make inference efficient, the factors are added in such an order that the slots involved in a single turn are linked in a linear way.",
        "Pairwise-slots with identical value Slots with identical value may appear in the N best list for multiple times.",
        "Besides, user can mention the same slot in different turns, making these slots more reliable.",
        "Similar ordering mechanism is utilized to avoid redundant loops.",
        "(2) Label-observation templates are used to add features for the identification of the truthfulness of slots.",
        "SLU score and rank of slot The score generated by the ASR and SLU components is a direct indicator of the correctness degree of slots.",
        "However, a slot's true reliability is not necessarily linear with its score.",
        "The relationship is quite different for various ASR and SLU algorithms, and scores produced by some ASR are not valid probabilities.",
        "As we adopt a data-driven approach, we are able to learn this relationship from data.",
        "In addition to the SLU score, the slot rank is also added to the feature set.",
        "Dialog history (grounding information) In most spoken dialog systems, explicit and implicit groundings are adapted to indicate the correctness of the system belief.",
        "This information is useful to determine the correctness of slots.",
        "The grounding information includes grounding type (implicit or explicit grounding), user reply (negation or confirmation) and corresponding SLU scores.",
        "Count of slots with identical value As previously mentioned, slots with identical values can appear several times and slots with more frequent occurrences are more likely to be correct.",
        "Domain-specific features Slots for some domain concepts often have values with specific forms.",
        "For example, in the DSTC data sets, the route slots are usually filled with values like ?61d?, ?35b?, and SLU often generates noisy outputs like ?6d?, ?3d?.",
        "Thus the lexical form is a very useful feature.",
        "Baseline Tracker The simple and fast 1-best tracking algorithm is used as the baseline tracker and exhibits a satisfying performance.",
        "Thus the tracking result is added as an additional feature.",
        "This indicates the possibility of combining tracking outputs from different algorithms in this discriminative model, which may improve the overall tracking performance."
      ]
    },
    {
      "heading": "4 Experiment",
      "text": []
    },
    {
      "heading": "4.1 Task and Data",
      "text": [
        "The Dialog State Tracking Challenge (DSTC)2 aims at evaluating dialog state tracking algorithms on shared real-user dialog corpus.",
        "In each dialog session, ASR and SLU results are annotated, making it possible to conduct direct comparison among various algorithms.",
        "For further details, please refer to the DSTC handbook (Williams et al., 2013b)."
      ]
    },
    {
      "heading": "4.2 Corpus Preprocessing",
      "text": [
        "The ASR and SLU components can generate many noisy hypotheses which are completely wrong, rendering the dialog corpus seriously imbalanced at the level of slots (there are more wrong slots than true slots).",
        "We use resampling to prevent",
        "the model from biasing towards making negative judgements.",
        "Before training, the total number of correct slots in a set is counted, and equal number of wrong slots are sampled from the subset of all the wrong slots.",
        "These chosen negative slots plus all the positive slots together constitute the effective slot set for model training, with remaining slots fixed to their true value and regarded as observed variables.",
        "To make full use of the dialog corpus, this process is repeated for eight times, producing a bigger and balanced corpus."
      ]
    },
    {
      "heading": "4.3 Model Training",
      "text": [
        "In the training phase, the log-likelihood function is optimized using the LBFGS method with L2-regularization.",
        "Loopy belief propagation is used as the inference routine.",
        "It can be shown that for factor graphs without loops, the marginal probabilities produced by loopy belief propagation are exact.",
        "Model selection is done according to the log-likelihood on the development set."
      ]
    },
    {
      "heading": "4.4 Predicting and Tracking",
      "text": [
        "For each dialog session, the predicted slot labels are mapped to tracking results.",
        "To produce a N - best list of tracking results, the top N configurations of slots and corresponding probability scores are generated.",
        "Gibbs sampling is adopted.",
        "The sampling is repeated for 1000 times in each corpus, after each sampling the configuration of slots is mapped to certain tracking state.",
        "More efficient inference routines, such as M-best belief propagation (Yanover and Weiss, 2004), could be utilized, which would be suitable for practical real-time application."
      ]
    },
    {
      "heading": "4.5 Results",
      "text": [
        "After tracker outputs are generated based on the sampling results, they are scored using evaluation tools provided by the DSTC organizers.",
        "Several metrics are evaluated, including precisions, ROC performance, etc.",
        "Individual and joint slots are scored respectively.",
        "And different schedules are used, which indicats the turns included for evaluation.",
        "Partial results are shown in table 1.",
        "A systematic analysis by the organizers is in the DSTC overview paper (Williams et al., 2013a).",
        "The complete challenge results can be found on DSTC website.",
        "On the test sets of test1, test2 and test3, the CRF-based model achieves better performance than the simple baseline in most metrics.",
        "However, on test4, the performance degrades seriously when there is a mismatch between training data and test data, since test4 is produced by a different group and does not match the training set.",
        "It shows that the CRF-based model is very flexible and is able to learn the properties of ASR and SLU, thus adapting to a specific system.",
        "But it has a tendency"
      ]
    },
    {
      "heading": "5 Conclusions and Future Directions",
      "text": [
        "We proposed a CRF-based discriminative approach for dialog state tracking.",
        "Preliminary results show that it achieves better performance than the 1-best baseline tracker in most metrics when the training set and testing set match.",
        "This indicates the feasibility of our approach which directly models joint probabilities of the N best items.",
        "In the future, we will focus on the following possible directions to improve the performance.",
        "Firstly, we will enrich the feature set and add more domain-related features.",
        "Secondly, interactions of slots between dialog turns are not well modeled currently.",
        "This problem can be alleviated by tuning graph structures, which deservers further studies.",
        "Moreover, it is challenging to use online training methods, so that the performance could be improved incrementally when more training samples are available."
      ]
    },
    {
      "heading": "6 Acknowledgments",
      "text": [
        "This work is partially supported by the Na"
      ]
    }
  ]
}
