{
  "info": {
    "authors": [
      "Avneesh Saluja",
      "Jiri Navratil"
    ],
    "book": "TextGraphs Workshop on Graph Based Methods for Natural Language Processing",
    "id": "acl-W13-5005",
    "title": "Graph-Based Unsupervised Learning of Word Similarities Using Heterogeneous Feature Types",
    "url": "https://aclweb.org/anthology/W13-5005",
    "year": 2013
  },
  "references": [
    "acl-D12-1003",
    "acl-N10-1017",
    "acl-P10-1040",
    "acl-P11-1061",
    "acl-P13-1109",
    "acl-W11-1107",
    "acl-W12-4104"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this work, we propose a graph-based approach to computing similarities between words in an unsupervised manner, and take advantage of heterogeneous feature types in the process.",
        "The approach is based on the creation of two separate graphs, one for words and one for features of different types (alignment-based, orthographic, etc.).",
        "The graphs are connected through edges that link nodes in the feature graph to nodes in the word graph, the edge weights representing the importance of a particular feature for a particular word.",
        "High quality graphs are learned during training, and the proposed method outperforms experimental baselines."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Data-driven approaches in natural language processing (NLP) have resulted in a marked improvement in a variety of NLP tasks, from machine translation to part-of-speech tagging.",
        "Such methods however, are generally only as good as the quality of the data itself.",
        "This issue becomes highlighted when there is a mismatch in domain between training and test data, in that the number of out-of-vocabulary (OOV) words increases, resulting in problems for language modeling, machine translation, and other tasks.",
        "An approach that specifically replaces OOV words with their synonyms from a restricted vocabulary (i.e., the words already contained in the training data) could alleviate this OOV word problem.",
        "?This work was done during the first author's internship at the IBM T.J. Watson Research Center, Yorktown Heights, NY in 2012.",
        "Vast ontologies that capture semantic similarities between words, also known as WordNets, have been carefully created and compiled by linguists for different languages.",
        "A WordNet-based solution could be implemented to fill the gaps when an OOV word is encountered, but this approach is not scalable in that it requires significant human effort for a number of languages in which the WordNet is limited or does not exist.",
        "Thus, a practical solution to this problem should ideally require as little human supervision and involvement as possible.",
        "Additionally, words can be similar to each other due to a variety of reasons.",
        "For example, the similarity between the words optimize and optimal can be captured via the high orthographical similarity between the words.",
        "However, relying too much on a single feature type may result in false positives, e.g., suggestions of antonyms instead of synonyms.",
        "Valuable information can be gleaned from a variety of feature types, both monolingual and bilingual.",
        "Thus, any potential solution to an unsupervised or mildly supervised word similarity algorithm should be able to take into account heterogeneous feature types and combine them in a globally effective manner when yielding the final solution.",
        "In this work, we present a graph-based approach to impute word similarities in an unsupervised manner and takes into account heterogeneous features.",
        "The key idea is to maintain two graphs, one for words and one for the all the features of different types, and attempt to promote concurrence between the two graphs in an effort to find a final solution.",
        "The similarity graphs learned during training are generally of high quality, and the testing approach proposed outperforms the chosen baselines."
      ]
    },
    {
      "heading": "2 Approach",
      "text": [
        "The eventual goal is to compute the most similar word to a given OOV word from a restricted, pre-existing vocabulary.",
        "We propose a graph-based solution for this problem, relying on undirected graphs to represent words and features as well as the similarities between them.",
        "The solution can be broadly divided into two distinct sub-problems, the training and testing components."
      ]
    },
    {
      "heading": "2.1 Learning the Graph",
      "text": [
        "The intuition of our approach is best expressed through a small example problem.",
        "Figure 1 shows an example graph of words (shaded) and features (unshaded).",
        "For exposition, let v1 = optimize, v2 = optimal, and v3 = ideal, while f1 = orth |opti, i.e., an orthographic feature corresponding to the sub-string ?opti?",
        "at the beginning of a word, and f5 = align ide?al, i.e., a bilingual feature corresponding to the alignment of the word ?optimal?",
        "to the French word ?ide?al?",
        "in the training data1.",
        "the feature graph.",
        "There are three types of edges in this scenario.",
        "Edges between word nodes (e.g., Wv1,v2) represent word similarities, and edges between features (e.g., Wf1,f5) represent feature similarities.",
        "Edges between words and features (e.g., Zv1,f1 , the dashed lines) represent pertinent or active features for a given word when computing its similarity with other words, with the edge weight reflecting the degree of importance.",
        "We restrict the values of all similarities to be between 0 and 1, as negative-valued edges in undi1such word alignments can be extracted through standard word alignment algorithms applied to a parallel corpus in two different languages.",
        "rected graphs are significantly more complicated and would make subsequent computations more intricate.",
        "In an ideal situation, the similarity matrices that represent the word and feature graphs should be positive semi-definite, which provides a nice probabilistic interpretation due to connections to covariance matrices of multivariate distributions, but this constraint is not enforced here.",
        "Future work will focus on improved optimization techniques that respect the positive semi-definiteness constraint.",
        "To learn the graph, the following objective function is minimized:",
        "where Wfp,fq is the current similarity between feature fp and feature fq (with corresponding initial value W ?fp,fq ), Wvi,vj is the current similarity between word viand word vj , Zvi,fp is the current importance weight of feature fp for word vi (with corresponding initial value Z?vi,fp), and ?0 to ?3 are parameters (that sum to 1) which represent the importance of a given term in the objective function.",
        "The intuition of the objective function is straightforward.",
        "The first two terms correspond to minimizing the `2-norm between the initial and current values of Wfp,fq and Zvi,fp (for further details on initialization, see Section 2.1.2).",
        "The intuition behind the third term is to minimize the difference between the word similarity of words viand vj and the feature similarity of features fp and fq in proportion to how important those features are for words viand vj respectively.",
        "If two features have high importance weights for two words, and those features are very similar to each other, then the corresponding words should also be similar.",
        "The fourth term has a similar rationale, in that it minimizes the difference between importance weights in proportion to the similarities.",
        "In other words, we attempt to promote parameter concurrence between the word and feature",
        "graphs, which in turn ensures smoothness over the two graphs.",
        "The basic idea of minimizing two quantities of the graph in proportion to their link strength has been used before, for example (but not limited to) graph-based semi-supervised learning and label propagation (Zhu et al., 2003) where the concept is applied to node labels (as opposed to edge weights as presented in this work).",
        "In such methods, the idea is to ensure that the function varies smoothly over the graph (Zhou et al., 2004), i.e., to promote parameter concurrence within a graph, whereas we promote parameter concurrence across two graphs.",
        "In that sense, the ?",
        "parameters as control the trade-off between respecting initial values vs. achieving consistency between the two graphs.",
        "While not necessary, we decided to tie the parameters together, such that ?0 and ?2 (representing feature similarity preference for initial values vs. preference for consistency) sum to 0.5, and ?1 and ?3 sum to 0.5 as well, implicitly giving equal weight to feature similarities and importance weights.",
        "In the future, a more appropriate method of learning these ?",
        "parameters will be explored.",
        "In many unsupervised algorithms, e.g., EM, the initialization of parameters is of paramount importance, as these initial values guide the algorithm in its attempt to minimize a proposed objective function.",
        "In our problem, initial estimates for word similarities do not exist (otherwise the problem would be considerably easier!).",
        "Instead, word similarities are seeded from the initial feature similarities and initial importance weights, and all three quantities are then iteratively refined.",
        "The initial importance weight values are computed from the co-occurrence statistics between words and features, by taking the geometric mean of the conditional probabilities (feature given word and word given feature) in both directions: Z?vi,fp =?",
        "P(vi|fp)P(fp|vi).",
        "For the initial feature similarity values, the pointwise mutual information (PMI) vector for each feature is first computed, by taking the log ratio of the joint probability with each word to the marginal probabilities of the feature and the word (also done through the co-occurrence statistics).",
        "Subsequently, the initial similarity is then computed as the normalized dot product between feature vectors:",
        "After computing the initial feature similarity and weights matrices, we remove features that are densely connected in the feature similarity graph by trimming high entropy features (normalizing edge weights and treating the resulting values as a probability distribution).",
        "This pruning was done in order to speed up the optimization procedure, and we found that results were not affected by pruning away the top one percentile of features sorted by entropy.",
        "The objective function (Equations 1 to 4) is convex and differentiable with respect to the individual variables Wvi,vj ,Wfp,fq , and Zvi,fp .",
        "Hence, one way to minimize it is to evaluate the derivatives of the objective function with respect to these variables, set to 0 and solve.",
        "The final update equations are provided in the Appendix.",
        "The entire training pipeline is captured in Figure 2.",
        "We first compute the word similarities from the initial feature similarities and importance weights, and then update those values in turn, based on the alternating minimization method (Csisza?r and Tusna?dy, 1984).",
        "The process is repeated till convergence."
      ]
    },
    {
      "heading": "2.2 Link Prediction",
      "text": [
        "Given a learned word similarity graph (along with a learned feature similarity graph and the edges between the two graphs) and an OOV word with associated features, the proposed solution should also generate a list of synonyms.",
        "In a graph-based setting, this is analogous to the link prediction problem: given a graph and a new node that needs to be embedded in the graph, which links, or edges, do we add between the new node and all the existing ones?",
        "We experimented with two different approaches for link prediction.",
        "The first computes word similarities in the same manner as in training, as per Equation 5.",
        "However, since the learned importance weights Zvi,fp (or Zvj ,fq ) are specific to a given word, importance weights for the OOV word are initialized in the same manner as in Section 2.1.2 for the words in the training data.",
        "Thus, for a given OOV word, we obtain word similarities with all words in the vocabulary through Equation 5, and output the most similar words by this metric.",
        "The second method is based on a random walk approach, similar to (Kok and Brockett, 2010), wherein a probabilistic interpretation is imposed on the graphs by row-normalizing all of the matrices involved (word similarity, feature similarity, and importance weights), implying that the transition probability, say from node vi to vj , is proportional to the similarity between the two nodes.",
        "For this approach, only the active features for a given OOV word, i.e., the features that have at least one non-zero Z edge between the feature and a word, are used (see Section 2.3 for more details on active and inactive features).",
        "First, M random walks are initialized from each active feature node, each walk of maximum length T .",
        "For every walk, the number of steps needed to hit a word node in the word similarity graph for the first time is recorded.",
        "After averaging across the M runs, we need to average the hitting times across all of the active features, which is done by weighting the hitting times of each active feature f?",
        "by ?",
        "vi Zvi,f?",
        ", i.e., the sum across all rows of a given feature (represented by a column) in the importance weights matrix.",
        "The random walk-based approach introduces three new parameters: M , the number of random walks per active feature, T , the maximum length of each random walk, and ?, a parameter that controls how often a random walk should take a Z edge (thereby transitioning from one graph to the other) or a W edge (thereby staying within the same graph).",
        "If a node has both Z and W edges, then ?",
        "is the parameter for a simple Bernoulli distribution that samples whether to take one type of edge or the other; if the node has only one type of edge, then the walk traverses only that type."
      ]
    },
    {
      "heading": "2.3 Sparsification",
      "text": [
        "There is a crucial point regarding Equations 1 to 4, namely that restricting the inputted values to between 0 and 1 does not guarantee that the resulting similarity or weight value will also be between 0 and 1, due to the difference in terms in the numerator of the equations.",
        "In order to bypass this problem, a projection step is employed subsequent to an update, wherein the value obtained is projected into the correct part of the n-dimensional Euclidean space, namely the positive orthant.",
        "Although slightly more involved in the multidimensional case, i.e., where n > 1, since the partial derivatives as computed in Equations 5 to 7 are with respect to a single element, orthant projection in the unidimensional case amounts to nothing more than setting the value to 0 if it is less than 0.",
        "This effectively sparsifies the resulting matrix, and is similar to the soft-thresholding effect that comes about due to `1-norm regularization.",
        "Further exploration of this link is left to future work.",
        "However, the sparsification of the graphs/matrices is problematic for the random walk-based approach, in that an OOV word may consist of features that are all inactive, i.e., none of the features have a non-zero Z edge to the word similarity graph.",
        "In this case, we cannot compute which words in our vocabulary are similar to the OOV word.",
        "One method to alleviate this drawback is to add back Z edges that were removed during training with their initial weights.",
        "Yet, we found that adding back all of the features for a test word was worse than filtering out the features with the highest entropy (i.e., with the most edges to other features) out of the features to add back.",
        "The latter approach was thus adopted and is the setup used in Section 3.5."
      ]
    },
    {
      "heading": "3 Experiments & Results",
      "text": [
        "In our experiments, we looked at both the quality of the similarity graphs learned from the data, as well as the performance of the link prediction techniques."
      ]
    },
    {
      "heading": "3.1 Dataset",
      "text": [
        "Table 1 summarizes the statistics of the training and test sets used.",
        "We used the standard WMT 2010 evaluation dataset, and the training data consists of a combination of European Parliament and news commentary bitext, while the test set is from the news domain.",
        "Note that a parallel corpus is not needed as only the English side is used.",
        "While the current experiment is restricted to English, any language can be used in principle."
      ]
    },
    {
      "heading": "3.2 Features",
      "text": [
        "During the feature extraction phase, we first filtered the 30 most common words from the corpus and do not extract features for those words.",
        "However, these common words are still used when extracting distributional features.",
        "The following features are used: ?",
        "Orthographic: all substrings of length 3, 4, and 5 for a given word are extracted.",
        "For example, the feature ?orth |opt?, corresponding to the substring ?opt?",
        "at the beginning of a word, would be extracted from the word ?optimal?.",
        "?",
        "Distributional (a.k.a., contextual): for a given word, we extract the word immediately preceding and succeeding it as well as words within a window of 5.",
        "These features are extracted from a corpus without the 30 most common words filtered.",
        "An example of such a feature is ?LR the+cost?, representing an instance of a preceding and succeeding word for ?optimal?, extracted from the phrase ?the optimal cost?.",
        "Lastly, all distributional features that occur less than 5 times are removed.",
        "?",
        "Part-of-Speech (POS): for example, ?pos JJ?",
        "is a POS feature extracted for the word ?optimal?.",
        "?",
        "Alignment (a.k.a., bilingual): alignment features are extracted from alignment matrices across languages.",
        "For every word, we filter all words in the target language (treating English, our working language, as the source) that have a lexical probability less than half the maximum lexical probability, and use the resulting aligned words as features.",
        "For example, ?align ide?al?",
        "would be a feature for the word ?optimal?, since the French word ?ide?al?",
        "is aligned (with high probability) to the word ?optimal?.",
        "Note that the assumption during test time is that alignment features are not available for OOV words; if they were, then the word would not be OOV.",
        "Nonetheless, alignment information can be utilized indirectly in the link prediction stage from random walk traversals of in-vocabulary nodes.",
        "Statistics on the number of features broken down by type are presented in Table 2, for 3 different vocabulary sizes.",
        "In the experiments, we concentrated on the 10,000 and 50,000 size vocabularies."
      ]
    },
    {
      "heading": "3.3 Baselines",
      "text": [
        "When selecting the baselines, we had two goals in mind.",
        "Firstly, we wanted to compare the proposed approach against simpler alternatives for generating word similarities.",
        "The baselines were also chosen so as to correspond in some way to the various feature types, since a main advantage of our approach is that it effectively combines various feature types to yield global word similarity scores.",
        "This choice of baselines also provides insight into the impact of the various feature types chosen; the idea is that a baseline corresponding to a particular feature type would be indicative of word similarity performance using just that type.",
        "Three baselines were initially selected: ?",
        "Distributional: a PMI vector is computed for each word over the various distributional features.",
        "The inner product of two PMI vectors is computed to evaluate the similarity of two words.",
        "We found that this baseline performed poorly relative to the other ones, and thus decided not to include it in the final evaluation.",
        "?",
        "Orthographic: based on a simple edit distance-based approach, where all words within an edit distance of 25% of the length of the test word are retrieved.",
        "?",
        "Alignment: we compose the alignment matrices in both directions to generate an English to English matrix (using German as the pivot language), from which the three most similar",
        "distributional features are only those with count 5 and above.",
        "words (as per the lexical probabilities in the matrices) are extracted."
      ]
    },
    {
      "heading": "3.4 Evaluation",
      "text": [
        "Automatic evaluation of an algorithm that computes similarities between words is tricky.",
        "The judgment on whether two words are synonyms is still done best by a human, requiring significant manual effort.",
        "Therefore, during the experimentation and parameter selection process we developed an intermediate form of evaluation wherein a human annotator assisted in creating a pseudo ?ground truth?.",
        "Prior to creating the ground truth, all OOV words in the test set were identified (i.e., no match in our vocabulary), resulting in 978 OOV words.",
        "Named entities were then manually filtered, resulting in a final test set of 312 words for evaluation purposes.",
        "To create the ground truth, we generated for each test OOV word a set of possible synonyms using the alignment and orthographic baselines, as per Section 3.3.",
        "Naturally, many of the words generated were not legitimate synonyms; human evaluators thus removed all words that were not synonyms or near synonyms, ignoring mild grammatical inconsistencies, like singular vs. plural.",
        "Generally, a synonym was considered valid if substituting the word with the synonym preserved meaning in a sentence.",
        "The final evaluation was performed by a human evaluator.",
        "The two baselines and the proposed approach generated the top three synonym candidates for a given OOV test word and both 1-best and 3 best results were evaluated (as in Table 3).",
        "Final performance was evaluated using precision and recall.",
        "Recall is defined as the percentage of words for which at least one synonym was generated, and precision evaluates the number of correct synonyms from the ones generated."
      ]
    },
    {
      "heading": "3.5 Results",
      "text": [
        "Figure 3 looks at the neighborhood of words around the word ?guardian?.",
        "Note that while only two different ?",
        "parameter configurations are compared in Test Word Synonym 1 Synonym 2 Synonym 3 pubescent puberty adolescence nanotubes sportswoman sportswomen athlete draftswoman briny salty saline salinity",
        "onyms are real output from our algorithm.",
        "the figure, we investigated a variety of settings and found that ?0 = 0.3, ?1 = 0.4, ?2 = 0.2, ?3 = 0.1 worked best from a final evaluation perspective.",
        "The first point to note is that the graph in Figure 3b is generally more dense than that of Figure",
        "erence for smoothness over the graph and thus the idea is that ?custodian?",
        "and ?custodians?",
        "are linked via the smooth transition ?custodian??",
        "?guardian?",
        "?",
        "?guardians??",
        "?custodians?, whereas in the former, there is a higher preference to respect the initial values, which generates this additional edge.",
        "We also observed weak edges between words like ?custodian?",
        "and ?tutor?",
        "in Figure 3b but not in Figure 3a.",
        "The effect of the parameters on the sparsity of the graph is definitely apparent, but generally the learned graphs are of high quality.",
        "A further analysis reveals that for many of the words in the corpus, the highest weighted features are usually alignment features; their heavy use allows the algorithm to produce interesting synonym candidates, and emphasizes the importance of bilingual features.",
        "To underscore the point regarding impact of parameters on graph sparsity, Figures 4 and 5 present the number of elements in the resulting word similarity and weights matrices (graphs) vs. iteration for vocabulary sizes of 10,000 and 50,000 respec",
        "tively, with Table 4 providing a legend to the curves in those figures.",
        "Higher ?",
        "weights for terms 1 and 2 in the objective function result in less sparse solutions.",
        "The density of the matrices also drops drastically after a few iterations and stabilizes thereafter.",
        "Lastly, Tables 5 and 6 present the final results of the evaluation, as assessed by a human evaluator, on the 312 OOV words in the test set.",
        "While the results on the 1-best front are marginally better than the edit distance-based baseline, 3-best the performance of our approach is comfortably better than the baselines.",
        "Testing was done with the word similarity update method.",
        "The performance of the random walk-based link",
        "trained on a 50,000-word vocabulary.",
        "Our best approach (?50k-nhnl?)",
        "is bolded prediction approach was suboptimal for several reasons.",
        "Firstly, it was difficult to use the learned importance weights as is, since the resulting weights matrix was so sparse that many test words simply did not have active features.",
        "This issue resulted in the vanilla variant of the random walk approach to have very low recall.",
        "Therefore, we adopted a ?mixed weights?",
        "strategy, where we selectively introduced a number of features previously inactive for a test word, not including the features that had high entropy.",
        "Yet in this case, the random walks get stuck traversing certain edges, and a good sampling of similar words was not properly achievable.",
        "A general issue that arose during link prediction is that the orthographic features tend to dominate the candidate synonyms list since alignment features are not utilized.",
        "If instead we assume that alignment features are accessible during testing, then the random walk-based approaches do marginally better than the word similarity update method, but further investigation is warranted before drawing any definitive conclusions."
      ]
    },
    {
      "heading": "4 Related Work",
      "text": [
        "We used the objective function and basic formulation of (Muthukrishnan et al., 2011), but corrected their derivation of the optimization and introduced methods to handle the resulting complications.",
        "In addition, (Muthukrishnan et al., 2011) implemented their approach on just one feature type and with far fewer nodes, since their word similarity graph was actually over documents and their feature similarity graph was over words.",
        "Recently, an alternative graph-based approach for the same problem was presented in (Minkov and Cohen, 2012).",
        "However, in addition to requiring a dependency parse of the corpus, the emphasis of that work is more on the testing side.",
        "Indeed, we can incorporate some of the ideas presented in that work to improve our link prediction during query time.",
        "The label propagation-based approaches of (Tamura et al., 2012; Razmara et al., 2013), wherein ?seed distributions?",
        "are extracted from bilingual corpora and are propagated around a similarity graph, can also be easily integrated into our approach as a downstream method specific to machine translation.",
        "Another approach to handle OOVs, particularly in the translation domain, is (Zhang et al., 2005), wherein the authors leveraged the web as an expanded corpus for OOV mining.",
        "If web access is unavailable however, then this method would not work.",
        "The general problem of combining multiple views of similarity (i.e., across different feature types) can also be tackled through multiple kernel learning (MKL) (Bach et al., 2004).",
        "However, most of the work in this field has been on supervised MKL, whereas we required an unsupervised approach.",
        "An area that has seen a recent resurgence in popularity is deep learning, especially in its applications to continuous embeddings.",
        "Embeddings of word distributions have been explored in (Mnih and Hin-ton, 2007; Turian et al., 2010; Weston et al., 2008).",
        "Lastly, while not directly relevant to our work, the idea of using a graph-based framework to combine both monolingual and bilingual features was also presented in (Das and Petrov, 2011)."
      ]
    },
    {
      "heading": "5 Conclusion & Future Work",
      "text": [
        "In this work, we presented a graph-based approach to computing word similarities, based on dual word and feature similarity graphs, and the edges that go between the graphs, representing importance weights.",
        "We introduced an objective function that promotes parameter concurrence between the two graphs, and minimized this function with a simple alternating minimization-based approach.",
        "The resulting optimization recovers high quality word similarity graphs, primarily due to the bilingual features, and improves over the baselines during the link prediction stage.",
        "In the future, on the training side we would like to optimize the proposed objective function in a better manner, while enforcing the positive semi",
        "definiteness constraints.",
        "Other link prediction techniques should be explored, as the current techniques have pitfalls.",
        "Richer features that model more refined aspects can be introduced.",
        "In particular, features from a dependency parse of the data would be very useful in this situation."
      ]
    },
    {
      "heading": "References",
      "text": [
        "Francis R. Bach, Gert R. G. Lanckriet, and Michael I. Jordan.",
        "2004.",
        "Multiple kernel learning, conic duality, and the smo algorithm.",
        "In Proceedings of the twenty-first international conference on Machine learning,"
      ]
    },
    {
      "heading": "ICML ?04.",
      "text": []
    }
  ]
}
