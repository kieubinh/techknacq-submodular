{
  "info": {
    "authors": [
      "David Burkett",
      "Dan Klein"
    ],
    "book": "NAACL",
    "id": "acl-N12-1004",
    "title": "Fast Inference in Phrase Extraction Models with Belief Propagation",
    "url": "https://aclweb.org/anthology/N12-1004",
    "year": 2012
  },
  "references": [
    "acl-D07-1079",
    "acl-D08-1016",
    "acl-D08-1033",
    "acl-E09-1020",
    "acl-H05-1010",
    "acl-J07-2003",
    "acl-J97-3002",
    "acl-N03-1017",
    "acl-N04-1035",
    "acl-N06-1014",
    "acl-P05-1059",
    "acl-P06-1002",
    "acl-P06-1121",
    "acl-P07-2045",
    "acl-P08-1012",
    "acl-P09-1088",
    "acl-P09-1104",
    "acl-P10-1147",
    "acl-W02-1018",
    "acl-W06-3105",
    "acl-W07-0403",
    "acl-W08-0303",
    "acl-W08-0402"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Modeling overlapping phrases in an alignment model can improve alignment quality but comes with a high inference cost.",
        "For example, the model of DeNero and Klein (2010) uses an ITG constraint and beam-based Viterbi decoding for tractability, but is still slow.",
        "We first show that their model can be approximated using structured belief propagation, with a gain in alignment quality stemming from the use of marginals in decoding.",
        "We then consider a more flexible, non-ITG matching constraint which is less efficient for exact inference but more efficient for BP.",
        "With this new constraint, we achieve a relative error reduction of 40% in F5 and a 5.5x speed-up."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Modern statistical machine translation (MT) systems most commonly infer their transfer rules from word-level alignments (Koehn et al., 2007; Li and Khudanpur, 2008; Galley et al., 2004), typically using a deterministic heuristic to convert these to phrase alignments (Koehn et al., 2003).",
        "There have been many attempts over the last decade to develop model-based approaches to the phrase alignment problem (Marcu and Wong, 2002; Birch et al., 2006; DeNero et al., 2008; Blunsom et al., 2009).",
        "However, most of these have met with limited success compared to the simpler heuristic method.",
        "One key problem with typical models of phrase alignment is that they choose a single (latent) segmentation, giving rise to undesirable modeling biases (DeNero et al., 2006) and reducing coverage, which in turn reduces translation quality (DeNeefe et al., 2007; DeNero et al., 2008).",
        "On the other hand, the extraction heuristic identifies many overlapping options, and achieves high coverage.",
        "In response to these effects, the recent phrase alignment work of DeNero and Klein (2010) models extraction sets: collections of overlapping phrase pairs that are consistent with an underlying word alignment.",
        "Their extraction set model is empirically very accurate.",
        "However, the ability to model overlapping ?",
        "and therefore non-local ?",
        "features comes at a high computational cost.",
        "DeNero and Klein (2010) handle this in part by imposing a structural ITG constraint (Wu, 1997) on the underlying word alignments.",
        "This permits a polynomial-time algorithm, but it is still O(n6), with a large constant factor once the state space is appropriately enriched to capture overlap.",
        "Therefore, they use a heavily beamed Viterbi search procedure to find a reasonable alignment within an acceptable time frame.",
        "In this paper, we show how to use belief propagation (BP) to improve on the model's ITG-based structural formulation, resulting in a new model that is simultaneously faster and more accurate.",
        "First, given the model of DeNero and Klein (2010), we decompose it into factors that admit an efficient BP approximation.",
        "BP is an inference technique that can be used to efficiently approximate posterior marginals on variables in a graphical model; here the marginals of interest are the phrase pair posteriors.",
        "BP has only recently come into use in the NLP community, but it has been shown to be effective in other complex structured classification tasks, such as dependency parsing (Smith and Eis-ner, 2008).",
        "There has also been some prior success in using BP for both discriminative (Niehues and Vogel, 2008) and generative (Cromie`res and Kuro-hashi, 2009) word alignment models.",
        "By aligning all phrase pairs whose posterior under BP exceeds some fixed threshold, our BP approximation of the model of DeNero and Klein (2010) can",
        "achieve a comparable phrase pair F1.",
        "Furthermore, because we have posterior marginals rather than a single Viterbi derivation, we can explicitly force the aligner to choose denser extraction sets simply by lowering the marginal threshold.",
        "Therefore, we also show substantial improvements over DeNero and Klein (2010) in recall-heavy objectives, such as F5.",
        "More importantly, we also show how the BP factorization allows us to relax the ITG constraint, replacing it with a new set of constraints that permit a wider family of alignments.",
        "Compared to ITG, the resulting model is less efficient for exact inference (where it is exponential), but more efficient for our BP approximation (where it is only quadratic).",
        "Our new model performs even better than the ITG-constrained model on phrase alignment metrics while being faster by a factor of 5.5x."
      ]
    },
    {
      "heading": "2 Extraction Set Models",
      "text": [
        "Figure 1 shows part of an aligned sentence pair, including the word-to-word alignments, and the extracted phrase pairs licensed by those alignments.",
        "Formally, given a sentence pair (e, f), a word-level alignment a is a collection of links between target words ei and source words fj .",
        "Following past work, we further divide word links into two categories: sure and possible, shown in Figure 1 as solid and hatched grey squares, respectively.",
        "We represent a as a grid of ternary word link variables aij , each of which can take the value sure to represent a sure link between ei and fj , poss to represent a possible link, or off to represent no link.",
        "An extraction set pi is a set of aligned phrase pairs to be extracted from (e, f), shown in Figure 1 as green rounded rectangles.",
        "We represent pi as a set of boolean variables pighk`, which each have the value true when the target span [g, h] is phrase-aligned to the source span [k, `].",
        "Following previous work on phrase extraction, we limit the size of pi by imposing a phrase length limit d: pi only contains a variable pighk` if h?",
        "g < d and `?",
        "k < d. There is a deterministic mapping pi(a) from a word alignment to the extraction set licensed by that word alignment.",
        "We will briefly describe it here, and then present our factorized model.",
        "a48 = sure), and hatched squares possible links (e.g. a67 = poss).",
        "Rounded green rectangles are extracted phrase pairs (e.g. pi5667 = true).",
        "Target spans are shown as blue vertical lines and source spans as red horizontal lines.",
        "Because there is a sure link at a48, ?f8 = [4, 4] does not include the possible link at a38.",
        "However, f7 only has possible links, so ?f7 = [5, 6] is the span containing those.",
        "f9 is null-aligned, so ?f9 = [?1,?",
        "], which blocks all phrase pairs containing f9 from being extracted."
      ]
    },
    {
      "heading": "2.1 Extraction Sets from Word Alignments",
      "text": [
        "The mapping from a word alignment to the set of licensed phrase pairs pi(a) is based on the standard rule extraction procedures used in most modern statistical systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007), but extended to handle possible links (DeNero and Klein, 2010).",
        "We start by using a to find a projection from each target word ei onto a source span, represented as blue vertical lines in Figure 1.",
        "Similarly, source words project onto target spans (red horizontal lines in Figure 1).",
        "pi(a) contains a phrase pair iff every word in the target span projects within the source span and vice versa.",
        "Figure 1 contains an example for d = 2.",
        "Formally, the mapping introduces a set of spans ?.",
        "We represent the spans as variables whose values are intervals, where ?ei = [k, `] means that the target word ei projects to the source span [k, `].",
        "The set of legal values for ?ei includes any interval with 0 ?",
        "k ?",
        "` < |f |and ` ?",
        "k < d, plus the special interval [?1,?]",
        "that indicates ei is null-aligned.",
        "The span variables for source words ?fj have target spans [g, h] as values and are defined analogously.",
        "For a set I of positions, we define the range func",
        "tion:",
        "For a fixed word alignment a we set the target span variable ?ei :",
        "As illustrated in Figure 1, this sets ?ei to the minimal span containing all the source words with a sure link to ei if there are any.",
        "Otherwise, because of the special case for range(I) when I is empty, ?ei,s = [?1,?",
        "], so ?ei is the minimal span containing all poss-aligned words.",
        "If all word links to ei are off, indicating that ei is null-aligned, then ?ei is [?1,?",
        "], preventing the alignment of any phrase pairs containing ei.",
        "Finally, we specify which phrase pairs should be included in the extraction set pi.",
        "Given the spans ?",
        "based on a, pi(a) sets pighk` = true iff every word in each phrasal span projects within the other: ?ei ?",
        "[k, `] ?i ?",
        "[g, h] (5) ?fj ?",
        "[g, h] ?j ?",
        "[k, `]"
      ]
    },
    {
      "heading": "2.2 Formulation as a Graphical Model",
      "text": [
        "We score triples (a, pi, ?)",
        "as the dot product of a weight vector w that parameterizes our model and a feature vector ?",
        "(a, pi, ?).",
        "The feature vector decomposes into word alignment features ?a, phrase pair features ?pi and target and source null word features ?e?",
        "and ?",
        "This feature function is exactly the same as that",
        "ero and Klein (2010), all of their reported results include these features (DeNero, 2010).",
        "they formulated their inference problem as a search for the highest scoring triple (a, pi, ?)",
        "for an observed sentence pair (e, f), we wish to derive a conditional probability distribution p(a, pi, ?|e, f).",
        "We do this with the standard transformation for linear models: p(a, pi, ?|e, f) ?",
        "exp(w??",
        "(a, pi, ?)).",
        "Due to the factorization in Eq.",
        "(6), this exponentiated form becomes a product of local multiplicative factors, and hence our model forms an undirected graphical model, or Markov random field.",
        "In addition to the scoring function, our model also includes constraints on which triples (a, pi, ?)",
        "have nonzero probability.",
        "DeNero and Klein (2010) implicitly included these constraints in their representation: instead of sets of variables, they used a structured representation that only encodes triples (a, pi, ?)",
        "satisfying both the mapping pi = pi(a) and the structural constraint that a can be generated by a block ITG grammar.",
        "However, our inference procedure, BP, requires that we represent (a, pi, ?)",
        "as an assignment of values to a set of variables.",
        "Therefore, we must explicitly encode all constraints into the multiplicative factors that define the model.",
        "To accomplish this, in addition to the soft scoring factors we have already mentioned, our model also includes a set of hard constraint factors.",
        "Hard constraint factors enforce the relationships between the variables of the model by taking a value of 0 when the constraints they encode are violated and a value of 1 when they are satisfied.",
        "The full factor graph representation of our model, including both soft scoring factors and hard constraint factors, is drawn schematically in Figure 2.",
        "The scoring factors all take the form exp(w ?",
        "?",
        "), and so can be described in terms of their respective local feature vectors, ?.",
        "Depending on the values of the variables each factor depends on, the factor can be active or inactive.",
        "Features are only extracted for active factors; otherwise ?",
        "is empty and the factor produces a value of 1.",
        "SURELINK.",
        "Each word alignment variable aij has a corresponding SURELINK factor Lij to incorporate scores from the features ?a(aij).",
        "Lij is active whenever aij = sure.",
        "?a(aij) includes posteriors from unsupervised jointly trained HMM word alignment models (Liang et al., 2006), dictionary",
        "separated into two components: one containing the factors that only neighbor word link variables, and one containing the remaining factors.",
        "and identical word features, a position distortion feature, and features for numbers and punctuation.",
        "PHRASEPAIR.",
        "For each phrase pair variable pighk`, scores from ?pi(pighk`) come from the factor Rghk`, which is active if pighk` = true.",
        "Most of the model's features are on these factors, and include relative frequency statistics, lexical template indicator features, and indicators for numbers of words and Chinese characters.",
        "See DeNero and Klein (2010) for a more comprehensive list.",
        "NULLWORD.",
        "We can determine if a word is null-aligned by looking at its corresponding span variable.",
        "Thus, we include features from ?e?",
        "(?ei ) in a factor N ei that is active if ?ei = [?1,?].",
        "The features are mostly indicators for common words.",
        "There are also factors Nfj for source words, which are defined analogously.",
        "We encode the hard constraints on relationships between variables in our model using three families of factors, shown graphically in Figure 2.",
        "The SPAN and EXTRACT factors together ensure that pi = pi(a).",
        "The ITG factor encodes the structural constraint on a. SPAN.",
        "First, for each target word ei we include a factor Sei to ensure that the span variable ?ei has a value that agrees with the projection of the word alignment a.",
        "As shown in Figure 2b, Sei depends on ?ei and all the word alignment variables aij in column i of the word alignment grid.",
        "Sei has value 1 iff the equality in Eq.",
        "(4) holds.",
        "Our model also includes a factor Sfj to enforce the analogous relationship between each ?fj and corresponding row j of a.",
        "EXTRACT.",
        "For each phrase pair variable pighk` we have a factor Pghk` to ensure that pighk` = true iff it is licensed by the span projections ?.",
        "As shown in Figure 2b, in addition to pighk`, Pghk` depends on the range of span variables ?ei for i ?",
        "[g, h] and ?fj for j ?",
        "[k, `].",
        "Pghk` is satisfied when pighk` = true and the relations in Eq.",
        "(5) all hold, or when pighk` = false and at least one of those relations does not hold.",
        "ITG.",
        "Finally, to enforce the structural constraint on a, we include a single global factor A that depends on all the word link variables in a (see Figure 2a).",
        "A is satisfied iff a is in the family of block inverse transduction grammar (ITG) alignments.",
        "The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is",
        "in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al., 2008)."
      ]
    },
    {
      "heading": "3 Relaxing the ITG Constraint",
      "text": [
        "The ITG factor can be viewed as imposing two different types of constraints on allowable word alignments a.",
        "First, it requires that each word is aligned to at most one relatively short subspan of the other sentence.",
        "This is a linguistically plausible constraint, as it is rarely the case that a single word will translate to an extremely long phrase, or to multiple widely separated phrases.",
        "The other constraint imposed by the ITG factor is the ITG reordering constraint.",
        "This constraint is imposed primarily for reasons of computational tractability: the standard dynamic program for bi-text parsing depends on ITG reordering (Wu, 1997).",
        "While this constraint is not dramatically restrictive (Haghighi et al., 2009), it is plausible that removing it would permit the model to produce better alignments.",
        "We tested this hypothesis by developing a new model that enforces only the constraint that each word align to one limited-length subspan, which can be viewed as a generalization of the at-most-one-to-one constraint frequently considered in the word-alignment literature (Taskar et al., 2005; Cromie`res and Kurohashi, 2009).",
        "Our new model has almost exactly the same form as the previous one.",
        "The only difference is that A is replaced with a new family of simpler factors: ONESPAN.",
        "For each target word ei (and each source word fj) we include a hard constraint factor",
        "(length limit) and either ?ei,p = [?1,?]",
        "or ?j ?",
        "?ei,p, aij 6= off (no gaps), with ?ei,p as in Eq.",
        "(3).",
        "Figure 3 shows the portion of the factor graph from Figure 2a redrawn with the ONESPAN factors replacing the ITG factor.",
        "As Figure 3 shows, there is no longer a global factor; each U ei depends only on the word link variables from column i.",
        "3Short gaps can be accomodated within block ITG (and in our model are represented as possible links) as long as the total aligned span does not exceed the block size."
      ]
    },
    {
      "heading": "4 Belief Propagation",
      "text": [
        "Belief propagation is a generalization of the well known sum-product algorithm for undirected graphical models.",
        "We will provide only a procedural sketch here, but a good introduction to BP for inference in structured NLP models can be found in Smith and Eisner (2008), and Chapters 16 and 23 of MacKay (2003) contain a general introduction to BP in the more general context of message-passing algorithms.",
        "At a high level, each variable maintains a local distribution over its possible values.",
        "These local distribution are updated via messages passed between variables and factors.",
        "For a variable V , N (V ) denotes the set of factors neighboring V in the factor graph.",
        "Similarly, N (F ) is the set of variables neighboring the factor F .",
        "During each round of BP, messages are sent from each variable to each of its neighboring factors:",
        "and from each factor to each of its neighboring variables:",
        "where XF is a partial assignment of values to just the variables in N (F ).",
        "Marginal beliefs at time k can be computed by simply multiplying together all received messages and normalizing:",
        "Although messages can be updated according to any schedule, generally one iteration of BP updates each message once.",
        "The process iterates until some stopping criterion has been met: either a fixed number of iterations or some convergence metric.",
        "For our models, we say that BP has converged",
        "In general, the efficiency of BP depends directly on the arity of the factors in the model.",
        "Performed na?",
        "?vely, the sum in Eq.",
        "(8) will take time that grows exponentially with the size of N (F ).",
        "For the soft-scoring factors, which each depend only on a single variable, this isn't a problem.",
        "However, our model also includes factors whose arity grows with the input size: for example, explicitly enumerating all assignments to the word link variables that the ITG factor depends on would take O(3n2) time.5 To run BP in a reasonable time frame, we need efficient factor-specific propagators that can exploit the structure of the factor functions to compute outgoing messages in polynomial time (Duchi et al., 2007; Smith and Eisner, 2008).",
        "Fortunately, all of our hard constraints permit dynamic programs that accomplish this propagation.",
        "Space does not permit a full description of these dynamic programs, but we will briefly sketch the intuitions behind them.",
        "SPAN and ONESPAN.",
        "Marginal beliefs for Sei or U ei can be computed inO(nd2) time.",
        "The key observation is that for any legal value ?ei = [k, `], Sei and U ei require that aij = off for all j /?",
        "[k, `].6 Thus, we start by computing the product of all the off beliefs:",
        "j qaij (off).",
        "Then, for each of the O(nd) legal source spans [k, `] we can efficiently find a joint belief by summing over consistent assignments to the O(d) link variables in that span.",
        "EXTRACT.",
        "Marginal beliefs for Pghk` can be computed inO(d3) time.",
        "For each of theO(d) target words, we can find the total incoming belief that ?ei is within [k, `] by summing over the O(d2) values [k?, `?]",
        "where [k?, `?]",
        "?",
        "[k, `].",
        "Likewise for source words.",
        "Multiplying together these per-word beliefs and the belief that pighk` = true yields the joint belief of a consistent assignment with pighk` = true, which can be used to efficiently compute outgoing messages.",
        "ITG.",
        "To build outgoing messages, the ITG fac-torA needs to compute marginal beliefs for all of the word link variables aij .",
        "These can all be computed in O(n6) time by using a standard bitext parser to run the inside-outside algorithm.",
        "By using a normal form grammar for block ITG with nulls (Haghighi et al., 2009), we ensure that there is a 1-1 correspondence between the ITG derivations the parser sums over and word alignments a that satisfy A.",
        "The asymptotic complexity for all the factors is shown in Table 1.",
        "The total complexity for inference in each model is simply the sum of the complexities of its factors, so the complexity of the ITG model is O(n2d5 + n6), while the complexity of the relaxed model is just O(n2d5).",
        "The complexity of exact inference, on the other hand, is exponential in d for the ITG model and exponential in both d and n for the relaxed model."
      ]
    },
    {
      "heading": "6 Training and Decoding",
      "text": [
        "We use BP to compute marginal posteriors, which we use at training time to get expected feature counts and at test time for posterior decoding.",
        "For each sentence pair, we continue to pass messages until either the posteriors converge, or some maximum number of iterations has been reached.7 After running BP, the marginals we are interested in can all be computed with Eq.",
        "(9)."
      ]
    },
    {
      "heading": "6.1 Training",
      "text": [
        "We train the model to maximize the log likelihood of manually word-aligned gold training sentence pairs (with L2 regularization).",
        "Because pi and ?",
        "are determined when a is observed, the model has no latent variables.",
        "Therefore, the gradient takes the standard form for loglinear models: OLL = ?",
        "(a, pi, ?)",
        "?",
        "(10) ?",
        "a?,pi?,??",
        "p(a?, pi?, ?",
        "?|e, f)?",
        "(a?, pi?, ??)?",
        "?w The feature vector ?",
        "contains features on sure word links, extracted phrase pairs, and null-aligned words.",
        "Approximate expectations of these features can be efficiently computed using the marginal beliefs baij (sure), bpighk`(true), and b?ei ([?1,?])",
        "and b?fj ([?1,?",
        "]), respectively.",
        "We learned our final weight vectorw using AdaGrad (Duchi et al., 2010), an adaptive subgradient version of standard stochastic gradient ascent."
      ]
    },
    {
      "heading": "6.2 Testing",
      "text": [
        "We evaluate our model by measuring precision and recall on extracted phrase pairs.",
        "Thus, the decoding problem takes a sentence pair (e, f) as input, and must produce an extraction set pi as output.",
        "Our approach, posterior thresholding, is extremely simple: we set pighk` = true iff bpighk`(true) ?",
        "?",
        "for some fixed threshold ?",
        ".",
        "Note that this decoding method does not require that there be any underlying word alignment a licensing the resulting extraction set pi,8",
        "actly, but is especially true with approximate marginals from BP, which are not necessarily consistent.",
        "but the structure of the model is such that two conflicting phrase pairs are unlikely to simultaneously have high posterior probability.",
        "Most publicly available translation systems expect word-level alignments as input.",
        "These can also be generated by applying posterior threshold-ing, aligning target word i to source word j whenever baij (sure) ?",
        "t.9"
      ]
    },
    {
      "heading": "7 Experiments",
      "text": [
        "Our experiments are performed on Chinese-to-English alignment.",
        "We trained and evaluated all models on the NIST MT02 test set, which consists of 150 training and 191 test sentences and has been used previously in alignment experiments (Ayan and Dorr, 2006; Haghighi et al., 2009; DeNero and Klein, 2010).",
        "The unsupervised HMM word aligner used to generate features for the model was trained on 11.3 million words of FBIS newswire data.",
        "We test three models: the Viterbi ITG model of DeNero and Klein (2010), our BP ITG model that uses the ITG factor, and our BP Relaxed model that replaces the ITG factor with the ONESPAN factors.",
        "In all of our experiments, the phrase length d was set to 3.10"
      ]
    },
    {
      "heading": "7.1 Phrase Alignment",
      "text": [
        "We tested the models by computing precision and recall on extracted phrase pairs, relative to the gold phrase pairs of up to length 3 induced by the gold word alignments.",
        "For the BP models, we trade off precision and recall by adjusting the decoding threshold ?",
        ".",
        "The Viterbi ITG model was trained to optimize F5, a recall-biased measure, so in addition to F1, we also report the recall-biased F2 and F5 measures.",
        "The maximum number of BP iterations was set to 5 for the BP ITG model and to 10 for the BP Relaxed model.",
        "The phrase alignment results are shown in Figure 4.",
        "The BP ITG model performs comparably to the Viterbi ITG model.",
        "However, because posterior decoding permits explicit tradeoffs between precision and recall, it can do much better in the recall-biased measures, even though the Viterbi ITG model was explicitly trained to maximize F5 (DeNero and",
        "cision/Recall curve is plotted for the BP models, with the result from the Viterbi ITG model provided for reference.",
        "Klein, 2010).",
        "The BP Relaxed model performs the best of all, consistently achieving higher recall for fixed precision than either of the other models.",
        "Because of its lower asymptotic runtime, it is also much faster: over 5 times as fast as the Viterbi ITG model and over 10 times as fast as the BP ITG model.11"
      ]
    },
    {
      "heading": "7.2 Timing",
      "text": [
        "BP approximates marginal posteriors by iteratively updating beliefs for each variable based on current beliefs about other variables.",
        "The iterative nature of the algorithm permits us to make an explicit speed/accuracy tradeoff by limiting the number of iterations.",
        "We tested this tradeoff by limiting both of the BP models to run for 2, 3, 5, 10, and 20 iterations.",
        "The results are shown in Figure 5.",
        "Neither model benefits from running more iterations than used to obtain the results in Figure 4, but each can be sped up by a factor of almost 1.5x in exchange for a modest (< 1 F1) drop in accuracy.",
        "11The speed advantage of Viterbi ITG over BP ITG comes",
        "a logarithmic scale.",
        "From fastest to slowest, data points correspond to maximums of 2, 5, 10, and 20 BP iterations.",
        "F1 for the BP Relaxed model was very low when limited to 2 iterations, so that data point is outside the visible area of the graph."
      ]
    },
    {
      "heading": "7.3 Translation",
      "text": [
        "We ran translation experiments using Moses (Koehn et al., 2007), which we trained on a 22.1 million word parallel corpus from the GALE program.",
        "We compared alignments generated by the baseline HMM model, the Viterbi ITG model and the Relaxed BP model.12 The systems were tuned and evaluated on sentences up to length 40 from the NIST MT04 and MT05 test sets.",
        "The results, shown in Table 2, show that the BP Relaxed model achives a 0.8 BLEU improvement over the HMM baseline, comparable to that of the Viterbi ITG model, but taking a fraction of the time,13 making the BP Relaxed model a practical alternative for real translation applications."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "For performing inference in a state-of-the-art, but inefficient, alignment model, belief propagation is a viable alternative to greedy search methods, such as beaming.",
        "BP also results in models that are much more scalable, by reducing the asymptotic complexity of inference.",
        "Perhaps most importantly, BP permits the relaxation of artificial constraints that are generally taken for granted as being necessary for efficient inference.",
        "In particular, a relatively modest relaxation of the ITG constraint can directly be applied to any model that uses ITG-based inference (e.g. Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009)."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This project is funded by an NSF graduate research fellowship to the first author and by BBN under DARPA contract HR0011-06-C-0022."
      ]
    }
  ]
}
