{
  "info": {
    "authors": [
      "Graham Neubig",
      "Taro Watanabe",
      "Shinsuke Mori"
    ],
    "book": "EMNLP",
    "id": "acl-D12-1077",
    "title": "Inducing a Discriminative Parser to Optimize Machine Translation Reordering",
    "url": "https://aclweb.org/anthology/D12-1077",
    "year": 2012
  },
  "references": [
    "acl-C04-1073",
    "acl-C10-1043",
    "acl-D07-1080",
    "acl-D09-1105",
    "acl-D10-1092",
    "acl-D11-1017",
    "acl-D11-1018",
    "acl-D11-1045",
    "acl-E99-1010",
    "acl-J07-2003",
    "acl-J97-3002",
    "acl-N03-1017",
    "acl-N09-1028",
    "acl-N10-1128",
    "acl-P01-1067",
    "acl-P02-1040",
    "acl-P03-1054",
    "acl-P05-1066",
    "acl-P06-1096",
    "acl-P07-1091",
    "acl-P07-2045",
    "acl-P09-1104",
    "acl-P10-2033",
    "acl-P11-2093",
    "acl-W02-1001",
    "acl-W04-3250",
    "acl-W06-3119",
    "acl-W07-0401",
    "acl-W11-2102"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text.",
        "This is done by treating the parser's derivation tree as a latent variable in a model that is trained to maximize reordering accuracy.",
        "We demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree.",
        "Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrase-based SMT and previously proposed unsupervised syntax induction methods."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Finding the appropriate word ordering in the target language is one of the most difficult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax.",
        "As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or pre-ordering (Xia and McCord, 2004).",
        "In particular, systems that use source-language syntax allow for the handling of long-distance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology.",
        "decoding time.",
        "However, these require a good syntactic parser, which is not available for many languages.",
        "In recent work, DeNero and Uszko-reit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system.",
        "In this work, we present a method for inducing a parser for SMT by training a discriminative model to maximize reordering accuracy while treating the parse tree as a latent variable.",
        "As a learning framework, we use online large-margin methods to train the model to directly minimize two measures of reordering accuracy.",
        "We propose a variety of features, and demonstrate that learning can succeed when no linguistic information (POS tags or parse structure) is available in the source language, but also show that this linguistic information can be simply incorporated when it is available.",
        "Experiments find that the proposed model improves both reordering and translation accuracy, leading to average gains of 1.2 BLEU points on English-Japanese and Japanese-English translation without linguistic analysis tools, or up to 1.5 BLEU points when these tools are incorporated.",
        "In addition, we show that our model is able to effectively maximize various measures of reordering accuracy, and that the reordering measure that we choose has a direct effect on translation results."
      ]
    },
    {
      "heading": "2 Preordering for SMT",
      "text": [
        "Machine translation is defined as transformation of source sentence F = f1 .",
        ".",
        ".",
        "fJ to target sentence E = e1 .",
        ".",
        ".",
        "eI .",
        "In this paper, we take",
        "ordered into target order F ?, and its corresponding target sentence E. D is one of the BTG derivations that can produce this ordering.",
        "the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1).",
        "Reordering first deterministically transforms F into F ?, which contains the same words as F but is in the order of E. Translation then transforms F ?",
        "into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required.",
        "This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language.",
        "Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al. 2007; Li et al2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima?an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al2009; Carpuat et al2010; Isozaki et al2010b).",
        "However, as building a parser for each source language is a resource-intensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszko-reit, 2011; Visweswariah et al2011), and we will follow this thread of research in this paper.",
        "In particular, two methods deserve mention for being similar to our approach.",
        "First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training a reordering model that selects a reordering based on this parse structure.",
        "In contrast, our method trains the model in a single step, treating the parse structure as a latent variable in a discriminative reordering model.",
        "In addition Tromble and Eisner (2009) and Visweswariah et al.",
        "(2011) present models that use binary classification to decide whether each pair of words should be placed in forward or reverse order.",
        "In contrast, our method uses traditional context-free-grammar models, which allows for simple parsing and flexible parameterization, including features such as those that utilize the existence of a span in the phrase table.",
        "Our work is also unique in that we show that it is possible to directly optimize several measures of reordering accuracy, which proves important for achieving good translations.1"
      ]
    },
    {
      "heading": "3 Training a Reordering Model with",
      "text": []
    },
    {
      "heading": "Latent Derivations",
      "text": [
        "In this section, we provide a basic overview of the proposed method for learning a reordering model with latent derivations using online discriminative learning."
      ]
    },
    {
      "heading": "3.1 Space of Reorderings",
      "text": [
        "The model we present here is based on the bracketing transduction grammar (BTG, Wu (1997)) framework.",
        "BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1.",
        "Each non-terminal node can either be a straight (str) or inverted (inv) production, and terminals (term) span a non-empty substring f .2 The ordering of the sentence is determined by the tree structure and the non-terminal labels strand inv, and can be built bottom-up.",
        "Each subtree represents a source substring f and its reordered counterpart f ?.",
        "For each terminal node, no reordering occurs and f is equal to f ?.",
        "terminals produce a bilingual substring pair f/e, but as we are only interested in reordering the source F , we simplify the model by removing the target substring e.",
        "For each non-terminal node spanning f with its left child spanning f1 and its right child spanning f2, if the non-terminal symbol is str, the reordered strings will be concatenated in order as f ?",
        "= f ?1f ?2, and if the non-terminal symbol is inv, the reordered strings will be concatenated in inverted order as f ?",
        "= f ?2f ?1.",
        "We define the space of all reorderings that can be produced by the BTG as F ?, and attempt to find the best reordering F?",
        "?",
        "within this space.3"
      ]
    },
    {
      "heading": "3.2 Reorderings with Latent Derivations",
      "text": [
        "In order to find the best reordering F?",
        "?",
        "given only the information in the source side sentence F , we define a scoring function S(F ?|F ), and choose the ordering of maximal score:",
        "As our model is based on reorderings licensed by BTG derivations, we also assume that there is an underlying derivation D that produced F ?.",
        "As we can uniquely determine F ?",
        "given F and D, we can define a scoring function S(D|F ) over derivations, find the derivation of maximal score",
        "and use D?",
        "to transform F into F ?.",
        "Furthermore, we assume that the score S(D|F ) is the weighted sum of a number of feature functions defined over D and F",
        "where ?i is the ith feature function, and wi is its corresponding weight in weight vector w. Given this model, we must next consider how to learn the weights w. As the final goal of our model is to produce good reorderings F ?, it is natural to attempt to learn weights that will allow us to produce these high-quality reorderings."
      ]
    },
    {
      "heading": "4 Evaluating Reorderings",
      "text": [
        "Before we explain the learning algorithm, we must know how to distinguish whether the F ?",
        "produced by the model is good or bad.",
        "This section explains how to calculate oracle reorderings, and assign each F ?",
        "a loss and an accuracy according to how well it reproduces the oracle."
      ]
    },
    {
      "heading": "4.1 Calculating Oracle Orderings",
      "text": [
        "In order to calculate reordering quality, we first define a ranking function r(fj |F,A), which indicates the relative position of source word fj in the proper target order (Figure 2 (a)).",
        "In order to calculate this ranking function, we define A = a1, .",
        ".",
        ".",
        ",aJ , where each aj is a set of the indices of the words in E to which fj is aligned.4 Given these alignments, we define an ordering function aj1 < aj2 that indicates that the indices in aj1 come before the indices in aj2 .",
        "Formally, we define this function as ?the first index in aj1 is at most the first index in aj2 , similarly for the last index, and either the first or last index in aj1 is less than that of aj2 .?",
        "Given this ordering, we can sort every alignment aj , and use its relative position in the sentence to assign a rank to its word r(fj).",
        "In 4Null alignments require special treatment.",
        "To do so, we can place unaligned brackets and quotes directly before and after the spans they surround, and attach all other unaligned words to the word directly to the right for head-initial languages (e.g. English), or left for head-final languages (e.g. Japanese).",
        "the case of ties, where neither aj1 < aj2 nor aj2 < aj1 , both fj1 and fj2 are assigned the same rank.",
        "We can now define measures of reordering accuracy for F ?",
        "by how well it arranges the words in order of ascending rank.",
        "It should be noted that as we allow ties in rank, there are multiple possible F ?",
        "where all words are in strictly ascending order, which we will call oracle orderings."
      ]
    },
    {
      "heading": "4.2 Kendall's ?",
      "text": [
        "The first measure of reordering accuracy that we will consider is Kendall's ?",
        "(Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al2010a; Birch et al2010) and pre-ordering accuracy (Talbot et al2011).",
        "The fundamental idea behind the measure lies in comparisons between each pair of elements f ?j1 and f ?j2 of the reordered sentence, where j1 < j2.",
        "Because j1 < j2, f ?j1 comes before f ?j2 in the reordered sentence, the ranks should be r(f ?j1) ?",
        "r(f ?j2) in order to produce the correct ordering.",
        "Based on this criterion, we first define a loss Lt(F ?)",
        "that will be higher for orderings that are further from the oracle.",
        "Specifically, we take the sum of all pairwise orderings that do not follow the expected order",
        "where ?(?)",
        "is an indicator function that is 1 when its condition is true, and 0 otherwise.",
        "An example of this is given in Figure 2 (b).",
        "To calculate an accuracy measure for ordering F ?, we first calculate the maximum loss for the sentence, which is equal to the total number of non-equal rank comparisons in the sentence5",
        "no ties in rank, and thus the maximum loss can be calculated as J(J ?",
        "1)/2.",
        "Finally, we use this maximum loss to normalize the actual loss to get an accuracy",
        "which will take a value between 0 (when F ?",
        "has maximal loss), and 1 (when F ?",
        "matches one of the oracle orderings).",
        "In Figure 2 (b), Lt(F ?)",
        "="
      ]
    },
    {
      "heading": "4.3 Chunk Fragmentation",
      "text": [
        "Another measure that has been used in evaluation of translation accuracy (Banerjee and Lavie, 2005) and pre-ordering accuracy (Talbot et al2011) is chunk fragmentation.",
        "This measure is based on the number of chunks that the sentence needs to be broken into to reproduce the correct ordering, with a motivation that the number of continuous chunks is equal to the number of times the reader will have to jump to a different position in the reordered sentence to read it in the target order.",
        "One way to measure the number of continuous chunks is considering whether each word pair f ?j and f ?j+1 is discontinuous (the rank of f ?j+1 is not equal to or one",
        "and sum over all word pairs in the sentence to create a sentence-based loss",
        "While this is the formulation taken by previous work, we found that this under-penalizes bad reorderings of the first and last words of the sentence, which can contribute to the loss only once, as opposed to other words which can contribute to the loss twice.",
        "To account for this, when calculating the chunk fragmentation score, we additionally add two sentence boundary words f0 and fJ+1 with ranks r(f0) = 0 and",
        "r(f ?j) and redefine the summation in Equation (2) to consider these words (e.g.",
        "Figure 2 (c)).",
        "A, and weight vector w. ?",
        "is a very small constant, and ?",
        "and ?",
        "are defined by the update strategy.",
        "Similarly to Kendall's ?",
        ", we can also define an accuracy measure between 0 and 1 using the maximum loss, which will be at most J + 1, which corresponds to the total number of comparisons made in calculating the loss6",
        "Now that we have a definition of loss over reorderings produced by the model, we have a clear learning objective: we would like to find reorderings F ?",
        "with low loss.",
        "The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al2006), and extended to use large-margin training in an online framework (Watanabe et al2007)."
      ]
    },
    {
      "heading": "5.1 Learning Algorithm",
      "text": [
        "Learning uses the general framework of large-margin online structured prediction (Crammer et al2006), which makes several passes through the data, finding a derivation with high model score (the model parse) and a derivation with 6It should be noted that for sentences of length one or sentences with tied ranks, the maximum loss may be less than J +1, but for simplicity we use this approximation.",
        "minimal loss (the oracle parse), and updating w if these two parses diverge (Figure 3).",
        "In order to create both of these parses efficiently, we first create a parse forest encoding a large number of derivations Di according to the model scores.",
        "Next, we find the model parse D?i, which is the parse in the forest Di that maximizes the sum of the model score and the loss S(Dk|Fk,w)+L(Dk|Fk, Ak).",
        "It should be noted that here we are considering not only the model score, but also the derivation's loss.",
        "This is necessary for loss-driven large-margin training (Crammer et al2006), and follows the basic intuition that during training, we would like to make it easier to select negative examples with large loss, causing these examples to be penalized more often and more heavily.",
        "We also find an oracle parse D?i, which is selected solely to minimize the loss L(Dk|Fk, Ak).",
        "One important difference between the model we describe here and traditional parsing models is that the target derivation D?k is a latent variable.",
        "Because many Dk achieve a particular reordering F ?, many reorderings F ?",
        "are able to minimize the loss L(F ?k|Fk, Ak).",
        "Thus it is necessary to choose a single oracle derivation to treat as the target out of many equally good reorderings.",
        "DeNero and Uszkoreit (2011) resolve this ambiguity with four features with empirically tuned scores before training a monolingual parser and reordering model.",
        "In contrast, we follow previous work on discriminative learning with latent variables (Yu and Joachims, 2009), and break ties within the pool of oracle derivations by selecting the derivation with the largest model score.",
        "From an implementation point of view, this can be done by finding the derivation that minimizes L(Dk|Fk, Ak)?",
        "?S(Dk|Fk,w), where ?",
        "is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the score.",
        "Finally, if the model parse D?k has a loss that is greater than that of the oracle parse D?k, we update the weights to increase the score of the oracle parse and decrease the score of the model parse.",
        "Any criterion for weight updates may be used, such as the averaged perceptron (Collins, 2002) and MIRA (Crammer et al2006), but",
        "we opted to use Pegasos (Shalev-Shwartz et al. 2007) as it allows for the introduction of regularization and relatively stable learning.",
        "To perform this full process, given a source sentence Fk, alignment Ak, and model weights w we need to be able to efficiently calculate scores, calculate losses, and create parse forests for derivations Dk, the details of which will be explained in the following sections."
      ]
    },
    {
      "heading": "5.2 Scoring Derivation Trees",
      "text": [
        "First, we must consider how to efficiently assign scores S(D|F,w) to a derivation or forest during parsing.",
        "The most standard and efficient way to do so is to create local features that can be calculated based only on the information included in a single node d in the derivation tree.",
        "The score of the whole tree can then be expressed as the sum of the scores from each node:",
        "Based on this restriction, we define a number of features that can be used to score the parse tree.",
        "To ease explanation, we represent each node in the derivation as d = ?s, l, c, c + 1, r?, where s is the node's symbol (str, inv, or term), while l and r are the leftmost and rightmost indices of the span that d covers.",
        "c and c + 1 are the rightmost index of the left child and leftmost index of the right child for non-terminal nodes.",
        "All features are intersected with the node label s, so each feature described below corresponds to three different features (or two for features applicable to only non-terminal nodes).",
        "?",
        "?lex: Identities of words in positions fl, fr, fc, fc+1, fl?1, fr+1, flfr, and fcfc+1.",
        "?",
        "?class: Same as ?lex, but with words abstracted to classes.",
        "We use the 50 classes automatically generated by Och (1999)'s method that are calculated during alignment in standard SMT systems.",
        "?",
        "?balance: For non-terminals, features indicating whether the length of the left span (c?",
        "l+1) is lesser than, equal to, or greater than the length of the right span (r ?",
        "c).",
        "?",
        "?table: Features, bucketed by length, that indicate whether ?fl .",
        ".",
        ".",
        "fr?",
        "appears as a contiguous phrase in the SMT training data, as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011).",
        "Phrase length is limited to 8, and phrases of frequency one are removed.",
        "?",
        "?pos: Same as ?lex, but with words abstracted to language-dependent POS tags.",
        "?",
        "?cfg: Features indicating the label of the spans fl .",
        ".",
        ".",
        "fr, fl .",
        ".",
        ".",
        "fc, and fc+1 .",
        ".",
        ".",
        "fr in a supervised parse tree, and the intersection of the three labels.",
        "When spans do not correspond to a span in the supervised parse tree, we indicate ?no span?",
        "with the label ?X?",
        "(Zollmann and Venugopal, 2006).",
        "Most of these features can be calculated from only a parallel corpus, but ?pos requires a POS tagger and ?cfg requires a full syntactic parser in the source language.",
        "As it is preferable to have a method that is applicable in languages where these tools are not available, we perform experiments both with and without the features that require linguistic analysis tools."
      ]
    },
    {
      "heading": "5.3 Finding Losses for Derivation Trees",
      "text": [
        "The above features ?",
        "and their corresponding weights w are all that are needed to calculate scores of derivation trees at test time.",
        "However, during training, it is also necessary to find model parses according to the loss-augmented scoring function S(D|F,w)+L(D|F,A) or oracle parses according to the loss L(D|F,A).",
        "As noted by Taskar et al2003), this is possible if our losses can be factored in the same way as the feature space.",
        "In this section, we demonstrate that the loss L(d|F,A) for the evaluation measures we defined in Section 4 can (mostly) be factored over nodes in a fashion similar to features.",
        "For Kendall's ?",
        ", in the case of terminal nodes, Lt(d = ?term, l, r?|F,A) can be calculated by performing the summation in Equation (1).",
        "We can further define this sum recursively and use memoization for improved efficiency",
        "For non-terminal nodes, we first focus on straight non-terminals with parent node d = ?str, l, c, c+1, r?, and left and right child nodes dl = ?sl, l, lc, lc+1, c?",
        "and dr = ?sr, c+1, rc, rc+ 1, r?.",
        "First, we note that the loss for the subtree rooted at d can be expressed as",
        "In other words, the subtree's total loss can be factored into the loss of its left subtree, the loss of its right subtree, and the additional loss contributed by comparisons between the words spanning both subtrees.",
        "In the case of inverted terminals, we must simply reverse the comparison in the final sum to be ?",
        "(r(fj1) < r(fj2)).",
        "Chunk fragmentation loss can be factored in a similar fashion.",
        "First, it is clear that the loss for the terminal nodes can be calculated efficiently in a fashion similar to Equation (3).",
        "In order to calculate the loss for non-terminals d, we note that the summation in Equation (2) can be divided into the sum over the internal bi-grams in the left and right subtrees, and the bi-gram spanning the reordered trees",
        "However, unlike Kendall's ?",
        ", this equation relies not on the ranks of fc and fc+1 in the original sentence, but on the ranks of f ?c and f ?c+1 in the reordered sentence.",
        "In order to keep track of these values, it is necessary to augment each node in the tree to be d = ?s, l, c, c + 1, r, tl, tr?",
        "with two additional values tl and tr that indicate the position of the leftmost and rightmost words after reordering.",
        "Thus, a straight non-terminal parent d with children dl = ?sl, l, lc, lc+ 1, c, tl, tlr?",
        "and dr = ?sr, c+1, rc, rc+1, r, trl, tr?",
        "will have loss as follows",
        "with a similar calculation being possible for inverted non-terminals."
      ]
    },
    {
      "heading": "5.4 Parsing Derivation Trees",
      "text": [
        "Finally, we must be able to create a parse forest from which we select model and oracle parses.",
        "As all feature functions factor over single nodes, it is possible to find the parse tree with the highest score in O(J3) time using the CKY algorithm.",
        "However, when keeping track of target positions for calculation of chunk fragmentation loss, there are a total of O(J5) nodes, an unreasonable burden in terms of time and memory.",
        "To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007)."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "Our experiments test the reordering and translation accuracy of translation systems using the proposed method.",
        "As reordering metrics, we use Kendall's ?",
        "and chunk fragmentation (Talbot et al., 2011) comparing the system F ?",
        "and oracle F ?",
        "calculated with manually created alignments.",
        "As translation metrics, we use BLEU (Papineni et al2002), as well as RIBES (Isozaki et al. 2010a), which is similar to Kendall's ?",
        ", but evaluated on the target sentence E instead of the reordered sentence F ?.",
        "All scores are the average of three training runs to control for randomness in training (Clark et al2011).",
        "For translation, we use Moses (Koehn et al. 2007) with lexicalized reordering (Koehn et al. 2005) in all experiments.",
        "We test three types",
        "training and testing the reordering model (RM), translation model (TM), and language model (LM).",
        "of pre-ordering: original order with F ?",
        "?",
        "F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3- step), and the proposed model with latent derivations (lader).7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10?3 (chosen through cross-validation).",
        "We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011).",
        "We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1).",
        "We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sentences of more than 60 words.",
        "As default features for lader and the monolingual parsing and reordering models in 3-step,",
        "except ?pos and ?cfg.",
        "In addition, we test systems with ?pos and ?cfg added.",
        "For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing.",
        "For Japanese, we use the KyTea tagger (Neubig et al2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al. 2011) with simple manual head-rules to convert a dependency parse to a CFG parse."
      ]
    },
    {
      "heading": "6.1 Effect of Pre-ordering",
      "text": [
        "Table 2 shows reordering and translation results for orig, 3-step, and lader.",
        "It can be seen that the proposed lader outperforms the baselines in both reordering and translation.",
        "There are a number of reasons why lader outperforms 3-step.",
        "First, the pipeline of 3-step suffers from error propogation, with errors in monolingual parsing and reordering resulting in low overall accuracy.10 Second, as Section 5.1 describes, lader breaks ties between oracle parses based on model score, allowing easy-to-reproduce model parses to be chosen during training.",
        "In fact, lader generally found trees that followed from syntactic constituency, while 3-step more often used terminal nodes",
        "that spanned constituent boundaries (as long as the phrase frequency was high).",
        "Finally, as Section 6.2 shows in detail, the ability of lader to maximize reordering accuracy directly allows for improved reordering and translation results.",
        "It can also be seen that incorporating POS tags or parse trees improves accuracy of both lader and 3-step, particularly for English-Japanese, where syntax has proven useful for pre-ordering, and less so for Japanese-English, where syntactic pre-ordering has been less successful (Sudoh et al2011b).",
        "We also tested Moses's implementation of hierarchical phrase-based SMT (Chiang, 2007), which achieved BLEU scores of 23.21 and 19.30 for English-Japanese and Japanese-English respectively, approximately matching lader in accuracy, but with a significant decrease in decoding speed.",
        "Further, when pre-ordering with lader and hierarchical phrase-based SMT were combined, BLEU scores rose to 23.29 and 19.69, indicating that the two techniques can be combined for further accuracy improvements."
      ]
    },
    {
      "heading": "6.2 Effect of Training Loss",
      "text": [
        "Table 3 shows results when one of three losses is optimized during training: chunk fragmentation (Lc), Kendall's ?",
        "(Lt), or the linear interpolation of the two with weights chosen so that both losses contribute equally (Lt + Lc).",
        "In general, training successfully maximizes the criterion it is trained on, and Lt +Lc achieves good results on both measures.",
        "We also find that Lc and Lc+Lt achieve the best translation results, which is in concert with Talbot et al2011), who find chunk fragmentation is better correlated with translation accuracy than Kendall's ?",
        ".",
        "This is an important result, as methods such as that of Tromble and Eisner (2009) optimize pairwise en-ja ja-en"
      ]
    },
    {
      "heading": "BLEU/RIBES BLEU/RIBES",
      "text": [
        "manual or automatic alignments are used in training.",
        "word comparisons equivalent to Lt, which may not be optimal for translation."
      ]
    },
    {
      "heading": "6.3 Effect of Automatic Alignments",
      "text": [
        "Table 4 shows the difference between using manual and automatic alignments in the training of lader.",
        "lader is able to improve over the orig baseline in all cases, but when equal numbers of manual and automatic alignments are used, the reorderer trained on manual alignments is significantly better.",
        "However, as the number of automatic alignments is increased, accuracy improves, approaching that of the system trained on a smaller number of manual alignments."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We presented a method for learning a discriminative parser to maximize reordering accuracy for machine translation.",
        "Future work includes application to other language pairs, development of more sophisticated features, investigation of probabilistic approaches to inference, and incorporation of the learned trees directly in tree-to-string translation."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank Isao Goto, Tetsuo Kiso, and anonymous reviewers for their helpful comments, and Daniel Flannery for helping to run his parser."
      ]
    }
  ]
}
