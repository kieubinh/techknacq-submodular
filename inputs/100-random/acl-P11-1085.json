{
  "info": {
    "authors": [
      "Bing Zhao",
      "Young-Suk Lee",
      "Xiaoqiang Luo",
      "Liu Li"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-1085",
    "title": "Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations",
    "url": "https://aclweb.org/anthology/P11-1085",
    "year": 2011
  },
  "references": [
    "acl-C10-1071",
    "acl-C10-1080",
    "acl-D08-1022",
    "acl-D08-1060",
    "acl-D10-1014",
    "acl-D10-1027",
    "acl-D10-1060",
    "acl-J07-2003",
    "acl-J93-2003",
    "acl-J97-3002",
    "acl-N06-1031",
    "acl-N07-1008",
    "acl-N09-1053",
    "acl-N09-2006",
    "acl-N10-1015",
    "acl-N10-1016",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P03-2041",
    "acl-P07-2045",
    "acl-P08-1023",
    "acl-P08-1064",
    "acl-P08-1066",
    "acl-P08-1114",
    "acl-P09-1020",
    "acl-P10-1146",
    "acl-W06-1606",
    "acl-W06-3119",
    "acl-W09-0426"
  ],
  "sections": [
    {
      "text": [
        "We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars.",
        "We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering.",
        "In particular, we integrate synchronous bi-narizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries.",
        "We learn the structural preferences from the data in a generative framework.",
        "The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST-08 evaluations by 1.3 absolute BLEU, which is statistically significant."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Most syntax-based machine translation models with synchronous context free grammar (SCFG) have been relying on the off-the-shelf monolingual parse structures to learn the translation equivalences for string-to-tree, tree-to-string or tree-to-tree grammars.",
        "However, state-of-the-art monolingual parsers are not necessarily well suited for machine translation in terms of both labels and chunks/brackets.",
        "For instance, in Arabic-to-English translation, we find only 45.5% of Arabic NP-SBJ structures are mapped to the English NP-SBJ with machine alignment and parse trees, and only 60.1% of NP-SBJs are mapped with human alignment and parse trees as in ยง 2.",
        "The chunking is of more concern; at best only 57.4% source chunking decisions are translated contiguously on the target side.",
        "To translate the rest of the chunks one has to frequently break the original structures.",
        "The main issue lies in the strong assumption behind SCFG-style nonterminals - each nonterminal (or variable) assumes a source chunk should be rewritten into a contiguous chunk in the target.",
        "Without integrating techniques to modify the parse structures, the SCFGs are not to be effective even for translating NP-SBJ in linguistically distant language-pairs such as Arabic-English.",
        "Such problems have been noted in previous literature.",
        "Zollmann and Venugopal (2006) and Marcu et al.",
        "(2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process.",
        "Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context.",
        "Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space.",
        "In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest.",
        "Overall, the forest-based translation can reduce the risks from upstream parsing errors and expand the search space, but it cannot sufficiently address the syntactic divergences between various language-pairs.",
        "The tree sequence approach adds pseudo nodes and hyper edges to the forest, which makes the forest even denser and harder for navigation and search.",
        "As trees thrive in the search space, especially with the pseudo nodes and edges being added to the already dense forest, it is becoming harder to wade through the deep forest for the best derivation path out.",
        "We propose to simplify suitable subtrees to a reasonable level, at which the correct reordering can be easily identified.",
        "The transformed structure should be frequent enough to have rich statistics for learning a model.",
        "Instead of creating pseudo nodes and edges and make the forest dense, we transform a tree with a few simple operators; only meaningful frontier nodes, context nodes and edges are kept to induce the correct reordering; such operations also enable the model to share the statistics among all similar subtrees.",
        "On the basis of our study on investigating the language divergence between Arabic-English with human aligned and parsed data, we integrate several simple statistical operations, to transform parse trees adaptively to serve the translation purpose better.",
        "For each source span in the given sentence, a subgraph, corresponding to an elementary tree (in Eqn.",
        "1), is proposed for PSCFG translation; we apply a few operators to transform the subgraph into some frequent subgraphs seen in the whole training data, and thus introduce alternative similar translational equivalences to explain the same source span with enriched statistics and features.",
        "For instance, if we regroup two adjacent nodes IV and NP-SBJ in the tree, we can obtain the correct reordering pattern for verb-subject order, which is not easily available otherwise.",
        "By finding a set of similar elementary trees derived from the original elementary trees, statistics can be shared for robust learning.",
        "We also investigate the features using the context beyond the phrasal subtree.",
        "This is to further disambiguate the transformed subgraphs so that informative neighboring nodes and edges can influence the reordering preferences for each of the transformed trees.",
        "For instance, at the beginning and end of a sentence, we do not expect dramatic long distance reordering to happen; or under SBAR context, the clause may prefer monotonic reordering for verb and subject.",
        "Such boundary features were treated as hard constraints in previous literature in terms of re-labeling (Huang and Knight, 2006) or restructuring (Wang et al., 2010).",
        "The boundary cases were not addressed in the previous literature for trees, and here we include them in our feature sets for learning a MaxEnt model to predict the transformations.",
        "We integrate the neighboring context of the subgraph in our transformation preference predictions, and this improve translation qualities further.",
        "The rest of the paper is organized as follows: in section 2, we analyze the projectable structures using human aligned and parsed data, to identify the problems for SCFG in general; in section 3, our proposed approach is explained in detail, including the statistical operators using a MaxEnt model; in section 4, we illustrate the integration of the proposed approach in our decoder; in section 5, we present experimental results; in section 6, we conclude with discussions and future work."
      ]
    },
    {
      "heading": "2. The Projectable Structures",
      "text": [
        "A context-free style nonterminal in PSCFG rules means the source span governed by the nonterminal should be translated into a contiguous target chunk.",
        "A \"projectable\" phrase-structure means that it is translated into a contiguous span on the target side, and thus can be generalized into a nonterminal in our PSCFG rule.",
        "We carried out a controlled study on the projectable structures using human annotated parse trees and word alignment for 5k Arabic-English sentence-pairs.",
        "In Table 1, the unlabeled F-measures with machine alignment and parse trees show that, for only 48.71% of the time, the boundaries introduced by the source parses",
        "Table 1: The labeled and unlabeled F-measures for projecting the source nodes onto the target side via alignments and parse trees; unlabeled F-measures show the bracketing accuracies for translating a source span contiguously.",
        "H: human, M: machine.",
        "are real translation boundaries that can be explained by a nonterminal in PSCFG rule.",
        "Even for human parse and alignment, the unlabeled F-measures are still as low as 57.39%.",
        "Such statistics show that we should not blindly learn tree-to-string grammar; additional transformations to manipulate the bracketing boundaries and labels accordingly have to be implemented to guarantee the reliability of source-tree based syntax translation grammars.",
        "The transformations could be as simple as merging two adjacent nonterminals into one bracket to accommodate non-contiguity on the target side, or lexicalizing those words which have fork-style, many-to-many alignment, or unaligned content words to enable the rest of the span to be generalized into nonterminals.",
        "We illustrate several cases using the tree in Figure 1.",
        "Figure 1: Non-projectable structures in an SBAR tree with human parses and alignment; there are non-projectable structures: the deleted nonterminals PRON (+hA), the many-to-many alignment for IV(ttAlf) PREP(mn), fork-style alignment for NOUN (Azmp).",
        "In Figure 1, several non-projectable nodes were illustrated: the deleted nonterminals PRON (+hA), the many-to-many alignment for IV(ttAlf) PREP(mn), fork-style alignment for NOUN (Azmp).",
        "Intuitively, it would be good to glue the nodes NOUN(Al$rq) ADJ(AlAwsT) under the node of NP, because it is more frequent for moving ADJ before NOUN in our training data.",
        "It should be easier to model the swapping of (NOUN ADJ) using the tree (NP NOUN, ADJ) instead of the original bigger tree of (NP-SBJ Azmp, NOUN, ADJ) with one lexicalized node.",
        "Alignment",
        "Parse",
        "Labels",
        "Accuracy",
        "NP-SBJ",
        "0.6011",
        "H",
        "H",
        "PP",
        "0.3436",
        "NP unlabel",
        "0.4832 0.5739",
        "NP-SBJ",
        "0.5356",
        "M",
        "H",
        "PP",
        "0.2765",
        "NP unlabel",
        "0.3959 0.5305",
        "NP-SBJ",
        "0.4555",
        "M",
        "M",
        "PP",
        "0.1935",
        "NP unlabel",
        "0.3556 0.4871",
        "Approaches in tree-sequence based grammar (Zhang et al., 2009) tried to address the bracketing problem by using arbitrary pseudo nodes to weave a new \"tree\" back into the forest for further grammar extractions.",
        "Such approach may improve grammar coverage, but the pseudo node labels would be arguably a worse choice to split the already sparse data.",
        "Some of the interior nodes connecting the frontier nodes might be very informative for modeling reordering.",
        "Also, due to the introduced pseudo nodes, it would need exponentially many nonterminals to keep track of the matching tree-structures for translations.",
        "The created pseudo node could easily block the informative neighbor nodes associated with the subgraph which could change the reordering nature.",
        "For instance, IV and NP-SBJ tends to swap at the beginning of a sentence, but it may prefer monotone if they share a common parent of SBAR for a subclause.",
        "In this case, it is unnecessary to create a pseudo node \"IV+SBJ\" to block useful factors.",
        "We propose to navigate through the forest, via simplifying trees by grouping the nodes, cutting the branches, and attaching connected neighboring informative nodes to further disambiguate the derivation path.",
        "We apply explicit translation motivated operators, on a given monolingual elementary tree, to transform it into similar but simpler trees, and expose such statistical preferences to the decoding process to select the best rewriting rule from the enriched grammar rule sets, for generating target strings."
      ]
    },
    {
      "heading": "3. Elementary Trees to String Grammar",
      "text": [
        "We propose to use variations of an elementary tree, which is a connected subgraph fitted in the original monolingual parse tree.",
        "The subgraph is connected so that the frontiers (two or more) are connected by their immediate common parent.",
        "Let 7 be a source elementary tree:",
        "where vf is a set of frontier nodes which contain nonterminals or words; vi are the interior nodes with source labels/symbols; E is the set of edges connecting the nodes v = vf+vi into a connected subgraph fitted in the source parse tree; ยฃ is the immediate common parent of the frontier nodes vf.",
        "Our proposed grammar rule is formulated as follows:",
        "where a is the target string, containing the terminals and/or nonterminals in a target language; ~ is the one-to-one alignment of the nonterminals between 7 and a; t contains possible sequence of transform operations (to be explained later in this section) associated with each rule; m is a function of enumerating the neighborhood of the source elementary tree 7, and certain tree context (nodes and edges) can be used to further disambiguate the reordering or the given lexical choices.",
        "The interior nodes of Y.vi, however, are not necessarily informative for the reordering decisions, like the unary nodes WHNP,VP, and PP-CLR in Figure 1; while the frontier nodes Y.vf are the ones directly executing the reordering decisions.",
        "We can selectively cut off the interior nodes, which have no or only weak causal relations to the reordering decisions.",
        "This will enable the frequency or derived probabilities for executing the reordering to be more focused.",
        "We call such transformation operators t. We specified a few operators for transforming an elementary tree 7, including flattening tree operators such as removing interior nodes in vi, or grouping the children via binarizations.",
        "Let's use the trigram \"Alty ttAlf mn\" in Figure 1 as an example, the immediate common parent for the span is SBAR: 7.I = SBAR; the interior nodes are Y.vi = {WHNP VP S PP-CLR}; the frontier nodes are Y.vf = (x:PRON x:IV x:PREP).",
        "The edges 7.E (as highlighted in Figure 1) connect Y.vi and Y.vf into a subgraph for the given source ngram.",
        "For any source span, we look up one elementary tree 7 covering the span, then we select an operator t e T, to explore a set of similar elementary trees รฏ(y, m) = {7'} as simplified alternatives for translating that source tree (span) 7 into an optimal target string a* accordingly.",
        "Our generative model is summarized in Eqn.",
        "3:",
        "In our generative scheme, for a given elementary tree 7, wesampleanoperator(oracombinationofoperations) t with the probability of pc(t\\7); with operation t, we transform 7 into a set of simplified versions 7' e รฏ(y, m) with the probability of ph(7'\\t, 7); finally we select the transformed version ' to generate the target string a'with a probability of pa(a'7').",
        "Note here, 7' and 7 share the same immediate common parent ยฃ, but not necessarily the frontier, or interior, or even neighbors.",
        "The frontier nodes can be merged, lexicalized, or even deleted in the tree-to-string rule associated with ', as long as the alignment for the nonterminals are book-kept in the derivations.",
        "To simplify the model, one can choose the operator t to be only one level, and the model using a single operator t is to be deterministic.",
        "Thus, the final set of models to learn are pa(a'\\7') for rule alignment, and the preference model ph(7'\\t, 7, m), and the operator proposal model pc{t\\7, m), which in our case is a maximum entropy model โ the key model in our proposed approach in this paper for transforming the original elementary tree into similar trees for evaluating the reordering probabilities.",
        "Eqn.",
        "3 significantly enriches reordering powers for syntax-based machine translation.",
        "This is because it uses all similar set of elementary trees to generate the best target strings.",
        "In the next section, we'll first define the operators conceptually, and then explain how we learn each of the models.",
        "A log linear model is applied here to approximate pa(a'\\7') ยซ exp(A โข ff ) via weighted combination (A) of feature functions ff (a',7' ), including relative frequencies in both directions, and IBM Model-1 scores in both directions as 7' and a' have lexical items within them.",
        "We also employed a few binary features listed in the following table.",
        "7' is observed less than 2 times",
        "( a ' , 7' ) over generates a tgt content word",
        "(a', 7') over generates a tgt function word Ph(7'\\h, 7, m) is our preference model.",
        "For instance using the operator h of cutting an unary interior node in 7.vi, if 7.vi has more than one unary interior node, like the SBAR tree in Figure 1, having three unary interior node: WHNP, VP and PP-CLR, ph(7'\\h,7, m) specifies which one should have more probabilities to be cut.",
        "In our case, to make model simple, we simply choose histogram/frequency for modeling the choices.",
        "Pc(t 7, m) is our operator proposal model.",
        "It ranks the operators which are valid to be applied for the given source tree 7 together with its neighborhood m. Here, in our approach, we applied a Maximum Entropy model, which is also employed to train our Arabic parser: pc(t\\7, m) ยซ exp X โข ff (t, 7, m).",
        "The feature sets we use here are almost the same set we used to train our Arabic parser; the only difference is the future space here is operator categories, and we check bag-of-nodes for interior nodes and frontier nodes.",
        "The key feature categories we used are listed as in the Table 3.",
        "The headtable used in our training is manually built for Arabic.",
        "bag-of-nodes 7.vi bag-of-nodes and ngram of 7.vf chunk-level features: left-child, right-child, etc.",
        "lexical features: unigram and bigram pos features: unigram and bigram contextual features: surrounding words",
        "Obvious systematic linguistic divergences between language-pairs could be handled by some simple operators such as using binarization to regroup contiguously aligned children.",
        "Here, we start from the human aligned and parsed data as used in section 2 to explore potential useful operators.",
        "One of the simplest way for transforming a tree is via bi-narization.",
        "Monolingual binarization chooses to regroup children into smaller subtree with a suitable label for the newly created root.",
        "We choose a function mapping to select the top-frequent label as the root for the grouped children; if such label is not found we simply use the label of the immediate common parent for 7.",
        "In decoding time, we need to select trees from all possible binarizations, while in the training time, we restrict the choices allowed with the alignment constraint, that every grouped children should be aligned contiguously on the target side.",
        "Our goal is to simulate the synchronous binarization as much as we can.",
        "In this paper, we applied the four basic operators for binarizing a tree: left-most, rightmost and additionally head-out left and head-out right for more than three children.",
        "Two examples are given in Table 4, in which we used LDC style representation for the trees.",
        "With the proper binarization, the structure becomes rich in substructures which allow certain reordering to happen more likely than others.",
        "For instance for the subtree (VP PV NP-SBJ), one would apply stronger statistics from training data to support the swap of NP-SBJ and PV for translation.",
        "Verbs are keys for reordering especially for Araic-English with VSO translated into SVO.",
        "However, if the verb and its relevant arguments for reordering are at different levels in the tree, the reordering is difficult to model as more interior nodes combinations will distract the distributions and make the model less focused.",
        "We provide the following two operations specific for verb in VP trees as in",
        "Table 5.",
        "For reordering patterns, keeping the deep tree structure might not be the best choice.",
        "Sometimes it is not even possible due to the many-to-many alignment, insertions and deletions of terminals.",
        "So, we introduce the operators to remove the interior nodes 7/v selectively; this way, we can flatten the tree, remove irrelevant nodes and edges, and can use more frequent observations of simplified structures to capture the reordering patterns.",
        "We use two operators as shown in Table 6.",
        "The second operator deletes all the interior nodes, labels and edges; thus reordering will become a Hiero-alike (Chiang, 2007) unlabeled rule, and additionally a special glue rule: XiX2 ยป X1X2.",
        "This operator is necessary, we need a scheme to automatically back off to the meaningful glue or Hiero-alike rules, which may lead to a cheaper derivation path for constructing a partial hypothesis, at the decoding time.",
        "NOUN dfE",
        "DET+NOUN AlAwDAE",
        "PREP Aly",
        "DET+NOUN",
        "AlAnfjAr situation",
        "Figure 2: A NP tree with an \"inside-out\" alignment.",
        "The nodes \"NP*\" and \"PP*\" are not suitable for generalizing into NTs used in PSCFG rules.",
        "As shown in Table 1, NP brackets has only 35.56% of time to be translated contiguously as an NP in machine aligned & parsed data.",
        "The NP tree in Figure 2 happens to be an \"inside-out\" style alignment, and context free grammar such as ITG (Wu, 1997) can not explain this structure well without necessary lexicalization.",
        "Actually, the Arabic tokens of \"dfE Aly AlAnfjAr\" form a combination and is turned into English word \"ignite\" in an idiomatic way.",
        "With lexicalization, a Hiero style rule \"dfE X Aly AlAnfjAr ยป to ignite X\" is potentially a better alternative for translating the NP tree.",
        "Our operators allow us to back off to such Hiero-style rules to construct derivations, which share the immediate common parent NP, as defined for the elementary tree, for the given source span.",
        "For a given elementary tree, we use function 'fa to check the context beyond the subgraph.",
        "This includes looking the nodes and edges connected to the subgraph.",
        "Similar to the features used in (Dyer et al., 2009), we check the following three cases.",
        "When the tree 7 frontier sets contain the leftmost token, rightmost token, or both sides, we will add to the neighboring nodes the corresponding decoration tags L (left), R (right), and B (both), respectively.",
        "These decorations are important especially when the reordering patterns for the same trees are depending on the context.",
        "For instance, at the beginning or end of a sentence, we do not expect dramatic reordering - moving a token too far away in the middle of the sentences.",
        "We check siblings of the root for 7 for a few special labels, including SBAR, IP, PP, and FRAG.",
        "These labels indicate a partial sentence or clause, and the reordering patterns may get different distributions due to the position relative to these nodes.",
        "For instance, the PV and SBJ nodes under SBAR tends to have more monotone preference for word reordering (Carpuat et al., 2010).",
        "We mark the boundaries with position markers such as L-PP, to indicate having a left sibling PP, RIP for having a right sibling IP, and C-SBAR to indicate the elementary tree is a child of SBAR.",
        "These labels are selected mainly based on our linguistic intuitions and errors in our translation system.",
        "A data-driven approach might be more promising for identifying useful markups w.r.t specific reordering patterns.",
        "In the Figure 2, there are two special nodes under NP: NP* and PP*.",
        "These two nodes are aligned in a \"inside-out\" fashion, and none of them can be generalized into a nonterminal to be rewritten in a PSCFG rule.",
        "In other words, the phrasal brackets induced from NP* and PP* are not translation boundaries, and to avoid translation errors we should identify them by applying a PSCFG rule on top of them.",
        "During training, we label nodes with translation boundaries, as one additional function tag; during decoding, we employ the MaxEnt model to predict the translation boundary label probability for each span associated with a subgraph 7, and discourage derivations accordingly for using nonterminals over the nontranslation boundary span.",
        "The translation boundaries over elementary trees have much richer representation power.",
        "The previous works as in Xiong et al.",
        "(2010), defined translation boundaries on phrase-decoder style derivation trees due to the nature of their shift-reduce algorithm, which is a special case in our model.",
        "Binarization Operations",
        "Examples",
        "right-most left-most",
        "(NP Xnoun Xadil Xadj2 ) ยป (NP Xnoun (ADJP Xadj, Xadj2 ))",
        "(VP Xpv Xnp-sbj Xsbar) ยป (VP (VP Xpv Xnp-sbj) Xsbar)",
        "Operators for regroup verbs",
        "Examples",
        "regroup verb",
        "regroup verb and remove the top level VP",
        "(VPi Xv (VP2 Y)) ยป (VPi (VP2 Xv Y)) (R (VP!",
        "Xv (R2 Y))) ยป (R (R XvY))"
      ]
    },
    {
      "heading": "4. Decoding",
      "text": [
        "Decoding using the proposed elementary tree to string grammar naturally resembles bottom up chart parsing algorithms.",
        "The key difference is at the grammar querying step.",
        "Given a grammar G, and the input source parse tree n from a monolingual parser, we first construct the elementary tree for a source span, and then retrieve all the relevant subgraphs seen in the given grammar through the proposed operators.",
        "This step is called populating, using the proposed operators to find all relevant elementary trees 7 which may have contributed to explain the source span, and put them in the corresponding cells in the chart.",
        "There would have been exponential number of relevant elementary trees to search if we do not have any restrictions in the populating step; we restrict the maximum number of interior nodes \\y.v \\ to be 3, and the size of frontier nodes \\7.vf \\ to be less than 6; additional pruning for less frequent elementary trees is carried out.",
        "After populating the elementary trees, we construct the partial hypotheses bottom up, by rewriting the frontier nodes of each elementary tree with the probabili-ties(costs) for 7 ยป a* as in Eqn.",
        "3.",
        "Our decoder (Zhao and Al-Onaizan, 2008) is a template-based chart decoder in C++.",
        "It generalizes over the dotted-product operator in Earley style parser, to allow us to leverage many operators a G T as above-mentioned, such as binarizations, at different levels for constructing partial hypothesis."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "In our experiments, we built our system using most of the parallel training data available to us: 250M Arabic running tokens, corresponding to the \"unconstrained\" condition in NIST-MT08.",
        "We chose the testsets of newswire and weblog genres from MT08 and DEV10.",
        "In particular, we choose MT08 to enable the comparison of our results to the reported results in NIST evaluations.",
        "Our training and test data is summarized in Table 5.",
        "For testings, we have 129,908 tokens in our testsets.",
        "For language models (LM), we used 6-gram LM trained with 10.3 billion English tokens, and also a shrinkage-based",
        "Emami et al., 2010) with 150 word-clusters learnt from 2.1 million tokens.",
        "From the parallel data, we extract phrase pairs(blocks) and elementary trees to string grammar in various configurations: basic tree-to-string rules (Tr2str), elementary tree-to-string rules with boundaries a(elm2str+ma), and with both a and m (elm2str+a + fa).",
        "This is to evaluate the operators' effects at different levels for decoding.",
        "To learn our MaxEnt models defined in ยง 3.3, we collect the events during extracting elm2str grammar in training time, and learn the model using improved iterative scaling.",
        "We use the same training data as that used in training our Arabic parser.",
        "There are 16 thousand human parse trees with human alignment; additional 1 thousand human parse and aligned sent-pairs are used as unseen test set to verify our MaxEnt models and parsers.",
        "For our Arabic parser, we have a labeled F-measure of 78.4%, and POS tag accuracy 94.9%.",
        "In particular, we'll evaluate model pc(t\\7, fa) in Eqn.",
        "3 for predicting the translation boundaries in ยง 3.5.3 for projectable spans as detailed in ยง 5.1.",
        "Our decoder (Zhao and Al-Onaizan, 2008) supports grammars including monotone, ITG, Hiero, tree-to-string, string-to-tree, and several mixtures of them (Lee et al., 2010).",
        "We used 19 feature functions, mainly from those used in phrase-based decoder like Moses (Koehn et al., 2007), including two language models (one for a 6-gram LM, one for ModelM, one brevity penalty, IBM Model-1 (Brown et al., 1993) style alignment probabilities in both directions, relative frequency in both directions, word/rule counts, content/function word mismatch, together with features on tr2str rule probabilities.",
        "We use BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to evaluate translation qualities.",
        "Our baseline used basic elementary tree to string grammar without any manipulations and boundary markers in the model, a cased BLEU of 53.31, which is close to the best officially reported result 53.85 for unconstrained systems.We expose the statistical decisions in Eqn.",
        "3 as the rule probability as one of the 19 dimensions, and use Simplex Downhill algorithm with Armijo line search (Zhao and Chen, 2009) to optimize the weight vector for decoding.",
        "The algorithm moves all dimensions at the same time, and empirically achieved more stable results than MER(Och, 2003) in many of our experiments.",
        "operators for removing nodes/edges",
        "Examples",
        "remove unary nodes remove all labels",
        "(R Xtl (Ri (R2 ))) ยป (R Xtl (R2 Xt, ))) (R (Ri Xtl (R2 ))) ยป (R Xt2Xtl )",
        "The projectable structure is important for our proposed elementary tree to string grammar (elm2str).",
        "When a span is predicted not to be a translation boundary, we want the decoder to prefer alternative derivations outside of the immediate elementary tree, or more aggressive manipulation of the trees, such as deleting interior nodes, to explore unlabeled grammar such as Hi-ero style rules, with proper costs.",
        "We test separately on predicting the projectable structures, like predicting function tags in ยง 3.5.3, for each node in syntactic parse tree.",
        "We use one thousand test sentences with two conditions: human parses and machine parses.",
        "There are totally 40,674 nodes excluding the sentence-level node.",
        "The results are shown in Table 8.",
        "It showed our Max-Ent model is very accurate using human trees: 94.5% of accuracy, and about 84.7% of accuracy for using the machine parsed trees.",
        "Our accuracies are higher compared with the 71+% accuracies reported in (Xiong et al., 2010) for their phrasal decoder.",
        "We zoom in the translation boundaries for MT08-NW, in which we studied a few important frequent labels including VP and NP-SBJ as in Table 9.",
        "According to our MaxEnt model, 20% of times we should discourage a VP tree to be translated contiguously; such VP trees have an average span length of 16.9 tokens in MT08-NW.",
        "Similar statistics are 15.9% for S-tree with an average span of 13.8 tokens.",
        "Using the predicted projectable structures for elm2str grammar, together with the probability defined in Eqn.",
        "3 as additional cost, the translation results in Table 11 show it helps BLEU by 0.29 BLEU points (56.13 v.s.",
        "55.84).",
        "The boundary decisions penalize the derivation paths using nonterminals for non-projectable spans for partial hypothesis construction.",
        "We carried out a series of experiments to explore the impacts using a and m for elm2str grammar.",
        "We start from transforming the trees via simple operator a(7), and then expand the function with more tree context to include the neighboring functions: a(7, ff).",
        "Experiments in Table 10 focus on testing operators especially binarizations for transforming the trees.",
        "In Table 10, the four possible binarization methods all improve",
        "Data",
        "Train",
        "MT08-NW",
        "MT08-WB",
        "Dev10-NW",
        "Dev10-WB",
        "# Sents",
        "# Tokens",
        "8,032,837 349M(ar)/230M(en)",
        "813 25,926",
        "547 19,654",
        "1089 41,240",
        "1059 43,088",
        "Labels",
        "total",
        "NonProj",
        "Percent",
        "Avg.len",
        "VP*",
        "4479",
        "920",
        "20.5%",
        "16.9",
        "NP*",
        "14164",
        "825",
        "5.8%",
        "8.12",
        "S*",
        "3123",
        "495",
        "15.9%",
        "13.8",
        "NP-SBJ*",
        "1284",
        "53",
        "4.12%",
        "11.9",
        "Setups",
        "TER",
        "BLEUr4n4",
        "Baseline",
        "39.87",
        "55.01",
        "right-binz (rbz)",
        "39.10",
        "55.19",
        "left-binz (lbz)",
        "39.67",
        "55.31",
        "Head-out-left (hlbz)",
        "39.56",
        "55.50",
        "Head-out-right (hrbz)",
        "39.52",
        "55.53",
        "+all binzation (abz)",
        "39.42",
        "55.60",
        "+regroup-verb",
        "39.29",
        "55.72",
        "+deleting interior nodes 7/v",
        "38.98",
        "55.84",
        "Setups",
        "Accuracy",
        "Human Parses Machine Parses",
        "94.5% 84.7%",
        "Setups",
        "TER",
        "BLEUr4n4",
        "Baseline w/a",
        "38.98",
        "55.84",
        "+ TM Boundaries",
        "38.89",
        "56.13",
        "+ SENT Bound",
        "38.63",
        "56.46",
        "all a(7, fa)",
        "38.61",
        "56.87",
        "Table 12: BLEU scores on various test sets; comparing elementary tree-to-string grammar (tr2str), transformation of the trees (elm2str+h), using the neighboring function for boundaries ( elm2str+fh), and combination of all together ( elm2str+h(7, fh)).",
        "MT08-NW and MT08-WB have four references; Dev10-WB has three references, and Dev10-NW has one reference.",
        "BLEUn4 were reported.",
        "over the baseline from +0.18 (via rightmost binarization) to +0.52 (via head-out-right) BLEU points.",
        "When we combine all binarizations (abz), we did not see additive gains over the best individual case - hrbz.",
        "Because during our decoding time, we do not frequently see large number of children (maximum at 6), and for smaller trees (with three or four children), these operators will largely generate same transformed trees, and that explains the differences from these individual binarization are small.",
        "For other languages, these binarization choices might give larger differences.",
        "Additionally, regrouping the verbs is marginally helpful for BLEU and TER.",
        "Upon close exanimations, we found it is usually beneficial to group verb (PV or IV) with its neighboring nodes for expressing phrases like \"have to do\" and \"will not only\".",
        "Deleting the interior nodes helps on shrinking the trees, so that we can translate it with more statistics and confidences.",
        "It helps more on TER than BLEU for MT08-NW.",
        "Table 11 extends Table 10 with neighboring function to further disambiguate the reordering rule using the tree context.",
        "Besides the translation boundary, the reordering decisions should be different with regard to the positions of the elementary tree relative to the sentence.",
        "At the sentence-beginning one might expect more for monotone decoding, while in the middle of the sentence, one might expect more reorderings.",
        "Table 11 shows when we add such boundary markups in our rules, an improvement of 0.33 BLEU points were obtained (56.46 v.s.",
        "56.13) on top of the already improved setups.",
        "A close check up showed that the sentence-begin/end markups significantly reduced the leading \"and\" (from Arabic word w#) in the decoding output.",
        "Also, the verb subject order under SBAR seems to be more like monotone with a leading pronoun, rather than the general strong reordering of moving verb after subject.",
        "Overall, our results showed that such boundary conditions are helpful for executing the correct reorderings.",
        "We conclude the investigation with full function a(7, fa), which leads to a BLEUr4n4 of 56.87 (cased BLEUr4n4c 55.16), a significant improvement of 1.77 BLEU point over a already strong baseline.",
        "We apply the setups for several other NW and WEB datasets to further verify the improvement.",
        "Shown in Table 12, we apply separately the operators for aand fa first, then combine them as the final results.",
        "Varied improvements were observed for different genres.",
        "On DEV10-NW, we observed 1.29 BLEU points improvement, and",
        "WB and DEV10-WB, respectively.",
        "The improvements for newwire are statistically significant.",
        "The improvements for weblog are, however, only marginally better.",
        "One possible reason is the parser quality for web genre is reliable, as our training data is all in newswire.",
        "Regarding to the individual operators proposed in this paper, we observed consistent improvements of applying them across all the datasets.",
        "The generative model in Eqn.",
        "3 leverages the operators further by selecting the best transformed tree form for executing the reorderings.",
        "To illustrate the advantages of the proposed grammar, we use a testing case with long distance word reordering and the source side parse trees.",
        "We compare the translation from a strong phrasal decoder (DTM2) (Ittycheriah and Roukos, 2007), which is one of the top systems in NIST-08 evaluation for Arabic-English.",
        "The translations from both decoders with the same training data (LM+TM) are in Table 13.",
        "The highlighted parts in Figure 3 show that, the rules on partial trees are effectively selected and applied for capturing long-distance word reordering, which is otherwise rather difficult to get correct in a phrasal system even with a MaxEnt reordering model."
      ]
    },
    {
      "heading": "6. Discussions and Conclusions",
      "text": [
        "We proposed a framework to learn models to predict how to transform an elementary tree into its simplified forms for better executing the word reorderings.",
        "Two types of operators were explored, including (a) transforming the trees via binarizations, grouping or deleting interior nodes to change the structures; and (b) neighboring boundary context to further disambiguate the reordering decisions.",
        "Significant improvements were observed on top of a strong baseline system, and consistent improvements were observed across genres; we achieved a cased BLEU of 55.16 for MT08-NW, which is significantly better than the officially reported results in NIST MT08 Arabic-English evaluations.",
        "Data",
        "MT08-NW",
        "MT08-WB",
        "Dev10-NW",
        "Dev10-WB",
        "Tr2Str",
        "55.01",
        "39.19",
        "37.33",
        "41.77",
        "elm2str+a",
        "55.84",
        "39.43",
        "38.02",
        "42.70",
        "elm2str+fa",
        "55.57",
        "39.60",
        "37.67",
        "42.54",
        "elm2str+a(7, fa)",
        "56.87",
        "39.82",
        "38.62",
        "42.75",
        "Figure 3: A testing case: illustrating the derivations from chart decoder.",
        "The left panel is source parse tree for the Arabic sentence โ the input to our decoder; the right panel is the English translation together with the simplified derivation tree and alignment from our decoder output.",
        "Each \"X\" is a nonterminal in the grammar rule; a \"Block\" means a phrase pair is applied to rewrite a nonterminal; \"Glue\" and \"Hiero\" means the unlabeled rules were chosen to explain the span as explained in ยง 3.4.3 ; \"Tree\" means a labeled rule is applied for the span.",
        "For instance, for the source span [1,10], a rule is applied on a partial tree withPV and NP-SBJ; for the span [18,23], a rule is backed off to an unlabeled rule (Hiero-alike); for the span [21,22], it is another partial tree of NPs.",
        "Within the proposed framework, we also presented several special cases including the translation boundaries for nonterminals in SCFG for translation.",
        "We achieved a high accuracy of 84.7% for predicting such boundaries using MaxEnt model on machine parse trees.",
        "Future works aim at transforming such non-projectable trees into projectable form (Eisner, 2003), driven by translation rules from aligned data(Burkett et al., 2010), and informative features form both the source and the target sides (Shen et al., 2008) to enable the system to leverage more isomorphic trees, and avoid potential detour errors.",
        "We are exploring the incremental decoding framework, like (Huang and Mi, 2010), to improve pruning and speed."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was partially supported by the Defense Advanced Research Projects Agency under contract No.",
        "HR0011-08-C-0110.",
        "The views and findings contained in this material are those of the authors and do not necessarily reflect the position or policy of the U.S. government and no official endorsement should be inferred.",
        "We are also very grateful to the three anonymous reviewers for their suggestions and comments.",
        "Src Sent",
        "qAl AlAmyr EbdAlrHmn bn EbdAlEzyz nA}b wzyr AldfAE AlsEwdy AlsAbq fy tSryH SHAfy An +h mtfA}l b# qdrp Almmlkp Ely AyjAd Hl l# Alm$klp .",
        "Phrasal Decoder Elm2Str+h(7, fh)",
        "prince abdul rahman bin abdul aziz , deputy minister of defense former saudi said in a press statement that he was optimistic about the kingdom 's ability to find a solution to the problem .",
        "former saudi deputy defense minister prince abdul rahman bin abdul aziz said in a press statement that he was optimistic of the kingdom 's ability to find a solution to the problem ."
      ]
    }
  ]
}
