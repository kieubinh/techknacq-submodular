{
  "info": {
    "authors": [
      "Ondřej Bojar"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W07-0735",
    "title": "English-to-Czech Factored Machine Translation",
    "url": "https://aclweb.org/anthology/W07-0735",
    "year": 2007
  },
  "references": [
    "acl-C04-1045",
    "acl-E03-1076",
    "acl-E06-1006",
    "acl-H05-1085",
    "acl-J03-1002",
    "acl-J92-4003",
    "acl-N03-2002",
    "acl-N06-2051",
    "acl-P02-1039",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P03-2041",
    "acl-P05-1033",
    "acl-P06-1122",
    "acl-P98-1035",
    "acl-P98-1080",
    "acl-W01-1407",
    "acl-W04-3250",
    "acl-W04-3251",
    "acl-W06-3102"
  ],
  "sections": [
    {
      "text": [
        "Ondfej Bojar Institute of Formal and Applied Linguistics UFAL MFF UK, Malostranske namesti 25 CZ-11800 Praha, Czech Republic boj ar@ufal.mff.cuni.cz",
        "This paper describes experiments with English-to-Czech phrase-based machine translation.",
        "Additional annotation of input and output tokens (multiple factors) is used to explicitly model morphology.",
        "We vary the translation scenario (the setup of multiple factors) and the amount of information in the morphological tags.",
        "Experimental results demonstrate significant improvement of translation quality in terms of BLEU."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Statistical phrase-based machine translation (SMT) systems currently achieve top performing results.Known limitations of phrase-based SMT include worse quality when translating to morphologically rich languages as opposed to translating from them (Koehn, 2005).",
        "One of the teams at the 2006 summer engineering workshop at Johns Hopkins University attempted to tackle these problems by introducing separate factors in SMT input and/or output to allow explicit modelling of the underlying language structure.",
        "The support for factored translation models was incorporated into the Moses open-source SMT system.",
        "In this paper, we report on experiments with English-to-Czech multi-factor translation.",
        "After a brief overview of factored SMT and our data (Sections 2 and 3), we summarize some possible translating scenarios in Section 4.",
        "Section 5 studies the level of detail useful for morphological representation and Section 6 compares the results to a setting with more data available, albeit out of domain.",
        "The second part (Section 7) is devoted to a brief analysis of MT output errors.",
        "Czech is a Slavic language with very rich morphology and relatively free word order.",
        "The Czech morphological system (Hajic, 2004) defines 4,000 tags in theory and 2,000 were actually seen in a big tagged corpus.",
        "(For comparison, the English Penn Treebank tagset contains just about 50 tags.)",
        "In our parallel corpus (see Section 3 below), the English vocabulary size is 35k distinct token types but more than twice as big in Czech, 83k distinct token types.",
        "To further emphasize the importance of morphology in MT to Czech, we compare the standard BLEU (Papineni et al., 2002) of a baseline phrase-based translation with BLEU which disregards word forms (lemmatized MT output is compared to lem-matized reference translation).",
        "The theoretical margin for improving MT quality is about 9 BLEU points: the same MT output scores 12 points in standard BLEU and 21 points in lemmatized BLEU."
      ]
    },
    {
      "heading": "2. Overview of Factored SMT",
      "text": [
        "In statistical MT, the goal is to translate a source (foreign) language sentence // = fi... fj ... fj into a target language (Czech) sentence c{ = c1... cj .. .cl.",
        "In phrase-based SMT, the assumption is made that the target sentence can be constructed by segmenting source sentence into phrases, translating each phrase and finally composing the target sentence from phrase translations, sf denotes the segmentation of the input sentence into K phrases.",
        "Among all possible target language sentences, we choose the sentence with the highest probability,",
        "In a log-linear model, the conditional probability of c{ being the translation of / J under the segmentation sf is modelled as a combination of independent feature functions •, •)... hM(•, •, •) describing the relation of the source and target sentences:",
        "The denominator in 2 is used as a normalization factor that depends on the source sentence / J and segmentation sf only and is omitted during maximization.",
        "The model scaling factors AM are trained either to the maximum entropy principle or optimized with respect to the final translation quality measure.",
        "Most of our features are phrase-based and we require all such features to operate synchronously on the segmentation sf and independently of neighbouring segments.",
        "In other words, we restrict the form of phrase-based features to:",
        "where /k represents the source phrase and c represents the target phrase k given the segmentation sK.",
        "In factored SMT, source and target words / and c are represented as tuples of F and C factors, resp., each describing a different aspect of the word, e.g. its word form, lemma, morphological tag, role in a verbal frame.",
        "The process of translation consists of decoding steps of two types: mapping steps and generation steps.",
        "If more steps contribute to the same output factor, they have to agree on the outcome, i.e. partial hypotheses where two decoding steps produce conflicting values in an output factor are discarded.",
        "A mapping step from a subset of source factors S C {1 ...F} to a subset of target factors T C {1... C} is the standard phrase-based model (see e.g. (Koehn, 2004a)) and introduces a feature in the following form:",
        "The conditional probability of /S, i.e. the phrase /k restricted to factors S, given CjT, i.e. the phrase c k restricted to factors T is estimated from relative frequencies: \\c)T) = N(/ S,cT)/N(cT) where N(/S, cT) denotes the number of co-occurrences of a phrase pair (/S, cT) that are consistent with the word alignment.",
        "The marginal count N(cT) is the number of occurrences of the target phrase cT in the training corpus.",
        "For each mapping step, the model is included in the log-linear combination in source-to-target and target-to-source directions: p(/ T\\cS) andp(cS\\/T).",
        "In addition, statistical single word based lexica are used in both directions.",
        "They are included to smooth the relative frequencies used as estimates of the phrase probabilities.",
        "A generation step maps a subset of target factors T1 to a disjoint subset of target factors T2, T1)2 C {1 ...C}.",
        "In the current implementation of Moses, generation steps are restricted to word-to-word correspondences:",
        "length(cfc)",
        "where is the i-th words in the k-th target phrase restricted to factors T. We estimate the conditional probability ^(c^ \\ c^) by counting over words in the target-side corpus.",
        "Again, the conditional probability is included in the log-linear combination in both directions.",
        "In addition to features for decoding steps, we include arbitrary number of target language models over subsets of target factors, T C {1... C}.",
        "Typically, we use the standard n-gram language model:",
        "While generation steps are used to enforce \"vertical\" coherence between \"hidden properties\" of output words, language models are used to enforce sequential coherence of the output.",
        "operationally, Moses performs a stack-based beam search very similar to Pharaoh (Koehn, 2004a).",
        "Thanks to the synchronous-phrases assumption, all the decoding steps can be performed during a preparatory phase.",
        "For each span in the input sentence, all possible translation options are constructed using the mapping and generation steps in a user-specified order.",
        "Low-scoring options are pruned already during this phase.",
        "once all translation options are constructed, Moses picks source phrases (all output factors already filled in) in arbitrary order, subject to a reordering limit, producing output in left-to-right fashion and scoring it using the specified language models exactly as Pharaoh does."
      ]
    },
    {
      "heading": "3. Data Used",
      "text": [
        "The experiments reported in this paper were carried out with the News Commentary (NC) corpus as made available for the SMT workshop of the ACL 2007 conference.",
        "The Czech part ofthe corpus was tagged and lem-matized using the tool by Hajic and Hladka (1998), the English part was tagged MXPOST (Ratnaparkhi, 1996) and lemmatized using the Morpha tool (Minnen et al., 2001).",
        "After some final cleanup, the corpus consists of 55,676 pairs of sentences (1.1M Czech tokens and 1.2M English tokens).",
        "We use the designated additional tuning and evaluation sections consisting of 1023, resp.",
        "964 sentences.",
        "In all experiments, word alignment was obtained using the grow-diag-final heuristic for symmetrizing GIZA++ (Och and Ney, 2003) alignments.",
        "To reduce data sparseness, the English text was lowercased and Czech was lemmatized for alignment estimation.",
        "Language models are based on the target side of the parallel corpus only, unless stated otherwise.",
        "We evaluate our experiments using the (lowercase, tokenized) BLEU metric and estimate the empirical confidence using the bootstrapping method described in Koehn (2004b).",
        "We report the scores obtained on the test section with model parameters tuned using the tuning section for minimum error rate training (MERT, (Och, 2003))."
      ]
    },
    {
      "heading": "4. Scenarios of Factored Translation English^Czech",
      "text": [
        "We experimented with the following factored translation scenarios.",
        "The baseline scenario (labelled T for translation) is single-factored: input (English) lowercase word forms are directly translated to target (Czech) lowercase forms.",
        "A 3-gram language model (or more models based on various corpora) checks the stream of output word forms.",
        "The baseline scenario thus corresponds to a plain phrase-based SMT system:",
        "English Czech lowercase -*■ lowercase +LM",
        "lemma lemma morphology morphology",
        "In order to check the output not only for wordlevel coherence but also for morphological coherence, we add a single generation step: input word forms are first translated to output word forms and each output word form then generates its morphological tag.",
        "Two types of language models can be used simultaneously: a (3-gram) LM over word forms and a (7-gram) LM over morphological tags.",
        "We used tags with various levels ofdetail, see section 5.",
        "We call this the \"T+C\" (translate and check) scenario:",
        "English Czech lowercase lemma morphology - lowercase – i+LM",
        "lemma morphology ■<-' +LM",
        "As a refinement of T+C, we also used T+T+C scenario, where the morphological output stream is constructed based on both output word forms and input morphology.",
        "This setting should reinforce correct translation of morphological features such as number of source noun phrases.",
        "To reduce the risk of early pruning, the generation step operationally precedes the morphology mapping step.",
        "Again, two types of language models can be used in this \"T+T+C\" scenario:",
        "The most complex scenario we used is linguistically appealing: output lemmas (base forms) and morphological tags are generated from input in two independent translation steps and combined in a single generation step to produce output word forms.",
        "The input English text was not lemmatized so we used English word forms as the source for producing Czech lemmas.",
        "The \"T+T+G\" setting allows us to use three types of language models.",
        "Trigram models are used for word forms and lemmas and 7-gram language models are used over tags:",
        "lowercase lemma morphology – ' +LM",
        "Table 1 summarizes estimated translation quality of the various scenarios.",
        "In all cases, a 3-gram LM is used for word forms or lemmas and a 7-gram LM for morphological tags.",
        "The good news is that multi-factored models always outperform the baseline T.",
        "Unfortunately, the more complex multi-factored scenarios do not bring any significant improvement over T+C.",
        "Our belief is that this effect is caused by search errors: with multi-factored models, more hypotheses get similar scores and future costs of partial hypotheses might be estimated less reliably.",
        "With the limited stack size (not more than 200 hypotheses of the same number of covered input words), the decoder may more often find suboptimal solutions.",
        "Moreover, the more steps are used, the more model weights have to be tuned in the minimum error rate training.",
        "Considerably more tuning data might be necessary to tune the weights reliably."
      ]
    },
    {
      "heading": "5. Granularity of Czech Part-of-Speech",
      "text": [
        "As stated above, the Czech morphological tag system is very complex: in theory up to 4,000 different tags are possible.",
        "In our T+T+C scenario, we experiment with various simplifications of the system to find the best balance between richness and robustness of the statistics available in our corpus.",
        "(The more information is retained in the tags, the more severe data sparseness is.)",
        "Full Czech positional tags are used.",
        "A tag consists of 15 positions, each holding the value of a morphological property (e.g. number, case or gender).",
        "POS+case (184 unique seen): We simplify the tag to include only part and subpart of speech (distinguishes also partially e.g. verb tenses).",
        "For nouns, pronouns, adjectives and prepositions , also the case is included.",
        "CNG01 (621 unique seen): CNG01 refines POS.",
        "For nouns, pronouns and adjectives we include not only the case but also number and gender.",
        "CNG02 (791 unique seen): Tag for punctuation is refined: the lemma of the punctuation symbol is taken into account; previous models disregarded e.g. the distributional differences between a comma and a question mark.",
        "Case, number and gender added to nouns, pronouns, adjectives, prepositions, but also to verbs and numerals (where applicable).",
        "• Tags for nouns, adjectives, pronouns and numerals describe the case, number and gender; the Czech reflexive pronoun se or si is highlighted by a special flag.",
        "• Tag for verbs describes subpart of speech, number, gender, tense and aspect; the tag includes a special flag if the verb was the auxiliary verb byt (to be) in any of its forms.",
        "• Tag for prepositions includes the case and also the lemma of the preposition.",
        "• Lemma included for punctuation, particles and interjections.",
        "• Tag for numbers describes the \"shape\" of the number (all digits are replaced by the digit 5 but number-internal punctuation is kept intact).",
        "The tag thus distinguishes between 4 or 5-digit numbers or the precision of floating point numbers.",
        "• Part of speech and subpart of speech for all other words.",
        "CNO (£NC CzEng",
        "Our results confirm improvement over the single-factored baseline.",
        "Detailed knowledge of the morphological system also proves its utility: by choosing the most relevant features of tags and lemmas but avoiding sparseness, we can improve on BLEU score by about 0.3 absolute over T+T+C with full tags."
      ]
    },
    {
      "heading": "6. More Out-of-Domain Data in T and T+C Scenarios",
      "text": [
        "In order to check if the method scales up with more parallel data available, we extend our training data using the CzEng parallel corpus (Bojar and Zabokrtsky, 2006).",
        "CzEng contains sentence-aligned texts from the European Parliament (about 75%), e-books and stories (15%) and open source documentation.",
        "By \"Baseline\" corpus we denote NC corpus only, by \"Large\" we denote the combination of training sentences from NC and CzEng (1070k sentences, 13.9M Czech and 15.5 English tokens) where in-domain NC data amounts only to 5.2% sentences.",
        "Figure 1 gives full details of our experiments with the additional data.",
        "We varied the scenario (T or T+C), the level of detail in the T+C scenario (full tags vs. CNG03) and the size of the training corpus.",
        "We extract phrases from either the in-domain corpus only (NC) or the mixed corpus (mix).",
        "We use either one LM per output factor, varying the corpus size (NC or mix), or two LMs per output factors with weights trained independently in the MERT procedure (weighted).",
        "Independent weights allow us to take domain difference into account, but we exploit this in the target LM only, not the phrases.",
        "Scenario Phrases from",
        "LMs",
        "BLEU",
        "T",
        "NC",
        "nc",
        "12.9±0.6",
        "T",
        "mix",
        "mix",
        "11.8±0.6",
        "T",
        "mix",
        "weighted",
        "11.8±0.6",
        "T+C CNG03",
        "NC",
        "NC",
        "13.7±0.7",
        "T+C CNG03",
        "mix",
        "mix",
        "13.1±0.7",
        "T+C CNG03",
        "mix",
        "weighted",
        "13.7±0.7",
        "T+C full tags",
        "NC",
        "NC",
        "13.6±0.6",
        "T+C full tags",
        "mix",
        "mix",
        "13.1±0.7",
        "T+C full tags",
        "mix",
        "weighted",
        "13.8±0.7",
        "The only significant difference is caused by the scenario: T+C outperforms the baseline T, regardless of corpus size.",
        "Other results (insignificantly) indicate the following observations:",
        "• Ignoring the domain difference and using only the mixed domain LM in general performs worse than allowing MERT to optimize LM weights for in-domain and generic data separately.",
        "• CNG03 outperforms full tags only in small data setting, with large data (treating the domain difference properly), full tags perform better."
      ]
    },
    {
      "heading": "7. Untreated Morphological Errors",
      "text": [
        "The previous sections described improvements gained on small data sets when checking morphological agreement using T+T+C scenario (BLEU raised from 12.9% to 13.9% or up to 14.2% with manually tuned tagset, CNG03).",
        "However, the best result achieved is still far below the margin of lem-matized BLEU (21%), as mentioned in Section 1.1.",
        "When we searched for the unexploited morphological errors, visual inspection of MT output suggested that local agreement (within 3-word span) is relatively correct but Verb-Modifier relations are often malformed causing e.g. a bad case for the Modifier.",
        "To quantify this observation we performed a micro-study of our best MT output using an intuitive metric.",
        "We checked whether Verb-Modifier relations are properly preserved during the translation of 15 sample sentences.",
        "The source text of the sample sentences contained 77 Verb-Modifier pairs.",
        "Table 3 lists our observations on the two members in each Verb-Modifier pair.",
        "We see that only 56% of verbs are translated correctly and 79% of nouns are translated correctly.",
        "The system tends to skip verbs quite often (27% of cases).",
        "Translation of Verb Modifier",
        "More importantly, our analysis has shown that even in cases where both the Verb and the Modifier are lexically correct, the relation between them in Czech is either non-grammatical or meaning-disrupted in 56% of these cases.",
        "Commented samples of such errors are given in Figure 2 below.",
        "The first sample shows that a strong language model can lead to the choice of a grammatical relation that nevertheless does not convey the original meaning.",
        "The second sample illustrates a situation where two correct options are available but the system chooses an inappropriate relation, most probably because of backing off to a generic pattern verb-noun\"^\"\"\"*™.",
        "This pattern is quite common for expressing the object role of many verbs (such as vydat, see Correct option 2 in Figure 2), but does not fit well with the verb vybehnout.",
        "While the target-side data may be rich enough to learn the generalization vybehnout-s-instr, no such generalization is possible with language models over word forms or morphological tags only.",
        "The target side data will be hardly ever rich enough to learn this particular structure in all correct morphological and lexical variants: vybehl-s-reklamou, vybehla-s-reklamami, vybehl-s-prohlasenim, vybehli-s-oznamenim, .... We would need a mixed model that combines verb lemmas, prepositions and case information to properly capture the relations.",
        "Unfortunately, our preliminary experiments that made use of automatic Czech dependency parse trees to construct a factor explicitly highlighting the Verb (lexicalized) its Modifiers (case and the lemma of the preposition, if present) and boundary symbols such as punctuation or conjunctions and using a dummy token for all other words did not bring any improvement over the baseline.",
        "A possible reason is that we employed only a standard 7-gram language model to this factor.",
        "A more appropriate treatment is to disregard the dummy tokens in the language model at all and use an n-gram language model that looks at last n – 1 non-dummy items."
      ]
    },
    {
      "heading": "8. Related Research",
      "text": [
        "Class-based LMs (Brown et al., 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario.",
        "Given the small differences in all T+... scenarios' performance, class-based LM might bring equivalent improvement.",
        "Yang and Kirchhoff (2006) have recently documented minor BLEU improvement using factored LMs in single-factored SMT to English.",
        "The multi-factored approach to SMT of Moses is however more general.",
        "Many researchers have tried to employ morphology in improving word alignment techniques (e.g. (Popovic and Ney, 2004)) or machine translation quality (NieBen and Ney (2001), Koehn and Knight (2003), Zollmann et al.",
        "(2006), among others, for various languages; Goldwater and McClosky (2006) for Czech), however, they focus on translating from the highly inflectional language.",
        "Durgar El-Kahlout and Oflazer (2006) report preliminary experiments in English to Turkish single-factored phrase-based translation, gaining significant improvements by splitting root words and their morphemes into a sequence of tokens.",
        "In might be interesting to explore multi-factored scenarios for different Turkish morphology representation suggested the paper.",
        "de Gispert et al.",
        "(2005) generalize over verb forms and generate phrase translations even for unseen target verb forms.",
        "The T+T+G scenario allows a similar extension if the described generation step is replaced by a (probabilistic) morphological generator.",
        "Nguyen and Shimazu (2006) translate from English to Vietnamese but the morphological richness of Vietnamese is comparable to English.",
        "In fact the Vietnamese vocabulary size is even smaller than English vocabulary size in one of their corpora.",
        "The observed improvement due to explicit modelling of morphology might not scale up beyond small-data setting.",
        "As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages."
      ]
    },
    {
      "heading": "9. Conclusion",
      "text": [
        "We experimented with multi-factored phrase-based translation aimed at improving morphological coherence in MT output.",
        "We varied the setup of additional factors (translation scenario) and the level of detail in morphological tags.",
        "Our results on English-to-Czech translation demonstrate significant improvement in BLEU scores by explicit modelling of morphology and using a separate morphological language model to ensure the coherence.",
        "To our knowledge, this is one of the first experiments showing the advantages of using multiple factors in",
        "MT.",
        "Verb-modifier errors have been studied and a factor capturing verb-modifier dependencies has been proposed.",
        "Unfortunately, this factor has yet to bring any improvement."
      ]
    },
    {
      "heading": "10. Acknowledgement",
      "text": [
        "The work on this project was partially supported by the grants Collegium Informaticum GACR 201/05/H014, grants No.",
        "ME838 and",
        "GA405/06/0589 (PIRE), FP6-IST-5-034291-STP (Euromatrix), and NSF No.",
        "0530118.",
        "Input:",
        "brokerage",
        "firms rushed out ads ...",
        "MT Output:",
        "brokerske",
        "firmy",
        "vybehl",
        "reklamy",
        "Gloss:",
        "brokerage",
        "firmspi./em",
        "ransg.masc",
        "i pl.voc,sg.gen pi ,nom,pl .acc",
        "Correct option 1:",
        "brokerske",
        "firmy",
        "vybehly",
        "s reklamamipLirastr",
        "Correct option 2:",
        "brokerske",
        "firmy",
        "vydaly",
        "reklamypL(lcc"
      ]
    }
  ]
}
