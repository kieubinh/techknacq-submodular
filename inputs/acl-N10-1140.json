{
  "info": {
    "authors": [
      "Michel Galley",
      "Christopher D. Manning"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1140",
    "title": "Accurate Non-Hierarchical Phrase-Based Translation",
    "url": "https://aclweb.org/anthology/N10-1140",
    "year": 2010
  },
  "references": [
    "acl-C08-1064",
    "acl-D07-1077",
    "acl-D07-1104",
    "acl-H05-1095",
    "acl-J04-4002",
    "acl-J06-4004",
    "acl-J07-2003",
    "acl-J97-3002",
    "acl-N01-1019",
    "acl-N03-1017",
    "acl-N04-4026",
    "acl-N06-1014",
    "acl-P03-1021",
    "acl-P05-1032",
    "acl-P06-1098",
    "acl-P06-1123",
    "acl-P07-2045",
    "acl-P08-1114",
    "acl-W05-0908",
    "acl-W05-1507",
    "acl-W09-2303",
    "acl-W09-3805"
  ],
  "sections": [
    {
      "text": [
        "A principal weakness of conventional (i.e., non-hierarchical) phrase-based statistical machine translation is that it can only exploit continuous phrases.",
        "In this paper, we extend phrase-based decoding to allow both source and target phrasal discontinuities, which provide better generalization on unseen data and yield significant improvements to a standard phrase-based system (Moses).",
        "More interestingly, our discontinuous phrase-based system also outperforms a state-of-the-art hierarchical system (Joshua) by a very significant margin (+1.03 BLEU on average on five Chinese-English NIST test sets), even though both Joshua and our system support discontinuous phrases.",
        "Since the key difference between these two systems is that ours is not hierarchical – i.e., our system uses a string-based decoder instead of CKY, and it imposes no hard hierarchical reordering constraints during training and decoding – this paper sets out to challenge the commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases.",
        "By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings.",
        "Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003).",
        "However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; Callison-Burch et al., 2005), phrase pairs applicable at test time tend to be fairly short.",
        "Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne ... pas) separately instead of jointly.",
        "We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase.",
        "We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases.",
        "Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality.",
        "Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding.",
        "With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average.",
        "We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities.",
        "Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose.",
        "Recent work (Wellington et al., 2006; S0gaard and Kuhn, 2009; S0gaard and",
        "target: bm d„ ap Ct ak b, am b„ b, am °„",
        "Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (S0gaard and Kuhn, 2009); (iii) \"bonbon\" (Simard et al., 2005).",
        "Standard phrase-based decoders cope with (i), but not (ii) and (iii).",
        "Our phrase-based decoder handles all three cases.",
        "do not want X anymore I do not want X anymore do not want ... anymore",
        "I do not want ... anymore",
        "Figure 2: Due to hierarchical constraints, Hiero only extracts two discontinuous phrases from the alignment on the left, but our system extracts 11 (only 6 are shown).",
        "Wu, 2009) has questioned the empirical adequacy of 2-SCFG systems, which are unable to perform any of the transformations shown in Fig. 1.",
        "For instance, using manually-aligned bitexts for 12 European languages pairs, S0gaard and Kuhn found that inside-out and cross-serial discontinuous translation units (DTU) account for 1.6% (Danish-English) to 18.6% (French-English) of all translation units.",
        "The empirical adequacy of 2-SCFG models would presumably be lower with automatically-aligned texts and if the study also included non-European languages.",
        "In contrast, phrase-based systems can properly handle inside-out alignments when used with a reasonably large distortion limit, and all configurations in Fig. 1 are accounted for in our system.",
        "In our experiments, we show that our discontinuous phrase-based system outperforms Joshua (Li et al., 2009), a reimplementation of Hiero, by 1.03 BLEU points and 1.19 TER points on average.",
        "A final compelling advantage of our decoder is that it preserves the computational efficiency of Moses (i.e., time complexity is linear when a distortion limit is used), while SCFG decoders have a running time that is at least cubic (Huang et al., 2005)."
      ]
    },
    {
      "heading": "2. Discontinuous Phrase Extraction",
      "text": [
        "In this section, we introduce the extraction of discontinuous phrases for phrase-based MT.",
        "We will describe a decoder that can handle such phrases in the next section.",
        "Formally, we define the discontinuous phrase-based translation problem as follows.",
        "We are given a source sentence f = f J = fi,...,fj,..., fj, which is to be translated into a target sentence e = e{ = ei,..., ej,..., fi.",
        "Unlike (Och and Ney, 2004), in this work, a sentence pair may be segmented into phrases that are not continuous, so each phrase is characterized by a coverage set, i.e., a set of word indices.",
        "Assuming that the sentence pair (f, e) is decomposed into K discontinuous phrases, we use s = (s1,..., sK) and t = (t1, tK ) to respectively represent the decomposition of the source and target sentence into K word subsets that are complementary and non-overlapping.",
        "A pair of coverage sets (sk, tk) is said to be consistent with the word alignment A if the following condition holds:",
        "For continuous phrases, finding all phrase pairs that satisfy this condition can be done in O(nm) time (Och and Ney, 2004), where n is the length of the sentence and m is the maximum phrase length.",
        "The set of discontinuous phrases is exponential in the maximum span length, so phrase extraction must be tailored to a specific text (e.g., a given test sentence) for relatively large m values.",
        "Lopez (2007) presents an efficient solution using suffix arrays for finding all discontinuous phrases of the training data that are relevant to a given test sentence or test set.",
        "A complete overview of this technique is beyond the scope of this paper, though we will mention that it solves a phrase collocation problem by efficiently identifying collocated continuous phrases of the training data that also happen to be collocated in the test sentence.",
        "While this technique was primarily designed for extracting hierarchical phrases for Hiero (Chiang, 2007), it can readily be applied to the problem of finding all discontinuous phrases for our phrase-based system.",
        "Indeed, the suffix-array technique gives us for each input sentence a list of relevant source coverage sets.",
        "For each such sk, we can easily enumerate each tk satisfying Eq.",
        "1.",
        "The",
        "translation options (subset):",
        "state expansions:",
        "arrangements ... made",
        "Figure 3: A particular decoder search path for the input shown at the top.",
        "Note that this example contains a cross-serial DTU (which interleaves arrangements... made with are ... for this), a structure Hiero can't handle.",
        "Beam search algorithm."
      ]
    },
    {
      "heading": "1. create initial hypothesis ; add it to S g",
      "text": []
    },
    {
      "heading": "5. for each H new in consolidate (H j n )",
      "text": []
    },
    {
      "heading": "6. add H new to S g",
      "text": [
        "7 if j<J then"
      ]
    },
    {
      "heading": "10. u := first uncovered source word of H old",
      "text": []
    },
    {
      "heading": "11. for m = u to u + distortionLimit",
      "text": []
    },
    {
      "heading": "12. for each (sk,tk) in translation.options {m)",
      "text": []
    },
    {
      "heading": "13. if source s k does not overlap H old then",
      "text": [
        "14 Hnew :=combine(Hoid,Sk,tk)"
      ]
    },
    {
      "heading": "16. return arg max(S g )",
      "text": [
        "key difference between Hiero-style extraction and our work is that Eq.",
        "1 is the only constraint.",
        "Since our decoder doesn't impose hierarchical constraints, we exploit all discontinuous phrase pairs consistent with the word alignment, which often includes sound translations not captured by Hiero (e.g., ne... plus translating to not... anymore in Fig. 2)."
      ]
    },
    {
      "heading": "3. Decoder",
      "text": [
        "The core engine of our phrase-based system, Phrasal (Cer et al., 2010), is a multi-stack decoder similar to Moses (Koehn, 2004), which we extended to support variable-size gaps in the source and the target.",
        "In Moses, partial translation hypotheses are arranged into different stacks according to the total number of input words they cover.",
        "At every translation step, stacks are pruned using partial translation cost and a lower bound on the estimated future cost.",
        "Pruning is implemented using both threshold and histogram pruning, and Moses allows for hypothesis recombination between hypotheses that are indistinguishable according to the underlying models.",
        "The key difference between Moses and our system is that, in order to account for target discontinuities, phrases that contains gaps in the target are appended to a partial translation hypothesis in multiple steps.",
        "Specifically, each translation hypothesis in our decoder is not only represented as a translation prefix and a coverage set as in Moses, but it also contains a set of isolated phrases (shown in italic in Fig. 3) that must be added to the translation at some later time.",
        "For instance, the figure shows how the phrase pair (ff&^ffl, arrangements ... made) is being added to a partial translation.",
        "The prefix (arrangements) is immediately appended to form the hypothesis (he said arrangements), and the isolated phrase (made) is stored for later use.",
        "A beam search algorithm for discontinuous phrase-based MT is shown in Table 1.",
        "Pruning is done implicitly in the table to avoid cluttering the pseudo-code.",
        "The algorithm handles 2 J + 1 stacks Sg, Sg,..., Sg and Sf,..., Sj, where each stack may contain up to N hypotheses Hji,...,HjN.",
        "The main loop of the algorithm alternates two stages: grow (lines 7-15) and consolidate (lines 3-6).",
        "The grow stage is similar to standard phrasebased MT: we take a hypothesis Hjn from Sg and combine it with a translation option (sk, tk), which yields a new hypothesis that is added to stack Sj+l(where l = |sk|).",
        "The second stage, consolidate, lets the decoder select any number of isolated phrases (not necessarily all, and possibly zero) and append them in any order at the end of the current translation.",
        "Consolidation operations are marked with stars in the figure (for simplicity, the figure does not display consolidations that keep hypotheses unchanged).",
        "We limit the number of isolated phrases to 4, which is generally enough to account for most transformations seen in the data.",
        "Any hypothesis in the last beam SJg is automatically discarded if it contains any isolated phrase.",
        "make",
        "arrangements",
        "he said",
        "arrangements",
        "made",
        "are",
        "for this | made",
        "* made",
        "for this",
        "* for this",
        "visit",
        "oo-------",
        "oo-----oo",
        "ooooo – oo",
        "ooooo--oo",
        "ooooo--oo",
        "ooooooooo",
        "score = -1.3",
        ">*",
        "score = -3.2",
        ">*",
        "score = -4.8",
        "H",
        "score = -6.1",
        "score = -7.2",
        "score = -8.5",
        "One last difference with standard decoders is that we also handle source discontinuities.",
        "This problem is a known instance of MT by pattern matching (Lopez, 2007), which we already mentioned in the previous section.",
        "The function transla-tion_options(m) of Table 1 returns the set of options applicable at position m using this pattern matching algorithm.",
        "Since this function is invoked a large number of times, it is important to precompute its return values for each m prior to decoding."
      ]
    },
    {
      "heading": "4. Features",
      "text": [
        "Our system incorporates the same eight baseline features of Moses: two relative-frequency phrase translation probabilities p(e|f ) andp(f |e), two lexically-weighted phrase translation probabilities (Koehn et al., 2003) lex(e|f ) and /ex(f |e), a language model probability, word penalty, phrase penalty, and linear distortion, and we optionally add 6 lexicalized reordering features as computed in Moses.",
        "Our computation of linear distortion is different from the one in Moses, since we need to account for discontinuous phrases.",
        "We found that it is crucial to penalize discontinuous phrases that have relatively long gaps.",
        "Hence, in our computation of",
        "different stacks depending on the number of isolated phrases, we have not found various implementations of this idea to work better than the algorithm described here.",
        "linear distortion, we treat continuous subphrases of each discontinuous phrase as if they were continuous phrases on their own.",
        "Specifically, let s = (s i, sL) be the list of L (maximal) continuous subphrases of the K source phrases (L > K) selected for a given translation hypothesis.",
        "Subphrases in s are enumerated according to their order in the target language, which may be different from the source-side ordering.",
        "We then compute the linear distortion between pair of successive elements (s^ si+i) as follows:",
        "where the superscripts fir t and la t respectively refer to source position of the first and last word of a given subphrase.",
        "Fig.",
        "4 shows an example of how distortion is computed for phrases (s i, s2, s3), including the discontinuous phrase s2 split into three continuous subphrases.",
        "In practice, we compute intra-phrase (shown with thin arrows in the figure) and inter-phrase linear distortion separately in order to produce two distinct features, since translation tends to improves when the intra-phrase cost has a lower feature weight.",
        "Finally, we add two features that are not present in Moses.",
        "First, we penalize target discontinuities by including a feature that is the sum of the lengths of all target gaps.",
        "The second feature is the count of discontinuous phrases that are in configurations (cross-serial DTU (S0gaard and Kuhn, 2009) and \"bonbon\" (Simard et al., 2005)) that can't be handled by 2-SCFG systems.",
        "The advantage of such features is two-fold.",
        "First, similarly to hierarchical systems, they prevent many distorted reorderings that are unlikely to correspond to quality translations.",
        "Second, it imposes soft rather than hard constraints, which means that the decoder is entirely free to violate hierarchical constraints when these violations are supported by other features."
      ]
    },
    {
      "heading": "5. Experimental Setup",
      "text": [
        "Three systems are evaluated in this paper: Moses (Koehn et al., 2007), Joshua (Li et al., 2009) - a reimplementation of Hiero, and our phrase-based system.",
        "We made our best attempts to make our system comparable to Moses.",
        "That is, when no discontinuous phrases are provided to our system, it generates an output that is almost identical to Moses (only about 1% of translations differ on average).",
        "In both systems, we use the default settings of Moses, i.e., we set the beam size to 200, the distortion limit to 6, we limit to 20 the number of target phrases that are loaded for each source phrase, and we use the same default eight features of Moses.",
        "We use version 1.3 of Joshua with its default settings.",
        "Both Moses and our system are evaluated with and without lexical-ized reordering (Tillmann, 2004).",
        "We believe it to be fair to compare Joshua against phrase-based systems that exploit lexicalized reordering, since Hi-ero's hierarchical rules are also lexically sensitive.",
        "The language pair for our experiments is Chinese-to-English.",
        "The training data consists of about 28 million English words and 23.3 million Chinese words drawn from various news parallel corpora distributed by the Linguistic Data Consortium (LDC).",
        "In order to provide experiments comparable to previous work, we used the same corpora as (Wang et al., 2007).",
        "We performed word alignment using a cross-EM word aligner (Liang et al., 2006).",
        "For this, we ran two iterations of IBM Model 1 and two HMM iterations.",
        "Finally, we generated a symmetric word alignment from cross-EM Viterbi alignment using the Moses grow-diag heuristic in the case Moses and our system.",
        "In the case of Joshua, we used the grow-diag-final heuristic since this gave better results.",
        "In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections",
        "X1UX2V to X2UVXl.",
        "of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data.",
        "This data represents a total of about 700 million words.",
        "We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets.",
        "The language model was smoothed with the modified Kneser-Ney algorithm as implemented in SRILM (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data.",
        "For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence.",
        "We used the 1664 sentences of MT06 for tuning and development and all other sets for testing.",
        "Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Pap-ineni et al., 2001).",
        "Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions.",
        "We used n-best lists of size 200.",
        "In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased)."
      ]
    },
    {
      "heading": "6. Results",
      "text": [
        "We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section.",
        "Fig.",
        "5 shows translations of our development set MT06, which were selected because our system makes a crucial use of discontinuous phrases.",
        "In the first example, the Chinese input contains Éi ...Bf, which typically translates as when.",
        "Lacking an entry for the input phrase in its phrase table, Moses is unable to translate this segment appropriately, and must instead split this phrase to generate the translation when the right was deprived of, where Bf is translated into of.",
        "This is evidently a poor translation.",
        "Conversely, our system uses a discontinuous phrase to translate Éi ...Bf, and translates the intervening words separately.",
        "The remaining three translations all contain cross-serial DTUs (S0gaard and Kuhn, 2009) and thus would be difficult to generate using 2-SCFG systems.",
        "The second example motivates the idea",
        "Reference: Under such circumstances, when the right of existence was deprived, the only way remaining was to overthrow the existing dynasty by force and try to replace it.",
        "in this kind case",
        "under such circumstan Joshua: Under such circumstances, when life be deprived, can only resort to violence to overthrow the current dynasty, trying to replace, Moses: Under such circumstances, when the right was deprived of, can only adopt the means of violence, in an attempt to overthrow the present dynasty replaced, This work: Under such circumstances, when he was deprived of the right to life, it can only resort to violence in an attempt to overthrow the current dynasty replaced,",
        "right was deprive when only can use violence",
        "Reference: CCP organization ministry demands to further enlarge strength of supervision of leading cadres and cadre selection and appointment Joshua: Department demands further intensify supervision over the work of selecting and appointing leading cadres, and intensify Moses: The central organization department, called on leading cadres, further increase the intensity of supervision over work of selecting and appointing cadres.",
        "This work: The central organization department has called for further increase the intensity of supervision of leading cadres and the work of selecting and appointing cadres.",
        "request further increase to leading cadres and cadre selection appointment work of supervision intensity",
        "Reference: The government will take all possible measures to prevent similar incidents from happening in the future.",
        "government will",
        "Joshua: Government will take all measures to prevent the re-occurrence of similar incidents in the future.",
        "Moses: The government will take all measures to prevent the occurrence of similar incidents in the future.",
        "This work: The government will take all measures to prevent similar incidents from happening again in the future.",
        "measure to prevent future again happen similar incidents",
        "Reference: He also said that Joshua: He also said that Moses: He also said that the This work: He also said that the arrangements are being now is making arrange-current visit is to make ar- the current arrangements are made now for the visits.",
        "ments for this visit.",
        "rangements.",
        "made for the visit.",
        "Figure 5: Actual translations produced by Joshua, Moses, and our system.",
        "For our system, we also display phrase alignments, including discontinuous phrase alignments.",
        "Results for these three systems here are displayed in rows 2, 4, and 8 of Table 2.",
        "The thick blue arrows represent alignments between discontinuous phrases, while red segmented arrows align continuous phrases.",
        "i|/r*z",
        "the central organization department",
        "has called for further",
        "increase the intensity of",
        "supervision of leading cadres",
        "and\".",
        "?",
        "Ä -V)",
        "m it",
        "\\ \\ 1 \\ \\ 1",
        "1",
        "l",
        "i",
        "I",
        "the government",
        "will",
        "take all",
        "measures to",
        "prevent similar",
        "incidents from happening again",
        "in the future .",
        "fill 2",
        "i# ïfiÈ IEiË & – ist]",
        "fFi+i",
        "he also",
        "said that the current arrangements are",
        "made for the",
        "visit .",
        "Table 2: Our system compared again conventional and hierarchical phrase-based MT (Moses and Joshua).",
        "using uncased BLEUr4n4[%] and TER[%].",
        "LexR indicates whether lexicalized reordering is enabled or not.",
        "We use randomization tests (Riezler and Maxwell, 2005) to determine significance of our best results (row 8) against Joshua (row 2) and Moses (row 4): differences marked in bold are significant at the p < .01 level.",
        "that larger translation units, including discontinuous phrases, lead to better translations.",
        "The reference includes the translation enlarge strength of supervision of leading cadres, and our system is able to produce a translation that is almost identical (increase the intensity ofsupervision ofleading cadres) using only two phrases, pulling together input words that are fairly far apart in the sentence.",
        "The third Chinese sentence has a word order quite different from English, but our decoder flexibly reorders it in a manner that can't be handled with SCFG decoders to give a word order (prevent similar events from happening) that matches the one in the reference.",
        "The last Chinese sentence includes the topicalization word (for), which indicates the input sentence has no subject.",
        "One way to properly handle this translation is to turn the sentence into a passive in English (as in the reference), a transformation our system does, thanks to its support for complex reorderings.",
        "Our main results are displayed in Table 2.",
        "First, Joshua systematically outperforms the Moses baseline (+0.82 BLEU point and -0.92 TER point on average), but performance of the two is about the same when Moses incorporates lexicalized reordering.",
        "This finding is consistent with previous work (Lopez, 2008).",
        "The results of our system displayed in rows 5-8 demonstrate that our system consistently outperforms Moses, whether they both use lexicalized reordering or not.",
        "The performance of our best system – i.e., with lexicalized reordering and both source and target gaps – is significantly better than the best Moses system (+0.77 BLEU and – 1.53 TER).",
        "While the performance of our system without lexicalized reordering is close to that of Joshua, our system with lexicalized reordering significantly outperforms Joshua (p < .01) in 9 out of 10 evaluations.",
        "The single experiment where our improvement over Hiero is insignificant (i.e., BLEU on MT08) is mainly affected by a discrepancy of length (our brevity penalty on MT08 is 0.92).",
        "It is interesting to notice that our system allowing phrasal discontinuities only on the source (row 5) performs almost as well as the system that allows them on both sides (row 7).",
        "For instance, while source discontinuities improve performance by 0.7 BLEU point on MT06, further enabling target discontinuities only raises performance by a mere 0.09 BLEU point.",
        "This naturally raises the question of whether our support for target gaps is ineffective, or whether target-discontinuous phrases are somewhat superfluous to the MT task.",
        "While it is certainly difficult to either confirm or deny the latter hypothesis, we can at least compare our handling of target-discontinuous phrases with hierarchical systems.",
        "In one additional set of experiments, we removed target-discontinuous phrases in Joshua prior to MERT and test time.",
        "Specifically, we removed all hierarchical phrases whose target side has the form uXv, uXvX, XuXv, and uXvXw, and only allowed rules whose target side has the form uX, Xu, XuX, XXu, or uXX.",
        "After this filtering, we found that target-discontinuous phrases in Joshua are also not crucial to its performance, since their removal only caused a drop of 0.2 BLEU point (row 1) and almost no change in terms of TER.",
        "We speculate that using target discontinuous phrases is more diffi-",
        "MT06 (tune)",
        "MT03",
        "MT04",
        "MT05",
        "MT08",
        "ALL",
        "System",
        "Gaps",
        "LexR",
        "BLEU",
        "TER",
        "BLEU",
        "TER",
        "BLEU",
        "TER",
        "BLEU",
        "TER",
        "BLEU",
        "TER",
        "BLEU",
        "TER",
        "1",
        "hierarchical",
        "src",
        "yes",
        "33.55",
        "58.04",
        "33.25",
        "59.73",
        "36.03",
        "58.92",
        "32.03",
        "61.11",
        "26.30",
        "61.30",
        "31.70",
        "58.21",
        "2",
        "(Joshua)",
        "src+tgt",
        "yes",
        "33.84",
        "58.11",
        "33.47",
        "59.85",
        "36.10",
        "58.82",
        "32.17",
        "61.20",
        "26.61",
        "61.21",
        "31.90",
        "58.22",
        "3",
        "phrase-based",
        "no",
        "no",
        "33.17",
        "59.24",
        "32.60",
        "60.80",
        "35.38",
        "59.55",
        "31.15",
        "62.43",
        "25.56",
        "61.98",
        "31.08",
        "59.14",
        "4",
        "(Moses)",
        "no",
        "yes",
        "34.25",
        "58.23",
        "33.72",
        "60.42",
        "36.37",
        "59.18",
        "32.49",
        "61.80",
        "26.70",
        "61.48",
        "32.16",
        "58.56",
        "5",
        "discontinuous phrase-based (this work)",
        "src",
        "no",
        "33.77",
        "58.56",
        "33.20",
        "60.42",
        "36.17",
        "59.13",
        "31.75",
        "61.62",
        "25.99",
        "61.47",
        "31.68",
        "58.60",
        "6",
        "tgt",
        "no",
        "33.27",
        "58.98",
        "32.95",
        "60.42",
        "35.41",
        "59.35",
        "31.08",
        "62.45",
        "25.69",
        "61.71",
        "31.17",
        "58.93",
        "7",
        "src+tgt",
        "no",
        "33.86",
        "58.26",
        "33.32",
        "60.02",
        "36.36",
        "58.56",
        "31.87",
        "61.35",
        "26.13",
        "61.29",
        "31.81",
        "58.25",
        "8",
        "src+tgt",
        "yes",
        "35.00",
        "56.85",
        "34.96",
        "57.97",
        "37.44",
        "57.61",
        "33.39",
        "59.92",
        "26.74",
        "60.51",
        "32.93",
        "57.03",
        "Improvement over hierarchical",
        "+1.16",
        "1.26",
        "+1.49",
        "1.88",
        "+1.34",
        "1.21",
        "+1.22",
        "1.28",
        "+0.13",
        "-0.70",
        "+1.03",
        "1.19",
        "Improvement over phrase-",
        "jased",
        "+0.75",
        "1.38",
        "+1.24",
        "2.45",
        "+1.07",
        "-1.57",
        "+0.90",
        "1.88",
        "+0.04",
        "-0.97",
        "+0.77",
        "1.53",
        "Number of sentences",
        "1664",
        "919",
        "1788",
        "1082",
        "1357",
        "6810",
        "# of English words per phrase",
        "cult, since it represents a generation rather than just a matching problem.",
        "In this paper, we have also argued that a main benefit of discontinuous phrases – and particularly source-discontinuous phrases – is that the decoder is allowed to use larger translation units than when restricted to continuous phrases.",
        "This claim is confirmed in Fig. 6.",
        "We find that our decoder makes effective use of the extended set of translation options at its disposal: While the Moses baseline translates MT06 with an average 1.73 words per phrase, adding support for discontinuities increases this average to 2.16, and reduces by 43% the use of single word phrases.",
        "On MT06, 53% of the translated sentences produced by our best system use at least one source-discontinuous phrase, and 9% of them exploit one or more target-discontinuous phrases."
      ]
    },
    {
      "heading": "7. Related Work",
      "text": [
        "The main goal of this paper is to show that discontinuous phrases can greatly improve the performance of phrase-based systems.",
        "While some of the most recent phrase-based systems (Chiang, 2007; Watanabe et al., 2006) exploit context-free decoding algorithms (CKY, Earley, etc.)",
        "to cope with discontinuities, our system preserves the simplicity and speed of conventional phrase-based decoders, and in particular does not build any intermediate tree structure, does not impose any hard reordering constraints other than the distortion limit, and still achieves translation performance that is superior to that of a state-of-the-art hierarchical system.",
        "A few previous non-hierarchical systems have also exploited phrasal discontinuities.",
        "The most notable previous attempt to incorporate gaps is described in (Simard et al., 2005).",
        "Simard et al.",
        "presents an extension to Moses that allows gaps in both source and target phrases, though each of their gap symbols must span exactly one word.",
        "This fact makes decoding simpler, since the position of all target words in a translation hypothesis is known as soon as the hypothesis is laid down, but fixed-size discontinuous phrases are less general and increase sparsity.",
        "By comparison, our gaps may span any number of words, so we have an increased ability to flexibly match the input sentence effectively.",
        "(Crego and Yvon, 2009) also handles gaps, though this work is applicable to an n-gram-based SMT framework (Marioo et al., 2006), which is fairly different from the phrase-based framework."
      ]
    },
    {
      "heading": "8. Conclusions",
      "text": [
        "In this paper, we presented a generalization of conventional phrase-based decoding to handle discontinuities in both source and target phrases.",
        "Our system significantly outperforms Moses and Joshua, two standard implementations of conventional and hierarchical phrase-based decoding.",
        "We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that this turns out to also be the case with the hierarchical phrases of Joshua.",
        "In future work, we plan to extend the parameterization of phrase-based lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (Marton and Resnik, 2008; Chiang et al., 2009).",
        "The discontinuous phrase-based MT system described in this work is part of Phrasal, an open-source phrase-based system available for download at http://nlp.stanford.edu/software/phrasal."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The authors thank three anonymous reviewers, Dan Jurafsky, Spence Green, Steven Bethard, Daniel Cer, Chris Callison-Burch, and Pi-Chuan Chang for their helpful comments.",
        "This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM.",
        "The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.",
        "■",
        "■",
        "■ Mos",
        "es",
        "■",
        "c",
        "1 this",
        "work"
      ]
    }
  ]
}
