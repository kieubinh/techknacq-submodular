{
  "info": {
    "authors": [
      "Heng Li",
      "Yuan Dong",
      "Xinnian Mao",
      "Haila Wang",
      "Wu Liu"
    ],
    "book": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-I05-3022",
    "title": "Chinese Word Segmentation in FTRD Beijing",
    "url": "https://aclweb.org/anthology/I05-3022",
    "year": 2005
  },
  "references": [],
  "sections": [
    {
      "text": [
        "wu.liu@francetelecom .com.cn"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a word segmentation system in France Telecom R&D Beijing, which uses a unified approach to word breaking and OOV identification.",
        "The output can be customized to meet different segmentation standards through the application of an ordered list of transformation.",
        "The system participated in all the tracks of the segmentation bakeoff -- PK-open, PK-closed, AS-open, AS-closed, HK-open, HK-closed, MSR-open and MSR-closed -- and achieved the state-of-the-art performance in MSR-open, MSR-close and PK-open tracks.",
        "Analysis of the results shows that each component of the system contributed to the scores."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The development of the Chinese word segmentation system presented in this bakeoff began in Feb. this year, and will last for one year with the support of the ILAB Beijing initial project within France Telecom R&D. Although the project last only half year by now, the main components of the system has been implemented, including code identification and conversion, basic segmentation, factoid detection, morphological analysis, name entity identification, segmentation standards adaptor, except the components of code identification and conversion and segmentation standards adaptors, other components are integrated in a statistical framework of n-gram language model."
      ]
    },
    {
      "heading": "2 System Description",
      "text": []
    },
    {
      "heading": "2.1 Code identification and conversion",
      "text": [
        "For processing both Simplified and Traditional Chinese text from a variety of locales, including Mainland China, Hong Kong and Taiwan, we choose UTF-8 as internal character representation within the system.",
        "The ability to transparently handle Chinese text from any Chinese locale greatly simplifies the logic of the segmentation system."
      ]
    },
    {
      "heading": "2.2 N-gram language model",
      "text": [
        "In our system, Chinese words can be categorized into one of the following types: lexicon words, morphological words, factoids, name entities.",
        "These types of words are processed in different ways in our system, and are incorporated into a unified statistical framework of the trigram language model."
      ]
    },
    {
      "heading": "2.2.1 Basic segmentation",
      "text": [
        "Each input sentence is first segmented into individual characters.",
        "These characters and the character strings are then looked up in a lexicon.",
        "For the efficient search, the lexicon is represented by a TRIE compressed in a double-array data struc",
        "ture.",
        "Given a character string, all its prefix strings that form lexicon words can be retrieved efficiently by browsing the TRIE whose root represents its first character.",
        "There are twenty four kinds of factoid words, such as time, date, money, etc.",
        "All the factoid words are represented as regular expressions, and compiled into a compressed DFA with the row-index algorithm."
      ]
    },
    {
      "heading": "2.2.3 Morphological analysis",
      "text": [
        "As (Wu 2003) discussed in the paper, it is those morphologically derived words (MDWs hereafter) that are most controversial and most likely to be treated differently in different standards and different systems.",
        "In our system, there are six main categories of morphological processes, affixation, directional verb, resultative verb, splitting verb, reduplication and merging, and we employ a chart parsing algorithm augmented with word lattices structure which incorporates the morphological rules especially designed for Chinese languages with restrictive CFG.",
        "Our NE identification concentrates on three types of NEs, namely, personal names (PERs), location names (LOCs) and organization names (ORGs).",
        "For Chinese person names, we only consider PN candidates that begin with a family name stored in the family name list and follow a given name which is of one or two characters long.",
        "For transliterations of foreign person names, a PN candidate would be generated if it contains only characters stored in a transliterated character list.",
        "For location names and organizations names, we only use the LN list and ON list to generate the candidates."
      ]
    },
    {
      "heading": "2.3 Segmentation standards adaptor",
      "text": [
        "In this bakeoff, there are four segmentation standards and slightly different from ours.",
        "Standard adaptation is conducted with the application of an ordered list of transformations on the output of our segmentation system.",
        "The method we use is Transformation-Based Learning, and the transformation templates are lexicalized templates.",
        "In our system, we designed 14 lexicalized templates."
      ]
    },
    {
      "heading": "2.4 Speed",
      "text": [
        "As we optimized our lexicon and decoding process, the speed of segmentation is very fast.",
        "On a single 2.80 GHz, 1G bytes memory, Xeon machine, the system is able to process about 0.73 Mega bytes per second.",
        "The speed may vary according to the sentence lengths: given texts of the same size, those containing longer sentences will take more time.",
        "The number reported here is an average of the time taken to process the test sets of the eight tracks we participated in."
      ]
    },
    {
      "heading": "3 Evaluation",
      "text": []
    },
    {
      "heading": "3.1 Open tracks",
      "text": [
        "In the open tracks, we used four lexicons of 210,319 entries, 165,103 entries, 174,268 entries, 165,655 entries respectively on AS-open, HK-open, MSR-open, PK-open tracks, which include the entries of 2,430 MDWs, 12,487 PNs, 22,907 LNs and 29,032 ONs, 10,414 four-character idioms, plus the word lists generated from the training data provided by the bakeoff.",
        "We use the training data provided by the bakeoff for training our trigram word-based language model.",
        "We also used a family name list (which contains 399 entries in our system), and a 1,021- entry transliterated name character list."
      ]
    },
    {
      "heading": "3.2 Closed tracks",
      "text": [
        "In the close tracks, the lexicon we use could only be generated from the training data provided by the bakeoff.",
        "We could only use the training data provided by the bakeoff for training our word-based language model.",
        "Also, since the training data we used is only from the bakeoff, there does not exist any different standards, standards adaptor component is not necessarily needed."
      ]
    },
    {
      "heading": "3.3 Result analysis",
      "text": [
        "Our system is designed so that components such as the factoid detection and NE identification can be switched on or off, so that we can investigate the relative contribution of each component to the overall word segmentation performance.",
        "The results are summarized in the table 1.",
        "For comparison, we also include in the table (Row 1) the results of using FMM.",
        "Row 2 shows the baseline results of our system, where only the lexicon is used.",
        "Each cell in the table has six fields.",
        "From the top, there are respectively Precision, Recall, F-measure, OOV Recall, IV Recall and Speed (Mega bytes/second).",
        "We don't list the speed in Row 6 since it decreases a factor of 10 to 60 because of application of thousands of TBL rules.",
        "From Table 1 we can find that, in rows 1 and 2, sistently.",
        "The morphological analysis has no the dictionary-based methods already achieve contribution to the overall performance in Row quite good recall, but the precisions are not very 4.",
        "The main reason is that the number of MDWs good because they cannot correctly identify un- used in our system is very small (only 2,430) known words that are not in the lexicon such as and there may exist very small MDWs in the test factoids and name entities.",
        "We also find that sets.",
        "The similar cases occur on NE identifica-even using the same lexicon, our approach that tion in the close tracks in Row 5 since we would is based on the N-gram language models outper- not do NE identification at all in the close tracks.",
        "forms the greedy approach because the use of We also notice that the contribution of NE iden-context model resolves more ambiguities in tification is very little in the open tracks, which segmentation.",
        "As shown in Rows 3 to 5, when shows that the performance of NE identification components are switched on in turn, the overall is not very good in our system, and explains word segmentation performance increases con why our OOV recall is not very high compared",
        "with other participants in the bakeoff.",
        "This is one area of our future work to improve.",
        "The results of standards adaptation on four bakeoff test sets are shown in Row 6.",
        "It turns out that performance except IV recall improves slightly across the board in all four test sets.",
        "The main reason is that the training data and lexicon we used are mainly from the four providers in the bakeoff, there does not exist any different segmentation standards."
      ]
    },
    {
      "heading": "4 Conclusions",
      "text": [
        "The evaluation results show that the closed tests is not very good compared with other participants, the one main reason is that the word-based language model we used is not competitive compared with other algorithms in the closed tracks.",
        "One area of our future work is to apply other machine learning algorithm, like Maximum Entropy (ME), Support Vector Machine (SVM), Conditional Random Field (CRF), etc."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The work reported here was a team effort.",
        "We thank Wu Liu, Haitao Zeng, Nan He for their help in the experimentation and evaluation of the system."
      ]
    }
  ]
}
