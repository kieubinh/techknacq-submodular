{
  "info": {
    "authors": [
      "Rashmi Gangadharaiah",
      "Ralf D. Brown",
      "Jaime G. Carbonnell"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics – Short Papers",
    "id": "acl-N06-2011",
    "title": "Spectral Clustering for Example Based Machine Translation",
    "url": "https://aclweb.org/anthology/N06-2011",
    "year": 2006
  },
  "references": [
    "acl-C00-1019",
    "acl-J90-2002",
    "acl-P02-1040"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Prior work has shown that generalization of data in an Example Based Machine Translation (EBMT) system, reduces the amount of pre-translated text required to achieve a certain level of accuracy (Brown, 2000).",
        "Several word clustering algorithms have been suggested to perform these generalizations, such as k-Means clustering or Group Average Clustering.",
        "The hypothesis is that better contextual clustering can lead to better translation accuracy with limited training data.",
        "In this paper, we use a form of spectral clustering to cluster words, and this is shown to result in as much as 29.08% improvement over the baseline EBMT system."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In EBMT, the source sentence to be translated is matched against the source language sentences present in a corpus of source-target sentence pairs.",
        "When a partial match is found, the corresponding target translations are obtained through subsenten-tial alignment.",
        "These partial matches are put together to obtain the final translation by optimizing translation and alignment scores and using a statistical target language model in the decoding process.",
        "Prior work has shown that EBMT requires large amounts of data (in the order of two to three million words) (Brown, 2000) of pre-translated text, to function reasonably well.",
        "Thus, some modification of the basic EBMT method is required to make it effective when less data is available.",
        "In order to use the available text efficiently, systems such as, (Veale and Way, 1997) and (Brown, 1999), convert the examples in the corpus into templates against which the new text can be matched.",
        "Thus, source-target sentence pairs are converted to source-target generalized template pairs.",
        "An example of such a pair is shown below: The session opened at 2p.m La s´eance est ouverte a´ 2 heures The <event> <verb-past-tense> at <time> La <event> <verb-past-tense> a <time> This single template can be used to translate different source sentences, including for example, The session adjourned at 6p.m The seminar opened at 8a.m if ‘session’ and ‘seminar’ are both generalized to ‘<event>’, ‘opened’ and ‘adjourned’ are both generalized to ‘<verb-past-tense>’ and finally ‘6p.m’ and ‘8a.m’ are both generalized to ‘<time>’.",
        "The system used by (Brown, 1999) performs its generalization using both equivalence classes of words and a production rule grammar.",
        "This paper describes the use of spectral clustering (Ng.",
        "et.",
        "al., 2001; Zelnik-Manor and Perona, 2004), for automated extraction of equivalence classes.",
        "Spectral clustering is seen to be superior to Group Average Clustering (GAC) (Brown, 2000) both in terms of semantic similarity of words falling in a single cluster, and overall BLEU score (Papineni.",
        "et.",
        "al., 2002) in a large scale EBMT system.",
        "The next section explains the term vectors extracted for each word, which are then used to cluster words into equivalence classes and provides an outline of the Standard GAC algorithm.",
        "Section 3 describes the spectral clustering algorithm used.",
        "Sec",
        "tion 4 lists results obtained in a full evaluation of the algorithm.",
        "Section 5 concludes and discusses directions for future work."
      ]
    },
    {
      "heading": "2 Term vectors for clustering",
      "text": [
        "Using a bilingual dictionary, usually created using statistical methods such as those of (Brown et.",
        "al., 1990) or (Brown, 1997), and the parallel text, a rough mapping between source and target words can be created.",
        "This word pair is then treated as an indivisible token for future processing.",
        "For each such word pair we then accumulate counts for each token in the surrounding context of its occurrences (N words, currently 3, immediately prior to and N words immediately following).",
        "The counts are weighted with respect to distance from occurrence, with a linear decay (from 1 to 1/N) to give greatest importance to the words immediately adjacent to the word pair being examined.",
        "These counts form a pseudo-document for each pair, which are then converted into term vectors for clustering.",
        "In this paper, we compare our algorithm against the incremental GAC algorithm(Brown, 2000).",
        "This method examines each word pair in turn, computing a similarity measure to every existing cluster.",
        "If the best similarity measure is above a predetermined threshold, the new word is placed in the corresponding cluster, otherwise a new cluster is created if the maximum number of clusters has not yet been reached."
      ]
    },
    {
      "heading": "3 Spectral clustering",
      "text": [
        "Spectral clustering is a general term used to describe a group of algorithms that cluster points using the eigenvalues of ‘distance matrices’ obtained from data.",
        "In our case, the algorithm described in (Ng.",
        "et.",
        "al., 2001) was performed with certain variations that were proposed by (Zelnik-Manor and Perona, 2004) to compute the scaling factors automatically and for the k-Means orthogonal treatment (Verma and Meila, 2003) during the initialization.",
        "These scaling factors help in self-tuning distances between points according to the local statistics of the neighborhoods of the points.",
        "The algorithm is briefly described below.",
        "1.",
        "Let S =s17 s27 ....sr,,, denote the term vectors to be clustered into k classes.",
        "2.",
        "Form the affinity matrix A defined by",
        "and sj, E is used to prevent the ratio from becoming infinity QZ is the set of local scaling parameters for sZ.",
        "QZ = d(sZ7 sT) where, sT is the Tth neighbor of point sZ for some fixed T (7 for this paper).",
        "3.",
        "Define D to be the diagonal matrix given by, DZZ = �j AZj 4.",
        "Compute L = D – 1/2AD – 1/2 5.",
        "Select k eigenvectors corresponding to k largest eigenvalues (k is presently an externally set parameter).",
        "The eigenvectors are normalized to have unit length.",
        "Form matrix U by stacking all the eigenvectors in columns.",
        "6.",
        "Form the matrix Y by normalizing U’s rows,",
        "7.",
        "Perform k-Means clustering treating each row of Y as a point in k dimensions.",
        "The k-Means algorithm is initialized either with random centers or with orthogonal vectors.",
        "8.",
        "After clustering, assign the point sZ to cluster c if the corresponding row i of the matrix Y was assigned to cluster c. 9.",
        "Sum the distances between the members and",
        "the centroid of each cluster to obtain the classification cost.",
        "10.",
        "Goto step 7, iterate for a fixed number of iterations.",
        "In this paper, 20 iterations were performed with orthogonal k-Means initialization and 5 iterations with random k-Means initialization.",
        "11.",
        "The clusters obtained from the iteration with least classification cost are selected as the k clusters."
      ]
    },
    {
      "heading": "4 Preliminary Results",
      "text": [
        "The clusters obtained from the spectral clustering method are seen by inspection to correspond to more natural and intuitive word classes than those obtained by GAC.",
        "Even though this is subjective and not guaranteed to lead to improve translation performance, it shows that maybe the increased power of spectral clustering to represent non-convex classes",
        "(non-convex in the term vector domain) could be useful in a real translation experiment.",
        "Some example classes are shown in Table 1.",
        "The first class in an intuitive sense corresponds to measurement units.",
        "We see that in the <units> case, GAC misses some of the members which are actually distributed among many different classes and hence these are not well generalized.",
        "In the second class <months>, spectral clustering has primarily the months in a single class whereas GAC adds a number of seemingly unrelated words to the cluster.",
        "The classes were all obtained by finding 80 clusters in a 20,000-sentence pair subset of the IBM Hansard Corpus (Linguistic Data Consortium, 1997) for spectral clustering.",
        "80 was chosen as the number of clusters since it gave the highest BLEU score in the evaluation.",
        "For GAC, 300 clusters were used as this gave the best performance.",
        "To show the effectiveness of the clustering methods in an actual evaluation, we set up the following experiment for an English to French translation task on the Hansard corpus.",
        "The training data consists of three sets of size 10,000 (set1), 20,000 (set2) and 30,000 (set3) sentence pairs chosen from the first six files of the Hansard Corpus.",
        "Only sentences of length 5 to 21 words were taken.",
        "Only words with frequency of occurrence greater than 9 were chosen for clustering because more contextual information would be available when the word occurs frequently and this would help in obtaining better clusters.",
        "The test data was chosen to be a set of 500 sentences obtained from files 20, 40, 60 and 80 of the Hansard corpus with 125 sentences from each file.",
        "Each of the methods was run with different number of clusters and results are reported only for the optimal number of clusters in each case.",
        "The results in Table 2 show that spectral clustering requires moderate amounts of data to get a large improvement.",
        "For small amounts of data it is slightly worse than GAC, but neither gives much improvement over the baseline.",
        "For larger amounts of data, again both methods are very similar, though spectral clustering is better.",
        "Finally, for moderate amounts of data, when generalization is the most useful, spectral clustering gives a significant improvement over the baseline as well as over GAC.",
        "By looking at the clusters obtained with varying amounts of data, it can be concluded that high pu",
        "rity clusters can be obtained with even just moderate amounts of data."
      ]
    },
    {
      "heading": "5 Conclusions and future work",
      "text": [
        "From the experimental results we see that spectral clustering leads to relatively purer and more intuitive clusters.",
        "These clusters result in an improved BLEU score in comparison with the clusters obtained through GAC.",
        "GAC can only collect clusters in convex regions in the term vector space, while spectral clustering is not limited in this regard.",
        "The ability of spectral clustering to represent non-convex shapes arises due to the projection onto the eigenvectors as described in (Ng.",
        "et.",
        "al., 2001).",
        "As future work, we would like to analyze the variation in performance as the amount of data increases.",
        "It is widely known that increasing the amount of training data in a generalized EBMT system eventually leads to saturation of performance, where all clustering methods perform about as well as baseline.",
        "Thus, all methods have an operating region where they are the most useful.",
        "We would like to locate and extend this region for spectral clustering.",
        "Also, it would be interesting to compare the clusters obtained with spectral clustering and the Part of Speech tags of the words in the same cluster, especially for languages such as English where good taggers are available.",
        "Finally, an important direction of research is in automatically selecting the number of clusters for the clustering algorithm.",
        "To do this, we could use information from the eigenvalues or the distribution of points in the clusters."
      ]
    },
    {
      "heading": "Acknowledgment",
      "text": [
        "This work was funded by National Business Center award NBCHC050082."
      ]
    }
  ]
}
