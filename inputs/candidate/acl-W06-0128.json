{
  "info": {
    "authors": [
      "Guo-Wei Bian"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W06-0128",
    "title": "Chinese Word Segmentation Using Various Dictionaries",
    "url": "https://aclweb.org/anthology/W06-0128",
    "year": 2006
  },
  "references": [
    "acl-C92-1019",
    "acl-P94-1010",
    "acl-P97-1041"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Most of the Chinese word segmentation systems utilizes monolingual dictionary and are used for monolingual processing.",
        "For the tasks of machine translation (MT) and cross-language information retrieval (CLIR), another translation dictionary may be used to transfer the words of documents from the source languages to target languages.",
        "The inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary) may produce some problems for MT and CLIR.",
        "This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Most of the Chinese word segmentations are used for monolingual processing.",
        "In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al., 1994).",
        "For the tasks of machine translation (MT) (Bian and Chen, 1998) and cross-language information retrieval (CLIR) (Bian and Chen, 2000), another translation dictionary may be used to transfer the words of documents from the source languages to target languages.",
        "Because of the inconsistencies resulting from the two types of dictionaries (segmentation dictionary and transfer dictionary), this approach has the problems that some segmented words cannot be found in the transfer dictionary.",
        "In this paper, we focus on the effectiveness of the Chinese word segmentation using different dictionaries.",
        "Four different dictionaries (or word lists) and two different testing collections (testing data) are used to evaluate the results of the Chinese word segmentation."
      ]
    },
    {
      "heading": "2 Chinese Word Segmentation System",
      "text": [
        "The segmentation system used only the various dictionaries in this design.",
        "In this paper, the other possible resources (POS, segmentation rules, word segmentation guide, and statistical information) are ignored to test the average performance between different testing collections specially followed the different segmented guidelines.",
        "The longest-matching method is adopted in this Chinese segmentation system.",
        "The segmentation processing searches for a dictionary entry corresponding to the longest sequence of Chinese characters from left to right.",
        "The system provided the approximate matching to search a sub-string of the input with the entry in the dictionary if no total matching is found.",
        "For example, the system will segment the input “ €� � � ���PH�44” as “€� PH f r`J 44 ” which matched the term with the entry “ € ��” in dictionary if no entry “€�” found."
      ]
    },
    {
      "heading": "2.1 Various Dictionaries",
      "text": [
        "The word segmentation are evaluated using different dictionaries (or word lists) and different testing collections (testing data).",
        "There are four dictionaries are used: the first one is converted from an English-Chinese bilingual dictionary, and the other three are extracted from the training corpora.",
        "The original English-Chinese dictionary (Bian and Chen, 1998), which containing about 67,000 English word entries, is converted to a new Chinese-English dictionary (called CEDIC later).",
        "There are 125,719 Chinese word entries in this CEDIC.",
        "The terms in the various training corpora (the Sinica Corpus and the City University Corpus) are extracted to build the different word lists as the segmentation dictionaries (called CKIP and CityU later).",
        "The tokens starting with the special",
        "Sydney, July 2006. c�2006 Association for Computational Linguistics characters or punctuation marks are ignored.",
        "The following shows some examples: , � , , , , , 1 , , ─, Œ , � , 000, ..., - , TM a � a , y , ,d , ® , .com, Table 1 lists the number of tokens (#tokens), the number of ignored tokens (#ignored), the number of words (#words), and the unique words (#unique) for each dictionaries.",
        "There are 140,971 unique words are extracted from the training collection of Sinica Corpus, and 75,433 respected to the training set of the City University Corpus.",
        "These two dictionaries are combined to another dictionary which containing 174,398 unique words."
      ]
    },
    {
      "heading": "3 Experimental Results",
      "text": [
        "To evaluate the results of Chinese word segmentations, we implement 8 experiments (runs) using the 4 different dictionaries (CEDIC, CK, CT, and CK+CT) mentioned in previous section.",
        "Two test collections (the Sinica Corpus and the City University Corpus) are used to measure the precision, recall, and an evenly-weighted F-measure for the Chinese words segmentations.",
        "Table 2 shows the F-measure of the experimental results, and the Figure 1 illustrates the comparisons of the segmentation performances.",
        "The symbol (*) indicates that the run is a closed test, which only uses the training material from the training data for the particular corpus.",
        "We can find that the larger dictionary (CK+CT) produces better segmentation results even the word lists are combined from the different resources (corpora) and followed the different guidelines of word segmentations."
      ]
    },
    {
      "heading": "3.1 Error Analysis 3.1.1 Format Error of Result File",
      "text": [
        "The results file for word segmentation is required to appear with one line for each sentence/line in the test file with words and punctuation separated by whitespace.",
        "Our system makes some mistakes to produce no whitespace before English terms and Arabic numbers, and produce no whitespace after Chinese punctuation marks.",
        "This formatting problem has made many adjacent segmented words to be evaluated as errors.",
        "A sentence with such errors is listed below The standard answer of the testing collection (CityU) of the City University Corpus has 7,512 sentences and 220,147 words.",
        "The total number of English terms, Arabic numbers, and Chinese punctuation marks is 37,644.",
        "Such formatting problem makes the error rate of about 30% for the City University Corpus."
      ]
    },
    {
      "heading": "3.1.2 Different Viewpoints of Segmentations",
      "text": [
        "In our experiments, there are different word lists extracted from the different training corpora.",
        "Some errors are produced because of the differ",
        "ent results of word segmentations in the training corpora according to the different guidelines.",
        "Table 3 shows some different results.",
        "The first column (CKIP) is the standard answer of the testing collection of Sinica Corpus, and the second column (HFUIM) is our answer.",
        "The third and fourth columns are the words with their frequencies appeared in the training collections of Sinica Corpus and City University Corpus.",
        "For example, our system produces the word “� F[I”, but the standard answer of Sinica Corpus is “�” and “F[I”.",
        "However, the word “� F[I” appear 61 times in the training collection of City University Corpus."
      ]
    }
  ]
}
