{
  "info": {
    "authors": [
      "Wei Jiang",
      "Yi Guan",
      "Wang XiaoLong"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W06-0134",
    "title": "A Pragmatic Chinese Word Segmentation System",
    "url": "https://aclweb.org/anthology/W06-0134",
    "year": 2006
  },
  "references": [
    "acl-C04-1081",
    "acl-I05-3030",
    "acl-W03-1709"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents our work for participation in the Third International Chinese Word Segmentation Bakeoff.",
        "We apply several processing approaches according to the corresponding sub-tasks, which are exhibited in real natural language.",
        "In our system, Trigram model with smoothing algorithm is the core module in word segmentation, and Maximum Entropy model is the basic model in Named Entity Recognition task.",
        "The experiment indicates that this system achieves F-measure 96.8% in MSRA open test in the third SIGHAN-2006 bakeoff."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word is a logical semantic and syntactic unit in natural language.",
        "Unlike English, there is no delimiter to mark word boundaries in Chinese language, so in most Chinese NLP tasks, word segmentation is a foundation task, which transforms Chinese character string into word sequence.",
        "It is prerequisite to POS tagger, parser or further applications, such as Information Extraction, Question Answer system.",
        "Our system participated in the Third International Chinese Word Segmentation Bakeoff, which held in 2006.",
        "Compared with our system in the last bakeoff (Jiang 2005A), the system in the third bakeoff is adjusted intending to have a better pragmatic performance.",
        "This paper mainly focuses on describing two sub-tasks: (1) The basic Word Segmentation; (2) Named entities recognition.",
        "We apply different approaches to solve above two tasks, and all the modules are integrated into a pragmatic system (ELUS)."
      ]
    },
    {
      "heading": "2 System Description",
      "text": [
        "All the words in our system are categorized into five types: Lexicon words (LW), Factoid words (FT), Morphologically derived words (MDW),",
        "The input character sequence is converted into one or several sentences, which is the basic dealing unit.",
        "The “Basic Segmentation” is used to identify the LW, FT, MDW words, and “Named Entity Recognition” is used to detect NW words.",
        "We don’t adopt the New Word detection algorithm in our system in this bakeoff.",
        "The “Disambiguation” module performs to classify complicated ambiguous words, and all the above results are connected into the final result, which is denoted by XML format."
      ]
    },
    {
      "heading": "2.1 Trigram and Smoothing Algorithm",
      "text": [
        "We apply the trigram model to the word segmentation task (Jiang 2005A), and make use of Absolute Smoothing algorithm to overcome the sparse data problem.",
        "Trigram model is used to convert the sentence into a word sequence.",
        "Let w = w1 w2 ...wn be a word sequence, then the most likely word sequence w* in trigram is:",
        "where let P(w0|w-2 w-1) be P(w0) and let P(w1|w-1 w0) be P(w1|w0), and wi represents LW or a type of FT or MDW.",
        "In order to search the best segmentation way, all the word candidates are filled in the word lattice (Zhao 2005).",
        "And the Viterbi",
        "Sydney, July 2006. c�2006 Association for Computational Linguistics algorithm is used to search the best word segmentation path.",
        "FT and MDW need to be detected when constructing word lattice (detailed in section 2.2).",
        "The data structure of lexicon can affect the efficiency of word segmentation, so we represent lexicon words as a set of TRIEs, which is a treelike structure.",
        "Words starting with the same character are represented as a TRIE, where the root represents the first Chinese character, and the children of the root represent the second characters, and so on (Gao 2004).",
        "When searching a word lattice, there is the zero-probability phenomenon, due to the sparse data problem.",
        "For instance, if there is no cooc-curence pair “;JjP7/�/fA”(we eat bananas) in the training corpus, then P(fA|;JjP7, �) = 0.",
        "According to formula (1), the probability of the whole candidate path, which includes “;JjP7/�/ fA” is zero, as a result of the local zero probability.",
        "In order to overcome the sparse data problem, our system has applied Absolute Discounting Smoothing algorithm (Chen, 1999).",
        "The notation N1+ is meant to evoke the number of words that have one or more counts, and the • is meant to evoke a free variable that is summed over.",
        "The function c() represents the count of one word or the cooccurence count of multi-words.",
        "In this case, the smoothing probability",
        "Because we use trigram model, so the maximum n may be 3.",
        "A fixed discount D (0≤ D≤ 1) can be set through the deleted estimation on the training data.",
        "They arrive at the estimate where n1 and n2 are the total number of n-grams with exactly one and two counts, respectively.",
        "After the basic segmentation, some complicated ambiguous segmentation can be further disambiguated.",
        "In trigram model, only the previous two words are considered as context features, while in disambiguation processing, we can use the Maximum Entropy model fused more features (Jiang 2005B) or rule based method."
      ]
    },
    {
      "heading": "2.2 Factoid and Morphological words",
      "text": [
        "All the Factoid words can be represented as regular expressions.",
        "So the detection of factoid words can be achieved by Finite State Automaton(FSA).",
        "In our system, the following categories of factoid words can be detected, as shown in table 1.",
        "Deterministic FSA (DFA) is efficient because a unique “next state” is determined, when given an input symbol and the current state.",
        "While it is common for a linguist to write rule, which can be represented directly as a non-deterministic FSA (NFA), i.e. which allows several “next states” to follow a given input and state.",
        "Since every NFA has an equivalent DFA, we build a FT rule compiler to convert all the FT generative rules into a DFA.",
        "e.g.",
        "• “< digit > -> [0..9]; • < year > ::_ < digit > {< digit >+} 年”; • < integer > ::_ {< digit >+};",
        "where “->” is a temporary generative rule, and “::_” is a real generative rule.",
        "As for the morphological words, we erase the dealing module, because the word segmentation definition of our system adopts the PKU standard."
      ]
    },
    {
      "heading": "3 Named Entity Recognition",
      "text": [
        "We adopt Maximum Entropy model to perform the Named Entity Recognition.",
        "The extensive evaluation on NER systems in recent years (such as CoNLL-2002 and CoNLL-2003) indicates the best statistical systems are typically achieved by using a linear (or log-linear) classification algorithm, such as Maximum Entropy model, together with a vast amount of carefully designed linguistic features.",
        "And this seems still true at present in terms of statistics based methods.",
        "Maximum Entropy model (ME) is defined over H X T in segmentation disambiguation, where H is the set of possible contexts around target word that will be tagged, and T is the set of allowable tags, such as B-PER, I-PER, B-LOC, I-LOC etc.",
        "in our NER task.",
        "Then the model’s conditional probability is defined as",
        "where h is the current context and t is one of the possible tags.",
        "The several typical kinds of features can be used in the NER system.",
        "They usually include the context feature, the entity feature, and the total resource or some additional resources.",
        "While, we only point out the local feature template, some other feature templates, such as long distance dependency templates, are also helpful to NER performance.",
        "These trigger features can be collected by Average Mutual Information or Information Gain algorithm etc.",
        "Besides context features, entity features is another important factor, such as the suffix of Location or Organization.",
        "The following 8 kinds of dictionaries are usually useful (Zhao 2006):",
        "In addition, some external resources may improve the NER performance too, e.g. we collect a lot of entities for Chinese Daily Newspaper in 2000, and total some entity features.",
        "However, our system is based on Peking University (PKU) word segmentation definition and PKU NER definition, so we only used the basic features in table 2 in this bakeoff.",
        "Another effect is the corpus: our system is training by the Chinese Peoples’ Daily Newspaper corpora in 1998, which conforms to PKU NER definition.",
        "In the section 4, we will give our system performance with the basic features in Chinese Peoples’ Daily Newspaper corpora."
      ]
    },
    {
      "heading": "4 Performance and analysis",
      "text": []
    },
    {
      "heading": "4.1 The Evaluation in Word Segmentation",
      "text": [
        "The performance of our system in the third bakeoff is presented in table 4 in terms of recall(R), precision(P) and F score in percentages.",
        "The score software is standard and open by SIGHAN.",
        "Our system has good performance in terms of Riv measure.",
        "The Riv measure in close test and in open test are 99.1% and 98.9% respectively.",
        "This good performance owes to class-based trigram with the absolute smoothing and word disambiguation algorithm.",
        "In our system, it is the following reasons that the open test has a better performance than the close test:",
        "(1) Named Entity Recognition module is added into the open test system.",
        "And Named Entities, including PER, LOC, ORG, occupy the most of the out-of-vocabulary words.",
        "(2) The system of close test can only use the dictionary that is collected from the given training corpus, while the system of open test can use a better dictionary, which includes the words that exist in MSRA training corpus in SIGHAN2005.",
        "And we know, the dictionary is the one of important factors that affects the performance, because the LW candidates in the word lattice are generated from the dictionary.",
        "As for the dictionary, we compare the two collections in SIGHAN2005 and SIGHAN2006, and evaluating in SIGHAN2005 MSRA close test.",
        "There are less training sentence in SIGHAN2006, as a result, there is at least 1.2% performance decrease.",
        "So this result indicates that the dictionary can bring an important impact in our system.",
        "Comparing table 4 with table 5, we find that the OOV is 3.4 in third bakeoff, which is higher than the value in the last bakeoff.",
        "Obviously, it is one of reasons that affect our performance.",
        "In addition, based on pragmatic consideration, our system has been made some simplifier, for instance, we erase the new word detection algorithm and the is no morphological word detection."
      ]
    },
    {
      "heading": "4.2 Named Entity Recognition",
      "text": [
        "In MSRA NER open test, our NER system is training in prior six-month corpora of Chinese Peoples’ Daily Newspaper in 1998, which were annotated by Peking University.",
        "Table 6 shows the NER performance in the MSRA open test.",
        "As a result of insufficiency in preparing bakeoff, our system is only trained in Chinese Peoples’ Daily Newspaper, in which the NER is defined according to PKU standard.",
        "However, the NER definition of MSRA is different from that of PKU, e.g, “rP*/LOC K�”, “�/PER 3�q /PER -t-X” in MSRA, are not entities in PKU.",
        "So the training corpus becomes a main handicap to decrease the performance of our system, and it also explains that there is much difference between the recall rate and the precision in table 6.",
        "Table 7 gives the evaluation of our NER system in Chinese Peoples’ Daily Newspaper, training in prior five-month corpora and testing in the sixth month corpus.",
        "We also use the feature templates in table 2, in order to make comparison with table 6.",
        "This experiment indicates that our system can have a good performance, if the test corpus and the training corpora conform to the condition of independent identically distributed attribution."
      ]
    },
    {
      "heading": "4.3 Analysis and Discussion",
      "text": [
        "Some points need to be further considered: (1) The dictionary can bring a big impact to the performance, as the LW candidates come from the dictionary.",
        "However a big dictionary can be easily acquired in the real application.",
        "(2) Due to our technical and insufficiently preparing problem, we use the PKU NER definition, however they seem not unified with the MSRA definition.",
        "(3) Our NER system is a word-based model, and we have find out that the word segmentation with two different dictionaries can bring a big impact to the NER performance.",
        "(4) We erase the new word recognition algorithm in our system.",
        "While, we should explore the real annotated corpora, and add new word detection algorithm, if it has positive effect.",
        "e.g. “ 荷花 奖”(lotus prize) can be recognized as one word by the conditional random fields model."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We have briefly described our word segmentation system and NER system.",
        "We use word-based features in the whole processing.",
        "Our system has a good performance in terms of Riv measure, so this means that the trigram model with the smoothing algorithm can deal with the basic segmentation task well.",
        "However, the result in the bakeoff indicates that detecting out-of-vocabulary word seems to be a harder task than dealing with the segmentation-ambiguity task.",
        "The work in the future will concentrate on two sides: improving the NER performance and adding New Word Detection Algorithm."
      ]
    }
  ]
}
