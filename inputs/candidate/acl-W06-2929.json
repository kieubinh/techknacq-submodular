{
  "info": {
    "authors": [
      "Markus Dreyer",
      "David A. Smith",
      "Noah A. Smith"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W06-2929",
    "title": "Vine Parsing and Minimum Risk Reranking for Speed and Precision",
    "url": "https://aclweb.org/anthology/W06-2929",
    "year": 2006
  },
  "references": [
    "acl-H05-1066",
    "acl-P04-3032",
    "acl-P05-1022",
    "acl-P06-2101",
    "acl-P96-1023",
    "acl-P99-1059",
    "acl-W05-1504",
    "acl-W05-1506"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe our entry in the CoNLL-X shared task.",
        "The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006).",
        "The system is designed for fast training and decoding and for high precision.",
        "We describe sources of cross-lingual error and ways to ameliorate them.",
        "We then provide a detailed error analysis of parses produced for sentences in German (much training data) and Arabic (little training data)."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes.",
        "First, a parser produces a list of the most likely n parse trees under a generative, probabilistic model (usually some flavor of PCFG).",
        "A discriminative reranker then chooses among trees in this list by using an extended feature set (Collins, 2000).",
        "This paradigm has many advantages: PCFGs are fast to train, can be very robust, and perform better as more data is made available; and rerankers train quickly (compared to discriminative models), require few parameters, and permit arbitrary features.",
        "We describe such a system for dependency parsing.",
        "Our shared task entry is a preliminary system developed in only 3 person-weeks, and its accuracy is typically one s.d.",
        "below the average across systems and 10–20 points below the best system.",
        "On",
        "the positive side, its decoding algorithms have guaranteed O(n) runtime, and training takes only a couple of hours.",
        "Having designed primarily for speed and robustness, we sacrifice accuracy.",
        "Better estimation, reranking on larger datasets, and more fine-grained parsing constraints are expected to boost accuracy while maintaining speed."
      ]
    },
    {
      "heading": "2 Notation",
      "text": [
        "Let a sentence x = �x1, x2, ..., xn), where each xi is a tuple containing a part-of-speech tag ti and a word wi, and possibly more information.1 x0 is a special wall symbol, $, on the left.",
        "A dependency tree y is defined by three functions: yleft and y7ight (both 10, 1, 2,..., n} – * 2{1,2,...,n1) that map each word to its sets of left and right dependents, respectively, and ylabel : 11, 2,..., n} – * D, which labels the relationship between word i and its parent from label set D. In this work, the graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles or crossing dependencies.",
        "Using a simple dynamic program to find the minimum-error projective parse, we find that assuming projectivity need not harm accuracy very much (Tab.",
        "1, col. 3)."
      ]
    },
    {
      "heading": "3 Unlabeled Parsing",
      "text": [
        "The first component of our system is an unlabeled parser that, given a sentence, finds the U best unlabeled trees under a probabilistic model using a bottom-up dynamic programming algorithm.",
        "The model is a probabilistic head automaton grammar (Alshawi, 1996) that assumes conditional indepen",
        "in training data.",
        "We show oracle, 1-best, and reranked performance on the test set at different stages of the system.",
        "Boldface marks oracle performance that, given perfect downstream modules, would supercede the best system.",
        "Italics mark the few cases where the reranker increased error rate.",
        "Columns 8–10 show labeled accuracy; column 10 gives the final shared task evaluation scores.",
        "dence between the left yield and the right yield of a given head, given the head (Eisner, 1997).3 The best known parsing algorithm for such a model is O(n3) (Eisner and Satta, 1999).",
        "The U-best list is generated using Algorithm 3 of Huang and Chiang (2005)."
      ]
    },
    {
      "heading": "3.1 Vine parsing (dependency length bounds)",
      "text": [
        "Following Eisner and N. Smith (2005), we also impose a bound on the string distance between every 3To empirically test this assumption across languages, we measured the mutual information between different features of y�eft (j) and y�iyht (j ), given xj .",
        "(Mutual information is a statistic that equals zero iff conditional independence holds.)",
        "A detailed discussion, while interesting, is omitted for space, but we highlight some of our findings.",
        "First, unsurprisingly, the split-head assumption appears to be less valid for languages with freer word order (Czech, Slovene, German) and more valid for more fixed-order languages (Chinese, Turkish, Arabic) or corpora (Japanese).",
        "The children of verbs and conjunctions are the most frequent violators.",
        "The mutual information between the sequence of dependency labels on the left and on the right, given the head’s (coarse) tag, only once exceeded 1 bit (Slovene).",
        "child and its parent, with the exception of nodes attaching to $.",
        "Bounds of this kind are intended to improve precision of non-$ attachments, perhaps sacrificing recall.",
        "Fixing bound Bt, no left dependency may exist between child xi and parent xj such that j �i > Bt (similarly for right dependencies and Br).",
        "As a result, edge-factored parsing runtime is reduced from O(n3) to O(n(B�t + B�r)).",
        "For each language, we choose Bt (Br) to be the minimum value that will allow recovery of 90% of the left (right) dependencies in the training corpus (Tab.",
        "1, cols.",
        "1, 2, and 4).",
        "In order to match the training data to the parsing model, we reattach disallowed long dependencies to $ during training."
      ]
    },
    {
      "heading": "3.2 Estimation",
      "text": [
        "The probability model predicts, for each parent word xj, {xi}i�Y�eft(j) and {xi}i�Y��yht(j).",
        "An advantage of head automaton grammars is that, for a given parent node x j, the children on the same side, yle�t (j),",
        "for example, can depend on each other (cf. McDonald et al., 2005).",
        "Child nodes in our model are generated outward, conditional on the parent and the most recent same-side sibling (MRSSS).",
        "This increases our parser’s theoretical runtime to O(n(BP + Br)), which we found was quite manageable.",
        "Let pary : 11, 2,..., n} – * 10, 1, ..., n} map each node to its parent in y.",
        "Let predy : 11, 2,..., n} ---+ �0, 1, 2,..., n} map each node to the MRSSS in y if it exists and 0 otherwise.",
        "Let Ai = Ii – j I if j is i’s parent.",
        "Our (probability-deficient) model defines",
        "Due to the familiar sparse data problem, a maximum likelihood estimate for the ps in Eq.",
        "1 performs very badly (2–23% unlabeled accuracy).",
        "Good statistical parsers smooth those distributions by making conditional independence assumptions among variables, including backoff and factorization.",
        "Arguably the choice of assumptions made (or interpolated among) is central to the success of many existing parsers.",
        "Noting that (a) there are exponentially many such options, and (b) the best-performing independence assumptions will almost certainly vary by language, we use a mixture among 8 such models.",
        "The same mixture is used for all languages.",
        "The models were not chosen with particular care,4 and the mixture is not trained – the coefficients are fixed at uniform, with a unigram coarse-tag model for backoff.",
        "In principle, this mixture should be trained (e.g., to maximize likelihood or minimize error on a development dataset).",
        "The performance of our unlabeled model’s top choice and the top-20 oracle are shown in Tab.",
        "1, cols.",
        "5–6.",
        "In 5 languages (boldface), perfect labeling and reranking at this stage would have resulted in performance superior to the language’s best labeled 4Our infrastructure provides a concise, interpreted language for expressing the models to be mixed, so large-scale combination and comparison are possible.",
        "system, although the oracle is never on par with the best unlabeled performance."
      ]
    },
    {
      "heading": "4 Labeling",
      "text": [
        "The second component of our system is a labeling model that independently selects a label from D for each parent/child pair in a tree.",
        "Given the U best unlabeled trees for a sentence, the labeler produces the L best labeled trees for each unlabeled one.",
        "The computation involves an O (I D I n) dynamic programming algorithm, the output of which is passed to Huang and Chiang’s (2005) algorithm to generate the L-best list.",
        "We separate the labeler from the parser for two reasons: speed and candidate diversity.",
        "In principle the vine parser could jointly predict dependency labels along with structures, but parsing runtime would increase by at least a factor of I D I.",
        "The two stage process also forces diversity in the candidate list (20 structures with 50 labelings each); the 1,000-best list of jointly-decoded parses often contained many (bad) relabelings of the same tree.",
        "In retrospect, assuming independence among dependency labels damages performance substantially for some languages (Turkish, Czech, Swedish, Danish, Slovene, and Arabic); note the often large drop in oracle performance between Tab.",
        "1, cols.",
        "5 and 8.",
        "This assumption is necessary in our framework, because the O (I D I �+�n) runtime of decoding with an Mth-order Markov model of labels5 is in general prohibitive – in some cases I D I > 80.",
        "Pruning and search heuristics might ameliorate runtime.",
        "If xi is a child of xj in direction D, and xpred is the MRSSS (possibly 0), where Ai = Ii – j I, we estimate p(�, xi, xj, xpred, Ai I D) by a mixture (untrained, as in the parser) of four backed-off, factored estimates.",
        "After parsing and labeling, we have for each sentence a list of U x L candidates.",
        "Both the oracle performance of the best candidate in the (20 x 50)- best list and the performance of the top candidate are shown in Tab.",
        "1, cols.",
        "8–9.",
        "It should be clear from the drop in both oracle and 1-best accuracy that our labeling model is a major source of error.",
        "5We tested first-order Markov models that conditioned on parent or MRSSS dependency labels."
      ]
    },
    {
      "heading": "5 Reranking",
      "text": [
        "We train a log-linear model combining many feature scores (see below), including the log-probabilities from the parser and labeler.",
        "Training minimizes the expected error under the model; we use deterministic annealing to smooth the error surface and avoid local minima (Rose, 1998; D. Smith and Eisner, 2006).",
        "We reserved 200 sentences in each language for training the reranker, plus 200 for choosing among rerankers trained on different feature sets and different (U x L)-best lists.6 Features Our reranking features predict tags, labels, lemmata, suffixes and other information given all or some of the following non-local conditioning context: bigrams and trigrams of tags or dependency labels; parent and grandparent dependency labels; subcategorization frames (in terms of tags or dependency labels); the occurrence of certain tags between head and child; surface features like the lemma7 and the 3-character suffix.",
        "In some cases the children of a node are considered all together, and in other cases left and right are separated.",
        "The highest-ranked features during training, for all languages, are the parser and labeler probabilities, followed by p(Ai I tparent), p(direction tparent), p(label I labelpred, labels,,,,, subcat), and p(coarse(t) I D, coarse(tparent), Betw), where Betw is TRUE iff an instance of the coarse tag type with the highest mutual information between its left and right children (usually verb) is between the child and its head.",
        "Feature and Model Selection For training speed and to avoid overfitting, only a subset of the above features are used in reranking.",
        "Subsets of different sizes (10, 20, and 40, plus “all”) are identified for each language using two naive feature-selection heuristics based on independent performance of features.",
        "The feature subset with the highest accuracy on the 200 heldout sentences is selected.",
        "Performance Accuracy of the top parses after reranking is shown in Tab.",
        "1, cols.",
        "10–11.",
        "Reranking almost always gave some improvement over 1-best parsing.8 Because of the vine assumption and the preprocessing step that reattaches all distant children to $, our parser learns to over-attach to $, treating $-attachment as a default/agnostic choice.",
        "For many applications a local, incomplete parse may be sufficiently useful, so we also measured non-$ unlabeled precision and recall (Tab.",
        "1, cols.",
        "12–13); our parser has > 80% precision on 8 of the languages.",
        "We also applied reranking (with unlabeled features) to the 20-best unlabeled parse lists (col. 7)."
      ]
    },
    {
      "heading": "6 Error Analysis: German",
      "text": [
        "The plurality of errors (38%) in German were erroneous $ attachments.",
        "For ROOT dependency labels, we have a high recall (92.7%), but low precision (72.4%), due most likely to the dependency length bounds.",
        "Among the most frequent tags, our system has most trouble finding the correct heads of prepositions (APPR), adverbs (ADV), finite auxiliary verbs (VAFIN), and conjunctions (KON), and finding the correct dependency labels for prepositions, nouns, and finite auxiliary verbs.",
        "The German conjunction und is the single word with the most frequent head attachment errors.",
        "In many of these cases, our system does not learn the subtle difference between enumerations that are headed by A in A und B, with two children und and B on the right, and those headed by B, with und and A as children on its left.",
        "Unlike in some languages, our labeled oracle accuracy is nearly as good as our unlabeled oracle accuracy (Tab.",
        "1, cols.",
        "8, 5).",
        "Among the ten most frequent dependency labels, our system has the most difficulty with accusative objects (OA), genitive attributes (AG), and postnominal modifiers (MNR).",
        "Accusative objects are often mistagged as subject (SB), noun kernel modifiers (NK), or AG.",
        "About 32% of the postnominal modifier relations (ein Platz in der Geschichte, ‘a place in history’) are labeled as modifiers (in die Stadtfliegen, ‘fly into the city’).",
        "Genitive attributes are often tagged as NK since both are frequently realized as nouns."
      ]
    },
    {
      "heading": "7 Error Analysis: Arabic",
      "text": [
        "As with German, the greatest portion of Arabic errors (40%) involved attachments to $.",
        "Prepositions are consistently attached too low and accounted for 26% of errors.",
        "For example, if a form in construct (idafa) governed both a following noun phrase and a prepositional phrase, the preposition usually attaches to the lower noun phrase.",
        "Similarly, prepositions usually attach to nearby noun phrases when they should attach to verbs farther to the left.",
        "We see a more serious casualty of the dependency length bounds with conjunctions.",
        "In ground truth test data, 23 conjunctions are attached to $ and 141 to non-$to using the COORD relation, whereas 100 conjunctions are attached to $ and 67 to non-$using the AUXY relation.",
        "Our system overgeneralizes and attaches 84% of COORD and 71% of AUXY relations to $.",
        "Overall, conjunctions account for 15% of our errors.",
        "The AUXY relation is defined as “auxiliary (in compound expressions of various kinds)”; in the data, it seems to be often used for waw-consecutive or paratactic chaining of narrative clauses.",
        "If the conjunction wa (‘and’) begins a sentence, then that conjunction is tagged in ground truth as attaching to $; if the conjunction appears in the middle of the sentence, it may or may not be attached to $.",
        "Noun attachments exhibit a more subtle problem.",
        "The direction of system attachments is biased more strongly to the left than is the case for the true data.",
        "In canonical order, Arabic nouns do generally attach on the right: subjects and objects follow the verb; in construct, the governed noun follows its governor.",
        "When the data deviate from this canonical order – when, e.g, a subject precedes its verb – the system prefers to find some other attachment point to the left.",
        "Similarly, a noun to the left of a conjunction often erroneously attaches to its left.",
        "Such ATR relations account for 35% of noun-attachment errors."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "The tradeoff between speed and accuracy is familiar to any parsing researcher.",
        "Rather than starting with an accurate system and then applying corpus-specific speedups, we start by imposing carefully-chosen constraints (projectivity and length bounds) for speed, leaving accuracy to the parsing and reranking models.",
        "As it stands, our system performs poorly, largely because the estimation is not state-of-the-art, but also in part due to dependency length bounds, which are rather coarse at present.",
        "Better results are achievable by picking different bounds for different head tags (Eisner and N. Smith, 2005).",
        "Accuracy should not be difficult to improve using better learning methods, especially given our models’ linear-time inference and decoding."
      ]
    }
  ]
}
