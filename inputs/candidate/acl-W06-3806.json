{
  "info": {
    "authors": [
      "Fabio Massimo Zanzotto",
      "Alessandro Moschitti"
    ],
    "book": "TextGraphs Workshop on Graph Based Methods for Natural Language Processing",
    "id": "acl-W06-3806",
    "title": "Similarity Between Pairs of Co-Indexed Trees for Textual Entailment Recognition",
    "url": "https://aclweb.org/anthology/W06-3806",
    "year": 2006
  },
  "references": [
    "acl-A00-2018",
    "acl-N04-3012",
    "acl-P04-1043",
    "acl-W05-1203"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper we present a novel similarity between pairs of co-indexed trees to automatically learn textual entailment classifiers.",
        "We defined a kernel function based on this similarity along with a more classical intra-pair similarity.",
        "Experiments show an improvement of 4.4 absolute percent points over state-of-the-art methods."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Recently, a remarkable interest has been devoted to textual entailment recognition (Dagan et al., 2005).",
        "The task requires to determine whether or not a text T entails a hypothesis H. As it is a binary classification task, it could seem simple to use machine learning algorithms to learn an entailment classifier from training examples.",
        "Unfortunately, this is not.",
        "The learner should capture the similarities between different pairs, (T', H') and (T'', H''), taking into account the relations between sentences within a pair.",
        "For example, having these two learning pairs:",
        "1.",
        "T3 is structurally (and somehow lexically) similar to T1 and H3 is more similar to H1 than to H2; 2. relations between the sentences in the pairs",
        "(T3, H3) (e.g., T3 and H3 have the same noun governing the subject of the main sentence) are similar to the relations between sentences in the pairs (T1, H1) and (T1, H2) .",
        "Given this analysis we may derive that T3 ==>.",
        "H3.",
        "The example suggests that graph matching tec-niques are not sufficient as these may only detect the structural similarity between sentences of textual entailment pairs.",
        "An extension is needed to consider also if two pairs show compatible relations between their sentences.",
        "In this paper, we propose to observe textual entailment pairs as pairs of syntactic trees with co-indexed nodes.",
        "This shuold help to cosider both the structural similarity between syntactic tree pairs and the similarity between relations among sentences within a pair.",
        "Then, we use this cross pair similarity with more traditional intra pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function.",
        "We experimented with such kernel using Support Vector Machines on the Recognizing Textual Entailment (RTE) challenge test-beds.",
        "The comparative results show that (a) we have designed an effective way to automatically learn entailment rules",
        "Workshop on TextGraphs, at HLT-NAACL 2006, pages 33–36, New York City, June 2006. c�2006 Association for Computational Linguistics from examples and (b) our approach is highly accurate and exceeds the accuracy of the current state-of-the-art models.",
        "In the remainder of this paper, Sec. 2 introduces the cross-pair similarity and Sec. 3 shows the experimental results."
      ]
    },
    {
      "heading": "2 Learning Textual Entailment from examples",
      "text": [
        "To carry out automatic learning from examples, we need to define a cross-pair similarity K((T', H'), (T'', H'')).",
        "This function should consider pairs similar when: (1) texts and hypotheses are structurally and lexically similar (structural similarity); (2) the relations between the sentences in the pair (T', H') are compatible with the relations in (T'', H'') (intra pair word movement compatibility).",
        "We argue that such requirements could be met by augmenting syntactic trees with placeholders that co-index related words within pairs.",
        "We will then define a cross-pair similarity over these pairs of co-indexed trees."
      ]
    },
    {
      "heading": "2.1 Training examples as pairs of co-indexed trees",
      "text": [
        "Sentence pairs selected as possible sentences in entailment are naturally co-indexed.",
        "Many words (or expressions) wh in H have a referent wt in T. These pairs (wt, wh) are called anchors.",
        "Possibly, it is more important that the two words in an anchor are related than the actual two words.",
        "The entailment could hold even if the two words are substitued with two other related words.",
        "To indicate this we co-index words associating placeholders with anchors.",
        "For example, in Fig. 1, 2” indicates the (compa-nies,companies) anchor between T1 and H1.",
        "These placeholders are then used to augment tree nodes.",
        "To better take into account argument movements, placeholders are propagated in the syntactic trees following constituent heads (see Fig. 1).",
        "In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dectors: the exact matching between tokens or lemmas, a similarity between tokens based on their edit distance, the derivationally related form relation and the verb entailment relation in WordNet, and, finally, a WordNet-based similarity (Jiang and Con-rath, 1997).",
        "Each of these detectors gives a different weight to the anchor: the actual computed similarity for the last and 1 for all the others.",
        "These weights will be used in the final kernel."
      ]
    },
    {
      "heading": "2.2 Similarity between pairs of co-indexed trees",
      "text": [
        "Pairs of syntactic trees where nodes are co-indexed with placeholders allow the design a cross-pair similarity that considers both the structural similarity and the intra-pair word movement compatibility.",
        "Syntactic trees of texts and hypotheses permit to verify the structural similarity between pairs of sentences.",
        "Texts should have similar structures as well as hypotheses.",
        "In Fig. 1, the overlapping subtrees are in bold.",
        "For example, T1 and T3 share the sub-tree starting with S – * NP VP.",
        "Although the lexicals in T3 and H3 are quite different from those T1 and H1, their bold subtrees are more similar to those of T1 and H1 than to T1 and H2, respectively.",
        "H1 and H3 share the production NP – * DT JJ NN NNS while H2 and H3 do not.",
        "To decide on the entailment for (T3,H3), we can use the value of (T1, H1).",
        "Anchors and placeholders are useful to verify if two pairs can be aligned as showing compatible intra-pair word movement.",
        "For example, (T1, H1) and (T3, H3) show compatible constituent movements given that the dashed lines connecting placeholders of the two pairs indicates structurally equivalent nodes both in the texts and the hypotheses.",
        "The dashed line between 3 and b links the main verbs both in the texts T1 and T3 and in the hypotheses H1 and H3.",
        "After substituting 3 and T3 share the subtree S – * NP 2 VP 3.",
        "The same subtree is shared between H1 and H3.",
        "This implies that words in the pair (T1, H1) are correlated like words in (T3, H3).",
        "Any different mapping between the two anchor sets would not have this property.",
        "Using the structural similarity, the placeholders, and the connection between placeholders, the overall similarity is then defined as follows.",
        "Let A' and A'' be the placeholders of (T', H') and (T'', H''), respectively.",
        "The similarity between two co-indexed syntactic tree pairs Ks ((T', H'), (T'', H'')) is defined using a classical similarity between two trees KT(t1, t2) when the best alignment between the A' and A'' is given.",
        "Let C be the set of all bijective",
        "Ks((T', H'), (T'' H'')) = maxcEC(KT (t(H', c), t(H'', i)) + KT (t(T', c), t(T'', i)) where (1) t(5, c) returns the syntactic tree of the hypothesis (text) 5 with placeholders replaced by means of the substitution c, (2) i is the identity substitution and (3) KT(t1, t2) is a function that measures the similarity between the two trees t1 and t2."
      ]
    },
    {
      "heading": "2.3 Enhancing cross-pair syntactic similarity",
      "text": [
        "As the computation cost of the similarity measure depends on the number of the possible sets of correspondences C and this depends on the size of the anchor sets, we reduce the number of placeholders used to represent the anchors.",
        "Placeholders will",
        "have the same name if these are in the same chunk both in the text and the hypothesis, e.g., the placeholders 2’ and 2” are collapsed to 2."
      ]
    },
    {
      "heading": "3 Experimental investigation",
      "text": [
        "The aim of the experiments is twofold: we show that (a) entailments can be learned from examples and (b) our kernel function over syntactic structures is effective to derive syntactic properties.",
        "The above goals can be achieved by comparing our cross-pair similarity kernel against (and in combination with) other methods."
      ]
    },
    {
      "heading": "3.1 Experimented kernels",
      "text": [
        "We compared three different kernels: (1) the kernel Kl((T', H'), (T'', H'')) based on the intra-pair",
        "lexical similarity siml (T, H) as defined in (Corley and Mihalcea, 2005).",
        "This kernel is defined as Kl ((T', H'), (T'', H'')) = siml (T', H') x siml (T'', H'').",
        "(2) the kernel Kl+K3 that combines our kernel with the lexical-similarity-based kernel; (3) the kernel Kl + Kt that combines the lexical-similarity-based kernel with a basic tree kernel.",
        "This latter is defined as Kt ((T', H'), (T'', H'')) = KT (T', T'') + KT (H', H'').",
        "We implemented these kernels within SVM-light (Joachims, 1999)."
      ]
    },
    {
      "heading": "3.2 Experimental settings",
      "text": [
        "For the experiments, we used the Recognizing Textual Entailment (RTE) Challenge data sets, which we name as D1, T1 and D2, T2, are the development and the test sets of the first and second RTE challenges, respectively.",
        "D1 contains 567 examples whereas T1, D2 and T2 have all the same size, i.e. 800 instances.",
        "The positive examples are the 50% of the data.",
        "We produced also a random split of D2.",
        "The two folds are D2(50%)' and D2(50%)''.",
        "We also used the following resources: the Char-niak parser (Charniak, 2000) to carry out the syntactic analysis; the wn :: s imi larity package (Pedersen et al., 2004) to compute the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) needed to implement the lexical similarity siml (T, H) as defined in (Corley and Mihalcea, 2005); SVM-lightTK (Moschitti, 2004) to encode the basic tree kernel function, KT, in SVM-light (Joachims, 1999)."
      ]
    },
    {
      "heading": "3.3 Results and analysis",
      "text": [
        "Table 1 reports the accuracy of different similarity kernels on the different training and test split described in the previous section.",
        "The table shows some important result.",
        "First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline, i.e. 50%.",
        "This accuracy (second line) is comparable with the best systems in the first RTE challenge (Dagan et al., 2005).",
        "The accuracy reported for the best systems, i.e. 58.6% (Glickman et al., 2005; Bayer et al., 2005), is not significantly far from the result obtained with Kl, i.e. 58.88%.",
        "Second, our approach (last column) is significantly better than all the other methods as it provides the best result for each combination of training and test sets.",
        "On the “Train: D 1-Test: T 1” test-bed, it exceeds the accuracy of the current state-of-the-art models (Glickman et al., 2005; Bayer et al., 2005) by about 4.4 absolute percent points (63% vs. 58.6%) and 4% over our best lexical similarity measure.",
        "By comparing the average on all datasets, our system improves on all the methods by at least 3 absolute percent points.",
        "Finally, the accuracy produced by our kernel based on co-indexed trees Kl + K3 is higher than the one obtained with the plain syntactic tree kernel Kl + Kt.",
        "Thus, the use of placeholders and co-indexing is fundamental to automatically learn entailments from examples."
      ]
    }
  ]
}
