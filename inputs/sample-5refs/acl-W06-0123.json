{
  "info": {
    "authors": [
      "Kun Yu",
      "Sadao Kurohashi",
      "Hao Liu",
      "Toshiaki Nakazawa"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W06-0123",
    "title": "Chinese Word Segmentation and Named Entity Recognition by Character Tagging",
    "url": "https://aclweb.org/anthology/W06-0123",
    "year": 2006
  },
  "references": [
    "acl-C02-1145",
    "acl-C04-1067",
    "acl-I05-3017",
    "acl-I05-3025",
    "acl-W03-1026",
    "acl-W04-2208",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes our word segmentation system and named entity recognition (NER) system for participating in the third SIGHAN Bakeoff.",
        "Both of them are based on character tagging, but use different tag sets and different features.",
        "Evaluation results show that our word segmentation system achieved 93.3% and 94.7% F-score in UPUC and MSRA open tests, and our NER system got 70.84% and 81.32% F-score in LDC and MSRA open tests."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Dealing with word segmentation as character tagging showed good results in last SIGHAN Bakeoff (J.K.Low et al.,2005).",
        "It is good at unknown word identification, but only using character-level features sometimes makes mistakes when identifying known words (T.Nakagawa, 2004).",
        "Researchers use word-level features (J.K.Low et al.,2005) to solve this problem.",
        "Based on this idea, we develop a word segmentation system based on character-tagging, which also combine character-level and word-level features.",
        "In addition, a character-based NER module and a rule-based factoid identification module are developed for post-processing.",
        "Named entity recognition based on character-tagging has shown better accuracy than word-based methods (H.Jing et al.,2003).",
        "But the small window of text makes it difficult to recognize the named entities with many characters, such as organization names (H.Jing et al.,2003).",
        "Considering about this, we developed a NER system based on character-tagging, which combines word-level and character-level features together.",
        "In addition, in-NE probability is defined in this system to remove incorrect named entities and create new named entities as post-processing."
      ]
    },
    {
      "heading": "2 Character Tagging for Word Segmentation and NER",
      "text": []
    },
    {
      "heading": "2.1 Basic Model",
      "text": [
        "We look both word segmentation and NER as character tagging, which is to find the tag sequence T* with the highest probability given a sequence of characters S=c1c2 ... cn.",
        "Then we assume that the tagging of one character is independent of each other, and modify formula 1 as",
        "Beam search (n=3) (Ratnaparkhi,1996) is applied for tag sequence searching, but we only search the valid sequences to ensure the validity of searching result.",
        "SVM is selected as the basic classification model for tagging because of its robustness to over-fitting and high performance (Sebastiani, 2002).",
        "To simplify the calculation, the output of SVM is regarded as P(ti|ci)."
      ]
    },
    {
      "heading": "2.2 Tag Definition",
      "text": [
        "Four tags ‘B, I, E, S’ are defined for the word segmentation system, in which ‘B’ means the character is the beginning of one word, ‘I’ means the character is inside one word, ‘E’ means the character is at the end of one word and ‘S’ means the character is one word by itself.",
        "For the NER system, different tag sets are defined for different corpuses.",
        "Table 1 shows the",
        "Sydney, July 2006. c�2006 Association for Computational Linguistics tag set defined for MSRA corpus.",
        "It is the product of Segment-Tag set and NE-Tag set, because not only named entities but also words are segmented in this corpus.",
        "Here NE-Tag ‘O’ means the character does not belong to any named entities.",
        "For LDC corpus, because there is no segmentation information, we delete NE-Tag ‘O’ but add tag ‘NONE’ to indicate the character does not belong to any named entities (Table 2)."
      ]
    },
    {
      "heading": "2.3 Feature Definition",
      "text": [
        "First, some features based on characters are defined for the two tasks, which are:",
        "(a) Cn (n=-2,-1,0,1,2) (b) Pu(C0)",
        "Feature Cn (n=-2,-1,0,1,2) mean the Chinese characters appearing in different positions (the current character and two characters to its left and right), and they are binary features.",
        "A character list, which contains all the characters in the lexicon introduced later, is used to identify them.",
        "Besides of that, feature Pu(C0) means whether C0 is in a punctuation character list.",
        "It is also binary feature and all the punctuations in the punctuation character list come from Penn Chinese Treebank 5.1 (N.Xue et al.,2002).",
        "In addition, we define some word-level features based on a lexicon to enlarge the window size of text in the two tasks, which are: (c) Wn (n=-1,0,1) Feature Wn (n=-1,0,1) mean the lexicon words in different positions (the word containing C0 and one word to its left and right) and they are also binary features.",
        "We select all the possible words in the lexicon that satisfy the requirements, not like only selecting the longest one in (J.K.Low et al.,2005).",
        "To create the lexicon, we use following steps.",
        "First, a lexicon from NICT (National Institute of Information and Communications Technology, Japan) is used as the basic lexicon, which is extracted from Peking University Corpus of the second SIGHAN Bakeoff (T.Emerson, 2005), Penn Chinese Treebank 4.0 (N.Xue et al.,2002), a Chinese-to-English Word-list1 and part of NICT corpus (K.Uchimoto et al.,2004; Y.J.Zhang et al.,2005).",
        "Then, all the words containing digits and letters are removed",
        "from this lexicon.",
        "At last, all the punctuations in Penn Chinese Treebank 5.1 (N.Xue et al.,2002) and all the words in the training data of UPUC and MSRA corpuses are added into the lexicon.",
        "Besides of above features, some extra features are defined only for NER task.",
        "First, we add some character-based features to improve the accuracy of person name recognition, which are CNn (n=-2,-1,0,1,2).",
        "They mean whether C n (n=-2,-1,0,1,2) belong to a Chinese surname list.",
        "All of them are binary features.",
        "The Chinese surname list contains the most famous 100 Chinese surnames, such as , , , (Zhao, Qian, Sun, Li).",
        "Then, we add some word-based features to help identify the organization name, which are WORGn (n=-1,0,1).",
        "They mean whether W n (n= -1,0,1) belong to an organization suffix list.",
        "All of them are also binary features.",
        "The organization suffix list is created by extracting the last word from all the organization names in the training data of both MSRA and LDC corpuses."
      ]
    },
    {
      "heading": "3 Post-processing",
      "text": [
        "Besides of the basic model, a NER module and a factoid identification module are developed in our word segmentation system for post-processing.",
        "In addition, we define in-NE probability to delete the incorrect named entities and identify new named entities in the post-processing phrase of our NER system."
      ]
    },
    {
      "heading": "3.1 Named Entity Recognition for Word Segmentation",
      "text": [
        "In this module, if two or more segments in the outputs of basic model are recognized as one named entity, we combine them as one segment.",
        "This module uses the same basic NER model as what we introduced in the previous section.",
        "But it only identifies person and location names, because organization names often contain more than one word.",
        "In addition, to keep the high accuracy of person name recognition, the features about organization suffixes are not used here."
      ]
    },
    {
      "heading": "3.3 NER Deletion and Creation",
      "text": [
        "In-word probability has been used in unknown word identification successfully (H.Q.Li et al., 2004).",
        "Accordingly, we define in-NE probability to help delete and create named entities (NE).",
        "Formula 3 shows the definition of in-NE probability for character sequence cici+1 ... ci+n.",
        "Here ‘# of cici+1 ... ci+n as NE’ is defined as TimeInrvE and the occurrence of cici+1 ... ci+n in different type of",
        "Then, we use some criteriato delete the incorrect NEandcreate new possible NE, inwhich differentthresholds are set fordifferent tasks."
      ]
    },
    {
      "heading": "4.1 Evaluation Setting",
      "text": [
        "SVMlight(T.Joachims, 1999) was usedas SVM tool.",
        "In addition, we usedthe MSRAtrain-ing corpus ofNERtaskinthis Bakeoffto train ourNERpost-processing module."
      ]
    },
    {
      "heading": "4.2 Results ofWord Segmentation",
      "text": [
        "We attended the opentrackofwordsegmenta",
        "n reasons.",
        "First, in MSRA corpus, they tend to segment one organization name as one word, such as (China Chamber of Commerce in USA).",
        "But our basic segmentation model segmented such word into several words, e.g. / / (USA/China/Chamber of Commerce), and our post-processing NER module does not consider about organization names.",
        "Second, our factoid identification rule did not combine the consequent DATE factoids into one word, but they are combined in MSRA corpus.",
        "For example, our system segmented the word 9 (9 o’clock in the evening) into three parts /9 / (Evening/9 o’clock/Exact).",
        "This error can be solved by revising the rules for factoid identification.",
        "Besides of that, we also found although our large lexicon helped identify the known word successfully, it also decreased the recall of OOV words (our Riv of UPUC corpus ranked 2n d, with only 0.6% decrease than the highest one, but Roov ranked 4th, with 8.8% decrease than the highest one).",
        "The large size of this lexicon is looked as the main reason.",
        "Our lexicon contains 221,407 words, in which 6,400 words are single-character words.",
        "It made our system easy to segment one word into several words, for example word (Economy Group) in UPUC corpus was segmented into (Economy) and (Group).",
        "Moreover, the large size of this lexicon also brought errors of combining two words into one word if the word was in the lexicon.",
        "For example, words (Only) and (Have) in MSRA corpus were identified as one word because there existed the word (Only) in our lexicon.",
        "We will reduce our lexicon to a reasonable size to solve these problems."
      ]
    },
    {
      "heading": "4.3 Results of NER",
      "text": [
        "We also attended the opentrackofNERtask forbothLDC corpus andMSRAcorpus.",
        "Table 5 andTable 6 give the evaluationresults.",
        "There wereonly 3 participants in the open trackofLDC corpus and ourgroup gotthe best F-score.",
        "In addition, among all the 11 participants for MSRA corpus, our system ranked 6th",
        "was only 1.1% lowerthanthe highest one and 0.2% lowerthanthe secondone.",
        "It showed that ourcharacter-tagging approachwas feasible.",
        "But the F-score ofMSRAcorpus was only higher thanone participant in all the 10 groups (the highest one was 97.9%).",
        "Erroranalysis shows that there are two mai by F-score.",
        "It showed the validity of our character-tagging method for NER.",
        "But for location name (LOC) in LDC corpus, both the precision and recall of our NER system were very low.",
        "It was because there were too few location names in the training data (there were only 476 LOC in the training data, but 5648 PER, 5190 ORG and 9545 GPE in the same data set).",
        "Besides of that, error analysis shows there are four types of main errors in the NER results.",
        "First, some organization names were very long and can be divided into several words, in which parts of them can also be looked as named entities.",
        "In such case, our system only recognized the small parts as named entities.",
        "For example, (Fei Zhengqing Eastern Asia Research Center of Harvard Univ.)",
        "was an organization name.",
        "But our system recognized it as (Harvard Univ.",
        ")/ORG+ (Fei Zheng Qing)/PER+ (Eastern Asia)/LOC+ (Research Center)/ORG.",
        "Adding more context features may be useful to resolve this issue.",
        "In addition, our system was not good at recognizing foreign person names, such as (Riordan), and abbreviations, such as (Los Angeles), if they seldom or never appeared in training corpus.",
        "It is because the use of the large lexicon decreased the unknown word identification ability of our NER system simultaneously.",
        "Third, the in-NE probability used in post-processing is helpful to identify named entities which cannot be recognized by the basic model.",
        "But it also recognized some words which can only be regarded as named entities in the local context incorrectly.",
        "For example, our system recognized (Najing) as GPE in (Send to Najing for remedy) in LDC corpus.",
        "We will consider about adding the in-NE probability as one feature into the basic model to solve this problem.",
        "At last, in LDC corpus, they combine the attributive of one named entity (especially person and organization names) with the named entity together.",
        "But our system only recognized the named entity by itself.",
        "For example, our system only recognized (Liu Gui Fang) as PER in the reference person name (Liu Gui Fang who does not know the inside)."
      ]
    },
    {
      "heading": "5 Conclusion and Future Work",
      "text": [
        "Through the participation of the third SIGHAN Bakeoff, we found that tagging characters with both character-level and word-level features was effective for both word segmentation and NER.",
        "While, this work is only our preliminary attempt and there are still many works needed to do in the future, such as the control of lexicon size, the use of extra knowledge (e.g. pos-tag), the feature definition, and so on.",
        "In addition, our word segmentation system only combined the NER module as post-processing, which resulted in that lots of information from NER module cannot be used by the basic model.",
        "We will consider about combining the NER and factoid identification modules into the basic word segmentation model by defining new tag sets in our future work."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "We would like to thank Dr. Kiyotaka Uchi-moto for providing the NICT lexicon."
      ]
    },
    {
      "heading": "Reference",
      "text": []
    }
  ]
}
