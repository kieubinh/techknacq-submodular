{
  "info": {
    "authors": [
      "Doo Soon Kim",
      "Kunal Verma",
      "Peter Yeh"
    ],
    "book": "EMNLP",
    "id": "acl-D12-1099",
    "title": "Building a Lightweight Semantic Model for Unsupervised Information Extraction on Short Listings",
    "url": "https://aclweb.org/anthology/D12-1099",
    "year": 2012
  },
  "references": [
    "acl-D11-1144",
    "acl-H05-1059",
    "acl-N06-1041",
    "acl-P05-1046",
    "acl-P06-2094",
    "acl-P07-1036",
    "acl-P10-1031",
    "acl-P10-1079",
    "acl-P11-1037",
    "acl-P11-1054",
    "acl-P11-1098",
    "acl-P11-1120",
    "acl-P98-1013"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Short listings such as classified ads or product listings abound on the web.",
        "If a computer can reliably extract information from them, it will greatly benefit a variety of applications.",
        "Short listings are, however, challenging to process due to their informal styles.",
        "In this paper, we present an unsupervised information extraction system for short listings.",
        "Given a corpus of listings, the system builds a semantic model that represents typical objects and their attributes in the domain of the corpus, and then uses the model to extract information.",
        "Two key features in the system are a semantic parser that extracts objects and their attributes and a listing-focused clustering module that helps group together extracted tokens of same type.",
        "Our evaluation shows that the semantic model learned by these two modules is effective across multiple domains."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Short listings such as classified ads or product listings are prevalent on the web.",
        "These texts are generally concise ?",
        "around 10 words in length.",
        "Fig.",
        "1 shows some example listings.",
        "Due to the recent explosive growth of such listings, extracting information from them becomes crucial for tasks such as faceted search and reasoning.",
        "For example, consider an online shopping site on which information about merchandises for sale is posted.",
        "Detecting brands/styles/features that are frequently mentioned in the postings would allow a company to design a better marketing strategy.",
        "Most Information Extraction (IE) techniques developed for formal texts, however, would be inapplicable to listings because of their informal and idiosyncratic styles.",
        "For example, typos, abbreviations and synonyms often appear and should be resolved (e.g., apartment/apt, bike/bicycle).",
        "Symbols could have the special meanings (e.g., x in 2x2 in Fig. 1 indicates number of bedrooms and bathrooms).",
        "To-kenization based only on space is insufficient (e.g., RawlingBaseball in Fig. 1).",
        "Multiwords such as granite top should also be detected.",
        "Applying off-the-shelf parsers is infeasible because of unusual phrasal forms in most listings, such as a long sequence of nouns/adjectives (e.g., ?New Paint Wood Floors New Windows Gated Complex?)",
        "To address these challenges, several approaches have applied machine learning algorithms (Ghani et al., 2006) (Putthividhya and Hu, 2011) or an external knowledge base (Michelson and Knoblock, 2005).",
        "These approaches, however, commonly require human supervision to produce training data or to build a knowledge base.",
        "This is expensive, requiring repeated manual effort whenever a new domain or a new set of information to be extracted is introduced.",
        "In this paper, we present an unsupervised IE system for listings.",
        "The system extracts tokens from a corpus of listings and then clusters tokens of the same types, where each resulting cluster corresponds to an information type (e.g., size, brand, etc.).",
        "For formal texts, contexts (e.g., surrounding words) have been a major feature for word clustering (Turney and Pantel, 2010).",
        "This feature alone, however, is insufficient for short listings because of lack of contextual clues in short listings.",
        "To address this limitation of context-based clustering, we first identify common types of information (main objects and their attributes) represented in listings and apply customized clustering for these types.",
        "Specifically, we define a semantic model to explicitly represent these information types, and based on the semantic model, develop two components to improve clustering ?",
        "a shallow semantic parser and listing-focused clustering module.",
        "The semantic parser specifically focuses on extracting main objects and the listing-focused clustering module helps group together extracted tokens of the same type.",
        "Our evaluation shows that our two main contributions (shallow semantic parser and listing-focused clustering) significantly improve performance across three different domains ?",
        "Craigslist, EBay, and FatWallet.",
        "Our system achieves .50?.65 F1-score for the EBay and the FatWallet datasets based on gold standards constructed by human annotators.",
        "For the Craigslist dataset, which is more difficult than the other two, F1-score is .35."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "IE on Listings.",
        "(Ghani et al. 2006) (Putthividhya and Hu, 2011) propose semi-supervised approaches to extract product types and their attributes from product listings.",
        "(Ghani et al. 2006) applies the EM (Expectation-Maximization) algorithm to incorporate unlabelled data.",
        "(Putthividhya and Hu, 2011) uses unlabelled data to build dictionaries of values to be extracted (e.g., brand or model names), which are then used as a feature for a machine learning system.",
        "(Michelson and Knoblock, 2005) uses a manually-crafted knowledge base called reference set to define standard forms of values to be extracted.",
        "Using a string edit function, the system then identifies tokens in listings that have a low distance score with the values defined in the reference set.",
        "They also propose a semi-supervised method to building reference sets (Michelson and Knoblock, 2009).Unlike these systems, our approach is unsupervised.",
        "Unsupervised Information Extraction.",
        "Most unsupervised IE systems produce clusters for tokens of same type extracted from a corpus of unlabelled texts.",
        "(Chambers and Jurafsky, 2011) (Poon and Domingos, 2010) (Chen et al. 2011) focus on extracting frame-like structures (Baker et al. 1998) by defining two types of clusters, event clusters and role clusters.",
        "Event clusters define an event (a situation or a frame) such as BOMBING by clustering verbs/nominalized verbs such as {kill, explosion}.",
        "Role clusters define the semantic roles of the event (e.g., {terrorist, gunman} for the Perpetrator role in BOMBING).",
        "Similarly, our system defines two types of clusters ?",
        "the main concept clusters (e.g., TV or book) and the attribute clusters (e.g., size, color).",
        "(Chambers and Jurafsky, 2011) is similar to our approach in that it learns a semantic model, called template, from unlabelled news articles and then uses the template to extract information.",
        "Our system is different because it focuses on informal listings, which the components (such as a parser) used by these systems cannot handle.",
        "Field Segmentation (Sequence Modelling).",
        "This task focuses on segmenting a short text, such as bibliographies or listings.",
        "(Grenager et al. 2005) presents an unsupervised HMM based on the observation that the segmented fields tend to be of multiple words length.",
        "(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shop-ing for the NEIGHBORHOOD attribute) in an unsupervised setting.",
        "(Chang et al. 2007) incorporates domain specific constraints in semi-supervised learning.",
        "Our task is different than these systems because we focus on extracting information to enable a variety of automated applications such as business",
        "intelligence reporting, faceted search or automated reasoning, rather than segmenting the text.",
        "The segmented fields are often a long unstructured text (e.g., 2 bath 1 bed for size rather than 2 for BathroomNum and 1 for BedroomNum).",
        "IE on Informal Texts.",
        "IE on informal texts is getting much attention because of the recent explosive growth of these texts.",
        "The informal texts that are attempted for IE include online forums (Gruhl et al., 2009), SMS (Beaufort et al. 2010), twitter messages (Liu et al. 2011)."
      ]
    },
    {
      "heading": "3 Our Approach",
      "text": [
        "Given a corpus of listings for a domain of interest, our system constructs a semantic model that represents the types of information and their values to be extracted.",
        "Our system then uses the resulting model to extract both the type and value from the corpus 1.",
        "We first describe the semantic model and then the two key steps for creating this model ?",
        "shallow semantic parsing and listing-focused clustering.",
        "Fig.",
        "3 illustrates these two steps with example listings."
      ]
    },
    {
      "heading": "3.1 Semantic Model",
      "text": [
        "Our semantic model captures two important pieces of information ?",
        "the main concept and its attributes.",
        "This representation is based on the observation that most listings (e.g. rentals, products) describe the attributes of a single object (i.e. the main concept in our model).",
        "Our system takes advantage of this observation by applying customized clustering for each type of information in the model, which results in better performance compared to a one-size-fits-all algorithm.",
        "Moreover, this model is general enough to be applicable across a wide range of domains.",
        "We quantitatively show both benefits in our evaluation.",
        "Fig.",
        "2 illustrates our model along with an instantiation for rental listings.",
        "The main concept is a cluster containing tokens referencing the main object in the listing.",
        "For example, in the rental listing, the main concept cluster includes tokens such as house, condo, and townhouse.",
        "Each attribute of the main concept (e.g.",
        "Address, BedroomNum, etc.)",
        "is also a cluster, and two types",
        "listings.",
        "+ indicates multiple clusters can be created.",
        "of attributes are defined in our model ?",
        "quantitative attributes and qualitative ones.",
        "Quantitative attributes capture numeric values (e.g. 1 bedroom, 150 Hz, and 70 kg), and are generally a number followed by a token indicating the attribute (e.g., unit of measurement).",
        "Hence, clusters for quantitative attributes include these indicator tokens (see Fig. 2).",
        "Qualitative attributes capture descriptions about the main concept (e.g., address, shipping information, condition).",
        "The values of these attributes generally appear in listings without explicitly mentioning the names of these attributes.",
        "Hence, the clusters for qualitative attributes include tokens corresponding to the values themselves (e.g. washer hookup)."
      ]
    },
    {
      "heading": "3.2 Shallow Semantic Parser",
      "text": [
        "Our Shallow Semantic Parser (SSP) analyzes an input corpus to produce a partial semantic model.",
        "SSP first performs preprocessing and multiword detection.",
        "SSP then identifies which resulting tokens are the main concepts and which are their attributes.",
        "SSP preprocesses the corpus through three steps.",
        "(1) SSP cleans the corpus by removing duplicate listings and HTML expressions/tags.",
        "(2) SSP tok",
        "enizes each listing based on spaces along with custom heuristics ?",
        "e.g., handling alphanumeric tokens starting with numbers (e.g., 3bedroom to 3 bedroom) and mixed case tokens (e.g., NikeShoes to Nike Shoes).",
        "(3) SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005).",
        "To improve accuracy, SSP assigns to a token the most frequent POS across all occurrences of that token.",
        "This heuristic works well because most tokens in focused domains, like listings, have only one POS.",
        "SSP then detects multiword tokens based on the following rules:",
        "as Location, BedroomNum) are included to help understand the figure.",
        "They are not produced by the system.",
        "1.",
        "If a bigram (e.g., top floor) in a listing fre",
        "quently appears as either a single or dashed token (e.g., TopFloor or top-floor) in other listings, then the bigram is regarded as a multiword.",
        "2.",
        "For each bigram, w1 w2 (excluding symbols and numbers), if the conditional probability of the bigram given either w1 or w2 (i.e., p(w1w2 | w1(or w2)) is high (over 0.75 in our system)), the bigram is considered as a candidate multiword.",
        "This rule tests the tendency of two tokens appearing together when either one appears.",
        "However, this test alone is insufficient, as it often generates coarse-grained results ?",
        "e.g., baseball glove, softball glove, and Hi-Def TV 2.",
        "To prevent this problem, for each w2, we measure the entropy over the distribution of the tokens in the w1 position.",
        "Our intuition is that high variability in the w1 position (i.e., high entropy) indicates that the multiword is likely a breakable phrase.",
        "Hence, those candidates with high entropy are removed.",
        "SPP repeatedly applies the above rules to acquire multiwords of arbitrary length.",
        "In our implementation, we limit multiword detection up to four-gram.",
        "SSP then identifies the main concepts (mc words) and their attributes (attrs) to produce a partial semantic model.",
        "This process is guided by the observation that main concepts tend to appear as head nouns in a listing and attributes as the modifiers of these head nouns (see the examples in Fig. 3).",
        "2Even though these examples are legitimate multiwords, they overlook useful information such as baseball and softball are types of gloves and Hi-Def is an attribute of TV.",
        "Algorithm 1 describes the discovery process of mc words and attrs.",
        "First, SSP initializes attrs with tokens that are likely to be a modifier (line 2), by choosing tokens that frequently appear as the object of a preposition within the corpus ?",
        "e.g., for rent, with washer and dryer, for baseball.",
        "SSP then iteratively performs two steps ?",
        "PARSE and EXPANDMODEL (lines 3 ?",
        "6) ?",
        "in a bootstrap manner (see Fig. 4).",
        "PARSE tags the noun tokens in each listing as either head nouns or modifiers.",
        "Specifically, PARSE first assesses if a listing is ?hard?",
        "to parse (line 10) based on two criteria ?",
        "(1) the listing contains a long sequence of nouns (seven words or longer in our system) without any prepositions (e.g., worth shutout series 12?",
        "womens fast-pitch softball fielders glove s0120 lefty); and (2) the majority of these nouns do not appear in mc words and attrs (e.g., over 70% in our system).",
        "The listings meeting these criteria are generally difficult to recognize the head noun without any semantic knowledge.",
        "PARSE will revisit these listings in the next round as more mc words and attrs are identified.",
        "If a listing does not meet these criteria, PARSE tags nouns appearing in mc words and attrs as head nouns and modifiers respectively (line 11).",
        "If this step fails to recognize a head noun, a heuristic is used to identify the head noun ?",
        "it identifies the first noun phrase by finding a sequence of nouns/adjectives/numbers, and then tags as the head noun the last noun in the phrase that is not tagged as a modifier (line 13).",
        "For example, in the first listing of Fig. 3, brentwood apts.",
        "is the first noun phrase that meets the condition above; and hence apt.",
        "is tagged as the head noun.",
        "The remaining untagged nouns in the listing are tagged as modifiers (line 15).",
        "1: Input: POS-tagged corpus, corp 2: Initialize attrs 3: repeat 4: (hn,mod) = Parse(corp, mc words, attrs) 5: (mc words,attrs) = ExpandModel(hn,mod) 6: until mc words, attrs not changed 7: 8: function PARSE(mc words, attrs) 9: for all each listing do 10: if parsible then 11: Parse with mc words, attrs 12: if headnoun is not tagged then 13: Tag the last noun in the first noun phrase that are not a modifier as hn 14: end if 15: Tag the other nouns as mod 16: end if 17: end for 18: end function 19: 20: function EXPANDMODEL(corp) 21: For each token, calculate a ratio of as a head noun to as a modifier 22: Add tokens with high ratio to mc words 23: Add tokens with low ratio to attrs 24: end function",
        "EXPANDMODEL assigns tokens to either mc words or attrs based on the tags generated by PARSE.",
        "For each token, EXPANDMODEL counts the frequency of the token being tagged as a head noun and as a modifier.",
        "If a token is predominately tagged as a head noun (or a modifier), the token is added to mc words (or attrs) 3.",
        "This bootstrap method is advantageous 4 because SSP can initially focus on easy cases ?",
        "i.e., mc words and attrs that can be detected with high confidence, such as condo(mc words) and bed-room(attrs), which often appear as a head noun and a modifier in the easy-to-parse listings.",
        "These results can help the system to parse more difficult",
        "listings.",
        "For example, identifying condo(mc words) helps parsing the following more difficult listing, ?2 bedroom 1 bathroom condo large patio washer dryer available?",
        "?",
        "condo would be tagged as a head noun and the rest of the nouns as modifiers.",
        "The result of this step is a partial semantic model that contains a cluster for the main concept and a list of candidate attribute tokens."
      ]
    },
    {
      "heading": "3.3 Listing-Focused Clustering",
      "text": [
        "Listing-Focused Clustering (LFC) further expands the partial semantic model (constructed by SSP) by grouping the remaining candidate attribute tokens into attribute clusters ?",
        "i.e. one cluster for each attribute of the main concept.",
        "LFC may also add a token to the main concept cluster if appropriate.",
        "For formal texts, distributional similarity is widely used for clustering words because the contextual clues in these texts are sufficiently discriminative (Lin and Pantel, 2001).",
        "This feature alone, however, is insufficient for listings because they lack discriminative contexts due to the short length.",
        "Hence, our approach augments context-based similarity with the following rules (presented in order of precedence), based on general properties we observed from listing data across various domains.",
        "?",
        "Two quantitative attribute tokens cannot be placed into the same cluster if they frequently appear together in a listing.",
        "For example, bed and bath should not be clustered because they frequently appear together (e.g. 2 bed / 2bath).",
        "This rule is based on the observation that a quantitative attribute is likely to appear only once in a listing.",
        "To enforce this restriction, for all pairs of tokens, t1 and t2, LFC measures the conditional probability of the pair appearing together in a listing given the appearance of either",
        "t1 and t2.",
        "If any of these conditional probabilities are high, t1 and t2 are not clustered.",
        "?",
        "Attribute types are strongly enforced by never clustering together a quantitative attribute token and a qualitative attribute token.",
        "The type of a token is determined by analyzing the immediate preceding tokens throughout the corpus.",
        "If the preceding tokens are generally numbers, then LFC regards the token as a quantitative attribute.",
        "Otherwise, the token is regarded as a qualitative attribute.",
        "?",
        "Two tokens are similar if the characters in one token appear in the other, preserving the order (e.g., bdrm and bedroom) If the above rules fail to determine the similarity between two tokens, LFC reverts to context-based similarity.",
        "For each token, LFC creates a context vector containing frequencies of the context words around the token with a window size of two.",
        "For example, in the first sentence in Fig. 3, the context words around apts.",
        "are l-start (beginning of the sentence), l-brentwood, r-with, and r-3 5.",
        "The frequencies in these vectors are also weighted using PMI scores (pointwise mutual information) between a token and its context words, as suggested by (Turney and Pantel, 2010).",
        "The intuition is that a high PMI indicates a context word is strongly associated with a token and hence has high discriminative power.",
        "We also apply a smoothing function suggested in (Tur-ney and Pantel, 2010) to mitigate PMI's bias towards infrequent events.",
        "The similarity score is based on a cosine similarity between the two weighted vectors.",
        "Based on this similarity function, LFC applies agglomerative clustering (with average linkage) to produce attribute clusters (or to expand the main concept cluster).",
        "However, calculating similarity scores for all pairs of tokens is expensive.",
        "To address this problem, LFC performs clustering in two steps.",
        "First, LFC performs agglomerative clustering on all pairs of tokens with high frequency 6.",
        "LFC then calculates the similarity between each low-frequency token and the clusters resulting from the previous step.",
        "If the similarity score is over a user-specified",
        "for development/testing.",
        "Avg word indicates the average number of words in a listing.",
        "The development dataset is used to tune the parameters.",
        "threshold, then LFC addes the token to the cluster.",
        "If the score is less than the threshold but the token still appears relatively frequently, then LFC creates a new cluster for the token."
      ]
    },
    {
      "heading": "4 Evaluation",
      "text": [
        "We perform two evaluations to assess the performance of our approach.",
        "First, we evaluate how well our approach extracts the correct type (e.g., BedroomNum) and value (e.g., 2) across multiple domains.",
        "Next, we evaluate the contribution of each main component in our approach ?",
        "i.e., shallow semantic parsing and listing-focused clustering ?",
        "through an ablation study.",
        "We also provide an in-depth error analysis along with how our system's performance is affected by the corpus size."
      ]
    },
    {
      "heading": "4.1 Evaluation Setup",
      "text": [
        "We first assemble three different listing datasets for our evaluation ?",
        "housing rental advertisements from Craigslist (Rent Ad), auction listings of baseball gloves from EBay (Glove), and hot deal postings of TV/Projector from FatWallet (TV Deal) 7.",
        "Table 1 shows the size of each dataset, and example listings are shown in Fig. 1 8.",
        "We also include example extractions for each domain in Table.",
        "3.",
        "We then construct a gold standard by employing two independent human annotators.",
        "To do this, we first define the information types (i.e. main concept and attribute) for each dataset (and hence the targets for extraction).",
        "We use attributes from EBay",
        "system is sensitive to the similarity score threshold in agglomerative clustering but less sensitive to the other parameters.",
        "Hence, we tuned the similarity threshold for each domain while fixing the values for the other parameters across different domains.",
        "show only top five types in terms of the number of extractions made by the annotators.",
        "The parentheses indicate inter-annotator agreement based on exact match between two annotators (first number) and partial match (second number).",
        "Exact agreement for qualitative attributes (e.g., location, neighborhood, series name) is found to be difficult."
      ]
    },
    {
      "heading": "Rent Ad",
      "text": [
        "housing type studio, townhome, condo, townhouse, cottage, num bedroom bd, bed, br, bedroom, bdrm, bedrooms neighborhood downtown, San Francisco, china town Glove product type mitt, base mitt, glove, glove mitt size ?, inch, in brand rawlings, louisville slugger, mizuno, wilson"
      ]
    },
    {
      "heading": "TV Deal",
      "text": [
        "product type hdtv, wall mount, monitor size ?, inch seller wallmart, amazon, newegg, best buy",
        "qualitative attributes (e.g., num bedroom, size) and the quantitative attributes (e.g., neighborhood, brand, seller) used in the experiment as the starting point for Glove and TV Deal; and attributes from Rent.com9 for Rent Ad.",
        "We review these attributes with two independent Subject Matter Experts (SMEs) to identify (and include) additional, missing attributes that are useful for analytics and reporting.",
        "In total, 19 types (Rent Ad), 15 types (Glove) and 19 types (TV Deal) are defined.",
        "For each dataset, we randomly select 100 listings from the Test listings, and instruct each annotator to extract values from these listings, based on the information types for the dataset.",
        "Table 2 shows the defined attributes of each data set and the inter-annotator agreement across the attributes 10.",
        "Finally, we apply our system to the Test listings in each dataset; and evaluate the extraction results",
        "ings in which the two annotators agree divided by 100.",
        "P(e), the chance agreement probability, is calculated by",
        "notes a probability of extracting vi of the type Ti by the annotator j. P ((Ti, vi)) is calculated by the frequency of (Ti, vi) extracted divided by the frequency of Ti extracted.",
        "against the gold standards using the metrics of precision (P), recall (R), and F1-score (a harmonic mean of precision and recall).",
        "Each extraction result is a tuple (Ti, vi) where Ti is the information type (e.g., BedroomNum) and vi is the value (e.g. 2).",
        "# correct extractions by our system Total # extractions by an annotator We say that an extraction result is correct if vi matches exactly the value extracted by the annotator and Ti matches the information type assigned to vi by the annotator.",
        "To enforce this criteria, we match the attribute clusters produced by our system to the information types.",
        "This matching step is needed because our approach is unsupervised and hence the clusters are unlabelled.",
        "We use two methods for this matching step ?",
        "many-to-one mapping and one-to-one mapping.",
        "For many-to-one mapping (as used in (Chen et al. 2011)), we match an attribute cluster to the information type whose values (as extracted by the annotator) have the highest overlap with the",
        "values of the cluster.",
        "Hence, many attribute clusters can map to the same information type.",
        "This method, however, has one major disadvantage: high performance can be easily achieved by creating small-sized clusters ?",
        "e.g., singleton clusters in the extreme case.",
        "To mitigate this problem, we also use a one-to-one mapping method ?",
        "i.e. at most one attribute cluster (i.e. the one with the best overlap) can be mapped to one information type.",
        "We report results for both methods.",
        "We also report results for partial matches using the same metrics above.",
        "We say that an extraction result is partially correct if vi partially matches the value extracted by the annotator (hardwood vs. hardwood floors) and Ti matches the information type assigned to vi by the annotator.",
        "11"
      ]
    },
    {
      "heading": "4.2 Performance Across Multiple Domains",
      "text": [
        "Table 4 shows the system performance result across the domains.",
        "Considering our approach is unsupervised, the result is encouraging.",
        "For the baseball glove and the TV dataset, F-score is .51 and .66 (in one-to-one mapping).",
        "For the rent ad, which is more difficult, F-score is .39.",
        "We hypothesize that the low F1-score in the rent ad dataset is the result of poor extraction due to spurious tokens (e.g., our community, this weather).",
        "To test this hypothesis, we measure the performance of our system only on the extraction task (i.e., excluding the information type assignment task).",
        "Table 5 shows that the performance on extraction for the rent ad dataset is the lowest, confirming our hypothesis.",
        "We also measure the performance per each information type.",
        "Fig.",
        "5 shows the result, revealing several facts.",
        "First, the main concept clusters (housing type and product type) achieve a high F1-score, showing the benefit of our semantic parser.",
        "Sec-11We could not compare our system to other systems for several reasons.",
        "First, to the best of our knowledge, no unsupervised IE system has been built specifically for short listings.",
        "Second, semi-supervised systems such as (Putthividhya and Hu, 2011) (Michelson and Knoblock, 2005) require domain-specific dictionaries, which are expensive to build and scale.",
        "Third, even developing a supervised IE system is non-trivial.",
        "In our preliminary evaluation with the (linear chain) conditional random field (170/30 training/testing listings) using basic features (the lexemes and POS of the current word and words in the two left/right windows), precision/recall/F1-score are .5/.33/.4.",
        "This result is no better than our system.",
        "More training data and/or better features seem to be required.",
        "many-to-one mappings (second row) combined across both annotators.",
        "Full indicates exact match between system's extraction and annotators?",
        "extraction.",
        "(Par) indicates partial match.",
        "The parentheses indicate the total number of extraction made by our system and the annotators (averaged) respectively.",
        "ond, the quantitative attributes (e.g., num bedroom, glove size, screen size) generally have a higher F1 than the qualitative attributes (e.g., location, neighborhood, series name).",
        "These qualitative attributes, in fact, have a low inter-annotator agreement (e.g., - .64, .55 for location and neighborhood in Rent Ad and .51 for series name in Glove), indicating the difficulty of exactly predicting the extractions made by the annotators.",
        "If we consider the partial match or the extraction only match (Ext) for those qualitative attributes, their F1-scores are significantly higher than the exact match in the Full task."
      ]
    },
    {
      "heading": "4.3 Ablation Study",
      "text": [
        "To evaluate our semantic parser and listing-focused clustering module, we ablate these two components to create four versions of our system for comparison ?",
        "Baseline, Baseline+LFC, Baseline+SSP and Full System.",
        "Baseline performs a space-based tok-enization followed by clustering based only on the context feature.",
        "Baseline+LFC and Baseline+SSP add listing-focused clustering and shallow semantic parser features respectively.",
        "The Full system uses both features.",
        "The results shown in Table 6 lead to the following observations.",
        "First, the use of SSP (Base-line+SSP) makes an improvement for all categories except Full(Par) in the glove dataset.",
        "This is because SSP identifies main concepts accurately.",
        "Second, while LFC by itself (Bseline+LFC) is effective only in the glove dataset, it has the best F1-score in all three domains when combined with SSP (Full System).",
        "The result also shows that the simple space-based tokenization and the context-based clustering (Baseline) is insufficient for handling short listings."
      ]
    },
    {
      "heading": "4.4 Error Analysis and Corpus Size Effect",
      "text": [
        "We analyze the error types for the wrong extraction made by our system.",
        "Specifically, for each error, we assign it to one (or more) of the following causes: (1) a cluster (and hence attribute type) was excluded due to the 1-to-1 mapping methodology described above (NOT IN MAPPING); (2) the value extracted by the system was not extracted by any of the annotators (WRONG EXT); (3) wrong information type ?",
        "i.e., the token belonged to a wrong cluster (WRONG TYPE); (4) incorrect tokenization for an information type (TK-<type name>).",
        "Table 7 shows the result.",
        "In all three domains, NOT IN MAPPING is a major source of error, indicating the system's clusters are too fine-grained as compared to the gold standard.",
        "WRONG EXT is another source of error (especially in the housing rental), indicating the system should extract more informative tokens.",
        "Tokenization on the qualitative attributes (neighborhood, series name, display technology in Table 7) should be improved also.",
        "Finally, we measure the effect of the corpus size on the system performance.",
        "Fig.",
        "6 shows how the F1-score varies with the corpus size 12.",
        "It shows that a small corpus size is sufficient for achieving good performance.",
        "We hypothesize that, for focused domains such as our dataset, only a couple of hundred listings are sufficient to acquire meaningful statistics."
      ]
    },
    {
      "heading": "5 Conclusion and Future Work",
      "text": [
        "We presented an unsupervised IE system on short listings.",
        "The key features in our system are a shal-12Due to the space limitation, we include only the rent domain result.",
        "However, all three datasets follow a similar pattern.",
        "for the rent domain low semantic parser and a listing-focused clustering module.",
        "Our evaluation shows the benefits of the two features across multiple domains.",
        "To improve our system further, we plan the following works.",
        "First, we plan to compare our system with supervised systems to identify the gap between the two systems.",
        "Second, as in (Poon and Domingos, 2010), we plan to explore a joint learning method to combine the tasks of tokenization, forming the main concept cluster and forming the attribute clusters; these tasks depend on the outputs of one another.",
        "Finally, we plan to explore that external knowledge resources such as DBPedia (Auer et al. 2007) and FreeBase (Bollacker et al. 2008) can be used to further improve performance."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "We would like to thank Colin Puri and Rey Vasquez for their contribution to this work.",
        "We also thank the anonymous reviewers for their helpful comments and suggestions for improving the paper."
      ]
    }
  ]
}
