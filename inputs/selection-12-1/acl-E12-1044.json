{
  "info": {
    "authors": [
      "Atsushi Hanamoto",
      "Takuya Matsuzaki",
      "Junâ€™ichi Tsujii"
    ],
    "book": "EACL",
    "id": "acl-E12-1044",
    "title": "Coordination Structure Analysis using Dual Decomposition",
    "url": "https://aclweb.org/anthology/E12-1044",
    "year": 2012
  },
  "references": [
    "acl-C04-1204",
    "acl-D07-1064",
    "acl-D10-1001",
    "acl-H05-1105",
    "acl-J93-2004",
    "acl-P07-1086",
    "acl-P09-1109"
  ],
  "sections": [
    {
      "text": [
        "1.",
        "Web Search & Mining Group, Microsoft Research Asia, China"
      ]
    },
    {
      "heading": "Jun?ichi Tsujii 2 Abstract",
      "text": [
        "Coordination disambiguation remains a difficult sub-problem in parsing despite the frequency and importance of coordination structures.",
        "We propose a method for disambiguating coordination structures.",
        "In this method, dual decomposition is used as a framework to take advantage of both HPSG parsing and coordinate structure analysis with alignment-based local features.",
        "We evaluate the performance of the proposed method on the Genia corpus and the Wall Street Journal portion of the Penn Treebank.",
        "Results show it increases the percentage of sentences in which coordination structures are detected correctly, compared with each of the two algorithms alone."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Coordination structures often give syntactic ambiguity in natural language.",
        "Although a wrong analysis of a coordination structure often leads to a totally garbled parsing result, coordination disambiguation remains a difficult sub-problem in parsing, even for state-of-the-art parsers.",
        "One approach to solve this problem is a grammatical approach.",
        "This approach, however, often fails in noun and adjective coordinations because there are many possible structures in these coordinations that are grammatically correct.",
        "For example, a noun sequence of the form ?n0 n1 and n2 n3?",
        "has as many as five possible structures (Resnik, 1999).",
        "Therefore, a grammatical approach is not sufficient to disambiguate coordination structures.",
        "In fact, the Stanford parser (Klein and Manning, 2003) and Enju (Miyao and Tsujii, 2004) fail to disambiguate a sentence I am a freshman advertising and marketing major.",
        "Table 1 shows the output from them and the correct coordination structure.",
        "The coordination structure above is obvious to humans because there is a symmetry of conjuncts (-ing) in the sentence.",
        "Coordination structures often have such structural and semantic symmetry of conjuncts.",
        "One approach is to capture local symmetry of conjuncts.",
        "However, this approach fails in VP and sentential coordinations, which can easily be detected by a grammatical approach.",
        "This is because conjuncts in these coordinations do not necessarily have local symmetry.",
        "It is therefore natural to think that considering both the syntax and local symmetry of conjuncts would lead to a more accurate analysis.",
        "However, it is difficult to consider both of them in a dynamic programming algorithm, which has been often used for each of them, because it explodes the computational and implementational complexity.",
        "Thus, previous studies on coordination disambiguation often dealt only with a restricted form of coordination (e.g. noun phrases) or used a heuristic approach for simplicity.",
        "In this paper, we present a statistical analysis model for coordination disambiguation that uses the dual decomposition as a framework.",
        "We consider both of the syntax, and structural and semantic symmetry of conjuncts so that it outperforms existing methods that consider only either of them.",
        "Moreover, it is still simple and requires onlyO(n4) time per iteration, where n is the number of words in a sentence.",
        "This is equal to that of coordination structure analysis with alignment-based local features.",
        "The overall system still has a quite simple structure because we need just slight modifications of existing models in this approach,",
        "correct coordination structure so we can easily add other modules or features for future.",
        "The structure of this paper is as follows.",
        "First, we describe three basic methods required in the technique we propose: 1) coordination structure analysis with alignment-based local features, 2) HPSG parsing, and 3) dual decomposition.",
        "Finally, we show experimental results that demonstrate the effectiveness of our approach.",
        "We compare three methods: coordination structure analysis with alignment-based local features, HPSG parsing, and the dual-decomposition-based approach that combines both."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Many previous studies for coordination disambiguation have focused on a particular type of NP coordination (Hogan, 2007).",
        "Resnik (1999) disambiguated coordination structures by using semantic similarity of the conjuncts in a taxonomy.",
        "He dealt with two kinds of patterns, [n0 n1 and n2 n3] and [n1 and n2 n3], where ni are all nouns.",
        "He detected coordination structures based on similarity of form, meaning and conceptual association between n1 and n2 and between n1 and n3.",
        "Nakov and Hearst (2005) used the Web as a training set and applied it to a task that is similar to Resnik?s.",
        "In terms of integrating coordination disambiguation with an existing parsing model, our approach resembles the approach by Hogan (2007).",
        "She detected noun phrase coordinations by finding symmetry in conjunct structure and the dependency between the lexical heads of the conjuncts.",
        "They are used to rerank the n-best outputs of the Bikel parser (2004), whereas two models interact with each other in our method.",
        "Shimbo and Hara (2007) proposed an alignment-based method for detecting and disambiguating non-nested coordination structures.",
        "They disambiguated coordination structures based on the edit distance between two conjuncts.",
        "Hara et al(2009) extended the method, dealing with nested coordinations as well.",
        "We used their method as one of the two sub-models."
      ]
    },
    {
      "heading": "3 Background",
      "text": []
    },
    {
      "heading": "3.1 Coordination structure analysis with",
      "text": [
        "alignment-based local features Coordination structure analysis with alignment-based local features (Hara et al. 2009) is a hybrid approach to coordination disambiguation that combines a simple grammar to ensure consistent global structure of coordinations in a sentence, and features based on sequence alignment to capture local symmetry of conjuncts.",
        "In this section, we describe the method briefly.",
        "A sentence is denoted byx = x1...xk, where xi is the i-th word of x.",
        "A coordination boundaries set is denoted by y = y1...yk, where",
        "(bl, el, br, er) (if xi is a coordinating conjunction having left conjunct xbl ...xel and right conjunct xbr ...xer) null (otherwise) In other words, yi has a non-null value only when it is a coordinating conjunction.",
        "For example, a sentence I bought books and stationary has a coordination boundaries set (null, null, null, (3, 3, 5, 5), null).",
        "The score of a coordination boundaries set is defined as the sum of score of all coordinating conjunctions in the sentence.",
        "where f(x, ym) is a real-valued feature vector of the coordination conjunct xm.",
        "We used almost the same feature set as Hara et al(2009): namely, the surface word, part-of-speech, suffix and prefix of the words, and their combinations.",
        "We used the averaged perceptron to tune the weight vector w. Hara et al(2009) proposed to use a context-free grammar to find a properly nested coordination structure.",
        "That is, the scoring function Eq (1)",
        "is only defined on the coordination structures that are licensed by the grammar.",
        "We only slightly extended their grammar for convering more variety of coordinating conjunctions.",
        "Table 2 and Table 3 show the non-terminals and production rules used in the model.",
        "The only objective of the grammar is to ensure the consistency of two or more coordinations in a sentence, which means for any two coordinations they must be either non-overlapping or nested coordinations.",
        "We use a bottom-up chart parsing algorithm to output the coordination boundaries with the highest score.",
        "Note that these production rules don't need to be isomorphic to those of HPSG parsing and actually they aren't.",
        "This is because the two methods interact only through dual decomposition and the search spaces defined by the methods are considered separately.",
        "This method requires O(n4) time, where n is the number of words.",
        "This is because there are O(n2) possible coordination structures in a sentence, and the method requires O(n2) time to get a feature vector of each coordination structure."
      ]
    },
    {
      "heading": "3.2 HPSG parsing",
      "text": [
        "HPSG (Pollard and Sag, 1994) is one of the linguistic theories based on lexicalized grammar"
      ]
    },
    {
      "heading": "Complement Schema (right)",
      "text": [
        "and unbounded dependencies.",
        "SEM feature represents the semantics of a constituent, and in this study it expresses a predicate-argument structure.",
        "Figure 2 presents the Subject-Head Schema and the Head-Complement Schema1 defined in (Pollard and Sag, 1994).",
        "In order to express general constraints, schemata only provide sharing of feature values, and no instantiated values.",
        "Figure 3 has an example of HPSG parsing of the sentence ?Spring has come.?",
        "First, each of the lexical entries for ?has?",
        "and ?come?",
        "are unified with a daughter feature structure of the Head-Complement Schema.",
        "Unification provides the phrasal sign of the mother.",
        "The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs.",
        "Finally, the phrasal sign of the entire sentence is output on the top of the derivation tree."
      ]
    },
    {
      "heading": "3 Acquiring HPSG from the Penn Treebank",
      "text": [
        "As discussed in Section 1, our grammar development requires each sentence to be annotated with i) a history of rule applications, and ii) additional annotations to make the grammar rules be pseudo-injective.",
        "In HPSG, a history of rule applications is represented by a tree annotated with schema names.",
        "Additional annotations are",
        "required because HPSG schemata are not injective, i.e., daughters?",
        "signs cannot be uniquely determined given the mother.",
        "The following annotations are at least required.",
        "First, the HEAD feature of each non-head daughter must be specified since this is not percolated to the mother sign.",
        "Second, SLASH/REL features are required as described in our previous study (Miyao et al. 2003a).",
        "Finally, the SUBJ feature of the complement daughter in the Head-Complement Schema must be specified since this schema may subcategorize an unsaturated constituent, i.e., a constituent with a non-empty SUBJ feature.",
        "When the corpus is annotated with at least these features, the lexical entries required to explain the sentence are uniquely determined.",
        "In this study, we define partially-specified derivation trees as tree structures annotated with schema names and HPSG signs including the specifications of the above features.",
        "We describe the process of grammar development in terms of the four phases: specification, externalization, extraction, and verification."
      ]
    },
    {
      "heading": "3.1 Specification",
      "text": [
        "General grammatical constraints are defined in this phase, and in HPSG, they are represented through the design of the sign and schemata.",
        "Figure 1 shows the definition for the typed feature structure of a sign used in this study.",
        "Some more features are defined for each syntactic category al",
        "complement schema (right); taken from Miyao et al. (2004).",
        "formalism.",
        "In a lexicalized grammar, quite a small numbers of schemata are used to explain general grammatical constraints, compared with other theories.",
        "On the other hand, rich word-specific characteristics are embedded in lexical entries.",
        "Both of schemata and lexical entries are represented by typed feature structures, and constraints in parsing are checked by unification among them.",
        "Figure 1 shows examples of HPSG schema.",
        "Figure 2 shows an HPSG parse tree of the s n-tence ?Spring has come.?",
        "Fi st, the lexical entries of ?has?",
        "and ?come?",
        "are joined by head-complement schema.",
        "Unification gives the HPSG sign of mother.",
        "After applying schemata to HPSG signs repeatedly, the HPSG sign of the whole sentence is output.",
        "We use Enju for an English HPSG parser (Miyao et al. 2004).",
        "Figure 3 shows how a coordination tructure is built in the Enju grammar.",
        "First, a coordinating conju ction and the right conjunct are joined by coord right schema.",
        "Afterwards, the parent and the left conjunct are joined by coord left schema.",
        "The Enju parser is equipped with a disambiguation model trained by the maximum entropy method (Miyao and Tsujii, 2008).",
        "Since we do not need the probability of each parse tree, we treat the model just as a linear model that defines the score of a parse tree as the sum of feature weights.",
        "The features of the model are defined on local subtrees of a parse tree.",
        "The Enju parser takes O(n3) time since it uses the CKY algorithm, and each cell in the CKY parse table has at most a constant number of edges because we use beam search algorithm.",
        "Thus, we can regard the parser as a decoder for a weighted CFG."
      ]
    },
    {
      "heading": "3.3 Dual decomposition",
      "text": [
        "Dual decomposition is a classical method to solve complex optimization problems that can be de",
        "and unbounded dependencies.",
        "SEM feature represents the semantics of a constituent, and in this study it expresses a predicate-argument structure.",
        "Figure 2 presents the Subject-Head Schema and the Head-Complement Schema1 defined in (Pollard and Sag, 1994).",
        "In order to express general constraints, schemata only provide sharing of feature values, and no instantiated values.",
        "Figure 3 has an example of HPSG parsing of the sentence ?Spring has come.?",
        "First, each of the lexical entries for ?has?",
        "and ?come?",
        "are unified with a daughter feature structure of the Head-Complement Schema.",
        "Unification provides the phrasal sign of the mother.",
        "The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs.",
        "Finally, the phrasal sign of the entire sentence is output on the top of the derivation tree."
      ]
    },
    {
      "heading": "3 Acquiring HPSG from the Penn Treebank",
      "text": [
        "As discussed in Section 1, our grammar development requires each sentence to be annotated with i) a history of rule applications, and ii) additional annotations to make the grammar rules be pseudo-injective.",
        "In HPSG, a history of rule applications is represented by a tree annotated with schema names.",
        "Additional annotations are",
        "required because HPSG schemata are not injective, i.e., daughters?",
        "signs cannot be uniquely determined given the mother.",
        "The following annotations are at least required.",
        "First, the HEAD feature of each non-head daughter must be specified since this is not percolated to the mother sign.",
        "Second, SLASH/REL features are required as described in our previous study (Miyao et al. 2003a).",
        "Finally, the SUBJ feature of the complement daughter in the Head-Complement Schema must be specified since this schema may subcategorize an unsaturated constituent, i.e., a constituent with a non-empty SUBJ feature.",
        "When the corpus is annotated with at least these features, the lexical entries required to explain the sentence are uniquely determined.",
        "In this study, we define partially-specified derivation trees as tree structures annotated with schema names and HPSG signs including the specifications of the above features.",
        "We describe the process of grammar development in terms of the four phases: specification, externalization, extraction, and verification."
      ]
    },
    {
      "heading": "3.1 Specification",
      "text": [
        "General grammatical constraints are defined in this phase, and in HPSG, they are represented through the design of the sign and schemata.",
        "Figure 1 shows the definition for the typed feature structure of a sign used in this study.",
        "Some more",
        "composed into efficiently solvable sub-problems.",
        "It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al. 2010).",
        "We consider an optimization problem",
        "which is difficult to solve (e.g. NP-hard), while argmaxx f(x) and argmaxx g(x) are effectively solvable.",
        "In dual decomposition, we solve",
        "instead of the original problem.",
        "To find the minimum value, we can use a sub-gradient method (Rush et al. 2010).",
        "The subgradient method is given in Table 4.",
        "As the algorithm",
        "shows, you can use existing algorithms and don't need to have an exact algorithm for the optimization problem, which are features of dual decomposition.",
        "If x(k) = y(k) occurs during the algorithm, then we simply take x(k) as the primal solution, which is the exact answer.",
        "If not, we simply take x(K), the answer of coordination structure analysis with alignment-based features, as an approximate answer to the primal solution.",
        "The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al. 2010)) has shown that it is effective in practice.",
        "We use it in this paper."
      ]
    },
    {
      "heading": "4 Proposed method",
      "text": [
        "In this section, we describe how we apply dual decomposition to the two models."
      ]
    },
    {
      "heading": "4.1 Notation",
      "text": [
        "We define some notations here.",
        "First we describe weighted CFG parsing, which is used for both coordination structure analysis with alignment-based features and HPSG parsing.",
        "We follows the formulation by Rush et al. (2010).",
        "We assume a context-free grammar in Chomsky normal form, with a set of non-terminals N .",
        "All rules of the grammar are either the form A?",
        "BC or A?",
        "w where A,B,C ?",
        "N and w ?",
        "V .",
        "For rules of the form A?",
        "w we refer to A as the pre-terminal for w. Given a sentence with n words, w1w2...wn, a parse tree is a set of rule productions of the form ?A ?",
        "BC, i, k, j?",
        "where A,B,C ?",
        "N , and 1 ?",
        "i ?",
        "k ?",
        "j ?",
        "n. Each rule production represents the use of CFG rule A?",
        "BC where non-terminal A spans words wi...wj , non-terminal B",
        "spans word wi...wk, and non-terminal C spans word wk+1...wj if k < j, and the use of CFG rule A?",
        "wi if i = k = j.",
        "We now define the index set for the coordination structure analysis as",
        "Each parse tree is a vector y = {yr : r ?",
        "Icsa}, with yr = 1 if rule r is in the parse tree, and yr = 0 otherwise.",
        "Therefore, each parse tree is represented as a vector in {0, 1}m, where m = |Icsa|.",
        "We use Y to denote the set of all valid parse-tree vectors.",
        "The set Y is a subset of {0, 1}m. In addition, we assume a vector ?csa = {?csar : r ?",
        "Icsa} that specifies a score for each rule production.",
        "Each ?csar can take any real value.",
        "The optimal parse tree is y?",
        "= argmaxy?Y y ?",
        "?csa",
        "We use similar notation for HPSG parsing.",
        "We define Ihpsg , Z and ?hpsg as the index set for HPSG parsing, the set of all valid parse-tree vectors and the weight vector for HPSG parsing respectively.",
        "We extend the index sets for both the coordination structure analysis with alignment-based features and HPSG parsing to make a constraint between the two sub-problems.",
        "For the coordination structure analysis with alignment-based features we define the extended index set to be",
        "Here each triple (a, b, c) represents that word wc is recognized as the last word of the right conjunct and the scope of the left conjunct or the coordinating conjunction is wa...wb1.",
        "Thus each parse-tree vector y will have additional components ya,b,c.",
        "Note that this representation is over-complete, since a parse tree is enough to determine unique coordination structures for a sentence: more explicitly, the value of ya,b,c is 1This definition is derived from the structure of a coordination in Enju (Figure 3).",
        "The triples show where the coordinating conjunction and right conjunct are in coord right schema, and the left conjunct and partial coordination are in coord left schema.",
        "Thus they alone enable not only the coordination structure analysis with alignment-based features but Enju to uniquely determine the structure of a coordination.",
        "1 if rule COORDa,c ?",
        "CJTa,bCC , CJT ,c or COORD ,c ?",
        "CJT , CCa,bCJT ,c is in the parse tree; otherwise it is 0.",
        "We apply the same extension to the HPSG index set, also giving an over-complete representation.",
        "We define za,b,c analogously to ya,b,c."
      ]
    },
    {
      "heading": "4.2 Proposed method",
      "text": [
        "We now describe the dual decomposition approach for coordination disambiguation.",
        "First, we define the set Q as follows: Q = {(y, z) : y ?",
        "Y, z ?",
        "Z, ya,b,c = za,b,c for all (a, b, c) ?",
        "Iuni} Therefore, Q is the set of all (y, z) pairs that agree on their coordination structures.",
        "The coordination structure analysis with alignment-based features and HPSG parsing problem is then to solve",
        "where ?",
        "> 0 is a parameter dictating the relative weight of the two models and is chosen to optimize performance on the development test set.",
        "This problem is equivalent to",
        "where g : Z ?",
        "Y is a function that maps a HPSG tree z to its set of coordination structures z = g(y).",
        "We solve this optimization problem by using dual decomposition.",
        "Figure 4 shows the resulting algorithm.",
        "The algorithm tries to optimize the combined objective by separately solving the sub-problems again and again.",
        "After each iteration, the algorithm updates the weights u(a, b, c).",
        "These updates modify the objective functions for the two sub-problems, encouraging them to agree on the same coordination structures.",
        "If y(k) = z(k) occurs during the iterations, then the algorithm simply returns y(k) as the exact answer.",
        "If not, the algorithm returns the answer of coordination analysis with alignment features as a heuristic answer.",
        "It is needed to modify original sub-problems for calculating (1) and (2) in Table 4.",
        "Wemodified the sub-problems to regard the score of u(a, b, c) as a bonus/penalty of the coordination.",
        "The modified coordination structure analysis with alignment features adds u(k)(i, j,m) and u(k)(j+1, l?",
        "u(1)(a, b, c)?",
        "0 for all (a, b, c) ?",
        "Iuni for k = 1 to K do",
        "for all (a, b, c) ?",
        "Iuni do u(k+1)(a, b, c)?",
        "u(k)(a, b, c)?",
        "ak(y(k)(a, b, c)?",
        "z(k)(a, b, c))",
        "w ?",
        "f(x, (i, j, l,m)) to the score of the sub-tree, when the rule production COORDi,m ?",
        "CJTi,jCCj+1,l?1CJTl,m is applied.",
        "The modified Enju adds u(k)(i, j, l) when coord left schema is applied, where word wc is recognized as a coordinating conjunction and left side of its scope is wa...wb, or coord right schema is applied, where word wc is recognized as a coordinating conjunction and right side of its scope is wa...wb."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Test/Training data",
      "text": [
        "We trained the alignment-based coordination analysis model on both the Genia corpus (?)",
        "and the Wall Street Journal portion of the Penn Treebank (?",
        "), and evaluated the performance of our method on (i) the Genia corpus and (ii) the Wall Street Journal portion of the Penn Treebank.",
        "More precisely, we used HPSG treebank converted from the Penn Treebank and Genia, and further extracted the training/test data for coordination structure analysis with alignment-based features using the annotation in the Treebank.",
        "Table ??",
        "shows the corpus used in the experiments.",
        "The Wall Street Journal portion of the Penn Treebank has 2317 sentences from WSJ articles, and there are 1356 COOD tags in the sentences, while the Genia corpus has 1754 sentences from MEDLINE abstracts, and there are 1848 COOD tags in the sentences.",
        "COOD tags are further subcategorized into phrase types such as NP-COOD or VP-COOD.",
        "Table ??",
        "shows the percentage of each phrase type in all COOD tags.",
        "It indicates the Wall Street Journal portion of the"
      ]
    },
    {
      "heading": "COORD WSJ Genia",
      "text": [
        "each test set Penn Treebank has more VP-COOD tags and S-COOD tags, while the Genia corpus has more NP-COOD tags and ADJP-COOD tags."
      ]
    },
    {
      "heading": "5.2 Implementation of sub-problems",
      "text": [
        "We used Enju (?)",
        "for the implementation of HPSG parsing, which has a wide-coverage probabilistic HPSG grammar and an efficient parsing algorithm, while we reimplemented Hara et al. (2009)'s algorithm with slight modifications.",
        "We used the following step size in our algorithm (Figure ??).",
        "First, we initialized a0, which is chosen to optimize performance on the development set.",
        "Then we defined ak = a0 ?",
        "2?",
        "?k , where ?k is the number of times that L(u(k"
      ]
    },
    {
      "heading": "5.3 Evaluation metric",
      "text": [
        "We evaluated the performance of the tested methods by the accuracy of coordination-level bracketing (?",
        "); i.e., we count each of the coordination scopes as one output of the system, and the system",
        "1,m), as well as adding w ?",
        "f(x, (i, j, l,m)) to the score of the subtree, when the rule production COORDi,m ?",
        "CJTi,jCCj+1,l?1CJTl,m is applied.",
        "The modified Enju adds u(k)(a, b, c) when coord right schema is applied, where word wa...wb is recognized as a coordinating conjunction and the last word of the right conjunct is wc, or coord left schema is applied, where word wa...wb is recognized as the left conjunct and the last word of the right conjunct is wc."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Test/Training data",
      "text": [
        "We trained the alignment-based coordination analysis model on both the Geni corpus (Kim et al. 2003) and the Wall Street Jour al p rtion of the Penn Treebank (Marcus et al. 1993), and evaluated the performance of our method on (i) the Genia corpus and (ii) the Wall Street Journal portion of the Penn Tre bank.",
        "More precisely, we used HPSG treebank onverted from the Penn Treebank and Genia, and further extracted the training/test data for c ordinati n structure analysis with alignment-based features usi g the annotation in the reebank.",
        "Table 5 shows the corpus used in the experiments.",
        "The Wall Street Journal portion of the Penn Treebank in the test set ha 2317 sentences from WSJ articles, and there are 1356 coordinations in the sentences, while the Genia corpus in the test set has 1764 sentences from MEDLINE abstracts, and there are 1848 coordinations in the sentences.",
        "Coor inations are further subcatego"
      ]
    },
    {
      "heading": "COORD WSJ Genia",
      "text": [
        "each test set rized into phrase types such as a NP coordination or PP coordination.",
        "Table 6 shows the percentage of each phrase type in all coordianitons.",
        "It indicates the Wall Street Journal portion of the Penn Treebank has more VP coordinations and S coordianitons, while the Genia corpus has more NP coordianitons and ADJP coordiations."
      ]
    },
    {
      "heading": "5.2 Implementation of sub-problems",
      "text": [
        "We used Enju (Miyao and Tsujii, 2004) for the implementation of HPSG parsing, which has a wide-coverage probabilistic HPSG grammar and an efficient parsing algorithm, while we reimplemented Hara t al., (2009)'s algorithm with slight modificatio s.",
        "We used the following step size in our algorithm (Figure 4).",
        "First, we initialized a0, which is chosen to optimize performance on th development set.",
        "Then we defined ak = a0 ?",
        "2?",
        "?k , where ?k is the number of times that L(u(k",
        "sion, recall, and F1 (%) for the proposed method, Enju, and Coordination structure analysis with alignment-based features (CSA)"
      ]
    },
    {
      "heading": "5.3 Evaluation metric",
      "text": [
        "We evaluated the performance of the tested methods by the accuracy of coordination-level bracketing (Shimbo and Hara, 2007); i.e., we count each of the coordination scopes as one output of the system, and the system output is regarded as correct if both of the beginning of the first output conjunct and the end of the last conjunct match annotations in the Treebank (Hara et al. 2009)."
      ]
    },
    {
      "heading": "5.4 Experimental results of Task (i)",
      "text": [
        "We ran the dual decomposition algorithm with a limit of K = 50 iterations.",
        "We found the two sub-problems return the same answer during the algorithm in over 95% of sentences.",
        "We compare the accuracy of the dual decomposition approach to two baselines: Enju and coordination structure analysis with alignment-based features.",
        "Table 7 shows all three results.",
        "The dual decomposition method gives a statistically significant gain in precision and recall over the two methods2.",
        "Table 8 shows the recall of coordinations of each type.",
        "It indicates our reimplementation of CSA and Hara et al(2009) have a roughly similar performance, although their experimental settings are different.",
        "It also shows the proposed method took advantage of Enju and CSA in NP coordination, while it is likely just to take the answer of Enju in VP and sentential coordinations.",
        "This means we might well use dual decomposi",
        "the percentage of sentences that are correctly parsed.",
        "certificates (%): the percentage of sentences for which a certificate of optimality is obtained.",
        "tion only on NP coordinations to have a better result.",
        "Figure 5 shows performance of the approach as a function of K, the maximum number of iterations of dual decomposition.",
        "The graphs show that values of K much less than 50 produce almost identical performance to K = 50 (with K = 50, the accuracy of the method is 73.4%, with K = 20 it is 72.6%, and with K = 1 it is 69.3%).",
        "This means you can use smaller K in practical use for speed."
      ]
    },
    {
      "heading": "5.5 Experimental results of Task (ii)",
      "text": [
        "We also ran the dual decomposition algorithm with a limit of K = 50 iterations on Task (ii).",
        "Table 9 and 10 show the results of task (ii).",
        "They show the proposed method outperformed the two methods statistically in precision and recall3.",
        "Figure 6 shows performance of the approach as a function of K, the maximum number of iterations of dual decomposition.",
        "The convergence speed for WSJ was faster than that for Genia.",
        "This is because a sentence of WSJ often have a simpler coordination structure, compared with that of Genia.",
        "coordination structure analysis with alignment-based features (CSA) , and Hara et al(2009) of Task (i) on the development set.",
        "Note that Hara et al(2009) uses a different test set and different annotation rules, although its test data is also taken from the Genia corpus.",
        "Thus we cannot compare them directly.",
        "sion, recall, and F1 (%) for the proposed method, Enju, and Coordination structure analysis with alignment-based features (CSA)",
        "(#), and the recall (%) for the proposed method, Enju, and coordination structure analysis with alignment-based features (CSA) of Task (ii) on the development set."
      ]
    },
    {
      "heading": "6 Conclusion and Future Work",
      "text": [
        "In this paper, we presented an efficient method for detecting and disambiguating coordinate structures.",
        "Our basic idea was to consider both grammar and symmetries of conjuncts by using dual decomposition.",
        "Experiments on the Genia corpus and the Wall Street Journal portion of the Penn Treebank showed that we could obtain statistically significant improvement in accuracy when using dual decomposition.",
        "We would need a further study in the following points of view: First, we should evaluate our",
        "K of Task (ii) on the development set.",
        "accuracy (%): the percentage of sentences that are correctly parsed.",
        "certificates (%): the percentage of sentences for which a certificate of optimality is provided.",
        "method with corpus in different domains.",
        "Because characteristics of coordination structures differs from corpus to corpus, experiments on other corpus would lead to a different result.",
        "Second, we would want to add some features to coordination structure analysis with alignment-based local features such as ontology.",
        "Finally, we can add other methods (e.g. dependency parsing) as sub-problems to our method by using the extension of dual decomposition, which can deal with more than two sub-problems."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The second author is partially supported by KAK-ENHI Grant-in-Aid for Scientific Research C 21500131 and Microsoft CORE project 7."
      ]
    }
  ]
}
