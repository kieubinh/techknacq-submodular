{
  "info": {
    "authors": [
      "Arjun Mukherjee",
      "Bing Liu"
    ],
    "book": "ACL",
    "id": "acl-P12-1036",
    "title": "Aspect Extraction through Semi-Supervised Modeling",
    "url": "https://aclweb.org/anthology/P12-1036",
    "year": 2012
  },
  "references": [
    "acl-C10-1074",
    "acl-C10-1143",
    "acl-C10-2090",
    "acl-D07-1114",
    "acl-D09-1026",
    "acl-D09-1159",
    "acl-D10-1006",
    "acl-D10-1101",
    "acl-H05-1043",
    "acl-J11-1002",
    "acl-N10-1122",
    "acl-P08-1031",
    "acl-P09-1026",
    "acl-P10-2050",
    "acl-P11-1026",
    "acl-P11-1036",
    "acl-P11-1150"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Aspect extraction is a central problem in sentiment analysis.",
        "Current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling.",
        "By categorizing, we mean the synonymous aspects should be clustered into the same category.",
        "In this paper, we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously.",
        "This setting is important because categorizing aspects is a subjective task.",
        "For different application purposes, different categorizations may be needed.",
        "Some form of user guidance is desired.",
        "In this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants.",
        "Our experimental results show that the two proposed models are indeed able to perform the task effectively."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Aspect-based sentiment analysis is one of the main frameworks for sentiment analysis (Hu and Liu, 2004; Pang and Lee, 2008; Liu, 2012).",
        "A key task of the framework is to extract aspects of entities that have been commented in opinion documents.",
        "The task consists of two sub-tasks.",
        "The first subtask extracts aspect terms from an opinion corpus.",
        "The second subtask clusters synonymous aspect terms into categories where each category represents a single aspect, which we call an aspect category.",
        "Existing research has proposed many methods for aspect extraction.",
        "They largely fall into two main types.",
        "The first type only extracts aspect terms without grouping them into categories (although a subsequent step may be used for the grouping, see Section 2).",
        "The second type uses statistical topic models to extract aspects and group them at the same time in an unsupervised manner.",
        "Both approaches are useful.",
        "However, in practice, one also encounters another setting, where grouping is not straightforward because for different applications the user may need different groupings to reflect the application needs.",
        "This problem was reported in (Zhai et al., 2010), which gave the following example.",
        "In car reviews, internal design and external design can be regarded as two separate aspects, but can also be regarded as one aspect, called ?design?, based on the level of details that the user wants to study.",
        "It is also possible that the same word may be put in different categories based on different needs.",
        "However, (Zhai et al., 2010) did not extract aspect terms.",
        "It only categorizes a set of given aspect terms.",
        "In this work, we propose two novel statistical models to extract and categorize aspect terms automatically given some seeds in the user interested categories.",
        "It is thus able to best meet the user's specific needs.",
        "Our models also jointly model both aspects and aspect specific sentiments.",
        "The first model is called SAS and the second model is called ME-SAS.",
        "ME-SAS improves SAS by using Maximum-Entropy (or Max-Ent for short) priors to help separate aspects and sentiment terms.",
        "However, to train Max-Ent, we do not need manually labeled training data (see Section 4).",
        "In practical applications, asking users to provide some seeds is easy as they are normally experts in their trades and have a good knowledge what are important in their domains.",
        "Our models are related to topic models in general (Blei et al., 2003) and joint models of aspects and sentiments in sentiment analysis in specific (e.g., Zhao et al., 2010).",
        "However, these current models are typically unsupervised.",
        "None of them can use seeds.",
        "With seeds, our models are thus semi-supervised and need a different formulation.",
        "Our models are also related to the DF-LDA model in (Andrzejewski et al., 2009), which allows the user to set must-link and cannot-link constraints.",
        "A must-link means that two terms must be in the same topic (aspect category), and a cannot-link means that two terms cannot be in the same topic.",
        "Seeds may be expressed with must-links and cannot-links constraints.",
        "However, our models are very different from DF-LDA.",
        "First of all, we jointly model aspect and sentiment, while DF-LDA is only for topics/aspects.",
        "Joint modeling ensures clear separation of aspects from sentiments producing better results.",
        "Second, our way of treating seeds is also different from DF-LDA.",
        "We discuss these and other related work in Section 2.",
        "The proposed models are evaluated using a large number of hotel reviews.",
        "They are also compared with two state-of-the-art baselines.",
        "Experimental results show that the proposed models outperform the two baselines by large margins."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "There are many existing works on aspect extraction.",
        "One approach is to find frequent noun terms and possibly with the help of dependency relations (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhuang et al., 2006; Blair-Goldensohn et al., 2008; Ku et al., 2006; Wu et al., 2009; Somasundaran and Wiebe, 2009; Qiu et al., 2011).",
        "Another approach is to use supervised sequence labeling (Liu, Hu and Cheng 2005; Jin and Ho, 2009; Jakob and Gurevych, 2010; Li et al., 2010; Choi and Cardie, 2010; Kobayashi et al., 2007; Yu et al., 2011).",
        "Ma and Wan (2010) also exploited centering theory, and (Yi et al., 2003) used language models.",
        "However, all these methods do not group extracted aspect terms into categories.",
        "Although there are works on grouping aspect terms (Carenini et al., 2005; Zhai et al., 2010; Zhai et al., 2011; Guo et al., 2010), they all assume that aspect terms have been extracted beforehand.",
        "In recent years, topic models have been used to perform extraction and grouping at the same time.",
        "Existing works are based on two basic models, pLSA (Hofmann, 1999) and LDA (Blei et al., 2003).",
        "Some existing works include discovering global and local aspects (Titov and McDonald, 2008), extracting key phrases (Branavan et al., 2008), rating multi-aspects (Wang et al., 2010; Moghaddam and Ester, 2011), summarizing aspects and sentiments (Lu et al., 2009), and modeling attitudes (Sauper et al., 2011).",
        "In (Lu and Zhai, 2008), a semi-supervised model was proposed.",
        "However, their method is entirely different from ours as they use expert reviews to guide the analysis of user reviews.",
        "Aspect and sentiment extraction using topic modeling come in two flavors: discovering aspect words sentiment wise (i.e., discovering positive and negative aspect words and/or sentiments for each aspect without separating aspect and sentiment terms) (Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011) and separately discovering both aspects and sentiments (e.g., Mei et al., 2007; Zhao et al., 2010).",
        "Zhao et al. (2010) used Maximum-Entropy to train a switch variable to separate aspect and sentiment words.",
        "We adopt this method as well but with no use of manually labeled data in training.",
        "One problem with these existing models is that many discovered aspects are not understandable/meaningful to users.",
        "Chang et al. (2009) stated that one reason is that the objective function of topic models does not always correlate well with human judgments.",
        "Our seeded models are designed to overcome this problem.",
        "Researchers have tried to generate ?meaningful?",
        "and ?specific?",
        "topics/aspects.",
        "Blei and McAuliffe (2007) and Ramage et al. (2009) used document label information in a supervised setting.",
        "Hu et al. (2011) relied on user feedback during Gibbs sampling iterations.",
        "Andrzejewski et al. (2011) incorporated first-order logic with Markov Logic Networks.",
        "However, it has a practical limitation for reasonably large corpora since the number of non-trivial groundings can grow to O(N2) where N is the number of unique tokens in the corpus.",
        "Andrzejewski et al. (2009) used another approach (DF-LDA) by introducing must-link and cannot-link constraints as Dirichlet Forest priors.",
        "Zhai et al.",
        "(2011) reported that the model does not scale up",
        "when the number of cannot-links go beyond 1000 because the number of maximal cliques Q(r) in a connected component of size |r |in the cannot-link graph is exponential in r. Note that we could still experiment with DF-LDA as our problem size is not so large.",
        "We will show in Section 4 that the proposed models outperform it by a large margin."
      ]
    },
    {
      "heading": "3 Proposed Seeded Models",
      "text": [
        "The standard LDA and existing aspect and sentiment models (ASMs) are mostly governed by the phenomenon called ?higher-order co-occurrence?",
        "(Heinrich, 2009), i.e., based on how often terms co-occur in different contexts1.",
        "This unfortunately results in many ?non-specific?",
        "terms being pulled and clustered.",
        "We employ seed sets to address this issue by ?guiding?",
        "the model to group semantically related terms in the same aspect thus making the aspect more specific and related to the seeds (which reflect the user needs).",
        "For easy presentation, we will use aspect to mean aspect category from now on.",
        "We replace the multinomial distribution over words for each aspect (as in ASMs) with a special two-level tree structured distribution.",
        "The generative process of ASMs assumes that each vocabulary word is independently (i.e., not dependent upon other word-aspect association) and equally probable to be associated with any aspect.",
        "Due to higher-order co-occurrences, we find conceptually different terms yet related in contexts (e.g., in hotel domain terms like stain, shower, walls in aspect 1 w1 co-occurring with w2 which in turn co-occurs with w3 denotes a second-order co-occurrence between w1 and w3.",
        "Maintenance; bed, linens, pillows in aspect Cleanliness) equally probable of emission for any aspect.",
        "Figure 1(a) shows an example tree.",
        "Upon adding the seed sets {bed, linens, pillows} and {staff, service}, the prior structure now changes to the correlated distribution in Figure 1 (b).",
        "Thus, each aspect has a top level distribution over non-seed words and seed sets.",
        "Each seed set in each aspect further has a second level distribution over seeds in that seed set.",
        "The aspect term (word) emission now requires two steps: first sampling at level one to obtain a non-seed word or a seed set.",
        "If a non-seed word is sampled we emit it else we further sample at the second seed set level and emit a seed word.",
        "This ensures that seed words together have either all high or low aspect associations.",
        "Furthermore, seed sets preserve conjugacy between related concepts and also shape more specific aspects by clustering based on higher order co-occurrences with seeds rather than only with standard one level multinomial distribution over words (or terms) alone."
      ]
    },
    {
      "heading": "3.1 SAS Model",
      "text": [
        "We now present the proposed Seeded Aspect and Sentiment model (SAS).",
        "Let ????",
        "denote the entries in our vocabulary where ?",
        "?is the number of unique non-seed terms.",
        "Let there be ?",
        "seed sets ??????",
        "where each seed set ??",
        "is a group of semantically related terms.",
        "Let ???????",
        ",????????",
        "denote T aspect and aspect specific sentiment models.",
        "Also let ??,?",
        "denote the aspect specific distribution of seeds in the seed set ??.",
        "Following the approach of (Zhao et al., 2010), we too assume that a review sentence usually talks about one",
        "stain walls shower friendly room ?",
        "?",
        "aspect.",
        "A review document ????",
        "comprises of ??",
        "sentences and each sentence ?",
        "?",
        "??",
        "has ??,?words.",
        "Also, let ??????",
        "denote the sentence ?",
        "of document ?.",
        "To distinguish between aspect and sentiment terms, we introduce an indicator (switch) variable ??,?,?",
        "?",
        "?",
        "?",
        "?, ???",
        "for the ??",
        "?term of ??????,???,?,?.",
        "Further, let ??,?",
        "denote the distribution of aspects and sentiments in ??????.",
        "The generative process of the SAS model (see Figure 1(c)) is given by: 1.",
        "For each aspect ??",
        "?",
        "?1, ?",
        ", ??",
        ": i.",
        "Draw ????~????????",
        "ii.",
        "Draw a distribution over terms and seed sets ????~????????",
        "a) For each seed set ?",
        "?",
        "??",
        "?, ?",
        ", ???",
        "Draw a distribution over seeds ??,??~???????",
        "2.",
        "For each (review) document ?",
        "?",
        "?1, ?",
        ", ??",
        ": i.",
        "Draw ???~???????",
        "ii.",
        "For each sentence ?",
        "?",
        "?1, ?",
        ", ???",
        ": a) Draw ??,??~?????????",
        "b) Draw ??,??~????????",
        "c) For each term ??,?,??where???",
        "?",
        "?1, ?",
        ", ??,??",
        ": I.",
        "Draw ??,?,??~?????????????,?",
        "?, ??,?,?",
        "?",
        "?",
        "?",
        "?, ???",
        "II.",
        "if ??,?,?",
        "?",
        "?",
        "??",
        "// ??,?,?",
        "is a sentiment",
        "We employ collapsed Gibbs sampling (Griffiths and Steyvers, 2004) for posterior inference.",
        "As ?",
        "and ?",
        "are at different hierarchical levels, we derive their samplers separately as follows:",
        "where ?????",
        "?",
        "??",
        "????????????????????",
        "??????????????",
        "?is the multinomial Beta function.",
        "??,??",
        "is the number of times term ?",
        "was assigned to aspect ?",
        "as an opinion/sentiment word.",
        "??,??,?",
        "is the number of times non-seed term ?",
        "?",
        "??",
        "was assigned to aspect ?",
        "as an aspect.",
        "??,?,??,?",
        "is the number of times seed term ?",
        "?",
        "?",
        "??",
        "was assigned to aspect ?",
        "as an aspect.",
        "??,?????.",
        "is the number of sentences in document ?",
        "that were assigned to aspect ?.",
        "??,??",
        "and ??,??",
        "denote the number of terms in ??????",
        "that were assigned to aspects and opinions respectively.",
        "??,??",
        "is the number of times any term of seed set ??",
        "was assigned to aspect ?.",
        "Omission of a latter index denoted by [] in the above notation represents the corresponding row vector spanning over the latter index.",
        "For example, ??,???,?",
        "?",
        "???,????,?",
        ", ?",
        ", ??,????,?",
        "?",
        "and (?)",
        "denotes the marginalized sum over the latter index.",
        "The subscript ?",
        "?, ?",
        "denotes the counts excluding assignments of all terms in ??????.",
        "?",
        "?, ?, ?",
        "denotes counts excluding ?",
        "?,?,?.We perform hierarchical sampling.",
        "First, an aspect is sampled for each sentence ??,?",
        "using Eq.",
        "(1).",
        "After sampling the aspect, we sample ??,?,?.",
        "The probability of ??,?,?",
        "being an opinion or sentiment term, ????,?,?",
        "?",
        "???",
        "is given by Eq.",
        "(2).",
        "However, for ????,?,?",
        "?",
        "???",
        "we have two cases: (a) the observed term ?",
        "?",
        "??,?,?",
        "?",
        "??",
        "or (b) does not belong to any seed set, ?",
        "?, ?",
        "?",
        "?",
        "?, i.e., w is an non-seed term.",
        "These cases are dealt in Eq.",
        "(3).",
        "Asymmetric Beta priors: Hyper-parameters ?, ?O, ?A are not very sensitive and the heuristic values suggested in (Griffiths and Steyvers, 2004) usually hold well in practice (Wallach et al. 2009).",
        "However, the smoothing hyper-parameter ?",
        "(Figure 1(c)) is crucial as it governs the aspect or sentiment switch.",
        "Essentially, ??,?~?????????",
        "is the probability of emitting an aspect term2 in ??????",
        "with concentration parameter ?",
        "and base measure ??",
        "?",
        "??",
        "?, ???.",
        "Without any prior belief, uniform base measures ??",
        "?",
        "??",
        "?",
        "0.5 are used resulting in symmetric Beta priors.",
        "However, aspects are often more probable than sentiments in a sentence (e.g., ?The beds, sheets, and bedding were dirty.?).",
        "Thus, it is more principled to employ asymmetric priors.",
        "Using a labeled set of sentences, ???????",
        "?, where we know the per sentence probability of aspect emission (??,?",
        "), we can employ the method of moments to estimate the smoothing hyper",
        "2 ??,?,?~?????????????,??.",
        "??,?",
        ", 1 ?",
        "??,?",
        "are the success and failure probability of emitting an aspect/sentiment term."
      ]
    },
    {
      "heading": "3.2 MESAS Model",
      "text": [
        "We can further improve SAS by employing Maximum Entropy (Max-Ent) priors for aspect and sentiment switching.",
        "We call this new model MESAS.",
        "The motivation is that aspect and sentiment terms play different syntactic roles in a sentence.",
        "Aspect terms tend to be nouns or noun phrases while sentiment terms tend to be adjectives, adverbs, etc.",
        "POS tag information can be elegantly encoded by moving ??,?",
        "to the term plate (see Figure 1(d)) and drawing it from a Max-Ent??,??,?",
        "; ??",
        "model.",
        "Let ???,?,???????????",
        "?",
        "??????,?,??",
        "?, ?????,?,?",
        ", ?????,?,??",
        "?, ??,?,?",
        "?",
        "1, ?",
        "?,?,?, ??,?,?",
        "?",
        "1?",
        "denote the feature vector associated with ??,?,??",
        "encoding lexical and POS features of the previous, current and next term.",
        "Using a training data set, we can learn Max-Ent priors.",
        "Note that unlike traditional Max-Ent training, we do not need manually labeled data for training (see Section 4 for details).",
        "For ME-SAS, only the sampler for the switch variable r changes as follows:",
        "where ????",
        "are the parameters of the learned Max-Ent model corresponding to the ?",
        "binary feature functions ????",
        "of Max-Ent."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "This section evaluates the proposed models.",
        "Since the focus in this paper is to generate high quality aspects using seeds, we will not evaluate sentiments although both SAS and MESAS can also discover sentiments.",
        "To compare the performance with our models, we use two existing state-of-the-art models, ME-LDA (Zhao et al. 2010) and DF-LDA (Andrzejewski et al., 2009).",
        "As discussed in Section 2, there are two main flavors of aspect and sentiment models.",
        "The first flavor does not separate aspect and sentiment, and the second flavor uses a switch to perform the separation.",
        "Since our models also perform a switch, it is natural to compare with the latter flavor, which is also more advanced.",
        "ME-LDA is the representative model in this flavor.",
        "DF-LDA adds constraints to LDA.",
        "We use our seeds to generate constraints for DF-LDA.",
        "While ME-LDA cannot consider constraints, DF-LDA does not separate sentiments and aspects.",
        "Apart from other modeling differences, our models can do both, which enable them to produce much better results.",
        "Dataset and Settings: We used hotel reviews from tripadvisor.com.",
        "Our corpus consisted of 101,234 reviews and 692,783 sentences.",
        "Punctuations, stop words 3, and words appearing less than 5 times in the corpus were removed.",
        "For all models, the posterior inference was drawn after 5000 Gibbs iterations with an initial burn-in of 1000 iterations.",
        "For SAS and ME-SAS, we set ?",
        "= 50/T, ?A = ?O = 0.1 as suggested in (Griffiths and Steyvers, 2004).",
        "To make the seeds more effective, we set the seed set word-distribution hyper-parameter ?",
        "to be much larger than ?A, the hyper-parameter for the distribution over seed sets and aspect terms.",
        "This results in higher weights to seeded words which in turn guide the sampler to cluster relevant terms better.",
        "A more theoretical approach would involve performing hyper-parameter estimation (Wallach et al., 2009) which may reveal specific properties of the dataset like the estimate of ?",
        "(indicating how different documents are in terms of their latent semantics), ?",
        "(suggesting how large the groups of frequently appearing aspect and sentiment terms are) and ?",
        "(giving a sense of which and how large groupings of seeds are good).",
        "These are interesting questions and we defer it to our future work.",
        "In this work, we found that the setting ?",
        "= 250, a larger value compared to ?A, produced good results.",
        "For SAS, the asymmetric Beta priors were estimated using the method of moments (Section 3.1).",
        "We sampled 500 random sentences from the corpus and for each sentence identified the aspects.",
        "We thus computed the per-sentence probability of aspect emission (??,?)",
        "and used Eq.",
        "(4) to compute the final estimates, which give ?a = 2.35, ?b = 3.44.",
        "To learn the Max-Ent parameters ?",
        "of ME-SAS, we used the sentiment lexicon 4 of (Hu and Liu, 2004) to automatically generate training data (no manual labeling).",
        "We randomly sampled 1000 terms from the corpus which have appeared at least",
        "20 times (to ensure that the training set is reasonably representative of the corpus).",
        "Of those 1000 terms if they appeared in the sentiment lexicon, they were treated as sentiment terms, else aspect terms.",
        "Clearly, labeling words not in the sentiment lexicon as aspect terms may not always be correct.",
        "Even with this noisy automatically-labeled data, the proposed models can produce good results.",
        "Since ME-LDA used manually labeled training data for Max-Ent, we again randomly sampled 1000 terms from our corpus appearing at least 20 times and labeled them as aspect terms or sentiment terms, so this labeled data clearly has less noise than our automatically labeled data.",
        "For both MESAS and ME-LDA we used the corresponding feature vector of each labeled term (in the context of sentences where it occurs) to train the Max-Ent model.",
        "As DF-LDA requires must-link and cannot-link constraints, we used our seed sets to generate intra-seed set must-link and inter-seed set cannot-link constraints.",
        "For its hyper-parameters, we used the default values in the package5 (Andrzejewski et al., 2009).",
        "Setting the number of topics/aspects in topic models is often tricky as it is difficult to know the 5 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html exact number of topics that a corpus has.",
        "While non-parametric Bayesian approaches (Teh et al., 2006) do exist for estimating the number of topics, T, they strongly depend on the hyper-parameters (Heinrich, 2009).",
        "As we use fixed hyper-parameters, we do not learn T from Bayesian non-parametrics.",
        "We used 9 major aspects (T = 9) based on commonsense knowledge of what people usually talk about hotels and some experiments.",
        "These are Dining, Staff, Maintenance, Check In, Cleanliness, Comfort, Amenities, Location and Value for Money (VFM).",
        "However, it is important to note that the proposed models are flexible and do not need to have seeds for every aspect/topic.",
        "Our experiments simulate the real-life situation where the user may not know all aspects or have no seeds for some aspects.",
        "Thus, we provided seeds only to the first 6 of the 9 aspects/topics.",
        "We will see that without seeds for all aspects, our models not only can improve the seeded aspects but also improve the non-seeded aspects."
      ]
    },
    {
      "heading": "4.1 Qualitative Results",
      "text": [
        "This section shows some qualitative results to give an intuitive feeling of the results from different models.",
        "Table 1 shows the aspect terms and sentiment terms discovered by the 4 models for",
        "(curtains restroom floor beds cleanliness) carpets hall towels bathtub couch mattress linens wardrobe spa pillow clean dirty comfortable fresh wet filthy extra stain front worn hallcarpets towels pillow stain mattress filthy linens interior bathtub cleandirty fresh old nice good enough new front friendly cleanlinessfloor carpets bed lobby bathroom staff closet spa d?cor clean good dirty hot large nice fresh thin new little cleanpool beach carpets parking bed bathroom nice comfortable suite Comfort (comfort mattress furniture couch pillows) bedding bedcover sofa linens bedroom suites d?cor comforter blanket futon comfortable clean soft nice uncomfortable spacious hard comfy dirty quiet bedlinens sofa bedcover hard bedroom privacy double comfy futon",
        "three aspects.",
        "Due to space limitations, we are unable to show all 6 aspects for which we have seeds.",
        "Since DF-LDA cannot separate aspects and sentiments, we only show its topics (aspects).",
        "Red (bold) colored words show semantic clustering errors or inappropriate terms for different groups.",
        "It is important to note that we judge the results based on how they are related to the user seeds (which represent the user need).",
        "The judgment is to some extent subjective.",
        "What we reported here are based on our judgments what are appropriate and what are not for each aspect.",
        "For SAS, MESAS and ME-LDA, we mark sentiment terms as errors when they are grouped under aspects as these models are supposed to separate sentiments and aspects.",
        "For DF-LDA, the situation is different as it is not meant to separate sentiment and aspect terms, we use red italic font to indicate those adjectives which are aspect specific adjectives (see more discussion below).",
        "Our judgment may be slightly unfair to ME-LDA and DF-LDA as their results may make sense in some other ways.",
        "However, that is precisely the purpose of this work, to produce results that suit the user's need rather than something generic.",
        "We can see from Table 1 that MESAS performs the best.",
        "Next in order are SAS, ME-LDA, and DF-LDA.",
        "We see that only providing a handful of seeds (5) for the aspect Staff, MESAS can discover highly specific words like manager, attendant, bartender, and janitor.",
        "By specific, we mean they are highly related to the given seeds.",
        "While SAS also discovers specific words benefiting from seeds, relying on Beta priors for aspect and sentiment switching was less effective.",
        "Next in performance is ME-LDA which although produces reasonable results in general, several aspect terms are far from what the user wants based on the seeds, e.g., room, linens, wait, pillow.",
        "Finally, we observe that DF-LDA does not perform well either.",
        "One reason is that it is unable to separate aspects and sentiments.",
        "Although encoding the intra-seed set must-link and inter-seed set cannot-link constraints in DF-LDA discovers some specific words as ME-SAS, they are much lower in the ranked order and hence do not show up in the top 10 words in Table 1.",
        "As DF-LDA is not meant to perform extraction and to group both aspect and sentiment terms, we relax the errors of DF-LDA due to correct aspect specific sentiments (e.g., friendly, helpful for Staff are correct aspect specific sentiments, but still regard incorrect sentiments like front, comfortable, large as errors) placed in aspect models.",
        "We call this model DF-LDA-Relaxed."
      ]
    },
    {
      "heading": "4.2 Quantitative Results",
      "text": [
        "Topic models are often evaluated quantitatively using perplexity and likelihood on held-out test data (Blei et al., 2003).",
        "However, perplexity does not reflect our purpose since our aim is not to predict whether an unseen document is likely to be a review of some particular aspect.",
        "Nor are we trying to evaluate how well the unseen review data fits our seeded models.",
        "Instead our focus is to evaluate how well our learned aspects perform in clustering specific terms guided by seeds.",
        "So we directly evaluate the discovered aspect terms.",
        "Note again we do not evaluate sentiment terms as they are not the focus of this paper 6.",
        "Since aspects produced by the models are rankings and we do not know the number of correct aspect terms, a natural way to evaluate these rankings is to use precision @ n (or p@n), where n is a rank position.",
        "Varying number of seeds: Instead of a fixed number of seeds, we want to see the effect of the number of seeds on aspect discovery.",
        "Table 2 reports the average p@n vs. the number of seeds.",
        "The average is a two-way averaging.",
        "The first average was taken over all combinations of actual seeds selected for each aspect, e.g., when the number of seeds is 3, out of the 5 seeds in each aspect, all ?53?",
        "combinations of seeds were tried and the results averaged.",
        "The results were further averaged over p@n for 6 aspects with seeds.",
        "We start with 2 seeds and progressively increase them to 5.",
        "Using only 1 seed per seed set (or per aspect) has practically no effect because the top level distribution ??",
        "encodes which seed sets (and non-seed words) to include; the lower-level distribution ?",
        "constrains the probabilities of the seed words to be correlated for each of the seed sets.",
        "Thus, having only one seed per seed set will result in sampling that single word whenever that seed set is chosen which will not have the effect of correlating seed words so as to pull other words based on co-occurrence with constrained seed words.",
        "From Table 2, we can see that for all models p@n progressively improves as the number of seeds increases.",
        "Again MESAS performs the best followed by SAS and DF-LDA.",
        "6 A qualitative evaluation of sentiment extraction based on Table 1 yields the following order: ME-SAS, SAS, ME-LDA.",
        "Effect of seeds on non-seeded aspects: Here we compare all models aspect wise and see the results of seeded models SAS and MESAS on non-seeded aspects (Table 3).",
        "Shaded cells in Table 3 give the p@n values for DF-LDA, DF-LDARelaxed, SAS, and MESAS on three non-seeded aspects (Amenities, Location, and VFM)7.",
        "We see that across all the first 6 aspects with (5) seeds MESAS outperforms all other models by large margins in all top 3 ranked buckets p@10, p@20 and p@30.",
        "Next in order are SAS, ME-LDA and DF-LDA.",
        "For the last three aspects which did not have any seed guidance, we find something interesting.",
        "Seeded models SAS and especially MESAS result in improvements of non-seeded aspects too.",
        "This is because as seeds facilitate clustering specific and appropriate terms in seeded aspects, which in turn improves precision on non-seeded aspects.",
        "This phenomenon can be clearly seen in Table 1.",
        "In aspect Staff of ME-LDA, we find pillow and linens being clustered.",
        "This is not a ?flaw?",
        "of the model per se, but the point here is pillow and linens happen to co-occur many times with other words like maintenance, staff, and upkeep because ?room-service?",
        "generally includes staff members coming and replacing linens and pillow covers.",
        "Although pillow and linens are related to Staff, strictly speaking they are semantically incorrect because they do not represent the very concept ?Staff?",
        "based on the seeds (which reflect the user need).",
        "Presence of 7 Note that Tables 2 and 3 are different runs of the model.",
        "The variations in the results are due to the random initialization of the Gibbs sampler.",
        "seed sets in SAS and MESAS result in pulling such words as linens and pillow (due to seeds like beds and cleanliness in the aspect Cleanliness) and ranking them higher in the aspect Cleanliness (see Table 1) where they make more sense than Staff.",
        "Lastly, we also note that the improvements in non-seeded aspects are more pronounced for MESAS than SAS as SAS encounters more switching errors which counters the improvement gained by seeds.",
        "In summary, the averages over all aspects (Table 3 last row) show that the proposed seeded models SAS and MESAS outperform ME-LDA, DF-LDA and even DF-LDA-Relaxed considerably."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "This paper studied the issue of using seeds to discover aspects in an opinion corpus.",
        "To our knowledge, no existing work deals with this problem.",
        "Yet, it is important because in practice the user often has something in mind to find.",
        "The results obtained in a completely unsupervised manner may not suit the user's need.",
        "To solve this problem, we proposed two models SAS and MESAS which take seeds reflecting the user needs to discover specific aspects.",
        "ME-SAS also does not need any additional help from the user in its Max-Ent training.",
        "Our results showed that both models outperformed two state-of-the-art existing models"
      ]
    },
    {
      "heading": "ME-LDA and DF-LDA by large margins. Acknowledgments",
      "text": [
        "This work is supported in part by National Science"
      ]
    }
  ]
}
