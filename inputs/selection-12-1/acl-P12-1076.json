{
  "info": {
    "authors": [
      "Shingo Takamatsu",
      "Issei Sato",
      "Hiroshi Nakagawa"
    ],
    "book": "ACL",
    "id": "acl-P12-1076",
    "title": "Reducing Wrong Labels in Distant Supervision for Relation Extraction",
    "url": "https://aclweb.org/anthology/P12-1076",
    "year": 2012
  },
  "references": [
    "acl-D08-1106",
    "acl-D10-1099",
    "acl-D11-1132",
    "acl-D11-1142",
    "acl-P06-1015",
    "acl-P08-1004",
    "acl-P09-1113",
    "acl-P09-1115",
    "acl-P10-1030",
    "acl-P11-1055"
  ],
  "sections": [
    {
      "text": [
        "per sentence: correct labeling; lower sentence: incorrect labeling.",
        "With DS it is assumed that if a sentence contains an entity pair in a knowledge base, such a sentence actually expresses the corresponding relation in the knowledge base.",
        "However, the DS assumption can fail, which results in noisy labeled data and this causes poor extraction performance.",
        "An entity pair in a target text generally expresses more than one relation while a knowledge base stores a subset of the relations.",
        "The assumption ignores this possibility.",
        "For instance, consider the place of birth relation between Michael Jackson and Gary in Figure 1.",
        "The upper sentence indeed expresses the place of birth relation between the two entities.",
        "In DS place of birth is assigned to the sentence, and it becomes a useful training example.",
        "On the other hand, the lower sentence does not express this relation between the two entities, but the DS heuristic wrongly labels the sentence as expressing it.",
        "Riedel et al. (2010) relax the DS assumption as at least one sentence containing an entity pair ex",
        "pressing the corresponding relation in the knowledge base.",
        "They cast the relaxed assumption as multi-instance learning.",
        "However, even the relaxed assumption can fail.",
        "The relaxation is equivalent to the DS assumption when a labeled pair of entities is mentioned once in a target corpus (Riedel et al., 2010).",
        "In fact, 91.7% of entity pairs appear only once in Wikipedia articles (see Section 7).",
        "In this paper, we propose a method to reduce the number of wrong labels generated by DS without using either of these assumptions.",
        "Given the labeled corpus created with the DS assumption, we first predict whether each pattern, which frequently appears in text to express a relation (see Section 4), expresses a target relation.",
        "Patterns that are predicted not to express the relation are used to form a negative pattern list for removing wrong labels of the relation.",
        "The main contributions of this paper are as follows: ?",
        "To make the pattern prediction, we propose a generative model that directly models the process of automatic labeling in DS.",
        "Without any strong assumptions like Riedel et al. (2010)?s, the model predicts whether each pattern expresses each relation via hidden variables (see Section 5).",
        "?",
        "Our variational inference for our generative model lets us automatically calibrate parameters for each relation, which are sensitive to the performance (see Section 6).",
        "?",
        "We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al., 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multi-instance learning system for relation extraction (see Section 7)."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "The increasingly popular approach, called distant supervision (DS), or weak supervision, utilizes a knowledge base to heuristically label a corpus (Wu and Weld, 2007; Bellare and McCallum, 2007; Pal et al., 2007).",
        "Our work was inspired by Mintz et al. (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation extractors on Wikipedia.",
        "Previous works (Hoffmann et al., 2010; Yao et al., 2010) have pointed out that the DS assumption generates noisy labeled data, but did not directly address the problem.",
        "Wang et al. (2011) applied a rule-based method to the problem by using popular entity types and keywords for each relation.",
        "In (Bellare and McCallum, 2007; Riedel et al., 2010; Hoffmann et al., 2011), they used multi-instance learning, which deals with uncertainty of labels, to relax the DS assumption.",
        "However, the relaxed assumption can fail when a labeled entity pair is mentioned only once in a corpus (Riedel et al., 2010).",
        "Our approach relies on neither of these assumptions.",
        "Bootstrapping for relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006; Carlson et al., 2010) is related to our method.",
        "In bootstrapping, seed entity pairs of the target relation are given in order to select reliable patterns, which are used to extract new entity pairs.",
        "To avoid the selection of unreliable patterns, bootstrapping introduces scoring functions for each pattern candidate.",
        "This can be applied to our approach, which seeks to reduce the number of unreliable patterns by using a set of given entity pairs.",
        "However, the bootstrapping-like approach suffers from sensitive parameters that are critical to its performance.",
        "Ideally, the parameters such as a threshold for scoring function should be determined for each relation, but there are no principled methods (Komachi et al., 2008).",
        "In our approach, parameters are calibrated for each relation by maximizing the likelihood of our generative model."
      ]
    },
    {
      "heading": "3 Knowledge-based Distant Supervision",
      "text": [
        "In this section, we describe DS for relation extraction.",
        "We use the term relation as the relation between two entities.",
        "A relation instance is a tuple consisting of two entities and relation r. For example, place of birth(Michael Jackson, Gary) in Figure 1 is a relation instance.",
        "Relation extraction seeks to extract relation instances from text.",
        "An entity is mentioned as a named entity in text.",
        "We extract a relation instance from a",
        "single sentence.",
        "For example, from the upper sentence in Figure 1 we extract place of birth(Michael Jackson, Gary).",
        "Since two entities mentioned in a sentence do not always have a relation, we select entity pairs from a corpus when: (i) the path of the dependency parse tree between the corresponding two named entities in the sentence is no longer than 4 and (ii) the path does not contain a sentence-like boundary, such as a relative clause1 (Banko et al., 2007; Banko and Etzioni, 2008).",
        "Banko and Etzioni (2008) found that a set of eight lexico-syntactic forms covers nearly 95% of relation phrases in their corpus.",
        "(Fader et al. (2011) found that this set covers 69% of their corpus).",
        "Our rule is designed to cover at least the eight lexico-syntactic forms.",
        "We use the entity pairs extracted by this rule.",
        "DS uses a knowledge base to create labeled data for relation extraction by heuristically matching entity pairs.",
        "A knowledge base is a set of relation instances about predefined relations.",
        "For each sentence in the corpus, we extract all of its entity pairs.",
        "Then, for each entity pair, we try to retrieve the relation instances about the entity pair from the knowledge base.",
        "If we found such a relation instance, then the set of its relation, the entity pair, and the sentence is stored as a positive example.",
        "If not, then the set of the entity pair and the sentence is stored as a negative example.",
        "Features of an entity pair are extracted from the sentence containing the entity pair.",
        "As mentioned in Section 1, the assumption of DS can fail, resulting in wrong assignments of a relation to sentences that do not express the relation.",
        "We call such assignments wrong labels.",
        "An example of a wrong label is place of birth assigned to the lower sentence in Figure 1."
      ]
    },
    {
      "heading": "4 Wrong Label Reduction",
      "text": [
        "We define a pattern as the entity types of an entity pair2 as well as the sequence of words on the path of the dependency parse tree from the first entity to the second one.",
        "For example, from ?Michael Jackson was born in Gary?",
        "in Figure 1, the pattern ?",
        "[Person] born in [Location]?",
        "is extracted.",
        "We use entity",
        "Algorithm 1 Wrong Label Reduction labeled data generated by DS: LD negative patterns for relation r: NegPat(r) for each entry (r, Pair, Sentence) in LD do pattern Pat?",
        "the pattern from (Pair, Sentence)",
        "types to distinguish the sentences that express different relations with the same dependency path, such as ?ABBA was formed in Stockholm.?",
        "and ?ABBA was formed in 1970.?",
        "Our aim is to remove wrong labels assigned to frequent patterns, which cause poor precision.",
        "Indeed, in our Wikipedia corpus, more than 6% of the sentences containing the pattern ?",
        "[Person] moved to [Location]?, which does not express place of death, are labeled as place of death, and the labels assigned to these sentences hurt extraction performance (see Section 7.3.3).",
        "We would like to remove place of death from the sentences that contain this pattern.",
        "In our method, we reduce the number of wrong labels as follows: (i) given a labeled corpus with the DS assumption, we first predict whether a pattern expresses a relation and then (ii) remove wrong labels using the negative pattern list, which is defined as patterns that are predicted not to express the relation.",
        "In the first step, we introduce the novel generative model that directly models DS's labeling process and make the prediction (see Section 5).",
        "The second step is formally described in Algorithm 1.",
        "For relation extraction, we train a classifier for entity pairs using the resultant labeled data."
      ]
    },
    {
      "heading": "5 Generative Model",
      "text": [
        "We now describe our generative model, which predicts whether a pattern expresses relation r or not via hidden variables.",
        "In this section, we consider relation r since parameters are conditionally independent if relation r and the hyperparameter are given.",
        "An observation of our model is whether entity pair i appearing with pattern s in the corpus is labeled with relation r or not.",
        "Our binary observations are written as Xr = {(xrsi)|s = 1, .",
        ".",
        ".",
        ", S, i =",
        "Figure 2: Graphical model representation of our model.",
        "R indicates the number of relations.",
        "S is the number of patterns.",
        "Ns is the number of entity pairs that appear with pattern s in the corpus.",
        "xrsi is the observed variables.",
        "The circled variables except xrsi are parameters or hidden variables.",
        "?",
        "is the hyperparameter and mst is constant.",
        "The boxes are ?plates?",
        "representing replicates.",
        "1, .",
        ".",
        ".",
        ", Ns},3 where we define S to be the number of patterns and Ns to be the number of entity pairs appearing with pattern s. Note that we count an entity pair for given pattern s once even if the entity pair is mentioned with pattern s more than once in the corpus, because DS assigns the same relation to all mentions of the entity pair.",
        "Given relation r, our model assumes the following generative process:",
        "1.",
        "For each pattern s Choose whether s expresses relation r or not zrs ?",
        "Be(?r) 2.",
        "For each entity pair i appearing with pattern s",
        "Choose whether i is labeled or not xrsi ?",
        "P (xrsi|Zr, ar, dr, ?,M), where Be(?r) is a Bernoulli distribution with parameter ?r, zrs is a binary hidden variable that is 1 if pattern s expresses relation r and 0 otherwise, and Zr = {(zrs)|s = 1, .",
        ".",
        ".",
        ", S}.",
        "Given a value of zrs, we model two kinds of probabilities: one for patterns that actually express relation r, i.e., P (xrsi = 1|zrs = 1), and one for patterns that do not express r, i.e., P (xrsi = 1|zrs = 0).",
        "The former is simply parameterized as 0 ?",
        "ar ?",
        "1.",
        "We express the latter as brs = P (xrsi = 1|Zr, ar, dr, ?,M), which is a function of Zr, ar, dr, ?",
        "and M; we explain its modeling in the following two subsections.",
        "3Since a set of entity pairs appearing with pattern s is different, i should be written as is.",
        "For simplicity, however, we use i for each pattern.",
        "sets of entity pairs.",
        "E1/E2 has 6/4 entity pairs because the 6/4 entity pairs appear with pattern 1/2 in the target corpus.",
        "Pattern 1 expresses relation r and pattern 2 does not.",
        "Elements in E1 are labeled with probability ar = 3/6 = 0.5.",
        "Those in E2 are labeled with probability"
      ]
    },
    {
      "heading": "5.1 Example of Wrong Labeling",
      "text": [
        "Using a simple example, we describe how we model brs, the probability with which DS assigns relation r to pattern s via entity pairs when pattern s does not express relation r. Consider two patterns: pattern 1 that expresses relation r and pattern 2 that does not (i.e., zr1 = 1 and zr2 = 0).",
        "We also assume that there are entity pairs that appear with pattern 1 as well as with pattern 2 in different places in the corpus (for example, Michael Jackson and Gary in Figure 1).",
        "When such entity pairs are labeled, relation r is assigned to pattern 1 and at the same time to wrong pattern 2.",
        "Such entity pairs are observed as elements in the intersection of the two sets of entity pairs, E1 and E2.",
        "Here, Es is the set of entity pairs that appear with pattern s in the corpus.",
        "This situation is described in Figure 3.",
        "We model probability br2 as follows.",
        "In E1, an entity pair is labeled with probability ar.",
        "We assume that entity pairs in the intersection, E1 ?",
        "E2, are also labeled with ar.",
        "From the viewpoint of E2, entity pairs in its subset, E1 ?",
        "E2, are labeled with ar.",
        "Therefore, br2 is modeled as",
        "where |E |denotes the number of elements in set E. An example of this calculation is shown in Figure 3.",
        "We generalize the example in the next subsection."
      ]
    },
    {
      "heading": "5.2 Modeling of Probability brs",
      "text": [
        "We model brs so that it is proportional to the number of entity pairs that are shared with correct patterns",
        "indicates set intersections.",
        "However, the enumeration in Eq.1 requires O(SN2s ) computational cost and a huge amount of memory to store all of the entity pairs.",
        "We approximate the right",
        "This approximation is made, given the sizes of all Ess and those of all intersections of two Ess.",
        "This has a lower computational cost of O(S) and let us use less memory.",
        "We define S?S matrix M whose elements are mst = |Et ?",
        "Es|/|Es|.",
        "In reality, factors other than the process described in the previous subsection can cause wrong labeling (for example, errors in the knowledge base).",
        "We introduce a parameter 0 ?",
        "dr ?",
        "1 that covers such factors.",
        "Finally, we define brs as",
        "where 0 ?",
        "?",
        "?",
        "1 is the hyperparameter that controls how strongly brs is affected by the main labeling process explained in the previous subsection."
      ]
    },
    {
      "heading": "5.3 Likelihood",
      "text": [
        "Given observation Xr, the likelihood of our model",
        "For each pattern s, we define nrs as the number of entity pairs to which relation r is assigned (i.e.,",
        "where brs is in Eq.2."
      ]
    },
    {
      "heading": "6 Learning",
      "text": [
        "We learn parameters ar, ?r, and dr and infer hidden variables Zr by maximizing the log likelihood given Xr.",
        "Estimated Zr is used to predict which patterns express relation r. To infer zrs, we would like to calculate the posterior probability of zrs.",
        "However, this calculation is intractable because each zrs depends on the others, {(zrt)|t 6= s}, as shown in Eqs.2 and 3.",
        "This prevents us from using the EM algorithm.",
        "Instead, we apply variational approximation to the posterior distribution by using the following trial distribution:",
        "?zrsrs (1?",
        "?rs)1?zrs , where 0 ?",
        "?rs ?",
        "1 is a parameter for the trial distribution.",
        "The following function Fr is a lower bound of the log likelihood, and maximizing this function with respect to ?r is equivalent to minimizing the KL divergence between the trial distribution and the posterior distribution of Zr.",
        "EQ[?]",
        "represents the expectation over trial distribution Q.",
        "We maximize function Fr with respect to the parameters instead of the log likelihood.",
        "However, we need further approximation for two terms on expanding Eq.4.",
        "Both of the terms are expressed as EQ[log(f(Zr))], where f(Zr) is a function of Zr.",
        "We apply the following approximation",
        "This is based on the Taylor series of log at EQ[f(Zr)].",
        "In our problem, since the second derivative is sufficiently small, we use the zeroth-order approximation.4 Our learning algorithm is derived by calculating the stationary condition of the resultant evaluation function with respect to each parameter.",
        "We have the exact solution for ?r.",
        "For each ?rs and dr, we derive a fixed point iteration.",
        "We update ar by using the steepest ascent.",
        "We update each parameter in turn while keeping the other parameters fixed.",
        "Parameter updating proceeds until a termination condition is met.",
        "After learning, we have ?rs for each pair of relation r and pattern s. The greater the value of ?rs is, the more likely it is that pattern s expresses relation r. We set a threshold and determine zrs = 0 when ?rs is less than the threshold."
      ]
    },
    {
      "heading": "7 Experiments",
      "text": [
        "We performed two sets of experiments.",
        "Experiment 1 aimed to evaluate the performance of our generative model itself, which predicts whether a pattern expresses a relation, given a labeled corpus created with the DS assumption.",
        "Experiment 2 aimed to evaluate how much our wrong label reduction in Section 4 improved the performance of relation extraction.",
        "In our method, we trained a classifier with a labeled corpus cleaned by Algorithm 1 using the negative pattern list predicted by the generative model."
      ]
    },
    {
      "heading": "7.1 Dataset",
      "text": [
        "Following Mintz et al. (2009), we carried out our experiments using Wikipedia as the target corpus and Freebase (September, 2009, (Google, 2009)) as the knowledge base.",
        "We used more than 1,300,000 Wikipedia articles in the wex dump data (September, 2009, (Metaweb Technologies, 2009)).",
        "The properties of our data are shown in Table 1.",
        "In Wikipedia articles, named entities were identified by anchor text linking to another article and starting with a capital letter (Yan et al., 2009).",
        "We applied Open NLP POS tagger5 and MaltParser (Nivre et al., 2007) to sentences containing more",
        "containing related entity pairs with the method explained in Section 3.",
        "To match entity pairs, we used ID mapping between the dump data and Freebase.",
        "We used the most frequent 24 relations."
      ]
    },
    {
      "heading": "7.2 Experiment 1: Pattern Prediction",
      "text": [
        "We compared our model with baseline methods in terms of ability to predict patterns that express a given relation.",
        "The input of this task was Xrs, which expresses whether or not each entity pair appearing with each pattern is labeled with relation r, as explained in Section 5.",
        "In Experiment 1, since we needed entity types for patterns, we restricted ourselves to entities matched with Freebase, which also provides entity types for entities.",
        "We used patterns that appear more than 20 times in the corpus.",
        "We split the data into training data and test data.",
        "The training data was Xrs for 12 relations and the test data was that for the remaining 12 relations.",
        "The training data was used to calibrate parameters (see the following subsection for details).",
        "The test data was used for evaluation.",
        "We randomly split the data five times and took the average of the following evaluation values.",
        "We evaluated the performance by precision, recall, and F value.",
        "They were calculated using gold standard data, which was constructed by hand.",
        "We manually selected patterns that actually express a target relation as positive patterns for the relation.",
        "6 We averaged the evaluation values in terms of macro average over relations before averaging over the data splits.",
        "6Patterns that ambiguously express the relation, for instance ?",
        "[Person] in [Location]?",
        "for place of birth, were not selected as positive patterns.",
        "We compared the following methods: Baseline: This method assigns relation r to a pattern when the pattern is mentioned with at least one entity pair corresponding to relation r in Freebase.",
        "This method is based on the DS assumption.",
        "Ratio-based Selection (RS): Given relation r and pattern s, this method calculates nrs/Ns, which is the ratio of the number of labeled entity pairs appearing with pattern s to the number of entity pairs including unlabeled ones.",
        "RS then selects the top n patterns (RS(rank)).",
        "We also tested a version using a real-valued threshold (RS(value)).",
        "In training, we selected the threshold that maximized the F value.",
        "Some bootstrapping approaches (Carlson et al., 2010) use a rank-based threshold like RS(rank).",
        "Proposed Model (PROP): Using the training data, we determined the two hyperparameters, ?",
        "and the threshold to round ?rs to 1 or 0, so that they maximized the F value.",
        "When ?rs is greater than the threshold, we select pattern s as one expressing relation r.",
        "The results of Experiment 1 are shown in Table 2.",
        "Our model achieved the best precision, recall, and F value.",
        "RS(value) had the second best F value, but it completely removed more than one infrequent relation on average in test sets.",
        "This is problematic for real situations.",
        "RS(rank) achieved the second highest precision.",
        "However, its recall, which is also important in our task, was the lowest and its F value was almost the same as naive Baseline.",
        "The thresholds of RS, which directly affect their performance, should be calibrated for each relation, but it is hard to do this in advance.",
        "On the other",
        "place of birth .",
        "Entity types are omitted in patterns.",
        "nrs/Ns is the ratio of the number of labeled entity pairs to the number of entity pairs appearing with pattern s. pattern s nrs/Ns ?rs expresses r?",
        "hand, our model learns parameters such as ar for each relation and thus the hyperparameter of our model does not directly affect its performance.",
        "This results in a high prediction performance.",
        "Examples of estimated ?rs, the probability with which pattern s expresses relation r, are shown in",
        "[Location]?, which does not express place of birth, had low ?rs in spite of having higher nrs/Ns than the valid pattern ?",
        "[Person] native of [Location]?.",
        "The former pattern had higher brs, the probability with which relation r is wrongly assigned to pattern s via entity pairs, because there were more entity pairs that appeared not only with this pattern but also with patterns that was predicted to express place of birth."
      ]
    },
    {
      "heading": "7.3 Experiment 2: Relation Extraction",
      "text": [
        "We investigated the performance of relation extraction using our wrong label reduction, which uses the results of the pattern prediction.",
        "Following Mintz et al. (2009), we performed an automatic held-out evaluation and a manual evaluation.",
        "In both cases, we used 400,000 articles for testing and the remaining 903,000 for training."
      ]
    },
    {
      "heading": "7.3.1 Configuration of Classifiers",
      "text": [
        "Following Mintz et al. (2009), we used a multi-class logistic classifier optimized using L-BFGS with Gaussian regularization to classify entity pairs to the predefined 24 relations and NONE.",
        "In order to train the NONE class, we randomly picked 100,000 examples that did not match to Freebase as pairs.",
        "(Several entities in the examples matched and had entity types of Freebase.)",
        "In this experiment, we",
        "used not only entity pairs matched to Freebase but also ones not matched to Freebase (i.e., entity pairs that do not have entity types).",
        "We used syntactic features (i.e., features obtained from the dependency parse tree of a sentence) and lexical features, and entity types, which essentially correspond to the ones developed by Mintz et al. (2009).",
        "We compared the following methods: logistic regression with the labeled data cleaned by the proposed method (PROP), logistic regression with the standard DS labeled data (LR), andMultiR proposed in (Hoffmann et al., 2011) as a state-of-the-art multi-instance learning system.7 For logistic regression, when more than one relation is assigned to a sentence, we simply copied the feature vector and created a training example for each relation.",
        "In PROP, we used training articles for pattern prediction.8",
        "In the held-out evaluation, relation instances discovered from testing articles were automatically compared with those in Freebase.",
        "This let us calculate the precision of each method for the best n relation instances.",
        "The precisions are underestimated because this evaluation suffers from false negatives due to the incompleteness of Freebase.",
        "We changed n from 5 to 50,000 and measured precision and recall.",
        "Precision-recall curves for the held-out data are"
      ]
    },
    {
      "heading": "PROP MultiR LR",
      "text": [
        "place of birth 1.0 1.0 0.56 place of death 1.0 0.7 0.84 average 0.89?0.14 0.83?0.21 0.82?0.23 shown in Figure 4.",
        "PROP achieved comparable or higher precision at most recall levels compared with LR and MultiR.",
        "Its performance at n = 50,000 is much higher than that of the others.",
        "While our generative model does not use unlabeled examples as negative ones in detecting wrong labels, classifier-based approaches including MultiR do, suffering from false negatives.",
        "For manual evaluation, we picked the top ranked 50 relation instances for the most frequent 15 relations.",
        "The manually evaluated precisions averaged over the 15 relations are shown in table 4.",
        "PROP achieved the best average precision.",
        "For place of birth, LR wrongly extracted entity pairs with ?",
        "[Person] played with club [Location]?, which does not express the relation.",
        "PROP and MultiR avoided this mistake.",
        "For place of death, LR and MultiR wrongly extracted entity pairs with ?",
        "[Person] moved to [Location]?.",
        "Multi-instance learning does not work for wrong labels assigned to entity pairs that appear only once in a corpus.",
        "In fact, 72% of entity pairs that appeared with this pattern and were wrongly labeled as place of death appeared only once in the corpus.",
        "Only PROP avoided mistakes of this kind because our method works in such situations."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We proposed a method that reduces the number of wrong labels created with the DS assumption, which is widely applied.",
        "Our generative model directly models the labeling process of DS and predicts patterns that are wrongly labeled with a relation.",
        "The predicted patterns are used for wrong label reduction.",
        "The experimental results show that this method successfully reduced the number of wrong labels and boosted the performance of relation extraction."
      ]
    }
  ]
}
