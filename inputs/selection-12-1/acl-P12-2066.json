{
  "info": {
    "authors": [
      "Zhaopeng Tu",
      "Yifan He",
      "Jennifer Foster",
      "Josef van Genabith",
      "Qun Liu",
      "Shouxun Lin"
    ],
    "book": "ACL",
    "id": "acl-P12-2066",
    "title": "Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification",
    "url": "https://aclweb.org/anthology/P12-2066",
    "year": 2012
  },
  "references": [
    "acl-D09-1017",
    "acl-D09-1143",
    "acl-H05-1044",
    "acl-H05-1091",
    "acl-J08-2003",
    "acl-N10-1121",
    "acl-P03-1054",
    "acl-P04-1035",
    "acl-P06-1104",
    "acl-P06-2079",
    "acl-P08-2029",
    "acl-P09-2079",
    "acl-W03-1017",
    "acl-W08-1301",
    "acl-W10-2910",
    "acl-W11-0705"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Convolution kernels support the modeling of complex syntactic information in machine-learning tasks.",
        "However, such models are highly sensitive to the type and size of syntactic structure used.",
        "It is therefore an important challenge to automatically identify high impact substructures relevant to a given task.",
        "In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification.",
        "We show that minimal substructures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "An important subtask in sentiment analysis is sentiment classification.",
        "Sentiment classification involves the identification of positive and negative opinions from a text segment at various levels of granularity including document-level, paragraph-level, sentence-level and phrase-level.",
        "This paper focuses on document-level sentiment classification.",
        "There has been a substantial amount of work on document-level sentiment classification.",
        "In early pioneering work, Pang and Lee (2004) use a flat feature vector (e.g., a bag-of-words) to represent the documents.",
        "A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the documents.",
        "More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Senef-f (2009).",
        "However, as they select features manually, these methods would require additional labor when ported to other languages and domains.",
        "In this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules.",
        "While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008).",
        "We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences.",
        "It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task.",
        "It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe the plot.",
        "Such objective portions do not contain the author's opinion and are irrelevant with respect to the sentiment classifi",
        "cation task.",
        "Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009).",
        "Our research is inspired by these observations.",
        "Unlike in the previous work, however, we focus on syntactic substructures (rather than entire paragraphs or sentences) that contain subjective words.",
        "More specifically, we use the terms in the lexicon constructed from (Wilson et al., 2005) as the indicators to identify the substructures for the convolution kernels, and extract different substructures according to these indicators for various types of parse trees (Section 3).",
        "An empirical evaluation on a widely used sentiment corpus shows an improvement of 1.45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4)."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Our research builds on previous work in the field of sentiment classification and convolution kernels.",
        "For sentiment classification, the design of lexical and syntactic features is an important first step.",
        "Several approaches propose feature-based learning algorithms for this problem.",
        "Pang and Lee (2004) and Dave et al. (2003) represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output.",
        "Previous research has convincingly demonstrated a kernel's ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008).",
        "Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data.",
        "Wiegand and K-lakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word.",
        "Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively.",
        "However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent."
      ]
    },
    {
      "heading": "3 Kernels for Sentiment Classification",
      "text": []
    },
    {
      "heading": "3.1 Linguistic Representations",
      "text": [
        "We explore both sequence and convolution kernels to exploit information on surface and syntactic levels.",
        "For sequence kernels, we make use of lexical words with some syntactic information in the form of part-of-speech (POS) tags.",
        "More specifically, we define three types of sequences:",
        "?",
        "SW, a sequence of lexical words, e.g.: A tragic waste of talent and incredible visual effects.",
        "?",
        "SP, a sequence of POS tags, e.g.: DT JJ NN IN NN CC JJ JJ NNS.",
        "?",
        "SWP, a sequence of words and POS tags, e.g.: A/DT tragic/JJ waste/NN of/IN talent/NN and/CC incredible/JJ visual/JJ effects/NNS.",
        "In addition, we experiment with constituency tree kernels (CON), and dependency tree kernels (D), which capture hierarchical constituency structure and labeled dependency relations between words, respectively.",
        "For dependency kernels, we test with word (DW), POS (DP), and combined word-and-POS settings (DWP), and similarly for simple sequence kernels (SW, SP and SWP).",
        "We also use a vector kernel (VK) in a bag-of-words baseline.",
        "Figure 1 shows the constituent and dependency structure for the above sentence."
      ]
    },
    {
      "heading": "3.2 Settings",
      "text": [
        "As kernel-based algorithms inherently explore the whole feature space to weight the features, it is important to choose appropriate substructures to remove unnecessary features as much as possible.",
        "stituency (CON) and dependency (DWP) parse trees with tragic as the indicator word.",
        "Unfortunately, in our task there exist several cues indicating the polarity of the document, which are distributed in different sentences.",
        "To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al., 2005).",
        "For each polarity indicator, we define the ?scope?",
        "(the minimal syntactic structure containing at least one subjective word) of each indicator for different representations as follows: For a constituent tree, a node and its children correspond to a grammatical production.",
        "Therefore, considering the terminal node tragic in the constituent structure tree in Figure 1(a), we extract the subtree rooted at the grandparent of the terminal, see Figure 2(a).",
        "We also use the corresponding sequence",
        "average number of trees, and Size denotes the averaged number of words in each tree.",
        "of words in the subtree for the sequential kernel.",
        "For a dependency tree, we only consider the subtree containing the lexical items that are directly connected to the subjective word.",
        "For instance, given the node tragic in Figure 1(d), we will extract its direct parent waste integrated with dependency relations and (possibly) POS, as in Figure 2(b).",
        "We further add two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Setup",
      "text": [
        "We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of",
        "1000 positive reviews and 1000 negative reviews.",
        "To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003).",
        "To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008).",
        "We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1, respectively.",
        "A sequential kernel is applied for lexical sequences.",
        "Kernels were combined using plain (unweighted) summation.",
        "Corpus statistics are provided in Table 1.",
        "We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral).",
        "We only take into account the subjective terms with the degree of strong subjectivity.",
        "We consider two baselines: ?",
        "VK: bag-of-words features using a vector kernel (Pang and Lee, 2004; Ng et al., 2006) ?",
        "Rand: a number of randomly selected substructures similar to the number of extracted substructures defined in Section 3.2 All experiments were carried out using the SVM-Light-TK toolkit2 with default parameter settings.",
        "All results reported are based on 10-fold cross validation."
      ]
    },
    {
      "heading": "4.2 Results and Discussions",
      "text": [
        "Table 2 lists the results of the different kernel type combinations.",
        "The best performance is obtained by combining VK and DW kernels, gaining a significant improvement of 1.45 point in accuracy.",
        "As far as PT kernels are concerned, we find dependency trees with simple words (DW) outperform both dependency trees with POS (DP) and those with both words and POS (DWP).",
        "We conjecture that in this case, as syntactic information is already captured by 1A SubSet Tree is a structure that satisfies the constraint that grammatical rules cannot be broken, while a Partial Tree is a more general form of substructures obtained by the application of partial production rules of the grammar.",
        "document of the text, Sent denotes the sentences that contains subjective terms in the lexicon, Rand denotes randomly selected substructures, and Sub denotes the substructures defined in Section 3.2.",
        "We use ?*?",
        "and ?**?",
        "to denote a result is better than baseline VK significantly at p < 0.05 and p < 0.01 (sign test), respectively.",
        "the dependency representation, POS tags can introduce little new information, and will add unnecessary complexity.",
        "For example, given the substructure (waste (amod (JJ (tragic)))), the PT kernel will use both (waste (amod (JJ))) and (waste (amod (JJ (tragic)))).",
        "We can see that the former is adding no value to the model, as the JJ tag could indicate either positive words (e.g. good) or negative words (e.g. tragic).",
        "In contrast, words are good indicators for sentiment polarity.",
        "The results in Table 2 confirm two of our hypotheses.",
        "Firstly, it clearly demonstrates the value of incorporating syntactic information into the document-level sentiment classifier, as the tree kernels (CON and D*) generally outperforms vector and sequence kernels (VK and S*).",
        "More importantly, it also shows the necessity of extracting appropriate substructures when using convolution kernels in our task: when using the dependency kernel (VK+DW), the result on lexicon guided substructures (Sub) outperforms the results on document, sentence, or randomly selected substructures, with statistical significance (p<0.05)."
      ]
    },
    {
      "heading": "5 Conclusion and Future Work",
      "text": [
        "We studied the impact of syntactic information on document-level sentiment classification using convolution kernels, and reduced the complexity of the kernels by extracting minimal high-impact substructures, guided by a polarity lexicon.",
        "Experiments",
        "show that our method outperformed a bag-of-words baseline with a statistically significant gain of 1.45 absolute point in accuracy.",
        "Our research focuses on identifying and using high-impact substructures for convolution kernels in document-level sentiment classification.",
        "We expect our method to be complementary with sophisticated methods used in state-of-the-art sentiment classification systems, which is to be explored in future work."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "The authors were supported by 863 State Key Project No.",
        "2006AA010108, the EuroMatrixPlus F-P7 EU project (grant No 231720) and Science Foundation Ireland (Grant No.",
        "07/CE/I1142).",
        "Part of the research was done while Zhaopeng Tu was visiting, and Yifan He was at the Centre for Next Generation Localisation (www.cngl.ie), School of Computing, Dublin City University.",
        "We thank the anonymous reviewers for their insightful comments.",
        "We are also grateful to Junhui Li for his helpful feedback."
      ]
    }
  ]
}
