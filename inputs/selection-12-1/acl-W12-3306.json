{
  "info": {
    "authors": [
      "Kayo Tatsukawa"
    ],
    "book": "Proceedings of ACL 2012 Student Research Workshop",
    "id": "acl-W12-3306",
    "title": "Topic Extraction based on Prior Knowledge obtained from Target Documents",
    "url": "https://aclweb.org/anthology/W12-3306",
    "year": 2012
  },
  "references": [
    "acl-N10-1012",
    "acl-P11-1026",
    "acl-W09-2206"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper investigates the relation between prior knowledge and latent topic classification.",
        "There are many cases where the topic classification done by Latent Dirichlet Allocation results in the different classification that humans expect.",
        "To improve this problem, several studies using Dirichlet Forest prior instead of Dirichlet distribution have been studied in order to provide constraints on words so as they are classified into the same or not the same topics.",
        "However, in many cases, the prior knowledge is constructed from a subjective view of humans, but is not constructed based on the properties of target documents.",
        "In this study, we construct prior knowledge based on the words extracted from target documents and provide it as constraints for topic classification.",
        "We discuss the result of topic classification with the constraints."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "We have recently faced situations in which we have to deal with a huge amount of text resources.",
        "To deal with these text resources, unlike studies to analyze the surface information of the resources, but a lot of studies to analyze latent semantics by means of Latent Dirichlet Allocation (LDA) (Blei et al., 2003) have been being studied.",
        "When extracting latent topics by means of LDA, there are many cases where the words naturally expected to be in the same topic are classified into different topics.",
        "To deal with this problem, several studies to provide a constraint for words to be in the same topic have been studied.",
        "Andrzejewski (Andrzejewski et al., 2009) has proposed a method to provide a constraint for topic clustering as prior knowledge consisting of the words, which should be in the same topic by applying Dirichlet Forest Prior as word probability distribution instead of Dirichlet distribution.",
        "However, in many cases, the prior knowledge is constructed from a subjective view of humans but is not automatically constructed based on the properties of target documents.",
        "In this study, we extract the words, which will be prior knowledge for extracting topics, from target documents, and provide it as a constraint for topic clustering, and then discuss the result of topic clustering with constraints on the words.",
        "2 Related studies Many studies to incorporate prior knowledge into topic models to raise the accuracy of topic cluster-ing, introducing the techniques of semi-supervised learning(Andrzejewski et al., 2007; Andrzejewski et al., 2009; Andrzejewski and Zhu, 2009).",
        "Andrzejewski (Andrzejewski et al., 2009) has incorporated a constraint on words into topic clustering by using Dirichlet Forest Prior instead of Dirichlet distribution.",
        "They have introduced `Must-links' and `Cannot-links', referring to the techniques of semi-supervised learning.",
        "`Must-links' is a constraint that two words with similar probability dls'-tribution should be in the same topic.",
        "`Cannot-links' is a constraint that two words with different probability distribution for all topics should be separated into different topics.",
        "Hu (Hu et al., 2011) has proposed a method which repeatedly extracts latent topics through the interaction with humans con straints are added interactively by humans.",
        "In addi-31 Proceedings of the 2012 Student Research Workshop, pages 31-36, Jeju, Republic of Korea, 8-14 July 2012.",
        "@2012 Association for Computational Linguistics tion, Kobayashi (Kobayashi et al., 2011) has made it possible to use logical operation to combine the constraints, `Must-links' and `Cannot-links', in constructing prior knowledge.",
        "By this', they have proposed a method which can add new constraints constructed by logical operation of various constraints, and extract topics based on the constraints.",
        "In general, as for clustering with constraints, it Is~ often that the constraints are given by humans.",
        "However, there are many cases where the constraints constructed by humans are arbitrary, in addition, it is laborious to construct prior knowledge for each target document.",
        "In this context, Kaji (Kaji et al., 2007) extracted synonyms from corpus by using vocabulary syntactic patterns and constructed prior knowledge for word clustering based on the synonyms.",
        "However, the method Kaji proposed obtains prior knowledge by learning approximately 1 billion corpus.",
        "So, it also costs much to construct the knowledge, further-~more, the obtained knowledge might be constraints for general purposes, but not for target documents.",
        "So, the constructed knowledge might not be appropriate for the target documents.",
        "Considering these things, in this study, we use Dirichlet Forest Prior for word probability dls'tribution and extract latent topics by the prior knowledge obtained from target documents, without using any big corpus.",
        "Then we will discuss how our method improves the accuracy of topic extraction.",
        "3 Topic extraction by prior knowledge 3.1 Dirichlet Forest LDA We use Dirichlet Forest prior (DF) as word probability distribution instead of Dirichlet distribution to reflect constraints on latent topic clustering.",
        "DF is hierarchical Dirichlet distribution and it uses a for topic distribution and ~ for word probability distribution as the hyper-parameters of Dirichlet dls'tribution just like the conventional LDA.",
        "In addition, we use h which reflects the strength of given constraints on word occurrence distribution.",
        "In Dirichlet Forest, each leave has occurrence probability for each word and the sum of occurrence probability for all words becomes I .",
        "In the process of generating a docu-~ment with LDA using DF(LDA-DF), we firstly get a multinomial distribution 0 with a hyper-parameter a, and then according to this multinomial dls'tribu-tion, a topic Z is selected.",
        "Secondly, we get a multinomial distribution f with a hyper-parameter ~, and then under the topic Z selected at 0, a word or a constraint is selected.",
        "If a word is selected, it is used directly to generate a document and if a constraint is selected, a word Is~ selected according to a multino-~mial distribution 7r with hyper-parameter h. Here, let di denote the documents which contain the i-th word wi and zi denote the topic which assigns on wi .",
        "Using these parameters, LDA-DF is represented with the below equations.",
        "(1) (2) (3) (4) (5) 3.2 Construction of prior knowledge Newman (Newman et al., 2010) discusses various evaluation indices about the topic coherence.",
        "In this study, we choose Point-wise Mutual Informa-tion(PMI) as an index to measure topic coherence, and then estimate how much each obtained cluster increases topic coherence in itself.",
        "The reason why we choose PMI to measure topic coherence is based on the assumption that a topic is represented by the words with close relationship.",
        "To construct prior knowledge, it is necessary to select words regarded as representatives of a topic.",
        "In this study, we assume that the words regarded as representatives of a topic (`important words~, here-after) frequently appear in all documents or have ~many co-occurrence relations with a lot of other words.",
        "We select important words by following the two basic ideas shown below.",
        "(i) Important words based on frequency In the case of dealing with multiple documents about the same topic, the words which frequently appear in all documents are regarded as necessary words to represent the contents of the documents.",
        "So, we regard such words as important words.",
        "(ii) Important words based on co-occurrence In this study, we construct prior knowledge as we suppose that a pair of words with high PMI value 32 should be classified into the same topic.",
        "So, we regard the words, which have many co-occurrence relations with other words, as important words.",
        "The prior knowledge is constructed by the following process.",
        "step.1 Important words based on frequency or co-occurrence are selected.",
        "step.2 Important words obtained at step.",
        "1 are classified into some groups based on co-occurrence relation.",
        "At this time, we use PMI as index to measure co-occurrence relation between words, and unite important words, which have higher PMI than the predefined threshold value, into a group.",
        "step.3 Prior knowledge, i.e., the group obtained at step2, is constructed based on the words with high PMI values, therefore, the words which have high PMI value with the words in the group obtained at step.2 are further selected and added to the group, if necessary.",
        "Depending on the number of words added to the group, prior knowledge will be changed.",
        "So, we experiment to investigate the influence of the number of added words, changing the number of the words from 1 to 4.",
        "The detail about the experiment is mentioned in section 4.",
        "4 Experiment 4.1 Experinlental settings As the documents for the experiment to extract top-ics, we used news articles about the same incident.",
        "The news articles we used are ABC News in USA, BBC News in UK, CTV News in Canada, which are published by main newspaper companies and TV companies in Englls~ h-speaking countries.",
        "We used the following 4 articles for the exper-iment: 10 articles about Press conference about the convergence of atomic power plant dis aster by Japanese prime minis.ter, 2011/12/16~ consist of 212 documents and 853 terms; 24 articles about Grounding of pomp passenger ferry in Italy, 2012/1/16~ consist of 967 documents and 2267 terms; 25 articles about Protest from Wikipedia to Stop Online Piracy Act (SOPA), 2012/1/16~ consist of 700 documents and 1823 terms; 18 articles about Resignation of co-founder Yahoo!, 2012/1/16~ consist of 553 documents and 1113 terms.",
        "In the experiment, we used a - 0.1, ~ - 0.1, Tl 100 as hyper-parameters for LDA-DF and Collapsed Gibbs Sampling (Griffiths et al., 2004) for the presumption of probability dis.tribution with 50 iteration times.",
        "Although we could set the number of topics so as it fitted target documents by means of perplexity, since we aim to evaluate adequacy of grouping of words, adequacy of topic clustering in other words, in the same condition, so we conducted an experi-~ment, setting the number of topics as 10 for all the target articles.",
        "In Hu~s study (Hu et al., 201 1), in response to given words as constraints, they re-presumed latent topics by canceling a part of the topics already assigned to words by the topic model prior to addition of new constraints.",
        "They suggested 4 ways of selecting words to cancel a part of topics, and reported that in the 4 ways they got good results when new prior knowledge is added, topic assignment for all the words of the documents which include the words in the prior knowledge is once canceled and then applied again.",
        "Therefore, we also cancel the topics assigned to words in the same way of theirs.",
        "We calculate the value of perplexity of topic dis.-tribution and compare the stability of a model between before and after giving constraints.",
        "we calculate perplexity with equation (6).",
        "Here, N is the number of all words in the target documents, Wmn is the nth word in the m-th document; 0 is occurrence probability of topic for the documents, and f is occurrence probability of the words for every topic.",
        "4.2 Experinlent result Table 1 shows the groups of important words based on frequency and co-occurrence, and the words with high PMI score to the important words which are candidates to be added to the prior knowledge.",
        "We take up the article about Press conference about the convergence of atomic power plant disaster by Japanese prime minis.ter~ and explain how to interpret Table 1.",
        "Looking at the intersection between the row of frequency and the column of Atomic plant~ 33 Table 1: Groups of important words based on frequency or co-occurrence, and added words T~es/Articles Atomic plant Grounding of fe~y ~otest to SOPA Yahoo!",
        "co-founder Yahoo!",
        "co-founder Frequency grouping words {prime,minister,reactor, fukushima}' ' {power,tokyo},{cold}, {nuclear},{plant}, {shutdown}.",
        "{costa},{passenger}, {people} {wikipedia},{online}, {piracy},{in~met} {yang},{board}, {yahoo},{comp~y}, {thompson} ~ded words yoshihiko,electric,reached, noda,march,Sta~ appears,unaccounted, ~day wale,stop,protect,~ee bostock,posltlon, ch~rman,struggling,Scott Co-occurrence grouping words {cooling,contaminated, water},{site},{year} }staworsb~~as~er~ponse}~ {disaster,caused,sea}, {aground,ran},{gash}, }teaul~~ o~~~~ev~ ~ on} {medium,industry,group, tech,information,popular}, }~}~~~~s{c~ } {private,pursuing,deal, shareholder,asian}, {began},{leaving}, {resignation},{chief}, {medium} ~ded words ton,liquid,end,t~k Chernobyl technical,late,side, trained,human,survivor social,web,proposed, provider,wale large,umverslty, struggling,thompson, Scott,trading in Table 1, the extracted important words were united to one group depending on the value of PMI, and then we obtained the following 6 groups: {prime, mims'ter, reactor, fukushima},{power, tokyo},{cold},{nuclear},{plant},{shutdown}.",
        "After that, we added some words with high PMI value to each group to achieve the construction of prior knowledge.",
        "The words expected to be added to the groups are shown at the next row of grouping words.",
        "In fact, depending on the number of the words added to the groups of important words, prior knowledge will be changed and the result of topic clustering will also be changed.",
        "Furthermore, depending on the number of given constraints, the result of topic clustering will also be changed.",
        "Therefore, we examine how accuracy of topic clustering changes by ~means of perplexity as its index, increasing the number of given constraints one by one from the initial condition, i.e., without any constraint.",
        "Here, we think that the values of PMI and perplexity will be changed by the combination of prior knowledge, however, in this study, we gave the constraints in the order of a group with higher PMI value.",
        "Frequency Fl~ure 2: Grounding of pomp passenger fe~ in Italy~ ~one word \"w' two words ~ Frequency Co-occurrence Figure 3: I~otest of Wikl~pedia to SOPA~ Frequency Co-occurrence Figure 4: Resignation of Yahoo!",
        "co-founder~ Frequency Co-occuWence Figure 1: Convergence of atomic plant disaster~ 34 Table 2: Ton 10 renresentative words for tonics extracted from the article Convergence of atomic nlant disaster~ topic topicO topic} topic2 topic3 topic4 topicS topic6 topic7 topicS topic9 LDA water plant contaminated remains decade expert facility cool point problem plant return home government zone resident remain evacuation mile doe nuclear task told crisis meeting force disaster nod situation response nuclear tsunami crisis march plant earthquake announcement fukushima meltdown month cold shutdown reactor plant fukushima condition government stable power reached accident nuclear disaster Chernobyl university term country part engineering professor tokyo electric power leak time week knocked huge bring official year radiation plant area level government expected official start boundary cooling minister reactor prime noda system degree yoshihiko nuclear temperature reactor fuel tepco temperature rod melted spent inside damaged damage topic topicO topic} topic2 topic3 topic4 topicS topic6 topic7 topicS topic9 LDA-DF~ with constraints facility waste tepco doe contaminated water including cooling earlier sea plant home return mile zone government area resident official remain nuclear noda task power force meeting japan set measure declared tsunami plant march earthquake reactor crisis meltdown system daiichi people cold shutdown plant reactor condition fukushima government reached friday radiation nuclear cleanup university country significant disaster Chernobyl worst term engineering tokyo electric government power official told week bring company tepco plant radiation level year e~ decade government decommission expert accident nuclear prime minister degree announcement yoshihiko noda milestone mark news reactor fuel temperature tepco rod fukushima melted cool spent inside 4.3 I)iscussions We show the changes of perplexity in Figure 1, 2, 3, and 4 when increasing the number of constraints based on frequency and co-occurrence.",
        "In the Fig-ures, the horizontal axis indicates the number of pieces of prior knowledge, and the vertical axl s indicates the value of perplexity.",
        "Looking at these Fig-ures, we see that the case of providing constraints based on co-occurrence decreases perplexity as the number of constraints increases, and the topic model becomes more stable than the case without constraint.",
        "Furthermore, we see that perplexity of each graph of co-occurrence can be decreased if providing one or two additional words with high value of PMI as a part of prior knowledge, and also that perplexity becomes stable when approximately 3 constraints are provided.",
        "From these observations, we think that we do not have to provide so many constraints to get good topic clustering.",
        "On the other hand, unlike the case of providing constraints based on co-occurrence, we cannot get a general view for the case of providing constraints based on frequency from the results.",
        "The reason why we could get good results when providing constraints based on co-occurrence information is that we constructed the prior knowledge which simultaneously reflects both `Must-Links~ and `Cannot-Links~ used as prior knowledge in (An- drzejewski et al., 2009), because PMI represents the co-occurrence relation of words in a sentence, so we think that it could divide the words should be included or should not be included in a topic.",
        "On the other hand, we also see the case where perplexity gets increased even if selecting important words based on co-occurrence.",
        "Looking at the case of providing four additional words to prior knowledge in the graph of co-occurrence, especially in Figure 2,3,and 4, we see that perplexity increases as the number of pieces prior knowledge increases.",
        "We think the reason for this is because we added words to prior knowledge in the order of high PMI value, so the fourth word should not have had high PMI value, therefore, topic clusters became unstable.",
        "Table 2 shows the result of topic classification of the article about `Press conference of the convergence of atomic power plant disaster by Japanese prime mims.ter.~ We added the following constraints as prior knowledge: {worst, dis aster, cher-nobyl},{cooling, contaminated, water, ton}, and {year, end} which is constructed based on co-occurrence information in the objective article.",
        "The upper row of Table 2 is the result of the conventional LDA without any constraint and the lower row is that of LDA-DF with constraints.",
        "We see from Table 2 that the words consisting of 35 prior knowledge are split into two topics at the upper row, whereas, they are classified in the same topic, i.e., topic 0,5,and 7 at the lower row.",
        "We see that topic clustering with the constraints has been well achieved.",
        "5 Conclusion The conventional LDA sometimes results in topic classification different from what humans expect.",
        "To improve this, several studies providing constraints for topic clustering have been studied, referring to the techniques of semi-supervised learning.",
        "In this study, we have constructed prior knowl-edge, which becomes constraints for topic cluster-ing, with target documents which topics are ex-tracted, unlike the studies to construct the knowledge with huge corpus.",
        "The prior knowledge will be constructed as a collection of the words expected to be representative of a topic.",
        "Based on this, we have introduced two ways to construct the knowl-edge: one Is to select important words based on frequency and the other is to select words based on co-occurrence from target documents.",
        "We have compared the results of topic clustering by giving the two types of prior knowledge, and then recognized that the result of topic clustering based on the prior knowledge constructed based on co-occurrence is better than that by the prior knowledge constructed based on frequency.",
        "Furthermore, we have also investigated how much prior knowledge should be given as constraints for good topic cluster-ing, and then obtained a result that good clustering is achieved even with a few pieces of prior knowl-edge, if the prior knowledge is constructed based on word co-occurrence.",
        "However, we have also observed several cases where this result cannot be correct.",
        "We need more investigation about this, revising the way of constructing prior knowledge.",
        "For future work, we will investigate another possibility to construct prior knowledge, and will apply our proposed ~method to various kinds of many documents.",
        "References David M. Blei and Andrew Y. Ng and Michael I. Jordan and John Lafferty.",
        "2003.",
        "Latent dirichlet allocation, Journal of Machine Learning Research, Hayato Kobayashi and Hiromi Wakaki and Tomohiro Ya-masakl~ and Masaru Suzukl~ 2011.",
        "Topic Models with Logical Constraints on Words, Proc. of Workshop on Robust Unsupervised and Semisupervised Methods in Natural Language Processing, Andrzejewskl, Anne Mulhern, Ben Liblit, and Xiaojin Zhu, 2007.",
        "Statistical Debugging Using Latent Topic Models, Proceedings of the l8th European Conference on Machine Learning (ECML2OO7), pp. 6-17, Springer-Verlag.",
        "Andrzejewskl, David and Zhu, Xiaojin and Craven, Mark, 2009.",
        "/ncorporating domain knowledge into topic modeling via Dirichlet Forest priors, Proceedings of the 26th Annual International Conference on Machine Learning.",
        "ICML ~09, pp. 25-32, Montreal, Quebec, Canada.",
        "Andrzejewskl, David and Zhu, Xiaojin, 2009.",
        "Dirichlet A//ocation with Topic-\"in-Set Knowledge Proceedings of NAACL-HLT2OO9 Workshop on Semi-Supervised Learning for Natural Language Processing, pp. 4348.",
        "Hu, Yuening and Boyd-Graber, Jordan and Satinoff, Bri-anna, 2011.",
        "/nteractive topic modeling, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies Volume 1, pp.248-257, Portland, Oregon, USA.",
        "Nobuhiro Kaji and Masaru Kitsuregawa, 2007.",
        "Constrained distributional clustering af words using lexico-syntacticpatterns (in Japanese), SIG-KBS, 79, pp.61-66, 2007-12-03.",
        "Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy, 2010.",
        "Automatic evaluation af topic coherence, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 100-108,Los Angeles, California.",
        "S.Y.Dennis III.",
        "1991.",
        "On the Hyper-Dirichlet Type 1 and Hyper-Liouville distributions., Communications in Statics Theory and Methods 20(12):pp.4069-4081.",
        "Minka, T.P.",
        "1999.",
        "The Dirichlet-tree distribution, (Technical Report) http://research.microsoft.com/en-us/um/people/minka/papers/dlr.ichlet/minka- dirtree.pdf Kristina Toutanova and Mark Johnson.",
        "2008.",
        "A Baysian LDA-based model for semi-supervised part-of-speech tagging.",
        ", In Advances in Neural Information Processing Systems 20, pp.1521-1528, MIT Press.",
        "Thomas L.Griffiths and Mark Steyvers.",
        "2004.",
        "Finding scientijlc topics, Proceedings of the National Academy of Sciences, Vol.1O1,No.Suppl 1. pp.5228-5235.",
        "36"
      ]
    }
  ]
}
