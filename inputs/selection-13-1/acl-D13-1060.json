{
  "info": {
    "authors": [
      "Karl Pichotta",
      "John DeNero"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1060",
    "title": "Identifying Phrasal Verbs Using Many Bilingual Corpora",
    "url": "https://aclweb.org/anthology/D13-1060",
    "year": 2013
  },
  "references": [
    "acl-C10-1124",
    "acl-C96-2141",
    "acl-D07-1090",
    "acl-E06-1042",
    "acl-E06-1043",
    "acl-E09-1086",
    "acl-J90-1003",
    "acl-J93-2003",
    "acl-N06-1014",
    "acl-P10-1116",
    "acl-P99-1041",
    "acl-S13-1039",
    "acl-W01-0513",
    "acl-W02-2001",
    "acl-W03-1808",
    "acl-W03-1809",
    "acl-W03-1810",
    "acl-W03-1812",
    "acl-W06-1203",
    "acl-W06-1204",
    "acl-W06-2405",
    "acl-W09-2903",
    "acl-W09-2907",
    "acl-W10-3707",
    "acl-W10-3708",
    "acl-W11-0805",
    "acl-W11-0815",
    "acl-W11-0823",
    "acl-W97-0311"
  ],
  "sections": [
    {
      "text": [
        "?",
        "ei) using a toy corpus, for e = looking forward to.",
        "Note that the second aligned phrase pair contains the third, so the second's count of 3 must be included in the third's count of 5.",
        "When computing divergence in Equation (2), we use the smoothed distributions P ?e and P ?v(e):",
        "We use ?",
        "= 0.95, which distributes 5% of the total probability mass evenly among all events in D. Morphology.",
        "We calculate statistics for morphological variants of an English phrase.",
        "For a candidate English phrasal verb e (for example, look up), letE denote the set of inflections of that phrasal verb (for look up, this will be [look|looks|looked|looking] up).",
        "We extract the variants in E from the verb entries in English Wiktionary.",
        "The final score computed from a phrase-aligned parallel corpus translating English sentences into a language L is the average KL divergence of smoothed constituent translation distributions for any inflected form ei ?",
        "E:"
      ]
    },
    {
      "heading": "3.2 Monolingual Statistics",
      "text": [
        "We also collect a number of monolingual statistics for each phrasal verb candidate, motivated by the considerable body of previous work on the topic (Church and Hanks, 1990; Lin, 1999; McCarthy et al., 2003).",
        "The monolingual statistics are designed to identify frequent collocations in a language.",
        "This set of monolingual features is not comprehensive, as we focus our attention primarily on bilingual features in this paper.",
        "As above, define E to be the set of morphologically inflected variants of a candidate e, and let V be the set of inflected variants of the head verb v(e) of e. We define three statistics calculated from the phrase counts of a monolingual English corpus.",
        "First, we define ?1(e) to be the relative frequency of the candidate e, given e's head verb, summed over morphological variants:",
        "where N(x) is the number of times phrase x was observed in the monolingual corpus.",
        "Second, define ?2(e) to be the pointwise mutual information (PMI) between V (the event that one of the inflections of the verb in e is observed) and R, the event of observing the rest of the phrase:",
        "where N is the total number of tokens in the corpus, and logarithms are base-2.",
        "This statistic characterizes the degree of association between a verb and its phrasal extension.",
        "We only calculate ?2 for two-word phrases, as it did not prove helpful for longer phrases.",
        "Finally, define ?3(e) to be the relative frequency of the phrasal verb e augmented by an accusative pronoun, conditioned on the verb.",
        "Let A be the set of phrases in E with an accusative pronoun (it, them, him, her, me, you) optionally inserted either at the end of the phrase or directly after the verb.",
        "For e = look up, A = {look up, look X up, look up X, looks up, looks X up, looks up X, .",
        ".",
        ".",
        "}, with X an accusative pronoun.",
        "The ?3 statistic is similar to ?1, but allows for an intervening or following pronoun:",
        "This statistic is designed to exploit the intuition that phrasal verbs frequently have accusative pronouns either inserted into the middle (e.g. look it up) or at the end (e.g. look down on him)."
      ]
    },
    {
      "heading": "3.3 Ranking Phrasal Verb Candidates",
      "text": [
        "Our goal is to assign a single real-valued score to each candidate e, by which we can rank candidates according to semantic idiosyncrasy.",
        "For each language L for which we have a parallel corpus, we defined, in section 3.1, a function ?L(e) assigning real values to candidate phrasal verbs e, which we hypothesize is higher on average for more idiomatic compounds.",
        "Further, in section 3.2, we defined real-valued monolingual functions ?1, ?2, and ?3 for which we hypothesize the same trend holds.",
        "Because each score individually ranks all candidates, it is natural to view each ?L and ?i as a weak ranking function that we can combine with a supervised boosting objective.",
        "We use a modified version of AdaBoost (Freund and Schapire, 1995) that optimizes for recall.",
        "For each ?L and ?i, we compute a ranked list of candidate phrasal verbs, ordered from highest to lowest value.",
        "To simplify learning, we consider only the top 5000 candidate phrasal verbs according to ?1, ?2, and ?3.",
        "This pruning procedure excludes candidates that do not appear in our monolingual corpus.",
        "We optimize the ranker using an unranked, incomplete training set of phrasal verbs.",
        "We can evaluate the quality of the ranker by outputting the top N ranked candidates and measuring recall relative Algorithm 1 Recall-Oriented Ranking AdaBoost",
        "1: for i = 1 : |X |do 2: w[i]?",
        "1/|X| 3: end for 4: for t = 1 : T do 5: for all h ?",
        "H do 6: h ?",
        "0 7: for i = 1 : |X |do 8: if xi 6?",
        "h then 9: h ?",
        "h + w[i] 10: end if 11: end for 12: end for 13: ht ?",
        "argmaxh?H |B ?",
        "h| 14: ?t ?",
        "ln(B/ht) 15: for i = 1 : |X |do 16: if xi ?",
        "ht then 17: w[i]?",
        "1Zw[i] exp (?",
        "?t) 18: else 19: w[i]?",
        "1Zw[i] exp (?t) 20: end if 21: end for 22: end for",
        "to this gold-standard training set.",
        "We choose this recall-at-N metric so as to not directly penalize precision errors, as our training set is incomplete.",
        "DefineH to be the set of N element sets containing the top proposals for each weak ranker (we use N = 2000).",
        "That is, each element ofH is a set containing the 2000 highest values for some ?L or ?i.",
        "We define the baseline error B to be 1?E[R], with R the recall-at-N of a ranker ordering the candidate phrases in the set ?H at random.",
        "The value E[R] is estimated by averaging the recall-at-N of 1000 random orderings of ?H.",
        "Algorithm 1 gives the formulation of the AdaBoost training algorithm that we use to combine weak rankers.",
        "The algorithm maintains a weight vector w (summing to 1) which contains a positive real number for each gold standard phrasal verb in the training set X .",
        "Initially, w is uniformly set to 1/|X|.",
        "At each iteration of the algorithm, w is modified to take higher values for recently misclassified examples.",
        "We repeatedly choose weak rankers ht ?",
        "H (and corresponding real-valued coefficients ?t) that correctly rank examples with high w values.",
        "Lines 5?12 of Algorithm 1 calculate the weighted error values h for every weak ranker set h ?",
        "H. The error h will be 1 if h contains none of X and 0 if h contains all of X , as w always sums to 1.",
        "Line 13 picks the ranker ht ?",
        "H whose weighted error is as far as possible from the random baseline error B .",
        "Line 14 calculates a coefficient ?t for ht, which will be positive if ht < B and negative if ht > B .",
        "Intuitively, ?t encodes the importance of ht?it will be high if ht performs well, and low if it performs poorly.",
        "The Z in lines 17 and 19 is the normalizing constant ensuring the vector w sums to 1.",
        "After termination of Algorithm 1, we have weights ?1, .",
        ".",
        ".",
        ", ?T and lists h1, .",
        ".",
        ".",
        ", hT .",
        "Define ft as the function that generated the list ht (each ft will be some ?L or ?i).",
        "Now, we define a final combined function ?, taking a phrase e and returning a real number:",
        "?tft(e).",
        "We standardize the scores of individual weak rankers to have mean 0 and variance 1, so that their scores are comparable.",
        "The final learned ranker outputs a real value, instead of the class labels frequently found in AdaBoost.",
        "This follows previous work using boosting for learning to rank (Freund et al., 2003; Xu and Li, 2007).",
        "Our algorithm differs from previous methods because we are seeking to optimize for Recall-at-N , rather than a ranking loss."
      ]
    },
    {
      "heading": "4 Experimental Evaluation",
      "text": []
    },
    {
      "heading": "4.1 Training and Test Set",
      "text": [
        "In order to train and evaluate our system, we construct a gold-standard list of phrasal verbs from the freely available English Wiktionary.",
        "We gather phrasal verbs from three sources within Wiktionary:",
        "1.",
        "Entries labeled as English phrasal verbs4, 2.",
        "Entries labeled as English idioms5, and 3.",
        "The derived terms6 of English verb entries.",
        "about across after against along among around at before behind between beyond by down for from in into like off on onto outside over past round through to towards under up upon with within without",
        "verbs gathered from Wiktionary.",
        "Many of the idioms and derived terms are not phrasal verbs (e.g. kick the bucket, make-or-break).",
        "We filter out any phrases not of the form V P+, with V a verb, and P+ denoting one or more occurrences of particles and prepositions from the list in Table 2.",
        "We omit prepositions that do not productively form English phrasal verbs, such as amid and as.",
        "This process also omits some compounds that are sometimes called phrasal verbs, such as light verb constructions, e.g. have a go (Butt, 2003), and noncompositional verb-adverb collocations, e.g. look forward.",
        "There are a number of extant phrasal verb corpora.",
        "For example, McCarthy et al. (2003) present graded human compositionality judgments for 116 phrasal verbs, and Baldwin (2008) presents a large set of candidates produced by an automated system, with false positives manually removed.",
        "We use Wiktionary instead, in an attempt to construct a maximally comprehensive data set that is free from any possible biases introduced by automatic extraction processes."
      ]
    },
    {
      "heading": "4.2 Filtering and Data Partition",
      "text": [
        "The merged list of phrasal verbs extracted from Wiktionary included some common collocations that have compositional semantics (e.g. know about), as well as some very rare constructions (e.g. cheese down).",
        "We removed these spurious results systematically by filtering out very frequent and very infrequent entries.",
        "First, we calculated the log probability of each phrase, according to a language model built from a large monolingual corpus of news documents and web documents, smoothed with stupid back-off (Brants et al., 2007).",
        "We sorted all Wiktionary phrasal verbs according to this value.",
        "Then, we selected the contiguous 75% of the sorted phrases that minimize the variance of this statistic.",
        "This method",
        "and bilingual features (bottom) compared to three baselines (top) gives comparable performance to the human-curated upper bound.",
        "removed a few very frequent phrases and a large number of rare phrases.",
        "The remaining phrases were split randomly into a development set of 694 items and a held-out test set of 695 items."
      ]
    },
    {
      "heading": "4.3 Corpora",
      "text": [
        "Our monolingual English corpus consists of news articles and documents collected from the web.",
        "Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010).",
        "The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et"
      ]
    },
    {
      "heading": "4.4 Generating Candidates",
      "text": [
        "To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small set of second particles T = {with, to, on, }, with the empty string.",
        "The set of candidate phrasal verbs we consider during evaluation is the product V ?P ?T , which contains 96,880 items."
      ]
    },
    {
      "heading": "4.5 Results",
      "text": [
        "We optimize a ranker using the boosting algorithm described in section 3.3, using the features from Table 1, optimizing performance on the Wiktionary development set described in section 4.2.",
        "Monolingual and bilingual statistics are calculated using the corpora described in section 4.3, with candidate phrasal verbs being drawn from the set described in section 4.4.",
        "We evaluate our method of identifying phrasal verbs by computing recall-at-N .",
        "This statistic is the fraction of the Wiktionary test set that appears in the top N proposed phrasal verbs by the method, where N is an arbitrary number of top-ranked candidates held constant when comparing different approaches (we use N = 1220).",
        "We do not compute precision, because the test set to which we compare is not an exhaustive list of phrasal verbs, due to the development/test split, frequency filtering, and omissions in the original lexical resource.",
        "Proposing a phrasal verb not in the test set is not necessarily an error, but identifying many phrasal verbs from the test set is an indication of an effective method.",
        "Recall-at-N is a natural way to evaluate a ranking system where the gold-standard data is an incomplete, unranked set.",
        "Table 3 compares our approach to three baselines using the Recall-at-1220 metric evaluated on both the development and test sets.",
        "As a lower bound, we evaluated the 1220 most frequent candidates in our Monolingual corpus (Frequent Candidates).",
        "As a competitive baseline, we evaluated the set of phrasal verbs in WordNet 3.0 (Fellbaum, 1998).",
        "We selected the most frequent 1220 out of 1781 verb-particle constructions in WordNet (WordNet 3.0 Frequent).",
        "A stronger baseline resulted from applying the same filtering procedure to WordNet that we did to Wiktionary: sorting all verb-particle entries by their language model score and retaining the 1220 consecutive entries that minimized language model variance (WordNet 3.0 Filtered).",
        "WordNet is a human-curated resource, and yet its recall-at-N compared to our Wiktionary test set is only 48.8%, indicating substantial divergence between the two resources.",
        "Such divergence is typical: lexical resources often disagree about what multiword expressions to include (Lin, 1999).",
        "The three final lines in Table 3 evaluate our",
        "bining the k best-performing bilingual statistics and three monolingual statistics.",
        "The dotted line shows the individual performance of the kth best-performing bilingual statistic, when applied in isolation to rank candidates.",
        "boosted ranker.",
        "Automatically detecting phrasal verbs using monolingual features alone strongly outperformed the frequency-based lower bound, but underperformed the WordNet baseline.",
        "Bilingual features, using features from 50 languages, proved substantially more effective.",
        "The combination of both types of features yielded the best performance, outperforming the human-curated WordNet baseline on the development set (on which our ranker was optimized) and approaching its performance on the held-out test set."
      ]
    },
    {
      "heading": "4.6 Feature Analysis",
      "text": [
        "The solid line in Figure 2 shows the recall-at-1220 for a boosted ranker using all monolingual statistics and k bilingual statistics, for increasing k. Bilingual statistics are added according to their individual recall, from best-performing to worst.",
        "That is, the point at k = 0 uses only ?1, ?2, and ?3, the point at k = 1 adds the best individually-performing bilingual statistic (Spanish) as a weak ranker, the next point adds the second-best bilingual statistic (German), etc.",
        "Boosting maximizes performance on the development set, and evaluation is performed on the test set.",
        "We use T = 53 (equal to the total number of weak rankers).",
        "they are useful in addition to the 50 bilingual statistics combined, and no single statistic provides maximal performance.",
        "The dotted line in Figure 2 shows that individual bilingual statistics have recall-at-1220 ranging from 34.4% to 5.0%.",
        "This difference reflects the different sizes of parallel corpora and usefulness of different languages in identifying English semantic idiosyncrasy.",
        "Combining together the signal of multiple languages is clearly beneficial, and including many low-performing languages still offers overall improvements.",
        "Table 4 shows the effect of adding different subsets of the monolingual statistics to the set of all 50 bilingual statistics.",
        "Monolingual statistics give a performance improvement of up to 5.5% recall on the test set, but the comparative behavior of the various combinations of the ?i is somewhat unpredictable when training on the development set and evaluating on the test set.",
        "The pointwise mutual information of a verb and its particles (?2) appears to be the most useful feature.",
        "In fact, the test set performance of using ?2 alone outperforms the combination of all three.",
        "The best combination even outperforms the WordNet 3.0 baseline on the test set, though optimizing on the development set would not select this model."
      ]
    },
    {
      "heading": "4.7 Error Analysis",
      "text": [
        "Table 5 shows the 100 highest ranked phrasal verb candidates by our system that do not appear in either the development or test sets.",
        "Most of these candidates are in fact English phrasal verbs that happened to be missing from Wiktionary; some are present in Wiktionary but were removed from the reference",
        "pick up pat on tap into fit for charge with suit against catch up burst into muck up haul up give up get off get through get up get in tack on buzz about do like plump for haul in keep up with strap on catch up with suck into get round chop off slap on pitch into get into inquire into drop behind get on catch up on pass on cue from carry around get around get over shoot at pick over shoot by shoot in make up to get past cast down set up with rule off hand round piss on hit by break down move for lead off pluck off flip through edge over strike off plug into keep up go past set off pull round see about stay on put up sidle up to buzz around take off set up slap in head towards shoot past inquire for tuck up lie with well before go on with reel from drive along snap off barge into whip on put down instance through bar from cut down on let in tune in to move off suit in lean against well beyond get down to go across sail into lie over hit with chow down on look after catch at",
        "Candidates are presented in decreasing rank; ?pat on?",
        "is the second highest ranked candidate.",
        "sets during filtering, and the remainder are in fact not phrasal verbs (true precision errors).",
        "These errors fall largely into two categories.",
        "Some candidates are compositional, but contain pol-ysemous verbs, such as hit by, drive along, and head towards.",
        "In these cases, prepositions disambiguate the verb, which naturally affects translation distributions.",
        "Other candidates are not phrasal verbs, but instead phrases that tend to have a different syntactic role, such as suit against, instance through, fit for, and lie over (conjugated as lay over).",
        "A careful treatment of part-of-speech tags when computing corpus statistics might address this issue."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "The idea of using word-aligned parallel corpora to identify idiomatic expressions has been pursued in a number of different ways.",
        "Melamed (1997) tests candidate MWEs by collapsing them into single tokens, training a new translation model with these tokens, and using the performance of the new model to judge candidates?",
        "noncomposi-tionality.",
        "Villada Moiro?n and Tiedemann (2006) use word-aligned parallel corpora to identify Dutch MWEs, testing the assumption that the distributions of alignments of MWEs will generally have higher entropies than those of fully compositional compounds.",
        "Caseli et al. (2010) generate candidate multiword expressions by picking out sufficiently common phrases that align to single target-side tokens.",
        "Tsvetkov and Wintner (2012) generate candidate MWEs by finding one-to-one alignments in parallel corpora which are not in a bilingual dictionary, and ranking them based on monolingual statistics.",
        "The system of Salehi and Cook (2013) is perhaps the closest to the current work, judging noncompo-sitionality using string edit distance between a candidate phrase's automatic translation and its components?",
        "individual translations.",
        "Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification.",
        "A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003).",
        "Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework.",
        "There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010;",
        "Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009).",
        "We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005)."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs.",
        "We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task.",
        "We developed a recall-oriented learning method that integrates multiple weak ranking signals, and demonstrated experimentally that combining statistical evidence from a large number of bilingual corpora, as well as from monolingual corpora, produces the most effective system overall.",
        "We look forward to generalizing our approach to other types of noncompositional phrases."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Special thanks to Ivan Sag, who argued for the importance of handling multi-word expressions in natural language processing applications, and who taught the authors about natural language syntax once upon a time.",
        "We would also like to thank the anonymous reviewers for their helpful suggestions."
      ]
    }
  ]
}
