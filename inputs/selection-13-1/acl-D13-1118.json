{
  "info": {
    "authors": [
      "Hiroshi Noji",
      "Daichi Mochihashi",
      "Yusuke Miyao"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1118",
    "title": "Improvements to the Bayesian Topic N-Gram Models",
    "url": "https://aclweb.org/anthology/D13-1118",
    "year": 2013
  },
  "references": [
    "acl-D12-1020",
    "acl-H91-1057",
    "acl-P06-1124",
    "acl-P09-2085",
    "acl-P11-1087",
    "acl-P12-1101"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1180?1190, Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics Improvements to the Bayesian Topic N gram Models"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation.",
        "We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all n-grams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic.",
        "Our blocked sampler can efficiently search for higher probability space even with higher order n-grams.",
        "In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "N gram language model is still ubiquitous in NLP, but due to its simplicity it fails to capture some important aspects of language, such as difference of word usage in different situations, sentence level syntactic correctness, and so on.",
        "Toward language model that can consider such a more global context, many extensions have been proposed from lexical pattern adaptation, e.g., adding cache (Je-linek et al., 1991) or topic information (Gildea and Hofmann, 1999; Wallach, 2006), to grammaticality aware models (Pauls and Klein, 2012).",
        "Topic language models are important for use in e.g., unsupervised language model adaptation: we want a language model that can adapt to the domain or topic of the current situation (e.g., a document in SMT or a conversation in ASR) automatically and select the appropriate words using both topic and syntactic context.",
        "Wallach (2006) is one such model, which generate each word based on local context and global topic information to capture the difference of lexical usage among different topics.",
        "However, Wallach's experiments were limited to bigrams, a toy setting for language models, and experiments with higher-order n-grams have not yet been sufficiently studied, which we investigate in this paper.",
        "In particular, we point out the two fundamental problems caused when extending Wallach's model to a higher-order: sparseness caused by dividing all n-grams into exclusive topics, and local minima caused by the deep hierarchy of the model.",
        "On resolving these problems, we make several contributions to both computational linguistics and machine learning.",
        "To address the first problem, we investigate incorporating a global language model for ease of sparseness, along with some priors on a suffix tree to capture the difference of topicality for each context, which include an unsupervised extension of the doubly hierarchical Pitman-Yor language model (Wood and Teh, 2009), a Bayesian generative model for supervised language model adaptation.",
        "For the second inference problem, we develop a novel blocked Gibbs sampler.",
        "When the number of topics is K and vocabulary size is V , n-gram topic model has O(KV n) parameters, which grow exponentially to n, making the local minima problem even more severe.",
        "Our sampler resolves this problem by moving many customers in the hierarchical Chinese restaurant process at a time.",
        "We evaluate various models by incremental calculation of test document perplexity on 3 types of corpora having different size and diversity.",
        "By combining the proposed prior and the sampling method, our Bayesian model achieve much higher accuracies than the naive extension of Wallach (2006) and shows results competitive with the unigram rescaling (Gildea and Hofmann, 1999), which require",
        "huge computational cost at prediction, with much faster prediction time."
      ]
    },
    {
      "heading": "2 Basic Models",
      "text": [
        "All models presented in this paper are based on the Bayesian n-gram language model, the hierarchical Pitman-Yor process language model (HPYLM).",
        "In the following, we first introduce the HPYLM, and then discuss the topic model extension of Wallach (2006) with HPYLM."
      ]
    },
    {
      "heading": "2.1 HPYLM",
      "text": [
        "Let us first define some notations.",
        "W is a vocabulary set, V = |W |is the size of that set, and u, v, w ?W represent the word type.",
        "The HPYLM is a Bayesian treatment of the n-gram language model.",
        "The generative story starts with the unigram word distribution G?, which is a V dimensional multinomial where G?",
        "(w) represents the probability of word w. The model first generates this distribution from the PYP as G?",
        "?",
        "PYP(a, b,G0), where G0 is a V dimensional uniform distribution (G0(u) = 1V ;?u ?",
        "W ) and acts as a prior for G?",
        "and a, b are hyperparameters called discount and concentration, respectively.",
        "It then generates all bigram distributions {Gu}u?W as Gu ?",
        "PYP(a, b,G?).",
        "Given this distributions, it successively generates 3-gram distributions Guv ?",
        "PYP(a, b,Gu) for all (u, v) ?",
        "W 2 pairs, which encode a natural assumption that contexts having common suffix have similar word distributions.",
        "For example, two contexts ?he is?",
        "and ?she is?, which share the suffix ?is?, are generated from the same (bigram) distribution Gis, so they would have similar word distributions.",
        "This process continues until the context length reaches n ?",
        "1 where n is a pre-specified n-gram order (if n = 3, the above example is a complete process).",
        "We often generalize this process using two contexts h and h?",
        "as Gh ?",
        "PYP(a, b,Gh?",
        "), (1) where h = ah?, in which a is a leftmost word of h. We are interested in the posterior word distribution following a context h. Our training corpus w is a collection of n-grams, from which we can calculate the posterior p(w|h,w), which is often explained with the Chinese restaurant process (CRP):",
        "(2) where chw is an observed count of n-gram hw called customers, while thw is a hidden variable called tables.",
        "ch?",
        "and th?",
        "represents marginal counts: ch?",
        "=?",
        "w chw and th?",
        "= ?",
        "w thw.",
        "This form is very similar to the well-known Kneser-Ney smoothing, and actually the Kneser-Ney can be understood as a heuristic approximation of the HPYLM.",
        "This characteristic enables us to build the state-of-the-art language model into a more complex generative model."
      ]
    },
    {
      "heading": "2.2 Wallach (2006) with HPYLM",
      "text": [
        "Wallach (2006) is a generative model for a document collection that combines the topic model with a Bayesian n-gram language model.",
        "The latent Dirichlet alocation (LDA) (Blei et al., 2003) is the most basic topic model, which generates each word in a document based on a unigram word distribution defined by a topic allocated to that word.",
        "The bigram topic model of Wallach (2006) simply replaces this unigram word distribution (a multinomial) for each topic with a bigram word distribution 1.",
        "In other words, ordinary LDA generates word conditioning only on the latent topic, whereas the bigram topic model generates conditioning on both the latent topic and the previous word, as in the bigram language model.",
        "Extending this model with a higher order n-gram is trivial; all we have to do is to replace the bigram language model for each topic with an n-gram language model.",
        "The formal description of the generative story of this n-gram topic model is as follows.",
        "First, for each topic k ?",
        "1, ?",
        "?",
        "?",
        ",K, where K is the number of topics, the model generates an n-gram language model Gkh.",
        "These n-gram models are generated by the PYP, so Gkh ?",
        "PYP(a, b,Gkh?)",
        "holds.",
        "The model then generate a document collection.",
        "For each document j ?",
        "1, ?",
        "?",
        "?",
        ", D, it generates a K-1This is the model called prior 2 in Wallach (2006); it consistently outperformed the other prior.",
        "Wallach used the Dirichlet language model as each topic, but we only explore the model with HPYLM because its superiority to the Dirichlet language model has been well studied (Teh, 2006b).",
        "2We sometimes denote Gkh to represent a language model of topic k, not a specific multinomial for some context h, depending on the context.",
        "dimensional topic distribution ?j by a Dirichlet distribution Dir(?)",
        "where ?",
        "= (?1, ?2, ?",
        "?",
        "?",
        ", ?K) is a prior.",
        "Finally, for each word position i ?",
        "1, ?",
        "?",
        "?",
        ", Nj where Nj is the number of words in document j, i-th word's topic assignment zji is chosen according to ?j , then a word type wji is generated from Gzjihji where hji is the last n?",
        "1 words preceding wji.",
        "We can summarize this process as follows:",
        "1.",
        "Generate topics:"
      ]
    },
    {
      "heading": "3 Extended Models",
      "text": [
        "One serious drawback of the n-gram topic model presented in the previous section is sparseness.",
        "At inference, as in LDA, we assign each n-gram a topic, resulting in an exclusive clustering of n-grams in the corpora.",
        "Roughly speaking, when the number of topics is K and the number of all n-grams in the training corpus is N , a language model of topic k, Gkh is learned using only about O(N/K) instances of the n-grams assigned the topic k, making each Gkh much sparser and unreliable distribution.",
        "One way to alleviate this problem is to place another n-gram model, say G0h, which is shared with all topic-specific n-gram models {Gkh}Kk=1.",
        "However, what is the best way to use this special distribution?",
        "We explore two different approaches to incorporate this distribution in the model presented in the previous section.",
        "In one model, the HIERARCHICAL model, G0h is used as a prior for all other n-gram models, where G0h exploits global statistics across all topics {Gkh}.",
        "In the other model, the SWITCHING model, no statistics are shared across G0h and {Gkh}, but some words are directly generated from G0h regardless of the topic distribution."
      ]
    },
    {
      "heading": "3.1 HIERARCHICAL Model",
      "text": [
        "Informally, what we want to do is to establish hierarchies among the global G0h and other topics {Gkh}.",
        "In Bayesian formalism, we can explain this using an",
        "model.",
        "{u, v} are word types, k is a topic and each Gkh is a multinomial word distribution.",
        "For example, G2uv represents a word distribution following the context uv in topic 2. abstract distribution F as Gkh ?",
        "F(G0h).",
        "The problem here is making the appropriate choice for the distribution F .",
        "Each topic word distribution already has hierarchies among n?",
        "1-gram and n-gram contexts as Gkh ?",
        "PYP(a, b,Gkh?).",
        "A natural solution to this problem is the doubly hierarchical Pitman-Yor process (DHPYP) proposed in Wood and Teh (2009).",
        "Using this distribution, the new generative process of Gkh is Gkh ?",
        "PYP(a, b, ?Gkh?",
        "+ (1?",
        "?",
        ")G0h), (3) where ?",
        "is a new hyperparameter that determines mixture weight.",
        "The dependencies among G0h and {Gkh} are shown in Figure 1.",
        "Note that the generative process of G0h is the same as the HPYLM (1).",
        "Let us clarify the DHPYP usage differences between our model and the previous work of Wood and Teh (2009).",
        "A key difference is the problem setting: Wood and Teh (2009) is aimed at the supervised adaptation of a language model for a specific domain, whereas our goal is unsupervised adaptation.",
        "In Wood and Teh (2009), each Gkh for k ?",
        "1, 2, ?",
        "?",
        "?",
        "corresponds to a language model of a specific domain and the training corpus for each k is pre-specified and fixed.",
        "For ease of data sparseness of domain-specific corpora, latent model G0h exploits shared statistics amongGkh for k = 1, 2, ?",
        "?",
        "?",
        ".",
        "In contrast, with our model, each Gkh is a topic, so it must perform the clustering of n-grams in addition to ex",
        "ploiting the latent G0h.",
        "This makes inference harder and requires more careful design of ?.",
        "Modeling of ?",
        "We can better understand the role of ?",
        "in (3) by considering the posterior predictive form corresponds to (2), which is written as",
        "where c, t with superscript k corresponds to the count existing in topic k. This shows us that ?",
        "determines the back-off behavior: which probability we should take into account: the shorter context of the same topic Gkh?",
        "or the full context of the global model G0h.",
        "Wood and Teh (2009) shares this variable across all contexts of the same length, for each k, but this assumption may not be the best.",
        "For example, after the context ?in order?, we can predict the word ?to?",
        "or ?that?, and this tendency is unaffected by the topic.",
        "We call this property of context the topicality and say that ?in order?",
        "has weak topicality.",
        "Therefore, we place ?",
        "as a distinct value for each context h, which we share across all topics.",
        "We designate this ?",
        "determined by h ?h in the following.",
        "Moreover, similar contexts may have similar values of ?h.",
        "For example, the two contexts ?of the?",
        "and ?in the?, which share the suffix ?the?, both have a strong topicality3.",
        "We encode this assumption by placing hierarchical Beta distributions on the suffix tree across all topics: ?h ?",
        "Beta(??h?",
        ", ?(1?",
        "?h?))",
        "= DP(?, ?h?",
        "), (5) where DP is the hierarchical Dirichlet process (Teh et al., 2006), which has only two atoms in {0,1} and ?",
        "is a concentration parameter.",
        "As in HPYLM, we place a uniform prior ?0 = 1/2 on the base distribution of the top node (??",
        "?",
        "DP(?, ?0)).",
        "Having generated the topic component of the model, the corpus generating process is the same as the previous model because we only change the generating process of Gkh for k = 1, ?",
        "?",
        "?",
        ",K. 3These words can be used very differently depending on the context.",
        "For example, in a teen story, ?in the room?",
        "or ?in the school?",
        "seems more dominant than ?in the corpora?",
        "or ?in the topic?, which is likely to appear in this paper."
      ]
    },
    {
      "heading": "3.2 SWITCHING Model",
      "text": [
        "Our second extension also exploits the globalG0h, albeit differently than the HIERARCHICAL model.",
        "In this model, the relationship of G0h to the other {Gkh} is flat, not hierarchical: G0h is a special topic that can generate a word.",
        "The model first generates each language model of k = 0, 1, 2, ?",
        "?",
        "?",
        ",K independently as Gkh ?",
        "PYP(a, b,Gkh?).",
        "When generating a word, it first determines whether to use global model G0h or topic model {Gkh}Kk=1.",
        "Here, we use the ?h introduced above in a similar way: the probability of selecting k = 0 for the next word is determined by the previous context.",
        "This assumption seems natural; we expect theG0h to mainly generate common n-grams, and the topicality of each context determines how common that n-gram might be.",
        "The complete generative process of this model is written as follows:",
        "1.",
        "Generate topics:",
        "The difference between the two models is their usage of the global model G0h.",
        "For a better understanding of this, we provide a comparison of their graphical models in Figure 2."
      ]
    },
    {
      "heading": "4 Inference",
      "text": [
        "For posterior inference, we use the collapsed Gibbs sampler.",
        "In our models, all the latent variables are {Gkh, ?h, ?j , z,?",
        "}, where z is the set of topic assignments and ?",
        "= {a, b, ?,?}",
        "are hyperparameters, which are treated later.",
        "We collapse all multinomials in the model, i.e., {Gkh, ?h, ?j}, in which Gkh and ?h are replaced with the Chinese restaurant process of PYP and DP respectively.",
        "Given the training corpus w, the target posterior distribution is p(z,S|w,?",
        "), where S is the set of seating arrangements of all restaurants.",
        "To distinguish the two types of restaurant, in the following, we refer the restaurant to indi",
        "cate the collapsed state of Gkh (PYP), while we refer the restaurant of ?h to indicates the collapsed state of ?h (DP).",
        "We present two different types of sampler: a token-based sampler and a table-based sampler.",
        "For both samplers, we first explain in the case of our basic model (Section 2.2), and later discuss some notes on our extended models."
      ]
    },
    {
      "heading": "4.1 Token-based Sampler",
      "text": [
        "The token-based sampler is almost identical to the collapsed sampler of the LDA (Griffiths and Steyvers, 2004).",
        "At each iteration, we consider the following conditional distribution of zji given all other topic assignments z?ji and S?ji, which is the set of seating arrangements with a customer corresponds to wji removed, as",
        "where n?jijk is the number of words that is assigned topic k in document j excluding wji, which is the same as the LDA.",
        "Given the sampled topic zji, we update the language model of topic zji, by adding customer wji to the restaurant specified by zji and context hji.",
        "See Teh (2006a) for details of these customer operations.",
        "HIERARCHICAL Adding customer operation is slightly changed: When a new table is added to a restaurant, we must track the label l ?",
        "{0, 1} indicating the parent restaurant of that table, and add the customer corresponding to l to the restaurant of ?h.",
        "See Wood and Teh (2009) for details of this operation.",
        "where p(lji|hji) is a predictive of lji given by the CRP of ?hji .",
        "We need not assign a label to a new table, but rather we always add a customer to the restaurant of ?h according to whether the sampled topic is 0 or not."
      ]
    },
    {
      "heading": "4.2 Table-based Sampler",
      "text": [
        "One problem with the token-based sampler is that the seating arrangement of the internal restaurant would never be changed unless a new table is created (or an old table is removed) in its child restaurant.",
        "This probability is very low, particularly in the restaurants of shallow depth (e.g., unigram or",
        "table-based sampler when the number of topics is 2.",
        "{u, v, w} are word types.",
        "Each box represents a restaurant where the type in the upper-right corner indicates the context.",
        "In this case, we can change the topic of the three 3-grams (vvw, vvw, uvw) in some documents from 1 to 2 at the same time.",
        "bigram restaurants) because these restaurants have a larger number of customers and tables than those of deep depth, leading to get stack in undesirable local minima.",
        "For example, imagine a table in the restaurant of context ?hidden?",
        "(depth is 2) and some topic, served ?unit?.",
        "This table is connected to tables in its child restaurants corresponding to some 3-grams (e.g., ?of hidden unit?",
        "or ?train hidden unit?",
        "), whereas similar n-grams, such as those of ?of hidden units?",
        "or ?train hidden units?",
        "might be gathered in another topic, but collecting these n-grams into the same topic might be difficult under the token-based sampler.",
        "The table-based sampler moves those different n-grams having common suffixes jointly into another topic.",
        "Figure 3 shows a transition of state by the table-based sampler and Algorithm 4.2 depicts a high-level description of one iteration.",
        "First, we select a table in a restaurant, which is shown with a dotted line in the figure.",
        "Next, we descend the tree to collect the tables connected to the selected table, which are pointed by arrows.",
        "Because this connection cannot be preserved in common data structures for a restaurant described in Teh (2006a) or Blunsom et al.",
        "(2009), we select the child tables randomly.",
        "This is correct because customers in CRP are exchange-Algorithm 1 Table-based sampler for all table in all restaurants do Remove a customer from the parent restaurant.",
        "Construct a block of seating arrangement S by descending the tree recursively.",
        "Sample topic assignment zS ?",
        "p(zS |S,S?S , z?S).",
        "Move S to sampled topic, and add a customer to the parent restaurant of the first selected table.",
        "end for able, so we can restore the parent-child relations arbitrarily.",
        "We continue this process recursively until reaching the leaf nodes, obtaining a block of seating arrangement S. After calculating the conditional distribution, we sample new topic assignment for this block.",
        "Finally, we move this block to the sampled topic, which potentially changes the topic of many words across different documents, which are connected to customers in a block at leaf nodes (this connection is also arbitrary).",
        "Conditional distribution Let zS be the block of topic assignments connected to S and zS be a variable indicating the topic assignment.",
        "Thanks to the exchangeability of all customers and tables in one restaurant (Teh, 2006a), we can imagine that customers and tables in S have been added to the restaurants last.",
        "We are interested in the following conditional distribution: (conditioning ?",
        "is omitted)",
        "where p(S|S?S , k?)",
        "is a product of customers?",
        "actions moving to another topic, which can be decomposed as:",
        "Let us define some notations used above.",
        "Each s ?",
        "S is a part of seating arrangements in a restaurant, there being ts tables, i-th of which with csi customers, with hs as the corresponding context.",
        "A restaurant of context h and topic k has tkhw tables served dish w, i-th of which with ckhwi customers.",
        "Superscripts 's indicate excluding the contribution",
        "of customers in s, and xn = x(x+1) ?",
        "?",
        "?",
        "(x+n?1) is the ascending factorial.",
        "In (10) p(w|k?, h) is the parent distribution of the first selected table, and the other p(s|k?)",
        "is the seating arrangement of customers.",
        "The likelihood for changing topic assignments across documents must also be considered,",
        "where nj(S) is the number of word tokens connected with S in document j. HIERARCHICAL We skip tables on restaurants of k = 0, because these tables are all from other topics and we cannot construct a block.",
        "The effects of ?",
        "can be ignored because these are shared by all topics.",
        "SWITCHING In the SWITCHING, p(zS = k?|z?S) cannot be calculated in a closed form because p(lji|hji) in (9) would be changed dynamically when adding customers.",
        "This problem is the same one addressed by Blunsom and Cohn (2011), and we follow the same approximation in which, when we calculate the probability, we fractionally add tables and customers recursively."
      ]
    },
    {
      "heading": "4.3 Inference of Hyperparameters",
      "text": [
        "We also place a prior on each hyperparameter and sample value from the posterior distribution for every iteration.",
        "As in Teh (2006a), we set different values of a and b for each depth of PYP, but share across all topics and sample values with an auxiliary variable method.",
        "We also set different value of ?",
        "for each depth, on which we place Gamma(1, 1).",
        "We make the topic prior ?",
        "asymmetric: ?",
        "= ??0;?",
        "?",
        "Gamma(1, 1),?0 ?",
        "Dir(1)."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "HMM-LDA (Griffiths et al., 2005) is a composite model of HMM and LDA that assumes the words in a document are generated by HMM, where only one state has a document-specific topic distribution.",
        "Our SWITCHING model can be understood as a lexical extension of HMM-LDA.",
        "It models the topicality by context-specific binary random variables, not by hidden states.",
        "Other n-gram topic models have focused mainly on information retrieval.",
        "Wang et min.",
        "training set test set",
        "replace words appearing less than min.appear times in training + test documents, or appearing only in a test set with an unknown token.",
        "All numbers are replaced with #, while punctuations are remained.",
        "al.",
        "(2007) is a topic model on automatically segmented chunks.",
        "Lindsey et al. (2012) extended this model with the hierarchical Pitman-Yor prior.",
        "They also used switching variables, but for a different purpose: to determine the segmenting points.",
        "They treat these variables completely independently, while our model employs a hierarchical prior to share statistical strength among similar contexts.",
        "Our primary interest is language model adaptation, which has been studied mainly in the area of speech processing.",
        "Conventionally, this adaptation has relied on a heuristic combination of two separately trained models: an n-gram model p(w|h) and a topic model p(w|d).",
        "The unigram rescaling, which is a product model of these two models, perform better than more simpler models such as linear interpolation (Gildea and Hofmann, 1999).",
        "There are also some extensions to this method (Tam and Schultz, 2009; Huang and Renals, 2008), but these methods have one major drawback: at prediction, the rescaling-based method requires normalization across vocabulary at each word, which prohibits use on applications requiring dynamic (incremental) adaptation, e.g., settings where we have to update the topic distribution as new inputs come in.",
        "Tam and Schultz (2005) studied on this incremental settings, but they employ an interpolation.",
        "The practical interest here is whether our Bayesian models can rival the rescaling-based method in terms of prediction power.",
        "We evaluate this in the next section."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": []
    },
    {
      "heading": "6.1 Settings",
      "text": [
        "We test the effectiveness of presented models and the blocked sampling method on unsupervised language model adaptation settings.",
        "Specifically we",
        "concentrate on the dynamic adaptation: We update the posterior of language model given previously observed contexts, which might be decoded transcripts at that point in ASR or MT.",
        "We use three corpora: the Brown, BNC and NIPS.",
        "The Brown and BNC are balanced corpora that consist of documents of several genres from news to romance.",
        "The Brown corpus comprises 15 categories.",
        "We selected two documents from each category for the test set, and use other 470 documents for the training set.",
        "For the NIPS, we randomly select 1,500 papers for training and 50 papers for testing.",
        "For BNC, we first randomly selected 400 documents from a written corpus and then split each document into smaller documents every 100 sentences, leading to 6,262 documents, from which we randomly selected 100 documents for testing, and other are used for training.",
        "See Table 1 for the preprocessing of unknown types and the resulting corpus statistics.",
        "For comparison, besides our proposed HIERARCHICAL and SWITCHING models, we prepare various models for baseline.",
        "HPYLM is a n-gram language model without any topics.",
        "We call the model without the global G0h introduced in Section 2.2 HPYTM.",
        "To see the effect of the table-based sampler, we also prepare HPYTMtoken, which is trained only on the token-based sampler.",
        "RESCALING is the unigram rescaling.",
        "This is a product model of an n-gram model p(w|h) and a topic model p(w|d), where we learn each model separately and then combine them by:",
        "We set ?",
        "in (14) to 0.7, which we tuned with the Brown corpus."
      ]
    },
    {
      "heading": "6.2 Effects of Table-based Sampler",
      "text": [
        "We first evaluate the effects of our blocked sampler at training.",
        "For simplicity, we concentrate on the HPYTM with K = 50.",
        "Table 4(a)?",
        "(c) shows negative likelihoods of the model during training.",
        "On all corpora, the model with the table-based sampler reached the higher probability space with much faster speed on both 3-gram and 4-gram models."
      ]
    },
    {
      "heading": "6.3 Perplexity Results",
      "text": [
        "Training For burn-in, we ran the sampler as follows: For HPYLM, we ran 100 Gibbs iterations.",
        "For RESCALING, we ran 900 iterations on LDA and 100 iterations on HPYLM.",
        "For all other models, we ran 500 iterations of the Gibbs; HPYTMtoken is trained only on the token-based sampler, while for other models, the table-based sampler is performed after the token-based sampler.",
        "Evaluation We have to adapt to the topic distribution of unseen documents incrementally.",
        "Although previous works have employed incremental EM (Gildea and Hofmann, 1999; Tam and Schultz, 2005) because their inference is EM/VB-based, we use the left-to-right method (Wallach et al., 2009), which is a kind of particle filter updating the posterior topic distribution of a test document.",
        "We set the number of particles to 10 and resampled each particle every 10 words for all experiments.",
        "To get the final perplexity, after burn-in, we sampled 10 samples every 10 iterations of Gibbs, calculated a test perplexity for each sample, and averaged the results.",
        "Comparison of 3-grams Figure 4(d)?",
        "(f) shows perplexities when varying the number of topics.",
        "Generally, compared to the HPYTMtoken, the HPYTM got much perplexity gains, which again confirm the effectiveness of our blocked sampler.",
        "Both our proposed models, the HIERARCHICAL and the SWITCHING, got better performances than the HPYTM, which does not place the global model G0h.",
        "Our SWITCHING model consistently performed the best.",
        "The HIERARCHICAL performed somewhat worse than the RESCALING when K become large, but the SWITCHING outperformed that.",
        "Comparison of 4-grams and beyond We summarize the results with higher order n-grams in Table 2, where we also show the time for prediction.",
        "We fixed the number of topics K = 100 because we saw that all models but HPYTMtoken performed best at K = 100 when n = 3.",
        "Generally, the results are consistent with those of n = 3.",
        "The models with n = ?",
        "indicate a model extension using the Bayesian variable-order language model (Mochihashi and Sumita, 2008), which can naturally be integrated with our generative models.",
        "By this extension, we can prune unnecessary nodes stochas"
      ]
    },
    {
      "heading": "NIPS BNC",
      "text": [
        "for prediction (in seconds).",
        "The number of topics is fixed to 100 on all topic-based models.",
        "tically during training.",
        "We can see that this ?- gram did not hurt performances, but the sampled model get much more compact; in BNC, the number of nodes of the SWITCHING with 4-gram is about 7.9M, while the one with ?-gram is about 3.9M.",
        "Note that our models require no explicit normalization, thereby drastically reducing the time for prediction compared to the RESCALING.",
        "This difference is especially remarkable when the vocabulary size becomes large.",
        "We can see that our SWITCHING performed consistently better than the HIERARCHICAL.",
        "One reason for this result might be the mismatch of prediction of the topic distribution in the HIERARCHICAL.",
        "The HIERARCHICAL must allocate some (not global) topics to every word in a document, so even the words to which the SWITCHING might allocate the global topic (mainly function words; see below) must be allocated to some other topics, causing a mismatch of allocations of topic."
      ]
    },
    {
      "heading": "6.4 Qualitative Results",
      "text": [
        "To observe the behavior in which the SWITCHING allocates some words to the global topic, in Figure 5, we show the posterior of allocating the topic 0 or not at each word in a part of the NIPS training corpus.",
        "We can see that the model elegantly identified content and function words, learning the topic distribution appropriately using only semantic contexts.",
        "These same results in the HIERARCHICAL are presented in Table 3, where we show some relations between ?h and context h. Contexts that might be likely to precede nouns have a higher value of ?h,",
        "there has been much recent work on measuring image statistics and on learning probability distributions on images .",
        "we observe that the mapping from images to statistics is many-to-one and show it can be quantified by a phase space factor .",
        "NIPS by the ?-gram SWITCHING.",
        "Darker words indicate a higher probability of not being assigned topic 0.",
        "?h h 0.0?0.1 in spite, were unable, a sort, on behalf, .",
        "regardless 0.5?0.6 assumed it, rand mines, plans was, other excersises 0.9?1.0 that the, the existing, the new, their own, and spatial",
        "duced by the 3-gram HIERARCHICAL in BNC.",
        "while prefixes of idioms have a lower value.",
        "The?-gram extension gives us the posterior of n-gram order p(n|h), which can be used to calculate the probability of a word ordering composing a phrase in topic k as p(w, n|k, h) ?",
        "p(n|h)p(w|k, n, h).",
        "In Table 4, we show some higher probability topic-specific phrases from the model trained on the NIPS."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We have presented modeling and algorithmic contributions to the existing Bayesian n-gram topic model.",
        "We explored two different priors to incorporate a global model, and found the effectiveness of the flat structured model.",
        "We developed a novel blocked Gibbs move for these types of models to accelerate inference.",
        "We believe that this Gibbs operation can be incorporated with other models having a similar hierarchical structure.",
        "Empirically, we demonstrate that by a careful model design and efficient inference, a well-defined Bayesian model can rival the conventional heuristics."
      ]
    }
  ]
}
