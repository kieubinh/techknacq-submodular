{
  "info": {
    "authors": [
      "Jason Weston",
      "Antoine Bordes",
      "Oksana Yakhnenko",
      "Nicolas Usunier"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1136",
    "title": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction",
    "url": "https://aclweb.org/anthology/D13-1136",
    "year": 2013
  },
  "references": [
    "acl-D12-1042",
    "acl-D12-1093",
    "acl-N13-1008",
    "acl-P05-1045",
    "acl-P09-1011",
    "acl-P09-1113",
    "acl-P10-1013",
    "acl-P11-1055"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge.",
        "Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base.",
        "We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Information extraction (IE) aims at generating structured data from free text in order to populate Knowledge Bases (KBs).",
        "Hence, one is given an incomplete KB composed of a set of triples of the form (h, r , t); h is the left-hand side entity (or head), t the right-hand side entity (or tail) and r the relationship linking them.",
        "An example from the Freebase KB1 is (/m/2d3rf ,<director-of>, /m/3/324), where /m/2d3rf refers to the director ?Alfred Hitchcock\" and /m/3/324 to the movie ?The Birds\".",
        "This paper focuses on the problem of learning to perform relation extraction (RE) under weak supervision from a KB.",
        "RE is subtask of IE that considers that entities have already been detected by a different process, such as a named-entity recognizer.",
        "RE then aims at assigning to a relation mention m",
        "(i.e. a sequence of text which states that some relation is true) the corresponding relationship from the KB, given a pair of extracted entities (h, t) as context.",
        "For example, given the triple (/m/2d3rf ,?wrote and directed\", /m/3/324), a system should predict <director-of>.",
        "The task is said to be weakly supervised because for each pair of entities (h, t) detected in the text, all relation mentions m associated with them are labeled with all the relationships connecting h and t in the KB, whether they are actually expressed by m or not.",
        "Our key contribution is a novel model that employs not only weakly labeled text mention data, as most approaches do, but also leverages triples from the known KB.",
        "The model thus learns the plausibility of new (h, r , t) triples by generalizing from the KB, even though this triple is not present.",
        "A ranking-based embedding framework is used to train our model.",
        "Thereby, relation mentions, entities and relationships are all embedded into a common low-dimensional vector space, where scores are computed.",
        "We show that our method can successfully take into account information from a large-scale KB (Freebase: 4M entities, 23k relationships) to improve over systems that are only using text features.",
        "This paper is organized as follows: Section 2 presents related work, Section 3 introduces our model and its main influences, and experimental results are displayed in Section 4."
      ]
    },
    {
      "heading": "2 Previous Work",
      "text": [
        "Learning under weak supervision is common in natural language processing, especially for tasks where the annotation costs are significant such as in se",
        "mantic parsing (Kate and Mooney, 2007; Liang et al., 2009; Bordes et al., 2010; Matuszek et al., 2012).",
        "This is also naturally used in IE, since it allows to train large-scale systems without requiring to label numerous texts.",
        "The idea was introduced by (Craven et al., 1999), which matched the Yeast Protein Database with PubMed abstracts.",
        "It was also used to train open extractors based on Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010).",
        "Large-scale open IE projects (e.g. (Banko et al., 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it.",
        "Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multi-instance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).",
        "All these works only use textual information to perform extraction.",
        "Lao et al. (2012) proposed the first work aiming to perform RE employing both KB data and text, using a rule-based random walk method.",
        "Recently, Riedel et al. (2013) proposed another joint approach based on collaborative filtering for learning entity embeddings.",
        "This approach connects text with Freebase by learning shared embeddings of entities through weak supervision, in contrast to our method where no joint learning is performed.",
        "We do not compare to these two approaches since they use two different evaluation protocols that greatly differ from those used in all aforementioned previous works.",
        "Nevertheless, our method is easier to integrate into existing systems than those, since KB data is used via the addition of a scoring term, which is trained separately beforehand (with no shared embeddings).",
        "Besides, we demonstrate in our experimental section that our system can handle a large number of relationships, significantly larger than that presented in (Lao et al., 2012; Riedel et al., 2013)."
      ]
    },
    {
      "heading": "3 Embedding-based Framework",
      "text": [
        "Our work concerns energy-based methods, which learn low-dimensional vector representations (embeddings) of atomic symbols (words, entities, relationships, etc.).",
        "In this framework, we learn two models: one for predicting relationships given relation mentions and another one to encode the interactions among entities and relationships from the KB.",
        "The joint action of both models in prediction allows us to use the connection between the KB and text to perform relation extraction.",
        "One could also share parameters between models (via shared embeddings), but this is not implemented in this work.",
        "This approach is inspired by previous work designed to connect words and Wordnet (Bordes et al., 2012).",
        "Both submodels end up learning vector embeddings of symbols, either for entities or relationships in the KB, or for each word/feature of the vocabulary (denoted V).",
        "The set of entities and relationships in the KB are denoted by E and R, and nv , ne and nr denote the size of V , E and R respectively.",
        "Given a triple (h, r , t) the embeddings of the entities and the relationship (vectors in Rk ) are denoted with the same letter, in boldface characters (i.e. h, r, t)."
      ]
    },
    {
      "heading": "3.1 Connecting Text and Relationships",
      "text": [
        "The first part of the framework concerns the learning of a function Sm2r (m, r), based on embeddings, that is designed to score the similarity of a relation mention m and a relationship r .",
        "Our scoring approach is inspired by previous work for connecting word labels and images (Weston et al., 2010), which we adapted, replacing images by mentions and word labels by relationships.",
        "Intuitively, it consists of first projecting words and features into the embedding space and then computing a similarity measure (the dot product in this paper) between this projection and a relationship embedding.",
        "The scoring function is then:",
        "with f a function mapping words and features into Rk , f(m) = W>?(m).",
        "W is the matrix of Rnv?k containing all word embeddings w, ?",
        "(m) is the (sparse) binary representation of m (?",
        "Rnv ) indicating absence or presence of words/features, and r ?",
        "Rk is the embedding of the relationship r .",
        "This approach can be easily applied at test time to score (mention, relationship) pairs.",
        "Since this type of learning problem is weakly supervised, Bordes et al.",
        "(2010) showed that a convenient way to train it is by using a ranking loss.",
        "Hence, given a data set D = {(mi , ri ), i = 1, ... , |D|} consisting of (mention, relationship) training pairs, one could learn the",
        "embeddings using constraints of the form: ?i , ?r ?",
        "6= ri , f(mi )>ri > 1 + f(mi )>r?",
        ", (1) where 1 is the margin.",
        "That is, we want the relation that (weakly) labels a given mention to be scored higher than other relation by a margin of 1.",
        "Then, given any mention m one can predict the corresponding relationship r?",
        "(m) with:",
        "Learning Sm2r (?)",
        "under constraints (1) is well suited when one is interested in building a per-mention prediction system.",
        "However, performance metrics of relation extraction are sometimes measured using precision recall curves aggregated for all mentions concerning the same pair of entities, as in (Riedel et al., 2010).",
        "In that case the scores across predictions for different mentions need to be calibrated so that the most confident ones have the higher scores.",
        "This can be better encoded with constraints of the following form: ?i , j , ?r ?",
        "6= ri , rj , f(mi )>ri > 1 + f(mj)>r?",
        ".",
        "In this setup, scores of pairs observed in the training set should be larger than that of any other prediction across all mentions.",
        "In practice, we use ?soft?",
        "ranking constraints (optimizing the hinge loss), i.e. we minimize: ?i , j , ?r ?",
        "6= ri , rj , max(0, 1?f(mi )>ri +f(mj)>r?).",
        "Finally, we also enforce a (hard) constraint on the norms of the columns of W and r, i.e. ?i , ||Wi ||2 ?",
        "1 and ?j , ||rj ||2 ?",
        "1.",
        "Training is carried out by Stochastic Gradient Descent (SGD), updating W and r at each step, following (Weston et al., 2010; Bordes et al., 2013).",
        "That is, at the start of training the parameters to be learnt (the nv ?",
        "k word/feature embeddings in W and the nr ?",
        "k relation embeddings r ) are initialized to random weights.",
        "We initialize each k-dimensional embedding vector randomly with mean 0, standard deviation 1k .",
        "Then, we iterate the following steps to train them:",
        "1.",
        "Select at random a positive training pair (mi , ri ).",
        "2.",
        "Select at random a secondary training pair (mj , rj), used to calibrate the scores.",
        "3.",
        "Select at random a negative relation r ?",
        "such that r ?",
        "6= ri and r ?",
        "6= rj .",
        "4.",
        "Make a stochastic gradient step to minimize max(0, 1?",
        "f(mi )>ri + f(mj)>r?).",
        "5.",
        "Enforce the constraint that each embedding vector is normalized, i.e. if ||Wi ||2 > 1 then Wi ?Wi/||Wi ||2."
      ]
    },
    {
      "heading": "3.2 Encoding Structured Data of KBs",
      "text": [
        "Using only weakly labeled text mentions for training ignores much of the prior knowledge we can leverage from a large KB such as Freebase.",
        "In order to connect this relational data with our model, we propose to encode its information into entity and relationship embeddings.",
        "This allows us to build a model which can score the plausibility of new entity relationship triples which are missing from Freebase.",
        "Several models have been recently developed for that purpose (e.g. in (Nickel et al., 2011; Bordes et al., 2011; Bordes et al., 2012)): we chose in this work to follow the approach of (Bordes et al., 2013), which is simple, flexible and has shown very promising results on Freebase data.",
        "Given a training set S = {(hi , ri , ti ), i = 1, ... , |S|} of relations extracted from the KB, this model learns vector embeddings of the entities and of the relationships using the idea that the functional relation induced by the r -labeled arcs of the KB should correspond to a translation of the embeddings.",
        "That is, given a k-dimensional embedding of the left-hand side (head) entity, adding the k-dimensional embedding of a given relation should yield the point (or close to the point) of the k-dimensional embedding of the right-hand side (tail) entity.",
        "Hence, this method enforces that h + r ?",
        "t when (h, r , t) holds, while h + r should be far away from t otherwise.",
        "The model thus gives the following score for the plausibility of a relation: Skb(h, r , t) = ?||h + r ?",
        "t||22 .",
        "A ranking loss is also used for training Skb.",
        "The ranking objective is designed to assign higher scores",
        "to existing relations versus any other possibility: ?i ,?h?",
        "6= hi , Skb(hi , ri , ti ) ?",
        "1 + Skb(h?, ri , ti ), ?i ,?r ?",
        "6= ri , Skb(hi , ri , ti ) ?",
        "1 + Skb(hi , r ?, ti ), ?i ,?t ?",
        "6= ti , Skb(hi , ri , ti ) ?",
        "1 + Skb(hi , ri , t ?).",
        "That is, for each known triple (h, r , t), if we replaced the (i) head, (ii) relation or (iii) tail with some other possibility, the modified triple should have a lower score (i.e. be less plausible) than the original triple.",
        "The three sets of constraints defined above encode the three types of modification.",
        "As in Section 3.1 we use soft constraints via the hinge loss, enforce constraints on the norm of embeddings, i.e. ?h,r ,t , ||h||2 ?",
        "1, ||r ||2 ?",
        "1, ||t||2 ?",
        "1, and training is performed using SGD, as in (Bordes et al., 2013).",
        "At test time, one may again need to calibrate the scores Skb across entity pairs.",
        "We propose a simple approach: we convert the scores by ranking all relationshipsR by Skb and instead output:",
        "i.e. a function of the rank of r .",
        "We chose the simplified model ?",
        "(x) = 1 if x < ?",
        "and 0 otherwise; ?(?)",
        "is the Kronecker function."
      ]
    },
    {
      "heading": "3.3 Implementation for Relation Extraction",
      "text": [
        "Our framework can be used for relation extraction in the following way.",
        "First, for each pair of entities (h, t) that appear in the test set, all the corresponding mentionsMh,t in the test set are collected and a prediction is performed with:",
        "The predicted relationship can either be a valid relationship or NA ?",
        "a marker that means that there is no relation between h and t (NA is added to R during training and is treated like other relationships).",
        "If r?h,t is a relationship, a composite score is defined:",
        "That is, only the top scoring non-NA predictions are re-scored.",
        "Hence, our final composite model favors predictions that agree with both the mentions and the KB.",
        "If r?h,t is NA, the score is unchanged."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We use the training and test data, evaluation framework and baselines from (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).",
        "NYT+FB This dataset, developed by (Riedel et al., 2010), aligns Freebase relations with the New York Times corpus.",
        "Entities were found using the Stanford named entity tagger (Finkel et al., 2005), and were matched to their name in Freebase.",
        "For each mention, sentence level features are extracted which include part of speech, named entity and dependency tree path properties.",
        "Unlike some of the previous methods, we do not use features that aggregate properties across multiple mentions.",
        "We kept the 100,000 most frequent features.There are 52 possible relationships and 121,034 training mentions of which most are labeled as no relation (labeled ?NA?)",
        "?",
        "there are 4700 Freebase relations mentioned in the training set, and 1950 in the test set.",
        "Freebase Freebase is a large-scale KB that has around 80M entities, 23k relationships and 1.2B relations.",
        "We used a subset restricted to the top 4M entities for scalability reasons ?",
        "where top is defined as the ones with the largest number of relations to other entities.",
        "We used all the 23k possible relationships in Freebase.",
        "To make a realistic setting, we did not choose the entity set using the NYT+FB data set, so it may not overlap completely.",
        "For that reason, we needed to keep the set rather large.",
        "Keeping the top 4M entities gives an overlap of 80% with the entities in the NYT+FB test set.",
        "Most importantly, we then removed all the entity pairs present in the NYT+FB test set from Freebase, i.e. all relations they are involved in independent of the relationship.",
        "This ensures that we cannot just memorize the true relations for an entity pair ?",
        "we have to learn to generalize from other entities and relations.",
        "As the NYT+FB dataset was built on an earlier version of Freebase we also had to translate the deprecated relationships into their new variants (e.g. ?/p/business/company/place_founded ?",
        "?",
        "?/organization/organization/place_founded?)",
        "to make the two datasets link (then, the 52 relationships in NYT+FB are now a subset of the 23k from Freebase).",
        "We then trained the Skb model on the remaining triples.",
        "curves for a variety of methods.",
        "Bottom: the same plot zoomed to the recall [0-0.1] region.",
        "WsabieM2R is our method trained only on mentions, WsabieM2R+FB uses Freebase annotations as well.",
        "Modeling Following (Bordes et al., 2013) we set the embedding dimension k to 50.",
        "The learning rate for SGD was selected using a validation set: we obtained 0.001 for Sm2r , and 0.1 for Skb.",
        "For the calibration of S?kb, ?",
        "= 10 (note, here we are ranking all 23k Freebase relationships).",
        "Training Sm2r took 5 minutes, whilst training Skb took 2 days due to the large scale of the data set.",
        "Results Figure 1 displays the aggregate precision / recall curves of our approach WSABIEM2R+FB which uses the combination of Sm2r + Skb, as well as WSABIEM2R , which only uses Sm2r , and existing state-of-the-art approaches: HOFFMANN (Hoffmann et al., 2011)2, MIMLRE (Surdeanu et al., 2012).",
        "RIEDEL (Riedel et al., 2010) and MINTZ (Mintz et al., 2009).",
        "WSABIEM2R is comparable to, but slightly worse than, the MIMLRE and HOFFMANN methods, possibly due to its simplified assumptions (e.g. predicting a single relationship per entity pair).",
        "However, the addition of extra knowledge from other Freebase entities in WSABIEM2R+FB provides superior performance to all other methods, by a wide margin, at least between 0 and 0.1 recall (see bottom plot).",
        "Performance of WSABIEM2R and WSABIEM2R+FB for recall > 0.1 degrades rapidly, faster than that of other methods.",
        "This is also caused by the simplifications of WSABIEM2R that prevent it from reaching high precision when the recall is greater than 0.1.",
        "We recall that Freebase data is not used to detect relationships i.e. to discriminate between NA and the rest, but only to select the best relationship in case of detection.",
        "That is WSABIEM2R+FB only improves precision, not recall, so both versions of Wsabie are similar w.r.t.",
        "recall.",
        "This could be improved by borrowing ideas from HOFFMANN (Hoffmann et al., 2011) or MIMLRE (Surdeanu et al., 2012) for dealing with the multi-label case.",
        "Our approach, which uses Freebase to increase precision, is general and could improve any other method."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper we described a framework for leverag-ing large scale knowledge bases to improve relation extraction by training not only on (mention, relationship) pairs but using all other KB triples as well.",
        "We empirically showed that it allows to significantly improve precision on extracted relations.",
        "Our model-ing approach is general and should apply to other settings, e.g. for the task of entity linking."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was carried out in the framework of the Labex MS2T (ANR-11-IDEX-0004-02), and funded by the French National Agency for Research (EVEREST-12-JS02-005-01)."
      ]
    }
  ]
}
