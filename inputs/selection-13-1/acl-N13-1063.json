{
  "info": {
    "authors": [
      "Mo Yu",
      "Tiejun Zhao",
      "Daxiang Dong",
      "Hao Tian",
      "Dianhai Yu"
    ],
    "book": "NAACL",
    "id": "acl-N13-1063",
    "title": "Compound Embedding Features for Semi-supervised Learning",
    "url": "https://aclweb.org/anthology/N13-1063",
    "year": 2013
  },
  "references": [
    "acl-D07-1096",
    "acl-D11-1014",
    "acl-N12-1052",
    "acl-P05-1045",
    "acl-P08-1068",
    "acl-P09-1056",
    "acl-P10-1040"
  ],
  "sections": [
    {
      "text": [
        "4.2.",
        "This is possibly because in these tasks, either the target labels are correlated with combinations of different dimensions of word embeddings, or discriminative information may be coded in different intervals in the same dimension.",
        "So treating embeddings directly as inputs to a linear model could not fully utilize them.",
        "Moreover, since embeddings are dense vectors, it will introduce large amount of computations when they are directly used as inputs, making the method impractical.",
        "In this paper, we first introduced the idea of clustering embeddings to overcome the last two disadvantages discussed above.",
        "The high-dimensional cluster features make samples from different classes better separated by linear models.",
        "And models with these features can still run fast because the clusters are sparse and discrete.",
        "Second, we proposed the compound features based on clustering.",
        "Compound features, which are conjunctive features of neighboring words, have been widely used in NLP models for improving the performances because they are more discriminative.",
        "Compound features of embeddings can also help a model to better predict labels associated with rare-words and ambiguous words, because compound features composed of embeddings of nearby words can help to better describe the property of these words.",
        "Compound features are difficult to build on dense embeddings.",
        "However they are easy to induce from the sparse embedding clusters proposed in this paper.",
        "Experiments on chunking and NER showed that based on the same embeddings, the compound features managed to achieve better performances.",
        "Moreover, we proposed analyses to reveal the reasons for the improvements of embedding-clusters and compound features.",
        "They suggest that these features can better deal with rare-words and word ambiguity, and are more suitable for linear models.",
        "In addition, although Brown clustering was considered better in (Turian et al2010), our experiment results and comparisons showed that our compound features from embedding clustering is at least comparable with those from Brown clustering.",
        "Since embeddings can greatly benefit from the improvement and developing of deep learning in the future, we believe that our proposed method has a large space of performance growth and will benefit more applications in NLP.",
        "In the rest of the paper, Section 2 introduces how compound embedding features were obtained.",
        "Section 3 gives experimental results.",
        "In Section 4, we give analysis about the advantages of compound features.",
        "Section 5 gives the conclusions.",
        "2 Clustering of Word Embeddings"
      ]
    },
    {
      "heading": "2.1 Learning Word Embeddings Word embeddings in this paper were trained by NLMs (Bengio et al., 2003). The model predicts the scores of probabilities of words given their context information in the sentences. It first converts the current word and its context words (e.g. n-1 words before it as in n-gram models) into embeddings. Then these embeddings are put together and propagate forward on the network to compute the score of current word. After minimizing the loss on training data, embeddings are learned and can be further used as smoothing representations for words. 2.2 Clustering of embeddings In order to get compound features of embeddings, we first induce discrete clusters from the embeddings. Concretely, the k-means clustering algorithm is used. Each word is treated as a single sample. A cluster is represented as the mean of the embeddings of words assigned to it. Similarities between words and clusters are measured by Euclidean distance. As discussed and experimented later, different numbers of ks contain information of different granularity. So we combine clustering results achieved by different ks as features to better utilize the embeddings. 2.3 Compound features Based on embedding clusters, more powerful compound features can be built. Compound features are conjunctions between basic features of words and their contexts, which are widely used in NLP. Koo et al. (2008) also observed that compound features of Brown clusters achieved more improvements on parsing. It is also necessary to build compound embedding features since they can better deal with rare-words and ambiguous words. For example, although embedding of a rare-word is not fully trained and hence inaccurate, embeddings of its context words can still be accurate as long as they",
      "text": [
        "are not rare and are fully trained.",
        "So we could utilize the combination of embeddings before and after the word to predict its tag correctly.",
        "We conducted analysis to verify our theory in Section4.",
        "We combined the compound features together with other state-of-the-art human-craft features in supervised models.",
        "Examples of the resulted feature templates in chunking and NER are shown in Table 1 & 2.",
        "The feature 1101 ccyy ??",
        "in the last row is an example of compound feature made up of the embedding clusters of words before and after current word.",
        "Compound feature extraction can similarly be applied to form compound features of Brown clusters.",
        "For example, Brown clusters can replace embedding clusters in 3th row of Table 1.",
        "Words }1,0{,1}2:2{, , ????",
        "iiiii www POS }2,1{,1}2:2{, , ?????",
        "iiiii ppp Cluster 11}1,0{,1}2:2{, ,, ccccc iiiii ?????",
        "Transition },,,{ 1100001 cccpwyy ??",
        "Table 1: Chunking features.",
        "Cluster features are suitable for both Brown clusters and embedding clusters.",
        "Sym-bol iy is the tag predicted on word iw .",
        "Words }1,0{,1}2:2{, , ????",
        "iiiii www Pre/suffix 1: }4:1{,0:1 }4:2{,0 , ??",
        "??",
        "iiii ww Orthography ( ) ( )00 , wCapwHyp POS }2,1{,1}2:2{, , ?????",
        "iiiii ppp Chunking }2,1{,1}2:2{, , ?????",
        "iiiii bbb Cluster 11}1,0{,1}2:2{, ,, ccccc iiiii ?????",
        "Transition },,,{ 1100001 cccpwyy ??",
        "Table 2: NER features.",
        "Hyp indicates if word contains hyphen and Cap indicates if first letter is capitalized.",
        "3 Experiments"
      ]
    },
    {
      "heading": "3.1 Experimental settings We tested our compound features on the same chunking (CoNLL2000) and NER (CoNLL2003) tasks in (Turian et al., 2010). The Brown cluster features were used for comparison, which shared the same feature template used by clusters of embeddings. To compare with the work of (Turian et al. 2010), which aimed to solve the same problem but using embedding directly, we used the same word embeddings (CW 50) and Brown clusters (1000 clusters) they provided. The embeddings in (Turian et al. 2010) are trained on RCV corpus, while the CoNLL2000 data is a part of the WSJ corpus. Since we believe that word representations",
      "text": [
        "trained on similar domain may better help to improve the results, we also used embeddings and Brown clusters trained on unlabeled WSJ data from (Nivre et al. 2007) for comparison.",
        "Moreover, we wish to find out whether our method extends well to languages other than Eng-lish.",
        "So we conducted experiments on Chinese NER, where large amount of training data exists, which makes improving accuracies more difficult.",
        "We used data from People's Daily (Jan.-Jun.",
        "1998) and converted them following the style of Penn CTB (Xue et al. 2005).",
        "Data from April was chosen as test set (1,309,616 words in 55,177 sentenc-es), others for training (6,119,063 words in 255,951 sentences).",
        "The Chinese word representations were trained on Chinese Wikipedia until March 2011.",
        "The features used in Chinese NER are similar to those in English, except for the or-thography, pre/suffixes, and chunking features.",
        "We did little preprocessing work for the training of word representations on WSJ data.",
        "The datasets were tokenized and capital words were kept.",
        "For training of Chinese Wikipedia, we retained the bodies of all articles and replaced words with frequencies lower than 10 as an ?UK_WORD?",
        "token.",
        "On each dataset, we induced embeddings with 64 dimensions based on 7-gram models and 1000 Brown clusters.",
        "The method in (Schwenk, 2007) was used to accelerate the training processes of NLMs.",
        "All the NLMs were trained for 5 epochs.",
        "For clustering of embeddings we choose k=500 and 2500 since such combination performed best on development set as shown in the next section.",
        "We chose the Sofia-ml toolkit (Sculley 2010) for clustering of embeddings in order to save time.",
        "In the experiments CRF models were used and were optimized by ASGD (implemented by L?on Bottou).",
        "For comparison we reimplemented the direct usage of embeddings in (Turian et al. 2010) with CRFsuite (Okazaki, 2007) since their features contain continuous values.",
        "3.2 Performances Table 3 shows the chunking results.",
        "The results reported in (Turian et al. 2010) were denoted as ?direct?.",
        "Based on the same word representations, our compound features got better performances in all cases.",
        "The embedding features trained on unla-beled WSJ data yield further improvements, show",
        "2(b), where the numbers of senses for a word are measured by the numbers of different POS tags it has in Penn Treebank.",
        "12.1% of the errors on ambiguous words were reduced, comparing to 8.4% of the errors on unambiguous ones."
      ]
    }
  ]
}
