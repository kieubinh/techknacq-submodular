{
  "info": {
    "authors": [
      "Wang Ling",
      "Guang Xiang",
      "Chris Dyer",
      "Alan Black",
      "Isabel Trancoso"
    ],
    "book": "ACL",
    "id": "acl-P13-1018",
    "title": "Microblogs as Parallel Corpora",
    "url": "https://aclweb.org/anthology/P13-1018",
    "year": 2013
  },
  "references": [
    "acl-C10-1124",
    "acl-C10-2010",
    "acl-C96-2141",
    "acl-I08-2120",
    "acl-J03-3002",
    "acl-J93-2003",
    "acl-N03-1017",
    "acl-N10-1063",
    "acl-N12-1006",
    "acl-N12-1079",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P08-1113",
    "acl-P11-2008",
    "acl-W06-1008",
    "acl-W12-3152",
    "acl-W12-3153"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring parallel text: some users create post multilingual messages targeting international audiences while others ?retweet?",
        "translations.",
        "We present an efficient method for detecting these messages and extracting parallel segments from them.",
        "We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counterpart of Twitter) using only their public APIs.",
        "As a supplement to existing parallel training data, our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary.",
        "The resources in described in this paper are available at http://www.cs.cmu.edu/?lingwang/utopia."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Microblogs such as Twitter and Facebook have gained tremendous popularity in the past 10 years.",
        "In addition to being an important form of communication for many people, they often contain extremely current, even breaking, information about world events.",
        "However, the writing style of mi-croblogs tends to be quite colloquial, with frequent orthographic innovation (R U still with me or what?)",
        "and nonstandard abbreviations (idk!",
        "shm)?quite unlike the style found in more traditional, edited genres.",
        "This poses considerable problems for traditional NLP tools, which were developed with other domains in mind, which often make strong assumptions about orthographic uniformity (i.e., there is just one way to spell you).",
        "One approach to cope with this problem is to annotate in-domain data (Gimpel et al., 2011).",
        "Machine translation suffers acutely from the domain-mismatch problem caused by microblog text.",
        "On one hand, standard models are probably suboptimal since they (like many models) assume orthographic uniformity in the input.",
        "However, more acutely, the data used to develop these systems and train their models is drawn from formal and carefully edited domains, such as parallel web pages and translated legal documents.",
        "MT training data seldom looks anything like microblog text.",
        "This paper introduces a method for finding naturally occurring parallel microblog text, which helps address the domain-mismatch problem.",
        "Our method is inspired by the perhaps surprising observation that a reasonable number of microblog users tweet ?in parallel?",
        "in two or more languages.",
        "For instance, the American entertainer Snoop Dogg regularly posts parallel messages on Sina Weibo (Mainland China's equivalent of Twitter), for example, watup Kenny Mayne!!",
        "- Kenny Mayne????????",
        "?, where an English message and its Chinese translation are in the same post, separated by a dash.",
        "Our method is able to identify and extract such translations.",
        "Briefly, this requires determining if a tweet contains more than one language, if these multilingual utterances contain translated material (or are due to something else, such as code switching), and what the translated spans are.",
        "The paper is organized as follows.",
        "Section 2 describes the related work in parallel data extraction.",
        "Section 3 presents our model to extract parallel data within the same document.",
        "Section 4 describes our extraction pipeline.",
        "Section 5 describes the data we gathered from both Sina Weibo (Chinese-English) and Twitter (Chinese-English and Arabic-English).",
        "We then present experiments showing that our harvested data not only substantially improves translations of microblog text with",
        "existing (and arguably inappropriate) translation models, but that it improves the translation of more traditional MT genres, like newswire.",
        "We conclude in Section 6."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Automatic collection of parallel data is a well-studied problem.",
        "Approaches to finding parallel web documents automatically have been particularly important (Resnik and Smith, 2003; Fukushima et al., 2006; Li and Liu, 2008; Uszko-reit et al., 2010; Ture and Lin, 2012).",
        "These broadly work by identifying promising candidates using simple features, such as URL similarity or ?gist translations?",
        "and then identifying truly parallel segments with more expensive classifiers.",
        "More specialized resources were developed using manual procedures to leverage special features of very large collections, such as Europarl (Koehn, 2005).",
        "Mining parallel or comparable messages from microblogs has mainly relied on Cross-Lingual Information Retrieval techniques (CLIR).",
        "Jelh et al. (2012) attempt to find pairs of tweets in Twitter using Arabic tweets as search queries in a CLIR system.",
        "Afterwards, the model described in (Xu et al., 2001) is applied to retrieve a set of ranked translation candidates for each Arabic tweet, which are then used as parallel candidates.",
        "The work on mining parenthetical translations (Lin et al., 2008), which attempts to find translations within the same document, has some similarities with our work, since parenthetical translations are within the same document.",
        "However, parenthetical translations are generally used to translate names or terms, which is more limited than our work which extracts whole sentence translations.",
        "Finally, crowd-sourcing techniques to obtain translations have been previously studied and applied to build datasets for casual domains (Zbib et al., 2012; Post et al., 2012).",
        "These approaches require remunerated workers to translate the messages, and the amount of messages translated per day is limited.",
        "We aim to propose a method that acquires large amounts of parallel data for free.",
        "The drawback is that there is a margin of error in the parallel segment identification and alignment.",
        "However, our system can be tuned for precision or for recall."
      ]
    },
    {
      "heading": "3 Parallel Segment Retrieval",
      "text": [
        "We will first abstract from the domain of Microblogs and focus on the task of retrieving parallel segments from single documents.",
        "Prior work on finding parallel data attempts to reason about the probability that pairs of documents (x, y) are parallel.",
        "In contrast, we only consider one document at a time, defined by x = x1, x2, .",
        ".",
        ".",
        ", xn, and consisting of n tokens, and need to determine whether there is parallel data in x, and if so, where are the parallel segments and their languages.",
        "For simplicity, we assume that there are at most 2 continuous segments that are parallel.",
        "As representation for the parallel segments within the document, we use the tuple ([p, q], l, [u, v], r, a).",
        "The word indexes [p, q] and [u, v] are used to identify the left segment (from p to q) and right segment (from u to v), which are parallel.",
        "We shall refer [p, q] and [u, v] as the spans of the left and right segments.",
        "To avoid overlaps, we set the constraint p ?",
        "q < u ?",
        "v. Then, we use l and r to identify the language of the left and right segments, respectively.",
        "Finally, a represents the word alignment between the words in the left and the right segments.",
        "The main problem we address is to find the parallel data when the boundaries of the parallel segments are not defined explicitly.",
        "If we knew the indexes [p, q] and [u, v], we could simply run a language detector for these segments to find l and r. Then, we would use an word alignment model (Brown et al., 1993; Vogel et al., 1996), with source s = xp, .",
        ".",
        ".",
        ", xq, target t = xu, .",
        ".",
        ".",
        ", xv and lexical table ?l,r to calculate the Viterbi alignment a.",
        "Finally, from the probability of the word alignments, we can determine whether the segments are parallel.",
        "Thus, our model will attempt to find the optimal values for the segments [p, q][u, v], languages l, r and word alignments a jointly.",
        "However, there are two problems with this approach.",
        "Firstly, word alignment models generally attribute higher probabilities to smaller segments, since these are the result of a smaller product chain of probabilities.",
        "In fact, because our model can freely choose the segments to align, choosing only one word as the left segment that is well aligned to a word in the right segment would be the best choice.",
        "This is obviously not our goal, since we would not obtain any useful sentence pairs.",
        "Secondly, inference must be performed over the combination of all latent variables, which is intractable using",
        "a brute force algorithm.",
        "We shall describe our model to solve the first problem in 3.1 and our dynamic programming approach to make the inference tractable in 3.2."
      ]
    },
    {
      "heading": "3.1 Model",
      "text": [
        "We propose a simple (non-probabilistic) three-factor model that models the spans of the parallel segments, their languages, and word alignments jointly.",
        "This model is defined as follows: S([u, v], r, [p, q],l, a |x) = S?S ([p, q], [u, v] |x)?",
        "S?L(l, r |[p, q], [u, v], x)?",
        "S?T (a |[p, q], l, [u, v], r, x) Each of the components is weighted by the parameters ?, ?",
        "and ?.",
        "We set these values empirically ?",
        "= 0.3, ?",
        "= 0.3 and ?",
        "= 0.4, and leave the optimization of these parameters as future work.",
        "We discuss the components of this model in turn.",
        "Span score SS .",
        "We define the score of hypothesized pair of spans [p, q], [u, v] as:",
        "?",
        "([p, q], [u, v], x) The first factor is a distribution over all spans that assigns higher probability to segmentations that cover more words in the document.",
        "It is highest for segmentations that cover all the words in the document (this is desirable since there are many sentence pairs that can be extracted but we want to find the largest sentence pair in the document).",
        "The function ?",
        "takes on values of 0 or 1 depending on whether certain constraints are violated, these include: parenthetical constraints that enforce that spans must not break text within parenthetical characters and language constraints that ensure that we do break a sequence of Mandarin characters, Arabic words or Latin words.",
        "Language score SL.",
        "The language score SL(l, r |[p, q], [u, v], x) indicates whether the language labels l, r are appropriate to the document contents: SL(l, r |[p, q], [u, v], x) =?q",
        "where L(l, x) is a language detection function that yields 1 if the word xi is in language l, and 0 otherwise.",
        "We build the function simply by considering all words that are composed of Latin characters as English, Arabic characters as Arabic and Han characters as Mandarin.",
        "This approach is not perfect, but it is simple and works reasonably well for our purposes.",
        "Translation score ST .",
        "The translation score ST (a |[p, q], l, [u, v], r) indicates whether [p, q] is a reasonable translation of [u, v] with the alignment a.",
        "We rely on IBM Model 1 probabilities for this score: ST (a |[p, q], l, [u, v], r, x) =",
        "The lexical tables PM1 for the various language pairs are trained a priori using available parallel corpora.",
        "While IBM Model 1 produces worse alignments than other models, in our problem, we need to efficiently consider all possible spans, language pairs and word alignments, which makes the problem intractable.",
        "We will show that dynamic programing can be used to make this problem tractable, using Model 1.",
        "Furthermore, IBM Model 1 has shown good performance for sentence alignment systems previously (Xu et al., 2005; Braune and Fraser, 2010)."
      ]
    },
    {
      "heading": "3.2 Inference",
      "text": [
        "Our goal is to find the spans, language pair and alignments such that: argmax [p,q],l,[u,v],r,a S([p, q], l, [u, v], r, a |x) (1) A high score indicates that the predicted bispan is likely to correspond to a valid parallel span, so we set a constant threshold ?",
        "to determine whether a document has parallel data, i.e., the value of z: z?",
        "= max [u,v],r,[p,q],l,a S([u, v], r, [p, q], l, a |x) > ?",
        "Naively maximizing Eq.",
        "1 would require O(|x|6) operations, which is too inefficient to be practical on large datasets.",
        "To process millions of documents, this process would need to be optimized.",
        "The main bottleneck of the naive algorithm is finding new Viterbi Model 1 word alignments every time we change the spans.",
        "Thus, we propose",
        "an iterative approach to compute the Viterbi word alignments for IBM Model 1 using dynamic programming.",
        "Dynamic programming search.",
        "The insight we use to improve the runtime is that the Viterbi word alignment of a bispan can be reused to calculate the Viterbi word alignments of larger bis-pans.",
        "The algorithm operates on a 4-dimensional chart of bispans.",
        "It starts with the minimal valid span (i.e., [0, 0], [1, 1]) and progressively builds larger spans from smaller ones.",
        "Let Ap,q,u,v represent the Viterbi alignment (under ST ) of the bispan [p, q], [u, v].",
        "The algorithm uses the following recursions defined in terms of four operations ?",
        "{+v,+u,+p,+q} that manipulate a single dimension of the bispan to construct larger spans: ?",
        "Ap,q,u,v+1 = ?+v(Ap,q,u,v) adds one token to the end of the right span with index v + 1 and find the viterbi alignment for that token.",
        "This requires iterating over all the tokens in the left span, [p, q] and possibly updating their alignments.",
        "See Fig. 1 for an illustration.",
        "?",
        "Ap,q,u+1,v = ?+u(Ap,q,u,v) removes the first token of the right span with index u, so we only need to remove the alignment from u, which can be done in time O(1).",
        "?",
        "Ap,q+1,u,v = ?+q(Ap,q,u,v) adds one token to the end of the left span with index q + 1, we",
        "need to check for each word in the right span, if aligning to the word in index q+1 yields a better translation probability.",
        "This update requires n?",
        "token of the left span with index p. After removing the token, we need to find new alignments for all tokens that were aligned to p. Thus, the number of operations for this update is K ?",
        "(q ?",
        "p + 1), where K is the number of words that were aligned to p. In the best case, no words are aligned to the token in p, and we can simply remove it.",
        "In the worst case, if all target words were aligned to p, this update will result in the recalculation of all Viterbi Alignments.",
        "The algorithm proceeds until all valid cells have been computed.",
        "One important aspect is that the update functions differ in complexity, so the sequence of updates we apply will impact the performance of the system.",
        "Most spans are reachable using any of the four update functions.",
        "For instance, the span A2,3,4,5 can be reached using ?+v(A2,3,4,4), ?+u(A2,3,3,5), ?+q(A2,2,4,5) or ?+p(A1,3,4,5).",
        "However, we want to use ?+u",
        "light gray boxes show the parallel span and the dark boxes show the span's Viterbi alignment.",
        "In this example, the parallel message contains a ?translation?",
        "of a b to A B. whenever possible, since it only requires one operation, although that is not always possible.",
        "For instance, the state A2,2,2,4 cannot be reached using ?+u, since the state A2,2,1,4 is not valid, because the spans overlap.",
        "If this happens, incrementally more expensive updates need to be used, such as ?+v, then ?+q, which are in the same order of complexity.",
        "Finally, we want to minimize the use of ?+p, which is quadratic in the worst case.",
        "Thus, we use the following recursive formulation that guarantees the optimal outcome:",
        "This transition function applies the cheapest possible update to reach state Ap,q,u,v.",
        "Complexity analysis.",
        "We can see that ?+u is only needed in the following the cases [0, 1][2, 2], [1, 2][3, 3], ?",
        "?",
        "?",
        ", [n ?",
        "2, n ?",
        "1][n, n].",
        "Since, this update is quadratic in the worst case, the complexity of this operations is O(n3).",
        "The update ?+q, is applied to the cases [?, 1][2, 2], [?, 2][3, 3], ?",
        "?",
        "?",
        ", [?, n?1], [n, n], where ?",
        "denotes any number within the span constraints but not present in previous updates.",
        "Since, the update is linear and we need to iterate through all tokens twice, this update takes O(n3) operations.",
        "The update ?+v is applied for the cases [?, 1][2, ?",
        "], [?, 2][3, ?",
        "], ?",
        "?",
        "?",
        ", [?, n?",
        "1], [n, ?].",
        "Thus, with three degrees of freedom and a linear update, it runs in O(n4) time.",
        "Finally, update ?+u runs in constant time, but is run for all remaining cases, which constitute O(n4) space.",
        "By summing the",
        "executions of all updates, we observe that the order of magnitude of our exact inference process is O(n4).",
        "Note that for exact inference, it is not possible to get a lower order of magnitude, since we need to at least iterate through all possible span values once, which takes O(n4) time."
      ]
    },
    {
      "heading": "4 Parallel Data Extraction",
      "text": [
        "We will now describe our method to extract parallel data from Microblogs.",
        "The target domains in this work are Twitter and Sina Weibo, and the main language pair is Chinese-English.",
        "Furthermore, we also run the system for the Arabic-English language pair using the Twitter data.",
        "For the Twitter domain, we use a previously crawled dataset from the years 2008 to 2013, where one million tweets are crawled every day.",
        "In total, we processed 1.6 billion tweets.",
        "Regarding Sina Weibo, we built a crawler that continuously collects tweets from Weibo.",
        "We start from one seed user and collect his posts, and then we find the users he follows that we have not considered, and repeat.",
        "Due to the rate limiting established by the Weibo API1, we are restricted in terms of number of requests every hour, which greatly limits the amount of messages we can collect.",
        "Furthermore, each request can only fetch up to 100 posts from a user, and subsequent pages of 100 posts require additional API calls.",
        "Thus, to optimize the number of parallel posts we can collect per request, we only crawl all messages from users that have at least 10 parallel tweets in their first 100 posts.",
        "The number of parallel messages is estimated by running our alignment model, and checking if ?",
        "> ?, where ?",
        "was set empirically initially, and optimized after obtaining annotated data, which will be detailed in 5.1.",
        "Using this process, we crawled 65 million tweets from Sina Weibo within 4 months.",
        "In both cases, we first filter the collection of tweets for messages containing at least one trigram in each language of the target language pair, determined by their Unicode ranges.",
        "This means that for the Chinese-English language pair, we only keep tweets with more than 3 Mandarin characters and 3 latin words.",
        "Furthermore, based on the work in (Jelh et al., 2012), if a tweet A is identified as a retweet, meaning that it references another tweetB, we also consider the hypothesis that these tweets may be mutual translations.",
        "Thus, if A and B contain trigrams in different languages,",
        "allel data.",
        "This is done by concatenating tweets A and B, and adding the constraint that [p, q] must be within A and [u, v] must be within B.",
        "Finally, identical duplicate tweets are removed.",
        "After filtering, we obtained 1124k ZH-EN tweets from Sina Weibo, 868k ZH-EN and 136k AR-EN tweets from Twitter.",
        "These language pairs are not definite, since we simply check if there is a trigram in each language.",
        "Finally, we run our alignment model described in section 3, and obtain the parallel segments and their scores, which measure how likely those segments are parallel.",
        "In this process, lexical tables for EN-ZH language pair used by Model 1 were built using the FBIS dataset (LDC2003E14) for both directions, a corpus of 300K sentence pairs from the news domain.",
        "Likewise, for the EN-AR language pair, we use a fraction of the NIST dataset, by removing the data originated from UN, which leads to approximately 1M sentence pairs."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We evaluate our method in two ways.",
        "First, intrinsically, by observing how well our method identifies tweets containing parallel data, the language pair and what their spans are.",
        "Second, extrinsically, by looking at how well the data improves a translation task.",
        "This methodology is similar to that of Smith et al. (2010)."
      ]
    },
    {
      "heading": "5.1 Parallel Data Extraction",
      "text": [
        "Data.",
        "Our method needs to determine if a given tweet contains parallel data, and if so, what is the language pair of the data, and what segments are parallel.",
        "Thus, we had a native Mandarin speaker, also fluent in English, to annotate 2000 tweets sampled from crawled Weibo tweets.",
        "One important question of answer is what portion of the Microblogs contains parallel data.",
        "Thus, we also use the random sample Twitter and annotated 1200 samples, identifying whether each sample contains parallel data, for the EN-ZH and AR-EN filtered tweets.",
        "Metrics.",
        "To test the accuracy of the score S, we ordered all 2000 samples by score.",
        "Then, we calculate the precision, recall and accuracy at increasing intervals of 10% of the top samples.",
        "We count as a true positive (tp) if we correctly identify a parallel tweet, and as a false positive (fp) spuriously detect a parallel tweet.",
        "Finally, a true negative (tn) occurs when we correctly detect a non-parallel",
        "tweet, and a false negative (fn) if we miss a parallel tweet.",
        "Then, we set the precision as tptp+fp , recall as tptp+fn and accuracy as tp+tntp+fp+tn+fn .",
        "Forlanguage identification, we calculate the accuracy based on the number of instances that were identified with the correct language pair.",
        "Finally, to evaluate the segment alignment, we use the Word Error Rate (WER) metric, without substitutions, where we compare the left and right spans of our system and the respective spans of the reference.",
        "We count an insertion error (I) for each word in our system's spans that is not present in the reference span and a deletion error (D) for each word in the reference span that is not present in our system's spans.",
        "Thus, we set WER = D+IN , where N is the number of tokens in the tweet.",
        "To compute this score for the whole test set, we compute the average of the WER for each sample.",
        "Results.",
        "The precision, recall and accuracy curves are shown in Figure 2.",
        "The quality of the parallel sentence detection did not vary significantly with different setups, so we will only show the results for the best setup, which is the baseline model with span constraints.",
        "for parallel data detection.",
        "The y-axis denotes the scores for each metric, and the x-axis denotes the percentage of the highest scoring sentence pairs that are kept.",
        "From the precision and recall curves, we observe that most of the parallel data can be found at the top 30% of the filtered tweets, where 5 in 6 tweets are detected correctly as parallel, and only 1 in every 6 parallel sentences is lost.",
        "We will denote the score threshold at this point as ?, which is a good threshold to estimate on whether the tweet is parallel.",
        "However, this parameter can be tuned for precision or recall.",
        "We also see that in total, 30% of the filtered tweets are parallel.",
        "If we generalize this ratio for the complete set with 1124k tweets, we can expect approximately 337k parallel sentences.",
        "Finally, since 65 million tweets were extracted to generate the 337k tweets, we estimate that approximately 1 parallel tweet can be found for every 200 tweets we process using our targeted approach.",
        "On the other hand, from the 1200 tweets from Twitter, we found that 27 had parallel data in the ZH-EN pair, if we extrapolate for the whole 868k filtered tweets, we expect that we can find 19530.",
        "19530 parallel sentences from 1.6 billion tweets crawled randomly, represents 0.001% of the total corpora.",
        "For AR-EN, a similar result was obtained where we expect 12407 tweets out of the 1.6 billion to be parallel.",
        "This shows that targeted approaches can substantially reduce the crawling effort required to find parallel tweets.",
        "Still, considering that billions of tweets are posted daily, this is a substantial source of parallel data.",
        "The remainder of the tests will be performed on the Weibo dataset, which contains more parallel data.",
        "Tests on the Twitter data will be conducted as future work, when we process Twitter data on a larger scale to obtain more parallel sentences.",
        "For the language identification task, we had an accuracy of 99.9%, since distinguishing English and Mandarin is trivial.",
        "The small percentage of errors originated from other latin languages (Ex: French) due to our naive language detector.",
        "As for the segment alignment task.",
        "Our baseline system with no constraints obtains a WER of 12.86%, and this can be improved to 11.66% by adding constraints to possible spans.",
        "This shows that, on average, approximately 1 in 9 words on the parallel segments is incorrect.",
        "However, translation models are generally robust to such kinds of errors and can learn good translations even in the presence of imperfect sentence pairs.",
        "Among the 578 tweets that are parallel, 496 were extracted within the same tweet and 82 were extracted from retweets.",
        "Thus, we see that the majority of the parallel data comes from within the same tweet.",
        "Topic analysis.",
        "To give an intuition about the contents of the parallel data we found, we looked at the distribution over topics of the parallel dataset inferred by LDA (Blei et al., 2003).",
        "Thus, we grouped the Weibo filtered tweets by users, and ran LDA over the predicted English segments, with 12 topics.",
        "The 7 most interpretable topics are shown in Table 1.",
        "We see that the data contains a",
        "# Topic Most probable words in topic 1 (Dating) love time girl live mv back word night rt wanna 2 (Entertainment) news video follow pong image text great day today fans 3 (Music) cr day tour cn url amazon music full concert alive 4 (Religion) man god good love life heart would give make lord 5 (Nightlife) cn url beijing shanqi party adj club dj beijiner vt 6 (Chinese News) china chinese year people world beijing years passion country government 7 (Fashion) street fashion fall style photo men model vogue spring magazine",
        "Weibo.",
        "Topic labels (in parentheses) were assigned manually for illustration purposes.",
        "variety of topics, both formal (Chinese news, religion) and informal (entertainment, music).",
        "Example sentence pairs.",
        "To gain some perspective on the type of sentence pairs we are extracting, we will illustrate some sentence pairs we crawled and aligned automatically.",
        "Table 2 contains 5 English-Mandarin and 4 English-Arabic sentence pairs that were extracted automatically.",
        "These were chosen, since they contain some aspects that are characteristic of the text present in"
      ]
    },
    {
      "heading": "Microblogs and Social Media. These are:",
      "text": [
        "?",
        "Abbreviations - In most sentence pairs examples, we can witness the use of abbreviated forms of English words, such as wanna, TMI, 4 and imma.",
        "These can be normalized as want to, too much information, for and I am going to, respectively.",
        "In sentence 5, we observe that this phenomena also occurs in Mandarin.",
        "We find that TMD is a popular way to write???",
        "whose Pinyin rendering is ta?",
        "ma?",
        "de.",
        "The meaning of this expression depends on the context it is used, and can convey a similar connotation as adding the intensifier the hell to an English sentence.",
        "?",
        "Jargon - Another common phenomena is the appearance of words that are only used in sub-communities.",
        "For instance, in sentence pair 4, we the jargon word cday is used, which is a colloquial variant for birthday.",
        "?",
        "Emoticons - In sentence 8, we observe the presence of the emoticon :), which is frequently used in this media.",
        "We found that emoticons are either translated as they are or simply removed, in most cases.",
        "?",
        "Syntax errors - In the domain of microblogs, it is also common that users do not write strictly syntactic sentences, for instance, in sentence pair 7, the sentence onni this gift only 4 u, is clearly not syntactically correct.",
        "Firstly, onni is a named entity, yet it is not capitalized.",
        "Secondly, a comma should follow onni.",
        "Thirdly, the verb is should be used after gift.",
        "Having examples of these sentences in the training set, with common mistakes (intentional or not), might become a key factor in training MT systems that can be robust to such errors.",
        "?",
        "Dialects - We can observe a much broader range of dialects in our data, since there are no dialect standards in microblogs.",
        "For instance, in sentence pair 6, we observe an arabic word (in bold) used in the spoken Arabic dialect used in some countries along the shores of the Persian Gulf, which means means the next.",
        "In standard Arabic, a significantly different form is used.",
        "We can also see in sentence pair 9 that our aligner does not alway make the correct choice when determining spans.",
        "In this case, the segment RT @MARYAMALKHAWAJA: was included in the English segment spuriously, since it does not correspond to anything in the Arabic counterpart."
      ]
    },
    {
      "heading": "5.2 Machine Translation Experiments",
      "text": [
        "We report on machine translation experiments using our harvested data in two domains: edited news and microblogs.",
        "News translation.",
        "For the news test, we created a new test set from a crawl of the Chinese-English documents on the Project Syndicate website2, which contains news commentary articles.",
        "We chose to use this data set, rather than more standard NIST test sets to ensure that we had recent documents in the test set (the most recent NIST test sets contain documents published in 2007, well before our microblog data was created).",
        "We extracted 1386 parallel sentences for tuning and another 1386 sentences for testing, from the manually aligned segments.",
        "For this test set, we used 8 million sentences from the full NIST parallel dataset as the language model training data.",
        "We shall call this test set Syndicate.",
        "?",
        "??",
        "??",
        "@ H. ?Q ?",
        "9 RT @MARYAMALKHAWAJA: there is a call @Y ?",
        "?",
        "?A J?",
        "?Y?",
        "?",
        "?",
        "H@Q?A ???",
        "Z @Y K ?A J?for widespread protests in #bahrain tmrw",
        "sentences were extracted from Sina Weibo and the English-Arabic sentences were extracted from Twitter.",
        "Some messages have been shorted to fit into the table.",
        "Some interesting aspects of these sentence pairs are marked in bold.",
        "Microblog translation.",
        "To carry out the microblog translation experiments, we need a high quality parallel test set.",
        "Since we are not aware of such a test set, we created one by manually selecting parallel messages from Weibo.",
        "Our procedure was as follows.",
        "We selected 2000 candidate Weibo posts from users who have a high number of parallel tweets according to our automatic method (at least 2 in every 5 tweets).",
        "To these, we added another 2000 messages from our targeted Weibo crawl, but these had no requirement on the proportion of parallel tweets they had produced.",
        "We identified 2374 parallel segments, of which we used 1187 for development and 1187 for testing.",
        "We refer to this test set as Weibo.3 Obviously, we removed the development and test sets from our training data.",
        "Furthermore, to ensure that our training data was not too similar to the test set in the Weibo translation task, we filtered the training data to remove near duplicates by computing edit distance between each parallel sentence in the heldout set and each training instance.",
        "If either the source or the target sides of the a training instance had an edit distance of less than 10%, we removed it.4 As for the language models, we collected a further 10M tweets from Twitter for the English language model and another 10M tweets from Weibo for the Chinese language model.",
        "3We acknowledge that self-translated messages are probably not a typically representative sample of all microblog messages.",
        "However, we do not have the resources to produce a carefully curated test set with a more broadly representative distribution.",
        "Still, we believe these results are informative as long as this is kept in mind.",
        "ferent translation directions (left to right), broken with different training corpora (top to bottom).",
        "Baselines.",
        "We report results on these test sets using different training data.",
        "First, we use the FBIS dataset which contains 300K high quality sentence pairs, mostly in the broadcast news domain.",
        "Second, we use the full 2012 NIST Chinese-English dataset (approximately 8M sentence pairs, including FBIS).",
        "Finally, we use our crawled data (referred as Weibo) by itself and also combined with the two previous training sets.",
        "Setup.",
        "We use the Moses phrase-based MT system with standard features (Koehn et al., 2003).",
        "For reordering, we use the MSD reordering model (Axelrod et al., 2005).",
        "As the language model, we use a 5-gram model with Kneser-Ney smoothing.",
        "The weights were tuned using MERT (Och, 2003).",
        "Results are presented with BLEU-4 (Papineni et al., 2002).",
        "Results.",
        "The BLEU scores for the different parallel corpora are shown in Table 3 and the top 10 out-of-vocabulary (OOV) words for each dataset are shown in Table 4.",
        "We observe that for the Syndicate test set, the NIST and FBIS datasets",
        "test sets with three different training sets.",
        "perform better than our extracted parallel data.",
        "This is to be expected, since our dataset was extracted from an extremely different domain.",
        "However, by combining the Weibo parallel data with this standard data, improvements in BLEU are obtained.",
        "Error analysis indicates that one major factor is that names from current events, such as Romney and Wikileaks do not occur in the older NIST and FBIS datasets, but they are represented in the Weibo dataset.",
        "Furthermore, we also note that the system built on the Weibo dataset does not perform substantially worse than the one trained on the FBIS dataset, a further indication that harvesting parallel microblog data yields a diverse collection of translated material.",
        "For the Weibo test set, a significant improvement over the news datasets can be achieved using our crawled parallel data.",
        "Once again newer terms, such as iTunes, are one of the reasons older datasets perform less well.",
        "However, in this case, the top OOV words of the news domain datasets are not the most accurate representation of coverage problems in this domain.",
        "This is because many frequent words in microblogs, e.g., nonstandard abbreviations, like u and 4 are found in the news domain as words, albeit with different meanings.",
        "Thus, the OOV table gives an incomplete picture of the translation problems when using the news domain corpora to translate microblogs.",
        "Also, some structural errors occur when training with the news domain datasets, one such example is shown in table 5, where the character ?",
        "is incorrectly translated to said.",
        "This occurs because this type of constructions is infrequent in news datasets.",
        "Furthermore, we can see that compound expressions, such as the translation from ???",
        "?",
        "to party time are also learned.",
        "Finally, we observe that combining the datasets Source ?sam farrar??????",
        "Reference to sam farrar , party time FBIS farrar to sam said , in time NIST to sam farrar said , the moment WEIBO to sam farrar , party time",
        "training sets.",
        "yields another gain over individual datasets, both in the Syndicate and in the Weibo test sets."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We presented a framework to crawl parallel data from microblogs.",
        "We find parallel data from single posts, with translations of the same sentence in two languages.",
        "We show that a considerable amount of parallel sentence pairs can be crawled from microblogs and these can be used to improve Machine Translation by updating our translation tables with translations of newer terms.",
        "Furthermore, the in-domain data can substantially improve the translation quality on microblog data.",
        "The resources described in this paper and further developments are available to the general public at http://www.cs.cmu.edu/?lingwang/utopia."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The PhD thesis of Wang Ling is supported by FCT grant SFRH/BD/51157/2010.",
        "The authors wish to express their gratitude to thank William Cohen, Noah Smith, Waleed Ammar, and the anonymous reviewers for their insight and comments.",
        "We are also extremely grateful to Brendan O?Connor for providing the Twitter data and to Philipp Koehn and Barry Haddow for providing the Project Syndicate data."
      ]
    }
  ]
}
