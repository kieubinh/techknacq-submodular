{
  "info": {
    "authors": [
      "Man Lan",
      "Yu Xu",
      "Zhengyu Niu"
    ],
    "book": "ACL",
    "id": "acl-P13-1047",
    "title": "Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition",
    "url": "https://aclweb.org/anthology/P13-1047",
    "year": 2013
  },
  "references": [
    "acl-C08-2022",
    "acl-C10-2172",
    "acl-D09-1036",
    "acl-D10-1039",
    "acl-J09-3003",
    "acl-L08-1093",
    "acl-N03-1030",
    "acl-N06-2034",
    "acl-P02-1047",
    "acl-P05-1001",
    "acl-P09-1077",
    "acl-P10-1073",
    "acl-W01-1605",
    "acl-W06-1317"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples.",
        "However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data.",
        "In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition.",
        "Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The task of implicit discourse relation recognition is to identify the type of discourse relation (a.k.a.",
        "rhetorical relation) hold between two spans of text, where there is no discourse connective (a.k.a.",
        "discourse marker, e.g., but, and) in context to explicitly mark their discourse relation (e.g., Contrast or Explanation).",
        "It can be of great benefit to many downstream NLP applications, such as question answering (QA) (Verberne et al., 2007), information extraction (IE) (Cimiano et al., 2005), and machine translation (MT), etc.",
        "This task is quite challenging due to two reasons.",
        "First, without discourse connective in text, the task is quite difficult in itself.",
        "Second, implicit discourse relation is quite frequent in text.",
        "For example, almost half the sentences in the British National Corpus held implicit discourse relations (Sporleder and Lascarides, 2008).",
        "Therefore, the task of implicit discourse relation recognition is the key to improving end-to-end discourse parser performance.",
        "To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora.",
        "This line of research was followed by (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007).",
        "In these works, sentences containing certain words or phrases (e.g. but, although) were selected out from raw corpora using a pattern-based approach and then these words or phrases were removed from these sentences.",
        "Thus the resulting sentences were used as synthetic training examples for implicit discourse relation recognition.",
        "Since there is ambiguity of a word or phrase serving for discourse connective (i.e., the ambiguity between discourse and non-discourse usage or the ambiguity between two or more discourse relations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises.",
        "Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al., 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine) implicit discourse data (Pitler et al., 2009) (Lin et al., 2009) (Wang et al., 2010) with the use of linguistically informed features and machine learning algorithms.",
        "(Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data.",
        "They found some evidence that this behavior is largely independent of the classifiers used and seems to lie in the data itself (e.g., marked and unmarked examples may be too dissimilar linguistically and",
        "removing unambiguous markers in the automatic labelling process may lead to a meaning shift in the examples).",
        "We state that in some cases it is true while in other cases it may not always be so.",
        "A simple example is given here: (E1) a.",
        "We can't win.",
        "b.",
        "[but] We must keep trying.",
        "We may find that in this example whether the insertion or the removal of connective but would not lead to a redundant or missing information between the above two sentences.",
        "That is, discourse connectives can be inserted between or removed from two sentences without changing the semantic relations between them in some cases.",
        "Another similar observation is in the annotation procedure of PDTB.",
        "To label implicit discourse relation, annotators inserted connective which can best express the relation between sentences without any redundancy1.",
        "We see that there should be some linguistical similarities between explicit and implicit discourse examples.",
        "Therefore, the first question arises: can we exploit this kind of linguistic similarity between explicit and implicit discourse examples to improve implicit discourse relation recognition?",
        "In this paper, we propose a multi-task learning based method to improve the performance of implicit discourse relation recognition (as main task) with the help of relevant auxiliary tasks.",
        "Specifically, the main task is to recognize the implicit discourse relations based on genuine implicit discourse data and the auxiliary task is to recognize the implicit discourse relations based on synthetic implicit discourse data.",
        "According to the principle of multi-task learning, the learning model can be optimized by the shared part of the main task and the auxiliary tasks without bring unnecessary noise.",
        "That means, the model can learn from synthetic implicit data while it would not bring unnecessary noise from synthetic implicit data.",
        "Although (Sporleder and Lascarides, 2008) did not mention, we speculate that another possible reason for the reported worse performance may result from noises in synthetic implicit discourse data.",
        "These synthetic data can be generated from two sources: (1) raw corpora with the use of pattern-based approach in (Marcu and Echihabi, 1According to the PDTB Annotation Manual (PDTB-Group, 2008), if the insertion of connective leads to ?redundancy?, the relation is annotated as Alternative lexicalizations (AltLex), not implicit.",
        "2002) and (Sporleder and Lascarides, 2008), and (2) manually annotated explicit data with the removal of explicit discourse connectives.",
        "Obviously, the data generated from the second source is cleaner and more reliable than that from the first source.",
        "Therefore, the second question to address in this work is: whether synthetic implicit discourse data generated from explicit discourse data source (i.e., the second source) can lead to a better performance than that from raw corpora (i.e., the first source)?",
        "To answer this question, we will make a comparison of synthetic discourse data generated from two corpora, i.e., the BILLIP corpus and the explicit discourse data annotated in PDTB.",
        "The rest of this paper is organized as follows.",
        "Section 2 reviews related work on implicit discourse relation classification and multi-task learning.",
        "Section 3 presents our proposed multi-task learning method for implicit discourse relation classification.",
        "Section 4 provides the implementation technique details of the proposed multi-task method.",
        "Section 5 presents experiments and discusses results.",
        "Section 6 concludes this work."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": []
    },
    {
      "heading": "2.1 Implicit discourse relation classification",
      "text": [
        "Due to the lack of benchmark data for implicit discourse relation analysis, earlier work used un-labeled data to generate synthetic implicit discourse data.",
        "For example, (Marcu and Echihabi, 2002) proposed an unsupervised method to recognize four discourse relations, i.e., Contrast, Explanation-evidence, Condition and Elaboration.",
        "They first used unambiguous pattern to extract explicit discourse examples from raw corpus.",
        "Then they generated synthetic implicit discourse data by removing explicit discourse connectives from sentences extracted.",
        "In their work, they collected word pairs from synthetic data set as features and used machine learning method to classify implicit discourse relation.",
        "Based on this work, several researchers have extended the work to improve the performance of relation classification.",
        "For example, (Saito et al., 2006) showed that the use of phrasal patterns as additional features can help a word-pair based system for discourse relation prediction on a Japanese corpus.",
        "Furthermore, (Blair-Goldensohn, 2007) improved previous work with the use of parameter optimization,",
        "topic segmentation and syntactic parsing.",
        "However, (Sporleder and Lascarides, 2008) showed that the training model built on a synthetic data set, like the work of (Marcu and Echihabi, 2002), may not be a good strategy since the linguistic dissimilarity between explicit and implicit data may hurt the performance of a model on natural data when being trained on synthetic data.",
        "This line of research work approaches this relation prediction problem by recasting it as a classification problem.",
        "(Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al., 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988).",
        "(Wellner et al., 2006) presented a study of discourse relation disambiguation on GraphBank (Wolf et al., 2005).",
        "Recently, (Pitler et al., 2009) (Lin et al., 2009) and (Wang et al., 2010) conducted discourse relation study on PDTB (Prasad et al., 2008) which has been widely used in this field.",
        "Research work in this category exploited both labeled and unlabeled data for discourse relation prediction.",
        "(Hernault et al., 2010) presented a semi-supervised method based on the analysis of co-occurring features in labeled and unlabeled data.",
        "Very recently, (Hernault et al., 2011) introduced a semi-supervised work using structure learning method for discourse relation classification, which is quite relevant to our work.",
        "However, they performed discourse relation classification on both explicit and implicit data.",
        "And their work is different from our work in many aspects, such as, feature sets, auxiliary task, auxiliary data, class labels, learning framework, and so on.",
        "Furthermore, there is no explicit conclusion or evidence in their work to address the two questions raised in Section 1.",
        "Unlike their previous work, our previous work (Zhou et al., 2010) presented a method to predict the missing connective based on a language model trained on an unannotated corpus.",
        "The predicted connective was then used as a feature to classify the implicit relation."
      ]
    },
    {
      "heading": "2.2 Multi-task learning",
      "text": [
        "Multi-task learning is a kind of machine learning method, which learns a main task together with other related auxiliary tasks at the same time, using a shared representation.",
        "This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks.",
        "Many multi-task learning methods have been proposed in recent years, (Ando and Zhang, 2005a), (Argyriou et al., 2008), (Jebara, 2004), (Bonilla et al., 2008), (Evgeniou and Pontil, 2004), (Baxter, 2000), (Caruana, 1997), (Thrun, 1996).",
        "One group uses task relations as regularization terms in the objective function to be optimized.",
        "For example, in (Evgeniou and Pontil, 2004) the regularization terms make the parameters of models closer for similar tasks.",
        "Another group is proposed to find the common structure from data and then utilize the learned structure for multi-task learning (Argyriou et al., 2008) (Ando and Zhang, 2005b)."
      ]
    },
    {
      "heading": "3 Multi-task Learning for Discourse",
      "text": []
    },
    {
      "heading": "Relation Prediction 3.1 Motivation",
      "text": [
        "The idea of using multi-task learning for implicit discourse relation classification is motivated by the observations that we have made on implicit discourse relation.",
        "On one hand, since building a hand-annotated implicit discourse relation corpus is costly and time consuming, most previous work attempted to use synthetic implicit discourse examples as training data.",
        "However, (Sporleder and Lascarides, 2008) found that the model trained on synthetic implicit data has not performed as well as expected in natural implicit data.",
        "They stated that the reason is linguistic dissimilarity between explicit and implicit discourse data.",
        "This indicates that straightly using synthetic implicit data as training data may not be helpful.",
        "On the other hand, as shown in Section 1, we observe that in some cases explicit discourse relation and implicit discourse relation can express the same meaning with or without a discourse connective.",
        "This indicates that in certain degree they must be similar to each other.",
        "If it is true, the synthetic implicit relations are expected to be helpful for implicit discourse relation classification.",
        "Therefore, what we have to do is to find a way to train a model which has the capabilities to learn from their similarity and to ignore their dissimilarity as well.",
        "To solve it, we propose a multi-task learning method for implicit discourse relation classi",
        "fication, where the classification model seeks the shared part through jointly learning main task and multiple auxiliary tasks.",
        "As a result, the model can be optimized by the similar shared part without bringing noise in the dissimilar part.",
        "Specifically, in this work, we use alternating structure optimization (ASO) (Ando and Zhang, 2005a) to construct the multi-task learning framework.",
        "ASO has been shown to be useful in a semi-supervised learning configuration for several NLP applications, such as, text chunking (Ando and Zhang, 2005b) and text classification (Ando and Zhang, 2005a)."
      ]
    },
    {
      "heading": "3.2 Multi-task learning and ASO",
      "text": [
        "Generally, multi-task learning(MTL) considers m prediction problems indexed by ?",
        "?",
        "{1, ...,m}, each with n?",
        "samples (X?i , Y ?i ) for i ?",
        "{1, ...n?}",
        "(Xi are input feature vectors and Yi are corresponding classification labels) and assumes that there exists a common predictive structure shared by these m problems.",
        "Generally, the joint linear model for MTL is to predict problem ?",
        "in the following form: f?",
        "(?, X) = wT?",
        "X + vT?",
        "?X,?",
        "?T = I, (1) where I is the identity matrix,w?",
        "and v?",
        "are weight vectors specific to each problem ?, and ?",
        "is the structure matrix shared by all the m predictors.",
        "The main goal of MTL is to learn a common good feature map ?X for all the m problems.",
        "Several MTL methods have been presented to learn ?X for all the m problems.",
        "In this work, we adopt the ASO method.",
        "Specifically, the ASO method adopted singular value decomposition (SVD) to obtain ?",
        "and m predictors that minimize the empirical risk summed over all the m problems.",
        "Thus, the problem of optimization becomes the minimization of the joint empirical risk written as:",
        "(2) where loss function L(.)",
        "quantifies the difference between the prediction f(Xi) and the true output Yi for each predictor, and ?",
        "is a regularization parameter for square regularization to control the model complexity.",
        "To minimize the empirical risk, ASO repeats the following alternating optimization procedure until a convergence criterion",
        "is met: 1) Fix (?, V?",
        "), and find m predictors f?",
        "that minimize the above joint empirical risk.",
        "2) Fix m predictors f?, and find (?, V?)",
        "that minimizes the above joint empirical risk."
      ]
    },
    {
      "heading": "3.3 Auxiliary tasks",
      "text": [
        "There are two main principles to create auxiliary tasks.",
        "First, the auxiliary tasks should be automatically labeled in order to reduce the cost of manual labeling.",
        "Second, since the MTL model learns from the shared part of main task and auxiliary tasks, the auxiliary tasks should be quite relevant/similar to the main task.",
        "It is generally believed that the more the auxiliary tasks are relevant to the main task, the more the main task can benefit from the auxiliary tasks.",
        "Following these two principles, we create the auxiliary tasks by generating automatically labeled data as follows.",
        "Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label.",
        "In contrast, we adopt an automatic approach to generate synthetic labeled data, where each discourse connective between two texts serves as their relation label.",
        "The reason lies in the very strong connection between discourse connectives and discourse relations.",
        "For example, the connective but always indicates a contrast relation between two texts.",
        "And (Pitler et al., 2008) proved that using only connective itself, the accuracy of explicit discourse relation classification is over 93%.",
        "To build the mapping between discourse connective and discourse relation, for each connective, we count the times it appears in each relation and regard the relation in which it appears most frequently as its most relevant relation.",
        "Based on this mapping between connective and relation, we extract the synthetic labeled data containing the connective as training data for auxiliary tasks.",
        "For example, and appears 3, 000 times in PDTB as a discourse connective.",
        "Among them, it is manually annotated as an Expansion relation for 2, 938 times.",
        "So we regard the Expansion relation as its most relevant relation and generate a mapping pattern like: ?and ?",
        "Expansion?.",
        "Then we extract all sentences which contain discourse ?and?",
        "and remove this connective ?and?",
        "from sentences to generate synthetic implicit data.",
        "The resulting sentences are used in auxiliary task and automatically",
        "marked as Expansion relation."
      ]
    },
    {
      "heading": "4 Implementation Details of Multi-task",
      "text": []
    },
    {
      "heading": "Learning Method 4.1 Data sets for main and auxiliary tasks",
      "text": [
        "To examine whether there is a difference in synthetic implicit data generated from unannotated and annotated corpus, we use two corpora.",
        "One is a hand-annotated explicit discourse corpus, i.e., the explicit discourse relations in PDTB, denoted as exp.",
        "Another is an unannotated corpus, i.e., BLLIP (David McClosky and Johnson., 2008).",
        "PDTB (Prasad et al., 2008) is the largest hand-annotated corpus of discourse relation so far.",
        "It contains 2, 312 Wall Street Journal (WSJ) articles.",
        "The sense label of discourse relations is hierarchically with three levels, i.e., class, type and subtype.",
        "The top level contains four major semantic classes: Comparison (denoted as Comp.",
        "), Contingency (Cont.",
        "), Expansion (Exp.)",
        "and Temporal (Temp.).",
        "For each class, a set of types is used to refine relation sense.",
        "The set of subtypes is to further specify the semantic contribution of each argument.",
        "In this paper, we focus on the top level (class) and the second level (type) relations because the subtype relations are too fine-grained and only appear in some relations.",
        "Both explicit and implicit discourse relations are labeled in PDTB.",
        "In our experiment, the implicit discourse relations are used in the main task and for evaluation.",
        "While the explicit discourse relations are used in the auxiliary task.",
        "A detailed description of the data sources for different tasks is given below.",
        "Data set for main task Following previous work in (Pitler et al., 2009) and (Zhou et al., 2010), the implicit relations in sections 2-20 are used as training data for the main task (denoted as imp) and the implicit relations in sections 21-22 are for evaluation.",
        "Table 1 shows the distribution of implicit relations.",
        "There are too few training instances for six second level relations (indicated by * in Table 1), so we removed these six relations in our experiments.",
        "Data set for auxiliary task All explicit instances in sections 00-24 in PDTB, i.e., 18, 459 instances, are used for auxiliary task (denoted as exp).",
        "Following the method described in Section 3.3, we build the mapping patterns between con",
        "tions in the top and second level of PDTB nectives and relations in PDTB and generate synthetic labeled data by removing the connectives.",
        "According to the most relevant relation sense of connective removed, the resulting instances are grouped into different data sets."
      ]
    },
    {
      "heading": "4.1.2 BLLIP BLLIP North American News Text (Complete) is",
      "text": [
        "used as unlabeled data source to generate synthetic labeled data.",
        "In comparison with the synthetic labeled data generated from the explicit relations in PDTB, the synthetic labeled data from BLLIP contains more noise.",
        "This is because the former data is manually annotated whether a word serves as discourse connective or not, while the latter does not manually disambiguate two types of ambiguity, i.e., whether a word serves as discourse connective or not, and the type of discourse relation if it is a discourse connective.",
        "Finally, we extract 26, 412 instances from BLLIP (denoted as BLLIP) and use them for auxiliary task."
      ]
    },
    {
      "heading": "4.2 Feature representation",
      "text": [
        "For both main task and auxiliary tasks, we adopt the following three feature types.",
        "These features are chosen due to their superior performance in previous work (Pitler et al., 2009) and our previous work (Zhou et al., 2010).",
        "Verbs: Following (Pitler et al., 2009), we extract the pairs of verbs from both text spans.",
        "The number of verb pairs which have the same highest",
        "Levin verb class levels (Levin, 1993) is counted as a feature.",
        "Besides, the average length of verb phrases in each argument is included as a feature.",
        "In addition, the part of speech tags of the main verbs (e.g., base form, past tense, 3rd person singular present, etc.)",
        "in each argument, i.e., MD, VB, VBD, VBG, VBN, VBP, VBZ, are recorded as features, where we simply use the first verb in each argument as the main verb.",
        "Polarity: This feature records the number of positive, negated positive, negative and neutral words in both arguments and their cross product as well.",
        "For negated positives, we first locate the negated words in text span and then define the closely behind positive word as negated positive.",
        "The polarity of each word in arguments is derived from Multi-perspective Question Answering Opinion Corpus (MPQA) (Wilson et al., 2009).",
        "Modality: We examine six modal words (i.e., can, may, must, need, shall, will) including their various tenses or abbreviation forms in both arguments.",
        "This feature records the presence or absence of modal words in both arguments and their cross product."
      ]
    },
    {
      "heading": "4.3 Classifiers used multi-task learning",
      "text": [
        "We extract the above linguistically informed features from two synthetic implicit data sets (i.e., BLLIP and exp) to learn the auxiliary classifier and from the natural implicit data set (i.e., imp) to learn the main classifier.",
        "Under the ASO-based multi-task learning framework, the model of main task learns from the shared part of main task and auxiliary tasks.",
        "Specifically, we adopt multiple binary classification to build model for main task.",
        "That is, for each discourse relation, we build a binary classifier."
      ]
    },
    {
      "heading": "5 Experiments and Results",
      "text": []
    },
    {
      "heading": "5.1 Experiments",
      "text": [
        "Although previous work has been done on PDTB (Pitler et al., 2009) and (Lin et al., 2009), we cannot make a direct comparison with them because various experimental conditions, such as, different classification strategies (multi-class classification, multiple binary classification), different data preparation (feature extraction and selection), different benchmark data collections (different sections for training and test, different levels of discourse relations), different classifiers with various parameters (MaxEnt, Na?",
        "?ve Bayes, SVM, etc) and even different evaluation methods (F1, accuracy) have been adopted by different researchers.",
        "Therefore, to address the two questions raised in Section 1 and to make the comparison reliable and reasonable, we performed experiments on the top and second level of PDTB using single task learning and multi-task learning, respectively.",
        "The systems using single task learning serve as baseline systems.",
        "Under the single task learning, various combinations of exp and BLLIP data are incorporated with imp data for the implicit discourse relation classification task.",
        "We hypothesize that synthetical implicit data would contribute to the main task, i.e., the implicit discourse relation classification.",
        "Specifically, the natural implicit data (i.e., imp) are used to create main task and the synthetical implicit data (exp or BLLIP) are used to create auxiliary tasks for the purpose of optimizing the objective functions of main task.",
        "If the hypothesis is correct, the performance of main task would be improved by auxiliary tasks created from synthetical implicit data.",
        "Thus in the experiments of multi-task learning, only natural implicit examples (i.e., imp) data are used for main task training while different combinations of synthetical implicit examples (exp and BLLIP) are used for auxiliary task training.",
        "We adopt precision, recall and their combination F1 for performance evaluation.",
        "We also perform one-tailed t-test to validate if there is significant difference between two methods in terms of F1 performance analysis."
      ]
    },
    {
      "heading": "5.2 Results",
      "text": [
        "Table 2 summarizes the experimental results under single and multi-task learning on the top level of four PDTB relations with respect to different combinations of synthetic implicit data.",
        "For each relation, the first three rows indicate the results of using different single training data under single task learning and the last three rows indicate the results using different combinations of training data under single task and multi-task learning.",
        "The best F1 for every relation is shown in bold font.",
        "From this table, we can find that on four relations, our multi-task learning systems achieved the best performance using the combination of exp and BLLIP synthetic data.",
        "Table 3 summarizes the best single task and the best multi-task learning results on the second level of PDTB.",
        "For four relations, i.e., Synchrony, Con",
        "cession, Alternative and List, the classifier labels no instances due to the small percentages for these four types.",
        "Table 4 summarizes the one-tailed t-test results on the top level of PDTB between the best single task learning system (i.e., imp) and three multi-task learning systems (imp:exp+BLLIP indicates that imp is used for main task and the combination of exp and BLLIP are for auxiliary task).",
        "The systems with insignificant performance differences are grouped into one set and ?>?",
        "and ?>>?",
        "denote better than at significance level 0.01 and 0.001 respectively."
      ]
    },
    {
      "heading": "5.3 Discussion",
      "text": [
        "From Table 2 to Table 4, several findings can be found as follows.",
        "We can see that the multi-task learning systems perform consistently better than the single task learning systems for the prediction of implicit discourse relations.",
        "Our best multi-task learning system achieves an averaged F1 improvement of 5.86% over the best single task learning system on the top level of PDTB relations.",
        "Specifically, for",
        "Class One-tailed t-test results Comp.",
        "(imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp) Cont.",
        "(imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp) Exp.",
        "(imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp) Temp.",
        "(imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp) Table 4: Statistical significance tests results.",
        "the relations Comp., Cont., Exp., Temp., our best multi-task learning system achieve 4.26%, 7.06%, 8.8% and 3.34% F1 improvements over the best single task learning system.",
        "It indicates that using synthetic implicit data as auxiliary task greatly improves the performance of the main task.",
        "This is confirmed by the following t-tests in Table 4.",
        "In contrast to the performance of multi-task learning, the performance of the best single task learning system has been achieved on natural implicit discourse data alone.",
        "This finding is consistent with (Sporleder and Lascarides, 2008).",
        "It indicates that under single task learning, directly adding synthetic implicit data to increase the number of training data cannot be helpful to implicit discourse relation classification.",
        "The possible reasons result from (1) the different nature of implicit and explicit discourse data in linguistics and (2) the noise brought from synthetic implicit data.",
        "Based on the above analysis, we state that it is the way of utilizing synthetic implicit data that is important for implicit discourse relation classification.",
        "Although all three multi-task learning systems outperformed single task learning systems, we find that the two synthetic implicit data sets have not been shown a universally consistent performance on four top level PDTB relations.",
        "On one hand, for the relations Comp.",
        "and Temp., the performance of the two synthetic implicit data sets alone and their combination are comparable to each other and there is no significant difference between them.",
        "On the other hand, for the relations Cont.",
        "and Exp., the performance of exp data is inferior to that of BLLIP and their combination.",
        "This is contrary to our original expectation that exp data which has been manually annotated for discourse connective disambiguation should outperform BLLIP which contains a lot of noise.",
        "This finding indicates that under the multi-task learning, it may not be worthy of using manually annotated corpus to generate auxiliary data.",
        "It is quite promising since it can provide benefits to reducing the cost of human efforts on corpus annotation."
      ]
    },
    {
      "heading": "5.4 Ambiguity Analysis",
      "text": [
        "Although our experiments show that synthetic implicit data can help implicit discourse relation classification under multi-task learning framework, the overall performance is still quite low (44.64% in F1).",
        "Therefore, we analyze the types of ambiguity in relations and connectives in order to motivate possible future work.",
        "5.4.1 Ambiguity of implicit relation Without explicit discourse connective, the implicit discourse relation instance can be understood in two or more different ways.",
        "Given the example E2 in PDTB, the PDTB annotators explain it as Contingency or Expansion relation and manually insert corresponding implicit connective for one thing or because to express its relation.",
        "(E2) Arg1:Now the stage is set for the battle to play out Arg2:The anti-programmers are getting some helpful thunder from Congress",
        "Thus the ambiguity of implicit discourse relations makes this task difficult in itself."
      ]
    },
    {
      "heading": "5.4.2 Ambiguity of discourse connectives",
      "text": [
        "As we mentioned before, even given an explicit discourse connective in text, its discourse relation still can be explained in two or more different ways.",
        "And for different connectives, the ambiguity of relation senses is quite different.",
        "That is, the most frequent sense is not always the only sense that a connective expresses.",
        "In example E3, ?since?",
        "is explained by annotators to express Temporal or Contingency relation.",
        "(E3) Arg1:MiniScribe has been on the rocks Arg2:since it disclosed early this year that its earnings reports for 1988 weren't accurate.",
        "In PDTB, ?since?",
        "appears 184 times in explicit discourse relations.",
        "It expresses Temporal relation for 80 times, Contingency relation for 94 times and both Temporal and Contingency for 10 time (like example E3).",
        "Therefore, although we use its most frequent sense, i.e., Contingency, to automatically extract sentences and label them, almost less than half of them actually express Temporal relation.",
        "Thus the ambiguity of discourse connectives is another source which has brought noise to data when we generate synthetical implicit discourse relation."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "In this paper, we present a multi-task learning method to improve implicit discourse relation classification by leveraging synthetic implicit discourse data.",
        "Results on PDTB show that under the framework of multi-task learning, using synthetic discourse data as auxiliary task significantly improves the performance of main task.",
        "Our best multi-task learning system achieves an averaged F1 improvement of 5.86% over the best single task learning system on the top level of PDTB relations.",
        "Specifically, for the relations Comp., Cont., Exp., Temp., our best multi-task learning system achieves 4.26%, 7.06%, 8.8%, and 3.34% F1 improvements over a state of the art baseline system.",
        "This indicates that it is the way of utilizing synthetic discourse examples that is important for implicit discourse relation classification."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This research is supported by grants from Na"
      ]
    }
  ]
}
