{
  "info": {
    "authors": [
      "Fabienne Braune",
      "Nina Seemann",
      "Daniel Quernheim",
      "Andreas Maletti"
    ],
    "book": "ACL",
    "id": "acl-P13-1080",
    "title": "Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/P13-1080",
    "year": 2013
  },
  "references": [
    "acl-C04-1024",
    "acl-C08-1138",
    "acl-J03-1002",
    "acl-J07-2003",
    "acl-J97-3002",
    "acl-N03-1017",
    "acl-N04-1035",
    "acl-N10-1130",
    "acl-N12-1027",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P03-2041",
    "acl-P05-1022",
    "acl-P06-1077",
    "acl-P06-1121",
    "acl-P07-2045",
    "acl-P08-1064",
    "acl-P09-1063",
    "acl-P09-1103",
    "acl-P10-1146",
    "acl-P11-1083",
    "acl-W04-3250",
    "acl-W08-0411",
    "acl-W09-0401"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a new translation model integrating the shallow local multi bottom-up tree transducer.",
        "We perform a large-scale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English?German translation task.",
        "As an additional contribution we make the developed software and complete tool-chain publicly available for further experimentation."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Besides phrase-based machine translation systems (Koehn et al., 2003), syntax-based systems have become widely used because of their ability to handle non-local reordering.",
        "Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al., 2009).",
        "However, those systems use linguistic syntactic annotation at different levels.",
        "For example, the systems proposed by Wu (1997) and Chiang (2007) use no linguistic information and are syntactic in a structural sense only.",
        "Huang et al. (2006) and Liu et al. (2006) use syntactic annotations on the source language side and show significant improvements in translation quality.",
        "Using syntax exclusively on the target language side has also been successfully tried by Galley et al. (2004) and Galley et al.",
        "(2006).",
        "Nowadays, open-source toolkits such as Moses (Koehn et al., 2007) offer syntax-based components (Hoang et al., 2009), which allow experiments without expert knowledge.",
        "The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides.",
        "However, as noted by Lavie et al. (2008), Liu et al.",
        "(2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive.",
        "Several strategies such as (i) using parse forests instead of single parses (Liu et al., 2009) or (ii) soft syntactic constraints (Chiang, 2010) have been developed to alleviate this problem.",
        "Another successful approach has been to switch to more powerful formalisms, which allow the extraction of more general rules.",
        "A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars (STSSG) of Zhang et al. (2008a), Zhang et al. (2008b), and Sun et al. (2009), which allows sequences of trees on both sides of the rules [see also (Raoult, 1997)].",
        "The multi bottom-up tree transducer (MBOT) of Arnold and Dauchet (1982) and Lilin (1978) offers a middle ground between traditional syntax-based models and STSSG.",
        "Roughly speaking, an MBOT is an STSSG, in which all the discontinuities must occur on the target language side (Maletti, 2011).",
        "This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti (2010).",
        "Formally, they are expressive enough to express all sensible translations (Maletti, 2012)1.",
        "Figure 2 displays sample rules of the MBOT variant, called `MBOT, that we use (in a graphical representation of the trees and the alignment).",
        "In this contribution, we report on our novel statistical machine translation system that uses an `MBOT-based translation model.",
        "The theoretical foundations of `MBOT and their integration into our translation model are presented in Sections 2 and 3.",
        "In order to empirically evaluate the `MBOT model, we implemented a machine trans",
        "We have t(21) = VBD and t|221 is the subtree marked in red.",
        "lation system that we are going to make available to the public.",
        "We implemented `MBOT inside the syntax-based component of the Moses open source toolkit.",
        "Section 4 presents the most important algorithms of our `MBOT decoder.",
        "We evaluate our new system on the WMT 2009 shared translation task English ?",
        "German.",
        "The translation quality is automatically measured using BLEU scores, and we confirm the findings by providing linguistic evidence (see Section 5).",
        "Note that in contrast to several previous approaches, we perform large scale experiments by training systems with approx.",
        "1.5 million parallel sentences."
      ]
    },
    {
      "heading": "2 Theoretical Model",
      "text": [
        "In this section, we present the theoretical generative model used in our approach to syntax-based machine translation.",
        "Essentially, it is the local multi bottom-up tree transducer of Maletti (2011) with the restriction that all rules must be shallow, which means that the left-hand side of each rule has height at most 2 (see Figure 2 for shallow rules and Figure 4 for rules including non-shallow rules).",
        "The rules extracted from the training example of Figure 3 are displayed in Figure 4.",
        "Those extracted rules are forcibly made shallow by removing internal nodes.",
        "The application of those rules is illustrated in Figures 5 and 6.",
        "For those that want to understand the inner workings, we recall the principal model in full detail in the rest of this section.",
        "Since we utilize syntactic parse trees, let us introduce trees first.",
        "Given an alphabet ?",
        "of labels, the set T?",
        "of all ?-trees is the smallest set T such that ?",
        "(t1, .",
        ".",
        ".",
        ", tk) ?",
        "T for all ?",
        "?",
        "?, integer k ?",
        "0, and t1, .",
        ".",
        ".",
        ", tk ?",
        "T .",
        "Intuitively, a tree t consists of a labeled root node ?",
        "followed by a sequence t1, .",
        ".",
        ".",
        ", tk of its children.",
        "A tree t ?",
        "T?",
        "is shallow if t = ?",
        "(t1, .",
        ".",
        ".",
        ", tk) with ?",
        "?",
        "?",
        "and t1, .",
        ".",
        ".",
        ", tk ?",
        "?.",
        "To address a node inside a tree, we use its position, which is a word consisting of positive integers.",
        "Roughly speaking, the root of a tree is addressed with the position ?",
        "(the empty word).",
        "The position iw with i ?",
        "N addresses the position w in the ith direct child of the root.",
        "In this way, each node in the tree is assigned a unique position.",
        "We illustrate this notion in Figure 1.",
        "Formally, the positions pos(t) ?",
        "N?",
        "of a tree t = ?",
        "(t1, .",
        ".",
        ".",
        ", tk) are inductively defined",
        "Let t ?",
        "T?",
        "and w ?",
        "pos(t).",
        "The label of t at position w is t(w), and the subtree rooted at position w is t|w.",
        "These notions are also illustrated in",
        "w1 /?",
        "pos(t).",
        "In other words, leaves do not have any children.",
        "Given a subset N ?",
        "?, we let leafN (t) = {w ?",
        "pos(t) |t(w) ?",
        "N, w leaf in t} be the set of all leaves labeled by elements of N .",
        "When N is the set of nonterminals, we call them leaf nonterminals.",
        "We extend this notion to se",
        "Let w1, .",
        ".",
        ".",
        ", wn ?",
        "pos(t) be (pairwise prefix-incomparable) positions and t1, .",
        ".",
        ".",
        ", tn ?",
        "T?.",
        "Then t[wi ?",
        "ti]1?i?n denotes the tree that is obtained from t by replacing (in parallel) the subtrees at wi by ti for every 1 ?",
        "i ?",
        "n. Now we are ready to introduce our model, which is a minor variation of the local multi bottom-up tree transducer of Maletti (2011).",
        "Let ?",
        "and ?",
        "be the input and output symbols, respectively, and let N ?",
        "?",
        "??",
        "be the set of nonterminal symbols.",
        "Essentially, the model works on pairs ?t, (u1, .",
        ".",
        ".",
        ", uk)?",
        "consisting of an input tree t ?",
        "T?",
        "and a sequence u1, .",
        ".",
        ".",
        ", uk ?",
        "T?",
        "of output trees.",
        "Such pairs are pre-translations of rank k. The pre-translation ?t, (u1, .",
        ".",
        ".",
        ", uk)?",
        "is shallow if all trees t, u1, .",
        ".",
        ".",
        ", uk in it are shallow.",
        "Together with a pre-translation we typically have to store an alignment.",
        "Given a pre-translation ?t, (u1, .",
        ".",
        ".",
        ", uk)?",
        "of rank k and 1 ?",
        "i ?",
        "k, we call ui the ith translation of t. An alignment for this pre-translation is an injective mapping ?",
        ": leaf(k)N (u1, .",
        ".",
        ".",
        ", uk)?",
        "leafN (t)?N such that if (w, j) ?",
        "ran(?",
        "), then also (w, i) ?",
        "ran(?)",
        "for all 1 ?",
        "j ?",
        "i.2 In other words, if an alignment requests the ith translation, then it should also request all previous translations.",
        "Definition 1 A shallow local multi bottom-up tree transducer (`MBOT) is a finite set R of rules together with a mapping c : R ?",
        "R such that every rule, written t ??",
        "(u1, .",
        ".",
        ".",
        ", uk), contains a shallow pre-translation ?t, (u1, .",
        ".",
        ".",
        ", uk)?",
        "and an alignment ?",
        "for it.",
        "The components t, (u1, .",
        ".",
        ".",
        ", uk), ?, and c(?)",
        "are called the left-hand side, the right-hand side, the alignment, and the weight of the rule ?",
        "= t ??",
        "(u1, .",
        ".",
        ".",
        ", uk).",
        "Figure 2 shows two example `MBOT rules (without weights).",
        "Overall, the rules of an `MBOT are similar to the rules of an SCFG (synchronous context-free grammar), but our right-hand sides contain a sequence of trees instead of just a single tree.",
        "In addition, the alignments in an SCFG rule are bijective between leaf nonterminals, whereas our model permits multiple alignments to a single leaf nonterminal in the left-hand side (see Figure 2).",
        "Our `MBOT rules are obtained automatically from data like that in Figure 3.",
        "Thus, we (word) align the bilingual text and parse it in both the source and the target language.",
        "In this manner we obtain sentence pairs like the one shown in Figure 3.",
        "To these sentence pairs we apply the rule extraction method of Maletti (2011).",
        "The rules extracted from the sentence pair of Figure 3 are shown in Figure 4.",
        "Note that these rules are not necessarily shallow (the last two rules are not).",
        "Thus, we post-process the extracted rules and make them shallow.",
        "The shallow rules corresponding to the non-shallow rules of Figure 4 are shown in Figure 2.",
        "Next, we define how to combine rules to form derivations.",
        "In contrast to most other models, we 2ran(f) for a mapping f : A?",
        "B denotes the range of f ,",
        "only introduce a derivation semantics that does not collapse multiple derivations for the same input-output pair.3 We need one final notion.",
        "Let ?",
        "= t ??",
        "(u1, .",
        ".",
        ".",
        ", uk) be a rule and w ?",
        "leafN (t) be a leaf nonterminal (occurrence) in the left-hand side.",
        "The w-rank rk(?, w) of the rule ?",
        "is rk(?, w) = max {i ?",
        "N |(w, i) ?",
        "ran(?)}",
        ".",
        "For example, for the lower rule ?",
        "in Figure 2 we have rk(?, 1) = 1, rk(?, 2) = 2, and rk(?, 3) = 1.",
        "Definition 2 The set ?",
        "(R, c) of weighted pre-translations of an `MBOT (R, c) is the smallest",
        "set T subject to the following restriction: If there exist ?",
        "a rule ?",
        "= t??",
        "(u1, .",
        ".",
        ".",
        ", uk) ?",
        "R, ?",
        "a weighted pre-translation ?tw, cw, (uw1 , .",
        ".",
        ".",
        ", uwkw)?",
        "?",
        "T for every w ?",
        "leafN (t) with ?",
        "rk(?, w) = kw,4 ?",
        "t(w) = tw(?",
        "),5 and ?",
        "for every iw?",
        "?",
        "leaf(k)N (u1, .",
        ".",
        ".",
        ", uk),6 ui(w?)",
        "= uvj (?)",
        "with ?(iw?)",
        "= (v, j), then ?t?, c?, (u?1, .",
        ".",
        ".",
        ", u?k)?",
        "?",
        "T is a weighted pre-translation, where ?",
        "t?",
        "= t[w ?",
        "tw |w ?",
        "leafN (t)],",
        "?",
        "c?",
        "= c(?)",
        "?",
        "?w?leafN (t) cw, and ?",
        "u?i = ui[iw?",
        "?",
        "uvj |?(iw?)",
        "= (v, j)] for",
        "every 1 ?",
        "i ?",
        "k. Rules that do not contain any nonterminal leaves are automatically weighted pre-translations with their associated rule weight.",
        "Otherwise, each nonterminal leaf w in the left-hand side of a rule ?",
        "must be replaced by the input tree tw of a pre-translation ?tw, cw, (uw1 , .",
        ".",
        ".",
        ", uwkw)?, whose root islabeled by the same nonterminal.",
        "In addition, the rank rk(?, w) of the replaced nonterminal should match the number kw of components in the selected weighted pre-translation.",
        "Finally, the nonterminals in the right-hand side that are aligned to w should be replaced by the translation that the alignment requests, provided that the nonterminal matches with the root symbol of the requested translation.",
        "The weight of the new pre-translation is obtained simply by multiplying the rule weight and the weights of the selected weighted pre-translations.",
        "The overall process is illustrated in Figures 5 and 6."
      ]
    },
    {
      "heading": "3 Translation Model",
      "text": [
        "Given a source language sentence e, our translation model aims to find the best corresponding target language translation g?",
        ";7 i.e., g?",
        "= arg maxg p(g|e) .",
        "We estimate the probability p(g|e) through a log-linear combination of component models with parameters ?m scored on the pre-translations ?t, (u)?",
        "such that the leaves of t concatenated read e.8",
        "(2) The indirect translation weight using the rule weights as described in Section 2 (3) Lexical translation weight source?",
        "target (4) Lexical translation weight target?",
        "source (5) Target side language model (6) Number of words in the target sentences (7) Number of rules used in the pre-translation (8) Number of target side sequences; here k times the number of sequences used in the pre",
        "translations that constructed ?",
        "(gap penalty) The rule weights required for (1) are relative frequencies normalized over all rules with the same left-hand side.",
        "In the same fashion the rule weights required for (2) are relative frequencies normalized over all rules with the same right-hand side.",
        "Additionally, rules that were extracted at most 10 times are discounted by multiplying the rule weight by 10?2.",
        "The lexical weights for (2) and (3) are obtained by multiplying the word translationsw(gi|ej) [respectively,w(ej |gi)] of lexically aligned words (gi, ej) accross (possibly discontiguous) target side sequences.9 Whenever a source word ej is aligned to multiple target words, we average over the word translations.10",
        "The computation of the language model estimates for (6) is adapted to score partial translations consisting of discontiguous units.",
        "We explain the details in Section 4.",
        "Finally, the count c of target sequences obtained in (7) is actually used as a score 1001?c.",
        "This discourages rules with many target sequences."
      ]
    },
    {
      "heading": "4 Decoding",
      "text": [
        "We implemented our model in the syntax-based component of the Moses open-source toolkit by Koehn et al. (2007) and Hoang et al. (2009).",
        "The standard Moses syntax-based decoder only handles SCFG rules; i.e, rules with contiguous components on the source and the target language side.",
        "Roughly speaking, SCFG rules are `MBOT rules with exactly one output tree.",
        "We thus had to extend the system to support our `MBOT rules, in which arbitrarily many output trees are allowed.",
        "The standard Moses syntax-based decoder uses a CYK+ chart parsing algorithm, in which each source sentence is parsed and contiguous spans are processed in a bottom-up fashion.",
        "A rule is applicable11 if the left-hand side of it matches the nonterminal assigned to the full span by the parser and the (non-)terminal assigned to each subspan.12 In order to speed up the decoding, cube pruning (Chiang, 2007) is applied to each chart cell in order to select the most likely hypotheses for subspans.",
        "The language model (LM) scoring is directly integrated into the cube pruning algorithm.",
        "Thus, LM estimates are available for all considered hypotheses.",
        "To accommodate `MBOT rules, we had to modify the Moses syntax-based decoder in several ways.",
        "First, the rule representation itself is adjusted to allow sequences of shallow output trees on the target side.",
        "Naturally, we also had to adjust hypothesis expansion and, most importantly, language model scoring inside the cube pruning algorithm.",
        "An overview of the modified pruning procedure is given in Algorithm 1.",
        "The most important modifications are hidden in lines 5 and 8.",
        "The expansion in Line 5 involves matching all nonterminal leaves in the rule as defined in Definition 2, which includes matching all leaf nonterminals in all (discontiguous) output trees.",
        "Because the output trees can remain discontiguous after hypothesis creation, LM scoring has to be done individually over all output trees.",
        "Algorithm 2 describes our LM scoring in detail.",
        "In it we use k strings w1, .",
        ".",
        ".",
        ", wk to collect the lexical information from the k output com11Note that our notion of applicable rules differs from the default in Moses.",
        "12Theoretically, this allows that the decoder ignores unary parser nonterminals, which could also disappear when we make our rules shallow; e.g., the parse tree left in the pre-translation of Figure 5 can be matched by a rule with left-hand side NP(Official, forecasts).",
        "Algorithm 1 Cube pruning with `MBOT rules Data structures:",
        "- r[i, j]: list of rules matching span e[i .",
        ".",
        ".",
        "j] - h[i, j]: hypotheses covering span e[i .",
        ".",
        ".",
        "j] - c[i, j]: cube of hypotheses covering span e[i .",
        ".",
        ".",
        "j] 1: for all `MBOT rules ?",
        "covering span e[i .",
        ".",
        ".",
        "j] do 2: Insert ?",
        "into r[i, j] 3: Sort r[i, j] 4: for all (l??",
        "r) ?",
        "r[i, j] do 5: Create h[i, j] by expanding all nonterminals in l with best scoring hypotheses for subspans 6: Add h[i, j] to c[i, j] 7: for all hypotheses h ?",
        "c[i, j] do 8: Estimate LM score for h // see Algorithm 2 9: Estimate remaining feature scores 10: Sort c[i, j] 11: Retrieve first ?",
        "elements from c[i, j] // we use ?",
        "= 103",
        "ponents (u1, .",
        ".",
        ".",
        ", uk) of a rule.",
        "These strings can later be rearranged in any order, so we LM-score all of them separately.",
        "Roughly speaking, we obtain wi by traversing ui depth-first left-to-right.",
        "If we meet a lexical element (terminal), then we add it to the end of wi.",
        "On the other hand, if we meet a nonterminal, then we have to consult the best pre-translation ?",
        "?",
        "= ?t?, (u?1, .",
        ".",
        ".",
        ", u?k?",
        ")?, which will contribute the subtree at this position.",
        "Suppose that u?j will be substituted into the nonterminal in question.",
        "Then we first LM-score the pre-translation ?",
        "?",
        "to obtain the string w?j corresponding to u?j .",
        "This string w?j is then appended to wi.",
        "Once all the strings are built, we score them using our 4-gram LM.",
        "The overall LM score for the pre-translation is obtained by multiplying the scores for w1, .",
        ".",
        ".",
        ", wk.",
        "Clearly, this treats w1, .",
        ".",
        ".",
        ", wk as k separate strings, although they eventually will be combined into a single string.",
        "Whenever such a concatenation happens, our LM scoring will automatically compute n-gram LM scores based on the concatenation, which in particular means that the LM scores get more accurate for larger spans.",
        "Finally, in the final rule only one component is allowed, which yields that the LM indeed scores the complete output sentence.",
        "translation involving a rule with two (discontiguous) target sequences (the construction of the pre-translation is illustrated in Figure 6).",
        "When processing the rule rooted at S, an LM estimate is computed by expanding all nonterminal leaves.",
        "In our case, these are NP, VAFIN, PP, and VVPP.",
        "However, the nodes VAFIN and VVPP are assembled from a (discontiguous) tree sequence.",
        "This means that those units have been considered as in",
        "- (u1, .",
        ".",
        ".",
        ", uk): right-hand side of a rule - (w1, .",
        ".",
        ".",
        ", wk): k strings all initially empty 1: score = 1 2: for all 1 ?",
        "i ?",
        "k do 3: for all leaves ` in ui (in lexicographic order) do 4: if ` is a terminal then 5: Append ` to wi 6: else 7: LM score the best hypothesis for the subspan 8: Expand wi by the corresponding w?j 9: score = score ?",
        "LM(wi) dependent until now.",
        "So far, the LM scorer could only score their associated unigrams.",
        "However,",
        "we also have their associated strings w?1 and w?2, which can now be used.",
        "Since VAFIN and VVPP now become parts of a single tree, we can perform LM scoring normally.",
        "Assembling the string we obtain Offizielle Prognosen sind von nur 3 % ausgegangen which is scored by the LM.",
        "Thus, we first score the 4-grams ?Offizielle Prognosen sind von?, then ?Prognosen sind von nur?, etc."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Setup",
      "text": [
        "The baseline system for our experiments is the syntax-based component of the Moses open-source toolkit of Koehn et al. (2007) and Hoang et al. (2009).",
        "We use linguistic syntactic annotation on both the source and the target language side (tree-to-tree).",
        "Our contrastive system is the `MBOT-based translation system presented here.",
        "We provide the system with a set of SCFG as well as `MBOT rules.",
        "We do not impose any maximal span restriction on either system.",
        "The compared systems are evaluated on the English-to-German13 news translation task of WMT 2009 (Callison-Burch et al., 2009).",
        "For both systems, the used training data is from the 4th version of the Europarl Corpus (Koehn, 2005) and the News Commentary corpus.",
        "Both translation models were trained with approximately"
      ]
    },
    {
      "heading": "1.5 million bilingual sentences after length-ratio",
      "text": [
        "filtering.",
        "The word alignments were generated by GIZA++ (Och and Ney, 2003) with the grow-diag-final-and heuristic (Koehn et al., 2005).",
        "The 13Note that our `MBOT-based system can be applied to any language pair as it involves no language-specific engineering.",
        "are statistically significant improvements over the Baseline (at confidence p < 0.05).",
        "English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations.",
        "Our German 4 gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus (Web-as-Corpus Consortium, 2008), whose generation is detailed in (Baroni et al., 2009).",
        "The weights ?m in the log-linear model were trained using minimum error rate training (Och, 2003) with the News 2009 development set.",
        "Both systems use glue-rules, which allow them to concatenate partial translations without performing any reordering."
      ]
    },
    {
      "heading": "5.2 Results",
      "text": [
        "We measured the overall translation quality with the help of 4-gram BLEU (Papineni et al., 2002), which was computed on tokenized and lower-cased data for both systems.",
        "The results of our evaluation are reported in Table 1.",
        "For comparison, we also report the results obtained by a system that utilizes parses only on the source side (Moses tree-to-string) with its standard features.",
        "We can observe from Table 1 that our `MBOT-based system outperforms the baseline.",
        "We obtain a BLEU score of 13.06, which is a gain of 0.46 BLEU points over the baseline.",
        "This improvement is statistically significant at confidence p < 0.05, which we computed using the pairwise bootstrap resampling technique of Koehn (2004).",
        "Our system is also better than the Moses tree-to-string system.",
        "However this improvement (0.34) is not statistically significant.",
        "In the next section, we confirm the result of the automatic evaluation through a manual examination of some translations generated by our system and the baseline.",
        "In Table 2, we report the number of `MBOT rules used by our system when decoding the test set.",
        "By lex we denote rules containing only lexical",
        "(lex: only lexical items; non-term: at least one nonterminal).",
        "items.",
        "The label non-term stands for rules containing at least one leaf nonterminal.",
        "The results show that approx.",
        "6% of all rules used by our `MBOT-system have discontiguous target sides.",
        "Furthermore, the reported numbers show that the system also uses rules in which lexical items are combined with nonterminals.",
        "Finally, Table 3 presents the number of rules with k target side components used during decoding."
      ]
    },
    {
      "heading": "5.3 Linguistic Analysis",
      "text": [
        "In this section we present linguistic evidence supporting the fact that the `MBOT-based system significantly outperforms the baseline.",
        "All examples are taken from the translation of the test set used for automatic evaluation.",
        "We show that when our system generates better translations, this is directly related to the use of `MBOT rules.",
        "Figures 8 and 9 show the ability of our system to correctly reorder multiple segments in the source sentence where the baseline translates those segments sequentially.",
        "An analysis of the generated derivations shows that our system produces the correct translation by taking advantage of rules with discontiguous units on target language side.",
        "The rules used in the presented derivations are displayed in Figures 10 and 11.",
        "In the first example (Figure 8), we begin by translating ?",
        "((smuggle)VB (eight projectiles)NP (into the kingdom)PP)VP?",
        "into the discontiguous sequence composed of (i) ?",
        "(acht geschosse)NP?",
        "; (ii) ?",
        "(in das ko?nigreich)PP?",
        "and (iii) ?(schmuggeln)VP?.",
        "In a second step we assemble all sequences in a rule with contiguous target language side and, at the same time, insert the word ?(zu)PTKZU?",
        "between ?",
        "(in das ko?nigreich)PP?",
        "and ?(schmuggeln)VP?.",
        "The second example (Figure 9) illustrates a more complex reordering.",
        "First, we trans",
        "late ?",
        "((again)ADV commented on (the problem of global warming)NP)VP?",
        "into the discontiguous sequence composed of (i) ?",
        "(das problem der globalen erwa?rmung)NP?",
        "; (ii) ?(wieder)ADV?",
        "and (iii) ?(kommentiert)VPP?.",
        "In a second step, we translate the auxiliary ?(has)VBZ?",
        "by inserting ?(hat)VAFIN?",
        "into the sequence.",
        "We thus obtain, for the input segment ?",
        "((has)VBZ (again)ADV commented on (the problem of global warming)NP)VP?, the sequence (i) ?",
        "(das problem der globalen erwa?rmung)NP?",
        "; (ii) ?(hat)VAFIN?",
        "; (iii) ?(wieder)ADV?",
        "; (iv) ?(kommentiert)VVPP?.",
        "In a last step, the constituent ?",
        "(president va?clav klaus)NP?",
        "is inserted between the discontiguous units ?(hat)VAFIN?",
        "and ?(wieder)ADV?",
        "to form the contiguous sequence ?",
        "((das problem der globalen erwa?rmung)NP (hat)VAFIN (pra?sident va?clav klaus)NP (wieder)ADV (kommentiert)VVPP)TOP?.",
        "Figures 12 and 13 show examples where our system generates complex words in the target language out of a simple source language word.",
        "Again, an analysis of the generated derivation shows that `MBOT takes advantage of rules having several target side components.",
        "Examples of such rules are given in Figure 14.",
        "Through its ability to use these discontiguous rules, our system correctly translates into reflexive or particle verbs such as ?konzentriert sich?",
        "(for the English ?focuses?)",
        "or ?besteht darauf ?",
        "(for the English ?insist?).",
        "Another phenomenon well handled by our system are relative pronouns.",
        "Pronouns such as ?that?",
        "or ?whose?",
        "are systematically translated",
        ".",
        ".",
        ".",
        "geplant hatten 8 geschosse in das ko?nigreich zu schmuggeln .",
        ".",
        ".",
        "had planned to smuggle 8 projectiles into the kingdom .",
        ".",
        ".",
        "vorhatten zu schmuggeln 8 geschosse in das ko?nigreich"
      ]
    },
    {
      "heading": "6 Conclusion and Future Work",
      "text": [
        "We demonstrated that our `MBOT-based machine translation system beats a standard tree-to-tree system (Moses tree-to-tree) on the WMT 2009 translation task English ?",
        "German.",
        "To achieve this we implemented the formal model as described in Section 2 inside the Moses machine translation toolkit.",
        "Several modifications were necessary to obtain a working system.",
        "We publicly release all our developed software and our complete tool-chain to allow independent experiments and evaluation.",
        "This includes our `MBOT decoder",
        "clause/reflexive pronoun presented in Section 4 and a separate C++ module that we use for rule extraction (see Section 3).",
        "Besides the automatic evaluation, we also performed a small manual analysis of obtained translations and showcased some examples (see Section 5.3).",
        "We argue that our `MBOT approach can adequately handle discontiguous phrases, which occur frequently in German.",
        "Other languages that exhibit such phenomena include Czech, Dutch, Russian, and Polish.",
        "Thus, we hope that our system can also successfully be applied for other language pairs, which we plan to pursue as well.",
        "In other future work, we want to investigate full backwards application of `MBOT rules, which would be more suitable for the converse translation direction German?",
        "English.",
        "The current independent LM scoring of components has some negative side-effects that we plan to circumvent with the use of lazy LM scoring."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "The authors thank Alexander Fraser for his ongoing support and advice.",
        "All authors were financially supported by the German Research Foundation (DFG) grant MA 4959 / 1-1."
      ]
    }
  ]
}
