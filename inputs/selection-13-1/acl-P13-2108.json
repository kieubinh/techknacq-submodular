{
  "info": {
    "authors": [
      "Greg Coppola",
      "Mark Steedman"
    ],
    "book": "ACL",
    "id": "acl-P13-2108",
    "title": "The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing",
    "url": "https://aclweb.org/anthology/P13-2108",
    "year": 2013
  },
  "references": [
    "acl-C96-1058",
    "acl-D07-1101",
    "acl-D10-1002",
    "acl-D10-1069",
    "acl-D12-1030",
    "acl-D12-1091",
    "acl-E06-1011",
    "acl-J93-2004",
    "acl-N07-1051",
    "acl-N10-1069",
    "acl-N10-1095",
    "acl-P03-1021",
    "acl-P05-1012",
    "acl-P05-1022",
    "acl-P06-1043",
    "acl-P06-1055",
    "acl-P07-1019",
    "acl-P08-1067",
    "acl-P08-1109",
    "acl-P10-1001",
    "acl-P11-1069",
    "acl-P11-2033",
    "acl-P89-1018",
    "acl-P97-1003",
    "acl-W01-0521",
    "acl-W02-1001",
    "acl-W04-3201"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Higher-order dependency features are known to improve dependency parser accuracy.",
        "We investigate the incorporation of such features into a cube decoding phrase-structure parser.",
        "We find considerable gains in accuracy on the range of standard metrics.",
        "What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ).",
        "This suggests that higher-order dependency features are not simply over-fitting the training material."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.",
        "The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010; Zhang & Nivre, 2011; Zhang & McDonald, 2012).",
        "This finding suggests that the same benefits might be observed in phrase-structure parsing.",
        "But, this is not necessarily implied.",
        "Phrase-structure parsers are generally stronger than dependency parsers (Petrov et al., 2010; Petrov & McDonald, 2012), and make use of more kinds of information.",
        "So, it might be that the information modelled by higher-order dependency features adds less of a benefit in the phrase-structure case.",
        "1Examples of first-order and higher-order dependency features are given in ?3.2.",
        "To investigate this issue, we experiment using Huang's (2008) cube decoding algorithm.",
        "This algorithm allows structured prediction with non-local features, as discussed in ?2.",
        "Collins's (1997) strategy of expanding the phrase-structure parser's dynamic program to incorporate head-modifier dependency information would not scale to the complex kinds of dependencies we will consider.",
        "Using Huang's algorithm, we can indeed incorporate arbitrary types of dependency feature, using a single, simple dynamic program.",
        "Compared to the baseline, non-local feature set of Collins (2000) and Charniak & Johnson (2005), we find that higher-order dependencies do in fact tend to improve performance significantly on both dependency and constituency accuracy metrics.",
        "Our most interesting finding, though, is that higher-order dependency features show a consistent and unambiguous contribution to the dependency accuracy, both labelled and unlabelled, of our phrase-structure parsers on out-of-domain tests (which means, here, trained on WSJ, but tested on BROWN).",
        "In fact, the gains are even stronger on out-of-domain tests than on in-domain tests.",
        "One might have thought that higher-order dependencies, being rather specific by nature, would tend to pick out only very rare events, and so only serve to over-fit the training material, but this is not what we find.",
        "We speculate as to what this might mean in ?5.2.",
        "The cube decoding paradigm requires a first-stage parser to prune the output space.",
        "For this, we use the generative parser of Petrov et al. (2006).",
        "We can use this parser's model score as a feature in our discriminative model at no additional cost.",
        "However, doing so conflates the contribution to accuracy of the generative model, on the one hand, and the discriminatively trained, hand",
        "written, features, on the other.",
        "Future systems might use the same or a similar feature set to ours, but in an architecture that does not include any generative parser.",
        "On the other hand, some systems might indeed incorporate this generative model's score.",
        "So, we need to know exactly what the generative model is contributing to the accuracy of a generative-discriminative model combination.",
        "Thus, we conduct experiments in sets: in some cases the generative model score is used, and in others it is not used.",
        "Compared to the faster and more psychologically plausible shift-reduce parsers (Zhang & Nivre, 2011; Zhang & Clark, 2011), cube decoding is a computationally expensive method.",
        "But, cube decoding provides a relatively exact environment with which to compare different feature sets, has close connections with modern phrase-based machine translation methods (Huang & Chiang, 2007), and produces very accurate parsers.",
        "In some cases, one might want to use a slower, but more accurate, parser during the training stage of a semi-supervised parser training strategy.",
        "For example, Petrov et al. (2010) have shown that a fast parser (Nivre et al., 2007) can be profitably trained from the output of a slower but more accurate one (Petrov et al., 2006), in a strategy they call uptrain-ing.",
        "We make the source code for these experiments"
      ]
    },
    {
      "heading": "2 Phrase-Structure Parsing with Non-Local Features 2.1 Non-Local Features",
      "text": [
        "To decode using exact dynamic programming (i.e., CKY), one must restrict oneself to the use of only local features.",
        "Local features are those that factor according to the individual rule productions of the parse.",
        "For example, a feature indicating the presence of the rule S ?",
        "NP VP is local.3 But, a feature that indicates that the head word of this S is, e.g., joined, is non-local, because the head word of a phrase cannot be determined by looking at a single rule production.",
        "To find a phrase's head word (or tag), we must recursively find the",
        "are constant across hypothesized parses, and words can be referred to by their position with respect to a given rule production.",
        "See Huang (2008) for more details.",
        "head phrase of each local rule production, until we reach a terminal node (or tag node).",
        "This recursion would not be allowed in standard CKY.",
        "Many discriminative parsers have used only local features (Taskar et al., 2004; Turian et al., 2007; Finkel et al., 2008).",
        "However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance.",
        "And, our desire to make heavy use of headword dependency relations necessitates the use of non-local features."
      ]
    },
    {
      "heading": "2.2 Cube Decoding",
      "text": [
        "While the use of non-local features destroys the ability to do exact search, we can still do inexact search using Huang's (2008) cube decoding algorithm.4 A tractable first-stage parser prunes the space of possible parses, and outputs a forest, which is a set of rule production instances that can be used to make a parse for the given sentence, and which is significantly pruned compared to the entire space allowed by the grammar.",
        "The size of this forest is at most cubic in the length of the sentence (Billot & Lang, 1989), but implicitly represents exponentially many parses.",
        "To decode, we fix an beam width of k (an integer).",
        "Then, when parsing, we visit each node n in the same bottom-up order we would use for Viterbi decoding, and compute a list of the top k parses to n, according to a global linear model (Collins, 2002), using the trees that have survived the beam at earlier nodes."
      ]
    },
    {
      "heading": "2.3 The First-Stage Parser",
      "text": [
        "As noted, we require a first-stage parser to prune the search space.5 As a by-product of this pruning procedure, we are able to use the model score of the first-stage parser as a feature in our ultimate model at no additional cost.",
        "As a first-stage parser, we use Huang et al's (2010) implementation of the LA-PCFG parser of Petrov et al. (2006), which uses a generative, latent-variable model."
      ]
    },
    {
      "heading": "3 Features",
      "text": []
    },
    {
      "heading": "3.1 Phrase-Structure Features",
      "text": [
        "Our phrase-structure feature set is taken from Collins (2000), Charniak & Johnson (2005), and",
        "Huang (2008).",
        "Some features are omitted, with choices made based on the ablation studies of Johnson & Ural (2010).",
        "This feature set, which we call ?phrase, contains the following, mostly non-local, features, which are described and depicted",
        "in Charniak & Johnson (2005), Huang (2008), and Johnson & Ural (2010): ?",
        "CoPar The depth (number of levels) of parallelism between adjacent conjuncts ?",
        "CoParLen The difference in length between adjacent conjuncts ?",
        "Edges The words or (part-of-speech) tags on the outside and inside edges of a given XP6 ?",
        "NGrams Sub-parts of a given rule production ?",
        "NGramTree An n-gram of the input sentence, or the tags, along with the minimal tree containing that n-gram ?",
        "HeadTree A sub-tree containing the path from a word to its maximal projection, along with all siblings of all nodes in that path ?",
        "Heads Head-modifier bigrams ?",
        "Rule A single rule production ?",
        "Tag The tag of a given word ?",
        "Word The tag of and first XP above a word ?",
        "WProj The tag of and maximal projection of a word",
        "Heads is a first-order dependency feature."
      ]
    },
    {
      "heading": "3.2 Dependency Parsing Features",
      "text": [
        "McDonald et al. (2005) showed that chart-based dependency parsing, based on Eisner's (1996) algorithm, could be successfully approached in a discriminative framework.",
        "In this earliest work, each feature function could only refer to a single, bigram head-modifier relationship, e.g., Modifier, below.",
        "Subsequent work (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including tri-gram and 4-gram relationships, e.g., all features apart from Modifier, below.",
        "With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order (cf. Zhang & McDonald (2012)).",
        "Our dependency feature set, which we call ?deps, contains:",
        "?",
        "Modifier head and modifier 6The tags outside of a given XP are approximated using the marginally most likely tags given the parse.",
        "?",
        "Sibling head, modifier m, and m's nearest inner sibling ?",
        "Grandchild head, modifier m, and one of m's modifiers ?",
        "Sibling+Grandchild head, modifier m, m's nearest inner sibling, and one of m's modifiers ?",
        "Grandchild+Grandsibling head, modifier m, one of m's modifiers g, and g's inner sibling",
        "These features are insensitive to arc labels in the present experiments, but future work will incorporate arc labels.",
        "Each feature class contains more and less lexicalized versions."
      ]
    },
    {
      "heading": "3.3 Generative Model Score Feature",
      "text": [
        "Finally, we have a feature set, ?gen, containing only one feature function.",
        "This feature maps a parse to the logarithm of the MAX-RULEPRODUCT score of that parse according to the LA-PCFG parsing model, which is trained separately.",
        "This score has the character of a conditional likelihood for the parse (see Petrov & Klein (2007b))."
      ]
    },
    {
      "heading": "4 Training",
      "text": [
        "We have two feature sets ?phrase and ?deps, for which we fix weights using parallel stochastic optimization of a structured SVM objective (Collins, 2002; Taskar et al., 2004; Crammer et al., 2006; Martins et al., 2010; McDonald et al., 2010).",
        "To the single feature in the set ?gen (i.e. the generative model score), we give the weight 1.",
        "The combined models, ?phrase+deps, ?phrase+gen, and ?phrase+deps+gen, are then model combinations of the first three.",
        "The combination weights for these combinations are obtained using Och's (2003) Minimum Error-Rate Training (MERT).",
        "The MERT stage helps to avoid feature under-training (Sutton et al., 2005), and avoids the problem of scaling involved in a model that contains mostly boolean features, but one, real-valued, log-scale feature.",
        "Training is conducted in three stages (SVM, MERT, SVM), so that there is no influence of any data outside the given training set (WSJ2-21) on the combination weights."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Methods",
      "text": [
        "All models are trained on WSJ2-21, with WSJ22 used to pick the stopping iteration for online",
        "domain) and the BROWN test set (out-of-domain).",
        "G abbreviates generative, D abbreviates discriminative, and G+D a combination.",
        "Some cells are empty because ?deps features are only sensitive to unlabelled dependencies.",
        "Best results in D and G+D conditions appear in bold face.",
        "optimization, as is standard.",
        "The test sets are WSJ23 (in-domain test set), and BROWN9 (out-of-domain test set) from the Penn Treebank (Marcus et al., 1993).7 We evaluate using harmonic mean between labelled bracket recall and precision (EVALB F1), unlabelled dependency accuracy (UAS), and labelled dependency accuracy (LAS).",
        "Dependencies are extracted from full output trees using the algorithm of de Marneffe & Manning (2008).",
        "We chose this dependency extractor, firstly, because it is natively meant to be run on the output of phrase-structure parsers, rather than on gold trees with function tags and traces still present, as is, e.g., the Penn-Converter of Johans-son & Nugues (2007).",
        "Also, this is the extractor that was used in a recent shared task (Petrov & McDonald, 2012).",
        "We use EVALB and eval.pl to calculate scores.",
        "For hypothesis testing, we used the paired bootstrap test recently empirically evaluated in the context of NLP by Berg-Kirkpatrick et al. (2012).",
        "This 7Following Gildea (2001), the BROWN test set is usually divided into 10 parts.",
        "If we start indexing at 0, then the last (test) section has index 9.",
        "We received the BROWN data splits from David McClosky, p.c.",
        "involves drawing b subsamples of size n with replacement from the test set in question, and checking relative performance of the models on the sub-sample (see the reference).",
        "We use b = 106 and n = 500 in all tests."
      ]
    },
    {
      "heading": "5.2 Results",
      "text": [
        "The performance of the models is shown in Table 1, and Table 2 depicts the results of significance tests of differences between key model pairs.",
        "We find that adding in the higher-order dependency feature set, ?deps, makes a statistically significant improvement in accuracy on most metrics, in most conditions.",
        "On the in-domain WSJ test set, we find that ?phrase+deps is significantly better than either of its component parts on all metrics.",
        "But, ?phrase+deps+gen is significantly better than ?phrase+gen only on F1, but not on UAS or LAS.",
        "However, on the out-of-domain BROWN tests, we find that adding ?deps always adds considerably, and in a statistically significant way, to both LAS and UAS.",
        "That is, not only is ?phrase+deps better at dependency recovery than its component parts, but ?phrase+deps+gen is also considerably bet",
        "ter on dependency recovery than ?phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008).",
        "This result is perhaps counter-intuitive, in the sense that one might have supposed that higher-order dependency features, being highly specific by nature, might only have only served to over-fit the training material.",
        "However, this result shows otherwise.",
        "Note that the dependency features include various levels of lex-icalization.",
        "It might be that the more unlexicalized features capture something about the structure of correct parses, that transfers well out-of-domain.",
        "Future work should investigate this.",
        "And, it of course remains to be seen how this result will transfer to other train-test domain pairs.",
        "To our knowledge, this is the first work to specifically separate the role of the generative model feature from the other features of Collins (2000) and Charniak & Johnson (2005).",
        "We note that, even without the ?gen feature, the discriminative parsing models are very strong, but adding ?gen nevertheless yields considerable gains.",
        "Thus, while a fully discriminative model, perhaps implemented using a shift-reduce algorithm, can be expected to do very well, if the best accuracy is necessary (e.g., in a semi-supervised training strategy), it still seems to pay to use the generative-discriminative model combination.",
        "Note that the LAS scores of our models without ?gen are relatively weak.",
        "This is presumably largely because our dependency features are, at present, not sensitive to arc labels, so our results probably underestimate the capability of our general framework with respect to labelled dependency recovery.",
        "Table 3 compares our work with Huang's (2008).",
        "Note that our model ?phrase+gen uses essentially the same features as Huang (2008), so the fact that our ?phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy.",
        "Also, our ?phrase+deps model is as accurate as Huang?s, without even using the generative model score feature.",
        "Table 4 compares our work to McClosky et al's (2006) domain adaptation work with the Charniak & Johnson (2005) parser.",
        "Their three models shown have been trained on: i) the WSJ (supervised, out-of-domain), ii) the WSJ plus 2.5 million sentences of automatically labelled NANC newswire text (semi-supervised, out-of-domain), and iii) the BROWN corpus (supervised, in-domain).",
        "We test",
        "?phrase+deps+gen, on BROWN, with the Charniak & Johnson (2005) parser, denoted CJ, as reported in McClosky et al. (2006).",
        "Underline indicates best trained on WSJ, bold face indicates best overall.",
        "on BROWN.",
        "We see that our best (WSJ-trained) model is over 2% more accurate (absolute F1 difference) than the Charniak & Johnson (2005) parser trained on the same data.",
        "In fact, our best model is nearly as good as McClosky et al's (2006) self-trained, semi-supervised model.",
        "Of course, the self-training strategy is orthogonal to the improvements we have made."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have shown that the addition of higher-order dependency features into a cube decoding phase-structure parser leads to statistically significant gains in accuracy.",
        "The most interesting finding is that these gains are clearly observed on out-of-domain tests.",
        "This seems to imply that higher-order dependency features do not merely over-fit the training material.",
        "Future work should look at other train-test domain pairs, as well as look at exactly which higher-order dependency features are most important to out-of-domain accuracy."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported by the Scottish Informatics and Computer Science Alliance, The University of Edinburgh's School of Informatics, and ERC Advanced Fellowship 249520 GRAMPLUS.",
        "We thank Zhongqiang Huang for his extensive help in getting started with his LA-PCFG parser."
      ]
    }
  ]
}
