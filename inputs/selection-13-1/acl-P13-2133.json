{
  "info": {
    "authors": [
      "Dhouha Bouamor",
      "Nasredine Semmar",
      "Pierre Zweigenbaum"
    ],
    "book": "ACL",
    "id": "acl-P13-2133",
    "title": "Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora",
    "url": "https://aclweb.org/anthology/P13-2133",
    "year": 2013
  },
  "references": [
    "acl-C02-2020",
    "acl-C10-1070",
    "acl-C10-1073",
    "acl-P04-1067",
    "acl-P94-1019",
    "acl-P95-1050",
    "acl-W11-1205"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora.",
        "We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors.",
        "On two specialized French-English comparable corpora, empirical experimental results show that our method improves the results obtained by two state-of-the-art approaches."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003).",
        "The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages.",
        "The so-called standard approach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words.",
        "Each element in the context vector of a source or target word represents its association with a word which occurs within a window of N words.",
        "To enable the comparison of source and target vectors, words in the source vectors are translated into the target language using an existing bilingual dictionary.",
        "The core of the standard approach is the bilingual dictionary.",
        "Its use is problematic when a word has several translations, whether they are synonymous or polysemous.",
        "For instance, the French word action can be translated into English as share, stock, lawsuit or deed.",
        "In such cases, it is difficult to identify in flat resources like bilingual dictionaries which translations are most relevant.",
        "The standard approach considers all available translations and gives them the same importance in the resulting translated context vectors independently of the domain of interest and word ambiguity.",
        "Thus, in the financial domain, translating action into deed or lawsuit would introduce noise in context vectors.",
        "In this paper, we present a novel approach that addresses the word polysemy problem neglected in the standard approach.",
        "We introduce a Word Sense Disambiguation (WSD) process that identifies the translations of polysemous words that are more likely to give the best representation of context vectors in the target language.",
        "For this purpose, we employ five WordNet-based semantic similarity and relatedness measures and use a data fusion method that merges the results obtained by each measure.",
        "We test our approach on two specialized French-English comparable corpora (financial and medical) and report improved results compared to two state-of-the-art approaches."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach.",
        "In order to improve the results of this approach, recent researches based on the assumption that more the context vectors are representative, better is the bilingual lexicon extraction were conducted.",
        "In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic",
        "tionary to translate context vectors.",
        "Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary.",
        "(Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated.",
        "Gaussier et al. (2004) attempted to solve the problem of word ambiguities in the source and target languages.",
        "They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis.",
        "The best results, with an improvement of the F-Measure (+0.02 at Top20) were reported for a mixed method.",
        "Recently, (Morin and Prochas-son, 2011) proceed as the standard approach but weigh the different translations according to their frequency in the target corpus.",
        "Here, we propose a method that differs from Gaussier et al. (2004) in this way: If they focus on words ambiguities on source and target languages, we thought that it would be sufficient to disambiguate only translated source context vectors."
      ]
    },
    {
      "heading": "3 Context Vector Disambiguation",
      "text": []
    },
    {
      "heading": "3.1 Semantic similarity measures",
      "text": [
        "A large number of WSD techniques were proposed in the literature.",
        "The most widely used ones are those that compute semantic similarity1 with the help of WordNet.",
        "WordNet has been used in many tasks relying on word-based similarity, including document (Hwang et al., 2011) and image (Cho et al., 2007; Choi et al., 2012) retrieval systems.",
        "In this work, we use it to derive a semantic similarity between lexical units within the same context vector.",
        "To the best of our knowledge, this is the first application of WordNet to bilingual lexicon extraction from comparable corpora.",
        "Among semantic similarity measures using WordNet, we distinguish: (1) measures based on path length which simply counts the distance between two words in the WordNet taxonomy, (2) measures relying on information content in which a semantically annotated corpus is needed to compute frequencies of words to be compared and (3) the ones using gloss overlap which are designed to compute semantic relatedness.",
        "In this work, we use five similarity measures and compare their performances.",
        "These measures include three 1For consiseness, we often use ?semantic similarity?",
        "to refer collectively to both similarity and relatedness.",
        "path-based semantic similarity measures denoted PATH,WUP (Wu and Palmer, 1994) and LEA-COCK (Leacock and Chodorow, 1998).",
        "PATH is a baseline that is equal to the inverse of the shortest path between two words.",
        "WUP finds the depth of the least common subsumer of the words, and scales that by the sum of the depths of individual words.",
        "The depth of a word is its distance to the root node.",
        "LEACOCK finds the shortest path between two words, and scales that by the maximum path length found in the is?a hierarchy in which they occur.",
        "Path length measures have the advantage of being independent of corpus statistics, and therefor uninfluenced by sparse data.",
        "Since semantic relatedness is considered to be more general than semantic similarity, we also use two relatedness measures: LESK (Banerjee and Pedersen, 2002) and VECTOR (Patwardhan, 2003).",
        "LESK finds overlaps between the glosses of word pairs, as well as words?",
        "hyponyms.",
        "VECTOR creates a co-occurrence matrix for each gloss token.",
        "Each gloss is then represented as a vector that averages token co-occurrences."
      ]
    },
    {
      "heading": "3.2 Disambiguation process",
      "text": [
        "Once translated into the target language, the context vectors disambiguation process intervenes.",
        "This process operates locally on each context vector and aims at finding the most prominent translations of polysemous words.",
        "For this purpose, we use monosemic words as a seed set of disambiguated words to infer the polysemous word's translations senses.",
        "We hypothesize that a word is monosemic if it is associated to only one entry in the bilingual dictionary.",
        "We checked this assumption by probing monosemic entries of the bilingual dictionary against WordNet and found that 95% of the entries are monosemic in both resources.",
        "According to the above-described semantic similarity measures, a similarity value SimV alue is derived between all the translations provided for each pol-ysemous word by the bilingual dictionary and all monosemic words appearing within the same context vector.",
        "In practice, since a word can belong to more than one synset2 in WordNet, the semantic similarity between two words w1 and w2 is defined as the maximum of SimV alue between the synset or the synsets that include the synsets(w1) and",
        "synsets(w2) according to the following equation:",
        "Then, to identify the most prominent translations of each polysemous unit wp, an average similarity is computed for each translation wjp of wp:",
        "where N is the total number of monosemic words in each context vector and SemSim is the similarity value of wjp and the ith monosemic word.",
        "Hence, according to average similarity values Ave Sim(wjp), we obtain for each polysemous word wp an ordered list of translations w1p .",
        ".",
        ".",
        "wnp ."
      ]
    },
    {
      "heading": "4 Experiments and Results",
      "text": []
    },
    {
      "heading": "4.1 Resources and Experimental Setup",
      "text": [
        "We conducted our experiments on two French-English comparable corpora specialized on the corporate finance and the breast cancer sub-domains.",
        "Both corpora were extracted from Wikipedia3.",
        "We consider the domain topic in the source language (for instance cancer du sein [breast cancer]) as a query to Wikipedia and extract all its subtopics (i.e., subcategories in Wikipedia) to construct a domain-specific categories tree.",
        "Then we collected all articles belonging to one of these categories and used inter-language links to build the comparable corpus.",
        "Both corpora have been normalized through the following linguistic preprocessing steps: tokeni-sation, part-of-speech tagging, lemmatisation and function words removal.",
        "The resulting corpora4 sizes as well as their polysemy rate PR are given in Table 1.",
        "The polysemy rate indicates how much words in the comparable corpora are associated to more than one translation in the seed bilingual dictionary.",
        "The dictionary consists of an in-house bilingual dictionary which contains about 120,000 entries belonging to the general language with an average of 7 translations per entry.",
        "In bilingual terminology extraction from comparable corpora, a reference list is required to evaluate the performance of the alignment.",
        "Such lists are often composed of about 100 single",
        "words and polysemy rates (PR) associated to each corpus terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002).",
        "Here, we created two reference lists5 for the corporate finance and the breast cancer sub-domains.",
        "The first list is composed of 125 single terms extracted from the glossary of bilingual micro-finance terms6.",
        "The second list contains 79 terms extracted from the French-English MESH and the UMLS thesauri7.",
        "Note that reference terms pairs appear more than five times in each part of both comparable corpora.",
        "Three other parameters need to be set up, namely the window size, the association measure and the similarity measure.",
        "We followed (Laroche and Langlais, 2010) to define these parameters.",
        "They carried out a complete study of the influence of these parameters on the bilingual alignment.",
        "The context vectors were defined by computing the Discounted Log-Odds Ratio (equation 3) between words occurring in the same context window of size 7.",
        "where Oij are the cells of the 2 ?",
        "2 contingency matrix of a token s co-occurring with the term S within a given window size.",
        "As similarity measure, we chose to use the cosine measure."
      ]
    },
    {
      "heading": "4.2 Results of bilingual lexicon extraction",
      "text": [
        "To evaluate the performance of our approach, we used both the standard approach (SA) and the approach proposed by (Morin and Prochasson, 2011) (henceforth MP11) as baselines.",
        "The experiments were performed with respect to the five semantic similarity measures described in section 3.1.",
        "Each measure provides, for each polysemous word, a ranked list of translations.",
        "A question that arises here is whether we should introduce only the top-ranked translation into the context vector or consider a larger number of translations, mainly when a translation list contains synonyms.",
        "For this",
        "column, italics shows best single similarity measure, bold shows best result.",
        "Underline shows best result overall.",
        "reason, we take into account in our experiments different numbers of translations, noted WN-Ti, ranging from the pivot translation (i = 1) to the seventh word in the translation list.",
        "This choice is motivated by the fact that words in both corpora have on average 7 translations in the bilingual dictionary.",
        "Both baseline systems use all translations associated to each entry in the bilingual dictionary.",
        "The only difference is that in MP11 translations are weighted according to their frequency in the target corpus.",
        "The results of different works focusing on bilingual lexicon extraction from comparable corpora are evaluated on the number of correct candidates found in the first N first candidates output by the alignment process (the TopN ).",
        "Here, we use the Top20 F-measure as evaluation metric.",
        "The results obtained for the corporate finance corpus are presented in Table 2a.",
        "The first notable observation is that disambiguating context vectors using semantic similarity measures outperforms the SA.",
        "The highest F-measure is reported by VECTOR.",
        "Using the top two words (WN-T2) in context vectors increases the F-measure from 0.172 to 0.310.",
        "However, compared to MP11, no improvement is achieved.",
        "Concerning the breast cancer corpus, Table 2b shows improvements in most cases over both the SA and MP11.",
        "The maximum F-measure was obtained by LESK when for each polysemous word up to four translations (WN-T4) are considered in context vectors.",
        "This method achieves an improvement of respectively +0.097 and +0.037% over SA and MP11.",
        "Each of the tested 5 semantic similarity measures provides a different view of how to rank the translations of a given test word.",
        "Combining the obtained ranked lists should reinforce the confidence in consensus translations, while decreasing the confidence in non-consensus translations.",
        "We have therefore tested their combination.",
        "For this, we used a voting method, and chose one in the Condorcet family the Condorcet data fusion method.",
        "This method was widely used to combine document retrieval results from information retrieval systems (Montague and Aslam, 2002; Nu-ray and Can, 2006).",
        "It is a single-winner election method that ranks the candidates in order of preference.",
        "It is a pairwise voting, i.e. it compares every possible pair of candidates to decide the preference of them.",
        "A matrix can be used to present the competition process.",
        "Every candidate appears in the matrix as a row and a column as well.",
        "If there are m candidates, then we need m2 elements in the matrix in total.",
        "Initially 0 is written to all the elements.",
        "If di is preferred to dj , then we add 1 to the element at row i and column j (aij).",
        "The pro",
        "cess is repeated until all the ballots are processed.",
        "For every element aij , if aij > m/2 , then di beats dj ; if aij < m/2, then dj beats di; otherwise (aij = m/2), there is a draw between di and dj .",
        "The total score of each candidate is quantified by summing the raw scores it obtains in all pairwise competitions.",
        "Finally the ranking is achievable based on the total scores calculated.",
        "Here, we view the ranking of the extraction results from different similarity measures as a special instance of the voting problem where the Top20 extraction results correspond to candidates and different semantic similarity measures are the voters.",
        "The combination method referred to as CONDORCETMerge outperformed all the others (see Tables 2a and 2b): (1) individual measures, (2) SA, and (3) MP11.",
        "Even though the two corpora are fairly different (subject and polysemy rate), the optimal results are obtained when considering up to two most similar translations in context vectors.",
        "This behavior shows that the fusion method is robust to domain change.",
        "The addition of supplementary translations, which are probably noisy in the given domain, degrades the overall results.",
        "The F-measure gains with respect to SA are +0.207 for corporate finance and +0.121 for the breast cancer corpus.",
        "More interestingly, our approach outperforms MP11, showing that the role of disambiguation is more important than that of feature weighting."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We presented in this paper a novel method that extends the standard approach used for bilingual lexicon extraction.",
        "This method disambiguates polysemous words in context vectors by selecting only the most relevant translations.",
        "Five semantic similarity and relatedness measures were used for this purpose.",
        "Experiments conducted on two specialized comparable corpora indicate that the combination of similarity metrics leads to a better performance than two state-of-the-art approaches.",
        "This shows that the ambiguity present in specialized comparable corpora hampers bilingual lexicon extraction, and that methods such as the one introduced here are needed.",
        "The obtained results are very encouraging and can be improved in a number of ways.",
        "First, we plan to mine much larger specialized comparable corpora and focus on their quality (Li and Gaussier, 2010).",
        "We also plan to test our method on bilingual lexicon extraction from general-domain corpora, where ambiguity is generally higher and disambiguation methods should be all the more needed."
      ]
    }
  ]
}
