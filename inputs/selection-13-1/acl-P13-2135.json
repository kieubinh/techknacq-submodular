{
  "info": {
    "authors": [
      "José G.C. Souza",
      "Miquel Esplà-Gomis",
      "Marco Turchi",
      "Matteo Negri"
    ],
    "book": "ACL",
    "id": "acl-P13-2135",
    "title": "Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks",
    "url": "https://aclweb.org/anthology/P13-2135",
    "year": 2013
  },
  "references": [
    "acl-C04-1046",
    "acl-C96-2141",
    "acl-D11-1062",
    "acl-J03-1002",
    "acl-J93-2003",
    "acl-N10-1045",
    "acl-P12-2024",
    "acl-S12-1053",
    "acl-S12-1064",
    "acl-S12-1102",
    "acl-S13-2005",
    "acl-W08-0509",
    "acl-W12-3102",
    "acl-W12-3122"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The use of automatic word alignment to capture sentence-level semantic relations is common to a number of cross-lingual NLP applications.",
        "Despite its proved usefulness, however, word alignment information is typically considered from a quantitative point of view (e.g. the number of alignments), disregarding qualitative aspects (the importance of aligned terms).",
        "In this paper we demonstrate that integrating qualitative information can bring significant performance improvements with negligible impact on system complexity.",
        "Focusing on the cross-lingual textual entailment task, we contribute with a novel method that: i) significantly outperforms the state of the art, and ii) is portable, with limited loss in performance, to language pairs where training data are not available."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Meaning representation, comparison and projection across sentences are major challenges for a variety of cross-lingual applications.",
        "So far, despite the relevance of the problem, research on multilingual applications has either circumvented the issue, or proposed partial solutions.",
        "When possible, the typical approach builds on the reduction to a monolingual task, burdening the process with dependencies from machine translation (MT) components.",
        "For instance, in cross-lingual question answering and cross-lingual textual entailment (CLTE), intermediate MT steps are respectively performed to ease answer retrieval/presentation (Parton, 2012; Tanev et al., 2006) and semantic inference (Mehdad et al., 2010).",
        "Direct solutions that avoid such pivoting strategies typically exploit similarity measures that rely on bag-of-words representations.",
        "As an example, most supervised approaches to MT quality estimation (Blatz et al., 2003; Callison-Burch et al., 2012) and CLTE (Wa?schle and Fendrich, 2012) include features that consider the amount of equivalent terms that are found in the input sentence pairs.",
        "Such simplification, however, disregards the fact that semantic equivalence is not only proportional to the number of equivalent terms, but also to their importance.",
        "In other words, instead of checking what of a given sentence can be found in the other, current approaches limit the analysis to the amount of lexical elements they share, under the rough assumption that the more the better.",
        "In this paper we argue that:",
        "(1) Considering qualitative aspects of word alignments to identify sentence-level semantic relations can bring significant performance improvements in cross-lingual NLP tasks.",
        "(2) Shallow linguistic processing techniques (of",
        "ten a constraint in real cross-lingual scenarios due to limited resources availability) can be leveraged to set up portable solutions that still outperform current bag-of-words methods.",
        "To support our claims we experiment with the CLTE task, which allows us to perform exhaustive comparative experiments due to the availability of comparable benchmarks for different lan",
        "guage pairs.",
        "In the remainder of the paper, we: (1) Prove the effectiveness of our method over",
        "datasets for four language combinations; (2) Assess the portability of our models across languages in different testing conditions."
      ]
    },
    {
      "heading": "2 Objectives and Method",
      "text": [
        "We propose a supervised learning approach for identifying and classifying semantic relations between two sentences T1 and T2 written in different languages.",
        "Beyond semantic equivalence, which is relevant to applications such as MT quality es",
        "labeled data are available for language pair L1-L2.",
        "(b): the L1-L2 CLTE model is used to cope with the unavailability of labeled data for L3-L4.",
        "(c): the same problem is tackled by combining multiple models.",
        "timation (Mehdad et al., 2012b),1 we aim to capture a richer set of relations potentially relevant to other tasks.",
        "For instance, recognizing unrelated-ness, forward and backward entailment relations, represents a core problem in cross-lingual document summarization (Lenci et al., 2002) and content synchronization (Monz et al., 2011; Mehdad et al., 2012a).",
        "CLTE, as proposed within the Se-mEval evaluation exercises (Negri et al., 2012; Negri et al., 2013), represents an ideal framework to evaluate such capabilities.",
        "Within this framework, our goal is to automatically identify the following entailment relations between T1 and T2: forward (T1 ?",
        "T2), backward (T1 ?",
        "T2), bidirectional (T1 ?",
        "T2) and no entailment.",
        "Our approach (see Figure 1) involves two core components: i) a word alignment model, and ii) a CLTE classifier.",
        "The former is trained on a parallel corpus, and associates equivalent terms in T1 and T2.",
        "The information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative features (importance of the aligned terms) to train the CLTE classifier.",
        "Although in principle both components need training data (respectively a parallel corpus and labeled CLTE data), our goal is to develop a method that is also portable across languages.",
        "To this aim, while the parallel corpus is necessary to train the word aligner for any language pair we want to deal with, the CLTE clas1A translation has to be semantically equivalent to the source sentence.",
        "sifier can be designed to learn from features that capture language independent knowledge.2 This allows us to experiment in different testing conditions, namely: i) when CLTE training data are available for a given language pair (Figure 1a), and ii) when CLTE training data are missing, and a model trained on other language pairs has to be reused (Figure 1b-c).",
        "Features.",
        "Considering word alignment information, we extract three different groups of features: AL, POS, and IDF.",
        "The AL group provides quantitative information about the aligned/unaligned words in each sentence T?",
        "of the pair.",
        "These features are:",
        "1. proportion of aligned words in T?.",
        "We use this indicator as our baseline (B henceforth); 2. number of sequences of unaligned words, normalized by the length of T?",
        "; 3. length of the longest a) sequence of aligned words, and b) sequence of unaligned words, both normalized by the length of T?",
        "; 4. average length of a) the aligned word sequences, and b) the unaligned word sequences; 5. position of a) the first unaligned word, and b) the last unaligned word, both normalized by the lenght of T?",
        "; 6. proportion of word n-grams in T?",
        "contain",
        "puted separately for values of n = 1 .",
        ".",
        ".",
        "5).",
        "The POS group considers the part of speech (PoS) of the words in T?",
        "as a source of qualitative information about their importance.",
        "To compute these features we use the TreeTagger (Schmid, 1995), manually mapping the fine-grained set of assigned PoS labels into a more general set of tags (P ) based on the universal PoS tag set by Petrov et al. (2012).",
        "POS features differentiate between aligned words (words in T1 that are aligned to one or more words in T2) and alignments (the edges connecting words in T1 and T2).",
        "Features considering the aligned words in T?",
        "are: 7. for each PoS tag p ?",
        "P , proportion of aligned words in T?",
        "tagged with p; 8. proportion of words in T1 aligned with words with the same PoS tag in T2 (and vice-versa); 9. for each PoS tag p ?",
        "P , proportion of words in T1 tagged as p which are aligned to words with the same tag in T2 (and vice-versa).",
        "Features considering the alignments are: 10. proportion of alignments connecting words with the same PoS tag p; 11. for each PoS tag p ?",
        "P , proportion of alignments connecting two words tagged as p. IDF, the last feature, uses the inverse document frequency (Salton and Buckley, 1988) as another source of qualitative information under the assumption that rare words (and, therefore, with higher IDF) are more informative: 12. summation of all the IDF scores of the aligned words in T?",
        "over the summation of the IDF scores of all words in T?."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": [
        "Our experiments cover two different scenarios.",
        "First, the typical one, in which the CLTE model is trained on labeled data for the same pair of languages L1?L2 of the test set.",
        "Then, simulating the less favorable situation in which labeled training data for L1?L2 are missing, we investigate the possibility to use existing CLTE models trained on labeled data for a different language pair L3?L4.",
        "The SemEval 2012 CLTE datasets used in our experiments are available for four language pairs: Es?En, De?En, Fr?En, and It?En.",
        "Each dataset was created with the crowdsourcing-based method described in Negri et al. (2011), and consists of 1000 T1?T2 pairs (500 for training, 500 for test).",
        "To train the word alignment models we used the Europarl parallel corpus (Koehn, 2005), concatenated with the News Commentary corpus3 for three language pairs: De?En (2,079,049 sentences), Es?En (2,123,036 sentences), Fr?En (2,144,820 sentences).",
        "For It?En we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus.",
        "IDF values for the words in each language were calculated on the monolingual part of these corpora, using the average IDF value of each language for unseen terms.",
        "To build the word alignment models we used the MGIZA++ package (Gao and Vogel, 2008).",
        "Experiments have been carried out with the hidden Markov model (HMM) (Vogel et al., 1996) and IBM models 3 and 4 (Brown et al., 1993).4 We also explored three symmetrization techniques (Koehn et al., 2005): union, intersection, and grow-diagfinal-and.",
        "A greedy feature selection process on training data, with different combinations of word alignment models and symmetrization methods, indicated HMM/intersection as the best performing combination.",
        "For this reason, all our experiments use this setting.",
        "The SVM implementation of Weka (Hall et al., 2009) was used to build the CLTE model.5 Two binary classifiers were trained to separately check T1 ?",
        "T2 and T1 ?",
        "T2, merging their output to obtain the 4-class judgments (e.g. yes/yes=bidirectional, yes/no=forward)."
      ]
    },
    {
      "heading": "3.1 Evaluation with CLTE training data",
      "text": [
        "Figure 2 shows the accuracy obtained by the different feature groups.6 For the sake of comparison, state-of-the-art results achieved for each language combination at SemEval 2012 are also reported.",
        "As regards Es?En (63.2% accuracy) and De?En (55.8%), the top scores were obtained by the system described in (Wa?schle and Fendrich, 2012), where a combination of binary classifiers for each entailment direction is trained with a mix",
        "ture of monolingual (i.e. with the input sentences translated in the same language using Google Translate7) and cross-lingual features.",
        "Although such system exploits word-alignment information to some extent, this is only done at quantitative level (e.g. number of unaligned words, percentage of aligned words, length of the longest unaligned subsequence).",
        "As regards It?En, the state of the art (56.6%) is represented by the system described in (Jimenez et al., 2012), which uses a pure pivoting method (using Google Translate) and adaptive similarity functions based on ?soft?",
        "cardinality for flexible term comparisons.",
        "The two systems obtained the same result on Fr?En (57.0%).",
        "group on four language combinations.",
        "As can be seen in Figure 2, the combination of all our features outperforms the state of the art for each language pair.",
        "The accuracy improvement ranges from 6.6% for Es?En (from 63.2% to 67.4%) to 14.6% for De?En (from 55.8% to 64%).",
        "Except for Es?En, that has very competitive state-of-the-art results, the combination of AL with POS or IDF feature groups always outperforms the best systems.",
        "Furthermore, the performance increase with qualitative features (POS and IDF) shows coherent trends across all language pairs.",
        "It is worth noting that, while we rely on a pure cross-lingual approach, both the state-of-the-art CLTE systems include features from the translation of T1 into the language of T2.",
        "For De?En, quantitative features alone achieve lower results compared to the other languages.",
        "This can be motivated by the higher difficulty in aligning De?En pairs (this hypothesis is supported by the fact that the average number of alignments per sentence pair is 18 for De?En, and >22 for the other combinations).",
        "Nevertheless, qualitative features lead to results comparable",
        "with the other language pairs.",
        "The selection of the best performing features for each language pair produces further improvements of varying degrees in Es?En (from 67.4% to 68%), De?En (64% ?",
        "64.8%) and It?En (63.4% ?",
        "66.8%), while performance remains stable for Fr?En (63%).",
        "All these configurations include the IDF feature (12) and the proportion of aligned words for each PoS category (7), proving the effectiveness of qualitative word alignment features.",
        "The fact that HMM/intersection is the best combination of alignment model and symmetrization method is interesting, since it contradicts the general notion that IBM models 3 and 4 perform better than HMM (Och and Ney, 2003).",
        "A possible explanation is that, while word alignment models are usually trained on parallel corpora, the majority of CLTE sentence pairs are not parallel.",
        "In this setting, where producing reliable alignments is more difficult, IBM models are less effective for at least two reasons.",
        "First, including a word fertility model, IBM 3 and 4 limit (typically to the half of the source sentence length) the number of target words that can be aligned with the null word.",
        "Therefore, when such limit is reached, these models tend to force low probability, hence less reliable, word alignments.",
        "Second, in IBM model 4, the larger distortion limit makes it possible to align distant words.",
        "In the case of non-parallel sentences, this often results in wrong or noisy alignments that affect final results.",
        "For these reasons, CLTE data seem more suitable for the simpler and more conservative HMM model, and a precision-oriented symmetrization method like intersection."
      ]
    },
    {
      "heading": "3.2 Evaluation without CLTE training data",
      "text": [
        "The goal of our second round of experiments is to investigate if, and to what extent, our approach can be considered as language-independent.",
        "Confirming this would allow to reuse models trained for a given language pair in situations where CLTE training data is missing.",
        "This is a rather realistic situation since, while bitexts to train word aligners are easier to find, the availability of labeled CLTE data is far from being guaranteed.",
        "Our experiments have been carried out, over the same SemEval datasets, with two methods that do not use labeled data for the target language combination.",
        "The first one (method b in Figure 1) uses a CLTE model trained for a language pair L1?L2 for which labeled training data are avail",
        "able, and applies this model to a language pair L3?L4 for which only parallel corpora are available.",
        "The second method (c in Figure 1) addresses the same problem, but exploits a combination of CLTE models trained for different language pairs.",
        "For each test set, the models trained for the other three language pairs are used in a voting scheme, in order to check whether they can complement each other to increase final results.",
        "All the experiments have been performed using the best CLTE model for each language pair, comparing results with those presented in Section 3.1.",
        "As shown in Figure 3, reusing models for a new language pair leads to results that still outperform the state of the art.6 Remarkably, when used for other language combinations, the Es?En, It?En, and Fr?En models always lead to results above, or equal to the state of the art.",
        "For similar languages such as Spanish, French, and Italian, the accuracy increase over the state of the art is up to 14.8% (from 56.6% to 65.0%) and 13.4% (from 56.6% to 64.2%) when the Fr?En and Es?En models are respectively used to label the It?En dataset.",
        "Although not always statistically significant and below the performance obtained in the ideal scenario where CLTE training data are available (full sys.",
        "), such improvements suggest that our features can be re-used, at least to some extent, across different language settings.",
        "As expected, the major incompatibilities arise between German and the other languages due to the linguistic differences between this language and the others.",
        "However, it is interesting to note that: i) at least in one case (i.e. when tested on It?En) the De?En model still achieves results above the state of the art, and ii) on the De?En evaluation setting the worst model (Fr?En) still achieves state of the art results.",
        "The results obtained with the voting scheme suggest that our models can complement each other when used on a new language pair.",
        "Although statistically significant only over It?En data, voting results both outperform the state of the art and the results achieved by single models."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "We investigated the usefulness of qualitative information from automatic word alignment to identify semantic relations between sentences in different languages.",
        "With coherent results in CLTE, we demonstrated that features considering the importance of aligned terms can successfully integrate the quantitative evidence (number and proportion of aligned terms) used by previous supervised learning approaches.",
        "A study on the portability across languages of the learned models demonstrated that word alignment information can be exploited to train reusable models for new language combinations where bitexts are available but CLTE labeled data are not."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work has been partially supported by the EC-funded projects CoSyne (FP7-ICT-4-248531) and MateCat (ICT-2011.4.2?287688), and by Spanish Government through projects TIN2009-14009C02-01 and TIN2012-32615."
      ]
    }
  ]
}
