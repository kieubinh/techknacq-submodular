{
  "info": {
    "authors": [
      "Haifeng Hu",
      "Bingquan Liu",
      "Baoxun Wang",
      "Ming Liu",
      "Xiaolong Wang"
    ],
    "book": "ACL",
    "id": "acl-P13-2146",
    "title": "Multimodal DBN for Predicting High-Quality Answers in cQA portals",
    "url": "https://aclweb.org/anthology/P13-2146",
    "year": 2013
  },
  "references": [
    "acl-P10-1125"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we address the problem for predicting cQA answer quality as a classification task.",
        "We propose a multimodal deep belief nets based approach that operates in two stages: First, the joint representation is learned by taking both textual and non-textual features into a deep learning network.",
        "Then, the joint representation learned by the network is used as input features for a linear classifier.",
        "Extensive experimental results conducted on two cQA datasets demonstrate the effectiveness of our proposed approach."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Predicting the quality of answers in community based Question Answering (cQA) portals is a challenging task.",
        "One straightforward approach is to use textual features as a text classification task (Agichtein et al., 2008).",
        "However, due to the word over-sparsity and inherent noise of user-generated content, the classical bag-of-words representation, is not appropriate to estimate the quality of short texts (Huang et al., 2011).",
        "Another typical approach is to leverage non-textual features to automatically identify high quality answers (Jeon et al., 2006; Zhou et al., 2012).",
        "However, in this way, the mining of meaningful textual features usually tends to be ignored.",
        "Intuitively, combining both textual and non-textual information extracted from answers is helpful to improve the performance for predicting the answer quality.",
        "However, textual and non-textual features usually have different kinds of representations and the correlations between them are highly non-linear.",
        "Previous study (Ngiam et al., 2011) has shown that it is hard for a shallow model to discover the correlations over multiple sources.",
        "To this end, a deep learning approach, called multimodal deep belief nets (mDBN), is introduced to address the above problems to predict the answer quality.",
        "The approach includes two stages: feature learning and supervised training.",
        "In the former stage, a specially designed deep network is given to learn the unified representation using both textual and non-textual information.",
        "In the latter stage, the outputs of the network are then used as inputs for a linear classifier to make prediction.",
        "The rest of this paper is organized as follows: The related work is surveyed in Section 2.",
        "Then, the proposed approach and experimental results are presented in Section 3 and Section 4 respectively.",
        "Finally, conclusions and future directions are drawn in Section 5."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "The typical way to predict the answer quality is exploring various features and employing machine learning methods.",
        "For example, Jeon et al. (2006) have proposed a framework to predict the quality of answers by incorporating non-textual features into a maximum entropy model.",
        "Subsequently, Agichtein et al. (2008) and Bian et al. (2009) both leverage a larger range of features to find high quality answers.",
        "The deep research on evaluating answer quality has been taken by Shah and Pomer-antz (2010) using the logistic regression model.",
        "We borrow some of their ideas in this paper.",
        "In deep learning field, extensive studies have been done by Hinton and his co-workers (Hinton et al., 2006; Hinton and Salakhutdinov, 2006; Salakhutdinov and Hinton, 2009), who initially propose the deep belief nets (DBN).",
        "Wang et.al (2010; 2011) firstly apply the DBNs to model semantic relevance for qa pairs in social communities.",
        "Meanwhile, the feature learning for disparate sources has also been the hot research topic.",
        "Lee et al. (2009) demonstrate that the hidden representations computed by a convolutional DBN make excellent features for visual recognition."
      ]
    },
    {
      "heading": "3 Approach",
      "text": [
        "We consider the problem of high-quality answer prediction as a classification task.",
        "Figure 1 summarizes the framework of our proposed approach.",
        "First, textual features and non-textual features ex"
      ]
    },
    {
      "heading": "TextualFeatures Non-textualFeaturesCQAArchives ClassifierFusion Representation FeatureLearning Supervised Training High-qualityAnswers",
      "text": [
        "tracted from cQA portals are used to train two DBN models to learn the high-level representations independently for answers.",
        "The two high-level representations learned by the deep architectures are then joined together to train a RBM model.",
        "Finally, a linear classifier is trained with the final shared representation as input to make prediction.",
        "In this section, a deep network for the cQA answer quality prediction is presented.",
        "Textual and non-textual features are typically characterized by distinct statistical properties and the correlations between them are highly non-linear.",
        "It is very difficult for a shallow model to discover these correlations and form an informative unified representation.",
        "Our motivation of proposing the mDBN is to tackle these problems using an unified representation to enhance the classification performance."
      ]
    },
    {
      "heading": "3.1 The Restricted Boltzmann Machines",
      "text": [
        "The basic building block of our feature leaning component is the Restricted Boltzmann Machine (RBM).",
        "The classical RBM is a two-layer undirected graphical model with stochastic visible units v and stochastic hidden units h.The visible layer and the hidden layer are fully connected to the units in the other layer by a symmetric matrix w. The classical RBM has been used effectively in modeling distributions over binary-value data.",
        "As for real-value inputs, the gaussian RBM (Bengio et al., 2007) can be employed.",
        "Different from the former, the hypothesis for the visible unit in the gaussian RBM is the normal distribution."
      ]
    },
    {
      "heading": "3.2 Feature Learning",
      "text": [
        "The illustration of the feature learning model is given by Figure 2.",
        "Basically, the model consists of two parts.",
        "In the bottom part (i.e., V -H1, H1-H2), each data modality is modeled by a two-layer DBN separately.",
        "For clarity, we take the textual modality as an example to illustrate the construction of the mDBN in this part.",
        "Given a textual input vector v, the visible layer generates the hidden vector h, by",
        "iwijvi).",
        "Then the conditional distribution of v given h, is",
        "j wijhj).",
        "where ?",
        "(x) = (1 + e?x)?1 denotes the logistic function.",
        "The parameters are updated by performing gradient ascent using Contrastive Divergence (CD) algorithm (Hinton, 2002).",
        "After learning the RBMs in the bottom layer, we treat the activation probabilities of its hidden units driven by the inputs, as the training data for training a new layer.",
        "The construction procedures for the non-textual modality are similar to the textual one, except that we use the gaussian RBM to model the real-value inputs in the bottom layer.",
        "Finally, we combine the two models by adding an additional layer, H3, on the top of them to form the mDBN.",
        "The training method is also similar to the bottom?s, but the input vector is the concatenation of the mapped textual vector and the mapped non-textual vector.",
        "Figure 2: mDBN for Feature Learning It should be noted in the network, the bottom part is essential to form the joint representation because the correlations between the textual and non-textual features are highly non-linear.",
        "It is hard for a RBM directly combining the two disparate sources to learn their correlations."
      ]
    },
    {
      "heading": "3.3 Supervised Training and Classification",
      "text": [
        "After the above steps, a deep network for feature learning between textual and non-textual data is established.",
        "Classifiers, either support vector machine (SVM) or logistic regression (LR), can then be trained with the unified representation (Ngiam",
        "et al, 2011; Srivastava and Salakhutdinov, 2012).",
        "Specifically, the LR classifier is used to make the final prediction in our experiments since it keeps to deliver the best performance."
      ]
    },
    {
      "heading": "3.4 Basic Features",
      "text": [
        "Textual Features: The textual features extract from 1,500 most frequent words in the training dataset after standard preprocessing steps, namely word segmentation, stopwords removal and stemming1.",
        "As a result, each answer is represented as a vector containing 1,500 distinct terms weighted by binary scheme.",
        "Non-textual Features: Referring to the previous work (Jeon et al., 2006; Shah and Pomerantz, 2010), we adopt some features used in theirs and also explore three additional features marked by ?",
        "sign.",
        "The complete list is described"
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Experiment Setup",
      "text": [
        "Datasets: We carry out experiments on two datasets.",
        "One dataset comes from Baidu Zhi-dao2, which contains 33,740 resolved questions crawled by us from the ?travel?",
        "category.",
        "The other dataset is built by Chen and Nayak (2008) from Yahoo!",
        "Answers3.",
        "We refer to these two datasets as ZHIDAO and YAHOO respectively and randomly sample 10,000 questions from each to form our experimental datasets.",
        "According to the user name, we have crawled all the user profile web pages for non-textual feature collection.",
        "To alleviate unnecessary noise, we only select those questions with number of answers no less than 3 (one",
        "best answer among them), and those answers at least have 10 tokens.",
        "The statistics on the datasets used for experiments are summarized in Table 2.",
        "pare against the following methods as our baselines.",
        "(1) Logistic Regression (LR): We implement the approach used by Shah and Pomerantz (2010) with textual features LR-T, non-textual features LR-N and their simple combination LR-C. (2) DBN: Similar to the mDBN, the outputs of the last hidden layer by the DBN are used as inputs for LR model.",
        "Based on the feature sets, we have DBN-T for textual features and DBN-N for non-textual features.",
        "Since we mainly focus on the high quality answers, the precision, recall and f1 for positive class and the overall accuracy for both classes are employed as our evaluation metrics.",
        "Model Architecture and Training Details: To create the mDBN architecture, we use the classical RBM with 1500 visible units followed by 2 hidden layers with 1000 and 800 units respectively for the textual branch, and the gaussian RBM with 20 visible units followed by 2 hidden layers with 100 and 200 units respectively for the non-textual branch.",
        "On the joint layer of the network, the layer contains 1000 real-value units.",
        "Each RBM is trained using 1-step CD algorithm.",
        "During the training stage, a small weight-cost of 0.0002 is used, and the learning rate for textual data modal is 0.05 while the non-textual data is 0.001.",
        "We also adopt a monument of 0.5 for the first five epochs and 0.9 for the rest epochs.",
        "In addition, all non-textual data vectors are normalized to have zero mean and unit standard variance.",
        "More details for training the deep architecture can be found in Hinton (2012)."
      ]
    },
    {
      "heading": "4.2 Results and Analysis",
      "text": [
        "In the first experiment, we compare the performance of mDBN with different methods.",
        "To make a fare comparison, we use the liblinear toolkit4 for logistic regression model with L2 regularization and randomly select 70% QA pairs as training data",
        "and the rest 30% as testing data.",
        "Table 3 and Table 4 summarize the average results of the 5 round experiments on YAHOO and ZHIDAO respectively.",
        "It is promising to see that the proposed mDBN method notably outperforms almost all the other methods on both datasets over all the metrics as expected, except for the recall on ZHIDAO.",
        "The main reason for the improvements is that the joint representation learned by mDBN is able to complement each modality perfectly.",
        "In addition, the mDBN can extract stronger representation through modeling semantic relationship between textual and non-textual information, which can effectively help distinguish more complicated answers from high quality to low quality.",
        "The classification performance of the textual features are worse on average compared with non-textual features, even when the feature learning strategy is employed.",
        "More interestingly, we find the simple combinations of textual and non-textual features don't improve the classification results compared with using non-textual features alone.We conjecture that there are mainly three reasons for the phenomena: First, this is due to the fact that user-generated content is inherently noisy with low word frequency, resulting in the sparsity of employing textual feature.",
        "Second, non-textual features (e.g., answer length) usually own strongly statistical properties and feature sparsity problem can be better relieved to some extent.",
        "Finally, since correlations between the textual features and non-textual features are highly non-linear, concatenating these features simply sometimes can submerge classification performance.",
        "In contrast, mDBN enjoys the advantage of the shared representation between textual features and non-textual features using the deep learning architecture.",
        "We also note that neither the mDBN nor other approaches perform very well in predicting answer quality across the two datasets.",
        "The best precision on ZHIDAO and YAHOO are respectively 59.0% and 53.4%, which means that there are nearly half of the high quality answers not effectively identified.",
        "One of the possible reason is that the quality of the corpora influences the result significantly.",
        "As shown in Table 2, each question on average receives more than 4 answers on ZHIDAO and more than 10 on YAHOO.",
        "Therefore, it is possible that there are several answers with high quality to the same question.",
        "Selecting only one as the high quality answer is relatively difficult for our humans, not to mention for the models.",
        "In the second experiment, we intend to examine the performance of mDBN with different number of iterations.",
        "Figure 3 depicts the metrics on ZHIDAO when the iteration number is varied from 100 to 5000.",
        "From the result, the first observation is that increasing the number of iterations the performance of mDBN can improve significantly, obtaining the best results for iteration of 1000.",
        "This clearly shows the representation power of the mDBN again.",
        "However, after a large number of iterations (large than 1000), the mDBN has a detrimental performance.",
        "This may be explained by with large number of iterations, the deep learning architecture is easier to be overfitting.",
        "The similar trend is also observed on YAHOO."
      ]
    },
    {
      "heading": "5 Conclusions and Future work",
      "text": [
        "In this paper, we have provided a new perspective to predict the cQA answer quality: learning an informative unified representation between textual and non-textual features instead of concatenating them simply.",
        "Specifically, we have proposed a multimodal deep learning framework to",
        "form the unified representation.",
        "We compare this with the basic features both in isolation and in combination.",
        "Experimental results have demonstrated that our proposed approach can capture the complementarity between textual and non-textual features, which is helpful to improve the performance for cQA answer quality prediction.",
        "For the future work, we plan to explore more semantic analysis to approach the issue for short text quality evaluation.",
        "Additionally, more research will be taken to put forward other approaches for learning multimodal representation."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors are grateful to the anonymous reviewers for their constructive comments.",
        "Special thanks to Chengjie Sun and Deyuan Zhang for insightful suggestions.",
        "This work is supported by National Natural Science Foundation of China (NSFC) via grant 61272383 and 61100094."
      ]
    }
  ]
}
