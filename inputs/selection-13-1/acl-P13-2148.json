{
  "info": {
    "authors": [
      "Hongliang Yu",
      "Zhi-Hong Deng",
      "Shiyingxue Li"
    ],
    "book": "ACL",
    "id": "acl-P13-2148",
    "title": "Identifying Sentiment Words Using an Optimization-based Model without Seed Words",
    "url": "https://aclweb.org/anthology/P13-2148",
    "year": 2013
  },
  "references": [
    "acl-D07-1115",
    "acl-P10-1041",
    "acl-P11-1015",
    "acl-P11-2104",
    "acl-W02-1011",
    "acl-W06-1642"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications.",
        "Most existing researches exploit seed words, and lead to low robustness.",
        "In this paper, we propose a novel optimization-based model for SWI.",
        "Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words.",
        "Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In recent years, sentiment analysis (Pang et al., 2002) has become a hotspot in opinion mining and attracted much attention.",
        "Sentiment analysis is to classify a text span into different sentiment polarities, i.e. positive, negative or neutral.",
        "Sentiment Word Identification (SWI) is a basic technique in sentiment analysis.",
        "According to (Ku et al., 2006)(Chen et al., 2012)(Fan et al., 2011), SWI can be applied to many fields, such as determining critics opinions about a given product, tweeter classification, summarization of reviews, and message filtering, etc.",
        "Thus in this paper, we focus on SWI.",
        "Here is a simple example of how SWI is applied to comment analysis.",
        "The sentence below is an movie review in IMDB database: ?",
        "Bored performers and a lackluster plot and script, do not make a good action movie.",
        "In order to judge the sentence polarity (thus we can learn about the preference of this user), one must recognize which words are able to express sentiment.",
        "In this sentence, ?bored?",
        "and ?lackluster?",
        "are negative while ?good?",
        "should be positive, yet ?Corresponding author its polarity is reversed by ?not?.",
        "By such analysis, we then conclude such movie review is a negative comment.",
        "But how do we recognize sentiment words?",
        "To achieve this, previous supervised approaches need labeled polarity words, also called seed words, usually manually selected.",
        "The words to be classified by their sentiment polarities are called candidate words.",
        "Prior works study the relations between labeled seed words and unlabeled candidate words, and then obtain sentiment polarities of candidate words by these relations.",
        "There are many ways to generate word relations.",
        "The authors of (Turney and Littman, 2003) and (Kaji and Kitsuregawa, 2007) use statistical measures, such as point wise mutual information (PMI), to compute similarities in words or phrases.",
        "Kanaya-ma and Nasukawa (2006) assume sentiment words successively appear in the text, so one could find sentiment words in the context of seed words (Kanayama and Nasukawa, 2006).",
        "In (Hassan and Radev, 2010) and (Hassan et al., 2011), a Markov random walk model is applied to a large word relatedness graph, constructed according to the synonyms and hypernyms in WordNet (Miller, 1995).",
        "However, approaches based on seed words has obvious shortcomings.",
        "First, polarities of seed words are not reliable for various domains.",
        "As a simple example, ?rise?",
        "is a neutral word most often, but becomes positive in stock market.",
        "Second, manually selection of seed words can be very subjective even if the application domain is determined.",
        "Third, algorithms using seed words have low robustness.",
        "Any missing key word in the set of seed words could lead to poor performance.",
        "Therefore, the seed word set of such algorithms demands high completeness (by containing common polarity words as many as possible).",
        "Unlike the previous research work, we identify sentiment words without any seed words in this paper.",
        "Instead, the documents?",
        "bag-of-words in",
        "formation and their polarity labels are exploited in the identification process.",
        "Intuitively, polarities of the document and its most component sentiment words are the same.",
        "We call such phenomenon as ?sentiment matching?.",
        "Moreover, if a word is found mostly in positive documents, it is very likely a positive word, and vice versa.",
        "We present an optimization-based model, called WEED, to exploit the phenomenon of ?sentiment matching?.",
        "We first measure the importance of the component words in the labeled documents semantically.",
        "Here, the basic assumption is that important words are more sentiment related to the document than those less important.",
        "Then, we estimate the polarity of each document using its component words?",
        "importance along with their sentiment values, and compare the estimation to the real polarity.",
        "After that, we construct an optimization model for the whole corpus to weigh the overall estimation error, which is minimized by the best sentiment values of candidate words.",
        "Finally, several experiments demonstrate the effectiveness of our approach.",
        "To the best of our knowledge, this paper is the first work that identifies sentiment words without seed words."
      ]
    },
    {
      "heading": "2 The Proposed Approach",
      "text": []
    },
    {
      "heading": "2.1 Preliminary",
      "text": [
        "We formulate the sentiment word identification problem as follows.",
        "Let D = {d1, .",
        ".",
        ".",
        ", dn} denote document set.",
        "Vector l?",
        "="
      ]
    },
    {
      "heading": "2.2 Word Importance",
      "text": [
        "We assume each document di ?",
        "D is presented by a bag-of-words feature vector f?i =",
        "where fij describes the importance of cj to di.",
        "A high value of fij indicates word cj contributes a lot to document di in semantic view, and vice versa.",
        "Note that fij > 0 if cj appears in di, while fij = 0 if not.",
        "For simplicity, every f?i is normalized to a unit vector, such that features of different documents are relatively comparable.",
        "There are several ways to define the word importance, and we choose normalized TF-IDF (Jones, 1972).",
        "Therefore, we have fij ?",
        "TF?IDF (di, cj), and ?f?i?",
        "= 1."
      ]
    },
    {
      "heading": "2.3 Polarity Value",
      "text": [
        "In the above description, the sentiment polarity has only two states, positive or negative.",
        "We extend both word and document polarities to polarity values in this section.",
        "Definition 1 Word Polarity Value: For each word cj ?",
        "C, we denote its word polarity value as w(cj).",
        "w(cj) > 0 indicates cj is a positive word, while w(cj) < 0 indicates cj is a negative word.",
        "|w(cj) |indicates the strength of the belief of cj's polarity.",
        "Denote w(cj) as wj , and the word polarity value vector w?",
        "=",
        "??.",
        "For example, ifw(?bad?)",
        "< w(?greedy?)",
        "< 0, we can say ?bad?",
        "is more likely to be a negative word than ?greedy?.",
        "Definition 2 Document Polarity Value: For each document di, document polarity value is",
        "We denote y(di) as yi for short.",
        "Here, we can regard yi as a polarity estimate for di based on w?.",
        "To explain this, Table 1 shows an example.",
        "?MR1?, ?MR2?",
        "and ?MR3?",
        "are three movie review documents, and ?compelling?",
        "and ?boring?",
        "are polarity words in the vocabulary.",
        "we simply use TF to construct the document feature vectors without normalization.",
        "In the table, these three vectors, f?1, f?2 and f?3, are (3, 1), (2, 1) and (1, 3) respectively.",
        "Similarly, we can get w?",
        "= (1,?1), indicating ?compelling?",
        "is a positive word while ?boring?",
        "is negative.",
        "After normalizing f?1, f?2 and f?3, and calculating their cosine similarities with w?, we obtain y1 > y2 > 0 > y3.",
        "These inequalities tell us the first two reviews are positive, while the last review is negative.",
        "Furthermore, we believe that ?MR1?",
        "is more positive than ?MR2?.",
        "ture vectors of three movie reviews, and the last row shows the word polarity value vector w?.",
        "For simplicity, we use TF value to represent the word importance feature."
      ]
    },
    {
      "heading": "2.4 Optimization Model",
      "text": [
        "As mentioned above, we can regard yi as a polarity estimate for document di.",
        "A precise prediction makes the positive document's estimator close to 1, and the negative's close to -1.",
        "We define the polarity estimate error for document di as:",
        "Our learning procedure tries to decrease ei.",
        "We obtain w?",
        "by minimizing the overall estimation error of all document samples",
        "timization problem can be described as",
        "After solving this problem, we not only obtain the polarity of each word cj according to the sign of wj , but also its polarity belief based on |wj |."
      ]
    },
    {
      "heading": "2.5 Model Solution",
      "text": [
        "We use normalized vector x?",
        "to substitute w??w??",
        ", andderive an equivalent optimization problem:",
        "The equality constraint of above model makes the problem non-convex.",
        "We relax the equality constraint to ?x??",
        "?",
        "1, then the problem becomes convex.",
        "We can rewrite the objective function as the form of least square regression: E(x?)",
        "= ?F ?",
        "x?",
        "?",
        "l?",
        "?2, where F is the feature matrix, and",
        "Now we can solve the problem by convex optimization algorithms (Boyd and Vandenberghe, 2004), such as gradient descend method.",
        "In each iteration step, we update x?",
        "by ?x?",
        "= ?",
        "?",
        "(?",
        "?E) = 2?",
        "?",
        "(F T l?",
        "?",
        "F TF x?",
        "), where ?",
        "> 0 is the learning rate."
      ]
    },
    {
      "heading": "3 Experiment",
      "text": []
    },
    {
      "heading": "3.1 Experimental Setup",
      "text": [
        "We leverage two widely used document datasets.",
        "The first dataset is the Cornell Movie Review Data 1, containing 1,000 positive and 1,000 negative processed reviews.",
        "The other is the Stanford Large Dataset 2 (Maas et al., 2011), a collection of 50,000 comments from IMDB, evenly divided into training and test sets.",
        "The ground-truth is generated with the help of a sentiment lexicon, MPQA subjective lexicon 3.",
        "We randomly select 20% polarity words as the seed words, and the remaining are candidate ones.",
        "Here, the seed words are provided for the baseline methods but not for ours.",
        "In order to increase the difficulty of our task, several non-polarity words are added to the candidate word set.",
        "Table 2 shows the word distribution of two datasets.",
        "In order to demonstrate the effectiveness of our model, we select two baselines, SO-PMI (Turney and Littman, 2003) and COM (Chen et al., 2012).",
        "Both of them need seed words."
      ]
    },
    {
      "heading": "3.2 Top-K Test",
      "text": [
        "In face of the long lists of recommended polarity words, people are only concerned about the top-ranked words with the highest sentiment value.",
        "In this experiment we consider the accuracy of the top K polarity words.",
        "The quality of a polarity"
      ]
    },
    {
      "heading": "WEED SO-PMI COM",
      "text": [
        "positive words negative words positive words negative words positive words negative words great excellent bad stupid destiny lush cheap worst best great ridiculous bad perfect perfectly worst mess brilliant skillfully ridiculous annoying will star plot evil terrific best boring ridiculous courtesy courtesy damn pathetic bad fun star garish true wonderfully awful plot gorgeous magnificent inconsistencies fool better plot dreadfully stupid brilliant outstanding worse terrible temptation marvelously desperate giddy love horror pretty fun",
        "highly outperforms two baselines, and the precision is 14.4%-33.0% higher than the best baseline.",
        "p@10s of WEED for Cornell and Stanford datasets reach to 93.5% and 89.0%, and it shows the top 10 words in our recommended list is exceptionally reliable.",
        "As the size of K increases, the accuracy of all methods falls accordingly.",
        "This shows three approaches rank the most probable polarity words in the front of the word list.",
        "Compared with the small dataset, we obtain a better result with large K on the Stanford dataset."
      ]
    },
    {
      "heading": "3.3 Case Study",
      "text": [
        "We conduct an experiment to illustrate the characteristics of three methods.",
        "Table 3 shows top10 positive and negative words for each method, where the bold words are the ones with correct polarities.",
        "From the first two columns, we can see the accuracy of WEED is very high, where positive words are absolutely correct and negative word list makes only one mistake, ?plot?.",
        "The other columns of this table shows the baseline methods both achieve reasonable results but do not perform as well as WEED.",
        "Our approach is able to identify frequently used sentiment words, which are vital for the applications without prior sentiment lexicons.",
        "The sentiment words identified by SO-PMI are not so representative as WEED and COM.",
        "For example, ?skillfully?",
        "and ?giddy?",
        "are correctly classified but they are not very frequently used.",
        "COM tends to assign wrong polarities to the sentiment words although these words are often used.",
        "In the 5th and 6th columns of Table 3, ?bad?",
        "and ?horror?",
        "are recognized as positive words, while ?pretty?",
        "and ?fun?",
        "are recognized as negative ones.",
        "These concrete results show that WEED captures the generality of the sentiment words, and achieves a higher accuracy than the baselines."
      ]
    },
    {
      "heading": "4 Conclusion and Future Work",
      "text": [
        "We propose an effective optimization-based model, WEED, to identify sentiment words from the corpus without seed words.",
        "The algorithm exploits the sentiment information provided by the documents.",
        "To the best of our knowledge, this paper is the first work that identifies sentiment words without any seed words.",
        "Several experiments on real datasets show that WEED outperforms the state-of-the-art methods with seed words.",
        "Our work can be considered as the first step of building a domain-specific sentiment lexicon.",
        "Once some sentiment words are obtained in a certain domain, our future work is to improve WEED by utilizing these words."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is partially supported by National Natural Science Foundation of China (Grant No.",
        "61170091)."
      ]
    }
  ]
}
