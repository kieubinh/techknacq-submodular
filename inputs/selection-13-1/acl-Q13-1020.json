{
  "info": {
    "authors": [
      "Feifei Zhai",
      "Jiajun Zhang",
      "Yu Zhou",
      "Chengqing Zong"
    ],
    "book": "TACL",
    "id": "acl-Q13-1020",
    "title": "Unsupervised Tree Induction for Tree-based Translation",
    "url": "https://aclweb.org/anthology/Q13-1020",
    "year": 2013
  },
  "references": [
    "acl-C08-1136",
    "acl-D07-1078",
    "acl-D08-1092",
    "acl-D09-1037",
    "acl-D11-1018",
    "acl-D11-1019",
    "acl-D12-1021",
    "acl-D12-1078",
    "acl-D12-1079",
    "acl-J07-2003",
    "acl-J10-2004",
    "acl-J97-3002",
    "acl-N03-1017",
    "acl-N04-1035",
    "acl-N06-1033",
    "acl-N10-1015",
    "acl-N10-1028",
    "acl-N10-1033",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P03-2041",
    "acl-P05-1034",
    "acl-P06-1055",
    "acl-P06-1077",
    "acl-P06-1121",
    "acl-P07-1003",
    "acl-P07-2045",
    "acl-P08-1064",
    "acl-P08-1066",
    "acl-P09-1020",
    "acl-P09-1063",
    "acl-P09-1088",
    "acl-P11-1001",
    "acl-P11-1084",
    "acl-P96-1021",
    "acl-W04-3250",
    "acl-W06-1606",
    "acl-W06-1628",
    "acl-W06-3119",
    "acl-W09-0424",
    "acl-W11-2160"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In current research, most tree-based translation models are built directly from parse trees.",
        "In this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model.",
        "In the model, we utilize synchronous tree substitution grammars (STSG) to capture the bilingual mapping between language pairs.",
        "To train the model efficiently, we develop a Gibbs sampler with three novel Gibbs operators.",
        "The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes.",
        "Experimental results show that the string-to-tree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In recent years, tree-based translation models1 are drawing more and more attention in the community of statistical machine translation (SMT).",
        "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b).",
        "However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers."
      ]
    },
    {
      "heading": "1 A tree-based translation model is defined as a model",
      "text": [
        "using tree structures on one side or both sides.",
        "However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Treebank resources for training.",
        "2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.",
        "This indicates that parse trees are usually not the optimal choice for training tree-based translation models (Wang et al., 2010).",
        "Based on the above analysis, we can conclude that the tree structure that is independent from Treebank resources and simultaneously considers the bilingual mapping inside the bilingual sentence pairs would be a good choice for building tree-based translation models.",
        "Therefore, complying with the above conditions, we propose an unsupervised tree structure for tree-based translation models in this study.",
        "In the structures, tree nodes are labeled by combining the word classes of their boundary words rather than by syntactic labels, such as NP, VP.",
        "Furthermore, using these node labels, we design a generative Bayesian model to infer the final tree structure based on synchronous tree substitution grammars (STSG) 2 .",
        "STSG is derived from the word alignments and thus can grasp the bilingual mapping effectively.",
        "Training the Bayesian model is difficult due to the exponential space of possible tree structures for each training instance.",
        "We therefore develop an efficient Gibbs sampler with three novel Gibbs operators for training.",
        "The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes.",
        "2 We believe it is possible to design a model to infer the node label and tree structure jointly.",
        "We plan this as future work, and here, we focus only on inferring the tree structure in terms of the node labels derived from word classes.",
        "The tree structure formed in this way is independent from the Treebank resources and simultaneously exploits the bilingual mapping effectively.",
        "Experiments show that the proposed unsupervised tree (U-tree) is more effective and reasonable for tree-based translation than the parse tree.",
        "The main contributions of this study are as follows: 1) Instead of the parse tree, we propose a Bayesian model to induce a U-tree for tree-based translation.",
        "The U-tree exploits the bilingual mapping effectively and does not rely on any Treebank resources.",
        "2) We design a Gibbs sampler with three novel Gibbs operators to train the Bayesian model efficiently.",
        "The remainder of the paper is organized as follows.",
        "Section 2 introduces the related work.",
        "Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model.",
        "Section 5 describes the Gibbs sampling algorithm and Gibbs operators.",
        "In Section 6, we analyze the achieved U-trees and evaluate their effectiveness.",
        "Finally, we conclude the paper in Section 7."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures.",
        "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering.",
        "Our previous work (Zhai et al., 2012) designed an EM-based method to construct unsupervised trees for tree-based translation models.",
        "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively.",
        "Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus.",
        "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007).",
        "Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules.",
        "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models.",
        "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG.",
        "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment.",
        "They utilized the bilingual Treebank to train a joint model for both parsing and word alignment.",
        "Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees.",
        "Liu et al. (2012) retrained the linguistic parsers bilingually based on word alignment.",
        "Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation.",
        "Compared to their work, we do not rely on any Treebank resources and focus on generating effective unsupervised tree structures for tree-based translation models.",
        "Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories.",
        "Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes.",
        "Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules."
      ]
    },
    {
      "heading": "3 The STSG Generation Process",
      "text": [
        "In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution grammar (STSG) between source strings and target tree fragments.",
        "We take STSG as the generation grammar to match the translation model.",
        "Typically, such an STSG3 is a 5-tuple as follows: ( , , , , )s t t tG N S P ?",
        "?",
        "where: i s?",
        "and t?",
        "represent the set of source and target words, respectively,"
      ]
    },
    {
      "heading": "3 Generally, an STSG involves tree fragments on both",
      "text": [
        "sides.",
        "Here we only consider the special case where the source side is actually a string.",
        "Apart from the start non-terminal tS , we define all the other non-terminals in tN by word classes.",
        "Inspired by (Zollmann and Vogel, 2011), we divide these non-terminals into three categories: one-word, two-word and multi-word non-terminals.",
        "The one-word non-terminal is a word class, such as C, meaning that it dominates a word whose word class is C. Two-word non-terminals are used to stand for two word strings.",
        "They are labeled in the form of C1+C2, where C1 and C2 are the word classes of the two words separately.",
        "Accordingly, multi-word non-terminals represent the strings containing more than two words.",
        "They are labeled as C1?Cn, demanding that the word classes of the leftmost word and the rightmost word are C1 and Cn, respectively.",
        "We use POS tag to play the role of word class4.",
        "For example, the head node of the rule in Figure 1 is a multi-word non-terminal PRP?RB.",
        "It requires that the POS tags of the leftmost and rightmost word must be PRP and RB, respectively.",
        "Xiong et al.",
        "(2006) showed that the boundary word is an effective indicator for phrase reordering.",
        "Thus, we believe that combining the word class of boundary words can denote the whole phrase well.",
        "Each production rule in P consists of a source string and a target tree fragment.",
        "In the target tree fragment, each internal node is labeled with a non-terminal in tN , and each leaf node is labeled with either a target word in t?",
        "or a non-terminal in tN .",
        "The source string in a production rule comprises source words and variables.",
        "Each variable corresponds to a leaf non-terminal in the target tree fragment.",
        "In the STSG, the production rule is used to rewrite the root node into a string and a tree fragment.",
        "For example, in Figure 1, the rule rewrites the head node PRP?RB into the corresponding string and fragment.",
        "An STSG derivation refers to the process of generating a specific source string and target tree"
      ]
    },
    {
      "heading": "4 The demand of a POS tagger impairs the independence",
      "text": [
        "from manual resources to some extent.",
        "In future, we plan to design a method to learn effective unsupervised labels for the non-terminals.",
        "structure by production rules.",
        "This process begins with the start non-terminal tS and an empty source string.",
        "We repeatedly choose production rules to rewrite the leaf non-terminals and expand the string until no leaf non-terminal is left.",
        "Finally, we acquire a source string and a target tree structure defined by the derivation.",
        "The probability of a derivation is given as follows:",
        "where the derivation comprises a sequence of rules d=(r1,?,rn), and Ni represents the root node of rule ri.",
        "Hence, for a specific bilingual sentence pair, we can generate the best target-side tree structure based on the STSG, independent from the Treebank resources.",
        "The STSG used in the above process is learned by the Bayesian model that is detailed in the next section.",
        "Actually, SCFG can also be used to build the U-trees.",
        "We do not use SCFG because most of the tree-based models are based on STSG.",
        "In our Bayesian model, the U-trees are optimized through selecting a set of STSG rules.",
        "These STSG rules are consistent with the translation rules used in the tree-based models.",
        "Another reason is that STSG has a stronger expressive power on tree construction than SCFG.",
        "In a STSG-based U-tree or a STSG rule, although not linguistically informed, the nodes labeled by POS tags are also effective on distinguishing different ones.",
        "However, with SCFG, we have to discard all the internal nodes (i.e., flattening the U-trees or rules) to express the same sequence, leading to a poor ability of distinguishing different U-trees and production rules.",
        "Thus, using STSG, we can build more specific U-trees for translation.",
        "In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar (Blunsom et al. 2009)5.",
        "This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models.",
        "Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.",
        "5 In (Blunsom et al., 2009), for Chinese-to-English translation, the Bayesian SCFG grammar only outperform the heuristic SCFG grammar by 0.1 BLEU points on NIST MT 2004 and 0.6 BLEU points on NIST MT 2005 in the NEWS domain."
      ]
    },
    {
      "heading": "4 Bayesian Model",
      "text": [
        "In this section, we present a Bayesian model to learn STSG defined in section 3.",
        "In the model, we use ?N to denote the probability distribution ( |)p r N in Equation (1).",
        "?N follows a multinomial distribution and we impose a Dirichlet prior (DP)",
        "where 0 ( |)P N< (base distribution) is used to assign prior probabilities to the STSG production rules.",
        "?N controls the model's tendency to either reuse existing rules or create new ones using the base distribution 0 ( |)P N< .",
        "Instead of denoting the multinomial distribution explicitly with a specific ?N, we integrate over all possible values of ?N to achieve the probabilities of rules.",
        "This integration results in the following conditional probability for rule ri given the previously observed rules r-i = r1 ,?, ri-1:",
        "represents the total count of rules rewriting non-terminal N in ir .",
        "Thanks to the exchangeability of the model, all permutations of the rules are actually equiprobable.",
        "This means that we can compute the probability of each rule based on the previous and subsequent rules (i.e. consider each rule as the last one).",
        "This characteristic allows us to design an efficient Gibbs sampling algorithm to train the Bayesian model."
      ]
    },
    {
      "heading": "4.1 Base Distribution",
      "text": [
        "The base distribution 0 ( |)P r N is designed to assign prior probabilities to the STSG production rules.",
        "Because each rule r consists of a target tree fragment frag and a source string str in the model, we follow Cohn and Blunsom (2009) and decompose the prior probability 0 ( |)P r N into two factors as follows:",
        "where ( |)P frag N is the probability of producing the target tree fragment frag.",
        "To generate frag, Cohn and Blunsom (2009) used a geometric prior to decide how many child nodes to assign each node.",
        "Differently, we require that each multi-word non-terminal node must have two child nodes.",
        "This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al., 2007; Zhang et al., 2011a).",
        "The generation process starts at root node N. At first, root node N is expanded into two child nodes.",
        "Then, each newly generated node will be checked to expand into two new child nodes with probability pexpand.",
        "This process repeats until all the new non-terminal nodes are checked.",
        "Obviously, pexpand controls the scale of tree fragments, where a large pexpand corresponds to large fragments"
      ]
    },
    {
      "heading": "6. The",
      "text": [
        "new terminal nodes (words) are drawn uniformly from the target-side vocabulary, and the non-terminal nodes are created by asking two questions as follows: 1) What type is the node, one-word, two-word or multi-word non-terminal?",
        "2) What tag is used to label the node?",
        "The answer to question 1) is chosen from a uniform distribution, i.e., the probability is 1/3 for each type of non-terminal.",
        "The entire generation process is in a top-down manner, i.e., generating a parent node first and then its children.",
        "With respect to question 2), because the father node has determined the POS tags of boundary words, we only need one POS tag to generate the label of the current node.",
        "For example, in Figure 1, as the father node PRP?RB demands that the POS tag of the rightmost word is RB, the right child of PRP?RB must also satisfy this condition.",
        "Therefore, we choose a POS tag VBP and obtain the label VBP+RB.",
        "The POS tag is drawn uniformly from the POS tag set.",
        "If the current node is a one-word non-terminal, question 2) is unnecessary.",
        "Similarly, with respect to the two-word non-terminal node, questions 1) and 2) are both unnecessary for its two child nodes because they have already been defined by their father node.",
        "As an example of the generative process, the tree fragment in Figure 1 is created as follows:",
        "a.",
        "Determine that the left child of PRP?RB is a one-word non-terminal (labeled with PRP); b.",
        "Expand PRP and generate the word ?we?",
        "for PRP; 6 In our experiment, we set pexpand to 1/3 to encourage small tree fragments.",
        "c. Determine that the right child of PRP?RB is a two-word non-terminal; d. Utilize the predetermined RB and a POS tag VBP to form the tag of the two-word non-terminal: VBP+RB; e. Expand VBP+RB (to VBP and RB); f. Do not expand VBP and RB.",
        "generating the source string, which contains several source words and variables.",
        "Inspired by (Blunsom et al., 2009) and (Cohn and Blunsom, 2009), we define ( |)P str frag as follows:",
        "where csw is the number of words in the source string.",
        "'s means the source vocabulary set.",
        "Further, cvar denotes the number of variables, which is determined by the tree fragment frag.",
        "As shown in Equation(5), we first determine how many source words to generate using a Poisson distribution Ppoisson(csw;1), which imposes a stable preference for short source strings.",
        "Then, we draw each source word from a uniform distribution over ?s.",
        "Afterwards, we insert the variables into the string.",
        "The variables are inserted one at a time using a uniform distribution over the possible positions.",
        "This factor discourages more variables.",
        "For the example rule in Figure 1, the generative process of the source string is:",
        "a.",
        "Decide to generate one source word; b.",
        "Generate the source word ???",
        "(wo-men) ?",
        "; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable.",
        "Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information.",
        "DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees.",
        "Our base distribution is also designed based on this intuition.",
        "Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables.",
        "The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees.",
        "With these types of trees, we can extract small rules with good generality and simultaneously obtain large rules with enough context information by composition.",
        "We will show the effectiveness of our U-trees in the verification experiment.",
        "5 Model Training by Gibbs Sampling In this section, we introduce a collapsed Gibbs sampler, which enables us to train the Bayesian model efficiently."
      ]
    },
    {
      "heading": "5.1 Initialization State",
      "text": [
        "At first, we use random binary trees to initialize the sampler.",
        "To get the initial U-trees, we recursively and randomly segment a sentence into two parts and simultaneously create a tree node to dominate each part.",
        "The created tree nodes are labeled by the non-terminals described in section 3.",
        "Using the initial target U-trees, source sentences and word alignment, we extract minimal GHKM translation rules7 in terms of frontier nodes (Galley et al., 2004).",
        "Frontier nodes are the tree nodes that can map onto contiguous substrings on the source side via word alignment.",
        "For example, the bold italic nodes with shadows in Figure 2 are frontier nodes.",
        "In addition, it should be noted that the word alignment is fixed8, and we only explore the entire space of tree structures in our sampler.",
        "Differently, Cohn and Blunsom (2009) designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment.",
        "We believe that it is possible to investigate the space of both tree structure and alignment simultaneously.",
        "This subject will be one of our future work topics.",
        "For each training instance (a pair of source sentence and target U-tree structure), the extracted GHKM minimal translation rules compose a unique STSG derivation9.",
        "Moreover, all the rules developed from the training data constitute an initial STSG for the Gibbs sampler.",
        "7 We attach the unaligned word to the lowest frontier node that can cover it in terms of word alignment.",
        "8 The sampler might reinforce the frequent alignment errors (AE), which would harm the translation model (TM).",
        "Actually, the frequent AEs also greatly impair the conventional TM.",
        "Besides, our sampler encourages the correct alignments and simultaneously discourages the infrequent AEs.",
        "Thus, compared with the conventional TMs, we believe that our final TM would not be worse due to AEs.",
        "Our final experiments verify this point and we will conduct a much detailed analysis in future.",
        "9 We only use the minimal GHKM rules (Galley et al., 2004) here to reduce the complexity of the sampler.",
        "bold italic nodes with shadows are frontier nodes.",
        "Under this initial STSG, the sampler modifies the initial U-trees (initial sample) to create a series of new ones (new samples) by the Gibbs operators.",
        "Consequently, new STSGs are created based on the new U-trees simultaneously and used for the next sampling operation.",
        "Repeatedly and after a number of iterations, we can obtain the final U-trees for building translation models."
      ]
    },
    {
      "heading": "5.2 The Gibbs Operators",
      "text": [
        "In this section, we develop three novel Gibbs operators for the sampler.",
        "They explore the entire space of the U-tree structures by performing local changes on the tree nodes.",
        "For a U-tree of a given sentence, we define s-node as the non-root node covering at least two words.",
        "Thus, the set of s-node contains all the tree nodes except the root node, the pre-terminal nodes and leaf nodes, which we call non-s-node.",
        "For example, in Figure 2, PRB?RB and PRP+VBP are s-nodes, while NN and NN?RB are non-s-nodes.",
        "Since the POS tag sequence of the sentence is fixed, all non-s-nodes would stay unchanged in all possible U-trees of the sentence.",
        "Based on this fact, our Gibbs operators work only on s-nodes.",
        "Further, we assign 3 descendant candidates (DC) for each s-node: its left child, right child and its sibling.",
        "For example, in Figure 3, the 3 DCs for the s-node are node PRP, VBP and RB respectively.",
        "According to the different DCs it governs, every s-node might be in one of the two different states: 1) Left state: as Figure 3(a) shows, the s-node governs the left two DCs, PRP and VBP, and is labeled PRP+VBP.",
        "2) Right state: as Figure 3(b) shows, the s-node governs the right two DCs, VBP and RB, and is labeled VBP+RB.",
        "For a specific U-tree, the states of s-nodes are fixed.",
        "Thus, by changing an s-node's state, we can easily transform this U-tree to another one, i.e., from the current sample to a new one.",
        "To formulate the U-tree transformation process, we associate a binary variable ??",
        "{0,1} with each s-node, indicating whether the s-node is in the left ?",
        "or right state ?",
        "Then we can change the U-tree by changing value of the ?",
        "parameters.",
        "Our first Gibbs operator, Rotate, just works by sampling value of the ?parameters, one at a time, and changing the U-tree accordingly.",
        "For example, in Figure 3(a), the s-node is currently in the left VWDWH?",
        ":HVDPSOHWKH?RIWKLVQRGHDQGLI WKHVDPSOHGYDOXHRI?LVZHNHHSWKHVWUXFWXUH unchanged, i.e., in the left state.",
        "Otherwise, we change its state to the right state ?",
        ", and transform the U-tree to Figure 3(b) accordingly.",
        "state respectively.",
        "The bold italic nodes with shadows in the figure are frontier nodes.",
        "Obviously, towards an s-node for sampling, the two values of ?",
        "would define two different U-trees.",
        "Using the GHKM algorithm (Galley et al. 2004), we can get two different STSG derivations from the two U-trees based on the fixed word alignment.",
        "Each derivation carries a set of STSG rules (i.e., minimal GHKM translation rules) of its own.",
        "In the two derivations, the STSG rules defined by the two states include the one rooted at the s-node's lowest ancestor frontier node, and the one rooted at the s-node if it is a frontier node.",
        "For instance, in Figure 3(a), as the s-node is not a frontier node, the left state (? )",
        "defines only one rule:",
        "Differently, in Figure 3(b), the s-node is a frontier node and thus the right state (?",
        "1) defines two rules:",
        "Using these STSG rules, the two derivations are evaluated as follows (We use the value of ?",
        "to denote the corresponding STSG derivation):",
        "Where r refers to the conditional context, i.e., the set of all other rules in the training data.",
        "All the probabilities in the above formulas are computed by Equation(3).",
        "We then normalize the two scores and sample a value of ?",
        "based on them.",
        "With the Bayesian model described in section 4, the sampler ZLOOSUHIHUWKH?WKDWSURGXFHVVPDOODQGIUHTXHQW STSG rules.",
        "This tendency results in more frontier nodes in the U-tree (i.e., the s-node tends to be in the state that is a frontier node), which will factor the training instance into more small STSG rules.",
        "In this way, the overall likelihood of the bilingual data is improved by the sampler.",
        "Theoretically, the Rotate operator is capable of arriving at any possible U-tree from the initial U-tree.",
        "This is because we can first convert the initial U-tree to a left branch tree by the Rotate operator, and then transform it to any other U-tree.",
        "However, it may take a long time to do so.",
        "Thus, to speed up the structure transformation process, we employ a Two-level-Rotate operator, which takes a pair of s-nodes in a parent-child relationship as a unit for sampling.",
        "Similar to the Rotate operator, we also assign a binary variable ??",
        "{0,1} to each unit and update the U-tree by sampling the value of ?.",
        "The method of sampling ?",
        "is similar to the one used for ?.",
        "Figure 4 shows an example of the operator.",
        "As shown in Figure 4(a), the unit NN?VBP and PRP+VBP is in the left state (?=0), and governs the left three descendants: NN, PRP, and VBP.",
        "By the Two-level-Rotate operator, we can convert the unit to Figure 4(b), i.e., the ULJKWVWDWH?=1).",
        "Just as Figure 4(b) shows, the governed descendants of the unit are turned to PRP, VBP, and RB.",
        "It may be confusing when choosing the parent-child s-node pair for sampling because the parent node always faces two choices: combining the left child or right child for sampling.",
        "To avoid confusion, we split the Two-level-Rotate operator into two operators: Two-level-left-Rotate operator, which works with the parent node and its left child, and Two-level-right-Rotate operator, which only considers the parent node and its right child 10 .",
        "Therefore, the operator used in Figure 4 is a Two-level-right-Rotate operator.",
        "The bold italic nodes with shadows in the Figure are frontier nodes.",
        "During sampling, for each training instance, the sampler first applies the Two-level-left-Rotate operator to all candidate pairs of s-nodes (parent s-node and its left child s-node) in the U-tree.",
        "After that, the Two-level-right-Rotate operator is applied to all the candidate pairs of s-nodes (parent s-node and its right child s-node).",
        "Then, we use the Rotate operator on every s-node in the U-tree.",
        "By utilizing the operators separately, we can guarantee that our sampler satisfies detailed balance.",
        "We visit all the training instances in a random order (one iteration).",
        "After a number of iterations, we can obtain the final U-tree structures and build the tree-based translation model accordingly."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": []
    },
    {
      "heading": "6.1 Experimental Setup",
      "text": [
        "The experiments are conducted on Chinese-to-English translation.",
        "The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.",
        "We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.",
        "We train a 5 gram language model on the Xinhua portion of the English Gigaword corpus and the English part of 10 We can also take more nodes as a unit for sampling, but this would make the algorithm much more complex.",
        "the training data.",
        "For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.",
        "We use MERT (Och, 2004) to tune parameters.",
        "Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.",
        "The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.",
        "The statistical significance test is performed by the re-sampling approach (Koehn, 2004).",
        "To create the baseline system, we use the open-source Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively.",
        "The translation system used for testing the effectiveness of our U-trees is our in-house string-to-tree system (abbreviated as s2t).",
        "The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006).",
        "In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side.",
        "We then obtain the composed rules by composing two or three adjacent minimal rules.",
        "To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006).",
        "Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system.",
        "For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.",
        "The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.",
        "For the hyperparameters, we set ?",
        "to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments.",
        "We built an s2t translation system with the achieved U-trees after the 1000th iteration.",
        "We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large.",
        "11 From (Zollmann and Vogel, 2011), we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags.",
        "Thus, to be convenient, we only conduct experiments with the SAMT system."
      ]
    },
    {
      "heading": "6.2 Analysis of The Gibbs Sampler",
      "text": [
        "To evaluate the effectiveness of the Gibbs sampler, we explore the change of the training data's likelihood with increasing sampling iterations.",
        "the number of sampling iterations.",
        "In the figure, random 1 to 3 refers to three independent runs of the sampler with different initial U-trees as initialization states.",
        "Figure 5 depicts the negative-log likelihood of the training data after several sampling iterations.",
        "The results show that the overall likelihood of the training data is improved by the sampler.",
        "Moreover, comparing the three independent runs, we see that although the sampler begins with different initial U-trees, the training data's likelihood is always similar during sampling.",
        "This demonstrates that our sampler is not sensitive to the random initial U-trees and can always arrive at a good final state beginning from different initialization states.",
        "Thus, we only utilize the U-trees from random 1 for further analysis hereafter."
      ]
    },
    {
      "heading": "6.3 Analysis of the U-tree Structure",
      "text": [
        "Acquiring better U-trees for translation is our final purpose.",
        "However, are the U-trees achieved by the",
        "Gibbs sampler appropriate for the tree-based translation model?",
        "To answer this question, we first analyze the effect of the sampler on the U-trees.",
        "Figure 6 shows the total number of frontier nodes in the training data during sampling.",
        "The results show that the number of frontier nodes increases with increased sampling.",
        "This tendency indicates that our sampler prefers the tree structure with more frontier nodes.",
        "Consequently, the final U-tree structures can always be factored into many small minimal translation rules.",
        "Just as we have argued in section 4.1, this is beneficial for a good translation grammar.",
        "To demonstrate the above analysis, Figure 7 shows a visual comparison between our U-tree (from random 1) and the binary parse tree (found by head binarization).",
        "Because the traditional parse tree is not binarized, we do not consider it for this analysis.",
        "Figure 7 shows that whether it is the target tree fragment or the source string of the rule, our U-trees always tend to obtain the smaller ones12.",
        "This comparison verifies that our Bayesian tree induction model is effective in shifting the tree structures away from complex minimal rules, which tend to negatively affect translation.",
        "statistics comparing our U-trees and binary parse trees.",
        "12 Binary parse trees get more tree fragments with two nodes than U-trees.",
        "This is because there are many unary edges in the binary parse trees, while no unary edge exists in our U-trees.",
        "Specifically, we show an example of a binary parse tree and our U-tree in Figure 8.",
        "The example U-tree is more conducive to extracting effective translation rules.",
        "For example, to translate the Chinese phrase ??",
        "?",
        "?, we can extract a rule (R2 in Figure 9) directly from the U-tree because the phrase ??",
        "??",
        "is governed by a frontier node, i.e., node ?VBD+RB?.",
        "However, because no node governs ??",
        "??",
        "in the binary parse tree, we can only obtain a rule (R1 in Figure 9) with many extra nodes and edges, such as node CD in R1.",
        "Due to these extra things, R1 is too large to show good generality.",
        "??",
        "?",
        ".?",
        "R1 is extracted from Figure 8(a), i.e., the binary parse tree.",
        "R2 is from Figure 8(b), i.e., the U-tree.",
        "Based on the above analysis, we can conclude that our proposed U-tree structures are conducive to extracting small, minimal translation rules.",
        "This indicates that the U-trees are more consistent with the word alignment and are good at capturing bilingual mapping information.",
        "Therefore, because parse trees are always constrained by cross-lingual structure divergence, we believe that the proposed U-trees would result in a better translation grammar.",
        "We demonstrate this conclusion in the next sub-section."
      ]
    },
    {
      "heading": "6.4 Final Translation Results",
      "text": [
        "The final translation results are shown in Table 1.",
        "In the table, lines 3-6 refer to the string-to-tree systems built with different types of tree structures.",
        "Table 1 shows that all our s2t systems outperform the Joshua (HPB) and Joshua (SAMT) system significantly.",
        "This comparison verifies the superiority of our in-house s2t system.",
        "Moreover, the results shown in Table 1 also demonstrate the effectiveness of head binarization, which helps to improve the s2t system using parse trees in all translation tasks.",
        "To test the effectiveness of our U-trees, we give the s2t translation system using the U-trees (from random 1).",
        "The results show that the system using U-trees achieves the best translation result from all of the systems.",
        "It surpasses the s2t system using parse trees by 1.47 BLEU points on MT04 and 1.44 BLEU points on MT05.",
        "Moreover, even using the binary parse trees, the achieved s2t system is still lower than our U-tree-based s2t system by 0.97 BLEU points on the combined test set.",
        "From the translation results, we can validate our former analysis that the U-trees generated by our Bayesian tree induction model are more appropriate for string-to-tree translation than parse trees.",
        "s2t systems using different types of trees.",
        "The ?*?",
        "and ?#?",
        "denote that the results are significantly better than the Joshua (SAMT) system and the s2t system using parse trees (p<0.01)."
      ]
    },
    {
      "heading": "6.5 Large Data",
      "text": [
        "We also conduct an experiment on a larger bilingual training data from the LDC corpus13.",
        "The training corpus contains 2.1M sentence pairs with approximately 27.7M Chinese words and 31.9M English words.",
        "Similarly, we train a 5-gram language model using the Xinhua portion of the English Gigaword corpus and the English part of the training corpus.",
        "With the same settings as before, we run the Gibbs sampler for 1000 iterations and utilize the final U-tree structure to build a string-to-tree translation system.",
        "The final BLEU score results are shown in Table 2.",
        "In the scenario with a large data, the string-to-tree system using our U-trees still significantly outperforms the system using parse trees.",
        "the large training data.",
        "The meaning of ?*?",
        "and ?#?",
        "are similar to Table 1."
      ]
    },
    {
      "heading": "7 Conclusion and Future Work",
      "text": [
        "In this paper, we explored a new direction to build a tree-based model based on unsupervised Bayesian trees rather than supervised parse trees.",
        "To achieve this purpose, we have made two major efforts in this paper: (1) We have proposed a novel generative Bayesian model to induce effective U-trees for tree-based translation.",
        "We utilized STSG in the model to grasp bilingual mapping information.",
        "We further imposed a reasonable hierarchical prior on the tree structures, encouraging small and frequent minimal rules for translation.",
        "(2) To train the Bayesian tree induction model efficiently, we developed a Gibbs sampler with three novel Gibbs operators.",
        "The operators are designed specifically to explore the infinite space of tree structures by performing local changes on the tree structure."
      ]
    },
    {
      "heading": "13 LDC category number : LDC2000T50, LDC2002E18,",
      "text": [
        "Experiments on the string-to-tree translation model demonstrated that our U-trees are better than the parse trees.",
        "The translation results verify that the well-designed unsupervised trees are actually more appropriate for tree-based translation than parse trees.",
        "Therefore, we believe that the unsupervised tree structure would be a promising research direction for tree-based translation.",
        "In future, we plan to testify our sampler with various initial trees, such as the tree structure formed by (Zhang et al., 2008).",
        "We also plan to perform a detailed empirical comparison between STST and SCFG under our settings.",
        "Moreover, we will further conduct experiments to compare our methods with other relevant works, such as (Cohn and Blunsom, 2009) and (Burkett and Klein, 2012)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank Philipp Koehn and three anonymous reviewers for their valuable comments and suggestions.",
        "The research work has been funded by the Hi-Tech Research and Development Program (?863?",
        "Program) of China under Grant No.",
        "2011AA01A207, 2012AA011101, and 2012AA011102."
      ]
    }
  ]
}
