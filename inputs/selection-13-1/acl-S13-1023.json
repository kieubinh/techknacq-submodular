{
  "info": {
    "authors": [
      "Davide Buscaldi",
      "Joseph Le Roux",
      "Jorge J. Garcia Flores",
      "Adrian Popescu"
    ],
    "book": "*SEM",
    "id": "acl-S13-1023",
    "title": "LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Syntactic Analysis, ESA and Information Retrieval based Features",
    "url": "https://aclweb.org/anthology/S13-1023",
    "year": 2013
  },
  "references": [
    "acl-N04-3012",
    "acl-P05-1045",
    "acl-P09-1053",
    "acl-P94-1019",
    "acl-S12-1059",
    "acl-W06-3104",
    "acl-W09-3204"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes the system used by the LIPN team in the Semantic Textual Similarity task at *SEM 2013.",
        "It uses a support vector regression model, combining different text similarity measures that constitute the features.",
        "These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances like Explicit Semantic Analysis, WordNet-based similarity, IR-based similarity, and a similarity measure based on syntactic dependencies."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The Semantic Textual Similarity task (STS) at *SEM 2013 requires systems to grade the degree of similarity between pairs of sentences.",
        "It is closely related to other well known tasks in NLP such as textual entailment, question answering or paraphrase detection.",
        "However, as noticed in (Ba?r et al., 2012), the major difference is that STS systems must give a graded, as opposed to binary, answer.",
        "One of the most successful systems in *SEM 2012 STS, (Ba?r et al., 2012), managed to grade pairs of sentences accurately by combining focused measures, either simple ones based on surface features (ie n-grams), more elaborate ones based on lexical semantics, or measures requiring external corpora such as Explicit Semantic Analysis, into a robust measure by using a log-linear regression model.",
        "The LIPN-CORE system is built upon this idea of combining simple measures with a regression model to obtain a robust and accurate measure of textual similarity, using the individual measures as features for the global system.",
        "These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances like Explicit Semantic Analysis, WordNet-based similarity, IR-based similarity, and a similarity measure based on syntactic dependencies.",
        "The paper is organized as follows.",
        "Measures are presented in Section 2.",
        "Then the regression model, based on Support Vector Machines, is described in Section 3.",
        "Finally we discuss the results of the system in Section 4."
      ]
    },
    {
      "heading": "2 Text Similarity Measures",
      "text": []
    },
    {
      "heading": "2.1 WordNet-based Conceptual Similarity",
      "text": [
        "(Proxigenea) First of all, sentences p and q are analysed in order to extract all the included WordNet synsets.",
        "For each WordNet synset, we keep noun synsets and put into the set of synsets associated to the sentence, Cp and Cq, respectively.",
        "If the synsets are in one of the other POS categories (verb, adjective, adverb) we look for their derivationally related forms in order to find a related noun synset: if there is one, we put this synsets in Cp (or Cq).",
        "For instance, the word ?playing?",
        "can be associated in WordNet to synset (v)play#2, which has two derivationally related forms corresponding to synsets (n)play#5 and (n)play#6: these are the synsets that are added to the synset set of the sentence.",
        "No disambiguation process is carried out, so we take all possible meanings into account.",
        "GivenCp andCq as the sets of concepts contained in sentences p and q, respectively, with |Cp |?",
        "|Cq|,",
        "the conceptual similarity between p and q is calculated as:",
        "where s(c1, c2) is a conceptual similarity measure.",
        "Concept similarity can be calculated by different ways.",
        "For the participation in the 2013 Semantic Textual Similarity task, we used a variation of the Wu-Palmer formula (Wu and Palmer, 1994) named ?ProxiGenea?",
        "(from the french Proximite?",
        "Ge?ne?alogique, genealogical proximity), introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet.",
        "Among the different formulations proposed by (Dudognon et al., 2010), we chose the ProxiGenea3 variant, already used in the STS 2012 task by the IRIT team (Buscaldi et al., 2012).",
        "The ProxiGenea3 measure is defined as:",
        "(2) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS).",
        "The function returning the depth of a concept is noted with d."
      ]
    },
    {
      "heading": "2.2 IC-based Similarity",
      "text": [
        "This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik's Information Content (IC) and the Jiang-Conrath (Jiang and Conrath, 1997) similarity metric:",
        "where IC is the information content introduced by (Resnik, 1995) as IC(c) = ?",
        "logP (c).",
        "The similarity between two text segments T1 and T2 is therefore determined as:",
        "where idf(w) is calculated as the inverse document frequency of word w, taking into account Google Web 1T (Brants and Franz, 2006) frequency counts.",
        "The semantic similarity between words is calculated as:",
        "sjc(ci, cj).",
        "(5) where Wi and Wj are the sets containing all synsets in WordNet corresponding to word wi and wj , respectively.",
        "The IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus1."
      ]
    },
    {
      "heading": "2.3 Syntactic Dependencies",
      "text": [
        "We also wanted for our systems to take syntactic similarity into account.",
        "As our measures are lexically grounded, we chose to use dependencies rather than constituents.",
        "Previous experiments showed that converting constituents to dependencies still achieved best results on out-of-domain texts (Le Roux et al., 2012), so we decided to use a 2-step architecture to obtain syntactic dependencies.",
        "First we parsed pairs of sentences with the LORG parser2.",
        "Second we converted the resulting parse trees to Stanford dependencies3.",
        "Given the sets of parsed dependenciesDp andDq, for sentence p and q, a dependency d ?",
        "Dx is a triple (l, h, t) where l is the dependency label (for instance, dobj or prep), h the governor and t the dependant.",
        "We define the following similarity measure between two syntactic dependencies d1 = (l1, h1, t1)",
        "where idfh = max(idf(h1), idf(h2)) and idft = max(idf(t1), idf(t2)) are the inverse document frequencies calculated on Google Web 1T for the governors and the dependants (we retain the maximum for each pair), and sWN is calculated using formula 2, with two differences: ?",
        "if the two words to be compared are antonyms, then the returned score is 0;",
        "?",
        "if one of the words to be compared is not in WordNet, their similarity is calculated using the Levenshtein distance.",
        "The similarity score between p and q, is then calculated as:"
      ]
    },
    {
      "heading": "2.4 Information Retrieval-based Similarity",
      "text": [
        "Let us consider two texts p and q, an Information Retrieval (IR) system S and a document collection D indexed by S. This measure is based on the assumption that p and q are similar if the documents retrieved by S for the two texts, used as input queries, are ranked similarly.",
        "Let be Lp = {dp1 , .",
        ".",
        ".",
        ", dpK} and Lq = {dq1 , .",
        ".",
        ".",
        ", dqK}, dxi ?",
        "D the sets of the top K documents retrieved by S for texts p and q, respectively.",
        "Let us define sp(d) and sq(d) the scores assigned by S to a document d for the query p and q, respectively.",
        "Then, the similarity score is calculated as:",
        "For the participation in this task we indexed a collection composed by the AQUAINT-24 and the English NTCIR-85 document collections, using the Lucene6 4.2 search engine with BM25 similarity.",
        "The K value was empirically set to 20 after some tests on the STS 2012 data.",
        "are supposed to quantify the strength of the relation between a word and each Wikipedia concept using the tf-idf measure.",
        "A text is then represented as a high-dimensional real valued vector space spanning all along the Wikipedia database.",
        "For this particular task we adapt the research-esa implementation (Sorg and Cimiano, 2008)7 to our own home-made weighted vectors corresponding to a Wikipedia snapshot of February 4th, 2013."
      ]
    },
    {
      "heading": "2.6 N-gram based Similarity",
      "text": [
        "This feature is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi et al., 2009) for the passage retrieval task.",
        "The similarity between a text fragment p and another text fragment q is calculated as:",
        "Where P is the set of n-grams with the highest weight in p, where all terms are also contained in q; Q is the set of all the possible n-grams in q and n is the total number of terms in the longest passage.",
        "The weights for each term and each n-gram are calculated as: ?",
        "wi calculates the weight of the term tI as:",
        "Where ni is the frequency of term ti in the Google Web 1T collection, and N is the frequency of the most frequent term in the Google Web 1T collection.",
        "?",
        "the function h(x, P ) measures the weight of each n-gram and is defined as:",
        "Where wk is the weight of the k-th term (see Equation 10) and j is the number of terms that compose the n-gram x; ?",
        "1d(x,xmax) is a distance factor which reduces the weight of the n-grams that are far from the heaviest n-gram.",
        "The function d(x, xmax) determines numerically the value of the separation according to the number of words between a n-gram and the heaviest one: d(x, xmax) = 1 + k?",
        "ln(1 + L) (12) where k is a factor that determines the importance of the distance in the similarity calculation and L is the number of words between a n-gram and the heaviest one (see Equation 11).",
        "In our experiments, k was set to 0.1, the default value in the original model."
      ]
    },
    {
      "heading": "2.7 Other measures",
      "text": [
        "In addition to the above text similarity measures, we used also the following common measures:",
        "Given p = (wp1 , .",
        ".",
        ".",
        ", wpn) and q = (wq1 , .",
        ".",
        ".",
        ", wqn) the vectors of tf.idf weights associated to sentences p and q, the cosine distance is calculated as:",
        "The idf value was calculated on Google Web 1T.",
        "This similarity measure is calculated using the Levenshtein distance as:",
        "where Lev(p, q) is the Levenshtein distance between the two sentences, taking into account the characters.",
        "We used the Stanford Named Entity Recognizer by (Finkel et al., 2005), with the 7 class model trained for MUC: Time, Location, Organization, Person, Money, Percent, Date.",
        "Then we calculated a per-class overlap measure (in this way, ?France?",
        "as an Organization does not match ?France?",
        "as a Location):",
        "where Np and Nq are the sets of NEs found, respectively, in sentences p and q."
      ]
    },
    {
      "heading": "3 Integration of Similarity Measures",
      "text": [
        "The integration has been carried out using the ?-Support Vector Regression model (?-SVR) (Scho?lkopf et al., 1999) implementation provided by LIBSVM (Chang and Lin, 2011), with a radial basis function kernel with the standard parameters (?",
        "= 0.5)."
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "In order to evaluate the impact of the different features, we carried out an ablation test, removing one feature at a time and training a new model with the reduced set of features.",
        "In Table 2 we show the results of the ablation test for each subset of the *SEM 2013 test set; in Table 1 we show the same test on the whole test set.",
        "Note: the results have been calculated as the Pearson correlation test on the whole test set and not as an average of the correlation scores calculated over the composing test sets.",
        "The ablation test show that the IR-based feature showed up to be the most effective one, especially for the headlines subset (as expected), and, quite surprisingly, on the OnWN data.",
        "In Table 3 we show the correlation between each feature and the result (feature values normalised between 0 and 5): from this table we can also observe that, on average, IR-based similarity was better able to capture the semantic similarity between texts.",
        "The only exception was the FNWN test set: the IR-based similarity returned a 0 score 178 times out of 189 (94.1%), indicating that the indexed corpus did not fit the content of the FNWN sentences.",
        "This result shows also the limits of the IR-based similarity score which needs a large corpus to achieve enough coverage."
      ]
    },
    {
      "heading": "4.1 Shared submission with INAOE-UPV",
      "text": [
        "One of the files submitted by INAOE-UPV, INAOE-UPV-run3 has been produced using seven features produced by different teams: INAOE, LIPN and UMCC-DLSI.",
        "We contributed to this joint submission with the IR-based, WordNet and cosine features."
      ]
    },
    {
      "heading": "5 Conclusions and Further Work",
      "text": [
        "In this paper we introduced the LIPN-CORE system, which combines semantic, syntactic an lexical measures of text similarity in a linear regression model.",
        "Our system was among the best 15 runs for the STS task.",
        "According to the ablation test, the best performing feature was the IR-based one, where a sentence is considered as a query and its meaning represented as a set of documents indexed by an IR system.",
        "The second and third best-performing measures were WordNet similarity and Levenshtein's edit distance.",
        "On the other hand, worst performing similarity measures were Named Entity Overlap, Syntactic Dependencies and ESA.",
        "However, a correlation analysis calculated on the features taken one-by-one shows that the contribution of a feature",
        "on the overall regression result does not correspond to the actual capability of the measure to represent the semantic similarity between the two texts.",
        "These results raise the methodological question of how to combine semantic, syntactic and lexical similarity measures in order to estimate the impact of the different strategies used on each dataset.",
        "Further work will include richer similarity measures, like quasi-synchronous grammars (Smith and Eisner, 2006) and random walks (Ramage et al., 2009).",
        "Quasi-synchronous grammars have been used successfully for paraphrase detection (Das and Smith, 2009), as they provide a fine-grained model-ing of the alignment of syntactic structures, in a very flexible way, enabling partial alignments and the inclusion of external features, like Wordnet lexical relations for example.",
        "Random walks have been used effectively for paraphrase recognition and as a feature for recognizing textual entailment.",
        "Finally, we will continue analyzing the question of how to combine a wide variety of similarity measures in such a way that they tackle the semantic variations of each dataset."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the Quaero project and the LabEx EFL8 for their support to this work."
      ]
    }
  ]
}
