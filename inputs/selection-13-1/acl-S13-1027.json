{
  "info": {
    "authors": [
      "Sara Noeman"
    ],
    "book": "*SEM",
    "id": "acl-S13-1027",
    "title": "IBM_EG-CORE: Comparing multiple Lexical and NE matching features in measuring Semantic Textual similarity",
    "url": "https://aclweb.org/anthology/S13-1027",
    "year": 2013
  },
  "references": [
    "acl-N03-1017",
    "acl-N03-1020",
    "acl-N04-3012",
    "acl-N06-1003",
    "acl-N07-1008",
    "acl-P02-1040",
    "acl-P07-2045",
    "acl-P12-2028",
    "acl-P94-1019"
  ],
  "sections": [
    {
      "text": [
        "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 187?193, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics IBM_EG-CORE: Comparing multiple Lexical and NE matching features in measuring Semantic Textual similarity"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "We present in this paper the systems we participated with in the Semantic Textual Similarity task at SEM 2013.",
        "The Semantic Textual Similarity Core task (STS) computes the degree of semantic equivalence between two sentences where the participant systems will be compared to the manual scores, which range from 5 (semantic equivalence) to 0 (no relation).",
        "We combined multiple text similarity measures of varying complexity.",
        "The experiments illustrate the different effect of four feature types including direct lexical matching, idf-weighted lexical matching, modified BLEU N-gram matching and named entities matching.",
        "Our team submitted three runs during the task evaluation period and they ranked number 11, 15 and 19 among the 90 participating systems according to the official Mean Pearson correlation metric for the task.",
        "We also report an unofficial run with mean Pearson correlation of 0.59221 on STS2013 test dataset, ranking as the 3rd best system among the 90 participating systems."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The Semantic Textual Similarity (STS) task at SEM 2013 is to measure the degree of semantic equivalence between pairs of sentences as a graded notion of similarity.",
        "Text Similarity is very important to many Natural Language Processing applications, like extractive summarization (Salton et al., 1997), methods for automatic evaluation of machine translation (Papineni et al., 2002), as well as text summarization (Lin and Hovy, 2003).",
        "In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words.",
        "For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) introduced a sense similarity measure using the sentence similarity of the sense definitions.",
        "In this paper we illustrate the different effect of four feature types including direct lexical matching, idf-weighted lexical matching, modified BLEU N-gram matching and named entities matching.",
        "The rest of this paper will proceed as follows, Section 2 describes the four text similarity features used.",
        "Section 3 illustrates the system description, data resources as well as Feature combination.",
        "Experiments and Results are illustrated in section 4. then we report our conclusion and future work."
      ]
    },
    {
      "heading": "2 Text Similarity Features",
      "text": [
        "Our system measures the semantic textual similarity between two sentences through a number of matching features which should cover four main dimensions: i) Lexical Matching ii) IDF-weighted Lexical Matching iii) Contextual sequence Matching (Modified BLEU Score), and iv) Named Entities Matching.",
        "First we introduce the alignment technique used.",
        "For a sentence pair {s1, s2} matching is done in each direction separately to detect the sub-sentence of s1 matched to s2 and then detect the sub-sentence of s2 matched to s1.",
        "For each word wi in s1 we search for its match wj in s2 according to matching features.",
        "S1: w0 w1 w2 w3 w4 ?...",
        "wi ?...",
        "wn S2: w0 w1 w2 w3 w4 ?.......wj ?.........",
        "wm"
      ]
    },
    {
      "heading": "2.1 Lexical Matching:",
      "text": [
        "In this feature we handle the two sentences as bags of words to be matched using three types of matching, given that all stop words are cleaned out before matching: I) Exact word matching.",
        "II) Stemmed word matching: I used Porter Stemming algorithm (M.F.",
        "Porter, 1980) in matching, where it is a process for removing the commoner morphological and inflectional endings from words in English.",
        "Stemming will render inflections like ?requires, required, requirements, ...?",
        "to ?requir?",
        "so they can be easily matched III) Synonyms matching: we used a corpus based dictionary of 58,921 entries and their equivalent synonyms.",
        "The next section describes how we automatically generated this language resource."
      ]
    },
    {
      "heading": "2.2 IDF-weighted Lexical Matching",
      "text": [
        "We used the three matching criteria used in Lexical Matching after weighting them with Inverse-Document-Frequency.",
        "we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf-weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions.",
        "For a sentence pair s1, s2, if s1 consists of m words {w0, w1, ?., w(m-1)} and s2 consists of n words {w0, w1, ?., w(n-1)} ,after cleaning stop words from both, and the matched words are ?@Matched_word_List?",
        "of ?k?",
        "words, then"
      ]
    },
    {
      "heading": "2.3 Contextual Sequence Matching (Modified",
      "text": [
        "BLEU score) We used a modified version of Bleu score to measure n-gram sequences matching, where for sentence pair s1, s2 we align the matched words between them (through exact, stem, synonyms match respectively).",
        "Bleu score as presented by (K. Papineni et al., 2002) is an automated method for evaluating Machine Translation.",
        "It compares n-grams of the candidate translation with the n-grams of the reference human translation and counts the number of matches.",
        "These matches are position independent, where candidate translations with unmatched length to reference translations are penalized with Sentence brevity penalty.",
        "This helps in measuring n-gram similarity in sentences structure.",
        "We define ?matched sequence?",
        "of a sentence S1 as the sequence of words {wi, wi+1, wi+2, ?..",
        "wj}, where wi, and wj are the first and last words in sentence S1 that are matched with words in S2.",
        "For example in sentence pair S1, S2: S1: Today's great Pax Europa and today's pan-European prosperity depend on this.",
        "S2: Large Pax Europa of today, just like current prosperity paneurop?enne, depends on it.",
        "After stemming: S1: todai's great pax europa and todai's pan-european prosper depend on thi.",
        "S2: larg pax europa of todai, just like current prosper paneurop?enn, depend on it.",
        "?Matched sequence of S1?",
        ": [todai 's great pax europa todai 's pan - european prosper depend] ?Matched sequence of S2?",
        ": [pax europa todai just like current prosper paneurop?enn depend] We measure the Bleu score such that:",
        "The objective of trimming the excess words outside the ?Matched Sequence?",
        "range, before matching is to make use of the Sentence brevity penalty in case sentence pair S1, S2 may be not similar but having matched lengths."
      ]
    },
    {
      "heading": "2.4 Named Entities Matching",
      "text": [
        "Named entities carry an important portion of sentence semantics.",
        "For example: Sentence1: In Nigeria , Chevron has been accused by the All - Ijaw indigenous people of instigating violence against them and actually paying Nigerian soldiers to shoot protesters at the Warri naval base .",
        "Sentence2: In Nigeria , the whole ijaw indigenous showed Chevron to encourage the violence against them and of up to pay Nigerian soldiers to shoot the demonstrators at the naval base from Warri .",
        "The underlined words are Named entities of different types ?COUNTRY, ORG, PEOPLE, LOC, EVENT_VIOLENCE?",
        "which capture the most important information in each sentence.",
        "Thus named entities matching is a measure of semantic matching between the sentence pair."
      ]
    },
    {
      "heading": "3 System Description",
      "text": []
    },
    {
      "heading": "3.1 Data Resources and Processing",
      "text": [
        "All data is tokenized, stemmed, and stop words are cleaned.",
        "Corpus based resources: i. Inverse Document Frequency (IDF) language resource: The document frequency df(t) of a term t is defined as the number of documents in a large collection of documents that contain a term ?t?.",
        "Terms that are likely to appear in most of the corpus documents reflect less importance than words that appear in specific documents only.",
        "That's why the Inverse Document Frequency is used as a measure of term importance in information retrieval and text mining tasks.",
        "We used the LDC English Gigaword Fifth Edition (LDC2011T07) to generate our idf dictionary.",
        "LDC Gigaword contains a huge collection of newswire from (afp, apw, cna, ltw, nyt, wpb, and xin).",
        "The generated idf resource contains 5,043,905 unique lower cased entries, and then we generated a stemmed version of the idf dictionary contains 4,677,125 entries.",
        "The equation below represents the idf of term t where N is the total number of documents in the corpus.",
        "ii.",
        "English Synonyms Dictionary: Using the Phrase table of an Arabic-to-English Direct Translation Model, we generated English-to-English phrase table using the double-link of English-to-Arabic and Arabic-to-English phrase translation probabilities over all pivot Arabic phrases.",
        "Then English-to-English translation probabilities are normalized over all generated English synonyms.",
        "(Chris Callison-Burch et al., 2006) used a similar technique to generate paraphrases to improve their SMT system.",
        "Figure (1) shows the steps: Figure(1) English phrase-to-phrase synonyms generation from E2A phrase table.",
        "In our system we used the phrase table of the Direct Translation Model 2 (DTM2) (Ittycheriah and Roukos, 2007) SMT system, where each sentence pair in the training corpus was word-aligned, e.g. using a MaxEnt aligner (Ittycheriah and Roukos, 2005) or an HMM aligner (Ge, 2004).",
        "then Block Extraction step is done.",
        "The generated phrase table contains candidate phrase to phrase translation pairs with source-to-target and target-to source translation probabilities.",
        "However the open source Moses SMT system (Koehn et al., 2007)",
        "can be used in the same way to generate a synonyms dictionary from phrase table.",
        "By applying the steps in figure (1): a) English phrase-to-phrase synonyms table (or English-to-English phrase table), by applying the steps in a generic way.",
        "b) English word-to-word synonyms table, by limiting the generation over English single word phrases.",
        "For example, to get al possible synonyms of the English word ?bike?, we used all the Arabic phrases that are aligned to ?bike?",
        "in the phrase",
        "then we get al the English words in the phrase table aligned to these Arabic translations { ????",
        "?, ???????",
        "?, ??????",
        "?, ???????? }",
        "This results in an English word-to-word synonyms list for the word ?bike?",
        "like this:",
        "database of English.",
        "Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.",
        "Synsets are interlinked by means of conceptual-semantic and lexical relations.",
        "WordNet groups words together based on their meanings and interlinks not just word forms?strings of letters?but specific senses of words.",
        "As a result, words that are found in close proximity to one another in the network are semantically disambiguated.",
        "Second, WordNet labels the semantic relations among words.",
        "Using WordNet, we can measure the semantic similarity or relatedness between a pair of concepts (or word senses), and by extension, between a pair of sentences.",
        "We use the similarity measure described in (Wu and Palmer, 1994) which finds the path length to the root node from the least common subsumer (LCS) of the two word senses which is the most specific word sense they share as an ancestor."
      ]
    },
    {
      "heading": "3.2 Feature Combination",
      "text": [
        "The feature combination step uses the precomputed similarity scores.",
        "Each of the text similarity features can be given a weight that sets its importance.",
        "Mathematically, the text similarity score between two sentences can be formulated using a cost function weighting the similarity features as follows: N.B.",
        ": The similarity score according to the features above is considered as a directional score.",
        "where w1, w2, w3, w4 are the weights assigned to the similarity features (lexical, idf-weighted, modified_BLEU, and NE_Match features respectively).",
        "The similarity score will be normalized over (w1+w2+w3+w4).",
        "In our experiments, the weights are tuned manually without applying machine learning techniques.",
        "We used both *SEM 2012 training and testing data sets for tuning these weights to get the best feature weighting combination to get highest Pearson Correlation score."
      ]
    },
    {
      "heading": "4 Experiments and Results",
      "text": []
    },
    {
      "heading": "Submitted Runs",
      "text": [
        "Our experiments showed that some features are more dominant in affecting the similarity scoring than others.",
        "We performed a separate experiment for each of the four feature types to illustrate their effect on textual semantic similarity measurement",
        "using direct lexical matching, stemming matching, synonyms matching, as well as (stem+synonyms) matching.",
        "Table (1) reports the mean Pearson correlation results of these experiments on",
        "BLEU, Lexical, and idf-weighted matching features respectively on STS2012-test dataset.",
        "The submitted runs IBM_EG-run2, IBM_EG-run5, IBM_EG-run6 are the three runs with feature weighting and experiment set up that performed best on STS 2012 training and testing data sets.",
        "Run 2: In this run the word matching was done on exact, and synonyms match only.",
        "Stemmed word matching was not introduced in this experiment.",
        "we tried the following weighting between similarity feature scores, where we decreased the weight of BLEU scoring feature to 0.5, and increased the idf_Lexical match weight of 3.5. this is because our initial tuning experiments showed that increasing the idf lexical weight compared to BLEU weight gives improved results.",
        "The NE matching feature weight was as follows: NE_weight = 1.5* percent of NE word to sentence word count = 1.5* (NE_words_count/Sentence_word_count) Run 5: In this experiment we introduced Porter stemming word matching, as well as stemmed synonyms matching (after generating a stemmed version of the synonyms dictionary).",
        "BLEU score feature was removed from this experiment, while keeping the idf-weight= 3, lexical-weight = 1, and NE-matching feature weight = 1.",
        "Run 6: For this run we kept only IDF-weighted lexical matching feature which proved to be the dominant feature in the previous runs, in addition to Porter stemming word matching, and stemmed synonyms matching.",
        "Data: the training data of STS 2013 Core task consist of the STS 2012 train and test data.",
        "This data covers 5 datasets: paraphrase sentence pairs (MSRpar), sentence pairs from video descriptions (MSRvid), MT evaluation sentence pairs (SMTnews and SMTeuroparl) and gloss pairs (OnWN)."
      ]
    },
    {
      "heading": "Results on Training Data",
      "text": [
        "System outputs will be evaluated according to the official scorer which computes weighted Mean Pearson Correlation across the evaluation datasets, where the weight depends on the number of pairs in each dataset.",
        "Table (2), reports the results achieved on each of the STS 2012 training dataset.",
        "While table (3), reports the results achieved on STS 2012 test dataset."
      ]
    },
    {
      "heading": "Results on Test Data:",
      "text": [
        "The best configuration of our system was IBM_EG-run6 which was ranked #11 for the evaluation metric Mean (r = 0.5502) when submitted during the task evaluation period .",
        "Run6 as illustrated before was planned to measure idf-weighted lexical matching feature only, over Porter stemmed, and stemmed synonyms words.",
        "However when revising this experiment set up",
        "during preparing the paper, after the evaluation period, we found that the English-to-English synonyms table was not correctly loaded during matching, thus skipping synonyms matching feature from this run.",
        "So the official result IBM_EG-run6 reports only idf-weighted matching over Porter stemmed bag of words.",
        "By fixing this and replicating the experiment IBM_EG-run6-UnOfficial as planned to be, the mean Pearson correlation jumps 4 points (r = 0.59221) which ranks this system as the 3rd system among 90 submitted systems very slightly below the 2nd system (only 0.0006 difference on the mean correlation metric).",
        "In table (4), we report the official results achieved on STS 2013 test data.",
        "While table (5), reports the unofficial results achieved after activating the synonyms matching feature in IBM_EG-run6 (unofficial) and comparing this run to the best two reported systems.",
        "synonyms matching feature in IBM_EG-run6 compared to the best two performing systems in the evaluation.",
        "Results of unofficial run: One unofficial run was performed after the evaluation submission deadline due to the tight schedule of the evaluation.",
        "This experiment introduces the effect of WordNet Wu and Palmer similarity measure on the configuration of Run5 (Porter stemming word matching, with synonyms matching, zero weight for BLEU score feature, while keeping the idf-weight= 3, lexical-weight = 1, and NE-matching feature weight = 1) Table (6) reports the unofficial result achieved on STS 2013 test data, compared to the Official run IBM_Eg-run5.",
        "From the results in Table (6) it is clear that Corpus based synonyms matching outperforms dictionary-based WordNet matching over SEM2013 testset."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We proposed an unsupervised approach for measuring semantic textual similarity based on Lexical matching features (with porter stemming matching and synonyms matching), idf-Lexical matching features, Ngram Frquency (Modified BLEU) matching feature, as well as Named Entities matching feature combined together with a weighted cost function.",
        "Our experiments proved that idf-weighted Lexical matching in addition to porter stemming and synonyms-matching features perform best on most released evaluation datasets.",
        "Our best system officially ranked number 11 among 90 participating system reporting a Pearson Mean correlation score of 0.5502.",
        "However our best experimental set up ?idf-weighted Lexical matching in addition to porter stemming and synonyms-matching?",
        "reported in an unofficial run a mean correlation score of 0.59221 which ranks the system as number 3 among the 90 participating systems.",
        "In our future work we intend to try some machine learning algorithms (like AdaBoost for",
        "example) for weighting our similarity matching feature scores.",
        "Also we plan to extend the usage of synonyms matching from the word level to the n-gram phrase matching level, by modifying the BLEU Score N-gram matching function to handle synonym phrases matching."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the reviewers for their constructive criticism and helpful comments."
      ]
    }
  ]
}
