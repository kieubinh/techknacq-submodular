{
  "info": {
    "authors": [
      "Steven Bethard"
    ],
    "book": "*SEM",
    "id": "acl-S13-2002",
    "title": "ClearTK-TimeML: A minimalist approach to TempEval 2013",
    "url": "https://aclweb.org/anthology/S13-2002",
    "year": 2013
  },
  "references": [
    "acl-S10-1010",
    "acl-S10-1062",
    "acl-S10-1063",
    "acl-S13-2001",
    "acl-W06-1618",
    "acl-W07-2014",
    "acl-W07-2025"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The ClearTK-TimeML submission to TempEval 2013 competed in all English tasks: identifying events, identifying times, and identifying temporal relations.",
        "The system is a pipeline of machine-learning models, each with a small set of features from a simple morpho-syntactic annotation pipeline, and where temporal relations are only predicted for a small set of syntactic constructions and relation types.",
        "ClearTK-TimeML ranked 1st for temporal relation F1, time extent strict F1 and event tense accuracy."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The TempEval shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) have been one of the key venues for researchers to compare methods for temporal information extraction.",
        "In TempEval 2013, systems are asked to identify events, times and temporal relations in unstructured text.",
        "This paper describes the ClearTK-TimeML system submitted to TempEval 2013.",
        "This system is based off of the ClearTK framework for machine learning (Ogren et al., 2008)1, and decomposes TempEval 2013 into a series of sub-tasks, each of which is formulated as a machine-learning classification problem.",
        "The goals of the ClearTK-TimeML approach were: ?",
        "To use a small set of simple features that can be derived from either tokens, part-of-speech tags or syntactic constituency parses.",
        "?",
        "To restrict temporal relation classification to a subset of constructions and relation types for which the models are most confident.",
        "Thus, each classifier in the ClearTK-TimeML pipeline uses only the features shared by successful models in previous work (Bethard and Martin, 2006; Bethard and Martin, 2007; Llorens et al., 2010; UzZaman and Allen, 2010) that can be derived from a simple morpho-syntactic annotation pipeline2.",
        "And each of the temporal relation classifiers is restricted to a particular syntactic construction and to a particular set of temporal relation labels.",
        "The following sections describe the models, classifiers and datasets behind the ClearTK-TimeML approach."
      ]
    },
    {
      "heading": "2 Time models",
      "text": [
        "Time extent identification was modeled as a BIO token-chunking task, where each token in the text is classified as being at the B(eginning) of, I(nside) of, or O(utside) of a time expression.",
        "The following features were used to characterize tokens:",
        "?",
        "The token's text ?",
        "The token's stem ?",
        "The token's part-of-speech ?",
        "The unicode character categories for each character of the token, with repeats merged (e.g. Dec28 would be ?LuLlNd?)",
        "?",
        "The temporal type of each alphanumeric sub-token, derived from a 58-word gazetteer of time words ?",
        "All of the above features for the preceding 3 and",
        "following 3 tokens Time type identification was modeled as a multi-class classification task, where each time is classified 2 OpenNLP sentence segmenter, ClearTK PennTreebank-Tokenizer, Apache Lucene Snowball stemmer, OpenNLP part-of-speech tagger, and OpenNLP constituency parser",
        "as DATE, TIME, DURATION or SET.",
        "The following features were used to characterize times: ?",
        "The text of all tokens in the time expression ?",
        "The text of the last token in the time expression ?",
        "The unicode character categories for each character of the token, with repeats merged ?",
        "The temporal type of each alphanumeric sub-token, derived from a 58-word gazetteer of time words Time value identification was not modeled by the system.",
        "Instead, the TimeN time normalization system (Llorens et al., 2012) was used."
      ]
    },
    {
      "heading": "3 Event models",
      "text": [
        "Event extent identification, like time extent identification, was modeled as BIO token chunking.",
        "The following features were used to characterize tokens:",
        "?",
        "The token's text ?",
        "The token's stem ?",
        "The token's part-of-speech ?",
        "The syntactic category of the token's parent in the constituency tree ?",
        "The text of the first sibling of the token in the constituency tree ?",
        "The text of the preceding 3 and following 3 tokens",
        "Event aspect identification was modeled as a multi-class classification task, where each event is classified as PROGRESSIVE, PERFECTIVE, PERFECTIVE-PROGRESSIVE or NONE.",
        "The following features were used to characterize events: ?",
        "The part-of-speech tags of all tokens in the event ?",
        "The text of any verbs in the preceding 3 tokens Event class identification was modeled as a multi-class classification task, where each event is classified as OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE, I-STATE or I-ACTION.",
        "The following features were used to characterize events: ?",
        "The stems of all tokens in the event ?",
        "The part-of-speech tags of all tokens in the event Event modality identification was modeled as a multi-class classification task, where each event is classified as one of WOULD, COULD, CAN, etc.",
        "The following features were used to characterize events: ?",
        "The text of any prepositions, adverbs or modal verbs in the preceding 3 tokens Event polarity identification was modeled as a binary classification task, where each event is classified as POS or NEG.",
        "The following features were used to characterize events: ?",
        "The text of any adverbs in the preceding 3 tokens Event tense identification was modeled as a multi-class classification task, where each event is classified as FUTURE, INFINITIVE, PAST, PASTPART, PRESENT, PRESPART or NONE.",
        "The following features were used to characterize events: ?",
        "The last two characters of the event ?",
        "The part-of-speech tags of all tokens in the event ?",
        "The text of any prepositions, verbs or modal verbs in the preceding 3 tokens"
      ]
    },
    {
      "heading": "4 Temporal relation models",
      "text": [
        "Three different models, described below, were trained for temporal relation identification.",
        "All models followed a multi-class classification approach, pairing an event and a time or an event and an event, and trying to predict a temporal relation type (BEFORE, AFTER, INCLUDES, etc.)",
        "or NORELATION if there was no temporal relation between the pair.",
        "While the training and evaluation data allowed for 14 possible relation types, each of the temporal relation models was restricted to a subset of relations, with all other relations mapped to the NORELATION type.",
        "The subset of relations for each model was selected by inspecting the confusion matrix of the model's errors on the training data, and removing relations that were frequently confused and whose removal improved performance on the training data.",
        "Event to document creation time relations were classified by considering (event, time) pairs where each event in the text was paired with the document creation time.",
        "The classifier was restricted to the relations BEFORE, AFTER and INCLUDES.",
        "The following features were used to characterize such relations:",
        "Event to same sentence time relations were classified by considering (event, time) pairs where the syntactic path from event to time matched a regular expression of syntactic categories and up/down movements through the tree: ?((NP|PP|ADVP)?",
        ")* ((VP|SBAR|S)?",
        ")* (S|SBAR|VP|NP) (?",
        "(VP|SBAR|S))* (?(NP|PP|ADVP))*$.",
        "The classifier relations were restricted to INCLUDES and IS-INCLUDED.",
        "The following features were used to characterize such relations:",
        "?",
        "The event's class (as classified above) ?",
        "The event's tense (as classified above) ?",
        "The text of any prepositions or verbs in the 5 tokens following the event ?",
        "The time's type (as classified above) ?",
        "The text of all tokens in the time expression ?",
        "The text of any prepositions or verbs in the 5 tokens preceding the time expression Event to same sentence event relations were classified by considering (event, event) pairs where the syntactic path from one event to the other matched ?((VP?|ADJP?|NP?)?",
        "(VP|ADJP|S|SBAR) (?",
        "(S|SBAR|PP))* ((?VP|?ADJP)*|(?NP)*)$.",
        "The classi",
        "fier relations were restricted to BEFORE and AFTER.",
        "The following features were used to characterize such relations:",
        "?",
        "The aspect (as classified above) for each event ?",
        "The class (as classified above) for each event ?",
        "The tense (as classified above) for each event ?",
        "The text of the first child of the grandparent of the event in the constituency tree, for each event ?",
        "The path through the syntactic constituency tree from one event to the other ?",
        "The tokens appearing between the two events"
      ]
    },
    {
      "heading": "5 Classifiers",
      "text": [
        "The above models described the translation from TempEval tasks to classification problems and classifier features.",
        "For BIO token-chunking problems, Mallet3 conditional random fields and LIBLINEAR4 support vector machines and logistic regression were applied.",
        "For the other problems, LIBLINEAR, Mallet MaxEnt and OpenNLP MaxEnt5 were applied.",
        "All classifiers have hyper-parameters that must be",
        "tuned during training ?",
        "LIBLINEAR has the classifier type and the cost parameter, Mallet CRF has the iteration count and the Gaussian prior variance, etc.",
        "The best classifier for each training data set was selected via a grid search over classifiers and parameter settings.",
        "The grid of parameters was manually selected to provide several reasonable values for each classifier parameter.",
        "Each (classifier, parameters) point on the grid was evaluated with a 2-fold cross validation on the training data, and the best performing (classifier, parameters) was selected as the final model to run on the TempEval 2013 test set."
      ]
    },
    {
      "heading": "6 Data sets",
      "text": [
        "The classifiers were trained using the following sources of training data: TB The TimeBank event, time and relation annotations, as provided by the TempEval organizers.",
        "AQ The AQUAINT event, time and relation annotations, as provided by the TempEval organizers.",
        "SLV The ?Silver?",
        "event, time and relation annotations, from the TempEval organizers?",
        "system.",
        "BMK The verb-clause temporal relation annotations of (Bethard et al., 2007).",
        "These relations are added on top of the original relations.",
        "PM The temporal relations inferred via closure on the TimeBank and AQUAINT data by Philippe Muller7.",
        "These relations replace the original ones, except in files where no relations were inferred (because of temporal inconsistencies)."
      ]
    },
    {
      "heading": "7 Results",
      "text": [
        "Table 1 shows the performance of the ClearTK-TimeML models across the different tasks when trained on different sets of training data.",
        "The ?Data?",
        "column of each row indicates both the training data sources (as in Section 6), and whether the events and times were predicted by the models (?system?)",
        "or taken from the annotators (?human?).",
        "Performance is reported in terms of strict precision (P), Recall (R) and F1 for event extents, time extents and temporal relations, and in terms of Accuracy (A) on the correctly identified extents for event and time attributes.",
        "Scores in bold are at least as high as the highest in TempEval.",
        "Training on the AQUAINT (AQ) data in addition to the TimeBank (TB) hurt times and relations.",
        "Adding the AQUAINT data caused a -2.7 drop in extent precision, a -8.0 drop in extent recall, a -1.8 drop in value accuracy and a -0.4 drop in type accuracy, and a -3.6 to -4.3 drop in relation recall.",
        "Training on the ?Silver?",
        "(SLV) data in addition to TB+AQ data gave mixed results.",
        "There were big gains for time extent precision (+8.4), time value accuracy (+3.7), event extent recall (+2.5) and event class accuracy (+2.3), but a big drop for event tense accuracy (-6.6).",
        "Relation recall improved (+2.7 with system events and times, +6.0 with manual) but precision varied (-4.4 with system, +1.6 with manual).",
        "Adding verb-clause relations (BMK) and closure-inferred relations (PM) increased recall but lowered precision.",
        "With system-annotated events and times, the change was +2.2/-0.4 (recall/precision) for verb-clause relations, and +0.7/-1.2 for closure-inferred relations.",
        "With manually-annotated events and times, the change was +2.2/-0.3 for verb-clause relations, and (the one exception where recall improved) +1.5/+1.9 for closure-inferred relations."
      ]
    },
    {
      "heading": "8 Discussion",
      "text": [
        "Overall, the ClearTK-TimeML ranked 1st in relation F1, time extent strict F1 and event tense accuracy.",
        "Analysis across the different ClearTK-TimeML runs showed that including annotations from the AQUAINT corpus hurt model performance across a variety of tasks.",
        "A manual inspection of the AQUAINT corpus revealed many annotation errors, suggesting that the drop may be the result of attempting to learn from inconsistent training data.",
        "The AQUAINT corpus may thus have to be partially re-annotated to be useful as a training corpus.",
        "Analysis also showed that adding more relation annotations increased recall, typically at the cost of precision, even though the added annotations were highly accurate: (Bethard et al., 2007) reported agreement of 90%, and temporal closure relations were 100% deterministic from the already-annotated relations.",
        "One would expect that adding such high-quality relations would only improve performance.",
        "But not all temporal relations were annotated by the TempEval 2013 annotators, so the system could be marked wrong for a finding a true temporal relation that was not noticed by the annotators.",
        "Further analysis is necessary to investigate this hypothesis."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "Thanks to Philippe Muller for providing the closure-inferred relations.",
        "The project described was supported in part by Grant Number R01LM010090 from the National Library Of Medicine.",
        "The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Library Of Medicine or the National Institutes of Health."
      ]
    }
  ]
}
