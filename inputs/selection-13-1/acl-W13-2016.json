{
  "info": {
    "authors": [
      "Gerold Schneider",
      "Simon Clematide",
      "Tilia Ellendorff",
      "Don Tuggener",
      "Fabio Rinaldi",
      "GintarÄ – GrigonytÄ – "
    ],
    "book": "BioNLP",
    "id": "acl-W13-2016",
    "title": "UZH in BioNLP 2013",
    "url": "https://aclweb.org/anthology/W13-2016",
    "year": 2013
  },
  "references": [
    "acl-W09-1404",
    "acl-W11-1823",
    "acl-W13-2002"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the BioNLP Shared Task 2013 Workshop, pages 116?120, Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics UZH in the BioNLP 2013 GENIA Shared Task"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "We describe a biological event detection method implemented for the Genia Event Extraction task of BioNLP 2013.",
        "The method relies on syntactic dependency relations provided by a general NLP pipeline, supported by statistics derived from Maximum Entropy models for candidate trigger words, for potential arguments, and for argument frames."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The OntoGene team at the University of Zurich has developed text mining applications based on a combination of deep-linguistic analysis and machine learning techniques (Rinaldi et al., 2012b; Clematide and Rinaldi, 2012; Rinaldi et al., 2010).",
        "Our approaches have proven competitive in several shared task evaluations (Rinaldi et al., 2013; Clematide et al., 2011; Rinaldi et al., 2008).",
        "Additionally, we have developed advanced systems for the curation of the biomedical literature (Rinaldi et al., 2012a).",
        "Our participation in the Genia Event Extraction task of BioNLP 2013 (Kim et al., 2013) was motivated by the desire of testing our technologies on a more linguistically motivated task.",
        "In the course of our participation we revised several modules of our document processing pipeline, however we did not have sufficient resources to completely revise the final module which generates the event structures, and we still relied on a module which we had developed for our previous participation to the BioNLP shared task.",
        "The final submission was composed by our standard preprocessing module (described briefly in section 2) and novel probability models (section 3), combined within the old event generator (section 4)."
      ]
    },
    {
      "heading": "2 Preprocessing",
      "text": [
        "The OntoGene environment is based on a pipeline of several NLP tools which all operate on a common XML representation of the original document.",
        "Briefly, the pipeline includes modules for sentence-splitting, tokenization, part-of-speech tagging, lemmatization, stemming, term-recognition (not used for the BioNLP shared task), chunking, dependency-parsing and event generation.",
        "Different variants of those modules have been used in different instantiations of the pipeline.",
        "For the BioNLP 2013 participation, lingpipe was used for sentence splitting, tokenization and PoS tagging, morpha (Minnen et al., 2001) was used for lemmatization, a python implementation of the Porter stemmer for stemming, LTTT (Grover et al., 2000), was used for chunking, and the Pro3Gres parser (Schneider, 2008) for dependency analysis.",
        "As we have made good experiences with a rule based system for anaphora resolution in the BioNLP 2011 shared task (Tuggener et al., 2011), we implemented a similar approach that resolves anaphors to terms identified during preprocessing.",
        "Rules contain patterns like ?X such as Y?",
        "or ?X is a Y?, and pronouns are resolved to the nearest grammatical subject or object.",
        "Anaphora resolution led to an improvement of 0.2% recall on the development set, while precision was hardly affected."
      ]
    },
    {
      "heading": "3 Probability models",
      "text": [
        "Several probability models have been computed from the training data in order to be used to score and filter candidate events generated by the system.",
        "The following models played a role in the",
        "For all of them we computed global Maximum Likelihood Estimations (MLE), using the training and development datasets from the 2013 and 2011 challenges.",
        "For all of the models above, except for the last one, we also estimated the probabilities by a Maximum Entropy (ME) approach.",
        "The MegaM tool (Daume?",
        "III, 2004) allows for a supervised training of binary classifiers where the class probability is optimized by adjusting the feature weights and not just the binary classification decision itself.",
        "This helps to deal with the imbalanced classes such as the distribution of true or false trig-gerword candidates.",
        "For the classification of trigger candidates (Equation 1), a binary ME classifier for each event type is separately trained, based on local and global features as described below.",
        "The trigger-word candidates are collected from the training data using their stemmed representation as a selection criterion.",
        "We generally exclude triggerword candidates that occur in less than 1% as true triggers in the training set.",
        "Within the data, we found that triggers that consist of more than one word are rather rare (less than 5% of all triggers, most of them occurring once).",
        "However, we transformed these multiword triggers to singleword triggers, replacing them by their first content word.",
        "The choice of ME features, partly inspired by (Ekbal et al., 2013), can be grouped into features derived from the triggerword itself (word), features from the sentence of the triggerword (context), and features from article-wide information (global).",
        "Word features: (1) The text, lemma, part of speech (PoS), stem and local syntactic dependency of the triggerword candidate as computed by the Pro3Gres parser.",
        "(2) Information whether a triggerword candidate is head of a chunk as well as whether the chunk is nominal or verbal Context features: Unigrams and bigrams in a window of variable size to left and right of the triggerword candidate; three types of uni-and bigrams are used: PoS, lemmas and stems; for unigrams we also include the lower-cased words; for bigrams, the triggerword candidate itself is included in the first bigram to either side.",
        "Global features: (1) Presence or absence of a protein in a window of a given size around the triggerword candidate (Boolean feature); only the most frequent proteins of an article are considered.",
        "(2) The zone in an article where the triggerword candidate appears, e.g. Title/Abstract, Introduction, Background, Material and Methods, Results and Discussion, Caption and Conclusion.",
        "Feature engineering was done by testing different combinations of settings (window size, thresholds) with the aim of finding an optimal overall ME model which reaches the lowest error rates for all event types.",
        "The error rate of the candidate set was measured as the cumulative error mass computed from the assigned class probability as follows: if the trigger candidate is a true positive, the error is 1 minus the probability assigned by the classifier.",
        "If the candidate is a false positive, the error is the probability assigned by the classifier.",
        "Our approach does not allow us to compute an error rate for false negatives, because we simply rely on the set of trigger words seen in the training data as possible candidates.",
        "In these experiments, we discovered that for most event types an optimal setting for the context features considers a wide span of about 20 tokens to the left and right of the triggerword.",
        "Including bigrams of lemmas, stems and PoS delivered the best results compared to including only one or two of these bigram types.",
        "Context features can be parameterized according to how much positional information they contain: the distance of a word to the right and left of the trigger, only the direction (left or right) or no position information at all (bag of unigrams/bigrams).",
        "We found that the exact positional information is only important for the first word to the left and right (adjacent to the triggerword), whereas for all words that are further away it is favorable to only use the direction in relation to the trigger.",
        "A window size of 10 words within which proteins are found in the context of a triggerword gave the best results.",
        "The optimal number of the most frequent proteins considered within this window was found to be the 10 most frequent proteins within an article.",
        "The second type of ME classifier (Equation 2) has the purpose of calculating the probabilities of event frames for all event types given a trigger word.",
        "We use the term frame for a combination of arguments that an event is able to accept as theme and cause and whether these arguments are real",
        "ized as proteins or subevents.",
        "For the classification of proteins (Equation 3), again separate binary ME classifiers were built in order to estimate the probability that a protein has a role (theme or cause) in an event of a given type."
      ]
    },
    {
      "heading": "4 Event Generation",
      "text": [
        "We tested two independent event generation modules, one based on a revision of our previous 2009 submission (Kaljurand et al., 2009) and one which is a totally new implementation.",
        "We could do only preliminary tests with the second module, which however showed promising results, in particular with much better recall than the older module (up to 65.23%), despite the very little time that we could invest in its development.",
        "The best F-score that we could reach was still slightly inferior to the one of the old module at the deadline for submission of results.",
        "In the rest of this paper we will describe only the module which was used in the official submission.",
        "The event extraction process consists of three phases.",
        "First, event candidates are generated, based on trigger words and their context, using the ME and MLE probabilities pT (equation 1).",
        "Second, individual arguments of an event are generated.",
        "We calculate the MLE probability pR of an argument role (e.g. Theme) to occur as part of a given event type, as follows:",
        "We obtained the best results on the development corpus when combining the probabilities as:",
        "We generate arguments, using an MLE syntactic path and an ME argument model, as follows.",
        "The syntactic path between the trigger word and every term (protein or subordinate event) is considered.",
        "If they are syntactically connected, and if the probability of a syntactic path to express an event is above a threshold, it is selected.",
        "As this is a filtering step, it negatively affects recall.",
        "We calculate the MLE probability ppath that a syntactic configuration fills an argument slot.",
        "Syntactic configurations consist of the head word (trigger) HWord, the head event type HType, the dependent word DWord, the dependent event type DType, and the syntactic path Path between them.",
        "In order to deal with sparse data, we use a smoothed model.",
        "ppath(Arg |HWord, HType, DWord, DType, Path) =",
        "The weights were emprically set as w1 = 4, w2 = 2 and w3 = 1.5.",
        "The fact that the weights decrease approximates a back-off model.",
        "The final probability had to be larger than 0.2.",
        "We have also used an ME model which delivers the probability parg that a term is the argument of a specific event, see formula 3.",
        "If this ME model predicts with a probability of above 80% that the term is not an argument, the search fails.",
        "Otherwise, the probabilities are combined.",
        "On the development corpus, we achieved best results when using the harmonic mean:",
        "As a last step, the several arguments of an event are combined into a frame.",
        "We have tested models predicting an entire frame directly, and models combining the individual arguments generated in the previous step.",
        "The latter approach performed better.",
        "Any permutation of the argument candidates could constitute a frame.",
        "Only frames seen in the training corpus for a given event type are considered.",
        "We have again used an ME and an MLE model for predicting frames.",
        "The ME model predicts pframe.ME , see formula 2.",
        "We have also used two MLE models: the first one delivers the probability pframe.MLE based on the event type only, the second one pframeword.MLE also considers the trigger word and is much sparser (a low default is thus used for unseen words).",
        "The probability of the individual arguments also needs to be taken into consideration.",
        "We used the mean of the individual arguments?",
        "probabilities (pargs?mean)."
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": [
        "In our analysis of errors, we noticed that frames with more than one argument are created extremely rarely.",
        "The problem is that frames with several arguments are rarer because the context often does not offer the possibility to attach several arguments.",
        "Therefore, we consistently undergenerated with pargs?mean as outlined above.",
        "We have added a number of heuristics to boost multi-argument frames.",
        "Multiplying the probability of a frame by its cubed length (giving two-argument slots 9 times higher probability), and giving Cause-slots 50% higher scores globally led to best results.",
        "We mainly trained and evaluated using the ?strict equality?",
        "evaluation criteria as our reference.",
        "The results on the development data are shown in table 1.",
        "With more relaxed equality definitions, the results were always a few percentage points better.",
        "Our results in the official test run are shown in table 2.",
        "In sum, our submitted system has good performance for simple events, bad performance for Binding events, and a bias towards precision due to a syntactic-based filtering step."
      ]
    },
    {
      "heading": "6 Conclusions and Future work",
      "text": [
        "Our participation in the 2013 BioNLP shared task was a useful opportunity to revise components of the OntoGene pipeline and begin the implementation of a novel event generator.",
        "Due to lack of time, it was not completed in time for the official submission.",
        "We will continue its development and use the BioNLP datasets."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research is partially funded by the Swiss National Science Foundation (grant 105315 130558/1)."
      ]
    }
  ]
}
