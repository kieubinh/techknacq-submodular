{
  "info": {
    "authors": [
      "Phong Le",
      "Willem Zuidema",
      "Remko Scha"
    ],
    "book": "CVSC",
    "id": "acl-W13-3202",
    "title": "Learning from errors: Using vector-based compositional semantics for parse reranking",
    "url": "https://aclweb.org/anthology/W13-3202",
    "year": 2013
  },
  "references": [
    "acl-A00-2018",
    "acl-D11-1008",
    "acl-J03-4003",
    "acl-N10-1095",
    "acl-P03-1054",
    "acl-P05-1022",
    "acl-P06-1055",
    "acl-P13-1045",
    "acl-W03-0402",
    "acl-W06-1666"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we address the problem of how to use semantics to improve syntactic parsing, by using a hybrid reranking method: a k-best list generated by a symbolic parser is reranked based on parse-correctness scores given by a compositional, connectionist classifier.",
        "This classifier uses a recursive neural network to construct vector representations for phrases in a candidate parse tree in order to classify it as syntactically correct or not.",
        "Tested on the WSJ23, our method achieved a statistically significant improvement of 0.20% on F-score (2% error reduction) and 0.95% on exact match, compared with the state-of-the-art Berkeley parser.",
        "This result shows that vector-based compositional semantics can be usefully applied in syntactic parsing, and demonstrates the benefits of combining the symbolic and connectionist approaches."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Following the idea of compositionality in formal semantics, compositionality in vector-based semantics is also based on the principle of compositionality, which says that ?The meaning of a whole is a function of the meanings of the parts and of the way they are syntactically combined?",
        "(Partee, 1995).",
        "According to this principle, composing the meaning of a phrase or sentence requires a syntactic parse tree, which is, in most current systems, given by a statistical parser.",
        "This parser, in turn, is trained on syntactically annotated corpora.",
        "However, there are good reasons to also consider information flowing in the opposite direction: from semantics to syntactic parsing.",
        "Performance of parsers trained and evaluated on the Penn WSJ treebank has reached a plateau, as many ambiguities cannot be resolved by syntactic information alone.",
        "Further improvements in parsing may depend on the use of additional sources of information, including semantics.",
        "In this paper, we study the use of semantics for syntactic parsing.",
        "The currently dominant approach to syntactic parsing is based on extracting symbolic grammars from a treebank and defining appropriate probability distributions over the parse trees that they license (Charniak, 2000; Collins, 2003; Klein and Manning, 2003; Petrov et al., 2006; Bod et al., 2003; Sangati and Zuidema, 2011; van Cra-nenburgh et al., 2011).",
        "An alternative approach, with promising recent developments (Socher et al., 2010; Collobert, 2011), is based on using neural networks.",
        "In the present paper, we combine the 'symbolic' and 'connectionist' approaches through reranking: a symbolic parser is used to generate a k-best list which is then reranked based on parse-correctness scores given by a connectionist compositional-semantics-based classifier.",
        "The idea of reranking is motivated by analyses of the results of state-of-the-art symbolic parsers such as the Brown and Berkeley parsers, which have shown that there is still considerable room for improvement: oracle results on 50-best lists display a dramatic improvement in accuracy (96.08% vs. 90.12% on F-score and 65.56% vs. 37.22% on exact match with the Berkeley parser).",
        "This suggests that parsers that rely on syntactic corpus-statistics, though not sufficient by themselves, may very well serve as a basis for systems that integrate other sources of information by means of reranking.",
        "One important complementary source of information is the semantic plausibility of the constituents of the syntactically viable parses.",
        "The exploitation of that kind of information is the topic of the research we report here.",
        "In this work, we follow up on a proposal by Mark Steedman (1999), who suggested that the realm of semantics lacks the clearcut hierarchical structures that characterise syntax, and that semantic information may therefore be profitably treated by the classificatory mechanisms of neural nets?while the treatment of syntactic structures is best left to symbolic parsers.",
        "We thus developed a hybrid system, which parses its input sentences on the basis of a symbolic probabilistic grammar, and reranks the candidate parses based on scores given by a neural network.",
        "Our work is inspired by the work of Socher and colleagues (2010; 2011).",
        "They proposed a parser using a recursive neural network (RNN) for encoding parse trees, representing phrases in a vector space, and scoring them.",
        "Their experimental result (only 1.92% lower than the Stanford parser on unlabelled bracket F-score for sentences up to a length of 15 words) shows that an RNN is expressive enough for syntactic parsing.",
        "Additionally, their qualitative analysis indicates that the learnt phrase features capture some aspects of phrasal semantics, which could be useful to resolve semantic ambiguity that syntactical information alone can not.",
        "Our work in this paper differs from their work in that we replace the parsing task by a reranking task, and thus reduce the object space significantly to a set of parses generated by a symbolic parser rather than the space of all parse trees.",
        "As a result, we can apply our method to sentences which are much longer than 15 words.",
        "Reranking a k-best list is not a new idea.",
        "Collins (2000), Charniak and Johnson (2005), and Johnson and Ural (2010) have built reranking systems with performances that are state-of-the-art.",
        "In order to achieve such high F-scores, those rerankers rely on a very large number of features selected on the basis of expert knowledge.",
        "Unlike them, our feature set is selected automatically, yet the reranker achieved a statistically significant improvement on both F-score and exact match.",
        "Closest to our work is Menchetti et al. (2005) and Socher et al. (2013): both also rely on symbolic parsers to reduce the search space and use RNNs to score candidate parses.",
        "However, our work differs in the way the feature set for reranking is selected.",
        "In their methods, only the score at the tree root is considered whereas in our method the scores at all internal nodes are taken into account.",
        "Selecting the feature set like that gives us a flexible way to deal with errors accumulated from the leaves to the root.",
        "Figure 1 shows a diagram of our method.",
        "First, a parser (in this paper: the Berkeley parser) is used to generate k-best lists of the Wall Street Journal (WSJ) sections 02-21.",
        "Then, all parse trees in these lists and the WSJ02-21 are preprocessed by marking head words, binarising, and performing error-annotation (Section 2).",
        "After that, we use the annotated trees to train our parse-correctness classifier (Section 3).",
        "Finally, those trees and the classifier are used to train the reranker (Section 4)."
      ]
    },
    {
      "heading": "2 Experimental Setup",
      "text": [
        "The experiments presented in this paper have the following setting.",
        "We use the WSJ corpus with the standard splits: sections 2-21 for training, section 22 for development, and section 23 for testing.",
        "The latest implementation (version 1.7) of the Berkeley parser1 (Petrov et al., 2006) is used for generating 50-best lists.",
        "We mark head words and binarise all trees in the WSJ and the 50-best lists as in Subsection 2.1, and annotate them as in Subsection 2.2 (see Figure 2)."
      ]
    },
    {
      "heading": "2.1 Preprocessing Trees",
      "text": [
        "We preprocess trees by marking head words and binarising the trees.",
        "For head word marking, we used the head finding rules of Collins (1999) which are implemented in the Stanford parser.",
        "To binarise a k-ary branching, e.g. P ?",
        "C1 ... H ... Ck where H is the top label of the head constituent, we use the following method.",
        "If H is not the leftmost child, then",
        "where @P , which is called extra-P , now is the head of P .",
        "We then apply this transformation again on the children until we reach terminal nodes.",
        "In this way, we ensure that every internal node has one head word."
      ]
    },
    {
      "heading": "2.2 Error Annotation",
      "text": [
        "We annotate nodes (as correct or incorrect) as follows.",
        "Given a parse tree T in a 50-best list and a corresponding gold-standard tree G in the WSJ,",
        "other nodes are labelled correct.",
        "we first attempt to align their terminal nodes according to the following criterion: a terminal node t is aligned to a terminal node g if they are at the same position counting from-left-to-right and they have the same label.",
        "Then, a non-terminal node P [wh] with children C1, ..., Ck is aligned to a gold-standard non-terminal node P ?",
        "[w?h] with children C?1 , ..., C ?",
        "l (1 ?",
        "k, l ?",
        "2 in our case) if they have the same word head, the same syntactical category, and their children are all aligned in the right order.",
        "In other words, the following conditions have to be satisfied",
        "Ci is aligned to C?i , for all i = 1..k Aligned nodes are annotated as correct whereas the other nodes are annotated as incorrect."
      ]
    },
    {
      "heading": "3 Parse-Correctness Classification",
      "text": [
        "This section describes how a neural network is used to construct vector representations for phrases given parse trees and to identify if those trees are syntactically correct or not.",
        "In order to encode tree structures, we use an RNN2 (see Figure 3 and Figure 4) which is similar to the one proposed by Socher and colleagues (2010).",
        "However, unlike their RNN, our RNN can handle unary branchings, and also takes head words and syntactic tags as input.",
        "It is worth noting that, although we can use some transformation to remove unary branchings, handling them is helpful in our case because the system avoids dealing with so many syntactic tags that would result from the transfor-2The first neural-network approach attempting to operate and represent compositional, recursive structure is the Recursive Auto-Associative Memory network (RAAM), which was proposed by Pollack (1988).",
        "In order to encode a binary tree, the RAAM network contains three layers: an input layer for two daughter nodes, a hidden layer for their parent node, and an output layer for their reconstruction.",
        "Training the network is to minimise the reconstruction error such that we can decode the information captured in the hidden layer to the original tree form.",
        "Our RNN differs from the RAAM network in that its output layer is not for reconstruction but for classification.",
        "shown in the top-right of Figure 2.",
        "All unary branchings share a set of weight matrices, and all binary branchings share another set of weight matrices (see Figure 4).",
        "An RNN processes a tree structure by repeatedly applying itself at each internal node.",
        "Thus, walking bottom-up from the leaves of the tree to the root, we compute for every node a vector based on the vectors of its children.",
        "Because of this process, those vectors have to have the same dimension.",
        "It is worth noting that, because information at leaves, i.e. lexical semantics, is composed according to a given syntactic parse, what a vector at each internal node captures is some aspects of compositional semantics of the corresponding phrase.",
        "In the remainder of this subsection, we describe in more detail how to construct compositional vector-based semantics geared towards the parse-correctness classification task.",
        "Similar to Socher et al. (2010), and Collobert (2011), given a string of words (w1, ..., wl), we first compute a string of vectors (x1, ..., xl) representing those words by using a lookup table (i.e., word embeddings) L ?",
        "Rn?|V |, where |V |is the size of the vocabulary and n is the dimensionality of the vectors.",
        "This lookup table L could be seen as a storage of lexical semantics where each column is a vector representation of a word.",
        "Hence, let bi be the binary representation of word wi (i.e., all of the entries of bi are zero except the one corresponding to the index of the word in the dictionary), then",
        "We also encode syntactic tags by binary vectors but put an extra bit at the end of each vector to mark if the corresponding tag is extra or not (i.e.,",
        "branching (top) and a binary branching (bottom).",
        "The bias is not shown for the simplicity.",
        "Then, given a unary branching P [wh]?",
        "C, we can compute the vector at the node P by (see Fig",
        "where c, xh are vectors representing the child C and the head word, x?1, x+1 are the left and right neighbouring words of P , tp encodes the syntactic tag of P , Wu,Wh,W?1,W+1 ?",
        "Rn?n, Wt ?",
        "Rn?",
        "(|T |+1), |T |is the size of the set of syntactic tags, bu ?",
        "Rn, and f can be any activation function (tanh is used in this case).",
        "With a binary branching P [wh] ?",
        "C1 C2, we simply change the way the children's vectors added (see",
        "Finally, we put a sigmoid neural unit on the top of each internal node (except pre-terminal nodes because we are not concerned with POS-tagging) to detect the correctness of the subparse tree rooted at that node",
        "where Wcat ?",
        "R1?n, bcat ?",
        "R."
      ]
    },
    {
      "heading": "3.1 Learning",
      "text": [
        "The error on a parse tree is computed as the sum of classification errors of all subparses.",
        "Hence, the learning is to minimise the objective",
        "where ?",
        "are the model parameters, N is the number of trees, ?",
        "is a regularisation hyperparameter, T is a parse tree, y(?)",
        "is given by Equation 2, and t is the class of the corresponding subparse (t = 1 means correct).",
        "The gradient ?J??",
        "is computed efficiently thanks to backpropagation through the structure (Goller and Kuchler, 1996).",
        "L-BFGS (Liu and Nocedal, 1989) is used to minimise the objective function."
      ]
    },
    {
      "heading": "3.2 Experiments",
      "text": [
        "We implemented our classifier in Torch73 (Collobert et al., 2011a), which is a powerful Matlab-like environment for machine learning.",
        "In order to save time, we only trained the classifier on 10-best parses of WSJ02-21.",
        "The training phase took six days on a computer with 16 800MHz CPU-cores and 256GB RAM.",
        "The word embeddings given by Collobert et al. (2011b)4 were used as L in Equation 1.",
        "Note that these embeddings, which are the result of training a language model neural network on the English Wikipedia and Reuters, have been shown to capture many interesting semantic similarities between words.",
        "We tested the classifier on the development set WSJ22, which contains 1, 700 sentences, and measured the performance in positive rate and negative rate",
        "#true neg +#false pos The positive/negative rate tells us the rate at which positive/negative examples are correctly labelled positive/negative.",
        "In order to achieve high performance in the reranking task, the classifier must have a high positive rate as well as a high negative rate.",
        "In addition, percentage of positive examples is also interesting because it shows the unbalancedness of the data.",
        "Because the accuracy is not",
        "a reliable measurement when the dataset is highly unbalanced, we do not show it here.",
        "Table 1, Figure 5, and Figure 6 show the classification results.",
        "age of positive examples w.r.t.",
        "subtree depth."
      ]
    },
    {
      "heading": "3.3 Discussion",
      "text": [
        "Table 1 shows the classification results on the gold-standard, 1-best, 10-best, and 50-best lists.",
        "The positive rate on the gold-standard parses, 75.31%, gives us the upper bound of %-pos when this classifier is used to yield 1-best lists.",
        "On the 1 best data, the classifier missed less than one tenth positive subtrees and correctly found nearly two third of the negative ones.",
        "That is, our classifier might be useful for avoiding many of the mistakes made by the Berkeley parser, whilst not introducing too many new mistakes of its own.",
        "This fact gave us hope to improve parsing performance when using this classifier for reranking.",
        "Figure 5 shows positive rate, negative rate, and percentage of positive examples w.r.t.",
        "subtree depth on the 50-best data.",
        "We can see that the positive rate is inversely proportional to the subtree depth, unlike the negative rate.",
        "That is because the",
        "(excluding POS tags).",
        "deeper a subtree is, the lower the a priori likelihood that the subtree is positive (we can see this in the percentage-of-positive-example curve).",
        "In addition, deep subtrees are difficult to classify because uncertainty is accumulated when propagating from bottom to top."
      ]
    },
    {
      "heading": "4 Reranking",
      "text": [
        "In this section, we describe how we use the above classifier for the reranking task.",
        "First, we need to represent trees in one vector space, i.e., ?",
        "(T ) =",
        "for an arbitrary parse tree T .",
        "Collins (2000), Charniak and Johnson (2005), and Johnson and Ural (2010) set the first entry to the model score and the other entries to the number of occurrences of specific discrete hand-chosen properties (e.g., how many times the word pizza comes after the word eat) of trees.",
        "We here do the same with a trick to discretize results from the classifier: we use a 2D histogram to store predicted scores w.r.t.",
        "subtree depth.",
        "This gives us a flexible way to penalise low score subtrees and reward high score subtrees w.r.t.",
        "the performance of the classifier at different depths (see Subsection 3.3).",
        "However, unlike the approaches just mentioned, we do not use any expert knowledge for feature selection; instead, this process is fully automatic.",
        "Formally speaking, a vector feature ?",
        "(T ) is computed as following.",
        "?1(T ) is the model score (i.e., max-rule-sum score) given by the parser,",
        "is the histogram of a set of (y, h) where y is given by Equation 2 and h is the depth of the corresponding subtree.",
        "The domain of y (i.e., [0, 1]) is split into ?y equal bins whereas the domain of h (i.e., {1, 2, 3, ...}) is split into ?h bins such that the i-th (i < ?h) bin corresponds to subtrees of depth i and the ?h-th bin corresponds to subtrees of depth equal or greater than ?h.",
        "The parameters ?y and ?h are then estimated on the development set.",
        "After extracting feature vectors for parse trees, we then find a linear ranking function f(T ) = w>?",
        "(T ) such that f(T1) > f(T2) iff fscore(T1) > fscore(T2) where fscore(.)",
        "is the function giving F-score, and w ?",
        "Rv is a weight vector, which is efficiently estimated by SVM ranking (Yu and Kim, 2012).",
        "SVM was initially used for binary classification.",
        "Its goal is to find the hyperplane which has the largest margin to best separate two example sets.",
        "It was then proved to be efficient in solving the ranking task in information retrieval, and in syntactic parsing (Shen and Joshi, 2003; Titov and Henderson, 2006).",
        "In our experiments, we used SVM",
        "Rank5 (Joachims, 2006), which runs extremely fast (less than two minutes with about 38, 000 10- best lists)."
      ]
    },
    {
      "heading": "4.1 Experiments",
      "text": [
        "Using the classifier in Section 3, we implemented the reranker in Torch7, trained it on WSJ02-21.",
        "We used WSJ22 to estimate the parameters ?y and ?h by the grid search and found that ?y = 9 and ?h = 4 yielded the best F-score.",
        "Table 2 shows the results of our reranker on 50-best WSJ23 given by the Berkeley parser, using the standard evalb.",
        "Our method improves 0.20% on F-score for sentences with all length, and 0.22% for sentences with ?",
        "40 words.",
        "These differences are statistically significant6 with p-value < 0.003.",
        "Our method also improves exact match (0.95% for all sentences as well as for sentences with ?",
        "40 words).",
        "WSJ23 (LR is labelled recall, LP is labelled precision, LF is labelled F-score, and EX is exact match.)",
        "Table 3 shows the comparison of the three parsers that use the same hybrid reranking approach.",
        "On F-score, our method performed 0.1% lower than Socher et al. (2013), and 1.5% better than Menchetti et al. (2005).",
        "However, our method achieved the least improvement on F-score over its corresponding baseline.",
        "That could be because our baseline parser (i.e., the Berkeley parser) performs much better than the other two baseline parsers; and hence, detecting errors it makes on candidate parse trees is more difficult.",
        "brid reranking approach.",
        "The numbers in the blankets indicate the improvements on F-score over the corresponding baselines (i.e., the k-best parsers)."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "This paper described a new reranking method which uses semantics in syntactic parsing: a symbolic parser is used to generate a k-best list which is later reranked thanks to parse-correctness scores given by a connectionist compositional-semantics-based classifier.",
        "Our classifier uses a recursive neural network, like Socher et al., (2010; 2011), to not only represent phrases in a vector space given parse trees, but also identify if these parse trees are grammatically correct or not.",
        "Tested on WSJ23, our method achieved a statistically significant improvement on F-score (0.20%) as well as on exact match (0.95%).",
        "This result, although not comparable to the results reported by Collins (2000), Charniak and Johnson (2005), and Johnson and Ural (2010), shows an advantage of using vector-based compositional semantics to support available state-of-the-art parsers.",
        "One of the limitations of the current paper is the lack of a qualitative analysis of how learnt vector-based semantics has affected the reranking results.",
        "Therefore, the need for ?compositional semantics?",
        "in syntactical parsing may still be doubted.",
        "In future work, we will use vector-based semantics together with non-semantic features (e.g., the ones of Charniak and Johnson (2005)) to find out whether the semantic features are truly helpful or they just resemble non-semantic features."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank two anonymous reviewers for helpful comments."
      ]
    }
  ]
}
