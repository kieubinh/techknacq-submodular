{
  "info": {
    "authors": [
      "Fabio Massimo Zanzotto",
      "Lorenzo Dellâ€™Arciprete"
    ],
    "book": "CVSC",
    "id": "acl-W13-3205",
    "title": "Transducing Sentences to Syntactic Feature Vectors: an Alternative Way to Parse?",
    "url": "https://aclweb.org/anthology/W13-3205",
    "year": 2013
  },
  "references": [
    "acl-A00-2018",
    "acl-D07-1076",
    "acl-D09-1073",
    "acl-J02-3001",
    "acl-J03-4003",
    "acl-J04-4004",
    "acl-J08-2003",
    "acl-J93-2004",
    "acl-N06-1006",
    "acl-P02-1034",
    "acl-P04-1054",
    "acl-P05-1072",
    "acl-W07-1406"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Classification and learning algorithms use syntactic structures as proxies between source sentences and feature vectors.",
        "In this paper, we explore an alternative path to use syntax in feature spaces: the Distributed Representation ?Parsers?",
        "(DRP).",
        "The core of the idea is straightforward: DRPs directly obtain syntactic feature vectors from sentences without explicitly producing symbolic syntactic interpretations.",
        "Results show that DRPs produce feature spaces significantly better than those obtained by existing methods in the same conditions and competitive with those obtained by existing methods with lexical information."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Syntactic processing is widely considered an important activity in natural language understanding (Chomsky, 1957).",
        "Research in natural language processing (NLP) exploits this hypothesis in models and systems.",
        "Syntactic features improve performance in high level tasks such as question answering (Zhang and Lee, 2003), semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Moschitti et al., 2008; Collobert et al., 2011), paraphrase detection (Socher et al., 2011), and textual entailment recognition (MacCartney et al., 2006; Wang and Neumann, 2007; Zanzotto et al., 2009).",
        "Classification and learning algorithms are key components in the above models and in current NLP systems, but these algorithms cannot directly use syntactic structures.",
        "The relevant parts of phrase structure trees or dependency graphs are explicitly or implicitly stored in feature vectors.",
        "To fully exploit syntax in learning classifiers, kernel machines (Cristianini and ShaweTaylor, 2000) use graph similarity algorithms (e.g., (Collins and Duffy, 2002) for trees) as structural kernels (Ga?rtner, 2003).",
        "These structural kernels allow to exploit high-dimensional spaces of syntactic tree fragments by concealing their complexity.",
        "These feature spaces, although hidden, still exist.",
        "Then, even in kernel machines, symbolic syntactic structures act only as proxies between the source sentences and the syntactic feature vectors.",
        "In this paper, we explore an alternative way to use syntax in feature spaces: the Distributed Representation Parsers (DRP).",
        "The core of the idea is straightforward: DRPs directly bridge the gap between sentences and syntactic feature spaces.",
        "DRPs act as syntactic parsers and feature extractors at the same time.",
        "We leverage on the distributed trees recently introduced by Zan-zotto&Dell?Arciprete (2012) and on multiple linear regression models.",
        "Distributed trees are small vectors that encode the large vectors of the syntactic tree fragments underlying the tree kernels (Collins and Duffy, 2002).",
        "These vectors effectively represent the original vectors and lead to performances in NLP tasks similar to tree kernels.",
        "Multiple linear regression allows to learn linear DRPs from training data.",
        "We experiment with the Penn Treebank data set (Marcus et al., 1993).",
        "Results show that DRPs produce distributed trees significantly better than those obtained by existing methods, in the same non-lexicalized conditions, and competitive with those obtained by existing methods with lexical information.",
        "Finally, DRPs are extremely faster than existing methods.",
        "The rest of the paper is organized as follows.",
        "First, we present the background of our",
        "idea (Sec.",
        "2).",
        "Second, we fully describe our model (Sec.",
        "3).",
        "Then, we report on the experiments (Sec.",
        "4).",
        "Finally, we draw some conclusions and outline future work (Sec.",
        "5)"
      ]
    },
    {
      "heading": "2 Background",
      "text": [
        "Classification and learning algorithms for NLP tasks treat syntactic structures t as vectors in feature spaces ~t ?",
        "Rm.",
        "Each feature generally represents a substructure ?i.",
        "In simple weighting schemes, feature values are 1 if ?i is a substructure of t and 0 otherwise.",
        "Different weighting schemes are used and possible.",
        "Then, learning algorithms exploit these feature vectors in different ways.",
        "Decision tree learners (Quinlan, 1993) elect the most representative feature at each iteration, whereas kernel machines (Cristianini and Shawe-Taylor, 2000) exploit similarity between pairs of instances, s(t1, t2).",
        "This similarity is generally measured as the dot product between the two vectors, i.e. s(t1, t2) = ~t1 ?",
        "~t2.",
        "The use of syntactic features changed when tree kernels (Collins and Duffy, 2002) appeared.",
        "Tree kernels gave the possibility to fully exploit feature spaces of tree fragments.",
        "Until then, learning algorithms could not treat these huge spaces.",
        "It is infeasible to explicitly represent that kind of feature vectors and to directly compute similarities through dot products.",
        "Tree kernels (Collins and Duffy, 2002), by computing similarities between two trees with tree comparison algorithms, exactly determine dot products of vectors in these target spaces.",
        "After their introduction, different tree kernels have been proposed (e.g., (Vishwanathan and Smola, 2002; Culotta and Sorensen, 2004; Moschitti, 2006)).",
        "Their use spread in many NLP tasks (e.g., (Zhou et al., 2007; Wang and Neumann, 2007; Moschitti et al., 2008; Zanzotto et al., 2009; Zhang and Li, 2009)) and in other areas like biology (Vert, 2002; Hashimoto et al., 2008) and computer security (Du?ssel et al., 2008; Rieck and Laskov, 2007; Bockermann et al., 2009).",
        "Tree kernels have played a very important role in promoting the use of syntactic information in learning classifiers, but this method obfuscated the fact that syntactic trees are ultimately used as vectors in learning algorithms.",
        "To work with the idea of directly obtaining rich syntactic feature vectors from sentences, we need some techniques to make these high-dimensional vectors again explicit, through smaller but expressive vectors.",
        "A solution to the above problem stems from the recently revitalized research in Distributed Representations (DR) (Hinton et al., 1986; Ben-gio, 2009; Collobert et al., 2011; Socher et al., 2011; Zanzotto and Dell?Arciprete, 2012).",
        "Distributed Representations, studied in opposition to symbolic representations (Rumelhart and Mcclel-land, 1986), are methods for encoding data structures such as trees into vectors, matrices, or high-order tensors.",
        "The targets of these representations are generally propositions, i.e., flat tree structures.",
        "The Holographic Reduced Representations (HRR), proposed by Plate (1994), produce nearly orthogonal vectors for different structures by combining circular convolution and randomly generated vectors for basic components (as in (Anderson, 1973; Murdock, 1983)).",
        "Building on HRRs, Distributed Trees (DT) have been proposed to encode deeper trees in low dimensional vectors (Zanzotto and Dell?Arciprete, 2012).",
        "DTs approximate the feature space of tree fragments defined for the tree kernels (Collins and Duffy, 2002) and guarantee similar performances of classifiers in NLP tasks such as question classification and textual entailment recognition.",
        "Thus, Distributed Trees are good representations of syntactic trees, that we can use in our definition of distributed representation parsers (DRPs)."
      ]
    },
    {
      "heading": "3 Distributed Representation Parsers",
      "text": [
        "In this section, first, we sketch the idea of Distributed Representation ?Parsers?",
        "(DRPs).",
        "Then, we review the distributed trees as a way to represent trees in low dimensional vectors.",
        "Finally, we describe how to build DRPs by mixing a function that encodes sentences in vectors and a linear re-gressor that can be induced from training data."
      ]
    },
    {
      "heading": "3.1 The Idea",
      "text": [
        "The approach to using syntax in learning algorithms generally follows two steps: first, parse sentences s with a symbolic parser (e.g., (Collins, 2003; Charniak, 2000; Nivre et al., 2007)) and produce symbolic trees t; second, use an encoder to build syntactic feature vectors.",
        "Figure 1 sketches this idea when the final vectors are the distributed trees",
        "Dell?Arciprete, 2012)1.",
        "In this case, the last step",
        "Our proposal is to build a Distributed Representation ?Parser?",
        "(DRP) that directly maps sentences s into the final vectors.",
        "We choose the distributed trees ; t as these reduced vectors fully represent the syntactic trees.",
        "A DRP acts as follows (see Figure 1): first, a function D encodes sentence s into a distributed vector ;s ?",
        "Rd; second, a function P transforms the input vector ;s into a distributed tree ; t .",
        "This second step is a vector to vector transformation and, in a wide sense, ?parses?",
        "the input sentence.",
        "Given an input sentence s, a DRP is then a function defined as follows:",
        "In this paper, we design some functions D and we propose a linear function P , designed to be a re-gressor that can be induced from training data.",
        "In this study, we use a space with d dimensions for both sentences ;s and distributed trees ; t , but, in general, these spaces can be of different size."
      ]
    },
    {
      "heading": "3.2 Syntactic Trees as Distributed Vectors",
      "text": [
        "We here report on the distributed trees2 (Zanzotto and Dell?Arciprete, 2012) to describe how these vectors represent syntactic trees and how the dot product between two distributed trees approximates the tree kernel defined by Collins and Duffy (2002).",
        "fragments.",
        "2For the experiments, we used the implementation of the distributed tree encoder available at http://code.google.com/p/distributed-tree-kernels/ Given a tree t, the corresponding distributed tree",
        "is the small vector corresponding to tree fragment ?i and ?i is the weight of subtree ?i in the final feature space.",
        "As in (Collins and Duffy, 2002), the set S(t) contains tree fragments ?",
        "such that the root of ?",
        "is any non-terminal node in t and, if ?",
        "contains node n, it must contain all the siblings of n in t (see, for example, Slex(t) in Figure 2).",
        "The weight ?i is defined as:",
        "where |?i |is the number of non-terminal nodes of tree fragment ?i and ?",
        "is the traditional parameter used to penalize large subtrees.",
        "For ?",
        "= 0, ?i has a value 1 for productions and 0 otherwise.",
        "If different tree fragments are associated to nearly orthonormal vectors, the dot product",
        "t2 approximates the tree kernel (Zanzotto and Dell?Arciprete, 2012).",
        "A key feature of the distributed tree fragments ;?",
        "is that these vectors are built compositionally from a set N of nearly orthonormal random vectors ;n , associated to node labels n. Given a subtree ?",
        ", the related vector is obtained as:",
        "where node vectors ;n i are ordered according to a depth-first visit of subtree ?",
        "and ?",
        "is a vector composition operation, specifically the shuffled circular convolution3 .",
        "This function guarantees that two different subtrees have nearly orthonormal vectors (see (Zanzotto and Dell?Arciprete, 2012) for more details).",
        "For example, the fifth tree ?5 of set Sno lex(t) in Figure 2 is:",
        "We experiment with two tree fragment sets: the non-lexicalized set Sno lex(t), where tree fragments do not contain words, and the lexicalized set Slex(t), including all the tree fragments.",
        "An example is given in Figure 2."
      ]
    },
    {
      "heading": "3.3 The Model",
      "text": [
        "To build a DRP, we need to define the encoder D and the transformer P .",
        "In the following, we present a non-lexicalized and a lexicalized model for the encoder D and we describe how we can learn the transformer P by means of a linear regression model.",
        "Establishing good models to encode input sentences into vectors is the most difficult challenge.",
        "The models should consider the kind of information that can lead to a correct syntactic interpretation.",
        "Only in this way, the distributed representation parser can act as a vector transforming module.",
        "Unlike in models such as (Socher et al., 2011), we want our encoder to represent the whole sentence as a fixed size vector.",
        "We propose a non-lexicalized model and a lexicalized model.",
        "s1(~a)?",
        "s2(~b) where ?",
        "is the circular convolution and s1 and s2 are two different random permutations of vector elements.",
        "Non-lexicalized model The non-lexicalized model relies only on the pos-tags of the sentences s: s = p1 .",
        ".",
        ".",
        "pn where pi is the pos-tag associated with the i-th token of the sentence.",
        "In the following we discuss how to encode this information in a Rd space.",
        "The basic model D1(s) is the one that considers the bag-of-postags, that is:",
        "where ;p i ?",
        "N is the vector for label pi, taken from the set of nearly orthonomal random vectors N .",
        "It is basically in line with the bag-of-word model used in random indexing (Sahlgren, 2005).",
        "Due to the commutative property of the sum and since vectors in N are nearly orthonormal: (1) two sentences with the same set of pos-tags have the same vector; and, (2) the dot product between two vectors, D1(s1) and D1(s2), representing sentences s1 and s2, approximately counts how many pos-tags the two sentences have in common.",
        "The vector for the sentence in Figure 1 is then:",
        "The general non-lexicalized model that takes into account all n-grams of pos-tags, up to length j, is then the following:",
        "where ?",
        "is again the shuffled circular convolution.",
        "An n-gram pi .",
        ".",
        ".",
        "pi+j?1 of pos-tags is represented as ;p i ?",
        ".",
        ".",
        ".",
        "?",
        ";p i+j?1.",
        "Given the properties of the shuffled circular convolution, an n-gram of pos-tags is associated to a versor, as it composes j versors, and two different n-grams have nearly orthogonal vectors.",
        "For example, vector D3(s) for the sentence in Figure 1 is:",
        "Lexicalized model Including lexical information is the hardest part of the overall model, as it makes vectors denser in information.",
        "Here we propose an initial model that is basically as the non-lexicalized model, but includes a vector representing the words in the unigrams.",
        "The equation representing sentences as unigrams is:",
        "set N of nearly orthonormal random vectors.",
        "This guarantees that Dlex1 (s) is not lossy.",
        "Given a pair word-postag (w, p), it is possible to know if the sentence contains this pair, as Dlex1 (s)?",
        ";p?",
        ";w ?",
        "1 if (w, p) is in sentence s and Dlex1 (s)?",
        "otherwise.",
        "Other vectors for representing words, e.g., distributional vectors or those obtained as lookup tables in deep learning architectures (Collobert and Weston, 2008), do not guarantee this possibility.",
        "The general equation for the lexicalized version of the sentence encoder follows:",
        "This model is only an initial proposal in order to take into account lexical information."
      ]
    },
    {
      "heading": "3.3.2 Learning Transformers with Linear Regression",
      "text": [
        "The transformer P of the DRP (see Equation 1) can be seen as a linear regressor:",
        "where P is a square matrix.",
        "This latter can be estimated having training sets (T,S) of oracle vectors and sentence input vectors (",
        "si.",
        "Interpreting these sets as matrices, we need to solve a linear set of equations, i.e.: T = PS.",
        "An approximate solution can be computed us",
        "singular value decomposition (SVD).",
        "Matrices have the property SS+ = I.",
        "Using the iterative method for computing SVD (Golub and Kahan, 1965), we can obtain different approximations S+(k) of S + considering k singular values.",
        "Final approximations of DRP s are then: P(k) = TS+(k).",
        "Matrices P are estimated by pseudo-inverting matrices representing input vectors for sentences S. Given the different input representations for sentences, we can then estimate different DRPs: DRP1 = TS+1 , DRP2 = TS+2 , and so on.",
        "We need to estimate the best k in a separate parameter estimation set."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We evaluated three issues for assessing DRP models: the performance of DRPs in reproducing oracle distributed trees (Sec.",
        "4.2); the quality of the topology of the vector spaces of distributed trees induced by DRPs (Sec.",
        "4.3); and the computation run time of DRPs (Sec.",
        "4.4).",
        "Section 4.1 describes the experimental set-up."
      ]
    },
    {
      "heading": "4.1 Experimental Set-up",
      "text": [
        "Data We derived the data sets from the Wall Street Journal (WSJ) portion of the English Penn Treebank data set (Marcus et al., 1993), using a standard data split for training (sections 2-21 PTtrain with 39,832 trees) and for testing (section 23 PT23 with 2,416 trees).",
        "We used section 24 PT24 with 1,346 trees for parameter estimation.",
        "We produced the final data sets of distributed trees with three different ?",
        "values: ?=0, ?=0.2, and ?=0.4.",
        "For each ?, we have two versions of the data sets: a non-lexicalized version (no lex), where syntactic trees are considered without words, and a lexicalized version (lex), where words are considered.",
        "Oracle trees t are transformed into oracle distributed trees ;o using the Distributed Tree Encoder DT (see Figure 1).",
        "We experimented with two sizes of the distributed trees space Rd: 4096 and 8192.",
        "We have designed the data sets to determine how DRPs behave with ?",
        "values relevant for syntax-sensitive NLP tasks.",
        "Both tree kernels and distributed tree kernels have the best performances in tasks such as question classification, semantic role labeling, or textual entailment recognition",
        "with ?",
        "values in the range 0?0.4.",
        "System Comparison We compared the DRPs against the existing way of producing distributed trees (based on the recent paper described in (Zanzotto and Dell?Arciprete, 2012)): distributed trees are obtained using the output of a symbolic parser (SP) that is then transformed into a distributed tree using the DT with the appropriate ?.",
        "We refer to this chain as the Distributed Symbolic Parser (DSP ).",
        "The DSP is then the chain DSP (s) = DT (SP (s)) (see Figure 1).",
        "As for the symbolic parser, we used Bikel's version (Bikel, 2004) of Collins?",
        "head-driven statistical parser (Collins, 2003).",
        "For a correct comparison, we used the Bikel's parser with oracle part-of-speech tags.",
        "We experimented with two versions: (1) a lexicalized method DSPlex, i.e., the natural setting of the Collins/Bikel parser, and (2) a fully non-lexicalized version DSPno lex that exploits only part-of-speech tags.",
        "We obtained this last version by removing words in input sentences and leaving only part-of-speech tags.",
        "We trained these DSP s on PTtrain.",
        "Parameter estimation DRPs have two basic parameters: (1) parameter k of the pseudo-inverse, that is, the number of considered eigenvectors (see Section 3.3.2) and (2) the maximum length j of the n-grams considered by the encoder Dj (see Section 3.3.1).",
        "We performed the parameter estimation on the datasets derived from section PT24 by maximizing a pseudo f-measure.",
        "Section 4.2 reports both the definition of the measure and the results of the parameter estimation."
      ]
    },
    {
      "heading": "4.2 Parsing Performance",
      "text": [
        "The first issue to explore is whether DRP s are actually good ?distributed syntactic parsers?.",
        "We compare DRP s against the distributed symbolic parsers by evaluating how well these ?distributed syntactic parsers?",
        "reproduce oracle distributed trees.",
        "Method A good DRP should produce distributed trees that are similar to oracle distributed trees.",
        "To capture this, we use the cosine similarity between the system and the oracle vectors:",
        "t is the system's distributed tree and ;o is the oracle distributed tree.",
        "We compute these dim Model ?",
        "= 0 ?",
        "= 0.2 ?",
        "= 0.4",
        "DRPs (with different j) and the DSP on the non-lexicalized data sets with different 's and with the two dimensions of the distributed tree space (4096 and 8192).",
        "?",
        "indicates significant difference wrt.",
        "DSPno lex (p << .005 computed with the Stu",
        "and the DSPlex on the lexicalized data sets with different 's on the distributed tree space with 4096 dimensions the cosine similarity at the sentence-based (i.e., vector-based) granularity.",
        "Results report average values.",
        "Estimated parameters We estimated parameters k and j by training the different DRP s on the PTtrain set and by maximizing the similarity of the DRP s on PT24.",
        "The best pair of parameters is j=3 and k=3000.",
        "For completeness, we report also the best k values for the five different j we experimented with: k = 47 for j=1 (the linearly independent vectors representing pos-tags), k = 1300 for j=2, k = 3000 for j=3, k = 4000 for j=4, and k = 4000 for j=5.",
        "For comparison, some resulting tables report results for the different values of j.",
        "Results Table 1 reports the results of the first set of experiments on the non-lexicalized data sets.",
        "The first block of rows (seven rows) reports the average cosine similarity of the different methods on the distributed tree spaces with 4096 dimensions.",
        "The second block (the last three rows) reports the performance on the space with 8192 dimensions.",
        "The average cosine similarity is computed on the PT23 set.",
        "Although we already selected j=3 as the best parameterization (i.e. DRP3), the first",
        "4096 between the oracle's vector space and the systems?",
        "vector spaces (100 trials on lists of 1000 sentence pairs).",
        "five rows of the first block report the results of the DRPs for five values of j.",
        "This gives an idea of how the different DRPs behave.",
        "The last two rows of this block report the results of the two DSPs.",
        "We can observe some important facts.",
        "First, DRP s exploiting 2-grams, 3-grams, 4-grams, and 5-grams of part-of-speech tags behave significantly better than the 1-grams for all the values of ?.",
        "Distributed representation parsers need inputs that keep trace of sequences of pos-tags of sentences.",
        "But these sequences tend to confuse the model when too long.",
        "As expected, DRP3 behaves better than all the other DRPs.",
        "Second, DRP3 behaves significantly better than the comparable traditional parsing chain DSPno lex that uses only part-of-speech tags and no lexical information.",
        "This happens for all the values of ?.",
        "Third, DRP3 behaves similarly to DSPlex for ?=0.",
        "Both parsers use oracle pos tags to emit sentence interpretations but DSPlex also exploits lexical information that DRP3 does not access.",
        "For ?=0.2 and ?=0.4, the more informed DSPlex behaves significantly better than DRP3.",
        "But DRP3 still behaves significantly better than the comparable DSPno lex.",
        "All these observations are valid also for the results obtained for 8192 dimensions.",
        "Table 2 reports the results of the second set of experiments on the lexicalized data sets performed on a 4192-dimension space.",
        "The first row reports the average cosine similarity of DRP3 trained on the lexicalized model and the second row reports the results of DSPlex.",
        "In this case, DRP3 is not behaving well with respect to DSPlex.",
        "The additional problem DRP3 has is that it has to reproduce input words in the output.",
        "This greatly complicates the work of the distributed representation parser.",
        "But, as we report in the next section, this preliminary result may be still satisfactory for ?=0",
        "with the three different methods: similarities between sentences"
      ]
    },
    {
      "heading": "4.3 Kernel-based Performance",
      "text": [
        "This experiment investigates how DRP s preserve the topology of the oracle vector space.",
        "This correlation is an important quality factor of a distributed tree space.",
        "When using distributed tree vectors in learning classifiers, whether ;oi ?",
        ";oj in the oracle's vector space is similar to ; ti ?",
        "; tj in the DRP's vector space is more important than whether ;oi is similar to ; ti (see Figure 3).",
        "Sentences that are close using the oracle syntactic interpretations should also be close using DRP vectors.",
        "The topology of the vector space is more relevant than the actual quality of the vectors.",
        "The experiment on the parsing quality in the previous section does not properly investigate this property, as the performance of DRPs could be not sufficient to preserve distances among sentences.",
        "Method We evaluate the coherence of the topology of two distributed tree spaces by measuring the Spearman's correlation between two lists of pairs of sentences (si, sj), ranked according to the similarity between the two sentences.",
        "If the two lists of pairs are highly correlated, the topology of the two spaces is similar.",
        "The different methods and, thus, the different distributed tree spaces are compared against the oracle vector space (see Figure 3).",
        "Then, the first list always represents the oracle vector space and ranks pairs (si, sj) according to ;o i ?",
        ";o j .",
        "The second list instead represents the space obtained with a DSP or a DRP.",
        "Thus, it is respectively ranked with",
        "way, we can comparatively evaluate the quality of the distributed tree vectors of our DRP s with respect to the other methods.",
        "We report average and standard deviation of the Spearman's correlation on 100 runs over lists of 1000 pairs.",
        "We used the testing set PT23 for extracting vectors.",
        "tence length (dimension = 4092) Results Table 3 reports results both on the non-lexicalized and on the lexicalized data set.",
        "For the non-lexicalized data set we report three methods (DRP3, DSPno lex, and DSPlex) and for the lexicalized dataset we report two methods (DRP3 and DSPlex).",
        "Columns represent different values of ?.",
        "Experiments are carried out on the 4096- dimension space.",
        "For the non-lexicalized data set, distributed representation parsers behave significantly better than DSPno lex for all the values of ?.",
        "The upper-bound of DSPlex is not so far.",
        "For the harder lexicalized data set, the difference between DRP3 and DSPlex is smaller than the one based on the parsing performance.",
        "Thus, we have more evidence of the fact that we are in a good track.",
        "DRP s can substitute the DSP in generating vector spaces of distributed trees that adequately approximate the space defined by an oracle."
      ]
    },
    {
      "heading": "4.4 Running Time",
      "text": [
        "In this last experiment, we compared the running time of the DRP with respect to the DSP .",
        "The analysis has been done on a dual-core processor and both systems are implemented in the same programming language, i.e. Java.",
        "Figure 4 plots the running time of the DRP , the SP , and the full DSP = DT ?",
        "SP .",
        "The x-axis represents the sentence length in words and the y-axis represents the running time in milliseconds.",
        "The distance between SP and DSP shrinks as the plot is in a logarithmic scale.",
        "Figure 5 reports the average cosine similarity of DRP , DSPlex, and DSPno lex, with respect to the sentence length, on the non-lexicalized data set with ?=0.4.",
        "We observe that DRP becomes extremely convenient for sentences larger than 10 words (see Fig. 4) and the average cosine similarity difference between the different methods is nearly constant for the different sentence lengths (see Fig. 5).",
        "This test already makes DRPs very appealing methods for real time applications.",
        "But, if we consider that",
        "Units (GPUs), as dealing only with matrix products, fast-Fourier transforms, and random generators, we can better appreciate the potentials of the proposed methods."
      ]
    },
    {
      "heading": "5 Conclusions and Future Work",
      "text": [
        "We presented Distributed Representation Parsers (DRP) as a novel path to use syntactic structures in feature spaces.",
        "We have shown that these ?parsers?",
        "can be learnt using training data and that DRPs are competitive with respect to traditional methods of using syntax in feature spaces.",
        "This novel path to use syntactic structures in feature spaces opens interesting and unexplored possibilities.",
        "First, DRPs tackle the issue of computational efficiency of structural kernel methods (Rieck et al., 2010; Shin et al., 2011) from another perspective.",
        "DRPs could reduce structural kernel computations to extremely efficient dot products.",
        "Second, the tight integration of parsing and feature vector generation lowers the computational cost of producing distributed representations from trees, as circular convolution is not applied on-line.",
        "Finally, DRPs can contribute to treat syntax in deep learning models in a uniform way.",
        "Deep learning models (Bengio, 2009) are completely based on distributed representations.",
        "But when applied to natural language processing tasks (e.g., (Collobert et al., 2011; Socher et al., 2011)), syntactic structures are not represented in the neural networks in a distributed way.",
        "Syntactic information is generally used by exploiting symbolic parse trees, and this information positively impacts performances on final applications, e.g., in paraphrase detection (Socher et al., 2011) and in semantic role labeling (Collobert et al., 2011).",
        "Building on the results presented here, an interesting line of research is then the integration of distributed representation parsers and deep learning models."
      ]
    }
  ]
}
