{
  "info": {
    "authors": [
      "Jennifer Williams",
      "Rafael Banchs",
      "Haizhou Li"
    ],
    "book": "DiscoMT",
    "id": "acl-W13-3301",
    "title": "Meaning Unit Segmentation in English and Chinese: a New Approach to Discourse Phenomena",
    "url": "https://aclweb.org/anthology/W13-3301",
    "year": 2013
  },
  "references": [
    "acl-C88-1057",
    "acl-C96-1070",
    "acl-E09-1040",
    "acl-J02-1002",
    "acl-N04-1030",
    "acl-N12-1016",
    "acl-P03-1054",
    "acl-P10-2012",
    "acl-P98-2141",
    "acl-W02-1801",
    "acl-W03-1730",
    "acl-W10-1733",
    "acl-W12-3129"
  ],
  "sections": [
    {
      "text": [
        "ber of units identified, average number of words per unit identified, by language and genre.",
        "Since the number of units were not normally distributed for English LIT and English UDHR,",
        "detailed information about distributions for number of annotations, as well as the average number of units found, and average words per unit.",
        "This information informs us as to how large or small on average the meaning units are.",
        "Note that in Table 2 we include information for overall English UDHR and overall English LIT distributions for reference.",
        "The authors found it interesting that, from Table 2, the number of words per meaning unit generally followed the 7 +/- 2 ?chunks?",
        "phenomenon, where chunks are words.5.2 Annotator Agreement Even though some of the annotators agreed about the number of units, that does not imply that they agreed on where the boundaries were placed.",
        "We used segmentation similarity (S) as a metric for annotator agreement.",
        "The algorithm requires specifying a unit of measurement between boundaries ?",
        "in our case we used word-level units for English data and character-level units for Chinese data.",
        "We calculated average similarity agreement for segment boundaries pairwise within-group for annotators from each of the 9 language/genre datasets, as presented in Table 3.",
        "While the segmentation similarity agreements seem to indicate high annotator agreement, we wanted to find out if that agreement was better than what we could generate at random, so we compared annotator agreement with random baselines.",
        "To generate the baselines, we used the average number of segments per paragraph in each language/genre dataset and inserted boundaries at random.",
        "For each of the 9 language/genre datasets, we generated 30 baseline samples.",
        "We calculated the baseline segmentation similarity",
        "(SBL) in the same way using average pairwise agreement within-group for all of the baseline datasets, shown in Table 3.",
        "For English UDHR, we also calculated average pairwise agreement across groups, shown in Table 4.",
        "For example, we compared English UDHR G1 with English UDHR G2, etc.",
        "Human annotators consistently outperformed the baseline across groups for English UDHR.Dataset (S) (SBL)"
      ]
    },
    {
      "heading": "(SBL).6 Analysis",
      "text": [
        "Constructing concepts in this task is systematic as was shown from the segmentation similarity scores.",
        "Since we know that the annotators agreed on some things, it is important to find out what they have agreed on.",
        "In our analysis, we examined unit boundary locations across genres in addition to phrase structure using constituency parses.",
        "In this section, we begin to address another of our original research questions regarding how well speakers agree on meaning unit boundary positions across genres and which syntactic features are the most salient for meaning units."
      ]
    },
    {
      "heading": "6.1 Unit Boundary Positions for Genres",
      "text": [
        "Boundary positions are interesting because they can potentially indicate if there are salient parts of the texts which stand out to annotators across genres.",
        "We have focused this analysis across genres for the overall data for each of the 4 language/genre pairs.",
        "Therefore, we have omitted the subgroups ?",
        "English UDHR groups (G1,G2, G3) and English LIT groups (G1, G2).",
        "Although segmentation similarity is greater within-group from Table 3, this was not enough to inform us of which boundaries annotators fully agree on.",
        "For each of the datasets, we counted the number of annotators who agreed on a given boundary location and plotted histograms.",
        "In these plots we show the number of annotators of each potential boundary between words.",
        "We show the resulting distributions in Figures 5 ?",
        "8.",
        "While there were not many annotators for the Chinese UDHR data, we can see from Figure 5",
        "that at most 4 annotators agreed on boundary positions.",
        "We can see from Figures 6 ?",
        "8 that there is high frequency of agreement in the text which corresponds to paragraph boundaries for the English data, however paragraph boundaries were artificially introduced into the experiment because each paragraph was numbered.",
        "Since we had removed all punctuation markings, including periods and commas for both languages, it is interesting to note there was not full agreement about sentence boundaries.",
        "While we did not ask annotators to mark sentence boundaries, we hoped that these would be picked up by the annotators when they were constructing meaning units in the text.",
        "Only 3 sentence boundaries were identified by at most 2 Chinese UDHR annotators.",
        "On the other hand, all of the sentence boundaries were idenfied for English UDHR and English NEWS, and one sentence boundary was unmarked for English LIT.",
        "However, there were no sentence boundaries in the English data that were marked by all annotators - in fact the single most heavily annotated sentence boundary was for English NEWS, where 30% of the annotators marked it.",
        "The lack for identifying sentence boundaries could be due to an oversight by annotators, or it could also be indicative of the difficulty and ambiguity of the task.6.2 Phrase Structure To answer our question of whether or not there are salient syntactic features for meaning units, we did some analysis with constituency phrase structure and looked at the maximal projections of meaning units.",
        "For each of the 3 English genres (UDHR, NEWS, and LIT) we identified boundaries where at least 50% of the annotators agreed.",
        "For the Chinese UDHR data, we identified boundaries where at least 30% of annotators agreed.",
        "We used the Stanford PCFG Parser on the original English and Chinese text to obtain constituency parses (Klein & Manning, 2003), then aligned the agreeable segment boundaries with the constituency parses.",
        "We found the maximal projection corresponding to each annotated unit and we calculated the frequency of each of the maximal projections.",
        "The frequencies of part-of-speech for maximal projections are shown in Tables 5 - 8.",
        "Note that the part-of-speech tags reflected here come from the Stanford PCFG Parser.Max.",
        "Projection Description Freq.S, SBAR, SINV Clause 28PP Prepositional Phrase 14VP Verb Phrase 11NP Noun Phrase 5ADJP Adjective Phrase 3ADVP Adverb Phrase 1",
        "Clauses were by far the most salient boundaries for annotators of English.",
        "On the other hand, nouns, noun phrases, and verb phrases were the most frequent for annotators of Chinese.",
        "There is some variation across genres for English.",
        "This analysis begins to address whether or not it is possible to identify syntactic features of meaning units, however it leaves open another question as to if it is possible to automatically identify a 1-to-1 mapping of concepts across languages.7 Discussion and Future Work We have presented an experimental framework for examining how English and Chinese speakers make meaning out of text by asking them to label places that they could construct concepts with as few words as possible.",
        "Our results show that there is not a unique ?meaning unit?",
        "segmentation criteria among annotators.",
        "However, there seems to be some preferential trends on how to perform this task, which suggest that any random segmentation is not acceptable.",
        "As we have simplified the task of meaning unit identification by using well-structured text from the Universal Declaration of Human Rights, news, and literature, future work should examine identifying meaning units in transcribed speech.",
        "Annotators for the English UDHR and English LIT datasets could be characterized by their different granularities of annotation in terms of number of units identified.",
        "These observations are insightful to our first question: what granularity do people use to construct meaning units?",
        "For some, meaning units consist of just a few words, whereas for others they consist of longer phrases or possibly clauses.",
        "As we did not have enough responses for the Chinese UDHR data, we are unable to comment if identification of meaning units in Chinese fit a similar distribution as with English and we leave in-depth cross-language analysis to future work.",
        "A particularly interesting finding was that human annotators share agreement even across groups, as seen from Table 4.",
        "This means that although annotators may not agree on the number of meaning units found, they do share some agreement regarding where in the text they are creating the meaning units.",
        "These findings seem to indicate that annotators are creating meaning units systematically regardless of granularity.",
        "Our findings suggest that different people organize and process information differently.",
        "This is a very important conclusion for discourse analysis, machine translation and many other applications as this suggests that there is no optimal solution to the segmentation problems considered in these tasks.",
        "Future research should focus on better understanding the trends we identified and the observed differences among different genres.",
        "While we did not solicit feedback from annotators in this experiment, we believe that it will be important to do so in future work to improve the annotation task.",
        "We know that the perceived lag time in speech-to-speech translation cannot be completely eliminated but we are interested in systems that are ?fast?",
        "enough for humans to have quality conversations in different languages.Acknowledgments This work was partly supported by Singapore Agency for Science, Technology and Research (A-STAR) and the Singapore International Pre-Graduate Award (SIPGA) and was partly supported by the National Science Foundation (NSF) award IIS-1225629.",
        "Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of A-STAR and NSF."
      ]
    }
  ]
}
