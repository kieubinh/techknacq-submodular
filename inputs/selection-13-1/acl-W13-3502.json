{
  "info": {
    "authors": [
      "Michael Bloodgood",
      "John Grothendieck"
    ],
    "book": "CoNLL",
    "id": "acl-W13-3502",
    "title": "Analysis of Stopping Active Learning based on Stabilizing Predictions",
    "url": "https://aclweb.org/anthology/W13-3502",
    "year": 2013
  },
  "references": [
    "acl-C08-1059",
    "acl-C08-1142",
    "acl-D07-1051",
    "acl-D07-1082",
    "acl-D08-1112",
    "acl-I08-1048",
    "acl-J96-2004",
    "acl-N09-2035",
    "acl-P02-1064",
    "acl-P04-1075",
    "acl-P08-2017",
    "acl-P09-1021",
    "acl-P10-1088",
    "acl-W00-1306",
    "acl-W03-0407",
    "acl-W05-0619",
    "acl-W09-1107",
    "acl-W10-0101"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies.",
        "This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP).",
        "The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen's Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be la-beled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained.",
        "Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented.",
        "Specifically, if the Kappa agreement between two models exceeds a threshold T (where T > 0), then the difference in F-measure performance between those models is bounded above by 4(1?T )T in all cases.",
        "If precisionof the positive conjunction of the models is assumed to be p, then the bound can be tightened to 4(1?T )(p+1)T ."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Active learning (AL), also called query learning and selective sampling, is an approach to reduce the costs of creating training data that has received considerable interest (e.g., (Argamon-Engelson and Dagan, 1999; Baldridge and Osborne, 2008; Bloodgood and Vijay-Shanker, 2009b; Bloodgood and Callison-Burch, 2010; Hachey et al., 2005; Haertel et al., 2008; Haffari and Sarkar, 2009; Hwa, 2000; Lewis and Gale, 1994; Sassano, 2002; Settles and Craven, 2008; Shen et al., 2004; Thompson et al., 1999; Tomanek et al., 2007; Zhu and Hovy, 2007)).",
        "Within the NLP community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies.",
        "The main idea is that by judiciously selecting which examples to have labeled, annotation effort will be focused on the most helpful examples and less annotation effort will be required to achieve given levels of performance than if a passive learning policy had been used.",
        "Historically, the problem of developing methods for detecting when to stop AL was tabled for future work and the research literature was focused on how to select which examples to have labeled and analyzing the selection methods (Cohn et al., 1996; Seung et al., 1992; Freund et al., 1997; Roy and McCallum, 2001).",
        "However, to realize the savings in annotation effort that AL enables, we must have a method for knowing when to stop the annotation process.",
        "The challenge is that if we stop too early while useful generalizations are still being made, then we can wind up with a model that performs poorly, but if we stop too late after all the useful generalizations are made, then human annotation effort is wasted and the benefits of using active learning are lost.",
        "Recently research has begun to develop methods for stopping AL (Schohn and Cohn, 2000; Ertekin et al., 2007b; Ertekin et al., 2007a; Zhu and Hovy, 2007; Laws and Schu?tze, 2008; Zhu et al., 2008a; Zhu et al., 2008b; Vlachos, 2008; Bloodgood, 2009; Bloodgood and Vijay-Shanker, 2009a; Ghayoomi, 2010).",
        "The methods are all",
        "heuristics based on estimates of model confidence, error, or stability.",
        "Although these heuristic methods have appealing intuitions and have had experimental success on a small handful of tasks and datasets, the methods are not widely usable in practice yet because our community's understanding of the stopping methods remains too coarse and inexact.",
        "Pushing forward on understanding the mechanics of stopping at a more exact level is therefore crucial for achieving the design of widely usable effective stopping criteria.",
        "Bloodgood and Vijay-Shanker (2009a) introduce the terminology aggressive and conservative to describe the behavior of stopping methods1 and conduct an empirical evaluation of the different published stopping methods on several datasets.",
        "While most stopping methods tend to behave conservatively, stopping based on stabilizing predictions computed via inter-model Kappa agreement has been shown to be consistently aggressive without losing performance (in terms of F-Measure2) in several published empirical tests.",
        "This method stops when the Kappa agreement between consecutively learned models during AL exceeds a threshold for three consecutive iterations of AL.",
        "Although this is an intuitive heuristic that has performed well in published experimental results, there has not been any theoretical analysis of the method.",
        "The current paper presents the first theoretical analysis of stopping based on stabilizing predictions.",
        "The analysis helps to explain at a deeper and more exact level why the method works as it does.",
        "The results of the analysis help to characterize classes of problems where the method can be expected to work well and where (unmodified) it will not be expected to work as well.",
        "The theory is suggestive of modifications to improve the robustness of the stopping method for certain classes of problems.",
        "And perhaps most important, the approach that we use in our analysis provides an enabling framework for more precise analysis of stopping criteria and possibly other parts of the active learning decision space.",
        "In addition, the information presented in this pa",
        "note F1-measure, that is, the balanced harmonic mean of precision and recall, which is a standard metric used to evaluate NLP systems.",
        "per is useful for works that consider switching between different active learning strategies and operating regions such as (Baram et al., 2004; Do?nmez et al., 2007; Roth and Small, 2008).",
        "Knowing when to switch strategies, for example, is similar to the stopping problem and is another setting where detailed understanding of the variance of stabilization estimates and their link to performance ramifications is useful.",
        "More exact understanding of the mechanics of stopping is also useful for applications of co-training (Blum and Mitchell, 1998), and agreement-based co-training (Clark et al., 2003) in particular.",
        "Finally, the proofs of the Theorems regarding the relationships between Cohen's Kappa statistic and F-measure may be of broader use in works that consider inter-annotator agreement and its ramifications for performance appraisals, a topic that has been of long-standing interest in computational linguistics (Car-letta, 1996; Artstein and Poesio, 2008).",
        "In the next section we summarize the stabilizing predictions (SP) stopping method.",
        "Section 3 analyzes SP and Section 4 concludes."
      ]
    },
    {
      "heading": "2 Stopping Active Learning based on",
      "text": []
    },
    {
      "heading": "Stabilizing Predictions",
      "text": [
        "The intuition behind the SP method is that the models learned during AL can be applied to a large representative set of unlabeled data called a stop set and when consecutively learned models have high agreement on their predictions for classifying the examples in the stop set, this indicates that it is time to stop (Bloodgood and Vijay-Shanker, 2009a; Bloodgood, 2009).",
        "The active learning stopping strategy explicitly examined in (Bloodgood and Vijay-Shanker, 2009a) (after the general form is discussed) is to calculate Cohen's Kappa agreement statistic between consecutive rounds of active learning and stop once it is above 0.99 for three consecutive calculations.",
        "Since the Kappa statistic is an important aspect of this method, we now discuss some background regarding measuring agreement in general, and Cohen's Kappa in particular.",
        "Measurement of agreement between human annotators has received significant attention and in that context, the drawbacks of using percentage agreement have been recognized (Artstein and Poesio, 2008).",
        "Alternative metrics have been proposed that take chance agreement into account.",
        "Artstein and Poesio (2008) survey several agreement metrics.",
        "Most",
        "of the agreement metrics they discuss are of the form:",
        "ment expected by chance.",
        "The different metrics differ in how they compute Ae.",
        "All the instances of usage of an agreement metric in this article will have two categories and two coders.",
        "The two categories are ?+1?",
        "and ?-1?",
        "and the two coders are the two consecutive models for which agreement is being measured.",
        "Cohen's Kappa statistic3 (Cohen, 1960) measures agreement expected by chance by modeling each coder (in our case model) with a separate distribution governing their likelihood of assigning a particular category.",
        "Formally, Kappa is defined by Equation 1 with Ae computed as follows:",
        "where each ci is one of the coders (in our case, models), and P (k|ci) is the probability that coder (model) ci labels an instance as being in category k. Kappa estimates the P (k|ci) in Equation 2 based on the proportion of observed instances that coder (model) ci labeled as being in category k."
      ]
    },
    {
      "heading": "3 Analysis",
      "text": [
        "This section analyzes the SP stopping method.",
        "Section 3.1 analyzes the variance of the estimator of Kappa that SP uses and in particular the relationship of this variance to specific aspects of the operationalization of SP, such as the stop set size.",
        "Section 3.2 analyzes relationships between the Kappa agreement between two models and the difference in F-measure between those two models."
      ]
    },
    {
      "heading": "3.1 Variance of Kappa Estimator",
      "text": [
        "SP bases its decision to stop on the information contained in the contingency tables between the classifications of models learned at consecutive iterations during AL.",
        "In determining whether to stop at iteration t, the classifications of the current model Mt are compared with the classifications of the previous model Mt?1.",
        "Table 1 shows the population parameters for these two models, where: 3We note that there are other agreement measures (beyond Cohen's Kappa) which could also be applicable to stopping based on stabilizing predictions, but an analysis of these is outside the scope of the current paper.",
        "ties forMt (model learned at iteration t) andMt?1 (model learned at iteration t-1).",
        "population probability piij for i, j ?",
        "{+,?}",
        "is the probability of an example being placed in category i by model Mt?1 and category j by model Mt; population probability pi.j for j ?",
        "{+,?}",
        "is the probability of an example being placed in category j by model Mt; and population probability pii.",
        "for i ?",
        "{+,?}",
        "is the probability of an example being placed in category i by model Mt?1.",
        "The actual probability of agreement is pio = pi++ + pi??.",
        "As indicated in Equation 2, Kappa models the probability of agreement expected due to chance by assuming that classifications are made independently.",
        "Hence, the probability of agreement expected by chance in terms of the population probabilities is pie = pi+.pi.++pi?.pi.?.",
        "From the definition of Kappa (see Equation 1), we then have that the Kappa parameter K in terms of the population probabilities is given by",
        "For practical applications we will not know the true population probabilities and we will have to resort to using sample estimates.",
        "The SP method uses a stop set of size n for deriving its estimates.",
        "Table 2 shows the contingency table counts for the classifications of models Mt and Mt?1 on a sample of size n. The population probabilities piij can be estimated by the relative frequencies pij for",
        "Let po = p++ + p?",
        "?, the observed proportion of agreement and let pe = p+.p.+ + p?.p.",
        "?, the proportion of agreement expected by chance if we assume that Mt and Mt?1 make their classifications independently.",
        "Then the Kappa measure of agreement K between Mt and Mt?1 (see Equation 3) is estimated by",
        "iteration t-1).",
        "Using the delta method, as described in (Bishop et al., 1975), Fleiss et al. (1969) derived an estimator of the large-sample variance of K?.",
        "According to Hale and Fleiss (1993), the estimator simplifies",
        "where p?i = (pi.",
        "+ p.i)/2.",
        "From Equation 5, we can see that the variance of our estimate of Kappa is inversely proportional to the size of the stop set we use.",
        "Bloodgood and Vijay-Shanker (2009a) used a stop set of size 2000 for each of their datasets.",
        "Although this worked well in the results they reported, we do not believe that 2000 is a fixed size that will work well for all tasks and datasets where the SP method could be used.",
        "Table 3 shows the variances of K?",
        "computed using Equation 5 at the points at which SP stopped AL for each of the datasets4 from (Bloodgood and Vijay-Shanker, 2009a).",
        "These variances indicate that the size of 2000 was typically sufficient to get tight estimates of Kappa, helping to illuminate the empirical success of the SP method on these datasets.",
        "More generally, the SP method can be augmented with a variance check: if the variance of estimated Kappa at a potential stopping point exceeds some desired 4We note that each of the datasets was set up as a binary classification task (or multiple binary classification tasks).",
        "Further details and descriptions of each of the datasets can be found in (Bloodgood and Vijay-Shanker, 2009a).",
        "threshold, then the stop set size can be increased as needed to reduce the variance.",
        "Looking at Equation 5 again, one can note that when pe is relatively close to 1, the variance of K?",
        "can be expected to get quite large.",
        "In these situations, users of SP should expect to have to use larger stop set sizes and in extreme conditions, SP may not be an advisable method to use."
      ]
    },
    {
      "heading": "3.2 Relationship between Kappa agreement",
      "text": [
        "and change in performance between models Heretofore, the published literature contained only informal explanations of why stabilizing predictions is expected to work well as a stopping method (along with empirical tests demonstrating successful operation on a handful of tasks and datasets).",
        "In the remainder of this section we describe the mathematical foundations for stopping methods based on stabilizing predictions.",
        "In particular, we will prove that even in the worst possible case, if the Kappa agreement between two subsequently learned models is greater than a threshold T , then it must be the case that the change in performance between these two models is bounded above by 4(1?T )T .",
        "We then go on toprove additional Theorems that tighten this bound when assumptions are made about model precision.",
        "Lemma 3.1 Suppose F-measure F and Kappa K are computed from the same contingency table of counts, such as the one given in Table 2.",
        "Suppose ad?",
        "bc ?",
        "0.",
        "Then F ?",
        "K. Proof By definition, in terms of the contingency table counts,",
        "Rewriting F so that it will have the same numerator as K, we have:",
        "(using Equation 5) from the contingency table at the point at which SP stopped AL and the average of all the variances (across all folds of CV) is displayed.",
        "The last row contains the macro-average of the average variances for all the datasets.",
        "We can see that the expression for F in Equation 10 has the same numerator as K in Equation 6 but the denominator ofK in Equation 6 is?",
        "the denominator of F in Equation 10.",
        "Therefore, F ?",
        "K. Theorem 3.2 LetMt be the model learned at iteration t of active learning and Mt?1 be the model learned at iteration t ?",
        "1.",
        "Let Kt be the estimate of Kappa agreement between the classifications of Mt and Mt?1 on the examples in the stop set.",
        "Let F?t be the F-measure between the classifications of Mt and truth on the stop set.",
        "Let F?t?1 be the F-measure between the classifications of Mt?1 and truth on the stop set.",
        "Let ?Ft be F?t ?",
        "F?t?1.",
        "Suppose T > 0.",
        "Then Kt > T ?",
        "|?Ft |?",
        "4(1?T )T .",
        "Proof Suppose Mt, Mt?1, Kt, F?t, F?t?1, ?Ft, and T are defined as stated in the statement of Theorem 3.2.",
        "Let Ft be the F-measure between the classifications of Mt and Mt?1 on the examples in the stop set.",
        "Let Table 2 show the contingency table counts for Mt versus Mt?1 on the examples in the stop set.",
        "Then, from their definitions, we have Kt = 2(ad?bc)(a+b)(b+d)+(a+c)(c+d) and Ft = 2a2a+b+c .",
        "There exist true labels for the examples in the stop set, which we don't know since the stop set is unlabeled, but nonetheless must exist.",
        "We use the truth on the stop set to split Table 2 into two subtables of counts, one table for all the examples that are truly positive and one table for all the examples that are truly negative.",
        "Table 4",
        "at iteration t-1) for only the examples in the stop set that have truth = -1. shows the contingency table for Mt versus Mt?1 for all of the examples in the stop set that have true labels of +1 and Table 5 shows the contingency table for Mt versus Mt?1 for all of the examples in the stop set that have true labels of -1.",
        "From Tables 2, 4, and 5 one can see that a is the number of examples in the stop set that both Mt and Mt?1 classified as positive.",
        "Furthermore, out of these a examples, a1 of them truly are pos",
        "for Mt versus truth and Mt?1 versus truth can be derived from Tables 4 and 5.",
        "For convenience, Table 6 shows the contingency table for Mt versus truth and Table 7 shows the contingency table for Mt?1 versus truth.",
        "Suppose that Kt > T .",
        "This implies, by Lemma 3.15, that Ft > T .",
        "This implies that",
        "For notational convenience, let: dA = c1 ?",
        "b1 and dB = c?1 ?",
        "b?1.",
        "Then it follows that",
        "observe that the following three inequalities hold:",
        "Note that in deriving Inequality 26, we used the previously derived Inequality 14.",
        "Also, the proof of Theorem 3.2 assumes a worst possible case in the sense that all examples where the classifications of Mt and Mt?1 differ are assumed to have truth values that all serve to maximize one model's F-measure and minimize the other model's F-measure so as to maximize |?Ft |as much as possible.",
        "A resulting limitation is that the bound is loose in many cases.",
        "It may be possible to derive tighter bounds, perhaps by easing off to an expected case instead of a worst case and/or by making additional assumptions.6 Taking this possibility up, we now prove tighter bounds when assumptions about the precision of the models Mt and Mt?1 are made.",
        "Consider that in the proof of Theorem 3.2 when transitioning from Equality 27 to Inequality 28, we used the fact that ah+dA+dB ?",
        "1.",
        "Note that ah+dA+dB =a 2a1+b1+2c1+d1+a?1+c?1 , from which one sees thata h+dA+dB = 1 only if all of a1, b1, c1, d1 and c?1are all zero.",
        "This is a pathological case.",
        "In many practically important classes of cases to consider, a h+dA+dB will be strictly less than 1, and often substantially less than 1.",
        "The following two Theorems prove tighter bounds on |?Ft |than Theorem 3.2 by utilizing this insight.",
        "Theorem 3.3 Suppose Mt, Mt?1, Kt, F?t, F?t?1, ?Ft, and T are defined as stated in the statement of Theorem 3.2.",
        "Let the contingency tables be defined as they were in the proof of Theorem 3.3.",
        "Let MPositiveConjunction be a model that only classifies an example as positive if both models Mt and Mt?1 classify the example as positive.",
        "Suppose that MPositiveConjunction has perfect precision on the stop set, or in other words that every single example from the stop set that both Mt and Mt?1 classify as positive is truthfully positive (i.e., a?1 = 0).",
        "Then Kt > T ?",
        "|?Ft |?",
        "2(1?T )T .",
        "Proof The proof of Theorem 3.2 holds exactly as it is up until Equality 27.",
        "Now, using the additional assumption that a?1 = 0, we have",
        "Theorem 3.3 is a special case (in the limit) of a more general Theorem.",
        "Before stating and proving the more general Theorem, we prove a Lemma that will be helpful in making the proof of the general Theorem clearer.",
        "Lemma 3.4 Let f , dA, dB and contingency table counts be defined as they were in the proof of Theorem 3.2.",
        "Suppose a1 = xa?1.",
        "Then a",
        "by definition of contingency table counts.",
        "Hence,",
        "The following Theorem generalizes Theorem 3.3 to cases when MPositiveConjunction has precision p in (0, 1).7 Theorem 3.5 Suppose Mt, Mt?1, Kt, F?t, F?t?1, ?Ft, and T are defined as stated in the statement of Theorem 3.2.",
        "Let the contingency tables be defined as they were in the proof of Theorem 3.2.",
        "Let MPositiveConjunction be a model that only classifies an example as positive if both models Mt and Mt?1 classify the example as positive.",
        "Suppose that MPositiveConjunction has precision p on the stop set.",
        "Then Kt > T ?",
        "|?Ft |?",
        "4(1?T )(p+1)T .",
        "Proof The proof of Theorem 3.2 holds exactly as it is up until Equality 27.",
        "MPositiveConjunction has precision p on the stop set?",
        "p = a1a1+a?1 .",
        "Solving for a1 in terms of a?1 we have a1 = p1?pa?1.",
        "Therefore, applying Lemma 3.4 with x = p1?p , we",
        "rem 3.5 for different precision values.",
        "The scaling factor 1p+1 in Theorem 3.5 showshow the precision of the conjunctive model affects the bound.",
        "Theorem 3.2 had the scaling factor implicitly set to 1 in order to handle the pathological case where the positive conjunctive model has precision = 0.",
        "In Theorem 3.3, where the positive conjunctive model has precision = 1 on the examples in the stop set, the scaling factor is set to 1/2.",
        "Theorem 3.5 generalizes the scaling factor so that it is a function of the precision of the positive conjunctive model.",
        "For convenience, Table 8 shows the scaling factor values for a few different precision values.",
        "The bounds in Theorems 3.2, 3.3, and 3.5 all bound the difference in performance on the stop set of two consecutively learned models Mt and Mt?1.",
        "An issue to consider is how connected the difference in performance on the stop set is to the difference in performance on a stream of application examples generated according to the population probabilities.",
        "Taking up this issue, consider that the proof of Theorems 3.2, 3.3, and 3.5 would hold as it is if we had used sample proportions instead of sample counts (this can be seen by simply dividing every count by n, the size of the stop set).",
        "Since the stop set is unbiased (selected at random from the population), as n approaches infinity, the sample proportions will approach the population probabilities and the difference between the difference in performance between Mt and Mt?1 on the stop set and on a stream of application examples generated according to the population probabilities will approach zero."
      ]
    },
    {
      "heading": "4 Conclusions",
      "text": [
        "To date, the work on stopping criteria has been dominated by heuristics based on intuitions and experimental success on a small handful of tasks and datasets.",
        "But the methods are not widely usable in practice yet because our community's understanding of the stopping methods remains too inexact.",
        "Pushing forward on understanding the mechanics of stopping at a more exact level is therefore crucial for achieving the design of widely usable effective stopping criteria.",
        "This paper presented the first theoretical analysis of stopping based on stabilizing predictions.",
        "The analysis revealed three elements that are central to the SP method's success: (1) the sample estimates of Kappa have low variance; (2) Kappa has tight connections with differences in F-measure; and (3) since the stop set doesn't have to be labeled, it can be arbitrarily large, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time.",
        "We presented proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models.",
        "Specifically, if the Kappa agreement between two models is at least T, then the difference in F-measure performance between those models is bounded above by 4(1?T )T .",
        "If precision of the positive conjunction of the models is assumed to be p, then the bound can be tightened to 4(1?T )(p+1)T .",
        "The setup and methodology of the proofs can serve as a launching pad for many further investigations, including: analyses of stopping; works that consider switching between different active learning strategies and operating regions; and works that consider stopping co-training, and especially agreement-based co-training.",
        "Finally, the relationships that have been exposed between the Kappa statistic and F-measure may be of broader use in works that consider inter-annotator agreement and its interplay with system evaluation, a topic that has been of long-standing interest."
      ]
    }
  ]
}
