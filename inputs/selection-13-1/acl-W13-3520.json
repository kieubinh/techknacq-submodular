{
  "info": {
    "authors": [
      "Rami Al-Rfou",
      "Bryan Perozzi",
      "Steven Skiena"
    ],
    "book": "CoNLL",
    "id": "acl-W13-3520",
    "title": "Polyglot: Distributed Word Representations for Multilingual NLP",
    "url": "https://aclweb.org/anthology/W13-3520",
    "year": 2013
  },
  "references": [
    "acl-A00-1031",
    "acl-D07-1096",
    "acl-D12-1110",
    "acl-J92-4003",
    "acl-J93-2004",
    "acl-N12-1052",
    "acl-P08-1068",
    "acl-P10-1023",
    "acl-P10-1040",
    "acl-W06-1615",
    "acl-W09-1201",
    "acl-W12-4501"
  ],
  "sections": [
    {
      "text": [
        "?????",
        "Putin dongzhi Winter Solstice papa Pope ????????",
        "Yanukovych chunfen Vernal Equinox Papa Pope ???????",
        "Trotsky xiazhi Summer solstice pontefice pontiff ??????",
        "Hitler qiufen Autumnal Equinox basileus basileus ??????",
        "Stalin ziye Midnight canridnale cardinal ????????",
        "Medvedev chuxi New Year's Eve frate friar",
        "placement word in the corrupted phrase.",
        "Figure 2 shows a typical learning curve of the training.",
        "As the number of examples have been seen so far increased both the training error and the development error go down."
      ]
    },
    {
      "heading": "6 Qualitative Analysis",
      "text": [
        "In order to understand how the embeddings space is organized, we examine the subtle information captured by the embeddings through investigating the proximity of word groups.",
        "This information has the potential to help researchers develop applications that use such semantic and syntactic information.",
        "The embeddings not only capture syntactic features, as we will demonstrate in Section 4, but also demonstrate the ability to capture interesting semantic information.",
        "Table 3 shows different words in several languages.",
        "For each word on top of each list, we rank the vocabulary according to their Euclidean distance from that word and show the closest five neighboring words.",
        "?",
        "French & Spanish - Expected groupings of colors and professions is clearly observed.",
        "?",
        "English - The example shows how the embedding space is aware of the name change that happened to a group of Indian cities.",
        "?Mumbai?",
        "used to be called ?Bombay?, ?Chennai?",
        "used to be called ?Madras and ?Kolkata?",
        "used to be called ?Calcutta?.",
        "On the other hand, ?Hyderabad?",
        "stayed at a similar distance from both names as they point to the same conceptual meaning.",
        "?",
        "Arabic - The first example shows the word ?Thanks?.",
        "Despite not removing the diacritics from the text, the model learned that the two surface forms of the word mean similar things and, therefore, grouped them together.",
        "In Arabic, conjunction words do not get separated from the following word.",
        "Usually, ?and thanks?",
        "serves as a letter signature as ?sincerely?",
        "is used in English.",
        "The model learned that both words {?and thanks?, ?thanks? }",
        "are similar, regardless their different forms.",
        "The second example illustrates a specific syntactic morphological feature of Arabic, where enumeration of couples has its own form.",
        "?",
        "German - The example demonstrates that the compositional semantics of multi-unit words are still preserved.",
        "?",
        "Russian - The model learned to group Russian/Soviet leaders and other figures related to the Soviet history together.",
        "?",
        "Chinese - The list contains three solar terms that are part of the traditional East Asian lu-nisolar calendars.",
        "The remaining two terms correspond to traditional holidays that occur at the same dates of these solar terms.",
        "?",
        "Italian - The model learned that the lower and upper cases of the word has similar meaning."
      ]
    },
    {
      "heading": "7 Sequence Tagging",
      "text": [
        "Here we analyze the quality of the models we have generated.",
        "To test the quantitative performance of the embeddings, we use them as the sole features for a well studied NLP task, part of speech tagging.",
        "To demonstrate the capability of the learned dis",
        "over the test datasets.",
        "Third column represents the total accuracy of the tagger the former two columns reports the accuracy over known words and OOV words (unknown).",
        "The results are compared to the TnT tagger results reported by (Petrov et al., 2012).",
        "?CoNLL 2006 dataset tributed representations in extracting useful word features, we train a PoS tagger over the subset of languages that we were able to acquire free annotated resources for.",
        "We choose our tagger for this task to be a neural network because it has a fast convergence rate based on our initial experiments.",
        "The part of speech tagger has similar architecture to the one used for training the embeddings.",
        "However we have changed some of the network parameters, specifically, we use a hidden layer of size 300 and learning rate of 0.3.",
        "The network is trained by minimizing the negative of the log likelihood of the labeled data.",
        "To tag a specific word wi we consider a window with size 2n where n in our experiment is equal to 2.",
        "Equation 4 shows how we construct a feature vector F by concatenating (?)",
        "the embeddings of the words occurred in the window, where C is the matrix that contains the embeddings of the language vocabulary.",
        "The feature vector will be fed to the network and the error will back propagated back to the embeddings.",
        "The results of this experiment are presented in Table 4.",
        "We train and test our models on the universal tagset proposed by (Petrov et al., 2012).",
        "This universal tagset maps each original tag in a treebank to one out of twelve general PoS tags.",
        "This simplifies the comparison of classifiers performance across languages.",
        "We compare our results to a similar experiment conducted in their work, where they trained a TnT tagger (Brants, 2000) on several treebanks.",
        "The TnT tagger is based on Markov models and depends on trigram counts observed in the labeled data.",
        "It was chosen for its fast speed and (near to) state-of-the-art accuracy, without language specific tuning.",
        "The performance of embeddings is competitive in general.",
        "Surprisingly, it is doing better than the TnT tagger in English and Danish.",
        "Moreover, our performance is so close in the case of Swedish.",
        "This task is hard for our tagger for two reasons.",
        "The first is that we do not add OOV words seen during training of the tagger to our vocabulary.",
        "The second is that all OOV words are substituted with one representation, ?UNK?",
        "and there is no character level information used to inform the tagger about the characteristic of the OOV words.",
        "On the other hand, the performance on the known words is strong and consistent showing the value of the features learned about these words from the unsupervised stage.",
        "Although the word coverage of German and Czech are low in the original Wikipedia corpora (See Table 2), the features learned are achieving great accuracy on the known words.",
        "They both achieve above 98.5% accuracy.",
        "It is noticeable that the Slovene model performs the worst, under both known and unknown words categories.",
        "It achieves only 93.46% accuracy on the test dataset.",
        "Given that the Slovene embeddings were trained on the least amount of data among all other embeddings we test here, we expect the quality to go lower for the other smaller Wikipedias not tested here.",
        "In Table 5, we present how well the vocabulary of each language's embeddings covered the part of speech datasets.",
        "The datasets come from a different domain than Wikipedia, and this is reflected in the results.",
        "In Table 6, we present the results of training the same neural network part of speech tagger without using our embeddings as initializations.",
        "We found that the embeddings benefited all the languages we considered, and observed the greatest benefit in languages which had a small number of training examples.",
        "We believe that these results illustrate the performance",
        "vocabulary on the part of speech datasets after normalization.",
        "Token coverage is the raw percentage of words which were known, while the Word coverage ignores repeated words."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "Distributed word representations represent a valuable resource for any language, but particularly for resource-scarce languages.",
        "We have demonstrated how word embeddings can be used as off-the-shelf solution to reach near to state-of-art performance over a fundamental NLP task, and we believe that our embeddings will help researchers to develop tools in languages with which they have no expertise.",
        "Moreover, we showed several examples of interesting semantic relations expressed in the embeddings space that we believe will lead to interesting applications and improve tasks as semantic compositionality.",
        "While we have only considered the properties of word embeddings as features in this work, it has been shown that using word embeddings in conjunction with traditional NLP features can signifi",
        "ger compared to our results.",
        "Using the embeddings was generally helpful, especially in languages where we did not have many training examples.",
        "The scores presented are the best we found for each language (languages with more resources could afford to train longer before overfitting).",
        "cantly improve results on NLP tasks (Turian et al., 2010; Collobert et al., 2011).",
        "With this in mind, we believe that the entire research community can benefit from our release of word embeddings for over 100 languages.",
        "We hope that these resources will advance the study of possible pairwise mappings between embeddings of several languages and their relations.",
        "Our future work in this area includes improving the models by increasing the size of the context window and their domain adaptivity through incorporating other sources of data.",
        "We will be investigating better strategies for modeling OOV words.",
        "We see improvements to OOV word handling as essential to ensure robust performance of the embeddings on real-world tasks."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research was partially supported by NSF Grants DBI-1060572 and IIS-1017181, with additional support from TexelTek."
      ]
    }
  ]
}
