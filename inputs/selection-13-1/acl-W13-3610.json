{
  "info": {
    "authors": [
      "Zhongye Jia",
      "Peilu Wang",
      "Hai Zhao"
    ],
    "book": "CoNLL",
    "id": "acl-W13-3610",
    "title": "Grammatical Error Correction as Multiclass Classification with Single Model",
    "url": "https://aclweb.org/anthology/W13-3610",
    "year": 2013
  },
  "references": [
    "acl-C08-1109",
    "acl-N03-1033",
    "acl-N12-1067",
    "acl-W00-1308",
    "acl-W12-2025",
    "acl-W12-2028",
    "acl-W12-2032",
    "acl-W13-1703",
    "acl-W13-3601"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes our system in the shared task of CoNLL-2013.",
        "We illustrate that grammatical error detection and correction can be transformed into a multiclass classification task and implemented as a single-model system regardless of various error types with the aid of maximum entropy modeling.",
        "Our system achieves the F1 score of 17.13% on the standard test set."
      ]
    },
    {
      "heading": "1 Introduction and Task Description",
      "text": [
        "Grammatical error correction is the task of automatically detecting and correcting erroneous word usage and ill-formed grammatical constructions in text (Dahlmeier et al., 2012).",
        "This task could be helpful for hundreds of millions of people around the world that are learning English as a second language.",
        "Although there have been much of work on grammatical error correction, the current approaches mainly focus on very limited error types and the result is far from satisfactory.",
        "The CoNLL-2013 shared task, compared with the previous Help Our Own (HOO) tasks focusing on only determiner and preposition errors, considers a more comprehensive list of error types, including determiner, preposition, noun number, verb form, and subject-verb agreement errors.",
        "The evaluation metric used in CoNLL-2013 is Max-Matching (M2) (Dahlmeier and Ng, 2012) precision, recall and F1 between the system edits and a manually created set of gold-standard edits.",
        "The corpus used in CoNLL-2013 is NUS Corpus of Learner English (NUCLE) of which the details are described in (Dahlmeier et al., 2013).",
        "In this paper, we describe the system submission from the team 1 of Shanghai Jiao Tong Univer-?This work was partially supported by the National Natural Science Foundation of China (Grant No.60903119, Grant No.61170114, and Grant No.61272248), and the National Basic Research Program of China (Grant No.2009CB320901 and Grant No.2013CB329401).",
        "?Corresponding author sity (SJT1).",
        "Grammatical error detection and correction problem is treated as multiclass classification task.",
        "Unlike previous works (Dahlmeier et al., 2012; Ro-zovskaya et al., 2012; Kochmar et al., 2012) that train a model upon each error type, we use one single model for all error types.",
        "Instead of the original error type, a more detailed version of error types is used as class labels.",
        "A rule based system generates labels from the golden edits utilizing an extended version of Leven-shtein edit distance.",
        "We use maximum entropy (ME) model as classifier to obtain the error types and use rules to do the correction.",
        "Corrections are made using rules.",
        "Finally, the corrections are filtered using language model (LM)."
      ]
    },
    {
      "heading": "2 System Architecture",
      "text": [
        "Our system is a pipeline of grammatical error detection and correction.",
        "We treats grammatical error detection as a classification task.",
        "First all the tokens are relabeled according to the golden annotation and a sequence of modified version of error types is generated.",
        "This re-labeling task is rule based using an extended version of Levenshtein edit distance which will be discussed in the following section.",
        "Then with the modified error types as the class labels, a classifier using ME model is trained.",
        "The grammatical error correction is also rule based, which is basically the reverse of the rela-beling phase.",
        "The modefied version of error types that we used is much more detailed than the original five types so that it enables us to use one rule to do the correction for each modified error type.",
        "After all, the corrections are filtered by LM, to remove those corrections that seem much worse than the original sentence.",
        "As typical classification task, we have a training step and a test step.",
        "The training step consists three phases: ?",
        "Error types relabeling.",
        "?",
        "Training data refinement.",
        "?",
        "ME training.",
        "The test step includes three phases: ?",
        "ME classification.",
        "?",
        "Error correction according to lebels.",
        "?",
        "LM filtering."
      ]
    },
    {
      "heading": "2.1 Rebeling by Levenshtein Edit Distance",
      "text": [
        "with Inflection In CoNLL-2013 there are 5 error types but they cannot be used directly as class labels, since they are too general for error correction.",
        "For example, the verb form error includes all verb form inflections such as converting a verb to its infinitive form, gerund form, paste tense, paste participle, passive voice and so on.",
        "Previous approaches (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Kochmar et al., 2012) manually decompose each error types to more detailed ones.",
        "For example, in (Dahlmeier et al., 2012), the determinater error is decomposed into: ?",
        "replacement determiner (RD): { a?",
        "the } ?",
        "missing determiner (MD): { ??",
        "a } ?",
        "unwanted determiner (UD): { a?",
        "? }",
        "For limited error types such as merely determinatives error and preposition error in HOO 2012, manually decomposition may be sufficient.",
        "But for CoNLL-2013 with 5 error types including complicated verb inflection, an automatic method to decompose error types is needed.",
        "We present an extended version of Levenshtein edit distance to decompose error types into more detailed class labels and relabel the input with the labels generated.",
        "The original Levenshtein edit distance has 4 edit types: unchange (U), addition (A), deletion (D) and substitution (S).",
        "We extend the ?substitution?",
        "edit into two types of edits: inflection (I) and the orig-nal substitution (S).",
        "To judge whether two words can be inflected from each other, the extended algorithm needs lemmas as input.",
        "If the two words have the same lemma, they can be inflected from each other.",
        "The extended Levenshtein edit distance with inflection is shown in Algorithm 1.",
        "It takes the source tokens toksrc, destination tokens tokdst and their lemmas lemsrc, lemdst as input and returns the edits E and the parameters of edits P. For example, for the golden edit {look?",
        "have been looking at}, the edits E returned by DISTANCE will be {A,A,U ,A}, and the parameters P of edits returned by DISTANCE will be {have, been, looking, at}.",
        "Then with the output of DISTANCE, the labels can be generated by calculating the edits between original text and golden edits.",
        "For those tokens without errors, we directly assign a special label ???",
        "to them.",
        "The tricky part of the relabeling algorithm is the problem of the edit ?addition?, A.",
        "A new token can only be added before or after an existing token.",
        "Thus for labels with addition, we must find some token that the label can be assigned to.",
        "That sort of token is defined as pivot.",
        "A pivot can be a token that is not changed in Algorithm 1 Levenshtein edit distance with inflection",
        "1: function DISTANCE(toksrc, tokdst, lemsrc, lemdst) 2: (lsrc, ldst)?",
        "(len(toksrc), len(tokdst)) 3: D[0 .",
        ".",
        ".",
        "lsrc][0 .",
        ".",
        ".",
        "ldst]?",
        "0 4: B[0 .",
        ".",
        ".",
        "lsrc][0 .",
        ".",
        ".",
        "ldst]?",
        "(0, 0) 5: E[0 .",
        ".",
        ".",
        "lsrc][0 .",
        ".",
        ".",
        "ldst]?",
        "?",
        "6: for i?",
        "1 .",
        ".",
        ".",
        "lsrc do 7: D[i][0]?",
        "i 8: B[i][0]?",
        "(i?",
        "1, 0) 9: E[i][0]?",
        "D 10: end for 11: for j ?",
        "1 .",
        ".",
        ".",
        "ldst do 12: D[0][j]?",
        "j 13: B[0][j]?",
        "(0, j ?",
        "1) 14: E[0][j]?",
        "A 15: end for 16: for i?",
        "1 .",
        ".",
        ".",
        "lsrc; j ?",
        "1 .",
        ".",
        ".",
        "ldst do 17: if toksrc[i?",
        "1] = tokdst[j ?",
        "1] then 18: D[i][j]?",
        "D[i?",
        "1][j ?",
        "1] 19: B[i][j]?",
        "(i?",
        "1, j ?",
        "1) 20: E[i][j]?",
        "U 21: else 22: m ?",
        "min(D[i ?",
        "1][j ?",
        "1], D[i ?",
        "1][j], D[i][j ?",
        "1]) 23: if m = D[i?",
        "1][j ?",
        "1] then 24: D[i][j]?",
        "D[i?",
        "1][j ?",
        "1] + 1 25: B[i][j]?",
        "(i?",
        "1, j ?",
        "1) 26: if lemsrc[i ?",
        "1] = lemdst[j ?",
        "1] then 27: E[i][j]?",
        "S 28: else 29: E[i][j]?",
        "I 30: end if 31: else if m = D[i?",
        "1][j] then 32: D[i][j]?",
        "D[i?",
        "1][j] + 1 33: B[i][j]?",
        "(i?",
        "1, j) 34: E[i][j]?",
        "D 35: else if m = D[i][j ?",
        "1] then 36: D[i][j]?",
        "D[i][j ?",
        "1] + 1 37: B[i][j]?",
        "(i, j ?",
        "1) 38: E[i][j]?",
        "A 39: end if 40: end if 41: end for 42: (i, j)?",
        "(lsrc, ldst) 43: while i > 0 ?",
        "j > 0 do 44: insert E[i][j] into head of E 45: insert tokdst[j ?",
        "1] into head of P 46: (i, j)?",
        "B[i][j] 47: end while 48: return (E,P) 49: end function",
        "an edit, such as the ?apple?",
        "in edit {apple ?",
        "an apple}, or some other types of edit such as the inflection of ?look?",
        "to ?looking?",
        "in edit {look?",
        "have been look",
        "ing at}.",
        "In the CoNLL-2013 task, the addition edits are mostly adding articles or determinaters, so when generating the label, adding before the pivot is preferred and only those trailing edits are added after the last pivot.",
        "At last, the label is normalized to upper case.",
        "The BNF syntax of labels is defined in Figure 1.",
        "The the non-terminal ?inflection-rules?",
        "can be substituted by terminals of inflection rules that are used for correcting the error types of noun number, verb form, and subject-verb agreement errors.",
        "All the inflection rules are listed in Table 1.",
        "The ?stop-word?",
        "can be subsi-tuted by terminals of stop words which contains all articles, determinnaters and prepositions for error types of determiner and preposition, and a small set of other common stop words.",
        "All the stop words are listed in",
        "Algorithm 2 is used to generate the label from the extended Levenshtein edits according to the syntax defined in Figure 1.",
        "It takes the original tokens, tokorig and golden edit tokens, tokgold in an annotation and their lemmas, lemorig, lemgold as input and returns the generated label L. For our previous example of edit {looked ?",
        "have been looking at}, the L returned by RELABEL is {HAVE?BEEN?GERUND?AT}.",
        "Some other examples of relabeling are demonstrated in Table 3.",
        "The correction step is done by rules according to the labels.",
        "The labels are parsed with a simple LL(1) parser and edits are made according to labels.",
        "A bit of extra work has to be taken to handle the upper/lower case problem.",
        "For additions and substitutions, the words added or substituted are normalized to lowercase.",
        "For inflections, original case of words are reserved.",
        "Then a bunch of regular expressions are applied to correct upper/lower case for sentence head."
      ]
    },
    {
      "heading": "Catalog Words",
      "text": [
        "Determinater a an the my your his her its our their this that these those Preposition about along among around as at beside besides between by down during except for from in inside into of off on onto outside over through to toward towards under underneath until up upon with"
      ]
    },
    {
      "heading": "2.2 Training Corpus Refinement",
      "text": [
        "The corpus used to train the grammatical error recognition model is highly imbalanced.",
        "The original training corpus has 57,151 sentences and only 3,716 of them contain at least one grammatical error, and only 5,049 of the total 1,161K words are needed to be corrected.",
        "This characteristic makes it hard to get a well-performed machine learning model, since the samples to be recognized are so sparse and those large amount of correct data will severely affect the machine learning process as it is an optimization on the global training data.",
        "While developing our system, we found that only using sentences containing grammatical errors will lead to a notable improvement of the result.",
        "1: function RELABEL(tokorig , tokgold, lemorig , lemgold) 2: (E,P) ?",
        "DISTANCE(tokorig, tokgold, lemorig , lemgold) 3: pivot?",
        "number of edits in E that are not A 4: L?",
        "?",
        "5: L?",
        "??",
        "6: while i < length of E do 7: if E[i] = A then 8: L?",
        "L+ label of edit E[i] with P[i] 9: i?",
        "i + 1 10: else 11: l?",
        "L+ label of edit E[i] with P[i] 12: pivot?",
        "pivot?",
        "1 13: if pivot = 0 then 14: i?",
        "i + 1 15: while i < length of E do 16: l?",
        "l +?+ P[i] 17: i?",
        "i + 1 18: end while 19: end if 20: push l into L 21: L?",
        "??",
        "22: end if 23: end while 24: L?",
        "upper case of L 25: return L 26: end function",
        "Inspired by this phenomenon, we propose a method to refine the training corpus which will reduce the error sparsity of the training data and notably improve the recall rate.",
        "The refined training corpus is composed of contexts containing grammatical errors.",
        "To keep the information which may have effects on the error identification and classification, those contexts are selected based on both syntax tree and n-gram, of which the process is shown in Algorithm 3.",
        "For a word with error, its syntax context of size c is those words in the minimal subtree in the syntax tree with no less than c leaf nodes; and its n-gram context of size n is n?",
        "1 words before and after itself.",
        "In the CORPUSREFINE algorithm, the input c gives the size of syntax context and n provides the size of the n-gram context.",
        "These two parameters decide the amount of information that may affect the recognition of errors.",
        "To obtain the context, given a sentence containing a grammatical error, we first get a minimum syntax tree whose number of terminal nodes exceed the c threshold, then check whether the achieved context containing the error word's n-gram, if not, add the n-gram to the context.",
        "At last the context is returned by CORPUSREFINE.",
        "An example illustrating this process is presented in Figure 2.",
        "In this example, n and c are both set to 5, Algorithm 3 Training Corpus Refine Algorithm",
        "1: function CORPUSREFINE(sentence, c, n) 2: context?",
        "?",
        "3: if sentence contains no error then 4: return ?",
        "5: end if 6: build the syntax tree T of sentence 7: enode?",
        "the node with error in T 8: e?",
        "enode 9: while True do 10: pnode?",
        "parent node of e in T 11: cnodes ?",
        "all the children nodes of pnode in T 12: if len(cnodes) > c then 13: context?",
        "cnodes 14: break 15: end if 16: e?",
        "pnode 17: end while 18: i?",
        "the position of enode in context 19: l?",
        "size of context 20: if i < n then 21: add (n-i) words before context at the head of context 22: end if 23: if l-i<n then 24: append (l-i) words after context in context 25: end if 26: return context 27: end function",
        "the minimal syntax tree and the context decided by it are colored in green.",
        "Since the syntax context in the green frame does not contain the error word's 5-gram, therefore, the 5-gram context in the blue frame is added to the syntax context and the final achieved context of this sentence is ?have to stop all works for the development?."
      ]
    },
    {
      "heading": "2.3 LM Filter",
      "text": [
        "All corrections are filtered using a large LM.",
        "Only those corrections that are not too much worse than the original sentences are accepted.",
        "Perplexity (PPL) of the n-gram is used to judge whether a sentence is good:",
        "We use rPPL, the ratio between the PPL before and after correction, as a metric of information gain.",
        "Only those corrections with an rPPL lower than a certain threshold are accepted."
      ]
    },
    {
      "heading": "3 Features",
      "text": [
        "The single model approach enables us only to optimize one feature set for all error type in the task, which can drastically reduce the computational cost in feature selection.",
        "As many previous works have proposed various of features, we first collected features from different previous works including (Dahlmeier et al., 2012; Ro-zovskaya et al., 2012; HAN et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008).",
        "Then experiments with different features were built to test these features?",
        "effectiveness and only those have positive contribution to the final performance were preserved.",
        "The features we used are presented in Table 4, where word0 is the word that we are generating features for, and word and POS is the word itself and it's POS tag for various components.",
        "NPHead denotes the head of the minimum Noun Phrase (NP) in syntax tree.",
        "wordNP?1 represents the word appearing before NP in the sentence.",
        "NC stands for noun compound and is composed of the last n words (n ?",
        "2) in NP which are tagged as a noun.",
        "Verb feature is the word that is tagged as a verb whose direct object is the NP containing current word.",
        "Adj feature represents the first word in the NP whose POS is adjective.",
        "Prep feature denotes the preposition word if it immediately precedes the NP.",
        "DPHead is the parent of the current word in the dependency tree, and DPRel is the dependency relation with parent."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Data Sets",
      "text": [
        "The CoNLL-2013 training data consist of 1,397 articles together with gold-standard annotation.",
        "The documents are a subset of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013).",
        "The official test data consists of 50 new essays which are also from NUCLE.",
        "The first 25 essays were written in response to one prompt.",
        "The remaining 25 essays were written in response to a second prompt.",
        "One of the prompts had been used for the training data, while the other prompt is new.",
        "More details of the data set are described in (Ng et al., 2013).",
        "We split the the entire training corpus ALL by article.",
        "For our training step, we randomly pick 90% articles of ALL and build a training corpus TRAIN of 1,258 articles.",
        "The rest 10% of ALL with 139 articles are for developing corpus DEV .",
        "For the final submission, we use the entire corpus ALL for relabeling and training.",
        "stared+VBD, at+IN.",
        ".",
        ".",
        "The example sentence is:?That man stared at the old oak tree.?",
        "and the current word is ?the?",
        "."
      ]
    },
    {
      "heading": "?BY ?BETWEEN BESIDES BEEN?PART BEEN AT?AN AT?A AS?THE AS?HAS AROUND ARE?PAST ARE?A ARE?? A???OF AM?",
      "text": []
    },
    {
      "heading": "4.2 Resources",
      "text": [
        "We use the following NLP resources in our system.",
        "For relabeling and correction, perl module Lingua::EN::Inflect1 (Conway, 1998) is used for determining noun and verb number and Lin-gua::EN::VerbTense2 is used for determining verb tense.",
        "A revised and extended version of maximum entropy model3 is used for ME modeling.",
        "For lemmatization, the Stanford CoreNLP lemma annotator (Toutanova et al., 2003; Toutanova and Manning, 2000) is used.",
        "The language model is built by the SRILM toolkit (Stolcke and others, 2002).",
        "The corpus for building LM is the EuroParl corpus (Koehn, 2005).",
        "The English part of the German-English parallel corpus is actually used.",
        "We use such a corpus to build LM for the following reasons: 1.",
        "LM for grammatical error correction should be trained from corpus that itself is grammatically correct, and the EuroParl corpus has very good quality of writing; 2. the NUCLE corpus mainly contains essays on subjects such as environment, economics, society, politics and so on, which are in the same dormain as those of the EuroParl corpus."
      ]
    },
    {
      "heading": "4.3 Relabeling the Corpus",
      "text": [
        "There are some complicated edits in the annotations that can not be represented by our rules, for example substitution of non-stopwords such as {human?",
        "people} or {are not short of?",
        "do not lack}.",
        "The relabeling phase will ignore those ones thus it may not cover all the edits.",
        "After all, we get 174 labels after relabeling on the entire corpus as shown in Table 6.",
        "These labels are generated following the syntax defined in Figure1 and terminals defined in Table 1 and Table 2.",
        "Directly applying these labels for correction receives an M2 score of Precission = 91.43%,Recall = 86.92% and F1 = 89.12%.",
        "If the non-stopwords non-inflection edits are included, of course the labels will cover all the golden annotations and directly applying labels for correction can receive a score up to almost F1 = 100%.",
        "But that will get nearly 1,000 labels which is too computationally expensive for a classification task.",
        "Cut out labels with very low frequency such as those exists only once reduces the training time, but does not give significant performance improvement, since it hurts the coverage of error detection.",
        "So we use all the labels for training."
      ]
    },
    {
      "heading": "4.4 LM Filter Threshold",
      "text": [
        "To choose the threshold for rPPL, we run a series of tests on the DEV set with different thresholds.",
        "From our empirical observation on those right corrections and those wrong ones, we find the right ones seldomly",
        "With higher threshold, more correction with lower information gain will be obtained.",
        "Thus the recall grows higher but the precission grows lower.",
        "We can observe a peak of F1 arround 1.8 to 2.0, and the threshold chosen for final submission is 1.91."
      ]
    },
    {
      "heading": "4.5 Evaluation and Result",
      "text": [
        "The evaluation is done by calculating the M2 precission, recall and F1 score between the system output and golden annotation.",
        "All the error types are evaluated jointly.",
        "Only one run of a team is permitted to be submitted.",
        "Table 7 shows our result on our DEV data set and the official test data set."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we presented the system from team 1 of Shanghai Jiao Tong University that participated in the HOO 2012 shared task.",
        "Our system achieves an F1 score of 17.13% on the official test set based on gold-standard edits."
      ]
    }
  ]
}
