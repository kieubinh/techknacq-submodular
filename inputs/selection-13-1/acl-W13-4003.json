{
  "info": {
    "authors": [
      "Alejandra Lorenzo",
      "Lina Rojas-Barahona",
      "Christophe Cerisara"
    ],
    "book": "SIGDIAL",
    "id": "acl-W13-4003",
    "title": "Unsupervised structured semantic inference for spoken dialog reservation tasks",
    "url": "https://aclweb.org/anthology/W13-4003",
    "year": 2013
  },
  "references": [
    "acl-D09-1002",
    "acl-D09-1003",
    "acl-D11-1122",
    "acl-E12-1003",
    "acl-N10-1137",
    "acl-P09-1004",
    "acl-P11-1112",
    "acl-W04-3213",
    "acl-W06-1601",
    "acl-W11-0146",
    "acl-W11-2209",
    "acl-W11-2212"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This work proposes a generative model to infer latent semantic structures on top of manual speech transcriptions in a spoken dialog reservation task.",
        "The proposed model is akin to a standard semantic role labeling system, except that it is unsupervised, it does not rely on any syntactic information and it exploits concepts derived from a domain-specific ontology.",
        "The semantic structure is obtained with unsupervised Bayesian inference, using the Metropolis-Hastings sampling algorithm.",
        "It is evaluated both in terms of attachment accuracy and purity-collocation for clustering, and compared with strong baselines on the French MEDIA spoken-dialog corpus."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many concrete applications that involve human-machine spoken dialogues exploit some hand-crafted ontology that defines and relates the concepts that are useful for the application.",
        "The main challenge for the dialog manager used in the application is then to interpret the user's spoken input in order to correctly answer the user's expectations and conduct a dialogue that shall be satisfactory for the user.",
        "This whole process may be decomposed into the following stages: ?",
        "Automatic speech recognition, to transform the acoustic signal into a sequence of words (or sequences of word hypotheses); ?",
        "Spoken language understanding, to segment and map these sequences of words into concepts of the ontology;",
        "?",
        "Semantic analysis, to relate these concepts together and interpret the semantic of the user input at the level of the utterance, or of the speaker turn; ?",
        "Dialogue act recognition ?",
        "Dialogue planning ?",
        "Text generation ?",
        "...",
        "Note that the process sketched here often further involves several other important steps that are used internally within one or several of these broad stages, for instance named entity recognition, co-reference resolution, syntactic parsing, marcov decision process, reinforcement learning, etc.",
        "This work focuses mainly on the second and third stages, since we assume that segmentation is given and we want to discover the underlying concepts and relations in the data.",
        "The third stage is very important because it exhibits the latent semantic structure hidden in the user utterance: what is the object affected by a given predicate ?",
        "What are the modifiers that may alter the meaning of a predicate ?",
        "Without such a structure, the system can hardly push understanding beyond lexical semantics and reach fine-grained semantic representations, which are thus often limited to well-formed inputs and cannot handle spontaneous speech as considered here.",
        "But still, despite its importance, most spoken dialog systems do not make use of such structure.",
        "We propose an approach here to address this issue by directly inferring the semantic structure from the flat sequence of concepts using the unsupervised Bayesian learning framework.",
        "Hence, the proposed model does not rely on any predefined corpus annotated with semantic structure, which makes it much more robust to spoken inputs and adaptable to new domains than traditional supervised approaches."
      ]
    },
    {
      "heading": "2 Related work",
      "text": [
        "In recent years, an increasing number of works have addressed robustness and adaptability issues in most of standard Natural Language Processing tasks with unsupervised or semi-supervised machine learning approaches.",
        "Unsupervised learning attempts to induce the annotations from large amounts of unlabeled data.",
        "Several approaches have recently been proposed in this context for the semantic role labeling task.",
        "(Swier and Stevenson, 2004) were the first to introduce an unsupervised semantic parser, followed by (Grenager and Manning, 2006), (Lang and Lapata, 2010), (Lang and Lapata, 2011b) and (Lang and Lapata, 2011a).",
        "Finally, (Titov and Klementiev, 2012), introduced two new Bayesian models that achieve the best current state-of-the-art results.",
        "However, all these works use some kind of supervision (namely a verb lexicon or a supervised syntactic system, which is the case in most of the approaches).",
        "(Abend et al., 2009) proposed an unsupervised algorithm for argument identification that uses a fully unsupervised syntactic parser and where the only supervised annotation is part-of-speech (POS) tagging.",
        "Semi-supervised learning attempts to improve the performance of unsupervised algorithms by using both labeled and unlabeled data for training, where typically the amount of labeled data is smaller.",
        "A variety of algorithms have been proposed for semi-supervised learning1.",
        "In the context of semantic role labeling, (He and Gildea, 2006) and (Lee et al., 2007) hence tested self-training and co-training, while (Fu?rstenau and Lapata, 2009) used a graph-alignment method to semantic role labeling (SRL).",
        "Finally, in (De-schacht and Moens, 2009) the authors present a semi-supervised Latent Words Language Model, which outperforms a state-of-the-art supervised baseline.",
        "Although semi-supervised learning approaches minimize the manual effort involved, they still require some amount of annotation.",
        "This annotation is not always available, sometimes expensive to create and often domain specific.",
        "Moreover, these systems assume a specific role labeling (e.g. PropBank, FrameNet or VerbNet) and are not generally portable from one framework to another.",
        "A number of works related to semantic inference have already been realized on the French 1We refer the reader to (Zhu, 2005) or (Pise and Kulkarni, 2008) for an overview on semi-supervised learning methods.",
        "MEDIA corpus.",
        "Hence, dynamic Bayesian networks were proposed for semantic composition in (Meurs et al., 2009), however their model relies on manual semantic annotation (i.e. concept-value pairs) and supervised training through the definition of 70 rules.",
        "In (Huet and Lefe`vre, 2011; Camelin et al., 2011) unsupervised models were proposed that use stochastic alignment and Latent Dirichlet Allocation respectively, but these models infer a flat concept-value semantic representation.",
        "Compared to these works, we rather propose a purely unsupervised approach for structured semantic Metropolis-Hastings inference with a generative model specifically designed for this task."
      ]
    },
    {
      "heading": "3 Proposed model",
      "text": []
    },
    {
      "heading": "3.1 Principle",
      "text": [
        "We consider a human-machine dialog, with the objective of automatically building a semantic structure on top of the user's spoken utterances that shall help the dialog system to interpret the user inputs.",
        "This work focuses on inferring the semantic structure, and it assumes that a segmentation of users?",
        "utterances into concepts is given.",
        "More precisely, we exploit as input a manual segmentation of each utterance into word segments, where each segment represents a single concept that belongs to MEDIA ontology (Denis et al., 2006) (see Figure 1).",
        "This ontology identifies the concepts that can have arguments, and we thus use this information to further distinguish between head segments that can have arguments (noted Wh2 in Figure 3) and argument segments that cannot govern another concept (noted Wa).",
        "From these two classes of",
        "segments and the words?",
        "inflected forms that compose each segment we infer:",
        "?",
        "A semantic structure composed of triplets (Wa,Wh, A) where A is the type of argument, or, in other words, the type of semantic relation between both segments; ?",
        "A semantic class Ct for the head segment",
        "An example of the target structure we want to obtain is shown in Figure 2.",
        "Inference of these structure and classes is realized with an unsupervised Bayesian model, i.e., without training the model on any corpus annotated with such relations.",
        "Instead, the model is trained on an unlabeled dialog corpus composed of raw manual speech transcriptions, which have also been manually segmented into utterances and words?",
        "segments as described above.",
        "Training is actually realized on this corpus using an approximate Bayesian inference algorithm that computes the posterior distribution of the model's parameters given the dataset.",
        "We have used for this purpose the Metropolis-Hastings Markov Chain Monte Carlo algorithm."
      ]
    },
    {
      "heading": "3.2 Bayesian model",
      "text": [
        "Figure 3 shows the plate diagram of the proposed model.",
        "The plate Nh (respectively Nw) that surrounds a shaded node represents a single words?",
        "segment of length Nh (respectively Nw).",
        "The outer plate Nu indicates that the graphical model shall be repeated for each of the Nu utterances in the corpus.",
        "Each head word segment has a latent semantic type Ct, and governs Na arguments.",
        "Each argument is represented by an argument words?",
        "segment, which has a latent semantic typeA.",
        "Each argument is further characterized by its relative position Rp with respect to its head segment.",
        "Rp",
        "Nu represents the number of utterances; Nh, the number of words in a head segment; Nw, the number of words in an argument segment; and Na the number of arguments assigned to predicate t. can have 4 values, depending on whether the argument is linked to the closest (1) or another (2) verbal3 head, or the closest (3) or another (4) nominal head.",
        "Rp is derived from the argument-to-head assignment, which is latent.",
        "So, Rp is also latent.",
        "The sequence of Nc head segments in utterance u is captured by the HMM shown on top of the plate diagram, which models the temporal dependency between successive ?semantic actions?",
        "of the user.",
        "The variables of the model are explained in Table 1.",
        "The most important property of this model is that the number of arguments Na is not known beforehand.",
        "In fact, every argument segment can be governed by any of the Nc head segments in the utterance, and it is the role of the inference process to actually decide with which head it should be linked.",
        "This is why the model performs structured inference.",
        "Concretely, at any time during training, every argument is governed by a single head.",
        "Then, inference explores a new possible head attachment for an argument Wa, which impacts the model as follows: ?",
        "The number of arguments Na of the previous head is decreased by one; ?",
        "The number of argumentsNa of the new head is increased by one; 3Morphosyntactic classes are obtained with the Treetag-ger",
        "dependency notations are used: the head segment points to the argument segment, where segments are shown with boxes (arrows link segments, not words !).",
        "The semantic class assigned to each head segment is shown in bold below the translated text.",
        "?",
        "The relative position Rp of the argument is recomputed based on its new head position; ?",
        "The argument typeA is also re sampled given the new head type Ct.",
        "This reassignment process, which is at the heart of our inference algorithm, is illustrated in Figure 4."
      ]
    },
    {
      "heading": "3.3 Metropolis inference",
      "text": [
        "Bayesian inference aims at computing the posterior distribution of the model's parameters, given the observed data.",
        "We assume that all distributions in our model are multinomial with uniform priors.",
        "The parameters are thus:",
        "Distrib.",
        "of the relative position of a given argument to its head given the argument type",
        "Distrib.",
        "of the argument types given a head semantic class",
        "To perform inference, we have chosen a Markov Chain Monte Carlo algorithm.",
        "As our model is finite, parametric and identifiable, Doob's theorem guarantees the consistency of its posterior, and thus the convergence of MCMC algorithms towards the true posterior.",
        "Because changing the head of one argument affects several variables simultaneously in the model, it is problematic to use the basic Gibbs sampling algorithm.",
        "A block-Gibbs sampling would have been possible, but this would have increased the computational complexity and we also wanted to keep as much flexibility as possible in the jumps that could be realized in the search space, in order to prevent slow-mixing and avoid (nearly) non-ergodic Markov chains, which are likely to occur in such structured inference problems.",
        "We have thus chosen a Metropolis-Hastings sampling algorithm, which allows us to design an efficient proposal distribution that is adapted to our task.",
        "The algorithm proceeds by first initializing the variables with a random assignment of arguments to one of the heads in the utterance, and a uniform sampling of the class variables.",
        "Then, it iterates through the following steps:",
        "1.",
        "Sample uniformly one utterance u 2.",
        "Sample one jump following the proposal distribution detailed in Section 3.3.2.",
        "3.",
        "Because the proposal is uniform, compute the acceptance ratio between the model's joint probability at the proposed (noted with a ?)",
        "and current states:",
        "example illustrates the third Metropolis proposed move, which changes the head of argument ?le prix?",
        ": arcs above the text represent the initial state, while arcs below the text represent the new proposed state.",
        "5.",
        "When the sample is accepted, update the multinomials accordingly and iterate from step 1 until convergence.",
        "This process is actually repeated for 2,000,000 iterations, and the sample that gives the largest joint probability is chosen.",
        "The proposal distribution is used to explore the search space in an efficient way for the target application.",
        "Each state in the search space is uniquely defined by a value assignment to every variable in the model, for every utterance in the corpus.",
        "It corresponds to one possible sample of all variables, or in other words, to the choice of one possible semantic structure and class assignment to all utterances in the corpus.",
        "Given a current state in this search space, the proposal distribution ?proposes?",
        "to jump to a new state, which will then be evaluated by the Metropolis algorithm.",
        "Our proposal samples a",
        "new state in the following successive steps: 1.",
        "Sample uniformly one of the three possible moves: Move1: Change the semantic class of a head; Move2: Change the argument type of an argument segment; Move3: Change the assignment of an argument to a new head; 2.",
        "If Move1 is chosen, sample uniformly one head segment and one target semantic class; 3.",
        "If Move2 is chosen, sample uniformly one argument segment and one target argument type; 4.",
        "If Move3 is chosen, sample uniformly one argument segment Wa and ?detach?",
        "it from its current head.",
        "Then, sample uniformly one",
        "target head segment W ?h, and reattach Wa to its new head W ?h.",
        "Because the distribution of argument types differ from one head class to another, it would be interesting at this stage to resample the argument type of Wa from the new head class distribution.",
        "But in this work, we resample the argument type from the uniform distribution.",
        "This proposal distribution Q(x ?",
        "x?)",
        "is reversible, i.e., Q(x?",
        "x?)",
        "> 0?",
        "Q(x?",
        "?",
        "x) > 0.",
        "We can show that it is further symmetric, i.e., Q(x ?",
        "x?)",
        "= Q(x?",
        "?",
        "x), because the same move is sampled to jump from x to x?",
        "than to jump from x?",
        "to x, and because the proposal distribution within each move is uniform."
      ]
    },
    {
      "heading": "4 Experimental validation",
      "text": []
    },
    {
      "heading": "4.1 Experimental setup",
      "text": [
        "The French MEDIA corpus collects about 70 hours of spontaneous speech (1258 dialogues, 46k utterances, 494.048 words and 4068 distinct words) for the task of hotel reservation and tourist information (Bonneau-Maynard et al., 2005).",
        "Calls from 250 speakers to a simulated reservation system (i.e. the Wizard-of-Oz) were recorded and transcribed.",
        "Dialogues are full of disfluencies, hesitations, false starts, truncations or fillers words (e.g., euh or ben).",
        "gold annotation.",
        "This corpus has been semantically annotated as part of the French ANR project PORT-MEDIA (Rojas-Barahona et al., 2011).",
        "We are using a set of 330 utterances manually annotated with gold semantic relations (i.e.",
        "High-Level Semantics).",
        "This gold corpus gathers 653 head segments and 1555 argument segments, from which around 20% are both arguments and heads, such as une chambre in Figure 4.",
        "Table 2 shows the semantic relations frequencies in the gold annotation.",
        "12 head segment types and 19 different argument segment types are defined in the gold annotations.",
        "In the evaluation, we assume the number of both classes is given.",
        "A possible extension of the approach to automatically infer the number of classes would be to use a non-parametric model, but this is left for future work."
      ]
    },
    {
      "heading": "4.2 Evaluation metrics",
      "text": [
        "The proposed method infers three types of seman",
        "tic information: ?",
        "The semantic relation between an argument and its head; ?",
        "The argument type A ?",
        "The semantic class of the head Ct.",
        "The three outcomes are evaluated as follows.",
        "?",
        "The output structure is a forest of trees that is similar to a partial syntactic dependency structure.",
        "We thus use a classical unsupervised dependency parsing metric, the Unlabeled Attachment Score (UAS), which is simply the accuracy of argument attachment: an argument is correctly attached if and only if its inferred head matches the gold head.",
        "?",
        "Both argument and head classes correspond to the outcome of a clustering process into semantic classes, akin to the semantic classes obtained in unsupervised semantic role labeling tasks.",
        "We then evaluate them with a classical metric used to evaluate these classes in unsupervised SRL (as done for instance in (Lang and Lapata, 2011a) and (Titov and Klementiev, 2012)): purity and collocation.",
        "Purity measures the degree to which each cluster contains instances that share the same gold class, while collocation measures the degree to which instances with the same gold class are assigned to a single cluster.",
        "More formally, the purity of argument segments?",
        "(head segment?)",
        "clusters for the whole corpus is computed as follows:",
        "whereCi is the set of argument (head) segments in the ith cluster found, Gj is the set of argument (head) segments in the jth gold class, and N is the number of gold argument (head) segment instances.",
        "In a similar way, the collocation of argument segments?",
        "(head segment?)",
        "clusters is computed as follows:",
        "Finally the F1 measure is the harmonic mean of the purity and collocation:"
      ]
    },
    {
      "heading": "4.3 Experimental results",
      "text": [
        "We compare the proposed approach against two baselines: ?",
        "An argument-head ?attachment?",
        "baseline, which attaches each argument to the closest head segment.",
        "?",
        "A strong clustering baseline, which respectively clusters the head and argument segments using a very effective topic model: the Latent Dirichlet Allocation (LDA) approach (Blei et al., 2003).",
        "Table 3 shows the UAS obtained for the proposed model on the MEDIA corpus, while Table 4 shows the obtained Purity, Collocation and F1-measure.",
        "In both cases, we compare the performances of the proposed model with the respective baseline.",
        "Our system outperforms both baselines by a large margin.",
        "We further carried out a qualitative evaluation, where we inspected the inferred clusters and compared them with the baseline.",
        "Figures 7 and 8 show, for every head class Ct in each stacked column, the distribution of instances from all gold clusters.",
        "Each column can also be viewed as a graphical representation of the intersection of one inferred class with all gold clusters.",
        "Figure 7 illustrates this for our model, and Figure 8 for LDA.",
        "The same comparison for the argument types is shown, respectively, in Figure 5 and Figure 6.",
        "For head segment clusters, we can observe that most inferred clusters contain many instances of the Reservation type (in dark blue), both in the LDA baseline and in the proposed system.",
        "The main reason for that is that the corpus is very unbalanced in favor of the Reservation class, while we do not assume any prior knowledge about the data and thus use a uniform prior.",
        "Still, every other gold type that occurs with a reasonnably high enough frequency, apart from two special types that are discussed next, is well captured by one of",
        "color) into the clusters inferred by our system (shown on the X-axis) for argument segments.",
        "our inferred class: this is the case for ?Room?",
        "that mainly intersects with our class 1, ?Place?",
        "with our class 2 and ?Hotel?",
        "with our class 9.",
        "Some examples of instances for each case are: ?",
        "Reservation: ?voudrais re?server?, ?aimerais partir?, ?voudrais une *re?servation une re?servation?, ?prends?, ?recherche?",
        ", ?",
        "*de?sire de?sire?, ?il me faudrait?, ?opte?, ?aimerais s?",
        "il vous pla?",
        "?t si c?",
        "est possible avoir prendre?.",
        "?",
        "Room: ?deux chambres pour un coup(le) avec trois enfants avec bon standing?, ?trois singles?, ?deux chambres de bon standing a` peu pre`s niveau trois e?toiles?, ?trois doubles?.",
        "?",
        "Place: ?Paris?, ?a` Saintes?, ?a` Charleville?, ?dans le dix huitie`me ar-rondissement de Paris?.",
        "?",
        "Hotel: ?un ho?tel deux e?toiles?, ?dans un ho?tel beau standing?, ?un ho?tel formule un?, ?l?",
        "ho?t(el) le l?",
        "ho?tel?, ?un autre ho?tel dans les me?mes conditions?, ?le Beaugency?, ?l?",
        "autre?, ?au Novotel?, ?le premier?.",
        "Two ?special?",
        "head segment types that are neither nicely captured by our system nor LDA are Coordination and Inform, which are instead assigned to the clusters corresponding to the gold segments that they coordinate or inform about.",
        "For argument segments we also observed that the inferred clusters are semantically related to the gold types.",
        "We found, for instance, four clusters",
        "color) into the clusters inferred by our system (shown on the X-axis) for head segments.",
        "(2, 5, 12 and 15) containing mainly ?Time?",
        "arguments (?du premier au trois Novembre?, ?dix nuit?, ?le festival du film?, ?au seize Novembre?, etc.",
        "), two (3 and 14) dedicated to ?Location?",
        "arguments (?a` Menton?, ?au festival lyrique de belle euh Belle Ile En mer?, ?bastille?, ?sur le ville de Paris?, ?parking prive??",
        "), one (10) for ?Price?",
        "arguments (?pas plus de cent euros par personne?, ?un tarif infe?rieur a` quatre vingts euros?, ?pas trop che`re?, ?a` cent vingt euros?, ?moins de cent* cent euros?)",
        "etc.",
        "Finally, as noted for the head segments, we can observe that the most frequent gold types largely intersect with several inferred clusters, for the same reason: data is very unbalanced and we do not assume any prior knowledge about the data",
        "color) into the clusters inferred by the LDA baseline (shown on the X-axis) for head segments.",
        "and thus use an uniform prior.",
        "Nevertheless, several other important classes such as Event, Price and Agent are well captured by our system."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "This work proposes an unsupervised generative model to infer latent semantic structures on top of user spontaneous utterances.",
        "It relies on the Metropolis-Hastings sampling algorithm to jointly infer both the structure and semantic classes.",
        "It is evaluated in the context of the French MEDIA corpus for the hotel reservation task.",
        "Although the system proposed in this work is evaluated on a specific spoken dialog reservation task, it actually relies on a generic unsupervised structured inference model and can thus be applied to many other structured inference tasks, as long as observed word segments are given.",
        "An interesting future direction of research would be to modify this model so that it jointly infers both the latent syntactic and semantic structures, which are known to be closely related but still carry complementary information.",
        "We of course also plan to evaluate the proposed model with automatic speech transcriptions and concepts decoding.",
        "Another advantage of the proposed model is the possibility to build better Metropolis-Hastings proposals, which may greatly improve the convergence rate of the algorithm.",
        "In particular, we would like to investigate the use of some non-uniform proposal distributions when reattaching an argument to a new head, which shall improve mixing."
      ]
    }
  ]
}
