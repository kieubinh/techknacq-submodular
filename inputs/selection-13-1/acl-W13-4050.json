{
  "info": {
    "authors": [
      "Michael Johnston",
      "Patrick Ehlen",
      "Frederick G. Conrad",
      "Michael F. Schober",
      "Christopher Antoun",
      "Stefanie Fail",
      "Andrew Hupp",
      "Lucas Vickers",
      "Huiying Yan",
      "Chan Zhang"
    ],
    "book": "SIGDIAL",
    "id": "acl-W13-4050",
    "title": "Spoken Dialog Systems for Automated Survey Interviewing",
    "url": "https://aclweb.org/anthology/W13-4050",
    "year": 2013
  },
  "references": [
    "acl-J06-3004"
  ],
  "sections": [
    {
      "text": [
        "interviewer's ability to offer help or clarification in ways that might affect results.",
        "Automated dia-log systems can be thought of as the ultimate in standardization, as they can be designed to provide exactly the same interaction possibilities to all respondents.",
        "In effect, everyone can be interviewed by the same ?interviewer.?",
        "Or, if survey designers want to allow clarification in an inter-view, an automated spoken dialog system can ensure that the same possibilities are available to all respondents (Schober and Conrad 1997).",
        "Unlike systems that use human interviewers, there is marginal additional cost per interview after the initial investment of building a system.",
        "This offers significant potential for cost savings in large cross-sectional samples or repeated panel surveys, such as the U.S. Current Population Survey or the American Community Survey.",
        "Re-peated data collection allows refinement and retraining of speech models to improve perfor-mance.",
        "Spoken dialog system surveys can be administered on demand at any time of day, allowing a better fit with respondents?",
        "circum-stances and schedules.",
        "Compared to asynchronous text-based interviews like web or paper-and-pencil surveys, spoken dialog systems can capture richer verbal paradata (Couper 2009) or process data like pauses, disfluencies and prosody (Ehlen et al. 2007).",
        "Finally, survey tasks fit nicely within the limitations of current recognition and dialog technology, since they tend to have a purposefully structured and controlled interaction flow and generally require only a limited number of responses to each question.",
        "While spoken dialog systems have the potential to remove data error that is introduced by variation in human interviewer behaviors, they also introduce risks to survey data quality due to speech recognition and understanding error.",
        "Numerous strategies for mitigating error have been explored in research on dialog systems (Bohus and Rudnicky 2005, Litman et al. 2006).",
        "One approach is to use either an explicit or implicit confirmation of the user's input.",
        "Following previous research showing that explicit confirmation is less confusing for users (Shin et al. 2002), we adopt an explicit confirmation strate-gy, which is also more in keeping with standardized interview techniques.",
        "The effects of speech recognition and understanding errors may be different in a survey dia-log system than in most current spoken dialog applications.",
        "One consideration is speaker initia-tive, and the stake of the user in the interaction.",
        "In systems for customer service, information access, or transactions, the user generally initiates contact with the system and seeks to accomplish a task where the system's recognition accuracy will affect success of the user's own goal.",
        "But in a survey dialog, the system initiates contact, and most respondents do not have a stake in whether the designers of the survey system succeed at collecting high quality data from them.",
        "This is a key point where a survey interviewing system might differ from traditional SDS: From the survey researchers?",
        "perspective, the critical question is not whether individual users achieve some goal, but rather the extent to which individual errors in system recognition and understanding affect the distribution of responses across the population sample, affecting the quality of the estimates produced.",
        "If recognition errors do not affect the substantive conclusions based on the survey data, then survey researchers should be able to tolerate the imprecision of recognition error.",
        "This situation makes survey system evaluation rather different from how one would expect to evaluate the task success of a traditional SDS, like a customer service system.",
        "In Section 2, we characterize the content of the survey items, describe the dialog strategy, and provide examples of interaction.",
        "Section 3 describes the technical architecture of the survey dialog system.",
        "We provide experimental evaluation in Section 4, and conclusions in Section 5.",
        "2 Survey interview dialogs After an initial question assessing whether the respondent is in an environment where it is safe for them to talk, our system administers a series of 32 questions drawn from major U.S. social surveys, including the Behavioral Risk Factor Surveillance System (BRFSS), National Survey of Drug Use and Health (NSDUH), General Social Survey (GSS), and the Pew Internet and American Life Project.",
        "The sample transcribed dialogs in Appendix 1 illustrate various features of interaction with the system.",
        "Question types include Yes/No, categorical (where users pick from a specified set of response options), and numerical questions.",
        "Some categorical items are grouped into battery questions with the same response options for all the items.",
        "The system supports explicit requests to repeat the question or ask for help, and mimics a ?standardized interviewing?",
        "style of interaction that trained interviewers would use to repeat or clarify a question when the answer is rejected or requires confirmation.",
        "Thresholds set on acoustic and language confidence scores are used to de",
        "tion strategy resulted in a 60% relative reduction in error compared to the first response.",
        "4.2 Impact of Errors on Survey Estimates Recognition error is undoubtedly a key factor in overall user experience.",
        "But unlike dialog systems for information access, search, and transac-tions, the most important factor in a survey dia-log system is the impact of errors on the quality of the estimates derived from the survey.",
        "To examine the impact of the residual 4.4% concept error on overall survey error, we compared answer distributions derived from the system hypothesis for the last response versus the annotation of the last response using paired t-tests.",
        "For the 18 categorical questions, we conducted t-tests comparing the counts for each response option of each question.",
        "For all 18 questions (a total of 77 response categories) none of the differences were statistically significant (p<0.05).",
        "For the 14 numerical questions, for only one (?Number of times shopping in a grocery store in the last month?)",
        "did the interpretations differ significantly (Annotated: 7.8 times, Hypothesis: 7.6 times, p=0.04).1 This is strong evidence that speech recognition errors in this system did not have a major effect on survey estimates.",
        "How much survey error would have occurred without the dialog strategy?",
        "To test this, we compared the annotated last response to the system hypothesis for the first response, simulating an interaction without confirmation dialog, and thus lower recognition accuracy?see Table 1 (This is not a perfect simulation, as we have no independent evidence on whether the first or final response is true).",
        "There would indeed have been more survey error without dialog, although the overall level was still surprisingly low.",
        "For the 18 categorical questions, 14 of the 77 response categories show significant differences (p<0.05).",
        "For the 14 numerical questions, two showed significant differences.",
        "4.3 User Satisfaction One of the post-interview questionnaire items provided a qualitative measure of user satisfac-tion: ?Overall, how satisfied were you with the interview??",
        "The results were: Very satisfied (47.3%), Somewhat satisfied (41.8%), Somewhat dissatisfied (7.1%), and Very dissatisfied (0.6%).",
        "We examined the impact of various dialog features that seemed on intuitive grounds plausibly 1 If we treat the two interpretations as independent samples, the response distributions did not differ significantly at all.",
        "connected with satisfaction: average number of turns per question, average number of clarification prompts per session, and average number of no input response prompts.",
        "We conducted a series of logistic regressions with one variable controlled at a time to see the extent to which each of these features affected satisfaction.",
        "A Chi-squared test was used to measure significance.",
        "All three features were significant predictors when comparing Somewhat/Very Dissatisfied to Very/Somewhat satisfied (Table 2).",
        "Feature Odds ratio SE p # turns per Q 10.411 0.787 0.003 # clarifications 1.043 0.033 0.024 # no input 2.001 0.176 <0.001 Table 2: User satisfaction regression 5 Conclusion Our results demonstrate the viability of conducting survey interviews of the sort from which important national statistics are derived with spoken dialog systems.",
        "In our system, the speech recognition errors (with an overall concept recognition rate of 95.6%) did not substantially affect the error of the survey estimates; for only one of 32 questions was there a significant difference in the survey estimate determined by the automated spoken dialog system compared to the annotated result.",
        "Of course, we don't know whether these results generalize to dialog systems with other features, different questions, or different re-spondents; much remains to be learned.",
        "Nonetheless, our results provide some guidance for improving respondent satisfaction and minimizing survey error in future development of survey dialog systems.",
        "For example, for numerical questions, which generally involve larger numbers of response options, recognition errors may be reduced by adopting the strategy of asking the respondent to select among categories representing ranges (e.g.",
        "?none?, ?1 to 5 times?, ?6 to 10 times?).",
        "Recognition performance could be improved by tuning confirmation strategies, e.g. applying a tighter confidence threshold for numerical vs. categorical questions.",
        "In a broad scale application of a repeated spoken dialog survey, greater amounts of data could be available for training statistical models for the respons-es, for improved recognition accuracy and further reduced concept error.",
        "Finally, it is worth exploring the trade-offs for survey error and respondent satisfaction between adding potentially frustrating confirmation dialog and accepting lower-confidence recognition for subsequent human annotation and processing."
      ]
    }
  ]
}
