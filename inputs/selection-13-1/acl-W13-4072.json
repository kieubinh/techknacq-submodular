{
  "info": {
    "authors": [
      "Daejoong Kim",
      "Jaedeug Choi Choi",
      "Kee-Eung Kim",
      "Jungsu Lee",
      "Jinho Sohn"
    ],
    "book": "SIGDIAL",
    "id": "acl-W13-4072",
    "title": "Engineering Statistical Dialog State Trackers: A Case Study on DSTC",
    "url": "https://aclweb.org/anthology/W13-4072",
    "year": 2013
  },
  "references": [
    "acl-P00-1013",
    "acl-W10-4306",
    "acl-W13-4065"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe our experience with engineering the dialog state tracker for the first Dialog State Tracking Challenge (DSTC).",
        "Dialog trackers are one of the essential components of dialog systems which are used to infer the true user goal from the speech processing results.",
        "We explain the main parts of our tracker: the observation model, the belief refinement model, and the belief transformation model.",
        "We also report experimental results on a number of approaches to the models, and compare the overall performance of our tracker to other submitted trackers.",
        "An extended version of this paper is available as a technical report (Kim et al., 2013)."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In spoken dialog systems (SDSs), one of the main challenges is to identify the user goal from her utterances.",
        "The significance of accurately identifying the user goal, referred to as dialog state tracking, has emerged from the need for SDSs to be robust to inevitable errors in the spoken language understanding (SLU).",
        "A number of studies have been conducted to track the dialog state through multiple dialog turns using a probabilistic framework, treating SLU results as noisy observations and maintaining probability distribution (i.e., belief) on user goals (Bohus and Rudnicky, 2006; Mehta et al., 2010; Roy et al., 2000; Williams and Young, 2007; Thomson and Young, 2010; Kim et al., 2011).",
        "In this paper, we share our experience and lessons learned from developing the dialog state tracker that participated in the first Dialog State Tracking Challenge (DSTC) (Williams et al., 2013).",
        "Our tracker is based on the belief update in the POMDP framework (Kaelbling et al., 1998), particularly the hidden information state (HIS) model (Young et al., 2010) and the partition recombination method (Williams, 2010)."
      ]
    },
    {
      "heading": "2 Dialog State Tracking",
      "text": [
        "Our tracker mainly follows the belief update in HIS-POMDP (Young et al., 2010).",
        "The SDS executes system action a, and the user with goal g responds to the system with utterance u.",
        "The SLU processes the utterance and generates the result as anN best list o = [?u?1, f1?, .",
        ".",
        ".",
        ", ?u?N , fN ?]",
        "of the hypothesized user utterance u?i and its associated confidence score fi.",
        "Because the SLU is not perfect, the system maintains a probability distribution over user goals, called a belief.",
        "In addition, the system groups user goals into equivalence classes and assigns a single probability for each equivalence class since the number of user goals is often too large to perform individual belief updates for all possible user goals.",
        "The equivalence classes are called partitions and denoted as ?.",
        "Hence, given the current belief b, system action a, and recognized N best list o, the dialog state tracker updates the belief b?",
        "over partitions as follows:",
        "where Pr(o|u) is the observation model, Pr(u|?, a) is the user utterance model, Pr(??|?)",
        "is the belief refinement model."
      ]
    },
    {
      "heading": "2.1 Observation Model",
      "text": [
        "The observation model Pr(o|u) is the probability that the SLU produces the N best list o when the user utterance is u.",
        "We experimented with the following three models for the observation model.",
        "Confidence score model: as in HIS-POMDP, this model assumes that the confidence score fi obtained from the SLU is exactly the probability",
        "of generating the hypothesized user utterance u?i.",
        "Hence, fi = Pr(u?i, fi|u).",
        "Histogrammodel: this model estimates a function that maps the confidence score to the probability of correctness.",
        "We constructed a histogram of confidence scores from the training datasets to obtain the empirical probability Pr(cor(fi)) of whether the entry associated with confidence score fi is a correct hypothesis or not.",
        "Generative model: this model is a simplified version of a generative model in (Williams, 2008) that only uses confidence score: Pr(u?i, fi|u) =",
        "probability of the i-th entry being a correct hypothesis and Pr(fi|cor(i)) is the probability of the i-th entry having confidence score fi when it is a correct hypothesis."
      ]
    },
    {
      "heading": "2.2 User Utterance Model",
      "text": [
        "The user utterance model Pr(u|?, a) indicates how the user responds to the system action a when the user goal is in ?.",
        "We adopted the HIS-POMDP user utterance model, consisting of a bi-gram model and an item model.",
        "The details are described in (Kim et al., 2013)."
      ]
    },
    {
      "heading": "2.3 Belief Refinement Model",
      "text": [
        "Given the SLU result u?i and the system action a, the partition ?",
        "is split into ?",
        "?i with probability Pr(??i|?)",
        "and ?",
        "?",
        "?",
        "?i with probability Pr(?",
        "?",
        "??i|?).",
        "The belief refinement model Pr(??i|?)",
        "can be seen as the proportion of the belief that is carried from ?",
        "to ??i.",
        "This probability can be defined by the following models: Empirical model: we count n(?)",
        "from the training datasets, which is the number of user goals that are consistent with partition ?.",
        "The probability is then modeled as Pr(??i|?)",
        "=",
        "Word-match model: this model extends the empirical model by using the domain knowledge when the SLU result u?i does not appear in the training datasets.",
        "We calculated how many words w ?",
        "W in the user utterance u?i were included in a bus timetable D. The model is thus defined as",
        "tor function (?",
        "(x) = 1 if x holds and ?",
        "(x) = 0 otherwise) and ?",
        "is the parameter estimated via cross-validation.",
        "Mixture model: this model mixes the empirical model with a uniform probability, defined as",
        "possible user goals which is treated as the parameter of the model and found via cross-validation, together with the mixing parameter ?",
        "?",
        "[0, 1]."
      ]
    },
    {
      "heading": "2.4 Belief Transformation Model",
      "text": [
        "The belief update described above produces the M best hypotheses of user goals [?g?1, b(g?1)?, .",
        ".",
        ".",
        ", ?g?M , b(g?M )?]",
        "in each dialog turn, which consists of M most likely user goal hypotheses g?i and their associated beliefs b(g?i).",
        "The last hypothesis g?M is reserved as the null hypothesis ?",
        "with the belief b(?)",
        "= 1 ?",
        "?M?1i=1 b(g?i), which represents that the user goal is not known up to the current dialog turn.",
        "One of the problems with the belief update is that the null hypothesis often remains as the most probable hypothesis even when the SLU result contains the correct user utterance with a high confidence score.",
        "This is because an atomic hypothesis has a very small prior probability.",
        "To overcome this problem, we added a post-processing step which transforms each belief b(hi) to the final confidence score si.",
        "Threshold model: this model ensures that the top hypothesis has confidence score ?",
        "when a belief of the hypothesis is greater than a threshold ?.",
        "The final output list is [?h?, s?",
        "?, ?",
        "?, 1?s??]",
        "where",
        "b(h?",
        "), otherwise.",
        "(2) Full-list regression model: this model estimates the probability that each hypothesis is correct via casting the task as regression.",
        "The model uses two logistic regression functions F?",
        "and Fh.",
        "F?",
        "predicts the probability of correctness for the null hypothesis ?",
        "using the single input feature ??",
        "= b(?).",
        "Likewise, Fh predicts the probability of correctness for non-null hypotheses hi using the input feature ?i = b(hi).",
        "The model generates the final output",
        "Rank regression model: this model works in a similar way as in the full-link regression model, except that it uses a single logistic regression function Fr for both the non-null and null hypotheses, and takes the rank value of the hypotheses as an additional input feature.",
        "The final output list is [?h1, s1?, .",
        ".",
        ".",
        ", ?hM?1, sM?1?, ?",
        "?, sM ?]",
        "where hi = g?i and"
      ]
    },
    {
      "heading": "3 Experimental Setup",
      "text": [
        "In the experiments, we used three labeled training datasets (train1a, train2, train3) and three test datasets (test1, test2, test3) used in DSTC.",
        "There was an additional test dataset (test4), which we decided not to include in the experiments since we found that a significant number of labels were missing or incorrect.",
        "We measured the tracker performance according to the following evaluation metrics used in DSTC1: accuracy (acc) measures the rate of the most likely hypothesis h1 being correct, average score (avgp) measures the average of scores assigned to the correct hypotheses, l2 norm measures the Euclidean distance between the vector of scores from the tracker and the binary vector with 1 in the position of the correct hypotheses, and 0 elsewhere, mean reciprocal rank (mrr) measures the average of 1/R, where R is the minimum rank of the correct hypothesis, ROC equal error rate (eer) is the sum of false accept",
        "(FA) and false reject (FR) rates when FA rate=FR rate, andROC.",
        "{v1,v2}.P measures correct accept (CA) rate when there are at most P% false accept (FA) rate2."
      ]
    },
    {
      "heading": "4 Results and Analyses",
      "text": [
        "Since there are multiple slots to track in the dialog domain, we report the average performance over the ?marginal?",
        "slots including the ?joint?",
        "slot that assigns the values to all slots."
      ]
    },
    {
      "heading": "4.1 Observation Model",
      "text": [
        "Tbl.",
        "1 shows the cross-validation results of the three observation models.",
        "In train1a and train2, no model had a clear advantage to others, whereas in",
        "train3, the confidence score model outperformed others.",
        "Further analyses revealed that the confidence scores from the SLU results were not sufficiently indicative of the SLU accuracy in train1a and train2.",
        "The histogram and the generative models are expected to perform at least as well as the confidence score model in train3, but they didn't in the experiments.",
        "We suspect that this is due to the naive binning strategy we used to model the probability distribution."
      ]
    },
    {
      "heading": "4.2 Belief Refinement Model",
      "text": [
        "As shown in Tbl.",
        "2, the mixture model outperformed others throughout the metrics.",
        "It even outperforms the word-match model which tries to leverage the domain knowledge to handle novel user goals.",
        "This implies that, unless the domain knowledge is used properly, simply taking the mixture with the uniform distribution yields a sufficient level of performance."
      ]
    },
    {
      "heading": "4.3 Belief Transformation Model",
      "text": [
        "Tbl.",
        "3 summarizes the performances of the belief transformation models.",
        "All three models outperformed the pure belief update, although not shown",
        "in the table.",
        "The full-list and the rank regression models show a similar level of performance improvement.",
        "This is a naturally expected result since they use regression to convert the beliefs to final confidence scores, as an attempt to compensate for the errors incurred by approximations and assumptions made in the observation and belief refinement models."
      ]
    },
    {
      "heading": "4.4 DSTC Result",
      "text": [
        "In order to compare our tracker with others participated in DSTC, we chose tracker43 as the most effective one among our 5 submitted trackers since it achieved the top scores in the largest number of evaluation metrics.",
        "In the same way, we selected tracker2 for team3, tracker3 for team6, tracker3 for team8, and tracker1 for the rest of the teams.",
        "The results of each team are presented in Tbl.",
        "4.",
        "The baseline tracker is included as a reference, which simply outputs the hypothesis with the largest SLU confidence score in the N best list.",
        "Compared to other teams, our tracker showed strong performance in acc, avgp, l2 and mrr.",
        "A detailed discussion on the results is provided in the longer version of the paper (Kim et al., 2013)."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we described our experience with engineering a statistical dialog state tracker while participating in DSTC.",
        "Our engineering effort was focused on improving three important models in the tracker: the observation, the belief refinement, and the belief transformation models.",
        "Using standard statistical techniques, we were able",
        "to produce a tracker that performed competitively among the participants.",
        "As for the future work, we plan to refine the user utterance model for improving the performance of the tracker since there are a number of user utterances that are not handled by the current model.",
        "We also plan to re-evaluate our tracker with properly handling the joint slot, since the current tracker constructs models independently for each marginal slot and then combines the results by simply multiplying the predicted scores."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": []
    }
  ]
}
