{
  "info": {
    "authors": [
      "Matthew Henderson",
      "Blaise Thomson",
      "Steve Young"
    ],
    "book": "SIGDIAL",
    "id": "acl-W13-4073",
    "title": "Deep Neural Network Approach for the Dialog State Tracking Challenge",
    "url": "https://aclweb.org/anthology/W13-4073",
    "year": 2013
  },
  "references": [
    "acl-W11-2002",
    "acl-W12-1812",
    "acl-W13-4065"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "While belief tracking is known to be important in allowing statistical dialog systems to manage dialogs in a highly robust manner, until recently little attention has been given to analysing the behaviour of belief tracking techniques.",
        "The Dialogue State Tracking Challenge has allowed for such an analysis, comparing multiple belief tracking approaches on a shared task.",
        "Recent success in using deep learning for speech research motivates the Deep Neural Network approach presented here.",
        "The model parameters can be learnt by directly maximising the likelihood of the training data.",
        "The paper explores some aspects of the training, and the resulting tracker is found to perform competitively, particularly on a corpus of dialogs from a system not found in the training."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Statistical dialog systems, in maintaining a distribution over multiple hypotheses of the true dialog state, are able to behave in a robust manner when faced with noisy conditions and ambiguity.",
        "Such systems rely on probabilistic tracking of dialog state, with improvements in the tracking quality being important in the system-wide performance in a dialog system (see e.g. Young et al. (2009)).",
        "This paper presents a Deep Neural Network (DNN) approach for dialog state tracking which has been evaluated in the context of the Dialog State Tracking Challenge (DSTC) (Williams, 2012a; Williams et al., 2013)1.",
        "Using Deep Neural Networks allows for the modelling of complex interactions between arbitrary features of the dialog.",
        "This paper shows improvements in using deep networks over networks",
        "with fewer hidden layers.",
        "Recent developments in speech research have shown promising results using deep learning, motivating its use in the context of dialog (Hinton et al., 2012; Li et al., 2013).",
        "This paper presents a technique which solves the task of outputting a sequence of probability distributions over an arbitrary number of possible values using a single neural network, by learning tied weights and using a form of sliding window.",
        "As the classification task is not split into multiple subtasks for a given slot, the log-likelihood of the tracker on training data can be directly maximised using gradient ascent techniques.",
        "The domain of the DSTC is bus route information in the city of Pittsburgh, but the presented technique is easily transferable to new domains, with the learned models in fact being domain independent.",
        "No domain specific knowledge is used, and the classifier learned does not require knowledge of the set of possible values.",
        "The tracker performed highly competitively in the ?test4?",
        "dataset, which consists of data from a dialog system not seen in training.",
        "This suggests the model is capable of capturing the important aspects of dialog in a robust manner without overtuning to the specifics of a particular system.",
        "Most attention in the dialog state belief tracking literature has been given to generative Bayesian network models (Paek and Horvitz, 2000; Thomson and Young, 2010).",
        "Few trackers have been published using discriminative classifiers, a notable exception being Bohus and Rudnicky (2006).",
        "An analysis by Williams (2012b) demonstrates how such generative models can in fact degrade belief tracking performance relative to a simple baseline.",
        "The successful use of discriminative models for belief tracking has recently been alluded to by Williams (2012a) and Li et al. (2013), and was a prominent theme in the results of the DSTC."
      ]
    },
    {
      "heading": "2 The Dialog State Tracking Challenge",
      "text": [
        "This section describes the domain and methodology of the Dialog State Tracking Challenge (DSTC).",
        "The Challenge uses data collected during the course of the Spoken Dialog Challenge (Black et al., 2011), in which participants implemented dialog systems to provide bus route information in the city of Pittsburgh.",
        "This provides a large corpus of real phonecalls from members of the public with real information needs.",
        "challenge.",
        "Labelled training sets provide labels for the caller's true goal in each dialog for 5 slots; route, from, to, date and time.",
        "Participants in the DSTC were asked to report the results of their tracker on the four test sets in the form of a probability distribution over each slot for each turn.",
        "Performance was determined using a basket of metrics designed to capture different aspects of tracker behaviour Williams et al. (2013).",
        "These are discussed further in Section 4.",
        "The DNN approach described here is referred to in the results of the DSTC as ?team1/entry1?."
      ]
    },
    {
      "heading": "3 Model",
      "text": [
        "For a given slot s at turn t in a dialog, let St, s denote the set of possible values for s which have occurred as hypotheses in the SLU for turns ?",
        "t. A tracker must report a probability distribution over St, s ?",
        "{other} representing its belief of the user's true goal for the slot s. The probability of ?other?",
        "represents the probability that the user's true goal is yet to appear as an SLU hypothesis.",
        "A neural network structure is defined which gives a discrete distribution over the |St, s|+1 values, taking the turns ?",
        "t as input.",
        "Figure 1 illustrates the structure used in this approach.",
        "Feature functions fi (t, v) for i = 1 .",
        ".",
        ".M",
        "vector f is a concatenation of all the input nodes.",
        "are defined which extract information about the value v from the SLU hypotheses and machine actions at turn t. A simple example would be fSLU (t, v), the SLU score that s=v was informed at turn t. A list of the feature functions actually used in the trial is given in Section 3.1.",
        "For notational convenience, feature functions at negative t are defined to be zero: ?i ?v, t?",
        "< 0?",
        "fi (t?, v) = 0.",
        "The input layer for a given value v is fixed in size by choosing a window size T , such that the feature functions are summed for turns ?",
        "t ?",
        "T .",
        "The input layer therefore consists of (T ?M) input nodes set to fi (t?, v) for t?",
        "= t?T+1 .",
        ".",
        ".",
        "t and",
        "A feed-forward structure of hidden layers is chosen, which reduces to a single node denoted E (t, v).",
        "Each hidden layer introduces a weight matrix Wi and a bias vector bi as parameters, which are independent of v but possibly trained separately for each s. The equations for each layer in the network are given in Figure 1.",
        "The final distribution from the tracker is:",
        "where B is a new parameter of the network, independent of v and possibly trained separately for each slot s."
      ]
    },
    {
      "heading": "3.1 Feature Functions",
      "text": [
        "As explained above, a feature function is a function f (t, v) which (for a given dialog) returns a",
        "real number representing some aspect of the turn t with respect to a possible value v. A turn consists of a machine action and the subsequent Spoken Language Understanding (SLU) results.",
        "The functions explored in this paper are listed below: 1.",
        "SLU score; the score assigned by the SLU to the user asserting s=v.",
        "2.",
        "Rank score; 1/r where r is the rank of s=v in the SLU n-best list, or 0 if it is not on the list.",
        "3.",
        "Affirm score; SLU score for an affirm action if the system just confirmed s=v.",
        "4.",
        "Negate score; as previous but with negate.",
        "5.",
        "Go back score; the score assigned by the SLU to a goback action matching s=v.",
        "6.",
        "Implicit score; 1?",
        "the score given in the SLU to a contradictory action if the system just implicitly confirmed s=v, otherwise 0.",
        "7.",
        "User act type; a feature function for each possible user act type, giving the total score of the user act type in the SLU.",
        "Independent of s & v. 8.",
        "Machine act type; a feature function for each possible machine act type, giving the total number of machine acts with the type in the turn.",
        "Independent of s & v. 9.",
        "Cant help; 1 if the system just said that it cannot provide information on s=v, otherwise 0.",
        "10.",
        "Slot confirmed; 1 if s=v?",
        "was just confirmed by the system for some v?, otherwise 0.",
        "11.",
        "Slot requested; 1 if the value of s was just requested by the system, otherwise 0.",
        "12.",
        "Slot informed; 1 if the system just gave information on a set of bus routes which included a specific value of s, otherwise 0."
      ]
    },
    {
      "heading": "4 Training",
      "text": [
        "The derivatives of the training data likelihood with respect to all the parameters of the model can be computed using back propagation, i.e. the chain rule.",
        "Stochastic Gradient Descent with mini-batches is used to optimise the parameters by descending the negative log-likelihood in the direction of the derivatives (Bottou, 1991).",
        "Termination is triggered when performance on a held-out development set stops improving.",
        "Each turn t and slot s in a dialog for which |St, s |> 0 provides a non-zero summand to the total log-likelihood of the training data.",
        "These instances may be split up by slot to train a separate network for each slot.",
        "Alternatively the data can be combined to learn a slot independent model.",
        "The best approach found was to train a slot independent model for a few epochs, and then switch to training one model per slot (see Section 4.4).",
        "This section presents experiments varying the training of the model.",
        "In each case the parameters are trained using all of the labelled training sets.",
        "The results are reported for test4 since this system is not found in the training data.",
        "They are therefore unbiassed and avoid overtuning problems.",
        "The ROC curves, accuracy, Mean Reciprocal Rank (MRR) and l2 norm of the tracker across all slots are reported here.",
        "(A full definition of the metrics is found in Williams et al. (2013).)",
        "These are computed throughout using statistics at every turn t where |St, s |> 0 (referred to as ?schedule 2?",
        "in the terminology of the challenge.)",
        "Table 2 and Figure 3 in Appendix A show these metrics.",
        "The ?Baseline?",
        "system (?team0/entry1?",
        "in the challenge), considers only the top SLU hypothesis so far, and assigns the SLU confidence score as the tracker probability.",
        "It does not therefore incorporate any belief tracking."
      ]
    },
    {
      "heading": "4.1 Window Size",
      "text": [
        "The window size, T , was varied from 2 to 20.",
        "T must be selected so that it is large enough to capture enough of the sequence of the dialog, whilst ensuring sufficient data to train the weights connecting the inputs from the earlier turns.",
        "The results suggest that T = 10 is a good compromise."
      ]
    },
    {
      "heading": "4.2 Feature Set",
      "text": [
        "The features enumerated in Section 3.1 were split into 4 sets.",
        "F1 = {1} includes only the SLU scores; F2 = {1, ..., 6} includes feature functions which depend on the user act and the value;",
        "chine act types; and finally F4 = {1, ..., 12} includes functions which depend on the system act and the value.",
        "The results clearly show that adding more and more features in this manner monotonically increases the performance of the tracker."
      ]
    },
    {
      "heading": "4.3 Structure",
      "text": [
        "Some candidate structures of the hidden layers (h1, h2, ...) were evaluated, including having no hidden layers at all, which gives a logistic regression model.",
        "In Table 2 the structure is represented as a list giving the size of each hidden layer in turn.",
        "Three layers in a funnelling [20, 10, 2] configuration is found to outperform the other structures.",
        "The l2 norm is highly affected by the use of deeper network structure, suggesting it is most useful in tweaking the calibration of the confidence scores.",
        "4.",
        "By default, we train using the shared initialisation training method with T = 10, all the features enumerated in Section 3.1, and 3 hidden layers of size 20, 10 and 2."
      ]
    },
    {
      "heading": "4.4 Initialisation",
      "text": [
        "The three methods of training alluded to in Section 4 were evaluated; training a model for each slot without sharing data between slots (Separate); training a single slot independent model (Single Model); and training for a few epochs a slot independent model, then using this to initialise the training of separate models (Shared Initialisation).",
        "The method of shared initialisation appears to be the most effective, scoring the best on accuracy, MRR and l2.",
        "Training in this manner is particularly beneficial for slots which are under represented in the training data, as it initiates the pa",
        "A DNN tracker was trained for entry in the DSTC.",
        "Training used T=10, the full feature set, a [20, 10, 2] hidden structure and the shared initialisation training method.",
        "Other parameters such as the learning rate and regularisation coefficient were tweaked by analysing performance on a held out subset of the training data.",
        "All the labelled",
        "for all slots.",
        "Boxplots show minimum, maximum, quartiles and the median.",
        "Dark dot is location of the entry presented in this paper (DNN system).",
        "training data available was used.",
        "The tracker is labelled as ?team1/entry1?",
        "in the DSTC.",
        "The DNN approach performed competitively in the challenge.",
        "Figure 2 summarises the performance of the approach relative to all 28 entries in the DSTC.",
        "The results are less competitive in test2 and test3 but very strong in test1 and test4.",
        "The performance in test4, dialogs with an unseen system, was probably the best because the chosen feature functions forced the learning of a general model which was not able to exploit the specifics of particular ASR+SLU configurations.",
        "Features which depend on the identity of the slot-values would have allowed better performance in test2 and test3, allowing the model to learn different behaviours for each value and learn typical confusions.",
        "It would also have been possible to exploit the system-specific data available in the challenge, such as more detailed confidence metrics from the ASR.",
        "For a full comparison across the entries in the DSTC, see Williams et al. (2013).",
        "In making comparisons it should be noted that this team did not alter the training for different test sets, and submitted only one entry."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "This paper has presented a discriminative approach for tracking the state of a dialog which takes advantage of deep learning.",
        "While simple Gradient Ascent training was tweaked in this paper using the ?Shared Initialisation?",
        "scheme, a possible promising future direction would be to further experiment with more recent methods for training deep structures e.g. initialising the networks layer by layer (Hinton et al., 2006).",
        "Richer feature representations of the dialog contribute strongly to the performance of the model.",
        "The feature set presented is applicable across a broad range of slot-filling dialog domains, suggesting the possibility of using the models across domains without domain-specific training data.",
        "x-axis and y-axis are false acceptance and true acceptance respectively.",
        "Lines are annotated as per Table 2."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors would like to thank the organisers of the DSTC.",
        "The principal author was funded by a studentship from the EPSRC."
      ]
    }
  ]
}
