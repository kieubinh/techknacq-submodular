{
  "info": {
    "authors": [
      "Wanyin Li",
      "Qin Lu",
      "Wenjie Li"
    ],
    "book": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-I05-3012",
    "title": "Integrating Collocation Features in Chinese Word Sense Disambiguation",
    "url": "https://aclweb.org/anthology/I05-3012",
    "year": 2005
  },
  "references": [
    "acl-C02-1097",
    "acl-C02-1143",
    "acl-C96-1005",
    "acl-H93-1051",
    "acl-J98-1004",
    "acl-J98-1005",
    "acl-J98-1006",
    "acl-P03-1058",
    "acl-P91-1019",
    "acl-P94-1013",
    "acl-P95-1026",
    "acl-P97-1007",
    "acl-P98-1037",
    "acl-W03-1302",
    "acl-W04-0847",
    "acl-W97-0201"
  ],
  "sections": [
    {
      "text": [
        "u.hk Through Time, or the Chinese word ^^ in Abstract(local government) and The selection of features is critical in providing discriminative information for classifiers in Word Sense Disambiguation (WSD).",
        "Uninformative features will degrade the performance of classifiers.",
        "Based on the strong evidence that an ambiguous word expresses a unique sense in a given collocation, this paper reports our experiments on automatic WSD using collocation as local features based on the corpus extracted from Peoples Daily News (PDN) as well as the standard SENSEVAL-3 data set.",
        "Using the Nave Bayes classifier as our core algorithm, we have implemented a classifier using a feature set combining both local collocation features and topical features.",
        "The average precision on the PDN corpus has 3.2% improvement compared to 81.5% of the baseline system where collocation features are not considered.",
        "For the SENSEVAL-3 data, we have reached the precision rate of 37.6% by integrating collocation features into contextual features, to achieve 37% improvement over 26.7% of precision in the baseline system.",
        "Our experiments have shown that collocation features can be used to reduce the size of human tagged corpus."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "WSD tries to resolve lexical ambiguity which refers to the fact that a word may have multiple meanings such as the word walk in Walk or Bike to school and BBC Education Walk ^^^(He is also partly right).",
        "WSD tries to automatically assign an appropriate sense to an occurrence of a word in a given context.",
        "Various approaches have been proposed to deal with the word sense disambiguation problem including rule-based approaches, knowledge or dictionary based approaches, corpus-based approaches, and hybrid approaches.",
        "Among these approaches, the supervised corpus-based approach had been applied and discussed by many researches ().",
        "According to [1], the corpus-based supervised machine learning methods are the most successful approaches to WSD where contextual features have been used mainly to distinguish ambiguous words in these methods.",
        "However, word occurrences in the context are too diverse to capture the right pattern, which means that the dimension of contextual words will be very large when all words in the training samples are used for WSD [14].",
        "Certain uninformative features will weaken the discriminative power of a classifier resulting in a lower precision rate.",
        "To narrow down the context, we propose to use collocations as contextual information as defined in Section 3.1.2.",
        "It is generally understood that the sense of an ambiguous word is unique in a given collocation [19].",
        "For example, ^^ means burden but not baggage when it appears in the collocation ( burden of thought).",
        "In this paper, we apply a classifier to combine the local features of collocations which contain the target word with other contextual features to discriminate the ambiguous words.",
        "The intuition is that when the target context captures a collocation, the influence of other dimensions of",
        "contextual words can be reduced or even ignored.",
        "For example, in the expression ^^^^",
        "(terrorists burned down the gene laboratory), the influence of contextual word ^^ (gene) should be reduced to work on the target word ^^ because ^^^^ is a collocation whereas ^^ and ^^ are not collocations even though they do co-occur.",
        "Our intention is not to generally replace contextual information by collocation only.",
        "Rather, we would like to use collocation as an additional feature in WSD.",
        "We still make use of other contextual features because of the following reasons.",
        "Firstly, contextual information is proven to be effective for WSD in the previous research works.",
        "Secondly, collocations may be independent on the training corpus and a sentence in consideration may not contain any collocation.",
        "Thirdly, to fix the tie case such as^^^^^^ ^^^^^^(^terrorists gene checking^), ^^^^ means ^human ^ when presented in the collocation ^^^^^^, but ^particle^ in the collocation ^^^^^ .",
        "The primary purpose of using collocation in WSD is to improve precision rate without any sacrifices in recall rate.",
        "We also want to investigate whether the use of collocation as an additional feature can reduce the size of hand tagged sense corpus.",
        "The rest of this paper is organized as follows.",
        "Section 2 summarizes the existing Word Sense Disambiguation techniques based on annotated corpora.",
        "Section 3 describes the classifier and the features in our proposed WSD approach.",
        "Section 4 describes the experiments and the analysis of our results.",
        "Section 5 is the conclusion."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Automating word sense disambiguation tasks based on annotated corpora have been proposed.",
        "Examples of supervised learning methods for WSD appear in , [7-8].",
        "The learning algorithms applied including: decision tree, decision-list [15], neural networks [7], nave Bayesian learning ([5],[11]) and maximum entropy [10].",
        "Among these leaning methods, the most important issue is what features will be used to construct the classifier.",
        "It is common in WSD to use contextual information that can be found in the neighborhood of the ambiguous word in training data ([6], ).",
        "It is generally true that when words are used in the same sense, they have similar context and co-occurrence information [13].",
        "It is also generally true that the nearby context words of an ambiguous word give more effective patterns and features values than those far from it [12].",
        "The existing methods consider features selection for context representation including both local and topic features where local features refer to the information pertained only to the given context and topical features are statistically obtained from a training corpus.",
        "Most of the recent works for English corpus including [7] and [8], which combine both local and topical information in order to improve their performance.",
        "An interesting study on feature selection for Chinese [10] has considered topical features as well as local collocational, syntactic, and semantic features using the maximum entropy model.",
        "In Dangs [10] work, collocational features refer to the local PoS information and bi-gram co-occurrences of words within 2 positions of the ambiguous word.",
        "A useful result from this work based on (about one million words) the tagged Peoples Daily News shows that adding more features from richer levels of linguistic information such as PoS tagging yielded no significant improvement (less than 1%) over using only the bi-gram co-occurrences information.",
        "Another similar study for Chinese [11] is based on the Naive Bayes classifier model which has taken into consideration PoS with position information and bi-gram templates in the local context.",
        "The system has a reported 60.40% in both precision and recall based on the SENSEVAL-3 Chinese training data.",
        "Even though in both approaches, statistically significant bi-gram co-occurrence information is used, they are not necessarily true collocations.",
        "For example, in the express ^^^^^^^^ , the bi-grams in their system are ( ,, ^^^^^, ^^^, ^^^^Some bi-grams such as ^^^^may have higher frequency but may introduce noise when considering it as features in disambiguating the sense human|^ and symbol|^^ like in the example case of ^^^^^^^.",
        "In our system, we do not rely on co-occurrence information.",
        "Instead, we utilize true collocation information (^^^, which fall in the window size of (-5, +5) as fea,^^^, )",
        "tures and the sense of human|^ can be decided clearly using this features.",
        "The collocation information is a preprepared collocation list obtained from a collocation extraction system and verified with syntactic and semantic methods ([21], [24]).",
        "Yarowsky [9] used the one sense per collocation property as an essential ingredient for an unsupervised Word-Sense Disambiguation algorithm to perform bootstrapping algorithm on a more general high-recall disambiguation.",
        "A few recent research works have begun to pay attention to collocation features on WSD.",
        "Domminic [19] used three different methods called bilingual method, collocation method and UMLS (Unified Medical Language System) relation based method to disambiguate unsupervised English and German medical documents.",
        "As expected, the collocation method achieved a good precision around 79% in English and 82% in German but a very low recall which is 3% in English and 1% in German.",
        "The low recall is due to the nature of UMLS where many collocations would almost never occur in natural text.",
        "To avoid this problem, we combine the contextual features in the target context with the preprepared collocations list to build our classifier."
      ]
    },
    {
      "heading": "3 The Classifier With Topical Contextual",
      "text": []
    },
    {
      "heading": "and Local Collocation Features 3.1 The Feature Set",
      "text": [
        "As stated early, an important issue is what features will be used to construct the classifier in WSD.",
        "Early researches have proven that using lexical statistical information, such as bi-gram co-occurrences was sufficient to produce close to the best results [10] for Chinese WSD.",
        "Instead of including bi-gram features as part of discrimination features, in our system, we consider both topical contextual features as well as local collocation features.",
        "These features are extracted form the 60MB human sense-tagged Peoples Daily News with segmentation information.",
        "Niu [11] proved in his experiments that Nave Bayes classifier achieved best disambiguation accuracy with small topical context window size (< 10 words).",
        "We follow their method and set the contextual window size as 10 in our system.",
        "Each of the Chinese words except the stop words inside the window range will be considered as one topical feature.",
        "Their frequencies are calculated over the entire corpus with respect to each sense of an ambiguous word w. The sense definitions are obtained from HowNet.",
        "We chose collocations as the local features.",
        "A collocation is a recurrent and conventional fixed expression of words which holds syntactic and semantic relations [21].",
        "Collocations can be classified as fully fixed collocations, fixed collocations, strong collocations and loose collocations.",
        "Fixed collocations means the appearance of one word implies the co-occurrence of another one such as ^^^^ (burden of history), while strong collocations allows very limited substitution of the components, for example, ^^^^ (local college), or ^^^ ^ (local university).",
        "The sense of ambiguous words can be uniquely determined in these two types of collocations, therefore are the collocations applied in our system.",
        "The sources of the collocations will be explained in Section 4.1.",
        "In both Niu [11] and Dangs [10] work, topical features as well as the so called collocational features were used.",
        "However, as discussed in Section 2, they both used bi-gram co-occurrences as the additional local features.",
        "However, bi-gram co-occurrences only indicate statistical significance which may not actually satisfy the conceptual definition of collocations.",
        "Thus instead of using co-occurrences of bi-grams, we take the true bi-gram collocations extracted from our system and use this data to compare with bi-gram co-occurrences to test the usefulness of collocation for WSD.",
        "The local features in our system make use of the collocations using the template (wi, w) within a window size of ten (where i = 5).",
        "For example, ^^ ^^^^^^^^^ (Government departments and local government commanded that) fits the bi-gram collocation template (w, w1) with the value of (^^^^).",
        "During the training and the testing processes, the counting of frequency value of the collocation feature will be increased by 1 if a collocation containing the ambiguous word occurs in a sentence.",
        "To have a good analysis on collocation features, we have also developed an algorithm using lonely adjacent bi-gram as locals features(named Sys89 adjacent bi-gram as locals features(named System A) and another using collocation as local features(named System B)."
      ]
    },
    {
      "heading": "3.2 The Collocation Classifier",
      "text": [
        "We consider all the features in the features set F = Ft vFl = {f1, f2, ... , fm } as independent, where Ft stands for the topical contextual features set, and Fl stands for the local collocation features set.",
        "For an ambiguous word w with n senses, let Sw = {ws1, ws2, ... , wsn } be the sense set.",
        "For the contextual features, we directly apply the Nave Bayes algorithm using Add-Lambda Smoothing to handle unknown words:",
        "f, eFt To integrate the local collocation feature fj Fl with respect to each sense wsi of w, we use the follows formula:",
        "where ^ is tuned from experiments (Section 4.5), score1(wsi) refers the score of the topical contextual features based on formula (1) and score2( wsi) refers the score of collocation features with respect to the sense wsj of w defined below.",
        "where ^(fj |wsj) = 1 for fj Fl if the collocation occurs in the local context.",
        "Otherwise this term is set as 0.",
        "Finally, we choose the right wsk so that"
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": [
        "We have designed a set of experiments to compare the classifier with and without the collocation features.",
        "In system A, the classifier is built with local bi-gram features and topical contextual features.",
        "The classifier in system B is constructed from combining the local collocation features with topical features."
      ]
    },
    {
      "heading": "4.1 Preparation the Data Set",
      "text": [
        "We have selected 20 ambiguous words from nouns and verbs with the sense number as 4 in average.",
        "The sense definition is taken from HowNet [22].",
        "To show the effect of the algorithm, we try to choose words with high degree of ambiguity, high frequency of use [23], and high frequency of constructing collocations.",
        "The selection of these 20 words is not completely random although within each criterion class we do try to pick word randomly.",
        "Based on the 20 words, we extracted 28,000 sentences from the 60 MB Peoples Daily News with segmentation information as our train-ing/test set which is then manually sense-tagged.",
        "The collocation list is constructed from a combination of a digital collocation dictionary, a return result from a collocation automatic extraction system [21], and a hand collection from the Peoples Daily News.",
        "As we stated early, the sense of ambiguous words in the fixed collocations and strong collocations can be decided uniquely although they are not unique in loose collocations.",
        "For example, the ambiguous word ^^ in the collocation ^^^^^ may have both the sense of appearance|^^ or reputation|^^.",
        "Therefore, when labeling the sense of collocations, we filter out the ones which cannot uniquely determine the sense of ambiguous words inside.",
        "However, this does not mean that loose collocations have no contribution in WSD classification.",
        "We simply reduce its weight when combining it with the contextual features compared with the fixed and strong collocations.",
        "The sense and collocation distribution over the 20 words on the training examples can be found in Table 1.",
        "T#: total number of sentences contain the ambiguous word s1- s6: sense no; co#: number of collocations in each sense"
      ]
    },
    {
      "heading": "4.2 The Effect of Collocation Features",
      "text": [
        "We recorded 6 trials with average precision over sixfold validation for each word.",
        "Their average precision for the six trials in the system A, and B can be found in Table 2 and Table 3.",
        "From Table 3, regarding to precision, there are 16 words have improved and 4 words remained the same in the system B.",
        "The results from the both system confirmed that collocation features do improve the precision.",
        "Note that 4 words have the same precision in the two systems, which fall into two cases.",
        "In the first case, it can be seen that these words already have very high precision in the system A (over 93%) which means that one sense dominates all other senses.",
        "In this case, the additional collation information is not necessary.",
        "In fact, when we checked the intermediate outputs, the score of the candidate senses of the ambiguous words contained in the collocations get improved.",
        "Even though, it would not change the result.",
        "Secondly, nocollo-cation appeared in the sentences whichare tagged incorrectly in the system A.",
        "This is confirmed when we check the error files.",
        "For example, the word ^^ with the sense as ^^ (closeness) appeared in 4492 examples over the total 4885 examples (91.9%).",
        "In the mean time, 99% of collocation in its collocation list has the same sense of ^ ^ (closeness).",
        "Only one collocation ^^^ has the sense of ^ ^ (power).",
        "Therefore, the collocation features improved the score of sense ^^ which is already the highest one based on the contextual features.",
        "As can be seen from Table 3, the collocation features work well for the sparse data.",
        "For example, the word ^^ in the training corpus has only one example with the sense ^ (human), the other 30 examples all have the sense ^^ (management).",
        "Under this situation, the topical contextual features failed to identify the right sense for the only appearance of the sense ^ (human) in the training instance ^^^^^^^^^^^^^^^....",
        "However, it can be correctly identified in the system B because the appearance of the collocation ^ .",
        "To well show the effect of collocations on the accuracy of classifier for the task of WSD, we also tested both systems on SENSEVAL-3 data set, and the result is recorded in the Table 4.",
        "From the difference in the relative improvement of both data sets, we can see that collocation features work well when the statistical model is not sufficiently built up such as from a small corpus like SENSEVAL-3.",
        "Actually, in this case, the training examples appear in the corpus only once or twice so that the parameters for such sparse training examples may not be accurate to forecast the test examples, which convinces us that collocation features are effective on handling sparse training data even for unknown words.",
        "Fig.",
        "1 shows the precision comparison in the system A, and B on SENVESAL-3."
      ]
    },
    {
      "heading": "4.3 The Effect of Collocations on the Size of Training Corpus Needed",
      "text": [
        "Hwee [21] stated that a large-scale, human sense-tagged corpus is critical for a supervised learning approach to achieve broad coverage and high accuracy WSD.",
        "He conducted a thorough study on the effect of training examples on the accuracy of supervised corpus based WSD.",
        "As the result showed, WSD accuracy continues to climb as the number of training examples increases.",
        "Similarly, we have tested the system A, and B with the different size of training corpus Total Average Precision0.840based on the PDN corpus we prepared.",
        "Our experiment results shown in Fig 2 follow the same",
        "Amb.",
        "Total Ave. Prec.",
        "in Ave. Prec.size of training corpus needed.",
        "From Fig 2, we"
      ]
    },
    {
      "heading": "WordSSys Ain Sys Bcan see by using the collocation features, the",
      "text": [
        "precision of the system B has increased slower with the growth of training examples than the precision of the system A.",
        "The result is reasonable because with collocation feature, the statistical contextual information over the entire corpus becomes side effect.",
        "Actually, as can be seen from Fig. 2, after using collocation features",
        "in the system B, even we use 1/6 corpus as training, the precision is still higher than we use 5/6 train corpus in the system A."
      ]
    },
    {
      "heading": "4.4 Investigation of Sense Distribution on",
      "text": [
        "the Effect of Collocation Features To investigate the sense distribution on the effect of collocation features, we selected the ambiguous words with the number of sense varied from 2 to 6.",
        "In each level of the sense number, the words are selected randomly.",
        "Table 5 shows the effect of sense distribution on the effect of collocation features.",
        "From the table, we can see that the collocation features work well when the sense distribution is even for a particular ambiguous word under which case the classifier may get confused.",
        "*: over 90% samples fall in onedo ^: Even distribution over all senses o: 83% to 86% samples fall in one We have conducted a set of experi m on both the PDN corpus and SENSEVLA to set the best value of a for the formula scribed in Section 3.2.",
        "The best start value o f a is tested based on the precision rate which is shown in Fig. 3.",
        "It is shown from the experiment that a takes the start value of 0.5 for both cor-puses."
      ]
    },
    {
      "heading": "4.5 The Test of a",
      "text": [
        "This paper reports a corpus-basedNavePeoplesbution over the entire corpus under which the statistic calculation prone to identify it incorrectly.",
        "In the same time, because disambiguating using collocation fea-s classifier is constructed by combining the contextual features with the bi-gram features, the new system achieves 3% precision improvement in average in Peoples Daily News corpus and 10% improvement in SENSEVAL-3 data set.",
        "Actually, it works very well when disambiguating the sense with sparse distr bution over the entire corpus under which the statistic calculation prone to identify it incorrectly.",
        "In the same time, because disambiguating using collocation fea-do e",
        "tures does not need statistical calculation, it makes contribution to reduce the size of human tagged corpus needed which is critical and time consuming in corpus based approach.",
        "Because different types of collocations may play different roles in classifying the sense of an ambiguous word, we hope to extend this work by integrating collocations with different weight based on their types in the future, which may need a preprocessing job to categorize the collocations automatically."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "We would like to present our thanks to the IR Laboratory in HIT University of China for sharing their sense number definition automatically extracted from HowNet with us."
      ]
    }
  ]
}
