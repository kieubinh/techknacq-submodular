{
  "info": {
    "authors": [
      "Ting Liu",
      "Wanxiang Che",
      "Sheng Li",
      "Yuxuan Hu",
      "Huaijun Liu"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W05-0627",
    "title": "Semantic Role Labeling System Using Maximum Entropy Classifier",
    "url": "https://aclweb.org/anthology/W05-0627",
    "year": 2005
  },
  "references": [
    "acl-C04-1179",
    "acl-J96-1002",
    "acl-W04-2412",
    "acl-W04-3212",
    "acl-W05-0620"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "A maximum entropy classifier is used in our semantic role labeling system, which takes syntactic constituents as the labeling units.",
        "The maximum entropy classifier is trained to identify and classify the predicates’ semantic arguments together.",
        "Only the constituents with the largest probability among embedding ones are kept.",
        "After predicting all arguments which have matching constituents in full parsing trees, a simple rule-based post-processing is applied to correct the arguments which have no matching constituents in these trees.",
        "Some useful features and their combinations are evaluated."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The semantic role labeling (SRL) is to assign syntactic constituents with semantic roles (arguments) of predicates (most frequently verbs) in sentences.",
        "A semantic role is the relationship that a syntactic constituent has with a predicate.",
        "Typical semantic arguments include Agent, Patient, Instrument, etc.",
        "and also adjunctive arguments indicating Locative, Temporal, Manner, Cause, etc.",
        "It can be used in lots of natural language processing application systems in which some kind of semantic interpretation is needed, such as question and answering, information extraction, machine translation, paraphrasing, and so on.",
        "*This research was supported by National Natural Science Foundation of China via grant 60435020 Last year, CoNLL-2004 hold a semantic role labeling shared task (Carreras and M�arquez, 2004) to test the participant systems’ performance based on shallow syntactic parser results.",
        "In 2005, SRL shared task is continued (Carreras and M�arquez, 2005), because it is a complex task and now it is far from desired performance.",
        "In our SRL system, we select maximum entropy (Berger et al., 1996) as a classifier to implement the semantic role labeling system.",
        "Different from the best classifier reported in literatures (Pradhan et al., 2005) – support vector machines (SVMs) (Vapnik, 1995), it is much easier for maximum entropy classifier to handle the multi-class classification problem without additional post-processing steps.",
        "The classifier is much faster than training SVMs classifiers.",
        "In addition, maximum entropy classifier can be tuned to minimize over-fitting by adjusting gaussian prior.",
        "Xue and Palmer (2004; 2005) and Kwon et al.",
        "(2004) have applied the maximum entropy classifier to semantic role labeling task successfully.",
        "In the following sections, we will describe our system and report our results on development and test sets."
      ]
    },
    {
      "heading": "2 System Description",
      "text": []
    },
    {
      "heading": "2.1 Constituent-by-Constituent",
      "text": [
        "We use syntactic constituent as the unit of labeling.",
        "However, it is impossible for each argument to find its matching constituent in all auto parsing trees.",
        "According to statistics, about 10% arguments have no matching constituents in the training set of 245,353",
        "constituents.",
        "The top five arguments with no matching constituents are shown in Table 1.",
        "Here, Char-niak parser got 10.08% no matching arguments and Collins parser got 11.89%.",
        "Therefore, we can see that Charniak parser got a better result than Collins parser in the task of SRL.",
        "So we use the full analysis results created by Charniak parser as our classifier’s inputs.",
        "Assume that we could label all AM-MOD and AM-NEG arguments correctly with simple post processing rules, the upper bound of performance could achieve about 95% recall.",
        "At the same time, we can see that for some arguments, both parsers got lots of no matchings such as AM-MOD, AM-NEG, and so on.",
        "After analyzing the training data, we can recognize that the performance of these arguments can improve a lot after using some simple post processing rules only, however other arguments’ no matching are caused primarily by parsing errors.",
        "The comparison between using and not using post processing rules is shown in Section 3.2.",
        "Because of the high speed and no affection in the number of classes with efficiency of maximum entropy classifier, we just use one stage to label all arguments of predicates.",
        "It means that the “NULL” tag of constituents is regarded as a class like “ArgN” and “ArgM”."
      ]
    },
    {
      "heading": "2.2 Features",
      "text": [
        "The following features, which we refer to as the basic features modified lightly from Pradhan et al.",
        "(2005), are provided in the shared task data for each constituent.",
        "• Predicate lemma • Path: The syntactic path through the parse tree from the parse constituent to the predicate.",
        "• Phrase type • Position: The position of the constituent with respect to its predicate.",
        "It has two values, “before” and “after”, for the predicate.",
        "For the situation of “cover”, we use a heuristic rule to ignore all of them because there is no chance for them to become an argument of the predicate.",
        "• Voice: Whether the predicate is realized as an active or passive construction.",
        "We use a simple rule to recognize passive voiced predicates which are labeled with part of speech – VBN and sequences with AUX.",
        "• Head word stem: The stemming result of the constituent’s syntactic head.",
        "A rule based stemming algorithm (Porter, 1980) is used.",
        "Collins Ph.D thesis (Collins, 1999)[Appendix.",
        "A] describs some rules to identify the head word of a constituent.",
        "Especially for prepositional phrase (PP) constituent, the normal head words are not very discriminative.",
        "So we use the last noun in the PP replacing the traditional head word.",
        "• Sub-categorization We also use the following additional features.",
        "• Predicate POS • Predicate suffix: The suf�x of the predicate.",
        "Here, we use the last 3 characters as the feature.",
        "• Named entity: The named entity’s type in the constituent if it ends with a named entity.",
        "There are four types: LOC, ORG, PER and MISC.",
        "• Path length: The length of the path between a constituent and its predicate.",
        "• Partial path: The part of the path from the constituent to the lowest common ancestor of the predicate and the constituent.",
        "• Clause layer: The number of clauses on the path between a constituent and its predicate.",
        "• Head word POS • Last word stem: The stemming result of the last word of the constituent.",
        "• Last word POS",
        "We also use some combinations of the above features to build some combinational features.",
        "Lots of combinational features which were supposed to contribute the SRL task of added one by one.",
        "At the same time, we removed ones which made the performance decrease in practical experiments.",
        "At last, we keep the following combinations:",
        "• Position + Voice • Path length + Clause layer • Predicate + Path • Path + Position + Voice • Path + Position + Voice + Predicate • Head word stem + Predicate • Head word stem + Predicate + Path • Head word stem + Phrase • Clause layer + Position + Predicate",
        "All of the features and their combinations are used without feature filtering strategy."
      ]
    },
    {
      "heading": "2.3 Classifier",
      "text": [
        "Le Zhang’s Maximum Entropy Modeling Toolkit 1, and the L-BFGS parameter estimation algorithm with gaussian prior smoothing (Chen and Rosenfeld, 1999) are used as the maximum entropy classifier.",
        "We set gaussian prior to be 2 and use 1,000 iterations in the toolkit to get an optimal result through some comparative experiments."
      ]
    },
    {
      "heading": "2.4 No Embedding",
      "text": [
        "The system described above might label two constituents even if one embeds in another, which is not allowed by the SRL rule.",
        "So we keep only one argument when more arguments embedding happens.",
        "Because it is easy for maximum entropy classifier to output each prediction’s probability, we can label the constituent which has the largest probability among the embedding ones."
      ]
    },
    {
      "heading": "2.5 Post Processing Stage",
      "text": [
        "After labeling the arguments which are matched with constituents exactly, we have to handle the arguments, such as AM-MOD, AM-NEG and AM-D I S, which have few matching with the constituents described in Section 2.1.",
        "So a post processing is given by using some simply rules:",
        "• Tag target verb and successive particles as V. • Tag “not” and “n’t” in target verb chunk as AM-NEG.",
        "• Tag modal verbs in target verb chunk, such as words with POS of “MD”, “going to”, and so on, as AM-MOD.",
        "• Tag the words with POS of “CC” and “RB” at the start of a clause which include the target verb as AM-D IS."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": []
    },
    {
      "heading": "3.1 Data and Evaluation Metrics",
      "text": [
        "The data provided for the shared task is a part of PropBank corpus.",
        "It consists of the sections from the Wall Street Journal part of Penn Treebank.",
        "Sections 02-21 are training sets, and Section 24 is development set.",
        "The results are evaluated for precision, recall and F,3=1 numbers using the srl-eval.pl script provided by the shared task organizers."
      ]
    },
    {
      "heading": "3.2 Post Processing",
      "text": [
        "After using post processing rules, the final F,3=1 is improved from 71.02% to 75.27%."
      ]
    },
    {
      "heading": "3.3 Performance Curve",
      "text": [
        "Because the training corpus is substantially enlarged, this allows us to test the scalability of learning-based SRL systems to large data set and compute learning curves to see how many data are necessary to train.",
        "We divide the training set, 20 sections Penn Treebank into 5 parts with 4 sections in each part.",
        "There are about 8,000 sentences in each part.",
        "Figure 1 shows the change of performance as a function of training set size.",
        "When all of training data are used, we get the best system performance as described in Section 3.4.",
        "sentences training set with 1,000 iterations and more than 1.5 million samples and 2 million features.",
        "The predicting time is about 160 seconds on 1,346- sentences development set."
      ]
    },
    {
      "heading": "4 Conclusions",
      "text": [
        "We have described a maximum entropy classifier is our semantic role labeling system, which takes syntactic constituents as the labeling units.",
        "The fast training speed of the maximum entropy classifier allows us just use one stage of arguments identification and classification to build the system.",
        "Some useful features and their combinations are evaluated.",
        "Only the constituents with the largest probability among embedding ones are kept.",
        "After predicting all arguments which have matching constituents in full parsing trees, a simple rule-based post-processing is applied to correct the arguments which have no matching constituents.",
        "The constituent-based method depends much on the syntactic parsing performance.",
        "The comparison between WSJ and Brown test sets results fully demonstrates the point of view."
      ]
    }
  ]
}
