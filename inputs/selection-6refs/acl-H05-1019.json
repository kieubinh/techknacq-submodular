{
  "info": {
    "authors": [
      "Tsutomu Hirao",
      "Manabu Okumura",
      "Hideki Isozaki"
    ],
    "book": "Human Language Technology Conference and Empirical Methods in Natural Language Processing",
    "id": "acl-H05-1019",
    "title": "Kernel-Based Approach for Automatic Evaluation of Natural Language Generation Technologies: Application to Automatic Summarization",
    "url": "https://aclweb.org/anthology/H05-1019",
    "year": 2005
  },
  "references": [
    "acl-C04-1064",
    "acl-C04-1077",
    "acl-N03-1020",
    "acl-P02-1040",
    "acl-P04-1077",
    "acl-P04-1078",
    "acl-W04-1003",
    "acl-W04-1013"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In order to promote the study of automatic summarization and translation, we need an accurate automatic evaluation method that is close to human evaluation.",
        "In this paper, we present an evaluation method that is based on convolution kernels that measure the similarities between texts considering their substructures.",
        "We conducted an experiment using automatic summarization evaluation data developed for Text Summarization Challenge 3 (TSC-3).",
        "A comparison with conventional techniques shows that our method correlates more closely with human evaluations and is more robust."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Automatic summarization, machine translation, and paraphrasing have attracted much attention recently.",
        "These tasks include text-to-text language generation.",
        "Evaluation workshops are held in the U.S. and Japan, e.g., the Document Understanding Conference (DUC)1, NIST Machine Translation Evaluation2 as part of the TIDES project, the Text Summarization Challenge (TSC)3 of the NTCIR project, and the International Workshop on Spoken Language Translation (IWSLT)4.",
        "These evaluation workshops employ human evaluations, which are essential in terms of achieving",
        "high quality evaluations results.",
        "However, human evaluations require a huge effort and the cost is considerable.",
        "Moreover, we cannot automatically evaluate a new system even if we use the corpora built for these workshops, and we cannot conduct re-evaluation experiments.",
        "To cope with this situation, there is a particular need to establish a high quality automatic evaluation method.",
        "Once this is done, we can expect great progress to be made on natural language generation.",
        "In this paper, we propose a novel automatic evaluation method for natural language generation technologies.",
        "Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001).",
        "ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations.",
        "We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a).",
        "The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes.",
        "One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004).",
        "The other is the N-gram based approach (Papineni et al.,",
        "2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004).",
        "Hori et.",
        "al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy.",
        "They reported that their method is superior to BLEU (Papineni et al., 2002) in terms of the correlation between human assessment and automatic evaluation.",
        "Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L.",
        "They applied ROUGE-L to the evaluation of summarization and machine translation.",
        "The results showed that the LCS-based measure is comparable to N-gram-based automatic evaluation methods.",
        "However, these methods tend to be strongly influenced by word order.",
        "Various N-gram-based methods have been proposed since BLEU, which is now widely used for the evaluation of machine translation.",
        "Lin et al.",
        "(2003) proposed a recall-oriented measure, ROUGE-N, whereas BLEU is precision-oriented.",
        "They reported that ROUGE-N performed well as regards automatic summarization.",
        "In particular, ROUGE-1, i.e., uni-gram matching, provides the best correlation with human evaluation.",
        "Soricut et.",
        "al (2004) proposed a unified measure.",
        "They integrated a precision-oriented measure with a recall-oriented measure by using an extension of the harmonic mean formula.",
        "It performs well in evaluations of machine translation, automatic summarization, and question answering.",
        "However, N-gram based methods have a critical problem; they cannot consider co-occurrences with gaps, although the LCS-based method can deal with them.",
        "Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation.",
        "However, they did not consider longer skip-n-grams such as skip-trigrams.",
        "Moreover, their method does not distinguish between bigrams and skip-bigrams."
      ]
    },
    {
      "heading": "3 Kernel-based Automatic Evaluation",
      "text": [
        "The above N-gram-based methods correlated closely with human evaluations.",
        "However, we think some skip-n-grams (n>3) are useful.",
        "In this paper, we employ the Extended String Subsequence Kernel (ESK), which considers both n-grams and skip-n-grams.",
        "In addition, the ESK allows us to add word senses to each word.",
        "The use of word senses enables flexible matching even when paraphrasing is used.",
        "The ESK is a kind of convolution kernel (Collins and Duffy, 2001).",
        "Convolution kernels have recently attracted attention as a novel similarity measure in natural language processing."
      ]
    },
    {
      "heading": "3.1 ESK",
      "text": [
        "The ESK is an extension of the String Subsequence Kernel (SSK) (Lodhi et al., 2002) and the Word Sequence Kernel (WSK) (Cancedda et al., 2003).",
        "The ESK receives two node sequences as inputs",
        "and maps each of them into a high-dimensional vector space.",
        "The kernel’s value is simply the inner product of the two vectors in the vector space.",
        "In order to discount long-skip-n-grams, the decay parameter A is introduced.",
        "We explain the computation of the ESK’s value whose inputs are the sentences (S 1 and S2) shown below.",
        "In the example, word senses are shown in braces.",
        "S1 Becoming a cosmonaut: {SPACEMAN} is my great dream: {DREAM} S2 Becoming an astronaut: {SPACEMAN} is my ambition: {DREAM} In this case, “cosmonaut” and “astronaut” share the same sense {SPACEMAN} and “ambition” and “dream” also share the same sense {DREAM}.",
        "We can use WordNet for English and Goitaikei (Ikehara et al., 1997) for Japanese.",
        "Table 1 shows the subsequences derived from S 1 and S2 and its weights.",
        "Note that the subsequence length is two or less.",
        "From the table, there are fifteen subsequences5 that are common to S1 and S2.",
        "unigrams, one bigram, zero trigrams and three skip-bigrams common to S 1 and S2.",
        "Formally, the ESK is defined as follows.",
        "T and U are node sequences.",
        "Here, d is the upper bound of the subsequence length and K'„ , (ti, uj) is defined as follows.",
        "tj is the i-th node of T. uj is the j-th node of U.",
        "The function val (s, t) returns the number of attributes common to given nodes s and t.",
        "Finally, we define the similarity measure between T and U by normalizing ESK.",
        "This similarity can be regarded as an extension of the cosine measure.",
        "Suppose, C is a system output, which consists of f sentences, and R is a human written reference, which consists of m sentences.",
        "ei is a sentence in C, and rj is a sentence in R. We define two scoring functions for automatic evaluation.",
        "First, we define a precision-oriented measure as follows:",
        "Finally, we define a unified measure, measure, as follows:",
        "13 is a cost parameter for R,Sk and PeSk.",
        ",3’s value is selected depending on the evaluation task.",
        "Since summary should not miss important information given in the human reference, recall is more important than precision.",
        "Therefore,a large 13 will yield good results."
      ]
    },
    {
      "heading": "3.3 Extension for Multiple References",
      "text": [
        "When multiple human references (correct answers) are available, we define a simple function for multiple references as follows:",
        "Here, equation (9) gives the average score.",
        "R indicates a set of references; R = {RI, • • • , R,,}."
      ]
    },
    {
      "heading": "4 Experimental Evaluation",
      "text": [
        "To confirm and discuss the effectiveness of our method, we conducted an experimental evaluation using TSC-3 multiple document summarization",
        "evaluation data and our additional data."
      ]
    },
    {
      "heading": "4.1 Task and Evaluation Metrics in TSC-3",
      "text": [
        "The task of TSC-3 is multiple document summarization.",
        "Participants were given a set of documents about a certain event and required to generate two different length summaries for the entire document set.",
        "The lengths were about 5% and 10% of the total number of characters in the document set, respectively.",
        "Thirty document sets were provided for the official run evaluation.",
        "There were ten participant systems; one provided by the TSC organizers as a baseline system.",
        "The evaluation metric follows DUC’s SEE evaluation scheme (Harman and Over, 2004).",
        "For each document set, one human subject makes a reference summary and uses it as a basis for evaluating ten system outputs.",
        "This human evaluation procedure consists of the following steps: Step 1 For each reference sentence rj (E R), repeat Steps 2 and 3.",
        "Step 2 For rj, the human assessor finds the most relevant sentence set S from the system output.",
        "Step 3 The assessor assigns a score, e(rj, S), 0, 0.",
        "1, • • • ,1.0.",
        "1.0 means perfect.",
        "in terms of how much of the content of rj can be reproduced by using only sentences in S. Step 4 Finally, the evaluation score of output C for reference R is defined H(R, C)= E; e(rj, S)/I RI .",
        "The final score of a system is calculated by applying the above procedure and normalized by the number of topics, i.e., Et– °1 H(Rt Ct)/30.",
        "When multiple references R(={Rl, • • • , R,1,1) are available, the scores are given as follows: Hmean(R, C)= Ek H(Rk, C)/I RI ."
      ]
    },
    {
      "heading": "4.2 Variation of Human Assessors",
      "text": [
        "In TSC-3’s official run evaluation, system outputs were compared with one human written reference summary for each topic.",
        "There were five topic sets and five human assessors (A-E in Table 2) for each topic set.",
        "Before we use the one human written reference summary as the gold-standard-reference, to examine variations among human assessors, we prepared two additional human summaries for each topic sets.",
        "Table 2: The relationship between topics and reference summary creators, i.e., human assessors.",
        "S(A) indicates a subject A’s evaluation score for all systems for corresponding topics.",
        "Therefore, we obtained three reference summaries and evaluation results for each topic sets (Table 2).",
        "Moreover, we prepared unified evaluation results of three human judgment as Davg, which is calculated as the average of three human scores.",
        "The relationship between topics and human assessors is shown in Table 2.",
        "For example, subject B generates summaries and evaluates all systems for topics 7-12, 13-18 and 25-30 on Dl, D2, and D3 respectively.",
        "Note that each human subject, A to E, was a retired professional journalist; that is, they shared a common background.",
        "Table 3 shows the Pearson’s correlation coefficient (r) and Spearman’s rank correlation coefficient p for the human subjects.",
        "The results show that every pair has a high correlation.",
        "Therefore, changing the human subject has little influence as regards creating references and evaluating system summaries.",
        "The evaluation by human subjects is stable.",
        "This result agrees with DUC’s additional evaluation results (Harman and Over, 2004).",
        "However, the behavior of the correlations between humans with different backgrounds is uncertain.",
        "The correlation might be fragile if we introduce a human subject whose background is different from the others."
      ]
    },
    {
      "heading": "4.3 Compared Automatic Evaluation Methods",
      "text": [
        "We compared our method with ROUGE-N and ROUGE-L described below.",
        "We used only content words to calculate the ROUGE scores because the correlation coefficient decreased if we did not remove functional words.",
        "WSK-based method We use WSK instead of ESK in equation (6)-(8).",
        "ROUGE-N ROUGE-N is an N-gram-based evaluation measure defined as follows (Lin, 2004b): Here, Count (gramN) is the number of an N-gram and Countmatch(gramN) denotes the number of n-gram co-occurrences in a system output and the reference."
      ]
    },
    {
      "heading": "ROUGES",
      "text": [
        "ROUGES is an extension of ROUGE-2 defined as follows (Lin, 2004b):",
        "Here, function Skip2 returns the number of skip-bi-grams that are common to R and C. ROUGE-SU ROUGE-SU is an extension of ROUGE-S, which includes unigrams as a feature defined as follows (Lin, 2004b):",
        "Here, function SU returns the number of skip-bi-grams and unigrams that are common to R and C. ROUGE-L ROUGE-L is an LCS-based evaluation measure defined as follows (Lin, 2004b):",
        "where Rles and PieS are defined as follows:",
        "Here, LCSu(r �C) is the LCS score of the union longest common subsequence between reference sentences ri and C. u and v are the number of words contained in R, and C, respectively.",
        "The multiple reference version of ROUGE-N S, SU or L, RNmean RSmean RSUmean mean can be defined in accordance with equation (9)."
      ]
    },
    {
      "heading": "4.4 Evaluation Measures",
      "text": [
        "We evaluate automatic evaluation methods by using Pearson’s correlation coefficient (r) and Spearman’s rank correlation coefficient (p).",
        "Since we have ten systems, we make a vector x=(x1, X2, • • • Xi,• • • X10) from the results of an automatic evaluation.",
        "Here, -� �=130 E:30t1 f (Rt, C,,t).",
        "Rt indicates a ref� erence for the t-th topic.",
        "f indicates an automatic evaluation function such as Fesk, Fu,sk, ROUGE-N, ROUGE-S, ROUGE-SU and ROUGE-L. Next, we make another vector y=(Y1, Y2, • • • , Y-i, • • • , Y10) from the human evaluation results.",
        "Here, ,� �=130 Et:30 1 H(Rt Gj t).",
        "Finally, we compute r and p between x and y6."
      ]
    },
    {
      "heading": "4.5 Evaluation Results and Discussions",
      "text": [
        "Table 4 shows the evaluation results obtained by using Pearson’s correlation coefficient r. Table 5 shows the evaluation results obtained with Spear-man’s rank correlation coefficient p. The ta-6When using multiple references, functions f and H for making vectors x and y are substituted for finean and Hmean respectively.",
        "bles show results obtained with and without stop word exclusion for the entire ROUGE family.",
        "For ROUGES and ROUGE-SU, we use three variations following (Lin, 2004b): the maximum skip distances are 4, 9 and infinity 7.",
        "In addition, we examine ) = 2and3 for the ESK-based and WSK-based methods.",
        "The decay parameter A for Fesk and Fu,sk is set at 0.5.",
        "We will discuss these parameter values in Section 4.6.",
        "From the tables, ROUGE-N’s r and p decrease monotonically with N when we exclude stop words.",
        "In most cases, the performance is improved by including stop words for N (>2).",
        "There is a large difference between ROUGE-1 and ROUGE-4.",
        "The ROUGES family is comparable to the ROUGE-SU family and their performance is close to ROUGE1 without stop words and ROUGE-2 with stop words.",
        "ROUGE-L is better than both ROUGE-3 and ROUGE-4 but worse than ROUGE-1 or ROUGE-2.",
        "On the other hand, FeSk’s correlation coefficients (r) do not change very much with respect to d. Even if d is set at 4, we can obtain good correlations.",
        "The behavior of rank correlation coefficients (p) is 7W e use ,3=1,2, and 3.",
        "However there are little difference among correlation coefficient regardless of ,3 because the number of the words in reference and the number of the words in system output are almost the same.",
        "similar to the above.",
        "The difference between the ROUGE family and our method is particularly large for long summaries.",
        "By setting d=2, our method gives the good results.",
        "The optimal ) is varied in the data sets.",
        "However, the difference between )= 2 and )=3 is small.",
        "For p, our method outperforms the ROUGE family except for Dl.",
        "By contrast, we can see d=3 or d=4 provided the best results.",
        "The differences between our method and the ROUGE family are larger than for r. For both r and p, when multiple references are available, our method outperforms the ROUGE family.",
        "Although ROUGE-1 sometimes provides better results than our method for short summaries, it has a critical problem; ROUGE-1 disregards word sequences making it easy to cheat.",
        "For instance, we can easily obtain a high ROUGE-1 score by using a sequence of high Inverse Document Frequency (IDF) words.",
        "Such a summary is incomprehensible and meaningless but we obtain a good ROUGE-1 score comparable to those of the top TSC-3 systems.",
        "By contrast, it is difficult to cheat other members of the ROUGE family or our method.",
        "Our evaluation results imply that FeSk is robust",
        "for d and length of summary and correlates closely with human evaluation results.",
        "Moreover, it includes no trivial way of obtaining a good score.",
        "These are significant advantages over ROUGE family.",
        "In addition, our method outperformed the WSK-based method in most cases.",
        "This result confirms the effectiveness of semantic information and the significant advantage of the ESK."
      ]
    },
    {
      "heading": "4.6 Effects of Parameters",
      "text": [
        "Our method has three parameters, d, A, and ,).",
        "In this section, we discuss the effects of these parameters.",
        "Figure 1 shows r and p for various A and ) values with respect to D,g.",
        "Note that we set d at 2 in the figure because the tendency is similar when we use other values, namely d(=3 or 4).",
        "From Fig. 1, we can see that ,)=1 is not good.",
        "With automatic summarization, ‘precision’ is not necessarily a good evaluation measure because highly redundant summaries may obtain a very high precision.",
        "On the other hand, ‘recall’ is not good when a system’s output is redundant.",
        "Therefore, equal treatment of ‘precision’ and ‘recall’ does not give a good evaluation measure.",
        "The figure shows that )=2,3 and 5 are good for r and )=3, 4, 5 and infinity are good for p. Moreover, we can see a significant differences between A = 1 and others from the figure.",
        "This implies an advantage of our method compared to ROUGES and ROUGE-SU, which cannot handle decay factor for skip-n-grams.",
        "From Fig. 1, we can see that p is more sensitive to 3 than r. Here, )=3,4,5 and infinity obtained the best results.",
        ")=1 was again the worst.",
        "This result indicates that we have to determine the parameter value properly for different tasks.",
        "A does not greatly affect the correlation for d=3, 4, 5 and infinity as regards the middle range.",
        "Table 6 show the best results when we examined all parameter combinations.",
        "In the brackets, we show the best settings of these parameter combinations.",
        "For r, d= 2 provides the best result and middle range A and )=2 or 3 are good in most cases.",
        "On the other hand, the best settings for p vary with",
        "the data set.",
        "d=2 is not always good for p. In short, we can see that the decay parameter for skips is significant and long skip-n-grams are effective especially p. These results show that our method has an advantage over the ROUGE family.",
        "In addition, our method is robust and sufficiently good even if close attention is not paid to the parameters."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we described an automatic evaluation method based on the ESK, which is a method for measuring the similarities between texts based on sequences of words and word senses.",
        "Our experiments showed that our method is comparable to ROUGE family for short summaries and outperforms it for long summaries.",
        "In order to prove that our method is language independent, we will conduct an experimental evaluation by using DUC’s evaluation data.",
        "We believe that our method will also be useful for other natural language generation tasks.",
        "We are now planning to apply our method to an evaluation of machine translation."
      ]
    }
  ]
}
