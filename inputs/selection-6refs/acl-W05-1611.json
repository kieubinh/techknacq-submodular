{
  "info": {
    "authors": [
      "Tomasz Marciniak",
      "Michael Strube"
    ],
    "book": "European Workshop on Natural Language Generation ENLG",
    "id": "acl-W05-1611",
    "title": "Discrete Optimization As an Alternative to Sequential Processing in NLG",
    "url": "https://aclweb.org/anthology/W05-1611",
    "year": 2005
  },
  "references": [
    "acl-N01-1002",
    "acl-P04-1051",
    "acl-P84-1107",
    "acl-W04-2401",
    "acl-W90-0109",
    "acl-W94-0319",
    "acl-W98-0315"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present an NLG system that uses Integer Linear Programming to integrate different decisions involved in the generation process.",
        "Our approach provides an alternative to pipeline-based sequential processing which has become prevalent in today’s NLG applications."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "From an engineering perspective, one of the major considerations in building a Natural Language Generation (NLG) system is the choice of the architecture.",
        "Two important issues that need to be considered at this stage are firstly, the modularization of the linguistic decisions involved in the generation process and secondly, the processingflow (cf. [De Smedt et al., 1996]).",
        "On one side of the spectrum lie integrated systems, with all linguistic decisions being handled within a single process (e.g. [Appelt, 1985]).",
        "Such architectures are theoretically attractive, as they assume a close coordination of different types of linguistic decisions, which are known to be dependent on one another (cf. e.g. [Danlos, 1984]).",
        "A major disadvantage of integrated models is the complexity that they necessarily involve, which results in poor portability and scalability.",
        "On the other side of the spectrum there are highly modularized pipeline architectures.",
        "A prominent example of this second case is the consensus pipeline architecture recognized by [Re-iter, 1994] and further elaborated in [Reiter and Dale, 2000].",
        "The modularization of Reiter’s model occurs at two levels.",
        "First, individual linguistic decisions of the same type (e.g. involving lexical or syntactic choice) are grouped together within single low level tasks, such as lexicalization, aggregation or ordering.",
        "Second, tasks are allocated to three high-level generation stages, i.e.",
        "Document Planning, Microplanning and Surface Realization.",
        "The processing flow in the pipeline architecture is sequential, with individual tasks being executed in a predetermined order.",
        "A study of applied NLG systems [Cahill and Reape, 1999] reveals, however, that while most applied NLG systems rely on sequential processing, they do not follow the strict modularization that the consensus model assumes.",
        "Low-level tasks are spread over various generation stages and may in fact be executed more than once at diverse positions in the pipeline.",
        "An attempt to account for commonalities that many NLG systems share, without imposing too many restrictions, as is the case with Reiter’s ”consensus” model, is the Reference Architecture for Generation Systems (RAGS) [Mellish et al., 2004].",
        "RAGS is an abstract specification of an NLG architecture that focuses on two issues: data types that the generation process manipulates and a generic model of the interactions between modules, based on a common central server.",
        "An important feature of RAGS is that it leaves the question of processing flow to the actual implementation.",
        "Hence it is theoretically possible to build both fully integrated as well as pipeline-based systems that would observe the RAGS principles.",
        "Two implementations of RAGS presented in [Mellish and Evans, 2004] demonstrate an intermediate way.",
        "In this paper we present a novel approach to building an integrated NLG system, in which the generation process is modeled as a discrete optimization problem.",
        "It provides an extension to the classification-based generation framework, presented in [Marciniak and Strube, 2004].",
        "We first assume modularization of the generation process at the lowest possible level: individual tasks correspond to realizations of single form elements (FEs) that build up a linguistic expression.",
        "The decisions that these tasks involve are then represented as classification tasks and integrated via an Integer Linear Programming (ILP) formulation (see e.g. [Nemhauser and Wolsey, 1999].",
        "This way we avoid the well known ordering problem that is present in all pipeline-based systems.",
        "Observing, at least partially, the methodological principles of RAGS, we specify the architecture of our system at two independent levels.",
        "At the abstract level, the low-level generation tasks are defined, all based on the same input/output interface.",
        "At the implementation level, the processing flow and integration method are determined.",
        "The rest of the paper is organized as follows: in Section 2 we present briefly the classification-based generation framework and remark on the shortcomings of pipeline-based processing.",
        "In Section 3 we introduce the ILP formulation of the generation task, and in Section 4 we report on the experiments and evaluation of the system."
      ]
    },
    {
      "heading": "2 Classification-Based Generation",
      "text": [
        "In informal terms, classification can be characterized as the task of assigning a class label to an unknown instance, given a set of its properties and attributes represented as a feature vec",
        "tor.",
        "In recent years supervised machine learning methods relying on pre-classified training data have been applied in various areas of NLP to solve tasks formulated as classification problems.",
        "In NLG machine learning methods have been used to solve single tasks such as content selection and ordering (e.g. [Duboue, 2004; Dimitromanolaki and Androutsopoulos, 2003]), lexicalization (e.g. [Reiter and Sripada, 2004]) and referring expressions generation (e.g. [Cheng et al., 2001 ]).",
        "In these applications classifiers trained on labeled data have proven more robust and efficient than approaches using explicit expert knowledge.",
        "The difficulty of formalizing the linguistic knowledge involved in the development of a knowledge-based system (a.k.a.",
        "knowledge-acquisition-bottleneck) has been replaced with an effort of obtaining the right kind of data, which typically involves annotating manually a corpus of relevant texts with the required linguistic information (cf. [Daelemans, 1993]).",
        "The classification-based generation framework that we introduced in [Marciniak and Strube, 2004] is based on a simple idea that the linguistic form of an expression can be decomposed into a set of discrete form elements (FEs) representing both its syntactic and lexical properties.",
        "The generation process is then modeled as a series of classification tasks that realize individual FEs.",
        "Realization of each FE is then regarded as a single low-level generation task."
      ]
    },
    {
      "heading": "2.1 Route Directions",
      "text": [
        "As the main application for this work we consider the task of generating natural language route directions.",
        "An example of such a text is given below: (a) Facing the Wildcat statue, (b) turn left on the brick sidewalk (c) and continue along the road to the Sports Complex.",
        "(d) Make a right onto Concord Road, (e) and keep going straight, (f) passing Presbyterian Church on your left, (g) until you reach Copeland Street.",
        "(h) The library building will be just around the corner on your right.",
        "We analyze the content of instructional texts of this kind in terms of temporally related situations, i.e. actions (b, c, d, e) states (a, h) and events (f, g), denoted by individual discourse units.",
        "The temporal structure of the texts is then modeled as a tree, with nodes representing individual situation descriptions and edges signaling the relations (see Figure 2).",
        "The semantics of each discourse unit is further represented as a feature vector describing the aspectual category and frame structure"
      ]
    },
    {
      "heading": "2.2 From LTAG to Form Elements",
      "text": [
        "To specify an inventory of FEs that would become objects of the low-level generation tasks, we first apply the Lexicalized Tree Adjoining Grammar (LTAG) formalism (see e.g. [Joshi and Schabes, 1991 ]) to model the linguistic form of the texts.",
        "In LTAG, the derivation of a linguistic structure starts with a selection of elementary trees, anchored by lexical items, such as verbs or prepositions at the clause level and discourse connectives at the discourse level (cf. [Webber and Joshi, 1998]).",
        "In the next step, elementary trees are put together by means of adjunction operations that follow the dependency structure provided by the derivation tree.",
        "We take the temporal structure from Figure 2 to constitute the discourse level derivation tree, with the temporal relationships corresponding to the syntactic dependencies.",
        "At the clause level, the derivation tree is isomorphic with the frame-based ontological representation of individual situations (see [Marciniak and Strube, 2005]).",
        "The clause and discourse-level derivation of discourse unit (c) from the above example in the context of (a) and (b) is depicted in Figure 1.",
        "At the clause level, the set of elementary trees includes one initial tree α1 anchored by the main verb, which also specifies the syntactic frame of the clause, and auxiliary trees 01 and 02 corresponding to the verb arguments.",
        "At the discourse level, the discourse unit which occupies the root position in the temporal structure (cf.",
        "Figure 2) of the profiled situation.",
        "This tree-based representation of the semantic content of route directions constitutes the input to the generation process.",
        "A detailed description of the underlying conceptual model and the annotation process is presented in [Marciniak and Strube, 2005].",
        "the road to the sport complex.",
        "is modeled as the initial tree a2, and auxiliary trees 03 and 04 represent the remaining discourse units.",
        "To model the whole process in a uniform way we encode the elementary trees as feature vectors, with individual features conveying syntactic (e.g. s exp) and lexical (e.g. verb lex) information.",
        "Features adj rank and adj dir denote respectively the ordering of the adjunction operations and the adjunction direction, which both determine the linear structure of the text.",
        "Hence the form of the whole discourse can be represented in terms of feature-value pairs used to encode the initial trees and the derivation process.",
        "On that basis we define a set of form elements building up a discourse as directly corresponding to the individual features.",
        "A detailed description of the FEs is given below:",
        "FE1: Adjunction Rank / Disc.",
        "Level specifies the linear rank of each discourse unit at the local level, i.e. only clauses temporally related to the same parent clause are considered.",
        "FE2: Adjunction Direction is concerned with the position of the child discourse unit relative to the parent one (e.g. (a) left of (b), (c) right of (b), etc.).",
        "FE3: Connective determines the lexical form of the discourse connective (e.g. null in (a), until in (g)).",
        "FE4: S Expansion specifies whether a given discourse unit is realized as a clause with the explicit subject (i.e. np+vp expansion of the root S node in a clause) (e.g. (g, h)) or not (e.g. (a), (b)).",
        "FE5: Verb Form denotes the form of the main verb in a clause (e.g. gerund in (a), (c), bare infinitive in (b), finite present in (g), etc.).",
        "FEg: Verb Lex.",
        "specifies the lexical form of the main verb (e.g. turn in (b), pass in (f) or reach in (g)).",
        "FE7: Phrase Type determines for each argument in a clause its syntactic realization as a noun phrase (NP) , prepositional phrase (PP) or a particle (P).",
        "FE8: Preposition Lex.",
        "is concerned with the choice of a lexical form for prepositions or particles in argument phrases (e.g. left and on in (b) or along and to in (c)).",
        "If the value of FE7 is NP, then this FE is set to none.",
        "FE9: Adjunction Rank / Phr.",
        "Level specifies the linear rank of each verb argument within a clause.",
        "As an example, consider the FEs-based representation of the form of clause (c) presented in Table 1.",
        "Realization of each FEi is represented as a classification task Ti, with a set of possible class labels corresponding to the different forms that FEi may take.",
        "Only tasks T1 and T9 associated respectively with Adjunction Rank/Disc.",
        "Level and Adjunction Rank / Phr.",
        "Level are split into a series of binary precedence classifications that determine the relative position of two discourse units or phrasal arguments at a time (e.g. (a) � (c), (c) � (d), and similarly along the road � to the sports complex etc.).",
        "These partial results are later combined to determine the rank of the respective constituents.",
        "Arguably, the above FEs and the corresponding tasks are independent of the underlying grammatical model.",
        "In this work we use the abstraction of the grammatical structure provided by LTAG, but the same or a similar set of FEs can be readily derived from other formalisms (cf. e.g. [Meteer, 1990]).",
        "The role of the grammatical theory in defining form elements is twofold.",
        "First, it specifies the exact position of individual FEs in the grammatical structure, making it clear how they should be assembled.",
        "Second, it ensures a wide coverage: although the linguistic structures that we consider here are relatively simple, the use of LTAG as the underlying grammatical formalism guarantees that our generation framework can be applied to producing much more complex constructions, both at the clause and discourse levels.",
        "Apparently, this would require a richer feature vector representation of the initial trees, and hence a larger number of FEs and the corresponding generation tasks.",
        "The basic principles of the generation process, however, would remain unchanged.",
        "Notice also that the tasks considered here can be grouped under the conventional NLG labels, such as text structuring (i.e. T1, T2), lexicalization (i.e. T3, Ts, T8) and sentence realization (i.e. T4, T5, T9).",
        "Yet another important NLG task, i.e. aggregation appears to be handled indirectly by T3 (e.g.",
        "Turn left.",
        "Continue along the road.",
        "vs.",
        "Turn left and continue along the road.)",
        "and T5 (e.g. Keep going straight.",
        "You will pass the Presbyterian Church on your right.",
        "vs. Keep going straight, passing the Presbyterian Church on your right.).",
        "We view it as the strength of our approach that regardless of their different linguistic character all these tasks are modeled in exactly the same way."
      ]
    },
    {
      "heading": "2.3 System Architecture and Sequential Processing",
      "text": [
        "At an abstract level, the architecture of our system consists of an unordered set of classifiers solving individual generation tasks.",
        "Each classifier is trained on a separate set of data obtained from the corpus of route directions annotated with both semantic and grammatical information.",
        "In the previous work [Marciniak and Strube, 2004] we followed the sequential paradigm advocated by [Daelemans and van den Bosch, 1998] and implemented the system as a cascade of classifiers.",
        "In such systems the output representation is built incrementally, with subsequent classifiers having access to the outputs of previous modules.",
        "An important characteristic of this model is its extensibility.",
        "Since classifiers rely on a uniform representation of the input (i.e. a feature vector) and the output (i.e. a single feature value), it is easy to change the ordering or insert new modules at any place in the pipeline.",
        "Both operations only require retraining classifiers with a new selection of the input features.",
        "A major problem that we faced was that we found no satisfactory method to determine the right ordering of individual classifiers that would guarantee optimal realization of the grammatical form of the generated expression.",
        "We found out that no matter what ordering we adopted tasks that were solved at the begining had a lower accuracy as the necessary contextual information, i.e. based on the outcomes from other tasks, was missing.",
        "At the same time, subsequent",
        "tasks were influenced by the initial decisions, which in some cases led to error propagation.",
        "Apparently, this was due to the well known fact that elements of the linguistic structure are strongly correlated with one another (see e.g. [Danlos, 1984]).",
        "Hence individual generation decisions should not be handled in isolation and arranging them in a fixed order will always involve a specific ordering bias.",
        "To get a feeling for the limitations that sequential processing of generation tasks involves, consider its graphical representation in Figure 3.",
        "The process corresponds to the best-first traversal of a weighted multi-layered lattice.",
        "Separate layers T,,..., Tom correspond to the individual tasks, and the nodes at each layer (li,, ..., lim.)",
        "represent class labels for each task1.",
        "In the sequential model only transitions between nodes belonging to subsequent layers are granted.",
        "Each such transition is augmented with a transition cost, which may be affected by the traversal history but does not consider the future choices.",
        "Nodes selected in this process represent the outcomes of individual tasks.",
        "As can be seen, the process is locally driven and it does not guarantee an optimal realization of the tasks.",
        "As an example consider three interrelated form elements: Connective, S Exp.",
        "and Verb Form and their different realizations presented in Table 2.",
        "Apparently each of these FEs has the potential to affect the overall meaning of the discourse unit or its stylistics.",
        "It can also be seen that only certain combinations of different forms are allowed in the given semantic context.",
        "Different realization of any of these FEs would require other elements to be changed accordingly.",
        "To conclude, following Danlos’ observation, we see no a priori reason to impose any fixed ordering on the respective generation tasks, and the experiments that we describe in Section 4 support this position."
      ]
    },
    {
      "heading": "3 Discrete Optimization Model",
      "text": [
        "As an alternative to sequential ordering of the generation tasks we consider the metric labeling problem formulated by [Kleinberg and Tardos, 2000], and originally applied in an",
        "image restoration application, where classifiers determine the ”true” intensity values of individual pixels.",
        "This task is formulated as a labeling function f : P → L which maps a set P of n objects onto a set L of m possible labels.",
        "The goal is to find an assignment that minimizes the overall cost function Q (f ) which has two components: assignment costs, i.e. the costs of selecting a particular label for individual objects, and separation costs, i.e. the costs of selecting a pair of labels for two related objects2.",
        "[Chekuri et al., 2001] proposed an integer linear programming (ILP) formulation of the metric labeling problem, with both assignment cost and separation costs being modeled as binary variables of the linear cost function.",
        "Recently, [Roth and Yih, 2004] applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them.",
        "The assignment costs were calculated on the basis of predictions of basic classifiers, i.e. trained for both tasks individually with no access to the outcomes of the other task.",
        "The separation costs were formulated in terms of binary constraints which specified whether a specific semantic role could occur in a given relation, or not.",
        "In the remainder of this paper, we present a more general model, which we apply to the generation tasks presented in Section 2.",
        "We put no limits on the number of tasks being solved, and express the separation costs as stochastic constraints, which can be calculated off-line from the available linguistic data."
      ]
    },
    {
      "heading": "3.1 ILP Formulation",
      "text": [
        "We consider a general context in which the generation process comprises a range of linguistic decisions modeled as a set of n classification tasks T = {T,,..., Tom} which potentially form mutually related pairs.",
        "Each task Ti consists in assigning a label from Li = {li,, ..., lim. }",
        "to an instance that represents the particular decision.",
        "Assignments are modeled as variables of a linear cost function.",
        "We differentiate between simple variables that model individual assignments of labels and compound variables that represent respective assignments for each pair of related tasks.",
        "To represent individual assignments the following procedure is applied: for each task Ti, every label from Li is asso",
        "ciated with a binary variable x(lij).",
        "Each such variable represents a binary choice, i.e. a respective label lij is selected if x(lij) = 1 or rejected otherwise.",
        "The coefficient of variable x(lij) which models the assignment cost c(lij) is given by:",
        "where p(lij) is the probability of lij being selected as the outcome of task Ti.",
        "The probability distribution for each task is provided by the basic classifiers that do not consider the outcomes of other tasks3.",
        "The role of compound variables is to provide pairwise constraints on the outcomes of individual tasks.",
        "Since we are interested in constraining only those tasks are that truly dependent on one another we first apply the contingency coefficient C to measure the degree of correlation for each pair of tasks4.",
        "In the case of tasks Ti and Tk which are significantly correlated, for each pair of labels from Li × Lk we build a single variable x(lij, lkp).",
        "Each such variable is associated with a coefficient representing the constraint on the respective pair of labels lij, lkp calculated in the following way:",
        "with p(lij,lkp) denoting the prior joint probability of labels lij and lkp in the data, which is independent from the general classification context and hence can be calculated off-line5.",
        "The ILP model consists of the target function and a set of constraints which block illegal assignments (e.g. only one label of the given task can be selected)6.",
        "In our case the target function is the cost function Q(f ), which we want to minimize:",
        "Constraints need to be formulated for both the simple and compound variables.",
        "First we want to ensure that exactly one label lij belonging to task Ti is selected, i.e. only one simple variable x(lij) representing labels of a given task can be set to 1:",
        "ables, and hence adequate for the type of tasks that we consider here.",
        "The coefficient takes values from 0 (no correlation) to 1 (complete correlation) and is calculated by the formula: C = (χ2/(N + χ2))1/2, where χ2 is the chi-squared statistic and N the total number of instances.",
        "The significance of C is then determined from the value of χ2 for the given data.",
        "See e.g. [Goodman and Kruskal, 1972].",
        "We also require that if two simple variables x(lij) and x(lkp), modeling respectively labels lij and lkp, are set to 1, then the compound variable x(lij, lkp), which models co-occurrence of these labels, is also set to 1.",
        "This is done in two steps: we first ensure that if x(lij) = 1, then exactly one variable x(lij, lkp) must also be set to 1:",
        "and do the same for variable x(lkp):",
        "Finally, we constrain the values of both simple and compound variables to be binary:",
        "We can represent the decision process that our ILP model involves as a graph, with the nodes corresponding to individual labels and the edges marking the associations between labels belonging to correlated tasks.",
        "In Figure 4, task T1 is correlated with task T2 and task T2 with task T71,.",
        "No correlation exists for pair T1, T71,.",
        "Both nodes and edges are augmented with costs.",
        "The goal is to select a subset of connected nodes, minimizing the overall cost, given that for each group of nodes T1, T2, ..., T71, exactly one node must be selected, and the selected nodes, representing correlated tasks, must be connected.",
        "We can see that in contrast to the pipeline approach (cf.",
        "Figure 1), no local decisions determine the overall assignment as the global distribution of costs is considered."
      ]
    },
    {
      "heading": "4 Experiments and Results",
      "text": [
        "In order to evaluate our approach we conducted a series of experiments with two implementations of the ILP model and two different pipelines.",
        "Each system takes as input the tree-based representation of the semantic content of route directions described in Section 2.",
        "The generation process traverses the temporal tree in a depth-first fashion, and for each node a single discourse unit is realized.",
        "Connective (horizontal) and Verb Form (vertical), computed for all discourse units in a corpus."
      ]
    },
    {
      "heading": "4.1 Correlations Between Tasks",
      "text": [
        "We started with running the correlation tests for all pairs of tasks.",
        "The obtained correlation network is presented in Figure 5.",
        "It is interesting to observe that tasks which realize FEs belonging to the same levels of linguistic organization, and have traditionally been handled within the same generation stages (i.e.",
        "Text Planning, Microplanning and Realization) are closely correlated with one another.",
        "This fact supports empirically some assumptions behind Reiter’s consensus model.",
        "On the other hand, there exist quite a few correlations that extend over the stage boundaries, and all three lexicalization tasks i.e. T3, T6 and T8 are correlated with many tasks of a totally different linguistic character."
      ]
    },
    {
      "heading": "4.2 ILP Systems",
      "text": [
        "We used the ILP model described in Section 3 to implement two generation systems.",
        "To obtain assignment costs, both systems get a probability distribution for each task from basic classifiers trained on the training data.",
        "To calculate the separation costs, modeling the stochastic constraints on the co-occurrence of labels, we considered correlated tasks only (cf.",
        "Figure 5) and applied two calculation methods, which resulted in two different system implementations.",
        "In ILP1, for each pair of tasks we computed the joint distribution of the respective labels considering all discourse units in the training data before the actual input was known.",
        "Such obtained joint distributions were used for generating all discourse units from the test data.",
        "An example matrix with joint distribution for selected labels of tasks Connective and Verb Form is given in Table 3.",
        "An advantage of this approach is that the computation can be done in an offline mode and has no impact on the runtime.",
        "In ILP2, the joint distribution for a pair of tasks was calculated at run-time, i.e. only after the actual input had been",
        "you see the river side in front ofyou, at Phi-threshold > 0.8. known.",
        "This time we did not consider all discourse units in the training data, but only those whose meaning, represented as a feature vector, was similar to the meaning of the input discourse unit.",
        "As a similarity metric we used the Phi coefficient7, and set the similarity threshold at 0.8.",
        "As can be seen from Table 4, the probability distribution computed in this way is better suited to the specific semantic context.",
        "This is especially important if the available corpus is small and the frequency of certain pairs of labels might be too low to have a significant impact on the final assignment."
      ]
    },
    {
      "heading": "4.3 Pipeline Systems",
      "text": [
        "As a baseline we implemented two pipeline systems.",
        "In the first one we used the ordering of tasks that resembles most closely the standard NLG pipeline and which we also used before in [Marciniak and Strube, 2004]8.",
        "Individual classifiers had access to both the semantic features, and the features output by the previous modules.",
        "To train the classifiers, the correct feature values were extracted from the training data and during testing the generated, and hence possibly erroneous, values were taken.",
        "In the other pipeline system we wanted to minimize the error-propagation effect and placed the tasks in the order of decreasing accuracy.",
        "To determine the ordering of tasks we applied the following procedure: the classifier with the highest baseline accuracy was selected as the first one.",
        "The remaining classifiers were trained and tested again, but this time they had access to the additional feature.",
        "Again, the classifier with the highest accuracy was selected and the procedure was repeated until all classifiers were ordered."
      ]
    },
    {
      "heading": "4.4 Evaluation",
      "text": [
        "We evaluated our system using leave-one-out cross-validation, i.e. for all texts in the corpus, each text was used once for testing, and the remaining texts provided the training data.",
        "To solve individual classification tasks we used the decision tree learner C4.5 in the pipeline systems and the Naive Bayes algorithm9 in the ILP systems.",
        "Both learning schemes",
        "yielded highest results in the respective configurations10.",
        "To solve the ILP models we used lp solve, a highly efficient GNU-licence Mixed Integer Programming (MIP) solver11, that implements the Branch-and-Bound algorithm.",
        "For each task we applied afeature selection procedure (cf. [Kohavi and John, 1997]) to determine which semantic features should be taken as the input by the basic classifiers.",
        "To evaluate individual tasks we applied two metrics: accuracy, calculated as the proportion of correct classifications to the total number of instances, and the rc statistic, which corrects for the proportion of classifications that might occur by chance.",
        "For end-to-end evaluation, we applied the Phi coefficient to measure the degree of similarity between the vector representations of the generated form (i.e. built from the outcomes of individual tasks) and the reference form obtained from the test data.",
        "The Phi-based similarity metric is similar to r. as it compensates for the fact that a match between two multi-label features is more difficult to obtain than in the case of binary features.",
        "This measure tells us how well all the tasks have been solved together, which in our case amounts to generating the whole text.",
        "The results presented in Table 5 show that the ILP systems achieved highest accuracy and r. for most tasks and reached the highest overall Phi score.",
        "Notice that ILP2 improved the accuracy of both pipeline systems for the three correlated tasks that we discussed before, i.e. Connective, S Exp.",
        "and Verb Form.",
        "Another group of correlated tasks for which the results appear interesting are i.e. Verb Lex., Phrase Type and Phrase Rank (cf.",
        "Figure 3).",
        "Notice that Verb Lex.",
        "got higher scores in Pipeline2, with outputs from both Phrase Type and Phrase Rank (see the respective pipeline positions), but the reverse effect did not occur: scores for both phrase tasks were lower in Pipeline1 when they had access to the output from Verb Lex., contrary to what we might expect.",
        "Apparently, this was due to the low accuracy for Verb Lex.",
        "which caused the",
        "already mentioned error propagation.",
        "This example shows well the advantage that optimization processing brings: both ILP systems reached much higher scores for all three tasks.",
        "Finally, it appears as no coincidence that the three tasks involving lexical choice, i.e. Connective, Verb Lex.",
        "and Preposition Lex.",
        "scored lower than the syntactic tasks in all systems.",
        "This can be attributed partially to the limitations of retrieval measures which do not allow for the fact, that in a given semantic content more than one lexical form can be appropriate."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "In this paper we showed that the pipeline architecture in an NLG application can be successfully replaced with an integrated ILP-based model which is better suited to handling correlated generation decisions.",
        "To the best of our knowledge, linear programming has been used in an NLG related work only by [Althaus et al., 2004] to solve a single task of determining the order of discourse constituents.",
        "In a somewhat related context [Dras, 1999] used ILP to optimize the task of text paraphrasing, given global constraints such as text and sentence length, readibilty, etc.",
        "In contrast, in this work we use an ILP model to organize the entire process of generating the surface form from an underlying semantic representation, which involves an integration of different types of NLG tasks.",
        "Although in our system we use machine learning as the primary decision making mechanism, we believe that the ILP model can also be used with knowledge-based systems that observe the classification-oriented formulation of the NLG tasks.",
        "Finally, we are convinced that an adequate evaluation of an NLG system must at some stage go beyond the application of quantitative measures.",
        "Nevertheless, it is reasonable to expect that the improvement that we reached with the ILP system, especially the increase of the overall Phi score, must correlate to some extent with the quality improvement.",
        "To verify it we are currently proceeding with qualitative evaluation of the output from our system.",
        "Acknowledgements: The work presented here has been funded by the Klaus Tschira Foundation, Heidelberg, Germany.",
        "The first author receives a scholarship from KTF (09.001.2004)."
      ]
    }
  ]
}
