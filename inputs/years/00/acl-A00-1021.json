{
  "info": {
    "authors": [
      "Dragomir R. Radev",
      "John Prager",
      "Valerie Samn"
    ],
    "book": "Applied Natural Language Processing Conference and Meeting of the North American Association for Computational Linguistics",
    "id": "acl-A00-1021",
    "title": "Ranking Suspected Answers to Natural Language Questions Using Predictive Annotation",
    "url": "https://aclweb.org/anthology/A00-1021",
    "year": 2000
  },
  "references": [
    "acl-A97-1030",
    "acl-A97-1033"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe a system to rank suspected answers to natural language questions.",
        "We process both corpus and query using a new technique, predictive annotation, which augments phrases in texts with labels anticipating their being targets of certain kinds of questions.",
        "Given a natural language question, our IR system returns a set of matching passages, which we then rank using a linear function of seven predictor variables.",
        "We provide an evaluation of the techniques based on results from the TREC Q&A evaluation in which our system participated."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Question Answering is a task that calls for a combination of techniques from Information Retrieval and Natural Language Processing.",
        "The former has the advantage of years of development of efficient techniques for indexing and searching large collections of data, but lacks of any meaningful treatment of the semantics of the query or the texts indexed.",
        "NLP tackles the semantics, but tends to be computationally expensive.",
        "We have attempted to carve out a middle ground, whereby we use a modified IR system augmented by shallow NL parsing.",
        "Our approach was motivated by the following problem with traditional IR systems.",
        "Suppose the user asks \"Where did <some event> happen?\"",
        ".",
        "If the system does no preprocessing of the query, then \"where\" will be included in the bag of words submitted to the search engine, but this will not be helpful since the target text will be unlikely to contain the word \"where\" .",
        "If the word is stripped out as a stop-word, then • The work presented in this paper was performed while the first and third authors were at IBM Research.",
        "the search engine will have no idea that a location is sought.",
        "Our approach, called predictive annotation, is to augment the query with semantic category markers (which we call QA-Tokens), in this case with the PLACE$ token, and also to label with QA-Tokens all occurrences in text that are recognized entities, (for example, places).",
        "Then traditional bag-of-words matching proceeds successfully, and will return matching passages.",
        "The answer-selection process then looks for and ranks in these passages occurrences of phrases containing the particular QA-Token(s) from the augmented query.",
        "This classification of questions is conceptually similar to the query expansion in (Voorhees, 1994) but is expected to achieve much better performance since potentially matching phrases in text are classified in a similar and synergistic way.",
        "Our system participated in the official TREC Q&A evaluation.",
        "For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus.",
        "The results are shown in Section 7.",
        "Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., 1999).",
        "Some earlier systems related to our work are FaqFinder (Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text."
      ]
    },
    {
      "heading": "2 System description",
      "text": [
        "Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection compo",
        "nent (AnSel/Werlect) that extracts and ranks potential answers from these texts.",
        "This paper focuses on the process of ranking potential answers selected by the IR engine, which is itself described in (Prager et al., 1999)."
      ]
    },
    {
      "heading": "2.1 The Information Retrieval component",
      "text": [
        "In the context of fact-seeking questions, we made the following observations:",
        "• In documents that contain the answers, the query terms tend to occur in close proximity to each other.",
        "• The answers to fact-seeking questions are usually phrases: \"President Clinton\", \"in the Rocky Mountains\", and \"today\").",
        "• These phrases can be categorized by a set of a dozen or so labels (Figure 2) corresponding to question types.",
        "• The phrases can be identified in text by pattern matching techniques (without full NLP).",
        "As a result, we defined a set of about 20 categories, each labeled with its own QA-Token, and built an IR system which deviates from the traditional model in three important aspects.",
        "• We process the query against a set of approximately 200 question templates which, may replace some of the query words with a set of QA-Tokens, called a SYN-class.",
        "Thus \"Where\" gets mapped",
        "to \"PLACE$\", but \"How long \" goes to \"@SYN(LENGTHS, DURATIONS)\".",
        "Some templates do not cause complete replacement of the matched string.",
        "For example, the pattern \"What is the population\" gets replaced by \"NUMBER$ population\".",
        "• Before indexing the text, we process it with Textract (Byrd and Ravin, 1998; Wacholder et al., 1997), which performs lemmatization, and discovers proper names and technical terms.",
        "We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching.",
        "Thus the text \"for 5 centuries\" matches the DURATION$ pattern \"for :CARDINAL _timeperiod\", where :CARDINAL is the label for cardinal numbers, and _timeperiod marks a time expression.",
        "• GuruQA scores text passages instead of documents.",
        "We use a simple document-and collection-independent weighting scheme: QA-Tokens get a weight of 400, proper nouns get 200 and any other word - 100 (stop words are removed in query processing after the pattern template matching operation).",
        "The density of matching query tokens within a passage is contributes a score of 1 to 99 (the highest scores occur when all matched terms are consecutive).",
        "Predictive Annotation works best for Where, When, What, Which and How+adjective questions than for How+verb and Why questions, since the latter are typically not answered by phrases.",
        "However, we observed that \"by\" + the present participle would usually indicate the description of a procedure, so we instantiate a METHOD$ QA-Token for such occurrences.",
        "We have no such QA-Token for Why questions, but we do replace the word \"why\" with \"OSYN(result, cause, because)\", since the occurrence of any of these words usually betokens an explanation."
      ]
    },
    {
      "heading": "3 Answer selection",
      "text": [
        "So far, we have described how we retrieve relevant passages that may contain the answer to a query.",
        "The output of GuruQA is a list of 10 short passages containing altogether a large",
        "number (often more than 30 or 40) of potential answers in the form of phrases annotated with QA-Tokens."
      ]
    },
    {
      "heading": "3.1 Answer ranking",
      "text": [
        "We now describe two algorithms, AnSel and Werlect, which rank the spans returned by GuruQA.",
        "AnSel and Werlectl use different approaches, which we describe, evaluate and compare and contrast.",
        "The output of either system consists of five text extracts per question that contain the likeliest answers to the questions."
      ]
    },
    {
      "heading": "3.2 Sample Input to AnSel/Werlect",
      "text": [
        "The role of answer selection is to decide which among the spans extracted by GuruQA are most likely to contain the precise answer to the questions.",
        "Figure 3 contains an example of the data structure passed from GuruQA to our answer selection module.",
        "The input consists of four items:",
        "• a query (marked with <QUERY> tokens in the example), • a list of 10 passages (one of which is shown above), • a list of annotated text spans within the passages, annotated with QA-Tokens, and 'from ANswer SELect and ansWER seLECT, respectively • the SYN-class corresponding to the type of question (e.g., \"PERSON$ NAME$\").",
        "The text in Figure 3 contains five spans (potential answers), of which three (\"Biography of Margaret Thatcher\", \"Hugo Young\", and \"Margaret Thatcher\") are of types included in the SYN-class for the question (PERSON NAME).",
        "The full output of GuruQA for this question includes a total of 14 potential spans (5 PERSONs and 9 NAMEs)."
      ]
    },
    {
      "heading": "3.3 Sample Output of AnSel/Werlect",
      "text": [
        "The answer selection module has two outputs: internal (phrase) and external (text passage).",
        "Internal output: The internal output is a ranked list of spans as shown in Table 1.",
        "It represents a ranked list of the spans (potential answers) sent by GuruQA.",
        "External output: The external output is a ranked list of 50-byte and 250-byte extracts.",
        "These extracts are selected in a way to cover the highest-ranked spans in the list of potential answers.",
        "Examples are given later in the paper.",
        "The external output was required for the TREC evaluation while system's internal output can be used in a variety of applications, e.g., to highlight the actual span that we believe is the answer to the question within the context of the passage in which it appears.",
        "In this section we describe the corpora used for training and evaluation as well as the questions contained in the training and evaluation question sets."
      ]
    },
    {
      "heading": "4.1 Corpus analysis",
      "text": [
        "For both training and evaluation, we used the TREC corpus, consisting of approximately 2 GB of articles from four news agencies."
      ]
    },
    {
      "heading": "4.2 Training set TR38",
      "text": [
        "To train our system, we used 38 questions (see Figure 4) for which the answers were provided by NIST."
      ]
    },
    {
      "heading": "4.3 Test set T200",
      "text": [
        "The majority of the 200 questions (see Figure 5) in the evaluation set (T200) were not substan",
        "tially different from these in TR38, although the introduction of \"why\" and \"how\" questions as well as the wording of questions in the format \"Name X\" made the task slightly harder.",
        "Some examples of problematic questions are shown in Figure 6.",
        "Q: Why did David Koresh ask the FBI for a word processor?",
        "Q: Name the first private citizen to fly in space.",
        "Q: What is considered the costliest disaster the insurance industry has ever faced?",
        "Q: What did John Hinckley do to impress Jodie Foster?",
        "Q: How did Socrates die?"
      ]
    },
    {
      "heading": "5 AnSel",
      "text": [
        "AnSel uses an optimization algorithm with 7 predictive variables to describe how likely a given span is to be the correct answer to a question.",
        "The variables are illustrated with examples related to the sample question number 10001 from TR38 \"Who was Johnny Mathis' high school track coach?\".",
        "The potential answers (extracted by GuruQA) are shown in Table 2."
      ]
    },
    {
      "heading": "5.1 Feature selection",
      "text": [
        "The seven span features described below were found to correlate with the correct answers.",
        "Number: position of the span among all spans returned from the hit-list.",
        "Rspanno: position of the span among all spans returned within the current passage.",
        "Count: number of spans of any span class retrieved within the current passage.",
        "Noting: the number of words in the span that do not appear in the query.",
        "Type: the position of the span type in the list of potential span types.",
        "Example: Type (\"Lou Vasquez\") = 1, because the span type of \"Lou Vasquez\", namely \"PERSON\" appears first in the SYN-class \"PERSON ORG NAME ROLE\".",
        "Avgdst: the average distance in words between the beginning of the span and query words that also appear in the passage.",
        "Example: given the passage \"Tim O'Donohue, Wood-bridge High School's varsity baseball coach, resigned Monday and will be replaced by assistant Johnny Ceballos, Athletic Director Dave Cowen said.\"",
        "and the span \"Tim O'Donohue\" , the value of avgdst is equal to 8.",
        "Sscore: passage relevance as computed by GuruQA.",
        "Number: the position of the span among all retrieved spans."
      ]
    },
    {
      "heading": "5.2 AnSel algorithm",
      "text": [
        "The TOTAL score for a given potential answer is computed as a linear combination of the features described in the previous subsection:",
        "The algorithm that the training component of AnSel uses to learn the weights used in the formula is shown in Figure 7.",
        "At runtime, the weights are used to rank potential answers.",
        "Each span is assigned a TOTAL score and the top 5 distinct extracts of 50 (or 250) bytes centered around the span are output.",
        "The 50-byte extracts for question 10001 are shown in Figure 8.",
        "For lack of space, we are omitting the 250-byte extracts."
      ]
    },
    {
      "heading": "6 Werlect",
      "text": [
        "The Werlect algorithm used many of the same features of phrases used by AnSel, but employed a different ranking scheme."
      ]
    },
    {
      "heading": "6.1 Approach",
      "text": [
        "Unlike AnSel, Werlect is based on a two-step, rule-based process approximating a function with interaction between variables.",
        "In the first stage of this algorithm, we assign a rank to",
        "every relevant phrase within each sentence according to how likely it is to be the target answer.",
        "Next, we generate and rank each N-byte fragment based on the sentence score given by GuruQA, measures of the fragment's relevance, and the ranks of its component phrases.",
        "Unlike AnSel, Werlect was optimized through manual trial-and-error using the TR38 questions."
      ]
    },
    {
      "heading": "6.2 Step One: Feature Selection",
      "text": [
        "The features considered in Werlect that were also used by AnSel, were Type, Avgdst and Sscore.",
        "Two additional features were also taken into account: NotinqW: a modified version of Noting.",
        "As in AnSel, spans that are contained in the query are given a rank of 0.",
        "However, partial matches are weighted favorably in some cases.",
        "For example, if the question asks, \"Who was Lincoln's Secretary of State?\"",
        "a noun phrase that contains \"Secretary of State\" is more likely to be the answer than one that does not.",
        "In this example, the phrase, \"Secretary of State William Se-ward\" is the most likely candidate.",
        "This criterion also seems to play a role in the event that Resporator fails to identify relevant phrase types.",
        "For example, in the training question, \"What shape is a porpoise's tooth?\"",
        "the phrase \"spade-shaped\" is correctly selected from among all nouns and adjectives of the sentences returned by Guru-QA.",
        "Frequency: how often the span occurs across different passages.",
        "For example, the test question, \"How many lives were lost in the Pan Am crash in Lockerbie, Scotland?\"",
        "resulted in four potential answers in the first two sentences returned by Guru-QA.",
        "Table 3 shows the frequencies of each term, and their eventual influence on the span rank.",
        "The repeated occurrence of \"270\", helps promote it to first place."
      ]
    },
    {
      "heading": "6.3 Step two: ranking the sentence spans",
      "text": [
        "After each relevant span is assigned a rank, we rank all possible text segments of 50 (or 250) bytes from the hit list based on the sum of the phrase ranks plus additional points for other words in the segment that match the query.",
        "The algorithm used by Werlect is shown in Figure 9."
      ]
    },
    {
      "heading": "7 Evaluation",
      "text": [
        "In this section, we describe the performance of our system using results from our four official runs."
      ]
    },
    {
      "heading": "7.1 Evaluation scheme",
      "text": [
        "For each question, the performance is computed as the reciprocal value of the rank (RAR) of the highest-ranked correct answer given by the system.",
        "For example, if the system has given the correct answer in three positions: second, third, and fifth, RAR for that question is -1i.",
        "The Mean Reciprocal Answer Rank (MRAR) is used to compute the overall performance of systems participating in the TREC evaluation:"
      ]
    },
    {
      "heading": "7.2 Performance on the official evaluation data",
      "text": [
        "Overall, Ansel (runs A50 and A25) performed marginally better than Werlect.",
        "However, we noted that on the 14 questions we were unable to classify with a QA-Token, Werlect (runs W50 and W250) achieved an MRAR of 3.5 to Ansel's 2.0.",
        "The cumulative RAR of A50 on T200 (Table 4) is 63.22 (i.e., we got 49 questions among the 198 right from our first try and 39 others within the first five answers).",
        "The performance of A250 on T200 is shown in Table 5.",
        "We were able to answer 71 questions with our first answer and 38 others within our first five answers (cumulative RAR = 85.17).",
        "To better characterize the performance of our system, we split the 198 questions into 20 groups of 10 questions.",
        "Our performance on groups of questions ranged from 0.87 to 5.50 MRAR for A50 and from 1.98 to 7.5 MRAR for A250 (Table 6).",
        "Finally, Table 7 shows how our official runs compare to the rest of the 25 official submissions.",
        "Our performance using AnSel and 50- byte output was 0.430.",
        "The performance of Werlect was 0.395.",
        "On 250 bytes, AnSel scored 0.319 and Werlect - 0.280."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We presented a new technique, predictive annotation, for finding answers to natural language questions in text corpora.",
        "We showed that a system based on predictive annotation can deliver very good results compared to other competing systems.",
        "We described a set of features that correlate with the plausibility of a given text span being a good answer to a question.",
        "We experi",
        "mented with two algorithms for ranking potential answers based on these features.",
        "We discovered that a linear combination of these features performs better overall, while a non-linear algorithm performs better on unclassified questions."
      ]
    },
    {
      "heading": "9 Acknowledgments",
      "text": [
        "We would like to thank Eric Brown, Anni Co-den, and Wlodek Zadrozny from IBM Research for useful comments and collaboration.",
        "We would also like to thank the organizers of the TREC Q&A evaluation for initiating such a wonderful research initiative."
      ]
    }
  ]
}
