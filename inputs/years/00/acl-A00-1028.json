{
  "info": {
    "authors": [
      "Nicola Cancedda",
      "Christer Samuelsson"
    ],
    "book": "Applied Natural Language Processing Conference and Meeting of the North American Association for Computational Linguistics",
    "id": "acl-A00-1028",
    "title": "Experiments With Corpus-Based LFG Specialization",
    "url": "https://aclweb.org/anthology/A00-1028",
    "year": 2000
  },
  "references": [
    "acl-E93-1006",
    "acl-P94-1026",
    "acl-P94-1040",
    "acl-P95-1036",
    "acl-P96-1030",
    "acl-P97-1028",
    "acl-P98-1022"
  ],
  "sections": [
    {
      "text": [
        "first, and the sentence is passed on to the original unpruned grammar whenever the pruned grammar fails to return a parse (see Figure 3).",
        "The measured speedup of this simulated architecture, which preserves the anyparse measure of the original grammar, takes into account the contribution of uncovered sentences, as it penalizes sweeping difficult sentences under the carpet."
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": [
        "The results of the experiments described in the section above are summarized in the table in Figure 4.",
        "The upper part of the table refers to experiments with the French grammar, the lower part to experiments with the English grammar.",
        "For each language, the first line presents data gathered for the original grammar for comparison with the pruned grammars.",
        "The figures in the second line were collected by pruning the grammar based on the whole corpus, and then testing on the corpus itself.",
        "The grammars obtained in this way contain 516 and 388 disjuncts – corresponding to purely concatenative rules – for French and English respectively.",
        "Any-parse and coverage are not, of course, relevant in this case, but the statistics on parsing time are, especially the one on the maximum parsing time.",
        "For each iteration in the 10-fold cross-validation experiment, the maximum parsing time was retained, and those ten times were eventually averaged.",
        "If pruning tended to leave sentences which take long to parse uncovered, then we would observe a significant difference between the average over maximum times on the grammar trained and tested on the same corpus (which parses all sentences, including the hardest), and the average over maximum times for grammars trained and tested on different sets.",
        "The fact that this does not seem to be the case indicates that pruning does not penalize difficult sentences.",
        "Note also that the average number of parses per sentence is significantly smaller than with the full grammar, of almost a factor of 9 in the case of the French grammar.",
        "The third line contains results for the fully pruned grammar.",
        "In the case of the French grammar a speedup of about 6 is obtained with a loss in coverage of 13%.",
        "The smaller speedup gained with the English grammar can be explained by the fact that here, the parsing times are lower in general, and that a non-negligible part of this time, especially that needed for morphological analysis, is unaffected by pruning.",
        "Even in the case of the English grammar, though, speedup is substantial (2.67).",
        "For both grammars, the reduction in the average maximum parsing time is particularly good, confirming our hypothesis that trimming the grammar by removing heavy constructs makes it considerably more efficient.",
        "A partially negative note comes from the average number of disjuncts in the pruned grammars, which is 501 for French and 374 for English.",
        "Comparing this figures to the number of disjuncts in grammars pruned on the full corpus (516 and 388), we find that after training on nine tenths of the corpus, adding the last tenth still leads to an increase of 3-4% in the size of the resulting grammars.",
        "In other words, the marginal gain of further training examples is still significant after considering about 900 sentences, indicating that the training corpora are somewhat too small.",
        "The last two lines for each language show figures for grammars with pruning inhibited on the most.",
        "variable and the two most variable symbols respectively.",
        "For both languages, inhibiting pruning on the most variable symbol has the expected effect of increasing both parsing time and coverage.",
        "Inhibiting pruning also on the second most variable symbol has almost no effect for French, and only a small effect for English.",
        "The table in Figure 5 summarizes the measures on the simulated two-stage architecture.",
        "For both languages the best trade-off, once the distribution of uncovered sentences has been taken into account, is achieved by the fully pruned grammars."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "The work presented in the current article is related to previous work on corpus-based grammar specialization as presented in (Rayner, 1988; Samuels-son and Rayner, 1991; Rayner and Carter, 1996; Samuelsson, 1994; Srinivas and Joshi, 1995; Neumann, 1997).",
        "The line of work described in (Rayner, 1988; Samuelsson and Rayner, 1991; Rayner and Carter, 1996; Samuelsson, 1994) deals with unification-based grammars that already have a purely-concatenative context-free backbone, and is more concerned with a different form of specialization, consisting in the application of explanation-based learning (EBL).",
        "Here, the central idea is to collect the most frequently occurring subtrees in a treebank and use them as atomic units for parsing.",
        "The cited works differ mainly in the criteria adopted for selecting subtrees from the treebank.",
        "In (Rayner, 1988; Samuelsson and Rayner, 1991; Rayner and Carter, 1996) these criteria are handcoded: all subtrees satisfying some properties are selected, and a new grammar rule is created by flattening each such subtree, i.e., by taking the root as left-hand side and the yield as right-hand side, and in the process performing all unifications corresponding to the thus removed internal nodes.",
        "Experiments carried out on a corpus of 15,000 trees from the ATIS domain using a version of the SRI Core Language Engine resulted in a speedup of about 3.4 at a cost of 5% in grammatical coverage, which however was compensated by an increase in parsing accuracy.",
        "Finding suitable tree-cutting criteria requires a considerable amount of work, and must be repeated for each new grammar and for each new domain to which the grammar is to be specialized.",
        "Samuelsson (Samuelsson, 1994) proposes a technique to automatically selects what subtrees to retain.",
        "The selection of appropriate subtrees is done by choosing a subset of nodes at which to cut trees.",
        "Cutnodes are determined by computing the entropy of each node, and selecting only those nodes whose entropy exceeds a given threshold.",
        "Intuitively, nodes with low entropy indicate locations in the trees where a given symbol was expanded using a predictable set of rules, at least most of the times, so that the loss of coverage that derives from ignoring the remaining cases is low.",
        "Nodes with high entropy, on the other hand, indicate positions in which there is a high uncertainty in what rule was used to expand the symbol, so that it is better to preserve all alternatives.",
        "Several schemas are proposed to compute entropies, each leading to a different trade-off between coverage reduction and speedup.",
        "In general, results are not quite as good as those obtained using handcoded criteria, though of course the specialized grammar is obtained fully automatically, and thus with much less effort.",
        "When ignoring issues related to the elimination of complex operators from the RHS of rule schemata, the grammar-pruning strategy described in the current article is equivalent to explanation-based learning where all nodes have been selected, as cutnodes.",
        "Conversely, EBL can be viewed as higher-order grammar pruning, removing not grammar rules, but grammar-rule combinations.",
        "Some of the work done on data-oriented parsing (DOP) (Bod, 1993; Bod and Scha, 1996; Bod and Kaplan, 1998; Sima'an, 1999) can also be considered related to our work, as it can be seen as a way to specialize in an EBL-like way the (initially unknown) grammar implicitly underlying a treebank.",
        "(Srinivas and Joshi, 1995) and (Neumann, 1997) apply EBL to speed up parsing with tree-adjoining grammars and sentence generation with HPSGs respectively, though they do so by introducing new components in their systems rather then by modifying the grammars they use."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "Sophisticated grammar formalisms are very useful and convenient when designing high-coverage grammars for natural languages.",
        "Very expressive grammatical constructs can make the task of developing and maintaining such a large resource considerably easier.",
        "On the other hand, their use can result in a considerable increase in grammatical ambiguity.",
        "Grammar-compilation techniques based on grammar structure alone are insufficient remedies in those cases, as they cannot access the information required to determine which alternatives to retain and which alternatives to discard.",
        "The current article demonstrates that a relatively simple pruning technique, employing the kind of reference corpus that is typically used for grammar development and thus often already available, can significantly improve parsing performance.",
        "On large lexical functional grammars, speedups of up to a factor 6 were observed, at the price of a reduction in grammatical coverage of about 13%.",
        "A simple two-stage architecture was also proposed that preserves the anyparse measure of the original grammar, demonstrating that significant speedups can be obtained without increasing the number of parsing failures.",
        "Future work includes extending the study of corpus-based grammar specialization from first-order grammar pruning to higher-order grammar pruning, thus extending previous work on explanation-based learning for parsing, and applying it to the LFG formalism."
      ]
    }
  ]
}
