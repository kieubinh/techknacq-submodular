{
  "info": {
    "authors": [
      "John C. Henderson",
      "Eric Brill"
    ],
    "book": "Applied Natural Language Processing Conference and Meeting of the North American Association for Computational Linguistics",
    "id": "acl-A00-2005",
    "title": "Bagging and Boosting a Treebank Parser",
    "url": "https://aclweb.org/anthology/A00-2005",
    "year": 2000
  },
  "references": [
    "acl-J93-2004",
    "acl-P97-1003",
    "acl-P98-1083",
    "acl-W99-0606",
    "acl-W99-0623"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Bagging and boosting, two effective machine learning techniques, are applied to natural language parsing.",
        "Experiments using these techniques with a trainable statistical parser are described.",
        "The best resulting system provides roughly as large of a gain in F-measure as doubling the corpus size.",
        "Error analysis of the result of the boosting technique reveals some inconsistent annotations in the Penn Treebank, suggesting a semi-automatic method for finding inconsistent treebank annotations."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy.",
        "Finding an ensemble of parsers designed to complement each other is clearly desirable.",
        "The parsers would need to be the result of a unified research effort, though, in which the errors made by one parser are targeted with priority by the developer of another parser.",
        "A set of five parsers which each achieve only 40% exact sentence accuracy would be extremely valuable if they made errors in such a way that at least two of the five were correct on any given sentence (and the others abstained or were wrong in different ways).",
        "100% sentence accuracy could be achieved by selecting the hypothesis that was proposed by the two parsers that agreed completely.",
        "In this paper, the task of automatically creating complementary parsers is separated from the task of creating a single parser.",
        "This facilitates study of the ensemble creation techniques in isolation.",
        "The result is a method for increasing parsing performance by creating an ensemble of parsers, each produced from data using the same parser induction algorithm."
      ]
    },
    {
      "heading": "2 Bagging and Parsing",
      "text": []
    },
    {
      "heading": "2.1 Background",
      "text": [
        "The work of Efron and Tibshirani (1993) enabled Breiman's refinement and application of their techniques for machine learning (Breiman, 1996).",
        "His technique is called bagging, short for \"bootstrap ag-gregating\".",
        "In brief, bootstrap techniques and bagging in particular reduce the systematic biases many estimation techniques introduce by aggregating estimates made from randomly drawn representative resamplings of those datasets.",
        "Bagging attempts to find a set of classifiers which are consistent with the training data, different from each other, and distributed such that the aggregate sample distribution approaches the distribution of samples in the training set."
      ]
    },
    {
      "heading": "Algorithm: Bagging Predictors",
      "text": [
        "Given: training set G = {(y2, x2), i E {1 m}} drawn from the set A of possible training sets where y, is the label for example x.â€ž classification induction",
        "algorithm Ci : A I with classification algorithm E (I) and cb : X Y.",
        "1.",
        "Create k bootstrap replicates of L by sampling m items from L with replacement.",
        "Call them Li .",
        ".",
        "Lk.",
        "2.",
        "For each j E {1 k}, Let 03 = (L3) be the classifier induced using L3 as the training set.",
        "3.",
        "If Y is a discrete set, then for each x, observed in the test set, y, = mode(03 (Xi) ... 03(x1)).",
        "yt is the value predicted by the most predictors, the majority vote."
      ]
    },
    {
      "heading": "2.2 Bagging for Parsing",
      "text": [
        "An algorithm that applies the technique of bagging to parsing is given in Algorithm 2.",
        "Previous work on combining independent parsers is leveraged to produce the combined parser.",
        "The rest of the algorithm is a straightforward transformation of bagging for classifiers.",
        "Exploratory work in this vein was described by Hajit et al.",
        "(1999).",
        "Algorithm: Bagging A Parser (2) Given: A corpus (again as a function) C : SxT N, S is the set of possible sentences, and T is the set of trees, with size m = ICI = E.,,c(s, t) and parser induction algorithm g. 1.",
        "Draw k bootstrap replicates C1 .",
        ".",
        ".",
        "Ck of C each containing m samples of (s,t) pairs randomly",
        "mechanisms of a parser for Japanese.",
        "The creators of AdaBoost used it to perform text classification (Schapire and Singer, 2000).",
        "Abney et al.",
        "(1999) performed part-of-speech tagging and prepositional phrase attachment using AdaBoost as a core component.",
        "They found they could achieve accuracies on both tasks that were competitive with the state of the art.",
        "As a side effect, they found that inspecting the samples that were consistently given the most weight during boosting revealed some faulty annotations in the corpus.",
        "In all of these systems, AdaBoost has been used as a traditional classification system."
      ]
    },
    {
      "heading": "3.2 Boosting for Parsing",
      "text": [
        "Our goal is to recast boosting for parsing while considering a parsing system as the embedded learner.",
        "The formulation is given in Algorithm 4.",
        "The intuition behind the additive form is that the weight placed on a sentence should be the sum of the weight we would like to place on its constituents.",
        "The weight on constituents that are predicted incorrectly are adjusted by a factor of 1 in contrast to a factor of a for those that are predicted incorrectly."
      ]
    },
    {
      "heading": "Algorithm: Boosting A Parser (4)",
      "text": [
        "Given corpus C with size m = Es, C(s.t) and parser induction algorithm g. Initial uniform distribution Di (i) = 1/m.",
        "Number of iterations, T. Counter t = 1.",
        "1.",
        "Create Ct by randomly choosing with replacement in samples from C using distribution Dt.",
        "2.",
        "Create parser ft <- g(Ct).",
        "3.",
        "Choose at E R (described below).",
        "4.",
        "Adjust and normalize the distribution.",
        "Zt is a normalization coefficient.",
        "For all i, let parse tree 77: ft(se).",
        "Let 6(r. c) be a function indicating that c is in parse tree r, and I TI is the number of constituents in tree T. T(s) is the set of constituents that are found in the reference or hypothesized annotation for s.",
        "5.",
        "Increment t. Quit if t > T. 6.",
        "Repeat from step 1.",
        "7.",
        "The final hypothesis is computed by combining the individual constituents.",
        "Each parser Ot in the ensemble gets a vote with weight at for the constituents they predict.",
        "Precisely those constituents with weight strictly larger than Et at are put into the final hypothesis.",
        "A potential constituent can be considered correct if it is predicted in the hypothesis and it exists in the reference, or it is not predicted and it is not in the reference.",
        "Potential constituents that do not appear in the hypothesis or the reference should not make a big contribution to the accuracy computation.",
        "There are many such potential constituents, and if we were maximizing a function that treated getting them incorrect the same as getting a constituent that appears in the reference correct, we would most likely decide not to predict any constituents.",
        "Our model of constituent accuracy is thus simple.",
        "Each prediction correctly made over T(s) will be given equal weight.",
        "That is, correctly hypothesizing a constituent in the reference will give us one point, but a precision or recall error will cause us to miss one point.",
        "Constituent accuracy is then a/(a+b+c), where a is the number of constituents correctly hypothesized, b is the number of precision errors and c is the number of recall errors.",
        "In Equation 1, a computation of act, as described is shown.",
        "Boosting algorithms were developed that attempted to maximize F-measure, precision, and recall by varying the computation of a, giving results too numerous to include here.",
        "The algorithm given here performed the best of the lot, but was only marginally better for some metrics.",
        "Notice first that the left endpoints of the lines move from bottom to top in order of boosting iteration.",
        "The distribution becomes monotonically more skewed as boosting progresses.",
        "Secondly we see by the last iteration that most of the weight is concentrated on less than 100 samples.",
        "This graph shows behavior consistent with noise in the corpus on which the boosting algorithm is focusing."
      ]
    },
    {
      "heading": "4.2 Treebank Inconsistencies",
      "text": [
        "There are sentences in the corpus that can be learned by the parser induction algorithm in isolation but not in concert because they contain conflicting information.",
        "Finding these sentences leads to a better understanding of the quality of our corpus, and gives an idea for where improvements in annotation quality can be made.",
        "Abney et al.",
        "(1999) showed a similar corpus analysis technique for part of speech tagging and prepositional phrase tagging, but for parsing we must remove errors introduced by the parser as we did in Section 3.3.2 before questioning the corpus quality.",
        "A particular class of errors, inconsistencies, can then be investigated.",
        "Inconsistent annotations are those that appear plausible in isolation, but which conflict with annotation decisions made elsewhere in the corpus.",
        "In Figure 5 we show a set of trees selected from within the top 100 most heavily weighted trees at the end of 15 iterations of boosting the stable cor-pus.Collins's parser induction system is able to learn to produce any one of these structures in isolation, but the presence of conflicting information in different sentences prevents it from achieving 100% accuracy on the set."
      ]
    },
    {
      "heading": "5 Training Corpus Size Effects",
      "text": [
        "We suspect our best parser diversification techniques gives performance gain approximately equal to doubling the size of the training set.",
        "While this cannot be directly tested without hiring more annotators, an expected performance bound for a larger training set can be produced by extrapolating from how well the parser performs using smaller training sets.",
        "There are two characteristics of training curves for large corpora that can provide such a bound: training curves generally increase monotonically in the absence of over-training, and their first derivatives generally decrease monotonically.",
        "The training curves we present in Figure 4 and Table 4 suggest that roughly doubling the corpus size"
      ]
    }
  ]
}
