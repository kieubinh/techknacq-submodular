{
  "info": {
    "authors": [
      "Patrick Pantel",
      "Dekang Lin"
    ],
    "book": "Applied Natural Language Processing Conference and Meeting of the North American Association for Computational Linguistics",
    "id": "acl-A00-2011",
    "title": "Word-For-Word Glossing With Contextually Similar Words",
    "url": "https://aclweb.org/anthology/A00-2011",
    "year": 2000
  },
  "references": [
    "acl-J90-2002",
    "acl-P91-1022",
    "acl-P91-1023",
    "acl-P98-2127",
    "acl-P99-1068",
    "acl-W99-0905"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Many corpus-based machine translation systems require parallel corpora.",
        "In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus.",
        "To gloss a word, we first identify its similar words that occurred in the same context in a large corpus.",
        "We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order.",
        "Automating this process would benefit many NLP applications.",
        "For example, in cross-language information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts.",
        "Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system.",
        "Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999).",
        "Kikui (1999) used a word sense disambiguation algorithm and a non-parallel bilingual corpus to resolve translation ambiguity.",
        "In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus.",
        "The intuitive idea behind our algorithm is the following.",
        "Suppose w is a word to be translated.",
        "We first identify a set of words similar to w that occurred in the same context as w in a large corpus.",
        "We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty include responsibility, obligation, role, ...",
        "This list is then used to select a translation for duty.",
        "In the next section, we describe the resources required by our algorithm.",
        "In Section 3, we present an algorithm for constructing the contextually similar words of a word in a context.",
        "Section 4 presents the word-for-word glossing algorithm and Section 5 describes the group similarity metric used in our algorithm.",
        "In Section 6, we present some experimental results and finally, in Section 7, we conclude with a discussion of future work."
      ]
    },
    {
      "heading": "2. Resources",
      "text": [
        "The input to our algorithm includes a collocation database (Lin, 1998b) and a corpus-based thesaurus (Lin, 1998a), which are both available on the Internet'.",
        "In addition, we require a bilingual thesaurus.",
        "Below, we briefly describe these resources."
      ]
    },
    {
      "heading": "2.1. Collocation database",
      "text": [
        "Given a word w in a dependency relationship (such as subject or object), the collocation database can be used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies2.",
        "Figure 1 shows excerpts of the entries in the collocation database for the words corporate, duty, and fiduciary.",
        "The database contains a total of 11 million unique dependency relationships."
      ]
    },
    {
      "heading": "4 personnel 0.073, staff 0.073 5 training 0.072, work 0.064, exercise 0.061 6 privilege 0.069, right 0.057, license 0.056",
      "text": []
    },
    {
      "heading": "2.2. Corpus-based thesaurus",
      "text": [
        "Using the collocation database, Lin used an unsupervised method to construct a corpus-based thesaurus (Lin, 1998a) consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs.",
        "Given a word w, the thesaurus returns a clustered list of similar words of w along with their similarity to w. For example, the clustered similar words of duty are shown in Table 1."
      ]
    },
    {
      "heading": "2.3. Bilingual thesaurus",
      "text": [
        "Using the corpus-based thesaurus and a bilingual dictionary, we manually constructed a bilingual thesaurus.",
        "The entry for a source language word w is constructed by manually associating one or more clusters of similar words of w to each candidate translation of w. We refer to the assigned clusters as Words Associated with a Translation (WAT).",
        "For example, Figure 2 shows an excerpt of our English/French bilingual thesaurus for the words account and duty.",
        "Although the WAT assignment is a manual process, it is a considerably easier task than providing lexicographic definitions.",
        "Also, we only require entries for source language words that have multiple translations.",
        "In Section 7, we",
        "object-of: assume 177, breach 111, carry out 71, do 114, have 257, impose 114, perform 151, ... subject-of: affect 4, apply 6, include 42, involve 8, keep 5, officer 22, protect 8, require 13, ... adj-modifier: active 202, additional 46, administrative 44, fiduciary 317, official 66, other 83, ...",
        "discuss a method for automatically assigning the WATs."
      ]
    },
    {
      "heading": "9. Contextually Similar Words",
      "text": [
        "The contextually similar words of a word w are words similar to the intended meaning of w in its context.",
        "Figure 3 gives the data flow diagram for our algorithm for identifying the contextually similar words of w. Data are represented by ovals, external resources by double ovals and processes by rectangles.",
        "By parsing a sentence with Minipar3, we extract the dependency relationships involving w. For each dependency relationship, we retrieve",
        "3 Available at www.cs.umanitoba.ca/-lindek/minipar.htm account: 1. compte:",
        ".",
        "fund, deposit, loan, asset, portfolio, investment, transaction, payment, saving, money, contract, Budget, reserve, security, contribution, debt, property, holding, interest, bond, plan, business, ... 2. rapport: report, statement, testimony, card, story, record, document, data, information, view, check, figure, article, description, estimate, assessment, number, statistic, comment, letter, picture, note, ... responsibility, obligation, task, function, role, post, position, job, chore, mission, assignment, liability, ... tariff, restriction, tax, regulation, requirement, procedure, penalty, quota, rule, levy, ...",
        "from the collocation database the words that occurred in the same dependency relationship as w. We refer to this set of words as the cohort of w for that dependency relationship.",
        "Consider the word duty in the contexts corporate duty and fiduciary duty.",
        "The cohort of duty in corporate duty consists of nouns modified by corporate in Figure 1 (e.g. client, debt, development, ...) and the cohort of duty in fiduciary duty consists of nouns modified by fiduciary in Figure 1 (e.g. act, behaviour, breach, ...).",
        "Intersecting the set of similar words and the cohort then forms the set of contextually similar words of w. For example, Table 2 shows the contextually similar words of duty in the contexts corporate duty and fiduciary duty.",
        "The words in the first row are retrieved by intersecting the words in Table 1 with the nouns modified by corporate in Figure 1.",
        "Similarly, the second row represents the intersection of the words in Table 1 and the nouns modified by fiduciary in Figure 1."
      ]
    },
    {
      "heading": "CONTEXT CONTEXTUALLY SIMILAR WORDS OF DUTY",
      "text": [
        "corporate duty fee, function, levy, liability, obligation, personnel, responsibility, rule, staff, tax, training fiduciary duty obligation, requirement, responsibility, role the responsibility and tax senses of duty, reflecting the fact that the meaning of duty is indeed ambiguous if corporate duty is its sole context.",
        "In contrast, the second row in Table 2 clearly indicates the responsibility sense of duty.",
        "While previous word sense disambiguation algorithms rely on a lexicon to provide sense inventories of words, the contextually similar words provide a way of distinguishing between different senses of words without committing to any particular sense inventory.",
        "For example, suppose we wish to translate into French the word duty in the context corporate fiduciary duty.",
        "Step 1 retrieves the candidate translations for duty and its WATs from Figure 2.",
        "In Step 2, we construct two lists of contextually similar words, one for the dependency context corporate duty and one for the dependency context fiduciary duty, shown in Table 2.",
        "The proposed translation for the context is obtained by maximizing the group similarities between the lists of contextually similar words and the WATs.",
        "Using the group similarity measure from Section 5, Table 3 lists the group similarity scores between each list of contextually similar words and each WAT as well as the final combined score for each candidate translation.",
        "The combined score for a candidate is the sum of the logs of all group similarity scores involving its WAT.",
        "The correct proposed translation for duty in this context is devoir since its WAT received the highest score.",
        "Step 3: Compute the group similarity (see details in Section 5) between each set of contextually similar words and each WAT; the results are stored in a matrix t, where t[i,j] is the group similarity between the 1 h list of contextually similar words and the jh WAT.",
        "Step 4: Add the logs of the group similarity scores in column of t to obtain a score for each WAT.",
        "Output: The candidate translation corresponding to the WAT with the highest score."
      ]
    },
    {
      "heading": "5. Group Similarity",
      "text": [
        "The corpus-based thesaurus contains only the similarities between individual pairs of words.",
        "In our algorithm, we require the similarity between groups of words.",
        "The group similarity measure",
        "we use is proposed by Karypis et al.",
        "(1999).",
        "It takes as input two groups of elements, GI and G2, and a similarity matrix, sim, which specifies the similarity between individual elements.",
        "GI and G2 are describable by graphs where the vertices are the words and each weighted edge between vertices 11)1 and w2 represents the similarity, sim(w1, w2), between the words w, and w2.",
        "Karypis et al.",
        "consider both the interconnectivity and the closeness of the groups.",
        "The absolute interconnectivity between G, andG2,Al(Gt, G2), is defined as the aggregate similarity between the two groups:",
        "The absolute closeness between G1 and G2, AC(G,, G2), is defined as the average similarity between a pair of elements, one from each group:",
        "The difference between the absolute interconnectivity and the absolute closeness is that the latter takes zero similarity pairs into account.",
        "In Figure 6, the interconnectivity in (a) and (b) remains constant.",
        "However, the closeness in (a) is higher than in (b) since there are more zero similarity pairs in (b).",
        "Karypis et al.",
        "normalized the absolute interconnectivity and closeness by the internal interconnectivity and closeness of the individual groups.",
        "The normalized measures are referred to as relative interconnectivity, RI(G1, G2), and relative closeness, RC(G1, G2).",
        "The internal interconnectivity and closeness are obtained by first computing a minimal edge bisection of each group.",
        "An even-sized partition {G', G\"} of a group G is called a minimal edge bisection of G if AI(G', G\") is minimal among all such partitions.",
        "The internal interconnectivity of G, II(G), is defined as II(G) = AI(G', G\") and the internal closeness of G, IC(G), as IC(G) = AC(G', G\").",
        "Minimal edge bisection is performed for all WATs and all sets of contextually similar words.",
        "However, the minimal edge bisection problem is NP-complete (Garey and Johnson, 1979).",
        "Fortunately, state of the art graph partitioning algorithms can approximate these bisections in polynomial time (Goehring and Saad, 1994; Karypis and Kumar, 1999; Kernighan and Lin, 1970).",
        "We used the same approximation methods as in (Karypis et al., 1999).",
        "The similarity between G1 and G2 is then defined as follows: groupSim(Gâ€ž G2 ) = RI(GI, G2 ) x RC(GI, G2 ) where",
        "is the relative closeness."
      ]
    },
    {
      "heading": "6. Experimental Results",
      "text": [
        "The design of our glossing algorithm is applicable to any source/destination language pair as long as a source language parser is available.",
        "We considered English-to-French translations in our experiments.",
        "We experimented with six English nouns that have multiple French translations: account, duty, race, suit, check, and record.",
        "Using the 1987 Wall Street Journal files on the LDC/DCI CD-is the relative interconnectivity and",
        "ROM, we extracted a testing corpus4 consisting of the first 100 to 300 sentences containing the non-idiomatic usage of the six nounss.",
        "Then, we manually tagged each sentence with one of the candidate translations shown in Table 4.",
        "Each noun in Table 4 translates more frequently to one candidate translation than the other.",
        "In fact, always choosing the candidate proces as the translation for suit yields 94% accuracy.",
        "A better measure for evaluating the system's classifications considers both the algorithm's precision and recall on each candidate translation.",
        "Table 5 illustrates the precision and recall of our glossing algorithm for each candidate translation.",
        "Albeit precision and recall are used to evaluate the quality of the classifications, overall accuracy is sufficient for comparing different approaches with our system.",
        "In Section 3, we presented an algorithm for identifying the contextually similar words of a word in a context using a corpus-based thesaurus and a collocation database.",
        "Each of the six nouns has similar words in the corpus-based thesaurus.",
        "However, in order to find contextually similar words, at least one similar word for each noun must occur in the collocation database in a given context.",
        "Thus, the algorithm for constructing contextually similar words is dependent on the coverage of the collocation database.",
        "We estimated this coverage by counting the number of times each of the six nouns, in several different contexts, has at least one contextually similar word.",
        "The result is shown in Table 6.",
        "In Section 5, we described a group similarity metric, groupSim, which we use for comparing a WAT with a set of contextually similar words.",
        "In Figure 7, we compare the translation accuracy of our algorithm using other group similarity metrics.",
        "Suppose G1 and G2 are two groups of words and w is the word that we wish to translate.",
        "The metrics used are:",
        "1. closest3:",
        "sum of similarity of the three closest pairs of words from each group.",
        "3.",
        "AC: as defined in Section 5.",
        "4.",
        "AI: as defined in Section 5.",
        "5.",
        "RC: as defined in Section 5.",
        "6.",
        "RI: as defined in Section 5.",
        "In mostFrequent, we include the results obtained if we always choose the translation that occurs most frequently in the testing corpus.",
        "We also compared the accuracy of our glossing algorithm with Systran's translation system by feeding the testing sentences into Systran's web interface6 and manually examining the results.",
        "Figure 8 summarizes the overall accuracy obtained by each system and the baseline on the testing corpus.",
        "Systran tended to prefer one candidate translation over the other and committed the majority of its errors on the non-preferred senses.",
        "Consequently, Systran is very accurate if its preferred sense is the frequent sense (as in account and duty) but is very inaccurate if its preferred sense is the infrequent one (as in race, suit, and check)."
      ]
    },
    {
      "heading": "7. Conclusion and Future Work",
      "text": [
        "This paper presents a word-for-word glossing algorithm.",
        "The gloss of a word is determined by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus.",
        "The algorithm presented in this paper can be improved and extended in many ways.",
        "At present, our glossing algorithm does not take the prior probabilities of translations into account.",
        "For example, in WSJ, the bank account sense of account is much more common than the report sense.",
        "We should thus tend to prefer this sense of account.",
        "This is achievable by weighting the translation scores by the prior probabilities of the translations.",
        "We are investigating an Expectation-Maximization (EM) (Dempster et al., 1977) algorithm to learn these prior probabilities.",
        "Initially, we assume that the candidate translations for a word are uniformly distributed.",
        "After glossing each word in a large corpus, we refine the prior probabilities using the frequency counts obtained.",
        "This process is repeated several times until the empirical prior probabilities closely approximate the true prior probabilities.",
        "Finally, as discussed in Section 2.3, automatically constructing the bilingual thesaurus is necessary to gloss whole documents.",
        "This is attainable by adding a corpus-based destination language thesaurus to our system.",
        "The process of assigning a cluster of similar words as a WAT to a candidate translation c is as follows.",
        "First, we",
        "automatically obtain the candidate translations for a word using a bilingual dictionary.",
        "With the destination language thesaurus, we obtain a list S of all words similar to c. With the bilingual dictionary, replace each word in S by its source language translations.",
        "Using the group similarity metric from Section 5, assign as the WAT the cluster of similar words (obtained from the source language thesaurus) most similar to S."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The authors wish to thank the reviewers for their helpful comments.",
        "This research was partly supported by Natural Sciences and Engineering Research Council of Canada grants OGP121338 and PGSA207797."
      ]
    }
  ]
}
