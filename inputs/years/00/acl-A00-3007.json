{
  "info": {
    "authors": [
      "Mary Xiaoyong Liu",
      "Ted Diamond",
      "Anne R. Diekema"
    ],
    "book": "Applied Natural Language Processing Conference and Meeting of the North American Association for Computational Linguistics – Student Research Workshop",
    "id": "acl-A00-3007",
    "title": "Word Sense Disambiguation for Cross-Language Information Retrieval",
    "url": "https://aclweb.org/anthology/A00-3007",
    "year": 2000
  },
  "references": [
    "acl-C92-2070",
    "acl-C96-1005",
    "acl-H93-1051",
    "acl-J98-1001",
    "acl-W97-0213",
    "acl-W98-0705",
    "acl-X96-1035"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We have developed a word sense disambiguation algorithm, following Cheng and Wilensky (1997), to disambiguate among WordNet synsets.",
        "This algorithm is to be used in a cross-language information retrieval system, CINDOR, which indexes queries and documents in a language-neutral concept representation based on WordNet synsets.",
        "Our goal is to improve retrieval precision through word sense disambiguation.",
        "An evaluation against human disambiguation judgements suggests promise for our approach."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The CINDOR cross-language information retrieval system (Diekema et al., 1998) uses an information structure known as \"conceptual interlingua\" for query and document representation.",
        "This conceptual interlingua is a hierarchically organized multilingual concept lexicon, which is structured following WordNet (Miller, 1990).",
        "By representing query and document terms by their WordNet synset numbers we arrive at essentially a language neutral representation consisting of synset numbers representing concepts.",
        "This representation facilitates cross-language retrieval by matching term synonyms in English as well as across languages.",
        "However, many terms are polysemous and belong to multiple synsets, resulting in spurious matches in retrieval.",
        "The noun figure for example appears in 13 synsets in WordNet 1.6.",
        "This research paper describes the early stages1 of our efforts to develop a word sense disambiguation (WSD) algorithm aimed at improving the precision of our cross-language retrieval system."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "To determine the sense of a word, a WSD algorithm typically uses the context of the ambiguous word, external resources such as machine-readable dictionaries, or a combination of both.",
        "Although dictionaries provide useful word sense information and thesauri provide additional information about relationships between words, they lack pragmatic information as can be found in corpora.",
        "Corpora contain examples of words that enable the development of statistical models of word senses and their contexts (Ide and Veronis, 1998; Leacock and Chodorow, 1998).",
        "There are two general problems with using corpora however; 1) corpora typically do not come pre-tagged with manually disambiguated senses, and 2) corpora are often not large nor diverse enough for all senses of a word to appear often enough for reliable statistical models (data sparseness).",
        "Although researchers have tried sense-tagging corpora automatically by using either supervised or unsupervised training methods, we have adopted a WSD algorithm which avoids the necessity for a sense-tagged training corpus.",
        "The problem of data sparseness is usually solved by using either smoothing methods, class-based methods, or by relying on similarity-based methods between words and co-occurrence data.",
        "Since we are using a WordNet-based resource for retrieval, using class-based methods seems a natural choice.",
        "Appropriate word classes can be formed by synsets or groups of synsets.",
        "The evidence of a certain sense (synset) is then no longer dependent on one word but on all the members of a particular synset.",
        "Yarowsky (1992) used Rogets Thesaurus categories as classes for WSD.",
        "His approach was based on selecting the most likely Roget category for nouns given their context of 50 words on either side.",
        "When any of the category indicator words appeared in the context of an ambiguous word, the indicator weights for each category were summed to determine the most likely category.",
        "The category with the largest sum was then selected.",
        "A similar approach to that of Yarowsky was followed by Cheng and Willensky (1997) who used a training matrix of associations of words with a certain category.",
        "Their algorithm was appealing to us because it requires no human intervention, and more importantly, it avoids the use of sense-tagged data.",
        "Our methodology described in the next section is therefore based on Cheng and Wilensky's approach.",
        "Methods to reduce (translation) ambiguity in cross-language information retrieval have included using part-of-speech taggers to restrict the translation options (Davis 1997), applying pseudo-relevance feedback loops to expand the query with better terms aiding translation (Ballesteros and Croft 1997), using corpora for term translation disambiguation (Ballesteros and Croft, 1998), and weighted Boolean models which tend to have a self-disambiguating quality (Hull, 1997; Diekema et al., 1999; Hiemstra and Kraaij, 1999)."
      ]
    },
    {
      "heading": "3 Methodology",
      "text": [
        "To disambiguate a given word, we would like to know the probability that a sense occurs in a given context, i.e., P(senselcontext).",
        "In this study, WordNet synsets are used to represent word senses, so P(senselcontext) can be rewritten as P(synseticontext), for each synset of which that word is a member For nouns, we define the context of word w to be the occurrence of words in a moving window of 100 words (50 words on each side) around w2.",
        "By Bayes Theorem., we can obtain the desired probability by inversion (see equation (1)).",
        "Since we are not specifically concerned with getting accurate probabilities but rather relative rank order for sense selection, we ignore P(context(w)) and focus on estimating P(context(w)Isynset)P(synset).",
        "The event space from which \"context(w)\" is drawn is the set of sets of words that ever appear with each other in the window around w. In other words, w induces a partition on the set of words.",
        "We define \"context(w)\" to be true whenever any of the words in the set appears in the window around w, and conversely to be false whenever none of the words in the set appears around w. If we assume independence of appearance of any two words in a given context, then we get:",
        "Due to the lack of sense-tagged corpora, we are not able to directly estimate P(synset) and P(wiIsynset).",
        "Instead, we introduce \"noisy estimators\" (Pe(synset) and Pe(wdsynset)) to approximate these probabilities.",
        "In doing so, we make two assumptions: 1) The presence of any word wk that belongs to synset si signals the presence of si; 2) Any word wk belongs to all its synsets simultaneously, and with equal probability.",
        "Although the assumptions underlying the \"noisy estimators\" are not strictly true, it is our belief that the \"noisy estimators\" should work reasonably well if:",
        "• The words that belong to synset si tend to appear in similar contexts when sf, is their intended sense; • These words do not completely overlap with the words belonging to some synset ( i j ) that partially overlaps with si; 2 For other parts of speech, the window size should be much smaller as suggested by previous research.",
        "36 • The common words between si and si",
        "appear in different contexts when si and s, are their intended senses."
      ]
    },
    {
      "heading": "4 The WSD Algorithm",
      "text": [
        "We chose as a basis the algorithms described by Yarrowsky (1992) and by Cheng and Wilensky (1997).",
        "In our variation, we use the synset numbers in WordNet to represent the senses of a word.",
        "Our algorithm learns associations of WordNet synsets with words in a surrounding context to determine a word sense.",
        "It consists of two phases.",
        "During the training phase, the algorithm reads in all training documents in collection and computes the distance-adjusted weight of co-occurrence of each word with each corresponding synset.",
        "This is done by establishing a 100-word window around a target word (50 words on each side), and correlating each synset to which the target word belongs with each word in the surrounding window.",
        "The result of the training phase is a matrix of associations of words with synsets.",
        "In the sense prediction phase, the algorithm takes as input randomly selected testing documents or sentences that contain the polysemous words we want to disambiguate and exploits the context vectors built in the training phase by adding up the weighted \"votes\".",
        "It then returns a ranked list of probability values associated with each synset, and chooses the synset with the highest probability as the sense of the ambiguous word.",
        "In this algorithm, \"noisy estimators\" are employed in the sense prediction phase.",
        "They are calculated using following formulas:",
        "where wi is a stem, x is a given synset, M[w][x] is a cell in the correlation matrix that corresponds to word w and synset x, and",
        "where w is any stem in the collection, x is a given synset, y is any synset ever occurred in collection.",
        "IneW ,yeY For each document d in collection read in a noun stem w from d for each synset s in which w occurs get the column b in the association matrix M that corresponds to s if the column already exists; create a new column for s otherwise for each word stem j appearing in the 100-word window around w get the row a in M that corresponds to j if the row already exists; create a new row for j otherwise add a distance-adjusted weight to M[a][b]",
        "Set value = 1 For each word w to be disambiguated get synsets of w"
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": [
        "As suggested by the WSD literature, evaluation of word sense disambiguation systems is not yet standardized (Resnik and Yarowsky, 1997).",
        "Some WSD evaluations have been done using the Brown Corpus as training and testing resources and comparing the results against SemCor3, the sense-tagged version of the Brown Corpus (Agirre and Rigau, 1996; Gonzalo et al., 1998).",
        "Others have used common test suites such as the 2094-word line data of Leacock et al.",
        "(1993).",
        "Still others have tended to use their own metrics.",
        "We chose an evaluation with a user-based component that allowed a ranked list of sense selection for each target word and enabled a comprehensive comparison between automatic and manual WSD results.",
        "In addition we wanted to base the disambiguation matrix on a corpus that we use for retrieval.",
        "This approach allows for a much richer evaluation than a simple hit-or-miss test.",
        "For validation purpose, we will conduct a fully automatic evaluation against SemCor in our future efforts.",
        "We use in vitro evaluation in this study, i.e. the WSD algorithm is tested independent of the retrieval system.",
        "The population consists of all the nouns in WordNet, after removal of monosemous nouns, and after removal of a problematic class of polysemous nouns.4 We drew a random sample of 87 polysemous nouns from this population.",
        "In preparation, for each noun in our sample we identified all the documents containing that noun from the Associated Press (AP) newspaper corpus.",
        "The testing document set was then formed by randomly selecting 10 documents from the set of identified documents for each of the 87 nouns.",
        "In total, there are 867 documents in the 3 SemCor is a semantically sense-tagged corpus comprising approximately 250, 000 words.",
        "The reported error rate is around 10% for polysemous words.",
        "This class of nouns refers to nouns that are in synsets in which they are the sole word, or in synsets whose words were subsets of other synsets for that noun.",
        "This situation makes disambiguation extremely problematic.",
        "This class of noun will be dealt with in a future version of our algorithm but for now it is beyond the scope of this evaluation.",
        "5 A polysemous noun is defined as a noun that belongs to two or more synsets.",
        "testing set.",
        "The training document set consists of all the documents in the AP corpus excluding the above-mentioned 867 documents.",
        "For each noun in our sample, we selected all its corresponding WordNet noun synsets and randomly selected 10 sentence occurrences with each from one of the 10 random documents.",
        "After collecting 87 polysemous nouns with 10 noun sentences each, we had 870 sentences for disambiguation.",
        "Four human judges were randomly assigned to two groups with two judges each, and each judge was asked to disambiguate 275 word occurrences out of which 160 were unique and 115 were shared with the other judge in the same group.",
        "For each word occurrence, the judge put the target word's possible senses in rank order according to their appropriateness given the context (ties are allowed).",
        "Our WSD algorithm was also fed with the identical set of 870 word occurrences in the sense prediction phase and produced a ranked list of senses for each word occurrence.",
        "Since our study has a matched-group design in which the subjects (word occurrences) receive both the treatments and control, the measurement of variables is on an ordinal scale, and there is no apparently applicable parametric statistical procedure available, two nonparametric procedures the Friedman two-way analysis of variance and the Spearman rank correlation coefficient were originally chosen as candidates for the statistical analysis of our results.",
        "However, the number of ties in our results renders the Spearman coefficient unreliable.",
        "We have therefore concentrated on the Friedman analysis of our experimental results.",
        "We use the two-alternative test with a=0.05.",
        "The first tests of interest were aimed at establishing inter-judge reliability across the 115 shared sentences by each pair of judges.",
        "The null hypothesis can be generalized as \"There is no difference in judgments on the same word occurrences between two judges in the same group\".",
        "Following general steps of conducting a Friedman test as described by Siegel (1956), we cast raw ranks in a two-way table having 2 conditions/columns (K = 2) with each of the human judges in the pair serving as one condition and 365 subjects/rows (N = 365) which are all the senses of the 115 word occurrences that were judged by both human judges.",
        "We then ranked"
      ]
    }
  ]
}
