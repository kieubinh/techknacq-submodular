{
  "info": {
    "authors": [
      "José-Miguel Benedí",
      "Joan Andreu Sanchez Peiro"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1009",
    "title": "Combination of N-Grams and Stochastic Context-Free Grammars for Language Modeling",
    "url": "https://aclweb.org/anthology/C00-1009",
    "year": 2000
  },
  "references": [
    "acl-J91-3004",
    "acl-J93-2004",
    "acl-P92-1017",
    "acl-P98-1035"
  ],
  "sections": [
    {
      "heading": "COMBINATION OF N-GRAMS AND STOCHASTIC CONTEXT-FREE GRAMMARS FOR LANGUAGE MODELING*",
      "text": []
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper describes a hybrid proposal to combine n-grams and Stochastic Context-Free Grammars (SCFGs) for language modeling.",
        "A classical n-gram model is used to capture the local relations between words, while a stochastic grammatical model is considered to represent the long-term relations between syntactical structures.",
        "In order to define this grammatical model, winch will be used on large-vocabulary complex tasks, a category-based SCFG and a probabilistic model of word distribution in the categories have been proposed.",
        "Methods for learning these stochastic models for complex tasks are described, and algorithms for computing the word transition probabilities are also presented.",
        "Finally, experiments using the Penn Treebank corpus improved by 30% the test set perplexity with regard to the classical in-grans models."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Language modeling is an important aspect to consider in large-vocabulary speech recognition systems (Bahl et al., 1983; Jelinek, 1998).",
        "The n-gram models are the most widely-used for a wide range of domains (Bahl et al., 1983).",
        "The n-grams are simple and robust models and adequately capture the local restrictions between words.",
        "Moreover, it is well-known how to estimate the parameters of the model and how to integrate them in a speech recognition system.",
        "However, the n-gram models cannot adequately characterize the long-term constraints of the sentences of the tasks.",
        "On the other hand, Stochastic Context-Free Grammars (SCFGs) allow us a better model-This work has been partially supported by the Spanish CICYT under contract (TIC98/0423-006).",
        "ing of long-term relations and work well on limited-domain tasks of low perplexity.",
        "However, SCFGs work poorly for large-vocabulary, general-purpose tasks because learning SCFGs and the computation of word transition probabilities present serious problems for complex real tasks.",
        "In the literature, a number of works have proposed ways to generalize the n-grain models (.To-litick, 1998; Sin and Ostendorf, 2000) or combining with other structural models (Bellegarda, 1998; Gilet and Ward, 1998; Chellia and Jelinek, 1998).",
        "In this paper, we present; a combined language model defined as a linear combination of n-grams, winch are used to capture the local relations between words, and a stochastic grammatical model which is used to represent the global relation between syntactic structures.",
        "In order to capture these long-term relations and to solve the main problems derived from the large-vocabulary complex tasks, we propose here to define: a category--based SCFG and a probabilistic model of word distribution in the categories.",
        "Taking into account this proposal, we also describe here how to solve the learning of these stochastic models and their integration prob-With regard to the learning problem, several algorithms that learn SCEGs by means of estimation algorithms have been proposed (Lars and Young, 1990; Pereira and Schabes, 1992; Sanchez and 13enedf, 1998), and promising results have been achieved with category-based SCFGs on real tasks (Sanchez and Benedi, 1999).",
        "In relation to the integration problem, we present two algorithms that compute the word transition probability: the first algorithm is based on the Left-to-Bight Inside algorithm",
        "(LRI) (Jelinek and Lafferty, 1991), and the second is based on an application of a Viterbi scheme to the LRI algorithm (the VLIII algorithm) (Sanchez and Benedi, 1997).",
        "Finally, in order to evaluate the behavior of this proposal, experiments with a part of the Wall Street Journal processed in the Penn Treebank project were carried out and significant improvements with regard to the classical n-gram models were achieved."
      ]
    },
    {
      "heading": "2 The language model",
      "text": [
        "An important problem related to language modeling is the evaluation of Pr(wk 101 wkd In order to compute this probability, we propose a hybrid language model defined as a simple linear combination of n-gram models and a stochastic grammatical model Gs:",
        "where 0 < a < 1 is a weight factor winch depends on the task.",
        "The expression Pr(wk Iwk_ wk_i) is the word probability of occurrence of wk given by the n-gram model.",
        "The parameters of this model call be easily estimated, and the expression Pr(wkwk_i) can be efficiently computed (Bahl et al., 1983; Jelinek, 1998).",
        "In order to define the stochastic grammatical model Gs of the expression Pr(wkiwi wk-1,Gs) for large-vocabulary complex tasks, we propose a combination of two different stochastic models: a category-based SCFG (G,), that allows us to represent the long-term relations between these syntactical structures and a probabilistic model of word distribution into categories (C).",
        "This proposal introduces two important aspects, which are the estimation of the parameters of the stochastic models, G, and C, and the computation of the following expression:"
      ]
    },
    {
      "heading": "3 Training of the models",
      "text": [
        "The parameters of the described model are estimated from a training sample, that is, from a set of sentences.",
        "Each word of the sentence has a part-of-speech tag (POStag) associated to it.",
        "These POStags are considered as word categories and are the terminal symbols of the SCFG.",
        "From this training sample, the parameters of G, and C can be estimated as follows.",
        "First, the parameters of C, represented by",
        "where N(w, c) is the number of times that the word w has been labeled with the POStag c. It is important to note that a word w can belong to different categories.",
        "In addition, it may happen that a word in a test set does not appear in the training set, and therefore some smoothing technique has to be carried out.",
        "With regard to the estimation of the category-based SCFGs, one of the most widely-known methods is the Inside-Outside (JO) algorithm (Lafi and Young, 1990).",
        "The application of tins algorithm presents important problems which are accentuated in real tasks: the time complexity per iteration and the large number of iterations that are necessary to converge.",
        "An alternative to the JO algorithm is an algorithm based on the Viterbi score (VS algorithm) (prey, 1992).",
        "The convergence of the VS algorithm is faster than the JO algorithm.",
        "However, the SCFGs obtained are, in general, not as well learned (Sanchez et al., 1996).",
        "Another possibility for estimating SCFGs, which is somewhere between the I0 and VS algorithms, has recently been proposed.",
        "This approach considers only a certain subset of derivations in the estimation process.",
        "In order to select tins subset of derivations, two alternatives have been considered: from structural information content in a bracketed corpus (Pereira and Schabes, 1992; Amaya et al., 1999), and from statistical information content in the k-best derivations (Sanchez and Benedi, 1998).",
        "In the first alternative, the 'Oh and VSb algorithms which learn SCFGs from partially bracketed corpora were defined (Pereira and Schabes, 1992; Amaya et al., 1999).",
        "In the second alternative, the kVS algorithm for the estimation of the probability distributions of a SCFG from the k-best derivations was proposed (Sanchez and Benedi, 1998).",
        "All of these algorithms have a time complexity 0(71311)1), where it is the length of the input string, and IP1 is the size of the SCFG.",
        "These algorithms have been tested in real tasks for estimating category-based SCFGs (Sanchez and Benedf, 1999) and the results obtained justify their application in complex real tasks."
      ]
    },
    {
      "heading": "4 Integration of the model",
      "text": [
        "From expression (2), it can bee seen that in order to integrate the model, it is necessary to efficiently compute the expression:",
        "In order to describe how this computation can be made, we first introduce some notation.",
        "(only grammars with non empty rules are considered).",
        "For simplicity (but without loss of generality) only context-free grammars in Chomsky Normal F01711 are considered, that is, grammars with rules of the form A > BC or A > 7) where A, B,C EN and)1.",
        "A Stochastic Context-Pim Grammar Cs is a pair (G, p), where G is a context-free grammar and p : P 11 is a probability function of rule application such that VA E N: >--/GE(NuY,p- P(A(1) = 1.",
        "Now, we present; two algorithms in order to compute the word transition probability.",
        "The first algorithm is based on the Mil algorithm, and the second is based on an application of a Viterbi scheme to the algorithm (the VLIII algorithm).",
        "Probability of generating an initial substring The computation of (4) is based on an algorithm.",
        "which is a modification of the 1,111 algorithm (Jelinek and Lafferty, 1.991).",
        "This new algorithm is based on the definition of Pr(A <<",
        "probability that A generates the initial substring vwi.",
        ".",
        ".",
        "111i .",
        ".",
        ".",
        "given G, and Cw.",
        "This can be computed with the following dynamic programming scheme:",
        "In this expression, Q(A D) is the probability that D is the leftmost nonterminal in all sentential forms which are derived from A.",
        "The value Q(A13C) is the probability that 13C is the initial substring of all sentential forms derived from A. Pr(B <>) is the probability that; the substring .",
        ".",
        ".",
        "w1 is generated from B given G and C,.",
        "Its computation will be defined later.",
        "It; should be noted that the combination of the models G, and C, is carried out in the value Pr(A << I, i).",
        "This is the main difference with respect the HU algorithm.",
        "Probability of the best derivation generating an initial substring An algorithm which is similar to the previous one can be defined based on the Viterbi scheme.",
        "In this way, it is possible to obtain the best parsing of an initial substring.",
        "This new algorithm is also related to the VLI1,1 algorithm (Sanchez and 13enedi, 1997) and is based on the definition of Pr(A < < j))Pr(A... Wi ... 1G, c) as the probability of the most probable parsing which generates 7/),;.",
        ".",
        ".",
        "from A given G and C,,,.",
        "This can be computed as follows:",
        "the most probable sentential form which is derived from A.",
        "The value (2(A BC) is the probability that BC is the initial substring of most the probable sentential form derived from A. Pr(B < i, 1 >) is the probability of the most probable parse which generates wifrom B. Probability of generating a string",
        "that the substring wj is generated from A given G c, and Cw.",
        "To calculate this probability a modification of the well-known Inside algorithm (Laid and Young, 1990) is proposed.",
        "Tins computation is carried out by using the following dynamic programming scheme:",
        "In tins way, Pr(wlIG c,Pr(S < 1,77 >).",
        "As we have commented above, the combination of the two parts of the grammatical model is carried out in the value Pr(A <>).",
        "Probability of the best derivation generating a string The probability of the best derivation that generates a string, Pr(wi wIG, Cw), can be evaluated using a Viterbi-like scheme (Ney, 1992).",
        "As in the previous case, the computation of tins probability is based on the definition of",
        "the probability of the best derivation that generates the substringwj from A given Ge and Cw.",
        "Similarly:",
        "Finally, the time complexity of these algorithms is the same as the algorithms they are related to, therefore the time complexity is 0(k31PD, where k is the length of the input string and I P1 is the size of the SCFG."
      ]
    },
    {
      "heading": "5 Experiments with the Penn",
      "text": []
    },
    {
      "heading": "Treebank Corpus",
      "text": [
        "The corpus used in the experiments was the part of the Wall Street Journal which had been processed in the Penn Treebank project' (Marcus et al., 1993).",
        "This corpus consists of English texts collected from the Wall Street Journal from editions of the late eighties.",
        "It contains approximately one million words.",
        "This corpus was automatically labelled, analyzed and manually checked as described in (Marcus et al., 1993).",
        "There are two kinds of labelling: a POStag labelling and a syntactic labelling.",
        "The size of the vocabulary is greater than 25,000 different words, the POStag vocabulary is composed of.",
        "45 labels2 and the syntactic vocabulary is composed of 14 labels.",
        "The corpus was divided into sentences according to the bracketing.",
        "In this way, we obtained a corpus whose main characteristics are shown in Table 1.",
        "We took advantage of the category-based SCFGs estimated in a previous work (Sanchez and Benedi, 1998).",
        "These SCFGs were estimated with sentences which had less than 15 words.",
        "Therefore, in this work, we assumed such restriction.",
        "The vocabulary size of the new corpus was 6,333 different words.",
        "For the experiments, the corpus was divided into a training corpus (directories 00 to 19) and a test corpus (directories 20 to 24).",
        "The characteristics of these sets can be seen in Table 2.",
        "The part of the",
        "corpus labeled with POStags was used to estimate the parameters of the grammatical model, while the non-labeled part was used to estimate the parameters of the n-gramOdd.. We now describe the estimation process in detail.",
        "for the experiments when the sentences with more than 15 POStags were removed.",
        "DataNo.",
        "ofAv.Std.",
        "The parameters of a 3-gram model were estimated with the software tool described in (Rosenfeld, 1.995) .",
        "We used the linear interpolation smooth technique supported by this tool.",
        "The out-of-vocabulary words were grouped in the same class and were used in the computation of the perplexity.",
        "The test set perplexity with this model was 180.4.",
        "The values of expression (3) were computed from the tagged and non-tagged part of the training corpus.",
        "In order to avoid null values, the unseen events were labeled with a special symbol in' which did not appear in the vocabulary, in such a way that Pr(wilc) 0, Vc C C, where C was the set of categories.",
        "That is, all the categories could generate die unseen event.",
        "This probability took a very small value (several orders of magnitude less than minwev-,,;(47 Pr (1n10 where V was the vocabulary of the training corpus), and different values of this probability (lid not change the results.",
        "The parameters of an initial ergodic SCFG were estimated with each one of the estimation methods mentioned in Section 3.",
        "This SCFG had 3,374 rules, composed from 45 terminal symbols (the, number of POStags) and 1.4 non-terminal symbols (the number of syntactic labels).",
        "The probabilities were randomly generated and three different seeds were tested, but only one of them is reported given that the results were very similar.",
        "The training corpus was the labeled part of the described corpus.",
        "The perplexity of the labeled part of the test set for",
        "test set with the SCFC: estimated with the methods mentioned in Section 3.",
        "VS k VS IOb VSb 21.56 20.65 13.14 21.84 Once we had estimated the parameters of the defined model, we applied expression (1) by using the BRI algorithm and the 'URI algorithm in expression (4).",
        "The test set perplexity that was obtained in function of (1 for different estimation algorithms (VS, 10b and VS1m) can be seen in Fig. 1.",
        "In the best case, the proposed language model obtained more than a.",
        "30% improvement over results obtained by the 3-gram language model (see Table 4).",
        "This result was obtained when the SCRE4 estimated with the 1.01) algorithm was used.",
        "The SCFGs estimated with other algorithms also obtained important improvements compared to the 3-gram.",
        "In addition, it can be observed that both the LRI algorithm and the VLIII algorithm obtained good results.",
        "'cable 4: Best test perplexity for different SC1'C1 estimation algorithms, and the percentage of improvement with respect to the 3-gram model.",
        "An important aspect to note is that the weight of the grammatical part was approximately 50%, which means that this part provided important information to the language model."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "A new language model has been introduced.",
        "Timis new language model is defined as a linear combination of an n-gram which represents relations between words, and a. stochastic",
        "the proposed language models in function of gamma.",
        "Different curves correspond to SCFGs estimated with different algorithms.",
        "The upper graphic corresponds to the results obtained when the MI algorithm was used in the language models, and the lower graphic corresponds to the results obtained with the VLRI algorithm.",
        "grammatical model which is used to represent the global relation between syntactic structures.",
        "The stochastic grammatical model is composed of a category-based SCFG and a probabilistic model of word distribution in the categories.",
        "Several algorithms have been described to estimate the parameters of the model from a the sample.",
        "In addition, efficient algorithms for solving the problem of the interpretation with this model have been presented.",
        "The proposed model has been tested on the part of Wall Street Journal processed in the Penn Treebank project, and the results obtained improved by more than 30% the test set perplexity over results obtained by a simple 3-gram model."
      ]
    }
  ]
}
