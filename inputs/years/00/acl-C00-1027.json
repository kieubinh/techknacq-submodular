{
  "info": {
    "authors": [
      "Kenneth Ward Church"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1027",
    "title": "Empirical Estimates of Adaptation: The Chance of Two Noriegas Is Closer to P/2 Than P2",
    "url": "https://aclweb.org/anthology/C00-1027",
    "year": 2000
  },
  "references": [
    "acl-P99-1022"
  ],
  "sections": [
    {
      "text": [
        "whereD=a+b+c+d.",
        "Positive adaptation tends to be much larger than the prior, which is just a little larger than negative adaptation, as illustrated in the table below for the word \"hostages\" in four years of the Associated Press (AP) newswire.",
        "We find remarkably consistent results when we compare one year of the AP news to another (though topics do come and go over time).",
        "Generally, the differences of interest are huge (orders of magnitude) compared to the differences among various control conditions (at most factors of two or three).",
        "Note that values are more similar within columns than across columns.",
        "We find that some words adapt more than others, and that words that adapt more in one year of the AP also tend to adapt more in another year of the AP.",
        "In general, words that adapt a lot tend to have more content (e.g., good keywords for information retrieval (IR)) and words that adapt less have less content (e.g., function words).",
        "It is often assumed that word frequency is a good (inverse) correlate of content.",
        "In the psycholinguistic literature, the term \"high frequency\" is often used synonymously with \"function words,\" and \"low frequency\" with \"content words.\" In IR, inverse document frequency (IDF) is commonly used for weighting keywords.",
        "The table below is interesting because it questions this very basic assumption.",
        "We compare two words, \"Kennedy\" and \"except,\" that are about equally frequent (similar priors).",
        "Intuitively, \"Kennedy\" is a content word and \"except\" is not.",
        "This intuition is supported by the adaptation statistics: the adaptation ratio, Pr(+ adapt)/ Pr(prior), is much larger for \"Kennedy\" than for \"except.\" A similar pattern holds for negative adaptation, but in the reverse direction.",
        "That is, P r(- adapt)/ Pr(prior) is much smaller for \"Kennedy\" than for \"except.\" Kennedy adapts more than except",
        "In general, we expect more adaptation for better keywords (e.g., \"Kennedy\") and less adaptation for less good keywords (e.g., function words such as \"except\").",
        "This observation runs counter to the standard practice of weighting keywords solely on the basis of frequency, without considering adaptation.",
        "In a related paper, Umemura and Church (submitted), we describe a term weighting method that makes use of adaptation (sometimes referred to as burstiness).",
        "Distinctive surnames adapt more",
        "The table above compares surnames with first names.",
        "These surnames are excellent keywords unlike the first names, which are nearly as useless for IR as function words.",
        "The adaptation ratio, Pr(+ adapt)/Pr(prior), is much larger for the surnames than for the first names.",
        "What is the probability of seeing two Noriegas in a document?",
        "The chance of the first one is pz0.006.",
        "According to the table above, the chance of two is about 0.75p, closer to p/2 than p2.",
        "Finding a rare word like Noriega in a document is like lightning.",
        "We might not expect",
        "lightning to strike twice, but it happens all the time, especially for good keywords.",
        "4.",
        "Smoothing (for low frequency words) Thus far, we have seen that adaptation can be large, but to demonstrate the shape property (lack of dependence on frequency), the counts in the contingency table need to be smoothed.",
        "The problem is that the estimates of a, b, c, d, and especially estimates of the ratios of these quantities, become unstable when the counts are small.",
        "The standard methods of smoothing in the speech recognition literature are Good-Turing (GT) and Held-Out (HO), described in sections 15.3 & 15.4 of Jelinek (1997).",
        "In both cases, we let r be an observed count of an object (e.g., the frequency of a word and/or ngram), and r* be our best estimate of r in another corpus of the same size (all other things being equal)."
      ]
    },
    {
      "heading": "4.1 Standard Held-Out (HO)",
      "text": [
        "HO splits the training corpus into two halves.",
        "The first half is used to count r for all objects of interest (e.g., the frequency of all words in vocabulary).",
        "These counts are then used to group objects into bins.",
        "The rlh bin contains all (and only) the words with count r. For each bin, we compute N1, the number of words in the rill bin.",
        "The second half of the training corpus is then used to compute Cr, the aggregate frequency of all the words in the r')' bin.",
        "The final result is simply: r*= C,./N,.",
        "If the two halves of the training corpora or the test corpora have different sizes, then r* should be scaled appropriately.",
        "We chose HO in this work because it makes few assumptions.",
        "There is no parametric model.",
        "All that is assumed is that the two halves of the training corpus are similar, and that both are similar to the testing corpus.",
        "Even this assumption is a matter of some concern, since major-stories come and go over time."
      ]
    },
    {
      "heading": "4.2 Application of HO to Contingency Tables",
      "text": [
        "As above, the training corpus is split into two halves.",
        "We used two different years of AP news.",
        "The first half is used to count document frequency dr (Document frequency will be used instead of standard (term) frequency.)",
        "Words are binned by df and by their cell in the contingency table.",
        "The first half of the corpus is used to compute the number of words in each bin: N ,, N J A I di.",
        "and A di.",
        ",d; the second half of the corpus is used to compute the aggregate document frequency for the words in each bin: Cdf,a, C df .1 Cand C 4.",
        "The final result is sim-ply:aC",
        "compute the probabilities as before, but replace a, b, c, d with a 1(, b*, c *, d*, respectively.",
        "History (h) Neighborhood (n) Prior (p) Document Frequency, 01) With these smoothed estimates, we are able to show that Pr(+ adapt), labeled 11 in the plot above, is larger and less dependent on frequency than Pr(prior), labeled p. The plot shows a third group, labeled n for neighbors, which will be described later.",
        "Note that the ns fall between the ps and the Its.",
        "Thus far, we have seen that adaptation can be huge: Pr(+ adapt)>> Pr(prior), often by two or three orders of magnitude.",
        "Perhaps even more surprisingly, although the first mention depends strongly on frequency (df), the second does not.",
        "Sonic words adapt more (e.g., Noriega, Aristide, Escobar) and some words adapt less (e.g.lohn, George, Paul).",
        "The results are robust.",
        "Words that adapt more in one year of AP news tend to adapt more in another year, and vice versa.",
        "5.",
        "Method 2: Pr(+ adapt?)",
        "So far, we have limited our attention to the relatively simple case where the history and the test arc the same size.",
        "In practice, this won't be the case.",
        "We were concerned that the observations above might be artifacts somehow caused by this limitation.",
        "We experimented with two approaches for understanding the effect of this limitation and found that the size of the history doesn't change Pr(+ adapt) very much.",
        "The first approach split the history and the test at various points ranging from 5% to 95%.",
        "Generally, Pr(+ adapt)) increases as the size of the test portion grows relative to the size of the history, but the effect is",
        "relatively small (more like a factor of two than an order of magnitude).",
        "We were even more convinced by the second approach, which uses Pr(+adapt2), a completely different argument for estimating adaptation and doesn't depend on the relative size of the history and the test.",
        "The two methods produce remarkably similar results, usually well within a factor of two of one another (even when adapted probabilities are orders of magnitude larger than the prior).",
        "Pr(+ adapt 2) makes use of clk (w), a generalization of document frequency.",
        "df (w) is the number of documents with j or more instances of w; (cif r is the standard notion of dn.",
        "Pr(+ adapt 2 ) P r(k 2 I k1) = df2/dfMethod 2 has some advantages and some disadvantages in comparison with method 1.",
        "On the positive side, method 2 can be generalized to compute the chance of a third instance: Pr(k311(2).",
        "But unfortunately, we do not know how to use method 2 to estimate negative adaptation; we leave that as an open question.",
        "Adaptation is huge (and hardly dependent on frequency)"
      ]
    },
    {
      "heading": "Document Frequency (di)",
      "text": [
        "The plot (above) is similar to the plot in section 4.2 which showed that adapted probabilities (labeled h) are larger and less dependent on frequency than the prior (labeled p).",
        "So too, the plot (above) shows that the second and third mentions of a word (labeled 2 and 3, respectively) are larger and less dependent on frequency than the first mention (labeled 1).",
        "The plot in section 4.2 used method 1 whereas the plot (above) uses method 2.",
        "Both plots use the HO smoothing, so there is only one point per bin (df value), rather than one per word."
      ]
    },
    {
      "heading": "6. Neighborhoods (Near)",
      "text": [
        "Florian and Yarowsky's example, \"It is at least on the Serb side a real setback to the x,\" provides a nice motivation for neighborhoods.",
        "Suppose the context (history) mentions a number of words related to a peace process, but doesn't mention the word \"peace.\" Intuitively, there should still be some adaptation.",
        "That is, the probability of \"peace\" should go up quite a bit (positive adaptation), and the probability of many other words such as \"piece\" should go down a little (negative adaptation).",
        "We start by partitioning the vocabulary into three exhaustive and mutually exclusive sets: hist, near and other (abbreviations for history, neighborhood and otherwise, respectively).",
        "The first set, hist, contains the words that appear in the first half of the document, as before.",
        "Other is a catchall for the words that are in neither of the first two sets.",
        "The interesting set is near.",
        "It is generated by query expansion.",
        "The history is treated as a query in an information retrieval document-ranking engine.",
        "(We implemented our own ranking engine using simple IDF weighting.)",
        "The neighborhood is the set of words that appear in the or lc---100 top documents returned by the retrieval engine.",
        "To ensure that the three sets partition the vocabulary, we exclude the history from the neighborhood: near = words in query expansion of hist hist The adaptation probabilities are estimated using a contingency table like before, but we now have a three-way partition (hist, near and other) of the vocabulary instead of the two-way partition, as illustrated below.",
        "Documents containing \"peace\" in 1991 AP",
        "In estimating adaptation probabilities, we continue to use a, b, c and d as before, but four new variables are introduced: e, f, g and h, where",
        "The table below shows that \"Kennedy\" adapts more than \"except\" and that \"peace\" adapts more than \"piece.\" That is, \"Kennedy\" has a larger spread than \"except\" between the history and the otherwise case.",
        "used to group words into bins by all Adaptation probabilities are computed for each bin, rather than for each word.",
        "Since these probabilities are implicitly conditional on df, they have already been weighted by dlf in some sense, and therefore, it is unnecessary to introduce an additional explicit weighting scheme based on df or a simple transform thereof such as I Dl The experiments below split the neighborhood into four classes, ranging from better neighbors to worse neighbors, depending On expansion frequency, ell 4(0 is a number between 1 and k, indicating how many of the k lop scoring documents contain t. (Better neighbors appear in more of the top scoring documents, and worse neighbors appear in fewer.)",
        "All the neighborhood classes fall between hist and other, with better neighbors adapting more than worse neighbors."
      ]
    },
    {
      "heading": "7. Experimental Results",
      "text": [
        "Recall that the task is to predict the test portion (the second half) of a document given the history (the first half).",
        "The following table shows a selection of words (sorted by the third column) from the test portion of one of the test documents.",
        "The table is separated into thirds by horizontal lines.",
        "The words in the top third receive much higher scores by the proposed method (S) than by a baseline (13).",
        "These words are such good keywords that one can fairly confidently guess what the story is about.",
        "Most of these words receive a high score because they were mentioned in the history portion of the document, but \"laid-off\" receives a high score by the neighborhood mechanism.",
        "Although \"laid-off\" is not mentioned explicitly in the history, it is obviously closely related to a number of words that were, especially \"layoffs,\" but also \"notices\" and \"cuts.\" It is reassuring to see the neighborhood mechanism doing what it was designed to do.",
        "The middle third shows words whose scores are about the same as the baseline.",
        "These words tend to be function words and other low content words that give us little sense of what the document is about.",
        "The bottom third contains words whose scores are much lower than the baseline.",
        "These words tend to be high in content, but misleading.",
        "The word \"arms,\" for example, might suggest that story is about a military conflict.",
        "if we near]",
        "where near] through near4 are four neighborhoods (k =100).",
        "Words in near4 are the best neighbors (ef10) and words in near I are the worst neighbors (er= 1).",
        "The baseline, B, shown in column 2, is: PrB(w)=4f/D.",
        "Column 3 compares the first two columns.",
        "We applied this procedure to a year of the AP news and found a sizable gain in information on",
        "average: 0.75 bits per word type per document.",
        "In addition, there were many more big winners (20% of the documents gained 1 bit/type) than big losers (0% lost 1 bit/type).",
        "The largest winners include lists of major cities and their temperatures, lists of major currencies and their prices, and lists of commodities and their prices.",
        "Neighborhoods are quite successful in guessing the second half of such lists.",
        "On the other hand, there were a few big losers, e.g., articles that summarize the major stories of the day, week and year.",
        "The second half of a summary article is almost never about the same subject as the first half.",
        "There were also a few end-of-document delimiters that were garbled in transmission causing two different documents to be treated as if they were one.",
        "These garbled documents tended to cause trouble for the proposed method; in such cases, the history comes from one document and the test comes from another.",
        "In general, the proposed adaptation method performed well when the history is helpful for predicting the test portion of the document, and it performed poorly when the history is misleading.",
        "This suggests that we ought to measure topic shifts using methods suggested by Hearst (1994) and Florian & Yarowsky (1999).",
        "We should not use the history when we believe that there has been a major topic shift."
      ]
    },
    {
      "heading": "8. Conclusions",
      "text": [
        "Adaptive language models were introduced to account for repetition.",
        "It is well known that the second instance of a word (or ngram) is much more likely than the first.",
        "But what we find surprising is just how large the effect is.",
        "The chance of two Noriegas is closer to p/2 than p2.",
        "In addition to the magnitude of adaptation, we were also surprised by the shape: while the first instance of a word depends very strongly on frequency, the second does not.",
        "Adaptation depends more on content than frequency; adaptation is stronger for content words such as proper nouns, technical terminology and good keywords for information retrieval, and weaker for function words, cliches and first names.",
        "The shape and magnitude of adaptation has implications for psycholinguistics, information retrieval and language modeling.",
        "Psycholinguistics has tended to equate word frequency with content, but our results suggest that two words with similar frequency (e.g., \"Kennedy\" and \"except\") can be distinguished on the basis of their adaptation.",
        "Information retrieval has tended to use frequency in a similar way, weighting terms by IDF (inverse document frequency), with little attention paid to adaptation.",
        "We propose a term weighting method that makes use of adaptation (burstiness) and expansion frequency in a related paper (Umemura and Church, submitted).",
        "Two estimation methods were introduced to demonstrate the magnitude and shape of adaptation.",
        "Both methods produce similar results.",
        "Neighborhoods were then introduced for words such as \"laid-off\" that were not in the history but were close (\"laid-off\" is related to \"layoff,\" which was in the history).",
        "Neighborhoods were defined in terms of query expansion.",
        "The history is treated as a query in an information retrieval document-ranking system.",
        "Words in the k top-ranking documents (but not in the history) are called neighbors.",
        "Neighbors adapt more than other terms, but not as much as words that actually appeared in the history.",
        "Better neighbors (larger el) adapt more than worse neighbors (smaller e/)."
      ]
    },
    {
      "heading": "References",
      "text": [
        "The table below shows that \"Kennedy\" adapts more than \"except\" and that \"peace\" adapts more than \"piece.\" That is, \"Kennedy\" has a larger spread than \"except\" between the history and the otherwise case.",
        "used to group words into bins by all Adaptation probabilities are computed for each bin, rather than for each word.",
        "Since these probabilities are implicitly conditional on df, they have already been weighted by dlf in some sense, and therefore, it is unnecessary to introduce an additional explicit weighting scheme based on df or a simple transform thereof such as I Dl The experiments below split the neighborhood into four classes, ranging from better neighbors to worse neighbors, depending On expansion frequency, ell 4(0 is a number between 1 and k, indicating how many of the k lop scoring documents contain t. (Better neighbors appear in more of the top scoring documents, and worse neighbors appear in fewer.)",
        "All the neighborhood classes fall between hist and other, with better neighbors adapting more than worse neighbors."
      ]
    },
    {
      "heading": "7. Experimental Results",
      "text": [
        "Recall that the task is to predict the test portion (the second half) of a document given the history (the first half).",
        "The following table shows a selection of words (sorted by the third column) from the test portion of one of the test documents.",
        "The table is separated into thirds by horizontal lines.",
        "The words in the top third receive much higher scores by the proposed method (S) than by a baseline (13).",
        "These words are such good keywords that one can fairly confidently guess what the story is about.",
        "Most of these words receive a high score because they were mentioned in the history portion of the document, but \"laid-off\" receives a high score by the neighborhood mechanism.",
        "Although \"laid-off\" is not mentioned explicitly in the history, it is obviously closely related to a number of words that were, especially \"layoffs,\" but also \"notices\" and \"cuts.\" It is reassuring to see the neighborhood mechanism doing what it was designed to do.",
        "The middle third shows words whose scores are about the same as the baseline.",
        "These words tend to be function words and other low content words that give us little sense of what the document is about.",
        "The bottom third contains words whose scores are much lower than the baseline.",
        "These words tend to be high in content, but misleading.",
        "The word \"arms,\" for example, might suggest that story is about a military conflict.",
        "if we near]",
        "where near] through near4 are four neighborhoods (k =100).",
        "Words in near4 are the best neighbors (ef10) and words in near I are the worst neighbors (er= 1).",
        "The baseline, B, shown in column 2, is: PrB(w)=4f/D.",
        "Column 3 compares the first two columns.",
        "We applied this procedure to a year of the AP news and found a sizable gain in information on",
        "average: 0.75 bits per word type per document.",
        "In addition, there were many more big winners (20% of the documents gained 1 bit/type) than big losers (0% lost 1 bit/type).",
        "The largest winners include lists of major cities and their temperatures, lists of major currencies and their prices, and lists of commodities and their prices.",
        "Neighborhoods are quite successful in guessing the second half of such lists.",
        "On the other hand, there were a few big losers, e.g., articles that summarize the major stories of the day, week and year.",
        "The second half of a summary article is almost never about the same subject as the first half.",
        "There were also a few end-of-document delimiters that were garbled in transmission causing two different documents to be treated as if they were one.",
        "These garbled documents tended to cause trouble for the proposed method; in such cases, the history comes from one document and the test comes from another.",
        "In general, the proposed adaptation method performed well when the history is helpful for predicting the test portion of the document, and it performed poorly when the history is misleading.",
        "This suggests that we ought to measure topic shifts using methods suggested by Hearst (1994) and Florian & Yarowsky (1999).",
        "We should not use the history when we believe that there has been a major topic shift."
      ]
    },
    {
      "heading": "8. Conclusions",
      "text": [
        "Adaptive language models were introduced to account for repetition.",
        "It is well known that the second instance of a word (or ngram) is much more likely than the first.",
        "But what we find surprising is just how large the effect is.",
        "The chance of two Noriegas is closer to p/2 than p2.",
        "In addition to the magnitude of adaptation, we were also surprised by the shape: while the first instance of a word depends very strongly on frequency, the second does not.",
        "Adaptation depends more on content than frequency; adaptation is stronger for content words such as proper nouns, technical terminology and good keywords for information retrieval, and weaker for function words, cliches and first names.",
        "The shape and magnitude of adaptation has implications for psycholinguistics, information retrieval and language modeling.",
        "Psycholinguistics has tended to equate word frequency with content, but our results suggest that two words with similar frequency (e.g., \"Kennedy\" and \"except\") can be distinguished on the basis of their adaptation.",
        "Information retrieval has tended to use frequency in a similar way, weighting terms by IDF (inverse document frequency), with little attention paid to adaptation.",
        "We propose a term weighting method that makes use of adaptation (burstiness) and expansion frequency in a related paper (Umemura and Church, submitted).",
        "Two estimation methods were introduced to demonstrate the magnitude and shape of adaptation.",
        "Both methods produce similar results.",
        "Neighborhoods were then introduced for words such as \"laid-off\" that were not in the history but were close (\"laid-off\" is related to \"layoff,\" which was in the history).",
        "Neighborhoods were defined in terms of query expansion.",
        "The history is treated as a query in an information retrieval document-ranking system.",
        "Words in the k top-ranking documents (but not in the history) are called neighbors.",
        "Neighbors adapt more than other terms, but not as much as words that actually appeared in the history.",
        "Better neighbors (larger el) adapt more than worse neighbors (smaller e/)."
      ]
    }
  ]
}
