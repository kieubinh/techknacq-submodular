{
  "info": {
    "authors": [
      "Stephen Clark",
      "David Weir"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1029",
    "title": "A Class-Based Probabilistic Approach to Structural Disambiguation",
    "url": "https://aclweb.org/anthology/C00-1029",
    "year": 2000
  },
  "references": [
    "acl-C92-2070",
    "acl-C94-2195",
    "acl-E95-1016",
    "acl-H94-1048",
    "acl-J93-1003",
    "acl-J93-1005",
    "acl-J98-2002",
    "acl-P96-1025",
    "acl-P97-1003",
    "acl-P97-1056",
    "acl-P98-2177",
    "acl-W95-0103",
    "acl-W97-0109",
    "acl-W99-0631"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Knowledge of which words are able to fill particular argument slots of a predicate can be used for structural disambiguation.",
        "This paper describes a proposal for acquiring such knowledge, and in line with much of the recent work in this area, a probabilistic approach is taken.",
        "We develop a novel way of using a semantic hierarchy to estimate the probabilities, and d.emon-strafe the general approach using a prepositional phrase attachment experiment."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Knowledge of which words are able to fill particular argument slots of a predicate can be used for structural disambiguation.",
        "In the .following example (Charniak, 1993), the fact that dog, rather than prize, is often the subject of run, can be used to decide on the attachment site of the relative clause: Fred awarded a prize for the dog that ran the fastest We describe a proposal for acquiring such knowledge, and as in other recent work in this area (Resnik, 1993; Li and Abe, 1998), a probabilistic approach is taken.",
        "Using probabilities accords with the intuition that there are no absolute constraints on the arguments of predicates, but rather that constraints are satisfied to a certain degree (Resnik, 1993).",
        "Unfortunately, defining probabilities in terms of words leads to a model with a vast number of parameters, resulting in a sparse data problem.",
        "To overcome this, we propose to define a probability model in terms of senses from a semantic hierarchy, exploiting the fact that senses of nouns can be grouped together into semantically similar classes.",
        "We use the semantic hierarchy of noun senses in WordNet (Fellbaum, 1998), which consists of `lexicalised concepts' related by the 'is-a-kindof' relation.",
        "If c' is a kind of c, then c is a hypernym of c' and e' a hyponym of e. Counts are passed up the hierarchy from the senses of nouns appearing in the data.",
        "Thus if eat chicken appears in the data, the count for this item passes up to (meat), (food), and all the other hyper-urns of that sense of chicken) in.",
        "order to estimate the probability that a sense of chicken appears as the object of the verb cat, we represent (chicken) using a suitable hypernyin, such as (food), and base our probability estimate on that instead..",
        "The level at which (chicken) is represented is crucial: it should be high enough for adequate counts to have accumulated., but not too high so that the hypernym is no longer representative of (chicken).",
        "An.",
        "example of a hype:11qm which would be too high is (entity), as not all entities are semantically similar with respect to the object position of eat.",
        "The problem.",
        "of ch.00sing an appropriate level in the hierarchy at which to represent a particular noun.",
        "sense (given.",
        "a predicate and argument position) has been.",
        "investigated by Resnik (1993), Li and Abe (1998) and Ribas (1995).",
        "The learning mechanism presented here is a novel approach.",
        "based on finding semantically similar sets of concepts in a hierarchy.",
        "We demonstrate the effectiveness of our approach using a PP-attachment experiment."
      ]
    },
    {
      "heading": "2 The Input Data and Semantic Hierarchy",
      "text": [
        "The data used to estimate the probabilities is a multiset of 'co-occurrence triples': a noun.",
        "lemma, verb lemma, and argument position.2 Let the universe of verbs, argument positions and nouns that can appear in the input data be denoted V = • • • , vkv = { , } and Al = 7/1 nkA, I, respectively.",
        "Such data can be obtained from a treebank, Or front a shallow parser.",
        "Note that we do not distinguish between alternative senses of verbs, and assume that each instance of a noun in the data refers to exactly on e concept.",
        "The semantic hierarchy used is the noun hypernym taxonomy of Word Net (version 1..6).3 Let C = { } be the set of concepts in WordNet (k c 66,000).",
        "A concept is represented in Word Net by a synset: a set of synonymous words which can be used to denote that concept.",
        "For example, the concept 'cocaine', as in the drug, is represented by the following synset: {cocaine, cocain, coke, snow, C} Let syn(c) C Al be the synset for the concept c, and let cn(n) = {e In E syn(e) be the set of concepts that can be denoted by the noun n. The hierarchy has the structure of a directed acyclic graph, although the nu m her of nodes in the graph with more than one parent is only around One percent of the total.",
        "'Flie edges in the graph form what we call the direct-isa relation (direct-isa C C x C).",
        "Let isa = direct-isa* be the transitive, .rellexive closure of direct-isa, so that (c', c.) c isa e is a Itypernym of e'; and let e = { c' I (c', c) isa } be the set consisting of the concept c and all of its hyponyms.",
        "Thus, the set (food) contains all the concepts which are kinds of food, including (food).",
        "Note that words in the data can appear in synsets anywhere in the hierarchy.",
        "F.xen concepts such as (entity), which appear near the root of the hierarchy, have synsets containing words which may appear hi the data.",
        "The synset for (entity) is {entity, something} , and the words entity and something can appear in the argument positions of verbs in the data."
      ]
    },
    {
      "heading": "3 Probability Estimation",
      "text": [
        "The problem being addressed in this section is to estimate p(clv, , r), for c EC,vE V, and",
        "r E The probability p(c11 r) is the probability that sonic noun in syn(c), when denoting con Cept c, appears in position 7 of verb v (given r and v).",
        "Using the relative clause example from the introduction, the probabilities (clog)I run , subj) and p((prize)I run, subj) can be compared to decide on the attachment site in Fred awarded a prize for the dog that ran the fastest.",
        "We expect p((dog)Irun,subj) to be greater than p( (prize) I run, subj).",
        "Although the focus is on P(ely, v), the techniques described here can be used to estimate other probabilities, such as p(c,r1v).",
        "(In fact, the latter probability is used in the PP-attachment experiments described in Section Using maximum likelihood to estimate p(clv, r) is not viable because of the huge number of parameters involved.",
        "Many combinations of e, v and n will not occur in the data.",
        "To reduce the MI other of parameters which need to be estimated, we utilise the fact that concepts can be grouped into classes, and represent c using a class e', for some hypernym e' of c. Ilowever, p(c'l , 7.)",
        "cannot he used as an estimate of p(cir , r), as p(elv, r) is given by the following: p( iv 7.)",
        "p(e17,, 7.)",
        "The probability p(cil v , r) in.creases as e' moves up the hierarchy.",
        "For example, p((foo0 eat, obi) is not a good estimate of p((chicken)I cat, \\Vital; can be done though., is to condition on sets of concepts, and use the probability p(vic' , r).",
        "If it can.",
        "be shown that p( ii 7.",
        "), for some hypernym c' of c, is a reasonable estimate of p(vIc,r), then we have a way of estimating r).",
        "To get p(v c, r) from p(clv,r) Bayes rule is used:",
        "The probabilities WIT) and p(vin) can be estimated using maximum likelihood estimates, as the conditioning event is likely to OCCUT often enough for sparse data not to be a problem.",
        "(Alternatively one could back-off to p(c) and p(v) respectively, or use a linear combination of p(clr) amid p(c), and p(vIr) and p(v), respectively.)",
        "The formulae for these estimates will be given shortly.",
        "This only leaves p(vle,r).",
        "The",
        "proposal is to estimate p(eal I (chicken), obj) using p(catl(f ood) , oh j), or something similar.",
        "The following proposition shows that if p(vic\", r) is the same for each c\" in c', where c' is some hypernym of c, then p(vie, r) will be equal to p(vjc, r):",
        "So in order to estimate p(vic, r), we need a way of searching for a set c', where ci is a hypernym of c, which consists of concepts c\" which have similar p(vIc\" , r) .",
        "Of course we cannot expect to find a set consisting of concepts which have identical p(vi c\" r), which the proposition strictly requires, but if the p(vIc\" , r) are similar, then we can expect /Ole, r) to be a reasonable estimate of p(vic, r).",
        "We refer to the set c' as the similarity-class' of c, and the suitable hypernym, c', as top(c, v, r).",
        "The next section explains how we determine similarity classes.",
        "The maximum likelihood estimates for the relevant probabilities are given in Table 1.4"
      ]
    },
    {
      "heading": "4 Finding Similarity-classes",
      "text": [
        "First we explain how we determine if a set of concepts has similar p(v1c\", r) for each concept c\" in.",
        "the set.",
        "Then we explain how we determine top(c, v, r).",
        "4Since we are assuming the data is not sense disambiguated, freq(c, v, r) cannot be obtained by simply counting senses.",
        "The standard approach, which is adopted here, is to estimate freq(c,v, r) by distributing the count for each noun n in syn(c) evenly among all senses of the noun.",
        "Yarowsky (1992) and Resnik (1993) explain how the noise introduced by this technique tends to dissipate as counts are passed up the hierarchy.",
        "The method used for comparing the P(v r) for c\" in some set c', is based on the technique in Clark and Weir (1999) used for finding homogeneous sets of concepts in the WordNet noun hierarchy.",
        "Rather than directly compare estimates of p(v1c\", r), which are likely to be unreliable, we consider the children of , and use estimates based on counts which have accumulated at the children.",
        "If c' has children e, c, , we compare p(victi, r) for each i.",
        "This is an approximation, but if the p(vic'i, r) are similar, then we assume that the p(vle”, v) for c\" in c' are similar too.",
        "To determine whether the children of some hypernym c' have similar p(viej), where c/i is the ithi child, we apply a x2 test to a contingency table of frequency counts.",
        "Table 2 shows some example frequencies for c' equal to (nutriment), in the object position of eat.",
        "The figures in brackets are the expected values, based on the marginal totals in the table.",
        "The null hypothesis of the test is that p(vic'i, r) is the same for each i .",
        "For Table 2 the null hypothesis is that for every child, c, of (nutriment), the probability Wale' obj) is the same.",
        "The log-likelihood x2 statistic corresponding to Table 2 is 4.8.",
        "The log-likelihood x2 statistic is used rather than the Pearson's x2 statistic because it is thought to be more appropriate when the counts in the contingency table are low (Dunning, 1993).",
        "This tends to occur when the test is being applied to a set of concepts near the foot of the hierarchy.5 We compared \"Fisher's exact test could be used for tables with low counts, but we do not do so because tables dominated by low counts are likely to have a high percentage of noise, due to the way counts for a noun are split among",
        "the performance of log-likelihood X2 and Pearson's X2 using the PP-attachment experiment described in Section 5.",
        "It was found that the log-likelihood X2 test did perform slightly better.",
        "For a significance level of 0.05 (which is the level used in the experiments), with /I degrees of freedom, the critical value is 11.71.86 (Rowell, 1997).",
        "Thus in this case, the null hypothesis would not be rejected.",
        "In order to determine top(c, v, n), we compare P(vri, for the children of the Itypernms of C. initially top(c, v, 7) is assigned to be the concept c itself.",
        "Then, by working up the hierarchy, top(c, n, r) is reassigned to lie successive hy per-nyms of c until the siblings of top(c, c. r) have significantly different probabilities.",
        "In cases where a concept has more than one parent, the parent is chosen which results in the lowest X2 value as this indicates the p(nin7,r) are more The set top(c, v,r) is the similarity-class of c for verb ii and position 7.",
        "The next section provides evidence that the tech ni (pi e for choosing top( c, v, r), which we call the 'similarity-class' technique, does select an appropriate level of generalisation."
      ]
    },
    {
      "heading": "5 Experiments using PP-attachment ambiguity",
      "text": [
        "The PP-attachment problem we address considers 4-tuples of the form v,ni,pr,n2, and the problem is to decide whether the prepositional phrase pr 712 attaches to the verb 73 or the noun For example, in the following case the problem_ is to decide whether from Ininistcr attaches to await or approval: await approval from minister We chose the PP-attachment problem because PP-attachment is a pervasive form of ambiguity, and there exist standard training and test data, which 111 akes for easy comparisons with other approaches.",
        "This problem has been tackled by a number of researchers.",
        "Brill and Resnik (199=1), Ratnaparkhi et al.",
        "(1991), Collins (1995), Za-yre] and Daelemans (1 997) all report results between 81% and 85%, with Stetina and Nagao (1997) reporting a result of 88%, which matches the human performance on this task reported by Ratuaparkhi et al.",
        "(1991).",
        "Although the PP-attachment problem has characteristics that In ake it suitable for evaluation, it presents a touch bigger sparse data problem than would be expected in other problems such as relative clause attachment.",
        "The reason for this is that we need to consider how a concept is associated with combinations of predicates and prepositions.",
        "The approach described here uses probabilities of the form p(c, pr and p( e, prini), where c cn(n2).",
        "This means that for many predicate/preposition combinations which occur infrequently in the data, there are few examples of n2 which can be used for populating Word Net in these cases.",
        "Despite this, we were still able to carry out an evaluation by considering subsets of the test data for ‘vhich the relevant predicate/preposition combinations did occur frequently in the training data.",
        "alternative senses.",
        "We rely on the log-likelihood x2 test.",
        "returning a non-significant result in these cases.",
        "We decide on the attachment site by coin par",
        "The sense of n2 is chosen which maximises the relevant probability in each potential attachment case.",
        "If p(c„ , prIv) is greater than P(cni , prim), the attachment is made to v, otherwise to n1.",
        "If n2 is not in Word.Net we compare p(prI v) and P(Pr Int).",
        "Probabilities of the form p(c, prIv) and p(c, prim]) are used rather than.",
        "P(elv, Pr) and P(elni,pr), because the association between the preposition and v and 7/1.",
        "contains useful information.",
        "In fact, for a lot of cases this information alone can be used to decide on the correct attachment site.",
        "The original corpus-based method of Hindle and Rooth (1993) used exactly this information.",
        "Thus the method described here can be thought of as Hin-dle and Rooth's method with additional class-based information about 712.",
        "In order to estimate p(c.,„ prI v) (and ,prn1 )) we apply the same procedure as described in Section 3, first rewriting the probability using Hayes' rule:",
        "The probabilities p(c1) and p(v) can be estimated using maximum likelihood estimates, and p(vIcy, pr) and p(prIcv) can be estimated using maximum likelihood estimates of p(vitop(ev,v, pr), pr) and P(PrIt°P(cv, Pr)) respectively.6 We used the training and test data described in Ratnaparklii et al.",
        "(1994), which was taken from the Penn Treebank and has now become the standard data set for this task.",
        "The data set consists of tuples of the form (v, ni,T, 772), together with the attachment site for each tu.ple.",
        "There is also a development set to prevent implicit training on the test set during development.",
        "We extracted (v, pr , 712) and (hi, pr, 772) G 1 n Section 4 we only gave the procedure for determining top(c„, v, pr), but top(c,,pr) can be determined in an analogous fashion.",
        "triples from the training set, and in order to increase the number of training triples, we also extracted triples from unambiguous cases of attachment in the Penn T.reebank.",
        "We preprocessed the training and test data by lemmatising the words, replacing numerical amounts with the words definite_quantity', replacing monetary amounts with the words `sum_of_money' etc.",
        "We then ignored those triples in the resulting training set (but not test set) for which 712 was not in WordNet, which left a total of 66, 887. triples of training data.",
        "The test set contains 3, 097 examples.",
        "Table 3 gives some examples of the extent to which the similarity-class technique is generalising, using the training data just described, and a significance level of 0.05.",
        "The chosen.",
        "hypernyin is shown.",
        "in upper case.",
        "Note that the WordNet hierarchy Consists of nine separate sub-hierarchies, headed.",
        "by such concepts as (entity), (abstraction), (psychological eature), but we assume the existence of a single root which dominates each of the sub-hierarchies, which is referred to as (root).",
        "In cases where Word.Net is very sparsely populated., it is preferable to go to (root), rather than stay at the root of One of the sub-hierarchies where the data may be noisy or too sparse to be of any use.",
        "The table shows that with the amount of data available from the Treebank, the similarity-class technique is selecting a level at or close to (root) in many cases.",
        "We compared the similarity-class technique with fixing the level of generalisation.",
        "Two fixed levels were used: the root of the entire hierarchy ((root)), and the set consisting of the roots of each of the 9 sub-hierarchies.",
        "The procedure which always selects (root) ignores any information about 712, and is equivalent to comparing p(prIv) and p(prI7-71) , which is the Hindle and booth approach.",
        "The results on the 3,097 test cases are shown in Table 4.",
        "We used a significance level a of 0.05 for the x2 test.7 As the table shows, the disambiguation accuracy is below the state of the art.",
        "However, the results are comparable with those of Li and 7Similar results were obtained using alternative levels of significance.",
        "Rather than simply selecting a value for a such as 0.05, a can be treated as a parameter of the model, whose optimum value call be obtained by running the disambiguation method on some held-out supervised data.",
        "Abe (1998) who adopt a similar approach using \\VordNet, but with a different training and test set.",
        "Li and Abe improved on the Hindle and Rooth technique by 1.5%, which is in line with our results.",
        "As an evaluation of the similarity-class technique, the result is inconclusive.",
        "The reason for this is that when the technique was being used to estimate p(olc„,pr) and p(nit Ic , pr), in many cases the root of the hierarchy was being chosen as the appropriate level of generalisation, due to a sparsely populated Word Net in that instance.",
        "Recall that this is largely due to the fact that we are attempting to populate Word Net for combinations of predicates and prepositions.",
        "In such cases the similarity-class technique is not helping because there is very little or no information about n2.8 8ln an effort to obtain inore data we applied the extraction heuristic of Rat naparkhi 998) to Wall Street Journal text, which increased the number of training triples by a factor of 1(1 'Phis only achieved comparable results, however, presumably because the high volume of noise in the data outweighs the benefit of the increase in data size.",
        "Ratnaparklii reports only 59% accuracy for",
        "In order to evaluate the similarity-class technique further; we took those test cases for which the root was not being selected when estimating both peol(:„, in.)",
        "and p(nik„,,pr).",
        "This applied to 1.13 cases.",
        "The results are given in Table 5.",
        "We also took those test cases for which the root was being selected when estimating at most one of p(vIc„, and p(nile„, , pr).",
        "This applied to :1.032 test cases.",
        "The results are shown in Table G. the extraction heuristic when applied to the Penn Treebank (excluding cases where the preposition is of)."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "We have shown that when instances of WordNet are well populated with examples of n2, the method d.escribecl.",
        "here for solving PP-attachment ambiguities is highly accurate.",
        "When WordNet is sparsely populated, the method automatically resorts to comparing just the preposition and each of the potential attachment sites, as the similarity-class technique will.",
        "select (root) as the appropriate level of generalisation for n2 in such cases.",
        "We have also shown the similarity-class technique to be superior to using a fixed level of generalisation in WordNet.",
        "Further work will look at how to integrate probabilities such as i.",
        ")(civ,r) into a model of dependency structure, similar to that of Collins (1996) and Collins (1997), which can be used 'for parse selection.",
        "However, knowledge of selectional preferences cannot by itself solve the problem of structural disambiguation, and this further work will also look at using additional knowledge, such as snbcategorisation information."
      ]
    }
  ]
}
