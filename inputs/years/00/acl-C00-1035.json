{
  "info": {
    "authors": [
      "Guy De Pauw"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1035",
    "title": "Aspects of Pattern-Matching in Data-Oriented Parsing",
    "url": "https://aclweb.org/anthology/C00-1035",
    "year": 2000
  },
  "references": [
    "acl-J93-2004",
    "acl-P98-1081",
    "acl-W96-0214"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Data-Oriented Parsing (DoP) ranks among the best parsing schemes, pairing state-of-the art parsing accuracy to the psycholinguistic insight that larger chunks of syntactic structures are relevant grammatical and probabilistic units.",
        "Parsing with the DoP-model, however, seems to involve a lot of CPU cycles and a considerable amount of double work, brought on by the concept of multiple derivations, winch is necessary for probabilistic processing, but which is not convincingly related to a proper linguistic backbone.",
        "It is however possible to reinterpret the DoP-model as a pattern-matching model, winch tries to maximize the size of the substructures that construct the parse, rather than the probability of the parse.",
        "By emphasizing this memory-based aspect of the D0P-model, it is possible to do away with multiple derivations, opening up possibilities for efficient Viterbi-style optimizations, while still retaining acceptable parsing accuracy through enhanced context-sensitivity."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The machine learning paradigm of Memory-Based Learning, based on the assumption that new problems are solved by direct reference to stored experiences of previously solved problems, has been successfully applied to a number of linguistic phenomena, such as part-of-speech tagging, NP-chunking and stress acquisition (consult Daelemans (1999) for an overview).",
        "To solve these particular problems, linguistic information needed to trigger the correct disambiguation, is encoded in a linear feature value representation and presented to a memory based learner, such as TiMBL (Daelemans et al., 1999).",
        "Yet, many of the intricacies of the domain of syntax do not translate well to a linear representation, so that established MBL-methods are necessarily limited to low-level syntactic analysis, like the aforementioned NP-chunking task.",
        "Data Oriented Parsing (Bod, 1999), a state-of-the art natural language parsing system, translates very well to a Memory Based Learning context.",
        "This paper describes a reinterpretation of the DoP-model, in which the pattern-matching aspects of the model are exploited, so that parses are analyzed by trying to match a new analysis to the largest possible substructures recorded in memory.",
        "A short; introduction to Data Oriented Parsing will be presented in Section 2, Rowed by an explanation of the term pattern-matching in the context of this paper.",
        "Section 4 describes the experimental setup and the corpus.",
        "The parsing phase that precedes the disambiguation phase will be outlined in Section 5 and a description of the 3 disambiguating models, PCFG, PMPG and the combined system PCFG±PMPG can be found in Sections 6, 7 and 8."
      ]
    },
    {
      "heading": "2 Data Oriented Parsing",
      "text": [
        "Data Oriented Parsing, originally conceived by R,eniko Scha (Scha, 1990), has been successfully applied to syntactic natural language parsing by R,ens Bod (1995), (1999).",
        "The aim of Data Oriented Parsing (henceforth DoP) is to develop a performance model of natural language, that models language use rather than some type of competence.",
        "It adapts the psycholinguistic insight that language users analyze sentences using previously registered constructions and that not only rewrite rules, but complete substructures of any given depth can be linguistically relevant; units for parsing."
      ]
    },
    {
      "heading": "2.1 Architecture",
      "text": [
        "The core of a DOP-system is its TREEBANK: an annotated corpus is used to induce all substructures of arbitrary depth, together with their respective probabilities, which is a expressed by",
        "its frequency in the TREEBANK relative to the number of substructures with the same root-node.",
        "Figure 1 shows the combination operation that is needed to form the correct parse tree for the sentence Peter killed a raccoon.",
        "Given a treebank of substructures, the system tries to match the leftmost open node of a substructure that is consistent with the parse tree, with the top-node of another substructure, consistent; with the parse tree.",
        "Usually, different combinations of substruc-tlITCS are possible, as is indicated in Figure 1: in the example at the left-hand side the tree-structure can be built by combining an s-structure with a specified NP and a fully specified VP-structure.",
        "The right example shows another possible combination, where a parse tree is built by combining the minimal substructures.",
        "Note that these are consistent with ordinary rewrite-rules, such as s NP VP.",
        "One particular parse tree may thus consist of several different derivations.",
        "To find the probability of a derivation, we multiply the probabilities of the substructures that were used to forum the derivation.",
        "To find the probability of a parse, we must in principle sum the probabilities of all its derivations.",
        "It is computationally hardly tractable to consider all derivations for each parse.",
        "Since VITERBI optimization only succeeds in finding the most probable derivation as opposed to the most probable parse, the MONTE CARLO algorithm is introduced as a proper approximation that randomly generates a large number of derivations.",
        "The most probable parse is considered to be the parse that is most often observed in this derivation forest."
      ]
    },
    {
      "heading": "2.2 Experimental Results of DoP",
      "text": [
        "The basic D0P-model, DoP1, was tested on a manually edited version of the ATIS-corpus (Marcus, Santorini, and Marcinkiewicz, 1993).",
        "The system was trained on 603 sentences (part-of-speech tag sequences) and evaluated on a test set of 75 sentences.",
        "Parse accuracy was used as an evaluation metric, expressing the percentage of sentences in the test set for which the parse proposed by the system is completely identical to the one in the original corpus.",
        "Different; experiments were conducted in which maximum substructure size was varied.",
        "With DoP1-limited to a substructure-size of 1 (equivalent; to a PcFo), parse accuracy is 47%.",
        "In the optimal DoP-model, ill which substructure-size is not limited, a parse accuracy of 85% is obtained."
      ]
    },
    {
      "heading": "2.3 Short Assessment of DOP",
      "text": [
        "Dor 1 in its optimal form achieves a very high parse accuarcy.",
        "The computational costs of the system, however, are equally high.",
        "Bod (1995) reported an average parse time of 3.5 hours per sentence.",
        "Even though current; parse time is reported to be more reasonable, the optimal DoP algorithm in which no constrains are made on the size of substructures, may not yet be tractable for liti-size corpora.",
        "In a context-free grammar framework (consistent with DoP limited 1;o a substructure-size Of 1), there is only one way a parse tree can be formed (fin example, the right hand side of Figure 1), meaning that there is only one derivation for a given parse tree.",
        "This allows efficient VITERBI style optimization.",
        "To encode context-sensitivity in the system, DOP is forced to introduce multiple derivations, so that repeatedly the same parse tree needs to be generated, bringing about a lot of computational overhead.",
        "Even though the use of larger syntactic contexts is highly relevant from a psycholinguistic point-of-view, there is no explicit preference being made fin: larger substructures in the DoP model.",
        "While the MONTE CARLO opthnization scheme maximizes the probability of the derivations and seems to prefer derivations made up of larger substructures, it may be interesting to",
        "When we look at natural language parsing from a memory-based point of view, one might say that a sentence is analyzed by looking up the most similar structure for the different analyses of that sentence in memory.",
        "The parsing system described in this paper tries to mimic this behavior by interpreting the DoP-model as a memory-based model, in which analyses are being matched with syntactic patterns recorded in memory.",
        "Similarity between the proposed analysis and the patterns in memory is computed according to:",
        "• the number of patterns needed to construct a tree (to be minimized) • the size of the patterns that are used to construct a tree (to be maximized)",
        "The nearest neighbor for a given analysis can be defined as the derivation that shares the largest amount of common nodes."
      ]
    },
    {
      "heading": "4 The experimental Setup",
      "text": [
        "10-fold cross-validation was used to appropriately evaluate the algorithms, as the dataset (see Section 4.1) is rather small.",
        "Like DOP1 the system is trained and tested on part-of-speech tag sequences.",
        "In a first phase, a simple bottom-up chart parser, trained on the training partitions, was used to generate parse forests for the part-of-speech tag sequences of the test partition.",
        "Next, the parse forests were sent to the 3 algorithms (henceforth the disambiguators) to order these parse forests, the first parse of the ordered parse forest being the one proposed by the disambiguator.",
        "In this paper, 3 disambiguators are described:",
        "• POPO: simple Probabilistic Context-Free Grammar • PMPG: the DOP approximation, Pattern-Matching Probabilistic Grammar • PCFG+PMPG: a combined system, integrating PCPG and PMPG",
        "The evaluation metric used is parse accuracy, but also the typical parser evaluation metric F-measure (precision/recall) is given as a means of reference to other systems."
      ]
    },
    {
      "heading": "4.1 The Corpus",
      "text": [
        "The experiments were conducted on an edited version of the ATIS-II-corpus (Marcus, Santorini, and Marcinkiewicz, 1993), which consists of 578 sentences.",
        "Quite a lot of errors and inconsistencies were found, but not corrected, since we want our (probabilistic) system to be",
        "able to deal with this kind of noise.",
        "Semantically oriented flags like -TMP and -Dm., most often used in conjunction with PP, have been renmed, since there is no way of retrieving this kind of semantic information from the part-of-spec ii tags of the ATIS-corpus.",
        "Syntactic flags like -SI3.1, on the other hand, have been maintained.",
        "Internal relations (denoted by numeric flags) were renliwed and for practical reasons, sentence-length was linfited to 15 words max.",
        "The edited corpus retained 562 sentences."
      ]
    },
    {
      "heading": "5 Parsing",
      "text": [
        "As a first phase, a bottom-up chart parser parsed the test set.",
        "This proved to be quite problematic, since overall, 106 out of 562 sentences (1.9%) could not be parsed, due to the sparseness of the grammar, meaning that the appropriate rewrite rule needed to construct the correct parse tree for a sentence in the test set, wasn't featured in the induced grammar.",
        "NP-annotation smiled to be the main cause for unparsability.",
        "An NP like restriction code AP/57 is represented by the rewrite rule:",
        "Highly specific and flat structures like these are scarce and are usually not induced from the training set when needed to parse the test set.",
        "Ongoing research tries to implement grammatical smoothing as a solution to this problem, but one might also consider generating parse forests with an independent grammar, induced from the entire corpus (training set-Itestset) or a different corpus.",
        "in both cases, however, we would need to apply probabilistic smoothing to be able to assign probabilities to unknown structures/rules.",
        "Neither grammatical, nor probabilistic smoothing was implemented in the context of the experiments, ,described in this paper.",
        "The sparseness of the grammar proves to be a serious bottleneck for parse accuracy, limiting our disambiguators to a maximum parse accuracy of 81%."
      ]
    },
    {
      "heading": "6 PcFG-experiments",
      "text": [
        "A. PCFG constructs parse trees by using simple rewrite-rules.",
        "The probability of a parse tree can be computed by multiplying the probabilities of the rewrite-rules that were used to construct the parse.",
        "Note that a PCFG iS identical to Dtw1.",
        "when we ii nit the maximum substructures size to 1, only allowing derivations of the type found at the right-hand side of Figure 1."
      ]
    },
    {
      "heading": "6.1 Experimental Results",
      "text": [
        "1.'he first line of Table].",
        "shows the results fOr the PcFG-experiments: 66.4% parse accuracy is an adequate result for this baseline model.",
        "We also look at parse accuracy for parsable sentences (an estimate of the parse accuracy we might get if we had a more suited parse threst generator) and we notice that we are able to achieve a 81.8% parse accuracy.",
        "This is already quite high, but on examining the parsed data, serious and fundamental limitations to the PcFG-model can be observed"
      ]
    },
    {
      "heading": "6.2 Error Analysis",
      "text": [
        "Figure 2, displays the most common type of mistake made by PcFG's. The correct parse tree could represent an analysis for the sentence: / want a flight from /37usse/s to Toronto.",
        "This example shows that a PCFG has a tendency to prefer flatter structures over embedded structures.",
        "This is a trivial effect of the mathematical formula used to compute the probability of a parse-tree: embedded structure require more rewrite rules, adding more factors to the multiplication, which will almmiost inevitably result in a lower probability.",
        "It is an unfortunate property of PCFG's that the number of nodes in the parse tree is inversely proportionate to its probability.",
        "One might be inclined to normalize a parse tree's probability relative to the number of nodes in the tree, but a more linguistically sound alternative is at hand: the enhancement of context sensitivity through the use of larger syntactic context within parse trees can make our disambiguator more robust."
      ]
    },
    {
      "heading": "7 PmPG-experiments",
      "text": [
        "The Pattern-Matching Probabilistic Grammar is a memory-based interpretation of a 1)01'1110(101, in which a sentence is analyzed by matching the largest possible chunks of syntactic structure on the sentence.",
        "To compile parse trees into patterns, all substructures in the training set are encoded by assigning them specific indexes, NP(Q345 e.g. denoting a fully specified NP-structure.",
        "This approach was inspired by Goodman (1996), in which Goodman",
        "unsuccessfully uses a system of indexed parse trees to transform DOP into an equivalent; PCFG.",
        "The system of indexing (which is detailed in De Pauw (2000)) used in the experiments described in this paper, is however specifically geared towards encoding contextual information in parse trees.",
        "Given an indexed training set, indexes can then be matched on a test set parse tree in a bottom-up fashion.",
        "In the following example, boxed nodes indicate nodes that have been retrieved from memory.",
        "In this example we can see that an NP, consisting of a fully specified embedded NP and l'P, has been completely retrieved from memory, meaning that the NP in its entirety can be observed in the training set.",
        "However, no VP was found that consists of a VBP and that particular NP.",
        "Disambiguating with PMPG consequently involves pruning all nodes retrieved from memory:",
        "Finally, the probability for this pruned parse tree is computed in a PCFG-type manner, not adding the retrieved nodes to the product:"
      ]
    },
    {
      "heading": "7.1 Experimental Results",
      "text": [
        "The results for the PmPG-experiments can be found on the second line of Table 1.",
        "On some partitions, PMPG performed insignificantly better than PCFG, but Table 1 shows that the results for the context sensitive scheme are much worse.",
        "58.2% overall parse accuracy and 71.7% parse accuracy on parsable sentences indicates that PMPG is not a valid approximation of DOP's context-sensitivity."
      ]
    },
    {
      "heading": "7.2 Error Analysis",
      "text": [
        "The dramatic drop in parsing accuracy calls for an error analysis of the parsed data.",
        "Figure 3 is a prototypical mistake PMPG has made.",
        "The correct analysis could represent a parse tree for a sentence like:",
        "The PMPG analysis would never have been considered a likely candidate by a common PCFG.",
        "This particular sentence in fact was effortlessly disambiguated by the PCFG .",
        "Yet the fact that large chunks of tree-structure are retrieved from memory, make it the preferred parse for the PMPG.",
        "We notice for instance that a large part of the sentence can be matched on an SBAR structure, which has no relevance whatsoever.",
        "Clearly, PMPG overestimates substructure size as a feature for disambiguation.",
        "It's interesting however to see that it is a working implementation of context sensitivity, eagerly matching patterns from momory.",
        "At the same time, it has lost track of common-sense PCFG tactics.",
        "it; is in the combination of the two that one may find a decent disambiguator and accurate implementation of context-sensitivity."
      ]
    },
    {
      "heading": "8 A Combined System (PMPG+PCFG)",
      "text": [
        "Table 1 showed that 81.8% of the time, a PCFG finds the correct parse (for parsable sentences), meaning that the correct parse is at the first place in the ordered parse forest.",
        "99% of the time, the correct; parse can be found among the 10 most probable parses in the ordered parse forest.",
        "This opens up a myriad of possibilities for optimization.",
        "One might for instance use a best-first strategy to generate only the 10 best parses, significantly reducing parse and disambiguation time.",
        "An optimized disambiguator might therefore include a preparatory phase in which a common-sense PCFG retains the most probable parses, so that a more sophisticated follow-up scheme need not bother with senseless analyses.",
        "In our experiments, we combined the common-sense logic of a PCFG and used its output as the PMPG's input.",
        "This is a well-established technique usually referred to as system, combination (see van Halteren, Zavrel, and Daelemans (1998) for an application of this",
        "We are also presented with the possibility to assign a weight to each algorithm's decision.",
        "The probability of a parse can the be described with the following formula: 1-1 Nrewrit er ule) non-indexed i iodesp, The weight of each algorithm's decision, as well as the number of most probable parses that are extrapolated lbr the pattern-matching algorithm, are parameters to be optimized.",
        "Future work will include evaluation on a validation set to retrieve the optimal values for these parameters."
      ]
    },
    {
      "heading": "8.1 Results",
      "text": [
        "The third line in Table 1 shows that the combined system performs better than either one, with a parse accuracy of 71.5% and close to 90% parse accuracy on parsable sentences, which we can consider an approximation of results reported for DoPl.",
        "Error analysis shows that the combined system is indeed able to overcome difficulties of both algorithms.",
        "The example in Figure 2 as well as the example in Figure 3 were disambiguated correctly using the combined system"
      ]
    },
    {
      "heading": "9 Future Research",
      "text": [
        "Even though the PMPG shows a lot of promise in its parse accuracy, the following extensions need to be researched: • Optimizing PMPG+PCFG for computational efficiency: the graph in Section 8 shows a possible optimized parsing system, in which a preprocessing PCFG generates the 7/, 1110St likely candidates to be extrapolated for the actual disambiguator.",
        "Full parse forests were generated for the experiments described in this paper, so that the efficiency gain of such a system cannot be properly estimated.",
        "• PMPG+PCFG as an approximation needs to be compared to actual DoP, by having DOP parse the data used in this experhnent, and by having PmPGH-PcFG parse the data used in the experiments described in Bod (1999).",
        "• The bottleneck of the sparse grammar problem prevents us from fully exploiting the disambiguating power of the pattern-matching algorithm.",
        "The GRAEL-system (GRammar Adaptation, Evolution and Learning) that is currently being developed, tries to address the problem of grammatical sparseness by using evolutionary techniques to generate, optimize uid complement grammars."
      ]
    },
    {
      "heading": "10 Conclusions",
      "text": [
        "Even though ixn4 exhibits outstanding parsing behavior, the efficiency of the model is rather problematic.",
        "The introduction of multiple derivations causes a considerable amount of computational overhead.",
        "Neither is it clear how the concept of multiple derivations translates to a psycholinguistic context: there is no proof that language users consider different instantiations of the same parse, when deciding on the correct analysis for a given sentence.",
        "A pattern-matching scheme was presented that tried to disambiguate parse forests by trying to maximize the size of the substructures that can be retrieved from memory.",
        "This straightforward memory-based interpretation yields substandard p arsAig accuracy.",
        "But the combination of common-sense probabilities and enhanced context-sensitivity provides a workable parse forest disambiguator, indicating that language users might exert a complex combination of memory-based recollection techniques and stored statistical data to analyze utterances."
      ]
    }
  ]
}
