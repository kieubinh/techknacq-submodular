{
  "info": {
    "authors": [
      "Toru Hisamitsu",
      "Yoshiki Niwa",
      "Jun'ichi Tsujii"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1047",
    "title": "A Method of Measuring Term Representativeness - Baseline Method Using Co-Occurrence Distribution",
    "url": "https://aclweb.org/anthology/C00-1047",
    "year": 2000
  },
  "references": [
    "acl-C94-1084",
    "acl-J90-1003",
    "acl-J93-1003",
    "acl-W99-0609"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper introduces a scheme, which we call the baseline method, to define a measure of term representativeness and measures defined by using the scheme.",
        "The representativeness of a term is measured by a normalized characteristic value defined for a set of all documents that contain the term.",
        "Normalization is done by comparing the original characteristic value with the characteristic value defined for a randomly chosen document set of the same size.",
        "The latter value is estimated by a baseline function obtained by random sampling and logarithmic linear approximation.",
        "We found that the distance between the word distribution in a document set and the word distribution in a whole corpus is an effective characteristic value to use for the baseline method.",
        "Measures defined by the baseline method have several advantages including that they can be used to compare the representativeness of two terms with very different frequencies, and that they have well-defined threshold values of being representative.",
        "In addition, the baseline function for a corpus is robust against differences in corpora; that is, it can be used for normalization in a different corpus that has a different size or is in a different domain."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Measuring the representativeness (i.e., the informativeness or domain specificity) of a term' is essential to various tasks in natural language processing (NLP) and information retrieval (IR).",
        "It is particularly crucial when applied to an IR interface to help a user find informative terms.",
        "For instance, when the number of retrieved documents is intractably large, an overview of representative words in the documents is needed to understand the contents.",
        "To enable this, an IR system, called DuaINAVI, that has two navigation windows where one displays a graph of representative words in the retrieved documents, was developed (Nishioka et al.",
        "1997).",
        "This window helps users grasp the contents of retrieved documents, but it also exposes problems concerning existing representativeness measures.",
        "Figure 1 shows an example of a graph for the query 'di (electronic money), with Nihon I A term is a word or a word sequence.",
        "I Graduate School of Science, the University of Tokyo 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8654, Japan tsujii@is.s.u-tokyo.ac.jp Keizai Shanbun (a financial newspaper) 1996 as the corpus.",
        "Frequently appearing words are displayed in the upper part of the window, and words are selected by a tflic(f-like measure (Niwa et al.",
        "1997).",
        "Typical non-representative words are filtered out by using a stop-word list.",
        "read cipher Figure 1 A topic word graph when the query is %;-T- -t, – ( el ectroni c money).",
        "One problem is the difficulty of suppressing uninformative words such as 41-= (year), – (one), and Ji (month) because classical measures, such as tf-idf, are too sensitive to word frequency arid no established method to automatically construct a stop-word list has been developed.",
        "Another problem is that the difference in the representativeness of words is not sufficiently indicated.",
        "In the example above, highlighting HP} (cipher) over less representative words such as Wiir7i6 (read) would be useful.",
        "Most classical measures based on only term frequency and document frequency cannot overcome this problem.",
        "To define a more elaborate measure, attempts to incorporate more precise co-occurrence information have been made.",
        "Caraballo et al.",
        "(1999) tried to define a measure for \"specificity\" of a noun by using co-occurrence information of a noun, but it was not very successful in the sense that the measure did not particularly outperformed the term frequency.",
        "Hisamitsu et al.",
        "(1999) developed a measure of the representativeness of a term by using co-occurrence information and a normalization",
        "technique.",
        "The measure is used on the distance between the word distribution in the documents containing a term and the word distribution in the whole corpus.",
        "Their measure overcomes previously mentioned problems and preliminary experiments showed that this measure worked better than existing measures in picking out representative/non-representative terms.",
        "Since the normalization technique plays a crucial part of constructing the measure, issues related to the normalization need more study.",
        "In this paper we review Hisamitsu's measure and introduce a generic scheme – which we call the baseline method for convenience – that can be used to define various measures including the above.",
        "A characteristic value of all documents containing a term T is normalized by using a baseline function that estimates the characteristic value of a randomly chosen document set of the same size.",
        "The normalized value is then used to measure the representativeness of the term T. A measure defined by the baseline-method has several advantages compared to classical measures.",
        "We compare four measures (two classical ones and two newly defined ones) from various viewpoints, and show the superiority of the measure based on the normalized distance between two word distributions.",
        "Another important finding is that the baseline function is substantially portable, that is, one defined for a corpus can be used for a different corpus even if the two corpora have considerably different sizes or are in different domains."
      ]
    },
    {
      "heading": "2. Existing measures of representativeness 2.1 Overview",
      "text": [
        "Various methods for measuring the informativeness or domain specificity of a word have been proposed in the domains of IR and term extraction in NIA) (see the survey paper by Kageura 1996).",
        "In characterizing a term, Kageura introduced the concepts of \"tmithood\" and \"termhood\": unithood is \"the degree of strength or stability of syntagmatic combinations or collocations,\" and termhood is \"the degree to which a linguistic unit is related to (or more straightforwardly, represents) domain-specific concepts.\"",
        "Kageura's termhood is therefore what we call representativeness here.",
        "Representativeness measures were first introduced in an IR domain for determining indexing words.",
        "The simplest measure is calculated from only word frequency within a document, For example, the weight /,-; of word w; in document dj is defined by",
        "wherefi is the frequency of word w, in document di (Sparck-Jones 1973, Noreault et al.",
        "1977).",
        "More elaborate measures for termhood combine word frequency within a document and word occurrence over a whole corpus.",
        "For instance, the most commonly used measure, was originally defined as",
        "where Ni and N„,,, are, respectively, the number of documents containing word w; and the total number of documents (Salton et al.",
        "1973).",
        "There are a variety of definitions of rf-hu,' but its basic feature is that a word appearing more frequently in fewer documents is assigned a higher value.",
        "If documents are categorized beforehand, we can use a more sophisticated measure based on the x2 test of the hypothesis that an occurrence of the target word is independent of categories (Nagao et al.",
        "1976).",
        "Research on automatic term extraction in NLP domains has led to several measures for weighting terms mainly by considering the nunhood of a word sequence.",
        "For instance, mutual information (Church et al.",
        "1990) and the log-likelihood (Dunning 1993) methods for extracting word bigrams have been widely used.",
        "Other measures for calculating the unithood of n-grams have also been proposed (Frantzi et al.",
        "1996, Nakagawa eta!.",
        "1998, Kita eta!.",
        "1994)."
      ]
    },
    {
      "heading": "2.2 Problems",
      "text": [
        "Existing measures suffer from at least one of the following problems:",
        "(1) Classical measures such as•t/Lidf are so sensitive to term frequencies that they fail to avoid very frequent non-informative words.",
        "(2) Methods using cross-category word distributions (such as the x2 method) can be applied only if documents in a corpus are categorized.",
        "(3) Most measures in NLP domains cannot treat single word terms because they use the unithood strength of multiple words.",
        "(4) The threshold value for being representative is defined in an ad hoc manner.",
        "The scheme that we describe here constructs measures that are free of these problems."
      ]
    },
    {
      "heading": "3. Baseline method for defining representativeness measures",
      "text": []
    },
    {
      "heading": "3.1 Basic idea",
      "text": [
        "This subsection describes the method we developed for defining a measure of term representativeness.",
        "Our basic idea is summarized by the famous quote (Firth 1957) You shall know a word by the company it keeps.\"",
        "We interpreted this as the following working hypothesis:",
        "For any term T, if the term is representative, D(T) , the set of all documents containing T, should have some characteristic property compared to the average.",
        "To apply this hypothesis, we need to specify a measure to obtain some \"property\" of a document set and the concept of \"average\".",
        "Thus, we converted this hypothesis into the following procedure: Choose a measure M characterizing a document set.",
        "For term T, calculate M(D(T)) , the value of the measure for D(T) .",
        "Then compare M(D(T)) with B,(#D(T)) , where #D( T) is the number of words contained in #D(T), and B, estimates the value of M(D) when D is a randomly chosen document set of size #D(T) .",
        "Here, M measures the property and B m estimates the average.",
        "The size of a document set is defined as the number of words it contains.",
        "We tried two measures as M One was the number of different words (referred to here as DIFFNUM) appearing in a document set.",
        "Teramoto conducted an experiment with a small corpus and reported that DIFFNUM was useful for picking out important words (Teramoto et a).",
        "1999) under the hypothesis that the number of different words co-occurring with a topical (representative) word is smaller than that with a generic word.",
        "The other measure was the distance between the word distribution in D(T) and the word distribution in the whole corpus Do.",
        "The distance between the two distributions can be measured in various ways, and we used the log-likelihood ratio as in Hisamitsu et al.",
        "1999, and denote this measure as LLR.",
        "Figure 2 plots (#D, Al(D))s when M is DIFFNUM or LLR, where D varies over sets of randomly selected documents of various sizes from the articles in Nikkei-Shinbun 1996.",
        "For measure M, we define Rep(T, M), the representativeness of T, by normalizing M(D(7)) by BAOD(T)).",
        "The next subsection describes the construction of BM and the normalization."
      ]
    },
    {
      "heading": "3.2 Baseline function and normalization",
      "text": [
        "Using the case of LLR as an example, this subsection explains why normalization is necessary and describes the construction of a baseline function.",
        "over lifi:-.",
        "(cipher), 41(year), J1 (month), ,-12 JR (read), – (one), I (do), and a a (economy).",
        "Figure 3 shows that, for example, LLR(D(t 6)) is smaller than LLR(D(g)), which reflects our linguistic intuition that words co-occurring with \"economy\" arc more biased than those with \"do\".",
        "However, LLR(D(Ilff---)) is smaller than LLR(D(gY,L ER6)) and smaller even than LLR(D(-4- 6)).",
        "This contradicts our linguistic intuition, and is why values of LLR are not directly used to compare the representativeness of terms.",
        "This phenomenon arises because LLR(D(7)) generally increases as #D(7.)",
        "increases.",
        "We therefore need to use some form of normalization to offset this underlying tendency.",
        "We used a baseline function to normalize the values.",
        "In this case, Bun(•) was designed so that it approximates the curve in Fig. 3.",
        "From the definition of the distance, it is obvious that BuR(0) = B1LR(#D0) = 0.",
        "At the limit when #1)0 – > Go, Ban(•) becomes a monotonously increasing function.",
        "The curve could be approximated precisely through logarithmic linear approximation near (0, 0).",
        "To make an approximation, up to 300 documents are randomly sampled at a time.",
        "(Let each randomly chosen document set be denoted by D. The number of sampled documents are increased from one to 300, repeating each number up to five times.)",
        "Each (#D, LLR(D)) is converted to (log(#D), log(LLR(D))).",
        "The curve formulated by the (log(#D), log(LLR(D))) values, which is very close to a straight line, is further divided into multiple parts and is part-wise approximated by a linear function.",
        "For instance, in the interval I = fx 10000 < 15,0001, log(LLR(D)) could be approximated by 1.103 + 1.023 x log(#D) with R2 = 0.996.",
        "For LLR, we define Rep(T, LLR), the representativeness of T by normalizing LLR(D(7)) by BLLR(#D(T)) as follows:",
        "For instance, when we used Nihon licizai Shimbun 1996, The average of 100x(log(LLR(D)) llog(Bak (#D)) – 1), Avr, was – 0.00423 and the standard deviation, a, was about 0.465 when D varies over randomly selected document sets.",
        "Every observed value fell within Avr-±4a- and 99°A) of observed values fell within Avri3a.",
        "This happened in all corpora (7 orpora) we tested.",
        "Therefore, we can define the threshold of being representative as, say, Avr + 4a."
      ]
    },
    {
      "heading": "3.3 Treatment of very frequent terms",
      "text": [
        "So far we have been unable to treat extremely frequent terms, such as J (do).",
        "We therefore used random sampling to calculate the Rep( T, LLR) of a very frequent term T. If the number of documents in D(7) is larger than a threshold value N, which was calculated from the average number of words contained in a document, N documents are randomly chosen from D(7) (we used N= 150).",
        "This subset is denoted D(T) and Rep(7; LL?)",
        "is defined by 100 x (log(LLR(D(T))) llog(BuR (#D(7))) – 1).",
        "This is effective because we can use a well-approximated part of the baseline curve; it also reduces the amount of calculation required.",
        "By using Rep(T, LLR) defined above, we obtained Rep(-,J-6, LIS) = – 0.573, Rep(at,7)-IR 1,1,10 = 4.08, and Rep(IM LU?)",
        "= 6.80, which reflect our linguistic intuition."
      ]
    },
    {
      "heading": "3.4 Features of Rep(T, M)",
      "text": [
        "Rep(T, M) has the following advantages by virtue of its definition:",
        "(1) Its definition is mathematically clear.",
        "(2) It can compare high-frequency terms with low-frequency terms.",
        "(3) The threshold value of being representative can be defined systematically.",
        "(4) It can be applied to n-gram terms for any n."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": []
    },
    {
      "heading": "4.1 Evaluation of monograms",
      "text": [
        "Taking topic-word selection for a navigation window for IR (see Fig. 1) into account, we examined the relation between the value of Rep(T, M) and a manual classification of words (monograms) extracted from 158,000 articles (excluding special-styled non-sentential articles such as company-personnel-affair articles) in the 1996 issues of the Nikkei Shinbun.",
        "We randomly chose 20,000 words from 86,000 words having document frequencies larger than 2, then randomly chose 2,000 of them and classified these into three groups: class a (acceptable) words useful for the navigation window, class d (delete) words not useful for the navigation window, and class ii (uncertain) words whose usefulness in the navigation window was either neutral or difficult to judge.",
        "In the classification process, a judge used the Dua/NAVI system and examined the informativeness of each word as guidance.",
        "Classification into class d words was done conservatively because the consequences of removing informative words from the window are more serious than those of allowing useless words to appear.",
        "Table I shows part of the classification of the 2,000 words.",
        "Words marked \"p\" are proper nouns.",
        "The difference between proper nouns in class a and proper nouns in other classes is that the former are wellknown.",
        "Most words classified as \"d\" are very common verbs (such as 1-6(do) and l',If-9(11ave)), adverbs, demonstrative pronouns, conjunctions, and numbers.",
        "It is therefore impossible to define a stop-word list by only using parts-of-speech because almost all parts-of-speech appear in class d words.",
        "To evaluate the effectiveness of several measures, we compared the ability of each measure to gather (avoid) representative (non-representative) terms.",
        "We randomly sorted the 20,000 words and then compared the results with the results of sorting by other criteria: Rep(., LLR), Rep(., DIFFNUM), (term frequency), and tfidl.",
        "The comparison was done by using the accumulated number of words marked by a specified class that appeared in the first N (1 N 2,000) words.",
        "The definition we used for tFielf was",
        "where T is a term, TF(7) is the term frequency of 7', is the number of total documents, and N(7) is the number of documents that contain T"
      ]
    },
    {
      "heading": "4.1.3 Results",
      "text": [
        "accumulated number of words marked \"a\".",
        "The total number of class a words was 911.",
        "Rep(., LLR) clearly outperformed the other measures.",
        "Although Rep(., DIFFNUM) outperformed tf and tf-iqf up to about the first 9,000 monograms, it otherwise underperformed them.",
        "If we use the threshold value of Rep(., LLR), from the first word to the 1,511th word is considered representative.",
        "In this case, the recall and precision of the 1,511 words against all class a words were 85% and 50%, respectively.",
        "When using tf:idf, the recall and precision of the first 1,511 words against all class a words were 79% and 47%, respectively (note that tlidf does not have a clear threshold value, though).",
        "Although the degree of out-performance by Rep(., LLR) is not seemingly large, this is a promising result because it has been pointed out that, in the related domains of term extraction, existing measures hardly outperform even the use of frequency (for example, Daille et al.",
        "1994, Caraballo et al.",
        "1999) when we use this type of comparison based on the accumulated numbers.",
        "In the experiments, proper nouns generally have a high Rep-value, and some have particularly high scores.",
        "Proper nouns having particularly high scores are, for instance, the names of sumo wrestlers or horses.",
        "This is because they appear in articles with special formats such as sports reports.",
        "We attribute the difference of the performance between Rep(., LLR) and Rep(., DIFFNUM) to the quantity of information used.",
        "Obviously information on the distribution of words in a document is more comprehensive than that on the number of different words.",
        "This encourages us to try other measures of document properties that incorporate even more precise information.",
        "4.2 Picking out frequent non-representative monograms When we concentrate on the most frequent terms, Rep(, DIFFNUM) outperformed Rep(., LLR) in the following sense.",
        "We marked \"clearly non-representative terms\" in the 2,000 most frequent monograms, then counted the number of marked terms that were assigned Rep-values smaller than the threshold value of a specified representativeness measure.",
        "The total number of checked terms was 563, and 409 of them are identified as non-representative by Rep(, LLR).",
        "On the other hand, Rep(., DIFFNUM) identified 453 terms as non-representative."
      ]
    },
    {
      "heading": "4.3 Rank correlation between measures",
      "text": [
        "We investigated the rank-correlation of the sorting results for the 20,000 terms used in the experiments described in subsection 4.1.",
        "Rank correlation was measured by Spearman's method and Kendall's method (see Appendix) using 2,000 terms randomly selected from the 20,000 terms.",
        "Table 2 shows the correlation between Rep(., LLR) and other measures.",
        "It is interesting that the ranking by Rep(., LLR) and that by Rep(., DIFFNUM) had a very low correlation, even lower than with tf or tfidj: This indicates that a combination of Rep(., LLR) and Rep(', DIFFNUM) should provide a strong discriminative ability in term classification; this possibility deserves further investigation."
      ]
    },
    {
      "heading": "4.4 Portability of baseline functions",
      "text": [
        "We examined the robustness of the baseline functions; that is, whether a baseline function defined from a corpus can be used for normalization in a different corpus.",
        "This was investigated by using Rep•, LLR) with seven different corpora.",
        "Seven baseline functions were defined from seven corpora, then were used for normalization for defining Rep(., LLR) in the corpus used in the experiments described in subesction 4.1.",
        "The performance of the Rep•, LLR)s defined using the different baseline functions was compared in the same way as in the subsection 4.1.",
        "The seven corpora used to construct baseline functions were as follows:",
        "slightly when the baseline defined from NC-ALL was used.",
        "In other cases, the differences was so small that they were almost invisible in Fig. 7.",
        "The same results were obtained when using class d words and class ap words.",
        "Sorting results on class a words We also examined the rank correlations between the ranking that resulted from each representativeness measure in the same way as described in subsection 4.2 (see Table 4).",
        "They were close to 100% except when combining the Kendall's method and NACSIS corpus baselines.",
        "Table 4 Rank correlation between the measure defined by an NK96-ORG baseline and ones defined by other baselines (%) NK96- NK96- Nk96 NK.98- NC l',',000 NC AI I 50000 100000 200000 158000 Spearmann 0.997 0.997 0.996 0.999 0.912 0.900 Kendall 0.970 0.956 0.951 0.979 0.789 0.780 T tese results suggest that a baseline function constructed from a corpus can be used to rank terms in considerably different corpora.",
        "This is particularly useful when we are dealing with a corpus similar to a known corpus but do not know the precise word distributions in the corpus.",
        "The same kind of robustness was observed when we used Repe,",
        "DIFFNUM).",
        "This baseline function robustness is an important lbature of measures defined using the baseline based."
      ]
    },
    {
      "heading": "5. Conclusion and future works",
      "text": [
        "We have developed a better method -- the baseline method -- for defining the representativeness of a term.",
        "A characteristic value of all documents containing a term T, D(T), is normalized by using a baseline function that estimates the characteristic value of a randomly chosen document set of the same size as D(1).",
        "The normalized value is used to measure the representativeness of the term T, and a measure defined by the baseline method offers several advantages compared to classical measures: (I) its definition is mathematically simple and clear, (2) it can compare high-frequency terms with low-frequency terms, (3) the threshold value for being representative can be defined systematically, and (4) it can be applied to ti-gram terms for any a.",
        "We developed two measures: one based on the normalized distance between two word distributions (Rep(..",
        "LL?))",
        "and another based on the number of different words in a document set (Rep(-, D1FFNUM)).",
        "We compared these measures with two classical measures from various viewpoints, and confirmed that Rep(., UR) was superior.",
        "Experiments showed that the newly developed measures were particularly effective for discarding frequent but uninformative terms.",
        "We can expect that these measures can be used for automated construction of a stop-word list and improvement of similarity calculation of documents.",
        "An important finding was that the baseline function is portable; that is, one defined on a corpus can be used for normalization in a different corpus even if the two corpora have considerably different sizes or arc in different domains.",
        "We can therefore apply the measures in a practical application when dealing with multiple similar corpora whose word distribution information is not fully known but we have the information on one particular corpus.",
        "We plan to apply Rep(., WO and Rep(., DIFFNUM) to several tasks in IR domain, such as the construction of a stop-word list for indexing and term weighting in document-similarity calculation.",
        "It will also be interesting to theoretically estimate the baseline functions by using fundamental parameters such as the total number of words in a corpus or the total different number in the corpus.",
        "The natures of the baseline functions deserve further study."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": []
    },
    {
      "heading": "Appendix",
      "text": [
        "Asusume that items /lb...A arc ranked by measures A and B, and that the rank of item l assigned by A (R) is RAW (RE(f)), where R4(i) 4?4(f) (RB(i) 42Ai)) if i Then, Speartnan's rank correlation between the two rankings is given as"
      ]
    }
  ]
}
