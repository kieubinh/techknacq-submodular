{
  "info": {
    "authors": [
      "Sung Young Jung",
      "SungLim Hong",
      "Eunok Paek"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1056",
    "title": "An English to Korean Transliteration Model of Extended Markov Window",
    "url": "https://aclweb.org/anthology/C00-1056",
    "year": 2000
  },
  "references": [
    "acl-C94-2210",
    "acl-C96-1041",
    "acl-J92-4003",
    "acl-J93-2003",
    "acl-J94-2001",
    "acl-P97-1017"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Automatic transliteration problem is to transcribe foreign words in one's own alphabet.",
        "Machine generated transliteration can be useful in various applications such as indexing in an information retrieval system and pronunciation synthesis in a text-to-speech system.",
        "In this paper we present a model for statistical English-to-Korean transliteration that generates transliteration candidates with probability.",
        "The model is designed to utilize various information sources by extending a conventional Markov window.",
        "Also, an efficient and accurate method for alignment and syllabification of pronunciation units is described.",
        "The experimental results show a recall of 0.939 for trained words and 0.875 for untrained words when the best 10 candidates are considered."
      ]
    },
    {
      "heading": "Introduction",
      "text": [
        "As the amount of international communication increases, more foreign words arc flooding into the Korean language.",
        "Especially in the area of computer and information science, it has been reported that 29.4% of index terms are transliterated from or directly written in English in the case of a balanced corpus, KT-SET [18].",
        "The transliteration of foreign words is indispensable in Korean language processing.",
        "In information retrieval, a simple method of processing foreign words is via query term translation based on a synonym dictionary of foreign words and their target transliteration.",
        "It is necessary to automate the construction process of a synonym dictionary since its maintenance requires continuous efforts for ever-incoming foreign words.",
        "Another area to which transliteration can be applied is a text-to-speech system where orthographic words are transcribed into phonetic symbols.",
        "In such applications, maximum likelihood [15], decision tree [1], neural network [10] or weighted finited-state acceptor [19] has been used for finding the best fit.",
        "English-to-Korean transliteration problem is that of generating an appropriate Korean word given an English word.",
        "In general, there can be various possible transliterations in Korean which correspond to a single English word.",
        "It is common that the newly imported foreign word is transliterated into several possible candidate words based on pronunciation, out of which only a few survive in competition over a period of time.",
        "In this respect, a statistical approach makes sense where multiple transliteration variations exist for one word, generating candidates in probable order.",
        "In this paper, we present a statistical method to transliterate English words in Korean alphabet to generate various candidates.",
        "In the next section, we describe a phonetic mapping table construction.",
        "In Section 2, we describe alignment and syllabification methods, and in Section 3, mathematical formulation for a statistical model is presented.",
        "Section 4 provides experimental results, and finally, we state our conclusions."
      ]
    },
    {
      "heading": "1 Phonetic mapping table construction",
      "text": [
        "First of all, we generate a mapping between English and Korean phonetic unit pairs (Table 6).",
        "In doing so, we use pronunciation symbols for English words (Table 5) as defined in the Oxford computer-usable dictionary [12].",
        "The English and Korean phonetic unit can be a consonant, a vowel or some composite of them so as to make transliteration mapping unique and accurate.",
        "The orthography for foreign word transliteration to Korean provides a simple mapping from English to Korean phonetic units.",
        "But in reality, there are a lot of transliteration cases that do not follow the orthography.",
        "Table 6-1 has been constructed by examining a significant amount of corpus so that we can cover as many cases as possible.",
        "Table 6-2 shows complex cases where a combination of two or more English phonemes are mapped to multiple candidates of a composite Korean phonetic unit.",
        "This phonetic mapping table is carefully constructed so as to produce a unique candidate in syllabification and alignment in the training stage.",
        "When a given English pronunciation can be syllabificated into serveral units or a single composite unit, we adopt a heuristic that only the composite unit consisting of longer phonetic units is considered.",
        "For example, the English phonetic unit \"u@\" can be mapped to a Korean phonetic unit \"--°F-0-1 [u@]\" or \"1 [w@]\" even though the composition of each unit mapping of \"u\" and \"@\" can result in other composite mappings such as \"-PH [1u@]\", \"-1 0-1 [wi@]\", [wuja]\", etc.",
        "This composite phonetic unit mapping is also useful for statistical tagging since composite units provide more accurate statistical information when they are well devised."
      ]
    },
    {
      "heading": "2 Alignment and syllabification method",
      "text": [
        "The alignment and syllabification process is critical for probabilistic tagging as it is closely linked to computational complexity.",
        "There can be combinatorial explosion of state sequences because potential syllables may overlap the same letter sequences.",
        "A statistical approach called, Forward-Backward parameter estimation algorithm, is used by Sharman in phonetic transcription problem [2].",
        "But a statistical approach for syllabification requires expensive computational resources and a large amount of training corpus.",
        "Moreover, it often results in many improper candidates.",
        "In this paper, we propose a simple heuristic alignment and syllabification method that is fast and efficient.",
        "The main principle in separating phonetic units is to manage a phonetic unit of English and that of Korean to be mapped in a unique way.",
        "For example, the pronunciation notation \"@R\" of the suffix \"-er\" in \"computer\" is mapped to \"0-1 [@R]\" in Korean.",
        "In this case, the complex pronunciation \"@R\" is treated as one phonetic unit.",
        "There are many such examples in complex vowels, as in \"we\" to \"11 [wer, •\"jo\" to \"Si.",
        "[jo]\", etc.",
        "It is essential to come up with a phonetic unit mapping table that can reduce the time complexity of a tagger and also contribute to accurate transliteration results.",
        "Table 6 shows the examples of phonetic units and their mapping to Korean.",
        "The alignment process in training consists of two stages.",
        "The first is consonant alignment which identifies corresponding consonant pairs by scanning the English phonetic unit and Korean notation.",
        "The second is vowel alignment which separates corresponding vowel pairs within the consonant alignment results of stage 1.",
        "Figure 1 shows an alignment example in training.",
        "The aligned and syllabificated units are used to extract statistical information from the training corpus.",
        "The alignment process always produces one result.",
        "This is possible because of the predefined English to Korean phonetic unit mapping in Table 6.",
        "Figure 1 .",
        "shows an example of syllabification and alignment.",
        "To take the English word \"computer\" as an example, the English pronunciation notation \"k@mpu@R\" is retrieved from the Oxford dictionary.",
        "In the first stage, it is segmented in front of the consonants \"k\", \"m\", \"p\" and \"t\" which arc aligned with the corresponding Korean consonants \"--7-1 [k]\", \"1-] [m]\", \" [p1\" and \"E [t]\".",
        "In the second stage, it is segmented in front of the vowels \"@\", \"u\" and \"@R\" and aligned with the corresponding Korean vowels \" [0/ R]\", \"Ti [k]\" and \" [@R]\".",
        "The composite vowel \"0/ R\" is not divided into two simple vowels \"@\" and \"R\" since it is aligned to Korean \" [@R]\" in accordance with entry in Table 6-2.",
        "When it is possible to syllabificate in more than one ways, only the longest phonetic unit is selected so that an alignment always ends up being unique during the training process.",
        "After the training stage, an input English word must be syllabificated automatically so that it can be transliterated by our tagger.",
        "During this stage, all possible syllabification candidates are generated and are given as inputs to the statistical tagger so that the proper Korean notation can be found."
      ]
    },
    {
      "heading": "3 Statistical transliteration model",
      "text": [
        "A probabilistic tagger finds the most probable set of Korean notation candidates from the possible syllabificated results of English pronunciation notation.",
        "Lee [7, 8, 9] proposed a statistical transliteration model based on the statistical translation model-1 by Brown [2] that uses only a simple information source of a word pair.",
        "Various kinds of information sources are involved in the English to Korean transliteration problem.",
        "But it is not easy to systematically exploit various information sources by extending the Markov window in a statistical model.",
        "The tagging model proposed in this paper exploits not only simple pronunciation unit-to-unit mapping from English to Korean, but also more complex contextual information of multiple units mapping.",
        "In what follows, we explain how the contextual information is represented as conditional probabilities.",
        "a phonetic dictionary.",
        "Suppose that S can be segmented into a sequence of syllabificated units sis2 • • s„ where s; is an English phonetic unit as in Table 6.",
        "Also suppose that K is a Korean word, where lc; is the i-th phonetic unit of K.",
        "arg max P(E, K) arg max P(S,K) (2) = arg max P(K I S)P(S) where P(S) is called language model and P(KIS) is called translation model.",
        "P(S) is not constant given a fixed input word because there can be a number of syllabification candidates.",
        "In determining k, four neighborhood variables arc taken into account, while conventional tagging models use only two neighborhood variables.",
        "The extended Markov window of information source is defined as in Figure 2.",
        "It also shows a conventional Markov window using a clashed line.",
        "Mathematical formulation for Markov window extension is not an easy problem since extended window aggravates data sparseness.",
        "We will explain our solution in the next step.",
        "Now, the translation model, P( KIS) in equation (2) can be approximated by Markov assumption as follows.",
        "Now, our statistical tagging model can be formulated as Equation (7) when the translation model (5) and the language model (6) are applied to the transliteration model (2)",
        "Equation(3) still has data sparseness problem in=V-1)13(s I k;s;) argmaxn P(k )P(s ke gathering information directly from the training K corpus.",
        "So we expand it using Markov chain in order to replace the conditional probability term in (3) with more fragmented probability terms.",
        "In Equation (4), there are two kinds of approximations in probability terms.",
        "First, P(s; „) and P(s s;_,) are substituted for P(s ki„lc and P(s, respectively.",
        "This approximation is based on our heuristic that and s, I provide somewhat redundant information in determining si .",
        "Secondly, k and are substituted for",
        "respectively, based on a heuristic that k_A_, is farther off than ksi , and is redundant.",
        "Equation (4) can be reduced to Equation (5) because P(s_, 1 ki-,k,) of (4) is equivalent to P(k1",
        "The language model we use is a bigram language model (Brown et al.",
        "[6])",
        "<Figure 3> Statistical information source used in the extended Markov window",
        "Figure 3 pictorially summarizes the final information sources that our statistical tagger utilizes.",
        "It can be thought of as a generalized case of prevalent Part-of-Speech tagging model.",
        "When P(k is approximated as",
        "reduced to a conventional bigram tagging model (Eq.",
        "8), that is a base model of Brown model-1 [2], Charniak [4], Merialdo [11] and Lee [7, 8, 9].",
        "arg max P(S, K) arg max fl P(ic, I 1(1_,)P(si ki) (8) Equation (7) is the final tagging model proposed in this paper.",
        "We use a back-off strategy [10, 11] as follows, because our tagging model may have a data sparseness problem.",
        "Each probability term in equation (7) is obtained from the training data.",
        "The statistical tagger modeled here uses Viterbi algorithm [12] for its search to get N-Best candidates."
      ]
    },
    {
      "heading": "4. Experimental results",
      "text": [
        "For the evaluation we constructed a training corpus of 8368 English-Korean word pairs.",
        "One English word can have one or more Korean transliteration entries in the corpus.",
        "90% of the corpus is used as training data and 10% of the corpus as test data.",
        "For more objective experiment evaluation, we estimated word-level accuracy based on exact string match even though many other papers are based on lexical-level distance to the correct word.",
        "We adopted a recall measure based on word-level accuracy.",
        "Recall measure is the average number of generated correct words divided by the total word count of prepared correct answer set given an input word (Eq.",
        "9).",
        "Precision measure is the average number of retrieved correct words divided by the number of generated candidates (Eq.",
        "10).",
        "For words not found in the pronunciation dictionary, a transcription automata is used to transform the English alphabet to the Korean alphabet.",
        "A transcription automata can be helpful because it uses alphabetic information that our statistical tagger does not use.",
        "The automata produces one result and attaches it at the end of N-best results of the statistical tagger.",
        "This automata has about 500 transcription rules, based on previous, current, and next context window and production alphabet.",
        "All experimental results are estimated by 10-fold cross validation for more accurate results.",
        "Table 1 shows the estimated recall values for the 10- best results generated by the tagger and for the case when transcription automata used as well.",
        "Figure 4 shows recall values given a number of candidates.",
        "We estimated recall values in the same environment for conventional tagging model (Eq.",
        "8) in order to compare accuracy improvement by the Extended Markov window model (Eq.",
        "7) without transcription automata (Table 2).",
        "results of the proposed Extended Markov model with conventional tagging model It cannot be compared directly with the results of other models since other related works are on a different domain and they adopt different evaluation measures such as lexical-level accuracy [1, 7, 10].",
        "There is a model that is in the same domain, i .e., English-to-Korean transliteration [1, 8, 9], but it adopts a lexical-level accuracy measure [7, 9], or a subjective evaluation measure such as human judgment [8, 9].",
        "Table 3 shows the comparison with Lee's model which adopted the average value of trained and untrained results, even though the average of trained and untrained results makes no sense since a registered dictionary lookup method can make all experiments on trained data 100% accurate.",
        "Accuracy measure on untrained data should be the measure for comparison among different experiments for fairness.",
        "Lee's model is a fully statistical approach even in pronunciation unit alignment and syllabification that may cause inaccurate results, while we use a heuristic approach in pronunciation unit alignment.",
        "Another significant difference is that Lee's model uses only conventional information sources such as a bigram while our model use various information sources from extended Markov window.",
        "Lee's model transliterates using English alphabet directly without pronounciation dictionary so that it can be better for unknown words or proper noun.",
        "Table 4 shows the comparison with MBRtalk [16], Neural Network [10], Weighted finite-state acceptor (WFSA) [19] and direct transliteration [9] even though they are based on different problem domains.",
        "MBRtalk and Neural Network models are based on English word's pronunciation generation.",
        "WFSA is for English-to-Japanese transliteration.",
        "Experiment for training data in MBRtalk makes no sense, since it finds the most similar word in a database that stores all the training data; thus the result would always produce the exact answer.",
        "The results show that the model we propose indicates the good performance."
      ]
    },
    {
      "heading": "Conclusion",
      "text": [
        "We have proposed a statistical English-to-Korean transliteration model that exploits various information sources.",
        "This model is a generalized model from a conventional statistical tagging model by extending Markov window with some mathematical approximation techniques.",
        "An alignment and syllabification method instead of a statistical method is proposed for accurate and fast operation.",
        "The experimental results show that the model proposed in this paper demonstrates significant improvement in its performance."
      ]
    }
  ]
}
