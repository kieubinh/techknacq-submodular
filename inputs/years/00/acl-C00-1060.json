{
  "info": {
    "authors": [
      "Hiroshi Kanayama",
      "Kentaro Torisawa",
      "Yutaka Mitsuishi",
      "Jun'ichi Tsujii"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1060",
    "title": "A Hybrid Japanese Parser With Hand-Crafted Grammar and Statistics",
    "url": "https://aclweb.org/anthology/C00-1060",
    "year": 2000
  },
  "references": [
    "acl-C00-2110",
    "acl-C92-2066",
    "acl-E99-1026",
    "acl-J93-1002",
    "acl-J96-1002",
    "acl-P98-1083",
    "acl-P98-2144",
    "acl-W97-0301",
    "acl-W98-1114"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "'This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique.",
        "The key feature of our system is that in order to estimate likelihood for a parse tree, the system uses information taken from alternative partial parse trees generated by the grammar.",
        "This utilization of alternative trees enables us to construct a new statistical model called nipleilQuadruplet Model.",
        "We show that this model can capture a certain tendency in Japanese syntactic structures and this point contributes to improvement of parsing accuracy on a shallow level.",
        "We report that, with an underspecified HPSC-based grammar and a maximum entropy estimation, our parser achieved high accuracy: 88.6% accuracy in dependency analysis of the EDR, annotated corpus, and that it outperformed other purely statistical parsing methods On the same corpus.",
        "This result suggests that proper treatment of hand-crafted grammars can contribute to parsing accuracy on a shallow level."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "There have been many attempts to combine hand-crafted high-level grammars, such as FB-LTAG, HPSG and LFG, and statistical disambiguation techniques to obtain precise linguistic structures (Schabes, 1992; Abney, 1996; Carroll et al., 1998).",
        "One evident advantage of this approach over purely statistical parsing techniques is that grammars can provide precise semantic representations.",
        "However, considering that remarkable parsing accuracy in a shallow level has been achieved by purely statistical techniques (e.g. R,atnaparkhi (1997)), it may be thought more reasonable to use high-level grammars of shallow just for postprocessing which maps results 1 11 syntactical analyses onto deep analyses.",
        "This work was conducted while the first author was a graduate student at Univ.",
        "of Tokyo.",
        "In this work we propose that hand-crafted high-level grammars can be useful in shallow-level analyses and statistical models.",
        "In our framework, grammars are used to obtain precise features for probability estimation, which are difficult to obtain without a grammar, and we show that such features contribute to high parsing accuracy on a shallow level.",
        "In this paper, the most preferable parse trees are chosen with a statistical model.",
        "In our method, the likelihood value L(M) of a (partial) tree M in Figure 1 is defined as in (1):",
        "where NH is M's non-head daughter (whose lexical head is n), H is the head-daughter (whose lexical head is h), and P(7), h) is the probability of u being related to h. For a single lexical item W, L(W) is defined as 1.0.",
        "In most models already proposed, the probability Pen /t) is calculated with the conditional probability (2): Pta h)dg P(T (I)„, 'Ph ) (2) where '1' indicates that the dependency is true; (1)„ and Th are attributes of n and h, respectively.",
        "And A„h, the distance between the two words, is widely used, because this attribute is believed to strongly affect whether those two words are going to be related.",
        "In contrast, in the statistical model proposed in this paper, P(n, h) depends not only on the attributes of the tree M, but also on alternative trees",
        "and r' denote the bunsetsus 1 and r belong to, respectively.",
        "in the parse forest generated by the grammar.",
        "More precisely, when P(n h) is calculated, we consider partial trees whose non-head daughter's lexical head is n, as displayed in Figure 2.",
        "Here alternative possible hk (k = 1, • • , 1) are taken into consideration, and ordered according to their distance to n. We call such set of hk modification candidates, and all modification candidates are placed together in the conditional part of the probability as in (3).",
        "Now assume Ii =",
        "where \"i\" indicates the ith candidate among the modification candidates.",
        "Equation (3) shows two important properties of our model.",
        "One point lies in the new distance metric.",
        "(3) is the probability that n chooses the ith candidate as the modifiee among the modification candidates which are ordered according to their distance to n. Thus, we no longer require the distance metric An,1„ instead we use the relative position among the modification candidates, winch works as an attribute of the modification.",
        "The other point is the use of the attributes of the alternative parse trees, that is, attributes of the modifier and all its modification candidates are considered simultaneously.",
        "We show that these techniques sophisticate our model, by providing linguistic examples in Section 3.2.",
        "In practice, however, treating all candidates is not feasible because of data-sparseness.",
        "We therefore apply a strategy of restricting the modification candidates to at most three.",
        "The strategy and its justification are discussed in Section 3.1.",
        "Applying the strategy to the equation (3), we obtain equations (4) and (5):",
        "When there are only two candidates, equation (4) is used; otherwise, equation (5) is used.",
        "Our statistical model is called the •iplet/Quadruplet Model, which was named after the number of constituents in the conditional parts of the equations.",
        "We report that our parsing framework achieved high accuracy (88.6%) in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar, SLUNG (Mitsu-ishi et al., 1998) and the maximum entropy method (Berger et al., 1996).",
        "Moreover, the resulting parse trees generated by our hybrid parser are legitimate trees in terms of given hand-crafted grammars, and we are expecting that we can enjoy advantages provided by high-level grammar formalisms, such as construction of semantic structures.",
        "In the above explanation, we used the notion of lexical heads for the estimation of probabilities of trees for the sake of simplicity.",
        "But, in the present implementation, we use bunsetsus instead of lexical heads, and a relation on a tree is converted to a bunsetsu-dependency as shown in Figure 3.",
        "A bunsetsu is a basic syntactic unit in Japanese.",
        "It consists of a content word and some functional morphemes such as a particle.",
        "In Section 2, we describe some existing statistical parsers, and the Japanese grammar which we adopted.",
        "Section 3 describes our statistical method and its advantages in detail.",
        "We report experimental results in Section 4."
      ]
    },
    {
      "heading": "2 Background",
      "text": [
        "In this section, we describe several models for Japanese dependency analysis and works on statistical approaches with grammars.",
        "Next, we introduce SLUNG, the HPSG-based Japanese grammar which is used in our hybrid parser."
      ]
    },
    {
      "heading": "2.1 Previous Dependency Analysis Models of Japanese",
      "text": [
        "Several statistical models for Japanese dependency analysis which do not utilize a hand-crafted grammar have been proposed.",
        "We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison.",
        "All models introduced below are based on the likelihood value of the dependency between two bunsetsus.",
        "But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated.",
        "There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998).",
        "Attributes (Di and kIfi consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on.",
        "And Ai,j",
        "is the number of intervening bunsetsus between i and PO: A V PCr (1)i,tIi(G) However, these models fail to reflect contextual information because attributes of t;he surrounding bunsetsus are not considered.",
        "Uchimoto et al.",
        "(2000) proposed a model using posterior context.",
        "The model utilizes not only attributes about; bunsetsus j but also attributes about all bunsetsus (including j) which follow bunsetsu i.",
        "That is, instead of learning two output values \"T(true)\" or \"F(false)\" for the dependency between two bunsetsus, three output values are used for learning: the bunsetsu i is \"bynd (dependent on a bunsetsu beyond j)\", \"dpnd (dependent on the bunsetsu j)\" or \"btwn (dependent on a bunsetsu between i and j)\".",
        "The probability is calculated by multiplying probabilities for all bunsetsus which follow bunsetsu i as in (7).",
        "They report; that this kind of contextual information improves accuracy.",
        "However, the model has to assume the independency of all the random variables, which may cause some errors.",
        "The difference between our model and these previous models are discussed in Section 3."
      ]
    },
    {
      "heading": "2.2 Statistical Approaches with a grammar",
      "text": [
        "There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997).",
        "The main issue in tins type of research is how to assign likelihoods to a single linguistic structure generated by a grammar.",
        "Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat; information on contexts, but the contextual information is derived only from a structure to winch the parser is trying to assign a likelihood value.",
        "Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for linguistic structures."
      ]
    },
    {
      "heading": "2.3 SLUNG : Japanese Grammar",
      "text": [
        "The Japanese grammar which we adopted, SLUNG (Mitsuishi et al., 1998), is an HPSG-based underspecified grammar.",
        "It consists of 8 rule schemata, 48 lexical templates for POSs and 105 lexical entries for functional words.",
        "As can be seen from these figures, the grammar does not contain detailed lexical information that needs intensive labor for development.",
        "However, it is precise ill the sense that it achieves 83.7% dependency accuracy with a simple heuristics for the EMI annotated corpus, and it can produce at least one parse tree for 98.4% sentences in the EMI annotated corpus.",
        "We use the grammar for generating parse tree forests, and our Triplet/Quadruplet Model is used for picking up a single tree from a forest."
      ]
    },
    {
      "heading": "3 The Hybrid Parsing Method",
      "text": [
        "This section describes the procedure of parsing with the Triplet/Quadruplet Model.",
        "Our hybrid parsing method proceeds as follows:",
        "• At the beginning, dependency structures are obtained from trees generated by SLUNG.",
        "For each bunsetsu, modification candidates are enumerated, and if there are four or more candidates, they are restricted to three.",
        "The heuristic used in tins process is described in Section 3.1.",
        "• Then, with the Triplet/Quadruplet Model and maximum entropy estimation, probabilities of the dependencies are calculated.",
        "Section 3.2 discusses the characteristics and advantages of the model.",
        "• Finally, t;he most preferable trees for the whole sentence are selected."
      ]
    },
    {
      "heading": "3.1 Restriction of Modification Candidates",
      "text": [
        "Kanayama et al.",
        "(1999) report that when modification candidates are enumerated according to SLUNG, 98.6% of the correct modifiees are in one of t;he following three positions among the candidates: the nearest one from the modifier, t;he second nearest one, and the farthest one.",
        "As a consequence, we can simplify the problem by considering only these three candidates and discarding the other candidates, with only 1.4% potential errors.",
        "We therefore assume that the number of modification candidates is always three or less.",
        "This idea is similar to that of Sekine (2000)'s study, which restricts the candidates to five, but in his case, without a grammar."
      ]
    },
    {
      "heading": "3.2 The Triplet/Quadruplet Model",
      "text": [
        "The Triplet/Quadruplet Model calculates the likelihood of the dependency between bunsetsu i and bunsetsu c„; P(i c„) with the formulas (8) and (9), where c„ denotes the nth candidate among bunsetsu i's candidates; (Di denotes some attributes of i; and the, denotes attributes of e0 (including attributes between i and c,1).",
        "As (8) and (9) suggest, the model considers attributes of the modifier bunsetsu and attributes of all modification candidates simultaneously in the conditional parts of the probabilities.",
        "Moreover, what is calculated is not the probability of \"whether the dependency is correct (T, see Formula(6))\", but the probability of \"which of the given candidates is chosen as the modifiee (n =1, 2, or 1)\".",
        "These characteristics imply the following two advantages.",
        "Advantage 1 A new distance metric.",
        "The correct modifiee can be chosen by considering relative position among grammatically licensed candidates, instead of the absolute distance between bunsetsus.",
        "Thus, P(kare-ga --quishiru-no-wo) has the same value for both examples.",
        "Our interpretation of this difference is summarized as follows.",
        "The word yukkuri is all adverb modifying the verb hashiru.",
        "Our linguistic intuition tells us that the presence of such adverb should not affect the strength for the dependency between kare-ga and hashiru-no-wo.",
        "According to this intuition, the existence of the adverb should be considered as a noise.",
        "Our model allows us to ignore such a noise in learning from annotated corpus, while previous models are affected by such noisy elements.",
        "Contrary to the previous examples, Tam-no in (11) modifies different modification candidates.",
        "In example (11a), \"Taro-no – > Tnusum,e\" is the correct dependency while \"747'0-no ---musume\" is not cor- rect in (11b).",
        "This difference is caused by the bunsetsu between Taro-no and musume, kawaii (Adj) ill (11a) and gn4jin-no (NP) in (lib).",
        "Actually, the grammar allows Taro-no to depend on either of these types of words.",
        "Thus, in our model,",
        "Then, P(ll'aro-no –*musume) has different values for the two examples.",
        "In the annotated corpus, P(21 Taro-no, kawaii, musume) tends to have a high value since kawaii is an adjective.",
        "However, since yuujin-no is an NP, P(21 Taro-no, yuujin-no, 97111,5117710 tends to have a low value.",
        "Now consider previous models.",
        "Then, contrary to our model, P(Taro-no – >musume) has exactly the same value for both examples.",
        "The outcome is determined by Pa(Taro-no kawaii) P(T1 Taro-no, kawaii, 1) In text corpora, P(TI Tare-no, yuujin-no, 1) tends to be high, and consequently, P(TI Taro-no, musume, 2) is very small.",
        "These values will make the correct prediction for (lib) as yuujin-no will be favored over musume.",
        "However, for (11a), these models are likely to incorrectly favor kawaii over musume.",
        "This is Advantage 2 Treating alternative trees.",
        "The candidates are taken into consideration simultaneously.",
        "But because the modification candidates are restricted to at most three, we considerably avoid data-sparseness problems.",
        "Below we discuss these advantages in order.",
        "These advantages clarify the differences from previous models described in Section 2.1, and are empirically confirmed through the experiments in Section 4.",
        "3.2.1 Advantage 1 : A new distance metric As discussed ill Section 2.1, the distance metric used in previous statistical methods was obtained simply by counting intervening words or bunsetsus between i and j.",
        "On the other hand, we use the relative position among the modification candidates as the distance metric.",
        "The following examples illustrate a difference between those two types of metric.",
        "The correct modifiee of kare-ga is hashiru-no-wo in both (10a) and (lob).",
        "(10)a. kare-ga hashiru-no-wo mita koto",
        "In previous models, (10a) and (10b) would yield, P„(kare-ga – > hashiru-no-wo)=P kare-ga, hashiru-no-wo, A ) Pb(tare-ga hashiru-no-wo)=P(Tlkare-ga, hashiru-no-wo, A2 ) respectively, where A1 = 1 and A2 = 2.",
        "Then, the two probabilities above do not have the same value in general.",
        "Our grammar does not allow the dependency \"kare-ga ---yukkuri\" for (10b).",
        "The modification candidates of kare-ga are hashiru-no-wo and mita, hence (8) gives the probabilities between kare-ga, and hashiru-no-wo as follows, in both examples.",
        "because P(.171 Thro-no, MILSILITLe, 2), being very small, is likely to be smaller than P(1.1 Thee-no, kawaii, 1)."
      ]
    },
    {
      "heading": "4 Experiments and Discussion",
      "text": [
        "This section reports a series of parsing experiments with our model, and gives sonic discussion."
      ]
    },
    {
      "heading": "4.1 Environments",
      "text": [
        "We used the EMI, Japanese Corpus (EDR, 1996) for training and evaluation of parsing accuracy.",
        "l'he EDR, Corpus is a Japanese treebank winch consists of 208,157 sentences from newspapers and magazines.",
        "We used 192,778 sentences for training, 6,744 for pre-analysis (as reported in Section 3.1), and 3,372 for testing:).",
        "With triplets constituted of a modifice and two modification candidates extracted from the learn-Mg corpus the Triplet Model is constructed.",
        "With the quadruplets constituted of a modifiee and three candidates, the Quadruplet Model is constructed.",
        "These models are estimated by the ChoiceMaker Maximum Entropy Estimator (Borthwick, 1999).",
        "The features for the estimation are listed in 'Ethic 1.",
        "The values partially follow other researches e.g. Uchimoto et al.",
        "(1999), and JUMAN's outputs are used for PUS classification.",
        "Mainly the head of the bunsetsu (the rightmost morpheme in a bunsetsu except for whose major POS is \"peculiar\", \"auxiliary verb\", \"particle\", \"suffix\" or \"copula\") and type of the bunsetsu (the rightmost morpheme in a bunsetsu except for whose major POS is \"peculiar\") arc used as the attributes.",
        "We show the meaning of some features below.",
        "PUS JUMAN's minor POS (for both \"head\" and \"type\").",
        "particle, adverb Frequent words: 20 particles and 69 adverbs.",
        "head lex 294 lexical forms regardless of their POS.",
        "type lex 70 suffixes or auxiliary verbs.",
        "inflection 6 types of inflection : \"normal\", \"adverbial\", \"adnominal\", \"tc-form\", \"ta-form\", and \"others\".",
        "The column \"variation\" in Table 1 denotes the number of possible values for the feature.",
        "\"Valid features\" indicates the number of features which appeared three times or more in the training corpus."
      ]
    },
    {
      "heading": "4.2 Results",
      "text": [
        "With our model and the features described above, the accuracy shown in Table 2 is achieved.",
        "We evaluate the following two types of accuracy: 35,263 sentences were removed because the order of the words in the annotation differed from that in the original sentences.",
        "Bunsetsu accuracy 'rhe percentage of banscisIN whose modifiee is correctly identified.",
        "The denominator includes all bunsetsus except for the last bunsetsv, of a sentence.",
        "Sentence accuracy The percentage of sentences whose dependencies are perfectly correct.",
        "\"In-coverage sentences\" is the accuracy for the sentences for which SLUNG could generate parse trees.",
        "We give the accuracy for \"All sentences\" too, by partially parsing sentences which SLUNG fail to parse.",
        "The coverage of SLUNG is about 99%, thus high accuracy is achieved even for \"All sentences\".",
        "Moreover, we conducted a series of experiments in order to evaluate the contribution of each characteristic in our parsing model.",
        "The parsing schemes used are the four in Figure 3.",
        "Major differences among them are (I) whether a grammar is used, (II) whether modification candidates are restricted to three, and (III) whether a previous pair model with Formula (6) or the Triplet/Quadruplet Model with Formula (8),(9) was used.",
        "W/O Grammar Model This model does not use a grammar.",
        "Likelihood values for dependen",
        "(8), (9).",
        "cies are calculated for all bunsetsus that follow a modifier bunsetsu.",
        "Formula (6) is used, and as a distance metric Ai,j, the number of bun-seism between the modifier and the modifiee4 are combined with all features.",
        "In general lines, this model corresponds to models such as (Fu-jio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999).",
        "W/O Restriction Model Modification candidates are restricted by SLUNG.",
        "The remaining is the same as the W/O Grammar Model.",
        "Pair Model Modification candidates are restricted to three, in the way described in Section 3.1.",
        "The remaining is the same as W/O Grammar Model.",
        "Triplet/Quadruplet Model This is the model proposed in the paper.",
        "Modification candidates are restricted to three, and Formula (8) or (9) are used.",
        "From the result shown in Table 3, we can say our method contributes to the improvement of our parser, because of the following reasons: • The Triplet/Quadruplet Model outperforms the Pair Model by 0.9%.",
        "Both of them restricts modification candidates to three, but the accuracy got higher when all candidates are considered simultaneously.",
        "It is because of the two advantages described in Section 3.2.",
        "• The Pair Model outperforms the W/O Restriction Model by 0.3%.",
        "Thus the restriction of modification candidates does not reduce the accuracy.",
        "• The W/O Restriction Model outperforms the W/O Grammar Model by 0.7%.",
        "This means that the use of a grammar as a preprocessor works well to pick up possible modffiee.",
        "We found that many structures similar to the ones described in Section 3.2 appeared in the EDR",
        "corpus.",
        "Our Triplet/Quadruplet model could treat these structures precisely as we intended.",
        "This is the main factor that contributed to the improvement of the overall parsing accuracy.",
        "Based on the above experiments, we can say that our approach to use the grammar as a preprocessor before the calculating of the probability is appropriate for the improvement of parsing accuracy."
      ]
    },
    {
      "heading": "4.3 Comparison to other models",
      "text": [
        "There are several works which use the EDR corpus for evaluation.",
        "The decision tree model (Haruno et al., 1998) achieves around 85%, the integrated model of lexical/syntactic information (Shirai et al., 1998) achieves around 86%, and the lexicalized statistical model (Fujio and Matsumoto, 1999) achieves 86.8% in bunsetsu accuracy.",
        "Our model outperforms all of them by 2 or 3%.",
        "Shirai et al.",
        "(1998) used the Kyoto University text corpus (Kurohashi and Nagao, 1997) for evaluation and achieved around 86%.",
        "Uchimoto et al.",
        "(2000) also used the Kyoto corpus, and their accuracy was 87.9%.",
        "For comparison, we applied our method to the same 1,246 sentences that Uchimoto et al.",
        "(2000) used.",
        "The result is shown in Table 4.",
        "Our result is worse than theirs.",
        "The reason is thought to be as follows: • We use the EDR corpus for training.",
        "Although we used around 24 times the amount of training data that Ucliimoto et al.",
        "used, our training data lead to errors in the analysis of the Kyoto Corpus, because of differences in the annotation schemes adopted.",
        "• Uchimoto et al.",
        "used the correct morphological analyses, but we used JUMAN.",
        "Sometimes this may cause errors.",
        "• The grammar SLUNG was designed for the EDR corpus, and some types of structures in the Kyoto Corpus are not allowed.",
        "Clearly, our parser should be improved to overcome these problems and compared with other works directly.",
        "4.4 Discussion and Future Work The following are some observations about the speed of our parser.",
        "Existing statistical parsers arc quite efficient compared to grammar-based systems.",
        "Particularly, our system used an HPSG-based grammar,",
        "whose speed is said to be slow.",
        "However, recent advances in HPSG parsing (lbrisawa et al., 2000) enabled us to obtain a unique parse tree with our system in 0.5 sec.",
        "in average for sentences in the EDR.",
        "corpus.",
        "Future work shall extend SLUNG so that semantic representations are produced.",
        "Carroll et al.",
        "(1.998) discussed the precision of argument structures.",
        "We believe that the focus of our study will shift from a shallow level to such a deeper level for our final aim, realization of intelligent natural language processing systems."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We presented a hybrid parsing scheme that uses a hand-crafted grammar and a statistical technique.",
        "As other hybrid parsing methods, the statistical technique is used for picking up the most preferable parse tree from the parse forest generated by the grammar.",
        "The difference from other works is that the precise contextual information needed to estimate the likelihood of a parse tree is obtained from alternative parse trees generated by the grammar, and that such contextual information from alternative trees enables us to construct our new statistical model called the Triplet/Quadruplet model.",
        "We have shown that these points contributed to substantial improvement of parsing accuracy in Japanese dependency analysis, through a series of experiments using an HPSG-based Japanese grammar SLUNG and the maximum entropy method."
      ]
    }
  ]
}
