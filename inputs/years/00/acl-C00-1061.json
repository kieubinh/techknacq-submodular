{
  "info": {
    "authors": [
      "In-Ho Kang",
      "Gilchang Kim"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1061",
    "title": "English-To-Korean Transliteration Using Multiple Unbounded Overlapping Phoneme Chunks",
    "url": "https://aclweb.org/anthology/C00-1061",
    "year": 2000
  },
  "references": [
    "acl-J96-1002",
    "acl-P97-1017"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present in this paper the method of English-to-Korean(E-K) transliteration and back-transliteration.",
        "In Korean technical documents, many English words are transliterated into Korean words in various forms in diverse ways.",
        "As English words and Korean transliterations are usually technical terms and proper nouns, it is hard to find a transliteration and its variations in a dictionary.",
        "Therefore an automatic transliteration system is needed to find the transliterations of English words without manual intervention.",
        "To explain E-K transliteration phenomena, we use phoneme chunks that do not have a length limit.",
        "By applying phoneme chunks, we combine different length information with easy.",
        "The E-K transliteration method has three steps.",
        "In the first, we make a phoneme network that shows all possible transliterations of the given word.",
        "In the second step, we apply phoneme chunks, extracted from training data, to calculate the reliability of each possible transliteration.",
        "Then we obtain probable transliterations of the given English word."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In Korean technical documents, many English words are used in their original forms.",
        "But sometimes they are transliterated into Korean in different forms.",
        "Ex.",
        "1, 2 show the examples of various transliterations in KTSET 2.0(Park et al., 1996).",
        "These various transliterations are not negligible for natural language processing, especially in information retrieval.",
        "Because same words are treated as different ones, the calculation based on the frequency of word would produce misleading results.",
        "An experiment shows that the effectiveness of information retrieval increases when various forms including English words are treated equivalently Wong et al., 1997).",
        "We may use a dictionary, to find a correct transliteration and its variations.",
        "But it is not feasible because transliterated words are usually technical terms and proper nouns that have rich productivity.",
        "Therefore an automatic transliteration system is needed to find transliterations without manual intervention.",
        "There have been some studies on E-K transliteration.",
        "They tried to explain transliteration as phoneme-per-phoneme or alphabet-per-phoneme classification problem.",
        "They restricted the information length to two or three units before and behind an input unit.",
        "In fact, many linguistic phenomena involved in the E-K transliteration are expressed in terms of units that exceed a phoneme and an alphabet.",
        "For example, 'a' in ' ace' is transliterated into \"all 01 (cyi,)\" but in 'acetic', \"01 (e)\" and in 'acetone', \"Of (a)\" .",
        "If we restrict the information length to two alphabets, then we cannot explain these phenomena.",
        "Three words get the same result for 'a'.",
        "(3) ace 011012---(eyisu) (4) acetic 01A1 (esithik)",
        "(5) acetone 01,112-(aseython) In this paper, we propose the E-K transliteration model based on phoneme chunks that do not have a length.",
        "lfinit and can explain transliteration phenomena in some degree of reliability.",
        "Not a alphabet-per-alphabet but a chunk-per-chunk classification problem.",
        "This paper is organized as follows.",
        "In section 2, we survey an E-K transliteration.",
        "In section 3, we propose phoneme chunks based transliteration and back-transliteration.",
        "In Section 4, the results of experiments are presented.",
        "Finally, the conclusion follows in section 5."
      ]
    },
    {
      "heading": "2 English-to-Korean transliteration",
      "text": [
        "E-K transliteration models are classified in two methods: the pivot method and the direct method.",
        "In the pivot method, transliteration is clone in two steps: converting English words into pronunciation symbols and then converting these symbols into Korean words by using the Korean standard conversion rule.",
        "In the direct method, English words are directly converted to Korean words without intermediate steps.",
        "An experiment shows that the direct method is better than the pivot method in finding variations of a transliteration(Lee and Choi, 1998).",
        "Statistical information, neural network and decision tree were used to implement the direct method."
      ]
    },
    {
      "heading": "2.1 Statistical Transliteration method",
      "text": [
        "An English word is divided into phoneme sequence or alphabet sequence as en , 02, , Then a corresponding Korean word is represented as k2, , k„.",
        "If a corresponding Korean character (ki) does not exist, we fill the blank with `-'.",
        "For example, an English word \"dressing\" and a Korean word \"1.:-Ti 4 '1(tuleysing)\" are represented as Fig. 1.",
        "The upper one in Fig. 1 is divided into an English phoneme unit and the lower one is divided into an alphabet unit.",
        "The problem in statistical transliteration method is to find out the most probable transliteration for a given word.",
        "Let p(K) be the probability of a Korean word K, then; for a given English word E, the transliteration probability of a word K can be written as p(K1E).",
        "By using the Bayes' theorem, we can rewrite the transliteration problem as follows:",
        "With the Markov Independent Assumption, we approximate p(K) and p( as follows:",
        "As we do not know the pronunciation of a given word, we consider all possible phoneme sequences.",
        "For example, 'data' has following possible phoneme sequences, `d-a-t-a, da-ta, As the history length is lengthened, we can get more discrimination.",
        "But long history information causes a data sparseness problem.",
        "In order to solve a sparseness problem, Maximum Entropy Model, Back-off, and Linear interpolation methods are used.",
        "They combine different statistical estimators.",
        "(Tae-il Kim, 2000) use up to five phonemes in feature function(Berger et al., 1996).",
        "Nine feature functions are combined with Maximum Entropy Method."
      ]
    },
    {
      "heading": "2.2 Neural Network and Decision Tree",
      "text": [
        "Methods based on neural network and decision tree deterministically decide a Korean character for a given English input.",
        "These methods take two or three alphabets or phonemes as an input and generate a Korean alphabet or phoneme as an output.",
        "(Jung-Jae Kim, 1.999) proposed a neural network method that uses two surrounding phonemes as an input.",
        "(Kang, 1999) proposed a decision tree method that uses six surrounding alphabets.",
        "If an input does not cover the phenomena of proper transliterations, we cannot get a correct answer.",
        "Even though we use combining methods to solve the data sparseness problem, the increase of an information length would double the complexity and the time cost of a problem.",
        "It is not easy to increase the information length.",
        "To avoid these difficulties, previous studies does not use previous outputs(ki_i).",
        "But it loses good information of target language.",
        "Our proposed method is based on the direct method to extract the transliteration and its variations.",
        "Unlike other methods that determine a certain input unit's output with history information, we increase the reliability of a certain transliteration, with known E-K transliteration phenomena (phoneme chunks).",
        "3 Transliteration using Multiple unbounded overlapping phoneme chunks For unknown data, we can estimate a Korean transliteration from handwritten rules.",
        "We can also predict a Korean transliteration with experimental information.",
        "With known English and Korean transliteration pairs, we can assume possible transliterations without linguistic knowledge.",
        "For example, 'scalar' has common part with `scalc:±1c0.",
        "(sukheyil)' , casino:A-1,-__(khaeino)' , 'koala:51<gal- (khoalla)' , and ear:4 (k/ia)' (Fig.",
        "2).",
        "We can assume possible transliteration with these words and their transliterations.",
        "From `scale' and its transliteration (sukheyil), the sc' scalar' can be transliterated as •=i (sukh)'.",
        "From a 'casino' example, the 'c' has more evidence that can be transliterated as --7(kh)'.",
        "We assume that we can get a correct Korean transliteration, if we get useful experimental information and their proper weight that represents reliability."
      ]
    },
    {
      "heading": "3.1 The alignment of an English word with a Korean word",
      "text": [
        "We can align an English word with its transliteration in alphabet unit or in phoneme unit.",
        "Korean vowels are usually aligned with English vowels and Korean consonants are aligned with English consonants.",
        "For example, a Korean consonant, (p)' can be aligned with English consonants 'b', 'p', and 'v'.",
        "With this heuristic we can align an English word with its transliteration in an alphabet unit and a phoneme unit with the accuracy of 99.4%(Kang, 1999)."
      ]
    },
    {
      "heading": "3.2 Extraction of Phoneme Chunks",
      "text": [
        "Prom aligned training data, we extract phoneme chunks.",
        "We enumerate all possible subsets of the given English-Korean aligned pair.",
        "During enumerating subsets, we add start and end position information.",
        "From an aligned data \"dressing\" and \"_Eell-AJ (tuleysing)\" , we can get subsets as Table 12.",
        "The context stands for a given English alphabets, and the output stands for its transliteration.",
        "We assign a proper weight to each phoneme chunk with Equation 4.",
        "Equation 4 shows that the ambiguous phenomenon gets the less evidence.",
        "The chunk weight is transmitted to each phoneme symbol.",
        "To compensate for the length of phoneme, we multiply the length of phoneme to the weight of the phoneme chunk(Fig.",
        "3).",
        "2© means the start and end position of a word",
        "This chunk weight; does not mean the reliability of a given transliteration phenomenon.",
        "We know real reliability, after all overlapping phoneme chunks are applied.",
        "The chunk that has some common part with other chunks gives a context information to them.",
        "Therefore a chunk is not only an input; unit but also a means to calculate the reliability of other chunks.",
        "We also extract the connection information.",
        "From aligned training data, we obtain all possible combinations of Korean characters and English characters.",
        "With this connection information, we exclude impossible connections of Korean characters and English phoneme sequences.",
        "We can get the following connection information from \"dressing\" example(Table 2)."
      ]
    },
    {
      "heading": "3.3 A Transliteration Network",
      "text": [
        "For a given word, we get; all possible phonemes and make a Korean transliteration network.",
        "Each node in a network has an English phoneme and a corresponding Korean character.",
        "Nodes are connected with sequence order.",
        "For example, 'scalar' has the Korean transliteration network as Fig. 4.",
        "In this network, we disconnect some nodes with extracted connection information.",
        "After drawing the Korean transliteration network, we apply all possible phoneme chunks to the network.",
        "Each node increases its own weight with the weight of phoneme symbol in a phoneme chunks (Fig.",
        "5).",
        "By overlapping the weight, nodes in the longer chunks get more evidence.",
        "Then we get the best path that has the"
      ]
    },
    {
      "heading": "4 E-K back-transliteration",
      "text": [
        "E-K back transliteration is a more difficult; problem.",
        "than E-K transliteration.",
        "During the E-K transliteration, different alphabets are treated equivalently.",
        "For example, 1, p' and 'v, b' are transliterated into (ph)' and (p)' respectively and the long sound and the short sound are also treated equivalently.",
        "Therefore the number of possible English phonemes per a Korean character is bigger than the number of Korean characters per an English phoneme.",
        "The ambiguity is increased.",
        "In E-K back-transliteration, Korean phonemes and English phonemes switch their roles.",
        "Just switching the position.",
        "A Korean word is aligned with an English word in a phoneme unit or a character unit(Fig.",
        "6)."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "Experiments were done in two points of view: the accuracy test and the variation coverage test"
      ]
    },
    {
      "heading": "5.1 Test Sets",
      "text": [
        "We use two data sets for an accuracy test.",
        "Test Set I is consists of 1,650 English and Korean word pairs that aligned in a phoneme unit.",
        "It was made by (Lee and Choi, 1998) and tested by many methods.",
        "To compare our method with other methods, we use this data set.",
        "We use same training data (1,500 words) and test data (150 words).",
        "Test Set II is consists of 7,185 English and Korean word pairs.",
        "We use Test Set II to show the relation between the size of training data and the accuracy.",
        "We use 90% of total size as training data and 10% as test data.",
        "For a variation coverage test, we use Test Set III that is extracted from KTSET 2.0.",
        "Test Set III is consists of 2,391 English words and their transliterations.",
        "An English word has 1.14 various transliterations in average."
      ]
    },
    {
      "heading": "5.2 Evaluation functions",
      "text": [
        "Accuracy was measured by the percentage of the number of correct transliterations divided by the number of generated transliterations.",
        "We call it as word aceuracy(W.A.).",
        "We use one more measure, called character accuracy(C.A.)",
        "that measures the character edit distance between a correct word and a generated word.",
        "where L is the length of the original string, and d, and s are the number of insertion, deletion and substitution respectively.",
        "If the dividend is negative (when L < (i d s)), we consider it as zero(Hall and Dowling, 1980).",
        "For the real usage test, we used variation coverage ( V. C.) that considers various usages.",
        "We evaluated both for the term frequency (tf) and document frequency (df), where tf is the number of term appearance in the documents and df is the number of documents that contain the term.",
        "If we set the usage tf (or df) of the transliterations to 1 for each transliteration, we can calculate the transliteration coverage for the unique",
        "{tf, s of used words"
      ]
    },
    {
      "heading": "5.3 Accuracy tests",
      "text": [
        "We compare our result [PCa, PCp]3 with the simple statistical information based model(Lee and Choi, 1998) [S7], the Maximum Entropy based model(Tae-il Kim, 2000) [MEM, the Neural Network model(Jung-Jae Kim, 1999) [NN and the Decision Tree based model(Kang, 1999)[DT].",
        "Table 3 shows the result of E-K transliteration and back-transliteration test"
      ]
    },
    {
      "heading": "5.4 Variation coverage tests",
      "text": [
        "To compare our result(PCp) with (Lee and Choi, 1998), we trained our methods with the training data of Test Set I.",
        "In ST, (Lee and Choi, 1998) use 20 high rank results, but we just use 5 results.",
        "Table 5 shows the coverage of our proposed method.",
        "Fig.",
        "9 shows the increase of coverage with the number of outputs."
      ]
    },
    {
      "heading": "5.5 Discussion",
      "text": [
        "We summarize the information length and the kind of information(Table 6).",
        "The results of experiments and information usage show that MEM combines various information better than DT and NN.",
        "ST does not use a previous input (c.i_t) but use a previous output(ki_l) to calculate the current output's probability like",
        "Part-of-Speech Tagging problem.",
        "But; ST gets the lowest accuracy.",
        "It means that surrounding alphabets give more information than previous output.",
        "In other words, E-K transliteration is not the alphabet-per-alphabet or phoneme-per-phoneme classification problem.",
        "A previous output does not give enough information for current unit's disambiguation.",
        "An input unit and an output unit should be extended.",
        "F-K transliteration is a chunk-per-chunk classification problem.",
        "We restrict the length of information, to see the influence of phoneme-chunk size.",
        "Fig.",
        "10 shows the results.",
        "2 3 5 6 7",
        "With the same length of information, we get the higher C.A.",
        "and W.A.",
        "than other methods.",
        "It means previous outputs give good information and our chunk-based method is a good combining method.",
        "It also suggests that we can restrict the max size of chunk in a permissible size.",
        "PCa gets a higher accuracy than PCp.",
        "It is due to the number of possible phoneme sequences.",
        "A transliteration network that consists of phoneme unit has more nodes than a transliteration network that consists of alphabet unit.",
        "With small training data, despite of the loss due to the phoneme sequences ambiguity a phoneme gives more information than an alphabet.",
        "When the information is enough, PCa outperforms PCp."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "We propose the method of English-to-Korean transliteration and back-transliteration with multiple unbounded overlapping phoneme chunks.",
        "We showed that E-K transliteration and back-transliteration are not a phoneme-p er-phoneme and alphabet-per-alphabet classification problem.",
        "So we use phoneme chunks that do not have a length limit and can explain E-K transliteration phenomena.",
        "We get the reliability of a given transliteration phenomenon by applying overlapping phoneme chunks.",
        "Our method is simple and does not need a complex combining method for various length of information.",
        "The change of an information length does not affect the internal representation of the problem.",
        "Our chunk-based method can be used to other classification problems and can give a simple combining method."
      ]
    }
  ]
}
