{
  "info": {
    "authors": [
      "Youngjoong Ko",
      "Jungyun Seo"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1066",
    "title": "Automatic Text Categorization by Unsupervised Learning",
    "url": "https://aclweb.org/anthology/C00-1066",
    "year": 2000
  },
  "references": [
    "acl-J98-1002",
    "acl-P97-1006"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The goal of text categorization is to classify documents into a certain number of predefined categories.",
        "The previous works in this area have used a large number of labeled training documents for supervised learning.",
        "One problem is that it is difficult to create the labeled training documents.",
        "While it is easy to collect the unlabeled documents, it is not so easy to manually categorize them for creating training documents.",
        "In this paper, we propose an unsupervised learning method to overcome these difficulties.",
        "The proposed method divides the documents into sentences, and categorizes each sentence using keyword lists of each category and sentence similarity measure.",
        "And then, it uses the categorized sentences for training.",
        "The proposed method shows a similar degree of performance, compared with the traditional supervised learning methods.",
        "Therefore, this method can be used in areas where low-cost text categorization is needed.",
        "It also can be used for creating training documents.",
        "Introduction With the rapid growth of the internet, the availability of on-line text information has been considerably increased.",
        "As a result, text categorization has become one of the key techniques for handling and organizing text data.",
        "Automatic text categorization in the previous works is a supervised learning task, defined as assigning category labels (pre-defined) to text documents based on the likelihood suggested by a training set of labeled documents.",
        "However, the previous learning algorithms have some problems.",
        "One of them is that they require a large, often prohibitive, number of labeled training documents for the accurate learning.",
        "Since the application area of automatic text categorization has diversified from newswire articles and web pages to electronic mails and newsgroup postings, it is a difficult task to create training data for each application area (Nigam K. et al., 1998).",
        "In this paper, we propose a new automatic text categorization method based on unsupervised learning.",
        "Without creating training documents by hand, it automatically creates training sentence sets using keyword lists of each category.",
        "And then, it uses them for training and classifies text documents.",
        "The proposed method can provide basic data for creating training documents from collected documents, and can be used in an application area to classify text documents in low cost.",
        "We use the statistic (Yang Y. et al., 1998) as a feature selection method and the naive Bayes classifier (McCallum A. et al., 1998) as a statistical text classifier.",
        "The naive Bayes classifier is one of the statistical text classifiers that use word frequencies as features.",
        "Other examples include k-nearest-neighbor (Yang Y. et al., 1994), TFIDF/Roccio (Lewis D.D.",
        "et al., 1996), support vector machines (Joachims T. et al., 1998) and decision tree (Lewis D.D.",
        "et al., 1994).",
        "1Proposal: A text categorization scheme The proposed system consists of three modules as shown in Figure 1; a module to preprocess collected documents, a module to create training sentence sets, and a module to extract features and to classify text documents.",
        "Figurel : Architecture for the proposed system"
      ]
    },
    {
      "heading": "1.1 Preprocessing",
      "text": [
        "First, the html tags and special characters in the collected documents are removed.",
        "And then, the contents of the documents are segmented into sentences.",
        "We extract content words for each sentence using only nouns.",
        "In Korean, there are active-predicative common nouns which become verbs when they are combined with verb-derivational suffixes (e.g., ha-ta 'do', toy-ta `become', etc.).",
        "There are also stati ve-predicative common nouns which become adjectives when they are combined with adjective-derivational suffixes such as ha.",
        "These derived verbs and adjectives are productive in Korean, and they are classified as nouns according to the Korean POS tagger.",
        "Other verbs and adjectives are not informative in many cases."
      ]
    },
    {
      "heading": "1.2 Creating training sentence sets",
      "text": [
        "Because the proposed system does not have training documents, training sentence sets for each category corresponding to the training documents have to be created.",
        "We define keywords for each category by hand, which contain special features of each category sufficiently.",
        "To choose these keywords, we first regard category names and their synonyms as keywords.",
        "And we include several words that have a definite meaning of each category.",
        "The average number of keywords for each category is 3.",
        "(Total 141 keywords for 47 categories) Table 1 lists the examples of keywords for each category.",
        "Next, the sentences which contain predefined keywords of each category in their content words are chosen as the initial representative sentences.",
        "The remaining sentences are called unclassified sentences.",
        "We scale up the representative sentence sets by assigning the unclassified sentences to their related category.",
        "This assignment has been done through measuring similarities of the unclassified sentences to the representative sentences.",
        "We will elaborate this process in the next two subsections.",
        "1.2.1 Extracting and verifying representative sentences We define the representative sentence as what contains predefined keywords of the category in its content words.",
        "But there exist error sentences in the representative sentences.",
        "They do not have special features of a category even though they contain the keywords of the category.",
        "To remove such error sentences, we can rank the representative sentences by computing the weight of each sentence as follows:",
        "in the j th category 0 In Information Retrival, Inverse Document Frequency (IDF) are used generally.",
        "But a sentence is a processing unit in the proposed method.",
        "Therefore, the document frequency cannot be counted.",
        "Also, since ICF was defined by Cho K. et al.",
        "(1997)",
        "and its efficiency was verified, we use it in the proposed method.",
        "ICF is computed as follows:",
        "where CT, is the number of categories that contain tj, and M is the total number of categories.",
        "0 The combination (TFICF) of the above Et) and 0, i.e., weight of word t; in jth category is computed as follows:",
        "2) Using word weights (w5) computed in 1), a sentence weight (Wu) in jth category are computed as follows:",
        "where N is the total number of words in a sentence.",
        "3) The representative sentences of each category are sorted in the decreasing order of weight, which was computed in 2).",
        "And then, the top 70% of the representative sentences are selected and used in our experiment.",
        "It is decided empirically.",
        "To extend the representative sentence sets, the unclassified sentences are classified into their related category through measuring similarities",
        "of the unclassified sentences to the representative sentences.",
        "(1) Measurement of word and sentence",
        "similarities As similar words tend to appear in similar contexts, we compute the similarity by using contextual information (Kim H. et al., 1999; Karov Y. et al., 1999).",
        "In this paper, words and sentences play complementary roles.",
        "That is, a sentence is represented by the set of words it contains, and a word by the set of sentences in which it appears.",
        "Sentences are similar to the extent that they contain similar words, and words are similar to the extent that they appear in similar sentences.",
        "This definition is circular.",
        "Thus, it is applied iteratively using two matrices as shown in Figure 2.",
        "In this paper, we set the number of iterations as 3, as is recommended by",
        "sentence similarities In Figure 2, each category has a word similarity matrix WSM and a sentence similarity matrix SSM.",
        "In each iteration n, we update WSM,,, whose rows and columns are labeled by all content words encountered in the representative sentences of each category and input unclassified sentences.",
        "In that matrix, the cell (i,j) holds a value between 0 and 1, indicating the extent to which the ith word is contextually similar to the jth word.",
        "Also, we keep and update a SSM, which holds similarities among sentences.",
        "The rows of SSM correspond to the unclassified sentences and the columns to the representative sentences.",
        "In this paper, the number of input sentences of row and column in SSM is limited to 200, considering execution time and memory allocation.",
        "To compute the similarities, we initialize WSM to the identity matrix.",
        "That is, each word is fully similar (1) to itself and completely dissimilar (0) to other words.",
        "The following steps are iterated until the changes in the similarity values are small enough.",
        "1.",
        "Update the sentence similarity matrix SSM, using the word similarity matrix WSM.",
        "2.",
        "Update the word similarity matrix WSM, using the sentence similarity matrix SSM.",
        "(2) Affinity formulae",
        "To simplify the symmetric iterative treatment of similarity between words and sentences, we define an auxiliary relation between words and sentences as affinity.",
        "A word W is assumed to have a certain affinity to every sentence, which",
        "is a real number between 0 and 1.",
        "It reflects the contextual relationships between W and the words of the sentence.",
        "If W belongs to a sentence S, its affinity to S is 1.",
        "If W is totally unrelated to S, the affinity is close to 0.",
        "If W is contextually similar to the words of S, its affinity to S is between 0 and 1.",
        "In a similar manner, a sentence S has some affinity to every word, reflecting the similarity of S to the sentences involving that word.",
        "Affinity formulae are defined as follows (Karov Y. et al., 1999).",
        "In these formulae, W e S means that a word belongs to a sentence: aff (W, S)= max wiEs sitn (W, 147, )(5) aff (S ,W ) = max wEs sitn (S , S j)(6) In the above formulae, ri denotes the iteration number, and the similarity values are defined by WSM and SSM.",
        "Every word has some affinity to the sentence, and the sentence can be represented by a vector indicating the affinity of each word to it."
      ]
    },
    {
      "heading": "(3) Similarity formulae",
      "text": [
        "The similarity of W, to W2 is the average affinity of the sentences that include W, to W2, and the similarity of a sentence S, to S2 is a weighted average of the affinity of the words in S, to S2.",
        "Similarity formulae are defined as follows",
        "The weights in Formula 7 are computed following the methodology in the next section.",
        "The sum of weights in Formula 8, which is a reciprocal number of sentences that contain w",
        "is 1.",
        "These values are used to update the corresponding entries of WSMand SSM,,.",
        "(4) Word weights In Formula 7, the weight of a word is a product of three factors.",
        "It excludes the words that are expected to be given unreliable similarity values.",
        "The weights are not changed in their process of iterations.",
        "1.",
        "Global frequency: Frequent words in total",
        "sentences are less informative of sense and of sentence similarity.",
        "For example, a word like `phil-yo(necessity)' frequently appears in any sentence.",
        "The formula is as follows (Karov Y. et al., 1999):",
        "In (9), max5freq(x) is the sum of the five highest frequencies in total sentences.",
        "2.",
        "Log-likelihood factor: In general, the words that are indicative of the sense appear in representative sentences more frequently than in total sentences.",
        "The log-likelihood factor captures this tendency.",
        "It is computed as follows (Karov Y. et al., 1999):",
        "In (10), Pr(W) is estimated from the frequency of 1,171 in the total sentences, and Pr(147/114) from the frequency of lit, in representative sentences.",
        "To avoid poor estimation for words with a low count in representative sentences, we multiply the log-likelihood by (11) where count(W) is the number of occurrences of W in representative sentences.",
        "For the words which do not appear in representative sentences, we assign weight",
        "(1.0) to them.",
        "And the other words are assigned weight that adds 1.0 to computed value: min mm 1,",
        "3.Part of speech: Each part of speech is assigned a weight.",
        "We assign weight (1.0) to proper noun, non-predicative common noun, and foreign word, and assign weight (0.6) to active-predicative common noun and stative-predicative common noun.",
        "The total weight of a word is the product of the above factors, each normalized by the sum of factors of the words in a sentence as follows",
        "In (12), factor(WiS) is the weight before",
        "normalization.",
        "(5) Assigning unclassified sentences to a category",
        "We first computed similarities of the unclassified sentences to the representative sentences.",
        "And then, we decided a similarity value of each unclassified sentence for each category using two alternate ways.",
        "according to their x2 statistic with respect to the category.",
        "Using the two-way contingency table of a word t and a category c i) A is the number of times t and c co-occur, ii) B is the number of times t occurs without c, iii) C is the number of times c occurs without t, iv) D is the number of times neither c nor I occurs, and vi) N is the total number of sentences the word-goodness measure is defined as follows (Yang Y. et al., 1997):",
        "To measure the goodness of a word in a global feature selection, we combine the category-specific scores of a word as follows:",
        "In (13) and (14), i) X is an unclassified sentence, ii)C={ , c,en,} is a category set, and iii) RCi = {SI S2 ,...1S is a representative sentence set of category Each unclassified sentence is assigned to a category which has a maximum similarity value.",
        "But there exist unclassified sentences which do not belong to any category.",
        "To remove these unclassified sentences, we set up a threshold value using normal distribution of similarity values as follows: max ( sim(X , ei)}p + 6c(15) cicC In (15), i) X is an unclassified sentence, ii) p is an average of similarity values, iii) a is a standard deviation of similarity values, and iv) 0 is a numerical value corresponding to threshold(%) in normal distribution table.",
        "1.3 Feature selection and text classifier",
        "The size of the vocabulary used in our experiment is selected by ranking words",
        "The method that we use for classifying documents is naive Bayes, with minor modifications based on Kullback-Leibler Divergence (Craven M. et al., 1999).",
        "The basic idea in naive Bayes approaches is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document.",
        "Given a document d for classification, we calculate the probabilities of each category c as follows:",
        "In the above formula, i) n is the number of words in d, ii) N(tild) is the frequency of word ti in document (1, iii) T is the size of the vocabulary, and iv) t; is the ith word in the vocabulary.",
        "Pr(tilc) thus represents the probability that a randomly drawn word from a randomly drawn document in category c will be the wordPr(tild) represents the proportion of words in document d that are word Each probability is estimated by formulae (19) and (20), which are called the expected likelihood (18)",
        "estimator (Li H. et al., 1997).",
        "The category predicted by the method for a given document is simply the category with the greatest score.",
        "This method performs exactly the same classifications as naive Bayes does, but produces classification scores that are less extreme."
      ]
    },
    {
      "heading": "2.1 Performance measures",
      "text": [
        "In this paper, a document is assigned to only one category.",
        "We use the standard definition of recall, precision, and F, measure as performance measures.",
        "For evaluating performance average across categories, we use the micro-averaging method.",
        "F, measure is defined by the following formula (Yang Y. et al., 1997):",
        "where r represents recall and p precision.",
        "It balances recall and precision in a way that gives them equal weight."
      ]
    },
    {
      "heading": "2.2 Experiment settings",
      "text": [
        "We used total 47 categories in our experiment.",
        "They consist of 2,286 documents to be collected in web.",
        "We did not use tag information of web documents.",
        "And a so-called bag of words or unigram representation was used.",
        "Table 2 shows the settings of experiment data in detail.",
        "similarity value decisions and thresholds We evaluated our method according to the different combinations of similarity value decisions and thresholds in section 1.2.2.",
        "We used thresholds of top 5%, top 10%, top 15%, top20% in formula (15), and tested the two options, average and maximum in formulae (13) and (14).",
        "We limited our vocabulary to 2,000 words in this experiment.",
        "Figure 3 shows results according to the two options in each threshold.",
        "Here, the result using maximum was better than that using average with regrad to all thresholds.",
        "The results of top 10% and top 15% were best.",
        "Therefore, we used the maximum in the decision of similarity value and top 15% in threshold in our experiments.",
        "2.3.2 The proposed system vs. the system by supervised learning For the fair evaluation, we embodied a traditional system by supervised learning using the same feature selection method (i statistic) and classifier (naive Bayes Classifier), as used in the proposed system.",
        "And we tested these systems and compared their performance:",
        "proposed system and the system by supervised learning.",
        "The best Ft score of the proposed system is 71.8% and that of the system by supervised learning is 75.6%.",
        "Therefore, the difference between them is only 3.8%.",
        "Conclusion This paper has described a new automatic text categorization method.",
        "This method automatically created training sets using keyword lists of each category and used them for training.",
        "And then, it classified text documents.",
        "This could be a significant method in text learning because of the high cost of hand-labeling training documents and the availability of huge volumes of unlabeled documents.",
        "The experiment results showed that with respect to performance, the difference between the proposed method and the method by supervised learning is insignificant.",
        "Therefore, this method can be used in areas where low-cost text categorization is required, and can be used for creating training data.",
        "This study awaits further research.",
        "First, a more scientific approach for defining keyword lists should be investigated.",
        "Next, if we use a word sense disambiguation system in the extraction step of representative sentences, we would be able to achieve a better performance."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported by KOSEF under Grant No.",
        "97-0102-03-01-3.",
        "We wish to thank Jeoung-seok Kim for his valuable comments to the earlier version of this paper."
      ]
    }
  ]
}
