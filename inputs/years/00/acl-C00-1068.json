{
  "info": {
    "authors": [
      "Kazunori Komatani",
      "Tatsuya Kawahara"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1068",
    "title": "Flexible Mixed-Initiative Dialogue Management Using Concept-Level Confidence Measures of Speech Recognizer Output",
    "url": "https://aclweb.org/anthology/C00-1068",
    "year": 2000
  },
  "references": [
    "acl-P99-1040"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present; a method to realize flexible mixed-initiative dialogue, in which the system can make effective confirmation and guidance using concept, level confidence measures (CNN) derived from speech.",
        "recognize]: output in order to handle speech recognition errors.",
        "We define two concept-level CMs, which are on content-words and on semantic-attributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars.",
        "Content-word CM is useful for selecting plausible interpretations.",
        "Less confident interpretations are given to confirmation process.",
        "The strategy improved the interpretation accuracy by 1.1.5%.",
        "Moreover, the semantic-attribute CM is used to estimate user's intention awl generates system-initiative guidances even when successful interpretation is not obtained."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "in a spoken dialogue system, it frequently occurs that the system incorrectly recognizes user utterances and the user makes expressions the system has not expected.",
        "These problems are essentially inevitable in handling the natural language by computers, even if vocabulary and grammar of the system are tuned.",
        "This lack of robustness is one of the reason why spoken dialogue systems have not been widely deployed.",
        "In order to realize a robust spoken dialogue system, it is inevitable to handle speech recognition errors.",
        "To suppress recognition errors, system-initiative dialogue is effective.",
        "]3ut it call be adopted only in a simple task.",
        "For instance, the form-filling task can be realized by a simple strategy where the system asks a user the slot values in a fixed order.",
        "In such a system-initiated interaction, the recognizer easily narrows down the vocabulary of the next; user's utterance, thus the recognition gets easier.",
        "On the other hand, in more complicated task such as information retrieval, the vocabulary of the next utterance cannot; be limited on all occasions, because the user should be able to input the values in various orders based on Ids preference.",
        "Therefore, without imposing a rigid template upon the user, the system must behave appropriately even when speech recognizer output contains some errors.",
        "Obviously, making confirmation is effective to avoid misunderstandings caused by speech recognition errors.",
        "However, when confirmations are made for every utterance, the dialogue will become too redundant; and consequently troublesome for users.",
        "Previous works have shown that confirmation strategy should be decided according to the frequency of speech recognition errors, using mathematical formula (Nihni and Kobayashi, 1.996) and using computer-to-computer simulation (Watanabe et al., 1998).",
        "These works assume fixed performance (averaged speech recognition accuracy) in whole dialogue with any speakers.",
        "For flexible dialogue management, however the confirmation strategy must be dynamically changed based on the individual utterances.",
        "For instance, we human make confirmation only when we are not confident.",
        "Similarly, confidence measures (CMs) of every speech recognition output should be modeled as a criterion to control dialogue management.",
        "CMs have been calculated in previous works using transcripts and various knowledge sources (Litman et al., 1.999) (Pao et al., 1998).",
        "For more flexible interaction, it is desirable that CMs are defined on each word rather than whole sentence, because the system can handle only unreliable portions of an utterance instead of accepting/rejecting whole sentence.",
        "In this paper, we propose two concept-level CMs that are on content-word level and on semantic-attribute level for every content word.",
        "Because the CMs are defined using only speech recognizer output, they can be computed in real time.",
        "The system can make efficient confirmation and effective guidance according to the CMs.",
        "Even when successful interpretation is not obtained on content-word level, the system generates system-initiative guidances based on the semantic-attribute level, which lead the next user's utterance to successful interpretation.",
        "2 Definition of Confidence Measures (CMs)",
        "Confidence Measures (CMs) have been studied for utterance verification that verifies speech recognition result as a post-processing (Kawahara et al., 1998).",
        "Since an automatic speech recognition is a process finding a sentence hypothesis with the maximum likelihood for an input speech, some measures are needed in order to distinguish a correct recognition result from incorrect one.",
        "In this section, we describe definition of two level CMs which are on content-words and on semantic-attributes, using 10-best output of the speech recognizer and parsing with phrase-level grammars."
      ]
    },
    {
      "heading": "2.1 Definition of CM for Content Word",
      "text": [
        "In the speech recognition process, both acoustic probability and linguistic probability of words are multiplied (summed up in log-scale) over a sentence, and the sequence having maximum likelihood is obtained by a search algorithm.",
        "A score of sentence derived from the speech recognizer is log-scaled likelihood of a hypothesis sequence.",
        "We use a grammar-based speech recognizer Julian (Lee et al., 1999), which was developed in our laboratory.",
        "It correctly obtains the N-best candidates and their scores by using A* search algorithm.",
        "Using the scores of these N-best candidates, we calculate content-word CMs as below.",
        "The content words are extracted by parsing with phrase-level grammars that are used in speech recognition process.",
        "In this paper, we set N = 10 after we examined various values of N as the number of computed candidates I.",
        "'Even if we set N larger than 10, the scores of i-th hypotheses (i > 10) are too small to affect resulting CMs.",
        "First, each i-th score is multiplied by a factor a(a < 1).",
        "This factor smoothes the difference of N-best scores to get adequately distributed CMs.",
        "Because the distribution of the absolute values is different among kinds of statistical acoustic model (monophone, triphone, and so on), different values must be used.",
        "The value of a is examined in the preliminary experiment.",
        "In this paper, we set a = 0.05 when using tri-phone model as acoustic model.",
        "Next, they are transformed from log-scaled value (a • scaled') to probability dimension by taking its exponential, and calculate a posteriori probability for each i-th candidate (Bouwman et al., 1999).",
        "This pi represents a posteriori probability of the i-th sentence hypothesis.",
        "Then, we compute a posteriori probability for a word.",
        "If the i-th sentence contains a word w, let 6„,,i = 1, and 0 otherwise.",
        "A posteriori probability that a word w is contained (The) is derived as summation of a posteriori probabilities of sentences that contain the word.",
        "We define this pi„ as the content-word CM (CM„,).",
        "This C My, is calculated for every content word.",
        "Intuitively, words that appear many times in N-best hypotheses get high CMs, and frequently substituted ones in N-best hypotheses are judged as unreliable.",
        "In Figure 1, we show an example in CM„, calculation with recognizer outputs (i-th recognized candidates and their a posteriori probabilities) for an utterance \"Futaishisetsu ni resutoran no am yado (Tell me hotels with restaurant facility.)\".",
        "It can be observed that a correct content word 'restaurant as facility' gets a high CM value (CMw = 1).",
        "The others, which are incorrectly recognized, get low CMs, and shall be rejected."
      ]
    },
    {
      "heading": "2.2 CM for Semantic Attribute",
      "text": [
        "A concept; category is semantic attribute assigned to content words, and it is identified by parsing with phrase-level grammars that are used in speech recognition process and represented with Finite State Automata (FSA).",
        "Since"
      ]
    },
    {
      "heading": "Recognition candidates",
      "text": [
        "as shisetsu ni resutoran no kayacho with restaurant facility / Kayacho(location) as shisetsu ni resutoran no katsura no with restaurant facility / Katsura(location) as ShiSeiSU ni reSZliOnIn 170 kamiganto with restaurant facility / Kamigamo(location) <g> shisetsu ni res7ltorall no kayacho with restaurant facility / Kayacho(location) <g> shisetsu ni MS711,07'0,11 no katsura with restaurant facility / Katsura(location) <g> shisetsu ni resutoran no kamigamo with restaurant facility / Kamiganno(location) as shisetsu ni reS7tiOran no kale",
        "these FSAs are classified into concept categories beforehand, we can automatically derive the concept categories of words by parsing with these grammars.",
        "In our hotel query task, there are seven concept categories such as 'location', `facility' and so on.",
        "For this concept category, we also define semantic-attribute CMs (CIVIC) as follows.",
        "First, we calculate a posteriori probabilities of N-best sentences in the same way of computing content-word CM.",
        "If a concept category c is contained in the i-th sentence, let (1„,i = 1., and otherwise.",
        "The probability that a concept category c is correct (pc) is derived as below.",
        "We define this pc as semantic-attribute CM (CM,).",
        "This CIVIC estimates which category the user refers to and is used to generate effective guidances.",
        "There are a lot of systems that have adopted a mixed-initiative strategy (Sturm et al., 1999) (Goddeau et al., 1996)(Bennacef et al., 1996).",
        "It has several advantages.",
        "As the systems do not impose rigid system-initiated templates, the user can input values he has in mind directly, thus the dialogue becomes more natural.",
        "In conventional systems, the system-initiated utterances are considered only when semantic ambiguity occurs.",
        "But hi order to realize robust interaction, the system should make confirmations to remove recognition errors and generate guidances to lead next user's utterance to successful interpretation.",
        "In this section, we describe how to generate the system-initiated utterances to deal with recognition errors.",
        "An overview of our strategy is shown in Figure 2."
      ]
    },
    {
      "heading": "3.1 Making Effective Confirmations",
      "text": [
        "Confidence Measure (CM) is useful in selecting reliable candidates and controlling confirmation strategy.",
        "By setting two thresholds 01, 02(01 > 02) on content-word CM (CM„,), we provide the confirmation strategy as follows.",
        "• CM,, > Oi -+ accept the hypothesis • 01 > CM,„ > 02 make confirmation to the user \"Did you say ...?\"",
        "• 02 > CMw – * reject the hypothesis",
        "The threshold 01 is used to judge whether the hypothesis is accepted or should be confirmed, and the threshold 02 is used to judge whether it is rejected.",
        "Because CM,„ is defined for every content word, judgment among acceptance, confirmation, or rejection is made for every content word when one utterance contains several content words.",
        "Suppose in a single utterance, one word has CA() between 01 and and the other has below 02, the former is given to confirmation process, and the latter is rejected.",
        "Only if all content words are rejected, the system will prompt the user to utter again.",
        "By accepting confident words and rejecting unreliable candidates, this strategy avoids redundant confirmations and focuses on necessary confirmation.",
        "We optimize these thresholds 01,02 considering the false acceptance (FA) and the false rejection (FR) using real data.",
        "Moreover, the system should confirm using task-level knowledge.",
        "It is not usual that users change the already specified slot values.",
        "Thus, recognition results that overwrite filled slots are likely to be errors, even though its CM,, is high.",
        "By making confirmations in such a situation, it is expected that false acceptance (FA) is suppressed."
      ]
    },
    {
      "heading": "3.2 Generating System-Initiated Guidances",
      "text": [
        "It is necessary to guide the users to recover from recognition errors.",
        "Especially for novice users, it is often effective to instruct acceptable slots of the system.",
        "It will be helpful that the system generates a guidance about the acceptable slots when the user is silent without carrying out the dialogue.",
        "The system-initiated guidances are also effective when recognition does not go well.",
        "Even when any successful output of content words is not obtained, the system can generate effective guidances based on the semantic attribute with",
        "high confidence.",
        "An example is shown in Figure 3.",
        "In this example, all the 10-best candidates are concerning a name of place but their CA, values are lower than the threshold (02).",
        "As a result, any word will be neither accepted nor confirmed.",
        "In this case, rather than rejecting the whole sentence and telling the user \"Please say again\", it is better to guide the user based on the attribute having high CM„, such as \"Which city is your destination?\".",
        "This guidance enables the system to narrow down the vocabulary of the next user's utterance and to reduce the recognition difficulty.",
        "It will consequently lead next user's utterance to successful interpretation.",
        "When recognition on a content word does not",
        "go well repeatedly in spite of high semantic-attribute CM, it is reasoned that the content word may be out-of-vocabulary.",
        "In such a case, the system should change the question.",
        "For example, if an utterance contains an out-of-vocabulary word and its semantic-attribute is inferred as \"location\", the system can make guidance, \"Please specify with the name of pre-fecture\", which will lead the next user's utterance into the system's vocabulary."
      ]
    },
    {
      "heading": "4 Experimental Evaluation",
      "text": []
    },
    {
      "heading": "4.1 Task and Data",
      "text": [
        "We evaluate our method on the hotel query task.",
        "We collected 120 minutes speech data by 24 novice users by using the prototype system with GUI (Figure 4) (Kawahara et al., 1999).",
        "The users were given simple instruction beforehand on the system's task, retrievable items, how to cancel input values, and so on.",
        "The data is segmented into 705 utterances, with a pause of 1.25 seconds.",
        "The vocabulary of the system contains 982 words, and the munber of database records is 2040.",
        "Out of 705 utterances, 124 utterances (17.6%) are beyond the system's capability, namely they are out-of-vocabulary, out-of-grammar, out-of-task, or fragment of utterance.",
        "In following experiments, we evaluate the system performance using all data including these unacceptable utterances in order to evaluate how the system can reject unexpected utterances appropriately as well as recognize normal.",
        "utterances correctly."
      ]
    },
    {
      "heading": "4.2 Thresholds to Make Confirmations",
      "text": [
        "In section 3.1., we presented confirmation strategy by setting two thresholds 01, 02 (0] > 02) for content-word CM (CM,„).",
        "We optimize these threshold values using the collected data.",
        "We count errors not by the utterance but by the content-word (slot).",
        "The number of slots is 804.",
        "The threshold 01 decides between acceptance and confirmation.",
        "The value of 01 should be determined considering both the ratio of incorrectly accepting recognition errors (False Acceptance; FA) and the ratio of slots that are not filled with correct values (Slot Error; SErr).",
        "Namely, FA and SErr are defined as time complements of precision and recall rate of the output,.",
        "respectively.",
        "# of incorrectly accepted words A =",
        "# of accepted words # of correctly accepted words SErr = # of all correct words",
        "After experimental optimization to minimize FA-ESErr, we derive a value of 01 as 0.9.",
        "Similarly, the threshold 02 decides confirmation and rejection.",
        "The value of* 02 should be decided considering both the ratio of incorrectly rejecting content words (False Rejection; FR.)",
        "and the ratio of accepting recognition errors into the confirmation process (conditional False Acceptance; cFA).",
        "# of incorrectly rejected words If we set the threshold 02 lower, FR decreases and correspondingly cFA increases, which means that more candidates are obtained but more confirmations are needed.",
        "fly minimizing 1?R-1-cl.■A, we derive a value of 02 as 0.6."
      ]
    },
    {
      "heading": "4.3 Comparison with Conventional Methods",
      "text": [
        "In many conventional spoken dialogue systems, only 1-best candidate of a speech recognizer output is used in the subsequent processing.",
        "We compare our method with a conventional method that uses only 1-best candidate in interpretation accuracy.",
        "The result is shown in Table 1.",
        "In the 'no confirmation' strategy, the hypotheses are classified by a single threshold (0) into either the accepted or the rejected.",
        "Namely, content words having CM,„ over threshold 0 are accepted, and otherwise simply rejected.",
        "In this case, a threshold value of 0 is set to 0.9 that gives minimum FA-}-SErr.",
        "In the 'with con-firmation' strategy, the proposed confirmation strategy is adopted using 01 and 02.",
        "We set 01 = 0.9 and 02 = O.G.",
        "The TA-FSEre in Table 1 means FA(01)+SErr(02), on the assumption that the confirmed phrases are correctly either accepted or rejected.",
        "We regard this assumption as appropriate, because users tend to answer 'yes' simply to express their affirmation (Hockey et al., 1997), so the system can distinguish affirmative answer and negative one by grasping simple 'yes' utterances correctly.",
        "As shown in Table 1, interpretation accuracy is improved by 5.4% in the 'no confirma-tion' strategy compared with the conventional method.",
        "And 'with confirmation' strategy, we achieve 11.5% improvement in total.",
        "This result proves that our method successfully eliminates recognition errors.",
        "By making confirmation, the interaction becomes robust, but accordingly the number of whole utterances increases.",
        "If all candidates having CMH, under 01 are given to confirmation process without setting 02, 332 vain confirmation for incorrect contents are generated out of 400 candidates.",
        "By setting 02, 102 candidates having CA, between 01 and 02 are confirmed, and the number of incorrect confirmations is suppressed to 53.",
        "Namely, the ratio of correct hypotheses and incorrect ones being confirmed are almost equal.",
        "This result shows indistinct candidates are given to confirmation process whereas scarcely confident candidates are rejected."
      ]
    },
    {
      "heading": "4.4 Effectiveness of Semantic-Attribute CM",
      "text": [
        "In Figure 5, the relationship between content-word CM and semantic-attribute CM is shown.",
        "It is observed that semantic-attribute CMs are estimated more correctly than content-word CMs.",
        "Therefore, even when successful interpretation is not obtained from content-word CMs, semantic-attribute can be estimated correctly.",
        "In experimental data, there are 1.48 slots2 that are rejected by content-word CMs.",
        "It is also observed that 52% of semantic-attributes 2Out-of-vocabulary and out-of-grammar utterances are included in their phrases.",
        "0.2 0.4 0.6 0.8 threshold",
        "with CM,, over 0.9 is correct.",
        "Such slots amount to 34.",
        "Namely, our system can generate effective guidances against 23% (34/148) of utterances that had been only rejected in conventional methods."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We present dialogue management using two concept-level CMs in order to realize robust interaction.",
        "The content-word CM provides a criterion to decide whether an interpretation should be accepted, confirmed, or rejected.",
        "This strategy is realized by setting two thresholds that are optimized balancing false acceptance and false rejection.",
        "The interpretation error (FA+SErr) is reduced by 5.4% with no confirmation and by 11..5% with confirmations.",
        "Moreover, we define CM on semantic attributes, and propose a new method to generate effective guidances.",
        "The concept-based confidence measure realizes flexible mixed-initiative dialogue in which the system can make effective confirmation and guidance by estimating user's intention."
      ]
    }
  ]
}
