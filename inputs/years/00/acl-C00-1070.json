{
  "info": {
    "authors": [
      "Sang-Zoo Lee",
      "Jun'ichi Tsujii",
      "Hae-Chang Rim"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1070",
    "title": "Lexicalized Hidden Markov Models for Part-Of-Speech Tagging",
    "url": "https://aclweb.org/anthology/C00-1070",
    "year": 2000
  },
  "references": [
    "acl-W96-0213",
    "acl-W99-0615"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Since most previous works for 11MM-based tagging consider only part-of-speech infOrmation in contexts, their models cannot utilize lexical information which is crucial for resolving some morphological ambiguity.",
        "In this paper we introduce uniformly lexicalized IIIVIMs for part-of-speech tagging in both English and Korean.",
        "The lexicalized models use a simplified back-off smoothing technique to overcome data sparseness.",
        "In experiments, lexicalized models achieve higher accuracy than non-lexicalized models and the back-off smoothing method mitigates data sparseness better than simple smoothing methods."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Part-of-speech(POS) tagging is a process in which a proper POS tag is assigned to each word in raw texts.",
        "Even though morphologically ambiguous words have more than one POS tag, they belong to just one tag in a context.",
        "To resolve such ambiguity, taggers have to consult various sources of information such as lexical preferences (e.g. without consulting context, table is more probably a noun than a verb or an adjective), tag n-gram contexts (e.g. after a non-possessive pronoun, table is more probably a verb than a noun or an adjective, as in they table an amendment), word 11-grain contexts (e.g. before lamp, table is more probably an adjective than a noun or a verb, as in I need a table lamp), and so on(Lee et al., 1999).",
        "However, most previous 11A4M-based taggers consider only POS information in contexts, and so they cannot capture lexical information which is necessary for resolving sonic morphological ambiguity.",
        "Some recent works have reported that tagging accuracy could be improved by using lexical information in their models such as the transformation-based patch rules(l3rill, 1994), the maximum entropy model(Ratnaparkhi, 1996), the statistical lexical rules(Lee et al., 1999), the IIMM considering multi-words(Kim, 1996), the selectively lexicalized IIMM(Kini et al., 1999), and so on.",
        "In the previous works(Kim, 1996)(Kim et al., 1999), however, their IINFIVIs were lexicalized selectively and restrictively.",
        "In this paper we propose a method of uniformly lexicalizing the standard HAM for part-of-speech tagging in both English and Koret.m.",
        "Because the sparse-data problem is MOM serious in lexicalized models than in the standard model, a simplified version of the well-known back-off smoothing method is used to overcome the problem.",
        "For experiments, the Brown cor-pus(Francis, 1982) is used for English tagging and the KUNLP corpus(Lee et al., 1999) is used for Korean tagging.",
        "The experimental results show that lexicalized models perform better than non-lexicalized models and the simplified back-off smoothing technique can mitigate data sparseness better than simple smoothing techniques.",
        "2 The \"standard\" HMM We basically follow the notation of (Charniak et al., 1993) to describe Bayesian models.",
        "In thus paper, we assume that {wl, to2 , .",
        ", 71)1 is a set of words, IP, 12, , is a set of PUS tags, a sequence of random variables 1111,„ =",
        "sequence of random variables T] ,71 T • • • T/t is a sequence of 7t PUS tags.",
        "Because each of random variables W can take as its value any of the words in the vocabulary, we denote the value of IV,: by wi and a particular sequence of values for 14,7i,1 (i < j) by wij.",
        "In a similar way, we denote the value of Ti by ti and a particular",
        "sequence of values for Tj,j (i < j) by t. For generality, terms wi,j and (i > j) are defined as being empty.",
        "The purpose of Bayesian models for POS tagging is to find the most likely sequence of POS tags for a given sequence of words, as follows: T(wi,„) argmax Pr(T1,„ = wi,n) Because reference to the random variables themselves can be omitted, the above equation becomes:",
        "Then, the probability Pr(ti,„, wi,n) is broken down into Eqn.",
        "3 by using the chain rule.",
        "Because it is difficult to compute Eqn.",
        "3, the standard TIMM simplified it by making a strict Markov assumption to get a more tractable form.",
        "In the standard HA4M, the probability of the current tag ti depends on only the previous K tags and the probability of the current word wi depends on only the current tagl.",
        "Therefore, this model cannot consider lexical information in contexts."
      ]
    },
    {
      "heading": "3 Lexicalized HMMs",
      "text": [
        "In English POS tagging, the tagging unit is a word.",
        "On the contrary, Korean POS tagging prefers a morpheme2.",
        "Figure I shows a word-unit lattice of an English sentence, \"Flies like a flower.\", where each node has a word and its word-unit tag.",
        "Figure 2 shows a morpheme-unit lattice of a Korean sentence, \"NeoNeun Hal Sit issDa.\", where each node has a morpheme and its morpheme-unit tag.",
        "In case of Korean, transitions across a word boundary, which are depicted by a solid line, are distinguished from transitions within a word, which are depicted by a dotted line.",
        "In both cases, sequences connected by bold lines indicate the most likely sequences."
      ]
    },
    {
      "heading": "3.1 Word-unit models",
      "text": [
        "Lexicalized HMMs for word-unit tagging are defined by making a less strict Markov assumption, as follows:",
        "In models A(T(K, I47(1,,o), the probability of the current tag ti depends on both the previous K tags tiK,jI and the previous .1 words and the probability of the current word wi depends on the current tag and the previous L tags and the previous I words So, they can consider lexical information.",
        "In experiments, we set K as 1 or 2, J as 0 or K, L as 1 or 2, and I as 0 or L. If „I and I are zero, the above models are non-lexicalized models.",
        "Otherwise, they are lexicalized models.",
        "In a lexicalized model A(I2,2),1117(2,2)), for example, the probability of a node \"a/AT\" of the most; likely sequence in Figure 1 is calculated as follows:"
      ]
    },
    {
      "heading": "3.2 Morpheme-unit models",
      "text": [
        "l3ayesian models for momheme-unit tagging find the most likely sequence of morphemes and corresponding tags for a given sequence of words, as follows:",
        "in the above equations, u(> TO denotes the number of morphemes in a sequence corresponding the given word sequence, C denotes a morpheme-unit tag, in denotes a morpheme, and p denotes a type of transition from the previous tag to the current tag.",
        "p can have one of two values, \"#\" denoting a transition across a word boundary and \"+\" denoting a transition within a word.",
        "Because it is difficult to calculate Eqn.",
        "6, the word sequence term wl,„ is usually ignored as in Egli.",
        "7.",
        "Instea(l, we introduce p",
        "The probability Pr(cl,„,p2,„, ml,„) is also broken down into Eqn.",
        "8 by using the chain rule.",
        "Because Eqn.",
        "8 is not easy to compute, it is simplified by making a Markov assumption to get a more tractable form.",
        "In a similar way to the case of word-unit tagging, lexicalized EIMMs for morpheme-unit tagging are defined by making a less strict Markov assumption, as follows: A (C[s](K,./), ill[s](L,[)) 1= Pr (el ,u P2,u, In] ,u) uI], ) fi 4(1, Pl",
        "in models A(CH(K,J), ili.k(L,I)), the probability of the current morpheme tag ci depends on both time previous K tags (optionally, the types of their transition pi_K+1,i]) and the previous .1- morphemes i and the probability of the current morpheme mi depends on the current tag and the previous L tags c (optionally, the types of their transition and the previous I morphemes 71,4_1 So, they can also consider lexical in-fOrmation.",
        "In a lexicalized model A(C,(2,2), M1(29)) where word-spacing is considered only in the tag probabilities, for example, the probability of a node \"Su/NNBG\" of the most likely sequence in Figure 2 is calculated as follows:"
      ]
    },
    {
      "heading": "3.3 Parameter estimation",
      "text": [
        "In supervised learning, the simpliest parameter estimation is the maximum likelihood(ML) es-timation(Duda et al., 1973) which maximizes the probability of a training set.",
        "The ML estimate of tag (K+1)-gram probability, Pr mi(ti 13_1(,i_i), is calculated as follows:",
        "where the function Fq(x) returns the frequency of x in the training set.",
        "When using the maximum likelihood estimation, data sparseness is more serious in lexicalized models than in non-lexicalized models because the former has even more parameters than the latter.",
        "In (Chen, 1996), where various smoothing techniques was tested for a language model by using the perplexity measure, a back-off smoothing(Katz, 1987) is said to perform better on a small traning set than other methods.",
        "In the back-off smoothing, the smoothed probability of tag (KO-gram PrsBo(ti I",
        "rm denotes the number of (K+1)-gram whose frequency is r, and the coefficient dr is called the discount ratio, which reflects the Good-Turing estimate(Good, 1953)4.",
        "Eqn.",
        "11 means that Pr sBo(ti is under-etimated by dr than its maximum likelihood estimate, if r > 0, or is backed off by its smoothing term PrsBo(li Iti_K+1,i_l) in proportion to the value of the function a(ti_K,i_d of its conditional term if r = 0.",
        "However, because Eqn.",
        "11 requires complicated computation in cr(ti_K,i_1), we simplify it to get a function of the frequency of a conditional term, as follows:",
        "In Eqn.",
        "12, the range of f is bucketed into 7",
        "regions such as f 0, 1, 2, 3, 4, 5 and f > 6 since it is also difficult to compute this equation for all possible values of f. Using the formalism of our simplified back-off smoothing, each of probabilities whose ML estimate is zero is backed off by its corresponding smoothing term.",
        "In experiments, the smoothing terms of Prspo(ti Wi-,/,i-1) are determined as follows:",
        "Also, the smoothing terms of Prsi30(wi I are determined as follows: Prsno(wi ) if L >1, > 1",
        "In Eqn.",
        "13 and 14, the smoothing term of a unigram probability is calculated by using an additive smoothing with S= 10-2 which is chosen through experiments.",
        "The equation for the additive smoothing(Chen, 1996) is as follows:",
        "In a similar way, the smoothing terms of parameters in Eqn.",
        "9 are determined."
      ]
    },
    {
      "heading": "3.4 Model decoding",
      "text": [
        "From the viewpoint of the lattice structure, the problem of POS tagging can be regarded as the problem of finding the most likely path from the start node ($/$) to the end node ($/$).",
        "The Viterbi search algorithm(Forney, 1973), which has been used for HMM decoding, can be effectively applied to this task just with slight modification5."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Environment",
      "text": [
        "In experiments, the Brown corpus is used for English POS tagging and the KUNLP corpus",
        "for Korean POS tagging.",
        "Table 1 shows sonic information about both corpora6.",
        "Each of them was segmented into two parts, the training set of 90% and the test set of 10%, in the way that each sentence in the test set was extracted from every 10 sentence.",
        "According to Table 1, Korean is said to be inore difficult to disambiguate than English.",
        "We assume \"closed\" vocabulary for English and \"open\" vocabulary for Korean since we do not have any English morphological analyzer consistent with the Brown corpus.",
        "Therefore, for morphological analysis of English, we just",
        "looked up the dictionary tailored to the Brown corpus.",
        "In case of Korean, we have used a Korean morphological analyzer(Lee, 1999) which is consistent; with the KUNLP corpus."
      ]
    },
    {
      "heading": "4.2 Results and evaluation",
      "text": [
        "Table 2 shows the tagging accuracy of the simplest 11M1\\4, A(C(iA), A4(0:o)), for Korean tagging, according to various smoothing methods7.",
        "Note that ML denotes a simple smoothing method where ML estimates with probability less than 10-9 are smoothed and replaced by 10-9.",
        "Because, in the outside-test, AD(ö = 10-2) performs better than ML and AD(S 10-2), we use 6 = 10-2 in our additive smoothing.",
        "According to Table 2, SBO performs well even in the simplest 141\\41\\4.",
        "Figure 3 illustrates 4 graphs'about the results of English tagging: (a) the number of parameters in each model, (b) the accuracy of each model for the training set, (c) the accuracy of each model for the test set, and (d) the accuracy of each model with SBO for both training and test set.",
        "Here, labels in x-axis specify models in the way that denotes A(Tpc/), Therefore, the first 6 models are non-lexicalized models and the others are lexicalized models.",
        "Actually, SBO uses more parameters than others.",
        "The three smoothing methods, ML, AD, SBO, perform well for the training set since the inside-tests usually have little data sparseness.",
        "On the other hand, for the unseen test set, the simple methods, ML and AD, cannot mitigate the data sparseness problem, especially in sophisticated models.",
        "However, our method SBO can overcome the problem, as shown in Figure 3(c).",
        "Also, we can see in Figure 3(d) that some lexicalized models achieve higher accuracy than non-lexicalized models.",
        "We can say that the best lexicalized model, A(/(l,1), W(14)) using SBO, improved the simple bigram model, A (T1,147(0,0)) using SBO, from 97.19% to 97.87% the error reduction ratio of 24.20%).",
        "Interestingly, some lexicalized models (such as A(T(1,1),147(o,())) and A(7.(i,1.",
        ", 147(1,0))), which have a relatively small number of parameters, perform better than non-lexicalized models in the case of outside-tests using SBO.",
        "Unfortunately, we cannot ex-7Inside-test means an experiment on the training set itself and outside-test an experiment on the test set.",
        "pect the result of outside-tests from that of inside-tests because there is no direct relation between them.",
        "Figure 4 includes 2 graphs about the results of Korean tagging: (a) the outside accuracy of each model A(C(K,J),M(L,1)) and (b) the outside accuracy of each model A(C[8](Km, M[8] (/,,n) with/without considering word-spacing when using SBO.",
        "Here, labels in x-axis specify models in the way that :/j denotes A(CH(K,J), M[8] (/,,D) and, for example, C8, M in (b) denotes A(Cs(Km, N(L,f)).",
        "As shown in Figure 4, the simple methods, ML and AD, cannot mitigate that sparse-data problem, but our method SBO can overcome it.",
        "Also, some lexicalized models perform better than non-lexicalized models.",
        "On the other hand, considering word-spacing gives good clues to the models sometimes, but yet we cannot say what is the best way.",
        "From the experimental results, we can say that the best model, A(C(2,2), M(2,2)) using SBO, improved the previous models, A(C(1,0), /111(0,())) us",
        "ing 114L(Lee, 1995), and A (C,( ,0),11//(0,0)) using ML(Kim et al., 1998), from 94.97% and 95.05% to 96.98% (the error reduction ratio of 39.95% and 38.99%) respectively."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We have presented uniformly lexicalized HM1V4s for POS tagging of English and Korean.",
        "In the models, data sparseness was effectively mitigated by using our simplified back-off smoothing.",
        "From the experiments, we have observed that lexical information is useful for POS tagging in IIMMs, as is in other models, and our lexicalized models improved non-lexicalized models by the error reduction ratio of 24.20% (in English tagging) and 39.95% (in Korean tagging).",
        "Generally, the uniform extension of models requires rapid increase of parameters, and hence suffers from large storage and sparse data.",
        "Recently in many areas where HMMs are used, many efforts to extend models non-uniformly have been made, sometimes resulting in noticeable improvement.",
        "For this reason, we are trying to transform our uniform models into non-uniform models, which May be more effective in terms of both space complexity and reliable estifnation of paremeters, without loss of accuracy."
      ]
    }
  ]
}
