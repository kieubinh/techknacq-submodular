{
  "info": {
    "authors": [
      "Qing Ma",
      "Masaki Murata",
      "Kiyotaka Uchimoto",
      "Hitoshi Isahara"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1074",
    "title": "Hybrid Neuro and Rule-Based Part of Speech Taggers",
    "url": "https://aclweb.org/anthology/C00-1074",
    "year": 2000
  },
  "references": [
    "acl-C94-1027",
    "acl-J94-2001",
    "acl-J95-4004",
    "acl-P98-2132"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector.",
        "The neuro tagger is an initial-state annotator that uses different lengths of contexts based on longest context priority.",
        "its inputs are weighted by information gains that are obtained by information maximization.",
        "The rule-based corrector is constructed by a set of transformation rules to make up for the shortcomings of the neuro tagger.",
        "Computer experiments show that almost 20% of the errors made by the neuro tagger are corrected by these transformation rules, so that the hybrid system can reach an accuracy of 95.5% counting only flue ambiguous words and 99.1% counting all words when a small Thai corpus with 22,311 ambiguous words is used for training.",
        "This accuracy is far higher than that using an 11MM and is also higher than that using a rule-based model."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many part of speech (POS) taggers proposed so far (e.g., Brill, 1994; Merialdo, 1994; Daele-mans, et al., 1996; and Schmid, 1994) have achieved a high accuracy partly because a very large amount of data was used to train them (e.g., on the order of 1,000,000 words for English).",
        "For many other languages (e.g., Thai, which we treat in this paper), however, it is not as easy to create large corpora from which large amounts of training data can be extracted.",
        "It is therefore desirable to construct a practical tagger that needs as little training data as possible.",
        "A multi-neuro tagger (Ma and Isahara, 1998) and its slimmed-down version called the elastic neuro tagger (Ma, et al., 1999), which have high generalizing ability and therefore are good at dealing with the problems of data sparseness, were proposed to satisfy this requirement.",
        "These taggers perform POS tagging using different lengths of contexts based on longest context priority, and each element of the input is weighted with information gains (Quinlan, 1993) for reflecting that the elements of the input have different relevances in tagging.",
        "They had a tagging accuracy of 94.4% (counting only the ambiguous words in part of speech) in computer experiments when a small Thai corpus with 22,31.1 ambiguous words was used for training.",
        "This accuracy is far higher than that using the hidden Markov model (11.MM), the main approach to part of speech tagging, and is also higher than that using a rule-based model.",
        "Nenro taggers, however, have several crucial shortcomings.",
        "First, even in the case where the POS of a word is uniquely determined by the word on its left, for example, a neural net will also try to perform tagging based on the complete context.",
        "As a result, even for when the word on the left is the same, the tagging results will be different if the complete contexts are different.",
        "That is, the neuro tagger can hardly acquire the rules with single inputs.",
        "Furthermore, although lexical information is very important in tagging, it is difficult for neural nets to use it because doing so would make the network enormous.",
        "That is, the 11C11170 tagger cannot acquire the rules with lexical information.",
        "Additionally, because of convergence and",
        "over-training problems, it is impossible and also not advisable to train neural nets to an accuracy of 100%.",
        "The training should be stopped at an appropriate level of accuracy.",
        "Consequently, neural nets may not acquire some useful rules.",
        "To make up for these shortcomings of the neuro tagger, we introduce in this paper a rule-based corrector as the post-processor and construct a hybrid system.",
        "The rule-based corrector is constructed by a set of transformation rules, which is acquired by transformation-based error-driven learning (Brill, 1994) from training corpus using a set of templates.",
        "The templates are designed to supply the rules that the neuro tagger can hardly acquire.",
        "Actually, by examining the transformation rules acquired in the computer experiments, the 99.9% of them are exactly those that the neuro tagger can hardly acquire, even when using a template set including those for generating the-rules that the neuro tagger can easily acquire.",
        "This reinforces our expectation that the rule-based approach is a well-suited method to cope with the shortcomings of the neuro tagger.",
        "Computer experiments shows that about 20% of errors made by the neuro tagger can be corrected by using these rules and that the hybrid system can reach an accuracy of 95.5% counting only the ambiguous words and 99.1% counting all words in the testing corpus, when the same corpus described above is used for training."
      ]
    },
    {
      "heading": "2 POS Tagging Problems",
      "text": [
        "In this paper, suppose there is a lexicon V, where the POSs that can be served by each word are listed, and there is a set of POSs, F. That is, unknown words that do not exist in the lexicon are not dealt with.",
        "The POS tagging problem is thus to find a string of POSs T",
        "co when sentence 141 = WOV2 w, (wi E V, 1, , s) is given.",
        ": 1-471Tt,(1) where t is the index of the target word (the word to be tagged), and Wt is a word sequence with length / +1+ r centered on the target word:",
        "where 1 l > 1,r < s. Tagging can thus be regarded as a classification problem by replacing the POS with class and can therefore be handled by using neural nets."
      ]
    },
    {
      "heading": "3 Hybrid System",
      "text": [
        "Our hybrid system (Fig.",
        "1) consists of a 11 el1170 tagger, which is used as an initial-state annotator, and a rule-based corrector, which corrects the outputs of the neuro tagger.",
        "When a word sequence Wi [see Eq.",
        "(2)] is given, the neuro tagger output a tagging result Tiv (ali) for the target word wt at first.",
        "The rule-based corrector then corrects the output of the neuro tagger as a fine tuner and gives the final tagging result"
      ]
    },
    {
      "heading": "3.1 Neuro tagger",
      "text": [
        "As shown in Fig. 2, the neuro tagger consists of a three-layer perceptron with elastic input.",
        "This section mainly describes the construction of input and output of the neuro tagger, and the elasticity by which it becomes possible to use variable length of context for tagging.",
        "For details of the architecture of perceptron see e.g., Haykin, 1994 and for details of the features of the neuro tagger see Ma and Isahara, 1998 and Ma, et al., 1999.",
        "Input IPT is constructed from word sequence Wt [Eq.",
        "(2)], which is centered on target word wt and has length 1 + 1 + r:",
        "provided that input length 1+1+r has elasticity, as described at the end of this section.",
        "When word w is given in position x ,1+0,",
        "defined as fix (end -to21 equ-y)(1) where gx is the information gain which can be obtained using information theory (for details see Ma and isahara, 1998) and y is the number of types of POSs.",
        "if w is a word that appears in the training data, then each bit eilli can be obtained:",
        "where Prob(7-ilw) is a prior probability of Ti that the word 'u can take.",
        "It is estimated from the training data:",
        "where C('r, w) is the number of times both Ti and W appear, and C (w) is the number of times w appears in the training data.",
        "If In is a word that does not appear in the training data, then each bit c.,i is obtained:",
        "provided that the output OPT is decoded as Tiif Oi -=.- 1 KJ, gi = 0 for j i (9) where TN(w1) is the tagging result obtained by the neuro tagger.",
        "There is more information available for constructing the input for words on the left, because they have already been tagged.",
        "In the tagging phase, instead of using (4)-(6), the input can be constructed simply as = PT (-7,) ,(1.0) where i 1, , 1, and OPT(i) means the output of the tagger for the ith word before the target word.",
        "However, in the training process, the output of the tagger is not always correct and cannot be fed back to the inputs directly.",
        "Instead, a weighted average of the actual output and the desired output is used:",
        "where E0B and EAoT are the objective and actual errors.",
        "Thus, at the beginning of training, the weighting of the desired output is large.",
        "It decreases to zero during training.",
        "Elastic inputs are used in the neuro tagger so that the length of context is variable in tagging based on longest context priority.",
        "In detail, (1,y) is initially set as large as possible for tagging.",
        "If TN(W1) Unknown, then (I, r) is reduced by some constant interval.",
        "This process is repeated until TN (wt) Unknown or (1, r) = (0, 0).",
        "On the other hand, to make the same set of connection weights of the neuro tagger with the largest (1,0 available as much as",
        "possible when using short inputs for tagging, in training phase the neuro tagger is regarded as a. neural network that has gradually grown from small one.",
        "The training is therefore performed step by step from small networks to large ones (for details see Ma, et al.",
        "1999)."
      ]
    },
    {
      "heading": "3.2 Rule-based corrector",
      "text": [
        "Even when the POS of a word can be determined with certainty by only the word on the left, for example, the neuro tagger still tries to tag based on the complete context.",
        "That is, in general, what the neuro tagger can easily acquire by learning is the rules whose conditional parts are constructed by all inputs ipt, = t 1, ,1r) that are joined with an AND logical operator, i.e., iptt+,.",
        "OPT).",
        "In other words, it is difficult for the neuro tagger to learn rules whose conditional parts are constructed by only a single input like (ipt, PT)0 .",
        "Also, although lexical information is very important in tagging, it is difficult for the 11C11.170 tagger to use it, because doing so would make the network enormous.",
        "That is, the neuro tagger cannot acquire rules whose conditional parts consist of lexical information like (wOPT), (weerOPT), and (w1 &W2 OPT), where w, w1, and u)9 are words and T is the POS.",
        "Furthermore, because of convergence and over-training problems, it is impossible and also not advisable to train neural nets to an accuracy of 100%.",
        "The training should be stopped at an appropriate level of accuracy.",
        "Thus, neural net may not acquire some useful rules.",
        "The transformation rule-based corrector makes up for these crucial shortcomings.",
        "The rules are acquired from a training corpus using a set of transformation templates by transformation-based error-driven learning (Brill, 1994).",
        "The templates are constructed using only those that supply the rules that the neuro tagger can hardly acquire, i.e., are those The neuro tagger can also learn this kind of rules because it can tag the word using only iptt (the input of the target word), in the case of reducing the (1, r) to (0,0), as described in Sec. 3.1.",
        "The rules with single input described here, however, are a more general case, in which the input can be ipt= t 1, , t r).",
        "for acquiring the rules with single input, with lexical information, and with AND logical input of POSs and lexical information.",
        "The set of templates is shown in Table 12).",
        "According to the learning procedure shown in Table 2, an ordered list of transformation rules are acquired by applying the template set to a training corpus, which had already been tagged by the neuro tagger.",
        "After the transformation rules are acquired, a corpus is tagged as follows.",
        "It is first tagged by the neuro tagger.",
        "The tagged corpus is then corrected by using the ordered list of transformation rules.",
        "The correction is a repetitive process applying the rules in order to the corpus, which is then updated, until all rules have been applied."
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": [
        "Data: For our computer experiments, we used the same Thai corpus used by Ma et al.",
        "(1999).",
        "Its 10,452 sentences were randomly divided into two sets: one with 8,322 sentences for training and the other with 2,1.30 sentences for testing.",
        "Time training set contained 124,331 words, of which 22,311 were ambiguous; the testing set contained 34,544 words, of which 6,717 were ambiguous.",
        "For training the neuro tagger, only the ambiguous words in the training set were used.",
        "For training the IIMM, all the words in the training set were used.",
        "In both cases, all the words in the training set were used to estimate Prob(r'jw), the probability of Tz that word w can be (for details on the IIMM, see Ma, et al., 1999).",
        "in the corpus, 47 types of POSs are defined (Charoenporn et al., 1997); i.e., = 47.",
        "Neuro tagger: The neuro tagger was constructed by a three-layer perceptron whose input-middle-output layers had p 2 7 units, respectively, where p 7 x (1 + 1 + r).",
        "The (1+1+ r) had the following elasticity.",
        "In training, the (1, r) was increased step by step as (1,1) 4 (2,1) -+ (2,2) 4 (3,2)(3,3) and gradual training from a small to a large network was performed.",
        "In tagging, on the other hand, the 2) To see whether this set is suitable, a number of additional experiments were conducted using various sets of templates.",
        "The details are described in Sec.",
        "<I.",
        "Change tag T\" to tag T when: (single input) (input consists of a POS) 1. left (right) word is tagged T. 2. second left (right) word is tagged T. 3. third left (right) word is tagged T. (input consists of a word) 4. target word is w. 5. left (right) word is w. 6. second left (right) word is w. (AND logical input of words) 7. target word is w1 and left (right) word is w2. 8. left (right) word is wl and second left (right) word is w2.",
        "9. left word is wt and right word is w2.",
        "(AND logical input of POS and words) 10. target word is Iv] and left (right) word is tagged T. 11. left (right) word is 7/4 and left (right) word is tagged T. 12. target word is wt, left (right) word is w2, and left (right) word is tagged T. Table 2: Procedure for learning transformation rules 1.",
        "Apply limo tagger to training corpus, which is then updated.",
        "2.",
        "Compare tagged results with desired ones and find errors.",
        "3.",
        "Match templates for all errors and obtain set of transformation rules.",
        "Select rule in corpus with the maximum value of (eta ._3tod h ent _bad), where cut _good: number that transforms incorrect tags to correct ones, ent _bad: number that transforms correct tags to incorrect ones, h: weight to control the strictness of generating the rule.",
        "5.",
        "Apply selected rule to training corpus, which is then updated.",
        "6.",
        "Append selected ride to ordered list of transformation 7.",
        "Repeat steps 2 through Ii until no such rule can be selected, i.e., eta _good h eta _bad < 0.",
        "was inversely rectum] step by step as (3,3) + (3,2) 4 (2,2) 4 (2,1) (1,1.",
        ")(.1,0) (0,0) as needed, provided that the number of units in the middle layer was kept at the maxi-intun value.",
        "Rule-based corrector: The parameter h in the evaluation function (ettLgood eta _bad) used in the learning procedure (Table 2) is a weight to control the strictness of generating a rule.",
        "If h is large, the weight of cut __bad is large and the possibility of generating incorrect rules is reduced.",
        "By regarding the neuro tagger as already having high accuracy and using the rule-based corrector as a fine tuner, weight It was set to a large value, 100.",
        "Applying the templates to the training corpus, which had already been tagged by the neuro tagger, we obtained an ordered list of 520 transformation rules.",
        "Table 3 shows the first 15 transformation rules.",
        "Results: Table 4 shows the results of POS tagging for the testing data.",
        "In addition to the accuracy of the neuro tagger and hybrid system, the table also shows the accuracy of a baseline model, the and a rule-based model for comparison.",
        "The baseline model is one that performs tagging without using the contextual information; instead, it performs tagging using only frequency information: the probability of POS that each word can be.",
        "The rule-based model, to be exact, is also a hybrid system con513",
        "*Accuracy] was determined only for ambiguous words.",
        "sisting of an initial-state annotator and a set of transformation.",
        "rules.",
        "As the initial-state annotator, however, the baseline model is used instead of the neuro tagger.",
        "And, its rule set has 1,177 transformation rules acquired from a more general template set, which is described at the end of this section.",
        "The reason for using a general template set is that the set of transformation rules in the rule-based model should be the main annotator, not a fine post-processing tuner.",
        "For the same reason, the parameter to control the strictness of generating a rule, Ii, was set to a small value, 1, so that a larger number of rules were generated.",
        "As shown in the table, the accuracy of the neuro tagger was far higher than that of the HMM and higher than that of the rule-based model.",
        "The accuracy of the rule-based model, on the other hand, was also far higher than that of the HMM, although it was inferior to that of the neuro tagger.",
        "The accuracy of the hybrid system was 1.1% higher than that of the neuro tagger.",
        "Actually, the rule-based corrector corrected 88.4% and 19.7% of the errors made by the neuro tagger for the training and testing data, respectively.",
        "Because the template set shown in Table 1 was designed only to make up for the shortcomings of the neuro tagger, the set is smal1 compared to that used by Brill (1994).",
        "To see whether this set is large enough for our system, we performed two additional experiments in which (1) a set constructed by adding the templates with OR logical input of words to the original set and (2) a set constructed by further adding the templates with AND and OR logical inputs of POSs to the set of case (1) were used.",
        "The set used in case (2) included the set used by Brill (1994) and all the sets used in our experiments.",
        "It was also used for acquiring the transformation rules in the rule-based model.",
        "The experimental results show that compared to the original case, the accuracy in case (1) was improved very little and the accuracy in case (2) was also improved only 0.03%.",
        "These results show that the original set is nearly large enough for our system.",
        "To see whether the set is suitable for our system, we performed an additional experiment using the original set in which the templates with OR logical inputs were used instead of the templates with AND logical inputs.",
        "The accuracy dropped by 0.1%.",
        "Therefore, the templates with AND logical inputs are more suitable than",
        "those with OR logical inputs.",
        "We also performed an experunent using a template set without lexical information.",
        "In this case, the accuracy dropped 1>y 0.9%, indicating that lexical information is important in tagging.",
        "To determine the effect of using a large It for generating rules, we performed an experiment with h In this case, the accuracy dropped by only 0.045%, an insignificant difference compared to the case of Ii = 1.00.",
        "By examining the acquired rules that were obtained by applying the most complete template set, i.e., the set used in case (2) described above, we found that 99.9% of them were those lliat can be obtained by applying the original set of templates.",
        "That is, the acquired rules were almost those that are difficult for the neuro tagger to acquire.",
        "This reinforced our expectation that, the rule-based approach is a well-suited method to cope with the shortcoming of the neuro tagger.",
        "Finally, it should be noted that in the literatures, the tagging accuracy is usually defined by counting all the words regardless of whether they are ambiguous or not.",
        "If we used this definition, the accuracy of our hybrid system would be 99.1%."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "To construct a practical tagger that needs as little training data as possible, neuro taggers, which have high generalizing ability and therefore are good at dealing with the problems of data sparseness, have been proposed so far.",
        "Neuro taggers, however, have crucial shortcomings: they cannot, utilize lexical information; they have trouble learning rules with single inputs; and they cannot learn training data, to an accuracy of 100%.",
        "To make up for these shortcomings, we introduced a rule-based corrector, which is constructed by a. set of transformation rules obtained by error-driven learning, for post processing and constructed a hybrid tagging system.",
        "By examining the transformation rules acquired in the computer experiments, we found that the 99.9% of them were those that the neuro tagger can hardly acquire, even when using a template set including those for generating the rules that the neuro tagger can easily acquire.",
        "This reinforced our expectation that the rule-based approach is a well-suited method to cope with the shortcoming of the neuro tagger.",
        "Computer experiments showed that 19.7% of the errors made by the neuro tagger were corrected by the transformation rules, so the hybrid system reached an accuracy of 95.5% counting only the ambiguous words and 99.1% counting all the words in the testing data, when a small corpus with only 22,31.1 ambiguous words was used for training.",
        "This indicates that our tagging system can nearly reach a practical level in terms of tagging accuracy even when a small Thai corpus is used for training.",
        "This kind of tagging system can be used to constructs multilingual corpora that include languages in which large corpora have not yet been constructed."
      ]
    }
  ]
}
