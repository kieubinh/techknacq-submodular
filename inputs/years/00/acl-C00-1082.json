{
  "info": {
    "authors": [
      "Masaki Murata",
      "Kiyotaka Uchimoto",
      "Qing Ma",
      "Hitoshi Isahara"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1082",
    "title": "Bunsetsu Identification Using Category-Exclusive Rules",
    "url": "https://aclweb.org/anthology/C00-1082",
    "year": 2000
  },
  "references": [
    "acl-E99-1023",
    "acl-E99-1026",
    "acl-J96-1002",
    "acl-P94-1013",
    "acl-W95-0107",
    "acl-W96-0213",
    "acl-W97-0301",
    "acl-W98-1118"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Tins paper describes two new bunsetsu identification methods using supervised learning.",
        "Since Japanese syntactic analysis is usually done after bunsetsu identification, bunsetsu identification is important for analyzing Japanese sentences.",
        "1.n experiments comparing the four previously available machine-learning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "This paper is about; machine learning methods for identifying bunsetsus, which correspond to English phrasal units such as 1101111 phrases and prepositional phrases.",
        "Since Japanese syntactic analysis is usually done after bunsetsu identification (Uchimoto et al., 1.999), identifying bunsetsu is important for analyzing Japanese sentences.",
        "The conventional studies on bunsetsu identification' have used handmade rules (Kameda, 1.995; Kurohashi, 1.998), but bunsetsu identification is not an easy task.",
        "Conventional studies used many handmade rules developed at the cost of many man-hours.",
        "Kurohashi, for example, made 146 rules for bunsetsu identification (Kurohashi, 1998).",
        "In an attempt to reduce the number of man-hours, we used machine-learning methods for bunsetsu identification.",
        "Because it was not clear which machine-learning method would be the one most appropriate for bunsetsu identification, so we tried a variety of them.",
        "In tins paper we report experiments comparing four machine-learning methods (decision tree, maximum entropy, example-based, and decision list methods) and our new methods using category-exclusive rules.",
        "Illunsetsu identification is a problem similar to chunking (Ranishaw and Marcus, .1995; Sang and Veenstra, 1999) in other languages."
      ]
    },
    {
      "heading": "2 Bunsetsu identification problem",
      "text": [
        "We conducted experiments on the following supervised learning methods for identifying bunsetsu:",
        "• Decision tree method • Maximum entropy method • Example-based method (use of similarity) • Decision list (use of probability and frequency) • Method 1 (use of exclusive rules) • Method 2 (use of exclusive rules with the highest similarity).",
        "In general, bunsetsu identification is done after morphological and before syntactic analysis.",
        "'Morphological analysis corresponds to part-of-speech tagging in English.",
        "Japanese syntactic structures are usually represented by the relations between bunsetsus, winch correspond to phrasal units such as a 1101111 phrase or a prepositional phrase in English.",
        "So, bunsetsu identification is important in Japanese sentence analysis.",
        "In this paper, we identify a bunsetsu by using information from a morphological analysis.",
        "Bunsetsu identification is treated as the task of deciding whether to insert a \"I\" mark to indicate the partition between two bunsetsus as in Figure 1.",
        "Therefore, bunsetsu identification is done by judging whether a partition mark should be inserted between two adjacent morphemes or not.",
        "(We do not use the inserted partition mark in the following analysis in tins paper for the sake of simplicity.)",
        "Our bunsetsu identification method uses the morphological information of the two preceding and two succeeding morphemes of an analyzed space between two adjacent morphemes.",
        "We use the following morphological information:",
        "(i) Major part-of-speech (PUS) category,' (ii) Minor PUS category or inflection type, (iii) Semantic information (the first three-digit number of a category number as used in \"BGII\" (MAU, 1964)),",
        "2Part-of-speech categories follow those of .111MAN (Kurohashi and Nagao, 1998).",
        "(iv) Word (lexical information).",
        "For simplicity we do not use the \"Semantic infor-mation\" and \"Word\" in either of the two outside morphemes.",
        "Figure 2 shows the information used to judge whether or not to insert a partition mark in the space between two adjacent morphemes, \"wo (obj)\" and \"kugiru (divide),\" in the sentence \"bun wo kugiru.",
        "((I) divide sentences).\""
      ]
    },
    {
      "heading": "3 Bunsetsu identification process for each machine-learning method",
      "text": []
    },
    {
      "heading": "3.1 Decision-tree method",
      "text": [
        "In this work we used the program C4.5 (Quinlan, 1995) for the decision-tree learning method.",
        "The four types of information, (i) major POS, (ii) minor PUS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method.",
        "As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes.",
        "In Figure 2, for example, the value of the feature `the major PUS of the far left morpheme' is 'Noun.'"
      ]
    },
    {
      "heading": "3.2 Maximum-entropy method",
      "text": [
        "The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999).",
        "In our maximum-entropy experiment we used Ristad's system (Ristad, 1998).",
        "The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system.",
        "Whichever probability is higher is selected as the desired answer.",
        "In the maximum-entropy method, we use the same four types of morphological information, (i) major PUS, (ii) minor PUS, (iii) semantic information, and (iv) word, as in the decision-tree method.",
        "However, it does not consider a combination of features.",
        "Unlike the decision-tree method, as a result we had to combine features manually.",
        "First we considered a combination of the bits of each morphological information.",
        "Because there were four types of information, the total number of combinations was 24 – 1.",
        "Since this number is large and intractable, we considered that (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word information gradually become more specific in this order, and we combined the four types of information in the following way:",
        "We used only Information A and B for the two outside morphemes because we did not use semantic and word information in the same way it is used in the decision-tree method.",
        "Next, we considered the combinations of each type of information.",
        "As shown in Figure 4, the number of combinations was 64 (2 x 4 x 4 x 2).",
        "For data sparseness, in addition to the above combinations, we considered the cases in which first, one of the two outside morphemes was not used, secondly, neither of the two outside ones were used, and thirdly, only one of the two middle ones is used.",
        "The number of features used in the maximum-entropy method is 152, which is obtained as follows:3",
        "In Figure 2, the feature that uses Information B in the far left morpheme, Information D in the left morpheme, Information C in the right morpheme, and Information A in time far right morpheme is \"Noun: Normal Noun; Particle: Case-Particle: none: wo; Verb: Normal Form: 217; Symbol\".",
        "In the maximum-entropy method we used for each space 152 features such as this one."
      ]
    },
    {
      "heading": "3.3 Example-based method (use of similarity)",
      "text": [
        "An example-based method was proposed by Nagao (Nagao, 1984) in an attempt to solve problems in machine translation.",
        "To resolve a problem, it uses the most similar example.",
        "In the present work, the example-based method impartially used the same four types of information (see Eq.",
        "(1)) as used in the maximum-entropy method.",
        "To use this method, we must define the similarity of an input to an example.",
        "We use the 152 patterns from the maximum-entropy method to establish the level of similarity.",
        "We define the similarity S between an input and an example according to which one of these 152 levels is the matching level, as follows.",
        "(Time equation reflects the importance of the two middle morphemes.)",
        "January 1, 7.905 of a Kyoto University corpus (the number of spaces between morphemes was 25,814) by using this method, the number of types of features was 1,534,701.",
        "Here m_1, 77b+1, 777_9, and m+2 refer respectively to the left, right, far left, and far right morphemes, and s(x) is the morphological similarity of a morpheme x, which is defined as follows:",
        "Figure 5 shows an example of the levels of similarity.",
        "When a pattern matches Information A of all four morphemes, such as \"Noun; Particle; Verb; Symbol\", its similarity is 40,004 (2 x 2 x 1.0,000 + 2 x 2).",
        "When a pattern matches a pattern, such as \"- Particle: Case-Particle: none: wo; -; \", its similarity is 50,001 (5 x 1 x 10, 000 + 1 x 1).",
        "The example-based method extracts the example with the highest level of similarity and checks whether or not that example is marked.",
        "A partition mark is inserted in the input data only when time example is marked.",
        "When multiple examples have the same highest level of similarity, the selection of the best example is ambiguous.",
        "In this case, we count the number of marked and unmarked spaces in all of the examples and choose the larger."
      ]
    },
    {
      "heading": "3.4 Decision-list method (use of probability and frequency)",
      "text": [
        "Time decision-list method was proposed by Rivest (Rivest, 1987), in which the rules are not expressed as a. tree structure like in the decision-tree method,",
        "but are expanded by combining all the features, and are stored in a one-dimensional list.",
        "A priority order is defined in a certain way and all of the rules are arranged in this order.",
        "The decision-list method searches for rules from the top of the list and analyzes a particular problem by using only the first applicable rule.",
        "In this study we used in the decision-list method the same 152 types of patterns that were used in the maximum-entropy method.",
        "To determine the priority order of the rules, we referred to Yarowsky's method (Yarowsky, 1994) and Nishiokayama's method (Nishiokayama et al., 1998) and used the probability and frequency of each rule as measures of this priority order.",
        "When multiple rules had the same probability, the rules were arranged in order of their frequency.",
        "Suppose, for example, that Pattern A \"Noun: Normal Noun; Particle: Case-Particle: none: WO; Verb: Normal Form: 217; Symbol: Punctuation\" occurs 13 times in a learning set and that ten of tile occurrences include the inserted partition mark.",
        "Suppose also that Pattern B \"Noun; Particle; Verb; Symbol\" occurs 123 times in a learning set and that 90 of the occurrences include the mark.",
        "This example is recognized by the following rules: Pattern A Partition 76.9% (10/ 13), Freq.",
        "23 Pattern B Partition 73.2% (90/123), Freq.",
        "123 Many similar rules were made and were then listed in order of their probabilities and, for any one probability, in order of their frequencies.",
        "This list was searched from the top and the answer was obtained by using the first applicable rule."
      ]
    },
    {
      "heading": "3.5 Method 1 (use of category-exclusive rules)",
      "text": [
        "So far, we have described the four existing machine learning methods.",
        "In the next two sections we describe our methods.",
        "It is reasonable to consider the 152 patterns used in three of the previous methods.",
        "Now, let us suppose that the 152 patterns from the learning set yield the statistics of Figure 6.",
        "\"Partition\" means that the rule determines that a partition mark should be inserted in the input data and \"non-partition\" means that the rule determines that a partition mark should not be inserted.",
        "Suppose that when we solve a hypothetical problem Patterns A to G are applicable.",
        "If we use the decision-list method, only Rule A is used, winch is applied first, and this determines that a partition mark should not be inserted.",
        "For Rules 13, C, and D, although the frequency of each rule is lower than that of Rule A, the sum of their frequencies of the rules is higher, so we think that it is better to use Rules B, C, and D than Rule A.",
        "Method 1 follows this idea, but we do not simply sum up the frequencies.",
        "Instead, we count the number of examples used in Rules B, C, and D and judge the category having the largest number of examples that satisfy the pattern with the highest probability to be the desired answer.",
        "For example, suppose that in the above example the number of examples satisfying Rules B, C, and is 65.",
        "(Because some examples overlap in multiple rules, the total number of examples is actually smaller than the total number of the frequencies of the three rules.)",
        "In this case, among the examples used by the rules having 100% probability, the number of examples of partition is 65, and the number of examples of non-partition is 34.",
        "So, we determine that the desired answer is to partition.",
        "A rule having 100% probability is called a category-exclusive rule because all the data satisfying it belong to one category, which is either partition or non-partition.",
        "Because for any given space the number of rules used can be as large as 152, category-exclusive rules are applied often.",
        "Method 1 uses all of these category-exclusive rules, so we call it the method using category-exclusive rules.",
        "Solving problems by using rules whose probabilities are not 100% may result in the wrong solutions.",
        "Almost all of the traditional machine learning methods solve problems by using rules whose probabilities",
        "are not 100%.",
        "By using such methods, we cannot hope to improve accuracy.",
        "If we want to improve accuracy, we must use category-exclusive rules.",
        "There are sonic cases, however, for which, even if we take this approach, category-exclusive rules are rarely applied.",
        "In such cases, we must add new features to the analysis to create a situation in which many category-exclusive rules can be applied.",
        "Ilowever, it is not sufficient to use category-exclusive rules.",
        "There are many meaningless rules which happen to be category-exclusive only in a learning set.",
        "We must consider how to eliminate such meaningless rules.",
        "3.6 Method 2 (using category-exclusive rules with the highest similarity) M.ethod 2 combines the example-based method and Method 1.",
        "That is, it combines tile method using similarity and the method using category-exclusive rules in order to eliminate the meaningless category-exclusive rules mentioned in the previous section.",
        "Method 2 also uses 152 patterns for identifying hunsetsu.",
        "These patterns are used as rules in the salve way as ill Method 1.",
        "Desired answers are determined by using the rule having the highest probal)ility.",
        "When multiple rules have the same probability, Method 2 uses the value of the similarity described in the section of the example-based method and analyzes the problem with the rule having the highest similarity.",
        "When multiple rules have the same probability and similarity, the method takes the examples used by the rules having the highest probability and the highest similarity, and chooses the category with the larger number of examples as the desired answer, in the smile way as in Method I.",
        "However, when category-exclusive rules having more than one frequency exist;, the above procedure is performed after eliminating all of the category-exclusive rules having one frequency.",
        "In other words, category-exclusive rules having more than one frequency are given a. higher priority than category-exclusive rules having only one frequency but having a higher similarity.",
        "This is because category-exclusive rules having only one frequency are not so reli able."
      ]
    },
    {
      "heading": "4 Experiments and discussion",
      "text": [
        "In our experiments we used a Kyoto University text corpus (Kurohashi and Nagao, 1997), which is a tagged corpus made up of articles from the Mainichi newspaper.",
        "All experiments reported in this paper were performed using articles dated from January 1 to 5, 1995.",
        "We obtained the correct information on morphology and bunsetsu identification from the tagged corpus.",
        "The following experiments were conducted to determine which supervised learning method achieves the highest accuracy rate.",
        "• Experiment 1 Learning set: January 1, 1995 Test set: January 3, 1995 • Experiment 2",
        "Learning set: January 4, 1995 Test; set: January 5, 1995 lbecause we used Experiment 1 in niaking Method 1 and Method 2, Experiment 1 is a closed data set for Method.",
        "1 and Method.",
        "2.",
        "So, we performed l',Ixperinient 2.",
        "The results are listed in Tables 1 to 4.",
        "We used KNP2.0b4 (Kurohashi,:1997) and KNP2.0b6 (Kurohashi„ 1998), which are bunsetsu identification and syntactic: analysis systems using many handmade rules in addition to the six methods described in Section 3.",
        "Because KNP is not based on a machine learning method but many handmade rules, in the KNP results \"Learning set\" and \"Test; set\" in the tables have no meanings.",
        "In the experiment of KNP, we also uses morphological information in a corpus.",
        "The \"IT in the tables indicates the F-ineasure, which is the harmonic mean of a recall and a precision.",
        "A recall is the fraction of correctly identified partitions out of all the partitions.",
        "A precision is the fraction of correctly identified partitions out of all the spaces which were judged to have a partition mark inserted.",
        "Tables 1 to 4 show the following results:",
        "• In the test set the decision-tree method was a little better than the maximum-entropy",
        "The number of spaces between two morphemes is 32,304.",
        "The number of partitions is 11,756. method.",
        "Although the maximum-entropy method has a weak point in that it does not learn the combinations of features, we could overcome tins weakness by making almost all of the combinations of features to produce a higher accuracy rate.",
        "• The decision-list method was better than the maximum-entropy method in this experiment.",
        "• The example-based method obtained the highest accuracy rate among the four existing methods.",
        "• Although Method 1, winch uses the category-exclusive rule, was worse than time example-based method, it was better than the decision-list method.",
        "One reason for this was that the decision-list method chooses rules randomly when multiple rules have identical probabilities and frequencies.",
        "• Method 2, which uses the category-exclusive",
        "rule with the highest similarity, achieved the highest accuracy rate among the supervised learning methods.",
        "• The example-based method, the decision-list method, Method 1 and Method 2 obtained accuracy rates of about 100% for the learning set.",
        "This indicates that these methods are especially strong for learning sets.",
        "• The two methods using similarity (example-based method and Method 2) were always better than the other methods, indicating that the use of similarity is effective if we can define it appropriately.",
        "• We carried out experiments by using KNP, a system that uses many handmade rules.",
        "The F-measure of KNP was highest in the test set.",
        "• We used two versions of KNP, KNP 2.0b4 and KNP 2.01)6.",
        "The latter was much better than the former, indicating that the improvements made by hand are effective.",
        "But, the maintenance of rules by hand has a limit, so the improvements made by hand are not always effective.",
        "The above experiments indicate that Method 2 is best among the machine learning methods.",
        "In Table 5 we show some cases winch were partitioned incorrectly with KNP but correctly with 51n these experiments, the differences were very small.",
        "13ut, we think that the differences are significant to some extent because we performed Experiment 1 and Experiment 2, the data we used are a large corpus containing about a few ten thousand morphemes and tagged objectively in advance, and the difference of about 0.1% is large in the precisions of 99%.",
        "(steadily) (be patient; with) (... be patient with ... steadily) joyou no I matte.",
        "IN L'El) shirizoke (enough strength) obj (have) (beat off) (... beat off ... having enough strength) kaisha no garripu-wakeliv RON\" shits company obj (grouping) (do) (... do grouping companies) Method 2.",
        ".A partition with \"NEED\" indicates that KNP missed inserting the partition mark, mid a partition with \"WRONG\" indicates that KNP inserted the partition mark incorrectly.",
        "In the test set of Experiment 1, the F-measure of KNP2.0b6 was 99.66%.",
        "The F-measure increases to 99.83%, under the assumption that when KNP2.0b6 or Method 2 is correct, the answer is correct.",
        "Although the accuracy rate for KNP2.0b6 was high, there were sonic cases in which KNP partitioned incorrectly and Method 2 partitioned correctly.",
        "A combination of Method 2 with KNP2.0b6 may be able to improve the F-measure.",
        "The only previous research resolving bunsetsu identification by machine learning methods, is the work by Zhang (Zhang and Ozeki, 1998).",
        "The decision-tree method was used in this work.",
        "But this work used only a small number of information for bunsetsu identification' and did not achieve high accuracy rates.",
        "(The recall rate was 97.6%(=2502/(2502+62)), the precision rate VMS 92.4%(--=2502/(2502+205)), and F-measure was 94.2%.)"
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "To solve the problem of accurate bunsetsu identification, we carried out experiments comparing four existing machine-learning methods (decision-tree method, maximum-entropy method, example-based method and decision-list method).",
        "We obtained the following order of accuracy in bunsetsu identification.",
        "Example-Based > Decision List > Maximum Entropy > Decision Tree We also described a new method which uses category-exclusive rules with the highest similarity.",
        "This method performed better than the other learning methods in our experiments."
      ]
    }
  ]
}
