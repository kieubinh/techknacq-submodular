{
  "info": {
    "authors": [
      "Christer Samuelsson"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-2099",
    "title": "A Statistical Theory of Dependency Syntax",
    "url": "https://aclweb.org/anthology/C00-2099",
    "year": 2000
  },
  "references": [
    "acl-A97-1011",
    "acl-C96-2215",
    "acl-P93-1005",
    "acl-P95-1037",
    "acl-P96-1023",
    "acl-P97-1003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "A generative statistical model of dependency syntax is proposed based on Tesniere's classical theory.",
        "It provides a stochastic formalization of the original model of syntactic structure and augments it with a model of the string realization process, the latter which is lacking in Tesniere's original work.",
        "The resulting theory models crossing dependency links, discontinuous nuclei and string merging, and it has been given an efficient computational rendering."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The theory of dependency grammar culminated in the seminal book by Lucien Tesniere, (Tesniere, 1959), to which also today's leading scholars pay homage, see, e.g., (Mel'hik, 1987).",
        "Unfortunately, Tesniere's book is only available in French, with a. partial translation into German, and subsequent descriptions of his work reported in English, Mays, 1964), (Gaifman, 1965), (Robinson, 1970), etc., stray increasingly further from the original, see (Engel, 1996) or (Firvinen, 1998) for au account of this.",
        "The first step when assigning a dependency description to an input string is to segment the input string into nuclei.",
        "A nucleus can be a word, a part of a word, or a sequence of words and subwords, and these need not appear contiguously in the input string.",
        "The best way to visualize this is perhaps the following: the string is tokenized into a sequence of tokens and each nucleus consists of a subsequence of these tokens.",
        "Alternative readings may imply different ways of dividing the token sequence into nuclei, and segmenting the input string into nuclei is therefore in general a nondeterministic process.",
        "The next step is to relate the nuclei to each other through dependency links, which are directed and typed.",
        "If there is a dependency link from one nucleus to another, the former is called a dependent of the latter, and the latter a regent of the former.",
        "Theories of dependency syntax typically require that each nucleus, save a single root nucleus, is assigned a unique regent, and that there is no chain of dependency links that constitutes a cycle.",
        "This means that the dependency links establish a tree structure,",
        "where each node is labeled by a nucleus.",
        "Thus, the label assigned to a node is a dependent of the label assigned to its parent node, and conversely, the label assigned to a node is the regent of the labels assigned to its child nodes.",
        "Figure 1 shows two dependency trees for the sentence John ate beans.",
        "In Tesniere's dependency syntax, only the dependency structure, not the order of the dependents, is represented by a dependency tree.",
        "This means that dependency trees are unordered, and thus that the two trees of Figure 1 are equivalent.",
        "This also means that specifying the surface-string realization of a dependency description becomes a separate issue.",
        "We will model dependency descriptions as two separate stochastic processes: one top-down process generating the tree structure T and one bottom-up process generating the surface string(s) S given the tree structure:",
        "This can be viewed as a variant of Shannon's noisy channel model, consisting of a language model of tree structures and a signal model converting trees to surface strings.",
        "In Section 2 we describe the top-down process generating tree structures and in Section 3 we propose a series of increasingly more sophisticated bottom-up processes generating surface strings, which result in grammars with increasingly greater expressive power.",
        "Section 4 describes how the proposed stochastic model of dependency syntax was realized as a probabilistic chart parser."
      ]
    },
    {
      "heading": "2 Generating Dependency Trees",
      "text": [
        "To describe a tree structure T, we will use a string notation, introduced in (Corn, 1962), for the nodes",
        "of the tree, where the node name specifies the path from the root node c to the node in question.",
        "If j is a node of the tree T, with j E N+ and o E N+, then 0 is also a. node of the tree T and 0j is a child of 0.",
        "Here, N+ denotes the set of positive integers {1, 2, and N+* is the set of strings over N+.",
        "This means that the label assigned to node 0j is a dependent of the label assigned to node 0.",
        "The first dependency tree of Figure 1. is shown in Figure 2 using this notation.",
        "We introduce three basic random variables, which incrementally generate the tree structure: E(0) = 1 assigns the label I to node 0, where is a nucleus, i.e., it is drawn from the set of strings over the set of tokens.",
        "D(0j)= d indicates the dependency type d linking the label of node 0j to its regent, the label of node 0.",
        "V(0)v indicates that node 0 has exactly v child nodes.",
        "Note the use of V(0) = 0, rather than a partitioning of the labels into terminal and nonterminal symbols, to indicate that 0 is a leaf node.",
        "Let D be the (finite) set of possible dependency types.",
        "We next introduce the composite variables .T(0) ranging over the power bag' ND, indicating the bag of dependency types of 0's children:",
        "Figure 3 encodes the dependency tree of Figure 2 accordingly.",
        "We will ignore the last column for now.",
        "A bag (multiset) can contain several tokens of the same type.",
        "We denote setsbags [...] and ordered triples but overload U, C, etc.",
        "We introduce the probabilities",
        "These probabilities are typically model parameters, or further decomposed into such.",
        "1),c(qlj) is the probability of the label ,C(0j) of a node given the label E(0) of its regent and the dependency type D(0j) linking them.",
        "Relating j) and ,C(0) yields lexical collocation statistics and including D(0j) makes the collocation statistics lexical-functional.",
        "P.r(c) is the probability of the bag of dependency types T(0) of a node given its label E(0) and its relation v(o) to its regent.",
        "This reflects the probability of the label's valency, or lexical-functional complement, and of optional adjuncts.",
        "Including 1)(0) makes this probability situated in taking its current role into account.",
        "These allow us to define the tree probability",
        "where the product is taken over the set of nodes AT of the free.",
        "We generate the random variables ,C and T using a top-down stochastic process, where ,C(0) is generated before I'M.",
        "The probability of the conditioning material of Pc(cij) is then known from Pr (0) and P:F(0), and that of Py(0j) is known from PE (OA and Pt-(cu).",
        "Figure 3 shows the process generating the dependency tree of Figure 2 by reading the E and T columns downwards in parallel, ,C before",
        "Consider calculating the probabilities at node 1:"
      ]
    },
    {
      "heading": "3 String Realization",
      "text": [
        "The string realization cannot be uniquely determined from the tree structure.",
        "To model the string-realization process, we introduce another fundamental random variable S(0), which denotes the string",
        "associated with node 0 and which should not be confused with the node label E(0).",
        "We will introduce yet another fundamental random variable A4(0) in Section 3.2, when we accommodate crossing dependency links.",
        "In Section 3.1, we present a projective stochastic dependency grammar with an expressive power not exceeding that of stochastic context-free grammars."
      ]
    },
    {
      "heading": "3.1 Projective Dependency Grammars",
      "text": [
        "We let the stochastic process generating the ,C and variables be as described above.",
        "We then define the stochastic string-realization process by letting the SO) variables, given 0's label 1(0) and the bag of strings s(ctij) of O's child nodes, randomly permute and concatenate them according to the probability distributions of the model:",
        "The latter equations should be interpreted as defining the random variable S, rather than specifying its probability distribution or some possible outcome.",
        "This means that each dependent is realized adjacent to its regent, where we allow intervening siblings, and that we thus stay within the expressive power of stochastic context-free grammars.",
        "We define the string-realization probability",
        "The stochastic process generating the tree structure is as described above.",
        "We then generate the string variables S using a bottom-up stochastic process.",
        "Figure 3 also shows the process realizing the surface string John cite beans from the dependency tree of Figure 2 by reading the S column upwards:",
        "Consider calculating the string probability at node",
        "1.",
        "Ps is the probability of the particular permutation observed of the strings of the children and the",
        "say that John ate?",
        "label of the node.",
        "To overcome the sparse-data.",
        "problem, we will generalize over the actual strings of the children to their dependency types.",
        "For example, s(subj) denotes the string of the subject child, regardless of what it actually might be.",
        "This is the probability of the permutation (s(subj), ate, s(dobj)) of the bag [s(subj), ate, s(dobj)] given this ba.g and the fact that we wish to form a main, declarative clause.",
        "This example highlights the relationship between the node strings and both Saussure's notion of constituency and the positional schemata of, amongst others, Didrichsen."
      ]
    },
    {
      "heading": "3.2 Crossing Dependency Links",
      "text": [
        "To accommodate long-distance dependencies, we allow a dependent to be realized adjacent to the label of any node that dominates it, immediately or not.",
        "For example, consider the dependency tree of Figure 4 for the sentence What beans did Mary say that John ate?",
        "as encoded in Figure 5.",
        "Here, What beans is a dependent of that ate, which in turn is a dependent of did say, and What beans is realized between did and say.",
        "This phenomenon is called movement in conjunction with phrase-structure grammars.",
        "It makes the dependency grammar non-projective, since it creates crossing dependency links if the dependency trees also depict the word order.",
        "We introduce variables M(0) that randomly select from C(0) a subbag Cm(0) of strings passed up to 4's regent:",
        "say that John ate?",
        "The rest of the strings, Cs (c6), are realized here:"
      ]
    },
    {
      "heading": "3.3 Discontinuous Nuclei",
      "text": [
        "We generalize the scheme to discontinuous nuclei by allowing 8(0) to insert the strings of Cs (0) anywhere in 1(0): 2",
        "This means that strings can only be inserted into ancestor labels, not into other strings, which enforces a type of reverse island constraint.",
        "Note how in Figure 6 John is inserted between that and ate to form the subordinate clause that John ate.",
        "We define the string-realization probability",
        "and again define the tree-string probability",
        "Tesniere's original implicit definition of a nucleus actually does not require that the order be preserved when realizing it; if has eaten is a nucleus, so is eaten has.",
        "This is obviously a useful feature for modeling verb chains in German subordinate clauses.",
        "To avoid derivational ambiguity when generating a tree-string pair, i.e., have more than one derivation generate the same tree-string pair, we require that no string be realized adjacent to the string of any node it wa.s passed up through.",
        "This introduces the practical problem of ensuring that zero probability mass is assigned to all derivations violating this constraint.",
        "Otherwise, the result will be approximating the parse probability with a derivation probability, as described in detail in (Samuelsson, 2000) based.",
        "on the seminal work of (Sima'an, 1.996).",
        "Schemes like (Alshawi, 1996) tacitly make this approximation.",
        "The tree-structure variables and .T are generated just as before.",
        "We then generate the string variables S and A4 using a bottom-up stochastic process, where A4(0) is generated before 8(0).",
        "The probability of the conditioning material of PA4(0) is then known either from the top-down process or from l',1/44(0j) and Ps(N), and that of Ps(0) is known either from the top-down process, or from PA4(0), P.A4(0j) and Ps(0j).",
        "The coherence of S(0) and.",
        "M(0) is enforced by explicit conditioning.",
        "Figure 5 shows a top-down process generating the dependency tree of Figure 4; the columns ,C and .T should be read downwards in parallel, ,C before _T.",
        "Figure 6 shows a bottom-up process generating the string What beans did Mary say that John ate?",
        "from the dependency description of Figure 5.",
        "The columns A4 and S should be read upwards in parallel, .A4 before S."
      ]
    },
    {
      "heading": "3.4 String Merging",
      "text": [
        "We have increased the expressive power of our dependency grammars by modifying the S variables, i.e., by extending the adjoin operation.",
        "In the first version, the adjoin operation randomly permutes the node label and the strings of the child nodes, and concatenates the result.",
        "In the second version, it randomly inserts the strings of the child nodes, and any moved strings to be realized at the current node, into the node label.",
        "The adjoin operation can be further refined to allow handling an even wider range of phenomena, such as negation in.",
        "French.",
        "Here, the dependent string is merged with the label of the regent, as 71,C .. .",
        "pas is wrapped around portions of the verb phrase, e.g., Ne me quitte pas!, see (13rel, 1959).",
        "Figure 7 shows a dependency tree for this.",
        "in addition to this, the node labels may be linguistic abstractions, e.g. \"negation\", calling on the S variables also for their surface-string realization.",
        "Note that the expressive power of the grammar depends on the possible distributions of the string probabilities Ps.",
        "Since each node label can be moved and realized at the root node, any language can be recognized to which the string probabilities allow assigning the entire probablity mass, and the grammar will possess at least this expressive power."
      ]
    },
    {
      "heading": "4 A Computational Rendering",
      "text": [
        "A close approximation of the described stochastic model of dependency syntax has been realized as a type of probabilistic bottom-up chart parser."
      ]
    },
    {
      "heading": "4.1 Model Specialization",
      "text": [
        "The following modifications, which are really just specializations, were made to the proposed model for efficiency reasons and to cope with sparse data.",
        "According to Tesniere, a nucleus is a unit that contains both the syntactic and semantic head and that does not exhibit any internal syntactic structure.",
        "We take the view that a nucleus consists of a content word, i.e., an open-class word, and all function words adding information to it that could just as well have been realized morphologically.",
        "For example, the definite article associates definiteness with a word, which could just has well have been manifested in the word form, as it is done in North-Germanic languages; a preposition could be realized as a locational or temporal inflection, as is done in Finnish.",
        "The longest nuclei we currently allow are verb chains of the form that have been eaten, as in John knows that the beans have been eaten.",
        "The variables were decomposed into generating the set of obligatory arguments, i.e., the valency or lexical complement, at once, as in the original model.",
        "Optional modifiers (adjuncts) are attached through one memoryless process for each modifier type, resulting in geometric distributions for these.",
        "This is the same separation of arguments and adjuncts as that employed by (Collins, 1.997).",
        "However, the E variables remained as described above, thus leaving the lexical collocation statistics intact.",
        "The movement probability was divided into three parts: the probability of moving the string of a particular argument dependent from its regent, that of a moved dependency type passing through a particular other dependency type, and that of a dependency type landing beneath a particular other dependency type.",
        "The one type of movement that is not yet properly handled is assigning arguments and adjuncts to dislocated heads, as in What book did John read by Chomsky?",
        "The string-realization probability is a straightforward generalization of that given at the end of Section 3.1, and they are defined through regular expressions.",
        "Basically, each unmoved dependent string, each moved string landed at the cur-?le",
        "and Did John eat xxx?",
        "rent node, and each token of the nucleus labeling the current node are treated a.s units that are randomly permuted.",
        "Whenever possible, strings are generalized to their dependency types, but accurately modelling dependent order in French requires inspecting the actual strings of dependent clitics.",
        "Open-class words are typically generalized to their word class.",
        "String merging only applies to a small class of nuclei, where we treat the individual tokens of the dependent string, which is typically its label, a.s separate units when performing the permutation."
      ]
    },
    {
      "heading": "4.2 The Chart Parser",
      "text": [
        "The parsing algorithm, which draws on the Cocke-Kasami-Younger (CKY) algorithm, see (Younger, 1967), is formulated as a probabilistic deduction scheme, which in turn is realized as an agenda-driven chart-parser.",
        "The top-level control is similar to that of (Pereira and Shieber, 1987), pp.",
        "196-210.",
        "The parser is implemented in Prolog, and it relies heavily on using set and bag operations as primitives, utilizing and extending existing SICStus libraries.",
        "The parser first non deterministically segments the input string into nuclei, using a. lexicon, and each possible nucleus spawns edges for the initial chart.",
        "Due to discontinuous nuclei, each edge spans not a single pair of string positions, indicating its start and end position, but a set of such string-position pairs, and we call this set an index.",
        "If the index is a singleton set, then it is continuous.",
        "We extend the notion of adjacent indices to be any two non-overlapping indices where one has a start position that equals an end position of the other.",
        "The lexicon contains information about the roles (dependency types linking it to its regent) and valencies (sets' of types of argument dependents) that are possible for each nucleus.",
        "These are hard constraints.",
        "Unknown words arc included in nuclei in a judicious way and the resulting nuclei are assigned all reasonable role/valency pairs in the lexicon.",
        "For example, the parser \"correctly\" analyzes the sentences Did John xxx beans?",
        "and Did John eat xxx?",
        "as shown in Figure 8, where xxx is not in the lexicon.",
        "For each edge added to the initial chart, the lexicon predicts a single valency, but a set of alternative roles.",
        "Edges are added to cover all possible valen-3Due to the uniqueness principle of arguments, these are sets, rather than bags.",
        "cies for each nucleus.",
        "The roles correspond to the \"goal\" of dotted items used in traditional chart parsing, and the unfilled valency slots play the part of the \"body\", i.e., the portion of the RIIS following the dot that remains to be found.",
        "if an argument is attached to the edge, the corresponding valency slot is filled in the resulting new edge; no argument can be attached to an edge unless there is a corresponding unfilled valency slot fin; it, or it is licensed by a. moved argument.",
        "For obvious reasons, the lexicon cannot predict all possible combinations of adjuncts for each nucleus, and in fact predicts none at all.",
        "There will in general be multiple derivations of any edge with more than one dependent, but the parser avoids adding duplicate edges to the chart in the same way as a traditional chart parser does.",
        "The parser employs a packed parse forest (PPF) to represent the set of all possible analyses and the probability of each analysis is recoverable from the PPF entries.",
        "Since optional modifiers are not predicted by the lexicon, the chart does not contain any edges that correspond directly to passive edges in traditional chart parsing; at any point, an adjunct can always be added to an existing edge to form a new edge.",
        "In some sense, though, the P PI' nodes play the role of passive edges, since the parser never attempts to combine two edges, only one edge and one PPF node, and the latter will always be a dependent of the former, directly, or indirectly through the lists of moved dependents.",
        "The edge and PPF node to be combined are required to have adjacent indices, and their union is the index of the new edge.",
        "The main point in using a packed parse forest is to perform local ambiguity packing, which means abstracting over differences in internal structure that do not matter for further parsing.",
        "When attching a PPF node to some edge as a direct or indirect dependent, the only relevant features are its index, its nucleus, its role and its moved dependents.",
        "Other features necessary for recovering the complete analysis are recorded in the PPF entries of the node, but are not used for parsing.",
        "To indicate the alternative that no more dependents are added to an edge, it is converted into a set of PPF updates, where each alternative role of the edge adds or updates one PPF entry.",
        "When doing this, any unfilled valency slots are added to the edge's set of moved arguments, which in turn is inherited by the resulting PM update.",
        "The edges are actually not assigned probabilities, since they contain enough information to derive the appropriate probabilities once they are converted into PPP entries.",
        "avoid the combinatorial explosion of unrestricted string merging, we only allow edges with continuous indices to be converted into PP entries, with the exception of a very limited class of lexically signaled nuclei, such as the ne pas, ne jamais, etc., scheme of French negation."
      ]
    },
    {
      "heading": "4.3 Pruning",
      "text": [
        "As opposed to traditional chart parsing, meaningful upper and lower bounds of the supply and demand for the dependency types of the \"goal\" (roles) and \"body\" (valency) of' each edge can be determined from the initial chart, which allows performing sophisticated pruning.",
        "The basic idea is that if some edge is proposed with a role that is not sought outside its index, this role can safely be removed.",
        "For example, the word me could potentially be an indirect object, but if there is no other word in the input string that can have an indirect object as an argument, this alternative can be discarded.",
        "This idea is generalized to a variant of pigeonhole reasoning, in the vein of If we select this role or edge, then there are by necessity too few or too many of some dependency type sought or offered in the chart.",
        "or alternatively if we select this nucleus or edge, then we cannot span the entire input string.",
        "Pruning is currently only applied to the initial chart to remove logically impossible alternatives and used to filter out impossible edges produced in the prediction step.",
        "Nonetheless, it reduces parsing times by an order of magnitude or more for many of the test examples.",
        "It would however be Possible to apply similar ideas to intermittently remove alternatives that are known to be suboptimal, or to heuristically prune unlikely search branches."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "We have proposed a generative, statistical theory of dependency syntax, based on Tesniere's classical theory, that models crossing dependency links, discontinuous nuclei and string merging.",
        "The key insight was to separate the tree-generation and string-realization processes.",
        "The model has been realized as a type of probabilistic chart parser.",
        "The only other high-fidelity computational rendering of Tesniere's dependency syntax that we are aware of is that of (Tapanainen and Jarvinen, 1997), which is neither generative nor statistical.",
        "The stochastic model generating dependency trees is very similar to other statistical dependency models, e.g., to that of (A.lshawi, 1996).",
        "Formulating it using Gorn's notation and the E and .T variables, though, is concise, elegant and novel.",
        "Nothing prevents conditioning the random variables on arbitrary portions of the partial tree generated this far, using, e.g., maximum-entropy or decision-tree models to extract relevant features of it; there is no difference",
        "in principle between our model and history-based parsing, see (Black et al., 1993; Magerman, 1995).",
        "The proposed treatment of string realization through the use of the S and M variables is also both truly novel and important.",
        "While phrase-structure grammars overemphasize word order by making the processes generating the S variables deterministic, Tesniere treats string realization as a secondary issue.",
        "We find a middle ground by using stochastic processes to generate the S and M variables, thus reinstating word order as a parameter of equal importance as, say, lexical collocation statistics.",
        "It is however not elevated to the hard-constraint status it enjoys in phrase-structure grammars.",
        "Due to the subordinate role of string realization in classical dependency grammar, the technical problems related to incorporating movement into the string-realization process have not been investigated in great detail.",
        "Our use of the M variables is motivated partly by practical considerations, and partly by linguistic ones.",
        "The former in the sense that this allows designing efficient parsing algorithms for handling also crossing dependency links.",
        "The latter as this gives us a quantitative handle on the empirically observed resistance against crossing dependency links.",
        "As Tesniere points out, there is locality in string realization in the sense that dependents tend to be realized adjacent to their regents.",
        "This fact is reflected by the model parameters, which also model, probabilistically, barrier constraints, constraints on landing sites, etc.",
        "It is noteworthy that treating movement as in GPSG, with the use of the \"slash\" feature, see (Gazdar et al., 1985), pp.",
        "137-.168, or as is done in (Collins, 1997), is the converse of that proposed here for dependency grammars: the former pass constituents down the tree, the .A4 variables pass strings up the tree.",
        "The relationship between the proposed stochastic model of dependency syntax and a number of other prominent stochastic grammars is explored in detail in (Samuelsson, 2000)."
      ]
    }
  ]
}
