{
  "info": {
    "authors": [
      "Manabu Sassano",
      "Takehito Utsuro"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-2102",
    "title": "Named Entity Chunking Techniques in Supervised Learning for Japanese Named Entity Recognition",
    "url": "https://aclweb.org/anthology/C00-2102",
    "year": 2000
  },
  "references": [
    "acl-P94-1013",
    "acl-W95-0107",
    "acl-W98-1120",
    "acl-W99-0612",
    "acl-W99-0613",
    "acl-X96-1050"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper focuses on the issue of named entity chunking in Japanese named entity recognition.",
        "We apply the supervised decision list learning method to Japanese named entity recognition.",
        "We also investigate and incorporate several pained-entity noun phrase chunking techniques and experimentally evaluate and compare their performance.",
        "In addition, we propose a method for incorporating richer contextual information as well as patterns of constituent morphemes within a named entity, winch have not been considered in previous research, and show that the proposed method outperforms these previous approaches."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "R is widely agreed that named entity recognition is an important step for various applications of natural language processing such as information retrieval, madiine translation, information extraction and natural language understanding.",
        "In the English language, the task of named entity recognition is one of the tasks of the Message Understanding Conference (MUC) (e.g., MUC-7 (1998)) and has been studied intensively.",
        "In time Japanese language, several recent; conferences, such as MET (Multilingual Entity Task, MET-1 (Maiorano, 1996) and MET-2 (MUC, 1998)) and IBEX.",
        "(Information Retrieval and Extraction Exercise) Workshop (IBEX Committee, 1999), focused on named entity recognition as one of their contest tasks, thus promoting research on Japanese named entity recognition.",
        "In Japanese named entity recognition, it is quite common to apply morphological analysis as a preprocessing step and to segment the sentence string into a sequence of morphemes.",
        "Then, hand-crafted pattern matching rules and/or statistical named entity recognizer are applied to recognize named entities.",
        "It is often the case that named entities to be recognized have different segmentation boundaries from those of morphemes obtained by the morphological analysis.",
        "For example, in our analysis of the IREX workshop's training corpus of named entities, about half of the named entities have segmentation boundaries that are differmi; from the result of morphological analysis by a Japanese morphological analyzer BREAKFAST (Sassano et al., 1.997) (section 2).",
        "Thus, in Japanese named entity recognition, among the most difficult problems is how to recognize such named entities that have segmentation boundary mismatch against; the morphemes obtained by morphological analysis.",
        "Furthermore, in almost 90% of cases of those segmentation boundary mismatches, named entities to be recognized can be decomposed into several morphemes as their constituents.",
        "Tins means that the problem of recognizing named entities in those cases can be solved by incorporating techniques of base noun phrase chunking (Ramshaw and Marcus, 1995).",
        "In this paper, we focus on the issue of named entity chunking in Japanese named entity recognition.",
        "First, we take a supervised learning approach rather than a hand-crafted rule based approach, because the former is more promising than the latter with respect; to the amount; of human labor if requires, as well as its adaptability to a new domain or a new definition of named entities.",
        "In general, creating training data for supervised learning is -somewhat; easier than creating pattern matching rules by hand.",
        "Next, we apply Yarowsky's method for supervised decision list; learning' (Yarowsky, 1994) to",
        "Japanese named entity recognition, into which we incorporate several noun phrase chunking techniques (sections 3 and 4) and experimentally evaluate their performance on the IREX • workshop's training and test data (section 5).",
        "As one of those noun phrase chunking techniques, we propose a method for incorporating richer contextual information as well as patterns of constituent morphemes within a named entity, compared with those considered in the previous research (Sekine et al., 1998; Borthwick, 1999), and show that the proposed method outperforms these approaches."
      ]
    },
    {
      "heading": "2 Japanese Named Entity Recognition",
      "text": []
    },
    {
      "heading": "2.1 Task of the IREX Workshop",
      "text": [
        "The task of named entity recognition of the IREX workshop is to recognize eight named entity types in Table 1 (IREX Committee, 1999).",
        "The organizer of the IREX workshop provided 1,174 newspaper articles which include 18,677 named entities as the training data.",
        "In the formal run (general domain) of the workshop, the participating systems were requested to recognize 1,510 named entities included in the held-out 71 newspaper articles."
      ]
    },
    {
      "heading": "2.2 Segmentation Boundaries of Morphemes and Named Entities",
      "text": [
        "In the work presented here, we compare the segmentation boundaries of named entities in the IBEX workshop's training corpus with those of supervised learning technique mainly because it is easy to implement and quite straightforward to extend a supervised learning version to a minimally supervised version (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999).",
        "We also reported in (Utsuro and Sassano, 2000) the experimental results of a minimally supervised version of Japanese named entity recognition.",
        "morphemes which were obtained through morphological analysis by a Japanese morphological analyzer BREAKFAST (Sassano et al., 1997).2 Detailed statistics of the comparison are provided in Table 2.",
        "Nearly half of the named entities have boundary mismatches against the morphemes and also almost 90% of the named entities with boundary mismatches can be decomposed into more than one morpheme.",
        "Figure 1 shows some examples of such cases.3"
      ]
    },
    {
      "heading": "3 Chunking and Tagging Named Entities",
      "text": [
        "In this section, we formalize the problem of named entity chunking in Japanese named entity recognition.",
        "We describe a novel technique as well as those proposed in the previous works on named entity recognition.",
        "The novel technique incorporates richer contextual information as well as patterns of constituent morphemes within a named entity, compared with the techniques proposed in previous research on named entity recognition and base noun phrase chunking."
      ]
    },
    {
      "heading": "3.1 Task Definition",
      "text": [
        "First, we will provide our definition of the task of .Japanese named entity chunking.",
        "Suppose 2 The set of part-of-speech tags of BREAKFAST consists of about 300 tags.",
        "BREAKFAST achieves 99.6% part-of-speech accuracy against newspaper articles.",
        "3 In most cases of the \"other boundary mismatch\" in Table 2, one or more named entities have to be recognized as a part of a correctly analyzed morpheme and those cases are not caused by errors of morphological analysis.",
        "One frequent example of this type is a Japanese verbal noun \"hou-bei (visiting United States)\" which consists of two characters \"hou (visiting)\" and \"bei (United States)\", where \"bei ( United States)\" has to be recognized as <LOCATION>.",
        "We believe that boundary mismatches of this type can be easily solved by employing a supervised learning technique such as the decision list learning method.",
        "Then, given that the current position is at the morpheme MiNi'), the task of named entity chunking is to assign a chunking state (to be described in Section 3.2) as well as a named entity type to the morpheme MNE at the current; position, considering the patterns of surrounding morphemes.",
        "Note that in the supervised learning phase we can use the chunking information on which morphemes constitute a named entity, and winch morphemes are in the left/right contexts of the named entity."
      ]
    },
    {
      "heading": "3.2 Encoding Schemes of Named Entity Chunking States",
      "text": [
        "In this paper, we evaluate the following two schemes of encoding chunking states of named entities.",
        "Examples of these encoding schemes are shown in Table 3."
      ]
    },
    {
      "heading": "3.2.1 Inside/Outside Encoding",
      "text": [
        "The Inside/Outside scheme of encoding chunking states of base noun phrases was studied in Ramshaw and Marcus (1995).",
        "This scheme distinguishes the following three states: 0 – the word at the current position is outside any base noun phrase.",
        "I – the word at the current position is inside some base 1101111 phrase.",
        "B – the word at the current position marks the beginning of a base noun phrase that immediately follows another base noun phrase.",
        "We extend this scheme to named entity chunking by further distinguishing each of the states I and B into eight named entity types.4 Thus, this scheme distinguishes 2 x 8 + = 17 states."
      ]
    },
    {
      "heading": "3.2.2 Start/End Encoding",
      "text": [
        "The Start/End scheme of encoding chunking states of named entities was employed in Sekine et al.",
        "(1998) and Borthwick (1999).",
        "This scheme distinguishes the following four states for each named entity type: S – the morpheme at the current position marks the beginning of a named entity consisting of more than one morpheme.",
        "C the morpheme at the current position marks the middle of a named entity consisting of more than one morpheme.",
        "E the morpheme at the current; position marks the ending of a named entity consisting of more than one morpheme.",
        "U – the morpheme at the current position is a named entity consisting of only one morpheme.",
        "The scheme also considers one additional state for the position outside any named entity: 0 – the morpheme at the current position is outside any named entity.",
        "Thus, in our setting, this scheme distinguishes 4 x 8 + 1 = 33 states."
      ]
    },
    {
      "heading": "3.3 Preceding/Subsequent Morphemes as Contextual Clues",
      "text": [
        "In this paper, we evaluate the following two models of considering preceding/subsequent",
        "morphemes as contextual clues to named entity chunking/tagging.",
        "Here we provide a basic outline of these models, and the details of how to incorporate them into the decision list learning framework will be described in Section 4.2.2.",
        "In this paper, we refer to the model used in Sekine et al.",
        "(1998) and Borthwick (1999) as a 3-gram model.",
        "Suppose that the current position is at the morpheme Mo, as illustrated below.",
        "Then, when assigning a chunking state as well as a named entity type to the morpheme Mo, the 3-gram model considers the preceding single morpheme /14.4 as well as the subsequent single morpheme M1 as the contextual clue.",
        "The major disadvantage of the 3-grain model is that in the training phase it does not take into account whether or not the preceding/subsequent morphemes constitute one named entity together with the morpheme at the current position."
      ]
    },
    {
      "heading": "3.3.2 Variable Length Model",
      "text": [
        "In order to overcome this disadvantage of the 3 gram model, we propose a novel model, namely the \"Variable Length Model\", which incorporates richer contextual information as well as patterns of constituent morphemes within a named entity.",
        "In principle, as part of the training phase this model considers which of the preceding/subsequent morphemes constitute one named entity together with the morpheme at the current; position.",
        "It also considers several morphemes in the left/right contexts of the named entity.",
        "Here we restrict this model to explicitly considering the cases of named entities of the length up to three morphemes and only implicitly considering those longer than three morphemes.",
        "We also restrict it to considering two morphemes in both left and right; contexts of the named entity.",
        "This section describes how to apply the decision list learning method to chunking/tagging named entities."
      ]
    },
    {
      "heading": "4.1 Decision List Learning",
      "text": [
        "A decision list (divest, 1987; Yarowsky, 1994) is a sorted list of decision rules, each of which decides the value of a decision D given some evidence E. Each decision rule in a decision list is sorted in descending order with respect to some preference value, and rules with higher preference values are applied first when applying the decision list to some new test; data.",
        "First, the random variable D representing a decision varies over several possible values, and the random variable E representing some evidence varies over '1' and '0' (where '1' denotes the presence of the corresponding piece of evidence, '0' its absence).",
        "Then, given some training data in which the correct; value of the decision D is annotated to each instance, the conditional probabilities P(D = x I E = 1) of observing the decision D =x under the condition of the presence of the evidence E (E = 1) are calculated and the decision list is constructed by the following procedure.",
        "1.",
        "For each piece of evidence, we calculate the log of likelihood ratio of the largest; conditional probability of the decision D = (given the presence of that piece of evidence) to the second largest conditional probability of the decision D = x2:",
        "Then, a decision list is constructed with pieces of evidence sorted in descending order with respect to their log of likelihood ratios, where the decision of the rule at each line is D = x1 with the largest conditional probability.5 ''Yarowsky (1994) discusses several techniques for avoiding the problems which arise when an observed count is 0.",
        "From among those techniques, we employ the simplest one, i.e., adding a small constant a (0.1 < cr < 0.25) to the numerator and denominator.",
        "With this modification, more frequent evidence is preferred when several evidence candidates exist with the same",
        "2.",
        "The final line of a decision list is defined as `a default', where the log of likelihood ratio is calculated from the ratio of the largest marginal probability of the decision D=34 to the second largest marginal probability of the decision D =x2:",
        "The 'default' decision of this final line is D =x1 with the largest marginal probability."
      ]
    },
    {
      "heading": "4.2 Decision List Learning for Chunking/Tagging Named Entities 4.2.1 Decision",
      "text": [
        "For each of the two schemes of encoding chunking states of named entities described in Section 3.2, as the possible values of the decision D, we consider exactly the same categories of chunking states as those described in Section 3.2."
      ]
    },
    {
      "heading": "4.2.2 Evidence",
      "text": [
        "The evidence E used in the decision list learning is a combination of the features of preceding/subsequent morphemes as well as the morpheme at the current position.",
        "The following describes how to form the evidence E for both the 3-gram model and variable length model.",
        "3-gram Model The evidence E represents a tuple Fo, F1), where F_1 and K denote the features of immediately preceding/subsequent morphemes and respectively, F0 the feature of the morpheme Mo at the current position (see Formula (1) in Section 3.3.1).",
        "The definition of the possible values of those features KA, F0, and Ft are given below, where Mi denotes the morpheme itself (i.e., including its lexical form as well as part-of-speech), Ci the character type (i.e., Japanese (hiragana or katakana), Chinese (kanji), numbers, English alphabets, symbols, and all possible combinations of these) of Ti the part-of-speech of",
        "unsmoothed conditional probability P(D x I F = 1).",
        "Yarowsky's training algorithm also differs somewhat in his use of the ratio P(D=xi1E-J) which is equivalent in the case of binary classifications, and also by the interpolation between the global probabilities (used here) and the residual probabilities further conditional on higher-ranked patterns failing to match in the list.",
        "::= MI_ I (CI, TO T1 I null F0 ::= 1170 I (Co I To As the evidence E, we consider each possible combination of the values of those three features."
      ]
    },
    {
      "heading": "Variable Length Model",
      "text": [
        "The evidence E represents a tuple (FL, FNE, Fl?.",
        "), where FL and FR, denote the features of the morphemes ML2ML1 and MIW in the left/right contexts of the current named entity, respectively, F FNE the features",
        "• • • miN E m• • • NE",
        "of the morphemes WE – m,(<3) constituting the current named entity (see Formula (2) in Section 3.3.2).",
        "The definition of the possible values of those features FL, FATE, and FR are given below, where Fp denotes the feature of the j-th constituent morpheme 11/11f L within the current named entity, and",
        "As the evidence E, we consider each possible combination of the values of those three features, except that the following three restrictions are applied.",
        "1.",
        "In the cases where the current named entity consists of up to three morphemes, as the possible values of the feature FNE in the definition (3), we consider only those which are consistent with the requirement that each morpheme M3E is a constituent of the current named entity.",
        "For example, suppose that the current named entity consists of three morphemes, where the current position is at the middle of those constituent morphemes as below:"
      ]
    },
    {
      "heading": "(Current Position)",
      "text": [
        "Then, as the possible values of the feature FNE, we consider only the following four:",
        "2.",
        "In the cases where the current named entity consists of more than three morphemes, only the three constituent morphemes are regarded as within the current named entity and the rest are treated as if they were outside the named entity.",
        "For example, suppose that the current named entity consists of four morphemes as below: In this case, pheme right context as below:",
        "3.",
        "As the evidence E, among the possible combination of the values of three features FL, ENE, and EH, we only accept those in which the positions of the morphemes are continuous, and reject those discontinuous combinations.",
        "For example, in the case of Formula (4) above, as the evidence E, we accept the combination (MA 1, MPTE r, null), while we reject"
      ]
    },
    {
      "heading": "4.3 Procedures for Training and Testing",
      "text": [
        "Next we will briefly describe the entire processes of learning the decision list for chunking/tagging named entities as well as applying it to chunking/tagging unseen named entities."
      ]
    },
    {
      "heading": "4.3.1 Training",
      "text": [
        "In the training phase, at the positions where the corresponding morpheme is a constituent of a named entity, as described in Section 4.2, each allowable combination of features is considered as the evidence E. On the other hand, at the positions where the corresponding morpheme is outside any named entity, the way the combination of features is considered is different in the variable length model, in that the exception 1 in the previous section is no longer applied.",
        "Therefore, all the possible values of the feature FNE in Definition (3) are accepted.",
        "Finally, the frequency of each decision D and evidence E is counted and the decision list is learned as described ill Section 4.1."
      ]
    },
    {
      "heading": "4.3.2 Testing",
      "text": [
        "When applying the decision list to chunking/tagging unseen named entities, first, at each morpheme position, the combination of features is considered as in the case of the nonentity position in the training phase.",
        "Then, the decision list is consulted and all the decisions of the rules with a log of likelihood ratio above a certain threshold are recorded.",
        "Finally, as in the case of previous research (Sekine et al., 1998; Borthwick, 1999), the most appropriate sequence of the decisions that are consistent throughout the whole sequence is searched for.",
        "By consistency of the decisions, we mean requirements such as that the decision representing the beginning of some named entity type has to he followed by that representing the middle of the same entity type (iii the case of Start/End encoding).",
        "Also, in our case, the appropriateness of the sequence of the decisions is measured by the sum of the log of likelihood ratios of the corresponding decision rules."
      ]
    },
    {
      "heading": "5 Experimental Evaluation",
      "text": [
        "We experimentally evaluate the performance of the supervised learning for Japanese named entity recognition on time IREX workshop's training and test data.",
        "We compare the results of the combinations of the two encoding schemes of named entity chunking states (the Inside/Outside and the Start/End encoding schemes) and the two approaches to contextual feature design (the 3-gram and the Variable Length models).",
        "For each of those combinations, we search for an optimal threshold of the log of likelihood ratio in the decision list.",
        "The performance of each combination measured by F-measure 1) is given in Table 4.",
        "In this evaluation, we exclude the named entities with \"other boundary mismatch\" in Table • 2.",
        "We also classify the system output according to the number of constituent morphemes of each named entity and evaluate the performance for each subset of the system output.",
        "For each sub",
        "set, we compare the performance of the four combinations of {3-gram, Variable Length} x {Inside/Outside, Start/End} and show the highest performance with boldfaced font.",
        "Several remarkable points of these results of performance comparison can be stated as below:",
        "• Among the four combinations, the Variable",
        "• In the recognition of named entities consisting of more than two morphemes (1 =",
        "3, n > 3, 1), the Variable Length Model performs significantly better than the 3 grain model.",
        "This result clearly supports Ow claim that our modeling of the Variable Length Model has an advantage in the recognition of long named entities.",
        "• in general, the Inside/Outside encoding scheme performs slightly better than the Start/End encoding scheme, even though the funnel: distinguishes considerably fewer states than time latter."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this paper, we applied the supervised decision list learning method to Japanese named entity recognition, into which we incorporated several noun phrase chunking techniques and experimentally evaluated their performance.",
        "We showed that a novel technique that we proposed outperformed those using previously considered contextual features."
      ]
    },
    {
      "heading": "7 Acknowledgments",
      "text": [
        "This research was carried out while the authors were visiting scholars at Department of Computer Science, johns Hopkins University.",
        "The authors would like to thank Prof. David Yarowsky of Johns Hopkins University for invaluable supports to this research."
      ]
    }
  ]
}
