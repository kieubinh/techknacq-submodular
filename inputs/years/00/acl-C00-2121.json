{
  "info": {
    "authors": [
      "Aristomenis Thanopoulos",
      "Nikos D. Fakotakis",
      "George K. Kokkinakis"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-2121",
    "title": "Automatic Extraction of Semantic Relations from Specialized Corpora",
    "url": "https://aclweb.org/anthology/C00-2121",
    "year": 2000
  },
  "references": [
    "acl-J92-4003",
    "acl-J93-1007",
    "acl-J96-2003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper we address the problem of discovering word semantic similarities via statistical processing of text corpora.",
        "We propose a knowledge-poor method that exploits the sentencial context of words for extracting similarity relations between them as well as semantic in nature word clusters.",
        "The approach aims at full portability across domains and languages and therefore is based on minimal resources."
      ]
    },
    {
      "heading": "1 Motivation",
      "text": [
        "Providing digital computers with the capability to acquire conceptual relations between lexical items by processing real-life text corpora is not only an exciting research activity but also a significant task in the framework of many NLP systems.",
        "Specifically:"
      ]
    },
    {
      "heading": "1. State-of-the-art Language Modeling techniques",
      "text": [
        "(McMahon and Smith., 1996) require lexical information about word classes.",
        "2.",
        "Thesauri creation in a (semi-) automatic manner in any domain and language with minimal dependence on specialized tools and resources is very important.",
        "Most thematic domains today in most of the languages lack semantic resources.",
        "Adopting a knowledge-poor corpus-based method not only much less labor is necessary in construction of conceptual structures but also domain-dependent semantic relations are obtained.",
        "New resources can be readily created in new domains or existing thesauri can be enlarged or refined by retraining on larger corpora as soon as they become available.",
        "3.",
        "Many currently implemented, both spoken and written, NLP systems operate in a specific domain and usually utilize a constrained vocabulary related directly to their task domain.",
        "Therefore semantic domain-dependent knowledge can be acquired directly from relevant corpora.",
        "4.",
        "Autonomous computational intelligence should rely mainly on processing of free flow electronic texts for acquiring new semantic and world knowledge.",
        "The present approach aims at corpus-based automatic extraction of domain-dependent semantic similarity relations between lexical items and the formation of corresponding semantic clusters.",
        "For this purpose, the usage of readily available domain-specific text corpora is imperative.",
        "The guideline of our approach was the adaptation to the special characteristics of this type of corpora (specialization, restricted size) without imposing the need for other domain-dependent resources and obtaining portability across languages."
      ]
    },
    {
      "heading": "2 Related work",
      "text": [
        "Three main approaches have been proposed for the automatic extraction of lexical semantics knowledge: syntax-based, n-gram-based and window-based.",
        "Syntax-based methods (referred also as knowledge-rich in contrast to the others - knowledge-poor methods) (Pereira and Thishby, 1992; Grefenstette, 1993; Li and Abe, 1997) represent the words under consideration as vectors containing statistic values of their syntactic properties in relation to a given set of words (e.g. statistics of object syntax relations referring to a set of verbs) and cluster the considered words according to similarity of the corresponding vectors.",
        "Methods that use bigrams (Brown et al., 1992) or trigrams (Martin et al., 1998) cluster words considering as a word's context the one or two immediately adjacent words and employ as clustering criteria the minimal loss of average",
        "mutual information and the perplexity improvement respectively.",
        "Such methods are oriented to language modeling and aim primarily at rough but fast clustering of large vocabularies.",
        "Brown et al.",
        "(1992) also proposed a window method introducing the concept of \"semantic stickiness\" of two words as the relatively frequent close occurrence between them (less than 500 words distance).",
        "Although this is an efficient and entirely knowledge-poor method for extracting both semantic relations and clusters, the extracted relations are not restricted to semantic similarity but extend on thematic roles.",
        "Moreover its applicability to small and specialized corpora is uncertain."
      ]
    },
    {
      "heading": "3 A knowledge-poor approach",
      "text": [
        "In order to achieve portability we approach the issue from a knowledge-poor perspective.",
        "Syntax-based methods employ partial parsers which require highly language-dependent resources (morphological/grammatical analysis), and/or properly tagged training corpus in order to detect syntactic relations between sentence constituents.",
        "On the other hand, n-gram methods operate on large corpora and, in order to reduce computational resources, consider as context words only the immediately adjacent ones.",
        "Medium-distance word context is not exploited.",
        "Since large corpora are available only for few domains we aimed at developing a method for processing small or medium sized corpora exploiting the most of contextual information, that is, the full sentential context of words.",
        "Our approach was driven by the observation that in domain-constrained corpora, unlike fiction or general journalese, the vocabulary is limited, the syntactic structures are not complex and that medium-distance lexical patterns are frequently used to express similar facts.",
        "Specifically we have developed two different algorithms in respect to the context consideration they employ: Word-based and Pattern-based.",
        "The former acquires word-based contextual data (extended up to sentence boundaries), according to the distributional similarity of which, word similarity relations are extracted.",
        "The latter detects common patterns throughout the corpus that indicate possible word similarities.",
        "For example, consider the sentence fragments: \"...while the S&P index inched up 0.3%.\" \"The DAX index inched up 0.70 point to close...\" Although their syntactic structures are different, the common contextual pattern (appearing beyond immediately adjacent words) indicates a possible similarity between the tokens 'S&P' and 'DAX'.",
        "Word pairs that persistently appear such context similarity throughout the corpus (frequently observed in technical texts) are confidently indicated as semantically similar.",
        "Our method captures such context similarity and extracts a proportionate measure about semantic similarity between lexical items.",
        "Most approaches (Brown et al., 1992; Li & Abe, 1997) inherently extract semantic knowledge in the abstracted form of semantic clusters.",
        "Our method produces semantic similarity relations as an intermediate (and information-richer) semantics representation formalism, from which cluster hierarchies can be generated.",
        "Of great importance is that soft clustering methods can also be applied to this set of relations and cluster polysemous words to more than one classes.",
        "Stock market-financial news and Modern Greek, were used as domain and language test case respectively.",
        "However demonstrative examples taken from the WSJ corpus have been used throughout the paper as well.",
        "The main idea supporting context-based word clustering is that two words that can substitute one another in several different contexts always providing meaningful word sequences are probably semantically similar.",
        "Present n-gram based methods utilize this assumption considering as a context of a focus word only the one or two immediately adjacent parameter words.",
        "In the present work, we consider as word context the whole sentence in which the examined word appears, excluding only the semantically empty (i.e. functional) words such as articles, conjunctions, particles, auxiliaries.",
        "Adopting this word context notion we proceed to the following analysis: Let us consider a text corpus Tc with vocabulary Vc and Vs c V( the set of words that are of interest in extracting semantic similarity relations between them.",
        "Vs comprises the non-functional words of",
        "Vc appearing in Tc with a frequency higher than a threshold (set to 20 in the presented experiments) in order to acquire sufficient data for every focus word.",
        "Let Vpc Vc be the set of words that will be used as context parameters.",
        "Ideally, any word appearing at least twice in the corpus could be used as context parameter.",
        "However we specified this word frequency threshold to 10 in order to diminish computational time.",
        "Consider a sentence of Tc: Sm w ,w2,,wi,wi+1, ,wk We define as sentential context of w, in S,,, the set of the pairs of the sentence words which are members of Vp, accompanied by their corresponding distance from w1: C5o, (w ) {(i - j, w ), i = 1..k, (i j), VW~EV,} Equation (1): Sentential context of wi in S, More formally, Cs (wi) can be represented as a binary-valued matrix defined over the set ox co where 13=, L, being the maximum word distance we regard that carries useful contextual information (for full sentence distance Lwhere L,x the maximum sentence length in Tc), and co the ordered set Vs:",
        "Summing over all corpus sentences we obtain the contextual data matrices for every wj E Vs:",
        "The word semantic similarity estimation has been reduced to matrices similarity estimation.",
        "The obtained contextual matrices are compared using a weighted Tanimoto measure (Charniak, 1993) and a word similarity measure Sm(whwj) is obtained:",
        "The weight function h(d) defines the desired influence that the distance between words should have to context similarity estimation.",
        "In this experiment we set: h(d)=1/Ids.",
        "In order to reduce computational time the denominator was set to c (d,w)+IIci(d,w), a modification d wd that has minimal effect on the final result.",
        "Experimental results of this method (Word-based Context Similarity Estimation WCSE), are shown in Table 1.",
        "Note that, since the CT, (w j) matrix is sparse, (1) was used as data storing formula instead of (2), in order to diminish computational cost.",
        "The previously described algorithm is handling all contextual data in a uniform way.",
        "However, study of the results showed that preference should be given to hits derived from many different similar contexts instead of few ones appearing many times.",
        "This would clearly give better results since the latter case may be due to often-used stereotyped expressions or repeated facts.",
        "In order to achieve this we modified (4) to: II[h(d) log2 [ci (d, w) c (d,w)11 smtw,, wi )(5) dNV(Iw Indeed the experimental results of this variation (Variant WCSE VWCSE) show a significant improvement (see Table 1)."
      ]
    },
    {
      "heading": "5 Dynamic pattern detection for",
      "text": [
        "context similarity estimation In the previously described method the notion of word context is based on independent intra-sentential word co-occurrences.",
        "However similarity of contextual patterns is much more reliable word similarity criterion than word-based context similarity.",
        "That is, if the sentential contexts Cs.",
        "(we) and Cs,, Ord have at least two common elements, we count this as a much more confident hit regarding the w1 and wj similarity.",
        "A measure expressing the weight of the common pattern is obtained.",
        "Since the patterns under detection vary across languages and domains we need a method that extracts them dynamically, regardless of the text genre, domain or language.",
        "For this purpose we propose an algorithm that performs a sentence-by-sentence comparison along the corpus.",
        "This comparison is based on the",
        "cross-correlation concept as it is used in digital signal processing.",
        "A sentence can be considered as a digital signal where every semantic token corresponds to a signal sample.",
        "In order to detect words with common contexts every sentence is checked on matching every other one partially (i.e. matching the semantic category of one or more tokens) on every possible relative position between the two sentences.",
        "Wherever common patterns of semantic tokens are found the neighboring respective tokens on the two sentences are stored as candidate semantic relatives.",
        "During this process contextual data are not maintained in memory; instead the detection of a common pattern in both sentences results to the storage of several hits (i.e. candidate similar word pairs) or to the increase of their corresponding similarity measure according to the pattern similarity of their contexts.",
        "Let S, and S be two sentences that undergo the cross-correlation procedure.",
        "If 6,---{dx, x=1..x1, x->1 }, is the set of word distances that satisfy the equality: c1n1(c1,wy)=ci(61,,wy),--- 1 , then the pair (w,,wj) is stored as a hit accompanied by the following context similarity measure: Keeping only the first term we obtain the same result as in the WCSE method with weight function h(d)=1/Ids.",
        "The second term augments the score in proportion to the cohesion and the size of the detected pattern depending on the position of w1 (or, equivalently, wi).",
        "Dividing (6) by the total length of S, and S (i.e. 1-,nm ) we obtain a normalized measure of the cross-correlation of the two sentences: The total similarity measure is obtained from:",
        "applied throughout the corpus.",
        "In order to reduce search time and required memory during the whole process a pruning mechanism is applied at regular time intervals to eliminate word pairs with a relatively very low semantic similarity score.",
        "Dividing (8) by the product of the word probabilities P(w,)-P(wi) we obtain the normalized similarity measure FN(wi,wi).",
        "In order to constrain the degradation of our results due to sparse data regarding less frequent words, we multiply (8) by Pc, a data sufficiency measure function of P(w,) and P(w;), obtaining Fij, a more reliable measure.",
        "Here we employed: where we used P-11,-30/1-ITcl being the size of the corpus.",
        "Finally, sorting the resulting pairs by F-1 and keeping the N-best scoring pairs, we obtain the preponderant semantically related candidates."
      ]
    },
    {
      "heading": "6Preprocessing",
      "text": [
        "In order to apply the above described algorithms some preprocessing is necessary:",
        "1.",
        "A trainable sentence splitter and a rule-based chunker are applied.",
        "Sentence boundaries confine the scope of context while phrase boundaries determine the maximum extent of semantic tokens (see below).",
        "2.",
        "The next step of the preprocessing is what we call \"semantic tokenization\".",
        "We try to reduce",
        "context parameters and simultaneously to incerease the volume of contextual data either by reducing the volume of both the focus and parameter word set or by discaring or merging lexical items resulting in reduction of the distance between semantic tokens.",
        "Words or word sequences are thus classified in common semantic categories employing syntactical, morphological and collocational information: a. Functionals (auxiliaries, determiners) are discarded since they do not modify semantically their head words.",
        "Words of indeterminable semantic content (pronouns, low frequency words) are treated as empty tokens.",
        "expressions (e.g. dates, numbers, amounts, etc.)",
        "are regarded as a single semantic token and tagged accordingly.",
        "Their information content is indifferent to semantic knowledge acquisition; therefore we preserve only class information.",
        "c. Frequently appearing lexical patterns which represent single semantic entities in the specific domain are treated as a single (albeit composite) \"semantic token\".",
        "Their detection is based on the following algorithm (cf. Smadja, 1993): 1.",
        "Extract \"significant bigrams\" confined inside noun phrases i.e. immediately adjacent words that contain a relatively high amount of mutual information: 2.",
        "Combine significant bigrams together to obtain \"significant n-grams\" found in the corpus and confined inside noun phrases as well.",
        "Discard subsumed m-grams (m<n) only if they do not occur indepentently in the corpus.",
        "3.",
        "Tag throughout the corpus the significant n-grams",
        "as single semantic tokens, starting from the higher-order ones.",
        "Semantic entities that are lexically represented as sticky word chains may be either standard in the framework of the information extraction task named entities, such as \"Latin America\" (location), \"Russian president Boris Yeltsin\" (person), \"TpaacCa Maxi;i5oviac-Opaimic\" (\"Bank of Macedonia and Thrace\"; organization) or representations of domain-specific typical events (\"a1 Hall 1.tztox)Koi) KapAction\" = rise of equity capital), abstract concepts (\"Dow Jones industrials\"), etc.",
        "To ensure that the detected \"sticky\" phrases actually represent semantic entities, human inspection is necessary for discarding the spurious ones, since repeated word sequences that do not constitute always single semantic entities often appear in specialized texts.",
        "From the above it is apparent that we use the term \"semantic token\" to refer to a recognized semantic pattern (e.g. <date>), a rigid word chain (e.g. \"Dow Jonesindustrials\") or a single content word.",
        "The context similarity estimation algorithms were run using vocabularies of focus and parameter words derived from the extracted set of semantic tokens."
      ]
    },
    {
      "heading": "7Incorporating heuristics",
      "text": [
        "From the study of the erroneously extracted semantic relations certain systematic errors were detected.",
        "For example, adjectives, adverbs or adjunctive nouns that occur interpolating in otherwise similar contexts lead to the extraction of spurious pairs.",
        "Consider for example the phrases: \"11 m5411,511 -n1; Ty* Tic lisvOvtic\" (= the increase of the benzine price) and \"11 ociti(711 nig Tip* 7rffl211m1c TIN rizvjviic\" (=the increase of the disposal benzine price).",
        "Every algorithm based on word adjacency data outputs as erroneous hits the pairs {benzine-disposal} and { increase-disposal}.",
        "A rule that was applied to deal with this problem is: If wieSm and wjES have similar contexts, count the pair (wi,wi) as a hit only if wiwi,\" and wjwi,.i.",
        "Such contextual rules can be applied only using the cross-correlation method for the context similarity estimation (either pattern-based or word-based)."
      ]
    },
    {
      "heading": "8 Word Clustering",
      "text": [
        "Although the obtained semantically related N-best pair list constitutes already a thesaurus-like and information-rich form of semantic knowledge representation, many NLP applications (e.g. language modeling) require word clusters instead of word relations.",
        "However, since a word similarity measure has been extracted, the formation of clusters is a rather trivial problem, although more complex for \"soft clustering\" (i.e. a word can be classified in more than one classes).",
        "In order to construct word classes we applied the unsupervised agglomerative hard clustering algorithm shown in Figure 1 over the set of semantic relations.",
        "Each distinct lexical item is initially assigned to a cluster and then clusters are merged into larger ones according to the average linkage measure.",
        "Merging of clusters stops when the distance between the more proximate clusters exceeds a threshold proportional to the average distance between words.",
        "Tracking the successive merges we obtain sub-cluster hierarchies, such as the one shown in Figure 2."
      ]
    },
    {
      "heading": "9 Experimental Results",
      "text": [
        "The reported experiments have been carried out on a 220.000 words corpus, comprised of financial news of 1998, which was constructed in the framework of a currently carried out R&D project for Information Extraction from raw text.",
        "The methods and their variations described in sections 4 and 5 for obtaining lexical semantic relations were tested and their accuracy per number of best hits was measured by human inspection.",
        "The VWCSE method was tested using only the previous and next word as context parameters (N&P method), to sketch a method baseline for the particular corpus.",
        "Using a Morphological Analyzer & Part-of-Speech tagger to restrict semantic relations only between words of the same Part-of-Speech (PoS) we obtain apparently higher accuracy, though we loose some interesting verb - noun pairs referring to the same action or condition, e.g. arOjOriKt: (=increased) and avo6oc, (=increment).",
        "The results indicate that the normalization factors indeed improve the accuracy of the methods and that context similarity detection based on dynamic pattern-matching yields significantly more reliable results than the word-based method.",
        "This demonstrates the importance of the cross-correlation algorithm, which is the only suitable for pattern-based context similarity detection.",
        "Regarding the clustering procedure, a set of 1300 words was clustered to 84 hierarchically structured clusters.",
        "Considering an interested cluster formed (Figure 2) we note that from the 18 lexical entities (words or rigid phrases) that constitute the cluster all but two refer to money Project \"MITOS\" of the Greek General Secretariat for"
      ]
    },
    {
      "heading": "Reseach and Technology",
      "text": [
        "investment or profit.",
        "From the vocabulary subject to clustering 4 words belonging to the same class were not detected; therefore accuracy and recall for the specific cluster were found at 88.9% and 80% respectively.",
        "Although comparision with other knowledge-poor methods would be very useful it was not realized, mainly because our method produces semantic relations while other methods produce semantic clusters and our clustering process is not yet elaborated enough to yield quality results."
      ]
    },
    {
      "heading": "10 Conclusion",
      "text": [
        "Initiating from the conception of word similarity estimation in terms of context similarity we have proposed an approach with several variations for extracting semantic similarity relations betweenlexical entities by processing word adjacency data obtained from small or medium sized corpora.",
        "The described cross-correlation procedure, offers the possibility to dynamically detect pattern context similarities offering strong evidence for semantic similarity.",
        "The presented algorithm features language and domain portability and the ability to classify keywords irrespective of their grammatical characteristics.",
        "The implementation of the soft clustering algorithm, the test of the method to a different domain and language and the quantified comparison with other knowledge-poor methods are quite interesting matters belonging to future work."
      ]
    }
  ]
}
