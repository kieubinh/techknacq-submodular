{
  "info": {
    "authors": [
      "Franz Josef Och",
      "Hermann Ney"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-2163",
    "title": "A Comparison of Alignment Models for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/C00-2163",
    "year": 2000
  },
  "references": [
    "acl-C96-2141",
    "acl-J93-2003",
    "acl-W99-0604"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we present and compare various alignment models for statistical machine translation.",
        "We propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a refined annotation scheme to produce suitable reference alignments.",
        "We also compare the impact of different alignment models on the translation quality of a statistical machine translation system."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In statistical machine translation (SMT) it is necessary to model the translation probability PrUi Here = f denotes the (French) source and of = e denotes the (English) target string.",
        "Most SMT models (Brown et al., 1993; Vogel et al., 1996) try to model word-to-word correspondences between source and target words using an alignment; mapping from source position j to target position i = ai.",
        "We can rewrite the probability Pr(fil lei) by introducing the `hidden' alignments all := ...ai...aj (ai E {0,...,I}):",
        "To allow for French words which do not directly correspond to any English word an artificial 'empty' word CO is added to the target sentence at position = 0.",
        "The different alignment models we present provide different decompositions of Pr ien• An alignment al for which holds",
        "for a specific model is called Viterbi alignment of this model.",
        "In this paper we will describe extensions to the Hidden-Markov alignment model from (Vogel et al., 1996) and compare these to Models 1 - 4 of (Brown et al., 1993).",
        "We propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment.",
        "This has the advantage that once having produced a reference alignment, the evaluation itself can be performed automatically.",
        "In addition, it results in a very precise and reliable evaluation criterion which is well suited to assess various design decisions in modeling and training of statistical alignment models.",
        "It is well known that manually performing a word alignment is a complicated and ambiguous task (Melamed, 1998).",
        "Therefore, to produce the reference alignment we use a refined annotation scheme which reduces the complications and ambiguities occurring in the manual construction of a word alignment.",
        "As we use the alignment models for machine translation purposes, we also evaluate the resulting translation quality of different models."
      ]
    },
    {
      "heading": "2 Alignment with HMM",
      "text": [
        "In the Hidden-Markov alignment model we assume a first-order dependence for the alignments ai and that the translation probability depends only on ai and not on Pr(fi)ailfri, '1,4) = v(ai -01)(filea;) Later, we will describe a refinement with a dependence on cai_i in the alignment model.",
        "Putting everything together, we have the following basic HMM-based model:",
        "with the alignment probability and the translation probability v(fle).",
        "To find a Viterbi alignment for the HMM-based model we resort to dynamic programming (Vogel et al., 1996).",
        "The training of the HMM is done by the EM-algorithm.",
        "In the E-step the lexical and alignment",
        "counts for one sentence-pair (f, e) are calculated:",
        "To avoid the summation over all possible alignments a, (Vogel et al., 1996) use the maximum approximation where only the Viterbi alignment path is used to collect counts.",
        "We used the Baum-Welch-algorithm (Baum, 1972) to train the model parameters in our experiments.",
        "Thereby it is possible to perform an efficient training using all alignments.",
        "To make the alignment parameters independent from absolute word positions we assume that the alignment probabilities depend only on the jump width (i – i').",
        "Using a set of non-negative parameters {c(i – we can write the alignment probabilities in the form:",
        "This (Orin ensures that for each word position i' = 1, ..., I, the alignment probabilities satisfy the normalization constraint."
      ]
    },
    {
      "heading": "Extension: refined alignment model",
      "text": [
        "The count table c(i – has only 2 – 1 entries.",
        "This might be suitable for small corpora, but for large corpora it is possible to make a more refined model of Pr(ai , (4-1 , ef ).",
        "Especially, we analyzed the effect of a dependence on eat_, or As a dependence on all English words would result in a huge number of alignment parameters we use as (Brown et al., 1993) equivalence classes G over the English and the French words.",
        "Here G is a mapping of words to classes.",
        "This mapping is trained automatically using a modification of the method described in (Kneser and Ney, 1991).",
        "We use 50 classes in our experiments.",
        "The most general form of alignment distribution that we consider ill the 111\\41\\4 is",
        "Extension: empty word In the original formulation of the 1-11\\41\\4 alignment; model there is no 'empty' word which generates French words having no directly aligned English word.",
        "A direct inclusion of an empty word in the H1\\41\\4 model by adding an co as in (Brown et al., 7.993) is not possible if we want to model the jump distances i – i', as the position i = 0 of the empty word is chosen arbitrarily.",
        "Therefore, to introduce the empty word we extend the HIVINI network by I empty words 61+1.",
        "The English word ei has a corresponding empty word ei+.1.",
        "The position of the empty word encodes the previously visited English word.",
        "We enforce the following constraints for the transitions in the 1-11\\41\\1 network (i < I, < I): + /1i', = Pff • 6(i, i')",
        "The parameter p [I is the probability of a transition to the empty word.",
        "In our experiments we set yf' = 0.2."
      ]
    },
    {
      "heading": "Smoothing",
      "text": [
        "For a. better estimation of infrequent; events we introduce the following smoothing of alignment probabilities: Pi(ailaj-i , = + (1 - (1) • 1)(a.11\"./-1, I) In our experiments we use (A = 0.4."
      ]
    },
    {
      "heading": "3 Model 1 and Model 2",
      "text": [
        "Replacing the dependence on aj_i in the IIMM alignment model by a dependence on j, we obtain a model which call be seen as a. zero-order Hidden-Markov Model which is similar to Model 2 proposed by (Brown et al., 1993).",
        "Assuming a uniform alignment probability v(ilj, I) = 1/1, we obtain Model 1.",
        "Assuming that the dominating factor in the alignment; model of Model 2 is the distance relative to the diagonal line of the (j, i) plane the model 1) can be structured as follows (Vogel et al., 1996): Air), = This model will be referred to as diagonal-oriented Model 2."
      ]
    },
    {
      "heading": "4 Model 3 and Model 4",
      "text": [
        "Model: The fertility models of (Brown et al., 1993) explicitly model the probability p(01c) that the English word c1 is aligned to",
        "Model 3 of (Brown et al., 1993) is a zero-order alignment model like Model 2 including in addition fertility parameters.",
        "Model 4 of (Brown et al., 1993) is also a first-order alignment model (along the source positions) like the IIMM, but includes also fertilities.",
        "In Model 4 the alignment position j of an English word depends on tile alignment position of the previous English word (with non-zero fertility) j'.",
        "It models a jump distance j – j1 (for consecutive English words) while in the HMM a jump distance i – i' (for consecutive French words) is modeled.",
        "The full description of Model 4 (Brown et al., 1993) is rather complicated as there have to be considered the cases that English words have fertility larger than one and that English words have fertility zero.",
        "For training of Model 3 and Model 4, we use an extension of the program GIZA (Al-Onaizan et al., 1999).",
        "Since there is no efficient way in these models to avoid the explicit summation over all alignments in the EM-algorithm, the counts are collected only over a subset of promising alignments.",
        "It is not known an efficient algorithm to compute the Viterbi alignment for the Models 3 and 4.",
        "Therefore, the Viterbi alignment is computed only approximately using the method described in (Brown et al., 1993).",
        "The models 1-4 are trained in succession with the final parameter values of one model serving as the starting point for the next.",
        "A special problem in Model 3 and Model 4 concerns the deficiency of the model.",
        "This results in problems in re-estimation of the parameter which describes the fertility of the empty word.",
        "In normal EM-training, tins parameter is steadily decreasing, producing too many alignments with the empty word.",
        "Therefore we set the probability for aligning a source word with the empty word at a suitably chosen constant value.",
        "As in the HMM we easily can extend the dependencies in the alignment model of Model 4 easily using the word class of the previous English word"
      ]
    },
    {
      "heading": "5 Including a Manual Dictionary",
      "text": [
        "We propose here a simple method to make use of a bilingual dictionary as an additional knowledge source in the training process by extending the training corpus with the dictionary entries.",
        "Thereby, the dictionary is used already in EM-training and can improve not only the alignment for words which are in the dictionary but indirectly also for other words.",
        "The additional sentences in the training corpus are weighted with a factor Fje during the EM-training of the lexicon probabilities.",
        "We assign the dictionary entries which really co-occur in the training corpus a high weight Fi„ and the remaining entries a very low weight.",
        "In our experiments we use Fie, = 10 for the co-occurring dictionary entries which is equivalent to adding every dictionary entry ten times to the training corpus."
      ]
    },
    {
      "heading": "6 The Alignment Template System",
      "text": [
        "The statistical machine-translation method described in (0th et al., 1999) is based on a word aligned training corpus and thereby makes use of single-word based alignment models.",
        "The key element of this approach are the alignment templates which are pairs of phrases together with an alignment between the words within the phrases.",
        "The advantage of the alignment template approach over word based statistical translation models is that word context and local re-orderings are explicitly taken into account.",
        "We typically observe that this approach produces better translations than the single-word based models.",
        "The alignment templates are automatically trained using a parallel training corpus.",
        "For more information about the alignment template approach see (Och et al., 1999)."
      ]
    },
    {
      "heading": "7 Results",
      "text": [
        "We present; results on the Verbmobil Task which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster, 1993).",
        "We measure the quality of tile above mentioned alignment models with respect to alignment quality and translation quality.",
        "To obtain a reference alignment for evaluating alignment quality, we manually aligned about 1.4 percent of our training corpus.",
        "We allowed the humans who performed the alignment to specify two different kinds of alignments: an S (sure) alignment winch is used for alignments which are unambiguously and a P (possible) alignment which is used for alignments which might or might not exist.",
        "The P relation is used especially to align words within idiomatic expressions, free translations, and missing function words.",
        "It is guaranteed that S C P. Figure 1 shows an example of a manually aligned sentence with S and P relations.",
        "The human-annotated alignment does not prefer any translation direction and may therefore contain many-to-one and one-to-many relationships.",
        "The annotation has been performed by two annotators, producing sets Si, P1, 82, P2.",
        "The reference alignment is produced by forming the intersection of the sure ,alignments (S = Sl n S2) and the union of the possible alignments (P = Pi U P2).",
        "The quality of an alignment A = { (j, ai)} is measured using the following alignment error rate:",
        "nary is small compared to the effect of using better alignment models.",
        "We see a significant difference in alignment quality if we exchange source awl target languages.",
        "This is due to the restriction in all alignment models that a source language word can he aligned to at most one target language word.",
        "If German is source language the frequently occurring German word compounds, cannot be aligned correctly, as they typically correspond to two or more English words.",
        "Obviously, if we compare the sure alignments of every single annotator with the reference alignment we obtain an AEB, of zero percent.",
        "Table 2 shows the alignment quality of different alignment models.",
        "Here the alignment models of HMM and Model 4 do not include a dependence on word classes.",
        "We conclude that; more sophisticated alignment models are crucial for good alignment quality.",
        "Consistently, the use of a first-order alignment model, modeling an empty word and fertilities result in better alignments.",
        "Interestingly, the simpler HMM alignment model outperforms Model 3 which shows the importance of first-order alignment models.",
        "The best performance is achieved with Model 4.",
        "The improvement by using a dictio",
        "For the evaluation of the translation quality we used the automatically computable Word Error Rate (WER) and the Subjective Sentence Error Rate (SSER,) (Niefien et al., 2000).",
        "The WEB, corresponds to the edit distance between the produced translation and one predefined reference translation.",
        "To obtain the SSER the translations are classified by human experts into a small number of quality classes ranging from \"perfect\" to \"absolutely wrong\".",
        "In comparison to the WER,, this criterion is more meaningful, but it is also very expensive to measure.",
        "The translations are produced by the alignment template system mentioned in the previous section.",
        "The results are shown in Table 5.",
        "We see a clear improvement in translation quality as measured by SSER, whereas WER, is more or less the same for all models.",
        "The improvement is due to better lexicons and better alignment templates extracted from the resulting alignments."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We have evaluated various statistical alignment models by comparing the Viterbi alignment of the model with a human-made alignment.",
        "We have shown that by using more sophisticated models the quality of the alignments improves significantly.",
        "Further improvements in producing better alignments are expected from using the HMM alignment model to bootstrap the fertility models, from making use of cognates, and from statistical alignment models that are based on word groups rather than single words."
      ]
    },
    {
      "heading": "Acknowledgment",
      "text": [
        "This article has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology."
      ]
    }
  ]
}
