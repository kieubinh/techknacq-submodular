{
  "info": {
    "authors": [
      "Stephan Vogel",
      "Hermann Ney"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P00-1004",
    "title": "Translation With Cascaded Finite State Transducers",
    "url": "https://aclweb.org/anthology/P00-1004",
    "year": 2000
  },
  "references": [
    "acl-C00-2172",
    "acl-C96-1030",
    "acl-P96-1021",
    "acl-P97-1047",
    "acl-W99-0604"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper we discuss the use of cascaded finite state transducers for machine translation.",
        "A number of small, dedicated transducers is applied to convert sentence pairs from a bilingual corpus into generalized translation patterns.",
        "These patterns, together with the transducers are then used as a hierarchical translation memory for fully automatic translation.",
        "Results on the German–English VER13MO13IL corpus are given."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Corpus based approaches to automatic translation come in a number of different flavors.",
        "In the simplest form, translations are stored and reused for the translation of new input.",
        "This approach, known as translation memory, example-based or case-based translation, can work on the word level as well as on structured examples as they are generated during analysis and generation in more grammar-based translation paradigms (Kitano, 1993; Brown, 1996).",
        "Finite state transducers, which can be learned from bilingual corpora, have been proposed for automatic translation (Amen-gual et al., 2000), as have been bilingual stochastic grammars (Wu, 1996).",
        "Statistical approaches (Wang and Waibel, 1997; Och et al., 1999) also fall into the category of corpus based approaches.",
        "In this paper, a translation method is proposed which is based on the very same principles as the aforementioned approaches.",
        "One difference is, that not a fully automatic training of the translation model is performed.",
        "Rather, a number of special purpose transducers are hand-crafted and used then at two points.",
        "First, to convert the bilingual training corpus into a translation memory containing translation patterns rather than merely sentence pairs, and which is itself used as a transducer in the translation process.",
        "Second, when new sentences are to be translated, the transducers are applied to transform the input sentence into one or many possible target sentences the best of which, according to some scoring scheme, is selected as the translation.",
        "In the next section, the construction of the transducers and the translation memory is outlined.",
        "Then, the application of the transducers for the translation of new sentences is described.",
        "In the last section the results of some translation experiments are given."
      ]
    },
    {
      "heading": "2 The Transducers 2.1 Overview",
      "text": [
        "A finite state transducer (FST) is a finite state device which reads symbols from one channel and outputs a stream of symbols to a second channel.",
        "So, a FST can be depicted as a transition net with edges and nodes, where the nodes represent the states and the edges the possible state transitions.",
        "The edges are labelled with an input symbol and an output string, which may be the empty words of the two vocabularies.",
        "The final states can produce additional output.",
        "We want to construct transducers for automatic machine translation from a given bilingual corpus.",
        "In fact, a collection of sentence pairs can be viewed as a trivial transducer, where each sentence pair is represented by a distinct line of nodes connected by edges labeled with the source sentence words and the target sentence emitted from the final state.",
        "This can be easily transformed into a tree transducer by building a prefix tree over the source sentences.",
        "In (Amengual et al., 2000) a method is given to propagate prefixes of the translations towards the root of such a tree transducer and to coalesce states to gain generalization power.",
        "We choose here a different route to generalization by using an approach similar to the one used for chunk parsing, where a cascade of FST is applied (Abney, 1997).",
        "Each transducer, defined by a set of regular-expression patterns, reads part of the input sentence and writes a stream of category labels, which form, together with the unanalyzed parts of the sentence, the input to the next transducer in the cascade.",
        "Our approach differs from the aforementioned chunk parsing in that an analyzed sequence of words is not replaced by the category label but is kept as a parallel option for transducers applied at a later stage.",
        "How this leads to the construction of a translation graph will be explained in Section 3.",
        "For translation, not only the analysis of the source sentence is required but also the generation of the target sentence.",
        "This can be achieved if the transducers write category labels as well as translations to the output channel.",
        "We allow for more than one translation for a given input sequence.",
        "This raises the question of how to select one translation over the others.",
        "Some kind of scoring is required, a point we will return to in section 2.3.",
        "To summarize: each transducer is given as a set of quadruples of the form: [label # source pattern # target pattern # score].",
        "At runtime these patterns are stored in a prefix tree with respect to the source patterns.",
        "We write the labels at first position as these translations patterns can be used in the reverse direction, i.e. from target language to source language.",
        "In section 2.4 this property is used to convert a bilingual corpus into a set of translation patterns which are formulated in terms of words and category labels.",
        "It also shows the structural identity to bilingual grammars as used in (Wu, 1996)."
      ]
    },
    {
      "heading": "2.2 Construction of the Transducers",
      "text": [
        "Most of the transducers are customized towards the domain for which the translation system is developed.",
        "In the VERSMOBIL Corpus, which is used for the experiments, time and date expressions are very prominent.",
        "To translate those expressions, a small grammar has been developed and coded as finite state transducer.",
        "Actually, two transducers are used.",
        "On the first level, words are replaced by labels, like DAYOFWEEK = {Montag/Monday, Dienstag/Tuesday, ...}.",
        "On the second level, these labels together with labeled numbers (ordinal, cardinal, fractions) from the number transducer are used to form complex time and date expressions.",
        "Some examples are given in Table 1.",
        "All in all we use currently seven of those dedicated transducers: names (persons, towns, places, events, etc), spelling sequences (e.g. `D A double L'), numbers (ordinal, cardinal, fractions, etc.",
        "), simple time and date expressions, compound time and date expressions, part-of-speech tagging, grammar (noun phrases, verb phrases).",
        "The relationship between these different transducers is depicted in figure 1.",
        "The arrows indicate that category labels introduces by one transducer are used by another transducer.",
        "The division into these transducers is mainly a conceptual one.",
        "The five base level transducers could be coalesced into one transducer.",
        "Actually, this is done at runtime for efficiency.",
        "However, to keep them appart at construction time gives more flexibility.",
        "For example, while for a closed vocabulary in a speech translation task these transducers boil down to simple substitution list an open vocabulary task will require a more elaborate approach to proper name spotting or handling of numbers.",
        "The part-of-speech transducer has been constructed semi-automatically.",
        "A tagger was",
        "used to get a word – POS tag list.",
        "This was combined with an automatically generated translation lexicon (Och et al., 1999) to produce a list of label – word – translation patterns.",
        "This was then manually corrected and augmented where necessary.",
        "Ideally, one would like to have a common tagset for both source and target language.",
        "If this is not available an alternative is to use a tagset for one language and induce via the word to word correspondences a tagging for the second language.",
        "This is the approach taken in this study.",
        "As tagset we use the Stuttgart-Tiibinger tagset for German (Schiller et al., 1995).",
        "Finally, a small bilingual grammar based on POS tags has been crafted manually.",
        "The purpose of the grammar is twofold: First, improving generalization by recognizing simple noun and prepositional phrases.",
        "Second, to handle the different word ordering in source and target language, especially in the verb phrases."
      ]
    },
    {
      "heading": "2.3 Scoring",
      "text": [
        "The scores attached to the translation patterns can be viewed as a kind of translation scores.",
        "In the current implementation a rather crude heuristics together with some manual tuning in the grammar transducer is applied.",
        "The idea is to give preference to longer translation patterns as they take more context into account and encode word reordering in an explicit manner.",
        "So, for simple and compound translation patterns the score is exponential to the length of the source pattern.",
        "The scores are negative by convention: not translating a word gives zero cost, translating it gives a benefit, i.e. negative costs."
      ]
    },
    {
      "heading": "2.4 Bilingual Labeling",
      "text": [
        "The sentence pairs in the bilingual training corpus could be used directly as a simple translation memory.",
        "However, to improve the coverage on unseen data, these segments are transformed into translation patterns containing category labels.",
        "For each transducer taken from the complete cascade – as given in Figure 1 – the transducers are applied to both, the source and the target sentences of the bilingual training corpus (Vogel and Ney, 2000).",
        "Those sentence pairs where number and types of category labels in source and target sentence match each other are selected into the database of compound translation patterns.",
        "Table 2 shows examples of some translation patterns which resulted from bilingual labeling."
      ]
    },
    {
      "heading": "3 The Translation Process",
      "text": [
        "The working of the transducers is best described as the construction of a translation",
        "graph.",
        "That is to say, the sentence to be translated is viewed as a graph which is traversed from left to right.",
        "For each matching source pattern, as stored in the transducers, a new edge is added to the graph.",
        "The edge is labeled with the category label of the translation pattern.",
        "The translation and the translation score are attached to the edge.",
        "In this way a translation graph is constructed.",
        "In those cases, where a source pattern has several translations, one edge for each translation is added to the graph.",
        "One advantage of this approach is that it can be applied to perform translation on word lattices as generated by speech recognition systems without any modifications.",
        "The left–right traversal of the graph is organized in such a way that all paths are traversed in parallel and the patterns stored in the transducer are matched synchronously.",
        "For each node n and each edge e leading to that node all patterns in the transducer starting with the word or category label of e are attached to n. This gives a number of hypotheses describing partially matching patterns.",
        "Already started hypotheses are expanded with the label of the edge running from the previous node to the current node.",
        "As an example, the translation graph for the sentence `Samstag and Februar sind gut, aber der vierte ware besser' is shown in Figure 2.",
        "Actually, the graph is much bigger.",
        "In the figure, only those edges are shown which contributed to the construction of the best path."
      ]
    },
    {
      "heading": "3.1 Error Tolerant Match",
      "text": [
        "To improve the coverage on unseen test data, it may be advantageous to allow for only approximative matching with the segments in the translation memory.",
        "The idea is to apply longer segments for syntactically better translations without losing too much as far as the content of the sentences is concerned.",
        "We use a weighted edit distance, i.e. each error (insertion, deletion, substitution) is associated with a score.",
        "Thereby, the deletion or insertion of typical filler words can be allowed, whereas the deletion or insertion of content words is avoided.",
        "Hypotheses with to high a matching error score are discarded.",
        "A threshold proportional to the number of covered positions is used.",
        "Thus, longer translation patterns can be matched with more insertions, deletions and substitutions.",
        "A drawback of this is, however, that for long patterns mismatches on content words may occur.",
        "Each transducer has its own list of insertion, deletion and substiution scores.",
        "Actually, only for those transducers where the translation patterns cover longer sequences of words and labels do we use error tolerant matching.",
        "Error-tolerant matching may also help to compensate for speech recognition errors in the case of speech translations.",
        "In that case the confusion matrix obtained by comparing the recognizer output for the training speech data with the transliteration can be used."
      ]
    },
    {
      "heading": "3.2 Using a Language Model",
      "text": [
        "The application of the transducers to a given source sentence yield a large number of target sentences.",
        "These are scored according to the cumulative scores of the applied translation patterns.",
        "As an independent and direct model of the likelihood of the target sentences a language model is applied.",
        "We use a word-based trigram language model (Sawaf et al., 2000).",
        "The logarithm of the language model probabilities is added to the transducer scores",
        "when the best path through the translation graph is extracted.",
        "A scaling factor allows for a bias on the effect of the language model."
      ]
    },
    {
      "heading": "4 Experiments and Results",
      "text": [
        "In this section, we will give some results obtained with the cascaded transducer approach.",
        "Experiments were performed on the VERSMOBIL corpus.",
        "This corpus consists of spontaneously spoken dialogs in the appointment scheduling domain (Wahlster, 2000).",
        "A summary of the corpus used in the experiments is given in Table 3.",
        "In Table 4 the sizes of the special purpose transducers are given.",
        "The sentences from the training corpus were segmented into shorter segments using sentence marks as breakpoints.",
        "This resulted in 43 609 bilingual phrases running form 1 word up to 82 words in length.",
        "The longest",
        "phrases were discarded as it is very unlikely that they will match other sentences.",
        "So, for the construction of the translation patterns only 40 000 sentence pairs were used, the longest sentences containing sixteen source words.",
        "Starting from those simple phrases, successively more transducers were applied up to the full cascade.",
        "A total of 15 682 translation patterns containing one or more labels resulted and nearly 4 500 sentence pairs became identical when words or word sequences were replaced by labels.",
        "For a test corpus consisting of 147 sentences, the translations have been evaluated according to two measures (NieBen et al., 2000): Multi-reference word error rate (mWER): for each source sentence several good translations are given.",
        "The word error rate between the generated translation and the closest reference is calculated.",
        "Subjective sentence error rate (SSER): the translations are evaluated by a human examiner using a scale ranging from 0 to 10.",
        "The average of these values is linearly transformed to give the sentence error rate in percent."
      ]
    },
    {
      "heading": "4.1 Effect of Grammar",
      "text": [
        "A simple translation memory without any categorization gives insufficient coverage on unseen test data.",
        "With the part-of-speech transducer we get one or more translations for each word in the vocabulary.",
        "But only by applying transducers which handle longer translation patterns is word reordering possible.",
        "In Table 6 the results are given for different combinations of transducers.",
        "The baseline (T) is the combination of all special purpose transducers (name, spell, number, date, word tags) plus the simple translations patterns.",
        "Then the grammar was added and finally the compound translation patterns.",
        "The trigram language model for the target language was applied in selecting the best translation, but no error tolerant matching was allowed.",
        "We observe a clear effect in word error rate and subjective sentence error rate.",
        "The use of the bilingual grammar, also very restricted, improves translation quality.",
        "Applying the compound translation patterns gives an additional small improvement.",
        "In Table 5 a simple and a more involved example for the reordering effect of the bilingual grammar are given.",
        "The first translation pattern operates solely on the level of POS tags whereas the second example generates a hierarchical structure.",
        "We are not concerned whether the source sentence parses are correct, good translations is what we are looking for."
      ]
    },
    {
      "heading": "4.2 Effect of Language Model",
      "text": [
        "The next experiment shows the effect of applying a language model for the target language.",
        "A word-based trigram language model was interpolated with the scores from the transducers.",
        "In Table 7 the effect of the scaling between the two models is shown.",
        "There is a clear drop in the WER when switching on the language model.",
        "This is due to the fact, that several translation hypotheses have the same score from the transducers.",
        "So, it is rather by chance if the best translation for a given word is chosen.",
        "The language model for the target language helps in doing this.",
        "There is a second benefit gained from the language model: sometimes the source sentence can be covered with only very short source patterns.",
        "That is to say, word context is hardly taken into account.",
        "With a language model context is brought into play again.",
        "If the language model scaling factor is increased too much translation quality deteriorates again.",
        "So, a good balance between both knowledge sources is necessary.",
        "In Table 8 some examples which show the effect of the language model are given.",
        "The first translation is without language model, the second is the translation obtained when the language model score is added using a scaling factor of 0.5."
      ]
    },
    {
      "heading": "4.3 Effect of Error Tolerant Matching",
      "text": [
        "Finally, the effect of error tolerant matching has been investigated.",
        "Only for the simple and compound translation patterns errors have been allowed in matching parts of the in",
        "erst wieder ab dem sechzehnten.",
        "no LM starting from the sixteenth only again.",
        "with LM only starting from the sixteenth.",
        "ja, wunderbar.",
        "machen wir das so, and dann treffen wir uns dann in Hamburg.",
        "no LM yes, nice.",
        "will we do which right, after all we meet us after all in Hamburg.",
        "with LM fine.",
        "let us do it like that, and then we will meet then in Hamburg.",
        "put sentences to stored translation patterns.",
        "The effect of increasing the error threshold is given in Table 9.",
        "We see a considerable improvement when allowing for a small number of errors in matching the translation patterns to the input sentence.",
        "However, if the match gets too sloppy serious errors occur which alter the meaning of the sentence.",
        "For longer sequences of words the number of errors allowed becomes higher than the default score for substitutions.",
        "In such a case content words can be substituted.",
        "An example of how the same source sentence gets different translations when more matching errors are allowed is given in Table 10."
      ]
    },
    {
      "heading": "5 Summary and future work",
      "text": [
        "In this paper a translation approach based on cascaded finite state transducers has been presented.",
        "A small number of simple transducers is hand-crafted and then used to convert a bilingual corpus into a translation memory consisting of source pattern – target pattern pairs, which include category labels.",
        "Translation is then performed by applying the complete cascade of transducers.",
        "With the simple heuristic for the translation scores a language model for the target language is paramount to select good translations.",
        "Error-tolerant matching improves translation quality.",
        "Experiments have shown the potential of this approach for machine translation.",
        "Good coverage on unseen test data could be obtained.",
        "A major advantage of this translation Table 10: Examples for the effect of error tolerant matching.",
        "ja , wunderbar .",
        "machen wir das so , and dann treffen wir uns dann in Hamburg .",
        "0.0 fine .",
        "let us do it like that , and then we will meet then in Hamburg .",
        "0.2 fine .",
        "let us do that , and then we will meet in Hamburg.",
        "0.4 fine .",
        "let us do it like that �and then we will meet in Hamburg.",
        "0.6 fine .",
        "let us do it like that �and then we will meet in your office .",
        "method is that it breaks the middle ground between direct translation methods like simple translation memory or word-based statistical translation and transfer based methods involving deep linguistic analysis of the input.",
        "In fact, the cascaded transducer approach allows for building quickly a first version and improving translation quality by gradually adding more linguistic and domain specific knowledge.",
        "We expect further improvement by assigning translation scores according to corpus statistics.",
        "This will be the main focus for future work.",
        "Acknowledgement.",
        "This work was partly supported by the German Federal Ministry of Education, Science, Research and Technology under the Contract Number 01 IV 701 T4 (VERBMOBIL)."
      ]
    }
  ]
}
