{
  "info": {
    "authors": [
      "Khalil Sima'an"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P00-1008",
    "title": "Tree-Gram Parsing: Lexical Dependencies and Structural Relations",
    "url": "https://aclweb.org/anthology/P00-1008",
    "year": 2000
  },
  "references": [
    "acl-C96-1058",
    "acl-C96-2215",
    "acl-H91-1060",
    "acl-H94-1052",
    "acl-J93-2004",
    "acl-P95-1037",
    "acl-P97-1003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper explores the kinds of probabilistic relations that are important in syntactic disambiguation.",
        "It proposes that two widely used kinds of relations, lexical dependencies and structural relations, have complementary disambiguation capabilities.",
        "It presents a new model based on structural relations, the Tree-gram model, and reports experiments showing that structural relations should benefit from enrichment by lexical dependencies."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Head-lexicalization currently pervades in the parsing literature e.g. (Eisner, 1996; Collins, 1997; Charniak, 1999).",
        "This method extends every treebank nonterminal with its headword: the model is trained on this head lexical-ized treebank.",
        "Head lexicalized models extract probabilistic relations between pairs of lexical-ized nonterminals ( \"bilexical dependencies\"): every relation is between a parent node and one of its children in a parse-tree.",
        "Bilexical dependencies generate parse-trees for input sentences via Markov processes that generate Context-Free Grammar (CFG) rules (hence Markov Grammar (Charniak, 1999)).",
        "Relative to Stochastic CFGs (SCFGs), bilexical dependency models exhibit good performance.",
        "However, bilexical dependencies capture many but not all relations between words that are crucial for syntactic disambiguation.We give three examples of kinds of relations not captured by bilexical-dependencies.",
        "Firstly, relations between non-head words of phrases, e.g. the relation between \"more\" and \"than\" in \"more apples than oranges\" or problems of PP attachments as in \"he ate pizza (with mushrooms)/(with a fork)\".",
        "Secondly, relations between three or more words are, by definition, beyond bilexical dependencies (e.g. between \"much more\" and \"than\" in \"much more apples than or-anges\") .",
        "Finally, it is unclear how bilexical dependencies help resolve the ambiguity of idioms, e.g. \"Time flies like an arrow\" (neither \"time\" prefers to \"fly\", nor the fictitios beasts \"Time flies\" have taste for an \"arrow\").",
        "The question that imposes itself is, indeed, what relations might complement bilexical dependencies ?",
        "We propose that bilexical dependencies can be complemented by structural relations (Scha, 1990), i.e. cooccurrences of syntactic structures, including actual words.",
        "An example model that employs one version of structural relations is Data Oriented Parsing (DOP) (Bod, 1995).",
        "DOP's parameters are \"subtrees\", i.e. connected subgraphs of parse-trees that constitute combinations of CFG rules, including terminal rules.",
        "Formally speaking, \"bilexical dependen-cies\" and \"structural relations\" define two disjoint sets of probabilistic relations.",
        "Bilexical dependencies are relations defined over direct dominance head lexicalized nonterminals (see (Satta, 2000)); in contrast, structural relations are defined over words and arbitrary size syntactic structures (with non-lexicalized nonterminals).",
        "Apart from formal differences, they also have complementary advantages.",
        "Bilexical-dependencies capture influential lexical relations between heads and dependents.",
        "Hence, all bilexical dependency probabilities are conditioned on lexical information and lexical information is available at every point in the parse-tree.",
        "Structural relations, in contrast, capture many relations not captured by bilexical-dependencies (e.g. the examples above).",
        "However, structural relations do not always percolate lexical information up the parse-tree since their probabilities are not always lexicalized.",
        "This is a serious disadvantage when parse-trees are generated for novel input sentences since e.g. subcat frames are hypothesized for nodes high in the parse-tree without reference to their head words.",
        "So, theoretically speaking, bilexical dependencies and structural relations have complementary aspects.",
        "But, what are the empirical merits and limitations of structural relations ?",
        "This paper presents a new model based on structural relations, the Tree-gram model, which allows head-driven parsing.",
        "It studies the effect of percolating head categories on performance and compares the performance of structural relations to bilexical dependencies.",
        "The comparison is conducted on the Wall Street Journal (WSJ) corpus (Marcus et al., 1993).",
        "In the remainder, we introduce the Tree-gram model in section 2, discuss practical issues in section 3, exhibit and discuss the results in section 4, and in section 5 we give our conclusions.",
        "2 The Tree-gram model For observing the effect of percolating information up the parse-tree on model behavior, we introduce pre-head enrichment, a structural variant of head-lexicalization.",
        "Given a training treebank TB, for every non-leaf node u we mark one of its children as the head-child, i.e. the child that dominates the head-word' of the constituent under y.",
        "We then enrich this treebank by attaching to the label of every phrasal node (i.e. nonterminal that is not a POS-tag) a pre-head representing its headword.",
        "The pre-head of node u is extracted from the constituent parse-tree under node y.",
        "In this paper, the pre-head of It consists of 1) the POS-tag of the headword of u (called 1St order pre-heads or 1PH), and 'Head-identification procedure by (Collins, 1997).",
        "possibly 2) the label of the mother node of that POS-tag (called 2nd order or 2P11).",
        "Pre-heads here also include other information defined in the sequel, e.g. subcat frames.",
        "The complex categories that result from the enrichment serve as the nonterminals of our training treebank; we refer to the original treebank symbols as \"WSJ labels\"."
      ]
    },
    {
      "heading": "2.1 Generative models",
      "text": [
        "A probabilistic model assigns a probability to every parse-tree given an input sentence S, thereby distinguishing one parse T* = argmaxT P (T S) = argmaxT P(T, S) .",
        "The probability P(T, S) is usually estimated from cooccurrence statistics extracted from a treebank.",
        "In generative models, the tree T is generated in top down derivations that rewrite the start symbol TOP into the sentence S. Each rewrite-step involves a \"rewrite-rule\" together with its estimated probability.",
        "In the present model, the \"rewrite-rules\" differ from the CFG rules and combinations thereof that can be extracted from the treebank.",
        "We refer to them as Tree-grams (abbreviated T-grams).",
        "T-grams provide a more general-form for Markov Grammar rules (Collins, 1997; Charniak, 1999) as well as DOP subtrees.",
        "In comparison with DOP subtrees, T-grams capture more structural relations, allow head-driven parsing and are easier to combine with bilexical-dependencies."
      ]
    },
    {
      "heading": "2.2 T-gram extraction",
      "text": [
        "Given a parse T from the training treebank, we extract three disjoint T-gram sets, called roles, from every one of its non-leaf nodes y: the head-role R(p), the left-dependent role G(y) and the right-dependent role R(y).",
        "The role of a T-gram signifies the T-gram's contribution to stochastic derivations: t C R carries a head-child of its root node label, t C G (t C R) carries left (resp.",
        "right) dependents for other head T-grams that have roots labeled the same as the root of t. Like in Markov Grammars, a head-driven derivation generates first a head-role T-gram and attaches to it left-and right-dependent role T-grams.",
        "We discuss",
        "these derivations right after we specify the T-gram extraction procedure.",
        "Let d represent the depth3 of the constituent tree-structure that is rooted at y, H represent the label of the head-child of y, and A represent the special stop symbol that encloses the children of every node (see figure 1).",
        "Also, for convenience, let Sj be equal to A iff k = n and NILL (i.e. the empty tree-structure) otherwise.",
        "We specify the extraction for d = 1 and for d > 1.",
        "When d = 1, the label of u is a POS-tag and the subtree under u is of the form pt – � OwA, where w is a word.",
        "In this case R(p) = {pt – � OwAl and L(y) = R(y) = 0.",
        "When d > 1: the subtree under u has the form A � ALn(tin ... L, (ti.",
        "H(tH) R,(ti) ... Rn(tm)A (figure 1), where every t?, tj' and to is the subtree dominated by the child node of u (labeled respectively Li, Rj or H) whose address we denote respectively with childL(y, i), childR(y, j) and childu(y).",
        "We extract three sets of T-grams from y: R(p) : contains d 1 < i < n and 1 < j < m, A – � 6 Li(X?)",
        "... H(Xh) ... Rj(Xjr )6j , where Xh is either in 'l (chil d u (y)) or NILL, and every Xz (resp.",
        "Xr) is either a T-gram from R(childL(lc, z)) (resp.",
        "� for all 1 < i < k < m, where every X,z7 i < z < k, is either a T-gram from R(childR(y, z)) or NILL, 3 The depth of a (sub)tree is the number of edges in the longest path from its root to a leaf node.",
        "last week a deal was VBN",
        "Note that every T-gram's non-root and non-leaf node dominates a head-role T-gram (specified by R(child• • •)).",
        "A non-leaf node u labeled by nonterminal A is called complete, denoted \"[A]\", iff A delimits its sequence of children from both sides; when A is to the left (right) of the children of the node, the node is called left (resp.",
        "right complete, denoted \"[A\" (resp.",
        "\"A]\").",
        "When u is not left (right) complete it is open from the left (resp.",
        "right); when u is left and right open, it is called open.",
        "Figure 2 exhibits a parse-tree 4: the number of the head-child of a node is specified between brackets.",
        "Figure 3 shows some of the T-grams that can be extracted from this tree.",
        "Having extracted T-grams from all non-leaf nodes of the treebank, we obtain 'H= Uj CTB'h(u), L= Uj,ETBL(u) and R= U,,ETBR(u)• RA, LA and RA represent the subsets of resp.",
        "R, L and R that contain those T-grams that have roots labeled A. XA(B) C ILA (B),RA(B),'HA(B)I specifies that the extraction took place on some treebank B other than the training treebank."
      ]
    },
    {
      "heading": "2.3 T-gram generative processes",
      "text": [
        "Now we specify T-gram derivations assuming that we have an estimate of the probability of a T-gram.",
        "We return to this issue right after this.",
        "A stochastic derivation starts from the start nonterminal TOP.",
        "TOP is a single node partial parse-tree which is simultaneously the root and the only leaf node.",
        "A derivation terminates when two conditions are met (1) every non-leaf node in the generated parse-tree is complete (i.e. A delimits its children from 4Pre-heads are omitted for readability.",
        "T-gram role,.",
        "e.g. the leftmost T-gram is in the left-dependent role.",
        "Non-leaf nodes are marked with \"[\" and \"]\" to specify whether they are complete from the left/right or both (leaving open nodes unmarked).",
        "both sides) and (2) all leaf nodes are labeled with terminal symbols.",
        "Let II represent the current partial parse-tree, i.e. the result of the preceding generation steps, and let CH represent that part of II that influences the choice of the next step, i.e. the conditioning history.",
        "The generation process repeats the following steps in some order, e.g. head-left-right: Head-generation: Select a leaf node u labeled by a nonterminal A, and let A generate a head T-gram t E 'HA with probability PH(tJA,Cn).",
        "This results in a partial parse-tree that extends 1Z at u with a copy of t (as in CFGs and in DOP).",
        "Modification: Select from II a non-leaf node u that is not complete.",
        "Let A be the label of u and T = A – � X1 (X1) • • • Xb(xb) be the tree dominated by u (see figure 4): Left: if u is not left-complete, let It generate to the left of T a left-dependent T-gram t = A(L) – � L, (I,) • • • L,, (1,,) from LA with probability PL(tlA,Cn) (see figure 4 (L)); this results in a partial parse-tree that is obtained by replacing T in II with A _� L, (I,) ... La(la)Xl(x1) ... Xb(Xb), Right: this is the mirror case (see figure 4 (R)).",
        "The generation probability is PR(tI A, Cn).",
        "Figure 5 shows a derivation using T-grams (e), (a) and (d) from figure 3 applied to T-gram TOP – � S. Note that each derivation-step probability is conditioned on A, the label of node u in II where the current rewriting is taking place, on the role (?l, L or R) of the T-gram involved, and on the relevant history Cn.",
        "Assuming beyond this that stochastic independence between the various derivation steps holds, the probability of a derivation is equal to the multiplication of the conditional probabilities of the individual rewrite steps.",
        "Unlike SCFGs and Markov grammars but like DOP, a parse-tree may be generated via different derivations.",
        "The probability of a parse-tree T is equal to the sum of the probabilities of the derivations that generate it (denoted der =* T), i.e.",
        "cause computing argmaxT P(T, S) can not be achieved in deterministic polynomial time (Sima'an, 1996), we apply estimation methods that allow tractable parsing."
      ]
    },
    {
      "heading": "2.4 Estimating T-gram probabilities",
      "text": [
        "Let count(Y1, • • • Y,,,,) represent the occurrence count for joint event (Yl • • • Y,,,,) in the training treebank.",
        "Consider a T-gram t E XA, XA E {LA, RA, 'hA}, and a conditioning history C11.",
        "The estimate count(t,XA,Cn)"
      ]
    },
    {
      "heading": "_ EXA Co2L7Zt(X,XA,CII)",
      "text": [
        "assumes no hidden elements (different derivations per parse-tree), i.e. it estimates the probability PX(tJA,Cn) directly from the treebank trees (henceforth direct-estimate).",
        "This estimate is employed in DOP and is not Maximum-Likelihood (Bonnema et al., 1999).",
        "We argue that the bias of the direct estimate allows approximating the preferred parse by the one generated by the Most Probable Derivation (MPD).",
        "This is beyond the scope",
        "Up till now CH represented conditioning information anonymously in our model.",
        "For the WSJ corpus, we instantiate CH as follows: 1.",
        "Adjacency: The flag FL (t) (FR(t)) tells whether a left-dependent (right-dependent) T-gram t extracted from some node u dominates a surface string that is adjacent to the headword of u (detail in (Collins, 1997)).",
        "2.",
        "Subcat-frames: (Collins, 1997) subcat frames are adapted: with every node u that dominates a rule A – � ALn ... L1 H Rl ... R,-,,,0 in the treebank (figure 1), we associate two (possibly empty) multisets of complements: SC\""
      ]
    },
    {
      "heading": "L and",
      "text": [
        "SCR.",
        "Every complement in SCL (SCR) represents some left (right) complement-child of y.",
        "This changes T-gram extraction as follows: with every non-leaf node in a T-gram that is extracted from a tree in this enriched treebank we have now a left and a right subcat frame associated.",
        "Consider the root node x in a T-gram extracted from node u and let the children of x be Yl • • • Yf (a subsequence of AL,,, • • • , H, • • • R,-,,A).",
        "The left (right) subcat frame of x is subsumed by SO' (resp.",
        "SCR) and contains those complements that correspond to the left-dependent (resp.",
        "right-dependent) children of u that are not among Yl • • • Yf.",
        "Tree-gram derivations are modified accordingly: whenever a T-gram is generated (together with the subcat frames of its nodes) from some node u in a partial-tree, the complements that its root dominates are removed from the subcat frames of y.",
        "Figure 6 shows a small example of a derivation.",
        "3.",
        "Markovian generation: When node u has empty subcat frames, we assume 1st-order Markov processes in generating both L and R T-grams around its R T-gram: LMT` and RM� denote resp.",
        "the left and rightmost children of node y.",
        "Let XRM�' and",
        "After the first rewriting, the subcat frame becomes empty since the NP complement was generated resulting in [S]{1L.",
        "The Other subcat frames are empty and are not shown here.",
        "XLMA be equal to resp.",
        "RMP and LMP if the name of the T-gram system contains the word +Markov (otherwise they are empty).",
        "Let y, labeled A, be the node where the current rewrite-step takes place, P be the WSJ-label of the parent of y, and H the WSJ-label of the head-child of y.",
        "Our probabilities are: PH(tIA,Cn) ,: PH(tIA,P),"
      ]
    },
    {
      "heading": "3 Implementation issues",
      "text": [
        "Sections 02-21 WSJ Penn Treebank (Marcus et al., 1993) (release 2) are used for training and section 23 is held-out for testing (we tune on section 24).",
        "The parser-output is evaluated by \"evalb\" 5, on the PAR-SEVAL measures (Black et al., 1991) comparing a proposed parse P with the corresponding treebank parse T on Labeled Recall (LR = number of correct constituents in P) Labeled Pre-number of constituents in T cision (LP – number of correct constituents in P) number of constituents in P and Crossing Brackets (CB = number of constituents in P that violate constituent boundaries in T)."
      ]
    },
    {
      "heading": "T-gram extraction: The number of T-grams",
      "text": [
        "is limited by setting constraints on their form much like n-grams.",
        "One upperbound is set on the depth6 (d), a second on the number of children of every node (b), a third on the",
        "sum of the number of nonterminal leafs with the number of (left/right) open-nodes (n), and a fourth (w) on the number of words in a T-gram.",
        "Also, a threshold is set on the frequency (f) of the T-gram.",
        "In the experiments n < 4, w < 3 and f > 5 are fixed while d changes.",
        "Unknown words and smoothing: We did not smooth the relative frequencies.",
        "Similar to (Collins, 1997), every word occurring less than 5 times in the training-set was renamed to CAP+UNKNOWN+SUFF, where CAP is 1 if its first-letter is capitalized and 0 otherwise, and SUFF is its suffix.",
        "Unknown words in the input are renamed this way before parsing starts.",
        "Tagging and parsing: An input word is tagged with all POS-tags with which it cooccurred in the training treebank.",
        "The parser is a two-pass CKY parser: the first pass employs T-grams that fulfill d = 1 in order to keep the parse-space under control before the second-pass employs the full Tree-gram model for selecting the MPD."
      ]
    },
    {
      "heading": "4 Empirical results",
      "text": [
        "First we review the lexical-conditionings in previous work (other important conditionings are not discussed for space reasons).",
        "Magerman95 (Magerman, 1995; Jelinek et al., 1994) grows a decision-tree to estimate P(TIS) through a history-based approach which conditions on actual-words.",
        "Charniak (Charniak, 1997) presents lexicalizations of SCFGs: the Minimal model conditions SCFG rule generation on the headword of its left-hand side, while Charniak97 further con",
        "ditions the generation of every constituent's headword on the headword of its parent-constituent, effectively using bilexical dependencies.",
        "Collins97 (Collins, 1997) uses a bilexicalized 0th-order Markov Grammar: a lexicalized CFG rule is generated by projecting the head-child first followed by every left and right dependent, conditioning these steps on the headword of the constituent.",
        "Collins97 extends this scheme to deal with subcat frames, adjacency, traces and wh-movement.",
        "Charniak99 conditions lexically as Collins does but also exploits up to 3,d-order Markov processes for generating dependents.",
        "Except for T-grams and SCFGs, all systems smooth the relative frequencies with much care.",
        "Sentences < 40 words (including punctuation) in section 23 were parsed by various T-gram systems.",
        "Table 1 shows the results of some systems including ours.",
        "Systems conditioning mostly on lexical information are contrasted to SCFGs and T-grams.",
        "Our result shows that T-grams improve on SCFGs but fall short of the best lexical-dependency systems.",
        "Being 10-12% better than SCFGs, comparable with the Minimal model and Magerman95 and about 7.0% worse than the best system, it is fair to say that (depth 5) T-grams perform more like bilexicalized dependency systems than bare SCFGs.",
        "Table 2 exhibits results of various T-gram systems.",
        "Columns 1-2 exhibit the traditional DOP observation about the effect of the size of subtrees/T-grams on performance.",
        "Columns 3-5 are more interesting: they show that even when T-gram size is kept fixed, systems that are pre-head enriched improve on systems that are not pre-head enriched (OPH).",
        "This is supported by the result of column 1 in contrast to SCFG and Collins97 (table 1): the D1 T-gram system differs from Collins97 almost only in pre-head vs. head enrichment and indeed performs midway between SCFG and Collins97.",
        "This all suggests that allowing bilexical dependencies in T-gram models should improve performance.",
        "It is noteworthy that pre-head enriched systems are also more efficient in time and space.",
        "Column 6 shows that adding Markovian conditioning to subcat frames further improves performance suggesting that further study of the conditional probabilities of dependent T-grams is necessary.",
        "Now for any node in a gold / proposed parse, let node-height be the average path-length to a word dominated by that node.",
        "We set a threshold on node-height in the gold and proposed parses and observe performance.",
        "Figure 7 plots the F-score (2*LP*LR)/(LP+LR) against node-height threshold.",
        "Clearly, performance degrades as the nodes get further from the words while pre-heads improve performance."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "We started this paper wondering about the merits of structural-relations.",
        "We presented the T-gram model and exhibited empirical evidence for the usefulness as well as the shortcomings of structural relations.",
        "We also provided evidence for the gains from enrichment of structural relations with semi-lexical information.",
        "In our quest for better modeling, we still need to explore how structural-relations and bilexical dependencies can be combined.",
        "Probability estimation, smoothing and efficient implementations need special attention.",
        "Acknowledgements: I thank Remko Scha, Remo Bonnema, Walter Daelemans and Jakub Zavrel for illuminating discussions, Remko Bonnema for his pre and post-parsing software, and the anonymous reviewers for their comments.",
        "This work is supported by the Netherlands Organization for Scientific Research."
      ]
    }
  ]
}
