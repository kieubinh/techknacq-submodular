{
  "info": {
    "authors": [
      "Sang-Zoo Lee",
      "Jun'ichi Tsujii",
      "Hae-Chang Rim"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P00-1034",
    "title": "Part-Of-Speech Tagging Based on Hidden Markov Model Assuming Joint Independence",
    "url": "https://aclweb.org/anthology/P00-1034",
    "year": 2000
  },
  "references": [
    "acl-C94-1027",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper we present part-of-speech taggers based on hidden Markov models, which adopt a less strict Markov assumption to consider rich contexts.",
        "In models whose parameters are very specific like lexicalized ones, sparse-data problem is very serious and also conditional probabilities tend to be estimated unreliably.",
        "To overcome data-sparseness, a simplified version of the well-known back-off smoothing method is used.",
        "To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability.",
        "In experiments for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Part-of-speech (POS) tagging can be defined as a process in which a proper POS tag is assigned to each word in texts and so it can be viewed as a classification problem (Mitchell, 1997).",
        "Over a decade, many works for POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Char-niak et al., 1993), a maximum entropy model (Ratnaparkhi, 1996), transformation rules (Brill, 1994), a decision tree (Lee et al., 1999), relaxation labeling (Padro, 1996), Bayesian inference (Samuelsson, 1993), discriminative learning (Lin, 1992), a neural network (Schmid, 1994), and so on.",
        "In this paper we propose hidden Markov models for part-of-speech tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts.",
        "Because such models have a large number of parameters, they must suffer from sparse-data problem unless they have an enough volume of training corpus.",
        "Moreover, because such models assume conditional independence, the probability estimates of their parameters may have statistically different reliability that depends on the number of samples of conditional terms.",
        "To overcome the first problem, a simplified version of the well-known back-off smoothing method is used.",
        "To mitigate unreliable estimation problem, our models assume joint independence between random variables instead of conditional independence because joint probabilities have the same degree of estimation reliability.",
        "2 HMM-based POS tagging Figure 1 shows a lattice structure of an English sentence, \"Flies like a flower.",
        "\", where each node has a word and its POS tag and where the sequence connected by bold lines indicates the most likely sequence."
      ]
    },
    {
      "heading": "2.1 Standard model",
      "text": [
        "We basically follow the notation of (Char-niak et al., 1993) to describe Bayesian models for POS tagging.",
        "In this paper, we assume that {wl, w2, ... , wW} is a set of",
        "words, {t', t2, ... , trI is a set of POS tags, a sequence of random variables Wl,,,, _ Wl W2 ... W,,, is a sentence of n words, and a sequence of random variables Tl,,,, _ Tl T2 ... T,,, is a sequence of n POS tags.",
        "Because each of random variables W can take as its value any of the words in the vocabulary, we denote the value of Wi by wi and a particular sequence of values for Wij (i G j) by wij.",
        "In a similar way, we denote the value of Ti by ti and a particular sequence of values for Tij (i G j) by ti,j.",
        "For generality, terms wij and ti,j (i > j) are defined as being empty.",
        "The purpose of Bayesian models for POS tagging is to find the most likely sequence of POS tags for a given sequence of words, as follows:",
        "Eqn.",
        "1 becomes Eqn.",
        "2 because reference to the random variables themselves can be omitted.",
        "Eqn.",
        "2 is then transformed into Eqn.",
        "3 since Pr(wl,n) is constant for all tl,n.",
        "Then, the probability Pr(tl,n, wi,n) is broken down into Eqn.",
        "4 by using the chain rule.",
        "The standard HMM simplifies them by making the following two strict Markov assumption (conditional independence), Eqn.",
        "5 and Eqn.",
        "6, to get a more tractable form,",
        "The standard HMM assumes that the probability of a current tag ti conditionally depends on only the previous K tags ti_x,i_1 and that the probability of a current word wi conditionally depends on only the current tag'.",
        "In the standard model (K=1), for example, the probability of a node \"a/AT\" of the most likely sequence in Figure 1 is calculated as follows:",
        "Generally, the standard HMM has a limitation that it can not solve complicated ambiguities because it does not consider rich contexts.",
        "To overcome this limitation, the standard HMM should be extended so that it can consult rich information in contexts."
      ]
    },
    {
      "heading": "2.2 Extended models",
      "text": [
        "An extended HMM, A(T(K J), W(Lj) ), for POS tagging is defined by making the following two less strict Markov assumption, Eqn.",
        "8 and Eqn.",
        "9, as follows:",
        "In a model A(T(K J),W(Lj)), the probability of the current tag ti conditionally depends on",
        "both the previous K tags ti_K,ti_I and the previous J words wti_Jti_1 and the probability of the current word wi conditionally depends on the current tag and the previous L tags ti_L,i and the previous I words wi_I,2_1.",
        "In experiments, we set K as 1 or 2, J as 0 or K, L as 1 or 2, and I as 0 or L. If J and I are zero, the above models are non-lexicalized models.",
        "Otherwise, they are lexicalized models.",
        "In an extended model A(T(2,2) , W(2,2)) , for example, the probability of a node \"a/AT\" of the most likely sequence in Figure 1 is calculated as follows: Pr(AT NNS, VB, Flies, like) x Pr(a AT, NNS, VB, Flies, like)"
      ]
    },
    {
      "heading": "3 Parameter estimation",
      "text": [
        "Because the extended models have a large number of parameters, they must suffer from both sparse-data problem and unreliable estimation problem.",
        "The models adopt a simplified back-off smoothing technique as a solution to the first problem, and joint independence assumption as a solution to the second."
      ]
    },
    {
      "heading": "3.1 Simplified back-off smoothing",
      "text": [
        "In supervised learning, the simpliest parameter estimation is the maximum like-lihood(ML) estimation(Duda et al., 1973) which maximizes the probability of a training set.",
        "The ML estimate of tag (K+1)-gram probability, PrML(ti I ti-K,i-l), is calculated as follows:",
        "where the function Fq(x) returns the frequency of x in the training set.",
        "When using the ML estimation, data sparseness is even more serious in the extended models than in the standard models because the former has even more parameters than the latter.",
        "(Chen, 1996), where various smoothing techniques was tested for a language model by using the perplexity measure, reported that a back-off smoothing(Katz, 1987) performs better on a small Craning set than other methods.",
        "In the back-off smoothing, the smoothed probability of tag (K+1)-gram PrsBo(ti",
        "In the equation above, nr denotes the number of (K+1)-gram whose frequency is r, and the coefficient dr is called the discount ratio, which reflects the Good-Turing es-timate(Good, 1953)2.",
        "Eqn.",
        "12 says that",
        "value of the function a(ti_K,2_1) of its conditional term ti_K,2_l, if r = 0.",
        "However, because Eqn.",
        "12 requires complicated computation in a(ti_K,i_l), we simplify it to get a function of the frequency of a conditional term, as follows:",
        "where",
        "In Eqn.",
        "13, the range of f is bucketed into 7 regions such as f = 0, 1, 2, 3, 4, 5 and f > 6 since it is also difficult to compute this equation for all possible values of f. Using the formalism of our simplified back-off smoothing, each of probabilities whose ML estimate is zero is backed off by its corresponding smoothing term.",
        "In experiments, the smoothing terms of PrsBO(ti I",
        "In the equations above, the unigram probabilities are calculated by using an additive smoothing with 6 = 10-2 which is chosen through experiments.",
        "The equation for the additive smoothing (Chen, 1996) is as follows:"
      ]
    },
    {
      "heading": "3.2 Joint independence",
      "text": [
        "The parameters of an HMM may have different degree of statistical reliability because parameter reliability depends on the frequency of conditional term.",
        "For example, let a corpus consist of 1 million words and let the following parameters be extracted from the corpus by using the maximum likelihood estimation.",
        "In this case, three conditional probabilities, Pr(d I a), Pr(d I b), and Pr(d I c) are all 0.1 but Pr(d I a) is statistically more reliable than others because its sample size (10,000 words = 1 million x Pr(a)) is bigger than others.",
        "Actually, this phenomenon is very serious in extended models, even though parameters of the models are seen in the training corpus.",
        "To consider such statistical reliability of a probability estimate, we introduce the concept of weighting Markov assumption, as fol",
        "If the probability function, Pr, is used as the weight function, W, the equations above become equations assuming joint independence between random variables as follows:",
        "The equations above assume that the probability of the current tag ti jointly depends on both the previous K tags ti_K,2_1 and the previous J words wi_J2_1 and that the probability of the current word wi jointly depends on the current tag and the previous L tags ti_L i and the previous I words wi_I 2_1.",
        "If a Bayesian model assumes joint independence, we call it a joint independence model (JIM).",
        "Actually, using the probability function as the weight function is mathematically incorrect and implausible.",
        "For example, while the sum of probabilities of all sentences with the same length becomes 1.0 in an HMM, it becomes naturally less than 1.0 in a JIM.",
        "Therefore, JIMs should not be used in calculating the probability of a sentence.",
        "However, if we want to find the most likely sequence for each sentence and the joint probability of each parameter is regarded as a score, JIMs have no problem.",
        "By replacing corresponding parameters, an extended HMM can be transformed into the corresponding JIM, which is defined as follows: '(T(K,J), W(L,I)) �-- Pr(tl,n, wl,n)",
        "In an extended JIM, 'D (T(2,2), W(2,2)), for example, the probability of a node \"a/AT\" of",
        "the most likely sequence in Figure 1 is calculated as follows: Pr(AT, NNS, VB, Flies, like) x Pr(a, AT, NNS, VB, Flies, like) The parameters of a JIM are estimated by using the parameters of the corresponding HMM as follows:"
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "For experiments, we used the Brown corpus which consists of 1,113,180 words and 53,885 sentences and is tagged with 82 POS tags3.",
        "It was segmented into two parts, the training set of 90% and the test set of 10%, in the way that each sentence in the test set was extracted from every 10 sentence.",
        "In the same way, we made 10-fold data set for 10-fold cross validation.",
        "In order to assign all possible tags to each word, we made two assumption: closed vocabulary assumption and open vocabulary assumption.",
        "For closed vocabulary assumption, we looked up a dictionary tailored to the Brown corpus.",
        "In this case, the average number of tags per word became 1.64.",
        "For open vocabulary assumption, we looked up a dictionary tailored only to a training set in order to assign possible tags to frequent words whose frequency is greater than 5.",
        "In case of rare words, tags in the dictionary were assigned and then 6 tags with highest score were assigned by using a naive Bayesian classifier (Mitchell, 1997) considering character features as follows:",
        "3 Note that some sentences, which have composite tags(such as \"HV+TO\" in \"hafts\"), \"ILLEGAL\" tag, or \"NIL\" tag, were removed from the Brown corpus and tags with \"*\" (not) such as \"BEZ*\" were replaced by corresponding tags without \"*\" such as `BEZ\".",
        "where f2 indicates j-th character features of wi and F(=12) is the number of character feature types including prefixes (whose length is 1 through 4), suffixes (whose length is 1 through 4), if wi contains numbers, if wi contains an initial uppercase letter, if wi contains any non-initial uppercase letter, if wi contains hyphens.",
        "In this case, the average number of tags per word became 2.00 and the rate of words that have the correct tag among all assigned tags became 99.85%.",
        "Figure 2 illustrates graphs showing the average accuracy rates of HMMs and JIMs under the closed vocabulary assumption.",
        "Here, labels in the x-axis specify models in the way that L,1 denotes A(T(K,J),W(L,1)) or `b(T(K,J),W(L,1)).",
        "The models are arranged by the ascending order of theoretical number of parameters.",
        "The first two models are standard models and the others are extended models.",
        "The average accuracy rates beyond the range of each graph are just below the figure.",
        "In this figure, we can observe that the simplified back-off smoothing technique mitigates sparse-data problems in both HMMs and JIMs.",
        "As expected, JIMs achieves higher accuracy than the corresponding HMMs in some extended models consulting rich contexts.",
        "It is statistically significant with confidence 99that the model, (b(T(22), W(11)) (98.05%), is better than any other models including the standard bigram HMM, A(T(1,o) , W(0,0)) (97.27%) and the best HMM, A(T(1,1), W(1,o)) (97.93%).",
        "Figure 3 depicts graphs indicating the average accuracy rates of HMMs and JIMs under the open vocabulary assumption.",
        "Unlike",
        "the best accuracy rate (96.86%) with confidence 99%."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We have presented the extended HMMs for English POS tagging, which can consider rich",
        "information in contexts.",
        "In the models, a simplified version of back-off smoothing is used to mitigate data sparseness problem.",
        "The models assume joint independence between random variables in order to make the parameter estimation more reliable.",
        "From the experiments, we have observed that extended models achieved even better results than the standard models in case of both HMMs and JIMs, that the simplified back-off smoothing technique mitigated data sparseness quite effectively, and that some extended JIMs outperformed the corresponding HMMs.",
        "Under the closed vocabulary assumption, the best JIM outperformed the best HMM.",
        "On the contrary, under the open vocabulary assumption, the best HMM outperformed the best JIM.",
        "Intuitively speaking, it is empirically proven that the joint independence assumption is more effective than the Markov assumption in some models that consult specific features such as lexicalized ones.",
        "Generally, the uniform extension of models requires rapid increase of parameters, and hence suffers from large storage and sparse data.",
        "Recently in many areas where HMMs are used, many efforts to extend models non-uniformly have been made, sometimes resulting in noticeable improvement.",
        "For this reason, we are trying to transform our uniform models into non-uniform models, which may be more effective in terms of both space complexity and reliable estimation of paremeters, without loss of accuracy.",
        "And also, we are trying to apply our models to different areas such as information extraction in the bio-molecular domain, noun phrase chuncking, and so on."
      ]
    }
  ]
}
