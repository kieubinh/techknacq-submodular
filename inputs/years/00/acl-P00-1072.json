{
  "info": {
    "authors": [
      "Kilyoun Kim",
      "Key-Sun Choi"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P00-1072",
    "title": "Dimension-Reduced Estimation of Word Co-Occurrence Probability",
    "url": "https://aclweb.org/anthology/P00-1072",
    "year": 2000
  },
  "references": [
    "acl-P99-1004",
    "acl-P99-1005"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We investigate a novel approach to solve the problem of sparse data through dimension reduction.",
        "Linear algebraic technique called LSA/SVD is used to find co-relationships of sparse words.",
        "Three variant estimation methods are suggested and they are evaluated for estimating unseen noun-verb co-occurrence probability.",
        "The model shows possibility to be alternative probability smoothing method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "One of the most suffering difficulties in statistical language processing is so-called data sparseness problem.",
        "No matter how large the training set is, a substantial portion of the data is unseen.",
        "For them, the Maximum Likelihood Estimation (MLE) probabilities are zero and these zeros give us bad result all through the statistical process.",
        "We are interested in P(x, y) and the prediction task P(ylx), that is a bigram language modeling of word co-occurrences.",
        "P(ylx) is the conditional probability that a pair has second element y E Y given that its first element is x E X.",
        "In other words, P(ylx) can be regarded as a measure of relationship between word x and y.",
        "For example, for a given object x = beer, a verb y = drink is more related than a verb y = eat, p(drinkl beer) >> p(eatlbeer).",
        "Many features can be used to predict a relationship between two words, but we assume here that the only information we have are the frequencies.",
        "To overcome the difficulty of sparse data, a smoothing technique like Good-Turing method is widely used.",
        "Estimator combining approaches such as linear interpolation and Katz's back-off method are popular also(Katz, 1987).",
        "They use unigram probability P(y) to estimate bigram probability P(ylx) for unseen data pair, disregarding the relationship between two words.",
        "If unseen bi-grams are made up of unigrams of the same frequency, the methods give them the same probability, causing a problem to estimate accurate probability.",
        "In addition to the classical methods, similarity-based schemes are successfully applied to data sparseness problem.",
        "The nearest-neighbors similarity-based method uses a set of k most similar words x' to estimate conditional probability P(ylx), being said to perform almost 40% better than back-off (Dagan et al., 1999).",
        "They use various distributional similarity measures to find similarity between words such as KL-divergence or JS-divergence (Lee, 1999).",
        "For the sparse word, however, the distribution P(ylx) itself is sparse and it is difficult to find correct similarity between words, since the only means for measuring word similarity is the frequency.",
        "The more sparse the distribution of word is, the more difficult finding acceptable similarities between words.",
        "In this paper, we investigate a novel approach to solve the problem of sparse data by capturing their latent relationships with only frequency information.",
        "Through reducing dimension by linear algebraic technique LSA/SVD1, we can eliminate zero values in",
        "p(ylx) as well as we can capture relationships between words.",
        "We believe that the dimension-reduced estimation model can be alternative probability smoothing method.",
        "The model consists of three parts: making a conditional probability matrix, projecting the matrix into lower space, and estimating probabilities on reduced space.",
        "In the third part, three variant estimating methods are suggested and they are compared with Katz's back-off method and simplified nearest-neighbor similarity-based method.",
        "We evaluated the methods in a pseudo wrod sense disambiguation task.",
        "and made promising result.",
        "Futher evaluation is needed on more realistic task, though.",
        "The optimal dimension size of subspace is also investigated, showing the best result between 90 – 200, about 10% of the original dimension size.",
        "Finally, we show that the model does not degrade performance as the sparseness increases."
      ]
    },
    {
      "heading": "2 Dimension-Reduced Model",
      "text": [
        "Dimension-reduced model uses linear algebraic technique called LSA/SVD, which projects a matrix into reduced space.",
        "First of all, to apply linear algebraic technique, we need to represent conditional probability P(glx) as a matrix form (Section 2.1).",
        "After that, we project the matrix into lower dimension subspace through SVD.",
        "We will show how the resultant space represents relationship between the given word x and the predicting word g well (Section 2.2).",
        "At last, we suggest three probability estimation methods on reduced space (Section 2.3)."
      ]
    },
    {
      "heading": "2.1 Conditional Probability Matrix",
      "text": [
        "Any discrete conditional probability distributions can be represented by a matrix form.",
        "For a distribution p(ylx), given words x E X make up row entries and predicting words g E y make up column entries.",
        "Each element of matrix has estimated conditional probability value of two words p(ylx).",
        "We define conditional probability matrix and the row, col",
        "In the table, the noun \"coffee\" and 'beer\" does not co-occur with the same verb and it is difficult to find similarity between them in this space.",
        "To find their latent relationship, we can project each row and column vector into lower dimension space through latent semantic anaylsis.",
        "Table 1 shows an example of MLE estimating matrix.",
        "The task is predicting the main verb with given object, that is estimating noun and verb co-occurrence probability p(vl n) noun can be regarded as a point or a vector in multidimensional space of which a dimension size equal to IV I.",
        "INIV swigsipdrinkdevour eat swallowl �beer � �whiskey 0.33 0 0.33 0.33 0 0 coffee � bread 0 0.5 0.5 0 0 0 sugar � 0 1 0 0 0 0 0 0 0 0.33 0.33 0.33 0 0 0 0 0.5 0.5"
      ]
    },
    {
      "heading": "2.2 Projection - Latent Semantic Analysis",
      "text": [
        "Latent Semantic Analysis (LSA) is known as a theory for extracting and representing the contextual-usage meaning of words.",
        "LSA uses singular value decomposition (SVD).",
        "It has been widely used in information retrieval task as a variant of the vector space model(Deerwester et al., 1990) (Dumais et al., 1997).",
        "Given the conditional probability matrix A and rank(A) =r, the SVD of A and the rank-i < m, where n E N, v E V. Note that each",
        "where U and V contains left and right singular vectors of A, respectively, and the E _ diag(ai, • • • , a,,,) is the diagonal matrix of singular values of A. Truncated SVD Ak, which is constructed from the k-largest signu-lar triples of A, is the closest rank-k matrix to A 2.",
        "The left singular vector u2 and the right singular vector v2 corresponds to the row vector �i and the column vector fii, respectively.",
        "By taking k elements of u2 and vi, each given word x and predicting word g of P(yIx) is represented as a vector in the reduced k-space.",
        "Figure 1 is an example of SVD on the noun-verb conditional probability matrix of Table 1.",
        "In Figure 1-D, both noun x and verb g are represented by a vector in two dimensional space.",
        "Nouns which occur with similar verbs are grouped each other even if they never co-occur with the same verb (diml: beverages, dim2: foods).",
        "For example, noun\" coffee\" and verb \"beer\" do not co-occur with the same verb in the original matrix (Table 1); however they are near in two dimensional space when measured with a cosine distance.",
        "This means that unseen word pairs (x, g) which do not co-occur in the training data may none the less be near in reduced k-space.",
        "This derived representation which captures word(x)-word(g) associations is used for estimating probabilities of unseen data."
      ]
    },
    {
      "heading": "2.3 Estimating Probabilities on Reduced Space",
      "text": [
        "Until now, we constructed word co-occurrence probability matrix and projected the matrix into lower dimension space.",
        "Now, we suggest",
        "space is chosen such that the representations in the original space are changed as little as possible when measured by the sum of the squares of the differences.",
        "One can prove that Ak is the best approximation to A for any unitray invariant norm(Michael W. Berry and Jessup, 1999) three variant probability estimation methods in dimension-reduced space.",
        "First is estimating p(yIx) by computing distance between given word x and predicting word g in reduced space.",
        "Second, we can use rank-k approximation matrix.",
        "Third, the state-of-the-art similarity-based methods can be merged to our dimension-reduced model.",
        "Because the first two methods are not based on statistical theory, it should be explored"
      ]
    },
    {
      "heading": "2.3.1 Method 1: Distance-based",
      "text": [
        "method Through LSA, the matrix A is factored into the product of three matrices as in Equation 3, and u2 and ir, are considered as the row vector �i and the column vector #j in k-dimension subspace respectively.",
        "Figure 1-D shows 2-dimensional plot of resultant U2, V2 matrix.",
        "The distance-based method use normalized distance between u2 and v� for estimating probability P(gj I xi):",
        "where Zk is normalizing factor and Dk is a cosine distnace in k-dimensional space."
      ]
    },
    {
      "heading": "2.3.2 Method 2: Rank-k",
      "text": [
        "approximation matrix method In LSA, we can create a rank-k approximation matrix Ak to the matrix A by setting all but the k largest singular values of A equal to zero (Equation 4).",
        "In this method, we consider each element of a rank-k approximation matrix Ak as probability distribution of p(ylx) (Figure 1-C).",
        "To satisfy the requirements Ep(ylx) = 1 and p(ylx) > 0, we use the following normalizing equation:",
        "where Z(n) is normalizing factor and 6 is a smoothing constant.",
        "The similarity-based method(Dagan et al., 1999) and dimension reduction technique can be merged into one model 3.",
        "Reduced dimension can be better representation space than the original space for finding similarities between words.",
        "This approach finds the most k nearest words to x in reduced space and use these word to estimate the probability p(ylx):",
        "where S = {x'j cos (YZ,YZ') > 0 on k dim.",
        "}, the k is the reduced dimension size not a count of nearest nouns.",
        "The count of nearest nouns are determined by 0, which is threshold of cosine value.",
        "where W (x, x') is a similarity measure derived from the dissimilarity measure JS divergence.",
        "S(n, k) is the set of k words with the smallest JS-divergence to"
      ]
    },
    {
      "heading": "3 An Illustrative Example",
      "text": [
        "Here we use a concrete example to illustrate effectiveness of our model.",
        "The example is based on Table 1 and the task is estimating noun and verb co-occurrence probability p(vin).",
        "There are two groups of words, beverage-related words([beer, whieskey, coffee]/N, swig, sip, drink]/V) and food-related words ([bread,sugar//N, devour, cat, swallow]/V ).",
        "In Table 1, \"coffee\" is a sparsely distributed noun and we expect P(drinklcof fee) > P (cat Icof fee).",
        "sible to rank two probabilities since they are all 0.",
        "Katz's back-off also fails to distinguish them since unigram probabilities p(drink) = p(eat) = 0.18.",
        "In the similarity-based scheme we compute JS-divergence to find similarity between nouns and JS(p(vjbeer),p(vjcof fee)) 4 = JS(p(vjbread),p(vjcof fee)) = 0.6931, which does not discriminate \"beer\" and \"bread\".",
        "The distance-based model, however, solves all these problem.",
        "When we observe the third row in Figure 1-C, that is pre-normalized p(vlcof fee), there are no zero values unlike MLE.",
        "Futhermore, we can end up with two groups of verbs: beverage-related verbs have positive values A2(8ip, coffee), A2 (drink, co f ee), A2 (swig, coffee) > 0 and food-related verbs have negative values",
        "With MLE, it is not pos-To concrete our example, p(vlcof fee) is constructed in Table 2 using probability estimation functions as described in the above section.",
        "MLE shows five zero values, causing data sparseness problems.",
        "Katz' back-off methods and similarity-based method cannot distinguish food-related verbs and drink-related verbs.",
        "In contrast, all dimension-reduced models resolve data sparseness problem and they cluster nouns and co-occurrence verbs reasonably.",
        "Thus, we can expect that dimension-reduced model will show promising result in a real experiment."
      ]
    },
    {
      "heading": "4 Experiment",
      "text": [
        "We evaluated the dimension-reduced models on a pseudo word sense disambiguation task as in (Dagan et al., 1999).",
        "Each method is presented with a noun and two verbs, deciding which verb is more likely to have the noun as a direct object.",
        "Data preparation method and error counting scheme are almost similar to that of similarity-based methods (Dagan et al., 1999)(Lee and Pereira, 1999).",
        "Performance is measured by the error rate, defined as error rate = T (# of incorrect choices) where T is the size of test set.",
        "Test instances consist of noun-verb-verb triples (n, vl, v2), where both (n, v1) and (n, v2) are both unseen in the training set.",
        "(n, vI) is selected such that it appeared at least twice as often than (n,v2) in the original verb-object pairs and p(n, v1) > p(n, v2) is a correct answer.",
        "In addition, to consider Katzs back-off method as the baseline, v2 is choosed as f rq(vl) < f rq(v2) <=> p(vl) < p(v2), and the error rate of back-off method is always 100% 5 .",
        "Running method is threefold cross-validation and all results are averages over the three test sets.",
        "5Katz's back-off estimator is defiend as the following egaution.",
        "We set a(xi) = 1 here.",
        "pbo \\�2 Iii) -_ ) Pd(yjlxii) frq(xi, y2) > Q",
        "For similarity-based method, the parameter tuning is important to improve performance but we use the simplified unweighted average equation as in (Lee and Pereira, 1999) 6 .",
        "Since this equation is the same as our estimation method in Section 2.3.3, we can say that the comparison is fair.",
        "Number of similar nouns k is determined such that shows best result on test set."
      ]
    },
    {
      "heading": "4.1 Data Preparation",
      "text": [
        "We prepared test sets as follows:",
        "1.",
        "Extract transitive verb and head noun pairs from Penn Treebank II.",
        "2.",
        "Select the pairs for the 1,000 most frequent nouns.",
        "3.",
        "Partition the selected pairs 70% for training set and 30% for test set.",
        "(3 fold).",
        "4.",
        "For each test set, (a) remove seen pairs.",
        "(b) for each (n, vl), create (n, vl, v2)",
        "such that f rq(n, v1) > 2 * f rq(n, v2) and f rq(vl) < f rq(v2).",
        "Step 2 makes p(vin) matrix size fixed.",
        "Since it is difficult to find (n, vI, v2) triples that satisfy Step 4-(b) criteria, average test set size is small.",
        "Hence, we used relative large portion, 30% of the pairs for building test set.",
        "Table 3 summarizes the experiment data."
      ]
    },
    {
      "heading": "4.2 Result",
      "text": [
        "Table 4 shows the experimental error rate on the three test sets, using Katz's back-off as the baseline.",
        "Two dimension-reduced methods show much better performance than other methods.",
        "The reason of such a good performance is that our model tries to find similarities between words toward two side (column and row space).",
        "The state-of-the-art similarity-based methods find similarities toward only one side.",
        "For example, their well-known similarity measures JS-divergence between given word x2 and another word x2 is defined as JS(p(yjx2),p(yjx2)).",
        "It means that each row",
        "performed.",
        "That's why the similarity-based fail to grasp true relationships on word co-occurrences.",
        "On the other hand, SVD which is the mathematical background of our model gives us a reduced-rank basis for both column space and the row space simultaneously (Figure 1) (Michael W. Berry and Jessup, 1999).",
        "As we showed in the previous example in Section 3, the dimension-reduced model extract underlying or latent structures of word co-occurrences well.",
        "Therefore, our model shows successful result on estimating word co-occurrence probabilities of sparse data.",
        "futher However, the experiment is artificial and SVD is not directly related to the probabilty theory, futher theoretical invetigation is required."
      ]
    },
    {
      "heading": "4.3 Optimal subspace and Degree of",
      "text": [
        "sparseness We also investigated the change of the performances as subspace size and degree of sparseness vary.",
        "Figure 2 shows performances of distance-based DR model as the dimension of subspace increase.",
        "When a dimension size is between 90 and 200, it shows the best result.",
        "Thus, we can conclude that the subspace of",
        "small dimension size (90 « 1000) is sufficent to capture latent word co-occurrence relationship.",
        "Figure 3 shows the effect of the degree of sparseness.",
        "The 1st ranked noun appears the most frequent times and 1000th ranked noun appears the least frequent times, in the training set.",
        "The average error rate does not change much as the sparseness increases.",
        "Therefore it is plausible to say that the dimension-reduced model does not show performance degration on very sparse data."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We proposed a novel approach called dimension-reduced estimation model for dealing with data sparseness problem.",
        "Three variant models are suggested and they are compared the performance against Katz's back-off method and similarity-based scheme.",
        "Dimension-reduced model can be alternative",
        "probability smoothing scheme.",
        "The ability of LSA that extracts and infers latent relations of words makes it possible to estimate probabilities of sparse data reasonably.",
        "LSA is a fully automatic mathematical technique.",
        "If we make a matrix from any given information once, we can use the reduced matrix for estimating probability.",
        "While the SVD analysis is somewhat costly in terms of time for large matrix, less expensive alternatives such as folding-in and SVD-updating have been suggested (Michael W. Berry and Jessup, 1999).",
        "Further investigation is needed in both theoretical and experimental side.",
        "The suggested model does not have deep background over probablity theory.",
        "Hopefully, (Hofmann, 1999) suggested probabilistic LSI which is based on a statitical latent class model for factor analysis of count data.",
        "In addition, we applied our model to estimate bigram probabilities only.",
        "Corpus-based NLP is so mature and the methods must be tested with more realistic tasks.",
        "Since any conditional probability distributions can be represented by a matrix form, we can combines other information in a matrix, applying our model to more general tasks, such as word sense disambiguation and word clustering."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "This work was supported by KOSEF through the \"Multilingual Information Retrieval\" project at the AITrc and was supported by Ministry of Culture and Tourism under the program of King Sejong Project through KORTERM.",
        "Many fundamental researches was supported by the R&D fund of Ministry of Science and Technology under a project of plan STEP2000."
      ]
    }
  ]
}
