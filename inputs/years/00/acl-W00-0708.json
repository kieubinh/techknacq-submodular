{
  "info": {
    "authors": [
      "Guido Minnen",
      "Francis Bond",
      "Ann Copestake"
    ],
    "book": "Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop CoNLL and LLL",
    "id": "acl-W00-0708",
    "title": "Memory-Based Learning for Article Generation",
    "url": "https://aclweb.org/anthology/W00-0708",
    "year": 2000
  },
  "references": [
    "acl-C90-2023",
    "acl-C94-1002",
    "acl-P97-1056",
    "acl-P98-1085",
    "acl-W00-1427",
    "acl-W97-0506"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Article choice can pose difficult problems in applications such as machine translation and automated summarization.",
        "In this paper, we investigate the use of corpus data to collect statistical generalizations about article use in English in order to be able to generate articles automatically to supplement a symbolic generator.",
        "We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) which predicts whether to generate an article with respect to an English base noun phrase.",
        "We discuss competitive results obtained using a variety of lexical, syntactic and semantic features that play an important role in automated article generation."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Article choice can pose difficult problems in natural language applications.",
        "Machine translation (MT) is an example of such an application.",
        "When translating from a source language that lacks articles, such as Japanese or Russian, to one that requires them, such as English or German, the system must somehow generate the source language articles (Bond and Ogura, 1998).",
        "Similarly in automated summarization: when sentences or fragments are combined or reduced, it is possible that the form of a noun phrase (NP) is changed such that a change of the article associated with the NP's head becomes necessary.",
        "For example, consider the sentences A talk will be given on Friday about NLP; The talk will last for one hour which might get summarized as Friday's NLP talk will last one hour.",
        "However, given the input sentences, it is not clear how to decide not to generate an article for the subject NP in the output sentence.",
        "Another important application is in the field known as augmentative and alternative communication (AAC).",
        "In particular, people who have lost the ability to speak sometimes use a text-to-speech generator as a prosthetic device.",
        "But most disabilities which affect speech, such as stroke or amyotrophic lateral sclerosis (ALS or Lou Gehrig's disease), also cause some more general motor impairment, which means that prosthesis users cannot achieve a text input rate comparable to normal typing speeds even if they are able to use a keyboard.",
        "Many have to rely on a slower physical interface (head-stick, head-pointer, eye-tracker etc).",
        "We are attempting to use a range of NLP technology to improve text input speed for such users.",
        "Article choice is particularly important for this application: many AAC users drop articles and resort to a sort of telegraphese, but this causes degradation in comprehension of synthetic speech and contributes to its perception as unnatural and robot-like.",
        "Our particular goal is to be able to use an article generator in conjunction with a symbolic generator for AAC (Copestake, 1997; Carroll et al., 1999).",
        "In this paper we investigate the use of corpus data to collect statistical generalizations about article use in English so as to be able to generate them automatically.",
        "We use data from the Penn Treebank as input to a memory-based learner (TiMBL 3.0; Daelemans et al., 2000) that is used to predict whether to generate the or a/an or no article.' We discuss a variety of lexical, syntactic and semantic features that",
        "play an important role in automated article generation, and compare our results with other re-searchers'.",
        "The paper is structured as follows.",
        "Section 2 relates our work to that of others.",
        "Section 3 introduces the features we use.",
        "Section 4 introduces the learning method we use.",
        "We discuss our results in Section 5 and suggest some directions for future research, then conclude with some final remarks in Section 6."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "There has been considerable research on generating articles in machine translation systems (Gawroriska, 1990; Murata and Nagao, 1993; Bond and Ogura, 1998; Heine, 1998).",
        "These systems use handwritten rules and lexical information to generate articles.",
        "The best cited results, 88% accuracy, are quoted by Heine (1998) which were obtained with respect to a very small corpus of 1,000 sentences in a restricted domain.",
        "Knight and Chander (1994) present an approach that uses decision trees to determine whether to generate the or a/an.",
        "They do not consider the possibility that no article should be generated.",
        "On the basis of a corpus of 400K NP instances derived from the Wall Street Journal, they construct decision trees for the 1,600 most frequent nouns by considering over 30,000 lexical, syntactic and semantic features.",
        "They achieve an accuracy of 81% with respect to these nouns.",
        "By guessing the for the remainder of the nouns, they achieve an overall accuracy of 78%."
      ]
    },
    {
      "heading": "3 Features Determining Automated Article Generation",
      "text": [
        "We have extracted 300K base noun phrases (NPs) from the Penn Treebank Wall Street Journal data (Bies et al., 1995) using the tgrep tool.",
        "The distribution of these NP instances with respect to articles is as follows: the 20.6%, a/an 9.4% and 70.0% with no article.",
        "We experimented with a range of features:",
        "1.",
        "Head of the NP: We consider as the head of the NP the rightmost noun in the NP.",
        "If an NP does not contain a noun, we take the last word in the NP as its head.",
        "2.",
        "Part-of-speech (PoS) tag of the head of the NP: PoS labels were taken from the Penn Treebank.",
        "We list the tags that occurred with",
        "3.",
        "Functional tag of the head of the NP: In the Penn Treebank each syntactic category can be associated with up to four functional tags as listed in Table 2.",
        "We consider the sequence of functional tags associated with the category of the NP as a feature; if a constituent has no functional tag, we give the feature the value NONE.",
        "4.",
        "Category of the constituent embedding the NP: We looked at the category of the embedding constituent.",
        "See Figure 1: The category of the constituent embedding the NP the problem is PP.",
        "5.",
        "Functional tag of the constituent embedding the NP: If the category of the con",
        "stituent embedding the NP is associated with one or more functional tags, they are used as features.",
        "The functional tag of the constituent embedding the problem in Figure 1 is DIR.",
        "6.",
        "Other determiners of the NP: We looked at the presence of a determiner in the NP.",
        "By definition, an NP in the Penn Treebank can only",
        "have one determiner (Bies et al., 1995), so we expect it to be a good predictor of situations where we should not generate an article.",
        "7.",
        "Head countability preferences of the head of the NP: In case the head of an NP is a noun we also use its countability as a feature.",
        "We anticipate that this is a useful feature because singular indefinite countable nouns normally take the article a/n, whereas singular indefinite uncountable nouns normally take no article: a dog vs water.",
        "We looked up the countability from the transfer lexicon used in the Japanese-to-English machine translation system ALT-J/E (Ikehara et al., 1991).",
        "We used six values for the countability feature: FC (fully countable) for nouns that have both singular and plural forms and can be directly modified by numerals and modifiers such as many; UC (uncountable) for nouns that have no plural form and can be modified by much; SC (strongly countable) for nouns that are more often countable than uncountable; WC (weakly countable) for nouns that are more often uncountable than countable; and PT (pluralia tantum) for nouns that only have plural forms, such as for example, scissors (Bond et al., 1994).",
        "Finally, we used the value UNKNOWN if the lexicon did not provide countability information for a noun or if the head of the NP was not a noun.",
        "41.4% of the NP instances received the value UNKNOWN for this feature.",
        "8.",
        "Semantic classes of the head of the NP: If the head of the NP is a noun we also take into account its semantic classification in a large semantic hierarchy.",
        "The underlying idea is that the semantic class of the noun can be used as a way to back off in case of unknown head nouns.",
        "The 2,710 node semantic hierarchy we used was also developed in the context of the ALT-J/E system (Ikehara et al., 1991).",
        "Edges in this hierarchy represent IS-A or HAS-A relationships.",
        "In case the semantic classes associated with two nodes stand in the IS-A relation, the semantic class associated with the node highest in the hierarchy subsumes the semantic class associated with the other node.",
        "Each of the nodes in this part of the hierarchy is represented by a boolean feature which is set to 1 if that node lies on the path from the root of the hierarchy to a particular semantic class.",
        "Thus, for example, the semantic features of a noun in the semantic class organization consists of a vector of 30 features where the features corresponding to the nodes noun, concrete, agent and organization are set to 1 and all other features are set to 0.2"
      ]
    },
    {
      "heading": "4 Memory-based learning",
      "text": [
        "We used the Tilburg memory based learner TiMBL 3.0.1 (Daelemans et al., 2000) to learn from examples for generating articles using the features discussed above.",
        "Memory-based learning reads all training instances into memory and classifies test instances by extrapolating a class from the most similar instance(s) in memory.",
        "Daelemans et al.",
        "(1999) have shown that for typical natural language tasks, this approach has the advantage that it also extrapolates from exceptional and low-frequency instances.",
        "In addition, as a result of automatically weighing features in the similarity function used to determine the class of a test instance, it allows the user to incorporate large",
        "numbers of features from heterogeneous sources: When data is sparse, feature weighing embodies a smoothing-by-similarity effect (Zavrel and Daelemans, 1997)."
      ]
    },
    {
      "heading": "5 Evaluation and Discussion",
      "text": [
        "We tested the features discussed in section 3 with respect to a number of different memory-based learning methods as implemented in the TiMBL system (Daelemans et al., 2000).",
        "We considered two different learning algorithms.",
        "The first, IB1 is a k-nearest neighbour algorithm.3.",
        "This can be used with two different metrics to judge the distance between the examples: overlap and modified value difference metric (MVDM).",
        "TiMBL automatically learns weights for the features, using one of five different weighting methods: no weighting, gain ratio, information gain, chi-squared and shared variance.",
        "The second algorithm, IGTREE, stores examples in a tree which is pruned according to the weightings.",
        "This makes it much faster and of comparable accuracy.",
        "The results for these different methods, for k = 1, 4, 16 are displayed in Table 3.",
        "IB1 is tested with leave-one-out cross-validation, IGTREE with tenfold cross validation.",
        "The best results were (82.6%) for IB1 with the MVDM metric, and either no weighting or weighting by gain ratio.",
        "IGTREE did not perform as well.",
        "We investigated more values of k, from 1 to 200, and found they had little influence on the accuracy results with k = 4 or 5 performing slightly better.",
        "We also tested each of the features described in Section 3 in isolation and then all together.",
        "We used the best performing algorithm from our earlier experiment: IB1 with MVDM, gain ratio and k = 4.",
        "The results of this are given in Table 4.",
        "When interpreting these results it is important to recall the figures provided in Table 1.",
        "The most common article, for any PoS, was no and for many PoS, including pronouns, generating no article is always correct.",
        "There is more variation in NPs headed by common nouns and adjectives, and a little in NPs headed by proper nouns.",
        "Our baseline therefore consists of never",
        "generating an article: this will be right in 70.0% of all cases.",
        "Looking at the figures in Table 4, we see that many of the features investigated did not improve results above the baseline.",
        "Using the head of the NP itself to predict the article gave the best results of any single feature, raising the accuracy to 79.4%.",
        "The functional tag of the head of the NP itself improved results slightly.",
        "The use of the semantic classes (72.1%) clearly improves the results over the baseline thereby indicating that they capture useful generalizations.",
        "The results from testing the features in combination are shown in Table 5.",
        "Interestingly, features which were not useful on their own, proved useful in combination with the head noun.",
        "The most useful features appear to be the category of the embedding constituent (81.1%) and the presence or absence of a determiner (80.9%).",
        "Combining all the features gave an accuracy of 82.9%."
      ]
    },
    {
      "heading": "Feature Accuracy",
      "text": [
        "Our best results (82.6%), which used all features are significantly better than the baseline of generating no articles (70.0%) or using only the head of the NP for training (79.4%).",
        "We",
        "also improve significantly upon earlier results of 78% as reported by Knight and Chander (1994), which in any case is a simpler task since it only involved choice between the and a/an.",
        "Further, our results are competitive with state of the art rule-based systems.",
        "Because different corpora are used to obtain the various results reported in the literature and the problem is often defined differently, detailed comparison is difficult.",
        "However, the accuracy achieved appears to approach the accuracy results achieved with handwritten rules.",
        "In order to test the effect of the size of the training data, we tested used the best performing algorithm from our earlier experiment (IB1 with MVDM, gain ratio and k = 4) on various subsets of the corpus: the first 10%, the first 20%, the first 30% and so on to the whole corpus.",
        "The results are given in Table 6.",
        "The accuracy is still improving even with 300,744 NPs, an even larger corpus should give even better results.",
        "It is important to keep in mind that we, like most other researchers, have been training and testing on a relatively homogeneous corpus.",
        "Furthermore, we took as given information about the number of the NP.",
        "In many applications we will have neither a large amount of homogeneous training data nor information about number."
      ]
    },
    {
      "heading": "5.1 Future Work",
      "text": [
        "In the near future we intend to further extend our approach in various directions.",
        "First, we plan to investigate other lexical and syntactic features that might further improve our results, such as the existence of pre-modifiers like superlative and comparative adjectives, and post-modifiers like prepositional phrases, relative clauses, and so on.",
        "We would also like to investigate the effect of additional discourse-based features such as one that incorporates information about whether the referent of a noun phrase has been mentioned before.",
        "Second, we intend to make sure that the features we are using in training and testing will be available in the applications we consider.",
        "For example, in machine translation, the input noun phrase may be all dogs, whereas the output could be either all dogs or all the dogs.",
        "At present, words such as all, both, half in our input are tagged as predeterminers if there is a following determiner (it can only be the or a possessive), and determiners if there is no article.",
        "To train for a realistic application we need to collapse the determiner and predeterminer inputs together in our training data.",
        "Furthermore, we are interested in training on corpora with less markup, like the British National Corpus (Burnard, 1995) or even no markup at all.",
        "By running a PoS tagger and then an NP chunker, we should be able to get a lot more training data, and thus significantly improve our coverage.",
        "If we can use plain text",
        "to train on, then it will be easier to adapt our tool quickly to new domains, for which there are unlikely to be fully marked up corpora."
      ]
    },
    {
      "heading": "6 Concluding remarks",
      "text": [
        "We described a memory-based approach to automated article generation that uses a variety of lexical, syntactic and semantic features as provided by the Penn Treebank Wall Street Journal data and a large hand-encoded MT dictionary.",
        "With this approach we achieve an accuracy of 82.6%.",
        "We believe that this approach is an encouraging first step towards a statistical device for automated article generation that can be used in a range of applications such as speech prosthesis, machine translation and automated summarization."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors would like to thank the Stanford NLP reading group, the LinGO project at CSLI, Timothy Baldwin, Kevin Knight, Chris Manning, Walter Daelemans and two anonymous reviewers for their helpful comments.",
        "This project is in part supported by the National Science Foundation under grant number IRI-9612682."
      ]
    }
  ]
}
