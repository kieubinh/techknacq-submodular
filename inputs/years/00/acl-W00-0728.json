{
  "info": {
    "authors": [
      "Christer Johansson"
    ],
    "book": "Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop CoNLL and LLL",
    "id": "acl-W00-0728",
    "title": "A Context Sensitive Maximum Likelihood Approach to Chunking",
    "url": "https://aclweb.org/anthology/W00-0728",
    "year": 2000
  },
  "references": [
    "acl-P97-1056",
    "acl-W98-1224"
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": [
        "In Brill's (1994) groundbreaking work on parts-of-speech tagging, the starting point was to assign each word its most common tag.",
        "An extension to this first step is to utilize the lexical context (i.e., words and punctuation) surrounding the word.",
        "This approach could obviously be used for ordering tags into higher order units (referred to as chunks) using chunk labels.",
        "This paper will investigate the performance of simply picking the most likely tag for a given context, under the condition that a larger context is allowed to override the most likely label of a smaller context.",
        "The results could be extended by secondary error correction as in Brill's tagger, but this exercise is left to the reader to allow us to concentrate on the performance based on storing and retrieving the most likely examples only.",
        "More sophisticated methods may use more than one stored context to determine the label that best fits the current context (Van den Bosch and Daelemans, 1998; Zavrel and Daelemans, 1997; Skousen, 1989, inter al.).",
        "The method of this paper uses only one context to determine the best label, but may decrease the size of the context until a full match is found."
      ]
    },
    {
      "heading": "2 Outline of the procedure",
      "text": []
    },
    {
      "heading": "2.1 \"Training\"",
      "text": [
        "The training of this mechanism is to determine which patterns in the training set are the most likely.",
        "Only tag information is used.",
        "A filter to convert a tag with a context into a chunk-label is constructed as follows: 0) Construct symmetric n-contexts from the training corpus.",
        "A 1-context is simply the most common chunk-label for each tag.",
        "A 3-context is the tag followed by the tag before and after it, i.e., [to t_i t±iPabel.",
        "Similarly, a 5-context, (i.e., [t_2 [ to t+i] t+2]: label (of to)), is represented [to t_i t+1 t_2 t+2Plabel.",
        "Finally, a 7-context is represented as [to t_1 t_2 t+2 t_3 t+3Pabel.",
        "It was verified that results do not significantly improve using larger contexts than 5-contexts.",
        "1) For each set of n-contexts, determine the most frequent label for each occurring n-context.",
        "For example, the tag CC most frequently has the label B-NP if the context is PRP CC RP.",
        "The most frequent label for CC without extra context is \"0\".",
        "2) To save some storage space, the most frequent label in an n-context is only added if it is different from its nearest lower order context.",
        "For example, the label B-NP can be added for a 3-context since PRP CC RP gives a different result from CC alone."
      ]
    },
    {
      "heading": "2.2 Testing",
      "text": [
        "Testing is done by constructing the maximum context for each tag, and look it up in the database of the most likely patterns.",
        "If the largest context cannot be found the context is diminished step-by-step.",
        "3) In the test phase we need to form the longest contexts used in training (e.g., 7 contexts).",
        "The first word to get a chunk label is 'Rockwell' (Rockwell International Corp. 's) and its corresponding 7-context (without its label) is NNP = NNP = NNP = POS, where is a tag for a blank line (i.e., no text tag) since this is the very first few words.",
        "4) The only rule for chunk-labeling is to look up the closest surviving n-context and output its label.",
        "Simply look up [to t_i t_2 t+2 t_3 t4.3] ... [to] in that order until the context is found.",
        "The [to] context alone produces a Fo=1 of 77."
      ]
    },
    {
      "heading": "3 Results",
      "text": [
        "The evaluation program shows that this simple procedure reaches its best result for 5-contexts (table 1) with 92.46% label accuracy and phrase correctness measured by Fo=1 = 87.23.",
        "However, the improvement from 3-contexts to 5 contexts is insignificant, as 3-contexts reached 92.41% accuracy and Fo=1=87.09.",
        "The results for 7-contexts is almost identical to 5-contexts (92.44% and Fo=1=87.21).",
        "This is taken as the limit performance due to the size of the training corpus.",
        "In a larger training corpus, the most common longer contexts are likely to be useful but in a small set the longer contexts may occur with very low frequencies making it hard to determine if the label of such contexts is the best guess for unseen samples.",
        "These results are the best that could be expected without generalization.",
        "In order to do better, the method has to generalize to unseen contexts, e.g., by using some notion of close matching contexts (instances), to be able to use longer context even when some of that context has not been previously recorded.",
        "In addition, the tag-structure could be productively utilized.",
        "The presented method has treated all labels as arbitrary, atomic and independent symbols."
      ]
    },
    {
      "heading": "3.1 Computational complexity",
      "text": [
        "Using rule 2 from section 2.1, 45 patterns 'survived' for 1-contexts, and 3225, 71022, 38541 for 3-,5- and 7-contexts respectively, i.e., a total of 45, 3270, 74292, 109563 using all contexts up to and including 1-, 3-, 5 and 7-contexts.",
        "Each unique context can be retrieved in one logical step (i.e., a hash-table lookup).",
        "There are obviously many patterns in the database - but the complexity of the task is limited to the number of lookups necessary.",
        "There is a maximum of four hash-table lookups for each tag (i.e., when the 7-, 5-, and 3 contexts does not exist in the database the most likely label of the current tag will be used).",
        "Good performance can be obtained within a maximum of 2 lookups for each label (i.e., using only 1 and 3-contexts) and the best results were obtained with a maximum of 3 lookups per label."
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "The memory-based approach seemingly postulates innate tags in the processing machinery.",
        "The author has found very little discussion on how the tags are thought to correspond to reality, a fact that was also pointed out, not so long ago, by Palmeri (1998).",
        "However, a few papers aiming towards automatic 'label', 'feature' or `tag' creation are available (Miikkulainen and Dyer, 1991; Johansson, 1999).",
        "It is undeniable that, from a practical perspective, it is possible to reach very high performance on tasks, such as tagging, that demand a choice from a known set of alternatives by estimating statistical properties (e.g., the most likely label) from a large enough training set.",
        "This makes the method extremely useful for quick development of tools, which can be used in practical applications such as text retrieval and machine translation; but also in linguistic research; e.g., finding examples of specific grammatical constructions in large collections of data.",
        "A challenge for future research is how tags could be constructed automatically, and what kind of information would be necessary to detect the relevant tag dimensions for some linguistically motivated task."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "It was shown that using context made it possible to improve performance of maximum likelihood prediction.",
        "It was suggested that the limit of performance for this method is implicitly given by the size of the training set, as this determines the significance of larger contexts, and increases the chance of finding a matching longer context.",
        "In smaller collections, large patterns are a) likely to occur at a low frequency with few competing labels and b) likely to not exist in the test set.",
        "A larger collection will increase the number of different contexts, as well as the significance of picking the best, most frequent, prediction from a set of (identical) competitors with different labels.",
        "The presented method does not generalize beyond what is recorded in the training set as the most likely alternative.",
        "However, it is expected to • improve with the size of the training set, as this makes it feasible to use longer contexts, and • have a low computational complexity, as the",
        "process is always limited to use a low number of hash table lookups (determined by the largest size of context).",
        "Training is limited to detecting the most likely outcome of each context (i.e., a sorting operation)."
      ]
    }
  ]
}
