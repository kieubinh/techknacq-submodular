{
  "info": {
    "authors": [
      "Miles Osborne"
    ],
    "book": "Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop CoNLL and LLL",
    "id": "acl-W00-0731",
    "title": "Shallow Parsing As Part-Of-Speech Tagging",
    "url": "https://aclweb.org/anthology/W00-0731",
    "year": 2000
  },
  "references": [
    "acl-P98-1034",
    "acl-W95-0107",
    "acl-W96-0213",
    "acl-W99-0629"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Treating shallow parsing as part-of-speech tagging yields results comparable with other, more elaborate approaches.",
        "Using the CoNLL 2000 training and testing material, our best model had an accuracy of 94.88%, with an overall FB1 score of 91.94%.",
        "The individual FB1 scores for NPs were 92.19%, VPs 92.70% and PPs 96.69%."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Shallow parsing has received a reasonable amount of attention in the last few years (for example (Ramshaw and Marcus, 1995)).",
        "In this paper, instead of modifying some existing technique, or else proposing an entirely new approach, we decided to build a shallow parser using an off-the-shelf part-of-speech (POS) tagger.",
        "We deliberately did not modify the POS tag-ger's internal operation in any way.",
        "Our results suggested that achieving reasonable shallow-parsing performance does not in general require anything more elaborate than a simple POS tagger.",
        "However, an error analysis suggested the existence of a small set of constructs that are not so easily characterised by finite-state approaches such as ours."
      ]
    },
    {
      "heading": "2 The Tagger",
      "text": [
        "We used Ratnaparkhi's maximum entropy-based POS tagger (Ratnaparkhi, 1996).",
        "When tagging, the model tries to recover the most likely (unobserved) tag sequence, given a sequence of observed words.",
        "For our experiments, we used the binary-only distribution of the tagger (Ratnaparkhi, 1996)."
      ]
    },
    {
      "heading": "3 Convincing the Tagger to Shallow Parse",
      "text": [
        "The insight here is that one can view (some of) the differences between tagging and (shallow) parsing as one of context: shallow parsing requires access to a greater part of the surrounding lexical/POS syntactic environment than does simple POS tagging.",
        "This extra information can be encoded in a state.",
        "However, one must balance this approach with the fact that as the amount of information in a state increases, with limited training material, the chance of seeing such a state again in the future diminishes.",
        "We therefore would expect performance to increase as we increased the amount of information in a state, and then decrease when overfitting and/or sparse statistics become dominate factors.",
        "We trained the tagger using 'words' that were various 'configurations' (concatenations) of actual words, POS tags, chunk-types, and/or suffixes or prefixes of words and/or chunk-types.",
        "By training upon these concatenations, we help bridge the gap between simple POS tagging and shallow parsing.",
        "In the rest of the paper, we refer to what the tagger considers to be a word as a configuration.",
        "A configuration will be a concatenation of various elements of the training set relevant to decision making regarding chunk assignment.",
        "A `word' will mean a word as found in the training set.",
        "'Tags' refer to the POS tags found in the training set.",
        "Again, such tags may be part of a configuration.",
        "We refer to what the tagger considers as a tag as a prediction.",
        "Predictions will be chunk labels."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We now give details of the experiments we ran.",
        "To make matters clearer, consider the following",
        "Words are wi, w2 and w3, tags are tl, t2 and t3 and chunk labels are c1, c2 and c3.",
        "Throughout, we built various configurations when predicting the chunk label for word w1.1 With respect to the situation just mentioned (predicting the label for word wi), we gradually increased the amount of information in each configuration as follows:",
        "1.",
        "A configuration consisting of just words (word wi).",
        "Results:",
        "2.",
        "A configuration consisting of just tags (tag ti).",
        "Results:",
        "3.",
        "Both words, tags and the current chunk label (wi, ti, ci) in a configuration.",
        "We allowed the tagger access to the current chunk label by training another model with",
        "'For space reasons, we had to remove many of these experiments.",
        "The longer version of the paper gives relevant details.",
        "configurations consisting of tags and words (wi and t1).",
        "The training set was then reduced to consist of just tag-word configurations and tagged using this model.",
        "Afterwards, we collected the predictions for use in the second model.",
        "Results:",
        "4.",
        "The final configuration made an attempt to take deal with sparse statistics.",
        "It consisted of the current tag t1, the next tag t2, the current chunk label c1, the last two letters of the next chunk label c2, the first two letters of the current word wi and the last four letters of the current word wi.",
        "This configuration was the result of numerous experiments and gave the best overall performance.",
        "The results can be found in Table 1.",
        "We remark upon our experiments in the comments section."
      ]
    },
    {
      "heading": "5 Error Analysis",
      "text": [
        "We examined the performance of our final model with respect to the testing material and found that errors made by our shallow parser could be grouped into three categories: difficult syntactic constructs, mistakes made in the training or testing material by the annotators, and errors peculiar to our approach.2 Taking each category of the three in turn, problematic constructs included: co-ordination, punctuation, treating ditransitive VPs as being transitive VPs, confusions regarding adjective or adverbial phrases, and copulars seen as being possessives.",
        "Mistakes (noise) in the training and testing material were mainly POS tagging errors.",
        "An additional source of errors were odd annotation decisions.",
        "The final source of errors were peculiar to our system.",
        "Exponential distributions (as used by our tagger) assign a non-zero probability to all possible events.",
        "This means that the tagger will at times assign chunk labels that are illegal, for example assigning a word the label I-NP when the word is not in a NP.",
        "Although these errors were infrequent, eliminating them would require `opening-up' the tagger and rejecting illegal hypothesised chunk labels from consideration."
      ]
    },
    {
      "heading": "6 Comments",
      "text": [
        "As was argued in the introduction, increasing the size of the context produces better results, and such performance is bounded by issues such as sparse statistics.",
        "Our experiments suggest that this was indeed true.",
        "We make no claims about the generality of our modelling.",
        "Clearly it is specific to the tagger used.",
        "In more detail, we found that:",
        "• PPs seem easy to identify.",
        "• ADJP and ADVP chunks were hard to identify correctly.",
        "We suspect that improvements here require greater syntactic information than just base-phrases.",
        "• Our performance at NPs should be improved-upon.",
        "In terms of modelling, we did not treat any chunk differently from any other chunk.",
        "We also did not treat any words differently from any other words.",
        "• The performance using just words and just",
        "POS tags were roughly equivalent.",
        "However, the performance using both sources was better than when using either source of information in isolation.",
        "The reason for this is that words and POS tags have different properties, and that together, the specificity of words can overcome the coarseness of tags, whilst the abundance of tags can deal with the sparseness of words.",
        "Our results were not wildly worse than those reported by Buchholz et al. (Sabine Buchholz and Daelemans, 1999).",
        "This comparable level of performance suggests that shallow parsing (base",
        "phrasal recognition) is a fairly easy task.",
        "Improvements might come from better modelling, dealing with illegal chunk sequences, allowing multiple chunks with confidence intervals, system combination etc, but we feel that such improvements will be small.",
        "Given this, we believe that base-phrasal chunking is close to being a solved problem."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank Erik Tjong Kim Sang for supplying the evaluation code, and Donnla Nic Gearailt for dictating over the telephone, and from the top-of-her-head, a Perl program to help extract wrongly labelled sentences from the results."
      ]
    }
  ]
}
