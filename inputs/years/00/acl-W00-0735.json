{
  "info": {
    "authors": [
      "Jorn Veenstra",
      "Antal van den Bosch"
    ],
    "book": "Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop CoNLL and LLL",
    "id": "acl-W00-0735",
    "title": "Single-Classifier Memory-Based Phrase Chunking",
    "url": "https://aclweb.org/anthology/W00-0735",
    "year": 2000
  },
  "references": [
    "acl-J93-2004",
    "acl-W99-0629"
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": [
        "In the shared task for CoNLL-2000, words and tags form the basic multivalued features for predicting a rich phrase segmentation code.",
        "While the tag features, containing WSJ part-of-speech tags (Marcus et al., 1993), have about 45 values, the word features have more than 10,000 values.",
        "In our study we have looked at how memory-based learning, as implemented in the TiMBL software system (Daelemans et al., 2000), can handle such features.",
        "We have limited our search to single classifiers, thereby explicitly ignoring the possibility to build a meta-learning classifier architecture that could be expected to improve accuracy.",
        "Given this restriction we have explored the following:",
        "1.",
        "The generalization accuracy of TiMBL with default settings (multi-valued features, overlap metric, feature weighting).",
        "2.",
        "The usage of MVDM (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) (Section 2), which should work well on word value pairs with a medium or high frequency, but may work badly on word value pairs with low frequency.",
        "3.",
        "The straightforward unpacking of feature values into binary features.",
        "On some tasks we have found that splitting multivalued features into several binary features can enhance performance of the classifier.",
        "4.",
        "A heuristic search for complex features on the basis of all unpacked feature values, and using these complex features for the classification task."
      ]
    },
    {
      "heading": "2 Methods and Data",
      "text": [
        "The data used for this shared task is comparable to the dataset used in (Buchholz et al., 1999), who found an optimal windowing context size of five words and POS tags to the left, the word itself, and three words and POS tags to the right.",
        "We also used this window size, and have applied TiMBL to the shared task data using default TiMBL settings.",
        "TiMBL and the abovementioned feature metrics are introduced in the following sections.",
        "IB1-IG The default TiMBL setting, IB1-IG, (Daelemans et al., 1997) is a memory-based learning algorithm that builds a database of instances (the instance base) during learning.",
        "An instance consists of a fixed-length vector of n feature-value pairs, and an information field containing the classification of that particular feature-value vector.",
        "After the instance base is built, new (test) instances are classified by matching them to all instances in the instance base, and by calculating with each match the distance between the new instance X and the memory instance Y.",
        "The most basic metric for patterns with symbolic features is the Overlap metric given in equation 1; where A(X, Y) is the distance between patterns X and Y, represented by n features, wi is a weight for feature i, and 6 is the distance per feature.",
        "The k-NN algorithm with this metric, and equal weighting for all features is called ml (Aha et al., 1991).",
        "Usually k is set to 1.",
        "A(X, Y) = E wi 6(xi,yi) (1) i=i where: 6(xi, yi) = 0 if xi = yi, else 1 This distance metric simply counts the number of (mis)matching feature values in both pat",
        "terns.",
        "In the absence of information about feature relevance, this is a reasonable choice.",
        "However, Information Theory gives us a useful tool for measuring feature relevance (Quinlan, 1986; Quinlan, 1993).",
        "Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label.",
        "The Information Gain of feature f is measured by computing the difference in uncertainty (i.e. entropy) between the situations without and with knowledge of the value of that feature.",
        "The resulting IG values can then be used as weights in equation 1.",
        "Modified Value Difference Metric The Modified Value Difference Metric (MVDM) (Cost and Salzberg, 1993) estimates the distance between two values of a feature by comparing the class distribution of both features.",
        "MVDM can give good estimates if there are enough occurrences of the two values, but for low-frequent values unreliable values of MVDM can occur.",
        "For this data we can expect that this sparseness effect hinders the word features more than the P OS features.",
        "Unpacking Features Unpacking features implies that all feature values receive individual weights.",
        "(Van den Bosch and Zavrel, 2000) warn that this operation forces feature weights to be based on less observations, which could make the weights unrealistic in view of test data.",
        "Moreover, the k nearest neighbors can be expected to contain less instances with a fixed k when unpacking features; this is usually not beneficial for generalization accuracy (Van den Bosch and Zavrel, 2000).",
        "Complex Features In the previously discussed versions of memory-based learning, features are treated as independent.",
        "However, sometimes combinations of features or feature values may be very good predictors.",
        "Since there are many possible combinations, search strategies are needed to select the best.",
        "Such strategies have been developed for rule induction algorithms (Clark and Niblett, 1989; Quinlan, 1993; Cohen, 1995), and they can be used to find complex features for memory-based learning as well.",
        "We followed the following procedure:",
        "1. apply Ripper (Cohen, 1995) to the training set, and collect the set of induced rules; 2. recode the instances in the training and test set, by setting binary features denoting the rules that apply to them; 3. apply memory-based learning to the recoded training set, and classify the recoded test set."
      ]
    },
    {
      "heading": "3 Experiments and Results",
      "text": [
        "In Table 2 we give an overview of the experiments with different metrics and settings.",
        "In the first block of rows we give the results of the default setting with IB1-IG and with a varying k parameter (number of nearest neighbours).",
        "We can see that a larger k improves performance to a certain extent.",
        "In the second series of experiments we have used the MVDM metric.",
        "Here, we also varied the",
        "value of k. We found that a larger k yielded better results.",
        "In a variant on this series we applied MVDM only to the POS features.",
        "As expected this variant gave slightly better results.",
        "In the third series we unpacked the features.",
        "Compared to the previous experiment the results were worse.",
        "Apparently, sparseness results in bad feature weights.",
        "This negative effect appears to have outweighted any positive effect of informative individual features.",
        "In the last experiments we used Ripper to generate 390 complex features.",
        "The results are comparable to the best TiMBL settings.",
        "In Table 2 we give an overview of the precision, recall and Fo = 1 of one of the best scoring setting: IB1-IG with k = 3"
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "We found in the experiments that minor improvements on the default settings of TiMBL can be obtained by applying MVDM, particularly to the POS tags.",
        "A larger k generally improved accuracy to a certain extent.",
        "Unpacking the features did not give the expected improvement.",
        "Complex features, however, did, and seem a promising alley to go."
      ]
    }
  ]
}
