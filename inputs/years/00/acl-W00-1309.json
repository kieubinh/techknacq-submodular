{
  "info": {
    "authors": [
      "Guodong Zhou",
      "Jian Su"
    ],
    "book": "Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora",
    "id": "acl-W00-1309",
    "title": "Error-Driven HMM-Based Chunk Tagger With Context-Dependent Lexicon",
    "url": "https://aclweb.org/anthology/W00-1309",
    "year": 2000
  },
  "references": [
    "acl-A88-1019",
    "acl-C92-3126",
    "acl-C92-3150",
    "acl-J93-2004",
    "acl-P93-1003",
    "acl-P98-1010",
    "acl-W93-0306",
    "acl-W95-0107",
    "acl-W96-0102",
    "acl-W99-0629",
    "acl-W99-0707"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes a new error-driven HM3/1-based text chunk tagger with context-dependent lexicon.",
        "Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry.",
        "Moreover, an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries.",
        "Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types, 93.60% and 94.64% for noun phrases, and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types, 96.49% and 96.99% for noun phrases, and 97.13% and 97.36% for verb phrases.",
        "Introduction Text chunking is to divide sentences into non-overlapping segments on the basis of fairly superficial analysis.",
        "Abney(1991) proposed this as a useful and relatively tractable precursor to full parsing, since it provides a foundation for further levels of analysis, while still allowing more complex attachment decisions to be postponed to a later phase.",
        "Text chunking typically relies on fairly simple and efficient processing algorithms.",
        "Recently, many researchers have looked at text chunking in two different ways: Some researchers have applied rule-based methods, combining lexical data with finite state or other rule constraints, while others have worked on inducing statistical models either directly from the words and/or from automatically assigned part-of-speech classes.",
        "On the statistics-based approaches, Skut and Brants(1998) proposed a HMM-based approach to recognise the syntactic structures of limited length.",
        "Buchholz, Veenstra and Daelemans(1999), and Veenstra(1999) explored memory-based learning method to fmd labelled chunks.",
        "Ratnaparkhi(1998) used maximum entropy to recognise arbitrary chunk as part of a tagging task.",
        "On the rule-based approaches, Bourigaut(1992) used some heuristics and a grammar to extract \"terminology noun phrases\" from French text.",
        "Voutilainen(1993) used similar method to detect English noun phrases.",
        "Kupiec(1993) applied finite state transducer in his noun phrases recogniser for both English and French.",
        "Ramshaw and Marcus(1995) used transformation-based learning, an error-driven learning technique introduced by Eric Brill(1993), to locate chunks in the tagged corpus.",
        "Grefenstette(1996) applied finite state transducers to fmd noun phrases and verb phrases.",
        "In this paper, we will focus on statistics-based methods.",
        "The structure of this paper is as follows : In section 1, we will briefly describe the new error-driven HMM-based chunk tagger with context-dependent lexicon in principle.",
        "In section 2, a baseline system which only includes the current part-of-speech in the lexicon is given.",
        "In section 3, several extended systems with different context-dependent lexicons are described.",
        "In section 4, an error-driven learning method is used to decrease memory requirement of the lexicon by keeping only positive lexical",
        "entries and make it possible to further improve the accuracy by merging different context-dependent lexicons into one after automatic analysis of the chunking errors.",
        "Finally, the conclusion is given.",
        "The data used for all our experiments is extracted from the PENN WSJ Treebank (Marcus et al.",
        "1993) by the program provided by Sabine Buchholz from Tilbug University.",
        "We use sections 00-19 as the training data and 20-24 as test data.",
        "Therefore, the performance is on large scale task instead of small scale task on CoNLL-2000 with the same evaluation program.",
        "For evaluation of our results, we use the precision and recall measures.",
        "Precision is the percentage of predicted chunks that are actually correct while the recall is the percentage of correct chunks that are actually found.",
        "For convenient comparisons of only one value, we also list the F5=1 value(Rijsbergen 1979) :",
        "The idea of using statistics for chunking goes back to Church(1988), who used corpus frequencies to determine the boundaries of simple non-recursive noun phrases.",
        "Skut and Brants(1998) modified Church's approach in a way permitting efficient and reliable recognition of structures of limited depth and encoded the structure in such a way that it can be recognised by a Viterbi tagger.",
        "This makes the process run in time linear to the length of the input string.",
        "Our approach follows Skut and Brants' way by employing HMM-based tagging method to model the chunking process.",
        "Given a token sequence Gin = gig2 gn , the goal is to find a stochastic optimal tag",
        "in ;')= to P(Tin ) + log The second item in the above equation is the mutual information between the tag sequence Tin and the given token sequence Gin .",
        "By assuming that the mutual information between Gin and Ti\" is equal to the summation of mutual information between Gin and the individual tag",
        "The first item of above equation can be solved by using chain rules.",
        "Normally, each tag is assumed to be probabilistic dependent on the N-1 previous tags.",
        "Here, backoff bigram(N=2) model is used.",
        "The second item is the summation of log probabilities of all the tags.",
        "Both the first item and second item correspond to the language model component of the tagger while the third item corresponds to the lexicon component of the tagger.",
        "Ideally the third item can be estimated by using the forward-backward algorithm(Rabiner 1989) recursively for the first-order(Rabiner 1989) or second-order HMMs(Watson and Chunk 1992).",
        "However, several approximations on it will be attempted later in this paper instead.",
        "The stochastic optimal tag sequence can be found by maxmizing the above equation over all the possible tag sequences.",
        "This is implemented by the Viterbi algorithm.",
        "The main difference between our tagger and other standard taggers lies in our tagger has a context-dependent lexicon while others use a context-independent lexicon.",
        "For chunk tagger, we have g1 = piwi where",
        "sequence.",
        "Here, we use structural tags to representing chunking(bracketing and labelling) structure.",
        "The basic idea of representing the structural tags is similar to Skut and Brants(1998) and the structural tag consists of three parts: 1) Structural relation.",
        "The basic idea is simple: structures of limited depth are encoded using a finite number of flags.",
        "Given a sequence of input tokens(here, the word and part-of-speech pairs), we consider the structural relation between the previous input token and the current one.",
        "For the recognition of chunks, it is sufficient to distinguish the following four different structural relations which uniquely identify the substructures of depth 1 (Skut and Brants used seven different structural relations to identify the substructures of depth 2).",
        "00 the current input token and the previous one have the same parent 90 one ancestor of the current input token and the previous input token have the same parent 09 the current input token and one ancestor of the previous input token have the same parent 99 one ancestor of the current input token and one ancestor of the previous input token have the same parent For example, in the following chunk tagged",
        "the corresponding structural relations between two adjacent input tokens are:",
        "Compared with the B-Chunk and I-Chunk used in Ramshaw and Marcus(1995), structural relations 99 and 90 correspond to B-Chunk which represents the first word of the chunk, and structural relations 00 and 09 correspond to I-Chunk which represnts each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence.",
        "2)Phrase category.",
        "This is used to identify the phrase categories of input tokens.",
        "3)Part-of-speech.",
        "Because of the limited number of structural relations and phrase categories, the part-of-speech is added into the structural tag to represent more accurate models.",
        "For the above chunk tagged sentence, the structural tags for all the corresponding input",
        "current part-of-speech is used as a lexical entry to determine the current structural chunk tag.",
        "Here, we define: (I) is the list of lexical entries in the chunking lexicon,",
        "1431 is the number of lexical entries(the size of the chunking lexicon) C is the training data.",
        "For the baseline system, we have : ={popi3C} , where 1,, is a part-of-speech existing in the training data C 1401=48 (the number of part-of-speech tags in the training data).",
        "Table 1 gives an overview of the results of the chunking experiments.",
        "For convenience, precision, recall and Fp.1 values are given seperately for the chunk types NP, VP, ADJP,"
      ]
    },
    {
      "heading": "3 Context-dependent Lexicons",
      "text": [
        "In the last section, we only use current part-of-speech as a lexical entry.",
        "In this section, we will attempt to add more contextual information to approximate P(t, / Gia).",
        "This can be done by adding lexical entries with more contextual information into the lexicon 43.",
        "In the following, we will discuss five context-dependent lexicons which consider different contextual information."
      ]
    },
    {
      "heading": "3.1 Context of current part-of-speech and",
      "text": [
        "current word In this case, the current part-of-speech and word pair is also used as a lexical entry to determine the current structural chunk tag and we have a total of about 49563 lexical entries(' 4)1=49563).",
        "Actually, the lexicon used here can be regarded as context-independent.",
        "The reason we discuss it in this section is to distinguish it from the context-independent lexicon used in the baseline system.",
        "Table 2 give an overview of the results of the chunking experiments on the test data.",
        "Table 2 shows that incorporation of current word information improves the overall Ffr4 value by 2.9%(especially for the ADJP, ADVP and PP chunks), compared with Table 1 of the baseline system which only uses current part-of-speech information.",
        "This result suggests that current word information plays a very important role in determining the current chunk tag."
      ]
    },
    {
      "heading": "3.2 Context of previous part-of-speech and",
      "text": [
        "current part-of-speech Here, we assume :",
        "where (13={piwopiwi3C}+{popi3C} and piwi is a part-of-speech and word pair existing in the training data C .",
        "0={Pi-1PoPi-IPi3C}+{PoPi3C} and is a pair of previous part-of-speech and current part-of-speech existing in the training data C. In this case, the previous part-of-speech and current part-of-speech pair is also used as a lexical entry to determine the current structural chunk tag and we have a total of about 1411 lexical entries( 4)1=1414 Table 3 give an overview of the results of the chunking experiments.",
        "Here, we assume:",
        "Compared with Table 1 of the baseline system, Table 3 shows that additional contextual information of previous part-of-speech improves the overall Fs.1 value by 0.5%.",
        "Especially, Ft3=1 value for VP improves by 1.25%, which indicates that previous part-of-speech information has a important role in determining the chunk type VP.",
        "Table 3 also shows that the recall rate for chunk type ADJP decrease by 3.7%.",
        "It indicates that additional previous part-of-speech information makes ADJP chunks easier to merge with neibghbouring chunks."
      ]
    },
    {
      "heading": "3.3 Context of previous part-of-speech,",
      "text": [
        "previous word and current part-of-speech Here, we assume :",
        "whereis a triple pattern existing in the training corpus.",
        "In this case, the previous part-of-speech, previous word and current part-of-speech triple is also used as a lexical entry to determine the current structural chunk tag and 1(131=136164.",
        "Table 4 gives the results of the chunking experiments.",
        "Compared with Table 1 of the baseline system, Table 4 shows that additional 136116 new lexical entries of format improves the overall Fp=1 value by 3.3%.",
        "Compared with Table 3 of the extended system 2.2 which uses previous part-of-speech and current part-of-speech as a lexical entry, Table 4 shows that additional contextual information of previous word improves the overall Ffi_1 value by 2.8%."
      ]
    },
    {
      "heading": "3.4 Context of previous part-of-speech, current",
      "text": [
        "part-of-speech and current word Here, we assume :",
        "whereis a triple pattern existing in the training and 14)1=131416.",
        "Table 5 gives the results of the chunking experiments.",
        "Compared with Table 2 of the extended system which uses current part-of-speech and current word as a lexical entry, Table 5 shows that additional contextual information of previous part-of-speech improves the overall F/3 value by 1.8%."
      ]
    },
    {
      "heading": "3.5 Context of previous part-of-speech,",
      "text": [
        "previous word, current part-of-speech and current word Here, the context of previous part-of-speech, current part-of-speech and current word is used as a lexical entry to determine the current",
        "where pi_lw,_1p,w, is a pattern existing in the training corpus.",
        "Due to memory limitation, only lexical entries which occurs more than 1 times are kept.",
        "Out of 364365 possible lexical entries existing in the training data, 98489 are kept("
      ]
    },
    {
      "heading": "1 (Ds 1=98489). 4Error-driven Learning",
      "text": [
        "In section 2, we implement a baseline system which only considers current part-of-speech as a lexical entry to deterrmine the current chunk tag while in section 3, we implement several extended systems which take more contextual information into consideration.",
        "P(ti 1 G;)Pi-iwi-iPiwiEHere, we will examine the effectiveness of",
        "Pi-Iwi-IPiwi Elexical entries to reduce the size of lexicon and P(ti I Pi)make it possible to further improve the chunking accuracy by merging several context-dependent lexicons in a single lexicon.",
        "Table 6 gives the results of the chunking experiments.",
        "system which uses current part-of-speech and current word as a lexical entry, Table 6 shows that additional contextual information of previous part-of-speech improves the overall F value by 1.8%.",
        "fi=1"
      ]
    },
    {
      "heading": "3.6 Conclusion",
      "text": [
        "Above experiments shows that adding more contextual information into lexicon significantly improves the chunking accuracy.",
        "However, this improvement is gained at the expense of a very large lexicon and we fmd it difficult to merge all the above context-dependent lexicons in a single lexicon to further improve the chunking accurracy because of memory limitation.",
        "In order to reduce the size of lexicon effectively, an error-driven learning approach is adopted to examine the effectiveness of lexical entries and make it possible to further improve the chunking accuracy by merging all the above context-dependent lexicons in a single lexicon.",
        "This will be discussed in the next section.",
        "For a new lexical entry ei, the effectiveness F, (e1) is measured by the reduction in error which results from adding the lexical entry to the lexicon : Fo(e,)= F:TOr (ei)_F:zoor (es ) Here, FoError (e1) is the chunking error number of the lexical entry e, for the old lexicon and Fe,EZ;(e1) is the chunking error number of the lexical entry e, for the new lexicon + AO where e, E(Ad is the list of new lexical entries added to the old lexicon (I) ).",
        "If Fo(e1)> 0, we define the lexical entry e, as positive for lexicon ED .",
        "Otherwise, the lexical entry e, is negative for lexicon (I) .",
        "Tables 7 and 8 give an overview of the effectiveness distributions for different lexicons applied in the extended systems, compared with the lexicon applied in the baseline system, on the test data and the training data, respectively.",
        "Tables 7 and 8 show that only a minority of lexical entries are positive.",
        "This indicates that discarding non-positive lexical entries will largely decrease the lexicon memory requirement while keeping the chunking accurracy.",
        "the training data Tables 9-13 give the performances of the five error-driven systems which discard all the non-positive lexical enrties on the training data.",
        "Here, V is the lexicon used in the baseline system.",
        "cto = {pi , pi3C} and 0:13= -41:0.",
        "It is found that Ffl=1 values of error driven systems for context of current part-of-speech and word pair and for context of previous part-of-speech and current part-of-speech increase by 1.2% and 0.6%.",
        "Although Ffi=1 values for other three cases slightly decrease by 0.02%, 0.02% and 0.19%, the sizes of lexicons have been greatly reduced by 85% to 97%.",
        "4. if pi wi E c, P(ti I Gin )=.",
        ": P(ti I piwi) 5. ifpi E, P(ti I)= P(ti I pi_ipi) 6.",
        "P(ti I Gr ).",
        "P(ti 1 pi_lpi)",
        "experiments using the above assumption.",
        "It shows that the F0=1 value for the merged context-dependent lexicon inreases to 93.68%.",
        "For a comparison, the Fp=1 value is 93.30% when all the possible lexical entries are included in (J) (Due to memory limitation, only the top",
        "with the merged context-dependent lexicon For the relationship between the training corpus size and error driven learning performance, Table 15 shows that the performance of error-driven learning improves stably when the training corpus size increases.",
        "learning with different training corpus size For comparison with other chunk taggers, we also evaluate our chunk tagger with the merged context-dependent lexicon by cross-validation on all 25 partitions of the PENN WSJ TreeBank.",
        "Table 16 gives an overview of such chunking experiments.",
        "tagger greatly outperforms other reported chunk taggers on the same training data and test data by 2%-3%.",
        "(Buchholz S., Veenstra J. and Daelmans W.(1999), Ramshaw L.A. and Marcus M.P.",
        "(1995), Daelemans W., Buchholz S. and Veenstra J.",
        "(1999), and Veenstra J.(1999)).",
        "Conclusion This paper proposes a new error-driven HMM-based chunk tagger with context-dependent lexicon.",
        "Compared with standard 11MM-based tagger, this new tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry by assuming MI(Tin ,G:i ) = i=1 Moreover, an error-driven learning approach is adopted to drease the memeory requirement and further improve the accuracy by including more context-dependent information into lexicon.",
        "It is found that our new chunk tagger singnificantly outperforms other reported chunk taggers on the same training data and test data.",
        "For future work, we will explore the effectivessness of considering even more contextual information on approximation of P(Tln Gi) by using the forward-backward algorithm(Rabiner 1989) while currently we only consider the contextual information of current location and previous location."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "We wish to thank Sabine Buchholz from Tilbug University for kindly providing us her program which is also used to extact data for Conll-2000 share task."
      ]
    }
  ]
}
