{
  "info": {
    "authors": [
      "Kyoji Umemura",
      "Kenneth Ward Church"
    ],
    "book": "Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora",
    "id": "acl-W00-1315",
    "title": "Empirical Term Weighting and Expansion Frequency",
    "url": "https://aclweb.org/anthology/W00-1315",
    "year": 2000
  },
  "references": [
    "acl-C00-1027"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We propose an empirical method for estimating term weights directly from relevance judgements, avoiding various standard but potentially troublesome assumptions.",
        "It is common to assume, for example, that weights vary with term frequency (tf) and inverse document frequency (idf) in a particular way, e.g., t f • idf , but the fact that there are so many variants of this formula in the literature suggests that there remains considerable uncertainty about these assumptions.",
        "Our method is similar to the Berkeley regression method where labeled relevance judgements are fit as a linear combination of (transforms of) tf, idf, etc.",
        "Training methods not only improve performance, but also extend naturally to include additional factors such as burstiness and query expansion.",
        "The proposed histogram-based training method provides a simple way to model complicated interactions among factors such as tf, idf, burstiness and expansion frequency (a generalization of query expansion).",
        "The correct handling of expanded term is realized based on statistical information.",
        "Expansion frequency dramatically improves performance from a level comparable to BKJJBIDS, Berkeley's entry in the Japanese NACSIS NTCIR-1 evaluation for short queries, to the level of JCB1, the top system in the evaluation.",
        "JCB1 uses sophisticated (and proprietary) natural language processing techniques developed by Just System, a leader in the Japanese word-processing industry.",
        "We are encouraged that the proposed method, which is simple to understand and replicate, can reach this level of performance."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "An empirical method for estimating term weights directly from relevance judgements is proposed.",
        "The method is designed to make as few assumptions as possible.",
        "It is similar to Berkeley's use of regression (Cooper et al., 1994) (Chen et al., 1999) where labeled relevance judgements are fit as a linear combination of (transforms of) tf, idf, etc., but avoids potentially troublesome assumptions by introducing histogram methods.",
        "Terms are grouped into bins.",
        "Weights are computed based on the number of relevant and irrelevant documents associated with each bin.",
        "The result",
        "• t: a term • d: a document • t f (t, d): term freq = # of instances of t in d • df (t): doc freq = # of docs d with t f (t, d) > 1 • N: # of documents in collection • idf (t): inverse document freq: – loge dfNe • df (t, rel , t fo): # of relevant documents d with t f (t, = tfo • df (t, rel , t fo): # of irrelevant documents d with t f (t, d) = tfo • e f (t): expansion frequency = # docs d in query expansion with tf(t, d) > 1 • TF(t): standard notion of frequency in corpus-based NLP: TF(t) = Ed t f (t, d) • B(t): burstiness: B(t) = 1 iff 70 is large.",
        "ing weights usually lie between 0 and idf , which is a surprise; standard formulas like t f idf would assign values well outside this range.",
        "The method extends naturally to include additional factors such as query expansion.",
        "Terms mentioned explicitly in the query receive much larger weights than terms brought in via query expansion.",
        "In addition, whether or not a term t is mentioned explicitly in the query, if t appears in documents brought in by query expansion (ef (t) > 1) then t will receive a much larger weight than it would have otherwise (ef(t) = 0).",
        "The interactions among these factors, however, are complicated and collection dependent.",
        "It is safer to use histogram methods than to impose unnecessary and potentially troublesome assumptions such as normality and independence.",
        "Under the vector space model, the score for a document d and a query q is computed by summing a contribution for each term t over an appropriate set of terms, T. T is often limited to terms shared by both the document and the query (minus stop words), though not always (e.g, query expansion).",
        "tf and idf.",
        "Terms are assieed to bins based on idf .",
        "The column labeled idf is the mean idf for the terms in each bin.",
        "A is estimated separately for each bin and each tf value, based on the labeled relevance judgements.",
        "Under the probabilistic retrieval model, documents are scored by summing a similar contribution for each term t. In this work, we use A to refer to term weights.",
        "scored, q) = E A(t, d, q) tET This paper will start by showing how to estimate A from relevance judgements.",
        "Three parameteriza-dons will be considered: (1) fit-G, (2) fit-B, which introduces burstiness, and (3) fit-E, which introduces expansion frequency.",
        "The evaluation section shows that each model improves on the previous one.",
        "But in addition to performance, we are also interested in the interpretations of the parameters."
      ]
    },
    {
      "heading": "2 Supervised Training",
      "text": [
        "The statistical task is to compute A, our best estimate of A, based on a training set.",
        "This paper will use supervised methods where the training materials not only include a large number of documents but also a few queries labeled with relevance judgements.",
        "To make the training task more manageable, it is common practice to map the space of all terms into a lower dimensional feature space.",
        "In other words, instead of estimating a different A for each term in the vocabulary, we can model A as a function of tf and idf and various other features of",
        "values in previous table.",
        "Most points fall between the dashed lines (lower limit of A = 0 and upper limit of A = idf).",
        "The plotting character denotes tf .",
        "Note that the line with tf = 4 is above the line with tf = 3, which is above the line with tf = 2, and so on.",
        "The higher lines have larger intercepts and larger slopes than the lower lines.",
        "That is, when we fit A a(tf) + b(tf) - idf , with separate regression coefficients, a(tf) and b(tf), for each value of t f , we find that both a(tf) and b(t f) increase with tf .",
        "terms.",
        "In this way, all of the terms in a bin are assigned the weight, Ã.",
        "The common practice, for example, of assigning tf • idf weights can be interpreted as grouping all terms with the same idf into a bin and assigning them all the same weight, namely tf -idf .",
        "Cooper and his colleagues at Berkeley (Cooper et al., 1994) (Chen et al., 1999) have been using regression methods to fit A as a linear combination of idf, log(tf) and various other features.",
        "This method is also grouping terms into bins based on their features and assigning similar weights to terms with similar features.",
        "In general, term weighting methods that are fit to data are more flexible than weighting methods that are not fit to data.",
        "We believe this additional flexibility improves precision and recall (table 8).",
        "Instead of multiple regression, though, we choose a more empirical approach.",
        "Parametric as",
        "Description (function of term t)",
        "freq of term in corpus: TF(t) Edtf d) # docs d in collection = N",
        "where: D (description), E (query expansion) burstiness: B where df (bin, rel, t f) is",
        "Similarly, the denominator can be approximated as:",
        "df (bin, rel, t f) where df (bin, rel, t f) is",
        "Ar.ret is an estimate of the total number of relevant documents.",
        "Since some queries have more relevant documents than others, Nrei is computed by averaging:",
        "is computed for each term (ngram) in each query in training set.",
        "sumptions, when appropriate, can be very powerful (better estimates from less training data), but errors resulting from inappropriate assumptions can outweigh the benefits.",
        "In this empirical investigation of term weighting we decided to use conservative non-parametric histogram methods to hedge against the risk of inappropriate parametric assumptions.",
        "Terms are assigned to bins based on features such as idf, as illustrated in table 2.",
        "(Later we will also use B and/or ef in the binning process.)",
        "is computed separately for each bin, based on the use of terms in relevant and irrelevant documents, according to the labeled training material.",
        "The estimation method starts with a training file which indicates, among other things, the number of relevant and irrelevant documents for each term t in each training query, q.",
        "That is, for each t and q, we are are given df (t, rel, t fo) and df (t, rel, t fo), where df (t,rel,tfo) is the number of relevant documents d with tf(t, d) = tfo, and df (t, rel, t fo) is the number of irrelevant documents d with t f (t, d) = t fo.",
        "The schema for the training file is described in table 3.",
        "From these training observations we wish to obtain a mapping from bins to As that can be applied to unseen test material.",
        "We interpret A as a log likelihood ratio:",
        "where the numerator can be approximated as: P(bin, t flrel) loge df (bin, re/, tf)",
        "To ensure that grei + ICITj = N, where N is the number of documents in the collection, we define Wrel N This estimation procedure is implemented with the simple awk program in figure 2.",
        "The awk pro-grarn reads each line of the training file, which contains a line for each term in each training query.",
        "As described in table 3, each training line contains 25 fields.",
        "The first five fields contain df (t, rel, t f) for five values of tf , and the next five fields contain df(t,rel,tf) for the same five values of tf.",
        "The next two fields contain Nrei and N. .",
        "As the awk program reads each of these lines from the training file, it assigns each term in each training query to a bin (based on ilog2(df)1, except when df < 100), and maintains running sums of the first dozen fields which are used for computing df (bin, rel, t f), df (bin, rel, t f), grel and ICr, for five values of t f .",
        "Finally, after reading all the training material, the program outputs the table of As shown in table 2.",
        "The table contains a column for each of the five t f values and a row for each of the dozen idf bins.",
        "Later, we will consider more interesting binning rules that make use of additional statistics such as burstiness and query expansion."
      ]
    },
    {
      "heading": "2.1 Interpolating Between Bins",
      "text": [
        "Recall that the task is to apply the As to new unseen test data.",
        "One could simply use the As in table 2 as is.",
        "That is, when we see a new term in the test material, we find the closest bin in table 2 and report the corresponding A value.",
        "But since the idf of a term in the test set could easily fall between two bins, it seems preferable to find the two closest bins and interpolate between them.",
        "ISTrel",
        "We use linear regression to interpolate along the idf dimension, as illustrated in table 4.",
        "Table 4 is a smoothed version of table 2 where A a+b-idf.",
        "There are five pairs of coefficients, a and b, one for each value of t f .",
        "Note that interpolation is generally not necessary on the tf dimension because tf is highly quantized.",
        "As long as tf < 4, which it usually is, the closest bin is an exact match.",
        "Even when t f > 4, there is very little room for adjustments if we accept the upper limit of A < idf.",
        "Although we interpolate along the idf dimension, interpolation is not all that important along that dimension either.",
        "Figure 1 shows that the differences between the test data and the training data dominate the issues that interpolation is attempting to deal with.",
        "The main advantage of regression is computational convenience; it is easier to compute a+ b idf than to perform a binary search to find the closest bin.",
        "Previous work (Cooper et al., 1994) used multiple regression techniques.",
        "Although our performance is similar (until we include query expansion) we believe that it is safer and easier to treat each value of tf as a separate regression for reasons discussed in table 5.",
        "In so doing, we are basically restricting the regression analysis to such an extent that it is unlikely to do much harm (or much good).",
        "Imposing the limits of 0 < A < idf also serves the purpose of preventing the regression from wandering too far astray.",
        "This table approximates the data in table 1 with A a(t f) + b(t f) • idf .",
        "Note that both the intercepts, a(t f), and the slopes, b(t f), increase with tf (with a minor exception for b(4+)).",
        "cients for method fit-G with comparable coefficients from the multiple regression: A = a2 + b2 • idf + c2 • log(1 + tf) where a2 = -4.1, b2 = 0.66 and c2 = 3.9.",
        "The differences in the two fits are particularly large when t f 0; note that b(0) is negligible (0.05) and b2 is quite large (0.66).",
        "Reducing the number of parameters from 10 to 3 in this way increases the sum of square errors, which may or may not result in a large degradation in precision and recall.",
        "Why take the chance?"
      ]
    },
    {
      "heading": "3 Burstiness",
      "text": [
        "Table 6 is like tables 4 but the binning rule not only uses idf , but also burstiness (B).",
        "Burstiness (Church and Gale, 1995)(Katz, 1996)(Church, 2000) is intended to account for the fact that some very good keywords such as \"Kennedy\" tend to be mentioned quite a few times in a document or not at all, whereas less good keywords such as \"except\" tend to be mentioned about the same number of times no matter what the document",
        "Note that the slopes and intercepts are larger when B = 1 than when B = 0 (except when tf = 0).",
        "Even though A usually lies .between- 0 and idf , we restrict A to 0 < A < idf, just to make sure.",
        "asterisk are worrisome because the bins are too small and/or the slopes fall well outside the normal range of 0 to 1.)",
        "The slopes rarely exceeded .8 is previous models (fit-G and fit-B), whereas fit-E has more slopes closer to 1.",
        "The larger slopes are associated with robust conditions, e.g., terms appearing in the query (where = D), the document (tf > 1) and the expansion (ef > 1).",
        "If a term appears in several documents brought in by query expansion (ef > 2), then the slope can be large even if the term is not explicitly mentioned in the query (where = E).",
        "The interactions among tf, idf, ef and where are complicated and not easily captured with a straightforward multiple regression.",
        "is about.",
        "Since \"Kennedy\" and \"except\" have similar idf values, they would normally receive similar term weights, which doesn't seem right.",
        "Kwok (1996) suggested average term frequency, avtf = TF(t)Idf(t), be used as a tiebreaker for cases like this, where TF(t) = Edtf(t,d) is the standard notion of frequency in the corpus-based NLP.",
        "Table 6 shows how Kwok's suggestion can be reformulated in our empirical framework.",
        "The table shows the slopes and intercepts for ten regressions, one for each combination of tf and B (B = 1 iff avtf is large.",
        "That is, B = 1 iff TF(t)Idf(t) > 1.83 - 0.048 - idf)."
      ]
    },
    {
      "heading": "4 Query Expansion",
      "text": [
        "We applied query expansion (Buckley et al., 1995) to generate an expanded part of the query.",
        "The original query is referred to as the description (D) and the new part is referred to as the expansion (E).",
        "(Queries also contain a narrative (N) part that is not used in the experiments below so that our results could be compared to previously published results.)",
        "The expansion is formed by applying a baseline query engine (fit-B model) to the description part of the query.",
        "Terms that appear in the top k = 10 retrieved documents are assigned to the E portion of the query (where(t) = E), unless they were previously assigned to some other portion of the query (e.g., where(t) = D).",
        "All terms, t, no matter where they appear in the query, also receive an expansion frequency ef, an integer from 0 to k = 10 indicating how many of the top k documents contain t. The fit-E model is: A = a(tf,where,ef) + b(tf,where,ef) idf, where the regression coefficients, a and b, not only depend on tf as in fit-G, but also depend on where the term appears in the query and expansion frequency ef.",
        "We consider 5 values of tf , 2 values of where (D and E) and 6 values of ef (0, 1, 2, 3, 4 or more).",
        "32 of these 60 pairs of coefficients are shown in table 7.",
        "As before, most of the slopes are between 0 and 1.",
        "A is usually between 0 and idf, but we restrict A to 0 < A < idf , just to make sure.",
        "In tables 4-7, the slopes usually lie between 0 and 1.",
        "In the previous models, fit-B and fit-G, the largest slopes were about 0.8, whereas in fit-E, the slope can be much closer to 1.",
        "The larger slopes are associated with very robust conditions, e.g., terms mentioned explicitly in all three areas of interest: (1) the query (where = D), (2) the document (tf > 1) and (3) the expansion (ef > 1).",
        "Under such robust conditions, we would expect to find very little shrinking (downweighting to compensate for uncertainty).",
        "On the other hand, when the term is not mentioned in one of these areas, there can be quite a bit of shrinking.",
        "Table 7 shows that the slopes are generally much smaller when the term is not in the query (where = E) or when the term is not in the expansion (ef = 0).",
        "However, there are some exceptions.",
        "The bottom right corner of table 7 contains some large slopes even though these terms are not mentioned explicitly in the query (where = E).",
        "The mitigating factor in this case is the large ef.",
        "If a term is mentioned in several documents in the expansion (ef > 2), then it is not as essential that it be mentioned explicitly in the query.",
        "With this model, as with fit-G and fit-B, A tends to increase monotonically with tf and idf, though there are some interesting exceptions.",
        "When the term appears in the query (where = D) but not in the expansion (ef = 0), the slopes are quite small (e.g., b(3, D, 0) = 0.11), and the slopes actually decrease as tf increases (b(2, D, 0) = 0.83 > b(3, D, 0) = 0.11).",
        "We normally expect to see slopes of .7 or more when tf > 3, but in this case (b(3, D, 0) = 0.11), there is a considerable shrinking because we very much expected to see the term in the expansion and we didn't. - - As we have seen, the interactions among tf, idf, ef and where are complicated and probably de",
        "use training (with the possible exception of JCB1); methods below the line do not.",
        "pend on many factors such as language, collection, typical query patterns and so on.",
        "To cope with such complications, we believe that it is safer to use histogram methods than to try to account for all of these interactions at once in a single multiple regression.",
        "The next section will show that fit-E has very encouraging performance."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "Two measures of performance are reported: (1) 11 point average precision and (2) R, precision after retrieving Nrei documents, where Nrel is the number of relevant documents.",
        "We used the \"short query\" condition of the NACSIS NTCIR-1 Test Collection (Kando et al., 1999) which consists of about 300,000 documents in Japanese, plus about 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements for testing.",
        "The result of \"short query\" is shown in page 25 of(Kando et al., 1999), which shows that \"short query\" is hard for statistical methods.",
        "Two previously published systems are included in the tables below: JCB1 and BKJJBIDS.",
        "JCB1, submitted by Just System, a company with a commercially successful product for Japanese word-processing, produced the best results using sophisticated (and proprietary) natural language processing techniques.",
        "(Fujita, 1999) BKJJBIDS used Berkeley's logistic regression methods (with about half a dozen variables) to fit term weights to the labeled training material.",
        "Table 8 shows that training often helps.",
        "The methods above the line (with the possible exception of JCB1) use training; the methods below the line do not.",
        "Fit-E has very respectable performance, nearly up to the level of JCB1, not bad for a purely statistical method.",
        "The performance of fit-B is close to that of BKJJBIDS.",
        "For comparison sake, fit-B is shown both with and without the K filter.",
        "The K filter restricts terms to sequences of Katakana and Kanji characters.",
        "BKJJBIDS uses a similar heuristic to eliminate Japanese function words.",
        "Although the K filter does not change performance very much, the use of this filter changes the relative order of fit-B and BKJJBIDS.",
        "These results suggest that",
        "• Ek: require terms to appear in more than k docs brought in by query expansion (ef(t) > k).",
        "formance of the best method (fit-E) to nearly the level of JCB1.",
        "the K filter is slightly unhelpful.",
        "A number of filters have been considered (table 9).",
        "Results vary somewhat depending on these choices, though not too much, which is fortunate, since since we don't understand stop lists very well.",
        "To the extent that there is a pattern, we suspect that words are slightly better than bigrams, and that the E filter is slightly better than the B filter which is slightly better than the K filter.",
        "Table 10 shows that the best filters (Ek) improve the performance of the best method (fit-E) to nearly the level of JCB1.",
        "slightly better than one, and one is slightly better than none.",
        "(UL = upper limit of A < idf; LL = lower limit of 0 < A)",
        "The final experiment (table 11) shows that restricting A to 0 < A < idf improves performance slightly.",
        "The combination of both the upper limit and the lower limit is slightly better than just one limit which is better than none.",
        "We view limits as a robustness device.",
        "Hopefully, they won't have to do much but every once in a while they prevent the system from wandering far astray."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "This paper introduced an empirical histogram-based supervised learning method for estimating term weights, A.",
        "Terms are assigned to bins based on features such as inverse document frequency, burstiness and expansion frequency.",
        "A different A is estimated for each bin and each t f by counting the number of relevant and irrelevant documents associated with the bin and t f value.",
        "Regression techniques are used to interpolate between bins, but care is taken so that the regression cannot do too much harm (or too much good).",
        "Three variations were considered: fit-G, fit-B and fit-E.",
        "The performance of query expansion (fit-E) is particularly encouraging.",
        "Using simple purely statistical methods, fit-E is nearly comparable to JCB1, a sophisticated natural language processing system developed by Just System, a leader in the Japanese word processing industry.",
        "In addition to performance, we are also interested in the interpretation of the weights.",
        "Empirical weights tend to lie between 0 and idf .",
        "We find these limits to be a surprise given that standard term weighting formulas such as tf • idf generally do not conform to these limits.",
        "In addition, we find that A generally grows linearly with idf , and that the slope is between 0 and 1.",
        "We interpret the slope as a statistical shrink.",
        "The larger slopes are associated with very robust conditions, e.g., terms mentioned explicitly in all three areas of interest: (1) the query (where = D), (2) the document (tf > 1) and (3) the expansion (e f > 1).",
        "There is generally more shrinking for terms brought in by query expansion (where = E), but if a term is mentioned in several documents in the expansion (ef > 2), then it is not as essential that the term be mentioned explicitly in the query.",
        "The interactions among t f , idf , where, B, e f , etc., are complicated, and therefore, we have found it safer and easier to use histogram methods than to try to account for all of the interactions at once in a single multiple regression."
      ]
    },
    {
      "heading": "Acknowdedgement",
      "text": [
        "Authors thank Prof. Mitchell P. Marcus of University of Pennsylvania for the valuable discussion about noise reduction in context of information retrieval.",
        "This reseach is supported by Sumitomo Electric."
      ]
    }
  ]
}
