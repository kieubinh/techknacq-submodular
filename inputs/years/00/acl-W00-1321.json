{
  "info": {
    "authors": [
      "Sung Dong Kim",
      "Byoung-Tak Zhang",
      "Yung Taek Kim"
    ],
    "book": "Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora",
    "id": "acl-W00-1321",
    "title": "Reducing Parsing Complexity by Intra-Sentence Segmentation Based on Maximum Entropy Model",
    "url": "https://aclweb.org/anthology/W00-1321",
    "year": 2000
  },
  "references": [
    "acl-A97-1004",
    "acl-C90-3088",
    "acl-C94-1014",
    "acl-J97-2002",
    "acl-P95-1006",
    "acl-W97-0304"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Long sentence analysis has been a critical problem because of high complexity.",
        "This paper addresses the reduction of parsing complexity by intra-sentence segmentation, and presents maximum entropy model for determining segmentation positions.",
        "The model features lexical contexts of segmentation positions, giving a probability to each potential position.",
        "Segmentation coverage and accuracy of the proposed method are 96% and 88% respectively.",
        "The parsing efficiency is improved by 77% in time and 71% in space."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Long sentence analysis has been a critical problem in machine translation because of high complexity.",
        "In EBMT (example-based machine translation), the longer a sentence is, the less possible it is that the sentence has an exact match in the translation archive, and the less flexible an EBMT system will be (Cranias et al., 1994).",
        "In idiom-based machine translation (Lee, 1993), long sentence parsing is difficult because more resources are spent during idiom recognition phase as sentence length increases.",
        "A parser is often unable to analyze long sentences owing to their complexity, though they have no grammatical errors (Nasukawa, 1995).",
        "In English-Korean machine translation, idiom-based approach is adopted to overcome the structural differences between two languages and to get more accurate translation.",
        "The parser is a chart parser with a capability of idiom recognition and translation, which is adapted to English-Korean machine translation.",
        "Idioms are recognized prior to syntactic analysis and the part of a sentence for an idiom takes an edge in a chart (Winograd, 1983).",
        "When parsing long sentences, an ambiguity of an idiom's range may cause more edges than the number of words included in the idiom (Yoon, 1994), which increases parsing complexity much.",
        "A parser of practical machine translation system should be able to analyze long sentences in a reasonable time.",
        "Most context-free parsing algorithms have 0(n3) parsing complexities in terms of time and space, where n is the length of a sentence (Tomita, 1986).",
        "Our work is motivated by the fact that parsing becomes more efficient, if n becomes shorter.",
        "This paper deals with the problem of parsing complexity by way of reducing the length of sentence to be analyzed.",
        "This reduction is achieved by intra-sentence segmentation, which is distinguished from intersentence segmentation that is used for text categorization (Beeferman et al., 1997) or sentence boundary identification (Palmer and Hearst, 1997) (Rey-nar and Ratnaparkhi, 1997).",
        "Intra-sentence segmentation plays a role as a preliminary step to a chart-based, context-free parser in English-Korean machine translation.",
        "There have been several methods for reducing parsing complexities by intra-sentence segmentation.",
        "In (Lyon and Frank, 1995)(Lyon and Dickerson, 1997), they took advantage of the fact that the declarative sentences almost always consist of three segments: [pre-subject : subject : predicate].",
        "The complexity could be reduced by decomposing a sentence into three sections.",
        "Pattern rules (Li et al., 1990) and sentence patterns (Kim and Kim, 1995) were used to segment long English sentences.",
        "They showed low segmentation coverage, which means that many of long sentences are not segmented by the pattern rules or sentence patterns.",
        "And they require much human efforts to construct pattern rules or collect sentence patterns.",
        "These factors may prevent them being applicable to practical machine translation systems.",
        "This paper presents a trainable model for identifying potential segmentation positions",
        "in a sentence and determining appropriate segmentation positions.",
        "Given a corpus annotated with segmentation positions, our model automatically learns the contextual evidences about segmentation positions, which relieves human of burden to construct pattern rules or sentence patterns.",
        "These evidences are combined under the maximum entropy framework (Jaynes, 1957) to estimate the probability for each position.",
        "By intra-sentence segmentation based on the proposed model, we achieve more improved parsing efficiency by 77% in time and 71% in space.",
        "In Section 2 we introduce the maximum entropy model.",
        "Section 3 describes features incorporated into the model and the process of identifying potential segmentation positions.",
        "The determination schemes of segmentation positions are described in Section 4.",
        "Segmentation performance of the model is presented with the degree of contribution to efficient parsing by the segmentation in Section 5.",
        "We also compare our approach with other intra-sentence segmentation approaches.",
        "Section 6 draws conclusions and presents some further works."
      ]
    },
    {
      "heading": "2 Maximum Entropy Modeling",
      "text": [
        "Sentence patterns or pattern ruels specify the substructures of the sentences.",
        "That is, segmentation positions are determined in view of the global sentence structure.",
        "If there is no matched rules or patterns with a given sentence, the sentence could not be segmented.",
        "We assume that whether a word is a segmentation position depends on its surrounding context.",
        "We try to find factors that affect the determination of segmentation positions.",
        "Maximum entropy is a technique for automatically acquiring knowledge from incomplete information, without making any unsubstantiated assumptions.",
        "It masters subtle effects so that we may accurately model subtle dependencies.",
        "It does not make any unwarranted assumptions, which means that maximum entropy learns exactly what the data says.",
        "Therefore it can perform well on unseen data.",
        "The idea is to construct a model that assigns a probability to each potential segmentation position in a sentence.",
        "We build a probability distribution p(y1x), where y E {0, 1} is a random variable specifying the potential segmentation position in a context x.",
        "A feature of a context is a binary-valued indicator function f expressing the information about a specific context.",
        "Given a training sample of size N, (xi, yi), - - , (xist, yN), an empirical probability distribution can be defined as",
        "where #(x, y) is the number of occurrences of (x, y).",
        "The expected value of feature fi with respect to the empirical distribution 23(x, y) is expressed as",
        "and the expected value of h with respect to the probability distribution p(ylx) is",
        "where /3(x) is the empirical distribution of x in the corpus.",
        "We want to build probability distribution p(ylx) that is required to accord to the feature fi useful in selecting segmentation positions: p(fi) = 13(fi) for all h E .F, where ..T. is the set of candidate features.",
        "This makes the probability distribution be built on only training data.",
        "Given a feature set .F, let C be the subset of all distributions P that satisfies the requirement",
        "We choose a probability distribution consistent with all the facts, but otherwise as uniform as possible.",
        "The uniformity of the probability distribution p(ylx) is measured by the conditional entropy:",
        "Thus, the probability distribution with maximum entropy is the most uniform distribution.",
        "In building a model, we consider the linear exponential family Q given as",
        "That is, the model we want to build is pi, = arg max H(p) = arg max LI-5(q).",
        "pECgE Q where Ai are real-valued parameters and",
        "An intersection of the class 2 of exponential models with the class of desired distribution",
        "(1) is nonempty, and the intersection contains the maximum entropy distribution and furthermore it is unique (Ratnaparkhi, 1994).",
        "Finding p,, E C that maximizes H(p) is a problem in constrained optimization, which cannot be explicitly written in general.",
        "Therefore,",
        "we take advantage of the fact that the models in 2 that satisfy p(fi) = 73(1i) can be explained under the maximum likelihood framework (Ratnaparkhi, 1994).",
        "Maximum likelihood principle also gives the unique distribution AK, the intersection of the class 2 with C. We assume each occurrence of (x,y) is sampled independently.",
        "Thus, log-likelihood LI3(p) of the empirical distribution ii as predicted by a model p can be defined as The parameters Ai of exponential model (2) are obtained by the Generalized Iterative Scaling algorithm (Darroch and Ratcliff, 1972)."
      ]
    },
    {
      "heading": "3 Construction of Features",
      "text": [
        "This section describes the features.",
        "From a corpus, contextual evidences of segmentation positions are collected and combined, resulting in features.",
        "The features are used in identifying potential segmentation positions and included in the model."
      ]
    },
    {
      "heading": "3.1 Segmentable Positions and Safe Segmentation",
      "text": [
        "A sentence is constructed by the combination of words, phrases, and clauses under the well-defined grammar A sentence can be segmented into shorter segments that correspond to the constituents of the sentence.",
        "That is, segments correspond to the nonterminal symbols of the context-free grammars The posi1Nonterminal symbols include the ones for phrases, such as NP (noun phrase) and VP (verb phrase), tion of a word is called segmentable position that can be a starting position of a specific segment.",
        "Though the analysis complexity can be reduced by segmenting a sentence, there is a mis-segmentation risk that causes parsing failures.",
        "A segmentation can be called safe segmentation that results in a coherent blocks of words.",
        "In English-Korean translation, safe segmentation is defined as the one which generates safe segments.",
        "A segment is safe, when there is a syntactic category symbol N1' dominating the segment and the segment can be combined with adjacent segments under a given grammar In Figure 1, (a) is an unsafe segmentation because the second segment cannot be analyzed into one syntactic category, resulting in parsing failure.",
        "By the safe segmentation (b), the first segment corresponds to a noun phrase and the second to a verb phrase, so that we can get a correct analysis result.",
        "in English-Korean translation."
      ]
    },
    {
      "heading": "3.2 Lexical Contextual Constraints",
      "text": [
        "A lexical context of a word includes seven-word window: three to the left of a word and three to the right of a word and a word itself.",
        "It also includes the part-of-speeches of these words, subcategorization information for two words to the left, and position value.",
        "The position value posi_v of the ith word wi is calculated as",
        "where n is the number of words and R2 represents the number of regions in the sentence.",
        "Region is the sequentially ordered block of and the ones for clauses like RLCL (relative clause), SUBCL (subordinate clause).",
        "21t is a heuristically set value, and we set R as 4.",
        "The students who study hard will pass the exam The students who study hard will pass the exam",
        "words in a sentence, and posi_v represents the region in which a word lies.",
        "It is included to reflect the influence of the position of a word on being a segmentation position.",
        "Thus, the lexical context of a word is represented by 17 attributes as shown in Figure 2. s_position?",
        "An example of a training data and a resulting lexical context is shown in Figure 3.",
        "A symbol 'It' represents a segmentation position marked by human annotators.",
        "Therefore, the lexical context of word when includes the value 1 for attribute s_position?",
        "and followings: three words to the left of when (became, terribly, and worried) and part-of-speeches of each word (VERB ADV ADJ), three words to the right (they, saw, and what) and part-of-speeches"
      ]
    },
    {
      "heading": "(PRON VERB PRON), subcategorization",
      "text": [
        "information for two words to the left (0 1), and position value (2).",
        "Of course his parents became terribly worried #when they saw what was happening",
        "a lexical context.",
        "To get reliable statistics, much training data is required.",
        "To alleviate this problem, we generate lexical contextual constraints by combining lexical contexts and collect statistics for them.",
        "To generate lexical contextual constraints and to identify segmentable positions, we define two operations",
        "where Y is don't-care term accepting any value.",
        "A lexical contextual constraint is generated as a result of join operation.",
        "The consistency is defined as",
        "The algorithm for generating lexical contextual constraints is shown in Figure 4.",
        "Input: a set of active lexical contexts LC,,, = {lci .",
        ".",
        ".lc,,} for word w, where lci = (a1, ... , an).",
        "Output: a set of lexical contextual",
        "2.",
        "Do the followings for each lci E LCu, (a) For all lci(j 0 i), Count(lci) = # of matched attributes with lci (b) max_cntCount(lc3) (c) For all Ici=, warhgermea3Cci ociEunLtc(lwci) = max_cnt,",
        "contextual constraints.",
        "A lcc plays the role of a feature.",
        "Following is an example of a feature.",
        "{ 1 if xword = \"that\" and",
        "We collect the statistics for each lcc.",
        "The frequency of each lcc is counted as the number of lexical contexts that satisfy the consistency operation with the Icc.",
        "Identifying segmentable positions is performed with the consistency operation with the lexical context of word w and lcc E LCC,,,.",
        "The word whose lexical context is consistent with /cc is identified as a segmentable position."
      ]
    },
    {
      "heading": "4 Determination Schemes of",
      "text": []
    },
    {
      "heading": "Segmentation Posit ions",
      "text": [
        "Segmentation positions are determined through two steps: identifying segmentable positions and selecting the most appropriate position among them.",
        "Segmentable positions are identified using the consistency operation.",
        "Maximum entropy model in Section 2 gives a probability to each position.",
        "Segmentation performance is measured in terms of coverage and accuracy.",
        "Coverage is the ratio of the number of actually segmented sentences to the number of segmentation target sentences that are longer than a words, where a is a fixed constant distinguishing long sentences from short ones.",
        "Accuracy is evaluated in terms of the safe segmentation ratio.",
        "They are defined as follows:",
        "# of actually segmented Sent.",
        "coverage = # of Sent.",
        "to be segmented (3) # of Sent.",
        "with safe segmentation accuracy = # of actually segmented Sent.",
        "(4)"
      ]
    },
    {
      "heading": "4.1 Baseline Scheme",
      "text": [
        "No contextual information is used in identifying segmentable positions.",
        "They are empirically identified.",
        "A word that is tagged as a segmentation position more than 5 times is identified as a segmentable position.",
        "A set of segmentable positions, D, is as follows.",
        "segmentation position w, is selected as the one that has highest p(wi) value:",
        "This scheme serves as a baseline for comparing the segmentation performance of the models."
      ]
    },
    {
      "heading": "4.2 A Scheme using Lexical Contextual Constraints",
      "text": [
        "Lexical contextual constraints are used in identifying segmentable positions.",
        "Compared with the baseline scheme, this scheme considers contextual information of a word.",
        "All consistent words with the defined lexical contextual constraints form a set of segmentable positions D. {wi (lcivi /cc,,,,) = 1}.",
        "The maximum likelihood principle gives a probability distribution for p(y I lccu,i), where y E 10, 11.",
        "Segmentation appropriateness is evaluated by p(1 I lccw2).",
        "A position with the highest p(1 lcc,i) becomes a segmentation position:"
      ]
    },
    {
      "heading": "4.3 A Scheme using Lexical Contextual Constraints with Word Sets",
      "text": [
        "Due to insufficient training samples for constructing lexical contextual constraints, some segmentable positions may not be identified.",
        "To alleviate this problem we introduce word sets whose elements have linguistically similar features.",
        "We define four word sets: coordinate conjunction set, subordinate conjunction set, interogative set, auxiliary verb set.",
        "The categories of word sets and the examples of their members are shown in Table 1.",
        "= {wi I wi is tagged as segmentation position and #(tagged wi) 5} In order to select the most appropriate position, the segmentation appropriateness of each position is evaluated by the probability of word wi: # of tagged wi p(wt)= # of wi in the corpus p(wi) represents the tendency that word wi will be used as a segmentation position.",
        "A",
        "members, interogatives 5 members, and auxiliary verbs have 12 members now.",
        "The words belonging to each word set are treated equally.",
        "Lexical contextual constraints are constructed for words and word sets, so the statistics is collected for both of them.",
        "The set of segmentable positions V is defined somewhat differently as:",
        "where wsj denotes a word set to which the jth word in a sentence belongs.",
        "In this scheme, p(1 1 lcc,t) or p(1 I lcc) expresses the segmentation appropriateness of the position.",
        "Therefore, a segmentation position is determined by tn.",
        "= arg max{p(1 I Iccu,i), p(1 I icews;)}.",
        "{10wsi}ED"
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Corpus and Construction of the Maximum Entropy Model",
      "text": [
        "We construct the corpus from two different domains, where the sentences longer than 15 words are extracted3.",
        "The training portion is used to generate lexical contextual constraints and to collect statistics for maximum entropy model construction.",
        "From high school English texts, 1500 sentences are tagged with segmentation positions by human.",
        "Two people who have some knowledge about English syntactic structures read sentences, and marked words as segmentation positions where they paused.",
        "After generating lexical contextual constraints, we constructed the maximum entropy model p(ylx), where x is a lexical contextual constraint and y E {0, 1}.",
        "The model incorporates features that occur more than 5 times in the training data.",
        "3626 candidate features were generated without word sets and 3878 features with word sets.",
        "In Table 2, training time and the number of active features of the model are shown.",
        "Segmentation performance is evaluated using test portion that consists of 1800 sentences from two domains: high school English texts and the Byte Magazine."
      ]
    },
    {
      "heading": "5.2 Segmentation Performance",
      "text": [
        "In addition to coverage and accuracy, SC value is also defined to express the degree of contribution to efficient parsing by segmentation.",
        "It is the ratio of the sentences that can benefit from intra-sentence segmentation.",
        "If a long sentence is not segmented or is segmented at unsafe segmentation positions, the sentence is called a segmentation error sentence.",
        "A sentence longer than a words is considered as the segmentation target sentence, where a is set to 12.",
        "Table 3 compares segmentation performance for each determination scheme."
      ]
    },
    {
      "heading": "Word Sets",
      "text": [
        "By the comparison of the baseline scheme with others, the accuracy is observed to depend on the context information.",
        "Word sets are helpful for increasing coverage with less degradation of accuracy.",
        "Each scheme has superiority in terms of the different measures.",
        "But in terms of applicability to practical systems, the third scheme is best for our purpose.",
        "Table 4 shows the segmentation performance of the scheme using LCC with word sets.",
        "SC value for the sentences from the same domain as training data is about 0.88, and",
        "about 0.85 for the sentenes from the Byte Magazine.",
        "Though they slightly differ between test domains, about 87% of long sentences can be parsed with less complexity and without causing parsing failures.",
        "It suggests that the intra-sentence segmentation method can be utilized for efficient parsing of the long sentences."
      ]
    },
    {
      "heading": "5.3 Parsing Efficiency",
      "text": [
        "Parsing efficiency is generally measured by the required time and memory for parsing.",
        "In most cases, parsing sentences longer than 30 words could not complete without intra-sentence segmentation.",
        "Therefore, the parsing is performed for the sentences longer than 15 and less than 30 words.",
        "Ultra-Sparc 30 machine is used for experiments.",
        "The efficiency improvement was measured by",
        "where tunseg and Munseg are time and memory during parsing without segmentation and tseg, mseg are for the parsing with segmentation.",
        "Table 5 summarizes the results.",
        "By segmenting long sentences into several manageable-sized segments, we can parse long sentences with much less time and space."
      ]
    },
    {
      "heading": "5.4 Comparison with Related Works",
      "text": [
        "The intra-sentence segmentation method based on the maximum entropy model is compared with other approaches in terms of the",
        "segmentation coverage and the improvement of parsing efficiency.",
        "In (Lyon and Frank, 1995)(Lyon and Dickerson, 1997), a sentence is segmented into three segments.",
        "Though parsing efficiency can be improved by segmenting a sentence, this method may be applied to only simple sentences4.",
        "Long sentences are generally coordinate sentences5 or complex sentences6.",
        "They have more than two subjects, so applying this method to such sentences seems to be inappropriate.",
        "In (Kim and Kim, 1995), sentence patterns are used to segment long sentences.",
        "This method improve parsing efficiency by 30% in time and 58% in space.",
        "However collecting sentence patterns requires much human efforts and segmentation coverage is only about 36%.",
        "Li's method (Li et al., 1990) for sentence segmentation also depends upon manual-intensive pattern rules.",
        "Segmentation coverage seems to be unsatisfactory for practical machine translation system.",
        "The proposed method can be applied to coordinate and complex sentences as well as simple sentences.",
        "It shows segmentation coverage of about 96%.",
        "In addition, it needs no other human efforts except for constructing training data.",
        "Human annotators have only to read sentences and mark segmentation positions, which is more simple than collecting pattern rules or sentence patterns.",
        "We can also get much improved parsing efficiency: about 77% in time and about 71% in space."
      ]
    },
    {
      "heading": "6 Conclusion and Future Work",
      "text": [
        "Practical machine translation systems should be able to accommodate long sentences.",
        "Thus intra-sentence segmentation is required as a means for reducing parsing complexity.",
        "This paper presents a method for intra-sentence segmentation based on the maximum entropy model.",
        "The method builds statistical models automatically from a text corpus to provide the segmentation appropriateness for safe segmentation.",
        "In the experiments with 1800 test sentences, about 87% of them were benefited from segmentation.",
        "The statistical intra-sentence segmentation method can also relieve human of the burden of constructing information, such as segmentation rules or sentence patterns.",
        "Experiments suggest that the proposed maximum entropy models can be incorporated into the parser for practical machine translation systems.",
        "Further works can be done in two directions.",
        "First, studies on recovery mechanisms for unsafe segmentation before parsing seem necessary since unsafe segmentation may cause parsing failures.",
        "Second, parsing control mechanisms should be studied that exploit the characteristics of segmentation positions and the parallelism among segments.",
        "This will enhance parsing efficiency further."
      ]
    }
  ]
}
