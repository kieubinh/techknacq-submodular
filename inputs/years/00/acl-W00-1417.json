{
  "info": {
    "authors": [
      "Jacques Robin",
      "Eloi L. Favero"
    ],
    "book": "International Conference on Natural Language Generation",
    "id": "acl-W00-1417",
    "title": "Content Aggregation in Natural Language Hypertext Summarization of OLAP and Data Mining Discoveries",
    "url": "https://aclweb.org/anthology/W00-1417",
    "year": 2000
  },
  "references": [
    "acl-A94-1002",
    "acl-J97-2001",
    "acl-P98-2199",
    "acl-W94-0319",
    "acl-W96-0403"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a new approach to paratactic content aggregation in the context of generating hypertext summaries of OLAP and data mining discoveries.",
        "Two key properties make this approach innovative and interesting: (1) it encapsulates aggregation inside the sentence planning component, and (2) it relies on a domain independent algorithm working on a data structure that abstracts from lexical and syntactic knowledge.",
        "1 Research context: hypertext executive summary generation for intelligent decision-support In this paper, we present a new approach to content aggregation in Natural Nanguage Generation (NLG).",
        "This approach has been developed for the NLG system HYSSOP (HYpertext Summary System of On-line analytical Processing) which summarizes OLAP (On-Line Analytical Processing) and Data Mining discoveries into an hypertext report.",
        "HYSSOP is itself part of the Intelligent Decision-Support System (IDSS) MATRIKS (Multidimensional Analysis and Textual Reporting for Insight Knowledge Search), which aims to provide a comprehensive knowledge discovery environment through seamless integration of data warehousing, OLAP, data mining, expert system and NLG technologies."
      ]
    },
    {
      "heading": "1.1 The MATRIKS intelligent decision-support system",
      "text": [
        "The architecture of MATRIKS is given in Fig.",
        "I.",
        "It extends previous cutting-edge environments for Knowledge Discovery in Databases (KDD) such as DBMiner (Han et al. 1997) by the integration of: e a data warehouse hypercube exploration expert system allowing automation and expertise legacy of dimensional data warehouse exploration strategies developed by human data analyst using OLAP queries and data mining tools; • an hypertext executive summary generator reporting data hypercube exploration insights in the most concise and familiar way: a few web pages of natural language.",
        "These two extensions allow an IDSS to be used directly by decision makers without constant mediation of a data analyst."
      ]
    },
    {
      "heading": "1.2 The HYSSOP natural language hypertext summary generator",
      "text": [
        "To our knowledge, the development of HYSSOP is pioneer work in coupling OLAP and data mining with natural language generation, Fig. 2.",
        "We view such coupling as a synergetic fit with tremendous potential for a wide range of practical applications.",
        "In a nutshell', while NLG is the only technology able to completely fulfill the reporting needs of ' See Favero (2000) for further justification for this view, as well as for details on the motivation and technology underlying MATRIKS.",
        "OLAP and data mining, these two technologies are reciprocally the only ones able to completely fulfill the content determination needs of a key NLG application sub-class: textual summarization of quantitative data.",
        "Generators that summarize large amount of quantitative data by a short natural language text (such as ANA (Kukich 1988), GOSSIP (Carcagno and lordanskaja 1993), PLANDoc (McKeown, Kukich and Shaw 1994) among others) generally perform content determination by relying on a fixed set of domain-dependent heuristic rules.",
        "Such an approach suffers from two severe limitations that prevent it from reporting the most interesting content from an underlying database:",
        "• it does not scale up for analytical contexts with high dimensionality and which take into account the historical – evolution of data through time; such complex context would require a combinatorially explosive number of summary content determination heuristic rules; • it can only select facts whose class have been thought ahead by the rule base author, while",
        "in most cases, it is its very unexpectedness that makes a fact interesting to report;",
        "OLAP and data mining are the two technologies that emerged to tackle precisely these two issues: for OLAP, efficient search in a high dimensionality, historical data search space, and for data mining, automatic discovery in such spaces, of hitherto unsuspected regularities or singularities.",
        "In the MATRIKS architecture, heuristic rules are not used to define content worth reporting in a data warehouse executive summary.",
        "Instead, they are used to guide the process of searching the warehouse for unexpected facts using OLAP and data mining operators.",
        "A data warehouse hypercube exploration expert system encapsulates such rules in its knowledge base to perform content determination.",
        "An example output of such expert system, and input to HYSSOP, is given in Fig. 3: the data cells selected for inclusion in the output textual summary are passed along with their OLAP context and the data mining annotations that justify their relevance.",
        "One output generated by HYSSOP from this input is given in Fig. 4. and Fig. 5.",
        "® Birch Beer with a 42% national increase from September to October • Diet Soda with a 40% decrease in the Eastern region from July to August.",
        "At the next level of idiosyncrasy came: • Cola's Colorado sales, falling 40% from July to August and then a further 32% from September to October; • again Diet Soda Eastern sales, falling 33% from September to October.",
        "Less aberrant but still notably atypical were: • again nationwide Birch Beer sales' -12% from June to July and -10% from November to December • Cola's 11% fall from July to August in the Central region and 30% dive in Wisconsin from August to September; ® Diet Soda sales' 19% increase in the Southern region from July to August, followed by its two opposite regional variations from August to September, +10% in the East but -17% in the West; • national Jolt Cola sales' +6% from August to September.",
        "To know what makes one of these variations unusual in the context of this year's sales, click on it.",
        "The 40% decrease in Diet Soda sales was very atypical mostly due to the combination of the two following facts:",
        "• across the rest of the regions, the July to August average variation for that product was 9% increase; • over the rest of the year, the average monthly decrease in Eastern sales for that product was only 7%.\" • across the rest of the product line, the Eastern sales variations from July to August was 2%",
        "The architecture of HYSSOP is given in Fig. 2.",
        "HYSSOP is entirely implemented in LIFE (Ait-Kaci and Lincoln.1989), a language that extends Prolog with functional programming, arityless feature structure unification and hierarchical type constraint inheritance.",
        "For content realization, HYSSOP relies on feature structure unification.",
        "Lexicalization is inspired from the approach described in (Elhadad.",
        "McKeown and Robin 1997), while surface syntactic realization follows the approach described in (Favero and Robin.200041-1Y-SSOP--tnakes two innovative contributions to NLG research: one to hypertext content planning presented in (Favero and Robin 2000a) and one to content aggregation presented in the rest of this paper."
      ]
    },
    {
      "heading": "2 Research focus: content aggregation",
      "text": [
        "in natural language generation Natural language generation system is traditionally decomposed in the following subtasks: content determination, discourse-level content organization, sentence-level content organization, lexical content realization and grammatical content realization.",
        "The first three .subtasks togethera.re_often.",
        "referred to .as zontent planning, and the last two together as linguistic realization.",
        "This separation is now fairly standard and most implementations encapsulate each task in a separate module (Robin 1995), (Reiter 1994).",
        "Another generation subtask that has recently received much attention is content aggregation.",
        "However, there is still no consensus on the exact scope of aggregation and on its precise relation with the five standard generation tasks listed above.",
        "To avoid ambiguity, we define aggregation here as: grouping several content units, sharing various semantic features, inside a single linguistic structure, in such a way that the shared features are maximally factored out and minimally repeated in the generated text.",
        "Defined as above, aggregation is essentially a key subtask of sentence planning, As such, aggregation choices are constrained by discourse planning decisions and they in turn constrain lexical choices.",
        "In HYSSOP, aggregation is carried out by the sentence planner in three steps:",
        "1. content factorization, which is performed on a tabular data structure called a Factorization Matrix (FM) ; 2. generation from the FM of a discourse tree representing the hypertext plan to pass down to the lexicalizer; 3. top-down traversal of the discourse tree to",
        "detect content units with shared features occurring in non-adjacent sentences and annotate them as .anaphora.",
        "- Such annotations are then used by the lexicalizer to choose the appropriate cue word to insert near or in place of the anaphoric item."
      ]
    },
    {
      "heading": "2.1 Content factorization in HYSSOP",
      "text": [
        "The key properties of the factorization matrix that sets it apart from previously proposed data structures on which to perform aggregation are that:",
        "▪ it fully abstracts from lexical and syntactic information; • ..it..fo.cuses.on two_types.",
        ":of...information kept separate in most generators, (1) the semantic features of each sentence constituent (generally represented only before lexicalization), and (2) the linear precedence constraints between them (generally represented only late during syntactic realization); • it visually captures the interaction between",
        "the two, which underlies the factorization phenomenon at the core of aggregation.",
        "In HYSSOP, the sentence planner receives as input from the discourse planner an FM representing the yet unaggregated content to be conveyed, together with an ordered list of candidate semantic dimensions to consider for outermost factoring.",
        "The pseudo-code of HYSSOP's aggregation algorithm is given in Fig. 10.",
        "We now illustrate this algorithm on the input example FM that appears inside the bold subframe of the overall HYSSOP input given in Fig. 3.",
        "For this example, we assume that the discourse planner directive is to factor out first the exception dimension, followed by the product dimension, i.e., FactoringStrategy [except,product].",
        "This example illustrates the mixed initiative choice of the aggregation strategy: part of it is dictated by the discourse planner to ensure that aggregation will not adversely affect the high-level textual organization that it carefully planned.",
        "The remaining part, in our example factoring along the place and time dimensions, is left to the initiative .of ,the sentence planner.",
        "The.",
        "first step of HYSSOP's aggregation algorithm is to shift the priority dimension D of the factoring strategy to the second leftmost column of the FM.",
        "The second step is to sort the FM rows in (increasing or decreasing) order of their D cell values.",
        "The third step is to horizontally slice the",
        "FM into row groups with identical D cell values.",
        "The fourth step is to merge these identical cells and annotate the merged cell with the number of cells that it replaced.",
        "The FM resulting from these four first steps on the input FM inside the bold subframe of Fig. 3 using exception as factoring dimension is given in Fig. 6.",
        "The fifth step consistsLofreenrsively calling the entire aggregation algorithm inside each row group on the sub-FM to the right of D, using the remaining dimensions of the factoring strategy.",
        "Let us now follow one such recursive call: the one on the sub-FM inside a bold subframe in Fig. 6 to the right of the exception column in the third row group.",
        "The result of the first four aggregation steps of this recursive call is given in Fig. 7.",
        "This time it is the product dimension that has been left-shifted and that provided the basis for row sorting, row grouping and cell merging.",
        "Further recursive calls are now triggered.",
        "These calls are different from the preceding ones, however, in that at this point all the input constraints provided by the discourse planner have already been satisfied.",
        "It is thus now up to the sentence planner to choose along which dimension to perform the next factorization step.",
        "In the current implementation, the column with the lowest number of distinct values is always chosen.",
        "In our example, this translates as factoring along the time dimension for some row groups and along the space dimension for the others.",
        "The result of the recursive aggregation call on the sub-FM inside the bold frame of Fig. 7 is given in Fig. 8.",
        "In this case, factoring occurred along the time dimension.",
        "The fully aggregated FM resulting from all the recursive calls is given in Fig. 9.",
        "Note how the left to right embedding of its cells reflects exactly the left to right embedding of the phrases in the natural language summary of Fig. 4 generated from it."
      ]
    },
    {
      "heading": "2.2 Cue word generation in HYSSOP",
      "text": [
        "Once content factorization is completed, the sentence planner builds in two passes the discourse tree that the lexicalizer expects as input.",
        "In the first pass, the sentence planner patterns the recursive structure of the tree (that itself prefigures the output text linguistic constituent structure) after the left to right and narrowing embedding of sub-matrices inside the FM.",
        "In the second pass, the sentence planner traverses this initial discourse tree to enrich it with anaphoric annotations that the lexicalizer needs to generated cue words such as \"again\", \"both\", \"neither\", \"except\" etc.",
        "Planning cue words can be considered part of aggregation since it makes the aggregation structures explicit to the reader and prevents ambiguities that may otherwise be introduced by aggressive content factorization.",
        "A fragment of the sentence planner output discourse tree built form \"the aggregated FM of Fig. 9 is given in Fig. 12, The discourse tree spans horizontally with its root to the left of the feature structure and its leaves to the right.",
        "Note in Fig. 12 the cue word directive: fanaph=loccur,--rd, repeated (product, region//I.",
        "It indicates that this is the second mention in the text of a content unit with product \"Birch Beer\" and Tegion' – -nation.",
        "'The-I exi cal izer uses this annotation to generate the cue word \"again\" before the second reference to \"nationwide Birch Beer sales\".",
        "O buildFactoringStrategy(Matrix): returns inside a list a pair (Dim,increasing) where Dim is the matrix's dimension (i.e., column) with the lowest number of distinct values.",
        "▪ leftShiftColumn (Matrix,Dim1): moves Dim 1 to the second leftmost column next to the cell id column.",
        "• sortRows(Matrix,Diml,Order): sorts the Matrx's rows in order of their Diml cell value; Order specifies whether the order should be increasing or decreasing.",
        "A special class of aggregation-related cue phrases involves not only the sentence planner and the lexicalizer but also the discourse planner.",
        "One discourse strategy option that HYSSOP implements is to precede each aggregation group by a cue phrase explicitly mentioning the group's cardinal.",
        "An example summary front page generated using such a strategy is given in Fig. 11.",
        "The count annotation in the cell merging function of HYSSOP's aggregation algorithm are computed for that purpose.",
        "While the decision to use an explicit",
        "count discourse strategy lies within the discourse planner and their realization as cue phrases are planner, the counts are computed by the sentence carried out by the lexicalizer.",
        "Last year, there were 13 exceptions in the beverage product line.",
        "The most striking was Birch Beer's 42% national fall from Sep to Oct.",
        "The remaining exceptions clustered around four products were: Again, Birch Beer's sales accounting for other two national exceptions, both decreasing mild values:",
        "1. a 12% from Jun to Jul; 2. a 10% from Nov to Dec; ▪ Cola's sales accountinglarfounexceptions: _ 1. two medium in Colorado, a 40% from Jul to Aug arid a 32% from Aug to Sep; 2. two mild, a 11% in Wisconsin from Jul to Aug and a 30% in Central region from Aug to Sep; ® Diet Soda accounting for 5 exceptions: 1. one strong, a 40% slump in Eastern region from Jul to Aug; 2. one medium, a 33% slump in Eastern region from Sep to Oct; 3. three mild: two increasing, a 10 % in Eastern region from Aug to Sep and a 19% in Southern region from Jul to Aug; and one falling, a 17% in Western region from Aug to Sep; • Finally, Jolt Cola's sales accounting for one mild exception, a 6% national fall from Aug to Sep.",
        "common Exceptionaltity = high %%The most atypical sales variations from one moth to the next occurred for distinct = cat =msg.",
        "attr =[product =\"Birch beer\", time =9, place =nation, var=+42] %%Birch Beer with a 42% national increase from Sept to Oct cat =msg, attr qproduct =\"Diet Soda\", time =7, place =east, var--401 %%Diet Soda with a 40% decrease in the Eastern region from Jut to' ug cat =aggr, level=1, ngroup=2, nmsg=3 common exceptionallity = medium %%At next level of idiosyncrasy came: cat =aggr, level =2.",
        "ngroup =2, nmsg=2.",
        "common 1 product=Cola, place=Colorado %% Cola's sales distinct cat=msg, attr=[time=7, var =40] %% falling 40% from Jun to Jul cal=msg, attr=[time=9 var =-32 %% and then a further 32 from Sep to Oct cat =msg, attr .",
        "[product =\"Diet Soda\", time =9, place =east, var=-33 anaph foccurr =2nd, repeated=fproduct, place] %% again Diet Soda Eastern sales, falling 33% from Sep to Oct %% Less aberrant but still notably atypical were:",
        "Fig.",
        "12 – Fragment al.",
        "LIFE feature structure representing the discourse tree output of the sentence planner and input to the lexicalizer."
      ]
    },
    {
      "heading": "3 Related work in content aggregation",
      "text": [
        "The main previous works on content aggregation are due to: O (Dalianis 1995.",
        "1996), whose ASTROGEN system generates natural language paraphrases of formal software specification for validation purposes; 6 (Huang and Fiedler 1997), whose PROVERB system generates natural language mathematical proofs from a theorem prover reasoning trace; o (Robin and McKeown, 1996), whose STREAK system generates basketball game summaries from a semantic network",
        "representing the key game statistics and their historical context; ® (Shaw 1998), whose CASPER discourse and sentence planner has been used both in the PLANDoc system that generates telecommunication equipment installation plan documentation from an expert system trace and the MAGIC system that generates ICU patient status - briefs from medical-measurements.",
        "In this section, we briefly compare these research efforts with ours along four dimensions: (1) the definition of aggregation and the scope of the aggregation task implemented in the generator, (2) the type of representation the generator takes as input and the type of output text that it produces, (3) the generator's architecture and the localization of the aggregation task within it, and (4) the data structures and algorithms used to implement aggregation."
      ]
    },
    {
      "heading": "3.1 Definition of the aggregation task",
      "text": [
        "The definition of aggregation that we gave at the beginning of previous section is similar to those provided by Dalianis and Huang, although it focuses on common feature factorization to insure aggregation remains a proper subset of sentence planning.",
        "By viewing aggregation only as a process of combining clauses, Shaw's definition is more restrictive.",
        "In our view, aggregation is best handled prior to commit to specific syntactic categories and the same abstract process, such the algorithm of Fig. 10, can be used to aggregate content units inside linguistic constituents of any syntactic category (clause, nominal, prepositional phrases, adjectival phrases, etc.).",
        "In terms of aggregation task coverage, HYSSOP focuses on paratactic forms of aggregation.",
        "In contrast, ASTROGEN, CASPER, PROVERB and STREAK also perform hypotactic and paradigmatic aggregation."
      ]
    },
    {
      "heading": "3.2 Input representation and generated",
      "text": [
        "output text A second characteristic that sets HYSSOP apart from other generators performing aggregation is the nature of its input: a set of data cells extracted from a dimensional data warehouse hypercube.",
        "In contrast, the other systems all take as input either a semantic .network extracted from a knowledge base or a pre-linguistic representation of the text to generate such as Meteer's text structure (Meteer 1992) or Jackendoffs semantic structure (Jackendoff 1985).",
        "Such natural language processing riented Inputs tend le'si-mptlify the overall text generation task and hide important issues that come up in real life applications for which raw data is often the only available input.",
        "In terms of output, HYSSOP differs from most other systems in that it generates hypertext instead of linear text.",
        "It thus tackles the content aggregation problem in a particularly demanding application requiring the generator to simultaneously start from raw data, produce hypertext output and enforce conciseness constraints."
      ]
    },
    {
      "heading": "3.3 Generation architecture and aggregation localization",
      "text": [
        "While its overall architecture is a conventional pipeline, HYSSOP is unique in encapsulating all aggregation processing in the sentence planner and carrying it out entirely on a deep semantic representation.",
        "In contrast, most other systems distribute aggregation over several processing components and across several levels of internal representations: deep semantic, thematic and even surface syntactic for some of them."
      ]
    },
    {
      "heading": "3.4 Data structures and algorithms for aggregation",
      "text": [
        "All previous approaches to aggregations relied on rules that included some domain-specific semantic or lexical information.",
        "In contrast, the aggregation algorithm used by HYSSOP is .domain independent since it relies only on (1) generic matrix row and column shuffling operations, and (2) on a generic similarity ,--measure-between-arbitrary data cells."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "We presented a new approach to content aggregation in the context of a very challenging and practical generation application: summarizing OLAP and data mining discoveries",
        "as a few linked web pages of fluent and concise natural language.",
        "We believe that the key contribution to our work is to show the feasibility to perform effective paratactic aggregation:",
        "• encapsulated within a single generation component (the sentence planner) • using a domain-independent algorithm and a",
        "simple data strueture,\" the factorization matrix, that captures the key structural and ordering constraints on paratactic aggregation while completely abstracting from domain semantic idiosyncrasies as well as from lexical and syntactic details.",
        "This is a first success towards the development of a plug-in content aggregation component for text generation, reusable across application domains.",
        "In future work, we intend to empirically evaluate the summaries generated by HYSSOP."
      ]
    }
  ]
}
