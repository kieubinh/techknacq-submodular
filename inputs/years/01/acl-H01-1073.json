{
  "info": {
    "authors": [
      "Bryan Pellom",
      "Wayne H. Ward",
      "John P. Hansen",
      "Ronald A. Cole",
      "Kadri Hacioglu",
      "J. Zhang",
      "Xiaofeng Yu",
      "Sameer S. Pradhan"
    ],
    "book": "Human Language Technology Conference",
    "id": "acl-H01-1073",
    "title": "University of Colorado Dialogue Systems for Travel and Navigation",
    "url": "https://aclweb.org/anthology/H01-1073",
    "year": 2001
  },
  "references": [
    "acl-H01-1058"
  ],
  "sections": [
    {
      "heading": "ABSTRACT",
      "text": [
        "This paper presents recent improvements in the development of the University of Colorado CU Communicator and CU Move spoken dialog systems.",
        "First, we describe the CU Communicator system that integrates speech recognition, synthesis and natural language understanding technologies using the DARPA Hub Architecture.",
        "Users are able to converse with an automated travel agent over the phone to retrieve up-to-date travel information such as flight schedules, pricing, along with hotel and rental car availability.",
        "The CU Communicator has been under development since April of 1999 and represents our test-bed system for developing robust human-computer interactions where reusability and dialogue system portability serve as two main goals of our work.",
        "Next, we describe our more recent work on the CU Move dialog system for in-vehicle route planning and guidance.",
        "This work is in joint collaboration with HRL and is sponsored as part of the DARPA Communicator program.",
        "Specifically, we will provide an overview of the task, describe the data collection environment for in-vehicle systems development, and describe our initial dialog system constructed for route planning."
      ]
    },
    {
      "heading": "1.2 Audio Server",
      "text": [
        "The audio server is responsible for answering the incoming call, playing prompts and recording user input.",
        "Currently, our system uses the MIT/MITRE audio server that was provided to DARPA Communicator program participants.",
        "The telephony hardware consists of an external serial modem device that connects to the microphone input and speaker output terminals on the host computer.",
        "The record process is pipelined to the speech recognition server and the play process is pipelined the text-tospeech server.",
        "This audio server does not support barge-in.",
        "Recently we have developed a new audio server that supports barge-in using the Dialogic hardware platform.",
        "The new audio server implements a Fast Normalized Least-Mean-Square (LMS) algorithm for software-based echo cancellation.",
        "During operation, the echo from the system speech is actively cancelled from the recorded audio to allow the user to cut through while",
        "Each commonly used word takes one class.",
        "The probability of word Wi given class Ci is estimated from training corpora.",
        "After the corpora are correctly tagged, a back-off class-based trigram language model can be computed from the tagged corpora.",
        "We use the CMU-Cambridge Statistical Language Modeling Toolkit to compute our language models.",
        "More recently, we have developed a dialog context dependent language model (LM) combining stochastic context free grammars (SCFGs) and n-grams [6,7].",
        "Based on a spoken language production model in which a user picks a set of concepts with respective values and constructs word sequences using phrase generators associated with each concept in accordance with the dialog context, this LM computes the probability of a word, P(W), as",
        "where W is the sequence of words, C is the sequence of concepts and S is the dialog context.",
        "Here, the assumptions are (i) S is given, (ii) W is independent of S but C, and (iii) W and C associations are unambiguous.",
        "This formulation can be considered as a general extension of the standard class word based statistical language model as seen in Fig. 2.",
        "The first term in (1) is modeled by SCFGs, one for each concept.",
        "The concepts are classes of phrases with the same meaning.",
        "Each SCFG is compiled into a stochastic recursive transition network (STRN).",
        "Our grammar is a semantic grammar since the nonterminals correspond to semantic concepts instead of syntactic constituents.",
        "The set of task specific concepts is augmented with a single word, multiple word and a small number of broad but unambigious part of speech (POS) classes to account for the phrases that are not covered by the grammar.",
        "These classes are considered as \"filler\" concepts within a unified framework.",
        "The second term in (1) is modeled as a pool of concept n-gram LMs.",
        "That is, we have a separate LM for each dialog context.",
        "At the moment, the dialog context is selected as the last question prompted by the system, as it is very simple and yet strongly predictive and constraining.",
        "SCFG and n-gram probabilities are learned by simple counting and smoothing.",
        "Our semantic grammars have a low degree of ambiguity and therefore do not require computationally intensive stochastic training and parsing techniques.",
        "Experimental results with N-best list rescoring were found promising (5-6% relative improvement in WER).",
        "In addition, we have shown that a dynamic combining of our new LM and the standard class word n-gram (the LM currently in use in our system) should result in further improvements.",
        "At the present, we are interfacing the grammar LM to the speech recognizer using a word graph."
      ]
    },
    {
      "heading": "1.4 Confidence Server",
      "text": [
        "Our prior work on confidence assessment has considered detection and rejection of word-level speech recognition errors and out-of-domain phrases using language model features [8].",
        "More recently [9], we have considered detection and rejection of misrecognized units at the concept level.",
        "Because concepts are used to update the state of the dialog system, we believe that concept level confidence is vitally important to ensuring a graceful human-computer interaction.",
        "Our current work on concept error detection has considered language model features (e.g., LM back-off behavior, language model score) as well as acoustic features from the speech recognizer (e.g., normalized acoustic score, lattice density, phone perplexity).",
        "Confidence features are combined to compute word-level, concept-level, and utterance-level confidence scores."
      ]
    },
    {
      "heading": "1.5 Language Understanding",
      "text": [
        "We use a modified version of the Phoenix [10] parser to map the speech recognizer output onto a sequence of semantic frames.",
        "A Phoenix frame is a named set of slots, where the slots represent related pieces of information.",
        "Each slot has an associated context-free semantic grammar that specifies word string patterns that can fill the slot.",
        "The grammars are compiled into Recursive Transition Networks, which are matched against the recognizer output to fill slots.",
        "Each filled slot contains a semantic parse tree with the slot name as root.",
        "Phoenix has been modified to also produce an extracted representation of the parse that maps directly onto the task concept structures.",
        "For example, the utterance I want to go from Boston to Denver Tuesday morning would produce the extracted parse: Flight_Constraint: Depart_Location.City.Boston Flight_Constraint: Arrive_Location.City.Denver Flight Constraints: [Date_Time].[Date].",
        "[Day_Name].tuesday"
      ]
    },
    {
      "heading": "[Time_Range].[Period_Of_Day].morning 1.6 Dialog Management",
      "text": [
        "The Dialogue Manager controls the systems interaction with the user and the application server.",
        "It is responsible for deciding what action the system will take at each step in the interaction.",
        "The Dialogue Manager has several functions.",
        "It resolves ambiguities in the current interpretation; Estimates confidence in the extracted information; Clarifies the interpretation with the user if required; Integrates new input with the dialogue context; Builds database queries (SQL); Sends information to NL generation for presentation to user; and prompts the user for missing information.",
        "We have developed a flexible, event driven dialogue manager in which the current context of the system is used to decide what to do next.",
        "The system does not use a dialogue network or a dialogue script, rather a general engine operates on the semantic representations and the current context to control the interaction flow.",
        "The Dialogue Manager receives the extracted parse.",
        "It then integrates the parse into the current context.",
        "Context consists of a set of frames and a set of global variables.",
        "As new extracted information arrives, it is put into the context frames and sometimes used to set global variables.",
        "The system provides a general-purpose library of routines for manipulating frames.",
        "This event driven architecture functions similar to a production system.",
        "An incoming parse causes a set of actions to fire which modify the current context.",
        "After the parse has been integrated into the current context, the DM examines the context to decide what action to take next.",
        "The DM attempts the following actions in the order listed: Clarify if necessary Sign off if all done Retrieve data and present to user Prompt user for required information The rules for deciding what to prompt for next are very straightforward.",
        "The frame in focus is set to be the frame produced in response to the user, or to the last system prompt.",
        "If there are unfilled required slots in the focus frame, then prompt for the highest priority unfilled slot in the frame.",
        "If there are no unfilled required slots in the focus frame, then prompt for the highest priority missing piece of information in the context.",
        "Our mechanism does not have separate user initiative and system initiative modes.",
        "If the system has enough information to act on, then it does it.",
        "If it needs information, then it asks for it.",
        "The system does not require that the user respond to the prompt.",
        "The user can respond with anything and the system will parse the utterance and set the focus to the resulting frame.",
        "This allows the user to drive the dialog, but doesnt require it.",
        "The system prompts are organized locally, at the frame level.",
        "The dialog manager or user puts a frame in focus, and the system tries to fill it.",
        "This representation is easy to author, there is no separate dialog control specification required.",
        "It is also robust in that it has a simple control that has no state to lose track of.",
        "An additional benefit of Dialog Manager mechanism is that it is very largely declarative.",
        "Most of the work done by a developer will be the creation of frames, forms and grammars.",
        "The system developer creates a task file that specifies the system ontology and templates for communicating about nodes in the hierarchy.",
        "The templates are filled in from the values in the frames to generate output in the desired language.",
        "This is the way we currently generate SQL queries and user prompts.",
        "An example task frame specification is:",
        "This example defines a frame with name Air and slot [Depart_Loc].",
        "The child nodes of Depart_Loc are are [City_Name] and [Airport _Code].",
        "The + after [Depart_Loc] indicates that it is a mandatory field.",
        "The Prompt string is the template for prompting for the node information.",
        "The * after [City_Name] and [Airport _Code] indicate that if either of them is filled, the parent node [Depart_Loc] is filled.",
        "The Confirm string is a template to prompt the user to confirm the values.",
        "The SQL string is the template to use the value in an SQL query to the database.",
        "The system will prompt for all mandatory nodes that have prompts.",
        "Users may specify information in any order, but the system will prompt for whatever information is missing until the frame is complete."
      ]
    },
    {
      "heading": "1.7 Database & Internet Interface",
      "text": [
        "The back-end interface consists of an SQL database and domain specific Perl scripts for accessing information from the Internet.",
        "During operation, database requests are transmitted by the Dialog Manager to the database server via a formatted frame.",
        "The back-end consists of a static and dynamic information component.",
        "Static tables contain data such as conversions between 3-letter airport codes and the city, state, and country of the airport (e.g., BOS for Boston Massachusetts).",
        "There are over 8000 airports in our database, 200 hotel chains, and 50 car rental companies.",
        "The dynamic information content consists of database tables for car, hotel, and airline flights.",
        "When a database request is received, the Dialog Managers SQL command is used to select records in local memory.",
        "If no records are found to match, the back-end can submit an HTTPbased request for the information via the Internet.",
        "Records returned from the Internet are then inserted as rows into the local SQL database and the SQL statement is once again applied."
      ]
    },
    {
      "heading": "1.8 Language Generation",
      "text": [
        "The language generation module uses templates to generate text based on dialog speech acts.",
        "Example dialog acts include prompt for prompting the user for needed information, summarize for summarization of flights, hotels, and rental cars, and clarify for clarifying information such as departure and arrival cities that share the same name."
      ]
    },
    {
      "heading": "1.9 Text-to-Speech Synthesis",
      "text": [
        "For audio output, we have developed a domain-dependent concatenative speech synthesizer.",
        "Our concatenative synthesizer can adjoin units ranging from phonemes, to words, to phrases and sentences.",
        "For domain modeling, we use a voice talent to record entire task-dependent utterances (e.g., What are your travel plans?)",
        "as well as short phrases with carefully determined break points (e.g., United flight, ten, thirty two, departs Anchorage at).",
        "Each utterance is orthographically transcribed and phonetically aligned using a HMM-based recognizer.",
        "Our research efforts for data collection are currently focused on methods for reducing the audible distortion at segment boundaries, optimization schemes for prompt generation, as well as tools for rapidly correcting boundary misalignments.",
        "In general, we find that some degree of hand-correction is always required in order to reduce distortions at concatenation points.",
        "During synthesis, the text is automatically divided into individual sentences that are then synthesized and pipelined to the audio server.",
        "A text-to-phoneme conversion is applied using a phonetic dictionary.",
        "Words that do not appear in the phonetic dictionary are automatically pronounced using a multilayer perceptron based pronunciation module.",
        "Here, a 5-letter context is extracted from the word to be pronounced.",
        "The letter input is fed through the MLP and a phonetic symbol (or possibly epsilon) is output by the network.",
        "By sliding the context window, we can extract the phonetic pronunciation of the word.",
        "The MLP is trained using letter-context and symbol output pairs from a large phonetic dictionary.",
        "The selection of units to concatenate is determined using a hybrid search algorithm that operates at the word or phoneme level.",
        "During synthesis, sections of word-level text that have been recorded are automatically concatenated.",
        "Unrecorded words or word sequences are synthesized using a Viterbi beam search across all available phonetic units.",
        "The cost function includes information regarding phonetic context, pitch, duration, and signal amplitude.",
        "Audio segments making up the best-path are then concatenated to generate the final sentence waveform."
      ]
    },
    {
      "heading": "3.1 Task Overview",
      "text": [
        "The CU Move system represents our work towards achieving gracefulhuman-computerinteractioninautomobile environments.",
        "Initially, we have considered the task of vehicle route planning and navigation.",
        "As our work progresses, we will expand our dialog system to new tasks such as information retrieval and summarization and multimedia access.",
        "The problem of voice dialog within vehicle environments offers some important speech research challenges.",
        "Speech recognition in car environments is in general fragile, with word-error-rates (WER) ranging from 30-65% depending on driving conditions.",
        "These changing environmental conditions include speaker changes (task stress, emotion, Lombard effect, etc.)",
        "as well as the acoustic environment (road/wind noise from windows, air conditioning, engine noise, exterior traffic, etc.).",
        "In developing the CU-Move system [13,14], there are a number of research challenges that must be overcome to achieve reliable and natural voice interaction within the car environment.",
        "Since the speaker is performing a task (driving the vehicle), the driver will experience a measured level of user task stress and therefore this should be included in the speaker-modeling phase.",
        "Previous studies have clearly shown that the effects of speaker stress and Lombard effect can cause speech recognition systems to fail rapidly.",
        "In addition, microphone type and placement for in vehicle speech collection can impact the level of acoustic background noise and speech recognition performance."
      ]
    },
    {
      "heading": "3.2 Signal Processing",
      "text": [
        "Our research for robust recognition in automobile environments is concentrated on development of an intelligent microphone array.",
        "Here, we employ a Gaussian Mixture Model (GMM) based environmental classification scheme to characterize the noise conditions in the automobile.",
        "By integrating an environmental classification system into the microphone array design, decisions can be made as to how best to utilize a noise adaptive frequency-partitioned iterative enhancement algorithm [15,16] or model-based adaptation algorithms [17,18] during decoding to optimize speech recognition accuracy on the beam formed signal."
      ]
    },
    {
      "heading": "3.3 Data Collection",
      "text": [
        "A five-channel microphone array was constructed using Knowles microphones and a multichannel data recorder housing built (Fostex) for in-vehicle data collection.",
        "An additional reference microphone is situated behind the drivers seat.",
        "Fig.",
        "3 shows the constructed microphone array and data recorder housing.",
        "As part of the CU-Move system formulation, a two phase data collection plan has been initiated.",
        "Phase I focuses on collecting acoustic noise and probe speech from a variety of cars and driving conditions.",
        "Phase II focuses on a extensive speaker collection across multiple U.S. sites.",
        "A total of eight vehicles have been selected for acoustic noise analysis.",
        "These include the following: a compact car, minivan, cargo van, sport utility vehicle (SUV), compact and full size trucks, sports car, full size luxury car.",
        "A fixed 10 mile route through Boulder, CO was used for Phase I data collection.",
        "The route consisted of city (25 & 45mph) and highway driving (45 & 65mph).",
        "The route included stop-and-go traffic, and prescribed locations where driver/passenger windows, turn signals, wiper blades, air conditioning were operated.",
        "Each data collection run per car lasted approximately 35-45 minutes.",
        "A detailed acoustic analysis of Phase I data can be found in [13].",
        "Our plan is to begin Phase II speech/dialogue data collection during spring 2001, which will include (i) phonetically balanced utterances, (ii) task-specific vocabularies, (iii) natural extemporaneous speech, and (iv) human-to-human and Wizard-of-Oz (WOZ) interaction with CU Communicator and CU-Move dialog systems."
      ]
    },
    {
      "heading": "3.4 Prototype Dialog System",
      "text": [
        "Finally, we have developed a prototype dialog system for data collection in the car environment.",
        "The dialog system is based on the MIT Galaxy-II Hub architecture with base system components derived from the CU Communicator system [1].",
        "Users interacting with the dialog system can enter their origin and destination address by voice.",
        "Currently, 1107 street names for Boulder, CO area are modeled.",
        "The system can resolve street addresses by business name via interaction with an Internet telephone book.",
        "This allows users to ask more natural route queries (e.g., I need an auto repair shop, or I need to get to the Boulder Marriott).",
        "The dialog system automatically retrieves the driving instructions from the Internet using an online WWW route direction provider.",
        "Once downloaded, the driving directions are queried locally from an SQL database.",
        "During interaction, users mark their location on the route by providing spoken odometer readings.",
        "Odometer readings are needed since GPS information has not yet been integrated into the prototype dialog system.",
        "Given the odometer reading of the vehicle as an estimate of position, route information such as turn descriptions, distances, and summaries can be queried during travel (e.g., \"What's my next turn\", \"How far is it\", etc.).",
        "The prototype system uses the CMU Sphinx-II speech recognizer with cellular telephone acoustic models along with the Phoenix Parser [10] for semantic parsing.",
        "The dialog manager is mixed initiative and event driven.",
        "For route guidance, the natural language generator formats the driving instructions before presentation to the user by the text-to-speech server.",
        "For example, the direction, \"Park Ave W. becomes 22nd St.\" is reformatted to, \"Park Avenue West becomes Twenty Second Street\".",
        "Here, knowledge of the task-domain can be used to significantly improve the quality of the output text.",
        "For speech synthesis, we have developed a Hub-compliant server that interfaces to the AT&T NextGen speech synthesizer."
      ]
    },
    {
      "heading": "3.5 Future Work",
      "text": [
        "We have developed a Hub compliant server that interfaces a Garmin GPS-III global positioning device to a mobile computer via a serial port link.",
        "The GPS server reports vehicle velocity in the X,Y,Z directions as well as real-time updates of vehicle position in latitude and longitude.",
        "HRL Laboratories has developed a route server that interfaces to a major navigation content provider.",
        "The HRL route server can take GPS coordinates as inputs and can describe route maneuvers in terms of GPS coordinates.",
        "In the near-term, we will interface our GPS server to the HRL route server in order to provide real-time updating of vehicle position.",
        "This will eliminate the need for periodic location update by the user and also will allow for more interesting dialogs to be established (e.g., the computer might proactively tell the user about upcoming points of interest, etc.",
        ")."
      ]
    },
    {
      "heading": "4. REFERENCES",
      "text": []
    }
  ]
}
