{
  "info": {
    "authors": [
      "Abraham Ittycheriah",
      "Martin Franz",
      "Wei-Jing Zhu",
      "Adwait Ratnaparkhi"
    ],
    "book": "Meeting of the North American Association for Computational Linguistics",
    "id": "acl-N01-1005",
    "title": "Question Answering Using Maximum-Entropy Components",
    "url": "https://aclweb.org/anthology/N01-1005",
    "year": 2001
  },
  "references": [
    "acl-J96-1002",
    "acl-M98-1028",
    "acl-W98-1118"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a statistical question answering system developed for TREC-9 in detail.",
        "The system is an application of maximum entropy classification for question/answer type prediction and named entity marking.",
        "We describe our system for information retrieval which did document retrieval from a local encyclopedia, and then expanded the query words and finally did passage retrieval from the TREC collection.",
        "We will also discuss the answer selection algorithm which determines the best sentence given both the question and the occurrence of a phrase belonging to the answer class desired by the question.",
        "A new method of analyzing system performance via a transition matrix is shown."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Systems that perform question answering automatically by computer have been around for some time as described by (Green et al., 1963).",
        "Only recently though have systems been developed to handle huge databases and a slightly richer set of questions.",
        "The types of questions that can be dealt with today are restricted to be short answer fact based questions.",
        "In TREC-8, a number of sites participated in the first question-answering evaluation (Voorhees and Tice, 1999) and the best systems identified four major subcomponents:",
        "• Question/Answer Type Classification • Query expansion/Information Retrieval • Named Entity Marking • Answer Selection",
        "Our system architecture for this year was built around these four major components as shown in Fig. 1.",
        "Here, the question is input and classified as asking for an answer whose category is one of the named entity classes to be described below.",
        "Additionally, the question is presented to the information retrieval (IR) engine for query expansion and document retrieval.",
        "This engine, given the query, looks at the database of documents and outputs the best documents or passages annotated with the named entities.",
        "The final stage is to select the exact answer, given the information about the answer class and the top scoring passages.",
        "Minimizing various distance metrics applied over phrases or windows of text results in the best scoring section that has a phrase belonging to the answer class.",
        "This then represents the best scoring answer.",
        "cuss the mathematical details of the algorithm here, instead we will only show the features that are used in such a model.",
        "The paper is organized as follows: we will describe Answer Type Classification, Information Retrieval and Answer Selection in the next 3 sections and then analyze the system using the TREC9 dataset in the next section and conclude.",
        "2 Answer Type Classification In answer type classification the problem is to label a question with the label of the named entity that the question seeks.",
        "Our labels are the standard MUC (Chinchor, 1997) categories with the addition of PHRASE which is a catch all for answers not of the standard categories.",
        "These categories are",
        "• Person, Location, Organization • Cardinal, Percent • Date, Time, Duration • Measure • Money • Phrase, Reason",
        "The REASON category was tied to WHY questions.",
        "Processing of REASON and PHRASE is the same in our system, interpreting it as desiring a clause which had a noun phrase embedded in it.",
        "A corpus of questions that has been annotated with the above mentioned categories was created.",
        "We created 1900 questions by presenting a human subject a document selected at random and having read a portion of the document, a question was phrased; the answer and the document number are noted in addition.",
        "We also used 1400 questions from a trivia database (Academic Hallmarks, 1999) annotated in a similar manner.",
        "This data is used to train the maximum entropy classifier.",
        "A separate set of 182 questions is used as the heldout test set.",
        "In the experiments, the types of feature functions shown in Table 1 were used.",
        "Each feature type expands on the one above it.",
        "The \"Expanded Hierarchy\" feature type uses WordNet (Miller, 1990) to expand words from a question word upto and including the first noun cluster.",
        "The \"Mark Question Word\" feature type identifies the question words and labels them as occuring in the beginning of a question (bqw), in the middle (mqw) of a question or at the end of a question (eqw).",
        "A feature, (D, is a binary function of the histories, h, and futures, f, of the problem being modelled.",
        "In answer type prediction, an example history stream is composed of the words of the question, QW, and the future is the class label, CL, associated with that question.",
        "A feature that uses the \"who' word to decide that the label is PERSON is,",
        "In this application of maximum entropy, we propose such feature functions on instances of the training data where an error is made.",
        "The pool of such feature functions are ranked and the top features are selected in each iteration.",
        "The features of the maximum entropy model are n-grams of words (required to be adjacent) and bag of words where position is not important.",
        "Note that the organization of the data include bigrams which are upto distance 2, and additionally we have ngram features in the maxent model.",
        "The performance of the algorithm is shown in Fig. 2.",
        "Each feature type adds to the accuracy of the system and choosing 700 features with the \"Mark Question Word\" feature type achieves the lowest error rate of (9.05%) on a held out subset of the data.",
        "A peculiar feature of the architecture is that improvements in answer type prediction do not correlate directly with improvements in the overall score of the system.",
        "The reason is that parallel improvements must be made in the named entity marking as well as in answer selection in order to realize them in the overall system."
      ]
    },
    {
      "heading": "3 Information Retrieval",
      "text": [
        "The purpose of the information retrieval module is to search the database of documents to select passages of text, containing information relevant to the query.",
        "The database used in TREC9 has 978952 documents from several sources including AP Newswire, Wall Street Journal, San Jose Mercury News, Financial Times, Los Angeles Times, and the Federal Broadcast Information Service (FBIS).",
        "The database consists of approximately 2.8 GB of text, representing 524 million words.",
        "Our information retrieval subsystem uses a two-pass approach.",
        "In the first pass, we searched an encyclopedia database.",
        "The highest scoring passages were then used to create expanded queries, applied in the second pass scoring of the TREC passages.",
        "The data preprocessing and relevance scoring techniques are similar to the ones applied in the TREC Ad-Hoc, SDR and CLIR participations (Franz and Roukos, 1998), (Franz et al., 1999).",
        "Relevance scoring was based on morph uni-gram and bigram features, extracted from the text data in the following way: after the initial filtering, we tokenized the documents using a statistical tokenizer.",
        "The tokenized text was processed by a statistical part of speech(POS) tagger (Merialdo, 1990).",
        "Based on the spellings and the POS tags, the morphs were found by looking up the morph corresponding to a given word and POS tag in a table, e.g., the word \"running\" tagged as verb was converted into \"run\", whereas the same word marked as adjective was left unchanged.",
        "The words not found in the morph table were kept in their original form.",
        "All the words were case-folded after the morphological analysis was done.",
        "Hyphenated words were then split into their components.",
        "We used a modified Okapi (Robertson et al.,",
        "1995) formula in the first-pass scoring.",
        "Uni",
        "grams and bigrams in the intersection of the query and document contributed a score of:",
        "where t f is the term count for a document, dl is the document length, avdl is the average length of the documents in the collection and idf is the inverse document frequency, computed as:",
        "where N is the total number of documents in the corpus and n is the number of documents containing a given n-gram.",
        "In Eq.",
        "(1) we used cl = 0.5, c2 = 1.5 for unigram scoring and cl = 0.05, c2 = 0.05 for the bigrams.",
        "The first pass score was a linear combination of unigram and bigram scores given by Eq.",
        "(1), with the unigram scores weight set to 0.8 and bigram scores weight equal to 0.2.",
        "We computed first-pass relevance scores for 82,277 overlapping passages, each containing approximately 100 non-stop words, extracted from 18,910 encyclopedia articles.",
        "Based on the first pass passage ranking, we constructed expanded queries using the local context analysis (LCA) technique (Xu and Croft, 1996).",
        "In the second pass scoring, the expanded queries are used to score 2,632,807 passages based on the TREC-9 Q&A corpus.",
        "The passages were selected to contain approximately",
        "results on the 146 development test set questions described below.",
        "The performance is measured by the Mean Reciprocal Rank (MRR) (Voorhees and Tice, 1999) of the highest ranking passage containing the answer string among the top five passages.",
        "The first line of the table shows the result of first pass scoring using the TREC-9 Q&A database.",
        "The second line contains the result obtained with queries expanded using the TREC database.",
        "The last line of the table shows the result corresponding to the system applied in our official submission, with queries expanded using the encyclopedia database.",
        "4 Named Entity Annotation Named entity (NE) annotation is a markup of the text with the class information.",
        "As mentioned above, our classes correspond to the MUC classes due to the availability of training data for these classes.",
        "We used the text corpora available from the LDC to train the maximum entropy model.",
        "Windows of +/- 2 words, morphs, part-of-speech (POS) tags and flags raised by pattern grammars for DATE, MONEY, CARDINAL, MEASURE, PERCENT, TIME, DURATION classes and dictionary hits, along with the two previous tags are created for each word.",
        "The window for predicting tag(0) is shown in Table 3.",
        "The window is the useful information given to the maximum entropy feature generation system to make features about the tag of the current word.",
        "The (-,+) signs give a clue to the feature functions about the relative position of this data to the word being tagged.",
        "Additionally, the (-2,-1,+1,+2) give position information to the feature functions.",
        "Each stream has a fixed vocabulary and n-grams from this vocabulary are created to be the features of the maximum entropy model.",
        "The training data is arranged to indicate a special category for be",
        "PERSON, to find the boundaries of the named entity.",
        "The system explores multiple NE hypotheses in parallel and keeps only those with high probability and proceeds with a beam-search algorithm to find the most likely path for the whole sentence.",
        "The performance of the named entity detector is comparable to the performance cited in (Borthwick et al., 1998) when training the maximum entropy algorithm on only annotated data.",
        "We omit the results here in the consideration of space, but note that in the analysis of the question answering system below, only 4 out of 64 errors are attributed directly to the named entity marking for the 250 byte system."
      ]
    },
    {
      "heading": "5 Answer Selection",
      "text": [
        "We receive in this module the question, the class of the answer that the question seeks and a ranked set of passages (70) annotated with the MUC classes.",
        "The optimal sentence that answers the question is now sought.",
        "The TREC length constraints of 250 byte and 50 byte are then applied on the sentence.",
        "The algorithm used in this module is listed",
        "are then output.",
        "The definition of the various distances are Matching Words The TFIDF sum of the number of question words that matched answer words identically in the morphed space.",
        "(+) Thesaurus Match The TFIDF sum of the number of question words that matched answer words using a thesaurus match using WordNet synonyms (Miller, 1990).",
        "(+) Mis-Match Words The TFIDF sum of the number of question content words that did not match in this answer.",
        "(-) Dispersion The number of answer words in the candidate sentence that occur between matching question words.",
        "(-) Cluster Words The number of answer words in the candidate sentence that occurred adjacently in both the question and answer candidate.",
        "(+) Each distance has a weight applied and the corresponding sign shown above attached to it.",
        "The score for an answer is the sum of the distances and the top 5 sentences are then output.",
        "To select the 250 or 50 byte answer chunk from these sentences, the system identified the longest mismatched pieces between the answer string and the question.",
        "It then analyzed the answer and the question to find where the center of the match was using a subject-verb-object assumption of the sentence.",
        "The system then output either the subject or object portion whichever had the least matches with the question.",
        "Answer selection as done above used ad-hoc and heuristic distance metrics to seek an answer.",
        "Future work by the authors will show how to treat these distance metrics as features and to develop a statistical model for answer selection for an open domain.",
        "We wanted to maintain the TREC-9 question database as a test set, but in order to do some post-evaluation analysis, we chose a subset of the questions as a development set for next year.",
        "There were two classes of questions in",
        "the TREC9 test: questions that had only one phrasing and questions that had more than one phrasing (rephrased).",
        "For example, the following questions form a set:",
        "• Original Form: • Rephrased: • Rephrased:",
        "We wanted 20% of questions of each class in the development test.",
        "The exact list of questions we used for our TREC-9 development test set are shown in Table 4.",
        "The variant questions we chose are shown in italics, and we added every seventh question skipping the ones in the above class to yield the 146 questions.",
        "A set of regular expressions (answer patterns) which detect the presense of the answer in a string was developed for the set using the judgements file provided by NIST.",
        "The MRR for the entire system for the 250 byte system and the 50 byte system is shown in",
        "and 50 byte numbers.",
        "Furthermore, the results indicate a 2% absolute MRR improvement using the encyclopedia source to expand the original questions.",
        "Analysis of the components are shown in Table 6.",
        "An error is attributed to a component if chosen for the",
        "dev set for 250 byte system.",
        "it is the first component that caused the failure working left to right in our system architecture.",
        "This analysis was carried out on the top-5 answer strings.",
        "Thus, a failure occurs if there is no answer produced by the system at all.",
        "Fixing this error though need not correct the final answer as it may invoke an error in a subsequent component.",
        "Answer selection is still seen to be the major cause of problems in our question answering system.",
        "Another viewpoint is to see the effect of the system on the IR ranking results.",
        "This is shown below in Figure 3.",
        "Finding the 250 bytes from a passage that is of typical length 2AK bytes shows some degradation, but further finding the 50 byte answer has considerable degradation.",
        "In Tables 7 and 8 we show the transition matrix for the rank from IR passages to the Q&A system results .",
        "Note that there are significant transitions between the IR rank and the Q&A rank, but that inspection of the final result in Figure 3 shows that overall system performance is similar to the performance of IR for the 250 byte system and degraded at 50 bytes.",
        "In Figure 3, we plot the number of queries which had an answer at rank 1..5 and indicate no answer produced by >5.",
        "These results we believe points to the possibility of making more improvements in answer selection by reranking the results."
      ]
    },
    {
      "heading": "7 Related Work",
      "text": [
        "In the last two years, several efforts at question answering for open domain (Moldovan and et.",
        "al., 1999; Voorhees and Tice, 1999) and FAQ domains (Burke and et.",
        "al., 1997) have appeared.",
        "Our approach at question answering has been to follow the lead of the other participants in the TREC evaluation but base our components on maximum entropy modelling.",
        "We believe that corpus based systems allow technologies to be compared in a systematic approach, thus furthering the field of question answering."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We presented above our architecture and a component-wise evaluation of the system in the question answering problem.",
        "We developed maximum entropy formulations for both question/answer classification and named entity marking.",
        "The results presented above indicate a 2% absolute MRR improvement using the encyclopedia source to expand the original questions.",
        "The transition matrix of the IR to Q&A position"
      ]
    },
    {
      "heading": "9 Acknowledgement",
      "text": [
        "This work is supported by DARPA under SPAWAR contract number N66001-99-2-8916.",
        "The authors thank Salim Roukos, Kishore Pap-ineni and Todd Ward for making this work possible and helping us with their enormous expertise."
      ]
    }
  ]
}
