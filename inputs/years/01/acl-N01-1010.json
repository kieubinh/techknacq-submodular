{
  "info": {
    "authors": [
      "Noriko Tomuro"
    ],
    "book": "Meeting of the North American Association for Computational Linguistics",
    "id": "acl-N01-1010",
    "title": "Tree-Cut and a Lexicon Based on Systematic Polysemy",
    "url": "https://aclweb.org/anthology/N01-1010",
    "year": 2001
  },
  "references": [
    "acl-J96-2004",
    "acl-J98-2002",
    "acl-P96-1006",
    "acl-P99-1032",
    "acl-W00-0103",
    "acl-W00-0104",
    "acl-W00-0802",
    "acl-W99-0502",
    "acl-W99-0512"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes a lexicon organized around systematic polysemy: a set of word senses that are related in systematic and predictable ways.",
        "The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut.",
        "We compare our lexicon to WordNet cousins, and the inter-annotator disagreement observed between WordNet Semcor and DSO corpora."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In recent years, the granularity of word senses for computational lexicons has been discussed frequently in Lexical Semantics (for example, (Kilgarriff, 1998a; Palmer, 1998)).",
        "This issue emerged as a prominent problem after previous studies and exercises in Word Sense Disambiguation (WSD) reported that, when fine-grained sense definitions such as those in WordNet (Miller, 1990) were used, entries became very similar and indistinguishable to human annotators, thereby causing disagreement on correct tags (Kilgarriff, 1998b; Veronis, 1998; Ng et al., 1999).",
        "In addition to WSD, the selection of sense inventories is fundamentally critical in other Natural Language Processing (NLP) tasks such as Information Extraction (IE) and Machine Translation (MT), as well as in Information Retrieval (IR), since the difference in the correct sense assignments affects recall, precision and other evaluation measures.",
        "In response to this, several approaches have been proposed which group fine-grained word senses in various ways to derive coarse-grained sense groups.",
        "Some approaches utilize an abstraction hierarchy defined in a dictionary (Kilgarriff, 1998b), while others utilize surface syntactic patterns of the functional structures (such as predicate-argument structure for verbs) of words (Palmer, 1998).",
        "Also, the current version of WordNet (1.6) encodes groupings of sim-ilar/related word senses (or synsets) by a relation called cousin.",
        "Another approach to grouping word senses is to utilize a linguistic phenomenon called systematic polysemy: a set of word senses that are related in systematic and predictable ways.'",
        "For example, ANIMAL and MEAT meanings of the word \"chicken\" are related because chicken as meat refers to the flesh of a chicken as a bird that is used for food .2 This relation is systematic, since many ANIMAL words such as \"duck\" and \"lamb\" have a MEAT meaning.",
        "Another example is the relation QUANTITY-PROCESS observed in nouns such as \"increase\" and \"supply\".",
        "Sense grouping based on systematic polysemy is lexico-semantically motivated in that it expresses general human knowledge about the relatedness of word meanings.",
        "Such sense groupings have advantages compared to other approaches.",
        "First, related senses of a word often exist simultaneously in a discourse (for example the QUANTITY and PROCESS meanings of \"increase\" above).",
        "Thus, systematic polysemy can be effectively used in WSD (and WSD evaluation) to accept multiple or alternative sense tags (Buitelaar, personal communication).",
        "Second, many systematic relations are observed between senses which belong to different semantic categories.",
        "So if a lexicon is defined by a collection of separate trees/hierarchies (such as the case of WordNet), systematic polysemy can express similarity between senses that are not hierarchically proximate.",
        "Third, by explicitly representing (inter-)relations between senses, a lexicon based on systematic polysemy can facilitate semantic inferences.",
        "Thus it is useful in knowledge-intensive NLP tasks such as discourse analysis, IE and MT.",
        "More recently, (Gonzalo et al., 2000) also discusses potential usefulness of systematic polysemy for clustering word senses for IR.",
        "However, extracting systematic relations from large sense inventories is a difficult task.",
        "Most often, this procedure is done manually.",
        "For example, WordNet cousin relations were identified manually by the WordNet lexicographers.",
        "A similar effort was also made in the EuroWordnet project (Vossen et 'Systematic polysemy (in the sense we use in this paper) is also referred to as regular polysemy (Apresjan, 1973) or logical polysemy (Pustejovsky, 1995).",
        "a Note that systematic polysemy should be contrasted with homonymy, which refers to words which have more than one unrelated sense (e.g. FINANCIALINSTITUTION and SLOPINGLAND meanings of the word \"bank\").",
        "al., 1999).",
        "The problem is not only that manual inspection of a large, complex lexicon is very time-consuming, it is also prone to inconsistencies.",
        "In this paper, we describes a lexicon organized around systematic polysemy.",
        "The lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut (Li and Abe, 1998).",
        "In our previous work (Tomuro, 2000), we applied this method to a small subset of WordNet nouns and showed potential applicability.",
        "In the current work, we applied the method to all nouns and verbs in WordNet, and built a lexicon in which word senses are partitioned by systematic polysemy.",
        "We report results of comparing our lexicon with the WordNet cousins as well as the inter-annotator disagreement observed between two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).",
        "The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better K values (Car-letta, 1996) than arbitrary sense groupings on the agreement data."
      ]
    },
    {
      "heading": "2 The Tree-cut Technique",
      "text": [
        "The tree-cut technique is an unsupervised learning technique which partitions data items organized in a tree structure into mutually-disjoint clusters.",
        "It was originally proposed in (Li and Abe, 1998), and then adopted in our previous method for automatically extracting systematic polysemy (Tomuro, 2000).",
        "In this section, we give a brief summary of this tree-cut technique using examples from (Li and Abe, 1998)'s original work."
      ]
    },
    {
      "heading": "2.1 Tree-cut Models",
      "text": [
        "The tree-cut technique is applied to data items that are organized in a structure called a thesaurus tree.",
        "A thesaurus tree is a hierarchically organized lexicon where leaf nodes encode lexical data (i.e., words) and internal nodes represent abstract semantic classes.",
        "A tree-cut is a partition of a thesaurus tree.",
        "It is a list of internal/leaf nodes in the tree, and each node represents a set of all leaf nodes in a subtree rooted by the node.",
        "Such a set is also considered as a cluster.'",
        "Clusters in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint.",
        "For instance, Figure 1 shows an example thesaurus tree and one possible tree-cut [AIRCRAFT, ball, kite, puzzle], which is indicated by a thick curve in the figure.",
        "There are also four other possible tree-cuts for this tree: [airplane, helicopter, ball, kite, puzzle], [airplane, helicopter, TOY], [AIRCRAFT, TOY] and [ARTIFACT].",
        "In (Li and Abe, 1998), the tree-cut technique was applied to the problem of acquiring general-3A leaf node is also a cluster whose cardinality is 1. ized case frame patterns from a corpus.",
        "Thus, each node/word in the tree received as its value the number of instances where the word occurred as a case role (subject, object etc.)",
        "of a given verb.",
        "Then the acquisition of a generalized case frame was viewed as a problem of selecting the best tree-cut model that estimates the true probability distribution, given a sample corpus data.",
        "Formally, a tree-cut model M is a pair consisting of a tree-cut F and a probability parameter vector O of the same length,",
        "where F and O are:",
        "where Ci (1 G i G k) is a cluster in the tree-cut, P(Ci) is the probability of a cluster Ci, and Ekii 1 P(Ci) = 1.",
        "Note that P(C) is the probability of cluster C = {nl, .., nm } as a whole, that is, P(C) _ Emj 1 P(nj).",
        "For example, suppose a corpus contains 10 instances of verb-object relation for the verb \"fly\", and the frequencies of object nouns n, denoted f (n), are as follows: f (airplane) = 5, f (helicopter) = 3, f (ball) _ 0, f (kite) = 2, f (puzzle) = 0.",
        "Then, the set of tree-cut models for the example thesaurus tree shown in Figure 1 includes ([airplane, helicopter, TOY], [.5, .3, .2]) and ([AIRCRAFT, TOY], [.8, .2])."
      ]
    },
    {
      "heading": "2.2 The MDL Principle",
      "text": [
        "To select the best tree-cut model, (Li and Abe, 1998) uses the Minimal Description Length (MDL).",
        "The MDL is a principle of data compression in Information Theory which states that, for a given dataset, the best model is the one which requires the minimum length (often measured in bits) to encode the model (the model description length) and the data (the data description length) (Rissanen, 1978).",
        "Thus, the MDL principle captures the trade-off between the simplicity of a model, which is measured by the number of clusters in a tree-cut, and the goodness of fit to the data, which is measured by the estimation accuracy of the probability distribution.",
        "The calculation of the description length for a tree-cut model is as follows.",
        "Given a thesaurus tree T and a sample S consisting of the case frame instances, the total description length L(M, S) for a tree-cut model M = (F, O) is",
        "where L(F) is the model description length, L(0117) is the parameter description length (explained shortly), and L(SIF, O) is the data description length.",
        "Note that L(F) + L(0117) essentially corresponds to the usual notion of the model description length.",
        "airplane helicopter ball kite puzzle",
        "Each length in L(M, S) is calculated as follows.",
        "The model description length L(F) is",
        "where G is the set of all cuts in T, and G1 denotes the size of G. This value is a constant for all models, thus it is omitted in the calculation of the total length.",
        "The parameter description length L(0117) indicates the complexity of the model.",
        "It is the length required to encode the probability distribution of the clusters in the tree-cut F. It is calculated as",
        "where k is the length of O, and S1 is the size of S. Finally, the data description length L(SIF, O) is the length required to encode the whole sample data.",
        "It is calculated as",
        "where, for each n E C and each C E F,",
        "Note the equation (7) essentially computes the Maximum Likelihood Estimate (MLE) for all n.5 A table in Figure 1 shows the MDL lengths for all five tree-cut models.",
        "The best model is the one with the tree-cut [AIRCRAFT, ball, kite, puzzle]."
      ]
    },
    {
      "heading": "3 Clustering Systematic Polysemy",
      "text": [
        "Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy from WordNet.",
        "In this section, we give a summary of this method, and describe the cluster pairs obtained by the method.",
        "That is because the lexicon represents true population, not samples; thus there is no additional data to estimate."
      ]
    },
    {
      "heading": "3.1 Extraction Method",
      "text": [
        "In our previous work, systematically related word senses are derived as binary cluster pairs, by applying the extraction procedure to a combination of two WordNet (sub)trees.",
        "This process is done in the following three steps.",
        "In the first step, all leaf nodes of the two trees are assigned a value of either 1, if a node/word appears in both trees, or 0 otherwise.6 In the second step, the tree-cut technique is applied to each tree separately, and two tree-cuts (or sets of clusters) are obtained.",
        "To search the best tree-cut for a tree (i.e., the model which requires the minimum total description length), a greedy algorithm called Find-MDL described in (Li and Abe, 1998) is used to speed up the search.",
        "Finally in the third step, clusters in those two tree-cuts are matched up, and the pairs which have substantial overlap (more than three overlapping words) are selected as systematic polysemies.",
        "Figure 2 shows parts of the final tree-cuts for the ARTIFACT and MEASURE classes.",
        "Note in the figure, bold letters indicate words which are polysemous in the two trees (i.e., assigned a value 1)."
      ]
    },
    {
      "heading": "3.2 Modification",
      "text": [
        "In the current work, we made a minor modification to the extraction method described above, by removing nodes that are assigned a value 0 from the trees.",
        "The purpose was to make the tree-cut technique less sensitive to the structure of a tree and produce more specific clusters defined at deeper levels.'",
        "The MDL principle inherently penalizes a complex tree-cut by assigning a long parameter length.",
        "Therefore, shorter tree-cuts partitioned at abstract levels are often preferred.",
        "This causes a problem when the tree is bushy, which is the case with WordNet trees.",
        "Indeed, many tree-cut clusters obtained in our previous work were from nodes at depth 1 (counting the root as depth 0) around 88% (122 6Prior to this, each WordNet (sub)tree is transformed into a thesaurus tree, since WordNet tree is a graph rather than a tree, and internal nodes as well as leaf nodes carry data.",
        "In the transformation, all internal nodes in a WordNet tree are copied as leaf nodes, and shared subtrees are duplicated.",
        "7Removing nodes with 0 is also warranted since we are not estimating values for those nodes (as explained in footnote 5).",
        "out of total 138 clusters) obtained for 5 combinations of WordNet noun trees.",
        "Note that we did not allow a cluster at the root of a tree; thus, depth 1 is the highest level for any cluster.",
        "After the modification above, the proportion of depth 1 clusters decreased to 49% (169 out of total 343 clusters) for the same tree combinations."
      ]
    },
    {
      "heading": "3.3 Extracted Cluster Pairs",
      "text": [
        "We applied the modified method described above to all nouns and verbs in WordNet.",
        "We first partitioned words in the two categories into basic classes.",
        "A basic class is an abstract semantic concept, and it corresponds to a (sub)tree in the WordNet hierarchies.",
        "We chose 24 basic classes for nouns and 10 basic classes for verbs, from WordNet Top categories for nouns and lexicographers' file names for verbs respectively.",
        "Those basic classes exhaustively cover all words in the two categories encoded in WordNet.",
        "For example, basic classes for nouns include ARTIFACT, SUBSTANCE and LOCATION, while basic classes for verbs include CHANGE, MOTION and STATE.",
        "For each part-of-speech category, we applied our extraction method to all combinations of two basic classes.",
        "Here, a combined class, for instance ARTIFACT-SUBSTANCE, represents an underspeciified semantic class.",
        "We obtained 2,377 cluster pairs in 99 underspecified classes for nouns, and 1,710 cluster pairs in 59 underspecified classes for verbs.",
        "Table 1 shows a summary of the number of basic and underspecified classes and cluster pairs extracted by our method.",
        "Although the results vary among category combinations, the accuracy (precision) of the derived cluster pairs was rather low: 50 to 60% on average, based on our manual inspection using around 5% randomly chosen samples.'",
        "This means our automatic method over-generates possible relations.",
        "We speculate that this is because in general, there are many homony-mous relations that are 'systematic' in the English language.",
        "For example, in the ARTIFACT-GROUP class, a pair [LUMBER, SOCIALGROUP] was extracted.",
        "Words which are common in the two clusters are \"picket\", \"board\" and \"stock\".",
        "Since there are enough number of such words (for our purpose), our automatic method could not differentiate them from true systematic polysemy."
      ]
    },
    {
      "heading": "4 Evaluation: Comparison with",
      "text": []
    },
    {
      "heading": "WordNet Cousins",
      "text": [
        "To test our automatic extraction method, we compared the cluster pairs derived by our method to WordNet cousins.",
        "The cousin relation is relatively new in WordNet, and the coverage is still incomplete.",
        "Currently a total of 194 unique relations are encoded.",
        "A cousin relation in WordNet is defined between two synsets, and it indicates that senses of a word that appear in both of the (sub)trees rooted by those synsets are related.",
        "The cousins were man-'Note that the relatedness between clusters was determined solely by our subjective judgement.",
        "That is because there is no existing large-scale lexicon which encodes related senses completely for all words in the lexicon.",
        "(Note that WordNet cousin relation is encoded only for some words).",
        "Although the distinction between related vs. unrelated meanings is sometimes unclear, systematicity of the related senses among words is quite intuitive and has been well studied in Lexical Semantics (for example, (Apresjan, 1973; Nunberg, 1995; Copestake and Briscoe, 1995)).",
        "A comparison with WordNet cousin is discussed in the next section 4.",
        "9Actually, cousin is one of the three relations which indicate the grouping of related senses of a word.",
        "Others are sister and twin.",
        "In this paper, we use cousin to refer to all relations listed in \"cousin.tps\" file (available in a WordNet distribution).",
        "ually identified by the WordNet lexicographers.",
        "To compare the automatically derived cluster pairs to WordNet cousins, we used the hypernym-hyponym relation in the trees, instead of the number or ratio of the overlapping words.",
        "This is because the levels at which the cousin relations are defined differ quite widely, from depth 0 to depth 6, thus the number of polysemous words covered in each cousin relation significantly varies.",
        "Therefore, it was difficult to decide on an appropriate threshold value for either criteria.",
        "Using the hypernym-hyponym relation, we checked, for each cousin relation, whether there was at least one cluster pair that subsumed or was subsumed by the cousin.",
        "More specifically, for a cousin relation defined between nodes cl and c2 in trees T1 and T2 respectively and a cluster pair defined between nodes rl and r2 in the same trees, we decided on the correspondence if cl is a hypernym or hyponym of rl, and c2 is a hypernym or hyponym r2 at the same time.",
        "Based on this criteria, we obtained a result indicating that 173 out of the 194 cousin relations had corresponding cluster pairs.",
        "This makes the recall ratio 89%, which we consider to be quite high.",
        "In addition to the WordNet cousins, our automatic extraction method discovered several interesting relations.",
        "Table 2 shows some examples."
      ]
    },
    {
      "heading": "5 A Lexicon based on Systematic",
      "text": []
    },
    {
      "heading": "Relations",
      "text": [
        "Using the extracted cluster pairs, we partitioned word senses for all nouns and verbs in WordNet, and produced a lexicon.",
        "Recall from the previous section that our cluster pairs are generated for all possible binary combinations of basic classes, thus one sense could appear in more than one cluster pair.",
        "For example, Table 3 shows the cluster pairs (and a set of senses covered by each pair, which we call a sense cover) extracted for the noun \"table\" (which has 6 senses in WordNet).",
        "Also as we have mentioned earlier in section accuracy-result, our cluster pairs contain many false positives ones.",
        "For those reasons, we took a conservative approach, by disallowing transitivity of cluster pairs.",
        "To partition senses of a word, we first assign each sense cover a value which we call a connectedness.",
        "It is defined as follows.",
        "For a given word w which has n senses, let S be the set of all sense covers generated for w. Let cij denote the number of sense covers in which sense i (si) and sense j (sj) occurred together",
        "Intuitively, cij represents the weight of a direct relation, and dij represents the weight of an indirect relation between any two senses i and j.",
        "The idea behind this connectedness measure is to favor sense covers that have strong intra-relations.",
        "This measure also effectively takes into account a one-level transitivity in dij.",
        "As an example, the connectedness of (2 3 4) is the summation of c23, c34, c24~ d23, d34 and d24.",
        "Here, c23 = 4 because sense 2 and 3 co-occur in four sense covers, and c34 = c24 = 1.",
        "Also,",
        "(omitting cases where either or both cik and ckj are zero), and similarly d34 = .5 and d24 = .5.",
        "Thus,",
        "shows the connectedness values for all sense covers for \"table\".",
        "Then, we partition the senses by selecting a set of non-overlapping sense covers which maximizes the total connectedness value.",
        "So in the example above, the set f(1 4),(2 3 5) g yields the maximum connectedness.",
        "Finally, senses that are not covered by any sense covers are taken as singletons, and added to the final sense partition.",
        "So the sense partition for \"table\" becomes f(1 4),(2 3 5),(6)g. Table 4 shows the comparison between WordNet and our new lexicon.",
        "As you can see, our lexicon contains much less ambiguity: the ratio of monosemous words increased from 84% (88,650/105,461ti.84) to 92% (96,964/105,461ti.92), and the average number of senses for polysemous words decreased from 2.73 to 2.52 for nouns, and from 3.57 to 2.82 for verbs.",
        "As a note, our lexicon is similar to CORELEX (Buitelaar, 1998) (or CORELEX-II presented in (Buitelaar, 2000)), in that both lexicons share the same motivation.",
        "However, our lexicon differs from CORELEX in that CORELEX looks at all senses of a word and groups words that have the same sense distribution pattern, whereas our lexicon groups",
        "word senses that have the same systematic relation.",
        "Thus, our lexicon represents systematic polysemy at a finer level than CORELEX, by pinpointing related senses within each word."
      ]
    },
    {
      "heading": "6 Evaluation: Inter-annotator",
      "text": []
    },
    {
      "heading": "Disagreement",
      "text": [
        "To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al., 1998) and DSO (Ng and Lee, 1996).",
        "The agreement between those corpora is previously studied in (Ng et al., 1999).",
        "In our current work, we first reproduced their agreement data, then used our sense partitions to see whether or not they yield a better agreement.",
        "In this experiment, we extracted 28,772 sen-tences/instances for 191 words (consisting of 121 nouns and 70 verbs) tagged in the intersection of the two corpora.",
        "This constitutes the base data set.",
        "Table 5 shows the breakdown of the number of instances where tags agreed and disagreed.10 As you 10 Note that the numbers reported in (Ng et al., 1999) are slightly more than the ones reported in this paper.",
        "For instance, the number of sentences in the intersected corpus reported in (Ng et al., 1999) is 30,315.",
        "We speculate the discrepancies are due to the different sentence alignment methTable",
        "This low agreement ratio is also reflected in a measure called the K statistic (Carletta, 1996; Bruce and Wiebe, 1998; Ng et al., 1999).",
        "K measure takes into account chance agreement, thus better representing the state of disagreement.",
        "A Y value is calculated for each word, on a confusion matrix where rows represent the senses assigned by judge 1 (DSO) and columns represent the senses assigned by judge 2 (Semcor).",
        "Table 6 shows an example matrix for the noun \"table\".",
        "A Y value for a word is calculated as follows.",
        "We use the notation and formula used in (Bruce and Wiebe, 1998).",
        "Let nij denote the number of instances where the judge 1 assigned sense i and the judge 2 assigned sense j to the same instance, and ni+ and n+i denote the marginal totals of rows and columns respectively.",
        "The formula is: k Pi Pii Pi Pi+P+i"
      ]
    },
    {
      "heading": "1 Ei Pi+P+i",
      "text": [
        "where Pii = nii + (i.e., proportion of nii, the number n+ of instances where both judges agreed on sense i, to the total instances), Pi+ = nz+ and P = n+i n+++i n++' The K value is 1.0 when the agreement is perfect (i.e., values in the off-diagonal cells are all 0, that is, Ei Pii = 1), or 0 when the agreement is purely ods used in the experiments.",
        "11 (Ng et al., 1999) reports a higher agreement of 57%.",
        "We speculate the discrepancy might be from the version of WordNet senses used in DSO, which was slightly different from the standard delivery version (as noted in (Ng et al., 1999)).",
        "distributed across rows (or columns), that is, Pii = Pi+P+i for all 1 G_ i G_ M, where M is the number of rows/columns).",
        "Y also takes a negative value when there is a systematic disagreement between the two judges (e.g., some values in the diagonal cells are 0, that is, Pii = 0 for some i).",
        "Normally, Y > .8 is considered a good agreement (Carletta, 1996).",
        "By using the formula above, the average Y for the 191 words was .264, as shown in Table 5.12 This means the agreement between Semcor and DSO is quite low.",
        "We selected the same 191 words from our lexicon, and used their sense partitions to reduce the size of the confusion matrices.",
        "For each word, we computed the Y for the reduced matrix, and compared it with the Y for a random sense grouping of the same partition pattern.",
        "13 For example, the partition pattern of J(1 4),(2 3 5),(6)1 for \"table\" mentioned earlier (where Table 7 shows its reduced matrix) is a multi6 nomial combination (2 3 1) .",
        "The K value for a random grouping is obtained by generating 5,000 random partitions which have the same pattern as the corresponding sense partition in our lexicon, then taking the mean of their K's.",
        "Then we measured the possible increase in Y by our lexicon by taking the difference between the paired Y values for all words (i.e., Kw by our sense partition - Kw by random partition, for a word w), and performed a significance 12 (Ng et al.",
        "1999)'s result is slightly higher: r, = .317.",
        "13 For this comparison, we excluded 23 words whose sense partitions consisted of only 1 sense cover.",
        "This is reflected in the total number of instances in Table 8.",
        "test, with a null hypothesis that there was no significant increase.",
        "The result showed that the P-values were 4.17 and 2.65 for nouns and verbs respectively, which were both statistically significant.",
        "Therefore, the null hypothesis was rejected, and we concluded that there was a significant increase in Y by using our lexicon.",
        "As a note, the average K's for the 191 words from our lexicon and their corresponding random partitions were .260 and .233 respectively.",
        "Those values are in fact lower than that for the original WordNet lexicon.",
        "There are two major reasons for this.",
        "First, in general, combining any arbitrary senses does not always increase Y.",
        "In the given formula 9, K actually decreases when the increase in Ei Pii (i.e., the diagonal sum) in the reduced matrix is less than the increase in Ei Pi+P+i (i.e., the marginal product sum) by some factor.",
        "14 This situation typically happens when senses combined are well distinguished in the original matrix, in the sense that, for senses i and j, nij and nji are 0 or very small (relative to the total frequency).",
        "Second, some systematic relations are in fact easily distinguishable.",
        "Senses in such relations often denote different objects in a context, for instance ANIMAL and MEAT senses of \"chicken\".",
        "Since our lexicon groups those senses together, the K's for the reduce matrices decrease for the reason we mentioned above.",
        "Table 8 shows the breakdown of the average Y for our lexicon and random groupings.",
        "14 This is because F_i Pi+P+i is subtracted in both the numerator and the denominator in the r, formula.",
        "Note that both F_i Pii and F_i Pi+P+i always increase when any arbitrary senses are combined.",
        "The factor mentioned here is"
      ]
    },
    {
      "heading": "7 Conclusions and Future Work",
      "text": [
        "As we reported in previous sections, our tree-cut extraction method discovered 89% of the WordNet cousins.",
        "Although the precision was relatively low (50-60%), this is an encouraging result.",
        "As for the lexicon, our sense partitions consistently yielded better K values than arbitrary sense groupings.We consider these results to be quite promising.Our data is available at www.depaul.edu/-ntomuro/research/naacl-Ol.html.",
        "It is significant to note that cluster pairs and sense partitions derived in this work are domain independent.",
        "Such information is useful in broad-domain applications, or as a background lexicon (Kilgarriff, 1997) in domain specific applications or text categorization and IR tasks.",
        "For those tasks, we anticipate that our extraction methods may be useful in deriving characteristics of the domains or given corpus, as well as customizing the lexical resource.",
        "This is our next future research.",
        "For other future work, we plan to investigate an automatic way of detecting and filtering unrelated relations.",
        "We are also planning to compare our sense partitions with the systematic disagreement obtained by (Wiebe, et al., 1998)'s automatic classifier."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The author wishes to thank Steve Lytinen at DePaul University and the anonymous reviewers for very useful comments and suggestions."
      ]
    }
  ]
}
