{
  "info": {
    "authors": [
      "Stephen Clark",
      "David Weir"
    ],
    "book": "Meeting of the North American Association for Computational Linguistics",
    "id": "acl-N01-1013",
    "title": "Class-Based Probability Estimation Using a Semantic Hierarchy",
    "url": "https://aclweb.org/anthology/N01-1013",
    "year": 2001
  },
  "references": [
    "acl-A00-2034",
    "acl-A97-1052",
    "acl-E95-1016",
    "acl-J93-1003",
    "acl-J98-2002",
    "acl-P93-1024",
    "acl-P99-1014",
    "acl-W95-0103"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper concerns the acquisition of a particular kind of lexical knowledge, namely the knowledge of which noun senses can fill argument slots of predicates.",
        "Probabilities are used to represent the knowledge, and classes from a semantic hierarchy are used to estimate the probabilities.",
        "There is a particular focus on the problem of how to determine a suitable class, or level of generalisation, in the hierarchy.",
        "A pseudo disambiguation task is used to compare different class-based estimation methods."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "This paper concerns the problem of how to estimate the probability of a noun sense appearing as a particular argument to a predicate.",
        "The problem with estimating a probability model over senses is that this involves a huge number of parameters, which results in a sparse data problem.",
        "The proposal here is to define a probability model over senses in a semantic hierarchy, and exploit the fact that senses can be grouped into classes consisting of semantically similar senses.",
        "Defining probabilities in terms of classes means that the number of parameters is reduced.",
        "The assumption underlying this approach is that the probability of a sense can be approximated by a probability based on a suitably chosen class.",
        "The hierarchy used is the noun hypernym hierarchy of WordNet (Fellbaum, 1998), which consists of senses, or `lexicalised concepts', related by the `is-a-kind-of' relation.",
        "If c is-a-kind-of c', then c' is a hypernym of c. To estimate the probability of a concept, c, appearing as an argument of a predicate, a set of concepts dominated by a hypernym of c is chosen to represent c. We develop a novel solution to the problem of how to determine a suitable hypernym, or level of generalisation, in the hierarchy.",
        "A pseudo disambiguation task is used to compare our class-based estimation method with some alternative proposals."
      ]
    },
    {
      "heading": "2 The Semantic Hierarchy",
      "text": [
        "We use the noun hypernym hierarchy of Word-Net, version 1.6.",
        "A sense, or concept, in WordNet is represented by a `synset', which is the set of synonymous words that can be used to denote that concept.'",
        "For example, the synset for the concept (cocaine) is { cocaine, cocain, coke, snow, C}.",
        "Let syn(c) be the synset for concept c, and let cn(n) = { c I n C syn(c) } be the set of concepts that can be denoted by noun n. The hierarchy has the structure of a DAG, with what we call the `direct-isa' relation connecting nodes in the graph.",
        "Let isa = direct-isa* be the transitive reflexive closure of direct-isa, so that (c, C') C isa =* c' is a hypernym of c. We use c' = { c I (c, c') C isa } to denote the set consisting of c' and those concepts dominated by C'.",
        "For example, (animai) is the set consisting of those concepts that denote kinds of animals.",
        "The probability of a concept appearing as an argument of a predicate is written p(CIv, r), where c is a concept in WordNet, v is a predicate and r is an argument position.",
        "The focus in this paper is on verbs, but the techniques can be applied to any predicate that takes nominal arguments.",
        "The probability p(CIv, r) is to be interpreted as follows: this is the probability that some noun n in syn(c), when denoting concept c, appears in position r of verb v (given v and r).",
        "The example used throughout the paper is p((dog) I run, subj), which is the conditional probability that some noun in the synset 'Note that we are using concept to refer to a lexicalised concept or sense, and not a set of senses.",
        "Angled brackets are used to denote concepts in the hierarchy.",
        "of (dog), when denoting (dog), appears in the subject position of the verb run.",
        "The data used to estimate the probabilities is assumed to be in the form of (n, v, r) triples: a noun, verb and argument position.",
        "Such data can be obtained from a treebank or from a robust parser.",
        "All the data used here have been obtained using the system of Briscoe and Carroll (1997).",
        "Note that no distinction is made between the different senses of a verb, and each noun is assumed to denote exactly one concept."
      ]
    },
    {
      "heading": "3 Class-Based Probability",
      "text": []
    },
    {
      "heading": "Estimation",
      "text": [
        "Using the method of maximum likelihood to estimate p((dog)Irun,subj) would not be appropriate.",
        "The problem with maximum likelihood estimation is that it tends to over-fit the data, giving too much probability mass to cases seen in the data, and no probability mass to unseen cases.",
        "The solution proposed here is to base the probability estimate on a suitably chosen class, such as animal), so that the probabilities of unseen cases can be inferred from the seen cases.",
        "Before explaining how a suitable class is chosen, we first explain how a set of concepts c' can be used to estimate p(cI v, r).",
        "An inappropriate strategy would be to simply substitute c' for the individual concept c, since p(c'I v, r) is the conditional probability that some noun denoting a concept in c' appears in position r of verb v. For example, p((animal) I run, subj) is the probability that some noun denoting a kind of animal appears in the subject position of run.",
        "Probabilities of sets of concepts are obtained by summing over the concepts in the set:",
        "This means that p((animal)Irun,subj) is likely to be much greater than p((dog) I run, subj), and not a good approximation of p((dog) I run, subj).",
        "What can be done, though, is to condition on sets of concepts.",
        "If it can be shown that p(vI c', r), for some hypernym, c', of c, is a reasonable approximation of p(vI c, r), then we have a way of estimating p(cI v, r).",
        "The probability",
        "Since p(cl r) and p(vl r) are conditioned on the argument slot only, it is more likely these can be estimated satisfactorily using maximum likelihood estimation.",
        "This leaves p(vI c, r).",
        "Continuing with the (dog) example, the proposal is to estimate p(runl (dog), subj) using a maximum likelihood estimate of p(runl(animal),subj), or something similar.",
        "In Figure 1, it is shown",
        "Figure 1 shows how probabilities conditioned on sets of concepts can remain constant when moving up the hierarchy, and this suggests a way of finding a suitable set, c', for concept c: initially set c' equal to c, and move up the hierarchy, changing the value of c', until there is",
        "f (c, v, r) is the number of (n, v, r) triples in the data in which n is being used to denote c; V is the set of verbs in the data, and C is the set of concepts.",
        "p(vlc2, r), for each child c2 of c', can be compared to see if p(vl c', r) has significantly changed.",
        "(We ignore the probability p(vl c', r), and consider the probabilities p(vlc'i,r) only.)",
        "Before giving the details of the generalisation procedure, we give the maximum likelihood estimates of the relevant probabilities, and deal with the problem of ambiguous data.",
        "The estimates are given in Table 1.",
        "The problem is that the estimates are defined in terms of frequencies of senses, whereas the data consists of nouns.",
        "In response to this, we estimate f (c, v, r) by simply distributing the count for each noun n in syn(c) evenly among all senses of the noun: where l cn(n) l is the cardinality of cn(n).",
        "Resnik (1998) explains how this apparently crude technique works surprisingly well."
      ]
    },
    {
      "heading": "4 Finding a suitable Level of Generalisation",
      "text": [
        "In this section we give the details of how to find a suitable class to represent a concept.",
        "We first show how to test if p(vl c', r) changes significantly by moving up a node in the hierarchy.",
        "Consider the problem of deciding if p(runl(canine),subj) is a good approximation of p(runl(dog),subj).",
        "((canine) is the parent of (dog) in WordNet.)",
        "To do this, the probabilities p(runl c2, subj) are compared p(vIc,r) is equal to p(vlc,r) when c is a leaf node.",
        "using a chi-squared test, where the c2 are the children of (canine).",
        "In this case, the null hypothesis of the test is that the probabilities p(runl c27 subj) are the same for each child c2.",
        "By judging the strength of the evidence against the null hypothesis, it can be determined how similar the true probabilities are likely to be.",
        "If the test indicates that the probabilities are likely to be very different, then the null hypothesis is rejected, and the conclusion is that p(runj ( canine), subj) is not a good approximation of p(runl (dog), subj).",
        "An example contingency table, based on counts obtained from a subset of the BNC, is given in Table 2.",
        "One column contains estimates of counts arising from concepts in c2 appearing in the subject position of run: f (c27 run, subj).",
        "A second column contains estimates of counts arising from concepts in c2 appearing in the subject position of a verb other than run.",
        "The figures in brackets are the expected values, given that the null hypothesis is true.",
        "There is a choice of which statistic to use in conjunction with the test.",
        "The usual statistic encountered in text books is the Pearson chi-squared statistic, denoted X2.",
        "However, Dunning (1993) claims that the log-likelihood chi-squared statistic (G2) is more appropriate for corpus-based NLP.",
        "In Section 6, we compare the two statistics in a task-based evaluation.",
        "For Table 2, the value of G2 is 3.8 and the value of X2 is 2.5.",
        "Assuming a level of significance of a = 0.05, the critical value is 12.6 (for 6 degrees of freedom).",
        "Thus, for this a value, the null hypothesis would not be rejected for either statistic, and the conclusion would be that there is no reason to suppose p(runl(canine), subj) is not a reasonable approximation of p(runl (dog), subj).",
        "A key question is how to select the value for a.",
        "We could just select a value, such as 0.05, but any value determined in this way is to some extent arbitrary.",
        "An alternative solution is to treat a as a parameter and set it empirically, by taking a held-out test set and choosing the value of a that maximises performance on the relevant task.",
        "Note that this approach sets no constraints on the value of a: the value could be as high as 0.995, or as low as 0.0005, depending on the particular application.",
        "The procedure for finding a suitable class,",
        "C', to represent concept c in position r of verb v works as follows.",
        "(We refer to c' as the `similarity-class' of c with respect to v and r, and the hypernym c' as top(c, v, r).)",
        "Initially, a variable top is assigned to the concept c itself.",
        "Then, by working up the hierarchy, top is reassigned to be successive hypernyms of c. This continues until the probabilities associated with the sets of concepts dominated by top and the siblings of top are significantly different.",
        "Once a node is reached that results in a significant result for the chi-squared test, the procedure stops, and top is returned as top(c, v, r).",
        "In cases where a concept has more than one parent, the parent is chosen that results in the lowest value of the chi-squared statistic, as this indicates the probabilities are more similar.",
        "The set top(c, v, r) is the similarity-class of c for verb v and position r. There may be cases where the conditions for the appropriate application of a chi-squared test are not met.",
        "One condition that is likely to be violated is the requirement that expected values in the contingency table should not be too small.",
        "(A rule of thumb often found in text books is that the expected values should be greater than 5.)",
        "One response to this problem is to apply some kind of thresholding, and either ignore counts below the threshold, or only apply the test to tables that do not contain low counts.",
        "The problem with this approach is that any threshold is, to some extent, arbitrary, and there is evidence to suggest that, for some tasks, low counts are important (Collins and Brooks, 1995).",
        "Another approach would be to use Fisher's exact test, which can be applied to tables regardless of the size of the counts.",
        "The main problem with this test is that it is computationally expensive, especially for large contingency tables (and it only applies to tables with whole number counts).",
        "What we have found in practice is that applying the chi-squared test to tables with low counts tends to produce an insignificant result, and the null hypothesis is not rejected.",
        "The consequences of this for the generalisation procedure are that low count tables tend to result in the procedure moving up to the next node in the hierarchy.",
        "But given that the purpose of the generalisation is to overcome the sparse data problem, this behaviour is desirable.",
        "Table 3 shows some example generalisation levels for a small number of hand-picked verbs, over a range of values for a.",
        "The G2 statistic was used in the chi-squared tests, and the data were extracted from a subset of the BNC using the system of Briscoe and Carroll.",
        "The number of times that each verb in the table occurred in the data (with some object) is shown.",
        "The table indicates that the extent of generalisation increases with a decrease in the value of a.",
        "This is to be expected, since, given a contingency table chosen at random, a higher value of a is more likely to lead to a significant result than a lower value of a.",
        "The point of the table is not to argue that the example generalisation levels are `correct', but simply to show some examples, and give some indication of how the generalisation level changes with values in a.",
        "We argue that, since the purpose of this work is probability estimation, the most suitable level is the one that leads to the best estimate.",
        "So if (hotdog), for example, generalises to (sandwich)(in the object po",
        "sition of eat), rather than the `more intuitive' (food), this should not be considered a failure.",
        "If there exist plenty of data about sandwiches, why generalise any higher?",
        "Indeed, we show in Section 6 that to generalise unnecessarily can be harmful for some tasks."
      ]
    },
    {
      "heading": "5 Alternative Approaches",
      "text": [
        "The approaches used for comparison are those of Resnik (1993), subsequently developed by Ribas (1995), and Li and Abe (1998), which has been adopted by McCarthy (2000).",
        "These have been chosen because they directly address the question of how to find a suitable level of generalisation in WordNet.",
        "The first alternative is to use the `association score', which is a measure of how well a set of concepts, C, satisfies the selectional preferences of a verb, v, for argument position, r:",
        "An estimate of the association score, A(C, v, r), can be obtained using maximum likelihood estimates of the probabilities.",
        "The key question is how to find a suitable set to represent concept c, assuming the choice is from those sets dominated by hypernyms of c. Resnik's suggestion is to choose the set that maximises the association score.",
        "The second alternative is to use the Minimum Description Length (MDL) Principle.",
        "Li and Abe use MDL to estimate probabilities of the form p(nl v, r), where n is a noun.",
        "Their use",
        "of MDL requires the hierarchy to be viewed as a thesaurus, in which nouns are represented at leaf nodes, and each internal node represents a class, which is the set of all the nouns at leaf nodes dominated by the internal node.",
        "Once the hierarchy is viewed in this way, MDL can be used to determine a `cut' across the hierarchy, where a cut defines a partition of the leaf nodes.",
        "The appropriate class for n is the class in the cut that contains n. The probabilities are estimated by dividing the probability of the class evenly among the nouns in the class.",
        "An example cut showing part of the hierarchy, based on an example from Li and Abe (1998), is given in Figure 2.",
        "This example is for the direct object slot of eat.",
        "In this case, the appropriate class for pizza is FOOD, and the probability p(pizzal eat, obj) is estimated by dividing the probability mass for FOOD evenly among all the nouns in the synsets dominated by (food.",
        "We used McCarthy's (2000) implementation If of MDL.",
        "In order that every noun is represented at a leaf node, McCarthy creates new leaf nodes for each synset at an internal node.",
        "However, unlike Li and Abe, McCarthy does not transform WordNet into a tree, which is strictly required for Li and Abe's application of MDL.",
        "This did create a problem, in that many of the cuts returned by MDL were overgeneralising at the (entity) node.",
        "The reason is that (person), which is close to (entity), and dominated by (entity), has two parents: (life_form) and (causal_agent).",
        "This DAG-like property was responsible for the over-generalisation, and so we removed the link between (person) and (causal_agent).",
        "This appeared to solve the problem, and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in Li and Abe (1998)."
      ]
    },
    {
      "heading": "6 Pseudo Disambiguation",
      "text": [
        "Experiments The task we used to compare different generalisation techniques is similar to that used by Pereira et al.",
        "(1993) and Rooth et al.",
        "(1999).",
        "The task is to decide which of two verbs, v and V', is more likely to take a given noun, n, as an object.",
        "The test and training data were obtained as follows.",
        "A number of verb direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll.",
        "All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatised.",
        "This resulted in a data set of around 1.3 million (v, n) pairs.",
        "To form a test set, 3, 000 of these pairs were randomly selected, such that each selected pair contained a fairly frequent verb.",
        "(Following Pereira et al., only those verbs that occurred between 500 and 5, 000 times in the data were considered.)",
        "Each instance of a selected pair was then deleted from the data.",
        "This was to ensure that the test data were unseen.",
        "The remaining pairs formed the training data.",
        "To complete the test set, a further fairly frequent verb, v', was randomly chosen for each (v, n) pair.",
        "The random choice was made according to the verb's frequency in the original data set, subject to the condition that the pair (v n) did not occur in the training data.",
        "Given the set of (v, n, v') triples, the task is to decide whether (v, n) or (v', n) is the correct pair.",
        "Using our approach, the disambiguation decision for each (v, n, v') triple was made as follows:",
        "chosen that maximises the relevant probability estimate; this explains the maximisation over cn(n).",
        "The probability estimates were obtained using our class-based method, and the G2 statistic was used for the chi-squared test.",
        "Using the association score, the decision for each test triple was made as follows:",
        "We use h(c) to denote the set consisting of the hypernyms of c. The inner maximisation is over h(c), assuming c is the chosen sense of n, which corresponds to Resnik's method of choosing a set to represent c. The outer maximisation is over the senses of n, cn(n), which determines the sense of n by choosing the sense that maximises the association score.",
        "Using MDL, the disambiguation decision was made as follows (p is used to denote an estimate using the MDL approach):",
        "Since some nouns appear more than once in WordNet, the instance of n is chosen that maximises the relevant probability estimate.",
        "We use sep(n) to denote the separate instances of n.",
        "The first set of results is given in Table 4.",
        "Our technique is referred to as the `similarity-class' technique, and the approach using the association score is referred to as `Assoc'.",
        "The results are given for a range of a values, and demonstrate clearly that the performance of similarity-class varies little with changes in a, and similarity-class outperforms both MDL and Assoc.",
        "We also give a score for our approach using a simple generalisation procedure, which we call \"Low-class\".",
        "The procedure is to select the first class that has a count greater than zero (relative to the verb and argument position), which is likely to return a low level of generalisation, on the whole.",
        "The results show that our generalisation technique only narrowly outperforms the simple generalisation procedure.",
        "Note that \"Low-class\" is still using our class-based estimation method, by applying Bayes' theorem and conditioning on a class, as described in Section 3; the difference is in how class is chosen.",
        "To investigate the results, we calculated the average number of generalised levels for each approach.",
        "The number of generalised levels for a concept c (relative to a verb v and argument position r) is the difference in depth between c and top(c, v, r).",
        "To give an example of how the difference in depth was calculated, suppose (dog) generalised to (placentalsna=al) via (canine) and (carnivore); in this case the difference would be 3.",
        "For each test case, the number of generalised levels for both verbs, v and v', was calculated, but only for the chosen sense of n. The results are given in the third column of Table 4, and demonstrate clearly that both MDL and Assoc are generalising to a greater extent than similarity-class.",
        "(The fourth column gives a standard deviation figure.)",
        "These results suggest that MDL and Assoc are over-generalising, at least for the purposes of this task.",
        "To investigate why the value for a had no impact on the results, we repeated the experiment, but with 15th of the data.",
        "A new data set was created by taking every 5th pair of the original"
      ]
    },
    {
      "heading": "1.3 million pairs. A test set of 3, 000 triples was",
      "text": [
        "created from this new data set, as before, but this time only verbs that occurred between 100 and 1, 000 times were considered.",
        "The results using these test and training data are given in Table 5.",
        "These results show a variation in performance across values for a, with an optimal performance when a is around 0.75.",
        "(Of course, in practice, the value for a would need to be optimised on a held-out set.)",
        "But even with this variation, similarity-class is still outperforming MDL and Assoc across the whole range of a values.",
        "Note that the a values corresponding to the lowest scores lead to a significant amount of generalisation, which provides additional evidence that MDL and Assoc are overgeneralising for this task.",
        "The Low-class method scores highly for this data set also, but given that the task is one that apparently favours a low level of generalisation, the high score is not too surprising.",
        "As a final experiment, we compared the task performance using the X2, rather than G2, statistic in the chi-squared test.",
        "The results are given in Table 6 for the complete data set.",
        "The",
        "figures in brackets give the average number of generalised levels.",
        "The X2 statistic is performing at least as well as G2, throwing doubt on the claim by Dunning (1993) that the G2 statistic is better suited for use in corpus-based NLP.",
        "The results show clearly that the average level of generalisation is slightly higher for G2 than X2.",
        "This suggests a possible explanation for the results presented here, and those in Dunning (1993), which is that the X2 statistic provides a less conservative test when counts in the contingency table are low.",
        "A less conservative test is better suited to the pseudo disambiguation task, since this results in a low level of generalisation, on the whole, which is good for this task.",
        "In contrast, the task that Dunning considers, the discovery of bigrams, is better served by a more conservative test."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We have presented a class-based estimation method that incorporates a procedure for finding a suitable level of generalisation in WordNet.",
        "This method has been shown to provide superior performance on a pseudo disambiguation task, compared with two alternative approaches.",
        "One of the features of the generalisation procedure is the way that a, the level of significance in the chi-squared test, is treated as a parameter.",
        "This allows some control over the extent of generalisation, which can be tailored to particular tasks."
      ]
    },
    {
      "heading": "8 Acknowledgements",
      "text": [
        "This work was supported by an EPSRC studentship to the first author.",
        "We would like to thank Diana McCarthy for suggesting the pseudo disambiguation task and providing the MDL software, John Carroll for supplying the data, and the anonymous reviewers for their helpful comments."
      ]
    }
  ]
}
