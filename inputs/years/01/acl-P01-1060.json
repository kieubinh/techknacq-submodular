{
  "info": {
    "authors": [
      "Helmut Schmid",
      "Mats Rooth"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P01-1060",
    "title": "Parse Forest Computation of Expected Governors",
    "url": "https://aclweb.org/anthology/P01-1060",
    "year": 2001
  },
  "references": [
    "acl-C00-2105",
    "acl-P89-1018",
    "acl-P99-1059"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In a headed tree, each terminal word can be uniquely labeled with a governing word and grammatical relation.",
        "This labeling is a summary of a syntactic analysis which eliminates detail, reflects aspects of semantics, and for some grammatical relations (such as subject of finite verb) is nearly uncontroversial.",
        "We define a notion of expected governor markup, which sums vectors indexed by governors and scaled by probabilistic tree weights.",
        "The quantity is computed in a parse forest representation of the set of tree analyses for a given sentence, using vector sums and scaling by inside probability and flow."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "A labeled headed tree is one in which each nonterminal vertex has a distinguished head child, and in the usual way non-terminal nodes are labeled with non-terminal symbols (syntactic categories such as NP) and terminal vertices are labeled with terminal symbols (words such as The governor algorithm was designed and implemented in the Reading Comprehension research group in the 2000 Workshop on Language Engineering at Johns Hopkins University.",
        "Thanks to Marc Light, Ellen Riloff, Pranav Anand, Brianne Brown, Eric Breck, Gideon Mann, and Mike Thelen for discussion and assistance.",
        "Oral presentations were made at that workshop in August 2000, and at the University of Sussex in January 2001.",
        "Thanks to Fred Jelinek, John Carroll, and other members of the audiences for their comments.",
        "reads).1 We work with syntactic trees in which terminals are in addition labeled with uninflected word forms (lemmas) derived from the lexicon.",
        "By percolating lemmas up the chains of heads, each node in a headed tree may be labeled with a lexical head.",
        "Figure 1 is an example, where lexical heads are written as subscripts.",
        "We use the notation h(v) for the lexical head of a vertex v, and c(v) for the ordinary category or word label of V. The governor label for a terminal vertex v in such a labeled tree is a triple which represents the syntactic and lexical environment at the top of the chain of vertices headed by v. Where u is the maximal vertex of which v is a head vertex, and u' is the parent of u, the governor label for v",
        "of integers t such that (i) if acct, then act; (ii) if anct and",
        "is the tuple (c(u), c(u'), h(u')).2 Governor labels for the example tree are given in Figure 2.",
        "As observed in Chomsky (1965), grammatical relations such as subject and object may be reconstructed as ordered pairs of category labels, such as (NP,S) for subject.",
        "So, a governor label encodes a grammatical relation and a governing lexical head.",
        "Given a unique tree structure for a sentence, governor markup may be read off the tree.",
        "However, in view of the fact that robust broad coverage parsers frequently deliver thousands, millions, or thousands of millions of analyses for sentences of free text, basing annotation on a unique tree (such as the most probable tree analysis generated by a probabilistic grammar) appears arbitrary.",
        "Note that different trees may produce the same governor labels for a given terminal position.",
        "Suppose for instance that the yield of the tree in Figure 1 has a different tree analysis in which the PP is a child of the VP, rather than NP.",
        "In this case, just as in the original tree, the label for the fourth terminal position (with word label paper) is (NP,VP,read).",
        "Supposing that there are only two tree analyses, this label can be assigned to the fourth word with certainty, in the face of syntactic ambiguity.",
        "The algorithm we will define pools governor labels in this way."
      ]
    },
    {
      "heading": "2 Expected Governors",
      "text": [
        "Suppose that a probabilistic grammar licenses headed tree analyses t1, ..., t,, for a sentence or, and assigns them probabilistic weights p1, ..., pv,.",
        "For a label m in column 2, column 3 gives E(m) as computed with a PCFG weighting of trees, and column 4 gives E(m) as computed with a head-lexicalized weighting of trees.",
        "Values below 0.1 are omitted.",
        "According to the lexicalized model, the PP headed by of probably attaches to VFP (finite verb phrase) rather than NP.",
        "Let ml ... mn be the governor labels for word position k determined by ti, ..., t,, respectively.",
        "We define a scheme which divides a count of 1 among the different governor labels.",
        "For a given governor tuple m, let",
        "The definition sums the probabilistic weights of trees with markup m, and normalizes by the sum of the probabilities of all tree analyses of or.",
        "The definition may be justified as follows.",
        "We work with a markup space M= C x C x W, where C is the set of category labels and W is the set of lemma labels.",
        "For a given markup triple m, let",
        "be the function which maps m to 1, and m' to 0 form' zA m. We define a random variate",
        "which maps a tree t to bm, where m is the governor markup for word position k which is determined by tree t. The random variate r is defined on labeled trees licensed by the probabilistic grammar.",
        "Note that [M – � IR] is a vector space (with pointwise sums and scalar products), so that expectations and conditional expectations may be defined.",
        "In these terms, E is the conditional expectation of r, conditioned on the yield being or.",
        "This definition, instead of a single governor label for a given word position, gives us a set of pairs of a markup m and a real number E(m) in [0,1 ], such that the real numbers in the pairs sum to 1.",
        "In our implementation (which is based on Schmid (2000a)), we use a cutoff of 0.",
        "1, and print only indices m where E(m) is above the cutoff.",
        "Figure 3 is an example.",
        "A direct implementation of the above definition using an iteration over trees to compute E would be unusable because in the robust grammar of English we work with, the number of tree analyses for a sentence is frequently large, greater than 109 for about 1/10 of the sentences in the British National Corpus.",
        "We instead calculate E in a parse forest representation of a set of tree analyses."
      ]
    },
    {
      "heading": "3 Parse Forests",
      "text": [
        "A parse forest (see also Billot and Lang (1989)) in labeled grammar notation is a tuple P = (NP, EP, RP, SP, LP) where (NP, EP, RP, SP) is a context free grammar (consisting of non-terminals NP, terminals EP, rules RP, and a start symbol SP) and Lp is a function which maps elements of Np to non-terminals in an underlying grammar G = (N, E, R, S) and elements of Ep to terminals in G. By using Lp on symbols on the left hand and right hand sides of a parse forest rule, Lp can be extended to map the set of parse forest rules Rp to the set of underlying grammar rules R. Lp is also extended to map trees licensed by the parse forest grammar to trees licensed by the underlying grammar.",
        "An example is given in figure 4.",
        "Where xENP U E p U RP, let IP (x) be the set of trees licensed by (NP, EP, RP, SP) which have root symbol x in the case of a symbol, and the set of trees which have x as the rule expanding the root in the case or a rule.",
        "I (x) is defined to be the multiset image of IP(x) under LP.",
        "I(x) is the multiset of inside trees represented by parse",
        "resenting two tree analyses of John reads every paper on markup.",
        "The labeling function drops subscripts, so that L p (VP 1) = VP.",
        "forest symbol or rule x.3 Let Cp(x) be the set of trees in IP(SP) which contain x as a symbol or use x as a rule.",
        "C(x) is defined to be the multiset image of Cp(x) under LP.",
        "C(x) is the multiset of complete trees represented by the parse forest symbol or rule x.",
        "Where p is a probability function on trees licensed by the underlying grammar and x is a symbol or rule in P,",
        "i(x) is called the inside probability for x and 0(x) is called the flow for x.4 Parse forests are often constructed so that all inside trees represented by a parse forest nonterminal nENP have the same span, as well as the same parent category.",
        "To deal with headedness and lexicalization of a probabilistic grammar, we construct parse forests so that, in addition, all inside trees represented by a parse forest nonterminal have the same lexical head.",
        "We add to the labeled grammar a function Hp which labels parse forest symbols with lexical heads.",
        "In our implementation, an ordinary context free parse forest is 3W e use multisets rather than set images to achieve correctness of the inside algorithm in cases where P represents some tree more than once, something which is possible given the definition of labeled grammars.",
        "A correct parser produces a parse forest which represents every parse for the input sentence exactly once.",
        "4 These quantities can be given probabilistic interpretations and/or definitions, for instance with reference to conditionally expected rule frequencies for flow.",
        "first constructed by tabular parsing, and then in a second pass parse forest symbols are split according to headedness.",
        "Such an algorithm is shown in appendix B.",
        "This procedure gives worst case time and space complexity which is proportional to the fifth power of the length of the sentence.",
        "See Eisner and Satta (1999) for discussion and an algorithm with time and space requirements proportional to the fourth power of the length of the input sentence in the worst case.",
        "In practical experience with broad-coverage context free grammars of several languages, we have not observed super-cubic average time or space requirements for our implementation.",
        "We believe this is because, for our grammars and corpora, there is limited ambiguity in the position of the head within a given category-span combination.",
        "The governor algorithm stated in the next section refers to headedness in parse forest rules.",
        "This can be represented by constructing parse forest rules (as well as ordinary grammar rules) with headed tree domains of depth one.5 Where u is a parse forest symbol on the right hand side of a parse forest rule r, we will simply state the condition “u is the head of r”.",
        "The flow and governor algorithms stated below call an algorithm PF-INSIDE(P, w) which computes inside probabilities in P, where w is a function giving probability parameters for the underlying grammar.",
        "Any probability weighting of trees may be used which allows inside probabilities to be computed in parse forests.",
        "The inside 5 See footnote 1.",
        "Constructed in this way, the first rule in parse forest in Figure 4 has domain {E, – 1, 0}, and labeling function W, S1), (-1,NP1), (0, VPl)}.",
        "When parse forest",
        "algorithm for ordinary PCFGs is given in figure 5.",
        "The parameter w maps the set of underlying grammar rules R which is the image of LP on LR to reals, with the interpretation of rule probabilities.",
        "In step 5, LP maps the parse forest rule r to a grammar rule LP (r) which is the argument of w. The functions lhs and rhs map rules to their left hand and right hand sides, respectively.",
        "Given an inside algorithm, the flow 0 may be computed by the flow algorithm in Figure 6, or by the inside-outside algorithm."
      ]
    },
    {
      "heading": "4 Governors Algorithm",
      "text": [
        "The governor algorithm annotates parse forest symbols and rules with functions from governor labels to real numbers.",
        "Let t be a tree in the parse forest grammar, let v be a symbol in t, let u be the maximal symbol in t of which v is a head, or v itself if v is a non-head child of its parent in t, and let u' be the parent of u in t. Recall that",
        "is a vector mapping the markup triple (LP(u), LP (u'), Hp (u')) to 1 and other markups to 0.",
        "We have constructed parse forests such that (LP(u), LP(u'), HP(u')) agrees with the governor label for the lexical head of the node corresponding to v in LP(t).",
        "A parse forest tree t and symbol v in t thus determine the vector (4), where u and u' are defined as above.",
        "Call the vector determined in this way b(t, v).",
        "Where v is parse forest symbol in P and r is a parse forest rule in P, let",
        "Assuming that P = (NP, EP, RP, SP, LP) is a parse forest representing each tree analysis for a sentence exactly once, the quantity E for terminal position k (as defined in section 1) is found by summing y(v) for terminal symbols v in EP which have string position k.",
        "The algorithm PF-GOVERNORS is stated in Figure 3.",
        "Working top down, if fills in an array y[•] which is supposed to agree with the quantity -y(.)",
        "defined above.",
        "Scaled governor vectors are created for non-head children in step 10, and summed down the chain of heads in step 9.",
        "In step 6, vectors are divided in proportion to inside probabilities (just as in the flow algorithm), because the set of complete trees for the left hand side of r are partitioned among the parse forest rules which expand the left hand side of r. Consider a parse forest rule r, and a parse forest symbol u on its right hand side which is not the head of r. In each tree in CP(r), u is the top of a chain of heads, because u is a non-head child in rule r. In step 10, the governor tuple describing the syntactic environment of u in trees in CP(r) (or rather, their images under LP) is constructed",
        "of grammar symbols and spans.",
        "Our parser constructs parse forests organized according to span.",
        "as bcp(u),cp(1hs(r)),Hp(1hs(r)).",
        "The scalar multi",
        "the relative weight of trees in CP(r).",
        "This is appropriate because y(u) as defined in equation (5) is to be scaled by the relative weight of trees in CP(u).",
        "In line 9 of the algorithm, y is summed into the head child u.",
        "There is no scaling, because every tree in CP(r) is a tree in CP(u).",
        "A probability parameter vector w is used in the inside algorithm.",
        "In our implementation, we can use either a probabilistic context free grammar, or a lexicalized context free grammar which conditions rules on parent category and parent lexical head, and conditions the heads of non-head children on child category, parent category, and parent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998).",
        "The requisite information is directly represented in our parse forests by CP and HP.",
        "Thus the call to PF-INSIDE in line 1 of PF-GOVERNORS may involve either a computation of PCFG inside probabilities, or head-lexicalized inside probabilities.",
        "However, in both cases the algorithm requires that the parse forest symbols be split according to heads, because of the reference to HP in line 10.",
        "Construction of head-marked parse forests is presented in the appendix.",
        "The LoPar parser (Schmid, 2000a) on which our implementation of the governor algorithm is based represents the parse forest as a graph with at most binary branching structure.",
        "Nodes with more than two daughter nodes in a conventional parse forest are replaced with a right-branching tree structure and common sub-trees are shared between different analyses.",
        "The worst-case space complexity of this representation is cubic (cmp.",
        "Billot and Lang (1989)).",
        "LoPar already provided functions for the computation of the head-marked parse forest, for the flow computation and for traversing the parse forest in depth-first and topologically-sorted order (see Cormen et al.",
        "(1994)).",
        "So it was only necessary to add functions for data initialization, for the computation of the governor vector at each node and for printing the result."
      ]
    },
    {
      "heading": "5 Pooling of grammatical relations",
      "text": [
        "The governor labels defined above are derived from the specific symbols of a context free grammar.",
        "In contrast, according to the general markup methodology of current computational linguistics, labels should not be tied to a specific grammar and formalism.",
        "The same markup labels should be produced by different systems, making it possible to substitute one system for another, and to compare systems using objective tests.",
        "Carroll et al.",
        "(1998) and Carroll et al.",
        "(1999) propose a system of grammatical relation markup to which we would like to assimilate our proposal.",
        "As grammatical relation symbols, they use atomic labels such as dobj (direct object) an ncsubj (non-clausal subject).",
        "The labels are arranged in a hierarchy, with for instance subj having subtypes ncsubj, xsubj, and csubj.",
        "There is another problem with the labels we have used so far.",
        "Our grammar codes a variety of features, such as the feature VFORM on verb projections.",
        "As a result, instead of a single object grammatical relation (NP,VP), we have grammatical relations (NP,VP.N), (NP,VP.FIN), (NP,VP.TO), (NP,VP.BASE), and so forth.",
        "This may result in frequency mass being split among different but similar labels.",
        "For instance, a verb phrase will have read every paper might have some analyses in which read is the head of a base form VP and paper is the head of the object of read, and others where read is a head of a finite form VP, and paper is the head of the object of read.",
        "In this case, frequencies would be split between (NP,VP.BASE,read) and (NP,VP.FIN,read) as governor labels for paper.",
        "To address these problems, we employ a pooling function GR which maps pairs of categories to symbols such as ncsubj or obj.",
        "The governor tuple (c(u), c(u'), h(u')) is then replaced by (GR(c(u), c(u')), h(u)) in the definition of the governor label for a terminal vertex v. Line 10 of PF-GOVERNORS is changed to -Y[U] �-- 'Y[u]+O[r]bGR(cp(u),cp(lhs(r))),Hp(lhs(r))More flexibility could be gained by using a rule and the address of a constituent on the right hand side as arguments of GR.",
        "This would allow the following assignments.",
        "The head of a rule is marked with a prime.",
        "In the first pair, the objects in double object construction are distinguished using the address.",
        "In each case, the child-parent category pair is (NP,VP.FIN), so that the original proposal could not distinguish the grammatical relations.",
        "In the second pair, a VP.",
        "TO argument is distinguished from a VP.TO modifier using the category of the head.",
        "In each case, the child-parent category pair is (VP.TO,VP.FIN).",
        "Notice that in Line 10 of PF-GOVERNORS, the rule r is available, so that the arguments of GR could be changed in this way."
      ]
    },
    {
      "heading": "6 Discussion",
      "text": [
        "The governor algorithm was designed as a component of Spot, a free-text question answering system.",
        "Current systems usually extract a set of candidate answers (e.g. sentences), score them and return the n highest-scoring candidates as possible answers.",
        "The system described in Harabagiu et al.",
        "(2000) scores possible answers based on the overlap in the semantic representations of the question and the answer candidates.",
        "Their semantic representation is basically identical to the head-head relations computed by the governor algorithm.",
        "However, Harabagiu et al.",
        "extract this information only from maximal probability parses whereas the governor algorithm considers all analyses of a sentence and returns all possible relations weighted with estimated frequencies.",
        "Our application in Spot works as follows: the question is parsed with a specialized question grammar, and features including the governor of the trace are extracted from the question.",
        "Governors are among the features used for ranking sentences, and answer terms within sentences.",
        "In collaboration with Pranav Anand and Eric Breck, we have incorporated governor markup in the question answering prototype, but not debugged or evaluated it.",
        "Expected governor markup summarizes syntactic structure in a weighted parse forest which is the product of exhaustive parsing and inside-outside computation.",
        "This is a strategy of dumbing down the product of computationally intensive statistical parsing into unstructured markup.",
        "Estimated frequency computations in parse forests have previously been applied to tagging and chunking (Schulte im Walde and Schmid, 2000).",
        "Governor markup differs in that it is reflective of higher-level syntax.",
        "The strategy has the advantage, in our view, that it allows one to base markup algorithms on relatively sophisticated grammars, and to take advantage of the lexically sensitive probabilistic weighting of trees which is provided by a lexicalized probability model.",
        "Localizing markup on the governed word increases pooling of frequencies, because the span of the phrase headed by the governed item is ignored.",
        "This idea could be exploited in other markup tasks.",
        "In a chunking task, categories and heads of chunks could be identified, rather than categories and boundaries."
      ]
    },
    {
      "heading": "References",
      "text": []
    },
    {
      "heading": "A Relation Between Flow and Inside-Outside Algorithm",
      "text": [
        "The inside-outside algorithm computes inside probabilities i[v] and outside probabilities o[v].",
        "We will show that these quantities are related to the flow 0(v) by the equation 0[v] _ o[v] i [v] /i [Sp] .",
        "i [Sp] is the inside probability of the root symbol, which is also the sum of the probabilities of all parse trees.",
        "According to Charniak (1993), the outside probabilities in a parse forest are computed by: � o[v] _ o[lhs(r)] i[v] The outside probability of the start symbol is 1.",
        "We prove by induction over the depth of the parse forest that the following relationship holds:",
        "It is easy to see that the assumption holds for the root symbol Sp:",
        "After a few transformations, we get the equation �[v]_ i[v] 0[lhs(r)]i[r] i[Sp] r:vErhs(r) 2[U] which is equivalent to � 0[v] = o[v] i[s]] according to the definition of o[v].",
        "So, the induction hypothesis is generally true."
      ]
    },
    {
      "heading": "B Parse Forest Lexicalization",
      "text": [
        "The function LEXICALIZE below takes an unlex-icalized parse forest as argument and returns a lexicalized parse forests, where each symbol is uniquely labeled with a lexical head.",
        "Symbols are split if they have more than one lexical head.",
        "LEXICALIZE creates new terminal symbols by calling the function NEWT.",
        "The new symbols are linked to the original ones by means of l[•].",
        "For each rule in the old parse forest, the set of all possible combinations of the lexicalized daughter symbols is generated.",
        "The function LEM(r) returns the lemma associated with lexical rule r. ADD(P, r, h, d) 1 if ]v E l[lhs(r)] s.t.",
        "Hp[v] = h 2 then v' �-- v"
      ]
    },
    {
      "heading": "3 else v' �--NEWNT(P) 4 Lp(v') Lp(lhs(r)) 5 Hp[v'] h 6 r' �--NEWRULE(P, v', d) 7 Lp(r') �-- Lp(r) 8 return r'",
      "text": [
        "For each combination of lexicalized daughter symbols, a new rule is inserted by calling ADD.",
        "ADD calls NEWNT to create new non-terminals and NEWRULE to generate new rules.",
        "A nonterminal is only created if no symbol with the same lexical head was linked to the original node."
      ]
    }
  ]
}
