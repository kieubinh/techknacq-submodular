{
  "info": {
    "authors": [
      "Elena Eneva",
      "Rose Hoberman",
      "Lucian Vlad Lita"
    ],
    "book": "SIGDAT Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W01-0503",
    "title": "Learning Within-Sentence Semantic Coherence",
    "url": "https://aclweb.org/anthology/W01-0503",
    "year": 2001
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In many domains such as speech recognition and machine translation it is extremely useful to be able able to distinguish coherent from non-coherent sentences.",
        "We introduce a set of word-based statistical features which measure semantic coherence and can be used to enhance any language application where coherent sentences need to be generated or recognized.",
        "We train a decision tree using the constructed feature set to automatically classify sentences as coherent or not.",
        "We find that our combination of boosted decision trees and coherence features achieves an accuracy of 80% when distinguishing trigram-generated sentences (non-coherent) from those in the Broadcast News dataset (coherent)."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In order to improve conventional statistical language models, we need to find aspects of language which are not adequately captured, and then incorporate them into the model.",
        "Although adding any computable feature of a language which discriminates between the 'true' English language model and the statistical model will give some improvement, an ideal feature should occur frequently, yet exhibit a significant discrepancy (Rosenfeld et al., 2001).",
        "Perhaps the most salient deficiency of conventional language models is their complete failure at modeling semantic coherence.",
        "These models capture short distance correlations among words in a sentence, yet are unable to distinguish meaningful sentences (where the content words come from the same semantic domain) from 'fake' sentences (where the content words * Supported by a National Science and Engineering Graduate Fellowship are drawn randomly) .",
        "As a result, in many language technology applications such as speech recognition, errors that are obvious to a human observer (e.g. a noun replaced by an acoustically similar but semantically different noun) cannot be salvaged by the model.",
        "For example, if the speaker says I would like a glass full of water, and the system recognizes class instead of glass, we would like to automatically identify and correct this error.",
        "One way to identify and thus avoid these types of errors is to identify sentences which are not semantically coherent.",
        "This work is an attempt to automatically construct features which can then be used to learn a function which will distinguish between coherent and non-coherent sentences.",
        "Once we have learned such a measure of semantic coherence, we can combine it with the initial baseline model.",
        "One model which is naturally suited for this task is the maximum entropy model introduced by Rosenfeld (1997), which directly models the probability of an entire sentence.",
        "The model is ideal for modeling whole-sentence phenomena because it treats each sentence as a 'bag of features' where features are arbitrary computable properties of the sentence.",
        "To incorporate semantic coherence into this exponential model, (Cai et al., 2000) did some initial work on constructing within-sentence semantic coherence features.",
        "A set of five features was extracted from each sentence and added directly to the exponential model.",
        "These features decreased perplexity over the baseline trigram model by 3 to 5%."
      ]
    },
    {
      "heading": "In this paper we augment the limited fea",
      "text": [
        "ture set presented in (Cai et al., 2000) by constructing roughly seventy features, which fall into nine broad classes.",
        "The majority of the fea",
        "ture classes rely on word-pair correlation values in sentences.",
        "To derive a measure of whole sentence coherence, we used our feature set to build a classifier which distinguishes between real sentences generated by a human (taken as examples of coherent sentences) and fake sentences generated by a conventional language model (taken as examples of non-coherent sentences).",
        "The confidence score produced by our classifier has an accuracy of 80%, which is a significant performance improvement over the results obtained by using only the features developed in previous work.",
        "This measure of semantic coherence can be used to improve the original language model, and thus improve performance in any application where language modeling is used.",
        "2 Approach We use semantic association between word-pairs and other sentence characteristics (described later) to construct useful features for distinguishing between coherent and non-coherent sentences."
      ]
    },
    {
      "heading": "2.1 Semantic Association between Word-Pairs",
      "text": [
        "We use the same correlation measure as in (Cai et al., 2000).",
        "For each word-pair in our dataset, we calculate a measure of association called Q (Yule's statistic) based on the appropriate 2x2 contingency table of training data co-occurrences of the two words in the pair.",
        "Table 1 shows such a contingency table for two words.",
        "C11 is the count of sentences in the training corpus which contain both words, C12 is obtained by subtracting C11 from the number of sentences with Word1 in it; C21 is obtained by subtracting C11 from the number of sentences with W ord2 in it; C22 is the total number of sentences minus the other three counts.",
        "Based on such tables we can compute the Q statistic",
        "The values of Q thus range from 1 to 1; the higher the Q value, the stronger the correlation between the two words.",
        "Q is 1 when two words have never occurred in the same sentence, and is 1 when they always occur together.",
        "Each sentence, coherent or non-coherent, can be represented by a variable length list of Q values â€“ a Q value for each pair of content words.",
        "By defining some statistics on these Q lists and comparing the true and fake data, we observe differences in the distributions of the statistics.",
        "A classifier can use these statistics to distinguish between coherent and non-coherent sentences."
      ]
    },
    {
      "heading": "2.2 Features",
      "text": [
        "In order to train a classifier to discern between coherent and non-coherent sentences, we first generated a large set of features.",
        "We did not explore features which use syntactic knowledge, information about specific words, or lexical resources like thesauri.",
        "The features are based only on the word-pair correlation values, the co-occurrence counts, and statistics on stop words.",
        "We have implemented roughly seventy features, which fall into nine classes.",
        "These classes are:",
        "1.",
        "Simple statistics - the simple correlation",
        "characteristics of a sentence: mean, median, maximum, minimum, range and variance of word pair correlation values.",
        "These fall into two sub-groups:",
        "(a) Statistics considering all word-pairs (b) Statistics considering only pairs separated by at least 5 words - short distance correlations and local coherence are usually well modelled by trigram language models 2.",
        "Sentence Length 3.",
        "Percentage of correlation values above some threshold.",
        "4.",
        "High/low correlations across large/small distances - very high correlations across large distances are strong evidence that",
        "the sentence is from Broadcast News, as are negative correlations at very close distances.",
        "We use thresholds to vary what is"
      ]
    },
    {
      "heading": "5 Future Work and Summary",
      "text": [
        "Yule's Q statistic is not necessarily the best measure of word-pair correlation because it is extremely susceptible to data sparseness.",
        "The point estimates of the Q values for words that appear infrequently in our corpus are often unreliable.",
        "One idea for reducing the effect of these unreliable estimates is to use a confidence distribution instead of the current point estimate.",
        "Another possible approach would be to use a different correlation statistic; one possibility is the X statistic.",
        "Another possibility is to use the method of (Dagan et al., 1995) to improve the estimates of the likelihood of co-occurrences that are rare in the training data.",
        "When people are given this classification task of distinguishing coherent and non-coherent sentences, they tend to group together words in the sentence that belong to the same topic.",
        "We would like to model this process by using other clustering methods to group the semantically similar words together in each sentence.",
        "Then statistics based on these clusters could be used as additional features to improve the coherence measure.",
        "In this paper we developed a successful approach for automatically distinguishing between coherent and non-coherent sentences.",
        "Our combination of feature sets and learners achieved a reduction in error of 24% from the features used in previous work (Cai et al., 2000).",
        "Our successful approach used the semantic association between word-pairs as well as simple features such as sentence length and percentage of stop words to construct a highly accurate classifier.",
        "More work remains to be done to evaluate how well the semantic coherence measure improves the baseline language model.",
        "This evaluation can be performed by incorporating the semantic coherence features into an exponential language model and then using the model for a practical application such as n-best list rescor-ing."
      ]
    }
  ]
}
