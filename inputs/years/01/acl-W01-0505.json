{
  "info": {
    "authors": [
      "Seonho Kim",
      "Juntae Yoon",
      "Mansuk Song"
    ],
    "book": "SIGDAT Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W01-0505",
    "title": "Improving Lexical Mapping Model of English-Korean Bitext Using Structural Features",
    "url": "https://aclweb.org/anthology/W01-0505",
    "year": 2001
  },
  "references": [
    "acl-C00-2123",
    "acl-C00-2131",
    "acl-C00-2135",
    "acl-C94-2178",
    "acl-C96-1040",
    "acl-H91-1026",
    "acl-J93-1004",
    "acl-J93-2003",
    "acl-J95-4004",
    "acl-J96-1001",
    "acl-J96-1002",
    "acl-J97-3002",
    "acl-P00-1056",
    "acl-P93-1003",
    "acl-P93-1004",
    "acl-P94-1051",
    "acl-P97-1063",
    "acl-P98-2191",
    "acl-P98-2221"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The problem of finding lexical alignments for given sentence pairs is computationally expensive.",
        "Furthermore, it is much difficult to find lexical alignments between Korean and English since they have considerably different syntactic structures and the coverage of word-for-word correspondences is low.",
        "This paper presents a method for extracting structural features which can reduce mapping space by allowing only probable alignments.",
        "We describe how the features improve the performance of the lexical alignment model.",
        "The structural features provide the information for the correspondences of parts-of-speech (POS) sequences which are useful in translation.",
        "Based on maximum entropy (ME) concept, the structural features are incrementally selected, which are later embedded in the lexical alignment model.",
        "It turns out that the features help get better lexical alignments of Korean and English by offering linguistic knowledge."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Aligned bitexts are useful for the derivation of bilingual lexical resources which are used for machine translation and cross languages information retrieval.",
        "Thus, a lot of approaches have been suggested to find sets of corresponding word tokens (Brown et al., 1993; Berger et al., 1996; Melamed, 1997), phrase (Shin et al., 1996), noun phrase (Ku-piec, 1993), and collocation (Smadja et al., 1996) in a bitext.",
        "Some works have used lexical association measures for finding word correspondences (Gale and Church, 1991; Fung and Church, 1994).",
        "However, the association measures can be misled in cases where a word in a source language frequently co-occurs with more than one word in a target language or in cases of indirect association'(Melamed, 1997).",
        "In other works, iterative parameter re-estimation 'Suppose that uk and vk are indeed mutual translation and uk and uk+l often co-occur in text.",
        "Then vk and uk+l will also co-occur more than expected by chance , which is represented as indirect association.",
        "techniques based on IBM model 1-5 2 have been employed (Brown et al., 1993).",
        "They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).",
        "However, we are often faced with some difficulties as follows, when the IBM model-based approaches are directly applied to the alignment, especially on bitext involving a less closely related language pair.",
        "1.",
        "It needs excessive iteration time for parameter estimation and high decoding complexity.",
        "Thus most systems assumed one-to-one correspondence to reduce computational complexity.",
        "However, word sequences are not translated literally word for word.",
        "For example, in cases of collocations, compound nouns, and ambiguous words with different meaning dependent on the context, they require phrase-level correspondences.",
        "2.",
        "Most systems use little or no linguistic knowledge to structure the underlying models.",
        "The distortion probability and the fertility probability for finding word correspondences is a weak description for word order change between languages and 1:n mapping modeling.",
        "As the result, lots of ungrammatical sentences and too many parameters to be estimated are allowed.",
        "In addition, many words are aligned to the empty word due to the overfitting problem (Och et al., 2000).",
        "3.",
        "In order to estimate parameters properly, it requires a very large volume of bilingual aligned text.",
        "In this paper, we use structural correspondences to",
        "overcome the above problem in bilingual text alignment.",
        "2 The problems of lexical alignment in Korean and English bitext Although one-to-one correspondence assumption has been shown to give highly accurate results in closely related language pair (Melamed, 1997), it does not fit in structurally different language pair such as Korean and English.",
        "According to Shin et al.",
        "(1996), the coverage of one-to-one correspondence in Korean and English bitext is approximately 34% and many-to-many mappings exceed 40% in lexical alignments.",
        "Besides, Korean and English show much difference in terms of unit of mutual translation as shown in",
        "Thus, we use structural or linguistic information to find accurate lexical alignments of bitext.",
        "In general, structural information can be represented by the form of bilingual correspondence of phrase structure (Watanabe et al., 2000) or dependency structure (Yamamoto, 2000).",
        "To find these structural correspondences between a language pair, unification-based grammar (Matsumoto, 1993) or bilingual grammar (Wu, 1997) can be used.",
        "That is, if we try to find structural match of bitext, syntactic analysis of each text should be done first.",
        "However, syntactic analysis of each text(or bitext) is also a hard and computationally complicated problem.",
        "In this paper, we propose POS sequence feature.",
        "We consider correspondences between POS tag sequences of bitext as the structural information which is important in determining structures and reducing unnecessary parameters of a statistical alignment model.",
        "We induce the correspondences (structural features) by a feature selection method based on the ME framework.",
        "Finally, the structural features are embedded in a lexical alignment model of English and Korean bitext.",
        "Our model offers the following",
        "advantages: 1.",
        "It can overcome the limitation of word-for-word correspondences.",
        "2.",
        "The model can take advantage of the explicit introduction of some knowledge about the language.",
        "Therefore, it can reduce a lot of param",
        "eters in statistical machine translation by eliminating syntactically unlikely alignments.",
        "The lexical alignment model iterates only over a sub",
        "set of probable alignments.",
        "3.",
        "It is possible to estimate the probability of lexical alignment in a relatively small size of corpora.",
        "4.",
        "The structural information is helpful in the construction of a bilingual grammar.",
        "3 Overview of the model",
        "In general, there exist constaints among POS sequences mapping when aligning bitext.",
        "For instance, in many cases a noun in Korean is translated to a noun or a noun preceded by an article in English.",
        "This POS constraint would be useful information in alignment of a closely related pair as well as a less closely related language pair.",
        "Some approaches design an alignment algorithm that maximizes the number of matching POS in aligned segments (Papageorgiou et al., 1994).",
        "We also assume that the mapping information of POS sequences of a language pair is useful in a model of statistical machine translation.",
        "If there are similarities between corresponding POS sequences in bitext, the structural feature would be easily computed or identified.",
        "However, a POS sequence in English often correspond to a totally different POS sequence in Korean as shown in the following example:",
        "It is caused by the discrepancy between two languages.",
        "Korean is an agglutinative language.",
        "A sentence in Korean consists of a series of syntactic units called eojeol.",
        "An eojeol delimited by whitespace is often composed of a content word and function words.",
        "Tense markers, clausal connectives, particles and so forth are contained in an eojeol.",
        "Thus, one or more words in English often correspond to an eojeol, i.e. a couple of morphemes.",
        "For instance, a phrase, `to the school', in English corresponds to an eojeol `-&4 ,2J-,_(haggyo-ro, school-to)' in Korean.",
        "Thus, we need a method for mapping POS sequences of bitext.",
        "In this paper, the correspondences of POS tag sequences are obtained by automatic feature selection based on the ME framework.",
        "Here, a feature is defined as a correspondence of POS tag sequences in bitext.",
        "The outline of finding correspondences of POS tag sequences is as follows: First, our model starts with initial features obtained by a supervision step.",
        "The initial features are extracted from a small portion of bitext, which of weights are trained by the IIS algorithm (Berger et al., 1996; Pietra et al., 1997).",
        "These are called initial active features.",
        "In the next step, a feature pool is constructed from training data.",
        "At this time, only features giving a large gain to the model are selected.",
        "The final output of feature selection is the set of active features and the correspondences of POS tag sequences that are represented with conditional probabilities.",
        "The resulted features are used for the parameters of bilingual text alignment.",
        "In the process, we look at the words to ensure a correct alignment.",
        "That is, the POS sequences are in fact encoding particular words.",
        "The underlying process is as follows: Input: a set L of POS-labeled sentence pairs.",
        "1.",
        "Make a set J7 of correspondences of tag sequences, (t,, tk) from a small portion of L by hand.",
        "2.",
        "Set J7 into a set of initial active features, A.",
        "3.",
        "Compute the weights of the initial active features A using IIS algorithm.",
        "4.",
        "Create a feature pool P which is a set of possible combinations of tag sequences from sentence pairs.",
        "5.",
        "Filter P using frequency counts and similarity with A.",
        "6.",
        "Compute the approximate gains of the features in P. 7.",
        "Select features(N) with large gain values, and add A.",
        "8.",
        "Compute the lexical alignment of bitext using p(tk It,) where (t,, tk) E A."
      ]
    },
    {
      "heading": "4 Feature Selection",
      "text": [
        "As mentioned above, features which represent mappings of POS sequences in two languages are automatically learned from bitext.",
        "The learning consists of two steps: (1) supervised step and (2) unsupervised feature selection.",
        "In the supervised step, we manually aligned English and Korean texts.",
        "From the manually aligned text, we consturct a feature pool which has initial POS sequence correspondence.",
        "In the unsupervised feature selection, the POS sequence correspondeces are added to the feature pool."
      ]
    },
    {
      "heading": "4.1 Supervision Step",
      "text": [
        "In the supervision step, a small portion of bitext is tagged using Brill's tagger (Brill, 1995) and `MORANY' (Moon et.",
        "al., 1999), each of which is for English and Korean tagging respectively.",
        "We manually aligned each sentence pair in the bitext and collected their correspondences of tag sequences.",
        "For simplicity, we adjusted some part of Brill's tag set.",
        "First of all, we classified sentential patterns of English and Korean.",
        "Then, we aligned English and Korean sentence pairs on the basis of the patterns.",
        "Also, we made tag sequence construction rules with respect to each language by analysis of the cases where the words of one unit are separated and adjacent.",
        "The rules are used when making a feature pool.",
        "After collecting the correspondences of POS tag sequences, we used them as a set of initial active features, A."
      ]
    },
    {
      "heading": "4.2 Construction of a Feature Pool",
      "text": [
        "In this chapter, we describe how a feature pool is constructed.",
        "Our task is to construct a probability model p that produces a corresponding Korean tag sequence Tk for a given English tag sequence Te.",
        "As features to represent the model, we use co-occurrence information of POS tag sequences.",
        "Let TS be all possible correspondences of tag sequences for a specific sentence pair, S. We then define a feature �function (or a feature) as follows:",
        "A feature ftejtb, which indicates co-occurrence between tags appearing in Ts, expresses information for predicting that an English POS tag sequence to maps into a Korean POS tag sequence tk.",
        "In order to make a feature pool, given a sentence pair, we first construct all possible combinations of English POS tag sequences and Korean POS tag sequences.",
        "Among them, only features ftejt, that satisfy the following two conditions are added into the feature pool P.",
        "• count(ftejt,) > 15 • there exist tk.",
        "such that (te7 tk.)",
        "E A and the similarity value (simply counting of same tag) between tk and tkx is greater than 0.6"
      ]
    },
    {
      "heading": "4.3 Feature selection",
      "text": [
        "Since the set of P is too vast, a feature selection process is needed to select useful features.",
        "For this, each feature is evaluated according to how much they contribute to the likelihood of training data, which is called gain.",
        "Before explaining feature gain and a process of feature selection, we will give a brief introduction of ME.",
        "In ME, a feature gives information to a probability model and it has a weight.",
        "Thus, the model is constrained by features we defined.",
        "In general weights of the features are trained by the improved iterative scaling (IIS) algorithm that minimizes the Kullback-Leibler divergence between the model and the empirical distribution of the training data.",
        "In fact, our model reduces to a simple type of probability model that can be derived simply from a ratio counts since the features do not overlap.",
        "Let PA be the optimal model constrained by a set of initial active features A, then it can be represented in (1).",
        "The weights (A) of active features are computed by the IIS before feature selection.",
        "Let Afi be AU fi, and PAf be the optimal model in the space of probability distribution after adding feature fi.",
        "The model PA f contains another parameter a, in addition to the parameters given by active features, which is a weight for the feature fi.",
        "In order to make it tractable to compute feature selection, we assume that the addition of a feature fi affects only the single parameter a.",
        "The only parameter which distinguishes models of (2) is a.",
        "The improvement (gain) of a model after adding a single feature fi can be estimated by measuring difference of maximum log-likelihood between L(PAf ) and L(PA).",
        "We denote the gain for feature fi by O(Afi), which is represented in (3).",
        "The following algorithm is used to compute the gain of the model with respect to fi.",
        "We adopted Berger's method for computing gains (Berger et al., 1996).",
        "For the details, the reader is referred to (Berger et al., 1996; Pietra et al., 1997).",
        "GSA f. (a) = – E. p(x)pat ((f – M(x))2 �x) where a = a,,,+1, Afi = A U fi",
        "won's method.",
        "With the gain value, we can recognize importance of a feature, i.e. how much the feature accords with the model.",
        "As a result, we can select useful features with high gain values and obtain their conditional probabilities.",
        "Put another way, we can get the correspondence probabilities of POS tag sequences."
      ]
    },
    {
      "heading": "5 lexical alignment",
      "text": [
        "In this section, we describe how the correspondence probabilities between bilingual POS sequences is embedded in lexical alignment.",
        "In Korean-English machine translation, a translation system finds the corresponding sentence e given a Korean sentence k. The fundamental equation of machine translation can be represented in (4), where the random variables K and E are a Korean sentence and a English sentence making up a translation, and the random variable A is an alignment between them.",
        "The equation is related with a language model probability, P(e), a translation model probability P(kle).",
        "In this paper, we are interested in estimating the translation model probability P(kle).",
        "In this work, we modified the IBM 1 model using the structural information for estimating translation probabilities.",
        "The translation probability of a specific sentence pair, k and e is",
        "2.",
        "Set ao = 0 3.",
        "Repeat the following until GAf (a,,,) has converged :",
        "Compute a,,,+1 from a,,, using In (5), an English sentence e has l possible phrases, eplep2•••epl, and a Korean sentence k has m possible phrases, kp1kp2•••kpTn and the phrases of e have corresponding sequences of POS categories, C,, CeP2•••C, , and the phrases of k have the sequences of P6S categories Ck'ICk'2•••Ckl-.",
        "e is",
        "1993).",
        "Note that the probability P(Ck,.lC,, .)",
        "is precomputed by the feature selection process and only lexical alignment parameter t(kphlepi) can be estimated by the EM algorithm using sentence pairs of bitext, (k*, a*), s = 1, ..., S. According to the equation (4), we can eliminate the combination that P(Ck, .",
        "IC,,,) is zero.",
        "Thus, the model iterates only over a subset of probable alignments.",
        "To estimate probability t(kplep), the expected number of times (fractional counts) that the phrase ep connects to kp in the translation (kle) is used.",
        "The count denoted by c(kp l ep- k, e) can be expressed in (7) using (6) and translation probabilities are rees-timated by (8)."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "We present results tested on English-Korean bitext that is extracted from the web site of `Korea Times' and a magazine for English learning (Table 2).",
        "We manually aligned 700 POS-tagged sentence pairs to obtain initial parameters for correspondences of tag sequences.",
        "As shown in Table 3, the coverage of word-for-word correspondences in English-Korean bitext was only 31.2%.",
        "As a result, 1,483 correspondences of tag sequences were collected from the manually aligned bitext (Table 4).",
        "The correspondences were used as initial active features and the weights of the initial active features were computed by IIS algorithm.",
        "Table 5",
        "Through the process of feature pool construction, 8,147 features(tag sequence correspondences) were selected from the 23,000 sentence pairs.",
        "Since we used the filtering method and tag sequence construction rules, the size of the feature pool was not large.",
        "In the feature selection step, we chose useful features with the gain threshold of 0.008.",
        "As a result of feature selection, we obtained the probability model that given a specific English POS tag sequence, a corresponding Korean POS tag sequence happens to occur.",
        "Table 6 shows some examples of conditional probabilities.",
        "The table shows that the determiner of English is generally translated into NULL or adnominal word in Korean.",
        "The conditional probabilities regarding the correspondences of POS tag sequences were used as known parameters of the lexical alignment model."
      ]
    },
    {
      "heading": "Effect of Smoothing",
      "text": [
        "As mentioned before, in previous IBM-based models, many words are aligned to the empty word and rare words are misaligned.",
        "Thus, we tested if the structural features are effective in smoothing.",
        "For this, the accuracy of lexical correspondences was evaluated both on low frequency words and high fre",
        "quency words.",
        "Table 7 shows the accuracy of some results obtained by lexical alignment.",
        "In the training bitext, English 2149 words and Korean 3317 eojeols with low frequency 4 and English 712 words and Korean 486 eojeols with high frequency (50300) were found.",
        "For an evaluation, 100 words and 100 eojeols were selected out of them.",
        "The alignments of the words (eojeols) were evaluated.",
        "As a result, it turns out that the structural features are effective in rare words.",
        "Effect of reducing a parameter space Another advantage of the use of structural features is to reduce parameter space of the lexical alignment model.",
        "To show the impacts on the size of the parameter space reduced by our model, we compared the results from our model with those from the IBM 1-model presented by Brown et al.",
        "(1993) on 100 sentence pairs.",
        "The number of parameters obtained means the counts of possible lexical mappings.",
        "As shown in Table 8, the number of prameters were drastically reduced in our model.",
        "Considering the complexity of another models (IBM 2-5) it is obvious that they have more parameters.",
        "Effect of improving the results of lexical alignments For evaluating the efficacy of the structural features, we compared the results with IBM model 1.",
        "As described before, only n(English):1 and 1:1 mapping are possible in IBM model since one-to-one correspondence is assumed.",
        "Thus, we selected only n:1 and 1:1 alignments out of the alignment results.",
        "For comparison, we investigated the alignments of the English 100 words with high frequency, which were explained above.",
        "Table 9 shows the accuracy of lexical alignments.",
        "It is shown that the structural features have an effect on the alignment even though the amount of data investigated is small.",
        "However, the overall accuracy of the alignment is somewhat low, which is mainly due to the small size of training samples.",
        "Table 10 shows some results of mutual translation.",
        "We see a considerable improvement when allowing for structural features (correspondences of POS tag sequences) in lexical alignments.",
        "Error Analysis Except the errors by the incorrect parameter estimation, most errors of correspondences of POS tag sequences are caused by POS tagging errors.",
        "In addition, the correspondences of adverbs turn out to be"
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "Because of the considerable difference between English and Korean, computation cost of lexical alignment is very high.",
        "One solution of the problem is to provide mapping information of syntactic structure between the two languages.",
        "In this paper, we defined the structural feature as the correspondence of POS tag sequences and presented a method for extraction of structural features for Korean-English bilingual alignment.",
        "Firstly, the initial active features were collected from a small size of manually aligned bitext, which are trained by IIS which is a training algorithm for ME.",
        "Secondly, extracted from training data, the features giving a large gain were added to the set of active features.",
        "Furthermore, the features extracted were tested for lexical alignment of bitext.",
        "The experiment showed that the features are helpful for reducing the mapping space in alignment.",
        "We expect that the alignment can be more accurate and efficient by combining the structural features with translation lexicon in the future."
      ]
    }
  ]
}
