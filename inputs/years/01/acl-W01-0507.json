{
  "info": {
    "authors": [
      "Hiroya Takamura",
      "Yuji Matsumoto"
    ],
    "book": "SIGDAT Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W01-0507",
    "title": "Feature Space Restructuring for SVMs With Application to Text Categorization",
    "url": "https://aclweb.org/anthology/W01-0507",
    "year": 2001
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we propose a new method of text categorization based on feature space restructuring for SVMs.",
        "In our method, independent components of document vectors are extracted using ICA and concatenated with the original vectors.",
        "This restructuring makes it possible for SVMs to focus on the latent semantic space without losing information given by the original feature space.",
        "Using this method, we achieved high performance in text categorization both with small number and large numbers of labeled data."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The task of text categorization has been extensively studied in Natural Language Processing.",
        "Most successful works rely on a large number of classified data.",
        "However, it is hard to collect classified data, so considering real applications, text categorization must be realized even with a small number of labeled data.",
        "Several methods to realize it have been proposed so far (Nigam et al., 2000), but they need to be further developed.",
        "For that purpose, we have to take advantage of invaluable information offered by the property of unlabeled data.",
        "In this paper, we propose a new categorization method based on Support Vector Machines (SVMs) (Vapnik, 1995) and Independent Component Analysis (ICA) (Herault and Jutten, 1986; Bell and Sejnowski, 1995).",
        "SVM is gaining popularity as a classifier with high performance, and ICA is one of the most prospective algorithms in the field of signal processing, which extracts independent components from mixed signals.",
        "SVM has been applied in many applications such as Image Processing and Natural Language Processing.",
        "The idea to apply SVM for text categorization was first introduced in (Joachims, 1998).",
        "However, when the number of labeled data are small, SVM often fails to produce a good result, although several efforts against this problem have been made.",
        "There are two strategies for improving performance in the case of a limited number of data.",
        "One is to modify the learning algorithm itself (Joachims, 1999a; Glenn and Mangasarian, 2001), and the other is to process training data (Weston et al., 2000), including the selection of features.",
        "In this paper, we focus on the latter, especially on feature space restructuring.",
        "For processing training data, Principal Component Analysis (PCA) is often adopted in classifiers such as k-Nearest Neighbor method (Mitchell, 1997).",
        "But the conventional dimension-reduction methods fail for SVM as shown by experiments in Section 6.",
        "Unlike the conventional ones, our approach uses the components obtained with ICA to augment the dimension of the feature space.",
        "ICA is built on the assumptions that the sources are independent of each other and that the signals observed at multiple-points are linear mixtures of the sources.",
        "While the theoretical aspects of ICA are being studied, its possibility to applications is often pointed out as in (Bell and Sejnowski, 1997).",
        "The idea of using ICA for text clustering is adopted in several works such as in (Isbell and Viola, 1998).",
        "In those works, vector representation model is adopted (i.e. each text is represented as a vector with the word-frequencies as the elements).",
        "It is reported however that the independent components do not always correspond to the desired classes, but represent some kind of characteristics of texts (Kolenda et al., 2000).",
        "In (Kaban and Girolami, 2000), they showed that the number of potential components were larger than that of human-annotated classes.",
        "These facts imply that it is not easy to apply ICA directly for text classification.",
        "Taking these observations into consideration, we take the following strategy: first we perform ICA on input document vectors, and second, create the restructured information by concatenating the reduced vectors (i.e. the values of the independent components) and the original feature vectors.",
        "PCA is an alternative restructuring method.",
        "So we conducted experiments using SVM with various input vectors: original feature vectors, reduced feature vectors and restructured feature vectors (reduction and restructuring are performed by PCA and ICA).",
        "For comparison, we conducted experiments using Transductive SVM (TSVM) (Joachims, 1999a) as well, which is designed for the case of a small number of labeled data.",
        "Using the proposed method (SVM with ICA), we obtain better results than ordinary SVM and TSVM, with both small and large numbers of labeled data."
      ]
    },
    {
      "heading": "2 Support Vector Machines",
      "text": []
    },
    {
      "heading": "2.1 Brief Overview of Support Vector Machines",
      "text": [
        "Support Vector Machine (SVM) is one of the large-margin classifiers (Smola et al., 2000).",
        "Given a set of pairs,",
        "of a feature vector and a label, SVM constructs a separating hyperplane with the largest margin (the distance between the hyperplane and the vectors, see Figure 1): f (x) = w • x + b.",
        "(2) Finding the largest margin is equivalent to minimizing the norm 11w11, which is expressed as:",
        "This is realized by solving the quadratic program (dual problem of (3)):",
        "(the solid line corresponds to the optimal hyperplane) .",
        "where az's are Lagrange multipliers.",
        "Using the az's that maximize (4), w is expressed as",
        "Unlabeled data are classified according to the signs of (6)."
      ]
    },
    {
      "heading": "2.2 Kernel Method",
      "text": [
        "SVM is a linear classifier and its separating ability is limited.",
        "To compensate this limitation, Kernel Method is usually combined with SVM (Vapnik, 1995).",
        "In Kernel Method, the dot-product in (4) and (6) is replaced by a more general inner-product K(xz, x), called the kernel function.",
        "Polynomial kernel (xz • xj + 1) d (d E N+) and RBF kernel exp{ – 11xz – xj112/2Q21 are often used.",
        "Using kernel method means that feature vectors are mapped into a (higher dimensional) Hilbert space and linearly separated there.",
        "This mapping structure makes non-linear separation possible, although SVM is basically a linear classifier.",
        "Another advantage of kernel method is that although it deals with a high dimensional (possibly infinite) Hilbert space, there is no need to compute high dimensional vectors explicitly.",
        "Only the general inner-products of two vectors are needed.",
        "This leads to a relatively small computational overhead."
      ]
    },
    {
      "heading": "2.3 Transductive SVMs",
      "text": [
        "The Transductive Support Vector Machine (TSVM) is introduced in (Joachims, 1999a), which is one realization of transductive learning in (Vapnik, 1995).",
        "It is designed for the classification with a small number of labeled data.",
        "Its algorithm is approximately as follows:",
        "1. construct a hyperplane using labeled data in the same way as the ordinary SVMs.",
        "2. classify the unlabeled (test) data according to the current hyperplane.",
        "3. select the pair of a positively classified sample and a negatively classified sample that are nearest to the hyperplane.",
        "4. exchange the labels of those samples, if the margin gets larger by exchanging them.",
        "5. terminate if a stopping-criterion is satisfied.",
        "Otherwise, go back to step 2.",
        "This is one way to search for the largest margin, permitting the relabeling of test data that have already been labeled by the classifier in the previous iteration."
      ]
    },
    {
      "heading": "3 Independent Component Analysis",
      "text": [
        "Independent Component Analysis (ICA) is a method by which source signals are extracted from mixed signals.",
        "It is based on the assumptions that the sources s E R' are statistically independent of each other and that the observed signals x E R' are linear mixtures of the sources:",
        "Here the matrix A is called a mixing matrix.",
        "We observe x as a time series and estimate both A and s = (s1 , • • • , s � ) .",
        "So our purpose here is to find a demixing matrix W such that sl, • • •, s., are as independent of each other as possible: The computation proceeds by way of descent learning with an objective function indicating independence.",
        "There are several criteria of independence and their learning rules, among which we take here Infomax approach (Bell and Sejnowski, 1995), but with natural gradient (Amari, 1998).",
        "Its learning rule is",
        "where, g(u) = 1/(1 + exp( – u))."
      ]
    },
    {
      "heading": "4 Text Categorization Enhanced with Feature Space Restructuring",
      "text": [
        "As in most previous works, we adopt Vector Space Model (Salton and McGill, 1983) for representing documents.",
        "In this framework, each document d is represented as a vector (fl, • • • , fd) with word-frequencies as its elements."
      ]
    },
    {
      "heading": "4.1 Feature Space Restructuring",
      "text": [
        "First we reduce the dimension of document vectors using PCA or ICA.",
        "As for PCA, we follow the previous work described in , e.g., (Deer-wester et al., 1990).",
        "In (Isbell and Viola, 1998), they use ICA for dimension reduction and obtain a good result in Information Retrieval.",
        "At the first step of our method, where the reduced vectors are obtained, we follow their method.",
        "In this framework, each document d is considered as a linear mixture of sources s representing topics.",
        "Each wordplays a role of \"microphone\" and receives a word-frequency in the document as a mixed signal at each time unit.",
        "This formulation is represented by the equation:",
        "where A is a mixing matrix.",
        "Although both A and s are unknown, they can be obtained using the independence assumption.",
        "The source signals s are considered as a reduced expression of this document.",
        "In the case of PCA, the restructuring is processed in the same way.",
        "The only difference is that independent components correspond to principal components for the PCA case.",
        "After computing a reduced vector s with PCA or ICA, we concatenate the original vector d and the reduced vector s:",
        "This transformation means that we do not rely only on the reduced information, but make use of both the reduced and the original information, that is, the restructured information."
      ]
    },
    {
      "heading": "4.2 Text Categorization",
      "text": [
        "Regarding d^ as the input feature vector of a document, we use SVM for categorization.",
        "Since SVMs are binary classifiers themselves, so we take here the one-versus-rest method to apply them for multi-class classification tasks."
      ]
    },
    {
      "heading": "5 Theoretical Perspective",
      "text": []
    },
    {
      "heading": "5.1 Validation as a Kernel Function",
      "text": [
        "The proposed feature restructuring method can be considered as the use of a certain kernel for the pre-restructured feature space.",
        "We give an explanation for the linear case.",
        "Given two vectors, d1 and d2, the kernel function K in the restructured space is expressed as,",
        "Considering the fact that each of two terms above is a kernel and that the sum of two kernels is also a kernel (Vapnik, 1995), the proposed restructuring is equivalent to using a certain kernel in the pre-restructured space."
      ]
    },
    {
      "heading": "5.2 Interpretation of Feature Space Restructuring",
      "text": [
        "The expression (12) shows that weights are put on the latent semantic indices determined by ICA and PCA respectively.",
        "The criterion of meaningfulness depends on which of ICA and PCA is used.",
        "Note that weighting is different from reducing.",
        "In the dimension-reduction methods, only the latent semantic space is considered, but in our method, the original feature space still directly influences the classification result.",
        "This property of our method makes it possible to focus on the information given by the latent semantic space, without losing information given by the original feature space.",
        "In text categorization, classes to be predicted are sometimes characterized by local information such as the occurrence of a certain word, but sometimes dominated by global information such as the total frequency of a certain group of words.",
        "Considering this situation and the above property of our method, it is not surprising that out method gives a good result."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "To evaluate the proposed method, we conducted several experiments.",
        "The data used here is the Reuters-21578 dataset.",
        "The most frequent 6 categories are extracted from the training-set of the corpus.",
        "This leaves 4872 documents (see Table 1).",
        "Some part of them is used as training data and the rest is used as test data.",
        "Only the words occurring more than twice are used.",
        "Both stemming and stop-word removal are performed.",
        "For computation, we used SVM-light (Joachims, 1999b).",
        "We conducted two kinds of experiments.",
        "The first one focuses on evaluating the performance of the proposed method for each category, with a fixed number of labeled data (Section 6.1).",
        "The second one is conducted to show that the proposed method gives a good result also when the number of labeled data increases (Section 6.2).",
        "The results are evaluated by F-measures.",
        "To evaluate the performance across categories, we computed Micro-average and Macro-average (Yang, 1999) of F-measures.",
        "Micro-average is obtained by first computing precision and recall for all categories and then using them to compute the F-measure.",
        "Macro-average is computed by first calculating F-measures for each category and then averaging them.",
        "Micro-average tends to be dominated by large-sized categories, and Macro-average by small-sized ones.",
        "The kernel function used here is a linear kernel.",
        "The number of independent or principal components extracted by ICA or PCA is set to 50."
      ]
    },
    {
      "heading": "6.1 Performance with a Fixed Number of Data",
      "text": [
        "In this experiment, we treated 100, 500, 1000 and 2000 samples as labeled respectively and kept the other 4772, 4372, 3872 and 2872 samples unlabeled.",
        "The experiment was conducted 10 times for each sample-size repeatedly with randomly selected labeled samples and their average values are computed.",
        "The result is shown in Tables 2, 3, 4 and 5.",
        "In the row of \"Method\",",
        "combinations of restructuring methods are written.",
        "\"Original\" means the data of original document vectors.",
        "\"PCA\" and \"ICA\" mean the data of only reduced vectors, respectively.",
        "\"Original+PCA\" and \"Original+ICA\" are the restructured data explained in Section 4.",
        "The proposed method yields a high F-measure in all the categories for 1000 and 2000 labeled data and in most categories for 100 and 500 labeled data.",
        "The last two rows of Tables 2, 3, 4 and 5 show that both Micro-average and Macro-average are the highest for the proposed method.",
        "This means that the proposed method performs well both for large-sized categories (e.g., earn) and small-sized categories (e.g., interest), regardless with the number of labeled data."
      ]
    },
    {
      "heading": "6.2 Performance for the Increase of the Labeled Data",
      "text": [
        "To investigate how each method behaves when the number of labeled data increases, we conducted this experiment.",
        "The number of labeled data ranges from 100 to 2000.",
        "The results are shown in Figure 2 and Figure 3.",
        "\"PCA\" gives a good score only with a small number of data and \"Original\" gives a good score only with a large number of data.",
        "In contrast to them, the proposed method produces high performance both with small and large numbers of data."
      ]
    },
    {
      "heading": "7 Conclusions",
      "text": [
        "We proposed a new method of feature space restructuring for SVM.",
        "In our method, independent components are extracted using ICA and concatenated with the original vectors.",
        "Using this new vectors in the restructured space, we achieved high performance both with small and large numbers of labeled data.",
        "The proposed method can be applied also to other machine learning algorithms provided",
        "that they are robust against noise and can handle a high-dimensional feature space.",
        "From this point of view, it is expected that the proposed method is useful for kernel-based methods, to which SVM belongs.",
        "As a future work, we need to find a way to decide the number of independent components to be extracted.",
        "In this paper, we set the number to 50 in an ad-hoc way.",
        "However, the appropriate number must be predicted based on a theo",
        "retical reason.",
        "Toward this problem, theories of model selection such as Minimum Description Length (Rissanen, 1987) or Akaike Information Criterion (Akaike, 1974) could be a good theoretical basis.",
        "As explained in Section 4, two terms d'1d2 and d1A'Ad2 are simply concatenated in our method.",
        "But either of these terms can be multiplied with a certain constant.",
        "This means that either of the original space and the Latent Semantic Space can be weighted.",
        "Searching for the best weighting scheme is one of the future works."
      ]
    },
    {
      "heading": "Acknowledgment",
      "text": [
        "We would like to thank Thomas Kolenda (Technical University of Denmark) for helping us with the code."
      ]
    }
  ]
}
