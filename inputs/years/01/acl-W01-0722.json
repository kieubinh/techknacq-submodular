{
  "info": {
    "authors": [
      "James Hammerton"
    ],
    "book": "Workshop on Computational Natural Language Learning CoNLL",
    "id": "acl-W01-0722",
    "title": "Clause Identification With Long Short-Term Memory",
    "url": "https://aclweb.org/anthology/W01-0722",
    "year": 2001
  },
  "references": [],
  "sections": [
    {
      "text": [
        "word, the chunk tags and POS tags consisted of 25, 5 and 7 units respectively.",
        "The word representations were derived using lexical space encodings (Zavrel and Veen-stra, 1996), the rest by hand'.",
        "An input unit is used to indicate which pass through the current sentence the network is currently in (see below).",
        "There are thus a total of 38 input units.",
        "• The outputs were represented by 13 unit vector representations.",
        "These were supposed to indicate the number of clauses starting or ending on the current word by having 2 sets of units, N units of which were set to the value \"1\" when N clauses started or ended on the current word.",
        "However a mistake in the encoding was only spotted when all the training and testing on the development data had been done.",
        "Note that when presented as targets to the network, the zeros are converted 0.1s and the ones to 0.9s.",
        "This helps training, most probably because it helps prevent the weights taking on very high values that make the network resistant to change.",
        "• Each sentence is presented to the network, one word/POS tag/chunk tag at a time, in two passes.",
        "On the first pass, the network is not presented with target outputs.",
        "On the second pass the network has to output the number of clauses starting and ending on the current word.",
        "• The purpose of the first pass is to enable the network to collect information to use in disambiguation during the second pass.",
        "• A network with 12 memory blocks of 8 cells was used in training.",
        "• The network was trained for 1000 iterations",
        "on the first 1000 sentences from the sentences from sections 15 to 18 of the Wall Street Journal corpus.",
        "The best set of weights as indicated by the number of erroneous patterns was then used in testing.",
        "When deciding if a pattern was erroneous a tolerance of 0.39 was used.",
        "I.e. if all output activations came within 0.39 of the target value the pattern was deemed to be 'Contact the author for details of the input/output representations.",
        "correct.",
        "The learning rate was 0.3 and no momentum was used.",
        "• The network was tested on the development and test data for the shared task.",
        "When evaluating the network's output, the outputs are converted to brackets.",
        "These are balanced by assuming that every opening bracket the net outputs is valid, and discarding any extra closing brackets.",
        "Should closing brackets need to be added, they are added after the final word in the sentence (i.e. before the final punctuation).",
        "The network used above was much larger than anything used in the LSTM literature and was intensive to train (hence the decision to train only on the first 1000 sentences).",
        "Whilst this may in part reflect the difficulty of the task, it may also reflect the possibility that the approach to training the network for the task is not optimal.",
        "E.g. correcting the mistake in the encoding of the output tags could improve performance.",
        "During investigation to see whether training of some LSTM networks could be improved for another task (noun-phrase bracketing), it was discovered that outputting all zeroes (as opposed to not using targets) during the first pass improved performance, as did using orthogonal representations for the POS and chunk tags.",
        "Applying these changes (and correcting the mistake in the encoding of the output tags described above) enabled a network with only 12 blocks of 4 cells to be trained and to reach a similar level of performance on the same training set as the network above.",
        "This network had 90 inputs due to the use of orthogonal vectors for the tags.",
        "Finally, a third network was trained on the first 2000 sentences of the training set, which also employed 12 blocks of 4 cells and the same vector representations of the words, tags and output as with the second network above."
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "Table 1 gives the results of training the networks mentioned above and Table 2 gives the results of testing them.",
        "Note that the fscores are below the baseline value given in the specification for the shared task in the first 2 cases but the fscore is just above baseline in the 3rd case.",
        "Note also",
        "that the recall is above baseline in all 3 cases.",
        "The column headings are as follows:",
        "• Iters.",
        "The number of iterations performed when the best error was reached.",
        "• MSE.",
        "The mean squared error between the target outputs and the actual outputs.",
        "• Errs.",
        "The number of erroneous patterns.",
        "• Weights.",
        "The number of weights in the network (the most accurate measure of network size) .",
        "• Precision.",
        "The percentage of clauses found that were correct.",
        "• Recall.",
        "The percentage of clauses defined in the corpus that were found.",
        "• Fscore.",
        "F = P+R where P=precision and R=recall."
      ]
    },
    {
      "heading": "5 Concluding Remarks",
      "text": [
        "As noted earlier this work was exploratory in nature as part of an investigation into using LSTM networks for shallow parsing tasks.",
        "Given that these preliminary attempts to do this task and the networks were trained on only a small part of the training set, which has a total of 8936 sentences, and that improved performance compared to the first network was achieved whilst using a smaller network, these results are not as disappointing as they might initially appear.",
        "Both the fscore and the recall are above baseline in the best case, and recall is above baseline in all cases.",
        "There is also scope for improving on these results.",
        "The differences in performance between Net 1 and Net 2 on the development data may simply be due to noise from different weight initialisations.",
        "A more thorough experiment would employ averages from several training runs to make allowances for this.",
        "The scope for improving on these results comes from the possibilities of training with more data, training for more iterations and possible further improvements in training times through different ways of presenting the task to the networks.",
        "However it is clear there is some way to go before these networks reach a good level of performance.",
        "Future work will look at improving the performance via increasing the amount of training data, using the difference between the current word vector and the previous word vector as an extra input to the network (which thus explicitly provides the transitions from one word to the next) and possibly employing a different processing strategy to the current 2-pass strategy.",
        "Acknowledgements The author would like to thank Ronan Reilly and Fred Cummins for their comments and advice on this work and for Fred's LSTM code.",
        "This work was funded by the EU TMR project \"Learning Computational Grammars\"."
      ]
    }
  ]
}
