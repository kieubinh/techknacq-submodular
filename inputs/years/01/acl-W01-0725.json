{
  "info": {
    "authors": [
      "Antonio Molina",
      "Ferran Pla"
    ],
    "book": "Workshop on Computational Natural Language Learning CoNLL",
    "id": "acl-W01-0725",
    "title": "Clause Detection Using HMM",
    "url": "https://aclweb.org/anthology/W01-0725",
    "year": 2001
  },
  "references": [
    "acl-W00-0732",
    "acl-W01-0708"
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": [
        "In this work, we apply a specialized HMM approach to the shared task: clause identification (Tjong Kim Sang and Dejean, 2001).",
        "The HMM formalism (Rabiner and Juang, 1986) has widely been used to solve other NLP problems, such as POS tagging, chunking, partial parsing, etc.",
        "A similar technique (Lexicalized HMM), that takes into account certain words to lexi-calize the contextual language model, was previously applied for solving POS tagging and chunking problems (Pla et al., 2000b).",
        "Usually, for these tasks, specialized HMMs perform better than non-specialized HMMs (Pla, 2000a).",
        "We used specialized HMMs to solve the three parts of the task.",
        "Part 1 (clause start detection) and Part 2 (clause end detection) are performed as a tagging problem, that is, we assign the more probable tag to each token of the input.",
        "Part 3 (embedded clause detection) is a more complex task because the output must be correctly balanced.",
        "Therefore, it is carried out in two phases: first, we find the best sequence of tags (segmentation in clauses) for the input sentence; second, we correct some balancing inconsistencies observed in the output by applying some rules.",
        "2 Clause detection as a tagging problem We consider clause detection to be tagging problem.",
        "From the statistical point of view, tagging can be defined as a maximization problem.",
        "Let 0 be a set of output tags and V the vocabulary of the application.",
        "Given an input sentence I = i1, ... , iT, where ij E V : Vj, the process consists of finding the sequence of states of maximum probability on the model.",
        "That is, the sequence of output tags, 0 = 01, ... , OT, where o2 E 0 : Vi.",
        "This process can be formalized as follows:",
        "Due to the fact that this maximization process is independent of the input sequence, and taking into account the Markov assumptions, the problem is reduced to solving the following equation (for a second – order HMM):",
        "The parameters of equation 2 can be represented as a second – order HMM whose states correspond to a tag pair.",
        "Contextual probabilities, P(ojl0j-1,oj-2), represent the transition probabilities between states and P(ijloj) represents the output probabilities.",
        "For this clause – splitting task, the available information consists of words, POS tags, chunk tags, start tags (Part 1), end tags (Part 2) and clause tags (Part 3).",
        "(See Figure 1).",
        "In our approach, we must define the input vocabulary (V) and the output vocabulary (0) from the available information.",
        "We tested our system with different combinations of this information for each part of the task.",
        "Following, we present the criteria that yield the best performance on the development set.",
        "For the three parts, we consider that input sentences are composed of the sequence of POS and Chunk tags associated to each word,",
        "parts of the task.",
        "I = (pi, chi), (p2, ch2), ... , (PT, chT).",
        "Therefore, the vocabulary of the application, V, is defined as tuples (POS tag, Chunk tag).",
        "The output vocabulary is different for each part of the task.",
        "In Part 1, 0 = {S, X }; in Part 2, 0 = {E, X}; and in Part 3, we considered the clause boundaries whose depth level is lower than a certain value, that is, 0 = {(S*, *S), *, (S * S), *S)S), ... }.",
        "In this case, this value corresponds to the maximum depth level observed in the training set.",
        "Thus, given an input sentence I, the process of clause detection consists of finding the sequence of states (the sequence of clause tags) of maximum probability on the model.",
        "This process is carried out by Dynamic Programming Decoding using the Viterbi algorithm.",
        "This basic model has two main drawbacks.",
        "The first one is that the output tag set is too generic to produce accurate models.",
        "Moreover, in Part 3, the model does not assure a correct balancing of the embedded clauses in the sentence.",
        "To solve these problems, we define a technique which specializes the models.",
        "This technique consists of enriching the language model by incorporating a set of features to the output vocabulary.",
        "In Part 1 and Part 2, we have relabeled the output tag of an input word by adding the corresponding POS tag.",
        "In Part 3, we have also added the number corresponding to the depth level of the clause in the sentence in order to reduce the incorrectly balanced clauses.",
        "An example of the result of applying this specialization criteria to a sentence from the training sets can be seen in Figure 1 (see columns with specialized – SP – tags).",
        "We learnt the corresponding specialized HMM for each part of the task using this new training set.",
        "Each model was smoothed by applying a standard linear interpolation method.",
        "Note that, when specialized HMMs are used, no change is needed both in the learning and the tagging processes.",
        "You simply have to map the sequence of specialized output tags to the original output tags.",
        "This substitution can be done in a direct way.",
        "In Part 3, the smoothed model guarantees a complete coverage of the language, but does not assure the correct balancing of the output.",
        "Therefore, we have used some correcting rules to repair the inconsistencies in the output.",
        "We have applied the following rules: 1.",
        "If the clause segmentation presents more start than end boundaries, we add the end boundaries that are needed to the last word in the sentence (just before the dot).",
        "2.",
        "If the clause segmentation presents more end that start boundaries, we add the start boundaries that are needed to the first word in the sentence.",
        "3.",
        "If the sentence does not start with a start boundary or does not finish with an end",
        "boundary, we add these start and end tags."
      ]
    },
    {
      "heading": "3 Experimental Results",
      "text": [
        "We considered second – order HMM (3 – grams) which were specialized according to the criteria described above, taking as input the tuples of POS and chunk tags associated to each word.",
        "We also tested the system using other criteria.",
        "We chose different input vocabularies: only words, only POS, only chunks, etc.",
        "Moreover, we used different specialization criteria: specializing with chunk tags, partial specialization, etc.",
        "None of these criteria outperformed the results reported in Table 1.",
        "Although Part 1 and Part 2 were tasks that seem to have a similar difficulty, the experimental results show that clause end detection is more difficult than clause start detection.",
        "We think this could be because the relation between POS and clause start is stronger than the relation between POS and clause end marks.",
        "We performed two additional experiments.",
        "First, we combined' the output of Part 1 and Part 2 in order to obtain Part 3 output.",
        "The obtained results on the test set are lower than the results presented in Table 1 (precision= 70.74%; recall= 58.62%; F,3_1= 64.11).",
        "Second, we derived the output for Part 1 and Part 2 from the output obtained in Part 3.",
        "In this case, we obtained worse results for Part 1 (F,3_1= 84.62) but better results for Part 2 (F,3_1= 84.24).",
        "However, these results are not correct for the shared task because we used the information of embedded clauses which is not available for solving the first two parts."
      ]
    },
    {
      "heading": "4 Conclusions",
      "text": [
        "In this work, we have successfully applied a specialized HMM to a clause detection task.",
        "These models have been specialized using different criteria.",
        "The best results for Part 3 were obtained considering POS and clause depth level 'We have used the baseline script provided by the workshop organizers.",
        "development precision recall F,3_1 part 1 89.2 1% 87.7 2% 88.4 6 part 2 78.8 1% 78.5 4% 78.6 8 part 3 70.7 0% 71.3 5% 71.0 3 test precision recall F,3_ 1 part 1 88.1 5% 84.8 8% 86.4 8 part 2 79.6 3% 77.1 7% 78.3 8 part 3 69.6 2% 64.1 7% 66.7 9",
        "and test data set for each part of the shared task.",
        "(F,3_1= 66.79).",
        "Future works would include testing other specialization criteria and studying the way to incorporate the words in the models (Lexicalized HMM).",
        "Moreover, we think that a more detailed study of the problem is needed to assure the correct balancing of the output by including restrictions on the model; for example, modifying the smoothing methods or including linguistic restrictions."
      ]
    },
    {
      "heading": "5 Acknowledgments",
      "text": [
        "This work has been supported by the Spanish research project CICYT TIC2000 – 0664 – C02 – 01"
      ]
    }
  ]
}
