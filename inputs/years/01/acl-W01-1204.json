{
  "info": {
    "authors": [
      "Gideon S. Mann"
    ],
    "book": "Workshop on Open-Domain Question Answering",
    "id": "acl-W01-1204",
    "title": "A Statistical Method for Short Answer Extraction",
    "url": "https://aclweb.org/anthology/W01-1204",
    "year": 2001
  },
  "references": [
    "acl-H01-1069",
    "acl-N01-1005",
    "acl-W01-0701"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a simple, general method for using the Mutual Information (MI) statistic trained on unannotated trivia questions to estimate question class/semantic tag correlation.",
        "This MI method and a variety of question classifiers and semantic taggers are used to build short-answer extractors that show improvement over a hand-built match module using a similar question classifier and semantic tagger."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many of the recent question answering systems integrate statistical NLP/IR tools with a hand-crafted component, a question class/semantic tag (QC/ST) match module (Prager et al., 1999), (Breck et al., 1999).",
        "Hovy et al.",
        "(2001) describes a parsing method for learning QC/ST match.",
        "It-tycheriah et al.",
        "(2001) trains on trivia questions annotated with the semantic tag of the answer to build a Maximum Entropy model which predicts semantic tags given a question.",
        "When the Max-Ent model is used, the estimated probabilities are thrown out and only the most likely tag is returned.",
        "This paper presents a novel method for learning QC/ST correlation from unannotated data.",
        "The method introduced is based on the Mutual Information (MI) statistic (Section 2) and is trained on a trivia question database (Section 3) using a question classifier and semantic tagger.",
        "The MI method is general in that it can be applied to a wide variety of question classifiers and semantic taggers.",
        "In this paper we examine a few different methods questions classifiers and semantic taggers described in Figure 1.",
        "This MI QC/ST match module, along with a question classifier and semantic tagger, can function as a short answer extractor that selects a short answer from a sentence given a question.",
        "Question Classifiers: 1.",
        "U : a simple initial unigram model 2.",
        "UH : a slightly more complicated model that combines initial unigram and wh-phrase heads 3.",
        "Qgrok : a hand-built question typing mechanism (Breck et al., 1999) Semantic Taggers: 1.",
        "NE : Phrag, a HMM Named Entity Tagger (Breck et al., 1999) 2.",
        "WN : WordNet (Miller, 1990)",
        "gers Investigated To simplify the problem, we make the assumption that all answers are strictly one word in length'.",
        "Even so, this task is non-trivial and relevant especially in the case of trivia questions where most of the answers are only one or two words long.",
        "The disparity of performance in the 50-byte and 250-byte TREC-8 Question Answering evaluations (Voorhees, 1999) gives further evidence that extracting a shorter, multi-word answer from a longer, sentence length answer is a task worthy of consideration in its own right.",
        "We empirically test the performance of the short answer extraction on a held-out set of trivia questions and compare with a number of baseline systems including a hand-crafted system that uses a similar question classifier and semantic tagger (Section 4).",
        "2 MI as an Estimator for Question Class/Semantic Tag Correlation A simple approach to building a question answering system would be to (1) collect a huge database of questions and answers, and (2) when a question is asked, look for it in the database and return the 'Recent experiments have used a Base NP tagger to extract full phrases given the best one-word answer but a thorough evaluation has not been completed.",
        "This derivation shows that if we compute (8) we approximate MI(Q,W) given the independence assumptions above.",
        "In effect, we have learned how predictive a QC/ST pair is.",
        "Table 1 gives an example of the type of information learned by the MI model."
      ]
    },
    {
      "heading": "2.3 Estimating the MI Model from Unannotated Data",
      "text": [
        "Estimating the above probabilities can be done with a trivia database that contains a large number of questions and answers.",
        "The method is the following:",
        "• For each question identify the question class.",
        "• Apply the semantic tagger to the trivia database to generate Pr(STJW).",
        "Alternatively, tag a very large corpus to generate high precision priors, and ensure that answers in the training data are tagged at least once.",
        "If high quality priors are available, as outputs from an HMM for example, they also might be able to be used as fractional counts to estimate the probabilities3.",
        "• Estimate",
        "As stated above, this estimation method makes the assumption, expressed in equation (3), that for each question there will only be one question class.",
        "For most current Q/A systems this is the case.",
        "Perhaps future systems will have more sophisticated question classifiers that assign a probability to a number of question classes.",
        "To accomodate increased sophistication, these formulas will change slightly, but the general method may still be applicable.",
        "When this MI method is trained by the above method, it takes into account the actual performance of the semantic tagger on data.",
        "Ittycheriah et al.",
        "(2001) builds a statistical model on annotated data which predicts semantic tag from the question and notes that improvement in this prediction does not necessarily lead to higher performance, since there is a complex interaction between this module and the semantic tagger and answer selector.",
        "One advantage of training on unannotated text using the MI approach is that the correlation between the question class and the performance of the semantic tagger is explicitly modeled."
      ]
    },
    {
      "heading": "3 Trivia Questions",
      "text": [
        "With the advent of the Internet, trivia games are becoming big business.",
        "The general public submits questions, and trivia game companies award prizes to those who correctly answer those questions.",
        "Some of these trivia databases are quite large, reaching nearly two hundred thousand trivia questions (Ford).",
        "In this paper we use two trivia databases as main resources : \"Phishy\" or PH (MacDonald) and \"Triviaspot\" or TS (Trivia Machine Inc.).",
        "PH has approximately 5k questions, each with the correct answer.",
        "TS is larger, but only a small part (11k questions) is currently available to us for research.",
        "Each TS database entry, along with the correct answer, includes three wrong answers and in many cases an \"explanation\".",
        "The explanations in TS vary in content.",
        "Some are, in fact, justifications for the answer (as in Figure 3).",
        "Others provide additional information for those interested in the topic of the question (e.g. \"Leonardo Da Vinci described ideas for contact lenses in 1508.\")",
        "or for those upset at answering wrong (e.g. \"Franklin wore glasses, but didn't invent them.\").",
        "Both",
        "3k questions.",
        "Of course the explanations were quite noisy.",
        "Some, though they included the short answer terms, did not have enough information to conclude that the short answer in fact answered the question.",
        "Many explanations were ungrammatical and many were odd mixtures of sentence fragments and random punctuation.",
        "Since the explanations were collected automatically, in some cases we found multiple explanations for the same question.",
        "The results reported for PH in the next section report scores computed over all question/explanations pairs.",
        "We tested the efficacy of the MI model by examining the performance of this model at selecting a one-word answer from the web explanations for PH and from the explanations provided by TS.",
        "Our experimental method for testing the statistical method was as follows:",
        "1.",
        "Divide the questions into a testing and training set (Table 3).",
        "Tokenize all explanations using a text chunker (Florian and Ngai 2001) and ensure that those in the test set contains an answer.",
        "2.",
        "Run the question classifier and semantic tagger over the training set.",
        "NE-U refers to the first MI system tested, where the semantic tagger = Phrag, and question classifier =",
        "Initial Unigram.",
        "In training, throw out uni-grams that don't occur more then ten times in the data.",
        "Estimate a back-off \"all\" as"
      ]
    },
    {
      "heading": "EQ EwEcorrect P(ST IW)P(W IQ) I:Q E J:W P(STIW)P(WIQ)",
      "text": [
        "Note that the numerator is P(STanswer), and the denominator is P(ST).",
        "3.",
        "Estimate MI(ST, QC) from the training set as described in Section 2.",
        "4.",
        "For each question in the test set run the ques",
        "tion classifier to determine the question class, backing off to \"all\" in the case of unseen un-igrams.",
        "Use the semantic tagger to assign a distribution Pr(STIW) to each word in the explanation.",
        "Rank all non-stop words in the sentence according to :",
        "This process is depicted in Figure 5 below.",
        "5.",
        "Evaluate using two methods : (a) correctness of the top ranked answer (correct) (b) the reciprocal answer rank (rar) as used in TREC, (1/rank of first correct answer)",
        "for the top 5 answers",
        "We compared the performance of this system with a number of baselines.",
        "The most naive method was Random which is the expected performance of a system that picks a word at random within the sentence (excluding stop words).",
        "We also tried Word Order, which ranked words by their position in the sentence (i.e. first word ranked first)."
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": [
        "Vanilla used Qgrok and Phrag as question classifier and semantic tagger respectively and a hand-crafted match module to detect matches.",
        "Qgrok is only slightly more complicated than the initial unigram method.",
        "NE-U is the first MI model, which uses Phrag as a semantic tagger and initial unigrams as a method of question classification.",
        "The improvement in both absolute correct and in absolute reciprocal answer rank achieved by NE over the baseline Vanilla system is surprising.",
        "Both systems use very similar question and semantic taggers, with the largest difference being that NE-U uses a Pr(STIW) distribution, while Vanilla chooses argmaxSTPr(STIW) as computed by Phrag.",
        "The question classification mechanism used in Vanilla is more complex than that used in this system (though this might be a source of problems – see Section 4.2).",
        "Another striking result from these experiments is the impressive performance that is achieved by using Word Order on the TS database.",
        "The fact that this method can do so well illustrates that the explanations in TS are unnatural – most of them start with the correct answer.",
        "This information was not used in the rest of the paper or in any of the models, since it is a property of this specific database of questions, as the results on PH demonstrate.",
        "It used not only the initial sentence unigram, but also the head word of the initial wh-phrase (when it could find one), again modeling these types only when they occured more than ten times in the training corpus.",
        "As a result of this process, it's possible to create a question class that hasn't appeared yet in data.",
        "In these cases, the model backed-off to the initial unigram or to \"all\".",
        "This back-off was not optimal, and future investigations into more effective methods may be profitable.",
        "The results from these comparisons are listed in Table 6.",
        "The performance on each of the test sets is strikingly different.",
        "On PH, the Unigram+Head (UH) method achieves the best performance, while on TS Qgrok does.",
        "One explanation behind these differences might be found in Table 2, which shows that PH has a much higher concentration in fewer initial unigrams (.881) which TS is more varied (.802).",
        "This might explain why Qgrok, which does more complex question classification achieves a bigger benefit in TS than UH.",
        "These results suggest that while simple question classification works reasonably well for simply phrased questions, it degrades with more complicated phrasing."
      ]
    },
    {
      "heading": "4.3 Using Wrong Answers",
      "text": [
        "One final piece of information in some trivia question databases is a set of wrong answers for each question.",
        "In multiple choice trivia questions, typically the correct answers and incorrect answers all could be possible responses to the question or else a contestant would be able to answer the question without any other knowledge.",
        "We assume that one of the main similarities is that all answers have a semantic tag that is highly correlated with the question.",
        "In other words, the tag of the wrong answers should be a possible tag expected by the question, and thus should help estimation of the question class/semantic (QC/ST) tag correlation.",
        "We tested this hypothesis by using these wrong answers to estimate MI(ST,QC), and seeing if indeed they improve performance.",
        "For these experiments, we took the training questions, correct answers (C) and incorrect answers (I) and recom",
        "For these experiments we used only the TS database, since PH did not contain wrong answers.",
        "Table 7 shows an improvement in performance with the use of wrong answers in estimating MI(ST,QC), though not a large one."
      ]
    },
    {
      "heading": "4.4 Probing the Open Domain Nature of Trivia Questions",
      "text": [
        "One important question is to what degree models learned from trivia question databases can be broadly applied to question answering in general.",
        "To begin an answer to this question, we examine how well models learned on one trivia database can be used on another unrelated one.",
        "We built models from PH and TS, exchanged Pr(STIQC) models, and retested.",
        "With this replacement, we expect a degradation solely due to QC/ST correlation differences.",
        "We did not replace the other two models since if one was given a new set of questions and a corpus of sentences, Pr (ST IW) and Pr (ST) could be calculated for that domain without knowing the correlation between questions and answers.",
        "Formally, we defined Pr PH to be probailities estimated from the PH corpus, and Pr TS to be those estimated on TS.",
        "The model we normally test on PH is built by: MIPH(Q,W)Pr PH(STIW)PrPH(STIQC) ST PrPH(ST) Instead in this model, we computed The performance detailed in Table 8 shows that the degradation is minimal when models are shared across two trivia databases.",
        "This result suggests either that the trivia questions, at least to a first approximation, are very similar or that what these models have learned is a general phenomena of question answering.",
        "Which hypothesis is correct is left to future research."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": []
    },
    {
      "heading": "In this paper we examined a component of Q/A",
      "text": [
        "systems which is often overlooked : the component which measures fit between question classes and semantic tags.",
        "We described a novel way to use the mutual information statistic and an unannotated corpus to automatically induce correlations between semantic tags and question classes.",
        "We have shown that wrong answers can help improve performance and that different semantic taggers can be combined to improve performance.",
        "The MI statistic as presented here can be used as \"glue\" to combine a variety of question class/semantic tag components, and as such it is of general usefulness to the Q/A community.",
        "The similarity between trivia databases, shown by cross-training experiments, is another interesting result.",
        "Although it does not prove that trivia questions constitute an open domain, it suggests that trivia questions are at least a self-consistent domain.",
        "We have shown that another component of a Q/A system can be built statistically to yield nice performance when even simple statistics are used.",
        "This prompts the question : what other components can be built statistically?",
        "Already some components are consistently built by statistical methods (semantic taggers, information retrieval engines), yet some remain predominately hand-crafted (e.g. question classifiers – with Ittycheriah et al.",
        "(2001) as an exception), and thus are prime targets for statistical methods.",
        "This paper demonstrated performance on extracting one-word answers, but this method can be extended to extracting multiple words.",
        "We built a system which chooses the base noun phrase containing the highest ranked word, but have not completed evaluation.",
        "Aside from its use in question answering, once the short answer extraction task can be performed with high precision, systems will be able to extract facts from heterogenous text.",
        "This will be an enabling technology which will allow integration with symbolic processing systems to allow for complex natural language understanding."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": []
    }
  ]
}
