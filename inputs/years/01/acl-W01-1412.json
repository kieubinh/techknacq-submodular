{
  "info": {
    "authors": [
      "Kaoru Yamamoto",
      "Yuji Matsumoto",
      "Mihoko Kitamura"
    ],
    "book": "Workshop on Data-Driven Methods in Machine Translation",
    "id": "acl-W01-1412",
    "title": "A Comparative Study on Translation Units for Bilingual Lexicon Extraction",
    "url": "https://aclweb.org/anthology/W01-1412",
    "year": 2001
  },
  "references": [
    "acl-C00-2131",
    "acl-C00-2135",
    "acl-C96-1089",
    "acl-J00-2004",
    "acl-J96-1001",
    "acl-P93-1003",
    "acl-W95-0115",
    "acl-W96-0107"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents ongoing research on automatic extraction of bilingual lexicon from English-Japanese parallel corpora.",
        "The main objective of this paper is to examine various N-gram models of generating translation units for bilingual lexicon extraction.",
        "Three N-gram models, a baseline model (Bound-length N-gram) and two new models (Chunk-bound N-gram and Dependency-linked N-gram) are compared.",
        "An experiment with 10000 English-Japanese parallel sentences shows that Chunk-bound N-gram produces the best result in terms of accuracy (83%) as well as coverage (60%) and it improves approximately by 13% in accuracy and by 5-9% in coverage from the previously proposed baseline model."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Developments in statistical or example-based MT largely rely on the use of bilingual corpora.",
        "Although bilingual corpora are becoming more available, they are still an expensive resource compared with monolingual corpora.",
        "So if one is fortune to have such bilingual corpora at hand, one must seek the maximal exploitation of linguistic knowledge from the corpora.",
        "This paper presents ongoing research on automatic extraction of bilingual lexicon from English-Japanese parallel corpora.",
        "Our approach owes greatly to recent advances in various NLP tools such as part-of-speech taggers, chunkers, and dependency parsers.",
        "All such tools are trained from corpora using statistical methods or machine learning techniques.",
        "The linguistic “clues” obtained from these tools may be prone to some error, but there is much partially reliable information which is usable in the generation of translation units from unannotated bilingual corpora.",
        "Three N-gram models of generating translation units, namely Bound-length N-gram, Chunk-bound N-gram, and Dependency-linked N-gram are compared.",
        "We aim to determine characteristics of translation units that achieve both high accuracy and wide coverage and to identify the limitation of these models.",
        "In the next section, we describe three models used to generate translation units.",
        "Section 3 explains the extraction algorithm of translation pairs.",
        "In Sections 4 and 5, we present our experimental results and analyze the characteristics of each model.",
        "Finally, Section 6 concludes the paper."
      ]
    },
    {
      "heading": "2 Models of Translation Units",
      "text": [
        "The main objective of this paper is to determine suitable translation units for the automatic acquisition of translation pairs.",
        "A word-to-word correspondence is often assumed in the pioneering works, and recently Melamed argues that one-to-one assumption is not restrictive as it may appear in (Melamed, 2000).",
        "However, we question his claim, since the tokenization of words for non-segmented languages such as Japanese is, by nature, ambiguous, and thus his one-to-one assumption is difficult to hold.",
        "We address this ambiguity problem by allowing ’overlaps’ in generation of translation units and obtain single and multi-word correspondences simultaneously.",
        "Previous works that focus on multi-word Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 28 .",
        "correspondences include (Kupiec, 1993) where NP recognizers are used to extract translation units and (Smadja et al., 1996) which uses the XTRACT system to extract collocations.",
        "Moreover, (Kitamura and Matsumoto, 1996) extracts an arbitrary length of word correspondences and (Haruno et al., 1996) identifies collocations through word-level sorting.",
        "In this paper, we compare three N-gram models of translation units, namely Bound-length N-gram, Chunk-bound N-gram, and Dependency-linked N-gram.",
        "Our approach of extracting bilingual lexicon is two-staged.",
        "We first prepare N-grams independently for each language in the parallel corpora and then find corresponding translation pairs from both sets of translation units in a greedy manner.",
        "The essence of our algorithm is that we allow some overlapping translation units to accommodate ambiguity in the first stage.",
        "Once translation pairs are detected during the process, they are decisively selected, and the translation units that overlaps with the found translation pairs are gradually ruled out.",
        "In all three models, translation units of N-gram are built using only content (open-class) words.",
        "This is because functional (closed-class) words such as prepositions alone will usually act as noise and so they are filtered out in advance.",
        "A word is classified as a functional word if it matches one of the following conditions.",
        "(The Penn Treebank part-of-speech tag set (Santorini, 1991) is used for English, whereas the ChaSen part-of-speech tag set (Matsumoto and Asahara, 2001) is used for Japanese.)",
        "part-of-speech(E) “CC”, “CD”, “DT”, “EX”, “FW”, “IN”, “LS”, “MD”, “PDT”, “PR”, “PRS”, “TO”, “WDT”, “WD”, “WP” stemmed-form(E) “be” symbols punctuations and brackets We now illustrate the three models of translation units by referring to the sentence in Figure 1.",
        "Nov. join-Nov"
      ]
    },
    {
      "heading": "Bound-length N-gram",
      "text": [
        "Bound-length N-gram is first proposed in (Kitamura and Matsumoto, 1996).",
        "The translation units generated in this model are word sequences from uni-gram to a given length N. The upper bound for N is fixed to 5 in our experiment.",
        "Figure 2 lists a set of N-grams generated by Bound-length N-gram for the sentence in Figure 1."
      ]
    },
    {
      "heading": "Chunk-bound N-gram",
      "text": [
        "Chunk-bound N-gram assumes prior knowledge of chunk boundaries.",
        "The definition of “chunk” varies from person to person.",
        "In our experiment, the definition for English chunk task complies with the CoNLL-2000 text chunking tasks and the definition for Japanese chunk is based on “bun-setsu” in the Kyoto University Corpus.",
        "Unlike Bound-length N-gram, Chunk-bound N-gram will not extend beyond the chunk boundaries.",
        "N varies depending on the size of the"
      ]
    },
    {
      "heading": "Dependency-linked N-gram",
      "text": [
        "Dependency-linked N-gram assumes prior knowledge of dependency links among chunks.",
        "In fact, Dependency-linked N-gram is an enhanced model of the Chunk-bound model in that, Dependency-linked N-gram extends chunk boundaries via dependency links.",
        "Although dependency links could be extended recursively in a sentence, we limit the use to direct dependency links (i.e. links of immediate mother-daughter relations) only.",
        "Two chunks of dependency linked are concatenated and treated as an extended chunks.",
        "Dependency-linked N-gram generates translation units within the",
        "extended boundaries.",
        "Therefore, translation units generated by Dependency-linked N-gram (Figure 4) become the superset of the units generated by Chunk-bound N-gram (Figure 3).",
        "The distinct characteristics of Dependency-linked N-gram from previous works are twofold.",
        "First, (Yamamoto and Matsumoto, 2000) also uses dependency relations in the generation of translation units.",
        "However, it suffers from data sparseness (and thus low coverage), since the entire chunk is treated as a translation unit, which is too coarse.",
        "Dependency-linked N-gram, on the other hand, uses more fine-grained N-grams as translation units in order to avoid sparseness.",
        "Second, Dependency-linked N-gram includes “flexible” or non-contiguous collocations if dependency links are distant in a sentence.",
        "These collocations cannot be obtained by Bound-length N-gram with any N."
      ]
    },
    {
      "heading": "3 Translation Pair Extraction",
      "text": [
        "We use the same algorithm as (Yamamoto and Matsumoto, 2000) for acquiring translation pairs.",
        "The algorithm proceeds in a greedy manner.",
        "This means that the translation pairs found earlier (i.e. at a higher threshold) in the algorithm are regarded as decisive entries.",
        "The threshold acts as the level of confidence.",
        "Moreover, translation units that partially overlap with the already found translation pairs are filtered out during the algorithm.",
        "The correlation score between translation units and is calculated by the weighted Dice Coefficient defined as:",
        "where and are the numbers of occurrences of and in Japanese and English corpora respectively, and is the number of co-occurrences of and .",
        "We repeat the following until the current threshold reaches the predefined minimum threshold .",
        "1.",
        "For each pair of English unit and Japanese unit appearing at least times, identify the most likely correspondences according to the correlation scores.",
        "For an English pattern , obtain the correspondence candidate set PJ = , , ..., such that sim( , ) for all k. Similarly, obtain the correspondence candidate set PE for a Japanese pattern Register ( , ) as a translation pair if The correlation score of ( , ) is the highest among PJ for and PE for .",
        "2.",
        "Filter out the co-occurrence positions for ,, and their overlapped translation units.",
        "3.",
        "Lower if no more pairs are found."
      ]
    },
    {
      "heading": "4 Experiment and Result",
      "text": []
    },
    {
      "heading": "4.1 Experimental Setting",
      "text": [
        "Data for our experiment is 10000 sentence-aligned corpus from English-Japanese business expressions (Takubo and Hashimoto, 1995).",
        "8000 sentences pairs are used for training and the remaining 2000 sentences are used for evaluation.",
        "Since the data are unannotated, we use NLP tools (part-of-speech taggers, chunkers, and dependency parsers) to estimate linguistic information such as word segmentation, chunk boundaries, and dependency links.",
        "Most tools employ a statistical model (Hidden Markov Model) or machine learning (Support Vector Machines).",
        "Translation units that appear at least twice are considered to be candidates for the translation",
        "pair extraction algorithm described in the previous section.",
        "This implies that translation pairs that co-occur only once will never be found in our algorithm.",
        "We believe this is a reasonable sacrifice to bear considering the statistical nature of our algorithm.",
        "Table 1 shows the number of translation units found in each model.",
        "Note that translation units are counted not by token but by type.",
        "We adjust the threshold of the translation pair extraction algorithm according to the following equation.",
        "The threshold is initially set to 100 and is gradually lowered down until it reaches the minimum threshold 2, described in Section 3.",
        "Furthermore, we experimentally decrement the threshold from 2 to 1 with the remaining uncorrelated sets of translation units, all of which appear at least twice in the corpus.",
        "This means that translation pairs whose correlation score is 1 sim( ,) 0 are attempted to find correspondences2.",
        "The result is evaluated in terms of accuracy and coverage.",
        "Accuracy is the number of correct translation pairs over the extracted translation pairs in the algorithm.",
        "This is calculated by type.",
        "Coverage measures “applicability” of the correct translation pairs for unseen test data.",
        "It is the number of tokens matched by the correct translation pairs over the number of tokens in the unseen test data.",
        "Acuracy and coverage roughly correspond to Melamed’s precision and percent correct respectively (Melamed, 1995).",
        "Accuracy is calculated on the training data (8000 sentences) manually, whereas coverage is calculated on the test data (2000 sentences) automatically."
      ]
    },
    {
      "heading": "4.2 Accuracy",
      "text": [
        "Stepwise accuracy for each model is listed in Table 2, Table 3, and Table 4.",
        "“ ” indicates the threshold, i.e. stages in the algorithm.",
        "“e” is the number of translation pairs found at stage “ ”, and “c” is the number of correct ones found at stage “ ”.",
        "The correctness is judged by an English-Japanese bilingual speaker.",
        "“acc” lists accuracy, the fraction of correct ones over extracted ones by type.",
        "The accumulated results for “e”, “c” and “acc” are indicated by '."
      ]
    },
    {
      "heading": "4.3 Coverage",
      "text": [
        "Stepwise coverage for each model is listed in Table 5, Table 6, and Table 7.",
        "As before, “ ” indicates the threshold.",
        "The brackets indicate language: “E” for English and “J” for Japanese.",
        "“found” is the number of content tokens matched with correct translation pairs.",
        "“ideal” is the upper bound of content tokens that should be found by the algorithm; it is the total number of content tokens in the translation units whose co-occurrence frequency is at least “ ” times in the original parallel corpora.",
        "“cover” lists coverage.",
        "The prefix “i ” is the fraction of found tokens over ideal tokens and the prefix “t ” is the fraction of found tokens over the total number of both content and functional tokens in the data.",
        "For 2000 test parallel sentences, there are 30255 tokens in the English half and 38827 tokens in the Japanese half.",
        "The gap between the number of “ideal” tokens and that of total tokens is due to filtering of functional words in the generation of translation units."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "Of the three models, Chunk-bound N-gram yields the best performance both in accuracy (83%) and in coverage (60%)3.",
        "Compared with the Bound-length N-gram, it achieves approximately 13% improvement in accuracy and 5-9% improvement in coverage at threshold 1.1.",
        "Although Bound-length N-gram generates more translation units than Chunk-bound N-gram, it extracts fewer correct translation pairs (and results in low coverage).",
        "A possible explanation for this phenomenon is that Bound-length N-gram tends to generate too many unnecessary translation units which increase the noise for the 3 We did not evaluate results when = 1.0, since it means threshold 0 , i.e. random pairing.",
        "extraction algorithm.",
        "Dependency-linked N-gram follows a similar transition of accuracy and coverage as Chunk-bound N-gram.",
        "Figure 5 illustrates the Venn diagram of the number of correct translation pairs extracted in each model.",
        "As many as 3439 translation pairs from Dependency-linked N-gram and Chunk-bound N-gram are found in common.",
        "Based on these observation, we could say that dependency links do not contribute significantly.",
        "However, as dependency parsers are still prone to some errors, we will need further investigation with improved dependency parsers.",
        "Table 8 lists the sample correct translation pairs that are unique to each model.",
        "Most translation pairs unique to Chunk-bound N-gram are named entities (NP compounds) and one-to-one correspondence.",
        "This matches our expectation, as translation units in Chunk-bound N-gram are limited within chunk boundaries.",
        "The reason why the other two failed to obtain these translation pairs is probably due to a large number of overlapped translation units generated.",
        "Our extraction algorithm filters out the overlapped entries once the correct pairs are identified, and thus a large number of overlapped translation units sometimes become noise.",
        "Bound-length N-gram and Dependency-linked N-gram include longer pairs, some of which are idiomatic expressions.",
        "Theoretically speaking, translation pairs like “look forward” should be extracted by Dependency-linked N-gram.",
        "A close examination of the data reveals that in some sentences, “look” and “forward” are not recognized as dependency-linked.",
        "These preprocessing failures can be overcome by further improvement of the tools used.",
        "Based on the above analysis, we conclude that chunking boundaries are useful clues in building bilingual seed dictionary as Chunk-bound N-gram has demonstrated high precision and wide coverage.",
        "However, for parallel corpora that include a great deal of domain-specific or idiomatic expressions, partial use of dependency links is desirable.",
        "There is still a remaining problem with our method.",
        "That is how to determine translation pairs which co-occur only once.",
        "One simple approach is to use a machine-readable bilingual dictionary.",
        "However, a more fundamental solution may lie in the partial structural matching of parallel sentences (Watanabe et al., 2000).",
        "We intend to incorporate these techniques to improve the overall coverage."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "This paper reports ongoing research on extracting bilingual lexicon from English-Japanese parallel corpora.",
        "Three models including a previously proposed one in (Kitamura and Matsumoto, 1996) are compared in this paper.",
        "Through preliminary experiments with 10000 bilingual sentences, we obtain that our new models (Chunk-bound N-gram and Dependency-linked N-gram) gain approximately 13% improvement in accuracy and 5-9% improvement in coverage from the baseline model (Bound-length N-gram).",
        "We present quantitative and qualitative analysis of the results in three models.",
        "We conclude that chunk boundaries are useful for building initial bilingual lexicon, and that idiomatic expressions may be partially handled with by dependency links."
      ]
    }
  ]
}
