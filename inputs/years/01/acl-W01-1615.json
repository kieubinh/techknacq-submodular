{
  "info": {
    "authors": [
      "Michael Paul",
      "Eiichiro Sumita"
    ],
    "book": "SIGdial Workshop on Discourse and Dialogue",
    "id": "acl-W01-1615",
    "title": "Integration of Referential Scope Limitations into Japanese Pronoun Resolution",
    "url": "https://aclweb.org/anthology/W01-1615",
    "year": 2001
  },
  "references": [
    "acl-P00-1053",
    "acl-P95-1017",
    "acl-W97-1307",
    "acl-W99-0207"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We propose a practical approach to the anaphora resolution of Japanese pronouns incorporating knowledge about referential scope limitations extracted from an annotated corpus.",
        "A machine learning approach (decision tree) is utilized for the classification of the coreference relation of a given anaphor and antecedent candidates.",
        "The resolution scope of each pronoun is limited according to the relative distance distribution of the training data, resulting in increases in the classification accuracy and analysis speed by causing only a minor decrease in the recall performance.",
        "I Introduction Various approaches have been proposed for anaphora resolution, like rule-based (Mu-rata and Nagao, 1997) and machine learning (Aone and Bennett, 1995) approaches.",
        "These approaches select the most salient candidate from among previously mentioned noun phrases (history).",
        "A problem with these systems is that the resolution costs increase in proportion to the history size.",
        "This problem becomes especially serious in the analysis of long conversations like in dialog understanding or spoken language translation systems.",
        "Most of the candidates in the history, however, are non-referential.",
        "This means that it might not be necessary to analyze the complete history.",
        "This paper focuses on the question of how far do we have to look back in history to find the antecedent of a specific referential expression?",
        "We propose a classification scheme that limits the scope of the resolution analysis according to the distribution of the relative distances between anaphora and antecedents tagged in the training corpus.",
        "Our anaphora resolution' is carried out using a machine learning approach.",
        "A decision tree classifier is trained on the annotated corpus described in Section 2 and determines the coreferential relationship of the given anaphor-candidate pairs (cf.",
        "Section 3).",
        "In the general framework, the decision tree is applied to all of the noun phrases preceding the anaphoric expression in the history.",
        "Its system performance is utilized for a baseline comparison to the practical resolution scheme proposed in this paper (cf.",
        "Section 5).",
        "An investigation into the statistics of the training corpus (cf.",
        "Section 4) reveals quite different characteristics concerning the referential scope of specific anaphoric expressions capable of being exploited not only to decrease the costs of the resolution process, but also to increase the accuracy of the decision tree classifier.",
        "The proposed approach carries out the classification of coreferential relationships and does not select a single candidate as the antecedent of a given anaphor.",
        "However, the decision tree classifier can be seen as a filter that reduces noise, i.e., the elimination of non-referential candidates, for a successive preference selection scheme, e.g., that in (Kameyama, 1997).",
        "For our experiments we use the _ATR-ITL Speech and Language Database (Takezawa et al., 1998) consisting of 500 Japanese spoken-language dialogs annotated with coreferential tags.",
        "The anaphoric expressions used in our experiments (described in Section 5) are limited to pronominal ones referring to nominal antecedents (637 pronouns).",
        "We also include morphosyntactic information like stem forms as well as semantic codes (Ohno and Haman-ishi, 1981) for content words in this corpus.",
        "In the example dialog between a clerk (r) and a customer (c) listed in Figure 1, noun phrases (candidates) are underlined and pronouns (anaphora) are marked with a box.",
        "According to the tagging guidelines used for our corpus, an anaphoric tag refers to the most recent antecedent found in the dialog, but this antecedent might also refer to a previous one.",
        "Therefore, the transitive closure between the anaphora and the first mention of the antecedent in the history defines the set of positive examples, whereas the nominal candidates outside the transitive closure are considered negative examples for coreferential relationships.",
        "In our example, the proper noun (c2)\" 57 71ÿ )�f 71 [Las Vegas]\" is tagged as the antecedent of the pronoun (c3)�� t 6 [there]\".",
        "On the other hand, the anaphor-candidate pair {(c3)\"t t 6 [there]\", (r1)\" _11- – 't 71 _11- – 't",
        "fore forms a negative example.",
        "The difficulty of our task can be verified according to the average number of antecedent candidates, i.e., the sum of positive and negative examples, for a given pronoun.",
        "In our corpus, the average number is 36.7."
      ]
    },
    {
      "heading": "3 Coreference analysis",
      "text": [
        "For the experiments described in Section 5, we utilize a trainable resolution approach using shallow information, i.e., syntactic and semantic word attributes as well as primitive discourse information extracted from a morphological analysis of the input.",
        "To learn the coreferential relationships from our corpus, we use the C5.0 machine learning algorithm (Quinlan, 2000).",
        "The set of attributes employed for the decision tree learning consists of discrete and continuous values extracted from the training corpus.",
        "Two decision tree classes are used to determine whether there is a coreferential relationship (class: corej) or not (class: no-rel)."
      ]
    },
    {
      "heading": "3.1 Training attributes",
      "text": [
        "For the learning of the decision tree we distinguish attributes by the stem forms of content words, their semantic classifications, and their parts-of-speech as illustrated in Table 1.",
        "Moreover, we use information about syntactical markers like particles or sentence conjunctions as well as primitive discourse information about distances and numbers of occurrences for the determination of coreferential relationships.",
        "content word For the resolution of pronouns, we check not only which anaphoric expressions are involved, but also the existence of other content words, like sentence predicates, for the respective input sentences.",
        "[tt,h semantic code For the semantic classification of content words, we use the Ruëgî-Sêën-Jëíen, a three-layered semantic hierarchy (Ohno and Hamanishi, 1981).",
        "The top two layers are utilized; they distinguish 100 classes.",
        "part-of-speech We distinguish 33 parts-of-speech for verbs (e.g., *9ta7, ��a7, VX�a7), nominal expressions (e.g., � � � a7 ,N�, a7), adjectives (e.g.,ÍÏa7 , a7), and functional words (e.g., �a7, I-OÕa7).",
        "functional word In Japanese, the grammatical role of specific content words is marked according to particles succeeding the expression.",
        "We distinguish case particles (e.g., tit, b`, �, 6L ), conjunction particles (e.g., L , -'' ), and adverbial particles (e.g., L b`, '& Z ).",
        "Moreover, the existence of specific conjunctions (e.g., '& b` 6, OD �) and the conjugation form of the sentence predicate, are verified for the determination of coreferential relationships.",
        "discourse We use information concerned with the number of occurrences of specific content words and their distances in the discourse.",
        "For the training of the decision tree, we provide the complete set of attributes described above.",
        "No other coreference indicators are used in our approach, such as the analysis of discourse marker or topic and focus information.",
        "This is because these indicators, which were proposed for previous resolution systems, require a more sophisticated linguistic analysis of the input data."
      ]
    },
    {
      "heading": "3.2 Learning phase",
      "text": [
        "During the iterative analysis of each dialog, anaphoric expressions are identified according to the assigned coreference tags.",
        "Previously mentioned nouns are considered as possible antecedent candidates.",
        "Questions are applied to each anaphor-candidate pair either by matching specified expressions in the respective utterances (discrete values) or by calculating attribute values in the given context (continuous values).",
        "The application of these questions yields a single attribute vector classifying the characteristics of the given reference.",
        "In the case of antecedents, this vector is assigned to the coreference class coref, whereas a separate class no-rel is used for the vectors of non-referential candidates.",
        "The amount of attribute vectors for all of the training samples forms the input of the learning method.",
        "By optimizing the entropy value for each subset, the automatic classifier algorithm produces a decision tree that ranks important attributes higher in the tree in order to achieve an early decision about the classification of the input (Quinlan, 1993)."
      ]
    },
    {
      "heading": "3.3 Application phase",
      "text": [
        "For each anaphoric expression of the test data, a candidate list, i.e., a list of the nominal candidates preceding the anaphoric element in the current discourse, is created.",
        "The decision tree classifier is then successively applied to all of the anaphor-candidate pairs.",
        "(ûóûüý(,þ – cûódñdûðî attribute vect(,r)",
        "Starting with the top node of the decision tree, the question assigned to this node is tested against the input, i.e., the respective anaphor-candidate attribute vector.",
        "Depending on the truth value of the question, the procedure descends to the respective sub-branch.",
        "The verification procedure is continued until a leaf containing the classification result (coref vs. no-rel) is reached (cf.",
        "Figure 2)."
      ]
    },
    {
      "heading": "4 Referential scope",
      "text": [
        "An investigation into the distribution of the relative distances of annotated anaphor-antecedent pairs in the training corpus shows quite different characteristics concerning the referential scope of the respective anaphoric expressions.",
        "Each relative distance is mea",
        "In order to prove the feasibility of our approach we compare the following classification systems:",
        "• general: a single decision tree classifier trained on the input samples of all of the pronouns • specific: decision tree classifiers (one for each pronoun) trained on the input samples of their respective pronoun",
        "Concerning the analysis scope of the above systems we distinguish:",
        "• history: all of the candidates preceding the anaphoric expression • scope: the candidates within a relative distance defined as the coverage (in %) of the distance distribution of the training samples",
        "The performance of the baseline system general+history and each specific classification system (specific+history) are reported in Section 5.2 and utilized for a comparison to those systems with scope limitations, i.e., gen-eral+scope and ìpecëfic�ìcîpe, described in Section 5.3."
      ]
    },
    {
      "heading": "5.1 Criteria",
      "text": [
        "For the evaluation of the system performance we calculate the resolution costs (i.e., the number of anaphor-candidate attribute vectors (cases) to which the decision tree is applied), the accuracy of the decision tree classifier (i.e., the proportion of correct classified objects), and the recall of the classification algorithm (i.e., the proportion of annotated antecedents (target cases) that the system identifies correctly).",
        "Let a denote the number of target cases classified correctly, b the number of non-referential cases classified coreferentially, c the number of target cases classified non-referentially, and d the number of non-referential cases classified correctly as illustrated below.",
        "classification coref no-rel , classified as",
        "The costs, accuracy, and recall of the system are defined as follows:",
        "In the case of a scope limitation all antecedent candidates beyond the limit are not classified by the decision tree.",
        "However, for evaluation purposes, we assign the default class no-rel to all out-of-scope candidates and modify the evaluation criteria as given below.",
        "out-of-scope classification no-rel coref no-rel , classified as e a c coref f b d no-rel annotation Here, e denotes the number of correct antecedents dropped due to the scope limitation and f is the number of out-of-scope candidates classified correctly by the default class no-rel.",
        "In the case of a scope limitation, the evaluation measures of the system are defined as follows:"
      ]
    },
    {
      "heading": "5.2 General framework",
      "text": [
        "In order to be able to judge the performance of the proposed approach, we utilize the general framework, i.e., the validation of all candidates in the history, for the baseline evaluation.",
        "In Table 2 we summarize the accuracy and recall for the open test evaluation of the baseline system (general+history) and the anaphor-specific classification system (specific+history) trained only on samples of the respective anaphoric expressions.",
        "The baseline accuracy is 62.7% and its recall is 82.1%.",
        "However, the application of the specific classification schemes to all candidates in",
        "The misclassification of non-referential candidates is less harmful than the omission of correct antecedents, because there is no recovery from the latter case; non-referential candidates can still be separated from coreferential ones later on using saliency-based selection or similar schemes.",
        "Therefore, we focus on the regression of the system recall for the selection of the optimal system parameter.",
        "In Table 3, we use a threshold (5/) for the maximal recall decrease of each classification system towards its history results that we are willing to accept.",
        "A threshold larger than 5/ causes an increase in the cost reduction, but only a small improvement in the system accuracy that does not warrant a drop in the recall performance anymore.",
        "If we do not apply any scope limitations to the resolution of the anaphora �t 6 [here] and 7c t 6 [there], since no gain in accuracy can be achieved, there is no cost reduction, but we can reduce the recall regression of the overall system performance.",
        "Table 4 shows the selected coverage rates for the limitation of the analysis scope of the specific decision tree classifiers and its performance.",
        "�t h [here]: none c h [there]: none tl [this one]: 97(A -ctl[that one]: 96(A OD [this]: 91(A c OD [that]: 93(A The overall system performance of the classification scheme specifcc+scope is then a cost reduction of 33.2/, an increase of 17.4/ in accuracy, and a drop of 7.1/ in recall."
      ]
    },
    {
      "heading": "6 Related Research",
      "text": [
        "Most of the resolution systems described in literature, focus on the selection of a single history candidate, whereby the recency of candidates is frequently utilized as a saliency measure.",
        "However, only a few systems try to limit the scope of their resolution modules according to the referential characteristics of the respective anaphoric expressions.",
        "(Kameyama, 1997) introduces a locality assumption, which restricts the analysis scope according to the anaphor type.3 However, these limits are selected arbitrarily by the author.",
        "Moreover, the pronominal anaphora contained in the evaluation of thirty newspaper articles (MUC-6 coreference task) consist mainly of 3rd person pronouns with intra-sentential references.",
        "(Ide and Cristea, 2000) analyzes the discourse structure of text taken from the MUC corpus in order to determine domains of referential accessibility for each referential expression.",
        "The search space is reduced by skipping subordinated discourse segments.",
        "However, this approach requires an enhanced structural analysis and does not exploit any upper boundary for the maximal referential scope of the respective anaphoric expressions.",
        "7 Conclusion This paper focuses on the incorporation of referential scope characteristics of anaphora into a corpus-based classification scheme for the resolution of Japanese pronouns.",
        "The result of this incorporation is an increase in the classification accuracy and a decrease in the analysis costs as shown in Table 5.",
        "The accuracy of the baseline system (gen-cral+historg) is 62.7/ and its recall is 82.1/.",
        "The usage of anaphor-specific classifiers (spe-cifcc+history) results in a lower performance of 55.2/ and 76.4/, respectively, because the learned referential characteristics of single anaphora leads to a performance drop when applied to all candidates in history.",
        "3 Unrestricted for proper nouns, 10 sentences for definite noun phrase references, three sentences for pronouns, and only the current sentence for reflexives.",
        "With a scope limitation applied to the general framework (general+scope), we achieve an accuracy of 64.0`/, a recall of 77.2`/, and a cost reduction of 7.7`/.",
        "However, the largest improvement in the overall system performance resulting in an accuracy of 80.1`/, a recall of 75.0`/, and a cost reduction of 33.2`/, is achieved by the specifcc+scope approach, i.e., the utilization of anaphor-specific classification systems in combination with analysis scope limitation according to the coverage of the relative distance distribution of the training data.",
        "Large differences in the feasibility of this approach can be seen for the various anaphoric expressions.",
        "An investigation into the relative distance distribution of annotated anaphor-antecedent pairs in the training corpus revealed an even distribution with a large referential scope for the pronouns �: t 6 [here] and 7c t 6 [there].",
        "Therefore, almost no effect could be achieved through the limitation of the analysis scope, i.e., the validation of the complete history is required in order to achieve a high system performance for the resolution of these anaphoric expressions.",
        "On the other hand, a drastic increase of around 55`/ in accuracy in combination with a high system recall of 90`/ (and more) could be achieved for the demonstratives �: �1 [this one] (accuracy: 86.5`/, recall: 94.9`/) and 7c �1 [that one] (accuracy: 89.8`/, recall: 88.5`/) due to a majority of short-ranged references.",
        "The application of the scope limitation also resulted in a high accuracy of over 75`/ and a small decrease in the recall of 2`/ for the determiners �: OD [this] (accuracy: 74.5`/, recall: 42.0`/) and 7c OD [that] (accuracy: 80.4`/, recall: 74`/).",
        "The system proposed in this paper does not select a single candidate as the antecedent of the anaphoric expression to be resolved, but the high accuracy rates of the system enable a large restriction of the search space, i.e., an identification of around 80`/ of non-referential candidates, for selection schemes using some kinds of preference measures for the determination of the most salient candidate.",
        "A problem with the current system is the large number (around 20`/) of correct antecedents classified as non-referential.",
        "One reason for this misclassification is an insufficient amount of training data.",
        "We used different numbers of training dialogs (50-400 dialogs) for the training of the decision tree.",
        "The steadily increasing performance results implied a lack of training data for the identification of potential candidates.",
        "Currently, we are extending our corpus and we expect that a larger number of coreferential variants will lead to an improvement of the system recall.",
        "Moreover, investigations into the feasibility of our approach for languages other than Japanese, e.g. the Englich MUC corpus, will enable us to compare this approach more precisely towards related research."
      ]
    }
  ]
}
