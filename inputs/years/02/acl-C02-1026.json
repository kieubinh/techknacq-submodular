{
  "info": {
    "authors": [
      "Chin-Yew Lin"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1026",
    "title": "The Effectiveness of Dictionary and Web-Based Answer Reranking",
    "url": "https://aclweb.org/anthology/C02-1026",
    "year": 2002
  },
  "references": [
    "acl-A00-1023",
    "acl-A00-1041",
    "acl-C02-1042",
    "acl-W01-1203"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe an in-depth study of using a dictionary (WordNet) and web search engines (Altavista, MSN, and Google) to boost the performance of an automated question answering system, Webclopedia, in answering definition questions.",
        "The results indicate applying dictionary and web-based answer reranking together increase the performance of Webclopedia on a set of 102 TREC-10 definition questions by 25% in mean reciprocal rank score and 14% in finding answers in the top 5."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In an attempt to further progress in information retrieval research, the Text REtrieval Conference (TREC) sponsored by the National Institute of Standards and Technology (NIST) started a series of large-scale evaluations of domain independent automated question answering systems in TREC-8 (Voorhees 2000) and continued in TREC-9 and TREC-10.",
        "NTCIR (NII-NACSIS Test Collection for IR Systems, TREC’s counterpart in Japan) initiated its question answering evaluation effort, Question Answering Challenge (QAC) in 2001 (Fukumoto et al.",
        "2001).",
        "Research systems participating in TRECs and the coming QAC focused on the problem of answering closed-class questions that have short fact-based answers (“factoids”) from a large collection of text.",
        "These systems bear a similar structure:",
        "(1) Question analysis – identify question",
        "keywords to be submitted to search engines (local or web), recognize question types, and suggest expected answer types.",
        "Although most systems rely on a taxonomy of expected answer types, the number of nodes in the taxonomy varies widely from single digits to a few thousands.",
        "For example, Abney et al.",
        "(2000) used 5; Ittycheriah et al.",
        "(2001), 31; Hovy et al.",
        "(2001), 140; Harabagiu et al.",
        "(2001), 8,797.",
        "These taxonomies were mostly based on named entities and WordNet (Fellbaum 1998).",
        "Special types such definition questions (ex: “What is an atom?”) were added as necessary.",
        "(2) Passage or Sentence retrieval – this aims to provide a text pool of manageable size for extracting candidate answers.",
        "Most top performing systems in TRECs use their own retrieval methods for passages (Brill et al.",
        "2001; Clarke et al.",
        "2001; Harabagiu et al.",
        "2001) or sentences (Hovy et al.",
        "2001).",
        "(3) Candidate answer extraction – extract candidate answers according to answer types.",
        "If the expected answer types are typical named entities, information extraction engines (Bikel et al.",
        "1999, Srihari and Li 2000) are used to extract candidate answers.",
        "Otherwise special answer patterns are used to pinpoint answers.",
        "For example, Soubbotin and Soubbotin (2001) create a set of 6 answer patterns for definition questions.",
        "(4) Answer ranking – assign scores to candidate answers according to their frequency in top ranked passages (Abney et al.",
        "2000; Clarke et al.",
        "2001), similarity to candidate answers extracted from external sources such as the web (Brill et al.",
        "2001; Buchholz 2001) or WordNet (Harabagiu et al.",
        "2001; Hovy et al.",
        "2001), density, distance, or order of question keywords around the",
        "candidates, similarity between the dependency structures of questions and candidate answers (Harabagiu et al.",
        "2001; Hovy et al.",
        "2001; Ittycheriah et al.",
        "2001), and match of expected answer types.",
        "In this paper, we describe an in-depth study of answer reranking for definition questions.",
        "Definition questions account for over 100 (20%) test questions in TREC-10.",
        "They are not named entities that have been the cornerstones of many .",
        "high performance QA systems (Srihari and Li 2000; Harabagiu et al.",
        "2001).",
        "By reranking we mean the following.",
        "Assume a QA system such as Webclopedia (Section 3) provides an initial set of ranked candidate answers from the TREC corpus.",
        "The ranking is based on the IR engine’s passage or sentence match scores.",
        "One can then measure the effectiveness of utilizing resources such as WordNet or the web to rerank the initial results, hoping to achieve better mean reciprocal rank (MRR) and percent of correctness in the top 5 (PTC5).",
        "Answer reranking is often overlooked.",
        "The answer candidates (<= 400 instances per question) generated by Webclopedia from TREC corpus included answers for 83% of 102 definition questions used in this study (the TREC-10 definition questions).",
        "However, Webclopedia ranked only 64% of them in the top 5, giving an MRR score of 45%.",
        "If a perfect answer reranking function had been used, the best achievable MRR would have been 83% (an 84% increase over the original 45%).",
        "Section 2 gives a brief overview of TREC-10.",
        "Section 3 outlines the Webclopedia system.",
        "Section 4 defines definition questions and describes our dictionary and web-based reranking methods.",
        "Section 5 presents experiments and results.",
        "We conclude with lessons learned and future work."
      ]
    },
    {
      "heading": "2 TREC-10 Q&A Track",
      "text": [
        "The main task of the TREC-10 (Voorhees and Harman 2002) QA track required participants to return a ranked list of five answers of no more than 50 bytes long per question that were supported by the TREC-10 QA text collection.",
        "The TREC-10 QA document collection consists of newspaper and newswire articles on TREC disks 1 to 5.",
        "It contains about 3 GB of texts.",
        "Test questions were drawn from filtered MSNSearch and AskJeeves logs.",
        "NIST assessors then sifted 500 questions from the filtered logs as test set.",
        "The questions were closed-class fact-based (“factoid”) questions such as “How far is it from Denver to Aspen?” and “What is an atom?”.",
        "Mean reciprocal rank (MRR) was used as the indicator of system performance.",
        "Each question receives a score as the reciprocal of the rank of the first correct answer in the 5 submitted responses.",
        "No score is given if none of the 5 responses contain a correct answer.",
        "MRR is then computed for a system by taking the mean of the reciprocal ranks of all questions.",
        "Besides MRR score, we are also interested in learning how well a system places a correct answer within the five responses regardless of its rank.",
        "We called this percent of correctness in the top 5 (PCT5).",
        "PCT5 is a precision related metric and indicates the upper bound that a system can achieve if it always places the correct answer as its first response."
      ]
    },
    {
      "heading": "3 Webclopedia: An Automated",
      "text": []
    },
    {
      "heading": "Question Answering System",
      "text": [
        "Webclopedia’s architecture follows the principle outlined in Section 1.",
        "We briefly describe each stage in the following.",
        "Please refer to (Hovy et al.",
        "2002) for more detail.",
        "(1) Question Analysis: We used an in-house parser, CONTEX (Hermjakob 2001), to parse and analyze questions and relied on BBN’s IdentiFinder (Bikel et al., 1999) to provide basic named entity extraction capability.",
        "(2) Document Retrieval/Sentence Ranking: The IR engine MG (Witten et al.",
        "1994) was used to return at least 500 documents using Boolean queries generated from the query formation stage.",
        "However, fewer than 500 documents may be returned when very specific queries are given.",
        "To decrease the amount of text to be processed, the documents were broken into sentences.",
        "Each sentence was scored using a formula that rewards word and phrase overlap with the question and expanded query words.",
        "The ranked sentences were then filtered by expected answer types (ex: dates, metrics, and countries) and fed to the answer extraction module.",
        "(3) Candidate Answer Extraction: We again used CONTEX to parse each of the top N sentences, marked candidate answers by named entities and special answer patterns such as definition patterns, and then started the ranking process.",
        "(4) Answer Ranking: For each candidate answer",
        "several steps of matching were performed.",
        "The matching process considered question keyword overlaps, expected answer types, answer patterns, semantic type, and the correspondence .",
        "of question and answer parse trees.",
        "Scores were given according to the goodness of the matching.",
        "The candidate answers’ scores were compared and ranked.",
        "(5) Answer Reranking, Duplication Removal, and Answer output: For some special question type such as definition questions (e.g., “What is cryogenics?”), we used WordNet glosses or web search results to rerank the answers.",
        "Duplicate answers were removed and only one instance was kept to increase coverage.",
        "The best 5 answers were output.",
        "Answer reranking is the main topic of this paper.",
        "Section 4 presents these methods in detail."
      ]
    },
    {
      "heading": "4 Dictionary and Web-Based Answer Reranking",
      "text": []
    },
    {
      "heading": "4.1 Definition Questions",
      "text": [
        "Compared to other question types, definition questions are special.",
        "They are typically very short and in the form of “What is |are (a |an) X?”, where X is a 1 to 3 words term1, for example: “What is autism?”, “What is spider veins?” and “What is bangers and mash?”.",
        "As we learned from past TREC experience, it was more difficult to find relevant documents for short queries.",
        "As stated earlier, over 20% of questions in TREC-10 were of definition type, which was a reflection of real user queries mined from the web search engine logs (Voorhees 2001).",
        "Several top performing systems in the evaluation treated this type of question as a special category and most of them used definition answer patterns.",
        "The best performing system, InsightSoft-M, (Soubbotin and Soubbotin 2001) used a set of six definition",
        "of patterns with WordNet extension extracted 59 out of 67 definition questions in TREC-8 and TREC-9.",
        "The success stories of these systems indicated that carefully crafted answer patterns were effective in candidate answer extraction.",
        "However, just applying answer patterns blindly might lead to disastrous results, as shown by Hermjakob (2002), since correct and incorrect answers were equally likely to match these patterns.",
        "For example, for the question “What is autism?”, the following answers are found in the TREC-10 corpus using the patterns described by the InsightSoft-M system: ① autismQ, a nourishingA, equivocal ... ② autismQ, the disorder isA, in fact, ... ③ autismQ, the discovery could open new approaches for treating tAhe ... ④ autismQ is a mental disorder that is a “severely incapacitatinAg ... ⑤ autismQ, the inability to communicate with othersA.",
        "Obviously, patterns alone cannot distinguish which one is the best answer.",
        "Some other mechanisms are necessary.",
        "We propose two different methods to solve this problem.",
        "One is a dictionary-based method using WordNet glosses and the other is to go directly to the web and compile web glosses on the fly to help select the best answers.",
        "The effect of combining both methods was also studied.",
        "We describe these two methods in the following sections."
      ]
    },
    {
      "heading": "4.2 Dictionary-Based Reranking",
      "text": [
        "Using a dictionary to look up the definition of a term is the most straightforward solution for answering definition questions.",
        "For example, the definition of autism in the WordNet is: “an abnormal absorption with the self; marked by communication disorders and short attention span and inability to treat others as people”.",
        "However, we need to find a candidate answer string from the TREC-10 corpus that is equivalent to this definition.",
        "By inspection, we find that candidate answers ②, ④, and ⑤ shown in the previous section are more compatible to the definition and ⑤ seems to be the best one.",
        "To automate the decision process, we construct a definition database based on the WordNet noun matching and 27 were from WordNet hypernym expansion.",
        "where ni is the number of times word wi occurring in the WordNet noun glosses and N is total number of occurrences of all noun gloss words in the WordNet.",
        "The goodness of the matching Mwn for each candidate answer is simply the sum of the weight of the matched word stems between its WordNet definition and itself.",
        "For example, candidate answer ⑤ and autism’s WordNet definition have these matches:",
        "reranking score Swn for each candidate answer is its original score multiplied by Mwn.",
        "The final ranking is then sorted according to Swn, duplicate answers are removed, and the top 5 answers are output.",
        "Table 1 shows the top 5 answers returned before and after applying dictionary-based reranking.",
        "It demonstrates that dictionary-based reranking not only pushes the best answer to the first place but also boosts other lower ranked good answers i.e. “a mental disorder” to the second place.",
        "Harabagiu et al.",
        "(2001) also used WordNet to assist in answering definition questions.",
        "However, they took the hypernyms of the term to be defined as the default answers while we used its glosses.",
        "The hypernym of “autism” is “syndrome”.",
        "In this case it would not boost the desired answer to the top but it would instead “validate” “Down’s syndrome” as a good answer.",
        "Further research is needed to investigate the tradeoff between using hypernyms and glosses.",
        "WordNet glosses were incorporated in IBM’s statistical question answering system as definition features (Ittycheriah et al.",
        "2001).",
        "However, they did not report the effectiveness of the features in definition answer extraction.",
        "Out of vocabulary words is the major problem of dictionary-based reranking.",
        "For example, no WordNet entry is found for “e-coli” but searching the term “e-coli” at www.altavista.com and www.google.com yield the following:",
        "• E.coli is a food borne illness.",
        "Learn about prevention, symptoms and risks, detection, ... Risks Detection Recent Outbreaks Resources The term E.coli is an abbreviation for the bacteria Escherichia.",
        "(1st hit, www.altavista.com) • The E.coli Index (part of the WWW Virtual Library) – Description: Guide to information relating to the model organism Escherichia coli.",
        "From the WWW Virtual Library.",
        "(1st hit, www.google.com)",
        "This brings us to the web-based reranking method that we introduce in the next section."
      ]
    },
    {
      "heading": "4.3 Web-Based Reranking",
      "text": [
        "The World Wide Web contains massive amounts of information covering almost any thinkable topic.",
        "The TREC-10 questions are typical instances of queries for which users tend to believe answers can be found from the web.",
        "However, the candidate answers extracted from the web have to find support in the TREC-10 corpus in order to be judged as correct otherwise they will be marked as unsupported.",
        "The search results of “e-coli” from two online search engines indicate that “e-coli” is an abbreviation for the bacteria Escherichia.",
        "However, to automatically identify “e-coli” as “Escherichia” from these two pages is the same QA problem that we set off to resolve.",
        "The only advantage of using the web instead of just the TREC-10 corpus is the assumption that the web contains many more redundant candidate answers due to its huge size.",
        "Compared to",
        "Google’s 2,073,418,204 web pages4, TREC-10 corpus contains only about 979,000 articles.",
        "For a given question, we first query the web, apply answer extraction algorithms over a set of top ranked web pages (usually in the lower hundreds), and then rank candidate answers according to their frequency in the set.",
        "This assumes the more a candidate answer occurs in the set the more likely it is the correct answer.",
        "Clarke et al.",
        "(2001) and Brill et al.",
        "(2001) both applied this principle and achieved good results.",
        "Instead of using Webclopedia to extract candidate answers from the web and then project back to the TREC-10 corpus, we treat the web as a huge dynamic dictionary.",
        "We compile web glosses on the fly for each definition question and apply the same reranking procedure used in the dictionary-based method.",
        "We detail the procedure in the following.",
        "(1) Query a search engine (e.g., Altavista) with the term (e.g., “e-coli”) to be defined.",
        "(2) Download the first R pages (e.g., R = 70).",
        "(3) Extract context word cwi within a window of",
        "W (e.g., W = 10) words centered at the term to be defined from each page.",
        "Closed class words are ignored.",
        "These context words are used as candidate web glosses.",
        "(4) The gloss weight s\"b for each word cwi is computed as follows5:",
        "where ti is the frequency of cwi in the set of context words extracted in (3), N is the total number of training questions, and ni is the number of training questions in which wZ occurs.",
        "(5) The goodness of the matching Mweb",
        "for each candidate answer is simply the sum of the weights of the matched word stems between its web gloss definition and itself.",
        "Only words with gloss weight s\"b ≥ T are used to compute Mweb.",
        "The value of T serves as a cut-off threshold to filter out low confidence words.",
        "(6) The reranking score Sweb for each candidate answer is its original score multiplied by Mweb.",
        "The final ranking is then sorted according to Sweb, duplicate answers are removed, and the top 5 answers are output.",
        "Table 2 shows the top 5 answers returned before and after applying web-based reranking for the question “What is Wimbledon?”.",
        "Google was used as the search engine with T=5, W=10, and R=70."
      ]
    },
    {
      "heading": "5 Experiments and Results",
      "text": [
        "We used a set of 102 definition questions from TREC-10 QA track as our test set.",
        "The performance of Webclopedia without dictionary or web-based answer reranking was used as the baseline.",
        "Webclopedia with dictionary-based answer reranking.",
        "To study the effect of using different search engines, context window sizes, number of top ranked web pages, and web gloss weight cut-off threshold on the performance of web-based answer reranking, we had the following setup:",
        "• Three search engines (E): Altavista (EA), Google (EG), and MSNSearch (EM).",
        "• A run that combined all three search engines’ results (EX).",
        "• Two different context window sizes (W): 5 (W5) and 10 (W10).",
        "• Eleven sets of top ranked web pages (Rx): top 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100.",
        "• Two different gloss weight cut-off thresholds (T): 5 (T5) and 10 (T10).",
        "To investigate the performance of combining dictionary and web-based answer reranking, we ran the above setup again but each question’s reranking score Sweb+wn was the multiplication of",
        "its original score, web-based matching score Mweb, and dictionary-based matching score Mwn.",
        "A total of 354 runs were performed.",
        "Manual evaluation of these 354 runs was not impossible but would be time consuming.",
        "We instead used the answer patterns provided by NIST to score all runs automatically.",
        "Due to space constraint, Table 3 shows the (MRR, PCT5) score pair for 90 runs out of 352 runs.",
        "The other two runs were the baseline run with a score pair of (0.450, 0.637) and the dictionary-based run, (0.535, 0.667).",
        "The best run was the combined dictionary and web-based run using Google as the search engine with 10-word context window, 70 top ranked pages, and a gloss weight cut-off threshold of 5.",
        "Analyzing all runs according to Table 3, we made the following observations.",
        "(1) Dictionary-based reranking improved baseline performance by 19% in MRR and 5% in PCT5 (MRR: 0.535, PCT5: 0.667).",
        "(2) The best web-based reranking (MRR: 0.539, PCT5: 0.676) was achieved with W=10, R=70, and T=5.",
        "It was comparable to the dictionary-based reranking.",
        "(3) Web-based reranking generally improved",
        "results.",
        "Only 6 runs6 (not shown in the table) did worse in their MRR scores than just using Webclopedia alone and these runs concentrated on low ranked page counts of 5 and 10.",
        "6 These were EAT5W5R5 (0.437, 0.598), EAT10W5R5 (0.434, 0.608), EAT10W10R5 (0.437, 0.598), EMT5W5R5 (0.436, 0.608), EMT10W5R5 (0.438, 0.608), and EMT10W10R5 (0.443, 0.618).",
        "(4) Different search engines reached their best performance at different parameter settings.",
        "Overall Google did better.",
        "(5) Combining multiple search engine results (runs designed with X and X+) did not always improve performance.",
        "In some cases, it even degraded system performance (ExT5W10R70: 0.519, 0.637).",
        "(6) Lower web gloss weight cut-off threshold was better at 5.",
        "(7) Longer context window was better at 10 (not shown in the table).",
        "(8) Taking top ranked pages of 50 to 90 pages provided better results.",
        "(9) Combining dictionary and web-based reranking always did better than using the web-based method alone.",
        "(10) Using WordNet and Google together was always better than just using WordNet alone in both MRR and PCT5 (the underlined cells)."
      ]
    },
    {
      "heading": "5.1 Question Difficulty",
      "text": [
        "To investigate the effectiveness of using dictionary and web-based answer reranking on question of different difficulty, we define question difficulty as: d =1− (n / N) , where n is the number of systems participating in TREC-10 that returned answers in top 5 and N is the number of total runs (that is, 67 for TREC-10).",
        "When d = 1 no systems provided an answer in top 5; while d = 0 if all runs provided at least one answer in top 5.",
        "Table 4 shows the improvement of MRR and PCT5 scores at four different question difficulty levels with four different system setups.",
        "The results indicate that using either dictionary or web-based answer reranking improved system performance at all levels.",
        "The best results were achieved when evidence from both resources was used.",
        "However, it also demonstrates the difficulty of improving performance on very hard questions (d>=0.75).",
        "This implies we might need to consider alternative methods to improve the system performance further."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "We described dictionary-based answer reranking using WordNet, web-based answer reranking using three different online search engines, and their evaluations at various parameter settings on a set of 102 TREC-10 definition questions.",
        "We showed that using either approach alone improved MRR score by 19% and PCT5 score by 5% over the baseline.",
        "However, the best performance was achieved when both methods were used together.",
        "In that setting a 25% increase in MRR score and 14% improvement in PCT5 score were obtained.",
        "The difference on the best MRR and PCT5 scores (0.56 vs. 0.73) suggests neither dictionary-based nor web-based will solve the reranking problem completely.",
        "To improve the performance further, we need better ways to compile web glosses and combine them with WordNet glosses.",
        "We also need a better combination function  a statistical model for combining patterns, dictionary, and web scores.",
        "We have started investigating the possibility of applying answer reranking to other question types and exploring specialized web resources."
      ]
    }
  ]
}
