{
  "info": {
    "authors": [
      "Ismael García-Varea",
      "Franz Josef Och",
      "Hermann Ney",
      "Francisco Casacuberta"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1032",
    "title": "Improving Alignment Quality in Statistical Machine Translation Using Context-Dependent Maximum Entropy Models",
    "url": "https://aclweb.org/anthology/C02-1032",
    "year": 2002
  },
  "references": [
    "acl-C00-2163",
    "acl-E99-1010",
    "acl-J93-2003",
    "acl-J96-1002",
    "acl-P01-1027",
    "acl-W00-0707"
  ],
  "sections": [
    {
      "text": [
        "Improving alignment quality in statistical machine translation using context-dependent maximum entropy models* Ismael Garcia Varea Dpto.",
        "de Informatica Univ.",
        "of Castilla-La Mancha Campus Universitario s/n 02071 Albacete, Spain Franz J. Och and Hermann Ney Lehrstuhl fur Inf.",
        "VI RWTH Aachen Ahornstr., 55 D-52056 Aachen, Germany Francisco Casacuberta Dpto.",
        "de Sist.",
        "Inf.",
        "y Comp.",
        "Inst.",
        "Tecn.",
        "de Inf.",
        "(UPV) Avda.",
        "de Los Naranjos, s/n 46071 Valencia, Spain Abstract Typically, statistical alignment models are based on single-word dependencies.",
        "These models do not include contextual information, which can lead to inadequate alignments.",
        "In this paper, we present an approach to include contextual dependencies in the statistical alignment model by using a refined lexicon model.",
        "Unlike previous work, we directly integrate this in the EM algorithm of statistical alignment models.",
        "Experimental results are given for the French-English Canadian Parliament Hansards task and the Verbmobil task.",
        "The evaluation is performed by comparing the obtained alignments with a manually annotated reference"
      ]
    },
    {
      "heading": "alignment. 1 Introduction",
      "text": [
        "The performance of a statistical machine translation system depends directly on the quality of the lexicon and the alignment models used.",
        "So far, most of the statistical machine translation systems are based on single-word alignment models as described in (Brown et al., 1993).",
        "Typically, the lexicon models used in these systems do not include any linguistic or contextual information, which often yields inadequate alignments in pairs of sentences.",
        "In this paper, we present an approach to improve the quality of the word-to-word alignments for this family of statistical translation models by using a maximum entropy (ME) approach.",
        "We define a set of context-dependent ME lexicon models, which is directly integrated into a conventional EM training of statistical alignment models.",
        "Experimental results are given for the French-English Canadian Parliament Hansards corpus and the",
        "Verbmobil task.",
        "The evaluation is performed by comparing the obtained alignment with a manually annotated reference alignment.",
        "The ME approach has been applied in natural language processing and machine translation to a variety of tasks.",
        "Berger et al.",
        "(1996) applies this approach to the so-called IBM Candide system to build context-dependent models, to compute automatic sentence splitting and to improve word reordering in translation.",
        "Garcia-Varea et al.",
        "(2001) use ME models to reduce translation test perplexities and translation errors by means of a rescoring algorithm, which is applied to n-best translation hypotheses.",
        "Foster (2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a ME translation model.",
        "2 Statistical machine translation The goal of the translation process in statistical machine translation can be formulated as follows: A source language string f = f1 = fi ... fJ is to be translated into a target language string e = ei = el ... eI.",
        "Every target string is regarded as a possible translation for the source language string with maximum a-posteriori probability Pr(elf).",
        "According to Bayes' decision rule, we have to choose the target string that maximizes the product of both the target language model Pr(e) and the string translation model Pr(fle).",
        "Alignment models to structure the translation model are introduced in (Brown et al., 1993).",
        "These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition.",
        "The alignment mapping is j – � i = aj from source position j to target position i = aj.",
        "In statistical alignment models, Pr(f, ale), the alignment a is introduced as a hidden variable.",
        "The translation probability Pr(f, ale) can be for one sentence pair (e, f) are calculated: rewritten as follows:",
        "Typically, the probability Pr(fj I fi -1, ai, ef) is approximated to by a lexicon model p(fj l eat ) by dropping the dependencies on fl-1, al-1, ea'-1 andeat+1.",
        "Obviously, this simplification is not true for many natural language phenomena.",
        "The straightforward approach to include more dependencies in the lexicon model would be to add additional dependencies (e.g. p(fj l eat , eat _ 1)) .",
        "This approach would yield a significant data sparseness problem."
      ]
    },
    {
      "heading": "3 EM training of simple alignment",
      "text": [
        "models (review) In this section, we describe the training of the model parameters.",
        "Every model has a specific set of free parameters.",
        "For example, the parameters 0 for Model 4 of (Brown et al., 1993), consist of alignment parameters Palig (') and fertility parameters p fert(•) in addition to the lexicon parameters p(f le):",
        "To train the model parameters 0, we pursue a maximum likelihood approach using a parallel training corpus consisting of S sentence pairs",
        "We do this by applying the EM algorithm (Baum, 1972).",
        "The different models are trained in succession on the same data, where the final parameter values of a simpler model serve as the starting point for a more complex model.",
        "In the E-step, the lexicon parameter counts",
        "In the M-step, we want to compute the lexicon parameters p(f le) that maximize the likelihood of the training corpus.",
        "This results in the following re-estimation (Brown et al., 1993): A f le) _ Es e(.f le; f (s), e(s)) �",
        "Similarly, the alignment and fertility probabilities can be estimated for all other alignment models (Brown et al., 1993).",
        "When bootstrapping from a simpler model to a more complex model, the simpler model is used to weigh the alignments and the counts are accumulated for the parameters of the more complex model."
      ]
    },
    {
      "heading": "4 Maximum entropy modeling",
      "text": [
        "Here, the role of ME is to build a stochastic model that efficiently takes a larger context into account.",
        "In the remainder of the paper, we shall use Pe (f lx) to denote the probability that the ME model (which is associated to e) assigns to f in the context x.",
        "Actually, the context x refers to the dropped dependencies.",
        "Please note that the ME model must be distinguished by the basic lexicon model p(f le).",
        "In the ME approach, we describe all properties that we deem to be useful by so-called feature functions Oe,k(x, f ), k = 1, ... , Ke.",
        "For example, let us suppose we want to model the existence or absence of a specific word e' in ' the context of an English word e, which can be translated by f k. We can express this dependence using the following feature function:",
        "Consequently the k-th feature for word e has associated the pair (elk, fk).",
        "The ME principle suggests that the optimal parametric form of a model p, (f lx) taking into account the feature functions 0,,k, k = 1, ... Ke",
        "Here, ZAe (x) is a normalization factor.",
        "The resulting model has an exponential form with free parameters Ae - {Ae,k, k = 1, ... , Ke}.",
        "The parameter values that maximize the likelihood for a given training corpus can be computed using the so-called CIS algorithm (generalized iterative scaling) (Darroch and Ratcliff, 1972) or its improved version IIS (Pietra et al., 1997; Berger et al., 1996).",
        "It is important to stress that, in principle, we obtain one ME model for each target language word e. To avoid data sparseness problems for rarely seen words, we use only words that have been seen a certain number of times.",
        "5 Contextual information and feature definition Berger et al.",
        "(1996) use a window of 3 words to the left and 3 words to the right of the target word as contextual information.",
        "As in (Garcia-Varea et al., 2001), in addition to a dependence on the words themselves, we also use a dependence on the word classes.",
        "We thereby, improve the generalization of the models and include some semantic and syntactic information.",
        "The word classes are computed automatically using the approach described in (Och, 1999).",
        "Table 1 summarizes the feature functions that we use for a specific pair of aligned words (ei, fj): Category 1 features depend only on the source word fj and the target word ei.",
        "Categories 2 and 3 describe features that also depend on an additional word e' that appears one position to the left or to the right of ei, respectively.",
        "The features of category 4 and 5 depend on an additional target word e' that appears in any position of the context x. Analogous features are defined using the word class associated to each word instead of the word identity.",
        "To reduce the number of features, we perform a threshold-based feature selection.",
        "Any feature that occurs less than T times is not used.",
        "The aim of the feature selection is twofold.",
        "Firstly, we obtain smaller models by using fewer features.",
        "Secondly, we hope to avoid overfitting on the training data.",
        "In addition, we use ME modeling for target words that are seen at least 150 times.",
        "6 Training of refined alignment models"
      ]
    },
    {
      "heading": "6.1 Basic/Dynamic approach",
      "text": [
        "Using a ME lexicon model for a target word e, we have to train the model parameters Ae - {ae,k : k = 1, ... , Ke} instead of the parameters {p(f le)}.",
        "We pursue the following approach.",
        "In the E-step, we perform a refined count collection for the lexicon parameters:",
        "Here, xj,a,j should denote the ME context that surrounds fi and ea,j .",
        "In the M-step, we want to compute the lexicon parameters that maximize the likelihood:",
        "Hence, the refined lexicon counts c(f le, x; e, f) are the weights of the set of training samples (f, e, x) which is used to train the ME model.",
        "The re-estimation of the alignment and fertility probabilities does not change if we use a ME lexicon model.",
        "Thus, we obtain the following steps of each iteration for the EM algorithm:",
        "1.",
        "E-step: • Collect counts for alignment and fertility parameters.",
        "• Collect refined lexicon counts.",
        "2.",
        "M-step: • Re-estimate alignment and fertility parameters.",
        "• Perform CIS training for lexicon parameters."
      ]
    },
    {
      "heading": "6.2 Simplification: Static approach",
      "text": [
        "A simplification of the approach described above can be obtained in the following way:",
        "First, perform a normal training of the EM algorithm.",
        "Then, after the final iteration, perform the ME training only once.",
        "Finally, a new EM training is performed where the lexicon parameters are fixed to the ME lexicon models obtained previously.",
        "This is why we call the basic approach the dynamic approach as well."
      ]
    },
    {
      "heading": "6.3 Avoiding overfitting",
      "text": [
        "ME modeling is maximum likelihood training for exponential models (Berger et al., 1996).",
        "As with other maximum likelihood methods, we have to deal with the problem of overfitting on the training data.",
        "To address this problem, we usually apply smoothing.",
        "We perform a linear interpolation of the baseline lexicon model with the ME lexicon model:",
        "The interpolation parameter A is optimized during training using held-out data.",
        "Hence, we choose the A that maximizes the log-likelihood of the test data.",
        "The value of A obtained in the results presented is 0.5.",
        "Overfitting in the CIS training should also be avoided.",
        "Therefore, we stop the training if the change in training perplexity from one iteration to the next is below a certain threshold.",
        "This threshold is adjusted empirically by taking into account the perplexity on a test corpus."
      ]
    },
    {
      "heading": "6.4 Comparison of the different approaches",
      "text": [
        "In this work, the type of features and contexts used are very similar to those used in (Berger et al., 1996) and (Garcia-Varea et al., 2001).",
        "In these studies, the ME models were obtained after the normal training of the translation models.",
        "These models had no effect on the training of the statistical alignment models itself.",
        "Thus, only a refined lexicon model was obtained, but the fertility and alignment model were not changed.",
        "In this work, the ME models are used and/or trained within the EM training to obtain a better set of parameters.",
        "In this work, all the other models (namely alignment and fertility models) are also indirectly improved thanks to the refined context-dependent lexicon parameters.",
        "The dynamic/basic approach gives us a more feasible parameter estimation than the static approach.",
        "In the dynamic approach, we do not know the Viterbi alignment of a given pair of sentences during EM training.",
        "This leads to the problem of constructing/extracting the corresponding training sample for the defined ME model training.",
        "To solve this problem, the set of all possible alignments for each sentence pair is considered.",
        "Static training has the following advantages: the training time is faster because only one ME training has to be performed; a bootstrapping strategy of refinement could be applied.",
        "Hence, iterate the process of.",
        "\"EM training – � use the Viterbi alignment to train the ME models – � repeat the EM training using the last ME models ...\" �and so on.",
        "On the other hand, dynamic training has the following advantages: a tight and feasible integration is provided; a refined set of ME models is obtained in each iteration of the EM algorithm; the set of pe models considered is refined from one iteration to another in the same way as the parameters of the other models."
      ]
    },
    {
      "heading": "7 Evaluation methodology",
      "text": [
        "We use the same annotation scheme for single-word-based alignments and a corresponding evaluation criterion as described in (Och and Ney, 2000).",
        "The annotation scheme explicitly allows for ambiguous alignments.",
        "The people .",
        "forestiers produits les de distribution la et fabrication la dans exp’erience de anne’es nombreuses de poss‘edent deux tous",
        "performing the annotation are asked to specify two different kinds of alignments: an Sure) alignment, which is used for alignments that are unambiguous and a Possible) alignment, which is used for ambiguous alignments.",
        "The P label is used particularly to align words within idiomatic expressions, free translations, and missing function words (S C_ P).",
        "The reference alignment thus obtained may contain many-to-one and one-to-many relationships.",
        "Figure 1 shows an example of a manually aligned sentence with S and P labels."
      ]
    },
    {
      "heading": "The quality of an alignment A = { (j, aj) I aj >",
      "text": [
        "01 is then computed by appropriately redefined precision and recall measures:",
        "and the following alignment error rate, which is derived from the well known F-measure: AER(S �P; A) = 1 IAI + ISI Thus, a recall error can only occur if a Sure) alignment is not found.",
        "A precision error can only occur if the alignment found is not even P(ossible).",
        "The set of sentence pairs, for which the manual alignment is produced, is randomly selected from the training corpus.",
        "It should be emphasized that all the training is done in a completely unsupervised way, i.e. no manual alignments are used.",
        "From this point of view, there is no need to have a separate test corpus.",
        "8 Experimental results We show results on the Verbmobil task and the Hansards task.",
        "The Verbmobil task is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation.",
        "The task is difficult because it consists of spontaneous speech and the syntactic structures of the sentences are less restricted and highly variable.",
        "The French-English Hansards task consists of the debates in the Canadian Parliament.",
        "This task has a very large vocabulary of more than 100,000 French words.",
        "The corpus statistics are shown in Table 2.",
        "The number of running words and the vocabularies are based on full-form words including the punctuation marks.",
        "We produced smaller training corpora by randomly choosing 500, 8000 and 34000 sentences from the Verbmobil task and 500, 8000 and 128000 sentences from the Hansards task.",
        "To train the context-dependent statistical alignment models, we extended the publicly available toolkit GIZA++ (Och and ✲ey, 2001).",
        "The training of the ME models was carried out using the YASMET toolkit (Och, 2002).",
        "All the results shown in this paper were obtained using the static ME integration.",
        "Table 3 and Table 4 show the alignment quality for different training sample sizes of the Hansards and Verbmobil tasks, respectively.",
        "These tables show the baseline AER for different training schemes and the corresponding values when the integration of the ME is done.",
        "The training scheme is defined in accordance with the number of iterations performed for each model (43 means 3 iterations of Model 4).",
        "In all the experiments, we started applying the ME models in the first iteration of Model 1.",
        "The recall and precision results for the Hansards task with and without ME training are shown in Figures 2 and 3.",
        "We observe that the alignment error rate im",
        "proves when using the context-dependent lexicon models.",
        "For the Verbmobil task, the improvements were smaller than for the Hansards task, which might be due to the fact that the baseline alignment quality was already very good.",
        "It can be seen that greater improvements were obtained for the simpler models.",
        "As expected, ME training plays a more important role when larger sizes of the corpus are used.",
        "For the smallest corpora, the number of training events for the ME models is very low, so it is not possible to disambiguate some translations/alignments for different contexts.",
        "For larger sizes of the corpora, greater improvements are obtained.",
        "Therefore, we expect to obtain better improvements when using even larger corpora.",
        "After observing the common alignment errors, we plan to include more discriminant-ing features that would provide greater improvements.",
        "We also expect improvements by performing a refined modeling of the rare/infrequent words, which are currently not taken into account by the ME models."
      ]
    },
    {
      "heading": "9 Conclusions",
      "text": [
        "In this paper, we show an efficient and straightforward integration of ME context-dependent models within a maximum likelihood training of statistical translation models.",
        "We evaluate the quality of the alignments obtained with this new training scheme comparing the results with the baseline results.",
        "As can",
        "for different corpus sizes.",
        "be seen in Section 8, we obtain better alignment quality using the context-dependent lexicon model.",
        "In the future, we plan to include more features in the ME model, such us dependencies with other source and target words, POS tags and syntactic constituents.",
        "We also plan to design ME alignment and fertility models.",
        "This will allow for an easy integration of more dependencies, such as second-order alignment models without running into the problem of an unmanageable number of alignment parameters.",
        "We have just started to perform experiments for a very distant pair of languages as is Chinese-English with very promising results."
      ]
    }
  ]
}
