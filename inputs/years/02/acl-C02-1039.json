{
  "info": {
    "authors": [
      "Rada Mihalcea"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1039",
    "title": "Instance Based Learning With Automatic Feature Selection Applied to Word Sense Disambiguation",
    "url": "https://aclweb.org/anthology/C02-1039",
    "year": 2002
  },
  "references": [
    "acl-J01-3001",
    "acl-J95-4004",
    "acl-J98-1006",
    "acl-J99-2002",
    "acl-N01-1011",
    "acl-P96-1006",
    "acl-W96-0208",
    "acl-W96-0211"
  ],
  "sections": [
    {
      "text": [
        "Instance Based Learning with Automatic Feature Selection Applied to Word Sense Disambiguation Rada MIHALCEA"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "We describe an algorithm for Word Sense Disambiguation (WSD) that relies on a lazy learner improved with automatic feature selection.",
        "The algorithm was implemented in a system that achieves excellent performance on the set of data released during the SENSEVAL-2 competition.",
        "We present the results obtained and discuss the performance of various features in the context of supervised learning algorithms for WSD."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The task of Word Sense Disambiguation consists in assigning the most appropriate meaning to a polysemous word within a given context.",
        "A large range of applications, including machine translation, knowledge acquisition, information retrieval, information extraction, and others, require knowledge about word meanings, and therefore WSD algorithms represent a necessary step in all these applications.",
        "Starting with SENSEVAL-1 in 1999, WSD has received growing attention from the Natural Language Processing community, and motivates a continuously increasing number of researchers to develop WSD systems and devote time for finding solutions to this challenging problem.",
        "The SENSEVAL' competitions provided a good environment for the development of supervised WSD systems, making freely available large amounts of sense tagged data.",
        "During SENSEVAL-1 in 1999, data for 35 words was made available adding up to about 20,000 examples tagged with respect to the Hector dictionary.",
        "The size of the tagged corpus increased with SENSEVAL-2 in 2001, when 13,000 additional examples were released for 73 polyse",
        "mous words.",
        "This time, the semantic annotations were performed with respect to WordNet.",
        "The experiments and results reported in this paper pertain to the SENSEVAL-2 data.",
        "However, similar experiments were performed on the SENSEVAL-1 data with comparable results.",
        "Most of the efforts in the WSD field were concentrated so far towards supervised learning algorithms, and these are the methods that usually achieve the best performance at the cost of low recall.",
        "Each sense tagged occurrence of a particular word is transformed into a feature vector, suitable for an automatic learning process.",
        "Two main decisions need to be made in the design of such a system: the set of features to be used and the learning algorithm.",
        "Commonly used features include surrounding words and their part of speech(Bruce and Wiebe, 1999), context keywords (Ng and Lee, 1996) or context bigrams (Pedersen, 2001), various syntactic properties (Fellbaum et al., 2001) etc.",
        "As for the learning methodology, a large range of algorithms have been employed, including neural networks (Leacock et al., 1998), decision trees (Pedersen, 2001), decision lists (Yarowsky, 2000), memory based learning (Veenstra et al., 2000) and others.",
        "An experimental comparison of seven learning algorithms used to disambiguate the meaning of the word line is presented in (Mooney, 1996).",
        "We investigate in this paper the use of a lazy learner, namely instance based learning, to solve the semantic ambiguity of words in context.",
        "The main advantage of instance based learners is the fact that they consider every single training example when making a classification decision.",
        "This characteristic proves particularly useful for NLP problems, where training data is usually expensive and exceptions are important.",
        "On the other side, lazy learners, including instance based learners, have the disadvantage of being easily misled by irrelevant features.",
        "In the algorithm described in this paper, this drawback is solved by improving the learner with a scheme for automatic feature selection.",
        "The methodology presented here is integral part of a larger system that has the capability of performing both supervised and open-text WSD (Mihalcea, 2002).",
        "For reasons of clarity and space, we focus in this paper only on the description of the supervised component.",
        "To our knowledge, instance based learning with per word automatic feature selection is a new approach in the WSD field, and we show that it leads to very good results.",
        "Previous work has considered the application of instance based learning with automatic feature selection for the problem of pronoun resolution (Cardie, 1996).",
        "In WSD, the work that is closest to ours was reported by (Bruce and Wiebe, 1999), where decomposable probabilistic models are used in combination with eager Naive Bayes algorithms.",
        "2 Learning with automatic feature selection Learning mechanisms for disambiguating word sense have a long tradition in the WSD field, including a large range of algorithms and feature types.",
        "For our system, we have decided for an instance based algorithm with information gain feature weighting.",
        "The reasons for this decision are threefold.",
        "First, it has been advocated that forgetting exceptions is harmful in language learning applications (Daelemans et al., 1999), and instance based algorithms are known for their property of taking into consideration every single training example when making a classification decision.",
        "Second, this type of algorithms have been successfully used in WSD applications (Veenstra et al., 2000).",
        "Finally, the last reason for our decision was the running time efficiency of these algorithms.",
        "We have initially used the MLC++2 implementation, and later on switched to TiMBL3 (Daelemans et al., 2001).",
        "(MVDM).",
        "The main disadvantage of lazy learners, including instance based learning algorithms, is their high sensitivity to irrelevant features.",
        "Severe degradation in accuracy may result as a consequence of too many such features in the training examples.",
        "It turns out that a critical factor influencing the performance of an instance based learner is the selection of features employed during the learning process.",
        "Our intuition was that different sets of features have different effects depending on the ambiguous word considered.",
        "Rather than creating a general learning model for all polysemous words, a separate feature space is built for each individual word.",
        "Usually, features are weighted using weighting schemes that are based on information gain, gain ratio, chi-squared or other information content measures.",
        "Feature weighting was clearly proven to be an advantageous approach for a large range of applications, including WSD.",
        "Still, weights are computed independently for each feature and therefore this strategy does not always guarantee to provide the best results.",
        "Sometimes it is better to leave features out than assign them even a small weight.",
        "We therefore face the problem of defining a procedure for feature selection that would ideally minimize the disambiguation error.",
        "Variable sets of features have been successfully used in other Artificial intelligence applications.",
        "(Cardie, 1996) proposes a linguistic and cognitive biased approach for relative pronoun resolution.",
        "In (Aha and Bankert, 1994), features are selected using searching algorithms, with increased performance obtained in the problem of cloud types classification.",
        "(Domin-gos, 1997) introduces an algorithm for context sensitive feature selection, with different features selected for each instance in the training set.",
        "Various efficient search algorithms for the detection of optimal feature subsets are proposed in (Moore and Lee, 1994) with successful experiments performed on several synthetic datasets.",
        "In our algorithm, features are automatically selected using a forward search algorithm.",
        "The classic approach is to build word experts via a learning process that determines the values for a preselected set of features.",
        "Instead, we first learn the set of features that would best model the word characteristics, and therefore we are exploiting at maximum the idiosyncratic nature of words.",
        "It is only at a second stage that we actually build the word experts by determining the values for the set of features previously determined.",
        "With this approach, we combine the advantages of instance based learning mechanisms that have the nice property of \"not forgetting exceptions\", with an optimized feature selection scheme."
      ]
    },
    {
      "heading": "3 Main Algorithm",
      "text": [
        "The corpus provided for each ambiguous word is first run through a preprocessing stage, where the text is annotated with lexical tags.",
        "Next, each example is transformed into a feature vector.",
        "Features are selected from a pool of features using an automatic selection algorithm.",
        "The train and test instances will therefore include only the features in the subset determined to be optimal by the selection algorithm.",
        "Notice that training and testing corpora are extracted for each ambiguous word.",
        "This means that examples pertaining to the compound \"dress down\" are separated from the examples for the single word \"dress\"."
      ]
    },
    {
      "heading": "3.1 Preprocessing",
      "text": [
        "During the preprocessing stage, SGML tags are eliminated, the text is tokenized, part of speech tags are assigned using Brill tagger (Brill, 1995), and Named Entities (NE) are identified with an in-house implementation of an NE recognizer.",
        "To identify collocations, we determine sequences of words that form compound concepts defined in WordNet (Miller, 1995).",
        "There are two possible problems with this approach.",
        "The first one concerns subsuming concepts, as in \"United States\" and \"United States of Amer-ica\".",
        "In such cases, priority is given to the longest sequence of words.",
        "The second possible conflict regards overlapping concepts, like the two different compounds \"English Chan-nel\" and \"Channel Tunnel\" found in the text \"English Channel Tunnel\".",
        "Here, we break the tie by keeping the last encountered collocation, with the only reason for this decision being the ease of implementation."
      ]
    },
    {
      "heading": "3.2 Algorithm for Automatic Feature Selection",
      "text": [
        "The algorithm for automatic feature selection is sketched below.",
        "function AutomaticFeatureSelection",
        "• generate a pool of features PF = {Fvl • initialize the set of selected features with the empty set SF={ 01 • extract training and testing corpora for the given target ambiguous word.",
        "• loop: for each feature F E PF - run a 10-fold cross validation on the training set; each example in the training set contains the features in SF and the feature Fv.",
        "- determine the feature Fv leading to the best accuracy - remove Fv from PF and add it to SF • goto loop until no improvements are obtained 4 Features that are good indicators of word sense",
        "There are several features acknowledged as good indicators of word sense, including surrounding words, part of speech tags, collocations, syntactic roles, keywords in contexts.",
        "More recently, other possible features have been investigated: bigrams in context, named entities, semantic relations with other words in context, etc.",
        "We distinguish three types of features:",
        "1.",
        "0-param features, which may be included in the optimal subset or not, without any parameters to set.",
        "For instance, the part of speech of a surrounding word is a zero parameters feature, since any learning example can either contain or omit this feature, without having to indicate a specific value.",
        "2.",
        "I-param features, which, once selected, have one variable parameter that can be set to a specific value (alternatively, this parameter is left with its default value).",
        "As an example, consider the context feature (CF), which includes the words in a surrounding window of length K. Deciding the value for K implicitly means setting one parameter for this feature.",
        "3.",
        "2-param features with two parameters associated.",
        "For example, one can select MX keywords representative for the context of",
        "an ambiguous word, where a keyword is defined as a word that occurs at least MN times.",
        "Therefore, two parameters have to be set for this feature, MX and MN.",
        "All features that have been considered so far are presented below.",
        "They form the pool of features PF from which features are selected using the algorithm described in Section 3.2.",
        "In the following, the ambiguous word is denoted with AW.",
        "New features can be easily added to the pool, with no changes required in the main algorithm.",
        "The system was initially tested with the SENSEVAL-1 data, and additional features were considered at that time to help towards performance.",
        "We decided not to use them in the current experiments, mainly for time considerations, since parsing is a highly computational intensive task.",
        "PPT Parse path (1-param) Maximum K parse components found on the path to the top of the parse tree (sentence top).",
        "Notation: PPT[=K], default=10.",
        "For instance, the value of this feature for the word school, given a parse tree (S (NP (JJ big) (NN house))), is NN, NP, S. SPC Same parse phrase components (1-param) Maximum K parse components found in the same phrase as AW.",
        "Notation: SPC[=K], default=J.",
        "For the example above, this feature would be set to JJ, NN."
      ]
    },
    {
      "heading": "5 Results on SENSEVAL-2 Data",
      "text": [
        "The overall performance of the system evaluated on the test words released during the SENSEVAL-2 English lexical sample task is 63.8% for fine-grained scoring (71.2% for coarse-grained scoring).",
        "These results are comparable with the best performing systems participating in the competition.",
        "Table 1 presents the results obtained for the lexical sample task, for 73 ambiguous words, including 29 nouns, 15 adjectives and 29 verbs.",
        "For each word, the table shows: number of examples in the training and test sets; features automatically selected as a result of the algorithm in Section 3.2; 10-fold cross validation precision obtained on the training data with the selected features; the precision for fine-grained and coarse-grained scoring when all features in PF are considered (i.e. no per word feature selection is performed); finally, we show the precision for fine-grained and coarse-grained scoring computed on the test data when features are automatically selected on an individual word basis.",
        "For the I-param and 2-param features, there",
        "is a range of values allowed for their parameters.",
        "This range was empirically set to [1-5] for the I-param features, respectively [1-10] for the 2-param features.",
        "It means that, for instance, CF can be set to CF=I, CF=2, CF=3, CF=4 or CF=5.",
        "The selection of the best value is per",
        "on training data; fine-grained and coarse-grained on test data using (2) all features; (3) per word feature selection.",
        "formed using the same algorithm.",
        "As mentioned earlier, collocations are identified since the preprocessing stage and the learning process is applied separately on each word.",
        "Therefore, the compound \"call for\" has training and test data different from the verb \"call\", and consequently features are selected in a distinct process.",
        "Due to space limitations, Table 1 shows the features selected only for single words.",
        "When no training data is provided (as it was the case with the SENSEVAL-2 verb \"keep go-ing\"), the first sense is applied by default.",
        "Also, when the training set size is smaller than 15 examples, the automatic feature selection algorithm is not invoked, instead a default set of features is used (CW CP CF=1 COL=1)."
      ]
    },
    {
      "heading": "5.1 Discussion",
      "text": [
        "Table 2 lists the number of times each feature was used in the semantic disambiguation of nouns, verbs and adjectives.",
        "The most often used features turn out to be CW, CP, CF and COL, which are also the features most frequently mentioned in the literature.",
        "Almost all words took advantage of the current part of speech (CP) feature.",
        "This is in agreement with (Stevenson and Wilks, 2001), who have emphasize the major role played by part of speech in WSD.",
        "It is interesting to observe that in terms of words in context, bigrams seem to be more effective than simple keywords.",
        "Also, the best setting for the CF feature was found to be one or two words window.",
        "In terms of average number of features, the semantic disambiguation of nouns requires the smallest number of features (3.7), followed by adjectives (4.4) and verbs (4.5).",
        "These statistics are not yet conclusive, since they are computed for a small number of words, but they are indicative for the complexity of the task for various parts of speech.",
        "Further investigations and larger amounts of data will eventually confirm this preliminary conclusion.",
        "The overall performance of the system when the module for per word feature selection is disabled and all features in PF are employed is 59.8% (68.1%).",
        "The increase in error rate is therefore about 11% with respect to the case when per word feature selection is employed.",
        "We also performed an experiment where the feature selection algorithm consists in finding features that perform best over all 73 words.",
        "The set of feature determined with this simplified approach is \"CW CP CF=1 COL= I\".",
        "The overall performance when this constant set of features is employed is 59.6% (67.4%).",
        "Again, the per word feature selection is proved to produce better results.",
        "Additionally, there were several interesting cases encountered in the SENSEVAL data, justifying our approach of using automatic feature selection.",
        "The influence of a feature greatly depends on the target word: a feature can increase the precision for a word, while making things worse for another word.",
        "For instance, a word such as free does not benefit from the SK feature, whereas colourless gains almost 7% in precision when this feature is used.",
        "Another interesting example is constituted by the noun chair, disambiguated with high precision by simply using the current word (CW) feature.",
        "This is explained by the fact that the most frequent senses are Chair meaning person and chair meaning furniture, and therefore the distinction between lower and upper case spellings makes the distinction among the different meanings of this word.",
        "The noun detention has the same precision computed during several 10-fold cross validation runs, independent on the feature or combination of features used.",
        "This is because one of its two senses occurs in 97% of the examples, and hence it statistically dominates the other sense.",
        "There were several other interesting cases, including the adjective local with a 20% gain in",
        "precision by simply using the feature NA, the word faithful best disambiguated with the CW feature, and others."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "Instance based learning with automatic feature selection is a new approach in the WSD field.",
        "The algorithm was implemented in a system that achieves excellent performance on the data released during the SENSEVAL-2 English lexical task.",
        "The feature selection process is completely automated and it practically creates a classifier tailored to the behaviour of each specific word."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The author would like to thank the anonymous reviewers for their helpful suggestions and constructive comments, which helped improving the quality of this manuscript."
      ]
    }
  ]
}
