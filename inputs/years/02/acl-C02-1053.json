{
  "info": {
    "authors": [
      "Tsutomu Hirao",
      "Hideki Isozaki",
      "Eisaku Maeda",
      "Yuji Matsumoto"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1053",
    "title": "Extracting Important Sentences With Support Vector Machines",
    "url": "https://aclweb.org/anthology/C02-1053",
    "year": 2002
  },
  "references": [
    "acl-C00-2167",
    "acl-C96-2166",
    "acl-N01-1025",
    "acl-P01-1041",
    "acl-P98-1009",
    "acl-W00-1303"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Extracting sentences that contain important information from a document is a form of text summarization.",
        "The technique is the key to the automatic generation of summaries similar to those written by humans.",
        "To achieve such extraction, it is important to be able to integrate heterogeneous pieces of information.",
        "One approach, parameter tuning by machine learning, has been attracting a lot of attention.",
        "This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs).",
        "To confirm the method’s performance, we conduct experiments that compare our method to three existing methods.",
        "Results on the Text Summarization Challenge (TSC) corpus show that our method offers the highest accuracy.",
        "Moreover, we clarify the different features effective for extracting different document genres."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Extracting important sentences means extracting from a document only those sentences that have important information.",
        "Since some sentences are lost, the result may lack coherence, but important sentence extraction is one of the basic technologies for generating summaries that are useful for humans to browse.",
        "Therefore, this technique plays an important role in automatic text summarization.",
        "Many researchers have been studied important sentence extraction since the late 1950’s (Luhn, 1958).",
        "Conventional methods focus on sentence features and define significance scores.",
        "The features include key words, sentence position, and certain linguistic clues.",
        "Edmundson (1969) and Nobata et al.",
        "(2001) have proposed scoring functions to integrate heterogeneous features.",
        "However, we can not tune the parameter values by hand when the number of features is large.",
        "When a large quantity of training data is available, tuning can be effectively realized by machine learning.",
        "In recent years, machine learning has attracted attention in the field of automatic text summarization.",
        "Aone et al.",
        "(1998) and Kupiec et al.",
        "(1995) employed Bayesian classifiers, Mani et al.",
        "(1998), Nomoto et al.",
        "(1997), Lin (1999), and Okumura et al.",
        "(1999) used decision tree learning.",
        "However, most machine learning methods overfit the training data when many features are given.",
        "Therefore, we need to select features carefully.",
        "Support Vector Machines (SVMs) (Vapnik, 1995) is robust even when the number of features is large.",
        "Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000).",
        "In this paper, we present an important sentence extraction technique based on SVMs.",
        "We verified the technique against the Text Summarization Challenge (TSC) (Fukushima and Okumura, 2001) corpus."
      ]
    },
    {
      "heading": "2 Important Sentence Extraction based on Support Vector Machines",
      "text": []
    },
    {
      "heading": "2.1 Support Vector Machines (SVMs)",
      "text": [
        "SVM is a supervised learning algorithm for 2 class problems.",
        "Training data is given by (xi, yi), ··· , (xu, yu), xj ∈ Rn, yj ∈ {+1, −1}.",
        "Here, xj is a feature vector of the j-th sample; yj is its class label, positive(+1) or negative(−1).",
        "SVM separates positive and negative examples by a hyperplane defined by",
        "In this paper, we use polynomial kernel functions that have been very effective when applied to other tasks, such as natural language processing (Joachims, 1998; Kudo and Matsumoto, 2001; Kudo and Matsumoto, 2000):",
        "where “·” represents the inner product.",
        "In general, such a hyperplane is not unique.",
        "Figure 1 shows a linearly separable case.",
        "The SVM determines the optimal hyperplane by maximizing the margin.",
        "A margin is the distance between negative examples and positive examples.",
        "Since training data is not necessarily linearly separable, slack variables (ξj) are introduced for all xj.",
        "These ξj incur misclassification error, and should satisfy the following inequalities:",
        "Under these constraints, the following objective function is to be minimized.",
        "The first term in (3) corresponds to the size of the margin and the second term represents misclassification.",
        "By solving a quadratic programming problem, the decision function f (x) = sgn(g(x)) can be derived where",
        "The decision function depends on only support vectors (xi).",
        "Training examples, except for support vectors, have no influence on the decision function.",
        "Non-linear decision surfaces can be realized by replacing the inner product of (4) with a kernel function K(x · xi) :"
      ]
    },
    {
      "heading": "2.2 Sentence Ranking by using Support Vector Machines",
      "text": [
        "Important sentence extraction can be regarded as a two-class problem: important or unimportant.",
        "However, the proportion of important sentences in training data will differ from that in the test data.",
        "The number of important sentences in a document is determined by a summarization rate that is given at runtime.",
        "A simple solution for this problem is to rank sentences in a document.",
        "We use g(x) the distance from the hyperplane to x to rank the sentences."
      ]
    },
    {
      "heading": "2.3 Features",
      "text": [
        "We define the boolean features discussed below that are associated with sentence Si by taking past studies into account (Zechner, 1996; Nobata et al., 2001; Hirao et al., 2001; Nomoto and Matsumoto, 1997).",
        "We use 410 boolean variables for each Si.",
        "Where x = (x[1], · · ·, x[410]).",
        "A real-valued feature normalized between 0 and 1 is represented by 10 boolean variables.",
        "Each variable corresponds to an internal [i/10,(i + 1)/10) where i = 0 to 9.",
        "For example, Posd = 0.75 is represented by “0000000100” because 0.75 belongs to [7/10,8/10)."
      ]
    },
    {
      "heading": "Position of sentences",
      "text": [
        "We define three feature functions for the position of Si.",
        "First, Lead is a boolean that corresponds to the output of the lead-based method described below' .",
        "Second, Posd is Si’s position in a document.",
        "Third, Posp is Si’s position in a paragraph.",
        "The first sentence obtains the highest score, the last obtains the lowest score:",
        "Here, |D(Si) |is the number of characters in the document D(Si) that contains Si; BD(Si) is the number of characters before Si in D(Si); |P(Si) |is the number of characters of the paragraph P(Si) that contains Si, and BP(Si) is the number of characters before Si in the paragraph.",
        "Length of sentences We define a feature function that addresses the length of sentence as",
        "Here, |Si|is the number of characters of sentence Si, and maxSz∈D |Sz |is the maximum number of characters in a sentence that belongs to D(Si).",
        "In addition, the length of a previous sentence Len−1(Si) = Len(Si−1) and the length of a next sentence Len+1(Si) = Len(Si+1) are also features of sentence Si.",
        "Weight of sentences We defined the feature function that weights sentences based on frequency-based word weighting as",
        "Here, Wf (Si) is the summention of weighting w(t, D(Si)) of words that appear in a sentence.",
        "t f (t, Si) is term frequency of t in Si.",
        "We used only nouns.",
        "In addition, we define word weight w(t, D(Si)) based on a specific field (Hara et al., 1997):",
        "Here, T is the number of sentence in a document, and Vz is the number of words in sentence Sz ∈ D(Si) (repetitions are ignored).",
        "Also, εz is a boolean value: that is 1 when t appears inSz.",
        "The first term of the equation above is the weighting of a word in a specific field.",
        "The second term is the occurrence probability of word t. We set parameters α and β as 0.8, 0.2, respectively.",
        "The weight of a previous sentence Wf−1(Si)= Wf (Si−1), and the weight of a next sentence Wf+1(Si)=Wf (Si+1) are also features of sentence Si."
      ]
    },
    {
      "heading": "Density of key words",
      "text": [
        "We define the feature function Den(Si) that represents density of key words in a sentence by using Hanning Window function (fH(k, m)):",
        "where fH(k, m) is given by",
        "The key words (KW) are the top 30% of words in a document according to w(t, D(Si)).",
        "Also, m is the center position of the window, Win = |Si|/2.",
        "In addition, a(k, Si) is defined as follows: w(t, D) Where a word t (∈ KW) begins at k 0 k is not the beginning position of a word in KW.",
        "Named Entities x[r]=1 (1≤r≤8) indicates that a certain Named Entity class appears in Si.",
        "The number of Named Entity classes is 8 (Sekine and Eriguchi, 2000), e.g., PERSON, LOCATION.",
        "We use Isozaki’s NE recognizer (Isozaki, 2001)."
      ]
    },
    {
      "heading": "Conjunctions",
      "text": [
        "x[r]=1 (9≤r≤61) if and only if a certain conjunction is used in the sentence.",
        "The number of conjunctions is 53.",
        ".Functional words x[r]=1 (62≤r≤234) if and only if a certain functional word such as ga, ha, and ta is used in the sentence.",
        "The number of functional words is 173."
      ]
    },
    {
      "heading": "Part of speech",
      "text": [
        "x[r]=1 (235≤r≤300) if and only if a certain part of speech such as “Noun-jiritsu” and “Verb-jiritsu” is used in the sentence.",
        "The number of part of speech is 66."
      ]
    },
    {
      "heading": "Semantical depth of nouns",
      "text": [
        "x[r]=1 (301≤r≤311) if and only if Si contains a noun at a certain semantical depth according to a Japanese lexicon, Goi-Taikei (Ikehara et al., 1997).",
        "The number of depth levels is 11.",
        "For instance, Semdep=2 means that a noun in Si belongs to the second depth level."
      ]
    },
    {
      "heading": "Document genre",
      "text": [
        "x[r]=1 (312≤r≤315) if and only if the document belongs to a certain genre.",
        "The genre is explicitly written in the header of each document.",
        "The number of genres is four: General, National, Editorial, and Commentary."
      ]
    },
    {
      "heading": "Symbols",
      "text": [
        "x[r]=1 (r=316) if and only if sentence includes a certain symbol (for example: •,�,o)."
      ]
    },
    {
      "heading": "Conversation",
      "text": [
        "x[r]=1 (r=317) if and only if Si includes a conversation style expression.",
        "Assertive expressions x[r]=1 (r=318) if and only if Si includes an assertive expression."
      ]
    },
    {
      "heading": "3 Experimental settings",
      "text": []
    },
    {
      "heading": "3.1 Corpus",
      "text": [
        "We used the data set of TSC (Fukushima and Okumura, 2001) summarization collection for our evaluation.",
        "TSC was established as a subtask of NTCIR-2 (NII-NACSIS Test Collection for IR Systems).",
        "The corpus consists of 180 Japanese documents2 from the Mainichi Newspapers of 1994, 1995, and 1998.",
        "In each document, important sentences were manually extracted at summarization rates of 10%, 30%, and 50%.",
        "Note that the summarization rates depend on the number of sentences in a document not the number of characters.",
        "Table 1 shows the statistics."
      ]
    },
    {
      "heading": "3.2 Evaluated methods",
      "text": [
        "We compared four methods: decision tree learning, boosting, lead, and SVM.",
        "At each summarization rate, we trained classifiers and classified test documents."
      ]
    },
    {
      "heading": "Decision tree learning method",
      "text": [
        "We used C4.5 (Quinlan, 1993) for our experiments with the default settings.",
        "We used the 2 Each document is presented in SGML style with sentence and paragraph separators attached.",
        "features described in section 2.",
        "Sentences were ranked according to their certainty factors given by C4.5."
      ]
    },
    {
      "heading": "Boosting method",
      "text": [
        "We used C5.0, which applies boosting to decision tree learning.",
        "The number of rounds was set to 10.",
        "Sentences were ranked according to their certainty factors given by C5.0."
      ]
    },
    {
      "heading": "Lead-based method",
      "text": [
        "The first N sentences of a document were selected.",
        "N was determined according to the summarization rates.",
        "SVM method This is our method as outlined in section 2.",
        "We used the second-order polynomial kernel, and set C (in equation (3)) as 0.0001.",
        "We used TinySVM3 ."
      ]
    },
    {
      "heading": "3.3 Measures for evaluation",
      "text": [
        "In the TSC corpus, the number of sentences to be extracted was explicitly given by the TSC committee.",
        "When we extract sentences according to that number, Precision, Recall, and F-measure become the same value.",
        "We call this value Accuracy.",
        "Accuracy is defined as follows:",
        "where a is the specified number of important sentences, and b is the number of true important sentences that were contained in system’s output."
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "Table 2 shows the results of fivefold cross validation by using all 180 documents.",
        "For all summarization rates and all genres, SVM achieved the highest accuracy, the lead-based method the lowest.",
        "Let the null hypothesis be “There are no differences among the scores of the four methods”.",
        "We tested this null hypothesis at a significance level of 1% by using Tukey’s method.",
        "Although the SVM’s performance was best, the differences were not statistically significant at 10%.",
        "At 30% and 50%, SVM performed better than the other methods with a statistical significance."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "Table 2 shows that Editorial and Commentary are more difficult than the other genres.",
        "We can consider two reasons for the poor scores of Editorial and Commentary:",
        "• These genres have no feature useful for discrimination.",
        "• Non-standard features are useful in these genres.",
        "Accordingly, we conduct an experiment to clarify genre dependency4 .",
        "1 Extract 36 documents at random from genre i for training.",
        "2 Extract 4 documents at random from genre j for test.",
        "3 Repeat this 10 times for all combinations of (i, j).",
        "Table 3 shows that the result implies that non-standard features are useful in Editorial and Commentary documents.",
        "Now, we examine effective features in each genre.",
        "Since we used the second order polynomial kernel, we can expand g(x) as follows:",
        "where E is the number of support vectors, and wi equals λiyi.",
        "We can rewrite it as follows when all vectors are boolean:",
        "where",
        "Therefore, W1 [k] indicates the significance of an individual feature and W2 [h, k] indicates the significance of a feature pair.",
        "When |W1 [k] |or |W2 [h, k] |was large, the feature or the feature pair had a strong influence on the optimal hyperplane.",
        "Table 4 shows some of the effective features that had large weights W1 [k], W2 [h, k] for each genre.",
        "Effective features common to three genres at three rates were sentence positions.",
        "Since National has a typical newspaper style, the beginning of the document was important.",
        "Moreover, “ga” and “ta” were important.",
        "These functional words are used when a new event is introduced.",
        "In Editorial and Commentary, the end of a paragraph and that of a document were important.",
        "The reason for this result is that subtopic or main topic conclusions are common in those positions.",
        "This implies that National has a different text structure from Editorial and Commentary.",
        "Moreover, in Editorial, “de” and sentence weight was important.",
        "In Commentary, semantically shallow words, sentence weight and the length of a next sentence were important.",
        "In short, we confirmed that the feature(s) effective for discriminating a genre differ with the genre."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "This paper presented a SVM-based important sentence extraction technique.",
        "Comparisons were made using the lead-based method, decision tree learning method, and boosting method with the summarization rates of 10%, 30%, and 50%.",
        "The experimental results show that the SVM-based method outperforms the other methods at all summarization rates.",
        "Moreover, we clarified the effective features for three genres, and showed that the important features vary with the genre.",
        "In our future work, we would like to apply our method to trainable Question Answering System SAIQA-II developed in our group."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "We would like to thank all the members of the Knowledge Processing Research Group for valuable comments and discussions."
      ]
    }
  ]
}
