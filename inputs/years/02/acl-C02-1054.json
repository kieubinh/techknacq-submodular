{
  "info": {
    "authors": [
      "Hideki Isozaki",
      "Hideto Kazawa"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1054",
    "title": "Efficient Support Vector Classifiers for Named Entity Recognition",
    "url": "https://aclweb.org/anthology/C02-1054",
    "year": 2002
  },
  "references": [
    "acl-C00-2167",
    "acl-N01-1025",
    "acl-P01-1041",
    "acl-W98-1120"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date.",
        "It is a key technology of Information Extraction and Open-Domain Question Answering.",
        "First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems.",
        "However, off-the-shelf SVM classifiers are too inefficient for this task.",
        "Therefore, we present a method that makes the system substantially faster.",
        "This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging.",
        "We also present an SVM-based feature selection method and an efficient training method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Named Entity (NE) recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person, organization, and date.",
        "It is a key technology of Information Extraction and Open-Domain Question Answering (Voorhees and Harman, 2000).",
        "We are building a trainable Open-Domain Question Answering System called SAIQA-II.",
        "In this paper, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems.",
        "SVMs have given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001).",
        "However, it turned out that off-the-shelf SVM classifiers are too inefficient for NE recognition.",
        "The recognizer runs at a rate of only 85 bytes/sec on an Athlon 1.3 GHz Linux PC, while rule-based systems (e.g., Isozaki, (2001)) can process several kilobytes in a second.",
        "The major reason is the inefficiency of SVM classifiers.",
        "There are other reports on the slowness of SVM classifiers.",
        "Another SVM-based NE recognizer (Yamada and Matsumoto, 2001) is 0.8 sentences/sec on a Pentium III 933 MHz PC.",
        "An SVM-based part-of-speech (POS) tagger (Nakagawa et al., 2001) is 20 tokens/sec on an Alpha 21164A 500 MHz processor.",
        "It is difficult to use such slow systems in practical applications.",
        "In this paper, we present a method that makes the NE system substantially faster.",
        "This method can also be applied to other tasks in natural language processing such as chunking and POS tagging.",
        "Another problem with SVMs is its incomprehensibility.",
        "It is not clear which features are important or how they work.",
        "The above method is also useful for finding useless features.",
        "We also mention a method to reduce training time."
      ]
    },
    {
      "heading": "1.1 Support Vector Machines",
      "text": [
        "Suppose we have a set of training data for a two-class problem: (xl, gi), ... , (xN,YN), where xi(E RD) is a feature vector of the i-th sample in the training data and gi E {+1, -1} is the label for the sample.",
        "The goal is to find a decision function that accurately predicts g for unseen x.",
        "A non-linear SVM classifier gives a decision function",
        "Here, f (x) _ +1 means x is a member of a certain class and f (x) _ – 1 means x is not a member.",
        "zi s are called support vectors and are representatives of training examples.",
        "a is the number of support vectors.",
        "Therefore, computational complexity of g(x) is proportional to e. Support vectors and other constants are determined by solving a certain quadratic programming problem.",
        "K(x, z) is a kernel that implicitly maps vectors into a higher dimensional space.",
        "Typical kernels use dot products: K(x, z) = k(x • z).",
        "A polynomial kernel of degree d is given by k(x) _ (1 + x)d. We can use vari•: positive example, o: negative example @, : support vectors",
        "ous kernels, and the design of an appropriate kernel for a particular application is an important research issue.",
        "Figure 1 shows a linearly separable case.",
        "The decision hyperplane defined by g(x) = 0 separates positive and negative examples by the largest margin.",
        "The solid line indicates the decision hyperplane and two parallel dotted lines indicate the margin between positive and negative examples.",
        "Since such a separating hyperplane may not exist, a positive parameter C is introduced to allow misclassifications.",
        "See Vapnik (1995)."
      ]
    },
    {
      "heading": "1.2 SVM-based NE recognition",
      "text": [
        "As far as we know, the first SVM-based NE system was proposed by Yamada et al.",
        "(2001) for Japanese.",
        "His system is an extension of Kudo’s chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.",
        "In their system, every word in a sentence is classified sequentially from the beginning or the end of a sentence.",
        "However, since Yamada has not compared it with other methods under the same conditions, it is not clear whether his NE system is better or not.",
        "Here, we show that our SVM-based NE system is more accurate than conventional systems.",
        "Our system uses the Viterbi search (Allen, 1995) instead of sequential determination.",
        "For training, we use ‘CRL data’, which was prepared for IREX (Information Retrieval and Extraction Exercise1, Sekine and Eriguchi (2000)).",
        "It has about 19,000 NEs in 1,174 articles.",
        "We also use additional data by Isozaki (2001).",
        "Both datasets are based on Mainichi Newspaper’s 1994 and 1995 CD-ROMs.",
        "We use IREX’s formal test data called GENERAL that has 1,510 named entities in 71 articles from Mainichi Newspaper of 1999.",
        "Systems are compared in terms of GENERAL’s F-measure",
        "which is the harmonic mean of ‘recall’ and ‘precision’ and is defined as follows.",
        "Recall = M/(the number of correct NEs), Precision = M/(the number of NEs extracted by a system), where M is the number of NEs correctly extracted and classified by the system.",
        "We developed an SVM-based NE system by following our NE system based on maximum entropy (ME) modeling (Isozaki, 2001).",
        "We simply replaced the ME model with SVM classifiers.",
        "The above datasets are processed by a morphological analyzer ChaSen 2.2.12.",
        "It tokenizes a sentence into words and adds POS tags.",
        "ChaSen uses about 90 POS tags such as common-noun and location-name.",
        "Since most unknown words are proper nouns, ChaSen’s parameters for unknown words are modified for better results.",
        "Then, a character type tag is added to each word.",
        "It uses 17 character types such as all-kanj i and small-integer.",
        "See Isozaki (2001) for details.",
        "Now, Japanese NE recognition is solved by the classification of words (Sekine et al., 1998; Borth-wick, 1999; Uchimoto et al., 2000).",
        "For instance, the words in “President George Herbert Bush said Clinton is ... ” are classified as follows: “President” = OTHER, “George” = PERSON-BEGIN, “Herbert”= PERSON-MIDDLE, “Bush” = PERSON-END, “said” = OTHER, “Clinton” = PERSON-SINGLE, “is” = OTHER.",
        "In this way, the first word of a person’s name is labeled as PERSON-BEGIN.",
        "The last word is labeled as PERSON-END.",
        "Other words in the name are PERSON-MIDDLE.",
        "If a person’s name is expressed by a single word, it is labeled as PERSON-SINGLE.",
        "If a word does not belong to any named entities, it is labeled as OTHER.",
        "Since IREX defines eight NE classes, words are classified into 33 (= 8 x 4 + 1) categories.",
        "Each sample is represented by 15 features because each word has three features (part-of-speech tag, character type, and the word itself), and two preceding words and two succeeding words are also used for context dependence.",
        "Although infrequent features are usually removed to prevent overfitting, we use all features because SVMs are robust.",
        "Each sample is represented by a long binary vector, i.e., a sequence of 0 (false) and 1 (true).",
        "For instance, “Bush” in the above example is represented by a",
        "Here, we have to consider the following problems.",
        "First, SVMs can solve only a two-class problem.",
        "Therefore, we have to reduce the above multi-class problem to a group of two-class problems.",
        "Second, we have to consider consistency among word classes in a sentence.",
        "For instance, a word classified as PERSON-BEGIN should be followed by PERSON-MIDDLE or PERSON-END.",
        "It implies that the system has to determine the best combinations of word classes from numerous possibilities.",
        "Here, we solve these problems by combining existing methods.",
        "There are a few approaches to extend SVMs to cover n-class problems.",
        "Here, we employ the “one class versus all others” approach.",
        "That is, each classifier f,(x) is trained to distinguish members of a class c from non-members.",
        "In this method, two or more classifiers may give +1 to an unseen vector or no classifier may give +1.",
        "One common way to avoid such situations is to compare g,(x) values and to choose the class index c of the largest g,(x).",
        "The consistency problem is solved by the Viterbi search.",
        "Since SVMs do not output probabilities, we use the SVM+sigmoid method (Platt, 2000).",
        "That is, we use a sigmoid function s (x) = 1/(1 + exp ( – , 3x)) to map g.. (x) to a probability-like value.",
        "The output of the Viterbi search is adjusted by a postprocessor for wrong word boundaries.",
        "The adjustment rules are also statistically determined (Isozaki, 2001)."
      ]
    },
    {
      "heading": "1.3 Comparison of NE recognizers",
      "text": [
        "We use a fixed value 3 = 100.",
        "F-measures are not very sensitive to ,3 unless ,3 is too small.",
        "When we used 1,038,986 training vectors, GENERAL’s F-measure was 89.64% for ,3 = 0.1 and 90.03% for ,3 = 100.",
        "We employ the quadratic kernel (d = 2) because it gives the best results.",
        "Polynomial kernels of degree 1, 2, and 3 resulted in 83.03%, 88.31%,",
        "when we used only CRL data.",
        "‘ME’ indicates our ME system and ‘RG+DT’ indicates a rule-based machine learning system (Isozaki, 2001).",
        "According to this graph, ‘SVM’ is better than the other systems.",
        "However, SVM classifiers are too slow.",
        "Famous SVM-Light 3.50 (Joachims, 1999) took 1.2 days to classify 569,994 vectors derived from 2 MB documents.",
        "That is, it runs at only 19 bytes/sec.",
        "TinySVM’s classifier seems best optimized among publicly available SVM toolkits, but it still works at only 92 bytes/sec."
      ]
    },
    {
      "heading": "2 Efficient Classifiers",
      "text": [
        "In this section, we investigate the cause of this inefficiency and propose a solution.",
        "All experiments are conducted for training data of 569,994 vectors.",
        "The total size of the original news articles was 2 MB and the number of NEs was 39,022.",
        "According to the definition of g(x), a classifier has to process e support vectors for each x.",
        "Table 1 shows es for different word classes.",
        "According to this table, classification of one word requires x’s dot products with 228,306 support vectors in 33 classifiers.",
        "Therefore, the classifiers are very slow.",
        "We have never seen such large es in SVM literature on pattern recognition.",
        "The reason for the large es is word features.",
        "In other domains such as character recognition, dimen",
        "sion D is usually fixed.",
        "However, in the NE task, D increases monotonically with respect to the size of the training data.",
        "Since SVMs learn combinations of features, a tends to be very large.",
        "This tendency will hold for other tasks of natural language processing, too.",
        "Here, we focus on the quadratic kernel k(x) (1 + x)2 that yielded the best score in the above experiments.",
        "Suppose x = (x[1], ... , x[D]) has only m (=15) non-zero elements.",
        "The dot product of x and zi = (zi [1], ... , zi [D]) is given by",
        "where",
        "where",
        "Now, g(x) can be given by summing up Wl'[h] for every non-zero element x [h] and W3 [h, k] for every non-zero pair x [h] x [k] .",
        "Accordingly, we only need to add 1 + m + rn(rn – 1)/2 (=121) constants to get g(x).",
        "Therefore, we can expect this method to be much faster than a na¨ıve implementation that computes tens of thousands of dot products at run time.",
        "We call this method ‘XQK’ (eXpand the Quadratic Kernel).",
        "Table 1 compares TinySVM and XQK in terms of CPU time taken to apply 33 classifiers to process the training data.",
        "Classes are sorted by e. Small numbers in parentheses indicate the initialization time for reading support vectors {zi} and allocating memory.",
        "XQK requires a longer initialization time in order to prepare Wi and W3.",
        "For instance, TinySVM took 11,490.26 seconds (3.2 hours) in total for applying OTHER’s classifier to all vectors in the training data.",
        "Its initialization phase took 2.13 seconds and all vectors in the training data were classified in 11,488.13 (= 11, 490.26 – 2.13) seconds.",
        "On the other hand, XQK took 225.28 seconds in total and its initialization phase took 174.17 seconds.",
        "Therefore, 569,994 vectors were classified in 51.11 seconds.",
        "The initialization time can be disregarded because we can reuse the above coefficents.",
        "Consequently, XQK is 224.8 (=11,488.13/51.11) times faster than TinySVM for OTHER.",
        "TinySVM took 6 hours to process all the word classes, whereas XQK took only 17 minutes.",
        "XQK is 102 times faster than SVM-Light 3.50 which took 1.2 days."
      ]
    },
    {
      "heading": "3 Removal of useless features",
      "text": [
        "XQK makes the classifiers faster, but memory requirement increases from O (EQ_ 1 rni) to O(EQ_1 Tni (Tni + 1)/2) where rni (=15) is the number of non-zero elements in zi.",
        "Therefore, removal of useless features would be beneficial.",
        "Conventional SVMs do not tell us how an individual feature works because weights are given not to features but to K(x, zi).",
        "However, the above weights (Wi and W3) clarify how a feature or a feature pair works.",
        "We can use this fact for feature selection after the training.",
        "We simplify f (x) by removing all features h that satisfy max(I W' [h] l, maxk IW3 [h, k] l, maxk jW3 [k, h] j)) < 0.",
        "The largest 0 that does not change the number of misclassifications for the training data is found by using the binary search for each word class.",
        "We call this method ‘XQK-FS’ (XQK with Feature Selection).",
        "This approximation slightly degraded GENERAL’s F-measure from 88.31% to 88.03%.",
        "Table 2 shows the reduction of features that appear in support vectors.",
        "Classes are sorted by the numbers of original features.",
        "For instance, OTHER has 56,220 features in its support vectors.",
        "According to the binary search, its performance did not change even when the number of features was reduced to 21,852 at 0 = 0.02676.",
        "The total number of features was reduced by 75% and that of weights was reduced by 60%.",
        "The table also shows CPU time for classification by the selected features.",
        "XQK-FS is 28.5 (=21754.23/ 763.10) times faster than TinySVM.",
        "Although the reduction of features is significant, the reduction of CPU time is moderate, because most of the reduced features are infrequent ones.",
        "However, simple reduction of infrequent features without considering weights damages the system’s performance.",
        "For instance, when we removed 5,066 features that appeared four times or less in the training data, the modified classifier for ORGANIZATION-END misclassified 103 training examples, whereas the original classifier misclassified only 19 examples.",
        "On the other hand, XQK-FS removed 12,141 features without an increase in misclassifications for the training data.",
        "XQK can be easily extended to a more general quadratic kernel k(x) = (Co + Clx)2 and to non-binary sparse vectors.",
        "XQK-FS can be used to select useful features before training by other kernels.",
        "As mentioned above, we conducted an experiment for the cubic kernel (d = 3) by using all features.",
        "When we trained the cubic kernel classifiers by using only features selected by XQK-FS, TinySVM’s classification time was reduced by 40% because f was reduced by 38%.",
        "GENERAL’s F-measure was slightly improved from 87.04% to 87.10%.",
        "On the other hand, when we trained the cubic kernel classifiers by using only features that appeared three times or more (without considering weights), TinySVM’s classification time was reduced by only 14% and the F-measure was slightly degraded to 86.85%.",
        "Therefore, we expect XQK-FS to be useful as a feature selection method for other kernels when such kernels give much better results than the quadratic kernel."
      ]
    },
    {
      "heading": "4 Reduction of training time",
      "text": [
        "Since training of 33 classifiers also takes a long time, it is difficult to try various combinations of parameters and features.",
        "Here, we present a solution for this problem.",
        "In the training time, calculation of k(x,, • X1), k(x,, • X2), ... , k(x,, • XN) for various X's is dominant.",
        "Conventional systems save time by caching the results.",
        "By analyzing TinySVM’s classifier, we found that they can be calculated more efficiently.",
        "For sparse vectors, most SVM classifiers (e.g., SVM-Light) use a sparse dot product algorithm (Platt, 1999) that compares non-zero elements of x and those of zi to get k(x • zi) in g(x).",
        "However, x is common to all dot products in k(x • zl ), ... , k(x ze).",
        "Therefore, we can implement a faster classifier that calculates them concurrently.",
        "TinySVM’s classifier prepares a list f i 2 s i [h] that contains all zi s whose h-th coordinates are not zero.",
        "In addition, counters for x • z1, ... , x • zQ are prepared because dot products of binary vectors are integers.",
        "Then, for each non-zero x[h], the counters are incremented for all zi E f i 2 s i [h] .",
        "By checking only members of f i 2 s i [h] for non-zero x [h] , the classifier is not bothered by fruitless cases: x [h] = 0, zi [h] 7� 0 or x[h] 7� 0, zj [h] = 0.",
        "Therefore, TinySVM’s classifier is faster than other classifiers.",
        "This method is applicable to any kernels based on dot products.",
        "For the training phase, we can build f i 2 s iV [h] that contains all xis whose h-th coordinates are not zero.",
        "Then, k(xa, • x1), ... , k(xa, • XN) can be efficiently calculated because xa, is common.",
        "This improvement is effective especially when the cache is small and/or the training data is large.",
        "When we used a 200 MB cache, the improved system took only 13 hours for training by the CRL data, while TinySVM and SVM-Light took 30 hours and 46 hours respectively for the same cache size.",
        "Although we have examined other SVM toolkits, we could not find any system that uses this approach in the training phase."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "The above methods can also be applied to other tasks in natural language processing such as chunking and POS tagging because the quadratic kernels give good results.",
        "Utsuro et al.",
        "(2001) report that a combination of two NE recognizers attained F = 84.07%, but wrong word boundary cases are excluded.",
        "Our system attained 85.04% and word boundaries are automatically adjusted.",
        "Yamada (Yamada et al., 2001) also reports that d = 2 is best.",
        "Although his system attained F = 83.7% for 5-fold cross-validation of the CRL data (Yamada and Matsumoto, 2001), our system attained 86.8%.",
        "Since we followed Isozaki’s implementation (Isozaki, 2001), our system is different from Yamada’s system in the following points: 1) adjustment of word boundaries, 2) ChaSen’s parameters for unknown words, 3) character types, 4) use of the Viterbi search.",
        "For efficient classification, Burges and Sch¨olkopf (1997) propose an approximation method that uses “reduced set vectors” instead of support vectors.",
        "Since the size of the reduced set vectors is smaller than f, classifiers become more efficient, but the computational cost to determine the vectors is very large.",
        "Osuna and Girosi (1999) propose two methods.",
        "The first method approximates g(x) by support vector regression, but this method is applicable only when C is large enough.",
        "The second method reformulates the training phase.",
        "Our approach is simpler than these methods.",
        "Downs et al.",
        "(Downs et al., 2001) try to reduce the number of support vectors by using linear dependence.",
        "We can also reduce the runtime complexity of a multi-class problem by cascading SVMs in the form of a binary tree (Schwenker, 2001) or a direct acyclic graph (Platt et al., 2000).",
        "Yamada and Matsumoto (2001) applied such a method to their NE system and reduced its CPU time by 39%.",
        "This approach can be combined with our SVM classifers.",
        "NE recognition can be regarded as a variable-length multi-class problem.",
        "For this kind of problem, probability-based kernels are studied for more theoretically well-founded methods (Jaakkola and Haussler, 1998; Tsuda et al., 2001; Shimodaira et al., 2001)."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "Our SVM-based NE recognizer attained F = 90.03%.",
        "This is the best score, as far as we know.",
        "Since it was too slow, we made SVMs faster.",
        "The improved classifier is 21 times faster than TinySVM and 102 times faster than SVM-Light.",
        "The improved training program is 2.3 times faster than TinySVM and 3.5 times faster than SVM-Light.",
        "We also presented an SVM-based feature selection method that removed 75% of features.",
        "These methods can also be applied to other tasks such as chunking and POS tagging."
      ]
    },
    {
      "heading": "Acknowledgment",
      "text": [
        "We would like to thank Yutaka Sasaki for the training data.",
        "We thank members of Knowledge Processing Research Group for valuable comments and discussion.",
        "We also thank Shigeru Katagiri and Ken-ichiro Ishii for their support."
      ]
    }
  ]
}
