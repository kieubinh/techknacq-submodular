{
  "info": {
    "authors": [
      "Horacio Saggion",
      "Dragomir R. Radev",
      "Simone Teufel",
      "Wai Lam"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1073",
    "title": "Meta-Evaluation of Summaries in a Cross-Lingual Environment Using Content-Based Metrics",
    "url": "https://aclweb.org/anthology/C02-1073",
    "year": 2002
  },
  "references": [
    "acl-A00-2024",
    "acl-A00-2035",
    "acl-P91-1023",
    "acl-W00-0401",
    "acl-W00-0403",
    "acl-W00-0408"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe a framework for the evaluation of summaries in English and Chinese using similarity measures.",
        "The framework can be used to evaluate extractive, non-extractive, single and multi-document summarization.",
        "We focus on the resources developed that are made available for the research community."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Evaluation is an essential step of any natural language processing task.",
        "In the field of text summarization almost all research is published with an in-house evaluation, which makes it difficult to replicate experiments, to compare results, or to use evaluation data for training purposes.",
        "The development of standards of evaluation and sharable resources, such as the Document Understanding Conference (DUC, 2000) among others, is of paramount importance for progress in text summarization.",
        "Evaluations can be intrinsic or extrinsic (Sparck Jones and Galliers, 1995): intrinsic evaluation measures the content of the summary by a comparison with an \"ideal\" or \"target\" summary.",
        "Extrinsic evaluation measures how helpful summaries are in the completion of a given task, for example in question answering or text categorization.",
        "If intrinsic evaluation is performed by comparing extracted sentences to a set of \"correct\" extracted sentences, then co-selection is measured by precision, recall and F-score (Firmin and Chrzanowski, 1999).",
        "But these measures only consider sentence identity and not sentence content to carry out Wai Lam Department of Systems Engineering & Engineering Management The Chinese University of Hong Kong Shatin, Hong Kong wlam@se.cuhk.edu.hk the comparison, which has the following negative effect: if two extracts consist of different sentences, whereby the sentences convey the same meaning, they are judged as very different by this measure, even though intuitively they would be judged as equivalent.",
        "As consequence of the fact that these measures consider only binary decisions (a sentence either is or is not in the extract), they ignore partially correct answers.",
        "Also, many researchers have opposed these measures; the generally accepted opinion is that there is no such thing as one ideal summary.",
        "Instead, a summary consists of a set of main ideas that should be conveyed (Jones and Paice, 1992) The most extensive extrinsic evaluation of summarization systems was the TIPSTER SUMMAC evaluation (Mani et al., 1998).",
        "In that evaluation, given a generic summary (or a full document), a human assessor had to perform different tasks, eg.",
        "relevance decision of a document given a query, or categorization of a document into one out of five categories to which the document is relevant.",
        "The evaluation seeks to determine whether the summary is effective in capturing whatever information in the document is needed to correctly categorize the document.",
        "SUMMAC was extremely labour-intensive because of the need for assessors who had to read each of the full documents or extracts, which is a clear disadvantage of extrinsic measures of evaluation.",
        "In our research we investigated measures for content evaluation based on the notion of vocabulary overlap.",
        "They are developed to palliate the problems with precision and recall.",
        "As they are completely automatic, they overcome the problems of task-based evaluations.",
        "These metrics are believed to be quite effective in determining the informativeness of a summary (Mani et al., 2001), and can be used in both extractive and non-extractive summarization, single and multi-document summarization.",
        "Recent research has shown how content-based evaluation can be carried out in automatic or semi-automatic fashion (Donaway et al., 2000; Paice and Oakes, 1999).",
        "In this paper, we are interested in meta-evaluation: a comparative evaluation of evaluation measures for summarization.",
        "We present a framework for evaluation of the content of automatic summaries, which relies on the availability of target summaries and extracts produced by humans.",
        "All the data created for this evaluation is available to the community for research purposes (http://www.clsp.jhu.edu/ws200l/groups/asmd)."
      ]
    },
    {
      "heading": "2 Experimental Framework",
      "text": [
        "The resources used in this research have been constructed in the context of the 2001 Workshop on Automatic Summarization of Multiple (Multilingual) Documents, a 6-week language engineering workshop at the Center for Language and Speech Processing, Johns Hopkins University.",
        "The objectives of the workshop were the integration of cross-lingual information retrieval with single and multi-document summarization and its evaluation."
      ]
    },
    {
      "heading": "2.1 Data and Annotation",
      "text": [
        "We use a parallel corpus of English and Chinese (Cantonese) texts which are translations or near translations of each other.",
        "The corpus consists of 18,461 document-pairs.",
        "The corpus, called the Hong Kong Newspaper Corpus, is provided by the Linguistic Data Consortium (LDC).",
        "We automatically separated the main title from the main body of text of the news article, inserted sentence and word boundaries (Grover et al., 2000).",
        "Semi-automatic corrections of sentence boundaries were made in those sets of documents where human sentence segmentation was available.",
        "The English corpus was further annotated with part of speech tags (Mikheev, 2000) and morphologic information, and both Chinese and English text were annotated with named entity tags.",
        "Sentence-level alignment was performed based on our reimplementation of Gale and Church's (1991) alignment algorithm.",
        "For a complete description of the corpus the reader is referred to (Saggion et al., 2002).",
        "We used 400 documents for our experiments.",
        "They were clustered into document sets of 10 documents about one subject (\"narcotics rehabilitation\", \"natural disaster victims aided\", \"customs staff doing good job\", etc.).",
        "LDC annotators developed 40 such queries according to our guidelines, then they used an in-house information retrieval engine and human revision, to find the 10 most relevant documents for that query.",
        "We provided a manual Chinese translation of each query.",
        "Three LDC judges then assessed each sentence in the 10 relevant documents, and assigned each sentence a score on a scale from 0 to 10, expressing how important this sentence is for the summary.",
        "This annotation, which is called \"utility judgement\", allows us to compile human-generated 'ideal' summaries at different compression rates, which is one gold-standard we use for our different measures of sentence-based agreement, both between the human agreement and between the system and the human annotators.",
        "We call this gold standard \"human extracts\".",
        "The judges also wrote multi-document summaries for each cluster at 50, 100, and 200 words (independently of the size of the documents).",
        "As human summary writing by trained professionals is very expensive, it was not possible to provide summaries of all 400 documents by several subjects (and several compression rates).",
        "However, our judges found the writing of multi-document summaries to be natural task.",
        "They followed the DUC guidelines to do so (DUC, 2000).",
        "These texts are a different gold standard we use (only for multi-document summaries); we call them \"human summaries\"."
      ]
    },
    {
      "heading": "2.2 Content-based Measures",
      "text": [
        "Content-based similarity measures are functions that take as arguments two text representations and compute a real value in the interval [0..1], the value 1 means that the two texts are closely related while the value 0 means that the two texts are quite different.",
        "We have specified and implemented the following measures: Cosine similarity is computed using the following formula (Salton, 1988): ,IE(X,)2*ï¿½/E(y,)2 where X and Y are text representations based on a vector space model.",
        "We use two possible weighting schemes for the terms: presence/absence of the term in the text or t f * idf computed using corpus and within text term distribution.",
        "Unit overlap is computed using the following formula:",
        "where X and Y are text representations based on sets.",
        "Here 11511 is the size of set S.",
        "where X and Y are representations based on sequences and where lcs(X,Y) is the length of the longest common subsequence between X and Y, length(X) is the length of the string X, and editdi(X,Y) is the minimum number of deletion and insertions needed to transform X into Y (Crochemore and Rytter, 1994).",
        "When comparing two texts, we compute a normalized pairwise lcs between the sentences of the two texts.",
        "Unlike cosine and overlap, longest common subsequence is sensitive on how information is sequenced in the text.",
        "As an illustration, consider the following two sentence fragments:",
        "(S1) the terrorist attacked the president.",
        "(S2) the president attacked the terrorist.",
        "the longest common subsequence between S1 and S2 has length 3 (because of the matching subsequences \"the... attacked the...\"), giving a similarity score of 3/5.",
        "Cosine similarity and token overlap consider S1 and S2 as \"identical\" with score 1.",
        "Metrics that consider the linguistic sequence, such as n-gram combinations or longest common subsequence can detect the difference in this case.",
        "For each source document and target length, three different target extracts produced from sentence utility judgement exist.",
        "Given an automatic extract S, the three target extracts Judge 0, Judge 1, Judge 2, and a similarity measure M we compute the following numbers:",
        "Average has been used before for content-based measures (Donaway et al., 2000) while Max and Min have been used only for co-selection (Salton et al., 1994)."
      ]
    },
    {
      "heading": "2.3 Text Representation",
      "text": [
        "One can compare text units at different levels of analysis: For example one can compare units relying on the number of word or token that two units share, or one can compare the number of lemmas they share.",
        "One can use only nouns as the representation, based on the idea that are the nouns that carry the content of the sentence; one might alternatively use main verbs.",
        "We experimented with all these parameters and allow our measures to operate at different granurality levels (cf. figure 1).",
        "In the case of texts in Chinese, we don't rely on parts of speech, but we do explore words and Chinese characters as possibilities, because we have developed algorithms to deal with these two text representations (http://www.mandarintools.com/segmenter.html)."
      ]
    },
    {
      "heading": "2.4 Summarization Technologies",
      "text": [
        "All summarizers considered in this evaluation are sentence extractors, i.e. they take as input a compression rate (n%) and a document (or cluster of documents) split into sentences, and output an n% extract of the document (or cluster of documents).",
        "Sentence extraction is a currently wide-spread, useful technique, but more research in summarization now is moving towards summarization by generation (Jing and McKeown, 2000; Saggion and Lapalme, 2000).",
        "Two ways of measuring summary length were explored in our framework: in sentences and in words.",
        "In our experiments we used three summarizers: Websumm (Mani and Bloedorn, 1999) for single and multi-document extracts for English texts; Mead (Radev et al., 2000) for English, Chinese, single and multi-document extracts; and Summariet (Hovy and Lin, 1999) for single-document extracts of Chinese texts.",
        "We consider two baselines in our experimental framework (for both English and Chinese): (i) Lead-based summarizer: n% sentences are picked up from the beginning of the text; and (ii) Random summarizer: n% sentences are picked up at random.",
        "Random summaries should give a lower bound for the performance any system should have, while it is well-known that lead-based summaries perform very well for certain text types."
      ]
    },
    {
      "heading": "2.5 Example",
      "text": [
        "We present a complete example of our evaluation measures using document 19980306_007.e ( \"Number of reported drug abusers dropped in 1997\").",
        "Figure 2 shows the summaries produced at sentence compression 10% by the three judges and by one of our summarizers.",
        "Table 1 shows the similarity between automatic and human extracts.",
        "We also include co-selection metrics for comparison purposes.",
        "Note that as extracts agree on how many sentences were extracted, Precision, Recall and F-measure are identical.",
        "This example clearly shows that there is no agreement between the summarizer and Judge 2 at the sentence level, nevertheless content-based measures show similarity on content.",
        "Also, co-selection measures are identical for Judge 0 and Judge 1, but the two target summaries are rather different and content-based measures are able to capture that difference.",
        "In Appendix A, we present Chinese versions of the extracts produced for the document 19980306_006.c (the Chinese version of document 19980306_007.e)."
      ]
    },
    {
      "heading": "3 Content-based results",
      "text": [
        "In this section we give an overview of the results obtained using content-based metrics, bearing in mind that our objective is not to demonstrate that one particular system is better than other, but to create a useful framework for evaluation.",
        "The numbers presented here are based on sentence-level compression, words, and all parts of speech.",
        "We present numbers for cosine (t f * idf) and longest common subsequence.",
        "The results obtained for a subset of target lengths using content-based evaluation can be seen in tables 2 and 3.",
        "In all our experiments with cosine (t f * idf), the lead-based summarizer obtained results close to the human extracts in most of the target lengths while Mead is ranked in second position.",
        "In all our experiments using longest common subsequence, results are inconclusive because no system appears to outperform the others.",
        "The experimental framework for evaluation of the Chinese summaries is based on the novel idea of using the aligned corpus as a source for obtaining a target abstract in Chinese.",
        "Given a collection of monolingual summaries, we can use our alignment tables to generate reasonable corresponding cross-lingual summaries and use the collection of these \"pseudo manual' chinese summaries in our experiments.",
        "This was at all possible because of the accuracy of the alignment program: A preliminary",
        "evaluation of our alignment algorithm measured precision and recall at 95.5% and 95.5% respectively.",
        "The numbers obtained in the evaluation of Chinese summaries for cosine (t f * idf) and longest common subsequence can be seen in tables 3 and 4.",
        "Both measures identify Mead as the summarizer that produced results closer to the ideal summaries (these results were replicated across measures and text representations).",
        "We have based this evaluation on human extracts produced by LDC assessors (and sentence-alignment in the Chinese case).",
        "Nevertheless, other alternatives exist: Content-based similarity measures do not require the target summary to be a subset of sentences from the source document, thus, content evaluation based on similarity measures can be done using human-written summaries."
      ]
    },
    {
      "heading": "4 Evaluation of Multi-document Summarizers using Human Summaries",
      "text": [
        "In this evaluation we compare human multi-document extracts with human multi-document summaries.",
        "We also compare automatic multi-document summaries produced by Mead with human multi-document summaries.",
        "As in the single document evaluation, the results for the human extracts are an average because three different multi-document extracts exists for each cluster.",
        "The results for all measures can be seen in tables 4 and 5.",
        "Not surprisingly, these results show that human extracts are closer to human summaries than automatic extracts are to human summaries.",
        "However, human multi-documemt extracts and automatic multi-document extracts are rather similar ac-Judge 0 (sentences 2, 3, 13): The number of drug abusers reported to the Central Registry of Drug Abuse (CRDA) in 1997 totalled 17,555, a drop of 10.8 per cent over the 19,671 reported in 1996, according to CRDA statistics presented to the Action Committee Against Narcotics (ALAN) today (Friday).",
        "Speaking at ACAN 's quarterly meeting, the Commissioner for Narcotics, Mrs Clarie Lo, said that young drug abusers reported in 1997 had also dropped by 14.3 per cent compared with 1996.",
        "Mrs Lo pointed out that there was a small decrease in the number of female drug abusers in 1997 when 2,216 abusers were reported, compared with 2,429 in 1996.",
        "Judge 1 (sentences 1, 2, 15): Number of reported drug abusers dropped in 1997.",
        "The number of drug abusers reported to the Central Registry of Drug Abuse (CRDA) in 1997 totalled 17,555, a drop of 10.8 per cent over the 19,671 reported in 1996, according to CRDA statistics presented to the Action Committee Against Narcotics (ALAN) today (Friday).",
        "Notwithstanding this observation, a thorough study on factors affecting the drug abuse trend in Hong Kong was recently commissioned by ACAN with a view to identifying the underlying factors that affect the size, complexity and characteristics of the drug abuse population in Hong Kong.",
        "Judge 2 (sentences 15, 17, 19): Notwithstanding this observation, a thorough study on factors affecting the drug abuse trend in Hong Kong was recently commissioned by ACAN with a view to identifying the underlying factors that affect the size, complexity and characteristics of the drug abuse population in Hong Kong.",
        "On preventive education, Mrs Lo said that more resources would be devoted to stepping up the beat drugs campaign despite the drop in the number of drug abusers figures.",
        "(i) to heighten awareness of the undesirable consequences of abusing drugs, no matter 'hard ' or 'soft ';\" ; MEAD (sentences 2, 26, 27): The number of drug abusers reported to the Central Registry of Drug Abuse (CRDA) in 1997 totalled 17,555, a drop of 10.8 per cent over the 19,671 reported in 1996, according to CRDA statistics presented to the Action Committee Against Narcotics (ALAN) today (Friday).",
        "ACAN was also informed by a Social Welfare Department 's representative at the meeting that the Subventions and Lotteries Fund Advisory Committee had supported the Government to grant $16.12 million to subvent the services provided by four non-medical voluntary drug treatment and rehabilitation agencies for the 1998/99 financial year.",
        "The four agencies are the Barnabas Charitable Service Association, the Christian New Being Fellowship, the Finnish Missionary Service Ling Oi Youth Centre and the Operation Dawn, all of which will also be granted a total subvention of $1.26 million to cover their expenses for the month of March in the 1997/98 financial year.",
        "cording to these measures.",
        "This experiment shows the use of our framework for comparing human and automatic extracts with human absâraââs, i.e. coherent, newly written summaries of the documents rather than sentence extracts.",
        "To our knowledge, no systematic experiments about agreement on the task of summary writing have been performed before.",
        "We believe that our metrics are very valuable, as the highest-quality automatic summaries of the future will probably mirror more and more human summaries, and move away from sentence extracts.",
        "â Conclusions In this paper, we have presented a framework for the evaluation of text summarization systems.",
        "The contributions of our research are as follows: First, we provide data and a test-bed for text-summarization evaluation, namely annotation of a preexisting parallel corpus, human-provided sentence-level utility-judgements which allow us to compile human-generated 'ideal' extracts, and multi-document human-written summaries at different compression rates, following DUC guidelines.",
        "These resources are being made available for the community.",
        "Second, we have implemented content-based similarity measures that can be used in both extractive and non-extractive summarization, single and multi-document summarization, and which can be used to compare texts in English and Chinese, and we have shown the advantages of these measures over single co-selection measures.",
        "Finally, we believe this is the first meta-evaluation directly comparing evaluation measures for text summarization on a large-scale level with unrestricted text.",
        "Acknowledgements We are grateful to Arda Celebi, John Blitzer, Hong âi, Danyu Liu, and Elliot Drabek for their work during the workshop.",
        "We thank Fred Jelinek, Sanjeev Khudanpur and the staff of the Center for Language and Speech Processing, Johns Hopkins University for their hospitality.",
        "We are grateful to Inderjeet Mani.",
        "We also thank Chin-âew Lin, Greg Silber, and Regina Barzilay.",
        "The 2001 Summer Workshop at Johns Hopkins University was sponsored by the National Science Foundation via Grant No.",
        "IIS0097467, which included support from the Defense Advanced Research Projects Agency."
      ]
    }
  ]
}
