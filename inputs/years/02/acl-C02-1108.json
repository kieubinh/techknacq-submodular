{
  "info": {
    "authors": [
      "Helmut Schmid"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1108",
    "title": "Lexicalization of Probabilistic Grammars",
    "url": "https://aclweb.org/anthology/C02-1108",
    "year": 2002
  },
  "references": [
    "acl-P96-1041",
    "acl-P97-1003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Two general methods for the lexicalization of probabilistic grammars are presented which are modular, powerful and require only a small number of parameters.",
        "The first method multiplies the unlexicalized parse tree probability with the exponential of the mutual information terms of all word-governor pairs in the parse.",
        "The second lexicalization method accounts for the dependencies between the different arguments of a word.",
        "The model is based on a EM clustering model with word classes and selectional restrictions as hidden features.",
        "This model is useful for finding word classes, selectional restrictions and word sense probabilities."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Unlexicalized probabilistic grammars like probabilistic context-free grammars (PCFGs) fail to disambiguate frequent syntactic ambiguities like PP-attachment and coordination ambiguities correctly, because they lack the necessary information about lexical dependencies.",
        "Compare e.g. the sentences He ate the cake with a spoon and He ate the cake with apricot glaze.",
        "A simple unlexicalized PCFG would assign the same structure to both sentences.",
        "Correct disambiguation of these sentences requires the knowledge that a spoon is a likely nominal head of with PPs modifying the verb eat, whereas glaze is a likely nominal head of with PPs modifying the noun cake.",
        "Probabilistic grammars are usually lexical-ized (Charniak, 1997; Collins, 1997; Carroll and Rooth, 1998) by replacing the grammar symbols with pairs consisting of a grammar symbol and a lexical head.",
        "The rule VP 4 V NP e.g. is replaced by a set of rules of the form (VP, eat)",
        "a data sparseness problem, the lexical heads of the arguments are usually assumed to be independent given the left-hand side of the rule (or the whole rule).",
        "In the simplest case, a probabilistic grammar with two types of rules is obtained.",
        "Rules of the first type select a grammar rule given the lexical head.",
        "They are exemplified by the rule (VP,eat) 4 (V,eat) (NP,VP,eat).",
        "Rules of the second type choose the lexical head of an argument.",
        "They are exemplified by the rule (NP,VP,eat) 4 (NP, cake).",
        "This type of lexicalization has the disadvantage that each unlexicalized parameter is replaced by a large number of lexicalized parameters.",
        "This paper presents an alternative method where the lexicalized model is a modular extension of the unlexicalized model and where the number of parameters is easy to control.",
        "The lexicalized probability of a parse is obtained by multiplying its unlexicalized probability with the exponential of the pointwise mutual information of each word and its lexical governor.",
        "The number of parameters is small because individual parameters are only required for pairs whose frequency differs significantly from the expected frequency under the assumption of independence.",
        "The model assumes that the different arguments of a word are statistically independent given the word.",
        "A second lexicalization method is described which captures the dependencies between the arguments of a word.",
        "It is based on a EM clustering model and is expected to have good smoothing properties."
      ]
    },
    {
      "heading": "2 Lexicalization With Governors",
      "text": [
        "The first lexicalization method imposes the following rather general restrictions on a probabilistic grammar.",
        "1.",
        "The grammar assigns an (unlexicalized) probability p(T) to each parse tree T. 2.",
        "The grammar assigns to each word w in a parse tree T at most one governor g.",
        "There are no other restrictions on the definition of the governor than its uniqueness (which will later be weakened).",
        "Governors may be words, lemmas or pairs consisting of a word/lemma and a grammatical function like subject, indirect object etc.",
        "For efficiency reasons, it is desirable that an efficient algorithm for the labeling of parse forests with governors is available.",
        "In order to motivate our solution, assume for the moment that a PCFG is to be lexicalized and that the words are generated by unary lexical rules C 4 w. We see in eq.",
        "1 that the lexicalized probability p(C 4 wIC, g) = p(wI C, g) is identical to the product of the unlexicalized probability p(C 4 wI C) = p(wI C) and EMI (w, gI C), the exponential of the pointwise mutual information between w and g given C.",
        "Therefore, the lexicalized probability of a parse tree is its unlexicalized probability multiplied by the EMI factors of the word-governor pairs.",
        "In order to simplify the model, a dummy governor is assigned to words without governor.",
        "The resulting formula for the computation of the lexicalized parse probability is shown in eq.",
        "2, where n is the sentence length and wi, Ci and gi are the i-th word, its category and its governor, respectively."
      ]
    },
    {
      "heading": "2.1 Parameter 'Raining",
      "text": [
        "The EMI parameters are either estimated from unparsed corpora using the Expectation Maximization (EM) algorithm or from treebank data.",
        "Treebank training requires a treebank which is annotated with governor information.",
        "If governor information is missing, it has to be added by running the annotation program whose availability is presumed.",
        "Given the annotated parse trees, the training algorithm counts how often a category C with governor g is expanded to some word w. For each tuple (w, C, g) with f (w, C, g) > 0, the algorithm checks whether its frequency diverges significantly from the expected value under the assumption that w is independent of g given C. To this end, it computes the probability that a sample of size n = Ew, f (w', C, g) which is randomly drawn from a Bernoulli distribution with probability p = p(wIC) contains",
        "threshold for significance), the algorithm estimates a separate EMI parameter for this tuple according to eq 3.",
        "The frequencies f (w, g, C) are smoothed frequencies obtained by absolute discounting (Ney et al., 1994) or similar smoothing methods (see e.g. (Chen and Goodman, 1996)).",
        "All words w which are not in the set WS (g, C) of words with significant frequency, share a single parameter EMI (w, gI C) = EMI( – , gI C) for a given context C and governor g. In order to obtain a probability distribution, this parameter has to",
        "Solving this equation for the unknown parameter EMI( – , gIC) gives the following formula for the computation of this parameter:",
        "Because EMI parameters are only stored for word-governor pairs with significant frequency, the number of parameters remains small."
      ]
    },
    {
      "heading": "2.2 Multiple Governors",
      "text": [
        "Linguistic grammars often analyze control verbs like to promise in such a way that an argument is governed by two verbs.",
        "The noun phrase Peter in the sentence Peter promised to come e.g. is governed by the two verbs promised and come.",
        "The lexicalized probability of an argument which is governed by two words is derived in the following way: (For simplicity, we omit here the C on which all probabilities depend.)",
        "We obtain an expression similar to the one in eq.",
        "1, but with two additional terms.",
        "The first additional term is the EMI score of the second governor.",
        "The second term captures the fact that the relation between w, gland 92 is not fully described by the pairwise dependencies.",
        "How could the second parameter be estimated?",
        "Estimating a separate parameter for each triple consisting of an argument and a governor pair is difficult due to data sparseness problems, although one of the governors has to be a control verb.",
        "Again, it seems a good idea to estimate this parameter O(w, 91, g2) = EMI (g1 i 92I w) /EMI (g1, g2) only for the small number of triples whose frequency diverges significantly from the expected value.",
        "To this end, we check whether b(r; n, p) < 0.05 for",
        "The other words w use the same parameter O(w, gl, g2) = 0( – ,91,92) whose value is chosen such that the following expression becomes 1: p(w) EMI(w, gl) EMI(w, 92) O(w, 91, 92) W Solving this equation for the unknown parameter O( – , 91, 92) gives the expression in eq.",
        "8 with",
        "For rare governor pairs, the value of O( – , 91, 92) has to be computed at runtime."
      ]
    },
    {
      "heading": "3 Lexicalization With Arguments",
      "text": [
        "So far, only the dependence of words on their governors (knife – cut, bread – cut) was considered.",
        "The different arguments of a word (knife, bread) were implicitly assumed to be independent of each other given the word.",
        "This is an oversimplification.",
        "Knifes are likely to cut bread, but managers are more likely to cut jobs.",
        "We drop the independence assumption and consider lexicalized models where a word w depends on its arguments al, ... , an.",
        "So, we are interested in the probability p(wl al, ... , an, C) which is similar to the probability of words with multiple governors.",
        "However, the proposed solution for multiple governors would not work here, because multiple arguments are much more frequent and because verbs have up to three arguments."
      ]
    },
    {
      "heading": "3.1 Incorporation of a Clustering Model",
      "text": [
        "Instead, the probability p(wI al, ... , an, C) is directly estimated with an EM clustering model (EMC) (see e.g. (Rooth, 1998)) which decomposes the probability p(w, al, ... , an, C) as fol",
        "We assume here that the number of arguments is fixed for each category C. The decomposition introduces the hidden features c and r. The clusters c are interpretable as word classes and the is are argument classes ( selectional restrictions)- r might be e.g. a concept of a taxonomy like Wordnet (Miller et al., 1993).",
        "The probability of a word-argument tuple is obtained by summing over all clusters and all argument classes for each argument.",
        "The probability p(wjal, ... , an, C) is computed as / p(w, al, .",
        ", an, C) plwl a1, ... .",
        ".",
        ", an, C) = p(al ... , an, C) (10) and the probability p(al, ... , an, C) is given by",
        "A separate set of clusters could be used for each category C, or, alternatively, a single cluster set for each set of similar categories like e.g. the verbal categories.",
        "There are no other prior restrictions on the assignment of words and argument classes to clusters, i.e. on the probabilities p(wlc) and p(rlc, i, Q.",
        "They are learned during training.",
        "The dependencies between the different argument positions of a word are modeled by the clusters.",
        "Training with the EM algorithm makes the clusters of an EMC model homogeneous, i.e. the words, categories and argument classes with a high probability for a given cluster are similar to each other.",
        "So, each cluster has high probabilities for a certain group of words w, a certain group of categories C, and certain argument classes.",
        "Word-argument-tuples which fit the restrictions of a cluster well, will have a high probability.",
        "Words with multiple readings are likely to have large probabilities for different clusters modeling the different readings.",
        "The distribution p(alr) of lexical heads a for a given argument class r is shared by all cluster sets.",
        "p(alr) is only positive if r dominates a in the taxonomy."
      ]
    },
    {
      "heading": "3.2 Advantages",
      "text": [
        "The resulting probability distribution will be smooth because",
        "• the dependencies between words and their arguments are modeled through the clusters which contain other words with similar arguments.",
        "An argument which occurred",
        "with one of the words is likely to occur with the others, too.",
        "• words select argument classes rather than individual words as arguments.",
        "If a word occurs with some argument a, then it is likely to occur also with semantically similar arguments.",
        "The introduction of hidden features reduces the number of parameters because",
        "• the argument classes ( selectional restrictions) depend only on the cluster and not on the governor or the classes of the other arguments.",
        "• the lexical heads of the arguments depend only on the argument class and are further restricted by the taxonomy.",
        "The lexicalization based on the EMC model has the following advantages compared to the previous model:",
        "• It models dependencies between arguments.",
        "• It models multiple readings of a word.",
        "• Word classes are learned.",
        "• Selectional restrictions are learned.",
        "• The resulting probability distribution is smooth.",
        "The components of the model are useful for other applications:",
        "• The p(wlc) probabilities allow different readings of a word to be separated.",
        "• p(Clc) could be useful for finding frequent alternations like the causative-inchoative alternation.",
        "• p(rlc, i, C) yields selectional restrictions for arguments.",
        "• p(alr) could be used to obtain probability estimates for the different senses of a word."
      ]
    },
    {
      "heading": "3.3 Interpolated Models",
      "text": [
        "The EMC model smooths the probability distribution.",
        "For fixed expressions like \"kick the buck-et\" or \"spill the beans\" the resulting probability distribution might actually be too smooth.",
        "Such phrases show very specific selectional properties which are not shared by other words and therefore modeled badly.",
        "This problem can be solved with the following method: 1.",
        "Compare the observed frequency of each word-argument tuple with the expected value according to its model-based probability.",
        "2.",
        "If there is a significant difference, add a new cluster which generates only this tuple.",
        "The resulting model is basically a weighted mixture of an EMC model and a family of distributions P(wjC, al, ... , an) which are non-zero for a small number of tuples.",
        "The latter parameters model fixed phrases whereas the EMC model assigns probabilities to the remaining data and to the literal meanings of phrases."
      ]
    },
    {
      "heading": "3.4 'Raining",
      "text": [
        "Because of the hidden parameters, the EMC model requires iterative EM training when it is trained on a treebank.",
        "Based on (estimates of) the frequencies of the word-argument-tuples in a corpus, the expected frequencies f (c, w), f (c, C, i �r), and f (a, r) are computed according to the current EMC model.",
        "f (c, w), e.g., is computed as follows:",
        "The model parameters are then reestimated based on these frequency estimates P(WIC) e.g. is computed as follows:",
        "The estimation of frequencies (E-step) and probabilities (M-step) is repeated until the likelihood of held-out data decreases.",
        "The training algorithm has three parameters, namely the number of clusters c, the set of selectional restrictions r and the set of start probabilities.",
        "These parameters should be varied in order to find a good combination.",
        "The different models could be compared on the basis of the likelihood of held-out data."
      ]
    },
    {
      "heading": "4 Summary",
      "text": [
        "Two methods for the lexicalization of probabilistic context-free grammars have been proposed.",
        "Both methods are modular extensions of unlexicalized models and both can be combined with a wide range of probabilistic grammars.",
        "They only require that an unlexicalized parse tree probability is defined and that the lexical governor/arguments of each word are defined and efficiently computable.",
        "The first method defines the lexicalized probability of a parse tree as the product of its unlexicalized probability and the exponentials of the pointwise mutual information scores of all words and their governors.",
        "Governors may be arbitrarily defined.",
        "They might e.g. be words or lemmas or words plus semantic relations.",
        "Even multiple governors can be handled in an extension of the model.",
        "Mutual information scores are only explicitly represented for word-governor pairs whose frequencies differ significantly from the expected values for independent words.",
        "Therefore the number of parameters is small.",
        "The second method is based on an EM clustering model which models the dependencies between the different arguments of a word.",
        "This model contains two hidden features, namely word classes and argument classes and requires iterative training with the EM algorithm.",
        "The model is improved by adding parameters which directly model frequent word-argument tuples which are not adequately modeled by the EMC model.",
        "This model is also useful for finding word classes, selectional restrictions and word sense probabilities."
      ]
    }
  ]
}
