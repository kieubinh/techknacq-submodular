{
  "info": {
    "authors": [
      "Armando Suárez",
      "Manuel Palomar"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1115",
    "title": "A Maximum Entropy-Based Word Sense Disambiguation System",
    "url": "https://aclweb.org/anthology/C02-1115",
    "year": 2002
  },
  "references": [
    "acl-A97-1011",
    "acl-N01-1011",
    "acl-P01-1027",
    "acl-P96-1006",
    "acl-W00-0804"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, a supervised learning system of word sense disambiguation is presented.",
        "It is based on conditional maximum entropy models.",
        "This system acquires the linguistic knowledge from an annotated corpus and this knowledge is represented in the form of features.",
        "Several types of features have been analyzed using the SENSEVAL-2 data for the Spanish lexical sample task.",
        "Such analysis shows that instead of training with the same kind of information for all words, each one is more effectively learned using a different set of features.",
        "This best-feature-selection is used to build some systems based on different maximum entropy classifiers, and a voting system helped by a knowledge-based method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word sense disambiguation (WSD) is an open research field in natural language processing (NLP).",
        "The task of WSD consists in assigning the correct sense to words using an electronic dictionary as the source of word definitions.",
        "This is a hard problem that is receiving a great deal of attention from the research community.",
        "Currently, there are two main methodological approaches in this research area: knowledge-based methods and corpus-based methods.",
        "The former approach relies on previously acquired linguistic knowledge, and the latter uses techniques from statistics and machine learning to induce models of language usage from large samples of text (Pedersen, 2001).",
        "Learning can be supervised or unsupervised.",
        "With supervised * This paper has been partially supported by the Spanish Government (CICYT) under project number TIC2000-0664-CO2-02.",
        "learning, the actual status (here, sense label) for each piece of data in the training example is known, whereas with unsupervised learning the classification of the data in the training example is not known (Manning and Schütze, 1999).",
        "At SENSEVAL-2, researchers showed the latest contributions to WSD.",
        "Some supervised systems competed in the Spanish lexical sample task.",
        "The Johns Hopkins University system (Yarowsky et al., 2001) combined, by means of a voting-based classifier, several WSD subsystems based on different methods: decision lists (Yarowsky, 2000), cosine-based vector models, and Bayesian classifiers.",
        "The University of Maryland system (UMD-SST) (Cabezas et al., 2001) used support vector machines.",
        "Pedersen (2002) proposes a baseline methodology for WSD based on decision tree learning and naive Bayesian classifiers, using simple lexical features.",
        "Several systems that combine different classifiers using distinct sets of features competed at SENSEVAL-2, both in the English and Spanish lexical sample tasks.",
        "This paper presents a system that implements a corpus-based method of WSD.",
        "The method used to perform the learning over a set of sense-disambiguated examples is that of maximum entropy (ME) probability models.",
        "Linguistic information is represented in the form of feature vectors, which identify the occurrence of certain attributes that appear in contexts containing linguistic ambiguities.",
        "The context is the text surrounding an ambiguity that is relevant to the disambiguation process.",
        "The features used may be of a distinct nature: word collocations, part-of-speech (POS) labels, keywords, topic and domain information, grammatical relationships, and so on.",
        "Instead of training with the same kind of information for all words, which underestimates which information is more relevant to each word, our research shows that each word is more effectively learned using a different set of features.",
        "Therefore, a more accurate feature selection can be done testing several combinations of features by means of a n-fold cross-validation over the training data.",
        "At SENSEVAL-2, Stanford University presented a metalearner (Ilhan et al., 2001) combining simple classifiers (naive-Bayes, vector space, memory-based and others) that use voting and conditional ME models.",
        "Garcia Varea et al.",
        "(2001) do machine translation tasks using ME to perform some kind of semantic classification, but they also rely on another statistical training procedure to define word classes.",
        "In the following discussion, the ME framework will be described.",
        "Then, feature implementation and the complete set of feature definitions used in this work will be detailed.",
        "Next, evaluation results using several combinations of these features will be shown.",
        "Finally, some conclusions will be presented, along with a brief discussion of work in progress and future work planned.",
        "2 The Maximum Entropy Framework ME modeling provides a framework for integrating information for classification from many heterogeneous information sources (Manning and Schütze, 1999).",
        "ME probability models have been successfully applied to some NLP tasks, such as POS tagging or sentence boundary detection (Ratnaparkhi, 1998).",
        "The WSD method used in this work is based on conditional ME models.",
        "It has been implemented using a supervised learning method that consists of building word-sense classifiers using a semantically tagged corpus.",
        "A classifier obtained by means of an ME technique consists of a set of parameters or coefficients which are estimated using an optimization procedure.",
        "Each coefficient is associated with one feature observed in the training data.",
        "The main purpose is to obtain the probability distribution that maximizes the entropy, that is, maximum ignorance is assumed and nothing apart from the training data is considered.",
        "Some advantages of using the ME framework are that even knowledge-poor features may be applied accurately; the ME framework thus allows a virtually unrestricted ability to represent problem-specific knowledge in the form of features (Rat-naparkhi, 1998).",
        "Let us assume a set of contexts X and a set of classes C. The function cl : X – � C chooses the class c with the highest conditional probability in the context x: cl (x) = arg max p(clx).",
        "Each feature is calculated by a function that is associated to a specific class c', and it takes the form of equation (1), where cp(x) is some observable characteristic in the contexts.",
        "The conditional probability p(clx) is defined by equation (2), where ai is the parameter or weight of the feature i, K is the number of features defined, and Z(x) is a value to ensure that the sum of all conditional probabilities for this context is equal to 1.",
        "The implementation of this ME framework for WSD was done in C++ and included the learning module, the classification module, the evaluation module, and the corpus translation module.",
        "The first two modules comprise the main components.",
        "The learning module produces the classifiers for each word using a corpus that is syntactically and semantically annotated.",
        "This module has two subsystems.",
        "The first subsystem consists of two component actions: in a first step, the module processes the learning corpus in order to define the functions that will apprise the linguistic features of each context; in a second step, the module then fills in the feature vectors.",
        "The second subsystem of the learning module performs the estimation of the coefficients and stores the classification functions.",
        "For example, let us assume that we want to build a classifier for noun interest and that POS label of the previous word is the type of feature to use and the training corpus has these three samples: 'The ME approach is not limited to binary features, but the optimization procedure used for the estimation of the parameters, the Generalized Iterative Scaling procedure, uses this kind of functions.",
        "... the widespread interest#1 in the ... ... the best interest#5 of both ... ... persons expressing interest#1 in the ...",
        "The learning module performs a sequential processing of this corpus looking for pairs <POS-label, sense>.",
        "Then, <adjective,#1>, <adjective,#5>, and <noun,#I> are used to define three functions (each context have a vector of three features).",
        "Next, each vector is filled in with the result of the evaluation of each function.",
        "Finally, the optimization procedure calculates the coefficients and the output is a classifier for the word interest.",
        "The classification module carries out the disambiguation of new contexts using the previously stored classification functions.",
        "When ME does not have enough information about a specific context, several senses may achieve the same maximum probability and thus the classification cannot be done properly.",
        "In these cases, the most frequent sense in the corpus is assigned.",
        "However, this heuristic is only necessary for minimum number of contexts or when the set of linguistic attributes processed is very small."
      ]
    },
    {
      "heading": "3 Feature Implementation",
      "text": [
        "An important issue in the implementation of this ME framework is the form of the functions that calculate each feature.",
        "These functions are defined in the training phase and depend upon the data in the corpus.",
        "A usual definition of features would substitute cp(x) in equation (1) with an expression like info(x,i) = a, where info(x,i) informs about • property that can be found at position i in • context x, and a is a predefined value.",
        "For example, if we consider that 0 is the position of the word to be learned and that i is related to 0, then POS(x,-1) = `adjective'.",
        "Therefore, equation (1) is used to generate a function for each possible value (sense, a) at position i.",
        "Henceforth, we will refer to this type of features as \"non-relaxed\" features, against \"relaxed\" features described below.",
        "In the example of the previous section, three \"non-relaxed\" functions could be defined.",
        "Other expressions, such as info(x,i) E W( ,�i)� may be substituted for the term cp(x) as a way to reduce the number of possible features.",
        "In the expression above, for example, Ww�i) is the set of attributes present in the learning examples at position i.",
        "Again, if we assume that POS(x, – 1), then for each sense of the ambiguous word, the system builds a set with the POS tags occurring in their previous positions.",
        "So this kind of function reduces the number of features to one per each sense at position i.",
        "In the example of the previous section, two \"relaxed\" functions could be defined from <{ adjective, nounl,#1> and <adjective,#5>.",
        "Due to the nature of the disambiguation task, the number of times that a feature generated by the first type of function ( \"non-relaxed\") is activated is very low, and feature vectors have a large number of values equal to 0.",
        "The new function drastically reduces the number of features, with a minimum of degradation in evaluation results.",
        "At the same time, new features can be incorporated into the learning process with a minimum impact on efficiency."
      ]
    },
    {
      "heading": "4 Description of Features",
      "text": [
        "The set of features defined for the training of the system is described in figure 1, and is based on the feature selection made by Ng and Lee (1996) and Escudero et al.",
        "(2000).",
        "Features are automatically defined as explained before and depend on the data in the training corpus.",
        "These features are based on words, collocations, and POS tags in the local context.",
        "Both \"relaxed\" and \"non-relaxed\" functions are used.",
        "• 0: ambiguous-word shape • s: words at positions fl, f2, f3 • p: POS-tags of words at positions fl, f2, f3 • b: lemmas of collocations at positions (-2, – 1), (-1, +1), (+1, +2) • c: collocations at positions (-2, – 1), (-1, +1), (+1,+2) • km: lemmas of nouns at any position in context, occurring at least mYo times with a sense • r: grammatical relation of the ambiguous word • d: the word that the ambiguous word depends on • L: lemmas of content-words at positions fl, f2, f3 (\"relaxed\" definition) • W: content-words at positions fl, f2, f3 ( \"relaxed\" definition) • S, B, C, P, and D: \"relaxed\" versions",
        "Actually, each item in figure 1 groups several sets of features.",
        "The majority of them depend on nearest words (for example, s comprises all possible features defined by the words occurring in each sample at positions w_3, w_2, w_1, w+1i w+2i w+3 related to the ambiguous word).",
        "Types nominated with capital letters are based on the \"relaxed\" function form, that is, these features consists of a simply recognition of an attribute as belonging to the training data.",
        "Keyword features (km) are vaguely inspired by Ng and Lee (1996).",
        "A nouns selection is done using frequency information for nouns co-occurring with a particular sense.",
        "For example, in a set of 100 examples of sense four of the noun interest, if the noun bank is found ten times or more (m = 10%), then a feature is defined for each possible sense of interest.",
        "Moreover, new features have also been defined using other grammatical properties: relationship features (r) that refer to the grammatical relationship of the ambiguous word (subject, object, complement, ...) and dependency features (d and D) extract the word related to the ambiguous one through the dependency parse tree."
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": [
        "In this section we present the results of our evaluation over the training and test data of the SENSEVAL-2 Spanish lexical sample task.",
        "This corpus was parsed using Conexor Functional Dependency Grammar parser for Spanish (Tapanainen and Jarvinen, 1997).",
        "Table 1 shows the five best results using several sets of features.",
        "The classifiers were built",
        "from the training data and evaluated over the test data.",
        "These values mean the maximum accuracy that the system can achieve at this moment with a fixed set of features for all words.",
        "Nevertheless, there are clear differences between nouns, verbs and adjectives.",
        "Our main goal is to find a method to automatically obtain the best feature selection from the training data.",
        "Such method consists of a n-fold cross-validation testing several combinations of features over the training data and the analysis of the results obtained for each word.",
        "Table 2 shows the best results obtained using a 3-fold cross-validation evaluation method on training data.",
        "Several feature combinations have been tested in order to find the best set for each selected word.",
        "The purpose was to achieve the most relevant information for each word from the corpus rather than applying the same combination of features to all of them.",
        "Therefore, column Features is the feature selection with the best result.",
        "Strings in each row represent the whole set of features used in the training of each classifier.",
        "For example, autoridad obtains its best result using nearest words, collocations of two lemmas, collocations of two words, and POS information; s, b, c and p features respectively (see figure 1).",
        "Functions is the number of functions generated from features, and Accur (for \"accuracy\") the number of correctly classified contexts divided by the total number of contexts.",
        "Column MFS is the accuracy obtained when the most frequent sense is selected.",
        "In order to perform the three tests on each word, some preprocessing of the corpus was done.",
        "For each word, all senses were uniformly distributed in the three folds (each fold contains one third of examples of each sense).",
        "Those senses that had fewer than three examples in the original corpus file were rejected and not processed.",
        "The data summarized in Table 2 reveal that utilization of \"relaxed\" features in the ME method is useful; both \"relaxed\" and \"nonrelaxed\" functions are used, even for the same word.",
        "For example, adjective vital obtains the best result with \"Sbcp\" (the \"relaxed\" version of words in a window (-3.. + 3), collocations of two lemmas and two words in a window (-2.. + 2), and POS labels, in a window (-3.. + 3) too); we can assume that single word information is less important than collocations in order to disambiguate vital correctly.",
        "Ambiguous word shape (0 features) is useful for nouns, verbs and adjectives, but many of the words do not use it for its best feature selection.",
        "In general, these words have not a relevant relationship between shape and senses.",
        "On the other hand, POS information (p and P features) is selected less often.",
        "When comparing lemma features against word features (e.g., L versus W, and B versus C), they are complementary in the majority of cases.",
        "Grammatical relationships (r features) and word-word dependencies (d and D features) seem very useful too if combined with other types of attributes.",
        "Moreover, keywords (km features) are used very often, possibly due to the source and size of contexts of SENSEVAL-2 data.",
        "each word; MEbfs.pos uses the best selection of each POS for all words of that POS; finally, vME is a majority voting system that has as input the answers of the three systems.",
        "Table 4 shows the comparison of these four systems, the less efficient is MEfix that applies the same set of types of features to all words.",
        "However, the best feature selection per word (MEbfs) is not the best, probably because deeper analysis and more training examples are necessary.",
        "The best choice seems to select a fixed set of types of features for each POS (MEbfs.pos).",
        "This last system obtains an accuracy slightly better than the best evaluation result in table 1, that is, a best-feature-selection strategy from training data guarantees a successful disambiguation.",
        "In general, verbs are difficult to learn and the accuracy of the method for them too low; in our opinion, more information (knowledge-based perhaps) is needed to build their classifiers, but the types of features used could be unsuitable too.",
        "The voting system (vME), based on the agreement between the other three systems, does not improve the accuracy.",
        "Finally, the results of the ME method were compared with the systems that competed at SENSEVAL-2 in the Spanish lexical sample task (table 5)23.",
        "If the ME systems described previously are ranked within this scoring table, nouns and adjectives obtain a excellent results; verbs obtain worse results.",
        "Table 5 also includes an enrichment of vME: vME+SM.",
        "This new voting system adds another classifier, specification marks (Montoyo and Palomar, 2001), a knowledge-based method that uses the semantic relationships between words stored in WordNet and EuroWordNet (Vossen, 1998).",
        "Because it works merely with nouns, vME+SM improves accuracy for them only, but obtains the same score than JHU(R).",
        "Overall score reaches the second place."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "A WSD system based on maximum entropy conditional probability models has been presented.",
        "Systems: JHU and JHU(R) by Johns Hopkins University; CSS244 by Stanford University; UMD-SST by University of Maryland; Duluth systems by University of Manitoba; UA by University of Alicante.",
        "It is a supervised learning method that needs a corpus previously annotated with sense labels.",
        "Using the training data of SENSEVAL-2 for the Spanish lexical sample task, several combinations of features were analyzed in order to identify which were the best.",
        "This information is the basis of various sets of classifiers, as well as two majority voting systems.",
        "The results obtained by these systems show that selecting best feature sets guarantees the success of the disambiguation method.",
        "As we work to improve the ME method with a deeper analysis of the feature selection strategy, we are also working to develop a cooperative strategy between several other methods as well, both knowledge-based and corpus-based.",
        "Future research will incorporate domain information as an additional information source for the system.",
        "WordNet Domains (Magnini and Strapparava, 2000) is an enrichment of WordNet with domain labels.",
        "These attributes will be incorporated into the learning of the system in the same way that features were incorporated, as described above, except that domain disambiguation will be evaluated as well; that is, WordNet senses (sgnsets) will be substituted for domains labels, thereby reducing the number of possible classes into which contexts can be classified."
      ]
    }
  ]
}
