{
  "info": {
    "authors": [
      "Toru Hisamitsu",
      "Yoshiki Niwa"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1125",
    "title": "A Measure of Term Representativeness Based on the Number of Co-Occurring Salient Words",
    "url": "https://aclweb.org/anthology/C02-1125",
    "year": 2002
  },
  "references": [
    "acl-C00-1047",
    "acl-J90-1003",
    "acl-J93-1003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We propose a novel measure of the representativeness (i.e., indicativeness or topic specificity) of a term in a given corpus.",
        "The measure embodies the idea that the distribution of words co-occurring with a representative term should be biased according to the word distribution in the whole corpus.",
        "The bias of the word distribution in the co-occurring words is defined as the number of distinct words whose occurrences are saliently biased in the co-occurring words.",
        "The saliency of a word is defined by a threshold probability that can be automatically defined using the whole corpus.",
        "Comparative evaluation clarified that the measure is clearly superior to conventional measures in finding topic-specific words in the newspaper archives of different sizes."
      ]
    },
    {
      "heading": "Introduction",
      "text": [
        "Measuring the representativeness (i.e., the informativeness or domain specificity) of a term1 is essential to various tasks in natural language processing (NLP) and information retrieval (IR).",
        "Such a measure is particularly crucial to automatic dictionary construction and IR interfaces to show a user words indicative of topics in retrievals that often consist of an intractably large number of documents (Niwa et al.",
        "2000).",
        "This paper proposes a novel and effective measure of term representativeness that reflects the bias of the words co-occurring with a term.",
        "In the following, we focus on extracting topic words from an archive of newspaper articles.",
        "In the literature of NLP and IR, there have been a number of studies on term weighting, and these are strongly related to measures of term 1 A term is a word or a word sequence.",
        "representativeness (see section 1).",
        "In this paper we employ the basic idea of the ‘baseline method’ proposed by Hisamitsu (Hisamitsu et al.",
        "2000).",
        "The idea is that the distribution of words co-occurring with a representative term should be biased according to the word distribution of the whole corpus.",
        "Concretely, for any term T and any measure M for the degree of bias of word occurrences in D(T), a set of words co-occurring with T, according to those of the whole corpus D0, the baseline method defines representativeness of term T by normalizing M(D(T)).",
        "In what follows, D0 is an archive of newspaper articles and D(T) is defined as the set of all articles containing T. The normalization of M(D(T)) is done by a function BM, called the baseline function, which estimates the value of M(Drand) using #Drand for any randomly sampled document (in our case, ‘article’) set Drand, where #Drand stands for the total number of words contained in Drand.",
        "By dividing M(D(T)) by BM(#D(T)), comparison of M(D(T1)) and M(D(T2)) becomes meaningful even if the frequencies of T1 and T2 are very different.",
        "We denote this normalized value by NormM(D(T)).",
        "Hisamitsu et al.",
        "reported that NormM(D(T)) is very effective in capturing topic-specific words when M(D(T)) is defined as the distance between two word distributions PD(T) and P0 (see subsection 1.2), which we denote by Dist(D(T)).",
        "Although NormDist(D(T)) outperforms existing measures, it has still an intrinsic drawback shared by other measures, that is, words which are irrelevant to T and simply happen to occur in D(T) --- let us call these words non-typical words --- contribute to the calculation of M(D(T)).",
        "Their contribution accumulates as background noise in M(D(T)), which is the part to be offset by the baseline function.",
        "In other words, if M(D(T)) were to exclude the contribution of non-typical words, it would not need to be normalized and would be more precise.",
        "This consideration led us to propose a different approach to measure the bias of word occurrences in a discrete way: that is, we only take words whose occurrences are saliently biased in D(T) into account, and let the number of such words be the degree of bias of word occurrences in D(T).",
        "Thus, SAL(D(T), s), the number of words in D(T) whose saliency is over a threshold value s, is expected to be free from the background noise and sensitive to number of major subtopics in D(T).",
        "The essential problem now is how to define the saliency of bias of word occurrences and the threshold value of saliency.",
        "This paper solves this problem by giving a mathematically sound measure.",
        "Furthermore, it is shown that the optimal threshold value can be defined automatically.",
        "The newly defined measure SAL(D(T), s) outperforms existing measures in picking out topic-specific words from newspaper articles."
      ]
    },
    {
      "heading": "1. Brief review of term representativeness measures",
      "text": []
    },
    {
      "heading": "1.1 Conventional measures",
      "text": [
        "Regarding term weighting, various measures of importance or domain specificity of a term have been proposed in NLP and IR domains (Kageura et al.",
        "1996).",
        "In his survey, Kageura introduced two aspects of a term: unithood and termhood.",
        "Unithood is \"the degree of strength or stability of syntagmatic combinations or collocations,\" and termhood is \"the degree to which a linguistic unit is related to (or more straightforwardly, represents) domain-specific concepts.\" Kageura's termhood is therefore what we call representativeness here.",
        "Representativeness measures were first introduced in the context of determining indexing words for IR (for instance, Salton et al.",
        "1973; Spark-Jones et al.",
        "1973; Nagao et al.",
        "1976).",
        "Among a number of measures introduced there, the most commonly used one is tf-idf proposed by Salton et al.",
        "There are a variety of modifications of tf-idf (for example, Singhal et al.",
        "1996) but all share the basic feature that a word appearing more frequently in fewer documents is assigned a higher value.",
        "In NLP domains several measures concentrating on the unithood of a word sequence have been proposed.",
        "For instance, the mutual information (Church et al.",
        "1990) and log-likelihood ratio (Dunning 1993; Cohen 1995) have been widely used for extracting word bigrams.",
        "Some measures for termhood have also been proposed, such as Imp (Nakagawa 2000), C-value and NC-value (Mima et al.",
        "2000).",
        "Although certain existing measures are widely used, they have major problems as follows: (1) classical measures such as tf-idf are so sensitive to term frequencies that they fail to avoid uninformative words that occur very frequently; (2) measures based on unithood cannot handle single-word terms; and (3) the threshold value for a term to be considered as being representative is difficult to define or can only be defined in an ad hoc manner.",
        "It is reported that measures defined by the baseline method do not have these problems (Hisamitsu et al.",
        "2000)."
      ]
    },
    {
      "heading": "1.2 Baseline method",
      "text": [
        "The basic idea of the baseline method stated in introduction can be summarized by the famous quote (Firth 1957) : \"You shall know a word by the company it keeps.\" This is interpreted as the following hypothesis: For any term T, if the term is representative, word occurrences in D(T), the set of words co-occurring with T, should be biased according to the word distribution in D0.",
        "This hypothesis is transformed into the following procedure: Given a measure M for the bias of word occurrences in D(T) and a term T, calculate M(D(T)), the value of the measure for D(T) .",
        "Then compare M(D(T) ) with BM(#D(T)) , where #D(T) is the number of words contained in #D(T), and BM estimates the value of M(D) when D is a randomly chosen document set of size #D(T).",
        "Here, as stated in introduction, D(T) is considered to be the set of all articles containing term T. Hisamitsu et al.",
        "tried a number of measures for M, and found that using Dist(D(T)), the distance between the word distribution PD(T) in D(T) and the word distribution P0 in the whole corpus D0 is effective in picking out topic-specific words in newspaper articles.",
        "The value of Dist(D(T)) can be defined in various ways, and they found that using log-likelihood ratio (see Dunning 1993) worked best which is represented as follows:",
        "where ki and Ki are the frequency of a word wi in D(W) and D0 respectively, and {w1,...,wM} is the set of all words in D0.",
        "As stated in introduction, Dist(D(T)) is normalized by the baseline function, which is referred as BDist(•) here.",
        "Figure 1(a) illustrates the necessity of the normalization: the graph’s coordinates are {(#D(T), Dist(D(T))) and {(#Drand, Dist(Dran d))}, where T varies over “cipher”, “do”, and “economy”, and Drand varies over a wide numerical range of randomly sampled articles.",
        "This figure shows that Dist(D(“do”)) is smaller than Dist(D(“electronic”)), which reflects our linguistic intuition that words co-occurring with “electronic” are more biased than those with “do”.",
        "However, Dist(D(“cipher”)) is smaller than Dist(D(“do”)), which contradicts our linguistic intuition.",
        "This is why values of Dist(D(T)) are not directly used to compare the representativeness of terms."
      ]
    },
    {
      "heading": "#Drand and #D(T) Figure 1(a)",
      "text": [
        "Baseline curve and sample word distribution This phenomenon can be explained by the curve, referred to as the baseline curve, composed of {(#Drand, Dist(Drand)}.",
        "The curve indicates that a part of Dist(D(T)) systematically varies depending only on #D(T) and not on T itself.",
        "It indicates the very notion of background noise stated in introduction, and by offsetting this part using the baseline function BDist(#D(T)), which approximates the baseline curve, the graph is converted into that of Figure 1(b).",
        "Since the baseline curve is not very meaningful as #Drand approaches to #D0, extremely frequent terms, such as “do” are treated in a special way: that is, if the number of documents in D(T) is larger than a threshold value N0, which was calculated from the average number of words contained in a document, N0 documents are randomly chosen from D(T).",
        "This is because the coordinates of the point corresponding to “do” differ in Fig. 1(a) and Fig. 1(b).",
        "As stated in introduction, Hisamitsu et al.",
        "(2000) reported on that the superiority of NormDist(D(T)), normalized Dist(D(T)), in picking out topic-specific words over various measures including existing ones and other ones developed by using the baseline method."
      ]
    },
    {
      "heading": "Effect of Normalization 1.3 Reconsideration of normalization",
      "text": [
        "The effectiveness of the baseline method’s normalization indicates that Dist(D(T)) can be decomposed into two parts, one depending on T itself and another depending only on the size of D(T), which is considered to be background noise.",
        "The essence of the baseline method is to make the background noise explicit as a baseline function and to offset the noise by using the baseline function.",
        "To put it the other way round, if a term representativeness measure is designed so that this noise part does not exist in the first place, there is no need for the baseline function and calculation of representativeness becomes much simpler.",
        "More importantly, the precision of the measure itself should improve.",
        "The definition of Dist(D(T)) shows, as with other measures, that every word in D(T) contributes to the value of Dist(D(T)).",
        "This explains why background noise, BDist(#D(T)), grows as #D(T) increases.",
        "One way to improve this situation is to eliminate the contribution of non-typical (see introduction) words.",
        "The simplest way to archive this is to focus only on saliently occurring words (precisely, words whose occurrences are saliently biased in D(T)) and let the number of words whose saliency is over a threshold value s, denoted by SAL(D(T), s), be the degree of bias of word 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 #Drand and #D(T) occurrences in D(T).",
        "SAL(D(T), s) should reflect the richness of subtopics in D(T) and should be free from the contribution of non-typical words in D(T).",
        "Thus, we need to define the saliency of occurrences of a word and a threshold value with which the occurrences of a word in D(T) is determined as salient."
      ]
    },
    {
      "heading": "2. Term representativeness measure based on the number of co-occurring salient words",
      "text": []
    },
    {
      "heading": "2.1 A measure of word occurrence saliency",
      "text": [
        "To define saliency of occurrences of a word w in D(T), we employ a probabilistic measure proposed by Hisamitsu et al.",
        "(2001) as follows: Let the total number (occurrences) of words in the whole corpus be N, the number (occurrences) of words in D(T) be n, the frequency of w in the whole corpus be K, and the frequency of w in D(T) be k. Denote the probability of “No less than k red balls are contained in n balls that are arbitrarily chosen from N balls containing K red balls” by hgs(N, K, n, k).",
        "Then the saliency of w in D(T) is defined as −log(hgs(N, K, n, k)) 2.",
        "Note that the probability of “k red balls are contained in n balls arbitrarily chosen from N balls containing K red balls”, which we denote as hg(N, K, n, k), is a hypergeometric distribution with variable k. We denote the value −log(hgs(N, K, n, k)) by",
        "Due to its probabilistic meaning, comparison of the 2 The reason why HGS(v) should be defined by −hgs(N, K, n, k) instead of −hg(N, K, n, k) is that the value of −hg(N, K, n, k) itself cannot tell whether occurrence of v k-times is saliently frequent or saliently infrequent.",
        "Only hgs(N, K, n, l), the sum of hg (N, K, n, l) over l (k≤l≤min{n,K}) can tell which is the case since the sum indicates how far the event “v occurs k-times in D(w)” is from the extreme event “v occurs min{n,K} times in D(w)”.",
        "value of HGS(w)= −log(hgs(N, K, n, k)) is always meaningful between any combination of N, K, n, and k. HGS(w) can be calculated very efficiently using an approximation technique (Hisamitsu et al.",
        "2001)."
      ]
    },
    {
      "heading": "2.2 Definition of SAL(D(T), s)",
      "text": [
        "Now we can define SAL(D(T), s) using the saliency measure defined above and a parameter s ≥ 0: SAL(D(T), s) = DIFFNUM{w∈ D(T) |HGS(w) ≥ s}, where DIFFNUM(X) stands for the number of distinct items in set X.",
        "That is, SAL(D(T), s) is the number of distinct words in D(T) whose saliency of occurrence is not less than s. For instance, using the 1996 archive of Nihon Keizai Shimbun (a Japanese financial newspaper), SAL(D(“Aum3”), 110) = 74, SAL(D(“Aum”), 200) = 50, SAL(D*(“do”), 110) = 1, and SAL(D*(“do”), 200) = 0, where D*(“do”) is a set of N0 randomly chosen articles from D(“do”) and N0 is the threshold value stated in subsection 1.2.",
        "This strongly suggests that SAL(D(T), s) can discriminate topic-specific words from non-topical words."
      ]
    },
    {
      "heading": "2.3 Optimizing threshold of saliency",
      "text": [
        "Note that SAL(D(T), 0) gives the number of distinct words in D(T), and as s increases to ∞, SAL(D(T), s) becomes a constant function (zero).",
        "If we straightforwardly follow the baseline method, we have to construct the baseline function BSAL(D(T), s) for varying s and test the performance of NormSAL(D(T), s), the normalized SAL(D(T), s).",
        "There are, however, a problem that BSAL(D(T), s) cannot be precisely approximated because SAL(D(T), s) is a discrete-valued function.",
        "By considering the meaning of the baseline function, we can solve the problem of determining the optimal value of saliency parameter s without approximating baseline functions.",
        "That is, since the baseline function is considered as background noise to be offset, the best situation should be that the baseline function is a constant-valued function while SAL(D(T), s) is a non-trivial function (i.e., not a constant function).",
        "If there exists s0 satisfying the condition, SAL(D(T), s0) does not need to be normalized and is reliable itself, and s0 is the optimal parameter.",
        "3 Aum is the name of a religious cult that attacked Tokyo subway with sarin gas in 1995.",
        "SAL(Drand, s)} for Drand and s, where Drand varies over randomly sampled article sets and s varies over several discrete values.",
        "Although BSAL(D(T), s) cannot be precisely approximated by using analytical functions, it can be seen that BSAL(D(T), s) changes from a monotone increasing function to a monotone decreasing function when s is greater than about 110, and the graph of BSAL(D(T), 110) is roughly parallel to the x-axis.",
        "Considering the meaning of baseline functions again, this means that s0 = 110 is the optimal value of saliency and that SAL(D(T), 110) can be used without normalization and is the most effective SAL.",
        "The important thing here is that this procedure to find the optimal value of s can be done automatically because it only requires random sampling of documents and curve fitting.",
        "Section 3 experimentally confirms the superiority of SAL(D(T), s0) as a representativeness measure."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "As in Hisamitsu et al.",
        "(2000), taking topic-word selection for IR navigation into account, we examined the relation between the value of representative measures and a manual classification of words (monograms) extracted from nearly 160,000 articles in the 1996 archive of the Nihon Keizai Shimbun (denoted by D0 later on)."
      ]
    },
    {
      "heading": "3.1 Preparation",
      "text": [
        "We randomly chose 20,000 words from 86,000 words having document frequencies larger than 2 in D0, then randomly chose 2,000 of them and classified these into three groups: (1) class P (positive): topic-specific words which are useful for the navigation of IR, (2) class N (negative): words, such as “do”, not topic-specific and useless for IR navigation, and (3) class U (uncertain): words whose usefulness in IR navigation was either neutral or difficult to judge.",
        "In the classification process, a judge used an IR system called DualNAVI (Niwa et al.",
        "2000) having dual windows one of which shows the titles of retrieved articles and another displays salient words occurring in the articles.",
        "The details of the guideline of classification are stated in Hisamitsu et al.",
        "(2001)."
      ]
    },
    {
      "heading": "3.2 Measures compared in the experiments",
      "text": [
        "Four measures were compared by Hisamitsu et al.",
        "(2000): NormDist(D(T)), NormDIFFNUM(D(T)), tf-idf, and tf(term frequency), where NormDIFFNUM(D(T)) is a normalized version of a measure called DIFFNUM(D(T)), which gives the number of distinct words in D(T).",
        "DIFFNUM is based on the hypothesis that the number of distinct words co-occurring with a representative word is smaller than that with a generic word (Teramoto et al.",
        "1999).",
        "The definition of tf-idf used in the comparison was as follows: ), where T is a term, TF(T) is the term frequency of T, Ntotal is the total number of documents, and N(T) is the number of documents that contain T. We compared these four measures with SAL(D(T), s), varying s."
      ]
    },
    {
      "heading": "3.3 Comparative experiments and results",
      "text": [
        "We compared the ability of each measure to gather class P words.",
        "We randomly sorted the 20,000 words mentioned above, and then compared the result with the results of sorting by other measures.",
        "The comparison was done using the accumulated number of words marked by class P that appeared in the first k (1 ≤ k ≤ 20,000) words.",
        "For simplicity, we use the following notation: Rand(P, k): the accumulated number of class P words appearing in the first k words when random sorting was applied, M(P, k): the accumulated number of class P words appearing in the first k words when sorting was done by measure M,",
        "The values of DP(M, k) and ADP(M, k) are called tf −idf =VTF(T)× log Nttal DP-score and ADP-score, respectively.",
        "For these scores, higher is better.",
        "Figure 3 compares DP(M, k) for 1 ≤ k ≤ 20,000 and Figure 4 compares ADP(M, 5,000), ADP(M, 10, 000), and ADP(M, 20, 000).",
        "Where M varies over {NormDist(D(•)), NormDIFFNUM(D(•)), tf-idf, tf, SAL(D(•), s)}.",
        "These figures shows that SAL(D(T), s0) is overall superior to other measures except NormDist(D(•)).",
        "It is also superior to NormDist(D(•)) for 0≤k≤15,000.",
        "In terms of ADP-scores, SAL(D(T), s0) is superior to all other measures for k=5,000, 10,000, 20,000.",
        "This means that SAL(D(•), s0) is superior to NormDist(D(•)) on the whole, and particularly superior in gathering topic-specific words near the top of the sorting.",
        "Comparison of SAL(D(T), s) for different values of s shows that s = s0 is actually the optimal value."
      ]
    },
    {
      "heading": "3.4 Effect of corpus size",
      "text": [
        "To see the effect of corpus size on the performance of SAL(D(• ), s), we conducted the same kind of experiments that compared NormDist(D(• )) and SAL(D(• ), s) by using different size of corpora D1/2 and D1/4, whose sizes were 1/2 and 1/4 of D0 respectively.",
        "The optimal value of s was determined for each corpus in the same way as stated in subsection 2.3.",
        "The optimal value was around 70 for D1/2 and around 40 for D1/4.",
        "Figure 5 compares DP-scores when D1/2 is used.",
        "Figure 6 compares the same when D1/4 is used.",
        "Figures 5 and 6 show that SAL(D(•), s0) is superior to NormDist(D(•)) for corpora of different sizes.",
        "Judging from the results, we expect that the superiority of SAL(D(•), s0) would be even more apparent for a larger corpus."
      ]
    },
    {
      "heading": "Conclusion",
      "text": [
        "We proposed a novel measure of the representativeness of a term T in a given corpus.",
        "Denoting the words co-occurring with T by D(T), the measure is defined as SAL(D(T), s), the number of words in D(T) whose saliency of occurrences is over a threshold s. This measure embodies the idea that the distribution of words in D(T) should be saliently biased according to that of the whole corpus if T is a representative term.",
        "The saliency of word occurrences is defined by using a combinatorial probability, and the threshold value s is defined automatically so that the baseline function of SAL(D(T), s) does not depend on #D(T), the number of words contained in D(T).",
        "Comparative evaluation clarified that the proposed measure is superior to conventional measures in finding topic-specific words in newspaper archives of different sizes."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to express our gratitude to Prof. Jun-ichi Tsujii of the University Tokyo and Prof. Kyo Kageura of National Institute of Informatics for their insightful comments.",
        "This project is supported in part by the Core Research for Evolutional Science and Technology (CREST) under the auspices of the Japan Science and Technology Corporation."
      ]
    }
  ]
}
