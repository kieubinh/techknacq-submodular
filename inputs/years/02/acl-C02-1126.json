{
  "info": {
    "authors": [
      "David Chiang",
      "Daniel M. Bikel"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1126",
    "title": "Recovering Latent Information in Treebanks",
    "url": "https://aclweb.org/anthology/C02-1126",
    "year": 2002
  },
  "references": [
    "acl-A00-2018",
    "acl-A97-1029",
    "acl-J93-2004",
    "acl-J95-4002",
    "acl-J98-4004",
    "acl-N01-1023",
    "acl-P00-1058",
    "acl-P95-1021",
    "acl-P95-1037",
    "acl-P98-1091",
    "acl-W00-1201",
    "acl-W97-0301"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Many recent statistical parsers rely on a preprocessing step which uses hand-written, corpus-specific rules to augment the training data with extra information.",
        "For example, head-finding rules are used to augment node labels with lexical heads.",
        "In this paper, we provide machinery to reduce the amount of human effort needed to adapt existing models to new corpora: first, we propose a flexible notation for specifying these rules that would allow them to be shared by different models; second, we report on an experiment to see whether we can use Expectation-Maximization to automatically fine-tune a set of handwritten rules to a particular corpus."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Most work in statistical parsing does not operate in the realm of parse trees as they appear in many treebanks, but rather on trees transformed via augmentation of their node labels, or some other transformation (Johnson, 1998).",
        "This methodology is illustrated in Figure 1.",
        "The information included in the node labels’ augmentations may include lexical items, or a node label suffix to indicate the node is an argument and not an adjunct; such extra information may be viewed as latent information, in that it is not directly present in the treebank parse trees, but may be recovered by some means.",
        "The process of recovering this latent information has largely been limited to the hand-construction of heuristics.",
        "However, as is often the case, hand-constructed heuristics may not be optimal or very robust.",
        "Also, the effort required to construct such rules can be considerable.",
        "In both respects, the use of such rules runs counter to the data-driven approach to statistical parsing.",
        "In this paper, we propose two steps to address this problem.",
        "First, we define a new, fairly simple syntax for the identification and transformation of node labels that accommodates a wide variety of node-label augmentations, including all those that",
        "are performed by existing statistical parsers that we have examined.",
        "Second, we explore a novel use of Expectation-Maximization (Dempster et al., 1977) that iteratively reestimates a parsing model using the augmenting heuristics as a starting point.",
        "Specifically, the EM algorithm we use is a variant of the Inside-Outside algorithm (Baker, 1979; Lari and Young, 1990; Hwa, 1998).",
        "The reestimation adjusts the model’s parameters in the augmented parse-tree space to maximize the likelihood of the observed (incomplete) data, in the hopes of finding a better distribution over augmented parse trees (the complete data).",
        "The ultimate goal of this work is to minimize the human effort needed when adapting a parsing model to a new domain."
      ]
    },
    {
      "heading": "2 Background",
      "text": []
    },
    {
      "heading": "2.1 Head-lexicalization",
      "text": [
        "Many of the recent, successful statistical parsers have made use of lexical information or an implicit lexicalized grammar, both for English and, more recently, for other languages.",
        "All of these parsers recover the “hidden” lexicalizations in a treebank and find the most probable lexicalized tree when parsing, only to strip out this hidden information prior to evaluation.",
        "Also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents.",
        "In fact, nearly identical head-lexicalizations were used in the dis",
        "criminative models described in (Magerman, 1995; Ratnaparkhi, 1997), the lexicalized PCFG models in (Collins, 1999), the generative model in (Char-niak, 2000), the lexicalized TAG extractor in (Xia, 1999) and the stochastic lexicalized TAG models in (Chiang, 2000; Sarkar, 2001; Chen and Vijay-Shanker, 2000).",
        "Inducing a lexicalized structure based on heads has a two-pronged effect: it not only allows statistical parsers to be sensitive to lexical information by including this information in the probability model’s dependencies, but it also determines which of all possible dependencies – both syntactic and lexical – will be included in the model itself.",
        "For example, in Figure 2, the nonterminal NP(boy–NN) is dependent on VP(caught–VBD) and not the other way around."
      ]
    },
    {
      "heading": "2.2 Other tree transformations",
      "text": [
        "Lexicalization via head-finding is but one of many possible tree transformations that might be useful for parsing.",
        "As explored thoroughly by Johnson (1998), even simple, local syntactic transformations on training trees for an unlexicalized PCFG model can have a significant impact on parsing performance.",
        "Having picked up on this idea, Collins (1999) devises rules to identify arguments, i.e., constituents that are required to exist on a particular side of a head child constituent dominated by a particular parent.",
        "The parsing model can then probabilistically predict sets of requirements on either side of a head constituent, thereby incorporating a type of subcategorization information.",
        "While the model is augmented to include this subcat-prediction feature, the actual identification of arguments is performed as one of many preprocessing steps on training trees, using a set of rules similar to those used for the identification of heads.",
        "Also, (Collins, 1999) makes use of several other transformations, such as the identification of sub-jectless sentences (augmenting S nodes to become SG) and the augmentation of nonterminals for gap threading.",
        "Xia (1999) combines head-finding with argument identification to extract elementary trees for use in the lexicalized TAG formalism.",
        "Other researchers investigated this type of extraction to construct stochastic TAG parsers (Chiang, 2000; Chen and Vijay-Shanker, 2000; Sarkar, 2001)."
      ]
    },
    {
      "heading": "2.3 Problems with heuristics",
      "text": [
        "While head-lexicalization and other tree transformations allow the construction of parsing models with more data-sensitivity and richer representations, crafting rules for these transformations has been largely an art, with heuristics handed down from researcher to researcher.",
        "What’s more, on top of the large undertaking of designing and implementing a statistical parsing model, the use of heuristics has required a further effort, forcing the researcher to bring both linguistic intuition and, more often, engineering savvy to bear whenever moving to a new treebank.",
        "For example, in the rule sets used by the parsers described in (Magerman, 1995; Ratnaparkhi, 1997; Collins, 1999), the sets of rules for finding the heads of ADJP, ADVP, NAC, PP and WHPP include rules for picking either the rightmost or leftmost FW (foreign word).",
        "The apparently haphazard placement of these rules that pick out FW and the rarity of FW nodes in the data strongly suggest these rules are the result of engineering effort.",
        "Furthermore, it is not at all apparent that tree-transforming heuristics that are useful for one parsing model will be useful for another.",
        "Finally, as is often the case with heuristics, those used in statistical parsers tend not to be data-sensitive, and ironically do not rely on the words themselves."
      ]
    },
    {
      "heading": "3 Rule-based augmentation",
      "text": [
        "In the interest of reducing the effort required to construct augmentation heuristics, we would like a notation for specifying rules for selecting nodes in bracketed data that is both flexible enough to encode the kinds of rule sets used by existing parsers, and intuitive enough that a rule set for a new language can be written easily without knowledge of computer programming.",
        "Such a notation would simplify the task of writing new rule sets, and facilitate experimentation with different rules.",
        "Moreover, rules written in this notation would be interchangeable between different models, so that, ideally, adaptation of a model to a new corpus would be trivial.",
        "We define our notation in two parts: a structure pattern language, whose basic patterns are speci"
      ]
    },
    {
      "heading": "3.1 Structure patterns",
      "text": [
        "Most existing head-finding rules and argument-finding rules work by specifying parent-child relations (e.g., NN is the head of NP, or NP is an argument of VP).",
        "A generalization of this scheme that is familiar to linguists and computer scientists alike would be a context-free grammar with rules of the form A → A1 ··· (Ai)l··· An, where the superscript l specifies that if this rule gets used, the ith child of A should be marked with the label l. However, there are two problems with such an approach.",
        "First, writing down such a grammar would be tedious to say the least, and impossible if we want to handle trees with arbitrary branching factors.",
        "So we can use an extended CFG (Thatcher, 1967), a CFG whose right-hand sides are regular expressions.",
        "Thus we introduce a union operator (∪) and a Kleene star (∗) into the syntax for right-hand sides.",
        "The second problem that our grammar may be ambiguous.",
        "For example, the grammar",
        "could mark with an h either the first or second symbol of YY.",
        "So we impose an ordering on the rules of the grammar: if two rules match, the first one wins.",
        "In addition, we make the ∪ operator noncommuta-tive: α ∪β tries to match α first, andβ only if it does not match α, as in Perl.",
        "(Thus the above grammar would mark the first Y.)",
        "Similarly, α∗ tries to match as many times as possible, also as in Perl.",
        "But this creates a third and final problem: in the grammar X → (YYh ∪ Yh)(YY ∪ Y), it is not defined which symbol of YYY should be marked, that is, which union operator takes priority over the other.",
        "Perl circumvents this problem by always giving priority to the left.",
        "In algebraic terms, concatenation left-distributes over union but does not right-distribute over union in general.",
        "However, our solution is to provide a pair of concatenation operators: >, which gives priority to the left, and <, which gives priority to the right:",
        "where ⊥ is a wildcard pattern which matches any single label (see below).",
        "Rule (3) mark with an h the rightmost VB child of a VP, whereas rule (4) marks the leftmost VB.",
        "This is because the Kleene star always prefers to match as many times as possible, but in rule (3) the first Kleene star’s preference takes priority over the last’s, whereas in rule (4) the last Kleene star’s preference takes priority over the first’s.",
        "Consider the slightly more complicated examples:",
        "Rule (5) marks the leftmost child which is either a VB or a MD, whereas rule (6) marks the leftmost VB if any, or else the leftmost MD.",
        "To see why this so, consider the string MD VB X.",
        "Rule (5) would mark the MD as h, whereas rule (6) would mark the VB.",
        "In both rules VB is preferred over MD, and symbols to the left over symbols to the right, but in rule (5) the leftmost preference (that is, the preference of the last Kleene star to match as many times as possible) takes priority, whereas in rule (6) the preference for VB takes priority."
      ]
    },
    {
      "heading": "3.2 Label patterns",
      "text": [
        "Since nearly all treebanks have complex nonterminal alphabets, we need a way of concisely specifying classes of labels.",
        "Unfortunately, this will necessarily vary somewhat across treebanks: all we can define that is truly treebank-independent is the ⊥ pattern, which matches any label.",
        "For Penn Treebank II style annotation (Marcus et al., 1993), in which a nonterminal symbol is a category together with zero or more functional tags, we adopt the following scheme: the atomic pattern a matches any label with category a or functional tag a; moreover, we define Boolean operators h, v, and -,.",
        "Thus NP n -,ADV matches NP–SBJ but not NP–ADV.1"
      ]
    },
    {
      "heading": "3.3 Summary",
      "text": [
        "Using the structure pattern language and the label pattern language together, one can fully encode the head/argument rules used by Xia (which resemble (5) above), and the family of rule sets used by Black, Magerman, Collins, Ratnaparkhi, and others (which resemble (6) above).",
        "In Collins’ version of the head rules, NP and PP require special treatment, but these can be encoded in our notation as well."
      ]
    },
    {
      "heading": "4 Unsupervised learning of augmentations",
      "text": [
        "In the type of approach we have been discussing so far, handwritten rules are used to augment the training data, and this augmented training data is then used to train a statistical model.",
        "However, if we train the model by maximum-likelihood estimation, the estimate we get will indeed maximize the likelihood of the training data as augmented by the handwritten rules, but not necessarily that of the training data itself.",
        "In this section we explore the possibility of training a model directly on unaugmented data.",
        "A generative model that estimates P(S, T, T+) (where T+ is an augmented tree) is normally used for parsing, by computing the most likely (T, T+) for a given S. But we may also use it for augmenting trees, by computing the most likely T+ for a given sentence-tree pair (S, T).",
        "From the latter perspective, because its trees are unaugmented, a treebank is a corpus of incomplete data, warranting the use of unsupervised learning methods to reestimate a model that includes hidden parameters.",
        "The approach we take below is to seed a parsing model using handwritten rules, and then use the Inside-Outside algorithm to reestimate its parameters.",
        "The resulting model, which locally maximizes the likelihood of the unaugmented training data, can then be used in two ways: one might hope that as a parser, it would parse more accurately than a model which only maximizes the likelihood of training data augmented by handwritten rules; and that as a tree-augmenter, it would augment trees in a more data-sensitive way than handwritten rules."
      ]
    },
    {
      "heading": "4.1 Background: tree adjoining grammar",
      "text": [
        "The parsing model we use is based on the stochastic tree-insertion grammar (TIG) model described 1 Note that unlike the noncommutative union operator u, the disjunction operator v has no preference for its first argument.",
        "by Chiang (2000).",
        "TIG (Schabes and Waters, 1995) is a weakly-context free restriction of tree adjoining grammar (Joshi and Schabes, 1997), in which tree fragments called elementary trees are combined by two composition operations, substitution and adjunction (see Figure 3).",
        "In TIG there are certain restrictions on the adjunction operation.",
        "Chiang’s model adds a third composition operation called sister-adjunction (see Figure 3), borrowed from D-tree substitution grammar (Rambow et al., 1995).2 There is an important distinction between derived trees and derivation trees (see Figure 3).",
        "A derivation tree records the operations that are used to combine elementary trees into a derived tree.",
        "Thus there is a many-to-one relationship between derivation trees and derived trees: every derivation tree specifies a derived tree, but a derived tree can be the result of several different derivations.",
        "The model can be trained directly on TIG derivations if they are available, but corpora like the Penn Treebank have only derived trees.",
        "Just as Collins uses rules to identify heads and arguments and thereby lexicalize trees, Chiang uses nearly the same rules to reconstruct derivations: each training example is broken into elementary trees, with each head child remaining attached to its parent, each argument broken into a substitution node and an initial root, and each adjunct broken off as a modifier auxiliary tree.",
        "However, in this experiment we view the derived trees in the Treebank as incomplete data, and try to reconstruct the derivations (the complete data) using the Inside-Outside algorithm."
      ]
    },
    {
      "heading": "4.2 Implementation",
      "text": [
        "The expectation step (E-step) of the Inside-Outside algorithm is performed by a parser that computes all possible derivations for each parse tree in the training data.",
        "It then computes inside and outside probabilities as in Hwa’s experiment (1998), and uses these to compute the expected number of times each event occurred.",
        "For the maximization step (M-step), we obtain a maximum-likelihood estimate of the parameters of the model using relative-frequency es2 The parameters for sister-adjunction in the present model differ slightly from the original.",
        "In the original model, all the modifier auxiliary trees that sister-adjoined at a particular position were generated independently, except that each sister-adjunction was conditioned on whether it was the first at that position.",
        "In the present model, each sister-adjunction is conditioned on the root label of the previous modifier tree.",
        "timation, just as in the original experiment, as if the expected values for the complete data were the training data.",
        "Smoothing presents a special problem.",
        "There are several several backoff levels for each parameter class that are combined by deleted interpolation.",
        "Let 01, 02 and 03 be functions from full history contexts Y to less specific contexts at levels 1, 2 and 3, respectively, for some parameter class with three backoff levels (with level 1 using the most specific contexts).",
        "Smoothed estimates for parameters in this class are computed as follows:",
        "where ei is the estimate of p(X |Oi(Y)) for some future context X, and the Ai are computed by the formula found in (Bikel et al., 1997), modified to use the multiplicative constant 5 found in the similar formula of (Collins, 1999):",
        "where di is the number of occurrences in training of the context Oi(Y) (and d0 = 0), and ui is the number of unique outcomes for that context seen in training.",
        "There are several ways one might incorporate this smoothing into the reestimation process, and we chose to depart as little as possible from the original smoothing method: in the E-step, we use the smoothed model, and after the M-step, we use the original formula (7) to recompute the smoothing weights based on the new counts computed from the E-step.",
        "While simple, this approach has two important consequences.",
        "First, since the formula for the smoothing weights intentionally does not maximize the likelihood of the training data, each iteration of reestimation is not guaranteed to increase the",
        "likelihood of the training data.",
        "Second, reestimation tends to increase the size of the model in memory, since smoothing gives nonzero expected counts to many events which were unseen in training.",
        "Therefore, since the resulting model is quite large, if an event at a particular point in the derivation forest has an expected count below 10-15, we throw it out."
      ]
    },
    {
      "heading": "4.3 Experiment",
      "text": [
        "We first trained the initial model on sections 02–21 of the WSJ corpus using the original head rules, and then ran the Inside-Outside algorithm on the same data.",
        "We tested each successive model on some held-out data (section 00), using a beam width of 10-4, to determine at which iteration to stop.",
        "The F-measure (harmonic mean of labeled precision and recall) for sentences of length ≤ 100 for each iteration is shown in Figure 4.",
        "We then selected the ninth reestimated model and compared it with the initial model on section 23 (see Figure 7).",
        "This model did only marginally better than the initial model on section 00, but it actually performs worse than the initial model on section 23.",
        "One explanation is that the",
        "head rules, since they have been extensively fine-tuned, do not leave much room for improvement.",
        "To test this, we ran two more experiments.",
        "The second experiment started with a simplified rule set, which simply chooses either the leftmost or rightmost child of each node as the head, depending on the label of the parent: e.g., for VP, the leftmost child is chosen; for NP, the rightmost child is chosen.",
        "The argument rules, however, were not changed.",
        "This rule set is supposed to represent the kind of rule set that someone with basic familiarity with English syntax might write down in a few minutes.",
        "The reestimated models seemed to improve on this simplified rule set when parsing section 00 (see Figure 5); however, when we compared the 30th reestimated model with the initial model on section 23 (see Figure 7), there was no improvement.",
        "The third experiment was on the Chinese Treebank, starting with the same head rules used in (Bikel and Chiang, 2000).",
        "These rules were originally written by Xia for grammar development, and although we have modified them for parsing, they have not received as much fine-tuning as the English rules have.",
        "We trained the model on sections 001– 270 of the Penn Chinese Treebank, and reestimated it on the same data, testing it at each iteration on sections 301–325 (Figure 6).",
        "We selected the 38th reestimated model for comparison with the initial model on sections 271–300 (Figure 7).",
        "Here we did observe a small improvement: an error reduction of 3.4% in the F-measure for sentences of length ≤ 40."
      ]
    },
    {
      "heading": "4.4 Discussion",
      "text": [
        "Our hypothesis that reestimation does not improve on the original rule set for English because that rule set is already fine-tuned was partially borne out by the second and third experiments.",
        "The model trained with a simplified rule set for English showed improvement on held-out data during reestimation, but showed no improvement in the final evaluation; however, the model trained on Chinese did show a small improvement in both.",
        "We are uncertain as to why the gains observed during the second experiment were not reflected in the final evaluation, but based on the graph of Figure 5 and the results on Chinese, we believe that reestimation by EM can be used to facilitate adaptation of parsing models to new languages or corpora.",
        "It is possible that our method for choosing smoothing weights at each iteration (see §4.2) is causing some interference.",
        "For future work, more careful methods should be explored.",
        "We would also like to experiment on the parsing model of Collins (1999), which, because it can recombine smaller structures and reorder subcategorization frames, might open up the search space for better reestimation."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "Even though researchers designing and implementing statistical parsing models have worked in the methodology shown in Figure 1 for several years now, most of the work has focused on finding effective features for the model component of the methodology, and on finding effective statistical techniques for parameter estimation.",
        "However, there has been much behind-the-scenes work on the actual transformations, such as head finding, and most of this work has consisted of hand-tweaking existing heuristics.",
        "It is our hope that by introducing this new syntax, less toil will be needed to write nonterminal augmentation rules, and that human effort will be lessened further by the use of unsupervised methods such as the one presented here to produce better models for parsing and tree augmentation.",
        "plified rule set.",
        "LR = labeled recall, LP = labeled precision; CB = average crossing brackets, 0 CB = no crossing brackets, < 2 CB = two or fewer crossing brackets.",
        "All figures except CB are percentages."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research was supported in part by NSF grant SBR-89-20230.",
        "We would like to thank Anoop Sarkar, Dan Gildea, Rebecca Hwa, Aravind Joshi, and Mitch Marcus for their valuable help."
      ]
    }
  ]
}
