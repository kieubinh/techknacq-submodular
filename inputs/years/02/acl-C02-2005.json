{
  "info": {
    "authors": [
      "Tibor Kiss",
      "Jan Strunk"
    ],
    "book": "International Conference on Computational Linguistics – Project Notes",
    "id": "acl-C02-2005",
    "title": "Scaled Log Likelihood Ratios for the Detection of Abbreviations in Text Corpora",
    "url": "https://aclweb.org/anthology/C02-2005",
    "year": 2002
  },
  "references": [
    "acl-J93-1003",
    "acl-J97-2002"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe a language-independent, flexible, and accurate method for the detection of abbreviations in text corpora.",
        "It is based on the idea that an abbreviation can be viewed as a collocation, and can be identified by using methods for collocation detection such as the log likelihood ratio.",
        "Although the log likelihood ratio is known to show a good recall, its precision is poor.",
        "We employ scaling factors which lead to a strong improvement of precision.",
        "Experiments with English and German corpora show that abbreviations can be detected with high accuracy."
      ]
    },
    {
      "heading": "Introduction",
      "text": [
        "The detection of abbreviations in a text corpus forms one of the initial steps in tokenization (cf. Liberman/Church 1992).",
        "This is not a trivial task, since a tokenizer is confronted with ambiguous tokens.",
        "For English, e.g., Palmer/Hearst (1997:241) report that periods () can be used as decimal points, abbreviation marks, end-of-sentence marks, and as abbreviation marks at the end of a sentence.",
        "In this paper, we will concentrate on the classification of the period as either an abbreviation mark or a punctuation mark.",
        "We assume that an abbreviation can be viewed as a collocation consisting of the abbreviated word itself and the following .",
        "In case of an abbreviation, we expect the occurrence of following the previous ‘word’ to be more likely than in a case of an end-of-sentence punctation.",
        "The starting point is the log likelihood ratio (log , Dunning 1993).",
        "If the null hypothesis (H0) – as given in (1) – expresses that the occurrence of a period is independent of the preceeding word, the alternative hypothesis (HA) in (2) assumes that the occurrence of a period is not independent of the occurrence of the word preceeding it.",
        "(1) H0: P( |w) = p = P( |¬w) (2) HA: P( |w) = p1 6 p2 = P( |¬w)",
        "The log of the two hypotheses is given in (3).",
        "Its distribution is asymptotic to a 2 distribution and can hence be used as a test statistic (Dunning 1993).",
        "L(HYL HA)"
      ]
    },
    {
      "heading": "1 Problems for an unscaled log approach",
      "text": [
        "Although log identifies collocations much better than competing approaches (Dunning 1993) in terms of its recall, it suffers from its relatively poor precision rates.",
        "As is reported in Evert et al.",
        "(2000), log is very likely to detect all collocations contained in a corpus, but as more collocations are detected with decreasing log , the number of wrongly classified items increases.",
        "The table in (4) is a sample from the Wall Street Journal (1987).1 According to the asymptotic 2 distribution all the pairs given in (4) count as candidates for abbreviations.",
        "Some of the ‘true’ abbreviations are either ranked lower than non-abbreviations or receive the same log values as non-abbreviations.",
        "Candidates which should not be analyzed as abbreviations are indicated in boldface.",
        "(4) Candidates for abbreviations from WSJ 1 As distributed by ACL/DCI.",
        "We have removed all annotations from the corpora before processing them.",
        "(3) log A = 2 log (1987)",
        "In the present sample, the likelihood of a period being dependent on the word preceeding it should be 99.99 % if its log is higher than 7.88.2 But, as has been illustrated in (4), even this figure leads to a problematic classification of the candidates, since many non-abbreviations are wrongly classified as being abbreviations.",
        "This means that an unmodified log approach to the detection of abbreviations will produce many errors and thus cannot be employed."
      ]
    },
    {
      "heading": "2 Scaling log likelihood ratios",
      "text": [
        "Since a pure log approach falsely classifies many non-abbreviations as being abbreviations, we use log as a basic ranking which is scaled by several factors.",
        "These factors have been experimentally developed by measuring their effect in terms of precision and recall on a training corpus from WSJ.3 The result of the scaling operation is a much more compact ranking of the true positives in the corpus.",
        "The effect of the scaling methods on the data presented in (4) are illustrated in (5).",
        "By applying the scaling factors, the asymptotic relation to the 2 distribution cannot be retained.",
        "The threshold value of the classification is hence no longer determined by the 2 distribution, but determined on the basis of the classification results derived from the training corpus.",
        "The scaling factors, once they have been determined on the basis of the training corpus, have not been modified any further.",
        "In this sense, the method described here can be characterized as a corpus-filter method, where a given corpus is used to",
        "filter the initial results (cf. Grefenstette 1999:128f.",
        ").",
        "(5) Result of applying scaling factors",
        "In the present setting, applying the scaling factors to the training corpus has led to to a threshold value of 1.0.",
        "Hence, a value above 1.0 allows a classification of a given pair as an abbreviation, while a value below that leads to an exclusion of the candidate.",
        "An ordering of the candidates from table (5) is given in (6), where the threshold is indicated through the dashed line.",
        "(6) Ranking according to S(log )",
        "As can be witnessed in (6), the scaling methods are not perfect.",
        "In particular, ounces is still wrongly considered as an initial element of an abbreviation, poiting to a weakness of the approach which will be discussed in section 5."
      ]
    },
    {
      "heading": "3 The scaling factors",
      "text": [
        "We have employed three different scaling factors, as given in (7), (8), and (9).4 Each scaling",
        "factor is applied to the log of a candidate pair.",
        "The weighting factors are formulated in such a way that allows a tension between them (cf. section 3.4).",
        "The effect of this tension is that an increase following from one factor may be cancelled out or reduced by a decrease following from the application of another factor, and vice versa."
      ]
    },
    {
      "heading": "3.1 Ratio of occurrence: S1",
      "text": [
        "By employing scaling factor (7), the log is additionally weighted by the ranking which is determined by the occurrence of pairs of the form (word, ) in relation to pairs of the form (word, ¬).",
        "If events of the second type are either rare or at least lower than events of the first type, the scaling factor leads to an increase of the initial log value.5"
      ]
    },
    {
      "heading": "3.2 Relative difference: S2",
      "text": [
        "The second scaling factor is a variation of the relative diference.",
        "Depending on the figures of C(word, ) and C(word, ¬), its value can be either positive, negative, or 0.",
        "(10) If C(word, ) > C(word, ¬), 0 < S2 < 1.",
        "(11) If C(word, ) = C(word, ¬), S2 = 0.",
        "(12) If C(word, ) < C(word, ¬), 1 S2 < 0.",
        "If C(word, ¬) = 0, S2 reaches a maximum of 1.",
        "Hence, S2 in general leads to a reduction of the initial log value.",
        "S2 also has a significant effect on log if the occurrence of word with equals the occurrence of word without .",
        "In this case, S2 will be 0.",
        "Since the log values are multiplied with each scaling factor, a value of 0 for S2 will lead to a value of 0 throughout.",
        "Hence the pair (word, ) will be excluded from being an abbreviation.",
        "This move seems extremely plausible: if 1999:172f.).",
        "5 If C(word, ¬) = 0, S1(log ) = log X • ec(-14.",
        "), reflecting an even higher likelihood that the pair should actually count as an abbreviation.",
        "word occurs approximately the same time with and without a following , it is quite unlikely that the pair (word, ) forms an abbreviation.6 Similarly, the value of S2 will be negative if the number of occurrences of word without is higher than the number of occurrences of word with .",
        "Again, the resulting decrease reflects that the pair (word, ) is even more unlikely to be an abbreviation.",
        "Both the relative difference (S2) and the ratio of occurrence (S1) allow a scaling that abstracts away from the absolute figure of occurrence, which strongly influences log .7"
      ]
    },
    {
      "heading": "3.3 Length of abbreviations: S3",
      "text": [
        "Scaling factor (9), finally, leads to a reduction of log depending on the length of the word which preceeds a period.",
        "This scaling factor follows the idea that an abbreviation is more likely to be short."
      ]
    },
    {
      "heading": "3.4 Interaction of scaling factors",
      "text": [
        "As was already mentioned, the scaling factors can interact with each other.",
        "Consequently, an increase by a factor may be reduced by another one.",
        "This can be illustrated with the pair (U.N, •) in (6).",
        "The application of the scaling factors does not change the value as the initial log X calculation.",
        "Since the length of word actually equals its occurrence together with a , and since U.N never occurs without a trailing , S1 leads to an increase by a factor of e3, which however is fully compensated by the application of S3."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "The scaling methods described in section 3 have been applied to test corpora from English (Wall Street Journal, WSJ) and German (Neue Zürcher Zeitung, NZZ).",
        "The scaled log was calculated for all pairs of the form (word, ).",
        "The test corpora were annotated in the following fashion: If the value was higher than 1, the tag <A> was assigned to the pair.",
        "All other candidates were tagged as <S>.8 The automatically classified corpora were compared with their hand-tagged references.",
        "We have chosen two different types of test corpora: First, we have used two test corpora of an approximate size of 2 and 6 MB, respectively.",
        "The WSJ corpus contained 19,776 candidates of the form (word, ); the NZZ corpus contained 37,986 such pairs.",
        "Second, we have tried to determine the sensitivity of the present approach to data sparseness.",
        "Hence, the approach was applied to ten individual articles from each WSJ and NZZ.",
        "For English, these articles contained between 7 and 26 candidate pairs, for German the articles comprised between 16 and 52 pairs.",
        "The reference annotation allowed the determination of a baseline which determines the percentage of correctly classified end-of-sentence marks if each pair (word, ) is classified as an end-of-sentence mark.",
        "The baseline varies from corpus to corpus, depending on a variety of factors (cf. Palmer/Hearst 1997).",
        "In the following tables, we have reported two measures: first, the error rate, which is defined in (15), and second, the F measure (cf. van Rijsbergen 1979:174), which is"
      ]
    },
    {
      "heading": "4.1 Results of first experiment",
      "text": [
        "The results of the classification process for the larger files are reported in table (17).",
        "F(B) and F(S) are the F measure of the baseline, and the present approach, respectively.",
        "E(B) is the error rate of the baseline, and E(S) is the error rate of the scaled log approach.",
        "(17) Results of classification for large files",
        "As (17) shows, the application of the scaled log a, leads to significant improvements for both files.",
        "In particular, the error rate has dropped from over 30 % to 0.6 % in the WSJ corpus.",
        "For both files, the accuracy is beyond 99 %."
      ]
    },
    {
      "heading": "4.2 Results of second experiment",
      "text": [
        "The results of the second experiment are reported in table (18) for the articles from the Wall Street Journal, and in table (19) for the articles from the Neue Zürcher Zeitung.",
        "The scaled log X approach generally outperforms the baseline approach.",
        "This is reflected in the F measure as well as in the error rate, which is reduced to a third.",
        "For one article (WSJ _1) the present approach actually performs below the baseline (cf. section 5).",
        "8 A tokenizer should treat pairs which have been annotated with <A> as single tokens, while tokens which have been annotated with <S> should be treated as two separate tokens.",
        "Three-dot-ellipses are currently not considered.",
        "Also <A><S> tags are not considered in the experiments (cf. section 5).",
        "9 Following this baseline, we assume that correctly classified end-of-sentence marks count as true positives in the evaluations.",
        "10 Manning/Schütze (1999:269) criticize the use of accuracy and error if the number of true negatives – C(<A> <A>) in the present case – is large.",
        "Since the number of true negatives is small here, accuracy and error escape this criticism.",
        "11 C(<X> <Y>) is the number of X which have been wrongly classified as Y.",
        "In (16), P stands for the precision, and R for the recall.",
        "(18) Results of classification for single articles from WSJ",
        "In general, the articles from NZZ contained fewer abbreviations, which is reflected in the comparatively high baseline scores.",
        "Still, the present approach is able to outperform the baseline approach.",
        "Particularly noteworthy are the articles NZZ_1, NZZ_4, and NZZ_8, where the error rate is reduced to 0.",
        "In general, the error rate has been reduced to a fifth."
      ]
    },
    {
      "heading": "5 Weaknesses and future steps",
      "text": [
        "We have noted in section 2 that the scaling factors do not lead to a perfect classification.",
        "This is particularly reflected in the application of S(log ) to WSJ _1 and NZZ_7, which actually show the same problem: In the training corpus, ounces was always followed by .",
        "In WSJ _1, the word said was always followed by , and this also happened in NZZ _7 for kann.",
        "Without the inclusion of additional metrics, non-abbreviations which exclusively occur at the end of sentences are wrongly classified.",
        "The table in (20) illustrates, however, that the error rate for false negatives drops significantly if plausible corpus sizes are considered.",
        "We have also ignored abbreviation occuring at the end of the sentence.",
        "The next step will be to integrate methods for the detection of abbreviations at the end of the sentence, e.g. by integrating additional phonotactic information, and also to cover the problematic cases reported above."
      ]
    },
    {
      "heading": "Conclusion",
      "text": [
        "We have presented an accurate and comparatively simple method for the detection of abbreviations which makes use of scaled log likelihood ratios.",
        "Experiments have shown that the method works well with large files and also with small samples with sparse data.",
        "We expect further improvements once additional classification schemata have been integrated."
      ]
    }
  ]
}
