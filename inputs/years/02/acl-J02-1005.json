{
  "info": {
    "authors": [
      "Mark Johnson"
    ],
    "book": "Computational Linguistics",
    "id": "acl-J02-1005",
    "title": "The DOP Estimation Method Is Biased and Inconsistent",
    "url": "https://aclweb.org/anthology/J02-1005",
    "year": 2002
  },
  "references": [],
  "sections": [
    {
      "text": [
        "A data-oriented parsing or DOP model for statistical parsing associates fragments of linguistic representations with numerical weights, where these weights are estimated by normalizing the empirical frequency of each fragment in a training corpus (see Bod [1998] and references cited therein).",
        "This note observes that this estimation method is biased and inconsistent; that is, the estimated distribution does not in general converge on the true distribution as the size of the training corpus increases."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The data-oriented parsing or DOP approach to statistical natural language analysis has attracted considerable attention recently and has been used to produce statistical language models based on various kinds of linguistic representation, as described in Bod (1998).",
        "These models are based on the intuition that statistical generalizations about natural languages should be stated in terms of “chunks” or “fragments” of linguistic representations.",
        "Linguistic representations are produced by combining these fragments, but unlike in stochastic models such as Probabilistic Context-Free Grammars, a single linguistic representation may be generated by several different combinations of fragments.",
        "These fragments may be large, permitting DOP models to describe nonlocal dependencies.",
        "Usually the fragments used in a DOP model are themselves obtained from a training corpus of linguistic representations.",
        "For example, in DOP1 or Tree-DOP the fragments are typically all the connected multinode trees that appear as subgraphs of any tree in the training corpus.",
        "This note shows that the estimation procedure standardly used to set the parameters or fragment weights of a DOP model (see, for example, Bod [1998]) is biased and inconsistent.",
        "This means that as sample size increases, the corresponding sequence of probability distributions estimated by this procedure does not converge to the true distribution that generated the training data.",
        "Consistency is usually regarded as the minimal requirement any estimation method must satisfy (Breiman 1973; Shao 1999), and the inconsistency of the standard DOP estimation method suggests it may be worth looking for other estimation methods.",
        "Note that while the bulk of DOP research uses the estimation procedure studied here, recently there has been research that has used other estimators for DOP models (Bonnema, Buying, and Scha 1999; Bod 2000), and it would be interesting to investigate the statistical properties of these estimators as well."
      ]
    },
    {
      "heading": "2. DOP1 Models",
      "text": [
        "For simplicity, this note focuses on DOP1 or Tree-DOP models, in which linguistic representations are phrase structure trees, but the results carry over to more complex models that use attribute-value feature structure representations such as LFG-DOP.",
        "The fragments used in DOP1 are multinode trees whose leaves may be labeled with nonterminals as well as terminals.",
        "A derivation starts with a fragment whose root is labeled with the start symbol, and it proceeds by substituting a fragment for the leftmost nonterminal leaf under the constraint that the fragment’s root node and the leaf node have the same label.",
        "The derivation terminates when there are no nonterminal leaves.",
        "Figure 1 depicts three different derivations that yield the same tree.",
        "The fragments used in these derivations could have been obtained from a training corpus of trees that contains trees for examples such as Sasha likes motorcycles, Alex eats pizza, and so on.",
        "In a DOP model, each fragment is associated with a real-valued weight, and the weight of a derivation is the product of the weights of the tree fragments involved.",
        "The weight of a representation is the sum of the weights of its derivations, and a probability distribution over linguistic representations is obtained by normalizing the representations’ weights.1 Given a combinatory operation and a fixed set of fragments, a DOP model is a parametric model where the fragment weights are the parameters.",
        "In DOP1 and DOP models based on it, the weight associated with a fragment is estimated as follows (Bod 1998).",
        "For each tree fragment f, let n (f ) be the number of times it appears in the training corpus, and let F be the set of all tree fragments with the same root as f .",
        "Then the weight w (f) associated with f is",
        "This relative-frequency estimation method has the advantage of simplicity, but as shown in the following sections, it is biased and inconsistent.",
        "1 In DOP1 and similar models, it is not necessary to normalize the representations’ weights if the fragments’ weights are themselves appropriately normalized.",
        "Depictions of three different derivations of the same tree representation of Alex likes pizza, with arrows indicating the sites of tree fragment substitutions.",
        "3.",
        "Bias and Inconsistency",
        "Bias and inconsistency are usually defined for parametric estimation procedures in terms that are not quite appropriate for evaluating the DOP estimation procedure, but their standard definitions (see Shao [1999] for a textbook exposition) will serve as the basis for the definitions adopted below.",
        "Let O be a vector space of real-valued parameters, so that Po, 0 E O is a probability distribution.",
        "In the DOP1 case, O would be the space of all possible weight assignments to fragments.",
        "An estimator 0 is a function from a vector x of n samples to a parameter value O(x) E O, and an estimation procedure specifies an estimator on for each sample size n. Let X be a vector of n independent random variables distributed according to Po.",
        "for some 0* E O.",
        "Then O(X) is also a random variable, ranging over parameter vectors O, with an expected value Eo.",
        "(O(X)).",
        "The bias of the estimator 0 at 0* is the difference Eo.",
        "(O(X)) − 0* between its expected value and the “true” parameter value 0* that determines the distribution X.",
        "A biased estimator is one with nonzero bias for some value of 0*.",
        "A loss function L is a function from pairs of parameter vectors to the nonnegative reals.",
        "Given a sample x drawn from the distribution 0*, L(B*, O(x)) measures the “cost” or the “loss” incurred by the error in the estimate O(x) of 0*.",
        "For example, a standard loss function is the Euclidean distance metric L(B*,O(x)) = 110(X) − 0* 112 (note that the results below do not depend on this choice of loss function).",
        "The risk of an estimator 0 at 0* is its expected loss E0.(L(0*,O(X)).",
        "An estimation procedure is consistent if and only if the limit of the risk of On is 0 as n – > oo for all 0*.",
        "(There are various different notions of consistency depending on how convergence is defined; however, the DOP1 estimator is not consistent with respect to any of the standard definitions of consistency.)",
        "Strictly speaking, the standard definitions of bias and loss function are not applicable to DOP estimation because there can be two distinct parameter vectors 01, 02 for which Po1 = Po2 even though 01 =# 02 (such a case is presented in the next section).",
        "Thus it is more natural to define bias and loss in terms of the probability distributions that the parameters specify, rather than in terms of the parameters themselves.",
        "In this paper, an estimator is unbiased iff PE.. (O(X)) = Po* for all 0*; that is, its expected parameter estimate specifies the same distribution as the true parameters.",
        "Similarly, the loss function is the mean squared difference between the “true” and estimated distributions; that is, if Ω is the event space (in DOP1, the space of all phrase structure trees), then",
        "As before, the risk of an estimator is its expected loss, and an estimation procedure is consistent iff the limit of the expected loss is 0 as n – > oo."
      ]
    },
    {
      "heading": "4. A DOP1 Example",
      "text": [
        "This section presents a simple DOP1 model that only generates two trees with probability p and 1 − p, respectively.",
        "The DOP relative frequency estimator is applied to a random sample of size n drawn from this population to estimate the tree weight parameters for the model.",
        "The bias and inconsistency of the estimator follow from the fact that these estimated parameters generate the trees with probabilities different from p and 1 − p. The trees used and their DOP1 fragments are shown in Figure 2.",
        "The trees t1, t2 and their associated fragments f1, ... ,f7 in the DOP1 model.",
        "Suppose the “true” weights for the fragments f1, ... ,f7 are 0 except for the following fragments:",
        "Then Pw.",
        "(t1) = p and Pw.",
        "(t2) = 1 – p. (Note that exactly the same tree distribution could be obtained by setting w* (f1) = p and w* (f5) = 1 – p and all other weights to 0; thus the tree weights are not identifiable.)",
        "Then in a sample of size n drawn from the distribution Pw.",
        "the expected number of occurrences of tree t1 is np and the expected number of occurrences of tree t2 is n(1 – p).",
        "Thus the expected number of occurrences of the fragments in a sample of size n is",
        "The value of PEw(t1) as a function of Pw- (t1) = p. The identity function p is also plotted for comparison.",
        "Figure 3 shows how PE(ˆw)(t1) varies as a function of Pw.",
        "(t1) = p. The difference PE(ˆw) (t1) − p reaches a maximum value of approximately 0.17 at p = ,r2- – 1.",
        "Thus except for p = 0 and p = 1, PE(ˆw) =# Pw.",
        "; that is, the DOP1 estimator is biased.",
        "Further, note that the estimated distribution PE(ˆw) does not approach Pw.",
        "as the sample size increases, so the expected loss does not converge to 0 as the sample size n increases.",
        "Thus the DOP1 estimator is also inconsistent."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "The previous section showed that the relative frequency estimation procedure used in DOP1 and related DOP models is biased and inconsistent.",
        "Bias is not necessarily a defect in an estimator, and Geman, Bienenstock, and Doursat (1992) argue that it may be desirable to trade variance for bias.",
        "However, inconsistency is usually viewed as a fatal flaw of an estimator.",
        "Nevertheless, excellent empirical results have been claimed for the DOP1 model, so perhaps there are some circumstances in which inconsistent estimators perform well.",
        "Undoubtedly there are other estimation procedures for DOP models that are unbiased and consistent.",
        "For example, maximum likelihood estimators are unbiased and consistent across a wide class of models, including, it would seem, all reasonable DOP models (Shao 1999).",
        "Bod (2000) describes a procedure for maximum likelihood estimation of DOP models based on an Expectation Maximization–like algorithm.",
        "In addition, Rens Bod (personal communication) points out that because the set of fragments in a DOP1 model includes all of the trees in the training corpus, the maximum likelihood estimator will assign the training corpus trees their empirical frequencies, and assign 0 weight to all other trees.",
        "However, this seems to be an overlearning problem rather than a problem with maximum likelihood estimation per se, and standard methods, such as cross-validation or regularization, would seem in principle to be ways to avoid such overlearning.",
        "Obviously, empirical investigation would be useful here."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "I would like to thank Rens Bod, Michael Collins, Eugene Charniak, David McAllester, and the anonymous reviewers for their excellent advice."
      ]
    }
  ]
}
