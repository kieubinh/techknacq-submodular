{
  "info": {
    "authors": [
      "Michael John Collins",
      "Nigel Duffy"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P02-1034",
    "title": "New Ranking Algorithms for Parsing and Tagging: Kernels Over Discrete Structures, And the Voted Perceptron",
    "url": "https://aclweb.org/anthology/P02-1034",
    "year": 2002
  },
  "references": [
    "acl-A00-2018",
    "acl-J02-1005",
    "acl-J93-2004",
    "acl-P01-1010",
    "acl-P02-1062",
    "acl-P99-1069",
    "acl-W02-1001",
    "acl-W96-0213",
    "acl-W96-0214",
    "acl-W98-1118"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm.",
        "We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the “all sub-trees” (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.",
        "We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and named-entity extraction from web data."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The perceptron algorithm is one of the oldest algorithms in machine learning, going back to (Rosen-blatt 1958).",
        "It is an incredibly simple algorithm to implement, and yet it has been shown to be competitive with more recent learning methods such as support vector machines – see (Freund & Schapire 1999) for its application to image classification, for example.",
        "This paper describes how the perceptron and voted perceptron algorithms can be used for parsing and tagging problems.",
        "Crucially, the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the “all subtrees” (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.",
        "It might seem paradoxical to be able to efficiently learn and apply a model with an exponential number of features.",
        "The key to our algorithms is the 1 Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper.",
        "“kernel” trick ((Cristianini and Shawe-Taylor 2000) discuss kernel methods at length).",
        "We describe how the inner product between feature vectors in these representations can be calculated efficiently using dynamic programming algorithms.",
        "This leads to polynomial time2 algorithms for training and applying the perceptron.",
        "The kernels we describe are related to the kernels over discrete structures in (Haussler 1999; Lodhi et al.",
        "2001).",
        "A previous paper (Collins and Duffy 2001) showed improvements over a PCFG in parsing the ATIS task.",
        "In this paper we show that the method scales to far more complex domains.",
        "In parsing Wall Street Journal text, the method gives a 5.1% relative reduction in error rate over the model of (Collins 1999).",
        "In the second domain, detecting named-entity boundaries in web data, we show a 15.6% relative error reduction (an improvement in F-measure from 85.3% to 87.6%) over a state-of-the-art model, a maximum-entropy tagger.",
        "This result is derived using a new kernel, for tagged sequences, described in this paper.",
        "Both results rely on a new approach that incorporates the log-probability from a baseline model, in addition to the “all-fragments” features."
      ]
    },
    {
      "heading": "2 Feature–Vector Representations of Parse Trees and Tagged Sequences",
      "text": [
        "This paper focuses on the task of choosing the correct parse or tag sequence for a sentence from a group of “candidates” for that sentence.",
        "The candidates might be enumerated by a number of methods.",
        "The experiments in this paper use the top n candidates from a baseline probabilistic model: the model of (Collins 1999) for parsing, and a maximum-entropy tagger for named-entity recognition.",
        "2i.e., polynomial in the number of training examples, and the size of trees or sentences in training and test data.",
        "The choice of representation is central: what features should be used as evidence in choosing between candidates?",
        "We will use a function h(x) E Rd to denote a d-dimensional feature vector that represents a tree or tagged sequence x.",
        "There are many possibilities for h(x) .",
        "An obvious example for parse trees is to have one component of h(x) for each rule in a context-free grammar that underlies the trees.",
        "This is the representation used by Stochastic Context-Free Grammars.",
        "The feature vector tracks the counts of rules in the tree x, thus encoding the sufficient statistics for the SCFG.",
        "Given a representation, and two structures x and y, the inner product between the structures can be defined as d",
        "The idea of inner products between feature vectors is central to learning algorithms such as Support Vector Machines (SVMs), and is also central to the ideas in this paper.",
        "Intuitively, the inner product is a similarity measure between objects: structures with similar feature vectors will have high values for h(x) • h (y) .",
        "More formally, it has been observed that many algorithms can be implemented using inner products between training examples alone, without direct access to the feature vectors themselves.",
        "As we will see in this paper, this can be crucial for the efficiency of learning with certain representations.",
        "Following the SVM literature, we call a function K(x, y) of two objects x and y a “kernel” if it can be shown that K is an inner product in some feature space h."
      ]
    },
    {
      "heading": "3 Algorithms",
      "text": []
    },
    {
      "heading": "3.1 Notation",
      "text": [
        "This section formalizes the idea of linear models for parsing or tagging.",
        "The method is related to the boosting approach to ranking problems (Freund et al.",
        "1998), the Markov Random Field methods of (Johnson et al.",
        "1999), and the boosting approaches for parsing in (Collins 2000).",
        "The set-up is as follows: .",
        "Training data is a set of example input/output pairs.",
        "In parsing the training examples are {si, ti} where each si is a sentence and each ti is the correct",
        "tree for that sentence.",
        "• We assume some way of enumerating a set of candidates for a particular sentence.",
        "We use xij to denote the j’th candidate for the i’th sentence in training data, and C(si) = {xil, xi2 • • •} to denote the set of candidates for si.",
        ".",
        "Without loss of generality we take xi, to be the correct candidate for si (i.e., xi, = ti).",
        ".",
        "Each candidate xij is represented by a feature",
        "vector h(xij) in the space Rd.",
        "The parameters of the model are also a vector w E Rd.",
        "The output of the model on a training or test example s is argmaxXEC(s)w • h(x).",
        "The key question, having defined a representation h, is how to set the parameters w. We discuss one method for setting the weights, the perceptron algorithm, in the next section."
      ]
    },
    {
      "heading": "3.2 The Perceptron Algorithm",
      "text": [
        "Figure 1(a) shows the perceptron algorithm applied to the ranking task.",
        "The method assumes a training set as described in section 3.1, and a representation h of parse trees.",
        "The algorithm maintains a parameter vector w, which is initially set to be all zeros.",
        "The algorithm then makes a pass over the training set, only updating the parameter vector when a mistake is made on an example.",
        "The parameter vector update is very simple, involving adding the difference of the offending examples’ representations",
        "itively, this update has the effect of increasing the parameter values for features in the correct tree, and downweighting the parameter values for features in the competitor.",
        "See (Cristianini and Shawe-Taylor 2000) for discussion of the perceptron algorithm, including an overview of various theorems justifying this way of setting the parameters.",
        "Briefly, the perceptron algorithm is guaranteed3 to find a hyperplane that correctly classifies all training points, if such a hyperplane exists (i.e., the data is “separable”).",
        "Moreover, the number of mistakes made will be low, providing that the data is separable with “large margin”, and",
        "this translates to guarantees about how the method generalizes to test examples.",
        "(Freund & Schapire 1999) give theorems showing that the voted perceptron (a variant described below) generalizes well even given non-separable data."
      ]
    },
    {
      "heading": "3.3 The Algorithm in Dual Form",
      "text": [
        "Figure 1(b) shows an equivalent algorithm to the perceptron, an algorithm which we will call the “dual form” of the perceptron.",
        "The dual-form algorithm does not store a parameter vector w, instead storing a set of dual parameters, aij for i =",
        "This is in contrast to F(x) = w • h(x), the score in the original algorithm.",
        "In spite of these differences the algorithms give identical results on training and test examples: to see this, it can be verified that w = Ei j aij (h(xii) – h(xij)), and hence that G(x) = F(x), throughout training.",
        "The important difference between the algorithms lies in the analysis of their computational complexity.",
        "Say T is the size of the training set, i.e., T = ri ni.",
        "Also, take d to be the dimensionality of the parameter vector w. Then the algorithm in figure 1(a) takes O(Td) time.",
        "This follows because F(x) must be calculated for each member of the training set, and each calculation of F involves O(d) time.",
        "Now say the time taken to compute the 4If the vectors h(x) are sparse, then d can be taken to be the number of non-zero elements of h, assuming that it takes O(d) time to add feature vectors with O(d) non-zero elements, or to take inner products.",
        "inner product between two examples is k. The running time of the algorithm in figure 1(b) is O(Tnk).",
        "This follows because throughout the algorithm the number of non-zero dual parameters is bounded by n, and hence the calculation of G(x) takes at most O(nk) time.",
        "(Note that the dual form algorithm runs in quadratic time in the number of training examples n, because T > n.) The dual algorithm is therefore more efficient in cases where nk << d. This might seem unlikely to be the case – naively, it would be expected that the time to calculate the inner product h(x) • h(y) between two vectors to be at least O(d).",
        "But it turns out that for some high-dimensional representations the inner product can be calculated in much better than O(d) time, making the dual form algorithm more efficient than the original algorithm.",
        "The dual-form algorithm goes back to (Aizerman et al.",
        "64).",
        "See (Cristianini and Shawe-Taylor 2000) for more explanation of the algorithm."
      ]
    },
    {
      "heading": "3.4 The Voted Perceptron",
      "text": [
        "(Freund & Schapire 1999) describe a refinement of the perceptron algorithm, the “voted perceptron”.",
        "They give theory which suggests that the voted perceptron is preferable in cases of noisy or unseparable data.",
        "The training phase of the algorithm is unchanged – the change is in how the method is applied to test examples.",
        "The algorithm in figure 1(b) can be considered to build a series of hypotheses Gt (x), for",
        "� � � Gt is the scoring function from the algorithm trained on just the first t training examples.",
        "The output of a model trained on the first t examples for a sentence s",
        "of n models, Vl ... Vn.",
        "On a test sentence s, each of these n functions will return its own parse tree, Vt(s) for t = 1... n. The voted perceptron picks the most likely tree as that which occurs most often in the set {V1(s),V2(s) ... Vn(s)}.",
        "Note that Gt is easily derived from Gt-1, through the identity Gt(x) = Gt-1(x) + Ej t 2 at,j (h(xtl) • h(x) – h(xtj) - h(x)).",
        "Because of this the voted perceptron can be implemented with the same number of kernel calculations, and hence roughly the same computational complexity, as the original perceptron."
      ]
    },
    {
      "heading": "4 A Tree Kernel",
      "text": [
        "We now consider a representation that tracks all subtrees seen in training data, the representation studied extensively by (Bod 1998).",
        "See figure 2 for an example.",
        "Conceptually we begin by enumerating all tree fragments that occur in the training data 1... d. Note that this is done only implicitly.",
        "Each tree is represented by a d dimensional vector where the i’th component counts the number of oc-curences of the i’th tree fragment.",
        "Define the function hi (x) to be the number of occurences of the i’th tree fragment in tree x, so that x is now represented as h(x) _ (h, (x), h2 (X), ... , hd(x)).",
        "Note that d will be huge (a given tree will have a number of subtrees that is exponential in its size).",
        "Because of this we aim to design algorithms whose computational complexity is independent of d. The key to our efficient use of this representation is a dynamic programming algorithm that computes the inner product between two examples xl and x2 in polynomial (in the size of the trees involved), rather than O(d), time.",
        "The algorithm is described in (Collins and Duffy 2001), but for completeness we repeat it here.",
        "We first define the set of nodes in trees xl and x2 as Nl and N2 respectively.",
        "We define the indicator function Ii (n) to be 1 if sub-tree i is seen rooted at node n and 0 otherwise.",
        "It follows that hi(xl) _ En1EN1 Ii(nl) and hi(x2) _ Kn2EN2 1i (n2).",
        "The first step to efficient computation of the inner product is the following property:",
        "where we define 0(nl, n2) _ Ei Ii(nl)Ii(n2).",
        "Next, we note that A(nl, n2) can be computed efficiently, due to the following recursive definition: .",
        "If the productions at nl and n2 are different",
        ".",
        "If the productions at nl and n2 are the same, and nl and n2 are pre-terminals, then A(nl, n2) = 1.5 .",
        "Else if the productions at nl and n2 are the same and nl and n2 are not pre-terminals,",
        "where nc(nl) is the number of children of nl in the tree; because the productions at nl/n2 are the same, we have nc(nl) = nc(%).",
        "The i’th child-node of nl is ch(nl, i).",
        "To see that this recursive definition is correct, note that 0(nl, n2) _ Ei Ii(nl)Ii(n2) simply counts the number of common subtrees that are found rooted at both nl and n2.",
        "The first two cases are trivially correct.",
        "The last, recursive, definition follows because a common subtree for nl and n2 can be formed by taking the production at nl/n2, together with a choice at each child of simply taking the non-terminal at that child, or any one of the common sub-trees at that child.",
        "Thus there are 5Pre-terminals are nodes directly above words in the surface string, for example the N, V, and D symbols in Figure 2.",
        "at the i’th child.",
        "(Note that a similar recursion is described by Goodman (Goodman 1996), Goodman’s application being the conversion of Bod’s model (Bod 1998) to an equivalent PCFG.)",
        "It is clear from the identity h(xi) • h(x2) Enl n2 A(nl, n2), and the recursive definition of A(n1, n2), that h(x1) • h(x2) can be calculated in 0(1N1IIN21) time: the matrix of A(n1,n2) values can be filled in, then summed .6 Since there will be many more tree fragments of larger size – say depth four versus depth three – it makes sense to downweight the contribution of larger tree fragments to the kernel.",
        "This can be achieved by introducing a parameter 0 < A < 1, and modifying the base case and recursive case of the definitions of A to be respectively A(nl, n2) _ A and A(nl, n2) _ .�j nc(' 1)(1 + A(ch(nl, j),ch(n2i j))).",
        "This corresponds to a modified kernel h(xi) • h(x2) Ei Asizei hi (xi) hi (x2) where sizei is the number of rules in the i’th fragment.",
        "This is roughly equivalent to having a prior that large sub-trees will be less useful in the learning task."
      ]
    },
    {
      "heading": "5 A Tagging Kernel",
      "text": [
        "The second problem we consider is tagging, where each word in a sentence is mapped to one of a finite set of tags.",
        "The tags might represent part-of-speech tags, named-entity boundaries, base noun-phrases, or other structures.",
        "In the experiments in this paper we consider named-entity recognition.",
        "of members (nl, n2) E N, x N2 such that the productions at nl and n2 are the same.",
        "In our data we have found the number of nodes with identical productions to be approximately linear in the size of the trees, so the running time is also close to linear in the size of the trees.",
        "A tagged sequence is a sequence of word/state pairs x = {wlIs, ... w,,,/s,,,} where wi is the i’th word, and si is the tag for that word.",
        "The particular representation we consider is similar to the all sub-trees representation for trees.",
        "A tagged-sequence “fragment” is a subgraph that contains a subsequence of state labels, where each label may or may not contain the word below it.",
        "See figure 3 for an example.",
        "Each tagged sequence is represented by a d dimensional vector where the i’th component hi (x) counts the number of occurrences of the i’th fragment in x.",
        "The inner product under this representation can be calculated using dynamic programming in a very similar way to the tree algorithm.",
        "We first define the set of states in tagged sequences xl and x2 as Nl and N2 respectively.",
        "Each state has an associated label and an associated word.",
        "We define the indicator function Ii (n) to be 1 if fragment i",
        "where we define A(nl, n2) _ Ei Ii(nl)Ii(n2).",
        "Next, for any given state nl E Nl define next(nl) to be the state to the right of nl in the structure xl.",
        "An analogous definition holds for next(n2).",
        "Then A(nl, n2) can be computed using dynamic programming, due to a recursive definition: .",
        "If the state labels at nl and n2 are different",
        ".",
        "If the state labels at nl and n2 are the same, but the words at nl and n2 are different, then",
        ".",
        "Else if the state labels at nl and n2 are the same, and the words at nl and n2 are the same, then A(nl, n2) = 2 + 2 x A(next(nl), next(n2)).",
        "There are a couple of useful modifications to this kernel.",
        "One is to introduce a parameter 0 < A < 1 which penalizes larger substructures.",
        "The recursive definitions are modfied to be A(nl, n2) _",
        "2AA(next(nl), next(n2)) respectively.",
        "This gives an inner product Ei A\"' hi (xi) hi (x2) where sizei is the number of state labels in the ith fragment.",
        "Another useful modification is as follows.",
        "Define",
        "Siml (WI, w2) for words wl and w2 to be 1 if wl = w2, 0 otherwise.",
        "Define Sim2(WI, w2) to be 1 ifwl and w2 share the same word features, 0 otherwise.",
        "For example, Sim2 might be defined to be 1 if wl and w2 are both capitalized: in this case Sim2 is a looser notion of similarity than the exact match criterion of Siml.",
        "Finally, the definition of 0 can be modified to:",
        "• If labels at nl/n2 are different, A(nl, n2) = 0.",
        "• Else A(nl, n2) =",
        "where wl, w2 are the words at nl and n2 respectively.",
        "This inner product implicitly includes features which track word features, and thus can make better use of sparse data."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": []
    },
    {
      "heading": "6.1 Parsing Wall Street Journal Text",
      "text": [
        "We used the same data set as that described in (Collins 2000).",
        "The Penn Wall Street Journal treebank (Marcus et al.",
        "1993) was used as training and test data.",
        "Sections 2-21 inclusive (around 40,000 sentences) were used as training data, section 23 was used as the final test set.",
        "Of the 40,000 training sentences, the first 36,000 were used to train the perceptron.",
        "The remaining 4,000 sentences were used as development data, and for tuning parameters of the algorithm.",
        "Model 2 of (Collins 1999) was used to parse both the training and test data, producing multiple hypotheses for each sentence.",
        "In order to gain a representative set of training data, the 36,000 training sentences were parsed in 2,000 sentence chunks, each chunk being parsed with a model trained on the remaining 34,000 sentences (this prevented the initial model from being unrealistically “good” on the training sentences).",
        "The 4,000 development sentences were parsed with a model trained on the 36,000 training sentences.",
        "Section 23 was parsed with a model trained on all 40,000 sentences.",
        "The representation we use incorporates the probability from the original model, as well as the all-subtrees representation.",
        "We introduce a parameter � which controls the relative contribution of the two terms.",
        "If L(x) is the log probability of a tree x under the original probability model, and h(x) = (hl (x), h2 (x), ... , hd(x)) is the feature vector under the all subtrees representation, then the new representation is h2 (x) = (A/�L(x), hl (x), h2 (x), ... , hd(x)), and the inner product between two examples x and y is h2(x) •",
        "perceptron algorithm to use the probability from the original model as well as the subtrees information to rank trees.",
        "We would thus expect the model to do at least as well as the original probabilistic model.",
        "The algorithm in figure 1(b) was applied to the problem, with the inner product h2 (x) • h2 (y) used in the definition of G(x).",
        "The algorithm in 1(b) runs in approximately quadratic time in the number of training examples.",
        "This made it somewhat expensive to run the algorithm over all 36,000 training sentences in one pass.",
        "Instead, we broke the training set into 6 chunks of roughly equal size, and trained 6 separate perceptrons on these data sets.",
        "This has the advantage of reducing training time, both because of the quadratic dependence on training set size, and also because it is easy to train the 6 models in parallel.",
        "The outputs from the 6 runs on test examples were combined through the voting procedure described in section 3.4.",
        "Figure 4 shows the results for the voted perceptron with the tree kernel.",
        "The parameters � and A were set to 0.2 and 0.3 respectively through tuning on the development set.",
        "The method shows a 0.6% absolute improvement in average precision and recall (from 88.2% to 88.8% on sentences < 100 words), a 5.1% relative reduction in error.",
        "The boosting method of (Collins 2000) showed 89.6%/89.9% recall and precision on reranking approaches for the same datasets (sentences less than 100 words in length).",
        "(Charniak 2000) describes a different method which achieves very similar performance to (Collins 2000).",
        "(Bod 2001) describes experiments giving 90.6%/90.8% recall and precision for sentences of less than 40 words in length, using the all-subtrees representation, but using very different algorithms and parameter estimation methods from the perceptron algorithms in this paper (see section 7 for more discussion)."
      ]
    },
    {
      "heading": "6.2 Named–Entity Extraction",
      "text": [
        "Over a period of a year or so we have had over one million words of named-entity data annotated.",
        "The data is drawn from web pages, the aim being to support a question-answering system over web data.",
        "A number of categories are annotated: the usual people, organization and location categories, as well as less frequent categories such as brand-names, scientific terms, event titles (such as concerts) and so on.",
        "As a result, we created a training set of 53,609 sentences (1,047,491 words), and a test set of 14,717 sentences (291,898 words).",
        "The task we consider is to recover named-entity boundaries.",
        "We leave the recovery of the categories of entities to a separate stage of processing.",
        "We evaluate different methods on the task through precision and recall.",
        "The problem can be framed as a tagging task – to tag each word as being either the start of an entity, a continuation of an entity, or not to be part of an entity at all.",
        "As a baseline model we used a maximum entropy tagger, very similar to the one described in (Ratnaparkhi 1996).",
        "Maximum entropy taggers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tagging (Ratnaparkhi 1996), and named-entity recognition (Borthwick et.",
        "al 1998).",
        "Thus the maximum-entropy tagger we used represents a serious baseline for the task.",
        "We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)).",
        "As a baseline we trained a model on the full 53,609 sentences of training data, and decoded the 14,717 sentences of test data using a beam search",
        "perceptron.",
        "P = precision, R = recall, F = F-measure.",
        "which keeps the top 20 hypotheses at each stage of a left-to-right search.",
        "In training the voted perceptron we split the training data into a 41,992 sentence training set, and a 11,617 sentence development set.",
        "The training set was split into 5 portions, and in each case the maximum-entropy tagger was trained on 4/5 of the data, then used to decode the remaining 1/5.",
        "In this way the whole training data was decoded.",
        "The top 20 hypotheses under a beam search, together with their log probabilities, were recovered for each training sentence.",
        "In a similar way, a model trained on the 41,992 sentence set was used to produce 20 hypotheses for each sentence in the development set.",
        "As in the parsing experiments, the final kernel incorporates the probability from the maximum entropy tagger, i.e. h2(x) • h2(y) _ , L(x)L(y) + h(x) • h(y) where L(x) is the log-likelihood of x under the tagging model, h(x) • h(y) is the tagging kernel described previously, and � is a parameter weighting the two terms.",
        "The other free parameter in the kernel is A, which determines how quickly larger structures are downweighted.",
        "In running several training runs with different parameter values, and then testing error rates on the development set, the best parameter values we found were � = 0.2, A = 0.5.",
        "Figure 5 shows results on the test data for the baseline maximum-entropy tagger, and the voted perceptron.",
        "The results show a 15.6% relative improvement in F-measure."
      ]
    },
    {
      "heading": "7 Relationship to Previous Work",
      "text": [
        "(Bod 1998) describes quite different parameter estimation and parsing methods for the DOP representation.",
        "The methods explicitly deal with the parameters associated with subtrees, with sub-sampling of tree fragments making the computation manageable.",
        "Even after this, Bod’s method is left with a huge grammar: (Bod 2001) describes a grammar with over 5 million substructures.",
        "The method requires search for the 1,000 most probable derivations under this grammar, using beam search, presumably a challenging computational task given the size of the grammar.",
        "In spite of these problems, (Bod 2001) gives excellent results for the method on parsing Wall Street Journal text.",
        "The algorithms in this paper have a different flavor, avoiding the need to explicitly deal with feature vectors that track all subtrees, and also avoiding the need to sum over an exponential number of derivations underlying a given tree.",
        "(Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set.",
        "The method uses a similar recursion to the common sub-trees recursion described in this paper.",
        "Good-man’s method still leaves exact parsing under the model intractable (because of the need to sum over multiple derivations underlying the same tree), but he gives an approximation to finding the most probable tree, which can be computed efficiently.",
        "From a theoretical point of view, it is difficult to find motivation for the parameter estimation methods used by (Bod 1998) – see (Johnson 2002) for discussion.",
        "In contrast, the parameter estimation methods in this paper have a strong theoretical basis (see (Cristianini and Shawe-Taylor 2000) chapter 2 and (Freund & Schapire 1999) for statistical theory underlying the perceptron).",
        "For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b).",
        "(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels.",
        "(Collins 2002b) describes how the voted perceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks.",
        "Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the experiments.",
        "Thanks to Rob Schapire and Yoram Singer for many useful discussions."
      ]
    }
  ]
}
