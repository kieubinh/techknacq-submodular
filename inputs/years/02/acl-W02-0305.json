{
  "info": {
    "authors": [
      "Lee Christensen",
      "Peter Haug",
      "Marcelo Fiszman"
    ],
    "book": "Workshop on Natural Language Processing in the Biomedical Domain",
    "id": "acl-W02-0305",
    "title": "MPLUS: A Probabilistic Medical Language Understanding System",
    "url": "https://aclweb.org/anthology/W02-0305",
    "year": 2002
  },
  "references": [
    "acl-A00-2043",
    "acl-P89-1005"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes the basic philosophy and implementation of MPLUS (M+), a robust medical text analysis tool that uses a semantic model based on Bayesian Networks (BNs).",
        "BNs provide a concise and useful formalism for representing semantic patterns in medical text, and for recognizing and reasoning over those patterns.",
        "BNs are noise-tolerant, and facilitate the training of M+."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In the field of medical informatics, computerized tools are being developed that depend on databases of clinical information.",
        "These include alerting systems for improved patient care, data mining systems for quality assurance and research, and diagnostic systems for more complex medical decision support.",
        "These systems require data that is appropriately structured and coded.",
        "Since a large portion of the information stored in patient databases is in the form of free text, manually coding this information in a format accessible to these tools can be time consuming and expensive.",
        "In recent years, natural language processing (NLP) methodologies have been studied as a means of automating this task.",
        "There have been many projects involving automated medical language analysis, including deciphering pathology reports (Smart and Roux, 1995), physical exam findings (Lin et al., 1991), and radiology reports (Friedman et al., 1994; Ranum, 1989; Koehler, 1998).",
        "M+ is the latest in a line of NLP tools developed at LDS Hospital in Salt Lake City, Utah.",
        "Its predecessors include SPRUS (Ranum, 1989) and SymText (Koehler, 1998).",
        "These tools have been used in the realm of radiology reports, admitting diagnoses (Haug et al., 1997), radiology utilization review (Fiszman, 2002) and syndromic detection (Chapman et al., 2002).",
        "Some of the character of these tools derives from common characteristics of radiology reports, their initial target domain.",
        "Because of the off-the-cuff nature of radiology dictation, a report will frequently contain text that is telegraphic or otherwise not well formed grammatically.",
        "Our desire was not only to take advantage of phrasal structure to discover semantic patterns in text, but also to be able to infer those patterns from lexical and contextual cues when necessary.",
        "Most NLP systems capable of semantic analysis employ representational formalisms with ties to classical logic, including semantic grammars (Friedman et al., 1994), unification-based semantics (Moore, 1989), and description logics (Romacker and Hahn, 2000).",
        "M+ and its predecessors employ Bayesian Networks (Pearl, 1988), a methodology outside this tradition.",
        "This study discusses the philosophy and implementation of M+, and attempts to show how Bayesian Networks can be useful in medical text analysis."
      ]
    },
    {
      "heading": "2 The M+ Semantic Model",
      "text": []
    },
    {
      "heading": "2.1 Semantic Bayesian Networks",
      "text": [
        "M+ uses Bayesian Networks (BNs) to represent the basic semantic types and relations within a medical domain such as chest radiology reports.",
        "M+ BNs are structurally similar to semantic networks, in that they are implemented as directed acyclic graphs, with nodes representing word and concept types, and links representing relations between those types.",
        "BNs also have a character as frames or slot-filler representations (Minsky, 1975).",
        "Each node is treated as a variable, with an associated list of possible values.",
        "For instance a node representing \"disease severity\" might include the possible values {\"severe\", \"moderate\", \"mild\"}.",
        "Each value has a probability, either assigned or inferred, of being the true value of that node.",
        "In addition to providing a framework for representation, a BN is also a probabilistic inference engine.",
        "The probability of each possible value of a node is conditioned on the probabilities of the values of neighboring nodes, through a training process that learns a Bayesian joint probability function from a set of training cases.",
        "After a BN is trained, a node can be assigned a value by setting the probability of that value to 1, and the probabilities of the alternate values to 0.",
        "This results in a cascading update of the value probabilities in all unassigned nodes, in effect predicting what the values of the unassigned nodes should be, given the initial assignments.",
        "The sum of the probabilities for the values of a given node is constrained to equal 1, making the values mutually exclusive, and reflecting uncertainty if more than one value has a nonzero probability.",
        "Please note that in this paper, \"BN instance\" refers to the state of a BN after assignments have been made.",
        "A training case for a BN is a list of node / value assignments.",
        "For instance, consider a simple BN for chest anatomy phrases, as shown in Figure 1.",
        "A training case for this BN applied to the phrase \"right upper lobe\" could be:",
        "In the context of the Bayesian learning, this case has an effect similar to a production rule which states \"If you find the words 'right', 'upper' and 'lobe' together in a phrase, infer the meaning *right-upper-lobe\".",
        "After training on this case, assigning one or more values from this case would increase the probabilities of the other values; for instance assigning side= \"right\" would increase the probability of the value interpretation= *right-upper-lobe.",
        "Interpretive concepts such as *right-upper-lobe are atomic symbols which are either invented by the human trainer, or else obtained from a medical knowledge database such as the UMLS metathesaurus.",
        "By convention, concept names in M+ are preceded with an asterisk.",
        "A medical domain is represented in M+ as a network of BNs, with word-level and lower concept-level BNs providing input to higher concept-level BNs.",
        "Figure 2 shows a partial view of the network of BNs used to model the M+ Head CT (Computerized Tomography) domain, instantiated with the phrase \"temporal subdural hemorrhage\".",
        "Each BN instance is shown with a list of nodes and most probable values.",
        "Note that input nodes of higher BNs in this model have the same name as, and take input from, the summary nodes of lower BNs.",
        "Word level BNs have input nodes named \"head\", \"mod1\" and \"mod2\", corresponding to the syntactic head and modifiers of a phrase.",
        "Each node in a BN has a distinguished \"null\" value, whose meaning is that no information relevant to that node, explicit or inferable, is present in the represented phrase.",
        "One way in which M+ differs from its predecessor SymText (Koehler, 1998) is in the size and modularity of its semantic BNs.",
        "The SymText BNs group observation and disease concepts together with state (\"present\", \"absent\"), change-of-state (\"old\", \"chronic\"), anatomic location and other concept types.",
        "M+ trades the inferential advantages of such monolithic BNs for the modularity and composability of smaller BNs such as those shown in figure 2.",
        "Figure 3 shows a single instance of the SymText Chest Radiology Findings BN, instantiated with the sentence \"There is dense infiltrative opacity in the right upper lobe\"."
      ]
    },
    {
      "heading": "2.2 Parse-Driven BN Instantiation",
      "text": [
        "M+ BNs are instantiated as part of the syntactic parse process.",
        "M+ syntactic and semantic analyses are interleaved, in contrast with NLP systems that perform semantic analysis after the parse has finished.",
        "M+ uses a bottom-up chart parser, with a context free grammar (CFG).",
        "As a word such as \"right\" is recognized by the parser, a word-level phrase object is created and a BN instance containing the assignment side= \"right\" is attached to that phrase.",
        "As larger grammatical patterns are recognized, the BN instances attached to subphrases within those patterns are unified and attached to the new phrases, as described in section 3.",
        "The result of this process is a set of completed BN instances, as illustrated in figure 2.",
        "Each BN instance is a template containing word and concept-level value assignments, and the interpretive concepts inferred from those assignments.",
        "The templates themselves are nested in a symbolic expression, as described in section 2.3, to facilitate composing multiple BN instances in representations of arbitrary complexity.",
        "Each phrase recognized by the parser is assigned a probability, based on a weighted sum of the joint probabilities of its associated BN instances, and adjusted for various syntactic and semantic constraint violations.",
        "Phrases are processed in order of probability; thus the parse involves a semantically-guided best-first search.",
        "Syntactic and semantic analysis in M+ are mutually constraining.",
        "If a grammatically possible phrase is uninterpretable, i.e. if its subphrase interpretations cannot be unified, it is rejected.",
        "If the interpretation has a low probability, the phrase is less likely to appear in the final parse tree.",
        "On the other hand, interpretations are constructed as phrases are recognized.",
        "The exception to this rule is when an ungrammatical fragment of text is encountered.",
        "M+ then uses a semantically-guided phrase repair procedure not described in this paper."
      ]
    },
    {
      "heading": "2.3 The M+ Abstract Semantic Language",
      "text": [
        "The probabilistic reasoning afforded by BNs is superior to classical logic in important ways (Pearl, 1988).",
        "However, BNs are limited in expressive power relative to first-order logics (Koller and Pfeffer, 1997), and commercially available implementations lack the flexibility of symbolic languages.",
        "Friedman et al. have made considerable headway in giving BNs many useful characteristics of first order languages, in what they call probabilistic relational models, or PRMs (e.g. Friedman at al.",
        "1999).",
        "While we are waiting for industry-standard PRMs, we have tried to make our semantic BNs more useful by combining them with a first-order language, called the M+ Abstract Semantic Language (ASL), implemented within M+.",
        "Specifically, BNs are treated as object types within the ASL.",
        "There is a \"chest anatomy\" type, for instance, and a \"chest radiology findings\" type, corresponding to BNs of those same names.",
        "The interpretation of a phrase is an expression in the ASL, containing predicates that state the relation of BN instances to one another, and to the phrase they describe.",
        "For instance, the interpretation of \"hazy right lower lobe opacity\" could be the expression (and (head-of #phrase1 #find1) (located-at #find1 #loc1)) where #phrase1 identifies a syntactic phrase object, and #find1 and #loc1 are tokens representing instances of the findings BN (instanced with the words \"hazy\" and \"opacity\") and the anatomic BN (instanced with \"right\", \"lower\" and \"lobe\"), respectively.",
        "The relation 'head-of' denotes that the findings BN is the main or \"head\" BN for that phrase.",
        "Conversely, \"hazy right lower lobe opacity\" can be thought of as a findings-type phrase, with an anatomic-type modifier.",
        "This expression captures the abstract or \"skeletal\" structure of the interpretation, while the BN instances contain the details and specific inferences.",
        "One can think of the meaning of an expression like (located-at #find1 #loc1) in abstract terms, e.g. \"some-finding located-at some-location\".",
        "Alternatively, the meaning of a BN token might be thought of as the most probable interpretive concept within that BN instance.",
        "In this case, (located-at #find1 #loc1) could mean \"*localized-infiltrate located-at *left-lower-lobe\".",
        "Because the object types in the ASL are the abstract concept types represented by the BNs, semantic rules formulated in this language constitute an \"abstract semantic grammar\" (ASG).",
        "The ASG recognizes patterns of semantic relations among the BNs, and supports analysis and inference based on those patterns.",
        "It also permits rule-based control over the creation, instantiation, and use of the BNs, including defining pathways for information sharing among BNs using virtual evidence (Pearl, 1988).",
        "One use of the ASG is in post-parse processing of interpretations.",
        "After the M+ parser has constructed an interpretation, post-parse ASG productions may augment or alter this interpretation.",
        "One rule instructs \"If two pathological conditions exist in a 'consistentwith' relation, and the first condition has a state modifier (i.e. *present or *absent), and the second condition does not, apply the first condition's state to the second condition\".",
        "For instance, in the ambiguous sentence \"There is no opacity consistent with pneumonia\", if the parser doesn't correctly determine the scope of \"no\", it may produce the an interpretation in which *pneumonia lacks a state modifier, and is therefore inferred (by default) to be present.",
        "This rule correctly attaches (state-of *pneumonia *absent) to this interpretation.",
        "One important consequence of the modularity of the M+ BNs, and of the ability to nest them within the ASL, is that M+ can compose BN instances in expressions of arbitrary complexity.",
        "For instance, it is straightforward to represent the multiple anatomic concepts in the phrase \"opacity in the inferior segment of the left upper lobe, adjacent to the heart\":",
        "where the interpretive concepts of #anat1, #anat2 and #anat3 are *left-upper-lobe, *inferior-segment, and *heart, respectively.",
        "The set of binary predicates that constitutes a phrase interpretation in M+ forms a directed acyclic graph; thus we can refer to the interpretation as an interpretation graph.",
        "The interpretation graph of a new phrase is formed by unifying the graphs of its subphrases, as described in section 3."
      ]
    },
    {
      "heading": "2.4 Advantages of Bayesian Networks",
      "text": [
        "As mentioned, a BN training case bears a similarity to a production rule.",
        "It would be straightforward to implement the training cases as a set of rules, and apply them to text analysis using a deductive reasoning engine.",
        "However, Bayesian reasoning has important advantages over first order logic, including:",
        "1- BNs are able to respond gracefully to input \"noise\".",
        "A semantic BN may produce reasonable inferences from phrasal patterns that only partially match any given training case, or that overlap different cases, or that contain words in an unexpected order.",
        "For instance, having trained on multi-word phrases containing \"opacity\", the single word \"opacity\" could raise the probabilities of several interpretations such as *localized-infiltrate and *parenchymal-abnormality, both of which are reasonable hypotheses for the underlying cause of opacity on a chest x-ray film.",
        "2- Bayesian inference works bidirectionally; i.e. it is abductive as well as deductive.",
        "If instead of assigning word-level nodes, one assigns the value of the summary node, the probability of word values having a high correlation with that summary will increase.",
        "For instance, assigning the value *localized-infiltrate will raise the probability that the topic word is \"opacity\".",
        "Bidirectional inference provides a means for modeling the effects of lexical context.",
        "A value assignment made to one word node can alter value probabilities at unassigned word nodes, in a path of inference that passes through the connecting concept nodes.",
        "For instance, if a BN were trained on \"right upper lobe\" and \"left upper lobe\", but had never seen the term \"bilateral\", applying the BN to the phrase \"bilateral upper lobes\" would increase the probabilities of both \"left\" and \"right\", suggesting that \"bilateral\" is semantically similar to \"left\" and \"right\".",
        "This is one approach to guessing the node assignments of unknown words, a step in the direction of automated learning of new training cases.",
        "Similarly, if the system encounters a phrase with a misspelling such as \"rght upper lobe\", by noting the orthographic similarity of \"rght\" to \"right\" and the fact that \"right\" is highly predicted from surrounding words, it can determine that \"rght\" is a misspelling of \"right\".",
        "The spell checker currently used by M+ employs this technique."
      ]
    },
    {
      "heading": "3 Generating Interpretation Graphs",
      "text": [
        "As mentioned, in M+ the interpretation graph of a phrase is created by unifying the graphs of its child phrases.",
        "High joint probabilities in the resulting BN instances are one source of evidence that the words thus brought together exist in the expected semantic pattern.",
        "However, corroborating evidence must be sought in the syntax of the text.",
        "Words which appear together in a training phrase may not be in that same relation in a given text.",
        "For instance, \"no\" and \"pneumonia\" support different conclusions in \"no evidence of pneumonia\" and \"patient has pneumonia with no apparent complicating factors\".",
        "M+ therefore only attempts to unify sub-interpretations that appear, on syntactic grounds, to be talking about the same things.",
        "This is less constraining than production rules that look for words in a specific order, but more constraining than simply pulling key words out of a string of text.",
        "The following are examples of rules used to guide the unification of ASL interpretation graphs.",
        "For convenience, several shorthand functional notations are used: If P represents a phrase on the parse chart, root-bn(P) represents the root or head BN instance in P's interpretation graph, and type-of(root-bn(P)) is the BN type of root-bn(P).",
        "If A and B are sibling child phrases of parent phrase C, then C = parent-phrase(A,B).",
        "Note that for convenience, BN instances in the interpretation graphs in Figures 4 - 6 are represented alternately as the words slotted in those instances, and as the most probable interpretive concepts inferred by those instances."
      ]
    },
    {
      "heading": "3.1 Same-type Unification",
      "text": [
        "If phrase A syntactically modifies phrase B, then M+ assumes that some semantic relation exists between A and B.",
        "The nature of that relation is partly determinable from type-of(root-bn(A)) and type-of(root-bn(B)).",
        "If type-of(root-bn(A)) = type-of(root-bn(B)), that relation is simply one where root-bn(A) and root-bn(B) are partial descriptions of a single concept.",
        "If root-bn(A) and root-bn(B) are unifiable, M+ composes their input to form root-bn(parent-phrase(A,B )).",
        "If in addition there are two unifiable same-type BN instances X and Y linked to root-bn(A) and root-bn(B) respectively, via arcs of the same name, then X and Y also describe a single concept, and the arcs describe a single relationship.",
        "For instance, if X and Y describe the anatomic locations of root-bn(A) and root-bn(B), and if root-bn(A) and root-bn(B) are partial descriptions of a single \"finding\", then X and Y are partial descriptions of a single anatomic location, and ought to be unified.",
        "In figure 4, in the Chest X-ray domain, the phrase \"bilateral hazy lower lobe opacity\" is interpreted by unifying the interpretations of its subphrases \"bilateral hazy\" and \"lower lobe opacity\".",
        "Note that without any corresponding syntactic transformation, this rule brings about a \"virtual transformation\", whereby words are grouped together within BN instances in a manner that reflects the conceptual structure of the text.",
        "In this example \"bilateral hazy lower lobe opacity\" is treated as (\"bilateral lower lobe\") (\"hazy opacity\")."
      ]
    },
    {
      "heading": "3.2 Different-type Unification",
      "text": [
        "If phrase A syntactically modifies phrase B, and type-of(root-bn(A)) <> type-of(root-bn(B)), then root-bn(A) and root-bn(B) represent different concepts within some semantic relation.",
        "M+ uses the ASG to identify that relation and to add it to the interpretation graph in the form of a path of named arcs connecting root-bn(A) and root-bn(B).",
        "This path may include implicit connecting BN instances.",
        "For instance, to interpret \"subdural hemorrhage\" in the Head CT domain, M+ attempts to unify the graphs for the subphrases \"subdural\" and \"hemorrhage\", where type-of(root-bn(\"subdural\")) = location, and type-of(root-bn(\"hemorrhage\")) = topic.",
        "M+ identifies the connecting path for these two types as shown in figure 2, and adds that path to the interpretation as shown in figure 5.",
        "Note that this path contains instances of the \"observation\" and \"anatomy\" BN types."
      ]
    },
    {
      "heading": "3.3 Grammar Rule Based Unification",
      "text": [
        "Individual grammar rules in M+ can recognize semantic relations, and add connecting arcs to the interpretation graph.",
        "For instance, M+ has a rule which recognizes findings-type phrases connected with strings of the \"suggesting\" variety, and connects their graphs with a 'consistent-with' arc.",
        "This is used to interpret \"opacity suggesting possible infarct\" in the Head CT domain, as shown in figure 6."
      ]
    },
    {
      "heading": "4 M+ Implementation",
      "text": [
        "M+ is written in Common Lisp, with some C routines for BN access.",
        "The M+ architecture consists of six basic components: The parser, concept space, rule base, lexicon, ASL inference engine, and Bayesian network component.",
        "As mentioned, the parser is an implementation of a bottom up chart parser with context free grammar.",
        "The concept space is a table of symbols representing types, objects and relations within the ASL.",
        "These include BN names, BN node value names, inter-BN relation names, and a small ontology of useful concepts such as those related to time.",
        "The rule base contains rules, which comprise the syntactic grammar and ASG.",
        "The lexicon is a table of Lisp-readable word information entries, obtained in part from the UMLS Specialist Lexicon.",
        "The ASL inference engine combines symbolic unification with backward-chaining inference.",
        "It can be used to match an ASG pattern against an interpretation graph, and to perform tests associated with grammar rules.",
        "The Bayesian network component utilizes the Norsys Netica(TM) API, and includes a set of Lisp and C language routines for instantiating and retrieving probabilities from BNs."
      ]
    },
    {
      "heading": "5 Training M+",
      "text": [
        "Porting M+ to a new medical domain involves gathering a corpus of training sentences for the domain, using the Netica(TM) graphical interface to create domain-specific BNs, and generating training cases for the new BNs.",
        "The most time-consuming task is the creation of training cases.",
        "We have developed a prototype version of a Web-based tool which largely automates this task.",
        "The basic idea is to enable M+ to guess the BN value assignments of unknown words, then use it to parse phrases similar to phrases already seen.",
        "For instance, having been trained on the phrase \"right upper lobe\", the parser is able to produce reasonable parses, with some \"guessed\" value assignments, for \"left upper lobe\", \"right middle lobe\", \"bilateral lungs\", etc.",
        "The BN assignments produced by the parse are output as tentative new cases to be reviewed and corrected by the human trainer.",
        "The training process begins with an initial set of interpreted \"seed\" phrases.",
        "From this set, the tool can apply the parser to phrases similar to this set, and so semi-automatically traverse ever widening semantically contiguous areas within the space of corpus phrases.",
        "As the training proceeds, the role of the human trainer increasingly becomes one of providing correction and interpretations for semantic patterns the system is increasingly able to discover on its own.",
        "To parse phrases containing unknown words, M+ uses a technique based on a variation of the vector space model of lexical semantic similarity (Manning and Schütze, 1999).",
        "As M+ encounters an unknown word, it gathers a list of training corpus words judged similar to that word, as predicted by the vector space measure.",
        "It then identifies BN nodes whose known values significantly overlap with this list, and provisionally assigns the unknown word as a new value for those nodes.",
        "The assignment resulting in the best parsetree is selected for the new provisional training case."
      ]
    },
    {
      "heading": "6 Evaluation",
      "text": [
        "M+ was evaluated for the extraction of American College of Radiology (ACR) utilization review codes from Head CT reports (Fiszman, 2002).",
        "The ACR codes compare the outcome in a report with the suspected diagnosis provided by emergency department physicians.",
        "If the outcome relates to the suspected diagnosis then the report should be encoded as positive (P).",
        "If the outcome is negative and does not relate to the suspected diagnosis then the report should be encoded as negative (N).",
        "In order to extract those ACR codes we trained M+ to extract eleven broad disease concepts, then inferred the ACR codes based on the application of a rule to the M+ output: If any of the concepts was present, the report was considered positive, else the report was considered negative.",
        "Twenty six hundred head CT scan reports were used for this evaluation.",
        "Six hundred reports were randomly selected for testing, and the rest were used to train M+ in this domain.",
        "The performance of M+ on this task was measured against that of four board certified physicians, using a gold standard based on majority vote, as described in (Fiszman, 2002).",
        "For each subject we calculated recall, precision and specificity with their respective 95 % confidence intervals for the capture of ACR utilization codes.",
        "From 600 head CT reports, 67 were judged to be positive (P) by the gold standard physicians and 534 were judged to be negative (N).",
        "Therefore the positive rate for head CT in this sample was 11%.",
        "Recall, precision and specificity for every subject are presented with their respective 95% confidence intervals in Table 1.",
        "The physicians had an average recall of 88% (CI, 84% to 92.%), an average precision of 86% (CI, 81% to 90%), and average specificity of 98% (CI, 97% to 99%).",
        "M+ had recall of 87% (CI, 78% to 95%), precision of 85% (CI, 77% to 94%) and specificity of 98% (CI, 97% to 99).",
        "The results on Head CT reports are encouraging, but there are limitations.",
        "We only evaluated 600 reports, because it's very hard to get physicians to produce gold standard data for medical reports.",
        "The prevalence of positive",
        "reports is only 11% and reflects the fact that the individual brain conditions have very low prevalence."
      ]
    },
    {
      "heading": "7 Conclusions",
      "text": [
        "M+ and its predecessors have demonstrated that BNs provide a useful semantic model for medical text processing.",
        "In practice, a medical NLP system will frequently encounter missing and unknown words, unknown and ungrammatical phrase structures, and telegraphic usages.",
        "Knowledge databases will be imperfect and incomplete.",
        "Using BNs for semantic representation brings a noise-tolerant, partial match-tolerant, context-sensitive character to the recognition of semantic patterns, and to relevant inferences based on those patterns.",
        "In addition, BNs can be used to guess the semantic types of unknown words, providing a basis for bootstrapping the system's semantic knowledge."
      ]
    },
    {
      "heading": "8 Acknowledgements",
      "text": [
        "Many thanks to Wendy W. Chapman for her advice and input in this paper, and her efforts to make M+ a useful addition to the RODS project at the University of Pittsburgh."
      ]
    }
  ]
}
