{
  "info": {
    "authors": [
      "Genichiro Kikui",
      "Hirohumi Yamamoto"
    ],
    "book": "Workshop on Speech-To-Speech Translation: Algorithms and Systems",
    "id": "acl-W02-0704",
    "title": "Finding Translation Pairs from English-Japanese Untokenized Aligned Corpora",
    "url": "https://aclweb.org/anthology/W02-0704",
    "year": 2002
  },
  "references": [
    "acl-C94-1032",
    "acl-J90-2002",
    "acl-W93-0301",
    "acl-W95-0114"
  ],
  "sections": [
    {
      "text": [
        "Finding Translation Pairs front English-Japanese Untokenized Aligned Corpora Genichiro I£II£UI and Hirofumi YAMAMOTO ATR Spoken Language Translation Research Laboratories 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288, JAPAN"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes a new algorithm for finding translation pairs in an English-Japanese parallel aligned corpus.",
        "Unlike previous methods, our algorithm does not presuppose a separate tokenizer for Japanese, but finds translation pairs as \"sideeffects\" of unsupervised tokenization of Japanese sentences by using information from the English sentences.",
        "The algorithm is based on the observation that two Japanese sentences tend to have a common word when their English mates (i.e., aligned sentences) contain the same word.",
        "N e implemented this idea as an unsupervised tokenization of Japanese with extended Hidden-Markov-Models (HNIMs), where hidden n-gram probabilities (i.e., state transition probabilities) are affected by co-occurring words in the English part.",
        "Our experiment on finding noun-noun translation pairs achieved 76.3`/ accuracy, which was 0.4 points lower than the result using supervised tokenization."
      ]
    },
    {
      "heading": "I Introduction",
      "text": [
        "As pointed out by Fung (Fung, 1995), automatic compilation of a domain-specific bilingual lexicon from corpora is needed to improve machine translation, cross-language information retrieval, etc.",
        "Actually, a number of methods have been already proposed for finding translation pairs from aligned sentences (Dagan et al., 1993), (Wu and Xia, 1994).",
        "These methods presuppose sentences from both languages are segmented (or tokenized) into words (e.g.,(NVu and Xia, 1994)).",
        "Thus, when applied to a language whose written form does not have trivial word boundaries (e.g., Chinese, Japanese, and Korean, sentences have to be segmented first by a separate tokenizer.",
        "Although accurate tokenizers have been proposed for a limited number of languages, they depend on the rules of individual languages or require large segmented corpora for training.",
        "Thus, they cannot be easily transferred to other languages.",
        "Another problem of relying on a separate tokenizer is that word boundaries determined by the tokenizer are not always optimal for constructing a bilingual lexicon.",
        "This paper proposes an algorithm for finding English-Japanese translation pairs from aligned corpora while simultaneously tokenizing the Japanese part of the corpora.",
        "The underlying idea of our algorithm is that two Japanese sentences tend to have a common word when their English mates (i.e., aligned sentences) contain the same word.",
        "N e implemented this idea as an unsupervised tokenization of Japanese using an extended version of Hidden-Markov-Models (HNIMs), where in-gram probabilities (i.e., state transition probabilities) are affected by co-occurring words in the English sentences.",
        "Since the algorithm does not rely on a specific feature of Japanesel or trained tokeniz-ers, it is applicable to parallel corpora consisting of English and another language without trivial word boundaries.",
        "N e expect that this kind of corpora will be available for many languages in the near future because texts in a local language will be translated into English and vice versa, considering that English is the \"common\" language of the world.",
        "The remaining part of the paper is organized as follows.",
        "Section 2 introduces the problem.",
        "Section 3 presents our algorithm.",
        "Section 4 reports the results of the experiments and the discussion."
      ]
    },
    {
      "heading": "2 Problem Setting",
      "text": []
    },
    {
      "heading": "2.1 Aligned Corpus",
      "text": [
        "Input to our algorithm is an English-Japanese parallel aligned corpus, where every sentence in English is paired with its translation in Japanese.",
        "An example of pairs is shown in Figure 1.",
        "Although this paper uses an English-Japanese corpus, the proposed algorithm is applicable to any language pair, one from a language with trivial word boundaries and the other from a language without them."
      ]
    },
    {
      "heading": "2.2 Finding Translation Pairs from Untokenized Corpus",
      "text": [
        "Our goal is to find translation pairs from the parallel corpus described above.",
        "This goal is achieved by performing the following two:",
        "1.",
        "Identifying words of aligned sentences,",
        "what we call segmentation or tokenization and",
        "2.",
        "Finding translation pairs from the segmented corpora, what we call extraction.",
        "Existing approaches, most of which are for pairs of N estern-European languages, concentrate on the second task, since words in these languages are already separated by explicit markers, such as spaces.",
        "For East-Asian le.g., character classes languages, (e.g., Chinese, Japanese, and Korean), however, the first task is equally important, since these languages have no explicit word boundaries in their written form.",
        "Let us consider a hypothetical language, LX, instead of Japanese.",
        "LX has 10 words, as shown in Figure 2, and its written form does not have explicit word separators.",
        "Alphabet A, B, C, D, E, F Lexicon A, B, BC, CDE, CEF, DE, DF, EF, F, FB",
        "Then, suppose our goal is to find English-L2 translation pairs, e.g., \"paper-BC\", from a parallel corpus shown in Figure 3.",
        "English",
        "E1: Here is a paper.",
        "E2: He wrote a paper.",
        "E3: He read the paper.",
        "Since we presuppose that no external information is available besides the lexicon, a sentence in LX can be segmented in many ways, as shown in Figure 4.",
        "The problem tackled in this paper is how to find a translation of each English noun from possible words in the LX, or Japanese, side of the parallel corpus."
      ]
    },
    {
      "heading": "3 Algorithm",
      "text": [
        "Our algorithm is an extension of unsupervised segmentation using Hidden-Markov-Models (HNMIs) (Takeuchi and Matsumoto, 1995).",
        "The original segmentation algorithm tries to estimate H1l-IM parameters in such a way that",
        "the total entropy of the (unsegmented) training corpus is maximized.",
        "This approach has a great advantage since it is free from language dependent heuristics and since in such applications as speech recognition, the estimated parameters sometimes achieve even better performance than those estimated by supervised training.",
        "The problem, however, is that it cannot definitely guarantee that the resulting word boundaries are semantically correct.",
        "✭n order to solve this problem, our segmentation algorithm tries to include semantic information by using the aligned English sentences.",
        "Afore specifically, we extended the original unsupervised segmentation algorithm based on the following assumption❉ ✭f a Japanese word in a segmentation candidate has its translated word in the aligned English sentence, the Japanese word gets higher probability according to the strength of their translation relation.",
        "We have no bilingual dictionary, which is the output of our algorithm, so the translation relation between two words should be calculated somehow from the parallel corpus, e.g., by using mutual information.",
        "This calculation, however, requires solving the initial problem of segmenting Japanese sentences.",
        "To get around this chicken-and-egg problem, our algorithm iteratively estimates translation relations, beginning with tentatively segmenting Japanese sentences without using bilingual information, then estimating translation relations using the tentative segmentation.",
        "➞ext, the algorithm re-segments Japanese sentences considering the translation relations, resulting in updated translation relations.",
        "✭n order to cope with ambiguous segmentation, our algorithm estimates translation relations using every word in possible segmentations by assuming that each word➟s occurrence count is not a natural number but the probability of the word calculated by the Hll4A4 segmentation model.",
        "This process is equivalent to relating an English word and Japanese word if they co-occur in many translation pairs.",
        "For example, the sample LX corpus shown in Figure 3 includes three English sentences that share the word \"paper\".",
        "The word `①C\" is a candidate for the translation of \"paper\" since it appears in the set of candidates for every sentence in Lx (as shown in 4).",
        "The remaining part of this section first explains unsupervised learning of an n-gram model, then extends it for handling a parallel corpus.",
        "➡✍✏ ❂✘✫✪★✚✧➢✕✫✚✜ ➤✚✘✸✚✘➥✚ ✲✥❄✚✘✕❅✵✸✕✥✘ ✪✫✕✘✖ ➧✵✧❄✥➢ ➧✥✜✚✓✫ Let us concentrate on tokenizing a Japanese monolingual sentence.",
        "✭n the statistical framework, the most likely word-sequence ➫ ➩ for a sentence ➭ is formalized as follows (Takeuchi and Matsumoto, 1995), (➞agata, 1994)❉",
        "where ➸(➫) is the probability of a word sequence ➫ ➳ ➼✂➺ ➽➽➽ ➺ ➼➾ whose surface string is equal to that of ➭.",
        "This formula is transformed to the following form, known as the ❖-❏r◗▲ ▲❚➪❍➶, by approximating the probability of each word depending on ➹ preceding words.",
        "A larger N will make the model stronger, but a larger amount of data is required for estimating probabilities.",
        "Thus, a value of 2 or 3 is usually used.",
        "Hereafter, we fix N to 2 for simplicity.",
        "In order to implement a tokenizer using the n-gram model, we must solve the following problems.",
        "1.",
        "How to efficiently find TV �for given n-gram probabilities P(wi I wi-1).",
        "2.",
        "How to estimate n-gram probabilities.",
        "Since the first problem can be solved by using the dynamic programming technique, we concentrate on the second problem.",
        "The standard way to answer the second question is to find the probability distributions that can generate the given corpus with highest probability.",
        "Formally, this is achieved by finding the probability distributions that maximize the log-likelihood log(L) of the training corpus.",
        "If the training corpus is already segmented, log(L) is easily calculated as shown:",
        "where hi is the history of the word, wi.",
        "In the bigram case, hi is simply the preceding word, wi .",
        "In our problem, however, the corpus has no explicit segmentation.",
        "Thus, we consider every possible segmentation for each string as follows:",
        "where wp,s corresponds to a word whose surface form is s, starting from the position p, and + represents the context of wp,s, in this case, the previous word.",
        "The probability distributions P that maximize the above formula cannot be determined analytically but by an iterative algorithm called the Forward-Backward _Algorithm or the Baum-1,11'elch _Algorithm as roughly described below 2.",
        "Note that the algorithm presupposes possible segmentations and P(wp,sIh) are represented by a lattice.",
        "• Step 0 (Initialization) Assign P(wp,s I +) to every word in the lattice.",
        "Since P(wp,s I +) is unknown, we use the 0-gram (uniform) probability as the initial value.",
        "• Step 1",
        "For each sentence, estimate posterior (bigram) probability for each word in each (Japanese) sentence Jk, written as P(wp,s, +I A).",
        "Posterior probabilities are calculated by using forward and backward probabilities (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999).",
        "• Step 2",
        "Re-estimate the bigram (prior) probabilities based on the maximum likelihood estimation, regarding the posterior probability of each word as the real-valued occurrence frequency.",
        "Formally,",
        "• Step 3",
        "If some condition is satisfied, then stop, otherwise go to step 2.",
        "The condition could be the iteration count or total entropy.",
        "Detailed explanations are found in textbooks (Ra-biner and Juang, 1993), (Jurafsky and Martin, 2000), (Manning and Schuetze, 1999)"
      ]
    },
    {
      "heading": "3.2 Including Aligned Sentences",
      "text": [
        "Since the algorithm in the previous section tries to segment each sentence to maximize the log-likelihood, it is not necessarily optimal for finding translation pairs.",
        "In this section, we modify the algorithm under the assumption presented in Section 2.1.",
        "We begin by considering that the probability of a sentence in Japanese, A, is conditioned by the aligned sentence, Ek.",
        "Thus, the log-likelihood of this model is",
        "Instead of summing up every A, which means to consider all the words in Ek, we approximate the formula by using the maximum value 3",
        "We can rewrite this formula in such a way that each Japanese word depends on both the preceding word(s) and the aligned sentence.",
        "It is, however, hard to estimate a reliable value for P(wp,s I +, Ek) since it includes the entire sentence, Ek.",
        "One possibility is to use Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000).",
        "In this paper, we take a simpler approach.",
        "First, we decompose the above conditional probability into a monolingual part and a bilingual part.",
        "The monolingual part is exactly same as the previous n-gram probability.",
        "The bilingual part, P(wp,s I Ek), is expressed as the sum of all possible alignment A (Brown et al., 1990):",
        "Since A just specifies a(wj), i.e., the translation of wj, the above formula can be approximated as follows:",
        "We estimate P(wj I we)P(we I wj) by counting numbers of co-occurring words:",
        "where C(wj, we) is the number of times wj and we co-occur in one aligned pair, Cj (wj) is the frequency of wj in the Japanese part of the aligned corpus and Ce(we) is the frequency of we in the English part.",
        "To estimate P(wi I +, Ek), we use the algorithm shown in Section 3.1 replacing P(wi +) with P(wi I +, Ek) .",
        "1.",
        "Step 0 (same as in Section 3.1) 2.",
        "Step 1 (same as in Section 3.1) 3.",
        "Step 2 Re-estimate P(wp,s 1 +,,5,) in the following form:",
        "where P(wp,s I +) is given by (5) and P(wp,,IEk) is given by (11).",
        "Note that Cj (wj) is calculated by summing the posterior probabilities of wj in the entire Japanese segmentation candidates.",
        "4.",
        "Step 3 (same as in Section 3.1)"
      ]
    },
    {
      "heading": "3.3 Bilingual Dictionary Construction",
      "text": [
        "After segmentation is completed, translation pairs can be found by simply retrieving such combinations of w and e where P(wj, we) and/or C(wj, we) are over a certain threshold."
      ]
    },
    {
      "heading": "4 Evaluation Experiments",
      "text": [
        "We applied the proposed algorithm to a Japanese-English corpus of travel expressions.",
        "The corpus includes 200 k sentence pairs that are considered to be frequently used in traveling abroad.",
        "This corpus is divided into two non-overlapping sets: a training set with 190 k sentences and a test set with 0.5 k sentences"
      ]
    },
    {
      "heading": "4",
      "text": [
        "The lexicon of Japanese is compiled by accumulating all the entries of the dictionary of ChaSen (Matsumoto et al., 1997), a Japanese morphological analyzer.",
        "Every word in the English part of the corpus was assigned a part-of-speech.",
        "Using the part-of-speech, we restricted ourselves to finding translations of nouns.",
        "In the following experiments, we modified the formula (S) as:",
        "where A determines the weight of the translation probability."
      ]
    },
    {
      "heading": "4.1 Segmentation Accuracy",
      "text": [
        "First, we evaluated the segmentation performance in terms of the entropy, word accuracy and noun recall for the test corpus.",
        "The relation between total entropy and the iteration count is shown in Figure 5.",
        "The word accuracy is defined as follows:",
        "where Ins, Del, and Sub are, respectively, counts of insertion, deletion, and substitution words as compared with the reference data.",
        "The noun recall is how many nouns in the correctly segmented sentences are identified as words (i.e., nouns) by the tokenizer.",
        "# of nouns correctly identi f ied by the system"
      ]
    },
    {
      "heading": "total # of nouns",
      "text": [
        "The values for different A are shown in Table 1.",
        "For comparison, results of supervised tokenization (Nagata, 1994)5 were included in the table.",
        "Since the unsupervised algorithm tries to find segmentation such that the total entropy is minimized', the total entropy for each result is smaller than that of supervised learning.",
        "This shows that our method can produce segmentations useful for building language models for speech recognition etc.",
        "On the other hand, word accuracies, which are roughly linear to segmentation accuracies, are worse than the results of supervised tokenization, as expected.",
        "Many errors were found in functional words and suffixes.",
        "Functional words are often wrongly concatenated to adjacent functional words when they occur frequently.",
        "The unsupervised tokenizer often separates an inflexive suffix of a verb from the main part, although the reference segmentation treats them as one unit.",
        "The noun recall for each run is significantly greater than the word accuracy.",
        "This shows that translation relations are useful for obtaining correct word boundaries and identifying nouns.",
        "Considering the fact that the number of functional words is limited and a small training corpus improves segmentation accu-racy(Takeuchi and Matsumoto, 1995), the best method would be to combine the supervised method with our algorithm which is effective for content words."
      ]
    },
    {
      "heading": "4.2 Correctness of Translation Pairs",
      "text": [
        "Next, we investigated how correctly our algorithm could compile a bilingual dictionary.",
        "In this evaluation, we collected every bilingual pair, j, e, that satisfied the following condition:",
        "C(wj, we) > = 1, ... , 5).",
        "This condition states that the co-occurrence frequency of the translation pair should be greater than or equal to a threshold, All.",
        "We compared our algorithm with the following two settings.",
        "1.",
        "Dictionary Only (DIC)",
        "As a baseline, we stopped our procedure after the first iteration.",
        "This means that every possible segmentation is equally likely."
      ]
    },
    {
      "heading": "2. Supervised Segmentation (SUP)",
      "text": [
        "This case also stops the procedure before re-estimation, but instead of uniform distribution for initialization, we used probabilities estimated from a manually segmented training corpus.",
        "The correctness of every translation pair produced by the above methods was evaluated by professional translators.",
        "The results are shown in Table 2, where accuracy is defined as: # of correctpairs total # of outputs This table shows that our algorithm is more accurate and found more bilingual pairs than the method using every possible word and is slightly less accurate than the method using supervised segmentation."
      ]
    },
    {
      "heading": "5 Concluding Remarks",
      "text": [
        "In this paper, we proposed an algorithm that extracts translation pairs from an English-Japanese parallel raw corpus.",
        "The unique feature of our algorithm is that it does not",
        "require any separate tokenizer for Japanese.",
        "This means that the algorithm can find translation pairs from any parallel corpus consisting of a language with trivial word boundaries and a language without them.",
        "Experimental results showed that the algorithm achieved only a 0.4 points lower accuracy than supervised segmentation.",
        "The proposed algorithm can be elaborated in many ways.",
        "One direction would be to use a better formula for the degree of translation relation.",
        "Another direction would be to improve the method of combining an n-gram probability and a translation probability.",
        "The maximum entropy approach as stated before (McCallum et al., 2000) for coupling two probabilities in a principled way is promising."
      ]
    },
    {
      "heading": "6 Acknowledgment",
      "text": [
        "The research reported here was supported in part by a contract with the Telecommunications Advancement Organization of Japan entitled, \"A study of speech dialogue translation technology based on a large corpus\"."
      ]
    }
  ]
}
