{
  "info": {
    "authors": [
      "Francisco Casacuberta",
      "Enrique Vidal",
      "Juan-Miguel Vilar"
    ],
    "book": "Workshop on Speech-To-Speech Translation: Algorithms and Systems",
    "id": "acl-W02-0706",
    "title": "Architectures for Speech-To-Speech Translation Using Finite-State Models",
    "url": "https://aclweb.org/anthology/W02-0706",
    "year": 2002
  },
  "references": [
    "acl-N01-1018",
    "acl-W00-0508"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Speech-to-speech translation can be approached using finite state models and several ideas borrowed from automatic speech recognition.",
        "The models can be Hidden Markov Models for the accous-tic part, language models for the source language and finite state transducers for the transfer between the source and target language.",
        "A “serial architecture” would use the Hidden Markov and the language models for recognizing input utterance and the transducer for finding the translation.",
        "An “integrated architecture”, on the other hand, would integrate all the models in a single network where the search process takes place.",
        "The output of this search process is the target word sequence associated to the optimal path.",
        "In both architectures, HMMs can be trained from a source-language speech corpus, and the translation model can be learned automatically from a parallel text training corpus.",
        "The experiments presented here correspond to speech-input translations from Spanish to English and from Italian to English, in applications involving the interaction (by telephone) of a customer with the front-desk of a hotel."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Present finite-state technology allows us to build speech-to-speech translation (ST) systems using ideas very similar to those of automatic speech recognition (ASR).",
        "In ASR the acoustic hidden Markov models (HMMs) can be integrated into the language model, which is typically a finite-state grammar (e.g. a N-gram).",
        "In ST the same HMMs can be integrated in a translation model which consists in a stochastic finite-state transducer (SFST).",
        "Thanks to this integration, the translation process can be efficiently performed by searching for an optimal path of states through the integrated network by using well-known optimization procedures such as (beam-search accelerated) Viterbi search.",
        "This “integrated architecture” can be compared with the more conventional “serial architecture”, where the HMMs, along with a suitable source language model, are used as a front-end to recognize a sequence of source-language words which is then processed by the translation model.",
        "A related approach has been proposed in (Bangalore and Ricardi, 2000; Bangalore and Ricardi, 2001).",
        "In any case, a pure pattern-recognition approach can be followed to build the required systems.",
        "Acoustic models can be trained from a sufficiently large source-language speech training set, in the very same way as in speech recognition.",
        "On the other hand, using adequate learning algorithms (Casacuberta, 2000; Vilar, 2000), the translation model can also be learned from a sufficiently large training set consisting of source-target parallel text.",
        "In this paper, we comment the results obtained using this approach in EUTRANS, a five-year joint effort of four European institutions, partially funded by the European Union.",
        "2 Finite-state transducers and speech translation The statistical framework allow us to formulate the speech translation problem as follows: Let x be an acoustic representation of a given utterance; typically a sequence of acoustic vectors or “frames”.",
        "The translation of x into a target-language sentence can be formulated as the search for a word sequence, ˆt, from the target language such that:",
        "Conceptually, the translation can be viewed as a two-step process (Ney, 1999; Ney et al., 2000): x→s→t, where s is a sequence of source-language words which would match the observed acoustic sequence x and t is a target-language word sequence associated with s. Consequently,",
        "and, with the natural assumption that Pr(x|s, t) does not depend on the target sentence t,",
        "Using a SFST as a model for Pr(s, t) and HMMs to model Pr(x|s), Eq.",
        "3 is transformed in the optimization problem: for q,q'∈Q,a∈Σ,ω∈Δ* and1 P:R→IR+ (transition probabilities) and F : Q → IR+ (final-stateprobabilities) are functions such that ∀q ∈ Q:",
        "Fig.",
        "1 shows a small fragment of a SFST for Spanish to English translation.",
        "A particular case of finite-state transducers are known as subsequential transducers (SSTs).",
        "These are finite-state transducers with the restriction of being deterministic (if (q, a, ω, q), (q, a, ω', q') ∈ R, then ω = ω' and q = q').",
        "SSTs also have output strings associated to the (final) states.",
        "This can fit well under the above formulation by simply adding an end-off-sentence marker to each input sentence.",
        "For a pair (s, t) ∈ Σ* × Δ*, a translation form, φ,is a sequence of transitions in a SFST T: φ: (q0,s1,˜t1,q1), (q1,s2,˜t2,q2), .",
        ".",
        ".",
        ", (qI−1, sI, ˜tI , qI), where ˜tj denotes a substring of target words (the empty string for ˜tj is also possible), such that ˜t1 ˜t2 ... ˜tI = t and I is the length of the source sentence s. The probability of φ is",
        "where PrT(s, t) is the probability supplied by the SFST and PrM(x|s) is the density value supplied by the corresponding HMMs associated to s for the acoustic sequence x."
      ]
    },
    {
      "heading": "2.1 Finite-state transducers",
      "text": [
        "A SFST, T, is a tuple (Q, Σ, Δ, R, q0, F, P), where Q is a finite set of states; q0 is the initial state; Σ and Δ are finite sets of input symbols (source words) and outputsymbols (target words), respectively (Σ∩Δ = ∅); R is a set of transitions of the form (q, a, ω, q') obtained by removing the target (source) words from each transition of the model.",
        "where d(s, t) is the set of all translation forms for the pair (s, t).",
        "These models have implicit source and target language models embedded in their definitions, which are simply the marginal distributions of PrT.",
        "In practice, the source (target) language model can be",
        "be translated to either “a double room” or “a room with two beds”.",
        "The most probable translation is the first one with probability of 0.09.",
        "The structural (states and transitions) and the probabilistic components of a SFST can be learned automatically from training pairs in a single process using the MGTI technique (Casacuberta, 2000).",
        "Alternatively, the structural component can be learned using the OMEGA technique (Vilar, 2000), while the probabilistic component is estimated in a second step using maximum likelihood or other possible criteria (Pic´o and Casacuberta, 2001).",
        "One of the main problems that appear during the learning process is the modelling of events that have not been seen in the training set.",
        "This problem can be confronted, in a similar way as in language modelling, by using smoothing techniques in the estimation process of the probabilistic components of the SFST (Llorens, 2000).",
        "Alternatively, smoothing can be applied in the process of learning both components (Casacuberta, 2000)."
      ]
    },
    {
      "heading": "2.2 Architectures for speech translation",
      "text": [
        "Using Eq.",
        "7 as a model for Pr(s, t) in Eq.",
        "4,",
        "For the computation of PrM (x s) in Eq.",
        "8, let b be an arbitrary segmentation of x into I acoustic subsequences, each of which associated with a source word (therefore, I is the number of words in s).",
        "Then:",
        "where ¯xi is the i-th.",
        "acoustic segment of b, and each source word si has an associated HMM that supplies the density value PrM (¯xi Isi).",
        "Finally, by substituting Eq.",
        "5 and Eq.",
        "9 into Eq.",
        "8 and approximating sums by maximisations:",
        "Solving this maximisation yields (an approximation to) the most likely target-language sentence tˆ for the observed source-language acoustic sequence x.",
        "This computation can be accomplished using the well known Viterbi algorithm.",
        "It searches for an optimal sequence of states in an integrated network (integrated architecture) which is built by substituting each edge of the SFST by the corresponding HMM of the source word associated to the edge.",
        "This integration process is illustrated in Fig. 2.",
        "A small SFST is presented in the first panel (a) of this figure.",
        "In panel (b), the source words in each edge are substituted by the corresponding phonetic transcription.",
        "In panel (c) each phoneme is substituted by the corresponding HMM of the phone.",
        "Clearly, this direct integration approach often results in huge finite-state networks.",
        "Correspondingly, a straightforward (dynamic-programming) search for an optimal target sentence may require a prohibitively high computational effort.",
        "Fortunately, this computational cost can be dramatically reduced by means of standard heuristic acceleration techniques such as beam search.",
        "An alternative, which sacrifices optimality more drastically, is to break the search down into two steps, leading to a so-called “serial architecture”.",
        "In the first step a conventional source-language speech decoding system (using just a source-language language model) is used to obtain a single (may be multiple) hypothesis for the sequence of uttered words.",
        "In the second step, this text sequence is translated into a target-language sentence.",
        "Using Pr(s, t) = Pr(t |s) · Pr(s) in Eq.",
        "3 and approximating the sum by the maximum, the optimization problem can be presented as",
        "In other words, the search for an optimal target-language sentence is now approximated as follows:",
        "1.",
        "Word decoding of x.",
        "A source-language sentence sˆ is searched for using a source language model, PrN(s), for Pr(s) and the corresponding HMMs, PrM(x|s), to model Pr(x|s): sˆ ≈ argmax (PrN(s) · PrM(x|s)) .",
        "s 2.",
        "Translation of ˆs.",
        "A target-language sentence tˆ is searched for using a SFST, PrT(ˆs, t), as a model of Pr(ˆs, t)",
        "A better alternative for this crude “two-step” approach is to use Pr(s, t) = Pr(s |t) · Pr(t) in Eq.",
        "3.",
        "Now, approximating the sum by the maximum, the optimization problem can be presented as",
        "The main problem of this approach is the term t that appears in the first maximisation (Eq.",
        "16).",
        "A possible solution is to follow an iterative procedure where t, that is used for computing ˆs, is the one obtained from argmaxt Pr(ˆs, t) in the previous iteration (Garc´ıa-Varea et al., 2000).",
        "In this case, Pr(s |t) can be modelled by a source language model that depends on a previously computed ˜t: PrN,˜t (s).",
        "In the first iteration no tˆ is known, but PrN,t(s) can be approximated by PrN(s).",
        "Following this idea, the search can be formulated as:"
      ]
    },
    {
      "heading": "Initialization:",
      "text": [
        "Let PrN ,t (s) be approximated by a source language model PrN(s).",
        "while not convergence",
        "1.",
        "Word decoding of x.",
        "A source-language sentence sˆ is searched for using a source language model that depends on the target sentence, PrN ,t(s), for Pr(s |t) (˘t is the tˆ computed in the previous iteration) and the corresponding HMMs, PrM (x |s), to model Pr(x |",
        "2.",
        "Translation of ˆs.",
        "A target-language sentence tˆ is searched for using a SFST, PrT(ˆs, t), as a model of Pr(ˆs, t)",
        "end of while The first iteration corresponds to the sequential architecture proposed above.",
        "While this seems a promising idea, only very preliminary experiments were carried out (Garc´ıaVarea et al., 2000) and it has not been considered in the experiments presented in the present paper."
      ]
    },
    {
      "heading": "3 Experiments and results",
      "text": [
        "Three sets of speech-to-speech translation prototypes have been implemented for Spanish to English and for Italian to English.",
        "In all of them, the application was the translation of queries, requests and complaints made by telephone to the front desk of a hotel.",
        "Three tasks of different degree of difficulty have been considered.",
        "In the first one (EUTRANS-0), Spanish-to-English translation systems were learned from a big and well controlled training corpus: about 170k different pairs (≈ 2M running words), with a lexicon of about 700 words.",
        "In the second one (EUTRANS-I), also from Spanish to English, the systems were learned from a random subset of 10k pairs (≈ 100k running words) from the previous corpus; this was established as a more realistic training corpus for the kind of application considered.",
        "In the third and most difficult one, from Italian to English (EUTRANS-II), the systems were learned from a small training corpus that was obtained from a transcription of a spontaneous speech corpus: about 3k pairs (≈ 60k running words), with a lexicon of about 2,500 words.",
        "For the serial architecture, the speech decoding was performed in a conventional way, using the same acoustic models as with the integrated architecture and trigrams of the source language models.",
        "For the integrated architecture, the speech decoding of an utterance is a sub-product of the translation process (the sequence of source words associated to the optimal sequence of transitions that produces the sequence of target words).",
        "The acoustic models of phone units were trained with the HTK Toolkit (Woodland, 1997).",
        "For the EUTRANS-0 and EUTRANS-I prototypes, a training speech corpus of 57,000 Spanish running words was used, while the EUTRANS-II Italian acoustic models were trained from another corpus of 52,000 running words Performance was assessed on the base of 336 Spanish sentences in the case of EUTRANS-0 and EUTRANS-I and 278 Italian sentences in EUTRANS-II.",
        "In all the cases, the test sentences (as well as the corresponding speakers) were different from those appearing in the training data.",
        "For the easiest task, EUTRANS-0, (well controlled and a large training set), the best result was achieved with an integrated architecture and a SFST obtained with the OMEGA learning technique.",
        "A Translation Word Error Rate of 7.6% was achieved, while the corresponding source-language speech decoding Word Error Rate was 8.4%.",
        "Although these figures may seem strange (and they would certainly be in the case of a serial architecture), they are in fact consistent with the fact that, in this task (corpus), the target language exhibits a significantly lower perplexity than the source language.",
        "For the second, less easy task EUTRANS-I, (well controlled task but a small training set), the best result was achieved with an integrated architecture and a SFST obtained with the MGTI learning technique (10.5% of word error rate corresponding to the speech decoding and 12.6% of translation word error rate).",
        "For the most difficult task, EUTRANS-II (spontaneous task and a small training set), the best result was achieved with a serial architecture and a SFST obtained with the MGTI learning technique (22.1% of word error rate corresponding to the speech decoding and 37.9% of translation word error rate).",
        "Several systems have been implemented for speech-to-speech translation based on SFSTs.",
        "Some of them were implemented for translation from Italian to English and the others for translation from Spanish to English.",
        "All of them support all kinds of finite-state translation models and run on low-cost hardware.",
        "They are currently accessible through standard telephone lines with response times close to or better than real time.",
        "From the results presented, it appears that the integrated architecture allows for the achievement of better results than the results achieved with a serial architecture when enough training data is available to train the SFST.",
        "However, when the training data is insufficient, the results obtained by the serial architecture were better than the results obtained by the integrated architecture.",
        "This effect is possible because the source language models for the experiments with the serial architecture were smoothed trigrams.",
        "In the case of sufficient training data, the source language model associated to a SFST learnt by the MGTI or OMEGA is better than trigrams (Section 2.1).",
        "However, in the other case (not sufficient training data) these source languages were worse than trigrams.",
        "Consequently an important degradation is produced in the implicit decoding of the input utterance."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors would like to thank the researchers that participated in the EUTRANS project and have developed the methodologies that are presented in this paper.",
        "This work has been partially supported by the European Union under grant IT-LTR-OS-30268, by the project TT2 in the “IST, V Framework Programme”, and Spanish project TIC 2000-1599-C02-01."
      ]
    }
  ]
}
