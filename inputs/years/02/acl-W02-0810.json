{
  "info": {
    "authors": [
      "Radu Florian",
      "Richard Wicentowski"
    ],
    "book": "Workshop on Word Sense Disambiguation: Recent Successes and Future Directions",
    "id": "acl-W02-0810",
    "title": "Unsupervised Italian Word Sense Disambiguation Using WordNets and Unlabeled Corpora",
    "url": "https://aclweb.org/anthology/W02-0810",
    "year": 2002
  },
  "references": [
    "acl-J01-3001",
    "acl-J98-1004",
    "acl-P00-1027",
    "acl-P95-1026",
    "acl-P97-1007",
    "acl-P98-1029",
    "acl-P98-1081",
    "acl-W97-0209"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a novel method for unsupervised word sense disambiguation, which combines multiple information sources, including semantic relations, large unlabeled corpora, and cross-lingual distributional statistics.",
        "This method extends and builds on the JHU system that participated in the SENSEVAL2 exercise.",
        "Experiments performed on the SENSEVAL2 Italian lexical-sample data show significant improvements over previously published results on this data set."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The goal of this paper is to present an unsupervised word sense disambiguation system which extends the JHU system for the Italian lexical sample task which participated in the SENSEVAL2 exercise (Yarowsky et al., 2001).",
        "Our system combines word semantic relations, large unlabeled corpora and cross-lingual distributional statistics.",
        "The combining system reduces the word sense error rate by 8.2% absolute (13.6% relative error reduction), when compared to the best system submitted in SENSEVAL2."
      ]
    },
    {
      "heading": "1.1 Previous Work",
      "text": [
        "Several approaches that address the problem of unsupervised word sense disambiguation (WSD henceforth) have been presented in the past few years.",
        "In one of the most widely-cited unsupervised WSD systems, Yarowsky (1995) uses a very small seed set (2-3 examples) to bootstrap a WSD algorithm based on decision lists; the algorithm yields highly accurate results, competitive with similar supervised systems.",
        "Schütze (1998) creates word vectors by extracting ambiguous words and their contexts from an unlabeled corpus.",
        "After clustering the vectors', the disambiguation is performed by selecting the sense centroid closest to the test word vector.",
        "Pedersen and Bruce (1998) use an EM-based algorithm to group sentences containing the target word into unlabeled clusters which are then mapped to sense tags.",
        "The existence of the freely available word sense relation database WordNet (Miller, 1995) has enabled the conception of several unsupervised WSD systems, including those by Resnik (1997) who uses syntactically parsed corpora, partially hand-labeled with senses from WordNet (Miller, 1995), to train a selectional preference system, and McCarthy et al.",
        "(2001), which uses a selectional preference model similar to Resnik (1997), but without the use of any labeled data, in the \"one sense per discourse\" paradigm (Gale et al., 1992), achieving good precision in the SENSEVAL2 English all-words task.",
        "Using a cross-lingual approach, Rigau et al.",
        "(1997) investigates an unsupervised system using a combination of monolingual dictionaries, bilingual dictionaries and the English WordNet.",
        "The bilingual dictionaries were used to map words from Spanish and French into English in order to leverage the semantic relationships in the English WordNet.",
        "In an approach similar in spirit to the one presented here, Mihalcea and Moldovan (1999) use the entire English WordNet to find sentences in online texts containing high-confidence examples of the target word.",
        "The WordNet glosses and acquired sentences are used as training data to automatically create a large sense-tagged corpus.",
        "In work on the same dataset as used in this research, Magnini et al.",
        "(2001) manually anno",
        "tates the relevant Italian synsets with a semantic class.",
        "The test samples are assigned to one of these classes using a supervised algorithm trained on an annotated English corpus and then these classes were mapped back to the WordNet synsets.",
        "Unsupervised word sense disambiguation using WordNet relations also has also been used in real-world tasks.",
        "Some of the earlier examples include Voorhees (1993) (using the hyponym/hypernym relations from WordNet) and Sussna (1993) (using weighted relations derived from WordNet), employing word sense disambiguation to increase the performance of information retrieval systems."
      ]
    },
    {
      "heading": "2 Feature Representation",
      "text": [
        "In the model presented in this research, documents are represented as bags of words and/or lemmas; in addition, local n-grams around the ambiguous word are also part of the document's vector:",
        "where c� is the number of times the feature j3 appears in document d, N is the number of words in d and Wj is a weight associated with feature j4.",
        "Confusion between the same word participating in multiple feature roles is avoided by appending the feature values with their positional type (e.g. uomo_ L is different from uomo in unmarked bag-of-words context).",
        "All test documents were part-of-speech tagged using the Italian version of the decision tree-based POS tagger described in Schmid (1994).",
        "Extracting lemma information from Italian is an important process – Section 6 evaluates a scenario where lemmatization is not used, and shows that a substantial decrease in performance occurs.",
        "Lemmatization is performed using the supervised morphological analyzer from",
        "Yarowsky and Wicentowski (2000), trained on the filtered output of the POS tagger.",
        "We tested the accuracy of the morphological analyzer by randomly selecting 500 adjectives, 500 nouns and 500 verbs5 in proportion to their token frequency in the unannotated corpus and had them hand-checked by a native speaker.",
        "Table 1 presents the POS and lemmatization accuracy for these 1500 words; the lemmatization accuracy is reported only on examples which were correctly labeled by the POS tagger."
      ]
    },
    {
      "heading": "3 Information Sources",
      "text": []
    },
    {
      "heading": "3.1 Italian WordNet",
      "text": [
        "Since no training data was available for this task, we rely on alternative sources of information for inducing sense classification.",
        "One central resource in this process is the ItalWordNet, version 1.0, developed by the Italian National Project, SI-TAL (Roventini et al., 2000), which was provided with the data.",
        "This structure describes various semantic relationships between words (usually binary relations), including:",
        "• synonymy – word a is a synonym of word v if word a has nearly the same definition as word v. • antonymy – word a is an antonym of word v",
        "if words a and v have nearly opposite meanings.",
        "• hyperonymy - word a is a hyperonym of word v if a is a generalization of word v; • hyponymy – the reverse of the hyperonymy relation; • meronymy – word a is a meronym of word v if the object represented by a has the object represented by v as a part (for instance, car is a meronym of wheel); • holonymy – is the reverse relation of meronymy;",
        "ItalWordNet is not a freely available resource; only the parts that were provided with the task have been used in this research'.",
        "5 As identified by the POS tagger.",
        "sOf the 40248 synsets present in the ItalWordNet, only 616 were provided .",
        "Of the 83 words that are part of the evaluation, 82 of them had entries in the ItalWordNet; one word, Bello, had no direct entry, but there were entries related to this particular word in the other entries, and we used those as inductive bias in the classification.",
        "Table 2 shows the number of different relations present in the selected ItalWordNet.",
        "Intuitively, some of the WordNet relations are more useful than others for sense disambiguation.",
        "For instance, the synonymy and near-synonymy relations are more relevant than the role_ instrument relation.",
        "To address this problem, each relation is assigned a intuitively-motivated weight7; each relation influences the overall behavior of the algorithm proportionally to its weight."
      ]
    },
    {
      "heading": "3.2 Relations to the English WordNet",
      "text": [
        "In addition to relations among Italian words, the ItalWordNet contains links to the English WordNet senses of the corresponding translations (if any exist).",
        "In some cases, direct translations are not present, but a relation to a English WordNet sense is present (such as eq_ has_ hyponym or eq_generalization).",
        "These resources provide access to an independent information source – the distributional frequency of these sense as found in the English WordNet (which is present in version 1.7) (Miller, 1995).",
        "This information is used to obtain a second word sense classifier, used in system combination (as presented in Section 5).",
        "Since the English senses in ItalWordNet are the senses in WordNet 1.5, we used the sense mappings wn1.5 – � wn1.6 – � wn1.7, as described in Daude et al.",
        "(1999)8.",
        "Since no training data was available, the weights could not be adjusted to minimize error rate.",
        "An alternative would be to estimate the weights on known classifications, e.g. English, and assume that they are language independent.",
        "\"The mappings were obtained from http://www.Isi.upc.",
        "es1-n1p1tools1mapping.html"
      ]
    },
    {
      "heading": "4 The Algorithm",
      "text": [
        "At a high level, the proposed algorithm for disambiguating a word v consists of first identifying words w that are similar in sense with word v, and selecting contexts of 2-3 sentences containing words the words w (including contexts containing the word v itself), creating sense centroids using these contexts, and bootstrapping a K-means clustering algorithm with the initial seeds.",
        "The algorithmic framework used in this research is based on the following assumption: Assumption 1 If a word u that has a sense s,,, similar to the sense s„ of word v (as identified by a relation in ItalWordNet between s,,, and s„), then any context containing word u is indicative of sense s,,.",
        "Assumption 1 can yield poor results in cases where two senses of word w are associated with different senses of the same word a (such as press and suit); in this case, sentences corresponding to word a will contribute to both senses of word w associated with u.",
        "The hope in this case is that all the words participating in disambiguation will cooperate in increasing the likelihood of the correct sense, and the effect of ambiguous examples will constitute white noise in the mass of relevant distinctions.",
        "If the noise is actually biased, the algorithm may fail to identify the correct sense."
      ]
    },
    {
      "heading": "4.1 Identifying Relevant Contexts",
      "text": [
        "From an engineering point of view, the ItalWordNet is considered to be a set of relations W defined on the set of word-sense pairs.",
        "Formally, to identify the degree to which two senses are related, we construct a weighted multigraph GW = (V, E), where the set of vertices V is defined as V = {(v, s) Is is a sense of v} and the set of edges is defined as",
        "The weight associated with an edge, W (e) is the weight of the relation associated with the edge; we will interpret these weights as distances rather than similarities (smaller weights indicate more similar senses).",
        "To identify the set of words that are related in meaning with an ambiguous word w, we start from the senses corresponding to word w, Go = {( w,s1),...,(w,sn)}, and we then expand the set G in the graph GW as follows",
        "Intuitively, we expand the set of words that are related to the ambiguous word w by examining the relations r in which its senses are involved, after which we expand the newly obtained senses, and so on.",
        "The relationships are expanded by examining the sense of each node, but the final output will extract the words associated with those senses'.",
        "Once the final set GK is computed, the relevance of each word in it is computed by W (l) ���p) path from Go to l w(7 l� and wG (lo, ... , In) _ Ei wG (Ii, li+1)• The next step in the algorithm is to extract contexts cl associated with each word l in GK – for this purpose, we used a corpus of clean Italian newspaper text (extracted from Corriere Della Sera, 1993).",
        "After expanding the set G1 (5543 words; initially, there are 83 ambiguous words), the selected contexts formed a corpus of approximately 700M wordsio"
      ]
    },
    {
      "heading": "4.2 Automatic Sense Clustering",
      "text": [
        "Algorithm 1 presents the proposed K-means-style clustering procedure, consisting of two major parts: computing the initial sense centroids, and the the application of K-means clustering.",
        "The initial centroids are computed by seeding given them by the contexts cl; each such context has a similarity to a particular centroid, inherited from the word that induced the context.",
        "In the following step, the test documents – including the contexts cl containing the ambiguous word – are assigned to the sense centroids (equation (4)), by computing the similarity between their corresponding vectors and the sense centroid vectors.",
        "There are several choices for the similarity measure; the one displayed in equation (4) is computing the similarity as",
        "1.",
        "Input: the ambiguous word w. 2.",
        "Create the extended set of lemmas GK as described in equation M. 3.",
        "For each lemma l E Gx, select contexts cl surrounding l from a large unlabeled corpus.",
        "4.",
        "Assign the contexts cl to centroids correspond",
        "ing to the senses of word w : S1 ... SN:",
        "5.",
        "Compute the similarity of each context ct (corresponding to the test samples) to the centroid si.",
        "For example:",
        "6.",
        "Assign all the test centroids ct to senses si, based on the similarity between the centroids ct and si: si =1:Sim (ct, si) • ct (5) ct test context 7.",
        "Repeat from step 6 until convergence or a desired number of iterations is reached.",
        "8.",
        "Classify each test document t with the sense corresponding to the closest centroid",
        "and makes use of the naive Bayes assumption that the words in document cl are independent given the sense, yielding",
        "Other possible choices for the similarity sim (cl, si) include Bayes ratio (Gale et al., 1992)",
        "and cosine similarity P (si Icl) _ (si, cl) II M12 IICI I12 (10) Once the similarities P (si Icl) have been computed, the centroids (si)i can be updated as in",
        "(2) equation (5) – the centroid assignment shown in (10) is a soft one – each document will contribute to every centroid, with a ratio corresponding to their similarity.",
        "An alternative is to use hard document assignment",
        "where each document will contribute only to the centroid closest to it; S is the Kronecker symbol:"
      ]
    },
    {
      "heading": "5 Combining Information Sources",
      "text": [
        "It is quite useful for a classification task to have access to multiple information sources; it follows from a standard argument that the information one has about a process (as measured by the entropy of the process) can only decrease if one obtains additional information:",
        "Several studies in the machine learning community have shown that combining the information obtained from several classifiers not only results in improved performance, but also improves robustness.",
        "Even in cases where one has access to several structurally different information sources that are not easily integrable (Stevenson and Wilks, 2001), it might be more beneficial to construct separate classifiers for each type of context and combine their outputs, than construct a more complex classifier that tries to handle the combination internally.",
        "Aside from the output of the classifier described in Section 4, we have access to two other information sources:",
        "• the English distributional usage of the translation of a particular Italian word sense; • the output of another word sense disambiguation system (Magnini et al., 2001)",
        "(downloadable from the SENSEVAL2 web site).",
        "In using the English distributional data, we make the following assumption: Assumption 2 If an Italian word sense sI has an English translation in sense SE, then the usage of sense SE in English is characteristic of the usage of sense sI in Italian.",
        "By using this assumption, we obtain another classifier, as depicted in Algorithm 2.",
        "Even though this classifier is relatively simple, it obtains reasonably good results, as we will see in the experimental section.",
        "Given N classifiers (possibly having probabilistic output), an easy and effective way of combining their output is through voting (Brill and Wu, 1998; van Halteren et al., 1998; Yarowsky et al., 2001), by computing the output classification as",
        "where si (d) = arg max, Pi (sId).",
        "In other words, each classifier votes for the sense which it considers most likely, weighted by the probability of that sense.",
        "In the end, the sense that has been voted the most winsil"
      ]
    },
    {
      "heading": "6 Experimental Evaluation",
      "text": [
        "The test data in the Italian lexical-sample task consists of 3889 contexts of 1 to 3 sentences, for 83 ambiguous words."
      ]
    },
    {
      "heading": "6.1 Influence of Morphological Analysis on Performance",
      "text": [
        "To investigate the impact of using the morphological analyzer, we created a second set Ek derived from an unlemmatized corpus.",
        "This means that we do not include documents which are associated with inflections of words related by ItalWordNet, and that the clustering algorithm is run on this unlemmatized data set.",
        "As presented in Table 3, line 2, there is a significant decrease in performance when the morphological analyzer was omitted.",
        "Using the same initial corpus, approximately 75YO of the originally extracted documents were selected 12; however, our algorithm on the set Ek achieves only 34.6010 accuracy, significantly lower than the results obtained by the full system on 75% of the data, 36.6% (as shown in Figure 1).",
        "The unlemmatized system's performance is most comparable to the performance achieved when using only 10% of the corpus.",
        "In other words, it takes a corpus 10 times larger in order to compensate for the inability to perform morphological analysis."
      ]
    },
    {
      "heading": "6.2 Classification Results",
      "text": [
        "Figure 1 presents the results obtained by the algorithm presented in Section 4, for varying sizes of the unlabeled corpus.",
        "The performance increases from 34.5% at 70M words to 37.1% at 700M words (the difference in performance is statistically significant at a confidence level of 10-3).",
        "The experiments we ran to test the performance of the classifier are presented in Table 3.",
        "There are many baselines against which one can measure performance.",
        "Line 1 of Table 3 shows the estimated performance of a system that chooses a sense at random.",
        "In line 2, the performance of the system is evaluated by running the system without lemmatization, as discussed in Section 6.1.",
        "Line 4 presents the performance of the K-means system presented in Section 4, and line 3 presents the performance of the",
        "original JHU system.",
        "The difference between the 2 consists mainly in the size of unlabeled corpus and quality of lemmatization.",
        "Perhaps the most interesting result in Table 3 is the fact that the estimation of the most likely sense for a given word can be done more robustly than the estimation of individual senses.",
        "By making the system return the most likely sense (of the senses output by the clustering algorithm) for a given word, the performance increases by nearly 4% (line 6 in the table), yielding the best individual system results.",
        "The classifier based on the sense distributions on the English WordNet, described in Algorithm 2, yields good performance (line 7), comparable with the best SENSEVAL2 system (line 5).",
        "By combining the two most likely systems (the one obtained on the Italian data by using K-means clustering and the one obtained from the English WordNet), one can obtain an impressive performance of 46.4%, and adding the best competing system from the Italian sENSEVAL2 exercise, we obtain a performance of 47.2%.",
        "The difference in performance between the last two systems is not, however, statistically significant at a confidence level of 0.05.",
        "As a final observation on the system performance, it is interesting to remark that there is still a long way to go before obtaining results that are competitive with the most-likely oracle performance, listed in the table on line 10.",
        "This oracle returns for each sample the \"true\" most-likely sense (computed on the test data); it obtains a performance of 65.3%, substantially better than the other results obtained on this data.",
        "This oracle is outperformed by a voting oracle, which returns the correct sense if at least 1e+08 2e+08 3e+08 4e+08 5e+08 6e+08 7e+08 Size of unlabeled corpus (words) one classifier predicted it, (line 11 of Table 3)13"
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "In conclusion, we have presented a novel method of word sense classification by using large amounts of unlabeled data, word semantic relations both in the target and a second language.",
        "The procedure integrates these knowledge sources to provide a more robust estimation.",
        "The performance obtained, while still lower than the true most likely classification, substantially outperforms previously published results on this data set.",
        "As future work, we plan to integrate more sophisticated syntactic knowledge/features into the model, to develop a better weighting scheme of individual semantic relations by training on labeled text (in another language, e.g. English) and also to improve the balance of the per sense training samples."
      ]
    },
    {
      "heading": "8 Acknowledgements",
      "text": [
        "The authors would like to thank David Yarowsky for his useful comments and support, Gideon Mann for his helpful comments on an early draft of this paper, the Johns Hopkins University NLP lab and JHU SENSEVAL2 team for a creating a stimulating research environment, to Paola Virga for doing that annoying annotation, and to the anonymous reviewers for their helpful comments and suggestions, especially in identifying a mismatch between the English WordNet versions.",
        "This work was partially supported by NSF grant IIS-9985033 and ONR/MURI contract N00014-01-1-0685."
      ]
    }
  ]
}
