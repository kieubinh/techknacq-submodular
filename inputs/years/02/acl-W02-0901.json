{
  "info": {
    "authors": [
      "Youngja Park"
    ],
    "book": "Workshop on Unsupervised Lexical Acquisition",
    "id": "acl-W02-0901",
    "title": "Identification of Probable Real Words: An Entropy-Based Approach",
    "url": "https://aclweb.org/anthology/W02-0901",
    "year": 2002
  },
  "references": [
    "acl-J93-2004",
    "acl-J93-2006",
    "acl-J95-2001",
    "acl-J95-4004",
    "acl-J97-3003"
  ],
  "sections": [
    {
      "text": [
        "Identification of Probable Real Words: An Entropy-based Approach Youngja Park pyoungja@us.ibm.com IBM T.J. Watson Research Center P.O.",
        "Box 704, Yorktown Heights New York 10598, USA Abstract This paper proposes a method for identifying probable real words among out-of-vocabulary (OOV) words in text.",
        "The identification of real words is done based on entropy of probability of character trigrams as well as the morphological rules of English.",
        "It also generates possible parts-of-speech (POS) of the identified real words on the basis of lexical formation rules and word endings.",
        "The method shows high performance both in precision and in recall.",
        "This method is very useful in recognizing domain-specific technical terms, and has successfully been embedded in a glossary extraction system, which identifies single or multi word glossary items and builds a domain-specific dictionary."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "No lexicon could be expected to contain entries for every possible word of a language, given the dynamic nature of language and the creativity of human beings.",
        "Nowadays, this phenomenon has become even more challenging as new technologies develop faster than before.",
        "Thus, inevitably, there always exist out-of-vocabulary words' in documents.",
        "Especially, new derived words, such as new compound words and morphological variations of existing words (mostly swords which are not found in a dictionary.",
        "Also called unknown words.",
        "by means of affixation), and technical words can be missing from a given lexicon.",
        "Words unknown to the lexicon cause a lot of problems to NLP systems which depend heavily on lexical information such as POS taggers and parsers.",
        "There has been a great effort to address this problem, especially in the areas of POS taggers (Brill, 1995; Dermatas and Kokkinakis, 1995; Weischedel et al., 1993) and speech recognition (Callwitz et al., 1996; Hazen and Issam, 2001).",
        "However, previous approaches begin the process based on the assumption that the out-of-vocabulary words are just unknown to the systems' lexicons but they are possible real words of the language.",
        "Then, they guess the most probable POS or the closest substitute of an unknown word.",
        "However, not every out-of-vocabulary word is a possible real word of the language.",
        "We have analyzed the out-of-vocabulary words in document collections from several domains, and found that only a small portion of the words are possible real words.",
        "The goal of this work is recognizing real words among out-of-vocabulary words in text and finding lexical information of the words.",
        "This work has been motivated by our effort to build domain-specific glossaries (Park et al., 2002).",
        "While we were working on automatic glossary extraction, we noticed that technical documents contain a lot of words missing from a general-purpose dictionary, and many of them are actually important domain-specific words.",
        "However, only a few domains (for example, biomedical domain) have domain-specific dictionaries available, and it is very difficult to obtain lexical resources for other domains.",
        "We concluded that the correct recognition of probable real words is very important not only to build a domain-specific dictionary but also to augment an existing general dictionary.",
        "Based on the analyses of large collections of documents, we classify out-of-vocabulary words into the following categories.",
        "• derived words • new words • proper nouns • non-word strings",
        "We address the problems of the recognition of real words and of guessing their POS on the basis of the types of out-of-vocabulary words.",
        "Derived words are morphological variations of words already known to the lexicon, mostly by means of affixation, i.e., adding prefixes to the beginning of words or suffixes to the end, and by means of compounding, i.e., two or more words are written as one word (Pickett et al., 1996).",
        "New words mean the words that can not be produced by the derivation (or word formation) rules from the existing words of the language.",
        "Many domain-specific technical terms belong to this category.",
        "Proper nouns are mostly person names and place nouns.",
        "We also consider upper case and non-initial mixed case words as proper nouns.",
        "Non-word strings mean alphabetic strings together with non-alphabetic characters such as numeric characters and other special characters.",
        "In this work, we don't take into account proper nouns and non-word strings because they are not valuable to be kept in dictionaries.",
        "Thus, in this work, out-of-vocabulary words are classified into two categories - derived words and new words.",
        "The overall process for identifying real words and for producing lexical information is as follows.",
        "First, we remove all proper names and non-word strings from the document collection.",
        "It is easy to recognize non-word strings and upper case and non-initial mixed case words.",
        "In addition to the capitalization feature, we use a precompiled names database (Ravin et al., 1997) for recognizing person names and place names.",
        "If a word exists in the database, we consider it as a proper noun.",
        "Second, we look up all the remaining words in the document collection in a general purpose English dictionary built by IBM (IBM, 2001) and collect all the out-of-vocabulary words, i.e., words unknown to the dictionary, and their frequencies in the collection.",
        "Third, we discard words which appear only once in the collection.",
        "Fourth, we check if an out-of-vocabulary word is comprised of existing words in the dictionary and/or morphological units such as a prefix and a suffix.",
        "If this process succeeds, possible parts-of-speech of the word are generated based on the morphological rules applied to produce the word.",
        "Fifth, if this process fails, we judge if the word may be a new word on the basis of entropy of the probability of its character trigrams and guess its parts-of-speech from its ending characters.",
        "The rest of this paper is structured as follows.",
        "We present morphological rule-based approach in section 2 and entropy-based approach in section 3.",
        "In section 4, we show experimental results and evaluate the performance of the proposed method.",
        "Previous related work is described in Section 5.",
        "Finally, we describe possible future improvements in section 6.",
        "2 Morphological Rule Approach This approach performs the recognition and POS guessing processes for out-of-vocabulary words given that the subcomponents of the words are already known.",
        "There are three types of morphological variations - words with prefixes, words with suffixes, and compound words.",
        "For words with prefixes or suffixes, we use pre-collected sets of prefixes and suffixes for English - currently containing 75 prefixes and 76 suffixes.",
        "For compound words, we try to divide an out-of-vocabulary word into two possible existing words.",
        "The process for prefixed words is as follows.",
        "First, the system checks if any of the prefixes in the prefix list appears at the beginning of the word.",
        "If a word contains a prefix, then the system chops the prefix off the word and looks up the remaining part (the root word) in the dictionary.",
        "We set the minimum length of a root word to two characters.",
        "If the dictionary contains the root word, the out-of-vocabulary word is regard as a real word, and the word inherits the lexical information of the root word.",
        "For example, antiasthmatic, autoinjector, electrocardiography, and hypothyroidism are discovered by the prefix process.",
        "The processing for suffixes is more complicated.",
        "We have a rule set for suffixes, which describes the pre-conditional POS of a root word for having a specific suffix and the resulting POS condition.",
        "The suffix rule structure is as follows.",
        "[suffix, { precondition-POS 4 result-POS }*] For instance, the rule for suffix able is [able, {VB – � JJ}, {NN – � JJ}].",
        "This means, a verb or a noun may have suffix able at the end of the word, and the resulting word's part-of-speech is an adjective.",
        "If a word contains a suffix, the system removes the suffix and recovers the root word.",
        "In English, when a suffix is added to a word, it may change spelling in the root word.",
        "For instance, words ending with a silent e usually drop the e before a suffix beginning with a vowel.",
        "An example of this case is browsable.",
        "The final e of browse was dropped as a result of adding able.",
        "Thus, after separating a suffix from the root word, we recover the original form of the root word by using linguistic information.",
        "If the recovered root word is found in the dictionary and it has one of the preconditioned POS, then the word is regarded as a real word and it has the result POS of the rule.",
        "Some examples of this case are migranious, oxidizability and ventilatory.",
        "Some words, for example, remanufacturability, may have a prefix and a suffix together.",
        "In this case, the word goes through both processes explained above.",
        "If a word fails both the prefix processing and the suffix processing, it is considered for the compound processing.",
        "If a word consists of two content words which are known to the lexicon, and their parts-of-speech are one of the predetermined combinations, it is considered as a real word and has the second component's part-of-speech.",
        "The possible combinations of words are Noun+Noun, Noun+Participle form of verbs.",
        "If a word is composed of two words but does not belong to the possible combinations, the word is discarded.",
        "Some examples of the compound words are eyedrops, photophobia, stereoselectiv-ity, airbreathing, and doubleblinded"
      ]
    },
    {
      "heading": "3 Entropy Approach",
      "text": []
    },
    {
      "heading": "3.1 Identification of new words",
      "text": [
        "Human beings can very successfully guess whether a word never seen before is a possible real word or not, even though the word is not comprised of already known words.",
        "We assume that human beings may conclude that a word is a possible word of the language, if the character sequences in the word look probable, and it is natural to pronounce.",
        "We base the recognition of non-derivational new words on this assumption.",
        "That is, this method is based on the prediction of a language; how well can the next letter of a text be predicted when the preceding n letters are known (see, Shannon, 1951 for more extensive description of estimating the entropy and redundancy of a language) .",
        "A word is a cohesive group of letters with strong internal statistical influences (Shannon, 1951).",
        "We regard a string as a possible real word if every letter in the string is likely to co-occur with its neighbors.",
        "That is, if the letters in a word have high chances to occur in their position given the preceding characters have been seen (i.e., an n-gram model), the word is regarded as a real word.",
        "More formally, we compute entropy of the probabilities of n-gram sequences in an out-of-vocabulary word, and if the entropy value is high, we conclude the word is a real word.",
        "In this work, the neighbors of a character are defined as the two preceding characters, i.e., a trigram model.",
        "The probability of a character, c3, given the two characters preceding it, cl, c2i is estimated as in Equation 1.",
        "In this equation, f (cic2c3) denotes the number of times the sequence of characters {cl, c2i c3} is observed and f (cic2) denotes the number of times the sequence {cl, c2} is observed in a training corpus.",
        "In this work, we produce training data by generating all the possible forms (base forms and inflectional forms) of the words in our dictionary (IBM, 2001).",
        "The training data consist of 81,274 words.",
        "To estimate the trigram probabilities, we add one leading space and one trailing space to every word, making a 28 letter alphabet.",
        "That is, for",
        "• word w with n characters, cl c2 • • • cn, we add • leading space (co), and an trailing space (cn+l)",
        "and generate w �= co cl c2 • • • cn cn+l.",
        "Then, we produce all the two character sequences, Co cl, • • • , cn cn+l, and the three character sequences, c0 cl c2, - - - , Cn-1 Cn Cn+l, and count their frequencies in the training data.",
        "At last, we compute the probabilities of all the possible trigrams by using Equation 1 and store them in a lookup table.",
        "To compute entropy of an out-of-vocabulary word, w = cl c2 • • • cn.",
        "we add a leading space (co) and a trailing space (cn+l) to the word and divide it into trigrams and search each trigram in the lookup table.",
        "The entropy of a word w, H(w), is defined as in Equation 2.",
        "• the number of unknown trigrams2 is less than a threshold value, 01 • the entropy of a word is greater than a threshold value, 02",
        "Currently, 01 is set to 2 if the length of a word is less than or equal to 10 and set to 3 if the length is greater than 10.",
        "02 is set to 2.3, which was determined from the average entropy minus the minimum entropy of the training data."
      ]
    },
    {
      "heading": "3.2 POS guessing for new words",
      "text": [
        "In addition to identifying probable new words, this system produces possible parts-of-speech of the words.",
        "We adopt the ending guessing method described in Mikheev (1997) for this purpose.",
        "We collect the ending guessing rules from the training data described in section 3.1.",
        "For all the words in the training data, we generate all possible endings from length 1 up to length 5, together with the parts-of-speech of the words.",
        "We set the minimum length of the remaining part to 3.",
        "Table 1 shows how ending guessing rules are generated from our training data.",
        "Throughout this paper, POS tags are represented by Penn Treebank Tag code (Marcus and Santorini, 1993).",
        "All the ending rules and their frequencies are collected from the training data, and infrequent rules (frequency = 1) are discarded from the rule set.",
        "The rule set contains 12,387 rules, and the most frequent 50 rules are as shown in Table 2.",
        "The numbers in parentheses denote the frequencies of the rules.",
        "If a given word satisfies the following two conditions, it is regarded as a possible real word.",
        "By using the rule set, the system produces all possible parts-of-speech of a word on the basis of the longest matching pattern.",
        "We look up the ending letters of the word in the rule set from the longest ending (5 letters if the word's length is larger than 7, otherwise the word's length minus 3) to the ending of length 1 (the final letter).",
        "If an ending exists in the rule set, the matching process stops, and the system produces all the parts-of-speech of the ending in the order of the rule frequencies.",
        "For instance, cortical is guessed as an adjective and a noun, but adjective reading is preferred because tical appears 105 times as an adjective and 4 times as a noun in the training data."
      ]
    },
    {
      "heading": "4 Experiments and Results",
      "text": [
        "In order to evaluate the performance of the proposed method, we ran the algorithm on a collection of 4,000 MEDLINE abstracts.",
        "MEDLINE is an on-line computer database of abstracts and references from biomedical journals3.",
        "The collection consists of about 900,000 words, and the file size is about 6.9 megabytes.",
        "The main reason we selected medical documents for the experiment is many of the medical terms are not included in general-purpose language dictionaries, but we can relatively easily verify these words because many medical dictionaries have been built, and some of them are available",
        "The system found a total of 6,997 out-of-vocabulary words from the MEDLINE collection.",
        "Figure 1 shows the number of out-of-vocabulary words, and Figure 2 shows the frequencies of these out-of-vocabulary words in the MEDLINE collection.",
        "As we can see from the pictures, the number of out-of-vocabulary words and their uses monotonically increase in proportion to the increase of the document size.",
        "We excluded words with frequency 1 in the collection from consideration, resulting in a total of 4,187 words.",
        "The detailed types and the frequencies of the extracted out-of-vocabulary words are shown in Table 3.",
        "The system decided 2,815 words among 4,187 out-of-vocabulary words are probably",
        "real words, and 1,372 words are not real words (see, Table 4).",
        "In order to verify the system's judgment, we have conducted two verification processes.",
        "At first, we looked up all out-of-vocabulary words in a medical dictionary.",
        "We used the on-line version of the Merriam-Webster medical dictionary for this purpose.",
        "Then, for words which do not exist in the medical dictionary (we assume they are mostly non-medical words), the human judges (non-domain experts) decided whether they are probable English words or not by referencing an on-line English dictionary 5.",
        "If an out-of-vocabulary word appears in one of the two dictionaries, we regard it as a real word.",
        "We developed a Perl (Wall and Schwartz, 1992) script program to automate the dictionary lookup processes.",
        "This program performs dictionary lookup with a URL and a word without any human intervention.",
        "It accesses a webpage of the given URL, and performs search with the given word, and returns the webpage of the search result.",
        "Then, it parses the returned webpage and decides if a word was found or not.",
        "Table 4 shows the result of this experiment.",
        "The first column (Dictionary Lookup-Yes) denotes the number of the words found in one of the dictionaries, and the second column (Dictionary Lookup-No) denotes the number of the words which don't exist in any of the dictionaries.",
        "The first row (System Guess-Yes) denotes the number of the words which the system considered as real words, and the second row (System Guess-No) is the number of the words which the system regarded as invalid words.",
        "The performance of this system on themed-line collection is as follows.",
        "However, many of the samples that the system decided real words but were not found in the dictionary (System Guess-Yes and Dictionary Lookup-No) are actually real words.",
        "This is because the dictionary used for this experiment is also limited.",
        "Some examples of the words – mostly biology terminology and drug/treatment names – are aggregometry, cardiomyocyte, col-forsin, nondihydropyridine, nylestriol."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "There has been a great effort to address this problem, especially in the areas of POS taggers and speech recognition.",
        "However, different applications recognize the problem of out-of-vocabulary words in different perspectives and have different goals.",
        "For POS taggers and parsers, which rely on lexical (syntactic) information about words, the goal is to guess the most plausible part-of-speech and other lexical information of an out-of-vocabulary word in a context.",
        "Dermatas and Kokkinakis (1995) estimated the probability that an unknown word has a particular POS tag from the probability distribution of words which occur only once in the previously seen texts.",
        "More advanced POS guessing methods use leading and trailing word segments to determine possible tags for unknown words.",
        "Weischedel et al.",
        "(1993) proposed a POS guessing method for unknown words by using the probability for an",
        "Brill (1995) describes a system of rules which uses both end-guessing and more morphologically motivated rules.",
        "Mikheev (1997) presents a technique for fully automatic acquisition of rules which guesses possible POS tags for unknown words using their starting and ending segments.",
        "For speech recognition systems, an out-of-vocabulary(OOV) word is either a word unknown to the system vocabulary or a word that the recognizer fails to recognize.",
        "The goal is to find the closest word (in terms of sound and meaning) to the OOV word from the system's vocabulary.",
        "Character ngram-based statistical approaches have been used in word-level language processing such as spelling correction (Angell et al., 1983), word segmentation (Juola et al., 1994), and language identification (Dunning, 1994).",
        "Angell, Freund and Willett (1983) describe a method of comparing misspellings with dictionary terms based on the number of trigrams that the two strings have in common, using Dice's similarity coefficient as the measure of similarity.",
        "The misspelled word is replaced by the word in the dictionary which best matches the misspelling.",
        "Juola, Hall and Boggs (1994) describes a system which segments full words into their constituent morphemes based on entropy of the probabilities of trigram sequences.",
        "Dunning (1994) implements a high accuracy language identification using character n-gram models and a Bayesian classifier.",
        "The performance of the n-gram language classifier is evaluated using different size of n-grams."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "We have developed an approach to augmenting a morphological lexicon with new words such as newly derived words and domain-specific technical words through text analysis of document collections.",
        "For morphologically derived words, we have employed morphological rule-base methods such as affixations and compound words.",
        "We have also proposed a new technique to identify non-derivational new words based on entropy of the probabilities of trigram sequences.",
        "The probabilities of trigram sequences are trained on an existing English dictionary.",
        "Some possible improvements and future plans are as follows.",
        "1.",
        "The rules used for compound word process is over-generalized.",
        "For example, all the combinations of two nouns may not be compound nouns.",
        "We anticipate the performance will be improved if we incorporate a corpus statistics-based compound word processing scheme into the existing method.",
        "2.",
        "The performance will be improved if we include domain-specific prefixes and suffixes such as amino, brancho, and cardio.",
        "3.",
        "We trained our entropy model on a general-purpose dictionary.",
        "However, many medical and biotechnological terms have their origin in Latin.",
        "We expect the performance would be better if we train our system by using a domain-specific lexicon or a tagged (specified if a word is correct or not) domain-specific corpus.",
        "4.",
        "We expect it is not difficult to apply this approach to other languages because this system only uses basic morphological rules of a language and language-independent statistical information.",
        "In addition, it does not require a large amount of annotated training data.",
        "Acknowledgement I am grateful to Marco Gruteser for the development of the Perl script for the evaluation of this work.",
        "I also thank anonymous reviewers for their helpful comments."
      ]
    }
  ]
}
