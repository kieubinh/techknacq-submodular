{
  "info": {
    "authors": [
      "Katja Markert",
      "Malvina Nissim"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W02-1027",
    "title": "Metonymy Resolution As a Classification Task",
    "url": "https://aclweb.org/anthology/W02-1027",
    "year": 2002
  },
  "references": [
    "acl-A00-2009",
    "acl-J93-1003",
    "acl-J96-2004",
    "acl-J99-4002",
    "acl-N01-1009",
    "acl-N01-1011",
    "acl-P92-1047",
    "acl-P93-1012",
    "acl-P95-1026",
    "acl-P96-1006",
    "acl-W00-1326",
    "acl-W98-0720",
    "acl-W99-0613"
  ],
  "sections": [
    {
      "text": [
        "of one particular word as input and assigns word senses to new test instances of the same word as output, (supervised) metonymy recognition can take a set of labelled training instances of different words belonging to one semantic class as input and assign literal readings and possible metonymic patterns to new test instances of possibly different words of the same semantic class.",
        "Thus, it needs to infer from training instances like Example (3) (when labelled as a place-for-people metonymy) that Examples (4) and (5) are also metonymic, a task which poses no problems for most humans.",
        "(3) \"Bosnia's view of \" (4) \"Hungary's view of \" (5) \"Hungary's position on\"",
        "In this paper, we explore this view of metonymy recognition as a class-based WSD task for the semantic class of locations.)",
        "The corpus data we use is described in Section 2.",
        "As resources reliably annotated for metonymy do not exist (see also Section 6), we constructed a corpus of location names annotated for literal and metonymic readings.",
        "The supervised classification algorithm we use are decision lists as they have been successfully used in several classification tasks (Yarowsky, 1995; Collins and Singer, 1999) (see Section 3).",
        "In Section 4, we explore whether features traditionally used in WSD carry over to metonymy resolution, concentrating on (i) cooccurrences; (ii) collocations; and (iii) grammatical features.",
        "Results are discussed in Section 5.",
        "We show that cooccurrences are in general not appropriate for metonymy resolution; collocations are useful but suffer from data sparseness when used as simple word forms; the grammatical relations subject and object perform well but are only applicable to a small part of the data.",
        "We then compare our algorithm to metonymy recognition approaches based on selectional restriction violations in Section 6."
      ]
    },
    {
      "heading": "2 Experimental Data",
      "text": [
        "We present a short overview of the collection of a corpus of location names and its annotation for literal and metonymic readings.",
        "A more detailed description can be found in (Markert and Nissim, 2002) and the annotation scheme is downloadable from http://www.ltg.ed.ac.",
        "uk/\"malvi/mascara/publications.html."
      ]
    },
    {
      "heading": "2.1 Corpus Collection",
      "text": [
        "We extracted all country names from Word-Net (Fellbaum, 1998) and the CIA factbook (http://www.cia.gov/cia/publications/ factbook/).",
        "This collection of names forms our sampling frame CountryList.",
        "We built a corpus of text samples that contains 1000 occurrences of country names, randomly extracted from the British National Corpus (http://info.ox.ac.uk/bnc), henceforth abbreviated as BNC.",
        "Any country name in CountryList was a Possibly Metonymic Word (PMW, henceforth) and was allowed to occur in the samples extracted.",
        "We searched the BNC using Gsearch (Corley et al., 2001).",
        "All samples include a PMW surrounded by three sentences of context.",
        "All examples introduced from now on are from the BNC.2"
      ]
    },
    {
      "heading": "2.2 Annotation Scheme for Location Names",
      "text": [
        "After excluding some undesired examples (i.e., noise) which our extraction method collected (e.g. homonyms, such as \"Greenland' in \"Õro� ÿíùùor Greenland\"), the annotation can proceed to identify literal, metonymic, and mixed readings.",
        "Our annotation scheme built on lists of metonymic patterns in the literature (Lakoff and Johnson, 1980; Fass, 1997; Stern, 1931), but diverted from these patterns when they did not provide full coverage or could not be distinguished reliably (Markert and Nissim, 2002).",
        "The literal reading for location names comprises a locative (see Example (6)) and a political entity interpretation (see Example (7)).",
        "(6) \"coral coast of Papua New Guinea\" 'At the moment we restrict ourselves to location names.",
        "2 A exception is Example (12).",
        "(7) \"Britain's current account deficit\"",
        "For metonymic readings, we distinguish between general patterns (valid for all physical objects) and location-specific ones.",
        "As general patterns were never encountered in our corpus, we describe here only the latter.",
        "• place-for-people: a place stands for any persons/organisations associated with it.",
        "In Example (8), \"San Marino\" stands for one of its sports teams.",
        "(8) \"a 29th-minute own goal from San"
      ]
    },
    {
      "heading": "Marino defender\"",
      "text": [
        "Often, the explicit referent is underspecified, as in Example (9), where the reference could be to the government, an organisation or the whole population.",
        "(9) \"The ... group expressed readiness",
        "to provide Albania with food aid\" We therefore adopt a hierarchical approach, and assign a pattern (place-for-people) at a higher level (supertype), as well as a more specific pattern (subtype), if identifiable, at a lower level.",
        "This deviates from common practice in the linguistic literature, but has the great advantage of `punishing' disagreement only at a later stage and allowing fall-back options for automatic systems.",
        "We also experienced a drop in human annotation agreement from supertype to subtype classifications (see (Markert and Nissim, 2002)).",
        "In this paper, we evaluate our system on supertype classification.",
        "• place-for-event: a location name stands for something that happened there (see Example (2)).",
        "• place-for-product: a place stands for a product manufactured there (e.g., \"Bordeaux\" can refer to the wine produced there) .",
        "The category othermet covers unconventional metonymies.",
        "Since they are open-ended and context-dependent, no specific category indicating the intended semantic class can be introduced.",
        "In Example (10), \"New Jersey\" metonymically refers to the local typical tunes.",
        "(10) \"The thing about the record is the influences of the music.",
        "The bottom end is very New York/New Jersey and the top is very melodic\"",
        "The category othermet is only used if none of the other categories fits.",
        "In addition to literal and metonymic readings, we found examples where two predicates are involved, triggering a different reading each, thus",
        "yielding a mixed reading.",
        "This often occurs with coordinations and appositions.",
        "(11) \"they arrived in Nigeria, hitherto a leading critic of... \"",
        "In Example (11), both a literal (triggered by \"arriving in\") and a place-for-people reading (triggered by \"leading critic\") are invoked.",
        "We therefore introduced the category mixed to deal with these cases (not treated as a category in the literature)."
      ]
    },
    {
      "heading": "2.3 Annotation Reliability, Distribution and Data Preparation",
      "text": [
        "The 1000 examples of our corpus have been independently annotated by two computational linguists, who are the authors of this paper.",
        "Reproducibility of results (Krippendorff, 1980) yielded a percentage agreement of .95 and a kappa (Car-letta, 1996) of .88.",
        "The annotation can therefore be considered reliable.",
        "In the corpus data used for our classification experiments, we only included the samples which both annotators could agree on and which were not marked as noise.",
        "Therefore our corpus for testing and training the algorithm includes 925 samples.",
        "The resulting distribution of readings is described in Table 1.",
        "The data was further stripped of all punctuation and capitalisation was removed.",
        "No stemming or lemmatisation was performed."
      ]
    },
    {
      "heading": "3 Decision lists for metonymy resolution",
      "text": [
        "The distribution in the corpus shows that metonymic readings that do not follow established metonymic patterns (othermet) are very rare.",
        "This seems to be the case for other kinds of metonymies, too (Verspoor, 1997).",
        "This strengthens our case for viewing metonymy recognition as a classification task between literal readings and metonymic patterns that can be identified in advance for particular semantic classes.",
        "We therefore explore the usage of a classification algorithm and features used in WSD for metonymy recognition.",
        "The target readings for the algorithm to distinguish are literal, place-for-people, place-for-event, place-for-product, othermet and mixed.",
        "As an algorithm we use decision lists.",
        "The advantage of decision lists for a first exploration of a feature space is that their choices are easy to follow as they make use of the most informative feature only instead of a combination of features.",
        "All features encountered in the training data are ranked in the decision list (best evidence first) according to a log-likelihood ratio calculated as follows (Yarowsky, 1995; Martinez and Agirre, 2000) : Log �Pr(readingZ If eaturek) Ej#Z Pr(readingj If eaturek) When applying the decision list to a test ex-3All experiments reported here have also been repeated using a Naive Bayes classifier.",
        "The results have not improved on decision lists.",
        "ample, the winning reading is selected by the feature in the test example with the highest rank in the decision list.",
        "We estimated probabilities via maximum likelihood, adopting a simple smoothing method: 0.1 is added to both the denominator and numerator.",
        "4 Exploration of feature space We investigated the following feature types.",
        "Examples are given in Table 2, together with examples of their distribution and the reading they trigger.",
        "Cooccurrences.",
        "They have proved useful for WSD (Gale et al., 1993; Pedersen, 2000).",
        "We used left and right windows of context of 8 different sizes: 0, 1, 2, 3, 4, 5, 10, and 25 words, thus yielding 64 possible combinations of left and right sizes (e.g., 13.r1 for 3 words to the left and 1 to the right).",
        "Any content word in the window considered was included as a feature.",
        "Collocations.",
        "We selected 4 different collocations frequently used in WSD (Ng and Lee, 1996; Martinez and Agirre, 2000).",
        "The word to the right of the PMW, the word to the left, two words to the left and the word to the right and the left.",
        "The first two features consist of a single word form, the latter two of a sequence of two word forms.",
        "Function words were allowed as collocations, as e.g., the presence of a preposition directly to the left of the PMW can be indicative (see also Table 2).",
        "Grammatical features.",
        "Following some WSD approaches (Ng and Lee, 1996; Yarowsky, 1995) we use grammatical features, namely,",
        "(i) the grammatical role (role) of the PMW, distinguishing between subjects, direct objects and any other grammatical role (including e.g. prepositional phrases, NP modifiers); (ii) both the grammatical role and the stemmed form of the corresponding verb for subjects and direct objects (role-of-verb)."
      ]
    },
    {
      "heading": "5 Results and Discussion",
      "text": [
        "We have tested the decision list algorithm on our annotated corpus, employing 10-fold cross-validation.",
        "Results as reported in Table 3 are averaged over all 10 folds.",
        "The first column describes the feature used in the experiment.",
        "Then we report accuracy and coverage.4 coverage – number of decisions made number of test data accuracy = number of correct decisions made number of decisions made We also used a backing-off strategy to the most frequent sense literal for the cases where no decision could be made (increasing coverage to 1) and report these results as accuracy/coverage-backoff.",
        "As it is of particular interest to us to see how many non-literal readings (metonymies and mixed readings) can be correctly identified we compute precision and recall (based on the algorithm including backing 4Please note that a test example might not be covered because of either the absence of a feature value in the training set or because the highest ranked feature gives equal evidence for two different readings.",
        "off strategy).",
        "Let A be the number of correctly identified non-literal readings and B the number of incorrectly identified non-literal readings.",
        "#non-literal examples in the test data When significance claims are made they are based on a 10-fold cross-validated t-test, using significance level 0.05.",
        "The baseline used for comparison is the assignment of the most frequent reading literal (see Table 1).",
        "It has a coverage of 1 as it is applicable to all examples.",
        "Recall is 0 as no metonymies can be recognised.",
        "5.1 Cooccurrences For all 64 window size combinations (for example results see Table 3), the accuracy never significantly beats the baseline.",
        "Both precision and recall are unsatisfactory and get steadily worse with increasing window sizes.",
        "We identified the following reasons for such a behaviour.",
        "Topical v. fine-grained sense distinctions.",
        "Cooccurrences and large window sizes traditionally work well for topical distinctions (Gale et al., 1993).",
        "Metonymy, though, does often not cross topical boundaries thus, whether a location name is used as a literal (political) reading or as a reading for the government often does not change coocurrence features.",
        "This is especially true for large window sizes.",
        "infrequently.",
        "The simple smoothing method we used did not fully take this problem into account.",
        "Therefore, for example, a content word wl occurring only once and with a metonymic reading can be ranked higher than a content word w2 occurring 10 times, 8 times with a literal reading and twice with a metonymic reading.",
        "A test example containing both content words will therefore use wl to decide in favour of a metonymic reading, despite the weak evidence.",
        "This might explain the low precision.",
        "We therefore tested the effect of deleting all non-informative features from the decision list, using the G2 test (Dunning, 1993) to measure independence between cooccurrence features and readings.",
        "Using pruned decision lists yielded some improvement in precision, but a significant drop in coverage, given the lower number of features used (for window l4.r1: precision=.609; recall=.210; coverage=.098).",
        "The general tendency to prefer smaller windows over larger ones still holds."
      ]
    },
    {
      "heading": "5.2 Collocations",
      "text": [
        "The one-word collocations had in general a high coverage as function words were included.",
        "Accuracy for collocations is quite good (ranking from 81.9% to 87.0%).",
        "But increasing coverage to 1.00 (��ví�ûþí-bû�k�ff) causes accuracy backoff to drop.",
        "Recall is very low.",
        "We discuss here two reasons why collocations do worse in metonymy recognition than in WSD (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001).",
        "Target readings.",
        "Readings like othermet and mixed are unsuited for a collocation-based approach.",
        "Sparse data.",
        "When we inspected the decision lists, we found that strong collocations are mostly found for literal readings (e.g. spatial prepositions to the left of the PMW), so that a high percentage of literal examples can be identified correctly.",
        "Some good collocations for metonymic readings were found only once or twice in the training data and then not again in the test data, thus causing low recall and accuracy-backoff.",
        "One reason for this is that the training data for literal readings is about 5 times as big as for metonymic readings.",
        "This is aggravated by the use of the BNC that includes a wide variety of genres using different style, register and vocabulary.'",
        "Often, though, a \"similar\" collocation was seen (compare e.g., \"view\" and \"position\" in Example (4) and (5)).",
        "Using word forms as collocations can only make the generalisation from Example (3) to Example (4), not the one to Example (5).",
        "Thus, we will in the future explore semantic generalisation of collocations by e.g., using synonym information from Wordnet."
      ]
    },
    {
      "heading": "5.3 Grammatical roles",
      "text": [
        "Grammatical roles yield significant improvements in accuracy backoff over the baseline and good precision.",
        "One reason is that they do not suffer as much from sparse data and generalise well over the whole semantic class.",
        "Regarding the classifier based on the feature role only, e.g., being a subject can be learned as a good indicator for place-for-people metonymies regardless of country name or verb.6 Recall (.344) is also promising considering that the roles of subject and object, which give good hints for metonymic readings, are relatively rare (only 120 of 925 examples in our corpus were subjects or direct objects).",
        "The classifier learns to assign literal readings to all other instances, whose grammatical roles are not further distinguished as feature values.",
        "Inclusion of more grammatical roles might further improve recall."
      ]
    },
    {
      "heading": "Precision can be improved without sacrificing",
      "text": [
        "recall by also considering the verb, if present in the training data (classifier role-of-verb+role).",
        "So, whereas considering the role only will lead to assigning a place-for-people metonymy to all subjects, this is avoided in some cases when considering the verb in addition (e.g., for being the subject of the full verb \"have\"; see also Table 2).",
        "If the grammatical role with this particular verb has not been seen in the training data, the classifier will default to role, thus keeping coverage",
        "high.",
        "Please note that the grammatical roles have been annotated by hand as we wanted to measure the contribution of different features to metonymy classification without encountering error chains from e.g., parsing or tagging processes.",
        "Therefore the results we present are an upper bound to what can be achieved with subject/object roles."
      ]
    },
    {
      "heading": "6 Related Work",
      "text": [
        "We compared our approach and results to WSD in Section 1 and 5, stressing word-to-word vs. class-to-class inference.",
        "Most traditional approaches to metonymy recognition use violations of selectional restrictions (plus sometimes syntactic violations) for recognition (Pustejovsky, 1995; Hobbs et al., 1993; Fass, 1997; Copestake and Briscoe, 1995; Stallard, 1993).7 Thus they furnish their algorithms with (mostly hand-modelled) selectional or grammatical restrictions.",
        "Note that selectional restrictions in these approaches are normally not seen as preferences but as absolute constraints.",
        "If and only if such an absolute constraint is violated a non-literal reading is proposed.",
        "In those experiments in which we also use grammatical knowledge, our system does not have any a priori knowledge of semantic argument-verb restrictions.",
        "Rather it refers to previously seen training data of country names as verb arguments and their labelled senses and computes the likelihood of each sense using this distribution.",
        "This is advantageous for the following reasons: • There are many verbs with weak selectional restrictions (e.g., the verb \"seem\").",
        "Both literal (see Example (12)) and metonymic (see Example (13)) readings of a location ocurring as subject of \"seem\" are therefore possible, although one of the readings might be more frequent given these features.",
        "(12) \"Hungary seemed far away.\" 7(Markert and Hahn, 2002) enhance this with anaphoric information.",
        "(13) \"Britain seemed close to interven",
        "tion.",
        "\" Selectional restrictions as used in most metonymy recognition approaches therefore do not detect any violation.",
        "In contrast, the training data we use supplies the information that the metonymic place-for-people reading is more frequent given these grammatical features, leading the classifier to assign the correct reading in the majority of cases.$",
        "• Our algorithm does not need to make any",
        "assumptions about the sense of the verb.",
        "Selectional restrictions, instead, must assume that the verb is disambiguated beforehand as they can vary between different verb senses (compare, e.g., the \"confront\" reading and the \"to be opposite\" reading of the verb \"face\") .",
        "To compare our decision list algorithm role-for-verb+role to a selectional restriction violations approach we limited our next empirical study to the 120 examples in our data that had the grammatical role of subjects or direct objects (SETGRAMM).",
        "Two native speakers of English (both linguists) were asked to annotate the 120 subj-verb/obj-verb tuples in SETGRAMM for selectional restriction violations.",
        "Agreement between the two subjects was satisfactory (kappa=.70).",
        "We then simulated two metonymy recognition algorithms based on the annotations of subjectl and subject2, postulating a non-literal reading when a selectional restriction violation was annotated and literal otherwise and computed corresponding evaluation measures.",
        "We also computed the evaluation measures for our role-of-verb+role classifier, limited to SET-GRAMM.",
        "Results are summarised in Table 4.",
        "Our classifier has higher recall, but lower precision than subject2 and subjectl .",
        "To compare the trade-off $(Briscoe and Copestake, 1999) propose using frequency information in addition to syntactic and semantic restrictions, but use only a priori sense frequencies without feature integration.",
        "between precision and recall we computed the F-measure for all algorithms, where our algorithm performed best.",
        "We also evaluate our approach more rigorously than other metonymy resolution algorithms .",
        "Some researchers use constructed examples only (Fass, 1997; Hobbs et al., 1993; Copestake and Briscoe, 1995; Pustejovsky, 1995; Verspoor, 1996), and do not report any numerical results.",
        "Others (Markert and Hahn, 2002; Harabagiu, 1998; Stallard, 1993) use naturally-occurring data that, however, seem to be analysed according to subjective intuitions of one individual only, not assessing the reliability of their annotation.",
        "We, instead, use a reliably annotated corpus that we can make available to other researchers.",
        "In addition, most previous evaluations report only recall figures for metonymy recognition, neglecting the question of precision and false positives as well as baseline comparisons and accuracy.",
        "Evaluations of metonymy interpretation (Zapata, 2001) include more disciplined evaluations, but do not handle metonymy recognition yet."
      ]
    },
    {
      "heading": "7 Conclusions",
      "text": [
        "We argued for viewing metonymy recognition as a WSD task based on semantic classes instead of individual words.",
        "This is motivated by the regularity of most metonymic readings.",
        "We presented a corpus reliably annotated for metonymic and literal usage which supports this claim.",
        "We also conducted several experiments with a decision list algorithm to explore the usefulness of common WSD features for metonymy recognition.",
        "We showed that coocurrence features are not useful for metonymy resolution, whereas collocation features need to be generalised from word forms to semantic classes to have wide application.",
        "Grammatical features perform well.",
        "We also compared our grammatical features to a selectional restriction based approach to recognition with promising results.",
        "In the future, we will explore two avenues for improvement: Firstly, we will experiment with more sophisticated machine learning algorithms, starting with improving on our smoothing pro",
        "cedure, which we experienced as too simplistic (see also (Yarowsky, 1997)).",
        "Secondly, we will generalise the collocation features we use, incorporate more grammatical relations and explore other feature types and feature combination."
      ]
    },
    {
      "heading": "Acknowledgements. Katja Markert is",
      "text": [
        "funded by an Emmy Noether Fellowship of the Deutsche Forschungsgemeinschaft (DFG) and Malvina Nissim by ESRC Project R000239444.",
        "We thank our colleagues Stephen Clark and Tim O'Donnell for their help with annotation as well as two anonymous reviewers for their comments and suggestions."
      ]
    }
  ]
}
