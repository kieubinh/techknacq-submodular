{
  "info": {
    "authors": [
      "Narendra Gupta",
      "Srinivas Bangalore"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W02-1035",
    "title": "Extracting Clauses for Spoken Language Understanding in Conversational Systems",
    "url": "https://aclweb.org/anthology/W02-1035",
    "year": 2002
  },
  "references": [
    "acl-A88-1019",
    "acl-H92-1060",
    "acl-J99-2004",
    "acl-N01-1016",
    "acl-P92-1008",
    "acl-P99-1053"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp.",
        "273-280.",
        "Association for Computational Linguistics.",
        "cus of spoken language parsing research for several years (Bear et al., 1992; Seneff, 1992; Heeman, 1997; Ruland et al., 1998; Core and Schubert, 1999).",
        "Most of the previous approaches cope with dysfluencies and speech repairs in the parser by providing ways for the parser to skip over syntactically ill-formed parts of an utterance.",
        "In more recent work (Stolcke and Shriberg, 1996; Charniak and Johnson, 2001), the problem of speech parsing is viewed as a two step process.",
        "A preprocessing step is used to identify speech repairs before parsing begins.",
        "The approach we present in this paper, is similar to (Charniak and Johnson, 2001) with a few differences.",
        "We do not constrain speech edits and restarts to conform to a particular structure.",
        "Further, we also segment the utterance into clauses which we believe would be easier to interpret.",
        "Finally, we use plain word strings with no punctuation marks since this is typically the output of a speech recognizer.",
        "The layout of the paper is as follows.",
        "In Section 2, we will define the task and illustrate with an example the encoding of the task that makes it suitable for training models for annotation.",
        "In Section 3, we discuss the two approaches for clausification - n-gram approach and discriminative approach.",
        "The experiments and evaluation results are presented in Section 4."
      ]
    },
    {
      "heading": "2 Task Definition",
      "text": [
        "Clausifier takes as input the result of recognition of a user's utterance and generates clauses as its output.",
        "The clausifier annotates its input with tags that help in segmenting it into clauses.",
        "The <s> tag is used to indicate sentence boundaries, strings within [ and ] are to be edited out and strings between {c and } indicate coordinating conjunctions.",
        "These tags are then interpreted to retrieve the set of clauses.",
        "This interpretation involves deleting the words within [ and ] and replacing <s> and the words enclosed within {c and } with a line feed.",
        "An example' illustrating the input, the annotated output and the set of clauses resulting from interpreting the output is '����������� in this example is a named entity.",
        "shown below.",
        "2"
      ]
    },
    {
      "heading": "Clausifier Input:",
      "text": [
        "yes I got the bill and and eh I have a question about I was surprised I got a phone call with in I mean for er $time amount is what the the bill said and you know you charged me eh $time amount plus tax so eh ...",
        "• yes • I got the bill • I have a question • I was surprised • I got a phone call for $time amount is what the bill said • you charged me $time amount plus tax",
        "In order to train models for the clausifier, we have encoded the sentence boundary, edit and conjunction information as tags following a word.",
        "If there is sentence boundary before a word, it is tagged as \"Segment\".",
        "Edit and conjunction tags also contain span information; <Edit1> is for edit of one word to the left,",
        "] <Edit2> is for edit of two words to the left and so on.",
        "A similar encoding is used for coordinating conjunctions.",
        "A word boundary that has neither of these tags is tagged as \"No Action\".3 Encoding the problem: yes <s> I got the bill and <Editi> and <Conj 1> eh <Edit 1> I have a question <s> about <Editi> I was surprised <s> I got a phone call with in <Edit2> I mean <Edit2> for er <Editi> $time amount is what the <Editi> the bill said and <Conji> you know <Edit2> you charged me eh <Editi> $time amount plus tax so eh <Edit2> ..."
      ]
    },
    {
      "heading": "3 Clausifier Method",
      "text": [
        "The task of annotating the input can be viewed as a tagging problem.",
        "Each word of the input is tagged with one of a few tags that indicate the type of annotation following the word.",
        "In particular, we consider the presence of sentence boundary tag <s> and its absence <nos> as two possible tags to associate with each word.",
        "We can then use an n-gram tagging model (similar to (Church, 1988)) as shown in equation 1 to retrieve the best tag sequence for a given input sentence.",
        "We follow the same notation as in (Church, 1988).",
        "Such an n-gram based sentence boundary detection method was presented in (Stolcke and Shriberg, 1996) who also point out that the advantage of n-gram based method is its natural integration within the language model of the speech recognizer.",
        "However, increasing the conditioning context in an n-gram increases the number of parameters combinatorially and estimating these parameters reliably becomes an issue.",
        "We present a discriminative classification approach to clausifier which allows us to add larger number of features, in contrast to the generative n-gram model.",
        "3 An alternate way of encoding is to use the \"InsideOutside-Boundary\" tags for each word as is typically done for chunking of noun groups."
      ]
    },
    {
      "heading": "3.1 Classifier",
      "text": [
        "We used a machine-learning tool called Boos-texter, which is based on the boosting family of algorithms first proposed in (Schapire, 1999).",
        "The basic idea of boosting is to build a highly accurate classifier by combining many \"weak\" or \"simple\" base classifiers, each one of which may only be moderately accurate.",
        "To obtain these base classifiers, it is assumed that a base learning algorithm is available that can be used as a black-box subroutine.",
        "The collection of base classifiers is iteratively constructed.",
        "On each iteration t, the base learner is used to generate a base classifier ht.",
        "Besides supplying the base learner with training data, the boosting algorithm also provides a set of nonnegative weights wt over the training examples.",
        "Intuitively, the weights encode how important it is that ht correctly classifies each training example.",
        "Generally, the examples that were most often misclassified by the preceding base classifiers will be given the most weight so as to force the base learner to focus on the \"hardest\" examples.",
        "As described in (Schapire and Singer, 1999), Boos-texter uses confidence rated classifiers h that, rather than providing a binary decision of 1 or +1, output a real number h(x) whose sign (-1 or +1) is interpreted as a prediction, and whose magnitude lh(x)l is a measure of \"confidence.\"",
        "The output of the final classifier f is f (x) _ Et_1 ht(x), i.e. the sum of confidence of all classifiers ht.",
        "The real-valued predictions of the final classifier f can be converted into probabilities by passing them through a logistic function; that is, we can regard the quantity",
        "as an estimate of the probability that x belongs to class +1.",
        "In fact, the boosting procedure is designed to minimize the negative conditional log likelihood of the data under this model,",
        "The extension of Boostexter to the multiclass problem is described in (Schapire and Singer,",
        "individual tags as well as the total error rate and the baseline error rate for each tagging task.",
        "This baseline error rate is calculated by using a classifier that assigns each example the tag that occurs most frequently in the data.",
        "Since we are eventually interested in parsing and understanding the resulting clauses, we also report recall and precision after each of the annotations are interpreted (i.e. after utterances are split at sentence boundaries, after edits are deleted and after utterances are split at conjunctions.).",
        "These scores are reported under the \"Sentence\" column of each model's performance table.",
        "Like other recall and precision numbers sentence level recall indicates the proportion of clauses in the input that are correctly identified in the clausifier output, and precision indicates the proportion of the output clauses that are in the input."
      ]
    },
    {
      "heading": "4.3 N-gram model: Baseline Model",
      "text": [
        "Table 2 shows the results of using a trigram model, similar to (Stolcke and Shriberg, 1996) for sentence boundary detection on the data described above.",
        "In our experiments, instead of using the true part-of-speech tags as was done in (Stolcke and Shriberg, 1996), we used the result of tagging from an n-gram part-of-speech tagger (similar to (Church, 1988)).",
        "In addition to providing recall and precision scores on the individual segmentation decision, we also provide sentence level performance.",
        "Notice that segmentation precision and recall of approximately 80% and 52% turn into sentence level precision and recall of 50% and 32% respectively.",
        "We also noticed that including POS improves the performance by approximately 1%."
      ]
    },
    {
      "heading": "4.4 Classifier Models",
      "text": [
        "Training data for the classifiers was prepared by labeling each word boundary with one of the tags described in Section 2 and features shown in Table 1.",
        "Apart from training individual classifiers for sentence boundary, edit and conjunction classification, we also trained a combined classifier which performs all the three tasks in one step and does not make any independence assumptions as shown in Figure 1."
      ]
    },
    {
      "heading": "4.5 Combined Classifier",
      "text": [
        "Table 3 shows the performance of a combined classifier that predicts a combined tag for each of the components.",
        "The tagset for the combined classifier is the crossproduct of the tagsets for segmentation,edits and conjunctions.",
        "Since this classifier makes all the decisions, the output of this classifier can be directly used to generate clauses from the input strings of words.",
        "As expected this classifier outperforms the N-gram based classifier both at segmentation decision level and at sentence level (compare column 8 and 9 of table 3 with column 3 and 4 of table 2 respectively)."
      ]
    },
    {
      "heading": "4.6 Individual Classifiers",
      "text": [
        "Tables 4,5,6 show the performance of the three classifiers used in the cascade shown in Figure 1.",
        "In these tables sentence level performances are with respect to their own inputs and outputs.",
        "Over all sentence level performance is shown in",
        "sifiers are significantly more accurate at making individual decisions which results in higher recall and precision at sentence level (compare column of table 7 with column 9 of table 3)."
      ]
    },
    {
      "heading": "5 Sensitivity Analysis",
      "text": [
        "In this section, we investigate the effect of various features on the performance of the Segmentation, Edit and Conjunction classifiers.",
        "Table 8 shows the results for each classifier using only words, only parts-of-speech (POS) tags, words and POS tags and words, POS tags combined with the similarity measure.",
        "It is not surprising to note that using only POS tags to predict sentence boundaries, edits and conjunctions results in a higher error rate compared to using only words.",
        "Adding POS features to the words-only model does not improve the performance of these classifiers.",
        "This is to be expected since the generalization provided by the POS tags is not really needed for these tasks, as there are not many unseen contexts even when using words contextual features.",
        "However, we suspect su-pertags (Bangalore and Joshi, 1999) can capture long-distance effects (eg.",
        "subcategorization frame of preceding verb) which could improve the segmentation performance.",
        "It is however surprising to note that the similarity features, which had been designed to specifically capture patterns in speech repairs does not contribute as much to the performance (Words+POS+Similarity=3.5% as compared to Words Only=3.9%).",
        "By separating discourse markers (eg.",
        "you know, well, so), explicit edit terms (eg.",
        "I mean, sorry, excuse me), and fillers (eg.",
        "um, uh) from the set of Edit tags, we are left only with restarts and repairs.",
        "We trained a classifier for identifying only these tags and the sensitivity results are shown in the fourth column of Table 8.",
        "Note that the baseline performance of Restarts and Repairs is much lower than that of Edits indicating that it is an easier task than identifying Edits.",
        "Incorporating the similarity feature reduces the error rate for identifying restarts by 37% over a model which uses only words.",
        "(Words+POS+Similarity=1.7% as compared to Words Only=2.7%).",
        "This suggests that an additional classifier to identify discourse markers and explicit edit terms would be beneficial."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "In this paper, we have presented a clausifier which would be used as a preprocessor in the context of speech parsing and understanding system.",
        "The clausifier contains three classifiers that are trained to detect sentence boundaries, speech repairs and coordinating conjunctions.",
        "These models have been trained and tested on Switchboard corpus and provide an end-to-end recall and precision of 54% and 56% respectively for the task of clause identification.",
        "We have shown that classifier models clearly outperform the n-gram models, and that a combined model does not perform as well as a model that makes individual predictions.",
        "We believe that the sentence level performance can be improved further by improving the training data quantity and quality.",
        "In the Switchboard corpus we found that average turn length is 6, and that the turn boundaries are very strong indicator of the sentence boundaries.",
        "This makes it hard for the classifier to learn other discriminating features.",
        "We plan to use this system to iteratively annotate additional data with longer turn lengths (customer-operator telephone conversations), manually correct it and retrain the models described in this paper."
      ]
    }
  ]
}
