{
  "info": {
    "authors": [
      "Kyoji Umemura"
    ],
    "book": "Workshop on SEMANET: Building and Using Semantic Networks",
    "id": "acl-W02-1115",
    "title": "Selecting the Most Highly Correlated Pairs Within a Large Vocabulary",
    "url": "https://aclweb.org/anthology/W02-1115",
    "year": 2002
  },
  "references": [
    "acl-J01-1001",
    "acl-J90-1003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Occurence patterns of words in documents can be expressed as binary vectors.",
        "When two vectors are similar, the two words corresponding to the vectors may have some implicit relationship with each other.",
        "We call these two words a correlated pair.",
        "This report describes a method for obtaining the most highly correlated pairs of a given size.",
        "In practice, the method requires O(N x log (N)) computation time, and O(N) memory space, where N is the number of documents or records.",
        "Since this does not depend on the size of the vocabulary under analysis, it is possible to compute correlations between all the words in a corpus."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In order to find relationships between words in a large corpus or between labels in a large database, we may use a distance measure between the binary vectors of N dimensions, where N is the number of documents or records, and the ith element is 1 if the ith document/record contains the word or the label, or 0 otherwise.",
        "There are several distance measures suitable for this purpose, such as the mutual informa-tion(Church and Hanks, 1990), the dice coefficient(Manning and Schueutze 8.5, 1999), the phi coefficient(Manning and Schuetze 5.3.3, 1999), the cosine measure(Manning and Schueutze 8.5, 1999) and the confidence(Arrawal and Srikant, 1995).",
        "There are also special functions for certain applications, such as then complimentary similarity measure (CSM)(Hagita and Sawaki, 1995) which is known as to be suitable for cases with a noisy pattern.",
        "All of these five measures can be obtained from a simple contingency table.",
        "This table has four numbers for each word/label x and word/label y.",
        "The first number is the number of documents/records that have both x and y.",
        "We define this number as dfa(x, y).",
        "The second number is the number of documents/records that have x but not y.",
        "We define this number as dfb(x, y).",
        "The third number is the number of documents/records that do not have x but do have y.",
        "We define this number as df,(x, y).",
        "The fourth and the last number is the number of documents/records that have neither x nor y.",
        "We define this number as dfd(x, y).",
        "An obvious method to obtain the most highly related pairs is to calculate dfa,, dfb, df, dfd for all pairs of words/labels, compute the similarity for all pairs and then select pairs of the highest values.",
        "Let I be the number of possible words/labels, and N be the total number of documents/records in a corpus/database.",
        "This method requires 0(12) memory space and 0(12 x N) computation time.",
        "However, its use is only feasible if l is smaller than 104.",
        "When I is larger than ten thousand, execution of this procedure becomes difficult.",
        "The method described here is based on the observation that there is an upper boundary to the number of different words in one document.",
        "The assumption of such a boundary could even made of a large scale corpus.",
        "For example, a collective corpus of a newspaper may become larger and larger, but the length of each article is stable.",
        "It is not likely that one article would contain thousands of different words.",
        "In view of this observation and the assumption, this method is effective for obtaining the most highly correlated pairs in a large corpus, and uses O(N) memory space, and O(N x log(N)) computation time."
      ]
    },
    {
      "heading": "2 Notations",
      "text": [
        "Several notations are introduced in this section to describe the method.",
        "Assuming a corpus C, which is a set of sets of words, values are assigned as follows.",
        "• d : documents (elements of the corpus).",
        "dEC • x, y, z : label (elements of a document).",
        "xEd,ycd,zEd • x < y : y is placed after x in the alphabetical order.",
        "• N : the total number of documents.",
        "N = I C I • df (x) : the number of documents that contain X. df (x) =1 {dux E d} • df,, (x, y) : the number of documents that contain x and contain y. df.",
        "(x, y) =I {dIx E d n y E d} • dfb(x, y) : the number of documents that contain x but not y. dfb(x, y) =I {dux E d n y ¢ d}"
      ]
    },
    {
      "heading": "3 Problem Definition",
      "text": [
        "When the corpus of a set of sets of labels is provided, and the function rel (x, y) of a pair of labels to the number in the following form is also provided, we will obtain P: the set of pairs of a given size that satisfies the following condition.",
        "where",
        "The following are examples of f (a, b, c, d).",
        "• cosine function",
        "• pairwise mutual information",
        "• dfd(x, y) : the number of documents that cone-tain neither x nor y. dfd(x, y) =I {dux ¢ d n y V d} • complementary similarity measure",
        "Implementation of a program that requires 4 x 12 memory space and 4 x 12 x N computation time is easy.",
        "A program of this type could be used to calculate df,,, dfb, df,, and dfd for all pairs of x and y, and could then provide the most highly correlated pairs.",
        "However, compuation with this method is not feasible when l is large.",
        "For example, in order to calculate the most highly correlated words within a newspaper over several years of publication, l becomes roughly 105, and N becomes 108.",
        "The amount of computation time is then increased to 1018."
      ]
    },
    {
      "heading": "4 Approach",
      "text": [
        "In terms of the actual data, the number of correlated pairs is usually much smaller than the number of uncorrelated pairs.",
        "Moreover, most of the uncorrelated pairs usually satisfy the condition: df,, (x, y) = 0, and are not of interest.",
        "This method takes this fact into account.",
        "Moreover, it also uses the relationship between { N, df. }",
        "and { df , dfb, df,, dfd } to make the computation feasible."
      ]
    },
    {
      "heading": "5 Relationship between IN, df,, } and",
      "text": [
        "{df, dfb, df, dfd} Proofs of the following equations are provided below.",
        "1.",
        "P n P is equivalent to P. df.",
        "(X, x) = J{dJx E d n x E dJ J = J{dJx E dJJ = df (x) 2.",
        "By definition, the sum of dfa, dfb, df,, and dfd always represents the total number of documents.",
        "3.",
        "Similarly, the sum of df,, (x, y) and dfb (x, y) is the number of documents that contain x.",
        "This equals df (x).",
        "4.",
        "Similarly, the sum of df,, (x, y) and df, (x, y) is the number of documents that contain y.",
        "This equals df (y).",
        "= l{dlx E do y E d}l +I{dlx 0 d n y E dl = l{dly E d1l = df (y)",
        "5.",
        "These four equations make it possi",
        "ble to express df, dfb, df, and dfd by df,, and N. These formulas indicate that the number of required two-dimensional tables is not four, but just one.",
        "In other words, if we create a table of dfa, (x, y) and one variable for N, we can obtain df (x), dfb (x, y), df, (x, y), and dfd (x, y)."
      ]
    },
    {
      "heading": "6 The memory requirement for df,",
      "text": [
        "Let k be the maximum number of different words/labels in one document.",
        "The following prop",
        "The left side of the formula equals the total number of all pairs of words/labels.",
        "This cannot exceed k2 x N. This relationship indicates that if the table is stored using tuples of (x, y, dfa (x, y)) where df.",
        "(x, y) > 0, the required memory space is O (N) .",
        "Tuples where df,, (x, y) = 0 are not necessary because we know that df,, (x, y) = 0 when the tuple for (x, y, df.",
        "(x, y)) does not exist in memory.",
        "This estimation is pessimistic.",
        "The actual size of the tuples will be smaller than k2 x N, since not all documents will have k different words/labels."
      ]
    },
    {
      "heading": "7 Obtaining df, and N",
      "text": [
        "The algorithm to obtain df,, (x, y), and N is straightforward.",
        "First, the corpus must be trasformed into a set of sets of words/labels.",
        "Since this is a set form, there are no duplications of the words/labels of one document.",
        "In the following program, the hashtable returns 0 for a non-existent item.",
        "(1) Let DFA be empty hashtable.",
        "(2) Let DF be empty hashtable (3) Let N be 0 (4) For each document, assign it to (5) |N = N + 1 (6) |For each word in D (7) |assign the word to X (8) ||For each word in D (9) ||assign the word to Y (10) DFA(X, Y)=DFA(X,Y)+1 (11) ||end of loop (12) |end of loop (13) end of loop",
        "The computation time for this program is less than k2 x N. Since k is independent from N, the computation time is O(N).",
        "Again, k2 x N is a pessimistic estimation, since not all documents will have k different words/labels."
      ]
    },
    {
      "heading": "8 Selecting Pairs",
      "text": [
        "Even though df,,, dfb, df,, and df, can be obtained in constant time after O(N) preprocessing, there are 12 values to consider to obtain the best N correlated pairs.",
        "Fortunately, many of the functions thatare usable as indicators of correlation and, at least, all five functions, return a lower value than the known threshold if df,, (x, y) = 0.",
        "The cosine measure, the dice coefficient, and pairwise mutual information have property 1 and property 2 as defined below.",
        "This implies that the value for (x, y) where df.",
        "(x, y) = 0 is actually the minimum value of all (x, y).",
        "Therefore, the first part of the total ordered sequence of (x, y) is the sorted list of (x, y) where df (x, y) > 0.",
        "The rest is an arbitary order of pairs where df (x, y) = 0.",
        "The phi coefficient and the complementary similarity measure have the following properties 1, 2 and 3.",
        "Therefore, the first part of the total ordered sequence where the value is positive, is equal to first part of the sorted list where df (x, y) > 0 and the value is positive.",
        "Moreover, this list contains all pairs that have a positive correlation.",
        "This list is long enough for the actual application.",
        "same time, the estimated value is positive.",
        "It should be recalled that the number of pairs where df,, (x, y) > 0 is less than k2 x N. The sorted list is obtained in 0(k2 x N x log(k2 x N)) computation time, where k is the maximum number of different words/labels in one document.",
        "Since k is constant, it becomes O(N x log(N)), even if the size ofvocabulary is very large.",
        "It is true that for the given some fixed vocabulary of size 1, k2 x N might be larger than 12 as we increase the size of corpus.",
        "Fortunately, the actual memory consumption of this procedure also have the upper bound of 0(12), and we will not loose any memory space.",
        "When 1 is not fixed and 1 may become very large compare to N as is the case for proper nouns, k2 x N is smaller than 12."
      ]
    },
    {
      "heading": "9 Case study of a Newspaper Corpus",
      "text": [
        "The computation time of the baseline system is 12 x N where 1 is the distinct number of labels in the",
        "corpus.",
        "When we analyzed labels of names of places in a newspaper over the course of one year, this corpus consisted of about 60,000 documents.",
        "The place names totalled 1902 after morphological analysis.",
        "The maximum number of names in one document was 142, and the average in one document was 4.02.",
        "In this case, the method described here, was much more efficient than the baseline system.",
        "Table 1 shows the actual execution time of the program in the appendix, changing the length of the corpus.",
        "This program computes similarity values for all pairs of words where df,, > 0.",
        "It indicates that the execution time is linear.",
        "Our observation shows that even if the corpus were extended year by year, k which is the maximum number of different words in one document is stable, even though the total number of words would increase with the ongoing addition of proper nouns and new concepts."
      ]
    },
    {
      "heading": "10 For a large corpus",
      "text": [
        "Although the program in the appendix cannot be applied to a corpus larger than memory size, we can obtain a table of df,, using sequential access to file.",
        "The program in the appendix stores every pair in memory.",
        "The space requirement of ✒2 x N may seem too great to hold in memory.",
        "However, sequential file can be used to obtain the df,, table, as follows.",
        "Although the computation time for df,, is O(N x log (N)) rather than O(N), the total computation time remains the same because computation of O(N x log (N)) is required to select pairs in both cases.",
        "Consider the following data.",
        "Each line corresponds to one document.",
        "When the pairs of words in each document are recorded, the following file is obtained.",
        "Note that since df.",
        "(x,y) = df.",
        "(y,x), it is not necessary to record pairs where x > y.",
        "This reduces the memory requirement.",
        "Using the merge sort algorithm which can sort a large file using sequential access only, the file can be sorted in O(N x log (N)) computation time.",
        "After sorting in alphabetical order, same pairs come together.",
        "Then, the pairs can be counted with sequential access, thereby providing the df,, table.",
        "An example of this table fllows:",
        "It should be noted that the df table can be obtained easily by extracting lines in which letter of the first column and that of the second column are the same, since df (x) = df,, (x, x).",
        "The df table can usually be stored in memory since it is a one dimensional array.",
        "After storing df in memory, similarity can be computed line by line.",
        "The following example uses the phi coefficient.",
        "The first column is the coefficient, followed by df., dfb, df,, dfd, x and y.",
        "Since the phi coefficient is reflective, the (x, y) value where x > y is not required.",
        "When the function is not symmetric, (x, y) and (dfb, df,) can be exchanged at the same time.",
        "The ordered list can be obtained by sorting this table with the first column.",
        "This example shows that pairs where df,, (x, y) = 0, such as (a, x) or (a, y), do not add any overhead to either memory or computation time."
      ]
    },
    {
      "heading": "11 Comparison with Apriori",
      "text": [
        "There is a well known algorithm for forming a list of related items termed Apriori(Arrawal and Srikant, 1995).",
        "Apriori lists all relationship using confidence, where df ,,(x, y) is larger than a specified value.",
        "Using Apriori, the df,, threshold can be specified in order to reduce computation, whereas with the proposed method, there is no way to adjust this threshold.",
        "This implies that Apriori may be faster than our algorithm in terms of confidence.",
        "However, since Apriori uses the property of confidence to reduce computation, it cannot be used for other functions, unlike the proposed method which can employ many standard functions, at least the five measures used here including confidence."
      ]
    },
    {
      "heading": "12 Correlation of All Substrings",
      "text": [
        "When computing correlations of all substrings in a corpus, l can be as large as N x (N – 1) /2.",
        "Since the memory space requirement and computation time does not depend on 1, this method can be used to generate a list of the most hightly correlated substrings of any length.",
        "In fact, in some cases, k may be too large to compute.",
        "The Yamamoto-Church method(Yamamoto and Church, 2001) allows for the creation of a df (x) table using O(N) memory space and O(N x log (N)) computation time, where x represents all substrings in a given corpus.",
        "Yamamoto’s method shows that although there may be N x (N – 1)/2 kinds of substrings in a corpus, there is 2 x N occurence patterns (or sets of substrings which have same occurence pattern) at most.",
        "The computational cost is greatly reduced if we deal with each pattern instead of each substring.",
        "Although the order of com-putional complexity does not depend on 1, k differs whether the pattern is used or not.",
        "We have also developed a system using the pattern which actually reduces the cost of computation.",
        "Although the number of k is still problematic even using the Yamamoto-Church method, and although the computation cost is much larger than using words, the program runs much faster than the simple method."
      ]
    },
    {
      "heading": "13 Conclusion",
      "text": [
        "This paper describes a method for selecting correlated pairs in O(N) memory space and O(N x log(N)) computation time, where N is the number of documents in a corpus, provided that there is an upper boundary in the number of different words/labels in one document/record.",
        "We have observed that a corpus usually has this kind of upper boundary, and have shown that we can uses a sequential file for most of our memory requirements.",
        "This method is useful not only for confidence but also for other functions whose values are decided by df,,, dfb, df, dfd.",
        "Examples of these functions are mutual information, the dice coefficient, the confidence measure, the phi coefficient and the complimentary similarity measure."
      ]
    }
  ]
}
