{
  "info": {
    "authors": [
      "Benoit Lavoie",
      "Michael White",
      "Tanya Korelsky"
    ],
    "book": "Workshop on Machine Translation in Asia",
    "id": "acl-W02-1610",
    "title": "Learning Domain-Specific Transfer Rules: An Experiment With Korean to English Translation",
    "url": "https://aclweb.org/anthology/W02-1610",
    "year": 2002
  },
  "references": [
    "acl-A00-1009",
    "acl-A97-1039",
    "acl-C90-3044",
    "acl-C92-3145",
    "acl-H01-1014",
    "acl-J94-4004",
    "acl-P00-1056",
    "acl-P01-1030",
    "acl-P93-1004",
    "acl-P98-2139",
    "acl-W01-1402",
    "acl-W01-1403"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results.",
        "The system learns lexico-structural transfer rules using syntactic pattern matching, statistical co-occurrence and error-driven filtering.",
        "In an experiment with domain-specific Korean to English translation, the approach yielded substantial improvements over three baseline systems."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In this paper, we describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation.",
        "Our approach is based on lexico-structural transfer (Nasr et.",
        "al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular.",
        "Whereas Han et al.",
        "focus on high quality domain-specific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules.",
        "The proposed approach is inspired by example-based machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses.",
        "However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.",
        "), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features.",
        "A more detailed description of the differences can be found in (Lavoie et.",
        "al., 2001)."
      ]
    },
    {
      "heading": "2 Overall Runtime System Design",
      "text": [
        "Our Korean to English MT runtime system relies on the following off-the-shelf software components: Korean parser For parsing, we used the wide coverage syntactic dependency parser for Korean developed by (Yoon et al., 1997).",
        "The parser was not trained on our corpus.",
        "Transfer component For transfer of the Korean parses to English structures, we used the same lexico-structural transfer framework as (Lavoie et al., 2000).",
        "Realizer For surface realization of the transferred English syntactic structures, we used the Re-alPro English realizer (Lavoie and Rambow, 1997).",
        "The training of the system is described in the next two sections."
      ]
    },
    {
      "heading": "3 Data Preparation",
      "text": []
    },
    {
      "heading": "3.1 Parses for the Bitexts",
      "text": [
        "In our experiments, we used a parallel corpus derived from bilingual training manuals provided by the U.S. Defense Language Institute.",
        "The corpus consists of a Korean dialog of 4,183 sentences about battle scenario message traffic and their English human translations.",
        "The parses for the Korean sentences were obtained using Yoon’s parser, as in the runtime system.",
        "The parses for the English human transla",
        "tions were derived from an English Tree Bank developed in (Han et al., 2000).",
        "To enable the surface realization of the English parses via RealPro, we automatically converted the phrase structures of the English Tree Bank into deep-syntactic dependency structures (DSyntSs) of the Meaning-Text Theory (MTT) (Mel’ˇcuk, 1988) using Xia’s converter (Xia and Palmer, 2001) and our own conversion grammars.",
        "The realization results of the resulting DSyntSs for our training corpus yielded a unigram and bigram accuracy (f-score) of approximately 95% and 90%, respectively.",
        "A DSyntS is an unordered tree where all nodes are meaning-bearing and lexicalized.",
        "Since the output of the Yoon parser is quite similar, we have used its output as is.",
        "The syntactic dependency representations for two corresponding Korean1 and English sentences are shown in Figure 1."
      ]
    },
    {
      "heading": "3.2 Training and Test Sets of Parse Pairs",
      "text": [
        "The average sentence lengths (in words) and parse sizes (in nodes) for the 4,183 Korean and English sentences in our corpus are given in Table 1.",
        "In examining the Korean parses, we found that many of the larger parses, especially those containing intra-sentential punctuation, had incorrect dependency assignments, incomplete lemmatization or were incomplete parses.",
        "In examining the English converted parses, we found that many of",
        "the parses containing intra-sentential punctuation marks other than commas had incorrect dependency assignments, due to the limitations of our conversion grammars.",
        "Consequently, in our experiments we have primarily focused on a higher quality subset of 1,483 sentence pairs, automatically selected by eliminating from the corpus all parse pairs where one of the parses contained more than 11 content nodes or involved problematic intra-sentential punctuation.",
        "We divided this higher quality subset into training and test sets.",
        "For the test set, we randomly selected 50 parse pairs containing at least 5 nodes each.",
        "For the training set, we used the remaining 1,433 parse pairs.",
        "The average sentence lengths and parse sizes for the training and test sets are represented in Tables 2 and 3."
      ]
    },
    {
      "heading": "3.3 Creating the Baseline Transfer Dictionary",
      "text": [
        "In our system, transfer dictionaries contain Korean to English lexico-structural transfer rules defined using the formalism described in (Nasr et.",
        "al., 1997), extended to include log likelihood ratios (Manning and Schütze, 1999: 172-175).",
        "Sample transfer rules are illustrated in Section 4.",
        "The simplest transfer rules consist of direct lexical mappings, while the most complex may contain source and target syntactic patterns composed of multiple nodes defined with lexical and/or syntactic features.",
        "Each transfer rule is assigned a log likelihood ratio calculated using the training parse set.",
        "To create the baseline transfer dictionary for our experiments, we had three bilingual dictionary resources at our disposal: A corpus-based handcrafted dictionary: This dictionary was manually assembled by (Han et al., 2000) for the same corpus used here.",
        "Note,",
        "however, that it was developed for different parse representations, and with an emphasis primarily on the lexical coverage of the source parses, rather than the source and target parse pairs.",
        "A corpus-based extracted dictionary: This dictionary was automatically created from our corpus by the RALI group from the University of Montreal.",
        "Since the extraction heuristics did not handle the rich morphological suffixes of Korean, the extraction results contained inflected words rather than lexemes.",
        "A wide coverage dictionary: This dictionary of 70,300 entries was created by Systran, without regard to our corpus.",
        "We processed and combined these resources as follows:",
        "• First, we replaced the inflected words with un-inflected lexemes using Yoon’s morphological analyzer and a wide coverage English morphological database (Karp and Schabes, 1992).",
        "• Second, we merged all morphologically analyzed entries after removing all non-lexical features, since these features generally did not match those found in the parses.",
        "• Third, we matched the resulting transfer dictionary entries with the training parse set, in order to determine for each entry all possible part-of-speech instantiations and dependency relationships.",
        "For each distinct instantiation, we calculated a log likelihood ratio.",
        "• Finally, we created a baseline dictionary using the instantiated rules whose source patterns had the best log likelihood ratios.",
        "Table 4 illustrates the concurrent lexical coverage of the training set using the resulting baseline dictionary, i.e. the percentage of nodes covered by rules whose source and target patterns both match.",
        "Note that since the baseline dictionary contained some noise, we allowed induced rules to override ones in the baseline dictionary where applicable."
      ]
    },
    {
      "heading": "4 Transfer Rule Induction",
      "text": [
        "The transfer rule induction process has the following steps described below (additional details can also be found in (Lavoie et.",
        "al., 2001)):",
        "• Nodes of the corresponding source and target parses are aligned using the baseline transfer dictionary and some heuristics based on the similarity of part-of-speech and syntactic context.",
        "• Transfer rule candidates are generated based on the sub-patterns that contain the corresponding aligned nodes in the source and target parses.",
        "• The transfer rule candidates are ordered based on their likelihood ratios.",
        "• The transfer rule candidates are filtered, one at a time, in the order of the likelihood ratios, by removing those rule candidates that do not produce an overall improvement in the accuracy of the transferred parses.",
        "Figures 2 and 3 show two sample induced rules.",
        "The rule formalism uses notation similar to the syntactic dependency notation shown in Figure 1, augmented with variable arguments prefixed with $ characters.",
        "These two lexico-structural rules can be used to transfer a Korean syntactic representation for ci-to-reul po-ra to an English syntactic representation for look at the map.",
        "The first rule lexicalizes the English predicate and inserts the corresponding preposition while the second rule inserts the English imperative attribute."
      ]
    },
    {
      "heading": "4.1 Aligning the Parse Nodes",
      "text": [
        "To align the nodes in the source and target parse trees, we devised a new dynamic programming",
        "alignment algorithm that performs a top-down, bidirectional beam search for the least cost mapping between these nodes.",
        "The algorithm is parameterized by the costs of (1) aligning two nodes whose lexemes are not found in the baseline transfer dictionary; (2) aligning two nodes with differing parts of speech; (3) deleting or inserting a node in the source or target tree; and (4) aligning two nodes whose relative locations differ.",
        "To determine an appropriate part of speech cost measure, we first extracted a small set of parse pairs that could be reliably aligned using lexical matching alone, and then based the cost measure on the co-occurrence counts of the observed parts of speech pairings.",
        "The remaining costs were set by hand.",
        "As a result of the alignment process, alignment id attributes (aid) are added to the nodes of the parse pairs.",
        "Some nodes may be in alignment with no other node, such as English prepositions not found in the Korean DSyntS."
      ]
    },
    {
      "heading": "4.2 Generating Rule Candidates",
      "text": [
        "Candidate transfer rules are generated by extracting source and target tree sub-patterns from the aligned parse pairs using the two set of constraints described below."
      ]
    },
    {
      "heading": "4.2.1 Alignment constraints",
      "text": [
        "Figure 4 shows an example alignment constraint.",
        "This constraint, which matches the structural patterns of the transfer rule illustrated in Figure 2, uses the aid alignment attribute to indicate that in a Korean and English parse pair, any source and target sub-trees matching this alignment constraint (where $X1 and $Y1 are aligned, i.e. have the same aid attribute values, and where $X2 and $Y3 are aligned) can be used as a point of departure for generating transfer rule candidates.",
        "We suggest that alignment constraints such as this one can be used to define most of the possible syntactic divergences between languages (Dorr, 1994), and that only a handful of them are necessary for two given languages (we have identified 11 general alignment constraints",
        "Each node of a candidate transfer rule must have its relation attribute (relationship with its governor) specified if it is an internal node, otherwise this relation must not be specified:",
        "necessary for Korean to English transfer so far)."
      ]
    },
    {
      "heading": "4.2.2 Attribute constraints",
      "text": [
        "Attribute constraints are used to limit the space of possible transfer rule candidates that can be generated from the sub-trees satisfying the alignment constraints.",
        "Candidate transfer rules must satisfy all of the attribute constraints.",
        "Attribute constraints can be divided into two types:",
        "• independent attribute constraints, whose scope covers only one part of a candidate transfer rule and which are the same for the source and target parts; • concurrent attribute constraints, whose scope extends to both the source and target parts of a candidate transfer rule.",
        "Figures 5 and 6 give examples of an independent attribute constraint and of a concurrent attribute constraint.",
        "As with the alignment constraints, we suggest that a relatively small number of attribute constraints is necessary to generate most of the desired rules for a given language pair."
      ]
    },
    {
      "heading": "4.3 Ordering Rule Candidates",
      "text": [
        "In the next step, transfer rule candidates are ordered as follows.",
        "First, they are sorted by their decreasing log likelihood ratios.",
        "Second, if two or more candidate transfer rules have the same log likelihood ratio, ties are broken by a specificity heuristic, with the result that more general rules are ordered ahead In a candidate transfer rule, inclusion of the lexemes of two aligned nodes must be done concurrently: e.g.",
        "of more specific ones.",
        "The specificity of a rule is defined to be the following sum: the number of attributes found in the source and target patterns, plus 1 for each for each lexeme attribute and for each dependency relationship.",
        "In our initial experiments, this simple heuristic has been satisfactory."
      ]
    },
    {
      "heading": "4.4 Filtering Rule Candidates",
      "text": [
        "Once the candidate transfer rules have been ordered, error-driven filtering is used to select those that yield improvements over the baseline transfer dictionary.",
        "The algorithm works as follows.",
        "First, in the initialization step, the set of accepted transfer rules is set to just those appearing in the baseline transfer dictionary.",
        "The current error rate is also established, by applying these transfer rules to all the source structures and calculating the overall difference between the resulting transferred structures and the target parses, using a tree accuracy recall and precision measure (determined by comparing the features and dependency relationships in the transferred parses and corresponding target parses).",
        "Then, in a single pass through the ordered list of candidates, each transfer rule candidate is tested to see if it reduces the error rate.",
        "During each iteration, the candidate transfer rule is provisionally added to the current set of accepted rules and the updated set is applied to all the source structures.",
        "If the overall difference between the transferred structures and the target parses is lower than the current error rate, then the candidate is accepted and the current error rate is updated; otherwise, the candidate is rejected and removed from the current set."
      ]
    },
    {
      "heading": "4.5 Discussion of Induced Rules",
      "text": [
        "In our experiments, the alignment constraints yielded 22,881 source and target sub-tree pairs from the training set of 1,433 parse pairs.",
        "Using the at",
        "tribute constraints, an initial list of 801,674 transfer rule candidates was then generated from these sub-tree pairs.",
        "The initial list was subsequently reduced to 32,877 unique transfer rule candidates by removing duplicates and by eliminating candidates that had the same source pattern as another candidate with a better log likelihood ratio.",
        "After filtering, 2,133 of these transfer rule candidates were accepted.",
        "We expect that the number of accepted rules per parse pair will decrease with larger training sets, though this remains to be verified.",
        "The rule illustrated in Figure 3 was accepted as the 65th best transfer rule with a log likelihood ratio of 33.37, and the rule illustrated in Figure 2 was accepted as the 189th best transfer rule candidate with a log likelihood ratio of 12.77.",
        "An example of a candidate transfer rule that was not accepted is the one that combines the features of the two rules mentioned above, illustrated in Figure 7.",
        "This transfer rule candidate had a lower log likelihood ratio of 11.40; consequently, it is only considered after the two rules mentioned above, and since it provides no further improvement upon these two rules, it is filtered out.",
        "In an informal inspection of the top 100 accepted transfer rules, we found that most of them appear to be fairly general rules that would normally be found in a general syntactic-based transfer dictionary.",
        "In looking at the remaining rules, we found that the rules tended to become increasingly corpus-specific.",
        "The induction results were obtained using a Java implementation of the induction component.",
        "Microsoft SQL Server was used to count and deduplicate the rule candidates.",
        "The data preparation and induction processes took about 12 hours on a 300 MHz PC with 256 MB RAM.",
        "("
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": []
    },
    {
      "heading": "5.1 Systems Compared",
      "text": [
        "Babelfish As a first baseline, we used Babelfish from Systran, a commercial large coverage MT system supporting Korean to English translation.",
        "This system was not trained on our corpus.",
        "GIZA++/RW As a second baseline, we used an off-the-shelf statistical MT system, consisting of the ISI ReWrite Decoder (Germann et al., 2001) together with a translation model produced by GIZA++ (Och and Ney, 2000) and a language model produced by the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).",
        "This system was trained on our corpus only.",
        "Lex Only As a third baseline, we used our system with the baseline transfer dictionary as the sole transfer resource.",
        "Lex+Induced We compared the three baseline systems against our complete system, using the baseline transfer dictionary augmented with the induced transfer rules.",
        "We ran each of the four systems on the test set of 50 Korean sentences described in Section 3.2 and compared the resulting translations using the automatic evaluation and the human evaluation described below."
      ]
    },
    {
      "heading": "5.2 Automatic Evaluation Results",
      "text": [
        "For the automatic evaluation, we used the Bleu metric from IBM (Papineni et al., 2001).",
        "The Bleu metric combines several modified N-gram precision measures (N = 1 to 4), and uses brevity penalties to penalize translations that are shorter than the reference sentences.",
        "Table 5 shows the Bleu N-gram precision scores for each of the four systems.",
        "Our system (Lex+Induced) had better precision scores than each of the baseline systems, except in the case of 4 grams, where it slightly trailed Babelfish.",
        "The statistical baseline system (GIZA++/RW) performed poorly, as might have been expected given the small amount of training data.",
        "Table 6 shows the Bleu overall precision scores.",
        "Our system (Lex+Induced) improved substantially over both the Lex Only and Babelfish baseline systems.",
        "The score for the statistical baseline system (GIZA++/RW) is not meaningful, due to the absence of 3-gram and 4-gram matches."
      ]
    },
    {
      "heading": "5.3 Human Evaluation Results",
      "text": [
        "For the human evaluation, we asked two English native speakers to rank the quality of the translation results produced by the Babelfish, Lex Only and Lex+Induced, with preference given to fidelity over fluency.",
        "(The translation results of the statistical system were not yet available when the evaluation was performed.)",
        "A rank of 1 was assigned to the best translation, a rank of two to the second best and a rank of 3 to the third, with ties allowed.",
        "Table 7 shows the pairwise comparisons of the three systems.",
        "The top section indicates that the Babelfish and Lex Only baseline systems are essentially tied, with neither system preferred more frequently than the other.",
        "In contrast, the middle and bottom sections show that our system improves substantially over both baseline systems; most strikingly, our system (Lex+Induced) was preferred almost 20% more frequently than the Babelfish baseline (46% to 27%, with ties 27% of the time)."
      ]
    },
    {
      "heading": "6 Conclusion and Future Work",
      "text": [
        "In this paper we have described the design of an MT system based on lexico-structural transfer rules induced from parsed bitexts.",
        "In a small scale experiment with Korean to English translation, we have demonstrated a substantial improvement over three baseline systems, including a nearly 20% improvement in the preference rate for our system over Babelfish (which was not trained on our corpus).",
        "Although our experimentation was aimed at Korean to English translation, we believe that our approach can be readily applied to other language pairs.",
        "It remains for future work to explore how well the approach would fare with a much larger training corpus.",
        "One foreseeable problem concerns the treatment of lengthy training sentences: since the number of transfer rule candidates generated grows exponentially with the size of the parse tree pairs, refinements will be necessary in order to make use of complex sentences; one option might be to automatically chunk longer sentences into smaller units."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We thank Richard Kittredge for helpful discussion, Daryl McCullough and Ted Caldwell for their help with evaluation and Fei Xia for her assistance with the automatic conversion of the phrase structure parses to syntactic dependency representations.",
        "We also thank Chung-hye Han, Chulwoo Park, Martha Palmer, and Joseph Rosenzweig for the handcrafted Korean-English transfer dictionary, and Graham Russell for the corpus-based extracted transfer dictionary.",
        "This work has been partially supported by DARPA TIDES contract no.",
        "N66001-00-C8009."
      ]
    }
  ]
}
