{
  "info": {
    "authors": [
      "William J. Black",
      "Argyrios Vasilakopoulos"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W02-2002",
    "title": "Language Independent Named Entity Classification by Modified Transformation-Based Learning and by Decision Tree Induction",
    "url": "https://aclweb.org/anthology/W02-2002",
    "year": 2002
  },
  "references": [
    "acl-C94-2195",
    "acl-E99-1001",
    "acl-J95-4004",
    "acl-M98-1015",
    "acl-P98-2188",
    "acl-W95-0107"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe our last results at the CoNLL2002 shared task of Named Entity Recognition and Classification using two approaches that we first applied to other NLL problems.",
        "We have been developing our own modified TBL learner initially to tackle the Part-of-Speech tagging problem, for integration in a hybrid NLL and rule based system for information extraction (Ciravegna et al., 1999).",
        "After encouraging results in applying decision tree induction to the CoNLL2001 task of chunking, Jones (2002), where we attained an overall F measure of 92.90 at this task, we have applied the same set-up to the NER task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Named Entity Classification (NEC) is the process of identifying and classifying names in text and is a crucial task for several natural language processing areas such as information retrieval, information extraction, machine translation and language understanding.",
        "From 1995 when the NE task was first introduced as part of the Message Understanding Conference (MUC6) most systems that have attempted this task are based in lists of common names in order to provide some clues.",
        "These lists are, in most cases, very large and can provide an efficient way of dealing with NEC but it is still a nave method for recognizing names.",
        "About the size of the lists, Krupke and Hausman (1998) found that the good quality of a name list is quite more important than its total size, while Mikheev et al.",
        "(1999) concluded that small but well elaborated lists can be as effective as the larger ones.",
        "Except from name lists many of the NEC systems also use a number of other NLP tools such as hand-crafted rules, morphological dis-ambiguators, chunkers and parsers taking advantage of what McDonald (1996) defines as internal and external evidence in the NEC.",
        "While many systems tend to recognize the names in text very efficiently - Zhou and Su (2002) report F-measures of 96.8% and 94.2% on the MUC6 and MUC7 datasets - most of them are designed for specific domains and specific languages.",
        "Our aim has been to build a language independent system for dealing with the NEC task.",
        "We have attempted two different approaches using only the data provided for the CoNLL2002 shared task of NEC.",
        "The first approach is based on the Transformation Based Learning method, a very efficient method which has been successfully attempted for other NLP tasks like PP-attachment disambiguation (Brill and Resnik, 1994), part-of-speech tagging (Brill, 1995), text chunking (Ramshaw and Marcus, 1995), dialog act tagging (Samuel et al., 1988) ellipsis resolution (Hardt, 1998) and spelling correction (Magnu and Brill, 1997).",
        "Our second approach is a simple decision tree induction scheme.",
        "In the next two sections we describe our approaches in more detail and in section 3 we present our results so far."
      ]
    },
    {
      "heading": "2 Modified TBL Approach",
      "text": [
        "The learner we used in our first experiment differs from Brill's TBL Learner (Brill, 1995) in that it produces a set of transformation rules in a single pass without basing each learning cycle on a new initial state.",
        "We were motivated to do this after noticing how little the revision stage contributes to the final precision of the tag-ger, having found that improved unknown word guessing contributes more than is lost by abandoning the multi-pass patch acquisition step.",
        "The approach followed here consists of two main stages for both the learner and the tagger: (i) In the first stage we try just to recognize all the named entities (NE's) in the training or test data by taking into account only the orthography feature of the NE's. (ii) In the second stage we classify the NE's found during the stage (i) by using a corpus derived lexicon and contextual rules."
      ]
    },
    {
      "heading": "2.1 Learning",
      "text": [
        "The learning process consists of the induction of two sets of rules and a lexicon.",
        "The first set of rules is induced in the first stage and results in a binary classification between the `O' of the training corpus to `NE' which generalizes over the remaining tags.",
        "This stage creates an initially annotated version of the text.",
        "An example of a rule at this phase of analysis is: Change tag 0 to NE of the current word if the preceding and following words are both Capitalized and the current word is \"de\" which would result in the correction of",
        "Janeiro/NE.",
        "In the second stage we firstly create the lexicon from the training corpus in the way that Brill does in (Brill, 1995).",
        "The lexicon then is applied on the initially annotated text and the result is the initial state for the TBL algorithm.",
        "The initial state is afterwards compared to the original training corpus, according to a set of user defined templates and a second set of transformation rules is induced.",
        "The difference with Brill's original TBL is that in our case we keep all the rules which satisfy the accuracy and score thresholds, instead of keeping the best one and iterate the process until no more rule is found.",
        "We finally rank these rules.",
        "At this point it is essential to note that only the word sequences tagged as `NE's at the first stage, are subject to stage (ii)."
      ]
    },
    {
      "heading": "2.2 Tagger",
      "text": [
        "For tagging an unknown corpus, we firstly create the initial state of the corpus as described in the previous section, and we then apply all the ranked rules sequentially."
      ]
    },
    {
      "heading": "3 Decision Tree Induction",
      "text": [
        "As with our modified TBL approach, we use no data outside of the training corpus for the decision tree experiment.",
        "We used an of-the-shelf system (Weka's J4.8 variant of C4.5 - Weka 3 (2001)) for this experiment.",
        "The training data is converted into a 49-attribute table, covering 12 attributes of the current token, its two predecessors and its successor.",
        "There are 46 nominal and 3 numeric features.",
        "These features are:",
        "• The token itself if it is one of the 150 most frequent tokens, otherwise `miskTok'.",
        "• The orthography.",
        "Each token can only belong in one of the next 17 categories which are distinguished via regular expressions: f null, lowercase, capitalized, caphyphenated, lowerhyphenated, uppercase, multicap, upperdotted, initial, initialdot, punct, doublequote, apostrophe, number, numberrange, bracket, other } (Hopefully most of these are self evident in meaning) • `True' if the token is a frequent word `false' otherwise.",
        "• `True' if the suffix of the token is a frequent one `false' otherwise.",
        "• The most frequent category in the training data, or if not found in the training data `O' for lowercase tokens and `I-PER' for all the others.",
        "• The total number of occurrences of the token in the training data.",
        "• Six more features indicating `True' or",
        "`False' if the token appears as `B-`, `I- `, `PER', `LOC', `ORG' and `MISC' somewhere in the training data.",
        "The decision tree induced from the training data by using these attributes is then used in order to predict the NE class of the unknown words of the test corpus.",
        "Finally, a filter is applied to the result, aiming at removing any discrepancies which refer to the patterns of the NE sequences.",
        "So, this filter corrects the following mistakes: a) it changes the following label sequence from \"<B-X> <B-X> ...\" to \"<BX> <I-X>...\"and b) it changes the following label sequence from \"<B-X> or <I-X> <I-Y> <I-X> ... \" to \"<B-X> or <I-X> <I-X> <IX>...\" if <I-Y> is the classification label of a functional word.",
        "In the above, the X and Y variables can take one of the following values: LOC, PER, ORG, MISC."
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "We have tested only our modified TBL approach with all the data available from the CoNLL2002.",
        "Especially, for the Dutch data we used only the NE tags of the tokens of the training corpus, as we did for the Spanish data.",
        "In the case of the decision tree induction, having spent most of our time on the data conversion and preparation, we have only conducted a run only for the Spanish data by using only the first 100,000 tokens of the training corpus.",
        "The overall results are as shown in the Tables 1, 2, 3 and 4."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "This paper has presented two different approaches for solving the named entity classification task using supervised learning.",
        "Our target has been to use the least resources for creating our rules, for the modified Transformation-Based approach, and inducing our decision tree for the Decision Tree Induction approach.",
        "So far, it seems that the modified TBL approach gives better results on Spanish data than our second approach.",
        "After developing our modified TBL learner using the Spanish data and trained them using both the Spanish and the Dutch training corpora we observed the fact that our approach works better with the Spanish test texts.",
        "Our both systems seem to per",
        "form better on `persons' regarding the recall, while the precision values look better for the `lo-cations' and the `organizations'.",
        "Our best result is an F-measure value of 68.21.",
        "This is not high enough but the result is encouraging if we take into consideration that both our approaches are very simple and they do not make use of any extended resource except from the information contained in the training text."
      ]
    }
  ]
}
