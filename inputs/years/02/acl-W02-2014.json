{
  "info": {
    "authors": [
      "Anna Korhonen",
      "Yuval Krymolowski"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W02-2014",
    "title": "On the Robustness of Entropy-Based Similarity Measures in Evaluation of Subcategorization Acquisition Systems",
    "url": "https://aclweb.org/anthology/W02-2014",
    "year": 2002
  },
  "references": [
    "acl-A00-2034",
    "acl-A97-1052",
    "acl-J02-3004",
    "acl-P96-1041",
    "acl-P98-2127",
    "acl-P99-1004",
    "acl-W02-0907"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Some statistical learning systems are evaluated using measures of distributional similarity.",
        "To deal with the problem of zero events in the distributions under comparison, smoothing is frequently performed before similarity measures are applied.",
        "Smoothing alters the information in the original distribution, and may add noise to the results.",
        "Here, we investigate the sensitivity of entropy-based similarity measures to noise from uninformative smoothing.",
        "Our experiments with two subcategorization acquisition systems show that similarity measures vary in their robustness.",
        "While some are led astray by noise from smoothing, others are more resilient."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many natural language processing (NLP) tasks involve measuring distributional similarity.",
        "Some examples are the estimation of word co-occurrence probabilities (Dagan et al., 1999), automatic construction of thesauri (Lin, 1998), automatic detection of diathesis alternations (McCarthy, 2000), disambiguation of nominal-izations (Zapata, 2002), and evaluation of statistical NLP learners (Carroll and Rooth, 1998; Korhonen, 2002b).",
        "Various similarity measures have been proposed and used for NLP purposes, including the Kullback-Leibler distance (Cover and Thomas, 1991), cross entropy (Cover and Thomas, 1991), the Jensen-Shannon divergence (Lin, 1991), the skew divergence (Lee, 1999), cosine (Frakes and Baeza-Yates, 1992), Jaccard's coefficient, L1 norm, and the confusion probability.",
        "In this paper, we discuss the use of similarity measures in evaluation of the type of statistical language learners which deliver as an output a distribution of events.",
        "A typical example is an automatic subcategorization acquisition system (e.g. (Briscoe and Carroll, 1997; Carroll and Rooth, 1998; Korhonen, 2002b)), which learns, from corpus data, a distribution of subcategorization frames (SCFS) specific to a certain predicate (p(scfilpredicatej)).",
        "These learners are frequently evaluated using the standard precision, recall and accuracy measures (Manning and Schütze, 1999).",
        "However, similarity measures provide an important means to evaluate the actual acquired frequencies.",
        "In similarity-based evaluation, a learned distribution is compared with a gold standard distribution in order to determine how closely the two correlate.",
        "Such evaluation is complicated by cases where the distributions under comparison have different supports, i.e. regions of positive probability.",
        "Due to the sparse data problem, zero events typically make up a substantial portion of joint data.",
        "To allow the comparison of all events, smoothing is frequently performed before similarity measures are applied.",
        "Smoothing tackles the problem by assigning non-zero values to zero events.",
        "It is usually done in an uninformative way (by assigning a uniform prior probababil-ity to events; e.g. (Laplace, 1814; Witten and Bell, 1991)), rather than in an informative way (by assigning an informative prior probability e.g. by backing-off (Katz, 1987)), as it is desirable – from the evaluation point of view – to preserve as much of the original, learned distribution as possiblel.",
        "Although uninformative smoothing makes the simplest possible assumption regarding the probability of unseen events, it involves also altering the information included in the original 'See Manning and Schütze (1999) for both informative and uninformative smoothing methods.",
        "distribution.",
        "It can add `noise' to the original data.",
        "This noise may, in turn, affect distributional similarity and potentially obscure similarity-based evaluation.",
        "Here, we investigate the sensitivity of widely-used entropy-based similarity measures to the noise from uninformative smoothing.",
        "We do this in the context of evaluation, by using the measures to evaluate the accuracy of verbal SCF distributions learned automatically from corpus data.",
        "By controlling the number of SCFS smoothed and examining the effect on distributional similarity, we observe differences in the robustness of various measures.",
        "Our results show that some entropy-based similarity measures are led astray by noise from smoothing, while others are more resilient and thus better suited for evaluation purposes.",
        "Section 2 introduces the sub categorization learners employed.",
        "The similarity measures are described in section 3 and the smoothing methods in section 4.",
        "Section 5 reports our experiments.",
        "We discuss our findings in section 6 and present our conclusions in section 7.",
        "2 Subcategorization Learners We used two subcategorization learners proposed by Korhonen (2002b) to obtain the SCF distributions employed in our experiments.",
        "The learners are variations of the subcategorization acquisition system of Briscoe and Carroll (1997).",
        "The system uses a shallow parser to obtain the subcategorization information from corpus data.",
        "It distinguishes 163 verbal SCFS and returns relative frequencies for each SCF found for a given verb.",
        "The resulting putative SCF distributions are processed by the two learners as follows: Learner 1: The SCFS in putative distributions are simply ranked in the order of the probability of their occurrence with the verb.",
        "The probabilities are estimated using a maximum likelihood estimate (MLE) from the observed relative frequencies.",
        "Learner 2: The SCF distributions obtained using Learner 1 are smoothed using linear interpolation (Chen and Goodman, 1996).",
        "The informative back-off distribution employed in smoothing is based on the semantic class of the verb in question (p(scfijsemanticclassj)), chosen according to the verb's most frequent sense in WordNet (Miller, 1990).",
        "For instance, the predominant sense of the verb fly in WordNet belongs to the semantic class of \"Motion\" verbs - hence, the distribution of \"Motion\" verbs is employed as a back-off distribution in smoothing.",
        "The semantic classes are based on Levin classes (Levin, 1993) and the back-off distribution for each class is obtained by merging SCF distributions of a few verbs in the same class.",
        "The parameters used in smoothing are obtained by optimising SCF acquisition performance on held-out training data so that most of the smoothed probability is determined by the MLE from the subcategorization acquisition system2.",
        "Learner 2 tends to perform better than learner 1, since it involves using a priori knowledge about generalizations of verb semantics to guide subcategorization acquisition.",
        "It corrects the putative SCF distribution and deals better with sparse data.",
        "Korhonen (2002a) used an empirically determined threshold on the probability estimates to filter noisy SCFS out, and then evaluated the two learners on a test set of 91 verbs using precision and recall based evaluation.",
        "The gold standard employed in the evaluation was obtained by manually analysing an average of 300 occurrences of each test verb in corpus data.",
        "Learner 1 yielded 82% type precision (the percentage of SCF types that the method proposes which are correct) and 49% type recall (the percentage of SCF types in the gold standard that the method proposes), while type precision was 81% and type recall 73% for learner 2.",
        "F-measure3 was thus 61 for learner 1 and 76 for learner 2.",
        "While this evaluation allows us to evaluate the set of acquired SCF types, similarity-based evaluation is needed to evaluate the frequen",
        "mative smoothing employed by learner 2 as part of the method for SCF acquisition.",
        "When we discuss the effect of smoothing on similarity measures, we essentially mean the uninformative smoothing performed on the output of learner 2 for evaluation purposes, not the informative smoothing performed for SCF acquisition.",
        "plement frame for believe (I believe that our theory is correct) but incorrectly assign this frequently occurring frame a very small probability."
      ]
    },
    {
      "heading": "3 Similarity Measures",
      "text": [
        "We used the following set of similarity measures to evaluate the accuracy of a learned SCF distribution q = {q2} with respect to a gold standard distribution p = {p2}.",
        "q2 and pi denote the probability of scf2 in the two distributions, respectively.",
        "1.",
        "IS: The intersection measure (Lin, 1998)",
        "where supp(p) and supp(q) are the sets of SCFS with non-zero probability in p and q, and com(p, q) is the intersection of these two sets.",
        "2.",
        "RC: The Spearman rank correlation coefficient (Spearman, 1904).",
        "It involves (i) calculating the ranks rp and rq for each of the SCF variables separately, using averaged ranks for tied values, and (ii) finding RC by calculating the Pearson correlation coefficient for the ranks: RC(p, q) = corr(rp, rq).",
        "RC lies in the range [-1, 1], with values near 0 denoting a low degree of association and values near 1 and 1 denoting strong association.",
        "3.",
        "CE: Cross entropy – a measure of the in",
        "formation needed to describe a true distribution p using a model distribution q:",
        "CE is minimal when p and q are identical.",
        "In this case CE(p, q) = H(p) is the Shannon entropy of p.",
        "4.",
        "KL: Kullback-Leibler distance – a measure",
        "of the additional information needed to describe p using q:",
        "5.",
        "JS: The Jensen-Shannon divergence – a",
        "measure which relies on the assumption that if p and q are similar, they are close to their average.",
        "(1999) reports the best results with a = 0.99.",
        "We adopted the same value.",
        "All these other measures, except is and RC (which are included in our experiment primarily for comparison), are entropy-based measures of distributional similarity.",
        "CE and KL are asymmetric and, unlike .is and SD, undefined if there exists a SCF for which pi > 0 but q2 = 0."
      ]
    },
    {
      "heading": "4 Smoothing",
      "text": [
        "Two different uninformative smoothing methods were selected for investigation: the add-one and Witten-Bell (Witten and Bell, 1991) methods.",
        "They both work by distributing a certain probability mass among unseen events and discounting the observed distribution accordingly, but differ in the way they estimate the discount."
      ]
    },
    {
      "heading": "4.1 Add-One",
      "text": [
        "Add-one smoothing involves assigning a uniform prior probability to all events so that q2 > 0 for all i.",
        "Let c(scf2) be the frequency of a SCF (given a verb) in q, N the total number of SCF tokens for this verb in q, and nscf the total number of SCF types considered.",
        "The estimated probability of the SCF is:"
      ]
    },
    {
      "heading": "4.2 Witten-Bell",
      "text": [
        "Witten and Bell (1991) present a set of smoothing methods which involve estimating the discount from the observed distribution.",
        "We adopt their method \"C\", the so-called Witten-Bell method.",
        "It considers each unseen SCF type as an event in addition to the N seen SCF tokens.",
        "Accordingly, the probability of an unseen SCF is"
      ]
    },
    {
      "heading": "5 Experiment",
      "text": []
    },
    {
      "heading": "5.1 Test Data and Methods",
      "text": [
        "We selected 17 test verbs for our experiments.",
        "Sentences containing an occurrence of one of these verbs were first extracted from 20 million words of the British National Corpus (BNC) (Leech, 1992), an average of 1000 citations of each, and then processed using the two subcategorization learners.",
        "This yielded two SCF distributions per test verb.",
        "The similarity measures introduced in the previous section were then applied to evaluate the accuracy of these acquired distributions (q) against gold standard distributions (p).",
        "The latter were obtained by manually analysing an average of 300 occurrences of each test verb in the BNC.",
        "Prior to calculating the similarity, we smoothed the distributions under comparison using the two uninformative methods introduced in section 4.",
        "To investigate in detail the"
      ]
    },
    {
      "heading": "effect of smoothing, we varied the number of",
      "text": [
        "SCFs considered, i.e. the number of SCFs which enter into the similarity measurement.",
        "This allowed us to control the number of SCFs which require smoothing.",
        "Three options were explored which involved considering only those SCFs, for which",
        "Option 1 involves considering the smallest number of SCFs - only those common for p2 and q2 - and never requires smoothing.",
        "Options 2 and 3 involve smoothing the gold standard SCFs absent in q2.",
        "Option 3 involves, in addition, smoothing the SCFs which occur in q2 but are absent in p2.",
        "Option 2 is perhaps the most conventional one when similarity measures are used in evaluation.",
        "It involves evaluating the gold standard events only.",
        "Option 3 takes into account the false positive events in q2 as well.",
        "It involves assigning them a very small probability in p27 accounting for the fact that these events are extremely unlikely to appear in the gold standard."
      ]
    },
    {
      "heading": "5.2 Results",
      "text": [
        "different similarity measures and options 1-3.",
        "Results are reported separately for add-one and Witten-Bell smoothing methods.",
        "Table 1 shows the average results for the 17 verbs with each similarity measure and smoothing option (the options 1-3) combination.",
        "According to all results reported, learner 2 (L2) is more accurate than learner 1 (L1).",
        "The results with is with option 2 indicate that learner 2 is good in detecting SCFs, finding 92% of the gold standard SCFs, while learner 1 finds only 77% of the SCFs5.",
        "Thus the number of SCFS smoothed is always higher for learner 1 than for learner 2, and therefore we expect the effect of smoothing to be always stronger for learner 1.",
        "With add-one smoothing, all the entropy-based similarity measures (CE, KL, JS, and SD) show worse results when the number of SCFs smoothed increases.",
        "Thus option 1 yields the best results and option 3 the worst.",
        "The effect of smoothing is indeed always stronger for learner 1 than learner 2.",
        "Interestingly, it also varies largely from one entropy-based measure to another.",
        "KL and CE prove the measures most sensitive to add-one smoothing.",
        "When we consider the results for learner 1 and observe the decline in results from option 1 to option 3, KL worsens by a factor of 3.1.",
        "CE proves nearly as sensitive.",
        "SD worsens by a factor of 2 and JS by a factor of 1.8.",
        "From the entropy-based measures, JS is thus the one most resistant to the effect of add-one smoothing.",
        "It shows results consistent with those obtained using RC.",
        "Similar observations regarding the robustness of the measures can be made with the more accurate learner 2.",
        "With Witten-Bell method, option 1 yields the best results as well.",
        "The results for option 2 are not, however, considerably better than those for option 3.",
        "In fact with learner 1, one measure – xL – shows the best results with option 3.",
        "This happens because Witten-Bell smoothing tends to assign a higher probability to unseen SCFS than add-one smoothing.",
        "Whenever an unseen SCF has a high probability in the distribution where it is seen, Witten-Bell method makes a better guess.",
        "This affects the distributional similarity favourably.",
        "The converse happens whenever an unseen SCF has a low probability.",
        "Although most unseen SCFS have a low probability, the few high probability SCFS have more effect, as entropy-based measures give them more weight.",
        "Hence the small differences in results between options 1-3.",
        "Whether or not this indicates that Witten-Bell method is more suitable for our task than add-one method is difficult to judge.",
        "However, the results do show that KL is the measure most SWith option 2, the fraction of detected frames is IS 2-IS' effected by Witten-Bell smoothing.",
        "Overall, the more sensitive measures (KL and CE) behave similarly with the more robust measures (SD and JS) when option 1 is used.",
        "This seems to suggest that from all options, option 1 is the most suitable for these measures when a learned distribution is sparse and a high number of SCFS require smoothing (this is frequently the case with learner 1).",
        "Although considering only the SCFS for which pi > 0 and qi > 0 means ignoring a number of events in pi, it avoids the noise from smoothing and yields more reliable results.",
        "Whether or not the latter unconventional use of similarity measures is generally applicable is a matter which requires further investigation.",
        "We conducted our experiments by comparing distributions that share, on average, around half of their SCFs.",
        "Further research is required to investigate the effect of smoothing on distributions that share fewer of their SCFs."
      ]
    },
    {
      "heading": "6 Discussion",
      "text": [
        "In our experiment, entropy-based similarity measures which require qi > 0 (KL and CE) proved more sensitive to the noise from uninformative smoothing, while those which do not require qi > 0 (SD and is) proved more robust.",
        "Interestingly, Carroll and Rooth (1998) made similar observations when using CE in evaluation of their subcategorization learner.",
        "They noted that uninformative smoothing (using the Poisson model (Witten and Bell, 1991)) introduces a high penalty on CE.",
        "They did not investigate other similarity measures or smoothing methods.",
        "Lee (1999) who compared the performance of a variety of similarity measures on a pair co-occurrence task, also reported best results with measures which concentrate effort on events for which both probability estimates are non-zero.",
        "She only considered two entropy-based measures – JS and SD – from which SD proved more accurate.",
        "Lee (2001) reported better results with SD than KL, even when highly sophisticated (informative) methods were used for smoothing.",
        "We restricted our investigation to entropy-based similarity measures.",
        "In the future, it would be interesting to examine the effect of smoothing on commonly used non-entropy based similarity measures as well.",
        "For example, L1 (Manhattan) norm and Jaccard's coefficient seem promising candidates on the basis of Lee's (1999) evaluation.",
        "In the future, we also plan to investigate other uninformative smoothing methods (e.g. Poisson and Good-Turing (Good, 1953)), and carry out experiments with a wider range of learners with varying degree of accuracy.",
        "Our experiments with the two learners substantially different in their accuracy were inadequate to establish whether the noise from smoothing can actually obscure inter-system comparison.",
        "In this paper, we have examined the use of similarity measures in evaluation of (particular type of) language learners.",
        "The results reported suggest that it is best to either evaluate these language learners using similarity measures known to be resistant to noise from smoothing, or possibly employ measures such as xL and CE in an unconventional way, without smoothing: by considering only the non-zero gold standard events in learned distributions.",
        "The SCF distributions we experimented with are typical zipf like ones, which we encounter frequently in natural language.",
        "Also, the topic we have investigated is not specific to evaluation: similarity measures are frequently applied to smoothed estimates in other domains as well.",
        "Therefore, our observations are likely to be of an interest to the range of NLP tasks which – one way or the other – involve measuring distributional similarity."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "Uninformative smoothing is frequently performed before the accuracy of automatically acquired scF distributions is evaluated using measures of distributional similarity.",
        "Smoothing allows the comparison of unseen events but adds noise to the original data.",
        "In this paper, we investigated the effect of uninformative smoothing on entropy-based similarity measures.",
        "We applied these measures to evaluate the accuracy of two sub categorization learners, and studied the effect of smoothing by controlling the number of scFS considered.",
        "We observed variation in the robustness of the different measures.",
        "Some measures proved highly sensitive to the noise from smoothing, while others proved more robust and thus preferable for evaluation purposes."
      ]
    },
    {
      "heading": "8 Acknowledgements",
      "text": [
        "We thank Ido Dagan and Diana McCarthy for useful comments on this paper.",
        "This work was partly supported by UK EPSRC project GR/N36462/93: `Robust Accurate Statistical Parsing (RASP)'."
      ]
    }
  ]
}
