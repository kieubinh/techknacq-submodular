{
  "info": {
    "authors": [
      "Wei-Hao Lin",
      "Hsin-Hsi Chen"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W02-2017",
    "title": "Backward Machine Transliteration by Learning Phonetic Similarity",
    "url": "https://aclweb.org/anthology/W02-2017",
    "year": 2002
  },
  "references": [
    "acl-C96-1039",
    "acl-J98-4003",
    "acl-P94-1010",
    "acl-P98-1036",
    "acl-P98-1069",
    "acl-P98-2220",
    "acl-W97-0315",
    "acl-W98-1005"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In many cross-lingual applications we need to convert a transliterated word into its original word.",
        "In this paper, we present a similarity-based framework to model the task of backward transliteration, and provide a learning algorithm to automatically acquire phonetic similarities from a corpus.",
        "The learning algorithm is based on Widdrooww-Hoff rule with some modifications.",
        "The experiment results show that the learning algorithm converges quickly, and the method using acquired phonetic similarities remarkably outperforms previous methods using predefined phonetic similarities or graphic similarities in a corpus of 1574 pairs of English names and transliterated Chinese names.",
        "The learning algorithm does not assume any underlying phonological structures or rules, and can be extended to other language pairs once a training corpus and a pronouncing dictionary are available."
      ]
    },
    {
      "heading": "1 Inttroduucttion",
      "text": [
        "As multilingual ddoocuumeentts increase rapidly on the Internet, the need to bridge the language barrier is highly demanded.",
        "Cross-language information retrieval (CLLR) (Chen, 1997) aims to retrieve documents in one language given queries in the other language, and proper nouns processing plays an important role in the query translation (Bian and Chen, 2000; Oard, 1999).",
        "The study (Thompson and Dozier, 1997) showed that large proportion of queries to news search engines contain proper nouns.",
        "Therefore, any CLIR systems must handle proper nouns transliteration approximately to achieve better performance.",
        "Transliteration can be classified into two directions.",
        "Given a pair (s, t) where s is the original proper noun in the source language and t is the transliterated word in the target language, forward transliteration is the process of phonetically convert s into t, and backward transliteration is the process of correctly find or generates given t. Examples of both types are shown in Table 1.",
        "Two directions and different language pairs have been exploered in previous works : forward transliteration from English to Chinese (Wan and Verspoor, 1998), backward transliteration from Japanese to English (Knight and Graehl, 1998), from Chinese to English (Chen et al., 1998; Lin and Chen, 2000), and from Arabic to English (Stalls and Knight, 1998).",
        "Backward transliteration is more challenging than forward transliteration.",
        "While forward transliteration can accomplish the mapping relationship throwgh tabble-lookup, backward transliteration is required to disambuiguate the noise produced in the forward transliteration and estimate the original word as close as possible.",
        "We mainly focus on backward transliteration here.",
        "In this paper, we propose a similarity-based framework to model the task of backward",
        "trannslliteratioonn.",
        "When human beings perform forward transliteration, the goal is to keep the original word and the transliterated word as close as possible in terms of the phonetic similarities.",
        "We base on the same idea to model backward transliteration.",
        "Compared with the generative models in previous studies (Knight and Graehl, 1998; Stalls and Knight, 1998), the similaarittyy-based framework directly addresses the problem of similarity measurement, and can be evaluated without human judgments.",
        "Similarity-based approaches have been tested in the grapheme level (Chen et al., 1998) and the phoneme level (Lin and Chen, 2000).",
        "The similarities in previous works, however, were ad hoc assiggned.",
        "In this paper, we address the problem by developing a learning algorithm to automatically acquire phonetic siimmilarities from a training corpus.",
        "With the learning algorithm, we can remove the labor of assigning phonetic similarities between two languages, and hopefully the performance will improve with learned siimmilarities.",
        "This paper is organized as follows.",
        "In Section 2, we describe the similarity-based framework and define the similarity between two words.",
        "The learning algorithm and training corpus preparation are in Section 3.",
        "Experiment design and results are in Section 4.",
        "Finally comes discussions and conclusions."
      ]
    },
    {
      "heading": "2 Similarity-based Framework",
      "text": [
        "In the similarity-based framework, given a transliterated word t, we compare t with a list of candidate words, and the one with the highest similarity will be chosen as the original word.",
        "The candidate lists can be collected manually by newspaper editors or automatically by name entity extraction systems (Fung and Yee, 1998).",
        "In CLLR applications, after foreign words in the queries are identified (Sproat et al., 1994; Chen and Lee, 1996), we perform the mate-matching process on these words as one step of query translation.",
        "The working flow is illustrated in",
        "measurement.",
        "We can mmeasuure similarities at three different levels, including physical sounds, graphemes, and phonemes.",
        "Soundex (Knuth, 1973), for example, measures similarities at the grapheme level.",
        "Here we choose phonemes because it is difficult to generate and compare physical sounds, and comparing at the phoneme level has been shown to outperform the grapheme level (Lin and Chen, 2000).",
        "Specifically, the phoneme representation we adopt here is the International",
        "traannsliteraatioonn"
      ]
    },
    {
      "heading": "2.1 Grapheme-to-Phoneme Transfformmattion",
      "text": [
        "For Chinese transliterated words, each character is first represented in Hanyu Pinyin by looking up a table, while the tone is ignored.",
        "The Hanyu Pinyin strings are then decomposed into two parts: the initial consonant and the remaining vowel.",
        "Each part is then mapped into IPA by looking up anootheer table (Hieronyms, 1997).",
        "English words require more efforts to be represented in IPA.",
        "First, if the word entry exists in the pronouncing dictionary (Cmudict, 1995), the pronunciation is taken and transformed to IPA.",
        "If the dictionary does not cover the word, we apply a speech synthesis system, MBRDICO (Pagel et al., 1998), to generate the pronunciation.",
        "Although speech synthesis for proper nouns is still a ongoing research problem (Llitjos and Black, 2001), instead of dropping them we prefer to keep those words that are not covered in the dictionary, and investigate the effect of imperfect speech synthesis in the task of backward trannslliteratioonn.",
        "The letter-ttoo-phoneme system will output pronunciations in SAMPA (Wells, 1997), which in turn are mapped to IPA.",
        "The duration information is not used."
      ]
    },
    {
      "heading": "2.2 Similarity Measurement",
      "text": [
        "The edit distance (Levenshtein, 1966) is widely used as relatedness measurement between two strings.",
        "The distance is defined as the minimum number of insertions, deletions, and substitutions required to transform one string into the other.",
        "The following similarity definition is equal to the edit distance with variable costs on insertion, deletion, and substitution, but the definition is more suitable than the edit distance foor some applications, for example, finding the substrings with high similarity (Gusfield, 1997).",
        "We first define the alignment of two strings upon which the similarity is measured, Definition 1 Set E is the alphabet set of two strings S1 and S2.",
        "-’ = {E; ‘ ’}, where ‘ ’stands for space.",
        "Space could be inserted into S1 and S2 such that they are of equal length and denoted as S1’ and S2’.",
        "S1’ and S2’ are aligned when every character in either string is opposite a unique character or space in the other string.",
        "The configuration of two aligned strings is denoted as A.",
        "The similarity score of two alignments is defined as follows, Definition 2 s(a,b) is a function which measures similarity between the character a and b in E’.",
        "Given an alignment A of two strings S1j’ and Sz’ with the same length 1, the similarity score of two alignments is defined as follows,",
        "where S(i) denotes the irh character in the string"
      ]
    },
    {
      "heading": "S’.",
      "text": [
        "Take a pair of a English name and its transliterated Chinese name, (Huugo, Yuu3-guuo3) as an example.",
        "After applying the grapheme-to-phoneme procedure described in the above section, we obtain the phoneme pair (v k uo, h j u g oU) 1.",
        "Here, E’ = {h, j , u, v, g, k, oU, uo, – I.",
        "There are many 1 All phonemes in this paper are represented in SAMPA, which can reppresennt IPA in ASCII.",
        "ways to align these two phoneme strings, two of which are shown in Table 2.",
        "The similarity function s(a,b) can be conveniently represented as a scoring matrix inn",
        "manually assigned or automatically learned.",
        "The score ranges froom 10 and –10.",
        "The higher the score is, the more similar two phonemes are.",
        "s(aa,bb) h j u v g k oU uo _ h 10 0 8 0 0 9 0 4 -10 j 0 10 1 0 0 1 0 1 3 u 0 0 10 3 0 4 0 2 -10 v 0 0 6 9 0 6 0 5 -10 g 0 0 -10 0 10 10 0 7 -10 k 0 0 -10 1 0 10 0 -10 -10 oU 0 0 2 4 0 4 10 10 -10 uo 0 0 0 0 0 0 0 10 -10 _ -10 -10 -10 5 -10 -10 -10 -10",
        "With Equation 1 and the scoring matrix in Figguure 2, we can then calculate the similarity score of two alignments in Table 2 as follows,",
        "Finally, the similarity score of two strings is defined as follows, Definition 3 Given an alphabet set E’ and a similarity scoring matrix M, the similarity score of two strings is the score of the optimal alignment, i.e. the alignment with the highest scooree.",
        "The optimal alignment of two strings can be computed efficiently using the technique called dynamic programming (Masek, 1980).",
        "Set Tis a n+l by m+l table where n is the length S1, m is the length of S2.",
        "By filling the table Trow by row, we can obtain the optimal alignment and the similarity score of S1 and S2.",
        "The base condition is defined as follows,",
        "where 1 < i < nn, 1 < jî < mm.",
        "If we speak in the language of the edit distance, the recurrence formula attempts to compare the coostts of substitution, deletion, and insertion and chooses the one with the minimum cost, i.e. the maximum similarity here.",
        "The table can be complete in the time complexity of O(nm), and T(n,m) will be the similarity score of the optimal alignment of S1 and S2.",
        "The optimal alignment can be obtained by bookkeeping the choice made in the recurrence formula.",
        "For example, given the scoring matrix in Figure 2, the optimal alighment of two phoneme strings S1 (j h u g",
        "oU) and S2 (v k uo) is A1 in Table 2.",
        "3 Learning Phonetic Similarity",
        "The design of the scoring matrix plays an important role in differeentiating which alignment is better than the other (Gusfield, 1997).",
        "The score reflects how humans perceive phonemes in the task of backward transliteration.",
        "The motivation to develop a learning algorithm is to remove the efforts of assigning scores in the matrix, and to capture the subtle difference that is not easy to be quantified by humans.",
        "Edit distance learning has been studied in a probability framework (Ristad and Yianilos, 1998).",
        "While the phonetic similarities can be represented and learned in the probabilistic model, a learning algorithm that can directly work on the aforementioned similarity-based framework and discriminate between phoneme strings will be more preferable.",
        "In this section, we first describe how to prepare a training corpus, followed by the learning algorithm."
      ]
    },
    {
      "heading": "3.1 Training Corpus Preparation",
      "text": [
        "In order to train a discriminative classifier, we have to prepare both positive examples and negative examples.",
        "However, a corpus with pairs of the original words and the transliterated words are positive examples only.",
        "Fortunately, we can generate negative examples by mismatching the original words and the transliterated words w❒ithout collecting more data.",
        "Consider a corpus with nn pairs of the phoneme strings (ei, ci), where ei is the original English and ci is its transliterated Chinese word, I < i <_ n. For each ci„ there exists the most similar transliterated word, i.e. ei, and n-1 other dissimilar transliterated words, i.e. ejè, where 1 <jî < n, jî # i.",
        "The similarity score of each pair is initialised as follows,",
        "Consequently, a corpus with nn pairs can generate n positive examples, and n(n-1) negative examples.",
        "To account for the discrepancy in the number of positive and negative samples, we duplicate the positive examples such that there are total 2n2 examples."
      ]
    },
    {
      "heading": "3.2 Learning Algorithm",
      "text": [
        "If we treat each training sample as a linear equation, Equation 1 can be rewritten as follows,",
        "where mm is the size of the phoneme sets, wiJ is the row i and the column jâ of the scoring matrix, xiJ is",
        "and thus m = 9.",
        "Each cell in Figure 2 corresponds to wi j, where l < i,j <– 9.",
        "The xñ1,9, x2,9, x3,4, x5,6, x7,8 for the alignment A1 in Table 2 are one, other xñiJ are zero.",
        "Furthermore, the system of linear equations in the corpus can be conveniently represented in the matrix form,",
        "where the superscript i stands for the iih sample pair in the corpus, I < – i < – R, R is the number of pairs in the corpus.",
        "The criterion we choose to optimize is the sum-of-squared error, i.e. IIXw - y||2.",
        "Therefore, the goal of the learning task will be to learn w of (2❚) Equation 6 such that the sum-of-quared errors are minimized.",
        "The classical solution is to take the pseudo inverse of X, i.e. X† = (VX)-1V, to obtain the w that minimizes the sum-of-squared error, i.e. w➺ = X†y.",
        "However, the pseudo inverse is an expensive computation when X is a large matrix",
        "cannot be ✂computed when VX is singular.",
        "Therefore, we adopt the Widrow-Hoff rule (Duda et al., 2001) to avoid these problems.",
        "The Widrow-Hoff, or Least-Mean-Squared (LMS) rule minimizes the error in gradient descent fashion.",
        "The pseudo code of the learning algorithm is listed in Figure 3, where the subscript k stands for the k* row in the matrix X, i for the number of iterations, w(i), η(i), and δ(i)) are functions of i, and η is the learning rate.",
        "soome modifications The ww(i) is updated iteratively until the learned ww appears to overfit on the training set.",
        "The learning rate η(i) decreases with the number of the iterations to ensure the w will converge to a vector satisfying V(Xw - y) = 0.",
        "In addition to the Widdrooww-Hoff rule, we apply the on-line learning technique (Biehl and Riegler, 1994) to speed up the convergence.",
        "We update w(i) immediately after encountering a new training sample instead of accumulating all errors of training samples.",
        "The other speedd-up technique is the momentum used to damp the oscillations.",
        "The α is the momentum coefficient The η(0) is empirically set as 5e10-6, α as 0.8, and w(0) as foolloowws, wi i (0) ,0 10 if i = j −10 if i or jú is ' otherwise Here we assume phonemes are self-similar, and discourages phonemes to be matches with the space character.",
        "Other phonetic similarities are initialized to zero, which is a reasonable initial values without any prior knowledge.",
        "In order to avoid overfitting the corpus and lose the power of generalization, we evaluate the learned w on the held-out validation set after a full iterations of the training set.",
        "If the performace does not improve three iterations in a row, we stop the gradient descent precedure and return w with the best performance so far."
      ]
    },
    {
      "heading": "4 Expperimentts",
      "text": [
        "In order to compare the learning approach with previous works Chen98 (Chen et al., 1998) and Lin00 (Lin and Chen, 2000), the same corpus is addooppted here.",
        "The corpus is consisted of 1574 pairs of English names and their transliterated Chinese naamees, 313 of which havve no entries in the pronouncing dictionary.",
        "There are total 97 phonemes used to represent these names, in which 59 and 51 phonemes are used foor Chinese and English names, respectively.",
        "To evaluate the performance of learned similarities, we conduct a tenfold cross validation on the corpus.",
        "In each fold, the corpus is divided into three sets: 8/10 is the training set, 1/10 is the validation set, and remaining 1/10 is the test set.",
        "The training set is used to generate positive and negative examples.",
        "The validation set prevents the learner from overfitting the training set.",
        "The test set that the learner has never seen is used to evaluate the performance.",
        "The average ww across ten folds is returned as the final result.",
        "The performance metrics aree the average raank and the average reciprocal rank.",
        "The rank is the position of the correct original word in a list of candidate words sorted descendently by similarity scores.",
        "The smaller the average rank, the better the performance.",
        "The other metric is the average reciprocal rank (ARR) (Voorhees and Tice, 2000), which evaluates same characteristics as the average rank but puts more stress on top ranks.",
        "The recirprocal rank is calculated as follows,",
        "where R((i)) is the rank of the ith trianing sample, M is the number of training samples.",
        "The value of ARR is between 0 and 1.",
        "The higher the ARR, the better the performance The learning curve of one fold is shown in Figure",
        "algorithm converges quickly.",
        "The method using learned phonetic similarities is compared with previous works, and the results are shown in Table 3.",
        "Both average rank and average reciprocal rank suggest that backward transliteration using learned phonetic similarities remarkably outperforms previous methods using predefined similarities, either at the grapheme the or the phoneme level."
      ]
    },
    {
      "heading": "5 Discuussions",
      "text": [
        "Backward transliteration is of particular interest in machine transliteration.",
        "In this paper backward transliteration is discussed in a siimmilarityy-based framework, and a learning algorithm is developed to automatically acquire phonetic similarities from a training corpus.",
        "The experiment results suggest that the learning algorithm can effectively extract the similarities, and learned similarities are more discriminative than manually assigned scoring matrix.",
        "Since neither underlying phonological structures are assumed nor alignments must be manually labelled, the efforts of extending the learning algorithm to other language pairs should be minimal once the training corpus and the pronouncing dictionary are available.",
        "However, not pronouncing dictionaries for all languages are readily available, and speech synthesis has difficulties generating pronunciations of proper nouns.",
        "Notice that about 115 of the training corpus have no entries in the pronouncing dictionary, but the learning approach appears to able to tolerate the noise from speech synthesis, and still outperforms previous method that totally ignore those pairs of no pronunciations.",
        "One of future direction will move toward getting pronunciations without dictionaries, and this is in line with the direction of speech synthesis research (Llitjos and Black, 2001).",
        "The learning algorithm can capture subtle similarities that cannot easily be manually assigned based on phonological knowledge.",
        "Take the matrix in Figure 2 as an example2.",
        "The vowel pairs (oU, u) and (oU, uo) have positive score 2 and 10, which means they are similar but in different degree.",
        "However, they are equally assiiggnnedd the score 5 in the previous study.",
        "The learning algorithm assigns the consonant pair (g, k) the positive score 10, which was assigned the score 8 based on the phonological knowledge that there is no distinction of voiced and voiceless consonants in Chinese.",
        "Without any phonological analysis, the learning algorithm can acquire those similarities without human inttervventtioon.",
        "2 The matirx in Figure 2 is actually part of the whole learned scoring matrix, which is 98 by 98, and too large to be listed here."
      ]
    }
  ]
}
