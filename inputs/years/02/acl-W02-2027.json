{
  "info": {
    "authors": [
      "S. H. Srinivasan"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W02-2027",
    "title": "Features for Unsupervised Document Classification",
    "url": "https://aclweb.org/anthology/W02-2027",
    "year": 2002
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Unsupervised document classification is an important problem in practical text mining since training data is seldom available.",
        "In this paper we study the problem of term selection and the performance of various features for unsupervised text classification.",
        "The features studied are: principal components, independent components, and non-negative components.",
        "The clustering algorithm used is based on bipartite graph partitioning (Zha et al., 2001).",
        "The evaluation is performed using the newsgroups corpus."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many natural language processing applications require text classification.",
        "Labeled data for training is typically unavailable in many situations.",
        "So unsupervised classification techniques are called for.",
        "Classification – both supervised and unsupervised – is usually done in two steps: feature extraction and classification.",
        "Feature extraction is basically a change of representation of the raw data.",
        "The most common representation used in document retrieval is the vector representation or the \"bag-of-words\" feature representation (Salton, 1989).",
        "Each document is represented as a vector in the term space.",
        "Let tl, t2, • • • , t,,, be the terms we use to represent the docu-ments.'",
        "This collection usually ignores very high and very low frequency terms.",
        "The terms can be arranged in some convenient order.",
        "The documents are represented as vectors of dimension n. Let v be the vector corresponding to a document d. We set vi to 1 if term ti occurs in d. Otherwise vi is set to 0.",
        "This representation uses information about presence or ab'Terms are obtained from words after stemming.",
        "sence of terms in the document.",
        "All the other information is ignored.",
        "Some representations use frequency information.",
        "The vectors corresponding to several documents can be arranged in a matrix – the so called \"term-document\" matrix.",
        "See (Berry et al., 1999) for a review of vector space representation.",
        "Other representation schemes can be in the vector space framework.",
        "Classification is done in the feature space.",
        "Several techniques exist for classification – naive bayes, support vector machines, k-means clustering, neural networks etc.",
        "See (Sebastiani, 2002) for a comprehensive survey.",
        "In this paper, we use the bipartite graph matching technique proposed in (Zha et al., 2001) for classification.",
        "See also (Shi and Malik, 1997).",
        "Bipartite graph matching (also known as spectral clustering) has several attractive properties including the property that if the data has a \"good\" clustering, the algorithm will find an \"optimal\" one.",
        "See (Kannan et al., 2000) for definitions and details.",
        "The term-by-document matrix can be considered as a representation of a weighted bipartite graph – the vertex sets being terms (T) and documents (D).",
        "The clustering technique partitions the vertices into disjoint sets (T = T1UT1 and D = D1 UDl) such that intra cluster weight (between Tl & Dl and Ti & Dl) is maximized and inter cluster weight (between Tl & Di and Tl & Dl) is minimized.",
        "The overall classification scheme is shown in figure 1.",
        "It can be seen that the final clustering scheme depends on the connectivity and weights of the bipartite graph.",
        "Any scheme which changes the connectivity or weights is likely to change the clustering results.",
        "In this paper we explore the following aspects.",
        "term selection: The terms used for representing the documents are crucial to the success of the classification scheme.",
        "Several techniques exist for term selection: document frequency, information gain, etc (Se-bastiani, 2002).",
        "In this paper, we use an iterative technique in conjunction with information gain.",
        "basis selection: Terms form the basis of the original representation.",
        "These bases may not be optimal for the classification task.",
        "We explore the following representation schemes: singular vectors, independent vectors, and non-negative parts.",
        "While these techniques have been applied to document processing (for example, (Dumais et al., 1988), (Kolenda and Hansen, 2000), (Lee and Seung, 1999)), there is no systematic comparison of these techniques for unsupervised document classification.",
        "The same clustering algorithm – bipartite graph partitioning – is used in the final stage.",
        "The paper is organized as follows.",
        "Section 2 introduces the representation schemes used, section 3 describes the bipartite graph partitioning, and section 4 lists the experimental setting and results."
      ]
    },
    {
      "heading": "2 Feature representation schemes",
      "text": [
        "The term-by-document matrix is a representation of the documents using one type of basis: terms.",
        "It is possible to extract other underlying bases which may aid classification.",
        "For example, matrix diagonalization (Strang, 1980) produces an orthogonal collection of basis vectors that are ordered according to their \"importance\".",
        "In this section, we provide a brief description of singular value decomposition, independent component analysis, and non-negative matrix factorization.",
        "In the following discussion, let A be the m x n document-by-term matrix.",
        "Here m is the number of documents and n is the number of words (or terms) used in the representation."
      ]
    },
    {
      "heading": "2.1 Singular value decomposition",
      "text": [
        "Matrix diagonalization can be performed only on square matrices.",
        "In contrast, singular value decomposition (SVD) exists for rectangular matrices also.",
        "SVD is a generalization of matrix diagonalization.",
        "The SVD of A can be written as",
        "where U and V are m x m and n x n orthogonal matrices and E is a m x n diagonal matrix (Strang, 1980).",
        "The (non-zero) diagonal entries of E are called singular values.",
        "The columns of U corresponding to the singular values form a basis for the column space of A and those of V form a basis for the row space of A.",
        "Since the document vectors lie in the row space, the columns of V form a basis for the document vectors.",
        "The use of SVD in document retrieval is known as latent semantic analysis (Dumais et al., 1988).",
        "LSA has been applied for the resolution of synonymy and polysemy in document retrieval.",
        "In this paper, we take the columns of V as the basis for the document space.",
        "We represent the document vectors in terms of these and then perform the clustering.",
        "In other words, the matrix VAT is subjected to bipartite matching.",
        "Since this matrix can have negative entries, we use the absolute values."
      ]
    },
    {
      "heading": "2.2 Independent component analysis",
      "text": [
        "Independent Component Analysis (ICA) is a generalization of stochastic interpretation of matrix diagonalization.",
        "Diagonalization, when applied to stochastic vectors, produces mutually uncorrelated basis.",
        "ICA produces a statistically independent basis.",
        "Thus the independent components of a matrix A can be thought of a collection of statistically independent sources for the rows (or columns) of A (Lee et al., 1998).",
        "The decomposition reveals the sources as well as mixing coefficients.",
        "The m x n matrix A is decomposed as"
      ]
    },
    {
      "heading": "A=WS+N",
      "text": [
        "where S is the r x n source signal matrix, W is the m x r mixing matrix, and N is the matrix of noise signals.",
        "Here r is the number of independent sources.",
        "The above decomposition can be performed for any number of independent components and the sizes of W and S vary accordingly.",
        "We subject the matrix SAT to clustering.",
        "(Absolute values are used, as before.)",
        "In this case, the documents are represented in terms of r independent components instead of words.",
        "We use the Fast ICA algorithm for performing the decomposition (Hyvarinen, 1999).",
        "ICA has been used for dimensionality reduction and representation of word histograms (Kolenda and Hansen, 2000)."
      ]
    },
    {
      "heading": "2.3 Non-negative matrix factorization",
      "text": [
        "In the above matrix factorizations, the components or basis vectors can have negative entries.",
        "The term-by-document matrix itself does not have negative entries.",
        "Thus negative factors are difficult to interpret.",
        "In particular, we cannot interpret them as parts of objects like documents.",
        "Non-negative matrix factorization (NMF) attempts a factorization in which the components have non-negative entries.",
        "The NMF of A is given by"
      ]
    },
    {
      "heading": "A=WH",
      "text": [
        "where the factors W and H contain only non-negative entries.",
        "The interpretation in (Lee and Seung, 1999) for the above decomposition is as follows: The columns of the m x n matrix A are the signals, the columns of the m x r matrix W are the basis signals, and the r x n matrix H is the mixing matrix.",
        "(Here r is the number of parts or non-negative components.)",
        "If we use the columns of W as the new basis, the new bipartite graph can be represented as WT A.",
        "[In our case, the the signals of interest are documents and documents form the rows of A.",
        "Hence we subject B = AT to factorization.",
        "Let B = W H. The matrix used for clustering is WT B = WT AT .]",
        "Non-negative matrix factorization has been shown to discover semantic features (Lee and Seung, 1999)."
      ]
    },
    {
      "heading": "3 Bipartite graph partitioning",
      "text": [
        "Consider a weighted bipartite graph such as the term-document graph.",
        "Let the vertex sets be X and Y and the weight matrix be W. A partition of the graph into two subgraphs can be represented by a vertex partition of X and Y.",
        "Let A C_ X and B C_ Y.",
        "Then the partition is given by the subgraphs induced by A & B and A' & Be.",
        "The cut between A and B is defined as cut (A, B) = W (A, Be) + W (A�, B) where",
        "The problem of finding partition with minimum Ncut can be posed as an eigenvalue problem (Shi and Malik, 1997) (Zha et al., 2001).",
        "The following is the algorithm derived in (Zha et al., 2001).",
        "Let e be a vector of all is (of appropriate dimension) .",
        "1.",
        "Compute diagonal matrices DX and Dy as We = DXe, WTe = Dye 2.",
        "Let W = D_1/2WD_1/2 X 3.",
        "Compute the second largest left and right singular vectors of W, ;� and �.",
        "4.",
        "Find cut points cx and cy for x = DX 1/2 � � and y = Dy1/2� 5.",
        "Let A={i:xi>cx},Ac={i:xi<cx},"
      ]
    },
    {
      "heading": "B={j:yj>cy}, and Be={j:yj<cy},",
      "text": [
        "The graphs G(A, B) and G(A�, Be) can be further partitioned."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "To test the effectiveness of different feature representations, we use the scheme of (Zha et al., Ncut (A, B) 2001).",
        "The task used is binary clustering - discrimination between two news groups.",
        "The corpus used is the 20 newsgroups database.",
        "The newsgroups and the associated labeling scheme of (Zha et al., 2001) are listed below:",
        "NG18 & NG19.",
        "It can be seen that NG1 and NG2 are well-separated, NG10 and NG11 have some overlap, and NG18 and NG19 have more overlap.",
        "To compare with the results of (Zha et al., 2001), we perform clustering for four different sample sizes: 50, 100, 150, 200 and 250 files for the second class.",
        "The first class always has 50 files.",
        "Each run of the experiment was performed as follows.",
        "1.",
        "The required number of files were chosen randomly from each class.",
        "2.",
        "The documents were represented using N terms.",
        "(In the experiments reported below, N = 200.)",
        "The terms were chosen according to maximum information gain criterion.",
        "The information gain between terms and documents is defined as IG(t, d) = E E p(t, d) log( p(t, d) l d t \\p(t)p(d) (1) where t is term and d is document.",
        "This was done using the bow toolkit.4 3.",
        "The documents were represented using the N terms.",
        "We call this representation in term basis.",
        "4.",
        "The document vectors were also represented using other bases - singular vectors, independent components, and non-negative components.",
        "5.",
        "The resulting matrices were subjected to bipartite graph partitioning.",
        "The best partitions were calculated by choosing c, and cy which minimize the objective function (Shi and Malik, 1997).",
        "The results of the experiments are shown in table 1.",
        "The results of (Zha et al., 2001) are shown in table 2 for comparison.",
        "The differences in results can be attributed to",
        "1.",
        "Zha et al.",
        "(2001) use maximum mutual information to choose terms.",
        "2.",
        "While we choose 200 terms in all experiments, number of terms used by Zha et al.",
        "(2001) is not available.",
        "It can be seen that the performance confirms to our initial expectation: classification accuracies are smaller when the classes are not well-separated.",
        "It is surprising that the default term-basis performs better than other derived bases.",
        "In all the derived bases, we have an extra degree of freedom: the number of components chosen.",
        "In the previous set of experiments, we used 200 independent components and 100 non-negative components.",
        "We reduced the number of independent components and non-negative components by half and the results are shown in table 3.",
        "ICA performs better in the presence of uncertainty (NG18/NG19 and unbalanced data sets for NG10/NG11)."
      ]
    },
    {
      "heading": "4.1 Term selection",
      "text": [
        "Note that when the initial terms selection is done using equation 1, the documents are not labeled.",
        "But after first round of classification, we have tentative class labels available.",
        "This can be used to refine the choice of terms.",
        "For this we use class labels instead of document labels in equation 1.",
        "The overall scheme is shown in figure 2.",
        "Figure 3 shows how the scores improve with this iterative term selection.",
        "We can use this idea to improve classification accuracy.",
        "Table 4 gives the best case results.",
        "The figures are obtained by using the knowledge of classes when choosing terms using equation 1.",
        "These can be viewed classification accuracies when best term selection is performed.",
        "Figure 4 shows the actual improvement of scores for a particular case."
      ]
    },
    {
      "heading": "5 Related work",
      "text": [
        "There have been several attempts to improve document clustering accuracy.",
        "(Moore et al., 1999) use hypergraph partitioning and show that the resulting clusters have smaller entropy compared to other clustering techniques.",
        "(Strehl et al., ) study the effect of various similarity measures and clustering algorithms on clustering web pages.",
        "They conclude that cosine and extended Jaccard similarities and weighted graph partitioning give good results.",
        "(Dhillon",
        "are reduced.",
        "The three tables correspond to NG1/NG2, NG10/NG11, and NG18/NG19.",
        "The columns of each table correspond to ICA and NMF.",
        "The rows correspond to different mixture compositions: 50/50, 50/100, 50/150, 50/200, and 50/250.",
        "It can be seen that ICA performs better than term-basis when the classes are overlapping (NG18/NG19).",
        "NG1 and an equal number of documents from NG2 are chosen.",
        "For ease visualization, scores of documents from NG1 occur before those of NG2.",
        "Terms for first iteration are chosen using equation 1.",
        "Terms for subsequent iterations are chosen using the classification labels of previous iteration in the same equation.",
        "It can be seen that the discrimination increases.",
        "and Modha, 2001) propose spherical k-means clustering and compare it to SVD.",
        "The basis produced by spherical k-means is more localized.",
        "The algorithms have been tried on a variety of datasets.",
        "(None of the above papers use the 20 newsgroups dataset.)"
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "The experiments presented here capture two common scenarios in document classification: class overlap and data insufficiency.",
        "Term-based representations perform well when there is little overlap and small amount of data.",
        "ICA-based representations perform well when there is sufficient data even for overlapping cases.",
        "A crucial parameter in case of ICA and NMF is the number of components chosen.",
        "Choosing the optimal number of components will be an interesting extension of the work reported here."
      ]
    }
  ]
}
