{
  "info": {
    "authors": [
      "Koichi Takeuchi",
      "Nigel Collier"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W02-2029",
    "title": "Use of Support Vector Machines in Extended Named Entity Recognition",
    "url": "https://aclweb.org/anthology/W02-2029",
    "year": 2002
  },
  "references": [
    "acl-A92-1021",
    "acl-A97-1011",
    "acl-A97-1028",
    "acl-A97-1029",
    "acl-C00-1030",
    "acl-J95-4004",
    "acl-W00-0730",
    "acl-W00-0904",
    "acl-W99-0613"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper explores the use of support Vector Machines (sVMs) for an extended named entity task.",
        "We investigate the identification and classification of technical terms in the molecular biology domain and contrast this to results obtained for traditional NE recognition on the MUC-6 data set.",
        "Furthermore we compare the performance of the sVM model to a standard HMM bigram model.",
        "Results show that the sVM utilizing a rich feature set of a +3 context window and POs features (MUC-6 only) had a significant performance advantage on both the MUC-6 and molecular biology data sets."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Named entity (NE) extraction is now firmly established as a core technology for understanding low level semantics of texts.",
        "NE was formalized in the DARPA-sponsored Message Understanding Conference (MUC)-6 (MUC, 1995) and since then several methodologies have been widely explored:",
        "• heuristics-based, using rules written by human experts after inspecting examples (Fukuda et al., 1998); • supervised such as (Bikel et al., 1997) using labelled training examples; • non-supervised methods such as (Collins and singer, 1999).",
        "NE's main role has been to identify expressions such as the names of people, places and organizations as well as date and time expressions.",
        "such expressions are hard to analyze using traditional natural language processing (NLP) because they belong to the open class of expressions, i.e. there is an infinite variety and new expressions are constantly being invented.",
        "The application of NE to non-news domains requires us to consider extending NE so that it can capture types, i.e. instances of conceptual classes as well as individuals.",
        "To distinguish between traditional NE and extended NE we refer to the later as NE+.",
        "There are several issues that may mean that NE+ is more challenging that NE.",
        "The most important is the number of variants of NE+ expressions due to graphical, morphological, shallow syntactic and discourse variations.",
        "For example the use of head sharing combined with embedded abbreviations in unliganded (apo)- and liganded (holo)-LBD.",
        "such expressions will require syntactic analysis beyond simple noun phrase chunking if they are to be successfully captured.",
        "NE+ expressions may also require richer contextual evidence than is needed for regular NEs - for example knowledge of the head noun or the predicate.",
        "At the ontology level there are complex issues related to granularity when deciding on which class a possible NE+ expression should be assigned to.",
        "NE+ expressions will typically belong to a much richer taxonomy than NE, opening up the possibility of combining information extraction (IE) with deep knowledge representations such as ontologies.",
        "This is an area we are currently exploring (Collier et al., 2002).",
        "Examples of NE+ classes include, a person's name, a protein name, a chemical formula or a computer product code.",
        "All of these may be valid candidates for tagging depending on whether they are contained in the ontology.",
        "NE+ can be viewed as a type of multiple classification task and there are several effective and well studied learning algorithms available for this such as Hidden Markov Models (HMMs) (Rabiner and Juang, 1986) and transformation-based error-driven learning (TBL) (Brill, 1995).",
        "Recently a new learning paradigm called support vector machines (sVMs) (Vapnik, 1995) has been the focus of intensive research in machine learning due to its capacity to learn effectively from large feature sets.",
        "sVMs have been applied very successfully in the past to several traditional classification tasks such as text classification.",
        "Promising results have been reported for NLP tasks such as part of speech tagging and chunking, e.g. (Kudoh and Matsumoto, 2000).",
        "We have implemented and compared two learning methods (sVM, HMM) and tested them on two data sets.",
        "The comparison between these models is informative because of the different nature of the two learning methods.",
        "In the case of the HMM the learning approach is generative, i.e.. it makes use of positive examples to build a model of NE classes and then evaluates each unseen sentence to see how well each of the words `fits' the model.",
        "The sVM on the other hand is a discriminative approach and makes use of both positive and negative examples to learn the distinction between the two classes.",
        "Another major difference is that the sVM outputs a measure of distance from the classification function whereas the HMM uses the Viterbi algorithm (Viterbi, 1967) to decode using maximum likelihood probabilities.",
        "Basically we expect the models to have quite different strengths and weaknesses and hopefully these can be complementary, allowing us eventually to combine the approaches to achieve a composite model.",
        "The two models are described further below along with their performance."
      ]
    },
    {
      "heading": "2 Method",
      "text": []
    },
    {
      "heading": "2.1 SVM",
      "text": [
        "We developed our method using the Tiny sVM package from NAIsT 1 which is an implementation of Vladimir Vapnik's sVM combined with an optimization algorithm (Joachims, 1999).",
        "sVMs like other inductive-learning approaches take as input a set of training examples (given as binary valued feature vectors) and finds a classification function that maps them to a class.",
        "There are several points about sVM models that are worth summarizing here.",
        "The first is that sVMs are known to robustly handle large feature sets and to develop models that maximize their generalizability.",
        "This makes them an ideal model for the NE+ task.",
        "Generalizability in sVMs is based on statistical learning theory and the observation that it is useful sometimes to misclassify some of the training data so that the margin between other training points is maximized (fortes and Vapnik, 1995).",
        "This is particularly useful for real world data sets that often contain inseparable data points.",
        "secondly, although training is generally slow, the resulting model is usually small and runs quickly as only the support vectors need to be retained, i.e.. the patterns that help define the function which separates positive from negative examples.",
        "Thirdly is that sVMs are binary classifiers and so we need to combine sVM models to obtain a multi-class classifier.",
        "Formally then we can consider the purpose of the sVM to be to estimate a classification function f : X --> {+1} using training examples from X x {+1} so that error on unseen examples is minimized.",
        "The classification function returns either +1 if the test data is a member of the class, or 1 if it is not.",
        "Although sVMs learn what are essentially linear decision functions, the effectiveness of the strategy is ensured by mapping the input patterns X to a feature space F using a nonlinear mapping function Ü: X --> F. since the algorithm is well described in the literature cited earlier we will focus our description from now on the features we used for",
        "training.",
        "In our implementation each training pattern is given as a vector which represents certain lexical features.",
        "All models use a surface word, an orthographic feature (Collier et al., 2000) and previous class assignments, but our experiments with part of speech (POs) features (Brill, 1992) showed that POs features actually inhibited performance in the molecular biology data set which we present below.",
        "This is probably because the POs tagger was trained on news texts.",
        "Therefore POs features are used only for the MUC-6 news data set where we show a comparison with and without these features.",
        "The form of the vector is basically a bag of words, i.e. word positions or ordering are not recorded.",
        "In the experiments we report below we use feature vectors consisting of differing amounts of `context' by varying the window around the focus word which is to be classified into one of the NE+ classes.",
        "The full window of context considered in these experiments is +3 about the focus word as shown in Figure 1.",
        "In pattern formation we took an IOB based approach to NE+ chunk identification in which each word was assigned a class tag from {ýþÿ, Bþÿ, Of where Ct is the class, B stands for a beginning of chunk tag, I stands for an in-chunk tag, and O stands for outside of chunk, i.e. not a member of one of the given classes.",
        "Due to the nature of the sVM as a binary classifier it is necessary in a multi-class task to consider the strategy for combining several classifiers.",
        "In our experiments with Tiny sVM the strategy used was one-against-one rather than one-against-the-rest.",
        "For example, if we have four classes A, B, C and D then Tiny sVM builds classifiers for (1) A against (B, C, D), (2) B against (C, D), and (3) C against D. The winning class is the one which obtains the most votes of the pairwise classifiers.",
        "We implemented two versions of the sVM.",
        "SVM1 uses a +3 window about the focus word and is implemented with the polynomial (poly) kernel function.",
        "SVM2 is used to directly compare the performance of the sVM with the HMM model described below and here we use only features for the focus word and previous word, i.e. a more limited context.",
        "Due to effects of data sparseness the HMM would be very difficult to train using a wider context window - this is one of the advantages we hope to test in SVM1.",
        "The kernel function k: X x X--> R mentioned above basically defines the feature space f by computing the inner product of pairs of data points.",
        "For x C X we explored the simple polynomial function k (xi , xj) = (xi • xj + 1)'."
      ]
    },
    {
      "heading": "2.2 HMM",
      "text": [
        "The HMM we implemented for comparison with the sVM was the version fully described in (Collier et al., 2000)2.",
        "Basically this is a linear interpolating HMM trained using maximum likelihood estimates from bigrams of the surface word and an orthographic feature which is deterministically chosen.",
        "No part of speech was used in the formulation of this model.",
        "We consider words to be ordered pairs consisting of a surface word, W, and a word feature, F, given as G W, F >.",
        "As is common practice, we need to calculate the probabilities for a word sequence for the first word's name class Co and every other word differently since we have no initial name-class to make a transition from.",
        "Accordingly we use the following equation to calculate the initial name class probability,",
        "and for all other words and their name classes Ct as follows:",
        "where f (I) is calculated with maximum-likelihood estimates from counts on training data.",
        "In our current system we set the constants Ai and 0-� by hand and let E 0-� = 1.0, E Ai = 1.0, 0-o >_ 0-Ò > 0-2, Ao > Al ... > A5.",
        "The current name-class Ct is conditioned on the current word and feature, the previous name-class, Ct-1, and previous word and feature.",
        "Once the state transition probabilities have been calculated according to Equations 1 and 2, the Viterbi algorithm (Viterbi, 1967) is used to search the state space of possible name class assignments in linear time to find the highest probability path, i.e.. to maximize P�(14,, C)."
      ]
    },
    {
      "heading": "2.3 Data Sets",
      "text": [
        "We used two data sets in our study one for NE+ and the other for traditional NE.",
        "The NE+ collection (Bio1) consists of 100 MEDLINE abstracts (23586 words) in the domain of molecular biology annotated for the names of genes and gene products (Tateishi et al., 2000).",
        "The second (MUC-6) is the collection of 60 executive succession texts (24617 words) used in MUC-6 for dryrun and testing.",
        "Details are shown in Tables 1 and 2."
      ]
    },
    {
      "heading": "3 Results and Analysis",
      "text": [
        "Results are given as F-scores (van Rijsbergen, 1979) and calculated using the CoNLL evaluation script3.",
        "F-score is defined as F = (2PR) / (P + R) .",
        "where P denotes Precision and R Recall.",
        "P is the ratio of the number of correctly found NE chunks to the number of found NE chunks, and R is the ratio of the number of correctly found NE chunks to the number of true NE chunks.",
        "Table 3 shows the overall F-score for the three models and two collections, calculated using 10-",
        "fold cross validation on the total test collection.",
        "Due to the size of the collections we did not observe an optimal result for each model but we found a clear and sustained advantage by sVMÒ over the HMM for the NE task in MUC-6 and the NE+ task in Bio1.",
        "The only drawback we observed with sVMz was that it seemed to be quite weak for the very low frequency classes such as RNA, sOURCE.mo or TIME where the HMM usually proved to be more robust.",
        "sVMz was the weakest model that we tested and we can conclude that when trained with similar knowledge to the HMM the sVM has no particular performance advantage that we could observe.",
        "However by exploiting the sVMs capability to easily handle large feature sets including a wide context window and POs tags the results suggest that the sVM will perform at a significantly higher level than the HMM.",
        "A detailed break down of results by class is shown in Table 4.",
        "What is not obvious from the tables is the effect we found of tokenization.",
        "In all the experiments reported for sVM in Table 3 we used the FDG parser (Tapanainen and J�arvinen, 1997) which we found gave much better results for Bio1 than a simple tokenization strategy that simply split each word at spaces or punctuation marks.",
        "On MUC-6 the advantage was less clear and we concluded that the frequent and ambiguous use of hyphen in Bio1 was the key factor.",
        "On the NE+ task in Bio1 we found that sVMÒ slightly but clearly outperformed the HMM.",
        "In analysis of sVMÒ results we identified several types of error.",
        "The first and perhaps most serious type was caused by local syntactic ambiguities such as head sharing in 39-kD SHP., SH3 domain which should have been classed as a PROTEIN, but the sVM split it into two PROTEIN expressions SHP.",
        "and SH3 domain.",
        "In particular the ambiguous use of hyphen, e.g. 14E1 single-chain (sc) Fv ����b��Y , and parentheses, e.g. scFv (14E1), seemed to cause the sVM more difficulties than the HMM.",
        "It is likely that the limited contextual information we gave to the sVM was the cause of this and can be improved on using grammatical features such as head noun or main verb.",
        "HMM seems to gain an advantage through the Viterbi algorithm by being able to partially consider evidence over the entire sentence.",
        "A second minor type of error seemed to be the result of inconsistencies in the annotation scheme for Bio1 such as the inclusion of a definite description in a term name for UT71TPO cells, a thrombopoietin-dependent ����k��Y��Y��� cell line which was all considered to be a sOURCE.ct expression."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "There are many more kernel parameters to explore than can be dealt with here and we will be continuing our investigation by tuning the sVM parameters.",
        "The results do however provide an indication of performance trends: the first is that sVM will outperform the HMM by a significant margin on both the MUC-6 and Bio1 data sets if it is given a wide context window (+3) and a rich feature set.",
        "The second is that the sVM lacked sufficient knowledge about complex structures in NE+ expressions to achieve its best performance on Bio1.",
        "We believe that with further tuning our sVM model will prove more useful in NE+ and allow us to combine evidence from large feature sets in order to model local structure and context.",
        "Furthermore, if a training set was developed for the POs tagger in the NE+ domain it seems likely that the sVM would strongly benefit from this.",
        "In its current configuration sVMÒ could be combined with the HMM on the Bio1 data set to achieve better performance in 5 of the 10 classes.",
        "While acknowledging the danger of drawing broad conclusions about the NE+ task from one domain-based data set, pending further analysis, we can cautiously say that performance on the two data sets has shown that the MUC-6 NE task is somewhat easier than the Bio1 NE+ task.",
        "Despite a few investigations into the nature of the NE task (Palmer and Day, 1997) (Nobata et al., 2000) the information theoretical aspects of the knowledge required for the task are still not well understood and this must be considered as a key area for future research.",
        "In order to test our method more accurately and develop a composite model we are now building a more",
        "and a 13 context window; sVM� results when a context window of 1 and the focus were used so that direct comparisons with the HMM can be made.",
        "f Results for models using surface word and orthographic features but no part of speech features; I Results for models using surface word, orthographic and part of speech features.",
        "realistic data set for molecular biology from full journal articles."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was supported in part by the Japanese Ministry of Education and science (grant no.",
        "14701020).",
        "We would like to thank Jun-ichi Tsujii for providing the data set Bio1 and to the anonymous referees whose comments have helped to improve the paper."
      ]
    }
  ]
}
