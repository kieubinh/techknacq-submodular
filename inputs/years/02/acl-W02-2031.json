{
  "info": {
    "authors": [
      "Koji Tsukamoto",
      "Yutaka Mitsuishi",
      "Manabu Sassano"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W02-2031",
    "title": "Learning With Multiple Stacking for Named Entity Recognition",
    "url": "https://aclweb.org/anthology/W02-2031",
    "year": 2002
  },
  "references": [
    "acl-A97-1029",
    "acl-E99-1023",
    "acl-P98-1081",
    "acl-W00-1303"
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": [
        "In this paper, we present a learning method using multiple stacking for named entity recognition.",
        "In order to take into account the tags of the surrounding words, we propose a method which employs stacked learners using the tags predicted by the lower level learners.",
        "We have applied this approach to the CoNLL-2002 shared task to improve a base system."
      ]
    },
    {
      "heading": "2 System Description",
      "text": [
        "Before describing our system, let us see one aspect of the named entity recognition, the outline of our method, and the relation to the previous works.",
        "The task of named entity recognition can be regarded as a process of assigning a named entity tag to each given word, taking into account the patterns of surrounding words.",
        "Suppose that a sequence of words is given as below: W ,W ,W ,W ,W Then, given that the current position is at word W , the task is to assign tag T to W .",
        "In the named entity recognition task, an entity is often made up of a sequence of words, rather than a single word.",
        "For example, an entity “the United States of America” consists of five words.",
        "In order to allocate a tag to each word, the tags of the surrounding words (we call these tags the surrounding tags) can be a clue to predict the tag of the word (we call this tag the current tag).",
        "For the test set, however, these tags are unknown.",
        "In order to take into account the surrounding tags for the prediction of the current tag, we propose a method which employs multiple stacked learners, an extension of stacking method (Wolpert, 1992).",
        "Stacking based method for named entity recognition usually employs two or more level learners.",
        "The higher level learner uses the current tags predicted by its lower level learners.",
        "In our method, by contrast, the higher level learner uses not only the current tag but also the surrounding tags predicted by the lower level learner.",
        "Our aim is to leverage the performance of the base system using the surrounding tags as the features.",
        "At least two groups have previously proposed systems which use the predicted surrounding tags.",
        "One system, proposed by van Halteren et al.",
        "(1998), also uses stacking method.",
        "This system uses four completely different types of taggers as the first level learners, because it has been assumed that first level learners should be as different as possible.",
        "The tags predicted by the first level learners are used as the features of the second level learner.",
        "The other system, proposed by (Kudo and Matsumoto, 2000; Yamada et al., 2001), uses the ”dynamic features”.",
        "In the test phase, the predicted tags of the preceding (or subsequent) words are used as the features, which are called “dynamic features”.",
        "In the training phase, the system uses the answer tags of the preceding (or subsequent) words as the features.",
        "More detailed descriptions of our system are shown below:"
      ]
    },
    {
      "heading": "2.1 Learning Algorithm",
      "text": [
        "As the learning algorithm for all the levels , we use an extension of AdaBoost, the real AdaBoost.MH which is extended to handle multiclass problems (Schapire and Singer, 1999).",
        "For weak learners, we use decision stumps (Schapire and Singer, 1999), which select only one feature to classify an example."
      ]
    },
    {
      "heading": "2.2 Features",
      "text": [
        "We use the following types of the features for the prediction of the tag of the word.",
        "surface form of W , W , W , W and W",
        "One of the eight word features in Table 1.",
        "These features are similar to those used in (Bikel et al., 1997).",
        "First and last two/three letters of W Estimated tag of W based on the word uni-gram model in the training set.",
        "Additionally, we use the surrounding tag feature.",
        "This feature is discussed in Section 2.3."
      ]
    },
    {
      "heading": "2.3 Multiple Stacking",
      "text": [
        "In order to take into account the tags of the surrounding words, our system employs stacked learners.",
        "Figure 1 gives the outline of the learning and applying algorithm of our system.",
        "In the learning phase, the base system is trained at first.",
        "After that, the higher level learners are trained using word features (described in Section 2.2), current tag T and surrounding tags T predicted by the lower level learner.",
        "While these tag may not be correctly predicted , if the accuracy of the prediction of the lower level learner is improved, the features used in each prediction become accurate.",
        "In the applying phase, all of the learners are cascaded in the order.",
        "Compared to the previous systems (van Halteren et al., 1998; Kudo and Matsumoto, 2000; Yamada et al., 2001), our system is: (i) employing more than two levels stacking, (ii) using only one algorithm and training only one learner at each level, (iii) using the surrounding tag given by the lower level learner.",
        "(iv) using both the preceding and subsequent tags as the features.",
        "(v) using the predicted tags instead of the answer tags in the training phase."
      ]
    },
    {
      "heading": "3 Experiments and Results",
      "text": [
        "In this section, the experimental conditions and the results of the proposed method are shown.",
        "In order to improve the performance of the base system, the tag sequence to be predicted is formatted according to IOB1, even though the sequence Let L denote the th level learner and let T denote th level output tags for W .",
        "in the original corpus was formatted according to IOB2 (Tjong Kim Sang and Veenstra, 1999).",
        "To reduce the computational cost, features appearing fewer than three times are eliminated in the training phase."
      ]
    },
    {
      "heading": "3.1 Base System",
      "text": [
        "To evaluate the effect of multiple stacking in the next section, the performance of the base system is shown in Figure 2.",
        "A performance peak is observed after 10,000 rounds of boosting.",
        "Note that a decision stump used in the real AdaBoost.MH takes into account only one feature.",
        "Hence the number of features used by real AdaBoost.MH is less than the number of the rounds.",
        "In our experiment, because the rounds of boosting are always less than the number of the features (about 40,000), a large proportion of features are not used by the learners.",
        "If the rounds of boosting in the base system are not enough, stacking effect may be similar to increasing the rounds of boosting.",
        "In Figure 2, however, we can see that 10,000 rounds is enough."
      ]
    },
    {
      "heading": "3.2 Multiple Stacking",
      "text": [
        "We examine the effect of multiple stacking compared to the base system.",
        "The F score of multiple stacking for the Spanish test set (esp.testa) is shown in Table 2.",
        "By stacking learners, the score of each named entity is im",
        "proved.",
        "Compared to the overall F score of L , the score of L , stacking one learner over the base system, is improved by 4.74 point.",
        "Further more, compared to the score of L , the score of L is higher by 1.67 point.",
        "Through five iterations of stacking, the score is continuously increased.",
        "The overall scores for the six tests are briefly shown in Table 3.",
        "The effect of two level stacking is higher for the Spanish tests.",
        "However, multiple staking effects greater for the Dutch test, especially for the corpus without part of speech.",
        "As discussed in Section 3.",
        "1, the improvement of the score is not due to the rounds of boosting.",
        "Thus, it is due to multiple stacking.",
        "In Table 2, stacking effects for MISC and ORG appear greater than those for LOC and PER.",
        "It is reasonable to suppose that MISC and ORG entities consist of a relatively long sequence of words, and the surrounding tags can be good clues for the prediction of the current tag.",
        "Indeed, in the Spanish training set, the ratios of entities which consist of more than three words are 9.7%, 22.4%, 4.4% and 3.5% for ORG, MISC, LOC and PER respectively.",
        "Table 4 and 5 show examples of the predicted tags through the stacked level.",
        "Let us see how multiple stacking works using the examples in Table 5.",
        "Let the word “fin” be the current position.",
        "The answer tag is “I-MISC”.",
        "When we use the base system",
        "L , the predicted tag of the word is “O”.",
        "In the next level, L uses the surrounding tag features “I-MISC, O, (O,) O, O” and also outputs “O”.",
        "In the third level, however, L correctly predicts the tag using the surrounding tag features “I-MISC, I-MISC, (O,) O, O”.",
        "Note that no other feature changes through the levels.",
        "The improvement in the example is clearly caused by multiple stacking.",
        "As a result, this MISC entity is allocated tags correctly by L .",
        "The above effect would not be achieved by two level stacking.",
        "This result clearly shows that multiple stacking method has an advantage.",
        "Next we examine the effect of the learning algorithm to multiple stacking.",
        "We use the real Ad-aBoost.MH for 300, 1,000, 3,000, 10,000, 20,000 rounds.",
        "Their F scores in each stacking level are plotted in Figure 3.",
        "The score improves by stacking for all algorithms.",
        "The highest score is achieved by 10,000 iterations at every stacking level.",
        "The shapes of the curves in Figure 3 are similar to each other.",
        "This result suggests that the stacking effect"
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "We have presented a new method for recognizing named entity by multiple stacking.",
        "This method can leverage the performance of the base system employing multiple stacked learner and using not only the current tag but also the surrounding tags predicted by the lower level learner.",
        "By stacking 5 real AdaBoost.MH learners, we can obtain F of 67.35 for the Spanish named entity recognition task."
      ]
    }
  ]
}
