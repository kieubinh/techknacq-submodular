{
  "info": {
    "authors": [
      "Judita Preiss"
    ],
    "book": "Conference of the European Association for Computational Linguistics",
    "id": "acl-E03-1025",
    "title": "Using Grammatical Relations to Compare Parsers",
    "url": "https://aclweb.org/anthology/E03-1025",
    "year": 2003
  },
  "references": [
    "acl-A00-2018",
    "acl-A00-2031",
    "acl-A94-1009",
    "acl-J93-1002",
    "acl-J93-2004",
    "acl-J94-4002",
    "acl-P97-1003",
    "acl-W00-0735",
    "acl-W96-0102",
    "acl-W96-0213",
    "acl-W99-0629"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We use the grammatical relations (GRs) described in Carroll et al.",
        "(1998) to compare a number of parsing algorithms.",
        "A first ranking of the parsers is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences: this required an implementation of GR extraction software for Penn Treebank style parsers.",
        "In addition, we perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm.",
        "This produces a second ranking of the parsers, and we investigate the number of errors that are caused by the incorrect GRs."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "We investigate the usefulness of a grammatical relation (GR) evaluation method by using it to compare the performance of four full parsers and a GR finder based on a shallow parser.",
        "It is usually difficult to compare performance of different style parsers, as the output trees can vary in structure.",
        "In this paper, we use GRs to provide a common basis for comparing full and shallow parsers, and Penn Treebank and Susanne structures.",
        "To carry out this comparison, we implemented a GR extraction mechanism for Penn Treebank This work was supported by UK EPSR.C project GR/N36462/93 'Robust Accurate Statistical Parsing'.",
        "parses.",
        "Evaluating parsers using GRs as opposed to crossing brackets or labelled precision/recall metrics can be argued to give a more robust measure of performance (Carroll et al., 1998), (Clark and Hockenmaier, 2002).",
        "The main novelty of this paper is the use of the Carroll et al's GR evaluation method to compare the Collins model 1 and model 2, and Charniak parsers.",
        "An initial evaluation is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences due to Carroll et al.",
        "To gain insight into the strengths and weaknesses of the different parsers, we present a breakdown of the results for each type of GR.",
        "It is not clear whether the ranking produced from the gold standard evaluation is representative: there may be corpus effects for parsers not trained on Susanne, and real life applications may not reflect this ranking.",
        "We therefore perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm.",
        "This produces a second ranking of the parsers, and we investigate the number of errors that are caused by incorrect GRs.",
        "We describe the parsers and the GR finder in Section 2.",
        "We introduce GRs in Section 3 and briefly describe our GR extraction software for Penn Treebank style parses.",
        "The evaluation, including a description of the evaluation corpus and performance results, is presented in Section 4.",
        "The results are analyzed in Section 5 and a performance comparison in the context of anaphora resolution is presented in Section 6.",
        "We draw our conclusions in Section 7."
      ]
    },
    {
      "heading": "2 Tools",
      "text": [
        "In this work we compare four full parsers from which GRs are extracted by walking over the trees.",
        "These parsers are Briscoe and Carroll (1993) (BC), Charniak (2000) (CH), model 1 and model 2 of Collins (1997) (CI and C2).",
        "A summary of published performance results can be found in Table 1.",
        "We also include in our comparison a GR finder (Buchholz, 2002) (BU) based on a shallow parser (Daelemans, 1996), (Buchholz et al., 1999).",
        "Table 2 summarizes the xNote that Collins' model 1 and Collins' model 2 are considered as two different parsers.",
        "grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate."
      ]
    },
    {
      "heading": "3 Grammatical Relations",
      "text": [
        "Lin (1995) proposed an evaluation based on grammatical dependencies, in which syntactic dependencies are described between heads and their dependents.",
        "This work was extended by Carroll et al.",
        "(1998), and it is this specification called grammatical relations which we employ in our work.",
        "An example, for the sentence John gave Mary the book, can be seen in Figure 1.",
        "Both the Briscoe and Carroll parser and Sentence: John gave Mary the book.",
        "Grammatical relations:",
        "Sections 2-21 of the Wall Street Sections 10-19 of the (Sampson, 1995)e Journal portion of the Penn Treebank (Marcus et al., 1993) WSJ corpus of the Penn Treebank II (ncsubj gave John) (dobj gave Mary) (obj2 gave book) (detmod book the) Buchholz's GR finder already output GRs in the desired format.",
        "Although Buchholz's work has focused mainly on extracting relations involving verbs, some non-verb relations (e.g. detmod) are also produced by the drunker she employs (Veenstra and van den Bosch, 2000).",
        "Therefore, to carry out a GR comparison, we need to extract GRs from Penn Treebank style parses.",
        "We manually created rules which find the relevant heads and their dependants by traversing the parse tree (for example, the NP in a S -> NP VP rule gives an instance of the ncsubj relation).",
        "In cases where a distinction is difficult/impossible to make from a Penn Treebank tree (e.g. xcomp vs xmod), we sacrificed recall for precision and only encoded rules which cause as few misclassifications as possible.",
        "Similar work has been carried out by Blaheta and Charniak (2000) who used statistical methods to add function tags to Penn Treebank I style parses; however, as well as converting the tags into a Carroll et al.",
        "format, we would need to add extra rules to extract other GRs needed for our application described in Section 6.",
        "E.g. the direct object is not immediately apparent from Penn Treebank II tags.",
        "We restrict the GRs we extract from the Penn Treebank to those that are necessary for the anaphora resolution application (the object relations, the complement relations and the ncmod relation), and those that are a simple by-product of extracting the necessary relations (e.g. aux)."
      ]
    },
    {
      "heading": "4 Evaluation",
      "text": [
        "As part of the development of their parser, Carroll et al.",
        "have manually annotated 500 sentences with their GRs.",
        "The sentences were selected at random from the Susanne corpus subject to the constraint that they are within the coverage of the Briscoe and Carroll parser.",
        "We used our own evaluation software which only scores correct an exact match of the output with the gold standard.",
        "This has caused some differences in performance with previously published results, for example the Briscoe and Carroll GRs do not produce the expected conjunction in the conj relation, causing the system to score zero.",
        "The results of all systems are presented in Table 3.",
        "For each system, we present two figures: precision (the number of instances of this GR the system correctly annotated divided by the number of instances labelled as this GR by the system), and recall (the number of instances of this GR the system correctly annotated divided by the number of instances of this GR in the corpus).",
        "In the #occs column of the table, we also present the number of occurrences of each GR in the 500 sentence corpus.",
        "A dash (-) indicates that a certain GR annotation was not present in the answer corpus at all.",
        "We also show the mean /j, precision and recall for each system, and the weighted mean fi\\y, where precision and recall values are weighted by the number of occurrences of each GR.",
        "To obtain a ranking of the parsers, we compare Fp=i using the i-test.",
        "The 500 sentence corpus is split into 10 segments and an F-measure is computed for each algorithm on all segments.",
        "These are then compared using the i-test.",
        "The results are presented in Table 4, which is to be interpreted as follows: This example means that the Collins model 2 parser does not outperform the Buchholz GR finder, but it outperforms the Briscoe parser with a statistical significance of 85%.",
        "Table 4 shows that the Buchholz' GR finder, based on a shallow parser, outperforms all the other parsers.",
        "This is followed in order by Char-niak's, Collins' model 2, Collins' model 1, and then Briscoe and Carroll's."
      ]
    },
    {
      "heading": "5 Error Analysis",
      "text": [
        "We investigated the cases where groups of systems failed to annotate some GRs (missing GRs) and cases where groups of systems returned the same wrong relation (extra GRs).",
        "The results of this are presented in Tables 5 and 6.",
        "In Table 5, we present the percentage of wrong cases covered by a particular combination of systems (i.e. BC represents the proportion of extra relations which were only suggested by the Briscoe and Carroll parser, whereas BC BU CH CI C2 represents those extras which were suggested by all parsers.)",
        "We present individual percentages, the percentage covered by the related Collins parsers (CI C2), the Penn Treebank parsers (CH CI C2) and all systems.",
        "The GRs wrongly suggested by all systems could be used to identify errors in the gold standard, since these break down into:",
        "• Extra ncmod relations, due to wrong identification of the head by the algorithms or in the gold standard.",
        "• Extra iobj relations, due to a misclassification of an ncmod relation.",
        "Table 6 classifies the cases of missing GRs and could therefore be used to discover missing classes of GRs, as well as mistakes in the gold standard.",
        "The main sources of errors are: • Missing ncmod relations where the modifier is temporal, e.g. (ncmod say Friday).",
        "• Missing detmods, due to certain words not being assigned a determiner tag by the taggers.",
        "Examples of such words are many and several.",
        "This error creates extra ncmod relations instead.",
        "The table also shows that the clausal relation would benefit from improvement since the clausal relations is frequently omitted from all the Penn Treebank parsers.",
        "However, in the case of this relation, we have sacrificed recall for precision."
      ]
    },
    {
      "heading": "6 Anaphora Resolution",
      "text": [
        "We investigate the effect of using different parsers in an anaphora resolution system.",
        "This will indicate the impact of a change in parser performance on a real task: although one parser may have a marginally higher precision than another on a particular evaluation corpus, it is not clear whether this will be reflected by the results of a system which makes use of this parser, and which may work on a different corpus.",
        "We choose to reimplement a non-probabilistic algorithm due to Lappin and Leass (1994), because this anaphora resolution algorithm can be encoded in terms of the GR information (Preiss and Briscoe, 2003).",
        "For each pronoun, this algorithm uses syntactic criteria to rule out noun phrases that cannot possibly corefer with it.",
        "An antecedent is then chosen according to a ranking based on salience weights.",
        "For all pronouns, noun phrases are ruled out if they have incompatible agreement features.",
        "Pronouns are split into two classes, lexical (reflexives and reciprocals) and non-lexical anaphors.",
        "There are additional syntactic filters for both of the two types of anaphors.",
        "Candidates which remain after filtering are ranked according to their salience.",
        "A salience value corresponding to a weighted sum of the relevant feature weights (summarized in Table 7) is computed.",
        "If we consider the sentence John walks, the salience of John will be: The weights are scaled by a factor of where s is the distance (number of sentences) of the candidate from the pronoun.",
        "The candidate with the highest salience is proposed as the antecedent.",
        "The algorithm uses GR information at two points: initially, it is used to eliminate certain intrasentential candidates from the candidates list.",
        "For example, in the sentence She likes her, she and her cannot corefer, which is expressed by a shared head in the following GRs: (ncsubj like she) (dobj like her) Secondly, GR information is used for obtaining salience values.",
        "In the above sentence, we would use the ncsubj relation to reward she for being a subject and the dobj relation to give her points for accusative emphasis.",
        "The algorithm makes use of the object relations (ncsubj, dobj, obj2, iobj), the complement relations (xcomp, ccomp, and clausal), and the non-clausal modifier ncmod relation.",
        "For this experiment, we use an anaphorically resolved 2400 sentence initial segment of the BNC (Leech, 1992), which we split into five segments containing roughly equal numbers of pronouns.",
        "The number of sentences and pronouns in each of the five segments is presented in Table 8.",
        "The results of the Lappin and Leass anaphora resolution algorithm using each of the parsers are presented in Table 9.",
        "The 'algorithms' are only evaluated on pronouns where all systems suggested an answer, so only precision is reported.",
        "The difference between the 'worst' and the 'best' systems' mean [i performance is about 1%.",
        "However, the variance er (a measure of robustness of a system) is lowest for Collins' model 1.",
        "We again investigate the significance of the performance results using a i-test on our five segments, and the results can be seen in Table 10.",
        "The ranking obtained in this case indicates very small differences in performance between the algorithms.",
        "In our error analysis, we found that in 40% (H!)",
        "OI the errors the anaphora resolution algorithm made a mistake with all the parsers.",
        "This suggests that for a large number of pronouns, the error is with the anaphora resolution algorithm and not with the parser employed.",
        "The breakdown of the number of systems that suggested each mistake for each pronoun can be seen in Table ll.",
        "dency to choose the same (potentially wrong) antecedent, since there are no cases where all versions of the Lappin and Leass algorithm chose different antecedents (versus 153 times all systems chose the wrong antecedent).",
        "The number of times that only one antecedent exists in the suggested answers is strikingly high.",
        "However, this may be slightly misleading, as a chosen pronominal antecedent (e.g. in Mary... She\\... She2, She2 will resolve to She±) counts as identical whether or not it refers to the same entity.",
        "In scoring the anaphora resolution, if She\\ was previously wrongly resolved, She2 is also treated as an error.",
        "This choice of evaluation method may be having an impact on our overall accuracy."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We have presented two evaluations of information derived from full and shallow parsers.",
        "The first compares the results of certain GRs against a gold standard, and the second investigates the change in accuracy of an anaphora resolution system when the parser is varied.",
        "When the systems' F-measures were compared, we found that Buchholz' GR finder outperformed the conventional full parsers.",
        "This is an interesting result, which shows that accurate GRs can be obtained without the expense of constructing a full parse.",
        "The ranking between the Penn Treebank parsers obtained from the GR evaluation reflects the ranking obtained from a direct parser comparison (from Table 1).",
        "In the task-based evaluation, the performance gap between the anaphora resolution algorithm using the various parsers narrowed.",
        "This may be due to the anaphora resolution algorithm making use of only certain instances of GRs which are 'equally difficult' for all parsers to extract.",
        "We expect the results of the anaphora res-It is also interesting to see the number of different antecedents suggested by the anaphora resolution algorithm using the various parsers (Table 12).",
        "We can see that there is a ten-systems attempt all pronouns which they are given; pronouns were only removed if the correct antecedent was wrongly tagged.",
        "Only about 10 pronouns were removed in this way.",
        "olution experiment to be typical of parser applications that make use of a large number of types of GRs.",
        "Future work is required to evaluate parsers on applications that make use of just a few types of GRs, for example selectional preference based word sense disambiguation."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": []
    }
  ]
}
