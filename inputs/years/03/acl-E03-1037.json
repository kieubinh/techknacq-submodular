{
  "info": {
    "authors": [
      "Sabine Schulte Im Walde"
    ],
    "book": "Conference of the European Association for Computational Linguistics",
    "id": "acl-E03-1037",
    "title": "Experiments on the Choice of Features for Learning Verb Classes",
    "url": "https://aclweb.org/anthology/E03-1037",
    "year": 2003
  },
  "references": [
    "acl-C00-2094",
    "acl-C00-2108",
    "acl-C96-1055",
    "acl-J01-3003",
    "acl-P02-1029",
    "acl-P98-1112"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The choice of verb features is crucial for the learning of verb classes.",
        "This paper presents clustering experiments on 168 German verbs, which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepositional phrase information and selectional preferences.",
        "In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Dorr, 1997), document classification (Kla-vans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002).",
        "A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes.",
        "Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpus-based features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002).",
        "Computational approaches on verb classification which take advantage of corpus-based and knowledge-based verb information offered by available tools and resources such as statistical parsers and semantic ontologies, suffer from severe problems to encode and benefit from the information, especially with respect to selectional preferences, cf. Schulte im Walde (2000); Joanis (2002).",
        "This paper presents clustering experiments on German verbs which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepositional phrase information and selectional preferences.",
        "The clustering results show that the choice and implementation of verb features is crucial for the induction of the verb classes.",
        "Intuitively, one might want to add and refine features ad infinitum, but we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the verb classification."
      ]
    },
    {
      "heading": "2 German Verb Classes",
      "text": [
        "A set of 168 German verbs is manually classified into 43 concise semantic verb classes.",
        "The purpose of the manual classification is (i) to evaluate the reliability and performance of the clustering experiments on a preliminary set of verbs, and (ii) to explore the potential and limit to apply the clustering method to large-scale verb data.",
        "The German classes are closely related to the English pendant in (Levin, 1993) and agree with the German verb classification in (Schumacher, 1986) as far as the relevant verbs appear in his semantic 'fields'.",
        "Table 1 presents the manual verb classification.",
        "The class size is between 2 and 7, with an average of 3.9 verbs per class.",
        "Eight verbs are ambiguous and marked by subscripts.",
        "The classes include both high and low frequency verbs, in order to exercise the clustering technology in both data-rich and data-poor situations.",
        "The class labels are given on two semantic levels; coarse labels such as Manner of Motion are subdivided into finer labels, such as Locomotion, Rotation.",
        "The fine labels are relevant for the clustering experiments, as indicated by the numbering in the left column.",
        "The classification is primarily based on semantic intuition, not on knowledge about the syntactic behaviour.",
        "As an extreme example, the Support class (34) contains the verb unterstützen, which syntactically requires a direct object, together with the three verbs dienen, folgen, helfen which mainly subcategorise an indirect object."
      ]
    },
    {
      "heading": "3 Clustering Methodology",
      "text": [
        "Clustering is a standard procedure in multivariate data analysis.",
        "It is designed to uncover an inherent natural structure of data objects, and the induced equivalence classes provide a means to generalise over the objects.",
        "We perform clustering by the k-Means algorithm (Forgy, 1965), an unsupervised hard clustering method assigning n data objects to k clusters.",
        "Initial verb clusters are iteratively reorganised by assigning each verb to its closest cluster and recalculating cluster centroids until no further changes take place.",
        "The clustering methodology in this work is based on parameter investigations in (Schulte im Walde and Brew, 2002): the clustering input is obtained from a hierarchical analysis on the German verbs (Ward's amalgamation method), the number of clusters being the number of manual classes; similarity measure is performed by the skew divergence, a variant of the Kullback-Leibler divergence.",
        "The 168 verbs are associated with probabilistic frame descriptions on various levels of verb information, and assigned to starting clusters by hierarchical clustering.",
        "The k-Means algorithm is then allowed to run until no further changes take place, and the resulting clusters are evaluated and interpreted against the manual classes.",
        "lrrhe verb frequency range in 35 million words newspaper data is 8-71,604.",
        "Table 1 : German semantic verb classes",
        "Weather: blitzen, donnern, dämmern, nieseln, regnen, schneien"
      ]
    },
    {
      "heading": "4 Clustering Evaluation",
      "text": [
        "Evaluating the result of a cluster analysis against the known gold standard of hand-constructed verb classes requires to assess the similarity between two partitions on the set of n verbs.",
        "The evaluation is performed by an adjusted version of the Rand index (Hubert and Arabie, 1985): The Rand index measures the agreement between object pairs in the partitions and is corrected for chance in comparison to the null model that the partitions are picked at random, given the original number of classes and objects.",
        "The agreement in the two partitions is represented by a contingency table C x M: tij denotes the number of verbs common to classes Cj in the clustering partition C and Mj in the manual classification M; the marginals tj.",
        "and t.j refer to the number of objects in d and Mj, respectively.",
        "The adjusted Rand index Raaj is given in Equation (1); the expected number of common object pairs attributable to a particular cell (Cj, Mj) in the contingency table is defined by (^O (*2 VCD- Therange of Radj is 0 < Radj < 1, with only extreme cases below zero.",
        "We choose Radj as evaluation measure compared to e.g. the measures presented in (Schulte im Walde and Brew, 2002), because (a) it does not show a bias towards extreme cluster sizes, and (b) it facilitates the interpretation with its normally used bounds of 0 and 1."
      ]
    },
    {
      "heading": "5 Verb Description",
      "text": [
        "The German verbs are described on three levels of subcategorisation definition Dl to D3, each refining the previous level by additional information.",
        "All information is extracted from a lexi-calised probabilistic grammar which is unsupervised trained on 35 million words of a German newspaper corpus, using the EM-algorithm.",
        "Dl provides a coarse syntactic definition of subcategorisation.",
        "The verbs are described by a probability distribution over 38 frame types.",
        "Possible arguments in the frames are nominative (n), dative (d) and accusative (a) noun phrases, reflexive pronouns (r), prepositional phrases (p), expletive es (x), non-finite clauses (i), finite clauses (s-2 for verb second clauses, s-dass for <iass-clauses, sob for oZ?-clauses, s-w for indirect w/z-questions), and copula constructions (k).",
        "For example, sub-categorising a direct (accusative case) object and a non-finite clause next to the obligatory nominative subject is labelled 'nai'.",
        "On Dl, the verbs are given a syntactico-semantic definition of subcategorisation with prepositional preferences.",
        "In addition to the syntactic frame information, D2 discriminates between different kinds of pp-arguments.",
        "This is done by distributing the probability mass of prepositional phrase frame types over the prepositional phrases, according to their frequencies in the corpus.",
        "Prepositional phrases are referred to by case and preposition, such as 'mho', 'fur^', with D=Dative and A=Accusative.",
        "We define 30 different PPs, according to the most frequent PPs which appear with at least 10 different verbs.",
        "D3 gives a syntactico-semantic definition of subcategorisation with prepositional and selectional preferences.",
        "The argument slots within a subcategorisation frame type are specified according to which 'kind' of argument they require.",
        "The grammar provides selectional preference information on a fine-grained level: it specifies argument realisations for a specific verb-frame-slot combination in form of lexical heads.",
        "For example, the most prominent nominal argument heads for the verb verfolgen 'to follow' in the accusative NP slot of the transitive frame type 'na' (the considered frame slot is underlined) are Ziel 'goal', Strategie 'strategy', Politik 'policy'.",
        "Obviously, we would run into a sparse data problem if we tried to incorporate selectional preferences on the nominal level into the verb descriptions.",
        "We need a generalisation of the selectional preference definition, for which we use the noun hierarchy in GermaNet (Kunze, 2000), the German pendant of the semantic ontology WordNet (Fellbaum, 1998).",
        "The hierarchy is realised as synsets, sets of synonymous nouns, which are organised into multiple inheritance hypernym relationships.",
        "A noun may appear in several synsets, according to its number of senses.",
        "For each nominal argument in a verbframe-slot combination, the joint frequency is split over the different senses of the noun and propagated upwards the hierarchy.",
        "In case of multiple hypernym synsets, the frequency is split, such that the sum of frequencies over the disjoint top synsets equals the total joint frequency.",
        "Repeating the frequency assignment and propagation for all nouns appearing in a verb-frame-slot combination, we define a frequency distribution of the verb-frame-slot combination over all GermaNet synsets.",
        "To restrict the variety of noun concepts, we consider only the 15 top GermaNet nodes: Lebewesen 'creature', Sache 'thing', Besitz 'property', Substanz 'substance', Nahrung 'food', Mittel 'means', Situation 'situation', Zustand 'state', Struktur 'structure', Physis 'body', Zeit 'time', Ort 'space', Attribut 'attribute', Kognitives Objekt 'cognitive object', Kognitiver Prozess 'cognitive process'.",
        "Since the 15 nodes exclude each other and the frequencies sum to the total joint verb-frame frequency, we can define a probability distribution over the top nodes representing coarse selectional preferences for the respective verb-frame-slot combination.",
        "To obtain D3, the verb-frame probability is distributed over those selectional preferences.",
        "Table 2 presents three verbs from different verb classes and their ten most frequent frame types with respect to the three levels of verb definition, accompanied by the probability values.",
        "Dl for beginnen 'to begin' defines 'np' and 'n' as the most probable frame types.",
        "Even by splitting the 'np' probability over the different PP types in D2, a number of prominent PPs are left, the time indicating uniA and nacho, mito referring to the begun event, anb as date and inu as place indicator.",
        "It is obvious that not all PPs are argument PPs, but also adjunct PPs describe a part of the verb behaviour.",
        "D2> illustrates that typical selectional preferences for beginner roles are Situation, Zustand, Zeit, Sache.",
        "D3 has the potential to indicate verb alternation behaviour, e.g. 'na(Situation)' refers to the same role for the direct object in a J Little manual intervention was necessary to define a coherent set of top level nodes, since GermaNet had not been completed.",
        "transitive frame as 'n(Situation)' in an intransitive frame.",
        "essen 'to eat' as an object drop verb shows strong preferences for both an intransitive and transitive usage.",
        "As desired, the argument roles are strongly determined by Lebewesen for both 'n' and 'na' and Nahrung for 'na'.",
        "fahren 'to drive' chooses typical manner of motion frames ('n', 'np', 'na') with the refining PPs being directional (ina, zud, nacho) or referring to a means of motion (mit£>, inp, aufs).",
        "The selectional preferences represent a correct alternation behaviour: Lebewesen in the object drop case for 'n' and 'na', Sache in the inchoative/causative case for 'n' and 'na'.",
        "find multiple variations.",
        "In order to illustrate that the most plausible variations have been considered, we describe and use linguistically intuitive mutations of the verb descriptions.",
        "Role Choice:• On Dl, there is little room to vary the verb information, since the valency encoding is close to standard German grammar, cf. Helbigand Buscha (1998).",
        "• On D2, we vary the amount of PP information: (a) Following standard German grammar books we define a more restricted set of prepositional phrases for argument usage, and (b) ignoring any frequency constraint on the PP information increases the kinds of PPs in the relevant frame types up to 140.",
        "• On 7J3, there is most room for variation:",
        "Instead of using the 15 top level nodes in GermaNet, (a) we use selectional preferences on a more fine-grained level, the word level, and (b) we define a more generalised description of selectional preferences, by merging the frequencies of the 15 top level nodes in GermaNet to only 2 (Lebewesen, Objekt) or 3 (Lebewesen, Sache, Abstraktum).",
        "Role Integration: To integrate the selectional preferences into the verb description, either (a) each argument slot in a subcategorisation frame is substituted by selectional roles separately, e.g. the joint frequency of a verb and transitive 'na' is distributed over the nominative slot preferences 'na(Lebewesen)', 'na(Sache)', etc.",
        "and also over the accusative slot preferences 'na(Lebewesen)', 'na(Sache)', etc.",
        "(as in Table 2).",
        "In this case, the argument slots of frame types with several arguments are considered independently, but the number of features remains in a reasonable magnitude, 15 per frame slot.",
        "Or (b) the subcategorisation frames are substituted by the combinations of selectional preferences for the argument slots, e.g. the joint probability of a verb and 'na' is distributed over 'na(Lebewesen:Nahrung)', 'na(Lebewesen:Sache)\\ lna(Sache:Nahrung)', etc.",
        "This encoding would directly represent the linguistic idea of alternations, but no direct frequencies are available, and the number of features explodes (15 features for an intransitive, 15 for a transitive, 15 for a ditransitive) and leads to differing magnitudes of probabilities.",
        "Role Means: We could use a different means for selectional role representation than GermaNet.",
        "But since the ontological idea of WordNet has been widely and successfully used and we do not have any comparable source at hand, we have to exclude this variation."
      ]
    },
    {
      "heading": "7 Clustering Results",
      "text": [
        "The baseline for the clustering experiments is Radj = – 0.004 and refers to 50 random clusterings: The verbs are randomly assigned to a cluster (with a cluster number between 1 and the number of manual classes 43), and the resulting clustering is evaluated.",
        "The baseline value is the average value of the 50 repetitions.",
        "The upper bound is Radj = 0.909 and calculated on a hard version of the manual classification, i.e. multiple senses of verbs are reduced to a single class affiliation, which represents the optimum for the hard clustering algorithm.",
        "Table 3 presents the clustering results for Dl and D2, with D2 distinguishing the amount of PP information (arg for arguments only, chosen for the manually defined PPs, all for all possible PPs).",
        "As stated by Schulte im Walde and Brew (2002), refining the syntactic verb information by prepositional phrases is helpful for the clustering; and the usage is not restricted to argument PPs, but extended by the more variable PP information.",
        "Underlying the results in Table 4, the argument roles for selectional preference information in D3 are varied.",
        "The left part presents the results when refining only a single argument within a single frame, in addition to D2.",
        "Obviously, the results do not match linguistic intuition.",
        "For example, we would expect the arguments in the two highly frequent intransitive 'n' and transitive 'na' to provide valuable information with respect to their selectional preferences, but only those in 'na' improve D2.",
        "On the other hand, 'ni' which is not expected to provide variable definitions of selectional preferences for the nominative slot, does work better than 'n'.",
        "The right part in Table 4 illustrates the clustering results for example combinations of argument slots refined by selectional preferences, e.g. n/na means that the nominative slot in 'n', and both the nominative and accusative slot in 'na' are refined by selectional preferences.",
        "The combined information does not necessarily improve the single slot clustering results, e.g. n/na achieves results below the ones for refining only na or na.",
        "The overall best result (including non-illustrated experiment results) is achieved by defining selectional preferences on n/na/nd/nad/ns-dass, better than refining all NP slots or all NP and all PP slots in the frame types.",
        "Summarising, Table 4 illustrates that a linguistic choice of features is worthwhile, but linguistic intuition and algorithmic clustering results do not necessarily align.",
        "On selected argument roles, the selectional preference information in D3 once more improves the clustering results compared to D2, but the improvement is not as persuasive as D2 improving Dl.",
        "With respect to further feature variation, merging the frequencies of the 15 top level nodes in GermaNet to 2 or 3 roles results in noisy distributions and destroys the coherence of the cluster analyses.",
        "Experiment setups which either include a nominal level of selectional preference information or an alternation-like combination of selectional roles were tried, but they suffer from their time demands and result in far worse analyses.",
        "Finally, we present representative parts of the cluster analysis based on D3, with selectional roles 'n', 'na', 'nd', 'nad', 'ns-dass', and compares the respective clusters with their pendants under Dl and D2.",
        "The manual class numbers as defined in Table 1 are given as subscripts.",
        "There are verbs as in cluster (e), whose properties are correctly stated as similar on Dl – 3, so a common cluster is justified; but the verbs only have coarse common meaning components, in this case toten and unterrichten agree in an action of one person or institution towards another.",
        "Summarising the cluster description, some verbs and verb classes are distinctive on a coarse feature level, some need fine-grained extensions, and some are not distinctive with respect to any combination of features."
      ]
    },
    {
      "heading": "8 Discussion and Conclusion",
      "text": [
        "We have presented a clustering methodology for German verbs whose results agree with a manual classification in many respects and should prove useful as automatic basis for a large-scale clustering.",
        "Without any doubt the cluster analysis would need manual correction and completion, but represents a plausible basis.",
        "The various verb descriptions illustrate that stepwise refining the features does improve the clustering.",
        "But the linguistic feature refinements not necessarily align with expected changes in clustering.",
        "This effect could be due to (i) noisy or (ii) sparse data, but (i) the example distributions in Table 2 demonstrate that even if noisy-our basic verb descriptions appear reliable with respect to their desired linguistic content.",
        "In addition, the subcategorisation information on Dl and D2 has been evaluated against manual definitions in a dictionary and proven useful (Schulte im Walde, 2002).",
        "And (ii) Table 4 illustrates that even with adding little information (e.g. refining a single argument by 15 selectional roles results Related work on automatic verb classes confirms the difficulty of selecting and encoding verb features.",
        "Schulte im Walde (2000) clusters 153 English verbs into 30 verb classes as taken from (Levin, 1993), using a hierarchical clustering method.",
        "The clustering is most successful when utilising syntactic subcategorisation frames enriched with PP information (comparable to our D2); selectional preferences are encoded by role combinations taken from WordNet.",
        "Schulte im Walde claims the detailed encoding and therefore sparse data to make the clustering worse with than without the selectional preference information.",
        "Merlo and Stevenson (2001) classify a smaller number of 60 English verbs into three verb classes, by utilising supervised decision trees.",
        "The features of the verbs are restricted to those which should capture the basic differences between the verb classes, and the feature values are approached by corpus-based heuristics (e.g. measuring the degree of animacy by personal pronoun realisation in the transitive subject slot).",
        "An extension of their work by loanis (2002) uses 802 verbs from 14 classes in (Levin, 1993).",
        "He defines an extensive feature space with 219 core features (such as part of speech, auxiliary frequency, syntactic categories, animacy as above) and 1,140 selectional preference features taken from WordNet.",
        "As in our approach, the selectional preferences do not improve the clustering.",
        "Why do we encounter such unpredictability concerning the encoding and effect of verb features, especially with respect to selectional preferences?",
        "In contrast to previous approaches concentrating on the sparse data problem, we have presented evidence for a linguistically defined limit on the usefulness of the verb features, driven by the idiosyncratic properties of the verbs.",
        "Recall the underlying idea of verb classes, that the meaning components of verbs to a certain extent determine their behaviour.",
        "This does not mean that all properties of all verbs in a common class are similar and we could extend and refine the feature description endlessly, still improving the clustering.",
        "The meaning of verbs comprises both (i) properties which are general for the respective verb classes, and (ii) idiosyncratic properties which distinguish the verbs from each other.",
        "As long as we define the verbs by those properties which represent the common parts of the verb classes, a clustering can succeed.",
        "But with stepwise refining the verb description by including lexical idiosyncrasy, the emphasis of the common properties vanishes.",
        "The exemplary description of cluster outcomes in the previous section confirms that it is impossible to determine an overall appropriate level of feature specification which suffices all kinds of verb classes defined in Table 1.",
        "Some verbs and verb classes are distinctive on a coarse feature level, some need fine-grained extensions, some are not distinctive with respect to any combination of features.",
        "There is no unique perfect choice and encoding of the verb features, even more with respect to a potential large-scale extension of verbs and classes.",
        "Further work on the verb classes should concern a choice of verb features with respect to the specific properties of the desired verb classification.",
        "We could think of either (i) performing several cluster analyses on the same set of verbs, but with different choices of verb features, and then find a way to merge the results to a unique classification, or (ii) not aiming for a fine-grained clustering, but create fewer but larger clusters on coarse features, which classify the verbs on a more general level.",
        "Both solutions should facilitate the demarcation of common and idiosyncratic verb features and improve the clustering results."
      ]
    }
  ]
}
