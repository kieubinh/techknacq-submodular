{
  "info": {
    "authors": [
      "Karl-Michael Schneider"
    ],
    "book": "Conference of the European Association for Computational Linguistics",
    "id": "acl-E03-1059",
    "title": "A Comparison of Event Models for Naive Bayes Anti-Spam E-Mail Filtering",
    "url": "https://aclweb.org/anthology/E03-1059",
    "year": 2003
  },
  "references": [
    "acl-W01-0506"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe experiments with a Naive Bayes text classifier in the context of anti-spam Email filtering, using two different statistical event models: a multivariate Bernoulli model and a multinomial model.",
        "We introduce a family of feature ranking functions for feature selection in the multinomial event model that take account of the word frequency information.",
        "We present evaluation results on two publicly available corpora of legitimate and spam Emails.",
        "We find that the multinomial model is less biased towards one class and achieves slightly higher accuracy than the multivariate Bernoulli model."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Text categorization is the task of assigning a text document to one of several predefined categories.",
        "Text categorization plays an important role in natural language processing (NLP) and information retrieval (IR) applications.",
        "One particular application of text categorization is anti-spam Email filtering, where the goal is to block unsolicited messages with commercial or pornographic content (UCE, spam) from a user's Email stream, while letting other (legitimate) messages pass.",
        "Here, the task is to assign a message to one of two categories, legitimate and spam, based on the message's content.",
        "In recent years, a growing body of research has applied machine learning techniques to text categorization and (anti-spam) Email filtering, including rule learning (Cohen, 1996), Naive Bayes (Sahami et al., 1998; Androutsopoulos et al., 2000b; Rennie, 2000), memory based learning (Androutsopoulos et al., 2000b), decision trees (Carreras and Marquez, 2001), support vector machines (Drucker et al., 1999) or combinations of different learners (Sakkis et al., 2001).",
        "In these approaches a classifier is learned from training data rather than constructed by hand, which results in better and more robust classifiers.",
        "The Naive Bayes classifier has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application areas despite its simplicity (Lewis, 1998).",
        "Bayesian classifiers are based on a probabilistic model of text generation.",
        "A text is generated by first choosing a class according to some prior probability and then generating a text according to a class-specific distribution.",
        "The model parameters are estimated from training examples that have been annotated with their correct class.",
        "Given a new document, the classifier outputs the class which is most likely to have generated the document.",
        "From a linguistic point of view, a document is made up of words, and the semantics of the document is determined by the meaning of the words and the linguistic structure of the document.",
        "The Naive Bayesian classifier makes the simplifying assumption that the probability that a document is generated in some class depends only on the probabilities of the words given the context of the class, and that the words in a document are independent of each other.",
        "This is called the Naive Bayes assumption.",
        "The generative model underlying the Naive Bayes classifier can be characterized with respect to the amount of information it captures about the words in a document.",
        "In information retrieval and text categorization, two types of models have been used (McCallum and Nigam, 1998).",
        "Both assume that there is a fixed vocabulary.",
        "In the first model, a document is generated by first choosing a subset of the vocabulary and then using the selected words any number of times, at least once, in any order.",
        "This model is called multivariate Bernoulli model.",
        "It captures the information of which words are used in a document, but not the number of times each words is used, nor the order of the words in the document.",
        "In the second model, a document is generated by choosing a set of word occurrences and arranging them in any order.",
        "This model is called multinomial model.",
        "In addition to the multivariate Bernoulli model, it also captures the information about how many times a word is used in a document.",
        "Note that in both models, a document can contain additional words that are not in the vocabulary, which are considered noise and are not used for classification.",
        "Despite the fact that the multivariate Bernoulli model captures less information about a document (compared to the multinomial model), it performs quite well in text categorization tasks, particularly when the set of words used for classification is small.",
        "However, McCallum and Nigam (1998) have shown that the multinomial model outperforms the multivariate Bernoulli model on larger vocabulary sizes or when the vocabulary size is chosen optimal for both models.",
        "Most text categorization approaches to anti-spam Email filtering have used the multivariate Bernoulli model (Androutsopoulos et al., 2000b).",
        "Rennie (2000) used a multinomial model but did not compare it to the multivariate model.",
        "Mladenic and Grobelnik (1999) used a multinomial model in a different context.",
        "In this paper we present results of experiments in which we evaluated the performance of a Naive Bayes classifier on two publicly available Email corpora, using both the multivariate Bernoulli and the multinomial model.",
        "The paper is organized as follows.",
        "In Sect.",
        "2 we describe the Naive Bayes classifier and the two generative models in more detail.",
        "In Sect.",
        "3 we introduce feature selection methods that take into account the extra information contained in the multinomial model.",
        "In Sect.",
        "4 we describe our experiments and discuss the results.",
        "Finally, in Sect.",
        "5 we draw some conclusions."
      ]
    },
    {
      "heading": "2 Naive Bayes Classifier",
      "text": [
        "We follow the description of the Naive Bayes classifier given in McCallum and Nigam (1998).",
        "A Bayesian classifier assumes that a document is generated by a mixture model with parameters 6, consisting of components C = {ci,..., cn} that correspond to the classes.",
        "A document is generated by first selecting a component cy € C according to the prior distribution P(cj\\9) and then choosing a document dj according to the parameters of Cj with distribution P(di\\cj; 6).",
        "The likelihood of a document is given by the total probability Of course, the true parameters 6 of the mixture model are not known.",
        "Therefore, one estimates the parameters from labeled training documents, i.e. documents that have been manually annotated with their correct class.",
        "We denote the estimated parameters with 6.",
        "Given a set of training documents V = {d\\,... , dm}, the class prior parameters are estimated as the fraction of training documents in Cj, using maximum likelihood: where P(cj\\d{) is 1 if e cj and 0 otherwise.",
        "The estimation of P(di\\cj] 6) depends on the generative model and is described below.",
        "Given a new (unseen) document d, classification of d is performed by computing the posterior probability of each class, given d, by applying Bayes' rule: The classifier simply selects the class with the highest posterior probability.",
        "Note that P(d\\8) is the same for all classes, thus d can be classified by computing The multivariate Bernoulli event model assumes that a document is generated by a series of \\V\\ Bernoulli experiments, one for each word wt in the vocabulary V. The outcome of each experiment determines whether the corresponding word will be included at least once in the document.",
        "Thus a document di can be represented as a binary feature vector of length \\V\\, where each dimension t of the vector, denoted as Bu € {0,1}, indicates whether word wt occurs at least once in dj.",
        "The Naive Bayes assumption assumes that the \\V\\ trials are independent of each other.",
        "By making the Naive Bayes assumption, we can compute the probability of a document given a class from the probabilities of the words given the class: Note that words which do not occur in di contribute to the probability of di as well.",
        "The paramP(wt\\cj: 9) of the mixture component Cj can be estimated as the fraction of training documents in c7 that contain wt:] The multinomial event model assumes that a document di of length \\di is generated by a sequence of \\di\\ word events, where the outcome of each event is a word from the vocabulary V. Following McCallum and Nigam (1998), we assume that the document length distribution P(\\di\\) does not depend on the class.",
        "Thus a document di can be represented as a vector of length \\ V\\, where each dimension t of the vector, denoted as Na > 0, is the 'McCallum and Nigam ( 1998) suggest to use a Laplacean prior to smooth the probabilities, but we found that this degraded the performance of the classifier.",
        "number of times word wt occurs in di.",
        "The Naive Bayes assumption assumes that the \\di\\ trials are independent of each other.",
        "By making the Naive Bayes assumption, the probability of a document given a class is the multinomial distribution: P(wt\\cf,e) Nul The parameters 6m\\c – P(wt\\cj;9) of the mixture component Cj can be estimated as the fraction of word occurrences in the training documents in cj that are wt:"
      ]
    },
    {
      "heading": "3 Feature Selection",
      "text": [
        "It is common to use only a subset of the vocabulary for classification, in order to reduce over-fitting to the training data and to speed up the classification process.",
        "Following McCallum and Nigam (1998) and Androutsopoulos et al.",
        "(2000b), we ranked the words according to their average mutual information with the class variable and selected the N highest ranked words.",
        "Average mutual information between a word wt and the class variable, denoted by M/(C; Wt), is the difference between the entropy of the class variable, H(C), and the entropy of the class variable given the information about the word, H(C\\Wi) (Cover and Thomas, 1991).",
        "Intuitively, MI(C\\Wt) measures how much bandwidth can be saved in the transmission of a class value when the information about the word is known.",
        "In the multivariate Bernoulli model, Wi is a random variable that takes on values ft & {0.1}, indicating whether word wt occurs in a document or not.",
        "Thus MI(C; Wt) is the average mutual information between C and the absence or presence of wt in a document: ceCfte{o,i} Mutual information as in (9) has also been used for feature selection in the multinomial model, either by estimating the probabilities P(c, ft), P(c) and P(ft) as in the multivariate model (McCal-lum and Nigam, 1998) or by using the multinomial probabilities (Mladenic and Grobelnik, 1999).",
        "Let us call the two versions mv-MI and mn-MI, respectively.",
        "mn-MI is not fully adequate as a feature ranking function for the multinomial model.",
        "For example, the token Subject: appears in every document in the two corpora we used in our experiments exactly once, and thus is completely uninformative.",
        "mv-MI assigns 0 to this token, but mn-MI yields a positive value because the average document length in the classes is different, and thus the class-conditional probabilities of the token are different across classes in the multinomial model.",
        "On the other hand, assume that some token occurs once in every document in c\\ and twice in every document in C2, and that the average document length in C2 is twice the average document length in ci.",
        "Then both mv-MI and mn-MI will assign 0 to the token, although it is clearly highly informative in the multinomial model.",
        "We experimented with feature scoring functions that take into account the average number of times a word occurs in a document.",
        "Let N(cj,wL) =",
        "In the multinomial model, a word is informative with respect to the class value if its mean term frequency in some class is different from its (global) mean term frequency, i.e. if mf^.^ ^ L We used feature ranking functions of the form in (10): R(cj, wt) measures the amount of information that wt gives about cj.",
        "f(cj,wt) is a weighting function.",
        "Table 1 lists the feature ranking functions that we used in our experiments.",
        "mn-MI is the average mutual information where the probabilities are estimated as in the multinomial model, dmn-MI differs from mn-MI in that the class prior probabilities are estimated as the fraction of documents in each class, rather than the fraction of word occurrences.",
        "tf-MI, dtf-MI and tftf-MI use mean term frequency to measure the correlation between wt and cj and use different weighting functions."
      ]
    },
    {
      "heading": "4 Experiments 4.1 Corpora",
      "text": [
        "We performed experiments on two publicly available Email corpora: Ling-Spam (Androutsopou-los et al., 2000b) and PU1 (Androutsopoulos et al., 2000a).",
        "We trained a Naive Bayes classifier with a multivariate Bernoulli model and a multinomial model on each of the two datasets.",
        "The Ling-Spam corpus consists of 2412 messages from the Linguist list and 481 spam messages.",
        "Thus spam messages are 16.6% of the corpus.",
        "Attachments, HTML tags and all Email headers except the Subject line have been stripped off.",
        "We used the lemmatized version of the corpus, with the tokenization given in the corpus and with no additional processing, stop list, etc.",
        "The total vocabulary size is 59829 words.",
        "The PU1 corpus consists of 618 English legitimate messages and 481 spam messages.",
        "Messages in this corpus are encrypted: Each token has been replaced by a unique number, such that different occurrences of the same token get the same number (the only non encrypted token is the Subject: header name).",
        "Spam messages are 43.8% of the corpus.",
        "As with the Ling-Spam corpus, we used the lemmatized version with no ad-http://www.aueb.gr/users/ion/ \"http://www.linguistlist.org/ ditional processing.",
        "The total vocabulary size is 21706 words.",
        "Both corpora are divided into 10 parts of equal size, with equal proportion of legitimate and spam messages across the 10 parts.",
        "Following (Androut-sopoulos et al., 2000b), we used 10-fold cross-validation in all experiments, using nine parts for training and the remaining part for testing, with a different test set in each trial.",
        "The evaluation measures were then averaged across the 10 iterations.",
        "We performed experiments on each of the corpora, using the multivariate Bernoulli model with mv-MI, as well as the multinomial model with mv-MI and the feature ranking functions in Table 1, and varying the number of selected words from 50 to 5000 by 50.",
        "For each event model and feature ranking function, we determined the minimum number of words with highest recall for which recall equaled precision (breakeven point).",
        "Tables 2 and 3 present the breakeven points with the number of selected words, recall in each class, and accuracy.",
        "In some cases, precision and recall were different over the entire range of the number of selected words.",
        "In these cases we give the recall and accuracy for the minimum number of words for which accuracy was highest.",
        "Figures 1 and 2 show recall curves for the multivariate Bernoulli model and three feature ranking functions in the multinomial model for Ling Spam, and Figures 3 and 4 for PU1.",
        "Some observations can be made from these results.",
        "First, the multivariate Bernoulli model favors the Ling resp.",
        "Legit classes over the Spam classes, whereas the multinomial model is more balanced in conjunction with mv-MI, tf-MI and tftf-MI.",
        "This may be due to the relatively specific vocabulary used especially in the Ling-Spam corpus, and to the uneven distribution of the documents in the classes.",
        "Second, the multinomial model achieves higher accuracy than the multivariate Bernoulli model.",
        "tf-MI even achieves high accuracy at a comparatively small vocabulary size (1200 and 2400 words, respectively).",
        "In general, PU1 seems to be more difficult to classify.",
        "Androutsopoulos et al.",
        "(2000b) used cost-sensitive evaluation metrics to account for the fact that it may be more serious an error when a legitimate message is classified as spam than vice versa.",
        "However, such cost-sensitive measures are problematic with a Naive Bayes classifier because the probabilities computed by Naive Bayes are not reliable, due to the independence assumptions it makes.",
        "Therefore we did not use cost-sensitive measures."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "We performed experiments with two different statistical event models (a multivariate Bernoulli"
      ]
    }
  ]
}
