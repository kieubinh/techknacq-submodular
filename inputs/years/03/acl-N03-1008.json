{
  "info": {
    "authors": [
      "Yonggang Deng",
      "Sanjeev P. Khudanpur"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics",
    "id": "acl-N03-1008",
    "title": "Latent Semantic Information in Maximum Entropy Language Models for Conversational Speech Recognition",
    "url": "https://aclweb.org/anthology/N03-1008",
    "year": 2003
  },
  "references": [],
  "sections": [
    {
      "heading": "Edmonton, May-June 2003",
      "text": [
        "By performing singular value decomposition of this matrix, a short vector representation is derived for each word and document.",
        "One advantage of the resulting word and document representations is that they all live in the same low-dimensional continuous vector space, enabling one to quantitatively measure closeness or similarity between words and documents.",
        "The cosine of the angle between two vectors is a standard measure of similarity in this framework.",
        "For language modeling, a pseudo-document is constructed from (possibly all) the words preceding a particular position in an utterance and the resulting vector is projected into the abovementioned low-dimensional vector space, sometimes referred to as the LSA-space.",
        "Intuition suggests that words with vectors close to the pseudo-document vector are more likely to follow than those far away from it.",
        "This is used to construct a conditional probability on the task-vocabulary.",
        "This probability, which depends on a long span of \"history\" is then suitably combined with an N-gram probability.",
        "An alternative to first constructing a conditional probability on the task-vocabulary independently of the N-gram model and then seeking ways to combine the two probabilities, is directly modeling the pseudo-document as yet another conditioning event – on par with the preceding N-1 words – and finding a single probability distribution conditioned on the entire \"history.\" Note that the co-occurrence of the predicted word with, say, the immediately preceding word in the history is a discrete event and amenable to simple counting.",
        "By contrast, the pseudo-document is a continuous-valued vector and simply counting how often a word follows a particular vector in a training corpus is meaningless; we must employ a parametric model for word-history co-occurrence, possibly together with discretization of the pseudo-document vector.",
        "The remainder of the paper explores these main themes as follows.",
        "For completeness, we briefly describe in Section 2 the standard LSA language modeling techniques we implemented.",
        "We then describe the maximum entropy alternative for combining N-gram and latent semantic information in Section 3.",
        "We present experimental results on the Switchboard corpus of conversational speech in Section 4 and conclude in Section 5.",
        "2 LSA-Based Language Models LSA requires a corpus separated into semantically coherent documents, and a vocabulary to cover words found in these documents.",
        "It is assumed that the co-occurrence of any two words within a document at a rate much greater than chance is an indication of their semantic similarity.",
        "This similarity is then used for language modeling, as explained below.",
        "The notation and exposition in this section closely follows that of Bellegarda (2000).",
        "2.1 Word-Document Frequency Matrix W The first step in LSA is to represent co-occurrence information by a large spare matrix.",
        "Let V, 1V1= M, be the underlying task vocabulary, and T a text corpus, with document boundaries marked, comprising N documents relevant to some domain of interest.",
        "Typically, M and N are of the order of 104 and 105, respectively.",
        "T, the language model training corpus, may thus have hundreds of millions of words.",
        "Unlike N-gram models, the construction of the M x N matrix W of co-occurrences between words and documents ignores word order within the document; it is accumulated from T by simply counting how many times a word appears in a document.",
        "In constructing the word-document co-occurrence matrix W, the raw count cij of a word wi E V in a document dj E T is weighted by",
        "• the \"relevance\" of a word in the vocabulary to the topic of a document, function words being given less weight than content words, and • the size of the document, a word with a given count in a longer document being given less weight than in a shorter one.",
        "To accomplish the former, pretend that a unique (unknown) document in our collection T is relevant for some task and our goal is to guess which one it is.",
        "Let the a priori probability of a document being relevant be uniform ( 1) on the collection and, further, let an oracle draw a single word at random from the relevant document and reveals it to us.",
        "The conditional probability of dj being the relevant document, given that the relevant document contains the word wi, is clearly =, where ci = EN t cij.",
        "The ratio of the average conditional entropy of the relevant document's identity, given wi, and its a priori entropy is thus a measure of the (un)informativeness of wi.",
        "Highly informative words wi have small values of",
        "Since 0 < Ei < 1, the raw counts in the i-throw of W are weighted by (1 – Ei).",
        "To achieve the latter effect, the counts in the j-th column of W are weighted by the total length cj =EMi cij of the document dj.",
        "In summary,",
        "is the resulting i j-th matrix entry."
      ]
    },
    {
      "heading": "2.2 Singular Value Decomposition of W",
      "text": [
        "Each column of the matrix W represents a document and each row represents a word.",
        "Typically, W is very sparse.",
        "To obtain a compact representation, singular value decomposition (SVD) is employed (cf. Berry et al. (1993)) to yield",
        "where, for some order R « min(M, N) of the decomposition, U is a M x R left singular matrix with rows ui, i = 1, ... , M, S is a R x R diagonal matrix of singular values s1 > 52 > ... > SR » 0, and V is a N x R right singular matrix with rows vg, j = 1, ... , N. For each i, the scaled R-vector uiS may be viewed as representing wi, the i-th word in the vocabulary, and similarly the scaled R-vector vg S as representing dg, the j-th document in the corpus.",
        "Note that the uiS's and vgS's both belong to RR, the so called LSA-space.",
        "The following similarity measure between the i-th and i'-th words wi and wi, is frequently used:",
        "IluiSIl x Ilui� SIl .",
        "Note that K(wi, wi,) is nothing but the cosine of the angle between the vectors uiS and ui, S. Algorithms such as K-means clustering have been applied to the vocabulary using (4) as a measure of similarity.",
        "Replacing the ui's with vg's in the definition above, a corresponding measure K(dg,dy)",
        "of similarity between the j-th and j'-th documents is obtained and has been used for document clustering, filtering and topic detection."
      ]
    },
    {
      "heading": "2.3 Calculating Word-Probabilities via LSA",
      "text": [
        "Given a sequence w1, w2, ... , wT of words in a sentence, the semantic coherence between wt, the word in the t-th position, and dt-1 - {w1 , ... , wt-1 }, all its predecessors, is used to construct a conditional probability on the vocabulary.",
        "Specifically, for a word wi in a training document dg, it is true by virtue of (3) that [W]ig , uiSvj .",
        "However, since the word-document similarity function by itself is not a bona fide probability mass function, a M x 1 pseudo-document vector it-1 is constructed by weighting the frequency of the preceding words in accordance with (2), and its scaled R-vector repre",
        "where Km�n(d�) = minty K(w,d) is an offset to make the resulting probabilities nonnegative.",
        "The coefficient y » 1, as noted by Coccaro and Jurafsky (1998), is chosen experimentally to increase the otherwise small dynamic range of K as w varies over the vocabulary.",
        "As one processes successive words in a sentence, the pseudo-document dt-1 is updated incrementally:",
        "where ewt is a M x 1 vector with a 1 in the position corresponding to wt and 0 elsewhere.",
        "Consequently, the vector vt-1S needed for the similarity computation of (6) towards the probability calculation of (7) is also incrementally updated:",
        "where a positive \"decay\" coefficient A < 1 is thrown in to accommodate dynamic shifts in topic."
      ]
    },
    {
      "heading": "2.4 Combining PLSA with N-grams",
      "text": [
        "Several strategies have been proposed (Coccaro and Jurafsky, 1998; Bellegarda, 2000) for combining the LSA-based probability (7) with standard N-gram probabilities, and we list those which we have investigated for conversational speech.",
        "Linear Interpolation: For some experimentally determined constants a, and a = 1 – a,",
        "Information Weighted Arithmetic Mean: Setting A,, = 1 2 w to account for the informativeness of a word w about its document, cf (1), and Aw=1-Aw",
        "Information Weighted Geometric Mean: With the same Aw and Aw as above,",
        "We compute language model perplexities for the Switchboard corpus using each of these methods and discuss the results in Section 4.1."
      ]
    },
    {
      "heading": "3 Exponential Models with Latent Semantic Features",
      "text": [
        "The ad hoc construction of PLSA(wldt_1) to somehow capture K(w, dt_1), and its combination with N-gram statistics described above are a somewhat unsatisfactory aspect of the LSA-based models.",
        "We propose, following Khudanpur (2000), an alternative family of exponential models",
        "where �1(YZ), f2(wt-1,wt) and f3(wt-2,wt-1,wt) are usually, but not necessarily, {0, 11-valued indicator functions of N-gram features and awt , awt-,,wt and awt-2,wt-1,wt are their corresponding feature weights, and where the semantic coherence between a word wt and its long-span history dt_1 has been thrown in as a feature, on par with the standard N-gram features.",
        "E.g., one could have",
        "We then find the maximum likelihood estimate of the model parameters a given the training data.",
        "Recall that the resulting model is also the maximum entropy (ME) model among models which satisfy constraints on the marginal probabilities or expected values of these features (Rosenfeld, 1996).",
        "An important decision that needs to be made in a model such as (14) is the parameterization a.",
        "In a traditional ME language model, in the absence of LSA-based features, each N-gram feature function is a {0,1}-valued indicator function, and there is a parameter associated with each feature: an aw for each unigram constraint, an aw,,w for each bi-gram constraint, etc.",
        "In extending this methodology to the LSA features, we note that K(wt,dt_1) is continuous-valued.",
        "That in itself is not a problem; the ME framework does not require the f (•)'s to be binary.",
        "What is problematic, however, is the fact that, almost surely, no two pseudo-documents it and it, will ever be identical.",
        "Therefore, assigning a distinct parameter ad ,w for each pseudo-document - word pair (d, w) is counterproductive, and some tying of parameters for similarly valued d is necessary.",
        "If we tie all the LSA parameters together, i.e., set",
        "ilarity modulated N-gram model (11), except that the choice of aLSA here is made jointly with the N-gram a's to maximize training data likelihood.",
        "If we let each vocabulary item to have its own a, i.e. ad w = aLSA,w b d \\ E IRR, (17) then (14) becomes directly comparable to the geometric interpolation method (13), again except that unlike Aw, the aLSA,w parameters are determined jointly with the N-gram a's to maximize a likelihood criterion.",
        "Since the goal of parameter tying, however, is to deal with the continuous nature of the pseudo",
        "where <D(d) represents a finite partition of IRR indexed by d. We choose to pursue this alternative.",
        "We use a standard K-means clustering of the representations vjS of the training documents dj, with (5) in the role of distance, to obtain a modest number of clusters.",
        "We then pool documents in each cluster together to form topic-centroids d, and the partition",
        "We also make two approximations to the feature function of (15).",
        "First, we approximate the pseudo-document dt_1 in K(•) with its nearest topic-centroid = d✔Z-1 = d ✔ whenever d�Z-1 E <D(d).",
        "This is motivated by the fact that we often deal with very small pseudo-documents dZ-1 in speech recognition, and d ✔ provides a more robust estimate of semantic coherence with wZ than dZ-1.",
        "Furthermore, keeping in mind the small dynamic range of the similarity measure of (6), as well as the interpretation (1) of Ew, we approximate the feature function of (�15) with",
        "This pragmatic approximation results in a simplified implementation, particularly for the computation of feature-expectations during parameter estimation.",
        "More importantly, when there is a free parameter a for each (d, w) pair, as is the case in (18), &s (d, w) = 1 and &s (d, w) = K(w, d) yield equivalent model families.",
        "Therefore, using",
        "For all pairs (d, w) with &s(d, w) = 1 in (19), the model-expectation of f is constrained to be the relative frequency of w within the cluster of training documents whose centroid is d. By virtue of their semantic coherence, it is usually higher than the relative frequency of w in the entire corpus.",
        "Another interesting way of tying the LSA parameters, which we have not investigated here, is ad,w = ad,,, b w E ✎ (w) , b d E <D(d) , (21) where ✎(w✔) is a finite, possibly d✔-dependent, partition of the vocabulary.",
        "This parameterization may be particularly beneficial when, due to a very large vocabulary or a small training corpus, we do not have sufficient counts to constrain the model-expectations of Ars(d, w) for all words w bearing high semantic similarity with a topic-centroid d. An automatically derived or knowledge-based semantic classification of words, e.g. from WordNet, may be used as ✎(•).",
        "▼�� X Similar ME Model from the Past An interesting consequence of (19) is that it makes the model of (14) identical in form to the model described by Khudanpur and Wu (1999).",
        "Two signifi",
        "cant ways in which (14) is novel are that • clustering of documents dj to obtain topic-centroids d ✔ during training, and assignment of pseudo-documents dZ-1 to topic-centroids dZ-1 during recognition, is based on similarity in LSA-space 1R✲, not document-space 1RM, and • the set of words with active semantic features (19) for any particular topic-centroid d ✔ is deter",
        "mined by a threshold ri on LSA similarity, not by a difference in within-topic v✕s corpus-wide relative frequency.",
        "The former results in some computational savings both during clustering and on-line topic assignment.",
        "The latter may result in a different choice of topic-dependent features.",
        "We present a comparison of LM performance between these two ME models in Section 4.5 following our main results.",
        "✗ Switchboard Experiments We conducted experiments on the Switchboard corpus of conversational telephone speech (Yodfrey et al., 1992), dividing the corpus into a LM training set of approximately 1500 conversations (2.2M words) and a test set of 19 conversations (20K words).",
        "The task vocabulary was fixed to 22K words, with an out-of-vocabulary rate under 0.5✘ on the test set.",
        "Acoustic models trained on roughly 60 hours of Switchboard speech and a bigram LM were used to generate lattices for the test utterances, and a 100- best list was generated by rescoring the lattice using a trigram model.",
        "All the results in this paper are based on rescoring this 100-best list with different language models.",
        "We treated each conversation-side as a separate document and created W of (2) with M ✒ 22, 000 and N ✒ 3000.",
        "Yuided by the fact that one of 70-odd topics was prescribed to a caller when the Switchboard corpus was collected, we computed the SVD of (3) with ✥=73 singular values.",
        "We implemented the LSA model of (7) with y = 20, and the four LSA ⑧ N-gram combinations of Section 2.4.",
        "To obtain the document clusters and topic-centroids d required for creating the partition <D(•) of (18), we randomly assigned the training documents to one of 50 clusters and used a K-means algorithm to iteratively (i) compute the topic-centroid d of each cluster by pooling together all the documents in the cluster, and (ii) reassigning each document dj to a cluster to whose centroid the document bore the greatest LSA similarity K(dj, d).",
        "Each cluster was required to have a minimum number of 10 documents in it, and if the number of documents in a cluster fell below this threshold following step (ii), then the cluster was eliminated and each of its documents reassigned to the nearest of the remaining centroids.",
        "The iteration stopped when no",
        "on average, and we observe convergence roughly 110 words into the conversation side."
      ]
    },
    {
      "heading": "4.3 Perplexity: ME Model with LSA",
      "text": [
        "Features In the process of comparing our ME model of (14) with the one described by Khudanpur and Wu (1999), we noticed that they built a baseline trigram model using the SRI LM toolkit.",
        "Other than this, our experimental setup – training and test set definitions, vocabulary, etc.",
        " – matches theirs exactly.",
        "We report the perplexity of our ME model against their baseline in Table 2, where the figures in the first two lines are quoted directly from Khudanpur and Wu (1999).",
        "A single topic-centroid dT selected",
        "for an entire test conversation-side was used in these experiments.",
        "The last line of Table 2 shows the best perplexity obtainable by any topic-centroid, suggesting that the automatically chosen, Voronoi region based topic-centroids are quite adequate.",
        "A comparison of Tables 1 and 2 also shows that the maximum entropy model is more effective in capturing semantic information than the information weighted geometric mean of the LSA-based unigram model and the trigram model.",
        "The correspondence of information weighted geometric mean with the parameterization of (17) and the corresponding richer parameterization of (18) are perhaps adequate to explain this improvement."
      ]
    },
    {
      "heading": "4.4 Word Error Rates for the ME Model",
      "text": [
        "We rescored the 100-best hypotheses generated by the baseline trigram model using the ME model with LSA features.",
        "In order to assign a topic-centroid d to a test utterance in the absence of its correct transcription, we investigated using a concatenation of the 1-best, 10-best or 100-best first-pass hypotheses of utterances in the test set, computed d once per test utterance, and found the performance of the 10-best hypotheses to yield a slightly lower word error rate (WER).",
        "This is perhaps the optimal trade-off between robustness in topic assignment resulting from considering additional word hypotheses, and noise introduced by considering erroneous words.",
        "We also",
        "investigated assigning topic for the entire conversation side based on the first-pass output and found it to yield a further reduction in WER.",
        "We report the results in Table 3 where the top two lines are, again, quoted directly from Khudanpur and Wu (1999).",
        "We performed the standard NIST MAPSSWE statistical significance test (Pallett et al., 1990) and found that",
        "• the WER improvement of the ME trigram model over the baseline SRI trigram model is not significant (p=0.529), • that of the ME model with LSA features and",
        "utterance-level topic assignment over the ME trigram model is significant (p=0.008), and",
        "• that of the ME model with LSA features and",
        "conversation-level topic assignment over the ME trigram model is also significant (p=0.002).",
        "The difference between the WER obtained by utterance-level v/s conversation-level topic assignment is not significant (p=0.395); nor are other WER differences (not reported here) between using the 1 v/s 10- v/s 100-best hypotheses for topic assignment."
      ]
    },
    {
      "heading": "4.5 Benefits of Dimensionality Reduction",
      "text": [
        "It was pointed out in Section 3.1 that the model proposed here differs from the model of Khudanpur and Wu (1999) mainly in the use of the R-dimensional LSA-space for similarity comparison rather than direct comparison in M-dimensional document-space.",
        "We present in Table 4 a summary comparison of the two modeling techniques.",
        "While, due to the sparse nature of the vectors, the 22K-dimensional space does not entail a proportional growth in similarity computation relative to the 73-dimensional space, the LSA similarities are still expected to be faster to compute.",
        "Furthermore, the LSA based model yields comparable perplexity and WER performance with considerably fewer topic-centroids, resulting in fewer comparisons during run time for determining the nearest centroid.",
        "Of lesser note is the observation that the 77-threshold based topic-feature selection of (19) results in a content word being an active feature for fewer topics than it does when topic-features are selected based on differences in within-topic and overall relative frequencies."
      ]
    },
    {
      "heading": "5 Summary and Conclusion",
      "text": [
        "We have presented a framework for incorporating latent semantic information together with standard N-gram statistics in a unified exponential model for statistical language modeling.",
        "This framework permits varying degrees of parameter tying depending on the amount of training data available.",
        "We have drawn parallels between some conventional ways of combining LSA-based models with N-grams and the parameter-tying decisions in our exponential models, and our results suggest that incorporating semantic information using maximum entropy principles is more effective than the ad hoc techniques.",
        "We have presented perplexity and speech recognition accuracy results on the Switchboard corpus which suggest that LSA-based features, while not as effective on conversational speech as on newspaper text, produce modest but statistically significant improvements in speech recognition performance.",
        "Finally, we have shown that the maximum entropy model presented here performs as well as a previously proposed maximum entropy model for incorporating topic-dependencies, but it is computationally more economical."
      ]
    },
    {
      "heading": "6 Acknowledgments",
      "text": [
        "We would like to thank Jun Wu of Google Inc. for assistance in the use of his tools for maximum entropy model estimation and application, and Woosung Kim of Johns Hopkins University for assistance in the use of other software.",
        "We also thank the anonymous referees for comments that helped improve this manuscript.",
        "This research was partially supported by the National Science Foundation via MLIAM Grant No � IIS 9982329."
      ]
    }
  ]
}
