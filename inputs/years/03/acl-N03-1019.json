{
  "info": {
    "authors": [
      "Shankar Kumar",
      "William Byrne"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics",
    "id": "acl-N03-1019",
    "title": "A Weighted Finite State Transducer Implementation of the Alignment Template Model for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/N03-1019",
    "year": 2003
  },
  "references": [
    "acl-J93-2003",
    "acl-N01-1018",
    "acl-P00-1056",
    "acl-W02-1021",
    "acl-W99-0604"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a derivation of the alignment template model for statistical machine translation and an implementation of the model using weighted finite state transducers.",
        "The approach we describe allows us to implement each constituent distribution of the model as a weighted finite state transducer or acceptor.",
        "We show that bitext word alignment and translation under the model can be performed with standard FSM operations involving these transducers.",
        "One of the benefits of using this framework is that it obviates the need to develop specialized search procedures, even for the generation of lattices or N-Best lists of bitext word alignments and translation hypotheses.",
        "We evaluate the implementation of the model on the French-to-English Hansards task and report alignment and translation performance."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The Alignment Template Translation Model (ATTM) (Och et al., 1999) has emerged as a promising modeling framework for statistical machine translation.",
        "The ATTM attempts to overcome the deficiencies of word-to-word translation models (Brown et al., 1993) through the use of phrasal translations.",
        "The overall model is based on a two-level alignment between the source and the target sentence: a phrase-level alignment between source and target phrases and a word-level alignment between words in these phrase pairs.",
        "The goal of this paper is to reformulate the ATTM so that the operations we intend to perform under a statistical translation model, namely bitext word alignment and translation, can be implementation using standard weighted finite state transducer (WFST) operations.",
        "Our main motivation for a WFST modeling framework lies in the resulting simplicity of alignment and translation processes compared to dynamic programming or decoders.",
        "The WFST implementation allows us to use standard optimized algorithms available from an off-the-shelf FSM toolkit (Mohri et al., 1997).",
        "This avoids the need to develop specialized search procedures, even for the gen"
      ]
    },
    {
      "heading": "SOURCE LANGUAGE SENTENCE",
      "text": [
        "eration of lattices or N-best lists of bitext word alignment or translation hypotheses.",
        "Weighted Finite State Transducers for Statistical Machine Translation (SMT) have been proposed in the literature to implement word-to-word translation models (Knight and Al-Onaizan, 1998) or to perform translation in an application domain such as the call routing task (Bangalore and Ricardi, 2001).",
        "One of the objectives of these approaches has been to provide an implementation for SMT that uses standard FSM algorithms to perform model computations and therefore make SMT techniques accessible to a wider community.",
        "Our WFST implementation of the ATTM has been developed with similar objectives.",
        "We start off by presenting a derivation of the ATTM that identifies the conditional independence assumptions that underly the model.",
        "The derivation allows us to specify each component distribution of the model and implement it as a weighted finite state transducer.",
        "We then show that bitext word alignment and translation can be performed with standard FSM operations involving these transducers.",
        "Finally we report bitext word alignment and translation performance of the implementation on the Canadian French-to-English Hansards task."
      ]
    },
    {
      "heading": "2 Alignment Template Translation Models",
      "text": [
        "We present here a derivation of the alignment template translation model (ATTM) (Och et al., 1999; Och, 2002) and give an implementation of the model using weighted finite state transducers (WFSTs).",
        "The finite state modeling is performed using the AT&T FSM Toolkit (Mohri et al., 1997).",
        "In this model, the translation of a source language sentence to a target language sentence is described by a joint probability distribution over all possible segmentations and alignments.",
        "This distribution is presented in Figure 1 and Equations 1-7.",
        "The components of the overall translation model are the source language model (Term 2), the source segmentation model (Term 3), the phrase permutation model (Term 4), the template sequence model (Term 5), the phrasal translation model (Term 6) and the target language model (Term 7).",
        "Each of these conditional distributions is modeled independently and we now define each in turn and present its implementation as a weighted finite state acceptor or transducer.",
        ".",
        "Similarly, a phrase in the source language sentence contains words , where is the NULL token.",
        "We assume that each word in each language can be assigned to a unique class so that unambiguously specifies a class sequence and specifies the class sequence .",
        "Throughout the model, if a sentence is segmented into phrases , we say to indicate that the words in the phrase sequence agree with the original sentence.",
        "Source Language Model The model assigns probability to any sentence in the source language; this probability is not actually needed by the translation process when is given.",
        "As the first component in the model, a finite state acceptor is constructed for .",
        "Source Segmentation Model We introduce the phrase count random variable which specifies the number of phrases in a particular segmentation of the source language sentence.",
        "For a sentence of length , there are ways to segment it into phrases.",
        "Motivated by this, we choose the distribution as so that .",
        "We construct a joint distribution over all phrase segmentations as",
        "where The normalization constant",
        "Here, is a “unigram” distribution over source language phrases; we assume that we have an inventory of phrases from which this quantity can be estimated.",
        "In this way, the likelihood of a particular segmentation is determined by the likelihood of the phrases that result.",
        "We now describe the finite state implementation of the source segmentation model and show how to compute the most likely segmentation under the model: .",
        "1.",
        "For each source language sentence to be trans",
        "lated, we implement a weighted finite state transducer that segments the sentence into all possible phrase sequences permissible given the inventory of phrases.",
        "The score of a segmentation under is given by .",
        "We then generate a lattice of segmentations of (implemented as an acceptor ) by composing it with the transducer , i.e. .",
        "2.",
        "We then decompose into disjoint subsets so that contains all segmentations of the source language sentence with exactly phrases.",
        "To construct , we create an unweighted acceptor that accepts any phrase sequence of length ; for efficiency, the phrase vocabulary is restricted to the phrases in .",
        "is then obtained by the finite state composition ."
      ]
    },
    {
      "heading": "3. For",
      "text": [
        "The normalization factors are obtained by summing the probabilities of all segmentations in .",
        "This sum can be computed efficiently using lattice forward probabilities (Wessel et al., 1998).",
        "For a fixed , the most likely segmentation in is found as",
        "(10) 4.",
        "Finally we select the optimal segmentation as",
        "We begin by distinguishing words and phrases.",
        "We assume that is a phrase in the target language sentence that has length and consists of words",
        "A portion of the segmentation transducer for the French sentence nous avons une inflation galopante is presented in Figure 2.",
        "When composed with , generates the following two phrase segmentations: nous avons une inflation galopante and nous avons une inflation galopante.",
        "The “ ” symbol is used to indicate phrases formed by concatenation of consecutive words.",
        "The phrases specified by the source segmentation model remain in the order that they appear in the source sentence.",
        "Phrase Permutation Model We now define a model for the reordering of phrase sequences as determined by the previous model.",
        "The phrase alignment sequence specifies a reordering of phrases into target language phrase order; the words within the phrases remain in the source language order.",
        "The phrase sequence is reordered into .",
        "The phrase alignment sequence is modeled as a first order Markov process with .",
        "The alignment sequence distribution is constructed to assign decreasing likelihood to phrase re-orderings that diverge from the original word order.",
        "Suppose and , we set the Markov chain probabilities as follows (Och et al., 1999) In the above equations, is a tuning factor and we normalize the probabilities so that .",
        "The finite state implementation of this model involves two acceptors.",
        "We first build a unweighted permutation acceptor that contains all permutations of the phrase sequence in the source language (Knight and Al-Onaizan, 1998) .",
        "We note that a path through corresponds to an alignment sequence .",
        "Figure 3 shows the acceptor for the source phrase sequence nous avons une inflation galopante.",
        "A source phrase sequence of length words requires a permutation acceptor of states.",
        "For long phrase sequences we compute a score for each arc and then prune the arcs by this score, i.e. phrase alignments containing are included only if this score is above a threshold.",
        "Pruning can therefore be applied while is constructed.",
        "source-language phrase sequence nous avons une inflation galopante.",
        "The second acceptor in the implementation of the phrase permutation model assigns alignment probabilities (Equation 13) to a given permutation of the source phrase sequence (Figure 4).",
        "In this example, the phrases in the source phrase sequence are specified as follows: (nous), (avons) and (une inflation galopante).",
        "We now show the computation of some of the alignment probabilities (Equation 13) in this example ( ) Normalizing these terms gives and .",
        "Template Sequence Model Here we describe the main component of the model.",
        "An alignment template specifies the allowable alignments between the class sequences and .",
        "is a binary, 0/1 valued matrix which is constructed as follows: If can be aligned to , then ; otherwise .",
        "This process may allow to align with the NULL token , i.e. , so that words can be freely inserted in translation.",
        "Given a pair of class sequences and , we specify exactly one matrix .",
        "We say that is consistent with the target language phrase and the source language phrase",
        "mutations of the source language phrase sequence nous avons une inflation galopante ( ).",
        "if is the class sequence for and is the class sequence for .",
        "In Section 4.1, we will outline a procedure to build a library of alignment templates from bitext word-level alignments.",
        "Each template used in our model has an index in this template library.",
        "Therefore any operation that involves a mapping to (from) template sequences will be implemented as a mapping to (from) a sequence of these indices.",
        "We have described the segmentation and permutation processes that transform a source language sentence into phrases in target language phrase order.",
        "The next step is to generate a consistent sequence of alignment templates.",
        "We assume that the templates are conditionally independent of each other and depend only on the source language phrase which generated each of them We will implement this model using the transducer that maps any permutation of the phrase sequence into a template sequence with probability as in Equation 14.",
        "For every phrase , this transducer allows only the templates that are consistent with with probability , i.e. enforces the consistency between each source phrase and alignment template.",
        "Phrasal Translation Model We assume that a target phrase is generated independently by each alignment template and source phrase This allows us to describe the phrase-internal translation model as follows.",
        "We assume that each word in the target phrase is produced independently and that the consistency is enforced between the words in and the class sequence so that if .",
        "We now introduce the word alignment variables , which indicates that is aligned to within and .",
        "We have assumed that , i.e. that given the template, word alignments do not depend on the source language phrase.",
        "For a given phrase and a consistent alignment template , a weighted acceptor can be constructed to assign probability to translated phrases according to Equations 16 and 17. is constructed from four component machines , ,and , constructed as follows.",
        "The first acceptor implements the alignment matrix .",
        "It has states and between any pair of states and , each arc corresponds to a word alignment variable .",
        "Therefore the number of transitions between states and is equal to the number of non-zero values of .",
        "The arc from state to has probability (Equation 17).",
        "The second machine is an unweighted transducer that maps the index in the phrase to the corresponding word .",
        "The third transducer is the lexicon transducer that maps the source word to the target word with probability",
        "Ney, 2000) and is obtained as .",
        "The fourth acceptor is unweighted and allows all target word sequences which can be specified by the",
        "class sequence .",
        "has states.",
        "The number of transitions between states and is equal to the number of target language words with class specified by .",
        "Figure 5 shows all the four component FSTs for building the transducer corresponding to an alignment template from our library.",
        "Having built these four machines, we obtain as follows.",
        "We first compose the four transducers, project the resulting transducer onto the output labels, and determinize it under the semiring.",
        "This is implemented using AT&T FSM tools as follows fsmcompose O I D C fsmproject o fsmrmepsilon fsmdeterminize .",
        "Given an alignment template and a consistent source phrase , we note that the composition and determinization operations assign the probability (Equation 16) to each consistent target phrase .",
        "This summarizes the construction of a transducer for a single alignment template.",
        "We now implement a transducer that maps sequences of alignment templates to target language word sequences.",
        "We identify all templates consistent with the phrases in the source language phrase sequence .",
        "The transducer is constructed via the FSM union operation of the transducers that implement these templates.",
        "For the source phrase sequence (nous avons une inflation galopante), we show the transducer in Figure 6.",
        "Our example library consists of three templates , and .",
        "maps the source word nous to the target word we via the word alignment matrix specified as .",
        "maps the source word avons to the target phrase have a via the word alignment matrix specified as .",
        "maps the source phrase une inflation galopante to the target phrase run away inflation via the word alignment matrix specified as .",
        "is built out of the three component acceptors , , and .",
        "The acceptor corresponds to the mapping from the template and the source phrase to all consistent target phrases .",
        "Target Language Model We specify this model as where enforces the requirement that words in the translation agree with those in the phrase sequence.",
        "We note that is modeled as a standard backoff trigram language model (Stolcke, 2002).",
        "Such a language model can be easily compiled as a weighted finite state acceptor (Mohri et al., 2002)."
      ]
    },
    {
      "heading": "3 Alignment and Translation Via WFSTs",
      "text": [
        "We will now describe how the alignment template translation model can be used to perform word-level alignment of bitexts and translation of source language sentences.",
        "Given a source language sentence and a target sentence , the word-to-word alignment between the sentences can be found as The variables specify the alignment between source phrases and target phrases while gives the word-to-word alignment within the phrase sequences.",
        "Given a source language sentence , the translation can be found as where is the translation of .",
        "We implement the alignment and translation procedures in two steps.",
        "We first segment the source sentence into phrases, as described earlier",
        "We have described how to compute the optimal segmentation (Equation 18) in Section 2.",
        "The segmentation process decomposes the source sentence into a phrase sequence .",
        "This process also tags each source phrase with its position in the phrase sequence.",
        "We will now describe the alignment and translation processes using finite state operations."
      ]
    },
    {
      "heading": "3.1 Bitext Word Alignment",
      "text": [
        "Given a collection of alignment templates, it is not guaranteed that every sentence pair in a bitext can be segmented into phrases for which there exist the consistent alignment templates needed to create an alignment between the sentences.",
        "We find in practice that this problem arises frequently enough that most sentence pairs are assigned a probability of zero under the template model.",
        "To overcome this limitation, we add several types of “dummy” templates to the library that serve to align phrases when consistent templates could not otherwise be found.",
        "The first type of dummy template we introduce allows any source phrase to align with any single word target phrase .",
        "This template is defined as a triple where and .",
        "All the entries of the matrix are specified to be ones.",
        "The second type of dummy template allows source phrases to be deleted during the alignment process.",
        "For a source phrase we specify this template as .",
        "The third type of template allows for insertions of single word target phrases.",
        "For a target phrase we specify this template as .",
        "The probabilities for these added templates are not estimated; they are fixed as a global constant which is set so as to discourage their use except when no other suitable templates are available.",
        "A lattice of possible alignments between and is then obtained by the finite state composition",
        "where is an acceptor for the target sentence .",
        "We then compute the ML alignment (Equation 19) by obtaining the path with the highest probability, in .",
        "The path determines three types of alignments: phrasal alignment between the source phrase and the target phrase ; deletions of source phrases ; and insertions of target words .",
        "To determine the word-level alignment between the sentences and,we are primarily interested in the first of these types of alignments.",
        "Given that the source phrase has aligned to the target phrase , we look up the hidden template variable that yielded this alignment.",
        "contains the the word-to-word alignment between these phrases."
      ]
    },
    {
      "heading": "3.2 Translation and Translation Lattices",
      "text": [
        "The lattice ofpossible translations of is obtained using the weighted finite state composition: (22) The translation with the highest probability (Equation 20) can now be computed by obtaining the path with the highest score in .",
        "In terms of AT&T FSM tools, this can be done as follows fsmbestpath fsmproject fsmrmepsilon A translation lattice (Ueffing et al., 2002) can be generated by pruning based on likelihoods or number of states.",
        "Similarly, an alignment lattice can be generated by pruning ."
      ]
    },
    {
      "heading": "4 Translation and Alignment Experiments",
      "text": [
        "We now evaluate this implementation of the alignment template translation model."
      ]
    },
    {
      "heading": "4.1 Building the Alignment Template Library",
      "text": [
        "To create the template library, we follow the procedure reported in Och (2002).",
        "We first obtain word alignments of bitext using IBM-4 translation models trained in each translation direction (IBM-4 F and IBM-4 E), and then forming the union of these alignments (IBM-4 ).",
        "We extract the library of alignment templates from the bitext alignment using the phrase-extract algorithm reported in Och (2002).",
        "This procedure identifies several alignment templates that are consistent with a source phrase .",
        "We do not use word classes in the experiments reported here; therefore templates are specified by phrases rather than by class sequences.",
        "For a given pair of source and target phrases, we retain only The translation is the same way as the matrix of alignments that occurs most frequently in the training corpus.",
        "This is consistent with the intended application of these templates for translation and alignment under the maximum likelihood criterion; in the current formulation, only one alignment will survive in any application of the models and there is no reason to retain any of the less frequently occuring alignments.",
        "We estimate the probability by the relative frequency of phrasal translations found in bitext alignments.",
        "To restrict the memory requirements of the model, we extract only the templates which have at most words in the source phrase.",
        "Furthermore, we restrict ourselves to the templates which have a probability for some source phrase ."
      ]
    },
    {
      "heading": "4.2 Bitext Word Alignment",
      "text": [
        "We present results on the French-to-English Hansards translation task (Och and Ney, 2000).",
        "We measured the alignment performance using precision, recall, and Alignment Error Rate (AER) metrics (Och and Ney, 2000).",
        "Our training set is a subset of the Canadian Hansards which consists of French-English sentence pairs (Och and Ney, 2000).",
        "The English side of the bitext had a total of words ( unique tokens) and the French side contained words ( unique tokens).",
        "Our template library consisted of templates.",
        "Our test set consists of 500 unseen French sentences from Hansards for which both reference translations and word alignments are available (Och and Ney, 2000).",
        "We present the results under the ATTM in Table 1, where we distinguish word alignments produced by the templates from the template library against those produced by the templates introduced for alignment in Section 3.1.",
        "For comparison, we also align the bitext using IBM-4 translation models.",
        "We first observe that the complete set of word alignments generated by the ATTM (ATTM-C) is relatively poor.",
        "However, when we consider only those word alignments generated by actual alignment templates (ATTM-A) (and discard the alignments generated by the dummy templates introduced as described in Section 3.",
        "1), we obtain very high alignment precision.",
        "This implies that word alignments within the templates are very accurate.",
        "However, the poor performance under the recall measure suggests that the alignment template library has relatively poor coverage of the phrases in the alignment test set."
      ]
    },
    {
      "heading": "4.3 Translation and Lattice Quality",
      "text": [
        "We next measured the translation performance of ATTM on the same test set.",
        "The translation performance was measured using the BLEU (Papineni et al., 2001) and the NIST MT-eval metrics (Doddington, 2002), and Word Error Rate (WER).",
        "The target language model was a trigram language model with modified Kneser-Ney smoothing trained on the English side of the bitext using the SRILM tookit (Stolcke, 2002).",
        "The performance of the model is reported in Table 2.",
        "For comparison, we also report performance of the IBM-4 translation model trained on the same corpus.",
        "The IBM Model-4 translations were obtained using the ReWrite decoder (Marcu and Germann, 2002).",
        "The results in Table 2 show that the alignment",
        "template model outperforms the IBM Model 4 under all three metrics.",
        "This verifies that WFST implementation of the ATTM can obtain a performance that compares favorably to other well known research tools.",
        "We generate N-best lists from each translation lattice, and show the variation of their oracle-best BLEU scores in Table 3.",
        "We observe that the oracle-best BLEU score",
        "increases with the size of the N-Best List.",
        "We can therefore expect to rescore these lattices with more sophisticated models and achieve improvements in translation quality."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "The main motivation for our investigation into this WFST modeling framework for statistical machine translation lies in the simplicity of the alignment and translation processes relative to other dynamic programming or decoders (Och, 2002).",
        "Once the components of the alignment template translation model are implemented as WFSTs, alignment and translation can be performed using standard FSM operations that have already been implemented and optimized.",
        "It is not necessary to develop specialized search procedures, even for the generation of lattices and N-best lists of alignment and translation alternatives.",
        "The derivation of the ATTM was presented with the intent of clearly identifying the conditional independence assumptions that underly the WFST implementation.",
        "This approach leads to modular implementations of the component distributions of the translation model.",
        "These components can be refined and improved by changing the corresponding transducers without requiring changes to the overall search procedure.",
        "However some of the modeling assumptions are extremely strong.",
        "We note in particular that segmentation and translation are carried out independently in that phrase segmentation is followed by phrasal translation; performing these steps independently can easily lead to search errors.",
        "It is a strength of the ATTM that it can be directly constructed from available bitext word alignments.",
        "However this construction should only be considered an initialization of the ATTM model parameters.",
        "Alignment and translation can be expected to improve as the model is refined and in future work we will investigate iterative parameter estimation procedures.",
        "We have presented a novel approach to generate alignments and alignment lattices under the ATTM.",
        "These lattices will likely be very helpful in developing ATTM parameter estimation procedures, in that they can be used to provide conditional distributions over the latent model variables.",
        "We have observed that that poor coverage of the test set by the template library may be why the overall word alignments produced by the ATTM are relatively poor; we will therefore also explore new strategies for template selection.",
        "The alignment template model is a powerful modeling framework for statistical machine translation.",
        "It is our goal to improve its performance through new training procedures while refining the basic WFST architecture."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank F. J. Och of ISI, USC for providing us the GIZA++ SMT toolkit, the mkcls toolkit to train word classes, the Hansards 50K training and test data, and the reference word alignments and AER metric software.",
        "We thank AT&T Labs - Research for use of the FSM Toolkit and Andreas Stolcke for use of the SRILM Toolkit.",
        "This work was supported by an ONR MURI grant N00014-01-1-0685."
      ]
    }
  ]
}
