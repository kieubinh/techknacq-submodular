{
  "info": {
    "authors": [
      "Kadri Hacioglu",
      "Wayne H. Ward"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics – Short Papers",
    "id": "acl-N03-2009",
    "title": "Target Word Detection and Semantic Role Chunking Using Support Vector Machines",
    "url": "https://aclweb.org/anthology/N03-2009",
    "year": 2003
  },
  "references": [
    "acl-E99-1023",
    "acl-J02-3001",
    "acl-P98-1013",
    "acl-W00-0730"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, the automatic labeling of semantic roles in a sentence is considered as a chunking task.",
        "We define a semantic chunk as the sequence of words that fills a semantic role defined in a semantic frame.",
        "It is straightforward to convert chunking into a tagging task using one of several JOB representations.",
        "Using this representation each word is tagged with I, which means that the word is inside a chunk, or with 0, which means that the word is outside a chunk, or B, which means that the word is the beginning of a chunk.",
        "Tagging can also be seen as a multi-class classification problem.",
        "After recasting the multi-class problem as a number of binary-class problems, we use support vector machines to implement the binary classifiers.",
        "We explore two semantic chunking tasks.",
        "In the first task we simultaneously detect the target word and segments of semantic roles.",
        "In the second task, in addition, we label the semantic segments with their respective semantic role types.",
        "For both tasks, we present encouraging results of experiments carried out using the annotated FrameNet database."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Semantic representation plays a very important role in natural language interfaces between humans and computers.",
        "In simple information query and transaction tasks it is used to understand the user's input and manage the interaction.",
        "For more complex tasks, e.g. a question/answering task, the semantic representation is used to understand the question, to expand the query, to find relevant documents that match the question and to present a summary of multiple documents as the answer.",
        "For limited domain applications, a relatively simple representation of meaning, known as frame based representation, has been succesfully used.",
        "In such cases, one can define a set of frames and slots that covers the task structure and domain objects; e.g. air frame of a travel planning task with slots from_city, to_city, depart_time, depart_date, airline etc.",
        "However, for another task one has to hand-craft a new set of frames and slots.",
        "To overcome the low coverage and poor portability of this approach, we need to define semantic units at a more abstract level and avoid time consuming hand encoding of semantic knowledge.",
        "The steps in creating such a model are",
        "• Decide on the type of semantic knowledge required • Develop a representation to encode it • Prepare annotated data • Design a method to acquire that knowledge by a machine",
        "We are interested in semantic knowledge that allows us capture, represent and understand the \"Who did what to whom, where, when, how and why\" in a sentence.",
        "We believe that the semantic representation by defining abstract domain independet semantic roles like AGENT, EXPE-RIENCER, INSTRUMENT, TIME, LOCATION, GOAL etc.",
        "fulfills our need.",
        "Availability of two databases annotated with roles, namely the FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2002) databases, allow us train a statistical \"meaning extrac-tor\"; it can be a general purpose parser for deep syntactic and semantic analysis, or a chunker for shallow semantic analysis.",
        "In this paper, we focus on shallow analysis and propose a semantic role chunker based on support vector machines (Burges, 1998).",
        "Our work is inspired by the work in (Kudo and Mat-suamto, 2000) for chunking sentences into their syntactic constituents.",
        "We extend their work to the chunking of semantic constituents in a sentence.",
        "This approach is quite",
        "different from a related work that has been proposed in (Gildea and Jurafsky, 2002).",
        "Although we use the same FrameNet semantic representation and train our models on the same database, our method differs in the selection and creation of features, and in the implementation of the classifier."
      ]
    },
    {
      "heading": "2 Description of Chunking Method",
      "text": [
        "The following example is taken from the FrameNet database: Funny joke said [AGENTGarvey] [TARGET clutching] [PATIENT his angel] [LOCATION by the shirt] Chunks are semantic roles that have been filled by one or more words.",
        "Note that one can also consider the label TARGET as a semantic role for the sake of unified discussion.",
        "Using IB02 representation (Sang and Veenstra, 1999), we can formulate two different chunking problems:",
        "Here each word in a sentence is labeled with a tag.",
        "I means that the word is inside a semantic role, 0 means that the word is outside a semantic role, and B means that the word is the beginning of a semantic role.",
        "In the former problem the number of classes is 4; TARGET, I, B, and 0.",
        "The number of classes in the latter problem depends on the number of semantic roles; given N semantic roles (filled with one or more words) and a single word TARGET, the number of classes is 2 * N + 1.",
        "Both tasks can be viewed as multi-class classification problems.",
        "The first problem is the detection of the target word along with the",
        "segmentation of constituents into semantic chunks.",
        "On the other hand, the second problem is a harder problem of detecting the target word, segmenting constituents into chunks andlabeling each chunk with its corresponding semantic role.",
        "We use SVMs for classification.",
        "However, they are binary classifiers.",
        "It is well known that a multi-class problem can be reduced to a number of binary-class problems.",
        "There are several approaches for extending SVMs to mul-ticlass classification problems (Allwein et al., 2000).",
        "In this paper, we explore two approaches.",
        "The first approach is the extension of SVMs to multi-class classification using codes.",
        "Each class is assigned a codeword of l's an -1's of length m. Here, m can be selected equal or greater than the number of classes.",
        "This splits the multi-class data into m binary class data.",
        "Therefore, one can design m SVM classifiers, combine their outputs and minimize a loss function to predict multi-class labels.",
        "One-vs-all, random and error correcting codes can be used for this purpose.",
        "The second approach is the creation of N(N2-1) binary problems by considering all possible pairs of classes, where N is the number of labels in the multi-class problem.",
        "This approach is called the pairwise classification.",
        "Some experiments have been reported that the pairwise approach outperforms the one-vs-all approach (Krebel, 1999).",
        "The next step after the choice of classification method is the selection of features for classification.",
        "As done in (Kudo and Matsuamto, 2000), we derive features from a context that surrounds each word to be tagged.",
        "The features can be the words themselves, their POS tags, their positions in syntactic phrases and semantic roles that have already been classified.",
        "Figure 1 clarifies the notion of context and illustrates the features used.",
        "The 5-word sliding window centered at the current word defines the set of features.",
        "In figure 2, we illustrate two possible implementations for feature extraction.",
        "However, at the moment, we don't know which one is better in terms of coverage, efficiency and accuracy."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": [
        "We carried out experiments with a small fraction of the FrameNet database.",
        "A total of 3000 and 750 sentences are used for training and testing, respectively.",
        "All the experiments are carried out using TinySVM I and the general purpose SVM based chunker YamCha 2• Table 1 summarizes the results for the simpler semantic chunking task using one-vs-all approach.",
        "The results corresponding to the pairwise approach are also presented in Table 2.",
        "Despite the claim in (Krebel, 1999) the pairwise approach has shown slightly inferior performance.",
        "To the best of our knowlege these are the first results on this type of task.",
        "Table 3 shows the results for the second task.",
        "The precision and recall rates for all semantic roles are shown.",
        "The overall precision and recall rates are 67.6% and 55.9%, respectively.",
        "Comparing these to the respective rates of 65.0% and 61.0% in (Gildea and Jurafsky, 2002), in which the target word is assumed known, we find our very first results very promising and encouraging.",
        "Since FrameNet annotates the part of each sentence relevant to a particular target word in question, it is not optimal for training this type of classifier which must segment and classify entire sentences.",
        "We believe that the performance of the system is significantly degraded by this fact.",
        "We will be training on the PropBank corpus, which annotates all targets in a sentence and therefore be more appropriate for this type of mechanism."
      ]
    },
    {
      "heading": "4 Summary and Future Work",
      "text": [
        "We have considered the automatic semantic role labeling as a chunking task and sucessfully ported a SVM based general chunker to our semantic chunking problem with encouraging results.",
        "As a future work, we plan to extend this work in several directions; (i) add new features (e.g. named entities, head words), (ii) use thresholding (e.g. based on information gain) to reduce the dimension of the",
        "feature space (iii) do both forward and backward parsing, and combine their results, and (iv) apply error correcting codes to improve the classification performance."
      ]
    }
  ]
}
