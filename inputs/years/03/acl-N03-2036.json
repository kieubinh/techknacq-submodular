{
  "info": {
    "authors": [
      "Christoph Tillmann",
      "Fei Xia"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics – Short Papers",
    "id": "acl-N03-2036",
    "title": "A Phrase-Based Unigram Model for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/N03-2036",
    "year": 2003
  },
  "references": [
    "acl-C96-2141",
    "acl-J93-2003",
    "acl-P02-1039",
    "acl-P02-1040",
    "acl-W02-1018",
    "acl-W99-0604"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe a phrase-based un-igram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models.",
        "The units of translation are blocks - pairs of phrases.",
        "During decoding, we use a block un-igram model and a word-based trigram language model.",
        "During training, the blocks are learned from source interval projections using an underlying word alignment.",
        "We show experimental results on block selection criteria based on unigram counts and phrase length."
      ]
    },
    {
      "heading": "1 Phrase-based Unigram Model",
      "text": [
        "Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993).",
        "In this paper, we present a similar system with a much simpler set of model parameters.",
        "Specifically, we compute the probability of a block sequence bn1 .",
        "The block sequence probability Pr(bn1) is decomposed into conditional probabilities using the chain rule:",
        "We try to find the block sequence that maximizes Pr(bn1 ): bn1 = arg maxb1 Pr(bn1).",
        "The model proposed is a joint",
        "model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly.",
        "The approach is illustrated in Figure 1.",
        "The source phrases are given on the x-axis and the target phrases are given on the y-axis.",
        "The two types of parameters in Eq 1 are defined as:",
        "• Block unigram model p(bi): we compute unigram probabilities for the blocks.",
        "The blocks are simpler than the alignment templates in (Och et al., 1999) in that they do not have any internal structure.",
        "• Trigram language model: the probability",
        "p(bilbi-1) between adjacent blocks is computed as the probability of the first target word in the target clump of bi given the final two words of the target clump of bi-1.",
        "The exponent a is set in informal experiments to be 0.5.",
        "No other parameters such as distortion probabilities are used.",
        "To select blocks b from training data, we compute unigram block co-occurrence counts N(b).",
        "N(b) cannot be",
        "learned from projecting three source intervals.",
        "The right picture shows three blocks that cannot be obtain from source interval projections.",
        "computed for all blocks in the training data: we would obtain hundreds of millions of blocks.",
        "The blocks are restricted by an underlying word alignment.",
        "The word alignment is obtained from an HMM Viterbi training (Vogel et al., 1996).",
        "The HMM Viterbi training is carried out twice with English as target language and Chinese as source language and vice versa.",
        "We take the intersection of the two alignments as described in (Och et al., 1999).",
        "To generate blocks from the intersection, we proceed as follows: for each source interval [j, j'], we compute the minimum target index i and maximum target index i' of the intersection alignment points that fall into the interval [j, j'].",
        "The approach is illustrated in Figure 2.",
        "In the left picture, for example, the source interval [1, 3] is projected into the target interval [1, 3] .",
        "The pair ([j, j'], [i, i']) together with the words at the corresponding positions yields a block learned from this training sentence pair.",
        "For source intervals without alignment points in them, no blocks are produced.",
        "We also extend a block corresponding to the interval pair ([j, j'] , [i, i']) by elements on the union of the two Viterbi HMM alignments.",
        "A similar block selection scheme has been presented in (Och et al., 1999).",
        "Finally, the target and source phrases are restricted to be equal or less than 8 words long.",
        "This way we obtain 23 millions blocks on our training data including blocks that occur only once.",
        "This baseline set is further filtered using the unigram count N(b): Nk denotes the set of blocks b for which N(b) > k. Blocks where the target and the source clump are of length 1 are kept regardless of their count.1 We compute the unigram probability p(b) as relative frequency over all selected blocks.",
        "We also tried a more restrictive projection scheme: source intervals are projected into target intervals and the reverse projection of the target interval has to be included in the original source interval.",
        "The results for this symmetrical projection are currently worse, since some blocks with longer target intervals are excluded.",
        "An example of 4 blocks obtained from the training data is shown in",
        "Figure 3.",
        "’$DATE’ is a placeholder for a date expression.",
        "Block b4 contains the blocks b1 to b3.",
        "All 4 blocks are selected in training: the unigram decoder prefers b4 even if b1,b2, and b3 are much more frequent.",
        "The solid alignment points are elements from the intersection, the striped alignment points are elements from the union.",
        "Using the union points, we can learn one-to-many block translations; for example, the pair (c1,’Xinhua news agency’) is learned from the training data.",
        "We use a DP-based beam search procedure similar to the one presented in (Tillmann, 2001).",
        "We maximize over all block segmentations bn1 for which the source phrases yield a segmentation of the input source sentence, generating the target sentence simultaneously.",
        "In the current experiments, decoding without block reordering yields the best translation results.",
        "The decoder translates about 180 words per second."
      ]
    },
    {
      "heading": "2 Experimental Results",
      "text": [
        "The translation system is tested on a Chinese-to-English translation task.",
        "The training data come from several news sources.",
        "For testing, we use the DARPA/NIST MT 2001 dry-run testing data, which consists of 793 sentences with 20,333 words arranged in 80 documents.",
        "The training data is provided by the LDC and labeled by NIST as the Large Data condition for the MT 2002 evaluation.",
        "The Chinese sentences are segmented into words.",
        "The training data contains 23.7 million Chinese and 25.3 million English words.",
        "Experimental results are presented in Table 1 and Table 2.",
        "Table 1 shows the effect of the unigram threshold.",
        "The second column shows the number of blocks selected.",
        "The third column reports the BLEU score (Papineni et al., 2002) along with 95% confidence interval.",
        "We use IBM 2We did not use the first 25 documents of the 105-document dry-run test set because they were used as a development test set before the dry-run and were subsequently added to our training data.",
        "Target Source selected from the training data.",
        "Longer phrases which occur less frequently do not help much.",
        "Model 1 as a baseline model which is similar to our block model: neither model uses distortion or alignment probabilities.",
        "The best results are obtained for the N2 and the N3 sets.",
        "The N3 set uses only 1.22 million blocks in contrast to N2 which has 4.23 million blocks.",
        "This indicates that the number of blocks can be reduced drastically without affecting the translation performance significantly.",
        "Table 2 shows the effect of the maximum phrase length on the BLEU score for the N2 block set.",
        "Including blocks with longer phrases actually helps to improve performance, although length 4 already obtains good results.",
        "We also ran the N2 on the June 2002 DARPA TIDES Large Data evaluation test set.",
        "Six research sites and four commercial off-the-shelf systems were evaluated in Large Data track.",
        "A majority of the systems were phrase-based translation systems.",
        "For comparison with other sites, we quote the NIST score (Doddington, 2002) on this test set: N2 system scores 7.44 whereas the official top two systems scored 7.65 and 7.34 respectively."
      ]
    },
    {
      "heading": "3 Conclusion",
      "text": [
        "In this paper, we described a phrase-based unigram model for statistical machine translation.",
        "The model is much simpler than other phrase-based statistical models.",
        "We experimented with different restrictions on the phrases"
      ]
    },
    {
      "heading": "Acknowledgment",
      "text": [
        "This work was partially supported by DARPA and monitored by SPAWAR under contract No.",
        "N66001-99-28916."
      ]
    }
  ]
}
