{
  "info": {
    "authors": [
      "Hua Yu",
      "Tanja Schultz"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics – Short Papers",
    "id": "acl-N03-2038",
    "title": "Implicit Trajectory Modeling Through Gaussian Transition Models for Speech Recognition",
    "url": "https://aclweb.org/anthology/N03-2038",
    "year": 2003
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Hence data sufficiency becomes a concern.",
        "In our experiments, we choose to model only frequent transitions.",
        "For everything else, we revert to the traditional HMM-GMM model: a2j = ,,j."
      ]
    },
    {
      "heading": "3.2 Pruning",
      "text": [
        "Even in conventional HMM training, it is common to ignore transition probabilities.",
        "Their contribution to the overall score is quite small, in comparison to observation probabilities in a continuous HMM (which is several orders of magnitude larger).",
        "The same is true for Gaussian transition probabilities.",
        "While GTM offers better discrimination between trajectories, all trajectories are nonetheless still permitted.",
        "Pruning unlikely transitions leads to a model that is both more compact and more prudent.",
        "In reality, however, we need to exercise great care in pruning so as not to prune away unseen trajectories (due to a limited training set)."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "Experiments are carried out on the Switchboard",
      "text": [
        "(SWB) task using the Janus system (Soltau et al., 2002).",
        "The test set is a 1 hour subset of the 2001 Hub5e evaluation set.",
        "Acoustic training uses a 66 hours subset of the SWB data.",
        "We use a 15k vocabulary and a trigram language model trained on SWB and CallHome.",
        "The front-end has vocal tract length normalization, cepstral mean normalization, an 11- frame window to derive delta and double-delta, linear discriminant analysis and semi-tied covariance with a single class.",
        "The acoustic model has roughly 6000 mixtures with a total of 86K Gaussians, on average 14 Gaussians per model.",
        "We apply a two-tiered strategy to cope with the data sufficiency issue.",
        "Before training, we count the number of transitions for each model pair on the training data, using Viterbi alignment.",
        "Only transitions with counts above a certain threshold are modeled with GTM.",
        "Of about 6000 mixture models, a total of 40K model pairs (out of a potential 6K x 6K=36M) has been observed.",
        "It turns out that most of the transitions (72%) are within the same model (corresponding to self-loop in HMM).",
        "We choose to model the most frequent 9400 model pairs with GTM.",
        "Not surprisingly, most of the 6000 same-model pairs are among those chosen.",
        "During training, we also apply a minimum count criterion: a transition model is updated only if the Gaussian receives enough training counts.",
        "One iteration of Baum-Welch training gives significant improvement in term of likelihood.",
        "Log likelihood per frame improves from – 50.67 to – 49.18, while conventional HMM training can only improve less than 0.1.",
        "Considering the baseline acoustic model has already been highly optimized, this indicates improved acoustic modeling.",
        "GTM transitions are pruned if their probabilities fall below a certain threshold (default is 1e-5).",
        "Table 1 shows word error rates for GTM models pruned"
      ]
    },
    {
      "heading": "5 Future Work",
      "text": [
        "In this paper, we have presented Gaussian Transition Model, a new approach to model trajectories within the HMM framework.",
        "Preliminary experiments have shown encouraging improvements.",
        "There are several possibilities for further improvements.",
        "First, when modifying the decoder to use GTM, we used Viterbi approximation at word boundaries, which means trajectory information is lost upon word transition.",
        "Second, we plan to extend GTM to handle deletions in sloppy speech, a major challenge in LVCSR."
      ]
    }
  ]
}
