{
  "info": {
    "authors": [
      "Akiko Aizawa"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P03-1049",
    "title": "Analysis of Source Identified Text Corpora: Exploring the Statistics of the Reused Text and Authorship",
    "url": "https://aclweb.org/anthology/P03-1049",
    "year": 2003
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper aims at providing a view of text recycled, within a short time, by the authors themselves.",
        "We first present a simple and general method for extracting reused term sequences, and then analyze several author-identified text collections to compare the statistical quantities.",
        "The ratio of recycling is also measured for each collection.",
        "Finally, related research topics arc introduced together with some discussion of future research directions.",
        "I Introduction In conventional information retrieval studies, the similarity between two documents is calculated based on the distribution of terms that appear in each document.",
        "However, in document databases, or on the Web, there exist numbers of documents that literally contain the same phrases.",
        "These documents not only maintain",
        "• good statistical resemblance but also share • long section of terms, sometimes spread over sentences.",
        "When the degree of the match is beyond the level of a simple coincidence, it is a natural consequence that these sections of terms arc duplicated and reused by the authors.",
        "Furthermore, we can assume that, in this digital age, this type of `recycling' is an ordinal practice when authoring text-based products because texts arc easily copied and reused.",
        "Another important aspect is that the reused texts arc often semantically meaningful; their survival across documents itself is an evidence of their usefulness.",
        "For example, some expressions contain the definitions of named entities that arc shared between the two documents.",
        "It should be emphasized here that the statistical similarity and the term sequence matching arc strongly associated, but essentially different, phenomena.",
        "The former is derived from the topical relationship between the two documents, whereas the author's editing, revising, or quoting a document, indicating some form of `social' relatedness, causes the latter.",
        "However, there have.",
        "been few attempts, to date, to analyze text corpora explicitly focusing on the reuse and reusability issues.",
        "Based on the above.",
        "observations, this paper aims at establishing a methodological basis for extracting featured term sequences reused in a group of documents.",
        "First, we define the following three types that correspond to distinctive reuse patterns of term sequences.",
        "Compounds and phrases.",
        "Permanent lexicon and idiomatic expressions that arc frequently and universally used in texts.",
        "Instantly lexiconized tuts.",
        "Passages and conventional expressions that arc only temporarily and locally reused.",
        "Reusable without credits to the authors, also referred to as instant lexicon.",
        "(3) Quoted tuts.",
        "Passages that arc attributed to a particular author.",
        "When used by other authors, usually copied with credits, also referred to as authored tuts.",
        "Note that we consider only the designated `writer' of the target text here.",
        "Issues in identifying a copyright holder of a specific text arc outside of the scope of this paper.",
        "While terms and compounds have.",
        "long been a central issue of natural language processing studies, little attention has been paid to the extraction and utilization of longer passages, namely, the instant lexicon and the authored tuts as previously defined.",
        "Nevertheless, these arc the featured text elements that arc most strongly related to particular topics or authors, and therefore could be useful resources in various text processing applications, such as authorship identification, duplication checking, document clustering and summarization.",
        "Because the exploration in this direction has just started, in this paper we limit our focus to the following three issues.",
        "First, in section 2, we present an efficient method for extracting reused term sequences together with the corresponding document subsets.",
        "Special attention is paid to make.",
        "the method simple and general so that it is easily applicable to wide variety of text resources.",
        "Next, in section 3, some analytical results arc reported where the proposed method was applied to several text collections and the statistical natures were compared.",
        "Finally, in section 4, we introduce related research topics and discuss the utilization of the proposed method in connection with existing text retrieval applications."
      ]
    },
    {
      "heading": "2 Suffix Tree based Clustering",
      "text": []
    },
    {
      "heading": "2.1 Definition of ST-Clusters",
      "text": [
        "Denote all the documents in the target corpus as D, all the terms in the target corpus as W. The ii7ord n-,gram (n > 1) is a sequence of n terms given by:",
        "Next, consider a suffix tree, each node of which corresponds to a distinctive.",
        "word n-gram observed in D. For every node on the tree, there exists an uniquely determined subset Sd(wi ) (C D) given as a subset of all the documents that contain wi .",
        "In other words, wi is a sequence of terms that is shared between Sd(wi ).",
        "Noting that multiple nodes may refer to the same document subset, we define a suffix tree based cluster (ST-cluster) as a subset of nodes on the suffix tree that is mapped to the same document set.",
        "Namely,",
        "For example, in Figure 1, nodes A and B both refer to subset { DOC#10, DOC#13 } and arc therefore merged into a single ST-cluster."
      ]
    },
    {
      "heading": "2.2 Procedure for ST-Clustering",
      "text": [
        "The basic procedure.",
        "for extracting ST-clusters is similar to that used by Zamir & Etzioni (1998) and is summarized as follows: Convert the.",
        "target collection into sequences of terms, each of which corresponds to a single.",
        "document.",
        "Apply morphological analysis or other preprocessing methods when it is necessary to determine the word boundaries.",
        "Neither stemming nor normalization is applied.",
        "(2) Generate a suffix array by a single sort.",
        "Suffix tree nodes, together with their corresponding document subset lists, arc then",
        "identified as ad✔acent members of the su✖x array.",
        "For each node, sort the document list according to some predetermined order.",
        "✘ort all the su✖x tree nodes using the sorted document list as a ✒ey.",
        "Then, the ad✔acent members of the node list with the same ✒ey constitute a single ✘T-cluster.",
        "The computation time of the above.",
        "procedure is basically determined by the sort operation in",
        "with the power of today's computers.",
        "What we found more problematic is the cost of memory to store all the su✖x tree nodes and the corresponding document lists at step (2).",
        "Figure 2 shows the count statistics for different levels of the su✖x tree generated from the ☛euters collection, which is also used in our later experiments.",
        "Based on the figure, it becomes clear that short length ❘-grams arc the most memory consuming.",
        "Because our focus is restricted to longer ❘-grams, in this paper we consider only the su✖x tree nodes with longer than four term sequences.",
        "number of the pointers",
        "✕✵✛ M✺a❑❏▲✺❑ ❇❄▲ ❈❊❋●❍❏❑❂✺▲❑ The.",
        "✘T-clusters generated arc evaluated using the.",
        "following two measures.",
        "First is the.",
        "term se❙ quen✝e ✝o☎n✝☎den✝e that quantifies the strength of the.",
        "coincidence.",
        "of the.",
        "extracted term sequences.",
        "✘econd is the.",
        "term d☎str☎❦ut☎on s☎m❙ ☎lar☎ty that calculates the.",
        "divergence.",
        "of the.",
        "documents in the.",
        "cluster based on the.",
        "conventional document similarity measure..",
        "(1) Term sequence.",
        "coincidence.",
        "The.",
        "coincidence.",
        "score.",
        "of term sequence.",
        "wi is calculated as the.",
        "specific mutual information M(wl) given, by definition, as:",
        "That is, M(al) is the difference between (i) the entropy calculated based on the assumption that the k terms (wl, • • • , wk) occurred independently, and (ii) the entropy calculated based on the actual observation.",
        "Intuitively, M(al) becomes greater for longer sequences.",
        "However, the scheme is different from simply counting the length of the sequence because it puts more weight on low frequency terms.",
        "In our preliminary experiments, we compared two different ran✒ings of the ✘T-clusters using M(al) and the sequence length, and observed the former has a better correlation with the term distribution similarity.",
        "The occurrence probability P(wi) in Eq.",
        "(2) is simply determined by f req(wi ), the frequency of wi in ◆, and the overall total frequency F, given as F = E,u,,cW freq(w❛), as follows:",
        "The occurrence probability of w❛ is also determined by Eq.",
        "(✌), considering that w❛ is a unit length sequence of terms.",
        "Because probability estimation of unobserved terms is not an issue here, we have, not applied any discounting or smoothing methods for simplicity, unli✒e.",
        "many language-modeling studies.",
        "The coincidence score is calculated for every term sequence in the ✘T-cluster, and then, either the maximum or the total value is used as an overall evaluation, depending on the purpose of the analysis.",
        "In this paper, we consistently use the maximum values.",
        "(2) Term distribution similarity",
        "The document similarity of the ✘T-cluster is defined using the cosine similarity commonly used in information retrieval studies.",
        "For each document d in the cluster, index terms arc first extracted by applying standard methods, such as morphological analysis, stemming and stop word removal.",
        "Then, the term vector d is generated for each document using ④❙☎d④ weighting number of the pointers to store the suffix tree nodes at level k number of the pointers to store the suffix tree clusters at level k",
        "scheme.",
        "In addition, the central vector of the cluster, denoted as c, is calculated as an average of all the term vectors.",
        "Next, the cosine similarities between the central and each term vector are obtained.",
        "Finally, the averaged pairwise similarity values becomes the overall evaluation of the term distribution similarity of the ST-cluster:",
        "Note that 0 < Sim(D) < 1, and the value becomes closer to one for the ST-cluster where the documents are statistically similar to each other."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": []
    },
    {
      "heading": "3.1 Target Corpora",
      "text": [
        "The six text collections used in our experiments arc shown in Table, 1.",
        "We used two sets of English newspaper articles extracted from either Reuters (Reuters, 2000) or San Jose.",
        "Mercury (SJM) (Harman & Mark, 1993), two sets of Japanese.",
        "newspaper articles extracted from either Mainichi (Mainichi, 2001) or NIKKEI (Nikkei, 2001), and two sets of Japanese.",
        "academic papers' abstracts both extracted from NTCIR-1 (NTCIR, 2001), one.",
        "presented at the.",
        "Information Processing Society in Japan (ntc-IPSJ), and the.",
        "other at the.",
        "Japan Society of Civil Engineers (ntc-JSCE).",
        "For the.",
        "newspaper articles, we selected only articles with their authors specified, either in the, `byline' (in the.",
        "case.",
        "of Reuters and SJM) or embedded in the.",
        "text in a particular form (in the.",
        "case.",
        "of Mainichi and NIKKEI).",
        "Morphological analyzer ChaSen was used for Japanese.",
        "text (Matsumoto, 2001).",
        "For each collection, ST-clusters with term sequences longer than four were.",
        "enumerated using the.",
        "method described in 3.2.",
        "The.",
        "numbers of resulting clusters and the.",
        "corresponding execution time.",
        "measured on 2.8GMHz Xcon/Linux arc also shown in Table, 1.",
        "(Note.",
        "that for comparison purpose., the.",
        "execution time.",
        "does not include.",
        "the, time.",
        "for morphological analysis and word dictionary generation.)",
        "For reference,, we have, also segmented the.",
        "target collections into sentences, and calculated the.",
        "average coincidence.",
        "score.",
        "together with the.",
        "average number of terms per a sentence.."
      ]
    },
    {
      "heading": "3.2 Experiment 1: Measuring the",
      "text": [
        "instantly lexiconized texts In our first experiment, we examine.",
        "the.",
        "distribution of the.",
        "coincidence.",
        "score.",
        "of term sequences that were.",
        "reproduced accidentally without referring to the original document.",
        "For this purpose, we first made.",
        "mixtures of the, two sources: (a) Reuters and SJM, (b) Mainichi and Nikkei, and (c) ntc-IPSJ and ntc-JSCE.",
        "Next, we applied the.",
        "ST-clustering to the.",
        "generated mixtures.",
        "Then, ST-clusters that contain documents from both collections were selected and term sequences with the.",
        "maximum coincidence.",
        "score, were examined.",
        "Note.",
        "that the.",
        "pairs were arranged so that both collections belong to the.",
        "same.",
        "type.",
        "(i.e., either newspaper stories or academic papers' abstracts) but originating from different publication sources (i.e., different newspaper companies or academic societies).",
        "The.",
        "topical overlap was also kept small, either by choosing collections of different years, in the case of newspaper articles, or by focusing on different academic fields, in the.",
        "case.",
        "of papers' abstracts.",
        "Figure.",
        "3 shows the.",
        "normalized histograms (i.e., empirical p.d.f.)",
        "of the.",
        "coincidence.",
        "score.",
        "of the three different mixed pairs.",
        "The maximum score is shown in Table 2, together with their length in the parenthesis.",
        "Also shown in the table is the 95% threshold value, such that 9✂☎ of the extracted ST-clusters have.",
        "smaller values than the threshold value.",
        "These results show that the coincidence score is reasonably consistent for all three pairs; between 100 ✝ 200 at the maximum, and below ✂0 for the 9✂☎ threshold value.",
        "Generally, term sequences beyond this value become candidates for the instant lexicon, indicating some topical relatedness between the documents.",
        "However, it should be noted that the threshold cannot be used for discriminating relevant and irrelevant documents because documents without a sequence match can still be similar to each other.",
        "It is also important to note that the extracted sequences mostly represent semantically meaningful piece of information.",
        "The top three examples by Reuters and SJM pairs were (i) ▲Kate, Larry, Mindy, Nicholas, Odette, Peter, Rose, Sam, Teresa,◆ (score 162.6; a hurricane name list), (ii) ▲said David Jones, chief economist at Aubrey G. Lanston & Co.◆ (score 124.8), and (iii) ▲the Golan Heights, which Israel captured from Syria in the 196❘ Middle◆ (score 113.6).",
        "We expect that these term sequences arc useful in capturing the topical relations between the documents in the same cluster, but this aspect of the ST-cluster is left for future study."
      ]
    },
    {
      "heading": "3.3 Experiment 2: Measuring the",
      "text": [
        "authored texts In our second experiment, we grouped the.",
        "extracted ST-clusters into the.",
        "following two groups: (i) unique ST-clusters arc composed of articles by the.",
        "same.",
        "author (i.e.., at least one.",
        "of the.",
        "authors is common for all the.",
        "articles), (ii) mixed ST-clusters arc composed of articles by different authors (i.e.., none.",
        "of the.",
        "authors is common for all the.",
        "articles).",
        "Figure.",
        "4 is the.",
        "result for Reuters, Mainichi, and ntc-IPSJ.",
        "The.",
        "left and the.",
        "right columns show the.",
        "relationship between the, term distribution similarity and the.",
        "term sequence.",
        "coincidence.",
        "for the.",
        "mixed and unique.",
        "ST-clusters respectively, where.",
        "each point corresponds to a distinctive.",
        "cluster.",
        "The.",
        "middle.",
        "column shows the.",
        "ratio of the, two types of the.",
        "ST-clusters against a different coincidence.",
        "score.. For reference., the.",
        "average score.",
        "of a single.",
        "sentence.",
        "is also shown as a dotted vertical line, motivated by a naive.",
        "heuristic that a whole.",
        "sentence.",
        "is unlikely to be repeated by chance.. For the.",
        "purpose.",
        "of readability, only a month's statistics, January 199❘, is shown for Reuters (due.",
        "to the.",
        "large.",
        "size.).",
        "In addition, mixed ST-clusters of documents with the.",
        "same.",
        "date, were excluded because, we found this case.",
        "contains many miss-identifications, specifically with a series of academic papers co-authored by many researchers.",
        "Based on the.",
        "Figure.",
        "4, it becomes clear that the.",
        "three.",
        "collections behave.",
        "differently in terms of the.",
        "author structure.",
        "of the.",
        "texts.",
        "With Reuters, the.",
        "distinction between the, mixed and unique.",
        "ST-clusters is not obvious.",
        "Considerable.",
        "numbers of articles share.",
        "exactly the.",
        "same.",
        "sentences even when their designated reporters arc different.",
        "Correspondingly, the.",
        "changes of the.",
        "two ratio curves become.",
        "slow with Reuters.",
        "On the.",
        "other hand, with Mainichi and ntc-IPSJ, there.",
        "exist only a few ST-mixed clusters",
        "with high coincidence scores.",
        "Accordingly, the ratio curves for these collections become steeper.",
        "In addition, with the Mainichi corpus, the ST-unique clusters are divided into two groups on the graph.",
        "On further examination, the clusters in the upper-right region were found to contain different local editions (Tokyo and Osaka) of the same overseas stories sent by the same reporters.",
        "Except for these particular cases, the coincidence score of the ST-unique cluster with the Mainichi collection is relatively low compared with the other two collections.",
        "For the ntc-IPSJ, exceptional cases were found where a series of papers were presented on the same day but in the name of different authors.",
        "However, these cases are excluded from the figure, as have.",
        "been already described, and cannot be seen in the figure.",
        "The 9✂☎ threshold value of the mixed ST-clusters was 90❘.4 (84) for Reuters, 164.2 (20) for Mainichi, and 110.2 (1✂) for ntc-IPSJ, where figures in the parenthesis are the length of the term sequences on the border.",
        "Compared with the previous case shown in Table 2, the values vary considerably across the different collections.",
        "To further clarify the difference, we also examined the influence of the time deviation on the threshold values.",
        "This time, we selected only ST-mixed clusters whose time deviation is greater than t, where t varied between 0 ✝ 30 days, and calculated the threshold values for each collection.",
        "Based on the result shown in Figure ✂, it becomes clear that the text reuse with Reuters is more related to the date of the stories.",
        "Only limited length of term sequences were reproduced after an interval of several days.",
        "In summary, the reuse pattern of the authored texts varies across different media and publication styles.",
        "With Reuters, the authored texts arc generally more likely to be associated with a specific event that occurs within a short period of time.",
        "On the other hand, with Mainichi, the texts arc instead connected to individual articles of the day, while with ntc-IPSJ, they arc simply associated with individual authors or author groups."
      ]
    },
    {
      "heading": "3.4 Experiment 3: Measuring the reuse",
      "text": [
        "ratio In our final experiment, we measured the.",
        "degree.",
        "of `recycling' with Reuters,, Mainichi, and ntc-IPSJ.",
        "We calculated the.",
        "ratio of term sequences that appeared for more.",
        "than a second time.",
        "in the.",
        "collection, with their coincidence.",
        "score.",
        "being greater than a given threshold c. (Their first appearance was not counted in the.",
        "number.)",
        "The.",
        "value.",
        "of c was varied 10 G c G 600.",
        "Figure.",
        "6 shows the.",
        "result.",
        "The.",
        "result shows that the.",
        "reuse.",
        "ratio rapidly decreases for c smaller than 50, and then becomes flat for c greater than 100.",
        "We noted that the.",
        "changes correspond approximately to the.",
        "borders of the.",
        "instant lexicon and authored texts in the.",
        "previous experiments, but the.",
        "details arc left for future.",
        "investigation.",
        "Finally, although the.",
        "purpose.",
        "of the.",
        "experiment is not to compare.",
        "the.",
        "reuse.",
        "ratio of these.",
        "particular documents, the.",
        "figures show that the.",
        "ratio is not negligible.",
        "for standard collections.",
        "The.",
        "value.",
        "could be much higher in environments such as the.",
        "Web."
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "Although the analysis of reused text discussed in this paper only captures a particular aspect of text dissemination, related studies arc found in several different fields of information retrieval.",
        "(1) Authorship identification",
        "There exist a group of studies concerning the identification of authors (Tony & Michael, 2000).",
        "Those stylometric studies quantify the `style' of a particular author using a combination of various statistical measures, such as the sentence lengths or vocabulary richness.",
        "In recent studies, word n-gram information (N = 3 – 10) is commonly used also.",
        "However, past studies mostly focused on extracting discriminators for indicating authors of older, disputed literature archives.",
        "It is only recently that the stylomctric scheme has been applied to identify anonymous authors (for example, Tsuboi & Matsumoto, 2002).",
        "Although the proposed scheme is relevant to copyright issues, the objective is not to detect illegal reprinting intentionally disguised by the authors.",
        "Instead, the scheme could be better used to prevent unintentional violation of copyright by inexperienced authors.",
        "(2) Duplicate document detection A related research topic is duplicate document detection.",
        "The topic has become specifically important in recent years due to the explosive.",
        "increase of documents on the Internet.",
        "Chowdhury et al.",
        "(2000) categorize the conventional text-based duplicate detection techniques into the following two types: The first is shingling techniques where sets of `shingles', typically contiguous terms, arc compared for duplicate detection (Broder et.al., 1997; Chowdhury, et.al., 2002).",
        "The second is similarity measure calculation where the term distribution similarity is used to detect potential duplicates (Molina, et.al, 1996; Sanderson, 1997).",
        "Although most studies allow minor syntactic variations, the duplication is detected for entire documents or Web sites.",
        "Because the proposed scheme is focused on partial duplications, it could be used as a complementary measure to improve the flexibility of the duplication check."
      ]
    },
    {
      "heading": "(3) Document clustering",
      "text": [
        "There also exist studies that generate clusters based on phrases shared between documents.",
        "Suffix Tree Clustering (STC), proposed for on-the-fly reorganization of the search results on the Web, is an example close to our approach (Zamir & Etzioni, 1998).",
        "Although both STC and our methods exploit suffix tree structure to realize efficient clustering, the adaptations are slightly different.",
        "Because the objective of STC is to create semantically associated document clusters, stemming and sentence level segmentation were applied at the preprocessing stage, term sequences longer than six were penalized with equal weights, and the extracted `base clusters' are further integrated into larger clusters.",
        "Because our focus is on the exact term sequence match, we analyze directly the `base clusters' extracted from the entire text collections.",
        "Finally, future research directions are as follows.",
        "First, the issue of quantifying the authorship of anonymous texts should be further explored, because the interpretation may depend on various factors including the language, the media, the editing policy, or the subject field.",
        "The proposed analytical method could be a promising tool to explore different types of textual resources, including Web documents, XML-based databases, or program source codes.",
        "The second potential research topic is the rapid detection of partially duplicated texts as well as the automatic generation of embedded text anchors using the proposed clustering method.",
        "The third issue concerns the extraction of event-specific expressions that can be utilized further in summarizing the contents of the cluster.",
        "The last issue also requires such techniques as re-segmentation and interpolation of terms, and automatic detection of media-specific expressions.",
        "In addition, it is important to develop a refined language-based method of identifying and classifying quoted descriptions that appear in the texts."
      ]
    }
  ]
}
