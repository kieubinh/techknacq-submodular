{
  "info": {
    "authors": [
      "John C. Henderson"
    ],
    "book": "Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond",
    "id": "acl-W03-0306",
    "title": "Word Alignment Baselines",
    "url": "https://aclweb.org/anthology/W03-0306",
    "year": 2003
  },
  "references": [
    "acl-J93-1004",
    "acl-P00-1056",
    "acl-P91-1022",
    "acl-W96-0201"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Simple baselines provide insights into the value of scoring functions and give starting points for measuring the performance improvements of technological advances.",
        "This paper presents baseline unsupervised techniques for performing word alignment based on geometric and word edit distances as well as supervised fusion of the results of these techniques using the nearest neighbor rule."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Simple baselines provide insights into the value of scoring functions and give starting points for measuring the performance improvements of technological advances.",
        "This paper presents baseline unsupervised techniques for performing word alignment based on geometric and word edit distances as well as supervised fusion of the results of these techniques using the nearest neighbor rule."
      ]
    },
    {
      "heading": "2 Alignment as binary classification",
      "text": [
        "One model for the task of aligning words in a left-hand-side (LHS) segment with those in a right-hand-side (RHS) segment is to consider each pair of tokens as a potential alignment and build a binary classifier to discriminate between correctly and incorrectly aligned pairs.",
        "Any of n source language words to align with any of m target language words, resulting in 2nm possible alignment configurations.",
        "This approach allows well-understood binary classification tools to address the problem.",
        "However, the assumption made in this approach is that the alignments are independent and identically distributed (IID).",
        "This is false, but the same assumption is made by the alignment evaluation metrics.",
        "This approach also introduces difficulty in incorporating knowledge of adjacency of aligned pairs, and HMM approaches to word alignment show that this knowledge is important (Och and Ney, 2000).",
        "All of the techniques presented in this work approach the problem as a binary classification task."
      ]
    },
    {
      "heading": "2.1 Random baseline",
      "text": [
        "A randomized baseline was created which flips a coin to mark alignments.",
        "The bias of the coin is chosen to maximize the F-measure on the trial dataset, and the resulting performance gives insight into the inherent difficulty of the task.",
        "If the categorization task was balanced, with exactly half of the paired tokens being marked as aligned, then the precision, recall, and F-measure of the coin with the best bias would have all been 50%.",
        "The preponderance of non-aligned tokens shifted the F-measure away from 50%, to the 5-10% range, suggesting that only about 10% of the pairs were aligned.",
        "An aligner performing worse than this baseline would perform better by inverting its predictions."
      ]
    },
    {
      "heading": "3 Unsupervised methods",
      "text": [
        "There are a number of alignment techniques that can be used to align texts when one lacks the benefit of a large aligned corpus.",
        "These unsupervised techniques take advantage of general knowledge of the language pair to be aligned.",
        "Their relative simplicity and speed allow them to be used in places where timeliness is of utmost importance, as well as to be quickly tuned on a small dataset."
      ]
    },
    {
      "heading": "3.1 Final punctuation",
      "text": [
        "Many LHS segments end in a punctuation mark that is aligned with the final punctuation of the corresponding RHS.",
        "A high precision aligner that marks only that alignment is useful for debugging the larger alignment system."
      ]
    },
    {
      "heading": "3.2 Length ratios",
      "text": [
        "Short words such as stop words tend to align with short words and long words such as names tend to align with long words.",
        "This weak hypothesis is worth pursuit because a similar hypothesis was useful for aligning sentences (Gale and Church, 1991; Brown et al., 1991).",
        "The observation can be codified as a distance between the word at position i on the LHS and the word at position j on the RHS",
        "where L(li) is the length of the token at position i on the LHS.",
        "Note that Dle71, is similar to a normalized harmonic mean, ranging from 0 to 1.0, with the minimum achieved when the lengths are the same.",
        "A threshold on Dle71, is used to turn this distance metric into a classification rule."
      ]
    },
    {
      "heading": "3.3 Edit distances",
      "text": [
        "The language pairs in the experiments were drawn from Western languages, filled with cognates and names.",
        "An obvious way to start finding cognates in languages that share character sets is by comparing the edit distance between words.",
        "Three word edit distances were investigated, and thresholds tuned to turn them into classification rules.",
        "Dexact indicates exact match with a zero distance and a mismatch with value of 1.0.",
        "Dwedit is the minimum number of character edits (insertions, deletions, substitutions) required to transform one word into another, normalized by the lengths.",
        "It can be interpreted as an edit distance rate, edits per character:",
        "Dlcedit is the same as Dwedit, except both arguments are lower-cased prior to the edit distance calculation."
      ]
    },
    {
      "heading": "3.4 Dotplot geometry",
      "text": [
        "Geometric approaches to bilingual alignment have been used with great success in both finding anchor points and aligning sentences (Fung and McKeown, 1994; Melamed, 1996).",
        "Three distance metrics were created to incorporate the knowledge that all of the aligned pairs use roughly the same word order.",
        "In every case, the distance of the pair of words from a diagonal in the dotplot was used.",
        "In the metrics below, the L 1 norm distance from a point (i, j) to a line from (0, 0) to (I, J) is The first metric, Dwdiag, is a normalized distance of the (i, j) pair of tokens to the diagonal on the word dotplot_",
        "where Lw (l) is the length of the LHS in words.",
        "The next two distances are character based, comparing the box containing aligned characters from the words at position (i, j) with the diagonal line on the character dotplot._ Let Lc(li) be the number of characters preceding the ith word in the LHS.",
        "Let the left edge of the box be bl = Lc(li), the right edge of the box be bT = Lc(li+1), the bottom edge of the box be bb = Lc(rj ), and the top edge of the box be bt = Lc(rj+1).",
        "The center of the box formed by the words at (i, j) is",
        "One character metric is the distance from the center of the character box to the diagonal line of the character dotplot, where Lc(l) is the character length of the entire LHS segment.",
        "The distance of the box to the diagonal line is the second character metric"
      ]
    },
    {
      "heading": "4 Data-driven and supervised methods",
      "text": [
        "The distance metrics and associated classifiers described above were all optimized on the trial data, but they required optimization of at most one parameter, a threshold on the distance.",
        "Four metrics were investigated that used the larger dataset to estimate larger models, with parameters for every pair of collocated words in the training dataset."
      ]
    },
    {
      "heading": "4.1 Likelihoods",
      "text": [
        "Three likelihood-based distance metrics were investigated, and the first is the relative likelihood of the aligned pairs of words.",
        "c(li, LHS) is the number of times the word li was seen in the LHS of the aligned corpus.",
        "DfTeqTatio(i, j = 1 min(c(li, LHS), c(rj, RHS)) ) max (c(li, LHS), c(rj, RHS)) The next two are conditional probabilities of seeing one of the words given that the other word from the pair was seen in an aligned sentence.",
        "Here RHSx means the right-hand-side of aligned pair number x in the parallel corpus.",
        "Note that neither of these is satisfactory as a probabilistic lexicon because they give stop words such as determiners high probability for every conditioning token."
      ]
    },
    {
      "heading": "4.2 Bag-of-segments distance",
      "text": [
        "The final data-driven measure that was investigated considers the bag of segments (bos) in which the words appear.",
        "The result of the calculation is the Tanimoto distance between the bag of segments that word li appears in and the bag of segments that word rj appears in."
      ]
    },
    {
      "heading": "5 Nearest neighbor rule",
      "text": [
        "The nearest neighbor rule is a well-known classification algorithm that provably converges to the Bayes Error Rate of a classification task as dataset size grows (Duda et al., 2001).",
        "The distance metrics described above were used to train a nearest neighbor rule classifier, each metric providing distance in one dimension.",
        "To provide comparability of distances in the different dimensions, the distribution of points in each dimension was normalized to have zero mean and unit variance (µ = 0, u = 1).",
        "The L2 norm, Euclidean distance, was used to compute distance between points.",
        "Two versions of the nearest neighbor rule were explored.",
        "In the first, the binary decisions of the classifiers were used as features, and in the second the distances provided by the classifiers were used as features."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "Two datasets of different language pairs were used to evaluate these measures: Romanian-English and English-French.",
        "The measures were optimized on a trial dataset and then evaluated blind on a test set.",
        "The Romanian-English trial data was 17 sentences long and the English-French trial dataset was 37 sentences.",
        "Additionally, approximately 1.1 million aligned English-French sentences and 48,000 Romanian-English sentences were used for the set of supervised experiments.",
        "Four measures were used to evaluate the classifiers: precision, recall, F-measure, and alignment error rate (AER).",
        "Precision and recall are the ratios of matching aligned pairs to the number of predicted pairs and the number of reference pairs respectively.",
        "F-measure is the harmonic mean of precision and recall.",
        "AER differentiates between “sure” and “possible” aligned pairs in the reference, requiring hypotheses to match those that are “sure” and permitting them to match those that are “possible”.",
        "(Och and Ney, 2000)."
      ]
    },
    {
      "heading": "7 Results",
      "text": [
        "English-French and the distance representation was superior for Romanian-English.",
        "Table 2 shows results of the explored methods on the test data.",
        "The presented order is the same as the order in Table 1.",
        "None of the results varied widely from observations on the trial dataset, suggesting that none of the classifiers were drastically overtrained in the course of optimization on the trial data."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "Several baseline alignment systems were presented.",
        "The individual scores of the different aligners give insight into the relative contributions of the features they exploit.",
        "Word length matching appears to be the least important feature, followed by character edit distance (attempting to match cognates), and geometric dotplot distances appear to contribute most strongly to alignment performance.",
        "The supervised probabilistic models perform poorly on their own, probably because of the unconstrained way in which they were trained and applied.",
        "When all features are combined in concert into a larger alignment system using the nearest neighbor rule, they perform better than individual aligners, but the question remains of what space should be used for modeling the points (distances versus binary decisions).",
        "Table 1 shows results of the explored methods on the trial data, ordered by degree of supervision and AER on the Romanian-English dataset.",
        "The biased coin random aligner is indicated as random and the final punctuation aligner is fpunct.",
        "The classifier based on relative length is len.",
        "The three edit distance measures are exact match (exact), edit distance (wedit), and lower-case edit distance (lcedit).",
        "The geometric measures are word distance to the diagonal (wdiag), distance to the character diagonal, (cdiag), and distance from the character box made by the word pair to the character diagonal, (cbox).",
        "The aligners that take advantage of the training data are below the first horizontal line inside the table.",
        "freqratio is the classifier based on the relative frequency of the two tokens, P(LIR) aligns words in the LHS with words from the RHS that are often collocated in the training sentences, and the reverse for P(RIL).",
        "The bag-of-documents distance classifier is evaluated in bos.",
        "The two supervised fusion methods are presented in the final two lines of the file: the binary nearest neighbor rule based on the classification output of the aligners (bnnrule), and the nearest neighbor rule based on the distances produced by the aligners (nnrule).",
        "Both of these results are leave-one-out estimates of performance from the trial set.",
        "Note that there is incomplete dominance: the binary representation was superior for"
      ]
    }
  ]
}
