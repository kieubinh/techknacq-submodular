{
  "info": {
    "authors": [
      "Christoph Tillmann"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W03-1001",
    "title": "A Projection Extension Algorithm for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/W03-1001",
    "year": 2003
  },
  "references": [
    "acl-C96-2141",
    "acl-H01-1035",
    "acl-J03-1005",
    "acl-J93-2003",
    "acl-N03-1017",
    "acl-P00-1056",
    "acl-P02-1039",
    "acl-P02-1040",
    "acl-W02-1018",
    "acl-W99-0604"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models.",
        "The units of translation are blocks – pairs of phrases.",
        "During decoding, we use a block unigram model and a word-based trigram language model.",
        "During training, the blocks are learned from source interval projections using an underlying high-precision word alignment.",
        "The system performance is significantly increased by applying a novel block extension algorithm using an additional high-recall word alignment.",
        "The blocks are further filtered using unigram-count selection criteria.",
        "The system has been successfully test on a Chinese-English and an Arabic-English translation task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Ya-mada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993).",
        "In this paper, we present a similar system with a much simpler set of model parameters.",
        "Specifically, we compute the probability of a block sequence .",
        "A block is a pair consisting of a contiguous source and a contiguous target phrase.",
        "The block sequence",
        "target and source phrases.",
        "The example is actual decoder output and the English translation is slightly incorrect.",
        "probability is decomposed into conditional probabilities using the chain rule:",
        "We try to find the block sequence that maximizes : .The model proposed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly.",
        "The approach is illustrated in Figure 1.",
        "The source phrases are given on the axis and the target phrases are given on the -axis.",
        "During block decoding a bijection between source and target phrases is generated.",
        "The two types of parameters in Eq 1 are defined as: Block unigram model : We compute unigram probabilities for the blocks.",
        "The blocks are simpler than the alignment templates (Och et al., 1999) in that they do not have an internal structure.",
        "Trigram language model: the probability between adjacent blocks is computed as the probability of the first target word in the target clump of given the final two words of the target clump of .",
        "The exponent is set in informal experiments to be .",
        "No other parameters such as distortion probabilities are used.",
        "To select blocks from training data, we compute unigram block co-occurrence counts .",
        "cannot be computed for all blocks in the training data: we would obtain hundreds of millions of blocks.",
        "The blocks are restricted by an underlying word alignment.",
        "In this paper, we present a block generation algorithm similar to the one in (Och et al., 1999) in full detail: source intervals are projected into target intervals under a restriction derived from a high-precision word alignment.",
        "The projection yields a set of high-precision block links.",
        "These block links are further extended using a high-recall word alignment.",
        "The block extension algorithm is shown to improve translation performance significantly.",
        "The system is tested on a Chinese-English (CE) and an Arabic-English (AE) translation task.",
        "The paper is structured as follows: in Section 2, we present the baseline block generation algorithm.",
        "The block extension approach is described in Section 2.1.",
        "Section 3 describes a DP-based decoder using blocks.",
        "Experimental results are presented in Section 4."
      ]
    },
    {
      "heading": "2 Block Generation Algorithm",
      "text": [
        "Starting point for the block generation algorithm is a word alignment obtained from an HMM Viterbi training (Vogel et al., 1996).",
        "The HMM Viterbi training is carried out twice with English as target language and Chinese as source language and vice versa.",
        "We obtain two alignment relations: is an alignment function from source to target positions and is an alignment function from target to source positions 1.",
        "We compute the union and the intersection of the two alignment relations and : We call the intersection relation , because it represents a high-precision alignment, and the union alignment , because it is taken to be a lower precision higher recall alignment (Och and Ney, 2000).",
        "The intersection is also a (partial) bijection between the target and source positions: it covers the same number of target and source positions and there is a bijection between source and target positions that are covered.",
        "For the CE experiments reported in Section 4 about % of the target and source positions are covered by word links in , for the AE experiments about % are covered.",
        "The extension algorithm presented assumes that , which is valid in this case since and are derived from intersection and union.",
        "We introduce the following additional piece of notation: is the set of all source positions that are covered by some word links in , where the source positions are shown along the axis and the target positions are shown along the -axis.",
        "To derive high-precision block links from the high-precision word links, we use the following projection definitions: and Here, projects source intervals into target intervals.",
        "projects target intervals into source intervals and is defined accordingly.",
        "Starting from the high-precision word alignment , we try to derive a high-precision block alignment: we project source intervals , where .",
        "We compute the minimum target index and maximum target index for the word links that fall into the 1 and denote a source positions.",
        "and denote a target positions.",
        "The right picture shows three blocks that cannot be obtain from source interval projections .",
        "input: High-precision alignment for each interval , where do Extend block link ’out-wards’ using the algorithm in Table 2 and add extended block link set to output: Sentence block link set .",
        "interval .",
        "This way, we obtain a mapping of",
        "source intervals into target intervals: (3)",
        "The approach is illustrated in Figure 2, where in the left picture, for example, the source interval is projected into the target interval .",
        "The pair defines a block alignment link .",
        "We use this notation to emphasize that the identity of the words is not used in the block learning algorithm.",
        "To denote the block consisting of the target and source words at the link positions, we write where denote target words and denote source words.",
        "denotes a function that maps intervals Figure 3: One, two, three, or four word links in lie on the frontier of a block.",
        "Additional word links may be inside the block.",
        "to the words at these intervals.",
        "The algorithm for generating the high-precision block alignment links is given in Table 1.",
        "The order in which the source intervals are generated does not change the final link set."
      ]
    },
    {
      "heading": "2.1 Block Extension Algorithm",
      "text": [
        "Empirically, we find that expanding the high-precision block links significantly improves performance.",
        "The expansion is parameterised and described below.",
        "For a block link , we compute its frontier by looking at all word links that lie on one of the four boundary lines of a block.",
        "We make the following observation as shown in Figure 3: the number of links (filled dots in the picture) on the frontier is less or equal , since in every column and row there is at most one link in , which is a partial bijetion.",
        "To learn blocks from a general word alignment that is not a bijection more than word links may lie on the frontier of a block, but to compute all possible blocks, it is sufficient to look at all possible quadruples of word links.",
        "We extend the links on the frontier by links of the high-recall alignment , where we use a parameterised way of locally extending a given word link.",
        "We compute an extended link set by extending each word link on the frontier separately and taking the union of the resulting links.",
        "The way a word link is extended is illustrated in Figure 4.",
        "The filled dot in the center of the picture is an element of the high-precision set .",
        "Starting from this link, we look for",
        "extensions in its neighborhood that lie in , where the neighborhood is defined by a cell width parameter and a distance parameter .",
        "For instance, link in Figure 4 is reached with cell width and distance , the link is reached with and , the link is reached with and .",
        "The word link is added to and it is itself extended using the same scheme.",
        "Here, we never make use of a row or a column covered by other than the rows and and the columns and .",
        "Also, we do not cross such a row or column using an extension with : this way only a small fraction of the word links in is used for extending a single block link.",
        "The extensions are carried out iteratively until no new alignment links from are added to .",
        "The block extension algorithm in Table 2 uses the extension set to generate all word link quadruples: the extended block that is defined by a given quadru-Table 2: Block link extension algorithm.",
        "The and function compute the minimum and the maximum of integer values.",
        "ple is generated and a check is carried out whether includes the seed block link .",
        "The following definition for block link inclusion is used: where the block is said to be included in .",
        "holds iff and .",
        "The ’seed’ block link is extended ’outwardly’: all extended blocks include the high-precision block .",
        "The block link itself may be included in other high-precision block links on its part, but holds.",
        "An extended block derived from the block never violates the projection restriction relative to i.e., we do not have to recheck the projection restriction for any generated block, which simplifies and fastens up the generation algorithm.",
        "The approach is illustrated in Figure 5, where a high-precision block with elements on its frontier is extended by two blocks containing it.",
        "The block link extension algorithm produces block links that contain new source and target intervals and that extend the interval mapping in Eq.",
        "3.",
        "This mapping is no longer a function, but rather a relation between source and target intervals i.e., a single source interval is mapped to several target intervals and vice versa.",
        "The extended block set constitutes a subset of the following set of interval",
        "The set of high-precision blocks is contained in this set.",
        "We cannot use the entire set of blocks defined by all pairs in the above relation, the resulting set of blocks cannot be handled due to memory restrictions, which motivates our extension algorithm.",
        "We also tried the following symmetric restriction and tested the resulting block set: The modified restriction is implemented in the context of the extension scheme in Table 1 by inserting an if statement before the alignment link is extended: the alignment link is extended only if the restriction also holds.",
        "Considering only block links for which the two way projection in Eq.",
        "4 holds has the following interesting interpretation: assuming a bijection that is complete i.e., all source and target positions are covered, an efficient block segmentation algorithm exists to compute a Viterbi block alignment as in Figure 1 for a given training sentence pair.",
        "The complexity of the algorithm is quadratic in the length of the source sentence.",
        "This dynamic programming technique is not used in the current block selection but might be used in future work."
      ]
    },
    {
      "heading": "2.2 Unigram Block Selection",
      "text": [
        "For selecting blocks from the candidate block links, we restrict ourselves to block links where target and source phrases are equal or less than words long.",
        "This way we obtain some tens of millions of blocks on our training data including blocks that occur only once.",
        "This baseline set is further filtered using the unigram count",
        ".",
        "as relative frequency over all selected blocks.",
        "An example of blocks obtained from the Chinese-English training data is shown in Figure 6.",
        "’$DATE’ is a placeholder for a date expression.",
        "Block contains the blocks to .",
        "All blocks are selected in training: the unigram decoder prefers even if , , and are much more frequent.",
        "The solid word links are word links in , the striped word links are word links in .",
        "Using the links in , we can learn one-to-many block translations, e.g. the pair ( ,’Xinhua news agency’) is learned from the training data."
      ]
    },
    {
      "heading": "3 DP-based Decoder",
      "text": [
        "We use a DP-based beam search procedure similar to the one presented in (Tillmann and Ney, 2003).",
        "We maximize over all block segmentations for which the source phrases yield a segmentation of the input source sentence, generating the target sentence simultaneously.",
        "The decoder processes search states of the following form: and are the two predecessor words used for the trigram language model, is the so-called coverage vector to keep track of the already processed source position, is the last processed source position.",
        "is the source phrase length of the block and (4) :denotes the set of blocks for which .",
        "For our Chinese-English experiments, we use the restriction as our baseline, and for the Arabic-English experiments the restriction.",
        "Blocks where the target and the source clump are of length are kept regardless of their count2.",
        "We compute the unigram probability",
        "currently being matched.",
        "is the length of the initial fragment of the source phrase that has been processed so far.",
        "is smaller or equal : .",
        "Note, that the partial hypotheses are not distinguished according to the identity of the block itself.",
        "The decoder processes the input sentence ’cardinality synchronously’: all partial hypotheses that are active at a given point cover the same number of input sentence words.",
        "The same beam-search pruning as described in (Tillmann and Ney, 2003) is used.",
        "The so-called observation pruning threshold is modified as follows: for each source interval that is being matched by a block source phrase at most the best target phrases according to the joint unigram probability are hypothesized.",
        "The list of blocks that correspond to a matched source interval is stored in a chart for each input sentence.",
        "This way the matching is carried out only once for all partial hypotheses that try to match the same input sentence interval.",
        "In the current experiments, decoding without block reordering yields the best translation results.",
        "The decoder translates about words per second."
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": []
    },
    {
      "heading": "4.1 Chinese-English Experiments",
      "text": [
        "The translation system is tested on a Chinese-to-English translation task.",
        "For testing, we use the DARPA/NIST MT 2001 dry-run testing data, which consists of sentences with words arranged in documents 3.",
        "The training data is provided by the LDC and labeled by NIST as the Large Data condition for the MT 2002 evaluation.",
        "The",
        "Chinese sentences are segmented into words.",
        "The training data contains million Chinese and million English words.",
        "The block selection algorithm described below runs less than one hour on a single Gigahertz linux machine.",
        "Table 3 presents results for various block extension schemes.",
        "The first column describes the extension scheme used.",
        "The second column reports the total number of blocks in millions collected - including all the blocks that occurred only once.",
        "The third column reports the number of blocks that occurred at least twice.",
        "These blocks are used to compute the results in the fourth column: the BLEU score (Pa-pineni et al., 2002) with reference translation using grams along with 95% confidence interval is reported 4.",
        "Line and line of this table show results where only the source interval projection without any extension is carried out.",
        "For the extension scheme, the high-recall union set itself is used for projection.",
        "The results are worse than for all other schemes, since a lot of smaller blocks are discarded due to the projection approach.",
        "The scheme, where just the word links are used is too restrictive leaving out bigger blocks that are admissible according to .",
        "For the Chinese-English test data, there is only a minor difference between the different extension schemes, the best results are obtained for the and the extension schemes.",
        "Table 4 shows the effect of the unigram selection threshold, where the blocks are used.",
        "The second column shows the number of blocks selected.",
        "The best results are obtained for the and the",
        "sets.",
        "The number of blocks can be reduced drastically where the translation performance declines only gradually.",
        "Table 5 shows the effect of the maximum phrase length on the BLEU score for the block set.",
        "Including blocks with longer phrases actually helps to improve performance, although already a length of obtains nearly identical results.",
        "We carried out the following control experiments (using as threshold): we obtained a block set of million blocks by generating blocks from all quadruples of word links in 5.",
        "This set is a proper superset of the blocks learned for the experiment in Table 3.",
        "The resulting BLEU score is .",
        "Including additional smaller blocks even hurts translation performance in this case.",
        "Also, for the extension scheme , we carried out the inverse projection as described in Section 2.1 to obtain a block set of million blocks and a BLEU score of .",
        "This number is smaller than the BLEU score of for the restriction: for the translation direction Chinese-to-English, selecting blocks with longer English phrases seems to be important for good translation performance.",
        "It is interesting to note, that the unigram translation model is symmetric: the translation direction can be switched to English-to-Chinese without retraining the model - just a new Chinese language model is needed.",
        "Our experiments, though, show that there is an unbalance with respect to the projection direction that has a significant influence on the translation results.",
        "Finally, we carried out an experiment where we used the block set as a baseline.",
        "The extension algorithm was applied only to blocks of target and source length producing one-to-many translations, e.g. the blocks and in Figure 6.",
        "The BLEU score improved to with a block set of million blocks.",
        "It seems to be important to carry out the block extension also for larger blocks.",
        "We also ran the N2 system on the June 2002 DARPA TIDES Large Data evaluation test set.",
        "Six research sites and four commercial off-the-shelf systems were evaluated in Large Data track.",
        "A majority of the systems were phrase-based translation systems.",
        "For comparison with other sites, we quote the 5 We cannot compute the block set resulting from all word link quadruples in , which is much bigger, due to CPU and memory restrictions.",
        "Table 5: Effect of the maximum phrase length on the BLEU score.",
        "Both target and source phrase are shorted than the maximum.",
        "The unigram threshold is ."
      ]
    },
    {
      "heading": "4.2 Arabic-English Experiments",
      "text": [
        "We also carried out experiments for the translation direction Arabic to English using training data from UN documents.",
        "For testing, we use a test set of sentences with words arranged in documents The training data contains million Arabic and million English words.",
        "The training data is preprocessed using some morphological analysis.",
        "For the Arabic experiments, we have tested the extension schemes ,, and as shown in Table 6.",
        "Here, the results for the different schemes differ significantly and the scheme produces the best results.",
        "For the AE experiments, only blocks up to a phrase length of are computed due to disk memory restrictions.",
        "The training data is split into several chunks of training sentence pairs each, and the final block set together with the unigram count is obtained by merging the block files for each of the chunks written onto disk memory.",
        "The word-to-word alignment is trained using iterations of the IBM Model training followed by iterations of the HMM Viterbi training.",
        "This training procedure takes about a day to execute on a single machine.",
        "Additionally, the overall block selection procedure takes about hours to execute."
      ]
    },
    {
      "heading": "5 Previous Work",
      "text": [
        "Block-based translation units are used in several papers on statistical machine translation.",
        "(Och et al., 1999) describe the alignment template system for statistical MT: alignment templates correspond to blocks that do have an internal structure.",
        "Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper.",
        "Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks.",
        "Block unigram counts are used to filter the blocks.",
        "The phrasal model is included into a syntax-based model.",
        "Projection of phrases has also been used in (Yarowsky et al., 2001).",
        "A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al., 2003)."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this paper, we describe a block-based unigram model for SMT.",
        "A novel block learning algorithm is presented that extends high-precision interval projections by elements from a high-recall alignment.",
        "The extension method is shown to improve translation performance significantly.",
        "For the Chinese-to-English task, we obtained a NIST score of on the June 2002 DARPA TIDES Large Data evaluation test set."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was partially supported by DARPA and monitored by SPAWAR under contract No.",
        "N6600199-2-8916.",
        "The paper has greatly profited from discussion with Fei Xia and Kishore Papineni."
      ]
    }
  ]
}
