{
  "info": {
    "authors": [
      "Woosung Kim",
      "Sanjeev P. Khudanpur"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W03-1003",
    "title": "Cross-Lingual Lexical Triggers in Statistical Language Modeling",
    "url": "https://aclweb.org/anthology/W03-1003",
    "year": 2003
  },
  "references": [
    "acl-H01-1035",
    "acl-J93-2003",
    "acl-P00-1056",
    "acl-W97-1014"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We propose new methods to take advantage of text in resource-rich languages to sharpen statistical language models in resource-deficient languages.",
        "We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihood-based adaptation scheme for combining a trigger model with an gram model.",
        "We describe the application of such language models for automatic speech recognition.",
        "By exploiting a side-corpus of contemporaneous English news articles for adapting a static Chinese language model to transcribe Mandarin news stories, we demonstrate significant reductions in both perplexity and recognition errors.",
        "We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via cross-lingual information retrieval and machine translation, proposed elsewhere."
      ]
    },
    {
      "heading": "1 Data Sparseness in Language Modeling",
      "text": [
        "Statistical techniques have been remarkably successful in automatic speech recognition (ASR) and natural language processing (NLP) over the last two decades.",
        "This success, however, depends crucially",
        "on the availability of accurate and large amounts of suitably annotated training data and it is difficult to build a usable statistical model in their absence.",
        "Most of the success, therefore, has been witnessed in the so called resource-rich languages.",
        "More recently, there has been an increasing interest in languages such as Mandarin and Arabic for ASR and NLP, and data resources are being created for them at considerable cost.",
        "The data-resource bottleneck, however, is likely to remain for a majority of the world’s languages in the foreseeable future.",
        "Methods have been proposed to bootstrap acoustic models for ASR in resource deficient languages by reusing acoustic models from resource-rich languages (Schultz and Waibel, 1998; Byrne et al., 2000).",
        "Morphological analyzers, noun-phrase chun-kers, POS taggers, etc., have also been developed for resource deficient languages by exploiting translated or parallel text (Yarowsky et al., 2001).",
        "Khudanpur and Kim (2002) recently proposed using cross-lingual information retrieval (CLIR) and machine translation (MT) to improve a statistical language model (LM) in a resource-deficient language by exploiting copious amounts of text available in resource-rich languages.",
        "When transcribing a news story in a resource-deficient language, their core idea is to use the first pass output of a rudimentary ASR system as a query for CLIR, identify a contemporaneous English document on that news topic, followed by MT to provide a rough translation which, even if not fluent, is adequate to update estimates of word frequencies and the LM vocabulary.",
        "They report up to a 28% reduction in perplexity on Chinese text from the Hong Kong News corpus.",
        "In spite of their considerable success, some shortcomings remain in the method used by Khudanpur and Kim (2002).",
        "Specifically, stochastic translation lexicons estimated using the IBM method (Brown et al., 1993) from a fairly large sentence-aligned Chinese-English parallel corpus are used in their approach – a considerable demand for a resource-deficient language.",
        "It is suggested that an easier-to-obtain document-aligned comparable corpus may suffice, but no results are reported.",
        "Furthermore, for each Mandarin news story, the single best matching English article obtained via CLIR is translated and used for priming the Chinese LM, no matter how good the CLIR similarity, nor are other well-matching English articles considered.",
        "This issue clearly deserves further attention.",
        "Finally, ASR results are not reported in their work, though their proposed solution is clearly motivated by an ASR task.",
        "We address these three issues in this paper.",
        "Section 2 begins, for the sake of completeness, with a review of the cross-lingual story-specific LM proposed by Khudanpur and Kim (2002).",
        "A notion of cross-lingual lexical triggers is proposed in Section 3, which overcomes the need for a sentence-aligned parallel corpus for obtaining translation lexicons.",
        "After a brief detour to describe topic-dependent LMs in Section 4, a description of the ASR task is provided in Section 5, and ASR results on Mandarin Broadcast News are presented in Section 6.",
        "The issue of how many English articles to retrieve and translate into Chinese is resolved by a likelihood-based scheme proposed in Section 6.1."
      ]
    },
    {
      "heading": "2 Cross-Lingual Story-Specific LMs",
      "text": [
        "For the sake of illustration, consider the task of sharpening a Chinese language model for transcribing Mandarin news stories by using a large corpus of contemporaneous English newswire text.",
        "Mandarin Chinese is, of course, not resource-deficient for language modeling – 100s of millions of words are available on-line.",
        "However, we choose it for our experiments partly because it is sufficiently different from English to pose a real challenge, and because the availability of large text corpora in fact permits us to simulate controlled resource deficiency.",
        "Let denote the text of test stories to be transcribed by an ASR system, and let denote their corresponding or aligned English newswire articles.",
        "Correspondence here does not imply that the English document needs to be an exact translation of the Mandarin story .",
        "It is quite adequate, for instance, if the two stories report the same news event.",
        "This approach is expected to be helpful even when the English document is merely on the same general topic as the Mandarin story, although the closer the content of a pair of articles the better the proposed methods are likely to work.",
        "Assume for the time being that a sufficiently good Chinese-English story alignment is given.",
        "Assume further that we have at our disposal a stochastic translation dictionary – a probabilistic model of the form – which provides the Chinese translation of each English word , where and respectively denote our Chinese and English vocabularies."
      ]
    },
    {
      "heading": "2.1 Computing a Cross-Lingual Unigram LM",
      "text": [
        "Let denote the relative frequency of a word in the document , , .",
        "It seems plausible that, , would be a good unigram model for the -th Mandarin story .",
        "We use this cross-lingual unigram statistic to sharpen a statistical Chinese LM used for processing the test story .",
        "One way to do this is via linear interpolation of the cross-lingual unigram model (1) with a static trigram model for Chinese, where the interpolation weight may be chosen off-line to maximize the likelihood of some held-out Mandarin stories.",
        "The improvement in (2) is expected from the fact that unlike the static text from which the Chinese trigram LM is estimated, is semantically close to and even the adjustment of unigram statistics, based on a stochastic translation model, may help.",
        "Figure 1 shows the data flow in this cross-lingual LM adaptation approach, where the output of the first pass of an ASR system is used by a CLIR system to find an English document , an MT system"
      ]
    },
    {
      "heading": "2.2 Obtaining Matching English Documents",
      "text": [
        "To illustrate how one may obtain the English document to match a Mandarin story , let us assume that we also have a stochastic reverse-translation lexicon .",
        "One obtains from the first pass ASR output, cf.",
        "Figure 1, the relative frequency estimate of Chinese words in , , and uses the translation lexicon to compute, ,",
        "an English bag-of-words representation of the Mandarin story as used in standard vector-based information retrieval.",
        "The document with the highest TF-IDF weighted cosine-similarity to is selected: sim Readers familiar with information retrieval literature will recognize this to be the standard query-translation approach to CLIR."
      ]
    },
    {
      "heading": "2.3 Obtaining Stochastic Translation Lexicons",
      "text": [
        "The translation lexicons and may be created out of an available electronic translation lexicon, with multiple translations of a word being treated as equally likely.",
        "Stemming and other morphological analyses may be applied to increase the vocabulary-coverage of the translation lexicons.",
        "Alternately, they may also be obtained automatically from a parallel corpus of translated and sentence-aligned Chinese-English text using statistical machine translation techniques, such as the publicly available GIZA++ tools (Och and Ney, 2000), as done by Khudanpur and Kim (2002).",
        "Unlike standard MT systems, however, we apply the translation models to entire articles, one word at a time, to get a bag of translated words – cf. (1) and (3).",
        "Finally, for truly resource deficient languages, one may obtain a translation lexicon via optical character recognition from a printed bilingual dictionary (cf. Doerman et al. (2002)).",
        "This task is arguably easier than obtaining a large LM training corpus."
      ]
    },
    {
      "heading": "3 Cross-Lingual Lexical Triggers",
      "text": [
        "It seems plausible that most of the information one gets from the cross-lingual unigram LM of (1) is in the form of the altered statistics of topic-specific Chinese words conveyed by the statistics of content-bearing English words in the matching story.",
        "The translation lexicon used for obtaining the information, however, is an expensive resource.",
        "Yet, if one were only interested in the conditional distribution of Chinese words given some English words, there is no reason to require translation as an intermediate step.",
        "In a monolingual setting, the mutual information between lexical pairs co-occurring anywhere within a long “window” of each-other has been used to capture statistical dependencies not covered by gram LMs (Rosenfeld, 1996; Tillmann and Ney, 1997).",
        "We use this inspiration to propose the following notion of cross-lingual lexical triggers.",
        "In a monolingual setting, a pair of words is considered a trigger-pair if, given a word-position in a sentence, the occurrence of in any of the preceding word-positions significantly alters the (conditional) probability that the following word in the sentence is : is said to trigger .",
        "E.g. the occurrence of either significantly increases the probability of or subsequently in the sentence.",
        "The set of preceding word-positions is variably defined to include all words from the beginning of the sentence, paragraph or document, or is limited to a fixed number",
        "of preceding words, limited of course by the beginning of the sentence, paragraph or document.",
        "In the cross-lingual setting, we consider a pair of words ,and , to be a trigger-pair if, given an English-Chinese pair of aligned documents, the occurrence of in the English document significantly alters the (conditional) probability that the word appears in the Chinese document: is said to trigger .",
        "It is plausible that translation-pairs will be natural candidates for trigger-pairs.",
        "It is, however, not necessary for a trigger-pair to also be a translation-pair.",
        "E.g., the occurrence of Belgrade in the English document may trigger the Chinese transliterations of Serbia and Kosovo, and possibly the translations of China, embassy and bomb!",
        "By infering trigger-pairs from a document-aligned corpus of Chinese-English articles, we expect to be able to discover semantically or topically-related pairs in addition to translation equivalences."
      ]
    },
    {
      "heading": "3.1 Identification of Cross-Lingual Triggers",
      "text": [
        "Average mutual information, which measures how much knowing the value of one random variable reduces the uncertainty of about another, has been used to identify trigger-pairs.",
        "We compute the average mutual information for every English-Chinese word pair as follows.",
        "Let ,, now be a document-aligned training corpus of English-Chinese article pairs.",
        "Let denote the document frequency, i.e., the number of aligned article-pairs, in which occurs in the English article and in the Chinese.",
        "Let denote the number of aligned article-pairs in which occurs in the English articles but does not occur in the Chinese article.",
        "Let The quantities and are similarly defined.",
        "Next let denote the number of English articles in which occurs, and define We propose to select word pairs with high mutual information as cross-lingual lexical triggers.",
        "There are possible English-Chinese word pairs which may be prohibitively large to search for the pairs with the highest mutual information.",
        "We filter out infrequent words in each language, say, words appearing less than 5 times, then measure for all possible pairs from the remaining words, sort them by , and select, say, the top",
        "1 million pairs."
      ]
    },
    {
      "heading": "3.2 Estimating Trigger LM Probabilities",
      "text": [
        "Once we have chosen a set of trigger-pairs, the next step is to estimate a probability in lieu of the translation probability in (1), and a probability in (3).",
        "Following the maximum likelihood approach proposed by Tillman and Ney (1997), one could choose the trigger probability to be based on the unigram frequency of among Chinese word tokens in that subset of aligned documents which have in , namely As an ad hoc alternative to (4), we also use where we set whenever is not a trigger-pair, and find it to be somewhat more effective (cf.",
        "Section 6.2).",
        "Thus (5) is used henceforth in this paper.",
        "Analogous to (1), we set and, again, we build the interpolated model"
      ]
    },
    {
      "heading": "4 Topic-Dependent Language Models",
      "text": [
        "The linear interpolation of the story-dependent unigram models (1) and (6) with a story-independent ; define via the quency and Similarly define ,via the document fre-document frequency , etc.",
        "Finally, let trigram model, as described above, is very reminiscent of monolingual topic-dependent language models (cf. e.g. (Iyer and Ostendorf, 1999)).",
        "This motivates us to construct topic-dependent LMs and contrast their performance with these models.",
        "To this end, we represent each Chinese article in the training corpus by a bag-of-words vector, and cluster the vectors using a standard K-means algorithm.",
        "We use random initialization to seed the algorithm, and a standard TF-IDF weighted cosine-similarity as the “metric” for clustering.",
        "We perform a few iterations of the K-means algorithm, and deem the resulting clusters as representing different topics.",
        "We then use a bag-of-words centroid created from all the articles in a cluster to represent each topic.",
        "Topic-dependent trigram LMs, denoted , are also computed for each topic exclusively from the articles in the -th cluster, .",
        "Each Mandarin test story is represented by a bag-of-words vector generated from the first-pass ASR output, and the topic-centroid having the highest TF-IDF weighted cosine-similarity to it is chosen as the topic of .",
        "Topic-dependent LMs are then constructed for each story as",
        "and used in a second pass of recognition.",
        "Alternatives to topic-dependent LMs for exploiting long-range dependencies include cache LMs and monolingual lexical triggers; both unlikely to be as effective in the presence of significant ASR errors."
      ]
    },
    {
      "heading": "5 ASR Training and Test Corpora",
      "text": [
        "We investigate the use of the techniques described above for improving ASR performance on Mandarin news broadcasts using English newswire texts.",
        "We have chosen the experimental ASR setup created in the 2000 Johns Hopkins Summer Workshop to study Mandarin pronunciation modeling, extensive details about which are available in Fung et al. (2000).",
        "The acoustic training data ( 10 hours) for their ASR system was obtained from the 1997 Mandarin Broadcast News distribution, and context-dependent state-clustered models were estimated using initials and finals as subword units.",
        "Two Chinese text corpora and an English corpus are used to estimate LMs in our experiments.",
        "A vocabulary of 51K Chinese words, used in the ASR system, is also used to segment the training text.",
        "This vocabulary gives an OOV rate of 5% on the test data.",
        "XINHUA: We use the Xinhua News corpus of about 13 million words to represent the scenario when the amount of available LM training text borders on adequate, and estimate a baseline trigram LM for one set of experiments.",
        "HUB-4NE: We also estimate a trigram model from only the 96K words in the transcriptions used for training acoustic models in our ASR system.",
        "This corpus represents the scenario when little or no additional text is available to train LMs.",
        "NAB-TDT: English text contemporaneous with the test data is often easily available.",
        "For our test set, described below, we select (from the North American News Text corpus) articles published in 1997 in The Los Angeles Times and The Washington Post, and articles from 1998 in the New York Times and the Associated Press news service (from TDT-2 corpus).",
        "This amounts to a collection of roughly 45,000 articles containing about 30-million words of English text; a modest collection by CLIR standards.",
        "Our ASR test set is a subset (Fung et al. (2000)) of the NIST 1997 and 1998 HUB-4NE benchmark tests, containing Mandarin news broadcasts from three sources for a total of about 9800 words.",
        "We generate two sets of lattices using the baseline acoustic models and bigram LMs estimated from XINHUA and HUB-4NE.",
        "All our LMs are evaluated by rescoring best lists extracted from these two sets of lattices.",
        "The best lists from the XINHUA bigram LM are used in all XINHUA experiments, and those from the HUB-4NE bigram LM in all HUB-4NE experiments.",
        "We report both word error rates (WER) and character error rates (CER), the latter being independent of any difference in segmentation of the ASR output and reference transcriptions."
      ]
    },
    {
      "heading": "6 ASR Performance of Cross-Lingual LMs",
      "text": [
        "We begin by rescoring the best lists from the bigram lattices with trigram models.",
        "For each test story , we perform CLIR using the first pass ASR output to choose the most similar English document from NAB-TDT.",
        "Then we create the cross-lingual unigram model of (1).",
        "We also find the interpolation weight which maximizes the likelihood of the 1-best hypotheses of all test utterances from the first ASR pass.",
        "Table 1 shows the perplexity and WER for XINHUA and HUB-4NE.",
        "All values reported in this paper are based on the standard NIST MAPSSWE test (Pallett et al., 1990), and indicate the statistical significance of a WER improvement over the corresponding trigram baseline, unless otherwise specified.",
        "Evidently, the improvement brought by CL-interpolated LM is not statistically significant on XINHUA.",
        "On HUB-4NE however, where Chinese text is scarce, the CL-interpolated LM delivers considerable benefits via the large English corpus."
      ]
    },
    {
      "heading": "6.1 Likelihood-Based Story-Specific Selection of Interpolation Weights and the Number of English Documents per Mandarin Story",
      "text": [
        "The experiments above naively used the one most similar English document for each Mandarin story, and a global in (2), no matter how similar the best matching English document is to a given Mandarin news story.",
        "Rather than choosing one most similar English document from NAB-TDT, it stands to reason that choosing more than one English document may be helpful if many have a high similarity score, and perhaps not using even the best matching document may be fruitful if the match is sufficiently poor.",
        "It may also help to have a greater interpolation weight for stories with good matches, and a smaller for others.",
        "For experiments in this subsection, we select a different for each test story, again based on maximizing the likelihood of the - best output given a CL-Unigram model.",
        "The other issue then is the choice and the number of English documents to translate.",
        "-best documents: One could choose a predetermined number of the best matching English documents for each Mandarin story.",
        "We experimented with values of , , , , and , and found that gave us the best LM performance, but only marginally better than as described above.",
        "Details are omitted, as they are uninteresting.",
        "All documents above a similarity threshold: The argument against always taking a predetermined number of the best matching documents may be that it ignores the goodness of the match.",
        "An alternative is to take all English documents whose similarity to a Mandarin story exceeds a certain predetermined threshold.",
        "As this threshold is lowered, starting from a high value, the order in which English documents are selected for a particular Mandarin story is the same as the order when choosing the best documents, but the number of documents selected now varies from story to story.",
        "It is possible that for some stories, even the best matching English document falls below the threshold at which other stories have found more than one good match.",
        "We experimented with various thresholds, and found that while a threshold of gives us the lowest perplexity on the test set, the reduction is insignificant.",
        "This points to the need for a story-specific strategy for choosing the number of English documents, instead of a global threshold.",
        "Likelihood-based selection of the number of English documents: Figure 2 shows the perplexity of the reference transcriptions of one typical test story under the LM (2) as a function of the number of English documents chosen for creating (1).",
        "For each choice of the number of English documents, the interpolation weight in (2) is chosen to maximize the likelihood (also shown) of the first pass output.",
        "This suggests that choosing the number of English documents to maximize the likelihood of the first pass ASR output is a good strategy.",
        "For each Mandarin test story, we choose the 1000-best-matching English documents and divide the dynamic range of their similarity scores evenly into 10 intervals.",
        "Next, we choose the documents in the top -th of the range of similarity scores, not necessarily the top documents, compute , determine the in (2) that maximizes the likelihood of the first pass output of only the utterances in that story, and record this likelihood.",
        "We repeat this with documents in the top -th of the range of similarity scores, the top -th, etc.,",
        "and obtain the likelihood as a function of the similarity threshold.",
        "We choose the threshold that maximizes the likelihood of the first pass output.",
        "Thus the number of English documents in (1), as well as the interpolation weight in (2), are chosen dynamically for each Mandarin story to maximize the likelihood of the ASR output.",
        "Table 2 shows ASR results for this likelihood-based story-specific adaptation scheme.",
        "Note that significant WER improvements are obtained from the CL-interpolated LM using likelihood-based story-specific adaptation even for the case of the XINHUA LM.",
        "Furthermore, the performance of the CL-interpolated LM is even better than the topic-dependent LM.",
        "This is remarkable, since the CL-interpolated LM is based on unigram statistics from English documents, while the topic-trigram LM is based on trigram statistics.",
        "We believe that the contemporaneous and story-specific nature of the English document leads to its relatively higher effectiveness.",
        "Our conjecture, that the contemporaneous cross-lingual statistics and static topic-trigram statistics are complementary, is supported by the significant further improvement in WER obtained by the interpolation of the two LMs, as shown on the last line for XINHUA.",
        "The significant gain in ASR performance in the resource deficient HUB-4NE case are obvious.",
        "The small size of the HUB-4NE corpus makes topic-models ineffective."
      ]
    },
    {
      "heading": "6.2 Comparison of Cross-Lingual Triggers with Stochastic Translation Dictionaries",
      "text": [
        "Once we select cross-lingual trigger-pairs as de.",
        "Therefore, given a set of cross-lingual trigger-pairs, the trigger-based models are free from requiring a translation lexicon.",
        "Furthermore, a document-aligned comparable corpus is all that is required to construct the set of trigger-pairs.",
        "We otherwise follow the same experimental procedure as above.",
        "As Table 2 shows, the trigger-based model (Trig-interpolated) performs only slightly worse than the CL-interpolated model.",
        "One explanation for this degradation is that the CL-interpolated model is trained from the sentence-aligned corpus while the trigger-based model is from the document-aligned corpus.",
        "There are two steps which could be affected by this difference, one being CLIR and the other being the translation of the ’s into Chinese.",
        "Some errors in CLIR may however be masked by our likelihood-based story-specific adaptation scheme, since it finds optimal retrieval settings, dynamically adjusting the number of English documents as well as the interpolation weight, even if CLIR performs somewhat suboptimally.",
        "Furthermore, a document-aligned corpus is much easier to build.",
        "Thus a much bigger and more reliable comparable corpus may be used, and eventually more accurate trigger-pairs will be acquired.",
        "We note with some satisfaction that even simple trigger-pairs selected on the basis of mutual information are able to achieve perplexity and WER reductions comparable to a stochastic translation lexicon: the smallest value at which the difference between the WERs of the CL-interpolated LM and the Trig-interpolated LM in Table 2 would be significant is for XINHUA and for HUB-4NE.",
        "Triggers (4) vs (5): We compare the alternative definitions (4) and (5) for replacing in (1).",
        "The resulting CL-interpolated LM (2) yields a perplexity of 370 on the XINHUA test set using (4), compared to 367 using (5).",
        "Similarly, on the HUB-4NE test set, using (4) yields 736, while (5) yields 727.",
        "Therefore, (5) has been used throughout."
      ]
    },
    {
      "heading": "7 Conclusions and Future Work",
      "text": [
        "We have demonstrated a statistically significant improvement in ASR WER (1.4% absolute) and in perplexity (23%) by exploiting cross-lingual side-information even when nontrivial amount of training data is available, as seen on the XINHUA corpus.",
        "Our methods are even more effective when LM training text is hard to come by in the language of interest: 47% reduction in perplexity and 1.3% absolute in WER as seen on the HUB-4NE corpus.",
        "Most of these gains come from the optimal choice of adaptation parameters.",
        "The ASR test data we used in our experiments is derived from a different source than the corpus on which the translation and trigger models are trained, and the techniques work even when the bilingual corpus is only document-aligned, which is a realistic reflection of the situation in a resource-deficient language.",
        "We are developing maximum entropy models to more effectively combine the multiple information sources we have used in our experiments, and expect to report the results in the near future."
      ]
    }
  ]
}
