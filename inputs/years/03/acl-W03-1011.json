{
  "info": {
    "authors": [
      "Julie Weeds",
      "David Weir"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W03-1011",
    "title": "A General Framework for Distributional Similarity",
    "url": "https://aclweb.org/anthology/W03-1011",
    "year": 2003
  },
  "references": [
    "acl-H94-1046",
    "acl-J02-4004",
    "acl-P90-1034",
    "acl-P93-1024",
    "acl-P98-2127",
    "acl-P99-1004",
    "acl-W96-0209"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a general framework for distributional similarity based on the concepts of precision and recall.",
        "Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.",
        "We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "There are many potential applications of sets of distributionally similar words.",
        "In the syntactic domain, language models, which can be used to evaluate alternative interpretations of text and speech, require probabilistic information about words and their co-occurrences which is often not available due to the sparse data problem.",
        "In order to overcome this problem, researchers (e.g. Pereira et al.",
        "(1993)) have proposed estimating probabilities based on sets of words which are known to be distributionally similar.",
        "In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity.",
        "Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budan-itsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002).",
        "However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the Ll Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard's Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the Kullback-Leibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the askew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle's Mutual Information (MI) Based Measure (Hindle, 1990) and Lin's MI-Based Measure (Lin, 1998).",
        "Further, there is no clear way of deciding which is the best measure.",
        "Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words.",
        "We take a generic approach that does not directly reduce distributional similarity to a single dimension.",
        "The way dimensions are combined together will depend on parameters tuned to the demands of a given application.",
        "Further, different parameter settings will approximate different existing similarity measures as well as many more which have, until now, been unexplored.",
        "The contributions of this paper are fourfold.",
        "First, we propose a general framework for distributional similarity based on the concepts of precision and recall (Section 2).",
        "Second, we evaluate the framework at its optimal parameter settings for two different applications (Section 3), showing that it outperforms existing state-of-the-art similarity measures for both high and low frequency nouns.",
        "Third, we begin to investigate to what extent existing similarity measures might be characterised in terms of parameter settings within the framework (Section 4).",
        "Fourth, we provide an understanding of why a single existing measure cannot achieve optimal results in every application of distributional similarity measures.",
        "2 The Framework In this section, we introduce the relevance of the Information Retrieval (IR) concepts of precision and recall in the context of word similarity.",
        "We provide combinatorial, probabilistic and mutual-information based models for precision and recall and discuss combining precision and recall to provide a single number in the context of a particular application.",
        "2.1 Precision and Recall The similarity) of two nouns can be viewed as a measure of how appropriate it is to use one noun (or its distribution) in place of the other.",
        "If we are using the distribution of one noun in place of the distribution the other noun, we can consider the precision and recall of the prediction made.",
        "Precision tells us how much of what has been predicted is correct whilst recall tells us how much of what is required has been predicted.",
        "In order to calculate precision and recall, we first need to consider for each noun n which verb co-occurrences will be predicted by it and, conversely, required in a description of it.",
        "We will refer to these verbs as the features of n, F(n):",
        "where D(n, v), is the degree of association between noun n and verb v. Possible association functions will be defined in the context of each model described below.",
        "If we are considering the ability of noun A to predict noun B then it follows that the set of True Positives is TP = F(A) fl F(B) and precision and recall can be defined as:",
        "Precision and recall both lie in the range [0,1] and are both equal to one when each noun has exactly the same features.",
        "It should also be noted that R(A, B) = P(B, A).",
        "We will now consider some different possibilities for measuring the degree of association between a noun n and a verb v."
      ]
    },
    {
      "heading": "2.2 Combinatorial Model",
      "text": [
        "In the combinatorial model, we simply consider whether a verb has ever been seen to co-occur with the noun.",
        "In other words, the degree of association (D) between a noun n and a verb v is 1 if they have co-occurred together and 0 otherwise.",
        "In this case, it should be noted that the definitions of precision and recall can be simplified as follows: ETP Dc(A, v) = JTPJ"
      ]
    },
    {
      "heading": "2.3 Probabilistic Model",
      "text": [
        "In the probabilistic model, more probable (or more frequent) co-occurrences are considered more significant.",
        "The degree of association between a noun n and verb v is defined in the probabilistic model as:",
        "The definitions for feature set membership, TP, precision and recall all remain the same except for the use of the new association function.",
        "Using the probabilistic model, the precision of A's prediction of B is the probability that a verb picked at random from those co-occurring with A will also co-occur with B; and the recall of A's prediction of B is the probability that a verb picked at random from those those co-occurring with B will also co-occur with A.",
        "Mutual Information Based Model Mutual information (MI) allows us to capture the idea that a co-occurrence of low probability",
        "events is more informative than a co-occurrence of high probability events.",
        "In this model, as before, we retain the definitions for feature set membership, TP, precision and recall but again change the association function.",
        "Here, the degree of association be",
        "Accordingly, verb v will be considered to be a feature of noun n if the probability of their co-occurrence is greater than would be expected if verbs and nouns occurred independently.",
        "2.4 Combining Precision and Recall Although we have defined a pair of numbers for similarity, in applications it will still be necessary to compute a single number in order to determine neighbourhood or cluster membership.",
        "There are two obvious ways to optimise a pair of numbers such as precision and recall.",
        "The first is to use an arithmetic mean, which optimises the sum of the numbers, and the second is to use a harmonic meant, which optimises the product of the numbers.",
        "In an attempt to retain generality, we can allow both alternatives by computing an arithmetic mean of the harmonic mean and the arithmetic mean, noting that the relative importance of each term in an arithmetic mean is controlled by weights (which sum to 1):",
        "ma,(A, B) = �Y(A, B) + (1 – �).R(A, B) sim(A, B) = ry.mh(A, B) + (1 – ry).ma,(A, B) where both � and ry lie in the range [0,1].",
        "The resulting similarity sim(A,B) will also lie in the range [0,1] where 0 represents complete lack of similarity and 1 represents equivalence.",
        "This formula can be used in combination with any of the models for precision and recall outlined above.",
        "Further, the generality allows us to investigate empirically the relative significance of the different terms and thus whether one (or more) might be omitted in future work.",
        "thing which will be computed for a specific task and will depend on the values of � and ry.",
        "Table 1 summarizes some special parameter settings."
      ]
    },
    {
      "heading": "3 Empirical Evaluation",
      "text": [
        "In this section, we evaluate the performance of the framework, using the combinatorial and MI-based models of precision and recall, at two application based tasks against Lin's MI-based Measure (simLln) and the askew Divergence Measure (simasd).",
        "The formulae for these measures are given in Figure 1.",
        "For the askew divergence measure we set a = 0.99 since this most closely approximates the Kullback-Leibler divergence measure.",
        "The two evaluation tasks used pseudo-disambiguation and Word-Net (Fellbaum, 1998) prediction are fairly standard for distributional similarity measures.",
        "However, in the future we wish to extend our evaluation to other tasks such as malapropism correction (Budanitsky and Hirst, 2001) and PP-attachment ambiguity resolution (Resnik, 1993) and also to the probabilistic model.",
        "Since we use the same data and methodology as in earlier work, some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir (2003)."
      ]
    },
    {
      "heading": "3.1 Pseudo-Disambiguation Task",
      "text": [
        "Pseudo-disambiguation tasks (e.g. Lee, 1999) have become a standard evaluation technique and, in the current context, we may use a word's neighbours to decide which of two co-occurrences is the most likely.",
        "Although pseudo-disambiguation itself is an artificial task, it has relevance in at least two real application areas.",
        "First, by replacing occurrences of a particular word in a test suite with a pair or set of words from which a technique must choose, we recreate a simplified version of the word sense disambiguation task; that is,",
        "choosing between a fixed number of homonyms based on local context.",
        "The second is in language modelling where we wish to estimate the probability of co-occurrences of events but, due to the sparse data problem, it is often the case that a possible co-occurrence has not been seen in the training data."
      ]
    },
    {
      "heading": "3.1.1 Methodology",
      "text": [
        "As is common in this field (e.g. Lee, 1999), we study similarity between nouns based on their co-occurrences with verbs in the direct object relation.",
        "We study similarity between high and low frequency nouns since we want to investigate any associations between word frequency and quality of neighbours found by the measures but it is impractical to evaluate a large number of similarity measures over all nouns.",
        "2,852,300 lemmatised (noun-verb) direct-object pairs were extracted from the BNC using a shallow parser (Briscoe and Carroll, 1995; Carroll and Briscoe, 1996).",
        "From those nouns also occurring in WordNet, we selected the 1000 most frequent3 nouns and a set of 1000 low frequency4 nouns.",
        "For each noun, 80% of the available data was randomly selected as training data and the other 20% set aside as test data.",
        "Precision and recall were computed for each pair of nouns using the combinatorial and MI models.",
        "This data is then available to the application task which will first have to compute the similarity for each pair of nouns based on current parameter set",
        "spond to a frequency range of [70,120].",
        "tings and select nearest neighbours accordingly.",
        "We converted each noun-verb pair (n, vl) in the set-aside test data into a noun-verb-verb triple (n, v1, v2) where P(v1) is approximately equal to P(v2) over all the training data and (n, v2) has not been seen in the test or training data.",
        "A high frequency noun test set and a low frequency noun test set, each containing 10,000 test instances, were then constructed by selecting ten test instances for each noun in a two step process of 1) whilst more than ten triples remained, discarding duplicate triples and 2) randomly selecting ten triples from those remaining after step 1.",
        "Each set of test triples was split into five disjoint subsets, containing two triples for each noun, so that average performance and standard error could be computed.",
        "Additionally, three of the five subsets were used as a development set to optimise parameters (k, � and -y) and the remaining two used as a test set to find error rates at these optimal settings.",
        "The task is then for the nearest neighbours of noun n to decide which of (n, v1) and (n, v2) was the original co-occurrence.",
        "Each of n's neighbours, m, is given a vote which is equal to the difference in frequencies of the co-occurrences (m, v1) and (m, v2) and which it casts to the co-occurrence in which it appears most frequently.",
        "The votes for each co-occurrence are summed over all of the k nearest neighbours of n and the co-occurrence with the most votes wins.",
        "Performance is measured as error rate.",
        "error = I (# of incorrect choices + # of ties2 ) where T is the number of test instances.",
        "is that the hyponymy relation in WordNet is a gold standard for semantic similarity which is, of course, not true.",
        "However, we believe that a distributional similarity measure which more closely predicts WordNet, is more likely to be a good predictor of semantic similarity."
      ]
    },
    {
      "heading": "3.2.1 Methodology",
      "text": [
        "We will first explain the WordNet-based distance measure (Lin, 1997) and then explain how we determine the similarity between neighbour sets generated using different measures.",
        "The similarity of two nouns in WordNet is defined as the similarity of their maximally similar senses.",
        "The commonality of two concepts is defined as the maximally specific superclass of those concepts.",
        "So, if syn(n) is the set of senses of the noun n in WordNet, sup(c) is the set of (possibly indirect) superclasses of concept c in WordNet and P(c) is the probability that a randomly selected noun refers to an instance of c, then the similarity between nl and n2 can be calculated using the formula for simv n in Figure 1.",
        "The probabilities P(c) are estimated by the frequencies of concepts in SemCor (Miller et al., 1994), a sense-tagged subset of the Brown corpus, noting that the occurrence of a concept refers to instances of all the superclasses of that concept (i.e. P(root of tree6) = 1).",
        "The k nearest neighbours7 of each noun, computed using each distributional similarity measure at each parameter setting, are then compared with the k nearest neighbours of the noun according to the WordNet based measure.",
        "In order to compute the similarity of two neighbour sets, we transform each neighbour set so that each neighbour is given a rank score of k – rank.",
        "We do not use the similarity scores directly since these require normalization if different similarity measures (using different scales) are to be compared.",
        "Having performed this transformation, the neighbour sets for the same word w may be represented by two ordered sets of words [wk, ..., wl] and [w', ..., w'1].",
        "The similarity between such sets is computed using the same calculation as used by Lin (1998) except for sim",
        "saurus Entries for WordNet Prediction Task plifications due to the use of ranks:",
        "where i and j are the rank scores of the words within each neighbour set."
      ]
    },
    {
      "heading": "3.2.2 Results",
      "text": [
        "Table 3 summarizes the optimal mean similarities and parameter settings for the general framework using both the combinatorial (sim,) and the MI-based (simr,,,2) models.",
        "Results for Lin's MI-based measure (simLin) and the askew divergence measure (simard) are also given and results are divided into those for high frequency nouns and those for low frequency nouns.",
        "Standard errors in the optimal mean similarities are not given but were of the order of 0.1.",
        "Our first observation is that the general framework using the MI-based model for precision and recall outperforms all of the other distributional similarity measures.",
        "We also observe that lower values of y produce better results, particularly for low frequency nouns.",
        "For example, when y = 1, similarity for low frequency nouns drops to 0.147 using the combinatorial model and 0.177 using the MI-based model.",
        "Third, from Figure 3, it appears that this WordNet prediction task favours measures which select high recall neighbours.",
        "Although optimum similarity for the combinatorial model occurs at =0.5, similarity is always higher for lower values of than for higher values of .",
        "ing the askew divergence measure and those found using the MI-Based model.",
        "Optimal similarity (0.760 and 0.725 respectively) was found at y = 0.0 and � = 0.0 for high frequency nouns and at y = 0.25 and � = 0.0 for low frequency nouns.",
        "Further, similarity between the measures drops rapidly once � rises above 0.3."
      ]
    },
    {
      "heading": "5 Conclusions and Further Work",
      "text": [
        "Using the MI-based model for precision and recall and with a parameter setting of y = 1.0, the general framework for distributional similarity proposed herein closely approximates Lin's (1998) Measure.",
        "However, we have shown that using a much lower value of y so that the combination of precision and recall is closer to a weighted arithmetic mean than a harmonic mean yields better results in the two application tasks considered here.",
        "This is because the relative importance of precision and recall can be tuned to the task at hand.",
        "Further, we have shown that pseudo-disambiguation is a task which requires high precision neighbours whereas WordNet prediction is a task which requires high recall neighbours.",
        "Accordingly, it is not clear how a single (unparameterised) similarity measure could give optimum results on both tasks.",
        "In the future, we intend to extend the work to the characterisation of other tasks and other existing similarity measures.",
        "As well as their, usually implicit, use of precision and recall, the main difference between existing similarity measures will be the models in which precision and recall are defined.",
        "We have explored two such models here - a combinatorial model and a MI-based model - and have shown that the MI-based model achieves significantly improved results over the combinatorial model.",
        "We propose to investigate other models such as the probabilistic one given in Section 2.3."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "We would like to thank John Carroll for the use of his parser, Adam Kilgarriff and Bill Keller for valuable discussions and the UK EPSRC for its studentship to the first author."
      ]
    }
  ]
}
