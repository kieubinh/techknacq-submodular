{
  "info": {
    "authors": [
      "Hideki Isozaki",
      "Tsutomu Hirao"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W03-1024",
    "title": "Japanese Zero Pronoun Resolution Based on Ranking Rules and Machine Learning",
    "url": "https://aclweb.org/anthology/W03-1024",
    "year": 2003
  },
  "references": [
    "acl-C02-1051",
    "acl-C02-1053",
    "acl-C02-1054",
    "acl-C02-1078",
    "acl-C96-2147",
    "acl-J94-2003",
    "acl-J95-2003",
    "acl-N01-1025",
    "acl-P86-1031",
    "acl-P95-1017"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Anaphora resolution is one of the most important research topics in Natural Language Processing.",
        "In English, overt pronouns such as she and definite noun phrases such as the company are anaphors that refer to preceding entities (antecedents).",
        "In Japanese, anaphors are often omitted, and these omissions are called zero pronouns.",
        "There are two major approaches to zero pronoun resolution: the heuristic approach and the machine learning approach.",
        "Since we have to take various factors into consideration, it is difficult to find a good combination of heuristic rules.",
        "Therefore, the machine learning approach is attractive, but it requires a large amount of training data.",
        "In this paper, we propose a method that combines ranking rules and machine learning.",
        "The ranking rules are simple and effective, while machine learning can take more factors into account.",
        "From the results of our experiments, this combination gives better performance than either of the two previous approaches."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Anaphora resolution is an important research topic in Natural Language Processing.",
        "For instance, machine translation systems should identify antecedents of anaphors (such as he or she) in the source language to achieve better translation quality in the target language.",
        "We are now studying open-domain question answering systems1, and we expect QA systems to benefit from anaphora resolution.",
        "Typical QA systems try to answer a user’s question by finding relevant phrases from large corpora.",
        "When a correct answer phrase is far from the keywords given in the question, the systems will not succeed in finding the answer.",
        "If the system can correctly resolve anaphors, it will find keywords or answers represented by anaphors, and the chances of finding the answer will increase.",
        "From this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles.",
        "In Japanese, anaphors are often omitted and these omissions are called zero pronouns.",
        "Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult.",
        "In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles.",
        "They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; Yamura-Takei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995).",
        "There are two approaches to the problem: the heuristic approach and the machine learning ap",
        "proach.",
        "The Centering Theory (Grosz et al., 1995) is important in the heuristic approach.",
        "Walker et al.",
        "(1994) proposed forward center ranking for Japanese.",
        "Kameyama (1986) emphasized the importance of a property-sharing constraint.",
        "Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences.",
        "However, these improvements are not sufficient for resolving zeros accurately.",
        "Murata and Na-gao (1997) proposed complicated heuristic rules that take various features of antecedents and anaphors into account.",
        "We have to take even more factors into account, but it is difficult to maintain such heuristic rules.",
        "Therefore, recent studies employ machine learning approaches.",
        "However, it is also difficult to prepare a sufficient number of annotated corpora.",
        "In this paper, we propose a method that combines these two approaches.",
        "Heuristic ranking rules give a general preference, while a machine learning method excludes inappropriate antecedent candidates.",
        "From the results of our experiments, the proposed method shows better performance than either of the two approaches alone.",
        "Before giving a description of our methodology, we briefly introduce the grammar of the Japanese language here.",
        "A Japanese sentence is a sequence of bunsetsus: .",
        "A bunsetsu is a sequence of content words (e.g., nouns, adjectives, and verbs) followed by zero or more functional words (e.g., particles and auxiliary verbs): .",
        "A bunsetsu modifies one of the following bunsetsus.",
        "A particle (joshi) marks the grammatical case of the noun phrase immediately before it.",
        "For example, ga is nominative (subject), wo is accusative (object), ni is dative (object2), and wa marks a topic.",
        "(Tom sent a book to Bob.)",
        "Bunsetsu dependency is represented by a list of bunsetsu pairs (modifier, modified).",
        "For instance, indicates that there are four bunsetsus in this sentence and that the first bunsetsu modifies the fourth bunsetsu and so on.",
        "The last bunsetsu modifies no bunsetsu, which is indicated by .",
        "It takes a long time to construct high-quality annotated data, and we want to compare our results with conventional methods.",
        "Therefore, we obtained Seki’s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus 2 2.0.",
        "These data are divided into two groups: general and editorial.",
        "General contains 30 general news articles, and editorial contains 30 editorial articles.",
        "According to his experiments, editorial is harder than general.",
        "Perhaps this is caused by the difference in rhetorical styles and the lengths of articles.",
        "The average number of sentences in an editorial article is 28.7, while that in a general article is 13.9.",
        "However, we found problems in his data.",
        "For instance, the data contained ambiguous antecedents like dou-shi (the same person) or dou-sha (the same company) as correct antecedents.",
        "We replaced these ‘correct answers’ with their explicit names.",
        "We also removed zeros in quoted sentences because they are quite different from other sentences.",
        "In addition, we decided to use the output of ChaSen 2.2 .93 and CaboCha 0.344 instead of the morphological information and the dependency information provided by the Kyoto Corpus since classification of the joshi (particles) in the Corpus was not satisfactory for our purpose.",
        "Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha’s dependency output is very similar to that of the Corpus."
      ]
    },
    {
      "heading": "2 Methodology",
      "text": [
        "In this paper, we combine heuristic ranking rules and machine learning.",
        "First, we describe how we extract possible antecedents (candidates).",
        "Second, we describe the rule-based ranking system and the machine learning system.",
        "Finally, we describe how to combine these two methods.",
        "We consider only anaphors for noun phrases following Seki and other studies.",
        "We assume that zeros are already detected.",
        "We also assume zeros are located at the starting point of a bunsetsu that contains a yougen (a verb, an adjective, or an auxiliary verb).",
        "From now on, we use ‘verb’ instead of ‘yougen’ for readability.",
        "A zero’s bunsetsu is a bunsetsu that contains the zero.",
        "We further assume that each zero’s grammatical case is already determined by a zero detector and represented by corresponding particles.",
        "If a zero is the subject of a verb, its case is represented by the particle ga.",
        "If it is an object, it is represented by wo.",
        "If it is an object2, it is represented by ni.",
        "We consider only these three cases.",
        "A zero’s particle means such a particle.",
        "Since complex sentences are hard to analyze, each sentence is automatically split at conjunctive postpositions (setsuzoku joshi) (Okumura and Tamura, 1996; Ehara and Kim, 1996).",
        "In order to distinguish the original complex sentence and the simpler sentences after the split, we call the former just a ‘sentence’ and the latter ‘post-split sentences’.",
        "When a conjunctive postposition appears in a relative clause, we do not split the sentence at that position.",
        "In the examples below, we split the first sentence at ‘and’ but do not split the second sentence at ‘and’.",
        "She bought the book and sold it to him.",
        "She bought the book that he wrote and sold.",
        "A zero’s sentence is the (original) sentence that contains the zero.",
        "From now on, stands for a zero and stands for a candidate of ’s antecedent.",
        "’s particle is denoted ZP, and CP stands for ’s next word that is ’s particle or a punctuation symbol."
      ]
    },
    {
      "heading": "2.1 Enumeration of possible antecedents",
      "text": [
        "Candidates (possible antecedents) are enumerated on the fly by using the following method.",
        "1.",
        "We extract a content word sequence as a candidate if it is followed by a case marker (kaku joshi, e.g., ga, wo), a topic marker (wa or mo), or a period.",
        "2.",
        "If ’s is a verb, an adjective, an auxi",
        "lary verb, an adverb, or a relative pronoun (ChaSen’s meishi-hijiritsu, e.g., koto (what he did) and toki (when she married)), is excluded.",
        "(If is a closing quotation mark, is checked instead.)",
        "3.",
        "If ’s is a pronoun or an adverbial noun (a",
        "noun that can also be used as an adverb, i.e., ChaSen’s meishi -fukushi-kanou), is excluded.",
        "4.",
        "If is dou-shi (the person), it is replaced by the latest person name.",
        "If is dou-sha (the company), it is replaced by the latest organization name.",
        "If is dou+suffix, it is replaced by the latest candidate that has the same suffix.",
        "For this task, we use a named entity recognizer (Isozaki and Kazawa, 2002).",
        "The first step extracts a content word sequence from a bunsetsu.",
        "The second step excludes verb phrases, adjective phrases, and clauses.",
        "As a result, we obtain only noun phrases.",
        "The third step excludes adverbial expressions like kotoshi (this year).",
        "The forth step resolves anaphors like definite noun phrases in English.",
        "We should also resolve pronouns, but we did not because useful pronouns are rare in newspaper articles.",
        "In addition, we register a resolved zero as a new candidate.",
        "If ’s antecedent is determined to be , a new candidate is created for future zeros.",
        "is a copy of except that ’s particle is ZP and ’s location is ’s location.",
        "In the training phase of the machine learning approach, we consider a correct answer as .",
        "Then, we can remove far candidates from the list.",
        "In this way, our zero resolver creates a ‘general purpose’ candidate list.",
        "However, some of the candidates are inappropriate for certain zeros.",
        "A verb usually does not have the same entity in two or more cases (Murata and Nagao, 1997).",
        "Therefore, our resolver excludes candidates that are filled in other cases of the verb.",
        "When a verb has two or more zeros, we resolve ga first, and its best candidate is excluded from the candidates of wo or ni."
      ]
    },
    {
      "heading": "2.2 Ranking rules",
      "text": [
        "Various heuristics have been reported in past literature.",
        "Here, we use the following heuristics.",
        "1.",
        "Forward center ranking (Walker et al., 1994): (topic empathy subject object2 object others).",
        "2.",
        "Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence.",
        "If a zero is an object, its antecedent is perhaps an object.",
        "3.",
        "Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a per",
        "son or an animal, and so on.",
        "We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate.",
        "Goi Taikei also has 300,000 words in about 3,000 semantic categories.",
        "(See Appendix A for details.)",
        "4.",
        "Demotion of candidates in a relative clause (rentai shuushoku setsu): Usually, Japanese zeros do not refer to noun phrases in relative clauses (Ehara and Kim, 1996).",
        "(See Appendix B for details.)",
        "Since sentences in newspaper articles are often complex and relative clauses are sometimes nested, we refine this rule in the following way.",
        "A candidate’s relative clause is the inmost relative clause that contains the candidate.",
        "A relative clause finishes at the noun modified by the clause.",
        "If appears before the finishing noun of ’s relative clause, the clause is still unfinished at .",
        "Otherwise, the clause is alreadyfinished.",
        "A quoted clause (with or without quotation marks “ ”) indicated by a quotation marker ‘to’ (‘that’ in ‘He said that she is ...’) is also regarded as a relative clause.",
        "We demote after ’s relative clause finishes.",
        "It is not clear how to combine the above heuristics consistently.",
        "Here, we sort the candidates in a lexicographical order based on the above features of candidates.",
        "For instance, we can use a lexicographically increasing order defined by Vi Re Ag Di Sa , where Vi (for violation) is 1 if the candidate violates the semantic constraint.",
        "Otherwise, Vi is 0.",
        "Re (for relative) is 1 if the candidate is in a relative clause that has already finished before .",
        "Otherwise, Re is 0.",
        "Ag (for agreement) is 0 if CP=ZP holds.",
        "(Since most of wa and mo are subjects, they are regarded as ga here.)",
        "Otherwise, Ag is 1.",
        "Di (for distance) is a non-negative integer that represents the number of post-split sentences between and .",
        "If a candidate’s Di is larger than maxDi, it is removed from the candidate list.",
        "Sa (for salience) is 0 if CP is wa.",
        "Sa is 1 if CP is ga. Sa is 2 if CP is ni.",
        "Sa is 3 if CP is wo.",
        "Otherwise, Sa is 4.",
        "We did not implement empathy because it makes the program more complex, and empathy verbs are rare in newspaper articles.",
        "For instance, holds.",
        "The first ranked (lexicographically smallest) candidate is regarded as the best candidate.",
        "We employ lexicographical ordering because it seems the simplest way to rank candidates.",
        "We put Vi in the first place because Vi was often regarded as a constraint in the past literature.",
        "We put Ag before Sa because Kameyama’s method was better than Walker’s in Okumura and Tamura (1996).",
        "Therefore, Vi Ag Sa is expected to be a good ordering.",
        "The above ordering is an instance of this."
      ]
    },
    {
      "heading": "2.3 Machine Learning",
      "text": [
        "Although we can consider various other features for zero pronoun resolution, it is difficult to combine these features consistently.",
        "Therefore, we use machine learning.",
        "Support Vector Machines (SVMs) have shown good performance in various tasks in Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002; Hirao et al., 2002).",
        "Yoshino (2001) and Iida et al.",
        "(2003b) also applied SVM to Japanese zero pronoun resolution, but the usefulness of each feature was not clear.",
        "Here, we add features for complex sentences and analyze useful features by examining the weights of features.",
        "We use the following features of as well as CP.",
        "CSem ’s semantic categories.",
        "(See Appendix A.)",
        "CPPOS CP’s part-of-speech (POS) tags (rough and detailed).",
        "CPOS The POS tags of the last word of .",
        "Siblings When CP is wa or mo, it is not clear whether is a subject.",
        "However, a verb rarely has the same entity in two or more cases.",
        "Therefore, if modifies a verb that has a subject, is not a subject.",
        "In the next example, hon is an object of katta.",
        "Ano / hon wa / Tomu ga / katta.",
        "that book=topic Tom=subj bought (As for that book, Tom bought it.)",
        "In order to learn such things, we use sibling case-markers that modify the same verb as ’s features.",
        "We also use the following features of as well as ZP.",
        "Conjunct The latest conjunctive postposition in the sentence and its classification (Okumura and Tamura, 1996; Yoshimoto, 1986).",
        "ZSem Semantic categories of the verb that modifies.",
        "We use them only when the verb is sahen meishi + ‘suru.’ Sahen meishi is a kind of noun that can be an object of the verb ‘suru’ (do) (e.g., ‘shopping’ in ‘do the shopping’).",
        "We also use the following relations between and as well as Ag, Vi, and Di.",
        "Relative Whether is in a relative clause.",
        "Unfinished Whether the relative clause is unfinished at .",
        "Intra (for intrasentential coreference) Whether explicitly appears in ’s sentence.",
        "Sometimes it is difficult to distinguish cataphora from anaphora.",
        "Even if an antecedent appears in a preceding sentence, it is sometimes easier to find a candidate after , as illustrated by the case of ‘his’ in the next English example.",
        "Bob and John separately drove to Charlie’s house....",
        "Since his car broke down, John made a phone call.",
        "Even if Di holds, Intra does not necessarily hold because we introduce resolved zeros as new candidaites.",
        "Parallel Whether appears in a clause parallel to a clause in which a zero appears.",
        "This will be useful for the resolution of a zero as with ‘it’ in the next English sentence.",
        "He turned on the TV set and she turned it off.",
        "Immediate Whether ’s bunsetsu appears immediately before ’s.",
        "In the following sentence, a candidate ryoushin is located immediately before the zero.",
        "(His parents believe that ( ) is still alive.)",
        "Here, we represent all of the above features by a boolean value: 0 or 1.",
        "Semantic categories can be represented by a 0/1 vector whose -th component corresponds to the -th semantic category.",
        "Similarly, POS tags can be represented by a 0/1 vector whose -th component corresponds to the -th POS tag.",
        "On the other hand, Di has a non-negative integer value.",
        "We also encode the distance by a 0/1 vector whose -th component corresponds to the fact that the distance is .",
        "The distance has an upper bound maxDi.",
        "In this way, we can represent a candidate by a boolean feature vector.",
        "A candidate ’s feature vector is denoted .",
        "If a boolean feature appears only once in the given data, we remove the feature from the feature vectors.",
        "The training data comprise the set of pairs , where is if is a correct antecedent of a zero.",
        "Otherwise, is .",
        "By using the training data, SVM finds a decision function , where is the feature vector of a candidate and s are support vectors selected from the training data.",
        "is a constant.",
        "is called a kernel function.",
        "If holds, is classified as a correct antecedent."
      ]
    },
    {
      "heading": "2.4 Combinations",
      "text": [
        "Here, we use the following method to combine the ordering and SVM.",
        "1.",
        "Sort candidates by using the lexicographical order.",
        "2.",
        "Classify each candidate by using SVM in this order.",
        "3.",
        "If is positive, stop there and sort the evaluated candidates by in decreasing order.",
        "4.",
        "If no candidate satisfies , return the best candidate in terms of"
      ]
    },
    {
      "heading": "3 Results",
      "text": [
        "We conducted leave-one(-article)-out experiments.",
        "For each article, 29 other articles were used for training.",
        "Table 1 compares the scores of the above methods.",
        "‘First’ picks up the first candidate given by a given lexicographical ordering.",
        "The acronym ‘vrads’ stands for the lexicographical ordering of Vi Re Ag Di Sa .",
        "‘Best’ picks up the best candidate in terms of without checking whether it",
        "= The combination is worse than ‘first’ or ‘best.’ = (Seki et al., 2002a), = (Seki et al., 2002b)",
        "is positive.",
        "Consequently, it is independent of the ordering (unless two or more candidates have the best value).",
        "‘Svm1’ uses the ordinary SVM (Vap-nik, 1995) while ‘svm2’ uses a modified SVM for unbalanced data (Morik et al., 1999), which gives a large penalty to misclassification of a minority (= positive) example.5 In general, svm2 accepts more cadidates than svm1.",
        "According to this table, svm1 is too severe to exclude only bad candidates.",
        "We also tried the maximum entropy model 6 (mem) and C4.5, but they were also too severe.",
        "When we use SVM, we have to choose a good kernel for better performance.",
        "Here, we used the linear kernel ( ) for SVM because it was best according to our preliminary experiments.",
        "We set maxDi at 3 because it gave the best results.",
        "The table also shows Seki’s scores for reference, but it is not fair to compare our scores with Seki’s scores directly because our data is slightly different from Seki’s.",
        "The number of zeros in general in our data is 347, while Seki resolved 355 detected zeros in (Seki et al., 2002a) and 404 in (Seki et al., 2002b).",
        "The number of zeros in our editorial is 514, while (Seki et al., 2002a) resolved 498 detected zeros.",
        "In order to overcome the data sparseness,",
        "where = number of negative examples/number of positive examples.",
        "Seki used unannotated articles to get co-occurrence statistics.",
        "Without the data, their scores degraded about 5 points.",
        "We have not conducted experiments that use unannotated corpora; this task is our future work.",
        "As we expected, instances of Vi Ag Sa show good performance.",
        "Without SVMs, ‘vrads’ is the best for general in the table.",
        "It is interesting that such a simple ordering gives better performance than SVMs.",
        "However, the combination of ‘vrads’ and ‘svm2’ (= vrads+svm2) gives even better results.",
        "In general, ‘ +svm2’ is better than ‘first’ and ‘ +svm1.’ With SVM, ‘davrs+svm2’ gave the best result for editorial.",
        "Editorial articles sometimes use anthropomorphism (e.g., The report says ... ) that violates semantic constraints.",
        "Therefore, ‘vrads’ does not work well for such cases.",
        "Table 2 shows the weights of the above features determined by svm2 for a fold of the leave-one-out experiment of ‘vrads+svm2.’ The weights can be given by rewriting as .",
        "This table shows that Kameyama’s property-sharing (Ag), semantic violation (Vi), candidate’s particle (CP), and distance (Di) are very important features.",
        "Our new features Parallel, Unfinished, and Intra also obtained relatively large weights.",
        "Semantic categories ‘suggestions’ and ‘report’ reflect the fact that some articles use anthropomorphism.",
        "These weights will be useful to design better heuristic rules.",
        "The fact that Unfinished’s weight almost cancels Relative’s weight justifies the"
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "Yoshino (2001) used an ordinary SVM with .",
        "He tried to find useful features by feature elimination.",
        "Since features are not completely independent, removing a heavily weighted feature does not necessarily degrade the system’s performance.",
        "Hence, feature elimination is more reliable for reducing the number of features.",
        "However, feature elimination takes a long time.",
        "On the other hand, feature weights can give rough guidance.",
        "According to the table, our new features (Parallel, Unfinished, and Intra) obtained relatively large weights.",
        "This implies their importance.",
        "When we eliminated these three features, vrads+svm2’s score for editorial dropped by 4 points.",
        "Therefore, combinations of these three features are useful.",
        "Recently, Iida et al.",
        "(2003a) proposed an SVM-based tournament model that compares two candidates and selects the better one.",
        "We would like to compare or combine their method with our method.",
        "For further improvement, we have to make the morphological analyzer and the dependency analyzer more reliable because they make many mistakes when they process complex sentences.",
        "SVM has often been criticized as being too slow.",
        "However, the above data were small enough for the state-of-the-art SVM programs.",
        "The number of examples in each set of training data was about 5,000– 6,100, and each training phase took only 5–18 seconds on a 2.4-GHz Pentium 4 machine."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "In order to make Japanese zero pronoun resolution more reliable, we have to maintain complicated heuristic rules or prepare a large amount of training data.",
        "In order to alleviate this problem, we combined simple lexicographical orderings and SVMs.",
        "It turned out that a simple lexicographical ordering performed better than SVM, but their combination gave even better performance.",
        "By examining feature weights, we found that features for complex sentences are important in zero pronoun resolution.",
        "We confirmed this by feature elimination."
      ]
    },
    {
      "heading": "References",
      "text": []
    },
    {
      "heading": "Appendix A: Semantic constraint check",
      "text": [
        "One word may belong to two or more semantic categories, and each semantic category has superclasses (e.g., ‘father’ has the superclass ‘parent’).",
        "Therefore, we keep all of these categories and their superclasses in a category list for the candidate.",
        "If the candidate is not registered in Goi Taikei and can be decomposed into shorter words, we use the semantic categories of the last candidate word because the last word is usually the head word.",
        "Furthermore, we use named entity recognition.",
        "When the candidate contains a person name, an organization name, or a location name, a corresponding semantic category is added to the list.",
        "A verb may have two or more translation patterns.",
        "Here, we use disjunction of the constraints.",
        "For instance, the verb ‘yomu’ (to read) has three translation patterns.",
        "The first and second patterns’ subjects are restricted to AGENT, and the third pattern’s subject is restricted to PEOPLE.",
        "Therefore, the subject of yomu is accepted if and only if it is AGENT or PEOPLE."
      ]
    },
    {
      "heading": "Appendix B: Relative clause analysis",
      "text": [
        "We have to be careful about parallel structures for this analysis.",
        "According to CaboCha, Kare gain the next example modifies a verb katte, which modifies another verb karita.",
        "However, katte is contained in a clause that modifies the noun hon.",
        "Kare ga / katte / kanojo ga / karita / he=subj bought she=subj borrowed hon wa / omoshiroi ."
      ]
    },
    {
      "heading": "book=topic interesting",
      "text": [
        "(The book that he bought and she borrowed is interesting.)",
        "The particle no (= “’s” in English) directly modifies a noun.",
        "For instance, Taro in Taro no hon (Taro’s book) is a book that Taro wrote or a book that Taro has.",
        "From this point of view, we also mark A in A no B (A’s B) as a candidate in a relative clause."
      ]
    }
  ]
}
