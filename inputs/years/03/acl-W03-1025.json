{
  "info": {
    "authors": [
      "Xiaoqiang Luo"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W03-1025",
    "title": "A Maximum Entropy Chinese Character-Based Parser",
    "url": "https://aclweb.org/anthology/W03-1025",
    "year": 2003
  },
  "references": [
    "acl-A94-1030",
    "acl-J96-1002",
    "acl-J96-3004",
    "acl-P00-1058",
    "acl-P96-1019",
    "acl-P97-1003",
    "acl-P97-1041",
    "acl-W00-1201",
    "acl-W97-0301"
  ],
  "sections": [
    {
      "heading": "Abstract 1 Introduction: Why Parsing Characters?",
      "text": [
        "After Linguistic Data Consortium (LDC) released the Chinese Treebank (CTB) developed at UPenn (Xia et al., 2000), various statistical Chinese parsers (Bikel and Chiang, 2000; Xu et al., 2002) have been built.",
        "Techniques used in parsing English have been shown working fairly well when applied to parsing Chinese text.",
        "As there is no word boundary in written Chinese text, CTB is manually segmented into words and then labeled.",
        "Parsers described in (Bikel and Chiang, 2000) and (Xu et al., 2002) operate at word-level with the assumption that input sentences are pre-segmented.",
        "The paper studies the problem of parsing Chinese unsegmented sentences.",
        "The first motivation is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter.",
        "The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter.",
        "It is widely known that Chinese word-segmentation is a hard problem.",
        "There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower .",
        "The agreement between multiple human subjects is even lower (Wu and Fung, 1994).",
        "The reason is that human subjects may differ in segmenting things like personal names (whether family and given names should be one or two words), number and measure units and compound words, although these ambiguities do not change a human being’s understanding of a sentence.",
        "Low agreement between humans affects directly evaluation of machines’ performance (Wu and Fung, 1994) as it is hard to define a gold standard.",
        "It does not necessarily imply that machines cannot do better than humans.",
        "Indeed, if we train a model with consistently segmented data, a machine may do a better job in “remembering” word segmentations.",
        "As will be shown shortly, it is straightforward to encode word-segmentation information in a character-The paper presents a maximum entropy Chinese character-based parser trained on the Chinese Treebank (“CTB” henceforth).",
        "Word-based parse trees in CTB are first converted into character-based trees, where word-level part-of-speech (POS) tags become constituent labels and character-level tags are derived from word-level POS tags.",
        "A maximum entropy parser is then trained on the character-based corpus.",
        "The parser does word-segmentation, POS-tagging and parsing in a unified framework.",
        "An average label F-measure and word-segmentation F-measure are achieved by the parser.",
        "Our results show that word-level POS tags can improve significantly word-segmentation, but higher-level syntactic strutures are of little use to word segmentation in the maximum entropy parser.",
        "A word-dictionary helps to improve both word-segmentation and parsing accuracy.",
        "based parse tree.",
        "Parsing Chinese character streams therefore does effectively word-segmentation, part-of-speech (POS) tagging and constituent labeling at the same time.",
        "Since syntactical information influences directly word-segmentation in the proposed character-based parser, CTB allows us to test whether or not syntactic information is useful for word-segmentation.",
        "A third advantage of parsing Chinese character streams is that Chinese words are more or less an open concept and the out-of-vocabulary (OOV) word rate is high.",
        "As morphology of the Chinese language is limited, extra care is needed to model unknown words when building a word-based model.",
        "Xu et al.",
        "(2002), for example, uses an independent corpus to derive word classes so that unknown words can be parsed reliably.",
        "Chinese characters, on the other hand, are almost closed.",
        "To demonstrate the OOV problem, we collect a word and character vocabulary from the first sentences of CTB, and compute their coverages on the corresponding word and character tokenization of the last of the corpus.",
        "The word-based OOV rate is while the character-based OOV rate is only .",
        "The first step of training a character-based parser is to convert word-based parse trees into character-based trees.",
        "We derive character-level tags from word-level POS tags and encode word-boundary information with a positional tag.",
        "Word-level POSs become a constituent label in character-based trees.",
        "A maximum entropy parser (Ratnaparkhi, 1997) parser is then built and tested.",
        "Many language-independent feature templates in the English parser can be reused.",
        "Lexical features, which are language-dependent, are used to further improve the baseline models trained with language-independent features only.",
        "Word-segmentation results will be presented and it will be shown that POSs are very helpful while higher-level syntactic structures are of little use to word-segmentation – at least in the way they are used in the parser.",
        "2 Word-Tree to Character-Tree CTB is manually segmented and is tokenized at word level.",
        "To build a Chinese character parser, we first need to convert word-based parse trees into character trees.",
        "A few simple rules are employed in this conversion to encode word boundary information: 1.",
        "Word-level POS tags become labels in character trees.",
        "2.",
        "Character-level tags are inherited from word-level POS tags after appending a positional tag; 3.",
        "For single-character words, the positional tag is “s”; for multiple-character words, the first character is appended with a positional tag “b”, last character with a positional tag “e”, and all middle characters with a positional tag “m”.",
        "An example will clarify any ambiguity of the rules.",
        "For example, a word-parse tree",
        "Note that the word-level POS “NR” becomes a label of the constituent spanning the three characters “ ”.",
        "The character-level tags of the constituent “ ” are the lower-cased word-level POS tag plus a positional letter.",
        "Thus, the first character “ ” is assigned the tag “nrb” where “nr” is from the word-level POS tag and “b” denotes the beginning character; the second (middle) character “ ” gets the positional letter “m”, signifying that it is in the middle, and the last character “ ” gets the positional letter “e”, denoting the end of the word.",
        "Other words in the sentence are mapped similarly.",
        "After the mapping, the number of terminal tokens of the character tree is larger than that of the word tree.",
        "It is clear that character-level tags encode word boundary information, and chunk-level' labels are word-level POS tags.",
        "Therefore, parsing a Chinese character sentence is effectively doing word-segmentation, POS-tagging and constructing syntactic structure at the same time."
      ]
    },
    {
      "heading": "3 Model and Features",
      "text": [
        "The maximum entropy parser (Ratnaparkhi, 1997) is used in this study, for it offers the flexibility of integrating multiple sources of knowledge into a model.",
        "The maximum entropy model decomposes , the probability of a parse tree given a sentence , into the product of probabilities of individual parse",
        "tions are an ordered sequence, where is the number of actions associated with the parse .",
        "The mapping from a parse tree to its unique sequence of actions is 1-to-1.",
        "Each parse action is either tag-gingaword, chunkingtagged words, extending an existing constituent to another constituent, or checking whether an open constituent should be closed.",
        "Each component model takes the exponential form: (2) where is a normalization term to ensure that is a probability, is a feature function (often binary) and is the weight of .",
        "Given a set of features and a corpus of training data, there exist efficient training algorithms (Dar-roch and Ratcliff, 1972; Berger et al., 1996) to find the optimal parameters .",
        "The art of building a maximum entropy parser then reduces to choosing “good” features.",
        "We break features used in this study into two categories.",
        "The first set of features are derived from predefined templates.",
        "When these templates are applied to training data, features are generated automatically.",
        "Since these templates can be used in any language, features generated this way are referred to language-independent features.",
        "The second category of features incorporate lexical information into the model and are primarily designed to improve word-segmentation.",
        "This set of features are language-dependent since a Chinese word dictionary is required."
      ]
    },
    {
      "heading": "3.1 Language-Independent Feature Templates",
      "text": [
        "The maximum entropy parser (Ratnaparkhi, 1997) parses a sentence in three phases: (1) it first tags the input sentence.",
        "Multiple tag sequences are kept in the search heap for processing in later stages; (2) Tagged tokens are grouped into chunks.",
        "It is possible that a tagged token is not in any chunk; (3) A chunked sentence, consisting of a forest of many subtrees, is then used to extend a subtree to a new constituent or join an existing constituent.",
        "Each extending action is followed by a checking action which decides whether or not to close the extended constituent.",
        "In general, when a parse action is carried out, the context information, i.e., the input sentence and preceding parse actions , is represented by a forest of subtrees.",
        "Feature functions operate on the forest context and the next parse action.",
        "They are all of the form:",
        "where is a binary function on the context.",
        "Some notations are needed to present features.",
        "We use to denote an input terminal token, its tag (preterminal), a chunk, and a constituent label, where the index is relative to the current subtree: the subtree immediately left to the current is indexed as , the second left to the current subtree is indexed as , the subtree immediately to the right is indexed as , so on and so forth.",
        "represents the root label of the child of the subtree.",
        "If , the child is counted from right.",
        "With these notations, we are ready to introduce language-independent features, which are broken down as follows:"
      ]
    },
    {
      "heading": "Tag Features",
      "text": [
        "In the tag model, the context consists of a window of five tokens – the token being tagged and two tokens to its left and right – and two tags on the left of the current word.",
        "The feature templates are tabulated in Table 1 (to save space, templates are grouped).",
        "At training time, feature templates are instantiated by the training data.",
        "For example, when the template “ ” is applied to the first character of the sample sentence, “(IP (NP (NP (NR /nrb /nrm /nre)) (NP (NN /nnb /nne ) (NN /nnb /nne))) (VP (VV /vvb /vve) ) (PU /pus ) )”, a feature *BOUNDARY* is generated.",
        "Note that is the token on the left and in this case, the boundary of the sentence.",
        "The template “ ” is instantiated similarly as ."
      ]
    },
    {
      "heading": "Chunk Features",
      "text": [
        "As character-level tags have encoded the chunk label information and the uncertainly about a chunk action is low given character-level tags, we limit the chunk context to a window of three subtrees – the current one plus its left and right subtree.",
        "in Table 2 denotes the label of the subtree if it is not",
        "is the chunk label plus the tag of its right most child if the tree is a chunk; Otherwise is the constituent label of the tree.",
        "Again, we use the sentence (1) as an example.",
        "Assume that the current forest of subtrees is (NR /nrb /nrm /nre ) /nnb /nne /nnb /nne /vvb /vve /pus , and the current subtree is “ /nnb”, then instantiating the template would result in a feature .",
        "Extend Features Extend features depend on previous subtree and the two following subtrees.",
        "Some features uses child labels of the previous subtree.",
        "For example, the interpretation of the template on line 4 of Table 3 is that is the root label of the previous subtree, is the label of the rightmost child of the previous tree, and is the root label of the current subtree."
      ]
    },
    {
      "heading": "Check Features",
      "text": [
        "Most of check feature templates again use constituent labels of the surrounding subtrees.",
        "The template on line 1 of Table 4 is unique to the check model.",
        "It essentially looks at children of the current constituent, which is intuitively a strong indication whether or not the current constituent should be closed.",
        "is the constituent label of the subtree (relative to the current one).",
        "is the child label of the current constituent."
      ]
    },
    {
      "heading": "3.2 Language-Dependent Features",
      "text": [
        "The model described so far does not depend on any Chinese word dictionary.",
        "All features derived from templates in Section 3.1 are extracted from training data.",
        "A problem is that words not seen in training data may not have “good” features associated with them.",
        "Fortunately, the maximum entropy framework makes it relatively easy to incorporate other sources of knowledge into the model.",
        "We present a set of language-dependent features in this section, primarily for Chinese word segmentation.",
        "The language-dependent features are computed from a word list and training data.",
        "Formerly, let be a list of Chinese words, where characters are separated by spaces.",
        "At the time of tagging characters (recall word-segmentation information is encoded in character-level tags), we test characters within a window of five (that is, two characters to the left and two to the right) and see if a character either starts, occurs in any position of, or ends any word on the list .",
        "This feature templates are summarized in Ta",
        "word on the list .",
        "Similarly, tests if the character occurs in any position of any word on the list , and tests if the character is the last position of any word on the list .",
        "A word list can be collected to encode different semantic or syntactic information.",
        "For example, a list of location names or personal names may help the model to identify unseen city or personal names; Or a closed list of functional words can be collected to represent a particular set of words sharing a POS.",
        "This type of features would improve the model robustness since unseen words will share features fired for seen words.",
        "We will show shortly that even a relatively small word-list improves significantly the word-segmentation accuracy."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "All experiments reported here are conducted on the latest LDC release of the Chinese Treebank, which consists of about words.",
        "Word parse trees are converted to character trees using the procedure described in Section 2.",
        "All traces and functional tags are stripped in training and testing.",
        "Two results are reported for the character-based parsers: the F-measure of word segmentation and F-measure of constituent labels.",
        "Formally, let be the number of words of the reference sentence and its parser output, respectively, and be the number of common words in the sentence of test set, then the word segmentation F-measure is The F-measure of constituent labels is computed similarly: where and are the number of constituents in the reference parse tree and parser output, respectively, and is the number of common constituents.",
        "Chunk-level labels converted from POS tags (e.g., “NR”, “NN” and “VV” etc in (1)) are included in computing label F-measures for character-based parsers."
      ]
    },
    {
      "heading": "4.1 Impact of Training Data",
      "text": [
        "The first question we have is whether CTB is large enough in the sense that the performance saturates.",
        "The first set of experiments are intended to answer this question.",
        "In these experiments, the first CTB is used as the training set and the rest as the test set.",
        "We start with of the training set and increase the training set each time by .",
        "Only language-independent features are used in these experiments.",
        "Figure 1 shows the word segmentation F-measure and label F-measure versus the amount of training data.",
        "As can be seen, F-measures of both word segmentation and constituent label increase monotonically as the amount of training data increases.",
        "If all training data is used, the word segmentation F-measure is and label F-measure"
      ]
    },
    {
      "heading": "Word seg F – measure and Label F – measure vs. training size Percent of training data",
      "text": [
        ".",
        "These results show that language-independent features work fairly well – a major advantage of data-driven statistical approach.",
        "The learning curve also shows that the current training size has not reached a saturating point.",
        "This indicates that there is room to improve our model by getting more training data."
      ]
    },
    {
      "heading": "4.2 Effect of Lexical Features",
      "text": [
        "In this section, we present the main parsing results.",
        "As it has not been long since the second release of CTB and there is no commonly-agreed training and test set, we divide the entire corpus into 10 equal partitions and hold each partition as a test set while the rest are used for training.",
        "For each training-test configuration, a baseline model is trained with only language independent features.",
        "Baseline word segmentation and label F-measures are plotted with dotted-line in Figure 2.",
        "We then add extra lexical features described in Section 3.1 to the model.",
        "Lexical questions are derived from a 58K-entry word list.",
        "The word list is broken into 4 sub-lists based on word length, ranging from 2 to 5 characters.",
        "Lexical features are computed by answering one of the three questions in Table 5.",
        "Intuitively, these questions would help the model to identify word boundaries, which in turn ought to improve the parser.",
        "This is confirmed by results shown in Figure 2.",
        "The solid two lines represent results with enhanced lexical questions.",
        "As can be seen, lexical questions improve significantly both word segmentation and parsing across all experiments.",
        "This is not surprising as lexical features derived from the word list are complementary to language-independent features computed from training sentences.",
        "Another observation is that results vary greatly across experiment configurations: for the model trained with lexical features, the second experiment has a label F-measure and word-segmentation F-measure , while the sixth experiment has a label F-measure and word-segmentation F-measure .",
        "The large variances justify multiple experiment runs.",
        "To reduce the variances, we report numbers averaged over the 10 experiments in Table 6.",
        "Numbers on the row starting with “WS” are word-segmentation results, while numbers on the last row are F-measures of constituent labels.",
        "The second column are average F-measures for the baseline model trained with only language-independent features.",
        "The third column contains F-measures for the model trained with extra lexical features.",
        "The last column are releative error reduction.",
        "The best average word-segmentation F-measure is and label F-measure is",
        "language-independent features.",
        "LexFeat: plus lexical features.",
        "Numbers are averaged over the 10 experiments in Figure 2."
      ]
    },
    {
      "heading": "4.3 Effect of Syntactic Information on Word-segmentation",
      "text": [
        "Since CTB provides us with full parse trees, we want to know how syntactic information affects word-segmentation.",
        "To this end, we devise two sets of experiments: 1.",
        "We strip all POS tags and labels in the Chinese Treebank and retain only word boundary information.",
        "To use the same maximum entropy parser, we represent word boundary by dummy constituent label “W”.",
        "For example, the sample sentence (1) in Section 2 is represented as: (W /wb /wm /we) (W /wb /we) (W /wb /we) (W /wb /we) (W /ws ).",
        "With these two representations of CTB, we repeat the 10 experiments of Section 4.2 using the same lexical features.",
        "Word-segmentation results are plotted in Figure 3.",
        "The model trained with word boundary information has the worst performance, which is not surprising as we would expect information such as POS tags to help disambiguate word boundaries.",
        "What is surprising is that syntactic information beyond POS tags has little effect on word-segmentation – there is practically no difference between the solid line (for the model trained with full parse trees) and the dashed-line (for the model trained with POS information) in Figure 3.",
        "This result suggests that most ambiguities of Chinese word boundaries can be resolved at lexical level, and high-level syntactic information does not help much to word segmentation in the current parser."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "Bikel and Chiang (2000) and Xu et al.",
        "(2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study.",
        "Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic context-free grammar (PCFG) similar to (Collins, 1997); the other is based on statistical TAG (Chiang, 2000).",
        "About F-measure is reported in (Bikel and Chiang, 2000).",
        "Xu et al.",
        "(2002) is also based on PCFG, but enhanced with lexical features derived from the ASBC corpus2.",
        "Xu et al.",
        "(2002) reports an overall F-measure when the same training and test set as (Bikel and Chiang, 2000) are used.",
        "Since our parser operates at character level, and more training data is used, the best results are not directly comparable.",
        "The middle point of the learning curve in Figure 1, which is trained with roughly 100K words, is at the same ballpark of (Xu et al., 2002).",
        "The contribution of this work is that the proposed character-based parser does word-segmentation, POS tagging and parsing in a unified framework.",
        "It is the first attempt to our knowledge that syntactic information is used in word-segmentation.",
        "Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.",
        "Without knowing and controlling testing conditions, it is nearly impossible to compare results in a meaningful way.",
        "Therefore, we will compare our approach with some related work only without commenting on segmentation accuracy.",
        "Wu and Tseng (1993) contains a good problem statement of Chinese word-segmentation and also outlines a few segmentation algorithms.",
        "Our method is supervised in that the training data is manually labeled.",
        "Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation.",
        "Sproat et al.",
        "(1996) employs stochastic finite state machines to find word boundaries.",
        "Luo and Roukos (1996) proposes to use a language model to select from ambiguous word-segmentations.",
        "All these work assume that a lexicon or some manually segmented data or both are available.",
        "There are numerous work exploring semi-supervised or unsupervised algorithms to segment Chinese text.",
        "Ando and Lee (2003) uses a heuristic method that does not require segmented training data.",
        "Peng and Schuurmans (2001) learns a lexicon and its unigram probability distribution.",
        "The automatically learned lexicon is pruned using a mutual information criterion.",
        "Peng and Schuurmans (2001) requires a validation set and is therefore semi-supervised."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "We present a maximum entropy Chinese character-based parser which does word-segmentation, POS tagging and parsing in a unified framework.",
        "The flexibility of maximum entropy model allows us to integrate into the model knowledge from other sources, together with features derived automatically from training corpus.",
        "We have shown that a relatively small word-list can reduce word-segmentation error by as much as , and a word-segmentation F-measure and label F-measure are obtained by the character-based parser.",
        "Our results also show that POS information is very useful for Chinese word-segmentation, but higher-level syntactic information benefits little to word-segmentation."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Special thanks go to Hongyan Jing and Judith Hochberg who proofread the paper and corrected many typos and ungrammatical errors.",
        "The author is also grateful to the anonymous reviewers for their insightful comments and suggestions.",
        "This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No.",
        "N66001-99-2-8916.",
        "The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred."
      ]
    }
  ]
}
