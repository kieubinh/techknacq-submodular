{
  "info": {
    "authors": [
      "Liang Chen",
      "Naoyuki Tokuda",
      "Akira Nagai"
    ],
    "book": "International Workshop on Information Retrieval With Asian Languages",
    "id": "acl-W03-1104",
    "title": "A Differential LSI Method for Document Classification",
    "url": "https://aclweb.org/anthology/W03-1104",
    "year": 2003
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We have developed an effective probabilistic classifier for document classification by introducing the concept of the differential document vectors and DLSI (differential latent semantics index) spaces.",
        "A simple posteriori calculation using the intra and extra-document statistics demonstrates the advantage of the DLSI space-based probabilistic classifier over the popularly used LSI space-based classifier in classification performance."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "This paper introduces a new efficient supervised document classification procedure, whereby given a number of labeled documents preclassified into a finite number of appropriate clusters in the database, the classifier developed will select and classify any of new documents introduced into an appropriate cluster within the learning stage.",
        "The vector space model is widely used in document classification, where each document is represented as a vector of terms.",
        "To represent a document by a document vector, we assign weights to its components usually evaluating the frequency of occurrences of the corresponding terms.",
        "Then the standard pattern recognition and machine learning methods are employed for document classification(Li et al., 1991; Farkas, 1994; Svingen, 1997; Hyotyniemi, 1996; Merkl, 1998; Benkhalifa et al., 1999; Iwayama and Tokunaga, 1995; Lam and Low, 1997; Nigam et al., 2000).",
        "In view of the inherent flexibility imbedded within any natural language, a staggering number of dimensions seem required to represent the featuring space of any practical document comprising the huge number of terms used.",
        "If a speedy classification algorithm can be developed (Sch¨utze and Silverstein, 1997), the first problem to be resolved is the dimensionality reduction scheme enabling the documents’ term projection onto a smaller subspace.",
        "Like an eigen-decomposition method extensively used in image processing and image recognition (Sirovich and Kirby, 1987; Turk and Pentland, 1991), the Latent Semantic Indexing (LSI) method has proved to be a most efficient method for the dimensionality reduction scheme in document analysis and extraction, providing a powerful tool for the classifier (Sch¨utze and Silverstein, 1997) when introduced into document retrieval with a good performance confirmed by empirical studies (Deer-wester et al., 1990; Berry et al., 1999; Berry et al., 1995).The LSI method has also demonstrated its efficiency for automated cross-language document retrieval in which no query translation is required (Littman et al., 1998).",
        "In this paper, we will show that exploiting both of the distances to, and the projections onto, the LSI space improves the performance as well as the robustness of the document classifier.",
        "To do this, we introduce, as the major vector space, the differential LSI (or DLSI) space which is formed from the differences between normalized intra and extra-document vectors and normalized centroid vectors of clusters where the intra and extra-document refers to the documents included within or outside of the given cluster respectively.",
        "The new classifier sets up a Baysian posteriori probability function for the differential document vectors based on their projections on DLSI space and their distances to the DLSI space, the document category with a highest probability is then selected.",
        "A similar approach is taken by Moghaddam and Pentland for image recognition (Moghaddam and Pentland, 1997; Moghaddam et al., 1998).",
        "We may summarize the specific features introduced into the new document classification scheme based on the concept of the differential document vector and the DLSI vectors:",
        "1.",
        "Exploiting the characteristic distance of the differential document vector to the DLSI space and the projection of the differential document onto the DLSI space, which we believe to denote the differences in word usage between the document and a cluster’s centroid vector, the differential document vector is capable of capturing the relation between the particular document and the cluster.",
        "2.",
        "A major problem of context sensitive semantic grammar of natural language related to synonymy and polysemy can be dampened by the major space projection method endowed in the LSIs used.",
        "3.",
        "A maximum for the posteriori likelihood func",
        "tion making use of the projection of differential document vector onto the DLSI space and the distance to the DLSI space provides a consistent computational scheme in evaluating the degree of reliability of the document belonging to the cluster.",
        "The rest of the paper is arranged as follows: Section 2 will describe the main algorithm for setting up the DLSI-based classifier.",
        "A simple example is computed for comparison with the results by the standard LSI based classifier in Section 3.",
        "The conclusion is given in Section 4."
      ]
    },
    {
      "heading": "2 Main Algorithm",
      "text": []
    },
    {
      "heading": "2.1 Basic Concepts",
      "text": [
        "A term is defined as a word or a phrase that appears at least in two documents.",
        "We exclude the so-called stop words such as “a”, “the” , ”of” and so forth.",
        "Suppose we select and list the terms that appear in the documents as .",
        "For each document in the collection, we assign each of the terms with a real vector , with , where is the local weighting of the term in the document indicating the significance of the term in the document, while is a global weight of all the documents, which is a parameter indicating the importance of the term in representing the documents.",
        "Local weights could be either raw occurrence counts, boolean, or logarithms of occurrence counts.",
        "Global ones could be no weighting (uniform), domain specific, or entropy weighting.",
        "Both of the local and global weights are thoroughly studied in the literatures (Raghavan and Wong, 1986; Luhn, 1958; van Rijsbergen, 1979; Salton, 1983; Salton, 1988; Lee et al., 1997), and will not be discussed further in this paper.",
        "An example will be given below: where ,is the total number of times that term appears in the collection, the number of times the term appears in the document , and the number of documents in the collection.",
        "The document vector can be normalized as",
        "The normalized centroid vector of a cluster can be calculated in terms of the normalized vector as , where and by the following formula: is a mean vector of the member documents in the cluster which are normalized as ; i.e., .",
        "We can always take itself as a normalized vector of the cluster.",
        "A differential document vector is defined as where and are normalized document vectors satisfying some criteria as given above.",
        "A differential intra-document vector is the differential document vector defined as , where and are two normalized document vectors of same cluster.",
        "A differential extra-document vector is the differential document vector defined as , where and are two normalized document vectors of different clusters.",
        "The differential term by intra and extra-document matrices and are respectively defined as a matrix, each column of which comprise a differential intra and extra document vector respectively."
      ]
    },
    {
      "heading": "2.2 The Posteriori Model",
      "text": [
        "Any differential term by document by matrix of , say, of rank , whether it is a differential term by intra-document matrix or a differential term by extra-document matrix can be decomposed by SVD into a product of three matrices: , such that (left singular matrix) and (right singular matrix) are an -by-and by unitary matrices respectively with the first columns of U and V being the eigenvectors of and respectively.",
        "Here is called singular matrix expressed by ) of , deleting other terms; we similarly obtain the two new matrices and by keeping the left most columns of and respectively.",
        "The product of ,and provide a reduced matrix of which approximately equals to .",
        "How we choose an appropriate value of , a reduced degree of dimension from the original matrix, depends on the type of applications.",
        "Generally we choose for , and the corresponding is normally smaller for the differential term by intra-document matrix than that for the differential term by extra document matrix, because the differential term by extra-document matrix normally has more columns than the differential term by intra-document matrix has.",
        "Each of differential document vector could find a projection on the dimensional fact space spanned by the columns of .",
        "The projection can easily be obtained by .",
        "Noting that the mean of the differential intra-(extra-) document vectors are approximately , we may assume that the differential vectors formed follows a high-dimensional Gaussian distribution so that the likelihood of any differential vector will be given by where , and is the covariance of the distribution computed from the training set expressed .",
        "Since constitutes the eigenvalues of , we have , and thus we have , where .",
        "Because is a diagonal matrix, can be represented by a simpler form as: .",
        "It is most convenient to estimate it as where .",
        "In practice, ( ) could be estimated by fitting a function (say, ) to the available ( ), or we could let since we only need to compare the relative probability.",
        "Because the columns of are orthogonal vectors, could be estimated by .",
        "Thus, the likelihood function could be estimated by (2) where , , ,and is the rank of matrix .",
        "In diag ), where are nonnegtive square roots of eigenvalues of ,for and for .",
        "The diagonal elements of are sorted in the decreasing order of magnitude.",
        "To obtain a new reduced matrix , we simply keep the k-by-k leftmost-upper corner matrix ( practice, may be chosen as , and may be substituted for .",
        "Note that in equation (2), the term describes the projection of onto the DLSI space, while approximates the distance from to DLSI space.",
        "When both and are computed, the Baysian posteriori function can be computed as: where is set to where is the number of clusters in the database 1 while is set to",
        "where , , ,and is the rank of matrix .",
        "In practice, may be set to , and to if both and are sufficiently large.",
        "6.",
        "Construct the term by extra document matrix , such that each of its column is an extra differential document vector.",
        ", , ,is the rank of matrix .",
        "In practice, may be set to , and to if both and are sufficiently large.",
        "8.",
        "Define the posteriori function:",
        "terms as well as their frequencies of occurrence in the document, so that a normalized document vector is obtained for the document from equation (1).",
        "For each of the clusters of the data base, repeat the procedure of item 2-4 below.",
        "2.",
        "Using the document to be classified, construct a differential document vector , where",
        "diag , then with a proper , define the to approximate .",
        "We now define the likelihood function as, is the normalized vector giving the center or centroid of the cluster.",
        "3.",
        "Calculate the intra-document likelihood func",
        "tion , and calculate the extra document likelihood function for the document.",
        "4.",
        "Calculate the Bayesian posteriori probability function .",
        "5.",
        "Select the cluster having a largest as the recall candidate."
      ]
    },
    {
      "heading": "3 Examples and Comparison",
      "text": []
    },
    {
      "heading": "3.1 Problem Description",
      "text": [
        "We demonstrate our algorithm by means of numerical examples below.",
        "Suppose we have the following 8 documents in the database: :Algebra and Geometry Education System.",
        ":The Software of Computing Machinery.",
        ":Analysis and Elements of Geometry.",
        ":Introduction to Modern Algebra and Geometry.",
        ":Theoretical Analysis in Physics.",
        ":Introduction to Elements of Dynamics.",
        ":Modern Alumina.",
        ":The Foundation of Chemical Science.",
        "And we know in advance that they belong to 4 clusters, namely, , , and where belongs to Computer related field, to Mathematics, to Physics, and to Chemical Science.",
        "We will show, as an example, below how we will set up the classifier to classify the following new document: : “The Elements of Computing Science.” We should note that a conventional matching method of “common” words does not work in this example, because the words “compute” and, “science” in the new document appear in and separately, while the word “elements” occur in both and simultaneously, giving no indication on the appropriate candidate of classification simply by counting the “common” words among documents.",
        "We will now set up the DLSI-based classifier and LSI-based classifier for this example.",
        "First, we can easily set up the document vectors of the database giving the term by document matrix by simply counting the frequency of occurrences; then"
      ]
    },
    {
      "heading": "3.2 DLSI Space-Based Classifier",
      "text": [
        "The normalized form of the centroid of each cluster is shown in Table 2.",
        "Following the procedure of the previous section, it is easy to construct both the differential term by intra-document matrix and the differential term by extra-document matrix.",
        "Let us denote the differential term by intra-document matrix by and the differential term by extra-document matrix by respectively.",
        "Note that the ’s and ’s can be found in the matrices shown in tables 1 and 2.",
        "Now that we know and ,we can decompose them into and by using SVD algorithm, where 0.25081 0.0449575 -0.157836 -0.428217 0.130941 0.172564 0.143423 0.0844264 -0.240236 0.162075 -0.043428 0.257507 -0.25811 -0.340158 -0.282715 -0.166421 -0.237435 -0.125328 0.439997 -0.15309 0.300435 -0.391284 0.104845 0.193711 0.0851724 0.0449575 -0.157836 0.0549164 0.184643 -0.391284 0.104845 0.531455 -0.25811 -0.340158 -0.282715 -0.166421 0.135018 0.0449575 -0.157836 -0.0904727 0.466072 -0.391284 0.104845 -0.289423 -0.237435 -0.125328 0.439997 -0.15309 0.296578 0.172564 0.143423 -0.398707 -0.124444 0.162075 -0.043428 -0.0802377 -0.25811 -0.340158 -0.282715 -0.166421 -0.237435 -0.125328 0.439997 -0.15309 0.0851724 0.0449575 -0.157836 0.0549164 -0.124444 0.162075 -0.043428 -0.0802377",
        "We now choose the number in such a way that remains sufficiently large.",
        "Let us choose and to test the classifier.",
        "Now using equations (3), (4) and (5), we can calculate the , is chosen as the cluster to which the new document belongs.",
        "Because both ,are actually quite small, we may here set , and .",
        "The last row of Table 3 clearly shows that Cluster , that is, “Mathematics” is the best possibility regardless of the parameters or chosen, showing the robustness of the computation."
      ]
    },
    {
      "heading": "3.3 LSI Space-Based Classifier",
      "text": [
        "As we have already explained in Introduction, the LSI based-classifier works as follows: First, employ an SVD algorithm on the term by document matrix to set up an LSI space, then the classification is completed within the LSI space.",
        "Using the LSI-based classifier, our experiment show that, it will return , namely “Physics”, as the most likely cluster to which the document belongs.",
        "This is obviously a wrong result."
      ]
    },
    {
      "heading": "3.4 Conclusion of the Example",
      "text": [
        "For this simple example, the DLSI space-based approach finds the most reasonable cluster for the document “The elements of computing science”, while the LSI approach fails to do so."
      ]
    },
    {
      "heading": "4 Conclusion and Remarks",
      "text": [
        "We have made use of the differential vectors of two normalized vectors rather than the mere scalar cosine of the angle of the two vectors in document classification procedure, providing a more effective means of document classifier.",
        "Obviously the concept of differential intra and extra-document vectors imbeds a richer meaning than the mere scalar measure of cosine, focussing the characteristics of each document wheere the new classifier demonstrates an improved and robust performance in document classification than the LSI-based cosine approach.",
        "Our model considers both of the projections and the distances of the differential vectors to the DLSI spaces, improving the adaptability of the conventional LSI-based method to the unique characteristics of the individual documents which is a common weakness of the global projection schemes including the LSI.",
        "The simple experiment demonstrates convincingly that the performance of our model outperforms the standard LSI space-based approach.",
        "Just as the cross-language ability of LSI, DLSI method should also be able to be used for document classification of docuements in multiple languages.",
        "We have tested our method using larger collection of texts, we will give details of the results elsewhere.",
        "."
      ]
    }
  ]
}
