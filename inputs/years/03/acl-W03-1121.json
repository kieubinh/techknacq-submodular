{
  "info": {
    "authors": [
      "Euisok Chung",
      "Yi-Gyu Hwang",
      "Myung-Gil Jang"
    ],
    "book": "International Workshop on Information Retrieval With Asian Languages",
    "id": "acl-W03-1121",
    "title": "Korean Named Entity Recognition Using HMM and CoTraining Model",
    "url": "https://aclweb.org/anthology/W03-1121",
    "year": 2003
  },
  "references": [
    "acl-A97-1029",
    "acl-P02-1060",
    "acl-W99-0613"
  ],
  "sections": [
    {
      "heading": "ABSTRACT",
      "text": []
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Named entity(NE) recognition is important for recent sophisticated information service such as question answering and text mining since it recognizes the words to present the core information in text.",
        "In particular, the NE recognizer is important module in the well-known question answering systems such as FALCON, IBM[3][10].",
        "NE recognizer is well suited for the recognition of answer type which can be equal to the NE type or not.",
        "Although an answer is not exactly matched to the NE, these two types can be mapping to each other by using WordNet[3].",
        "NE recognition can be explained with two steps, NE detection and NE classification.",
        "Whereas NE detection is to catch the named entities in the text, NE classification is to classify NE into person, organization or location.",
        "In Korean, NE detection is difficult since each word of name entity has not specific features such as the capitalizing feature of English.",
        "It has high dependence on the large amounts of hand-labeled data and the NE dictionary, even though these are tedious and expensive to create.",
        "In the case of NE classification, NE can be classified with the clues such as inner word and context word.",
        "Although these clue words present the feature of NE type, it can be used in detecting the NE since the contained word and context word can be used in determining the boundary of NE.",
        "However, the clue words can provoke ambiguity to determine the NE type since various NEs can share the same clue word.",
        "Therefore, we devise the statistical model based NE recognizer which can unify the detection and classification.",
        "Furthermore, we consider unlabeled data based statistical learning to extend the initial seed data.",
        "The weakly supervised learning technique is Co Training method.",
        "In this paper, we describe the HMM based Korean NE recognition and Co Training method for HMM based boosting."
      ]
    },
    {
      "heading": "2. The Problem",
      "text": [
        "NE dictionary is not enough to cover all of the NEs since there are a few types of NEs besides single word.",
        "We classify Korean NE into three types.",
        "The first is single word type, the second is compound noun type, and the third is noun phrase type.",
        "The single word type is usually single noun.",
        "The second type, compound noun, is composed of a few words and affix.",
        "The third type can have grammatical morphemes besides nouns.",
        "The example is described in Figure 1.",
        "It describes NEs with PERSON(PER), LOCATION (LOC), and ORGANIZATION(ORG).",
        "In each type, second sentence is the result of morphological analysis.",
        "The example shows the diversity of the Korean NE type.",
        "In the single word type, if the dictionary has the word, then the NE could be detected easily.",
        "However, compound noun type and noun phrase type require a tremendous number of entries in the dictionary.",
        "Moreover, in Korean these kinds of NE types are used differently in each people.",
        "Therefore, it should use the clue word to recognize NE which is shown as the compound noun or noun phrase type.",
        "This clue word, which is context or inner word of NE, can be used in NE detection and classification, but we found that the clue word can provoke another problem which is the ambiguity; thus different types of NEs can share the same clue word.",
        "Which means that the clue word dictionary cannot be the unique solution.",
        "Consequently, we suggest three approaches for NE recognition.",
        "The first is feature dictionary based approach which classify the clue words and generate feature types of the context or inner word of NE.",
        "The second is statistical approach which needs named entity tagged corpus and context model to recognize NE.",
        "The third is unlabeled data based boosting approach since statistical approach, which needs hand-labeled NE tagged corpus, cannot avoid data sparseness."
      ]
    },
    {
      "heading": "3. Related Works",
      "text": [
        "Statistical approach in NE recognition can be classified into supervised learning and weakly supervised learning.",
        "Supervised learning is based on labeled data.",
        "On the other hand, weakly supervised method is the learning approach to combine labeled data and unlabeled data.",
        "From the supervised learning point of view, the most representative research is HMM based NE recognition.",
        "It builds various",
        "bigram models of NEs and predicts next NE type with the previous history, lexical item and NE type.",
        "Using simple word feature Bikel shows F-measure 90% in English [4].",
        "Zhou’s HMM based chunk tagger approach adopt more detailed feature than Bikel’s, and show F-measure 94.3%[5].",
        "In this paper, when we designed feature model, we followed the HMM-based chunk tagger approach considering the property of Korean NE.",
        "Recently there have been many researches in weakly supervised learning technique to combine labeled data and unlabeled data.",
        "Co-Training method Blum is most famous approach to boost the initial learning data with unlabeled data[2].",
        "Blum showed that using a large unlabeled data to boost performance of a learning algorithm could be used to classify web pages when only a small set of labeled examples is available [2].",
        "Nigam demonstrated that when learning from labeled and unlabeled data, algorithms explicitly leveraging a natural independent split of the features outperform algorithms that do not[7][8].",
        "Collins and Singer showed that the use of unlabeled data can reduce the requirements for supervision to just 7 simple “seed” rules [9].",
        "In addition to a heuristic based on decision list learning, they also presented a boosting-like framework that builds on ideas from Blum[2]."
      ]
    },
    {
      "heading": "4. Named entity recognizer",
      "text": [
        "In this paper, we propose the Korean NE recognizing methodology.",
        "It is based on the feature model, HMM based statistical model and Co-Training based boosting model.",
        "■"
      ]
    },
    {
      "heading": "4.1 Feature model",
      "text": [
        "NE recognition depends on various clues to distinguish each type.",
        "The inside and outside properties of NE could be the clues which can be composed of a few clue class.",
        "The class consists of ‘character feature’, ‘named entity dictionary’, ‘inner word’, and ‘context word’.",
        "It is described in Table 1.",
        "(1) ‘character feature’ shows the digit or Chinese feature in Korean NE.",
        "Digit feature is used to recognize MONEY, TIME, and others.",
        "(2) ‘named entity dictionary’ means that we should build NE dictionaries.",
        "The dictionaries are composed of DATE, PERSON, LOCATION, and ORGANIZATION.",
        "(3) ‘inner word’ consists of suffix word and constituent word of NE.",
        "(4) ‘context word’ is the word set adjacent to NE.",
        "These feature values are constructed manually and are used with history in annotating NE type.",
        "As stated above, all the feature classes have ambiguity since one feature word can be another feature type.",
        "Therefore, we cannot recognize NE with only feature dictionaries."
      ]
    },
    {
      "heading": "4.2 HMM based statistical model",
      "text": [
        "NE recognizer should adopt statistical model since it is impossible to construct the dictionary having all of the NE entries and moreover, clue word dictionary can provoke ambiguity.",
        "For instance, proper noun such as person name creates everyday.",
        "It is unknown word problem.",
        "In the feature model, some feature classes can share the same clue words.",
        "Simply, both DATE and TIME can have the DIGIT character feature.",
        "Therefore, we adopt statistical model for NE recognition.",
        "For the statistical approach, we construct NE tagged documents, design NE context model, and suggest forward-backward view based boosting approach.",
        "We build 300 named entity tagged documents in the newspaper article domains such as economy, performance and travel.",
        "The labeled data is tagged by using tagging tools.",
        "We attached only NE tags to the text and do not consider morphological information because Korean morphological tag is various to the analyzer.",
        "Furthermore, we build statistical information extractor to learn the NE distributional information.",
        "The labeled data is used in constructing NE statistical data.",
        "To build NE context model for statistical NE recognizer, we analyze 201 NEs in labeled documents.",
        "From the three points of view, word feature, inner word feature, and context word feature, we analyze NE examples.",
        "From the analysis, word feature can be classified into single word and compound word.",
        "Inner feature has property of full string and inner word.",
        "From the context word feature, we recognized three kinds of features such as root, adjacent morpheme and no context.",
        "The result of analysis is described in Table 2.",
        "Finally, we can build four types of NE context model such as lexical model, feature model, POS(part of speech) model and root model.",
        "Whereas the rule based approach needs enormous hand-crafted rules, statistical model has the advantages of simplicity, expansity and robustness in the named entity model.",
        "The most representative approach of statistical model is HMM based approach.",
        "To adopt HMM based model, we define HMM state and build NE context model which can cover various NE contextual information.",
        "For HMM based approach, HMM state should be defined.",
        "HMM state is the type of NE constituent.",
        "Thus, S_LOC is the state which can be the first lexical item of LOC typed named entity.",
        "C_LOC is the middle state of the NE.",
        "E_LOC is the final morpheme.",
        "In the case of U_LOC, single word is the NE word.",
        "For example, location name “Inchon International Airport” can be tagged with “Inchon/S_LOC International /C_LOC Airport/ E_LOC” and another named entity “Seoul” can be labeled with “Seoul/U_LOC”.",
        "The HMM state is described in Table 5.",
        "The NE context model is composed of four types such as morp(morphology), root, POS and feature.",
        "Through this NE context model, the statistical data is learned from the tagged corpus, and is used in computing probability of predicting HMM state to lexical item.",
        "In the case of feature type, “Indiana” has “DicLOC” since it is discovered in LOC dictionary and “professor” is allocated with “Position-PERSON” since it is used with position clue word.",
        "The context model and example is presented in table 3.",
        "We designed the NE context model with the divided view types such as forward/backward views which means left-right NE context view.",
        "In each view type, the probability is computed with the product of state transitional probability and lexical probability.",
        "In forward view, state transitional probability is Pr(Si|Si-1, mi-1) which predict ith HMM state Si with i-1th state Si-1 and morpheme mi-1, and lexical probability is Pr(mi|Si, mi-1) which predict ith morpheme mi with ith state Si and morpheme mi -1.",
        "In table 4, the state of morpheme mi-1 “ eui”, the state Si, is “NE_U” and next Si-1 is “-“.",
        "Which means that state transitional probability can be computed by count(-, NE_U, ■ eui)/count(-, eui).",
        "In the case of lexical probability, it is computed by count(NE_U, eui, M stiglitz)/count(NE_U, eui).",
        "The difference of forward view and backward view can be explained with state transitional probability.",
        "Whereas forward view computes current state probability with pre -state, backward view computes current state probability with next-state.",
        "Thus forward view considers left contextual information.",
        "On the other hand, backward view considers right context.",
        "For the statistical approach, we build labeled data for supervised learning and propose four typed NE context model which considers left right contextual information.",
        "Furthermore, we adopt smoothing model based on the modified Kneser-Ney smoothing technique[6] since HMM based approach needs smoothing technique for better result.",
        "After allocating the lexical probability and state transitional probability to the HMM state which is coupled with the morpheme of sentence, the recognition is processed by Viterbi algorithm.",
        "Then, NE is recognized in the input sentence."
      ]
    },
    {
      "heading": "4.3 CoTraining based boosting model",
      "text": [
        "Co Training method is most famous approach to boost the initial learning data with unlabeled data.",
        "In this paper, we propose the method to apply Co Training method to the HMM based statistical approach.",
        "The main idea is to divide view type of the context model into forward view and backward view.",
        "Simply the forward view’s output, which is the result of NE recognition to the unlabeled data, is used for input data in the backward view and vise verse.",
        "From the iteration of the Co-Training procedure, both views could boost each other, which means that the both statistical data could increase by using unlabeled data.",
        "HMM based CoTraining approach is described in Figure 2.",
        "(1) CurrentPath is Forward View, k-th rounds (2) Unlabeled text random sampling (3) if CurrentPath = Forward View then (3-1) Forward View HMM based NE tagging else (3-2) Backward View HMM based NE tagging endif (4) extract n-best tagging result (5) if CurrentPath = Forward View then (5-1) extract Backward View based statistical data from n best taggin result (5-2) boost Backward data (5-3) CurrentPath = Backward View else (5-4) extract Forward View based statistical data from n best taggin result (5-5) boost Forward data (5-6) CurrentPath = Forward View endif (6) if this round is k-th rounds ?",
        "then CoTraining exit",
        "The CoTraining algorithm which boosts HMM based NE statistical model is described in Figure 3.",
        "Here the procedure of CoTraining is shown for boosting between divided statistical models.",
        "In first step, the current view type and the number of times of learning round are determined(1) since CoTraining approach, which is based on feature redundant principle, divides the learning model and reflects one learning result to the other learning model.",
        "The next step is random sampling of unlabeled text data(2).",
        "After random sampling, the NE statistical data should be extracted from the sampling data.",
        "At this time the next step depends on the current view type which can be Forward-View or BackwardView.",
        "If current learning view is Forward-View, the next step is forward model based NE tagging task(3-1), otherwise, the current learning view is Backward-View, the next step is backward view based task(3-2).",
        "After that, from the result of NE tagging, the nth best tagging results are selected(4) and added to the learning data.",
        "From the first step till now, NE tagging using unlabeled data is processed, and the tagging result prepares for boosting other view type.",
        "If CurrentPath is ForwardView(5), backward view data is extracted from the tagging result(5-1) and boost backward view data(5-2), and then CurrentPath change to BackwardView(5-3).",
        "Otherwise, CurrentPath is BackwardView(5), forward view data is extracted from the tagging result(5-4) and boost forward view data(5-5), and then CurrentPath change to ForwardView(5-6).",
        "Finally, the round is checked whether it is over the pre defined iteration time or not(6).",
        "If it pass the time, the procedure ends, otherwise the random sampling step repeats."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "We evaluate named entity recognition with two kinds of experiments.",
        "One is the performance of named entity recognition which unified morp model and feature model to learn statistical information.",
        "The other experiment is about CoTraining performance."
      ]
    },
    {
      "heading": "5.1 Named Entity Recognition Test",
      "text": [
        "We evaluate NE recognition with morp model and feature model since POS statistical data, which is extracted from labeled corpus, has some POS tagging error, and root model cannot be implemented due to the difficult to determine what the root is.",
        "Therefore, in this paper we suggest the evaluation of the morp model, feature model and morp/feature model considering forward/backward view: (1) morp model based forward view [M/F], (2) morp model based backward view [M/B], (3) morp/feature model based forward view [M F/F], (4) morp/feature model based backward view [MF/B], (5) morp /feature model based forward/backward view [MF /FB], which combination of forward/backward view is based on forward-backward algorithm.",
        "We give 0.93 weight to morp model and give 0.07 weight to feature model.",
        "It is optimized from many experiments.",
        "With 300 NE tagged documents, we train the recognizer with 270 documents which is compose of 90 economy, 90",
        "performance, and 90 travel articles.",
        "Other 30 documents, which is not used in training, are used as test data.",
        "Test data has three types such as untrained 10 economical documents (N10), untrained 30 documents(N30), and trained 270 documents(T270).",
        "The result of the experiment is described in Table 6.",
        "From the result, we find that the best result of economy 10 test data is morp/feature based backward view(MF/B) type with F-measure 0.67 considering all NE types.",
        "If we considered only PLO(Person/Location/Organization) type, MF/FB type is the best with F-measure 0.77.",
        "The first reason that PLO recognition is better than other types is that the PLO trained data is more abundant.",
        "Figure 4 shows that the PLO type is a large number in 300 labeled documents.",
        "The second reason is that the statistical model is not appropriate to some kinds of NE types such as DATE, QUANTITY.",
        "These type is more appropriate when the pattern based approach is adopted.",
        "In the future, we test the NE recognizer with balanced trained data in each NE types."
      ]
    },
    {
      "heading": "5.2 CoTraining Test",
      "text": [
        "We evaluate Co-Training for the NE recognition using an unlabeled economy domain newspaper articles(39,480 articles).",
        "For the training data, we train the NE recognizer with 270 labeled articles and use 10 evaluate articles as test data in each Co Training iteration.",
        "In each training step, we increase the number of the NE labels from the high ranked NE tagging results in proportion to the training data.",
        "We test CoTraining with morp model which is divided into forward view and backward view.",
        "After 145 iteration, backward view F-measure decreases from 0.615 to 0.6, but forward view F-measure increases from 0.558 to 0.57."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "In this paper, we suggest HMM based NE recognition and boosting technique.",
        "Through this research, we met some technical problems such as NE context model unification, unbalanced labeled corpus, and boosting degradation.",
        "(1) We consider HMM based NE recognition with four types NE context models which are derived from the analysis of NE labeled data.",
        "However, we cannot unify all of the models in unique way since the weighted integration of the models do not guarantee the good performance.",
        "(2) In using the labeled data, we meet the problem that the recognition of NE types depends highly on the size of learning data.",
        "(3) In HMM based CoTraining, the test result shows that high-performance model enhance low-performance model but high-performance model decrease step by step.",
        "Finally, we conclude that (1) unification issue of various context models can be resolved with Maximum Entropy model which can combine diverse forms of contextual information in a principled manner, (2) unbalanced labeled corpus issue may be resolved by gathering contextual information independently from the labeled corpus.",
        "It makes it possible to balance learning data in each type, and (3) degradation of the boosting approach is not difficult problem since the boosting step in each round can be controlled with pre -test."
      ]
    }
  ]
}
