{
  "info": {
    "authors": [
      "Ding Liu",
      "Chengqing Zong"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W03-1703",
    "title": "Utterance Segmentation Using Combined Approach Based on Bi-Directional N-Gram and Maximum Entropy",
    "url": "https://aclweb.org/anthology/W03-1703",
    "year": 2003
  },
  "references": [
    "acl-A00-1012",
    "acl-A97-1004",
    "acl-J96-1002",
    "acl-P98-1070",
    "acl-P99-1026"
  ],
  "sections": [
    {
      "text": [
        "Speech recognition Language analysis and generation Input speech Output (text or speech)"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes a new approach to segmentation of utterances into sentences using a new linguistic model based upon Maximum-entropy-weighted Bidirectional N-grams.",
        "The usual N-gram algorithm searches for sentence boundaries in a text from left to right only.",
        "Thus a candidate sentence boundary in the text is evaluated mainly with respect to its left context, without fully considering its right context.",
        "Using this approach, utterances are often divided into incomplete sentences or fragments.",
        "In order to make use of both the right and left contexts of candidate sentence boundaries, we propose a new linguistic modeling approach based on Maximum-entropy-weighted Bidirectional N-grams.",
        "Experimental results indicate that the new approach significantly outperforms the usual N-gram algorithm for segmenting both Chinese and English utterances."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Due to the improvement of speech recognition technology, spoken language user interfaces, spoken dialogue systems, and speech translation systems are no longer only laboratory dreams.",
        "Roughly speaking, such systems have the structure shown in Figure 1.",
        "In these systems, the language analysis module takes the output of speech recognition as its input, representing the current utterance exactly as pronounced, without any punctuation symbols marking the boundaries of sentences.",
        "Here is an example: ��YW����_f/ 9 ����#�� _9*2�'A#2'9_f/913 V91Y7 .",
        "(this way please please take this elevator to the ninth floor the floor attendant will meet you at your elevator entrance there and show you to room 913.)",
        "As the example shows, it will be difficult for a text analysis module to parse the input if the utterance is not segmented.",
        "Further, the output utterance from the speech recognizer usually contains wrongly recognized words or noise words.",
        "Thus it is crucial to segment the utterance before further language processing.",
        "We believe that accurate segmentation can greatly improve the performance of language analysis modules.",
        "Stevenson et al.",
        "have demonstrated the difficulties of text segmentation through an experiment in which six people, educated to at least the Bachelor’s degree level, were required to segment into sentences broadcast transcripts from which all punctuation symbols had been removed.",
        "The experimental results show that humans do not always agree on the insertion of punctuation symbols, and that their segmentation performance is not very good (Stevenson and Gaizauskas, 2000).",
        "Thus it is a great challenge for computers to perform the task automatically.",
        "To solve this problem, many methods have been proposed, which can be roughly classified into two categories.",
        "One approach is based on simple acoustic criteria, such as non-speech intervals (e.g. pauses), pitch and energy.",
        "We can call this approach acoustic segmentation.",
        "The other approach, which can be called linguistic segmentation, is based on linguistic clues, including lexical knowledge, syntactic structure, semantic information etc.",
        "Acoustic segmentation can not always work well, because utterance boundaries do not always correspond to acoustic criteria.",
        "For example: ��<pause> 7W�<pause> VXf *A� Z-)9-fl#<pause> �,O<pause> ���LI'�.",
        "Since the simple acoustic criteria are inadequate, linguistic clues play an indispensable role in utterance segmentation, and many methods relying on them have been proposed.",
        "This paper proposes a new approach to linguistic segmentation using a Maximum-entropy-weighted Bidirectional N-gram-based algorithm (MEBN).",
        "To evaluate the performance of MEBN, we conducted experiments in both Chinese and English.",
        "All the results show that MEBN outperforms the normal N-gram algorithm.",
        "The remainder of this paper will focus on description of our new approach for linguistic segmentation.",
        "In Section 2, some related work on utterance segmentation is briefly reviewed, and our motivations are described.",
        "Section 3 describes MEBN in detail.",
        "The experimental results are presented in Section 4.",
        "Finally, Section 5 gives our conclusion."
      ]
    },
    {
      "heading": "2 Related Work and Our Motivations 2.1 Related Work",
      "text": [
        "Stolcke et al.",
        "(1998, 1996) proposed an approach to detection of sentence boundaries and disfluency locations in speech transcribed by an automatic recognizer, based on a combination of prosodic cues modeled by decision trees and N-gram language models.",
        "Their N-gram language model is mainly based on part of speech, and retains some words which are particularly relevant to segmentation.",
        "Of course, most part-of-speech taggers require sentence boundaries to be pre-determined; so to require the use of part-of-speech information in utterance segmentation would risk circularity.",
        "Cet-tolo et al.’s (1998) approach to sentence boundary detection is somewhat similar to Stolcke et al.’s.",
        "They applied word-based N-gram language models to utterance segmentation, and then combined them with prosodic models.",
        "Compared with N-gram language models, their combined models achieved an improvement of 0.5% and 2.3% in precision and recall respectively.",
        "Beeferman et al.",
        "(1998) used the CYBERPUNC system to add intra-sentence punctuation (especially commas) to the output of an automatic speech recognition (ASR) system.",
        "They claim that, since commas are the most frequently used punctuation symbols, their correct insertion is by far the most helpful addition for making texts legible.",
        "CYBERPUNC augmented a standard trigram speech recognition model with lexical information concerning commas, and achieved a precision of 75.6% and a recall of 65.6% when testing on 2,317 sentences from the Wall Street Journal.",
        "Gotoh et al.",
        "(1998) applied a simple non-speech interval model to detect sentence boundaries in English broadcast speech transcripts.",
        "They compared their results with those of N-gram language models and found theirs far superior.",
        "However, broadcast speech transcripts are not really spoken language, but something more like spoken written language.",
        "Further, radio broadcasters speak formally, so that their reading pauses match sentence boundaries quite well.",
        "It is thus understandable that the simple non-speech interval model outperforms the N-gram language model under these conditions; but segmentation of natural utterances is quite different.",
        "Zong et al.",
        "(2003) proposed an approach to utterance segmentation aiming at improving the performance of spoken language translation (SLT) systems.",
        "Their method is based on rules which are oriented toward key word detection, template matching, and syntactic analysis.",
        "Since this approach is intended to facilitate translation of Chinese-to-English SLT systems, it rewrites long sentences as several simple units.",
        "Once again, these results cannot be regarded as general-purpose utterance segmentation.",
        "Furuse et al.",
        "(1998) similarly propose an input-splitting method for translating spoken language which includes many long or ill-formed expressions.",
        "The method splits an input into well-balanced translation units, using a semantic dictionary.",
        "Ramaswamy et al.",
        "(1998) applied a maximum entropy approach to the detection of command boundaries in a conversational natural language user interface.",
        "They considered as their features words and their distances to potential boundaries.",
        "They posited 400 feature functions, and trained their weights using 3000 commands.",
        "The system then achieved a precision of 98.2% in a test set of 1900 commands.",
        "However, command sentences for conversational natural language user interfaces contain much smaller vocabularies and simpler structures than the sentences of natural spoken language.",
        "In any case, this method has been very helpful to us in designing our own approach to utterance segmentation.",
        "There are several additional approaches which are not designed for utterance segmentation but which can nevertheless provide useful ideas.",
        "For example, Reynar et al.",
        "(1997) proposed an approach to the disambiguation of punctuation marks.",
        "They considered only the first word to the left and right of any potential sentence boundary, and claimed that examining wider context was not beneficial.",
        "The features they considered included the candidate’s prefix and suffix; the presence of particular characters in the prefix or suffix; whether the candidate was honorific (e.g. Mr., Dr.); and whether the candidate was a corporate designator (e.g. Corp.).",
        "The system was tested on the Brown Corpus, and achieved a precision of 98.8%.",
        "Elsewhere, Nakano et al.",
        "(1999) proposed a method for incrementally understanding user utterances whose semantic boundaries were unknown.",
        "The method operated by incrementally finding plausible sequences of utterances that play crucial roles in the task execution of dialogues, and by utilizing beam search to deal with the ambiguity of boundaries and with syntactic and semantic ambiguities.",
        "Though the method does not require utterance segmentation before discourse processing, it employs special rule tables for discontinuation of significant utterance boundaries.",
        "Such rule tables are not easy to maintain, and experimental results have demonstrated only that the method outperformed the method assuming pauses to be semantic boundaries."
      ]
    },
    {
      "heading": "2.2 Our motivations",
      "text": [
        "Though numerous methods for utterance segmentation have been proposed, many problems remain unsolved.",
        "One remaining problem relates to the language model.",
        "The N-gram model evaluates candidate sentence boundaries mainly according to their left context, and has achieved reasonably good results, but it can’t take into account the distant right context to the candidate.",
        "This is the reason that N-gram methods often wrongly divide some long sentences into halves or multiple segments.",
        "For example:小王病了一个星期.",
        "The N-gram method is likely to insert a boundary mark between “了” and “一”, which corresponds to our everyday impression that, if reading from the left and not considering several more words to the right of the current word, we will probably consider “小王病了” as a whole sentence.",
        "However, we find that, if we search the sentence boundaries from right to left, such errors can be effectively avoided.",
        "In the present example, we won’t consider “一个星期” as a whole sentence, and the search will be continued until the word “小” is encountered.",
        "Accordingly, in order to avoid segmentation errors made by the normal N-gram method, we propose a reverse N-gram segmentation method (RN) which does seek sentence boundaries from right to left.",
        "Further, we simply integrate the two N-gram methods and propose a bidirectional N-gram method (BN), which takes into account both the left and the right context of a candidate segmentation site.",
        "Since the relative usefulness or significance of the two N-gram methods varies depending on the context, we propose a method of weighting them appropriately, using parameters generated by a maximum entropy method which takes as its features information about words in the context.",
        "This is our Maximum-Entropy-Weighted Bidirectional N-gram-based segmentation method.",
        "We hope MEBN can retain the correct segments discovered by the usual N-gram algorithm, yet effectively skip the wrong segments."
      ]
    },
    {
      "heading": "3 Maximum-Entropy-Weighted Bidirectional N-gram-based Segmentation Method",
      "text": []
    },
    {
      "heading": "3.1 Normal N-gram Algorithm (NN) for Utterance Segmentation",
      "text": [
        "Assuming that W1W2 ... Wm (where m is a natural number) is a word sequence, we consider it as an n order Markov chain, in which the word Wi (1 <_ i <_ m) is predicted by the n-1 words to its",
        "From this conditional probability formula for a word, we can derive the probability of a word se",
        "Integrating the two formulas above, we get:",
        "Let us use SB to indicate a sentence boundary and add it to the word sequence.",
        "The value of",
        "Taking the trigram as our example and considering the two cases where Wi-1 is and is not the final word of a sentence, P(W1W2 ... WiSBWi+1) and P(W1 W2 .",
        ".",
        ".Wi Wi+1) is computed respectively by the following two formulas:",
        "In the normal N-gram method, the above iterative formulas are computed to search the sentence boundaries from W1 to Wm."
      ]
    },
    {
      "heading": "3.2 Reverse N-gram Algorithm (RN) for Utterance Segmentation",
      "text": [
        "In the reverse N-gram segmentation method, we take the word sequence W1W2 ... Wm as a reverse Markov chain in which Wi (1≤ i≤ m) is predicted by the n-1 words to its right.",
        "That is:",
        "As in the N-gram algorithm, we compute the occurring probability of word sequence",
        "By adding SB to the word sequence, we say Wi is the final word of a sentence if and only if",
        "In contrast to the normal N-gram segmentation method, we compute the above iterative formulas to seek sentence boundaries from Wm to W1."
      ]
    },
    {
      "heading": "3.3 Bidirectional N-gram Algorithm for Utterance Segmentation",
      "text": [
        "From the iterative formulas of the normal N-gram algorithm and the reverse N-gram algorithm, we can see that the normal N-gram method recognizes a candidate sentence boundary location mainly according to its left context, while the reverse N-gram method mainly depends on its right context.",
        "Theoretically at least, it is reasonable to suppose that, if we synthetically consider both the left and the right context by integrating the NN and the RN, the overall segmentation accuracy will be improved.",
        "Considering the word sequence W1W2 ... Wm , the candidate sites for sentence boundaries may be found between W1 and W2 , between W2 and W3, ..., or between Wm−1 and Wm.",
        "The number of candidate sites is thus m-1.",
        "We number those m-1 candidate sites 1, 2 ... m-1 in succession, and we use Pis (i) (1≤ i≤ m −1) and Pno (i) (1 ≤ i ≤ m −1) respectively to indicate the probability that the current site i really is, or is not, a sentence boundary.",
        "Thus, to compute the word sequence segmentation, we must compute Pis (i) and Pno (i) for each of the m-1 candidate sites.",
        "In the bidirectional BN, we compute Pis (i) and Pno (i) by combining the NN results and RN results.",
        "The combination is described by the following formulas:",
        "where Pis _NN(i) , Pno _NN(i) denote the probabilities calculated by NN which correspond to",
        "section 3.1 respectively and Pis _RN (i) , Pno _RN (i) denote the probabilities calculated by RN which",
        "We say there exits a sentence boundary at site i"
      ]
    },
    {
      "heading": "3.4 Maximum Entropy Approach for Utterance Segmentation",
      "text": [
        "In this section, we explain our maximum-entropy-based model for utterance segmentation.",
        "That is, we estimate the joint probability distribution of the candidate sites and their surrounding words.",
        "Since we consider information concerning the lexical context to be useful, we define the feature functions for our maximum method as follows:",
        "Sj denotes a sequence of one or more words which we can call the Matching String.",
        "(Note that Sj may contain the sentence boundary mark ‘SB’.)",
        "The candidate c’s state is denoted by b, where b=1 indicates that c is a sentence boundary and b= 0 indicates that it is not a boundary.",
        "Prefix(c) denotes all the word sequences ending with c (that is, c's left context plus c) and Suffix(c) denotes all the word sequences beginning with c (in other words, c plus its right context).",
        "For example: in the utterance: 2'<c1>#(,<c2>131<c3>/,.V,<c4> 6<c5>�, ‘131’ , ‘#(,131’, and ‘2'#(,131’ are c3’s Prefix, while ‘/,.V,’ , ‘/,.V, 6’and ‘/,.V,6�’ are c3’s Suffix.",
        "The value of function include(Pr efix(c), S j) is true when word sequence Sj is one of c’s Prefixes, and the value of function include(Suffix(c), S j) is true when Sj is one of c’s Suffixes.",
        "{ Corresponding to {the four feature functions Jj10(b,c),fj11(b,c),Jj20(b,c),fj21(b,c) are the four parameters αj 1 0, α j 1 1, α j 20 , α j 21 .",
        "Thus the joint probability distribution of the candidate sites and their surrounding contexts is given by:",
        "where k is the total number of the Matching Strings and π is a parameter set to make P(c, 1) and P(c, 0) sum to 1.",
        "The unknown parameters αj10,αj11,αj20,αj21 are chosen to maximize the likelihood of the training data using the Generalized Iterative Scaling (Darroch and Ratcliff, 1972) algorithm.",
        "In the maximum entropy approach, we say that a candidate site is a sentence boundary if and only if P(c, 1) > P(c, 0).",
        "(At this point, we can anticipate a technical problem with the maximum approach to utterance segmentation.",
        "When a Matching String contains SB, we cannot know whether it belongs to the Prefixes or Suffixes of the candidate site until the left and right contexts of the candidate site have been segmented.",
        "Thus if the segmentation proceeds from left to right, the lexical information in the right context of the current candidate site will always remain uncertain.",
        "Likewise, if it proceeds from right to left, the information in the left context of the current candidate site remains uncertain.",
        "The next subsection will describe a pragmatic solution to this problem.)"
      ]
    },
    {
      "heading": "3.5 Maximum-Entropy-Weighted Bidirectional N-gram Algorithm for Utterance Segmentation",
      "text": [
        "In the bidirectional N-gram based algorithm, we have considered the left-to-right N-gram algorithm and the right-to-left algorithm as having the same significance.",
        "Actually, however, they should be assigned differing weights, depending on the lexical contexts.",
        "The combination formulas are as follows:",
        "Wn _is (Ci) , Wn _no (Ci) , Wr _is (Ci ) , Wr _no (Ci) are the functions of the context surrounding candidate site i which denotes the weights of Pis _NN(i) , Pno _NN(i) , Pis _RN(i) and Pno _ RN (i) respectively.",
        "Assuming that the weights of Pis _NN (i) and Pno NN (i) depend upon the context to the left of the candidate site, and that the weights of",
        "boundary.)",
        "Therefore the value of W„ _ is (LeftCi ) is given by W„ is (LeftCi) = P(LeftCi, i = SB) .",
        "Similarly we can give the formulas for comput",
        "using the method described in the maximum entropy approach section.",
        "For example:"
      ]
    },
    {
      "heading": "4 Experiment",
      "text": []
    },
    {
      "heading": "4.1 Model Training",
      "text": [
        "We evaluate site ias a (( l Our models are trained on both Chinese and English corpora, which cover the domains of hotel reservation, flight booking, traffic information, sightseeing, daily life and so on.",
        "We replaced the full stops with “SB” and removed all other punctuation marks in the training corpora.",
        "Since in most actual systems part of speech information cannot be accessed before determining the sentence boundaries, we use Chinese characters and English words without POS tags as the units of our N-gram models.",
        "Trigram and reverse trigram probabilities are estimated based on the processed training corpus by using Modified Kneser-Ney Smoothing (Chen and Goodman, 1998).",
        "As to the maximum entropy model, the Matching Strings are chosen as all the word sequences occurring in the training corpus whose length is no more than 3 words.",
        "The unknown parameters corresponding to the feature functions are generated based on the training corpus using the Generalized Iterative Scaling algorithm.",
        "Table 1 gives an overview of the training corpus."
      ]
    },
    {
      "heading": "4.2 Testing Results",
      "text": [
        "We test our methods using open corpora which are also limited to the domains mentioned above.",
        "All punctuation marks are removed from the test corpora.",
        "An overview of the test corpus appears in table 2.",
        "Corpus SIZE SB Average Length Number of Sentence Chinese 412KB 12032 10 Chinese characters English 391KB 10518 7 words As mentionedinlastsubsection, we needseg-mentedcontexts formaximumentropy approach.",
        "Since the maximumentropy parameters forMEBN algorithmare usedas modifyingNNandRN, we justestimate the jointprobability ofthe candidate andits surroundingcontexts baseduponthe segments by NN andRN.",
        "Using",
        "We have implemented four segmentation algorithms using NN, RN, BN and MEBN respectively.",
        "If we use “RightNum” to denote the number of right segmentations, “WrongNum” denote the number of wrong segmentations, and “TotalNum” to denote the number of segmentations in the original testing corpus, the precision (P) can be computed using the formula P=RightNum/(RightNum+WrongNum), the recall (R) is computed as R=RightNum/TotalNum, and",
        "From the result tables it is clear that RN, BN, and MEBN all outperforms the normal N-gram algorithm in the F-score for both Chinese and English utterance segmentation.",
        "MEBN achieved the best performance which improves the precision by 7.3% and the recall by 1.5% in the Chinese experiment, and improves the precision by 5.4% and the recall by 1.9% in the English experiment."
      ]
    },
    {
      "heading": "4.3 Result analysis",
      "text": [
        "MEBN was proposed in order to maintain the correct segments of the normal N-gram algorithm while skipping the wrong segments.",
        "In order to see whether our original intention has been realized, RN we compared the segments as determined by with those determined by NN, compare the segments found by BN with those of NN and then compare the segments found by MEBN with those of NN.",
        "For RN, BN and MEBN, suppose TN denotes the number of total segmentations, CON denotes the number of correct segmentations overlapping with those found by NN; SWN denotes the number of wrong NN segmentations which were skipped; WNON denotes the number of wrong segmentations not overlapping with those of NN; and CNON denotes the number of segmentations which were correct but did not overlap with those of NN.",
        "The statistical results are listed in Table 5 and Table 6.",
        "Focusing upon the Chinese results, we can see that RN skips 1098 incorrect segments found by NN, and has 9525 correct segments in common with those of NN.",
        "It verifies our supposition that RN can effectively avoid some errors made by NN.",
        "But because at the same time RN brings in 1077 new errors, RN doesn’t improve much in precision.",
        "BN skips 753 incorrect segments and brings in 355 new segmentation errors; has 9906 correct segments in common with those of NN and brings in 622 new correct segments.",
        "So by equally integrating NN and RN, BN on one hand finds more correct segments, on the other hand brings in less wrong segments than NN.",
        "But in skipping incorrect segments by NN, BN still performs worse than RN, showing that it only exerts the error skipping ability of RN to some extent.",
        "As for MEBN, it skips 1274 incorrect segments and at the same time brings in only 223 new incorrect segments.",
        "Additionally it maintains 9646 correct segments in common with those of NN and brings in 678 new correct segments.",
        "In recall MEBN performs a little worse than BN, but in precision it achieves a much better performance than BN, showing that modified by the maximum entropy weights, MEBN makes use of the error skipping ability of RN more effectively.",
        "Further, in skipping wrong segments by NN, MEBN even outperforms RN, which indicates the weights we set on NN and RN not only act as modifying parameters, but also have direct beneficial affection on utterance segmentation.",
        "P+R The testing results are described in Table 3 and Table 4."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "This paper proposes a reverse N-gram algorithm, a bidirectional N-gram algorithm and a Maximum-entropy-weighted Bidirectional N-gram algorithm for utterance segmentation.",
        "The experimental results for both Chinese and English utterance segmentation show that MEBN significantly outperforms the usual N-gram algorithm.",
        "This is because MEBN takes into account both the left and right contexts of candidate sites: it integrates the left-to-right N-gram algorithm and the right-to-left N-gram algorithm with appropriate weights, using clues on the sites’ lexical context, as modeled by maximum entropy."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work is sponsored by the Natural Sciences Foundation of China under grant No.60175012, as well as supported by the National Key Fundamental Research Program (the 973 Program) of China under the grant G1998030504.",
        "The authors are very grateful to Dr. Mark Selig-man for his very useful suggestions and his very careful proofreading."
      ]
    }
  ]
}
