{
  "info": {
    "authors": [
      "Aitao Chen"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W03-1721",
    "title": "Chinese Word Segmentation Using Minimal Linguistic Knowledge",
    "url": "https://aclweb.org/anthology/W03-1721",
    "year": 2003
  },
  "references": [
    "acl-W03-1719"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a primarily data-driven Chinese word segmentation system and its performances on the closed track using two corpora at the first international Chinese word segmentation bakeoff.",
        "The system consists of a new words recognizer, a base segmentation algorithm, and procedures for combining single characters, suffixes, and checking segmentation consistencies."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "At the first Chinese word segmentation bakeoff, we participated in the closed track using the Academia Sinica corpus ( for short) and the Beijing University corpus ( for short).",
        "We will refer to the segmented texts in the training corpus as the training data, and to both the unsegmented testing texts and the segmented texts (the reference texts) as the testing data.",
        "For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003)."
      ]
    },
    {
      "heading": "2 Word segmentation",
      "text": [
        "New texts are segmented in four steps which are described in this section.",
        "New words are automatically extracted from the unsegmented testing texts and added to the base dictionary consisting of words from the training data before the testing texts are segmented, line by line."
      ]
    },
    {
      "heading": "2.1 Base segmentation algorithm",
      "text": [
        "Given a dictionary and a sentence, our base segmentation algorithm finds all possible segmentations of the sentence with respect to the dictionary, computes the probability of each segmentation, and chooses the segmentation with the highest probability.",
        "If a sentence of characters, , has a segmentation of words, ,then the probability of the segmentation is estimated as , where denotes a segmentation of a sentence.",
        "The probability of a word is estimated from the training corpus as , where is the number of times that the word occurs in the training corpus, and is the number of words in the training corpus.",
        "When a word is not in the dictionary, a frequency of 0.5 is assigned to the new word.",
        "The dynamic programming technique is applied to find the segmentation of the highest probability of a sentence without first enumerating all possible segmentations of the sentence with respect to the dictionary.",
        "Consider the text fragment with respect to a dictionary containing the words and it has three segmentations: (1) / (2) / and (3) / / The probabilities of the three segmentations are computed as: (1) p( )*p( ); (2) p( )*p( ); (3) p( )*p( )*p( ).",
        "The probability of a word is estimated by its relative frequency in the training data.",
        "Assume the first segmentation has the highest probability, then the text fragment will be segmented into /"
      ]
    },
    {
      "heading": "2.2 Combining single characters",
      "text": [
        "New words are usually two or more characters long and are often segmented into single characters.",
        "For example, the word is segmented into / when it is not in the dictionary.",
        "After a sentence is segmented using the base algorithm, the consecutive single Hanzi characters are combined into a word if the in-word probabilities of the single characters are over a threshold which is empirically determined from the training data.",
        "The in-word probability of a character is the probability that the character occurs in a word of two or more characters.",
        "Some Hanzi characters, such as and occur as words on their own in segmented texts much more frequently than in words of two or more characters.",
        "For example, in the PK training corpus, the character occurs as a word on its own 11,559 times, but in a word only 875 times.",
        "On the other hand, some Hanzi characters usually do not occur alone as words, instead they occur as part of a word.",
        "As an example, the character occurs in a word 17,108 times, but as a word alone only 794 times in the PK training data.",
        "For each character in the training data, we compute its in-word probability as follow: , where is the number of times that character occurs in the training data, and is the number of times that character is in a word of two or more characters.",
        "We do not want to combine the single characters that occur as words alone more often than not.",
        "For both the PK training data and the AS training data, we divided the training data into two parts, two thirds for training, and one third for system development.",
        "We found that setting the threshold of the in-word probability to 0.85 or around works best on the development data.",
        "After the initial segmentation of a sentence, the consecutive single-characters are combined into one word if their in-word probabilities are over the threshold of 0.85.",
        "The text fragment contains a new word which is not in the PK training data.",
        "After the initial segmentation, the text is segmented into / / / / /, which is subsequently changed into / / after combining the three consecutive characters.",
        "The in-word probabilities for the three characters and are 0.94, 0.98, and 0.99, respectively."
      ]
    },
    {
      "heading": "2.3 Combining suffixes",
      "text": [
        "A small set of characters , such as and frequently occur as the last character in words.",
        "We selected 145 such characters from the PK training corpus, and 113 from the AS corpus.",
        "After combining single characters, we combine a suffix character with the word preceding it if the preceding word is at least two-character long."
      ]
    },
    {
      "heading": "2.4 Consistency check",
      "text": [
        "The last step is to perform consistency checks.",
        "A segmented sentence, after combining single characters and suffixes, is checked against the training data to make sure that a text fragment in a testing sentence is segmented in the same way as in the training data if it also occurs in the training data.",
        "From the PK training corpus, we created a phrase segmentation table consisting of word quad-grams, trigrams, bigrams, and unigrams, together with their segmentations and frequencies.",
        "Our phrase table created from the AS corpus does not include word quad-grams to reduce the size of the phrase table.",
        "For example, from the training text / / / we create the following entries (only some are listed to save space): text fragment freq segmentation After a new sentence is processed by the first three steps, we look up every word quad-grams of the segmented sentence in the phrase segmentation table.",
        "When a word quad-gram is found in the phrase segmentation table with a different segmentation, we replace the segmentation of the word quad-gram in the segmented sentence by its segmentation found in the phrase table.",
        "This process is continued to word trigrams, word bigrams, and word unigrams.",
        "The idea is that if a text fragment in a new sentence is found in the training data, then it should be segmented in the same way as in the training data.",
        "As an example, in the PK testing data, the sentence is segmented into / / / / / / / after the first three steps (the two characters and are not, but should be, combined because the in-word probability of character which is 0.71, is below the predefined threshold of 0.85).",
        "The word bigram is found in the phrase segmentation table with a different segmentation, / / So the segmentation / is changed to the segmentation / / in the final segmented sentence.",
        "In essence, when a text fragment has two or more segmentations, its surrounding context, which can be the preceding word, the following word, or both, is utilized to choose the most appropriate segmentation.",
        "When a text fragment in a testing sentence never occurred in the same context in the training data, then the most frequent segmentation found in the training data is chosen.",
        "Consider the text again, in the testing data, is segmented into / / by our base algorithm.",
        "In this case, never occurred in the context of or The consistency check step changes / / into / / / since is segmented into / 515 times, but is treated as one word 105 times in the training data."
      ]
    },
    {
      "heading": "3 New words recognition",
      "text": [
        "We developed a few procedures to identify new words in the testing data.",
        "Our first procedure is designed to recognize numbers, dates, percent, time, foreign words, etc.",
        "We defined a set of characters consisting of characters such as the digits ‘0’ to ‘9’ (in ASCII and GB), the letters ‘a’ to ’z’, ‘A’ to ‘Z’ (in ASCII and GB), ‘ ➈ ’,and the like.",
        "Any consecutive sequence of the characters that are in this predefined set of characters is extracted and post-processed.",
        "A set of rules is implemented in the post-processor.",
        "One such rule is that if an extracted text fragments ends with the character and contains any character in then remove the ending character and keep the remaining fragment as a word.",
        "For example, our recognizer will extract the text fragment and since all the characters are in the predefined set of characters.",
        "The post-processor will strip off the trailing character and return and as words.",
        "For personal names, we developed a program to extract the names preceding texts such as and a program to detect and extract names in a sequence of names separated by the Chinese punctuation “ ”, such as a program to extract",
        "personal names (Chinese or foreign) following title or profession names, such as in the text and a program to extract Chinese personal names based on the preceding word and the following word.",
        "For example, the string in is most likely a personal name (in this case, it is) since is a Chinese family name, the string is three-character long (a typical Chinese personal name is either three or two-character long).",
        "Furthermore, the preceding word and the following word are highly unlikely to appear in a Chinese personal name.",
        "For the personal names extracted from the PK testing data, if the name is two or three-character long, and if the first character or two is a Chinese family name, then the family name is separated from the given name.",
        "The family names are not separated from the given names for the personal names extracted from the AS testing data.",
        "In some cases, we find it difficult to decide whether or not the first character should be removed from a personal name.",
        "Consider the personal name which looks like a Chinese personal name since the first character is a Chinese family name, and the name is three-character long.",
        "If it is a translated foreign name (in this case, it is), then the name should not be split into family name and given name.",
        "But if it is the name of a Chinese personal name, then the family name should be separated from the given name.",
        "For place names, we developed a simple program to extract names of cities, counties, towns, villages, streets, etc, by extracting the strings of up to three characters appearing between two place name designators.",
        "For example, from the text our program will extract The last row (in boldface) in Table 1 gives our official results for the PK closed track.",
        "Other rows in the table present the results under different experimental conditions.",
        "The column labeled steps refers to the executed steps of our Chinese word segmentation algorithm.",
        "Step 1 segments a text using the base segmentation algorithm, step 2 combines single characters, step 3 attaches suffixes to the preceding words, and step 4 performs consistency checks.",
        "The four steps are described in details in section 2.",
        "The column labeled dict gives the dictionary used in each experiment.",
        "The pkd1 consists of only the words from the PK training cor",
        "pus, pkd2 consists of the words in pkd1 and the words converted from pkd1 by changing the GB encoding to ASCII encoding for the numeric digits and the English letters, and pkd3 consists of the words in pkd2 and the words automatically extracted from the PK testing texts using the procedures described in section 3.",
        "The columns labeled R, P and F give the recall, precision, and F score, respectively.",
        "The columns labeled and show the recall on out-of-vocabulary words and the recall on in-vocabulary words, respectively.",
        "All evaluation scores reported in this paper are computed using the score program written by Richard Sproat.",
        "We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures.",
        "For example, row 4 in table 1 gives the results using pkd3 dictionary when a sentence is segmented by the base algorithm, and then the single characters in the initial segmentation are combined, but suffixes are not attached and consistency check is not performed.",
        "The last row in table 2 presents our official results for the closed track using the AS corpus.",
        "The asd1 dictionary contains only the words from the AS training corpus, while the asd2 consists of the words in asd1 and the new words automatically extracted from the AS testing texts using the new words recognition described in section 3.",
        "The results show that new words recognition and joining single characters contributed the most to the increase in precision, while the consistency check contributed the most to the increase in recall.",
        "Table 3 gives the results of the maximum matching using only the words in the training data.",
        "While the difference between the F-scores of the maximum matching and the base algorithm is small for the PK corpus, the F-score difference for the AS corpus is much larger.",
        "Our base algorithm performed substantially better than the maximum matching for the AS corpus.",
        "The performances of our base algorithm on the testing data using the words from the training data are presented in row 1 in table 1 for the corpus, and row 1 in table 2 for the corpus."
      ]
    },
    {
      "heading": "5 Discussions",
      "text": [
        "In this section we will examine in some details the problem of segmentation inconsistencies within the training data, and"
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "within the testing data, and between training data and testing data.",
        "Due to space limit, we will only report our findings in the PK corpus though the same kinds of inconsistencies also occur in the AS corpus.",
        "We understand that it is difficult, or even impossible, to completely eliminate segmentation inconsistencies.",
        "However, perhaps we could learn more about the impact of segmentation inconsistencies on a system’s performance by taking a close look at the problem.",
        "We wrote a program that takes as input a segmented corpus and prints out the shortest text fragments in the corpus that have two or more segmentations.",
        "For each text fragment, the program also prints out how the text fragment is segmented, and how many times it is segmented in a particular way.",
        "While some of the text fragments, such as and truly have two different segmentations, depending on the contexts in which they occur or the meanings of the text fragments, others are segmented inconsistently.",
        "We ran this program on the PK testing data and found 21 unique shortest text fragments, which occur 87 times in total, that have two different segmentations.",
        "Some of the text fragments, such as are inconsistently segmented.",
        "The fragment occurs twice in the testing data and is segmented into / in one case, but treated as one word in the other case.",
        "We found 1,500 unique shortest text fragments in the PK training data that have two or more segmentations, and 97 unique shortest text fragments that are segmented differently in the training data and in the testing data.",
        "For example, the text is treated as one word in the training data, but is segmented into / / / in the testing data.",
        "We found 11,136 unique shortest text fragments that have two or more segmentations in the AS training data, 21 unique shortest text fragments that have two or more segmentations in the AS testing data, and 38 unique shortest text fragments that have different segmentations in the AS training data and in the AS testing data.",
        "Segmentation inconsistencies not only exists within training and testing data, but also between training and testing data.",
        "For example, the text fragment occurs 35 times in the PK training data and is consistently segmented into ” / but the same text fragment, occurring twice in the testing data, is segmented into / / in both cases.",
        "The text occurs 67 times in the training data and is treated as one word in all 67 cases, but the same text, occurring 4 times in the testing data, is segmented into / in all 4 cases.",
        "The text occurs 16 times in the training data, and is treated as one word in all cases, but in the testing data, it is treated as one word in three cases and segmented into / in one case.",
        "The text is segmented into / in 8 cases, but treated as one word in one case in the training data.",
        "A couple of text fragments seem to be incorrectly segmented.",
        "The text in the testing data is segmented into / and the text segmented into / Our segmented texts of the PK testing data differ from the reference segmented texts for 580 text fragments (427 unique).",
        "Out of these 580 text fragments, 126 text fragments are among the shortest text fragments that have one segmentation in the training data, but another in the testing data.",
        "This implies that up to 21.7% of the mistakes committed by our system may have been impacted by the segmentation inconsistencies between the PK training data and the PK testing data.",
        "Since there are only 38 unique shortest text fragments found in the AS corpus that are segmented differently in the training data and the testing data , the inconsistency problem probably had less impact on our AS results.",
        "Out of the same 580 text fragments, 359 text fragments (62%) are new words in the PK testing data.",
        "For example, the proper name which is a new word, is incorrectly segmented into / by our system.",
        "Another example is the new word which is treated as one word in the testing data, but is segmented into / / /by our system.",
        "Some of the longer text fragments that are incorrectly segmented may also involve new words, so at least 62%, but under 80%, of the incorrectly segmented text fragments are either new words or involve new words."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have presented our word segmentation system and the results for the closed track using the corpus and the corpus.",
        "The new words recognition, combining single characters, and checking consistencies contributed the most to the increase in precision and recall over the performance of the base segmentation algorithm, which works better than maximum matching.",
        "For the closed track experiment using the corpus, we found that 62% of the text fragments that are incorrectly segmented by our system are actually new words, which clearly shows that to further improve the performance of our system, a better new words recognition algorithm is necessary.",
        "Our failure analysis also indicates that up to 21.7% of the mistakes made by our system for the PK closed track may have been impacted by the segmentation inconsistencies between the training and testing data."
      ]
    }
  ]
}
