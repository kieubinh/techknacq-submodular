{
  "info": {
    "authors": [
      "Angelo Dalli"
    ],
    "book": "Proceedings of the EACL 2003 Workshop on Evaluation Initiatives in Natural Language Processing: are evaluation methods, metrics and resources reusable?",
    "id": "acl-W03-2807",
    "title": "Adaptation of the F-measure to Cluster Based Lexicon Quality Evaluation",
    "url": "https://aclweb.org/anthology/W03-2807",
    "year": 2003
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Adaptation of the F-measure to Cluster Based Lexicon Quality",
        "Evaluation",
        "NLP Research Group Department of Computer Science",
        "An external lexicon quality measure called the L-measure is derived from the F-measure (Rijsbergen, 1979; Larsen and Aone, 1999).",
        "The typically small sample sizes available for minority languages and the evaluation of Semitic language lexicons are two main factors considered.",
        "Large-scale evaluation results for the Maltilex Corpus are presented (Rosner et al., 1999)."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Computational Lexicons form a fundamental component of any NLP system.",
        "Unfortunately, good quality lexicons are hard to create and maintain.",
        "The labour intensive process of lexicon creation is further compounded when minority languages are concerned.",
        "Inevitably, computational lexicons for minor languages tend to be quite small when compared to computational lexicons available for more common languages such as English.",
        "The Maltilex Corpus is used in this paper to evaluate a cluster based lexicon quality measure adapted from the F-measure.",
        "The Maltilex Corpus is the first large-scale computational lexicon for Maltese (Rosner et al., 1999).",
        "The choice of Maltese as the evaluation language presented some additional problems due to the Semitic morphology and grammar of Maltese (Mifsud, 1995).",
        "An innovative approach to lexicon creation using an automated technique called the Lexicon Structuring Technique (LST) was used to create an initial computational lexicon from a wordlist (Dalli, 2002a).",
        "LST decreased the amount of work that is normally required to create a lexicon from scratch by adapting a number of clustering, alignment, and approximate matching techniques to produce a set of clusters containing related wordforms.",
        "Lexicon clusters are thus analogous to lemmas in more traditional lexicons.",
        "This approach has many advantages for a language having a Semitic morphology and grammar due to the large number of wordforms that can be derived for a single lemma.",
        "Instead of processing every wordform individually, the whole cluster can be treated as a single entity, reducing processing requirements significantly.",
        "The close relationship of this lexicon definition and standard clustering systems (with lemmas corresponding to clusters), enabled the reuse of cluster quality evaluation measures to the task of lexicon quality evaluation.",
        "There are two main ways of evaluating cluster quality which are summarised in (Steinbach et al., 1999 pg.",
        "6) as follows:",
        "• Internal Quality Measure - Clusters are compared without reference to external knowledge against some predefined set of desirable qualities.",
        "• External Quality Measure - Clusters are compared to known external classes.",
        "Internal quality measures are not always desirable, since their very existence implies that better quality can be achieved by applying an internal quality measure in conjunction with some optimisation technique.",
        "An internal quality measure for cluster-based lexicons was not available either.",
        "The two main external quality measures applicable lexicon quality evaluation tasks are entropy (Shannon, 1948) and the F-measure (van Rijsbergen, 1979; Larsen and Aone, 1999).",
        "Entropy based quality measures assert that the best entropy that can be obtained is when each cluster contains the optimal number of members.",
        "In our context this corresponds to having clusters (corresponding to lemmas) that contain exactly all the wordforms associated with that cluster.",
        "The class distribution of the data is calculated by considering the probability of every member belonging to some class.",
        "The entropy of every cluster j is calculated using the standard entropy notes the probability that a member of cluster j belongs to class i.",
        "The total entropy is then calculated as E * = – ^ n • E^j) where «; is the size of cluster j, m the number of clusters, and n the total number of data points.",
        "The F-measure treats every cluster as a query and every class as the desired result set for a query.",
        "The recall and precision values for each given class are then calculated using information retrieval concepts.",
        "The F-measure of cluster j and where r denotes recall and p the precision.",
        "Recall class i members in cluster j, while and nt are the sizes of cluster j and class i respectively.",
        "The overall F-measure for the entire data set of size n is given by F* = ^ – maxfi^', j)]."
      ]
    },
    {
      "heading": "2. Lexicon Quality Measure",
      "text": [
        "Computational lexicons have an additional domain-specific external quality measure available in the form of existing non-computational language dictionaries.",
        "Dictionaries can be used to compare the results generated by the automated system against those produced by human experts.",
        "Generally it can be assumed that reputable printed dictionaries are of a very high quality and thus provide a gold standard for comparison.",
        "For some languages, especially minority languages, the only available quality data would be in printed dictionary form.",
        "Unfortunately most non-computational dictionaries are not amenable to automated analysis techniques since the process of re-inputting and restructuring data into a computational dictionary format is generally so labour intensive that it becomes too expensive.",
        "Additionally, since every cluster and class correspond to a lemma, the number of classes to be considered is expected to number in the thousands.",
        "This would make a straightforward application of the F-measure an overly long process.",
        "A modified statistical sampling technique based on the F-measure that gives results that are approximately as good as the full application of the F-measure and that caters for the particular nuances of lexicon quality evaluation is thus needed.",
        "The L-measure is such a new measure based on the F-measure that attempts to measure the quality of a given lexicon in relation to other existing lexicons that are possibly non-computational lexicons (i.e. human compiled language dictionaries), taking into consideration that a full population analysis may not be practical under most circumstances.",
        "The L-Measure works by comparing two lexicons, one derived from a gold standard representation in the form of human compiled dictionaries and the other being a computational lexicon whose quality is being assessed.",
        "In order to avoid confusion, formal definitions of the terms dictionary, lexicon and wordlists are now presented.",
        "A dictionary D is formally modeled as a sequence <ti .. th> of tuples of the form (/, def) where / denotes a lemma (i.e. a dictionary headword in a more traditional sense) and défis a 5-tuple (m, r, c, i, o) with m containing morphological information that enables members of the lemma to be inferred or generated, r a set of relations to other lemmas, c a description of the different contexts where the lemma may be normally used, i containing meta-information about lemma / itself, and o an object containing additional information (such as etymology, examples of common use, etc.)",
        "Since multiple entries of the same headword may be present in D the sequence is not injective, i.e. the sequence can contain duplicate elements.",
        "The main two differences between a dictionary and a lexicon are that different types of information are stored about every lemma in the def component, and secondly, that a lexicon has an injective sequence of tuples (i.e. a sequence that does not have duplicates and where the exact order is important) while a dictionary does not (since a dictionary does not need to force a headword to have one unique entry, especially in the case of printed dictionaries that often have the same headword appearing in multiple top-level entries).",
        "A dictionary D can be thus transformed into a lexicon L, denoted by L = lex(D), by filtering the tuple sequence <ti .. th> making up D to include only the / components of every tuple.",
        "The filtered sequence is then transformed into an injective sequence of unique lemmas </; .. /„>, satisfying the requirements for a lexicon.",
        "Appropriate transformations have to be defined to transform the def component from dictionary to lexicon format.",
        "The sequence of lemmas is then expanded to a canonical wordlist W. A canonical wordlist W is a sequence <wi .. wu> of sets of strings generated from a lexicon L, denoted by W = can(L), by listing all possible instances of every lemma in the lexicon (i.e. all possible wordforms of a particular lemma), in effect creating a full form lexicon.",
        "The canonical wordlist W thus has u sets of strings corresponding to u lemmas in the lexicon.",
        "The particular lemma used to generate a word-form w is obtained by the operator lem(w).",
        "The sequence of lemmas used to generate W is denoted as lemmas(W).",
        "The union of two wordlists Wi u W2 is defined to be the union of all sets of strings in both wordlists, provided that lem(x,) = lemfyj) v lem{xJ) g lemmas{W2) v lemiyj) i lemmas(Wi) holds.",
        "This definition ensures maximum coverage of the resulting canonical wordlist.",
        "An empty or null canonical wordlist results if no pair of strings obey the previously stated condition while the union of a wordlist with a null wordlist is the original wordlist itself.",
        "Similarly the intersection of two wordlists Wj n W2 is defined to be the union of all sets of strings in both wordlists that have corresponding lemmas appearing in both wordlists, i.e.",
        "provided that lem(x,) = lenity) holds.",
        "Note that this definition is concerned mainly with the lemmas and their associated wordforms themselves.",
        "Since lexicons are not just a list of lemmas and wordforms, other linguistic annotations will have to be evaluated using other techniques appropriate to the particular linguistic annotations added to the lemma entries.",
        "Given a lexicon L and a set of dictionaries D = {Di .. Dk} transform the set of dictionaries D into a set of lexicons V = {Li .. Lk} using the lex transformation on every dictionary, thus .",
        "Define W as the canonical wordlist obtained from L, W= can(L) and W as the canonical wordlist obtained from L\\ W'= l^Jcan^ ) under canonical wordlist union.",
        "Define Y to be the canonical wordlist of words common to both W and W, Y = W n W. The sample size S used for the L-measure is defined as a.\\lemmas(Y)\\ where a is some value in the range (0..1) that controls the random sample size.",
        "Typically a should be set to somewhere between 0.01 and 0.1.",
        "It is expected that the sample size will be large enough to assume that the sample is representative of the whole population.",
        "The L-measure of a lemma j in lemmas(W) and lemma i in lemmas(Y) is given by members in lemma j, while and nt are the sizes of lemma j and lemma z respectively.",
        "The overall L-measure for the entire sample of size n is given range [0..1] and is proportional to the lexicon quality, with an L score of 1 representing a perfect quality lexicon with respect to the lexicon being used as a standard.",
        "Y is used instead of W since lexical word coverage is largely determined by the quality of the corpus used to create the lexicon.",
        "While this kind of analysis might be useful in determining the coverage of a lexicon the L-measure is oriented towards measuring quality rather than quantity, independently of the corpus that was used to create the lexicon.",
        "The precision and recall for the samples were calculated individually to obtain the individual L-measure for a range of lemmas.",
        "A fully worked out example of the calculation of the L-measure for the lemma missier (father) is given.",
        "Lemmas in the Maltilex Computational Lexicon are aligned automatically using a technique adopted from bioinformatics and hence the presentation of the wordforms in their aligned format (Dalli, 2000b; Gusfield, 1997).",
        "The lemma missier (the Maltese word for father with the cluster showing different forms like my father, your father, etc.)",
        "taken from the Maltilex Computational Lexicon, which represents lemma z, contains seven members as displayed below:",
        "missierek missier n a missier kom missi ri jietna missier horn",
        "The lemma missier, taken from Aquilina's Dictionary, which represents lemma j, can be used to generate the following ten members as displayed below:"
      ]
    },
    {
      "heading": "3. Results",
      "text": [
        "The L-measure has been used to measure the quality of the Maltilex Computational Lexicon in relation to existing paper based dictionaries.",
        "The most comprehensive dictionary of Maltese was used to produce L', the comparison standard lexicon (Aquilina, 1987-1990).",
        "The capability of the L-measure to work with a statistical sample made a manual analysis of results possible without having L' in digital form.",
        "The value for the sample size S was determined through a parameter a that was set to 0.01, meaning that 1% of all lemmas in the Maltilex Computational Lexicon were covered by the statistical sample.",
        "Since around 63,000 lemmas exist in the combined lexicon the sample size S was determined to be 630.",
        "The set of 630 lemmas chosen at random from the Maltilex Corpus contained a total of 5,887 wordforms taken from the combined lexicon.",
        "m m m m m m m m m m îe ie ie ie i ie ie ie ie i",
        "n a kom i j ie t",
        "For this example, «; and nt are thus equal to 10 and 7 respectively.",
        "Recall and precision values are calculated as r(missier, missier') = – = 1 pymissier, missier ') = – = 0.7 respectively.",
        "The L-measure for the lemma missier is the lemma missier to the final L* score is thus given by precision floating point library was used to represent the individual contribution values since these are generally very small.",
        "Figures 1 and 2 show the precision and recall curves for the whole sample respectively.",
        "I.LI JUI1 kill, 1,1 I LilJI 111) IllLlllllli I",
        "Figure 3 shows moving average trendlines for precision and recall (precision is shown in a bold line on top, recall is the fainter line underneath).",
        "The average precision was 0.91748 and the average rate of recall was 0.661359.",
        "i If n mi ii 'fin I'l'i 1",
        "I'l, mI.",
        "'I\" , , I.I 111 ii..",
        "luiiivnu i umii i m",
        "Figure 4 shows the individual L-measure values for the sample.",
        "The values displayed in Figure 4 are those used to calculate the final L value.",
        "Figure 5 shows the moving average trend-line for the individual L-measure values.",
        "The average individual L-measure was 0.707256882 while the average individual contribution of a lemma to the L value was 0.000748924.",
        "The variance in the L-measure individual values was 0.065504369.",
        "The correlation between the L-measure and precision was 0.163665769 while the correlation between the L-measure and recall was 0.922214452.",
        "The overall L score for the Maltilex Computational Lexicon was 0.4718.",
        "This score is quite intuitive when the various problems in the existing Maltese corpus used to create the Computational Lexicon are considered.",
        "This score means that the number of wordforms that are stored or that can be generated by the current lexicon needs to be expanded by around 53% in order to match the quality of the lexicon underlying Aquilina's dictionary (Aquilina, 1987-1990)."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "The L-measure is a useful evaluation metric that can be used to measure the quality of a computational lexicon based on clustering concepts.",
        "The small data sample required by L-measure to give meaningful results makes it a practical measure to use in a variety of situations where massive amounts of data might not be available.",
        "This makes L-measure ideal for use in the evaluation of Language Resources for minority languages and also for quick benchmark studies that evaluate the quality of a computational lexicon as it is being created.",
        "Compared with the F-measure, the L-measure will give highly similar results using less data.",
        "Naturally the validity of the L-measure results depends on the choice of the a value, which in turn determines the sample size.",
        "The lemma/cluster based approach of the L-measure is suitable for the evaluation of Semitic language lexicons that often prove problematic to evaluation techniques based on English or Romance languages.",
        "The L-measure also has potential future applications in the comparison and evaluation of different lexicons.",
        "The individual L-measure scores can also be used to identify areas of similarities and differences between different lexicons quickly.",
        "The L-measure can also be adapted to other areas of Computational Linguistics as long as the concept of a cluster and some means of determining its precision and recall exist.",
        "Minimal changes are needed to adapt the L-measure to other domains making future adaptations likely.",
        "Acknowledgment",
        "This work has been made possible with the collaboration of the Maltilex Project at the University of Malta."
      ]
    }
  ]
}
