{
  "info": {
    "authors": [
      "Richard Zens",
      "Evgeny Matusov",
      "Hermann Ney"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1006",
    "title": "Improved Word Alignment Using a Symmetric Lexicon Model",
    "url": "https://aclweb.org/anthology/C04-1006",
    "year": 2004
  },
  "references": [
    "acl-C96-2141",
    "acl-J00-2004",
    "acl-J03-1002",
    "acl-J93-2003",
    "acl-J97-3002",
    "acl-P00-1056",
    "acl-P03-1011",
    "acl-P03-1012",
    "acl-W02-1012"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing.",
        "We improve the well-known IBM alignment models, as well as the Hidden-Markov alignment model using a symmetric lexicon model.",
        "This symmetrization takes not only the standard translation direction from source to target into account, but also the inverse translation direction from target to source.",
        "We present a theoretically sound derivation of these techniques.",
        "In addition to the symmetrization, we introduce a smoothed lexicon model.",
        "The standard lexicon model is based on full-form words only.",
        "We propose a lexicon smoothing method that takes the word base forms explicitly into account.",
        "Therefore, it is especially useful for highly inflected languages such as German.",
        "We evaluate these methods on the German–English Verbmobil task and the French–English Canadian Hansards task.",
        "We show statistically significant improvements of the alignment quality compared to the best system reported so far.",
        "For the Canadian Hansards task, we achieve an improvement of more than 30% relative."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing.",
        "Obvious applications are the extraction of bilingual word or phrase lexica (Melamed, 2000; Och and Ney, 2000).",
        "These applications depend heavily on the quality of the word alignment (Och and Ney, 2000).",
        "Word alignment models were first introduced in statistical machine translation (Brown et al., 1993).",
        "The alignment describes the mapping from source sentence words to target sentence words.",
        "Using the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993), as well as the Hidden-Markov alignment model (Vogel et al., 1996), we can produce alignments of good quality.",
        "In (Och and Ney, 2003), it is shown that the statistical approach performs very well compared to alternative approaches, e.g. based on the Dice coefficient or the competitive linking algorithm (Melamed, 2000).",
        "A central component of the statistical translation models is the lexicon.",
        "It models the word translation probabilities.",
        "The standard training procedure of the statistical models uses the EM algorithm.",
        "Typically, the models are trained for one translation direction only.",
        "Here, we will perform a simultaneous training of both translation directions, source-to-target and target-to-source.",
        "After each iteration of the EM algorithm, we combine the two lexica to a symmetric lexicon.",
        "This symmetric lexicon is then used in the next iteration of the EM algorithm for both translation directions.",
        "We will propose and justify linear and loglin-ear interpolation methods.",
        "Statistical methods often suffer from the data sparseness problem.",
        "In our case, many words in the bilingual sentence-aligned texts are singletons, i.e. they occur only once.",
        "This is especially true for the highly inflected languages such as German.",
        "It is hard to obtain reliable estimations of the translation probabilities for these rarely occurring words.",
        "To overcome this problem (at least partially), we will smooth the lexicon probabilities of the full-form words using a probability distribution that is estimated using the word base forms.",
        "Thus, we exploit that multiple full-form words share the same base form and have similar meanings and translations.",
        "We will evaluate these methods on the German–English Verbmobil task and the French–English Canadian Hansards task.",
        "We will show statistically significant improvements compared to state-of-the-art results in (Och and Ney, 2003).",
        "On the Canadian Hansards task, the symmetrization methods will result in an improvement of more than 30% relative."
      ]
    },
    {
      "heading": "2 Statistical Word Alignment Models",
      "text": [
        "In this section, we will give a short description of the commonly used statistical word alignment models.",
        "These alignment models stem from the source-channel approach to statistical machine translation (Brown et al., 1993).",
        "We are given a source language sentence fJ1 := f1... fj... fJ which has to be translated into a target language sentence eI1 := e1 ... ei ... eI.",
        "Among all possible target language sentences, we will choose the sentence with the highest probability:",
        "This decomposition into two knowledge sources allows for an independent modeling of target language model Pr(eI1) and translation model Pr(fJ1 |eI1).",
        "Into the translation model, the word alignment A is introduced as a hidden variable:",
        "Usually, we use restricted alignments in the sense that each source word is aligned to at most one target word, i.e. A = aJ1 .",
        "A detailed description of the popular translation models IBM-1 to IBM-5 (Brown et al., 1993), as well as the Hidden-Markov alignment model (HMM) (Vogel et al., 1996) can be found in (Och and Ney, 2003).",
        "All these models include parameters p(f |e) for the single-word based lexicon.",
        "They differ in the alignment model.",
        "A Viterbi alignment Aˆ of a specific model is an alignment for which the following equation holds:",
        "We measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced reference alignment.",
        "In Section 3, we will apply the lexicon symmetrization methods to the models described previously.",
        "Therefore, we will now sketch the standard training procedure for the lexicon model.",
        "The EM algorithm is used to train the free lexicon parameters p(f |e).",
        "In the E-step, the lexical counts for each sentence pair (fJ1 , eI1) are calculated and then summed over all sentence pairs in the training corpus:"
      ]
    },
    {
      "heading": "3 Symmetrized Lexicon Model",
      "text": [
        "During the standard training procedure, the lexicon parameters p(f |e) and p(e|f) were estimated independent of each other in strictly separate trainings.",
        "In this section, we present two symmetrization methods for the lexicon model.",
        "As a starting point, we use the joint lexicon probability p(f, e) and determine the conditional probabilities for the source-to-target direction p(f |e) and the target-to-source direction p(e|f) as the corresponding marginal distribution:",
        "The nonsymmetric auxiliary Q-functions for reestimating the lexicon probabilities during the EM algorithm can be represented as follows.",
        "Here, NST(f, e) and NTS(f, e) denote the lexicon counts for the source-to-target (ST) direction and the target-to-source (TS) direction, respectively."
      ]
    },
    {
      "heading": "3.1 Linear Interpolation",
      "text": [
        "To estimate the joint probability using the EM algorithm, we define the auxiliary Q-function",
        "as a linear interpolation of the Q-functions for the source-to-target and the target-to-source direction:",
        "The unigram counts N(e) and N(f) are determined, for each of the two translation directions, by taking a sum of N(f, e) over f and over e, respectively.",
        "We define the combined lexicon count Nα(f, e):",
        "Now, we derive the symmetrized Q-function over p(f, e) for a certain word pair (f,e).",
        "Then, we set this derivative to zero to determine the reestimation formula for p(f, e) and obtain the following equation:",
        "We do not know a closed form solution for this equation.",
        "As an approximation, we use the following term:",
        "This estimate is an exact solution, if the unigram counts for f and e are independent of the translation direction, i. e. NST (f ) = NTS (f ) and NST(e) = NTS(e).",
        "We make this approximation and thus we interpolate the lexicon counts linear after each iteration of the EM algorithm.",
        "Then, we normalize these counts (according to Equations 1 and 2) to determine the lexicon probabilities for each of the two translation directions."
      ]
    },
    {
      "heading": "3.2 Loglinear Interpolation",
      "text": [
        "We will show in Section 5 that the linear interpolation results in significant improvements over the nonsymmetric system.",
        "Motivated by these experiments, we investigated also the loglinear interpolation of the lexicon counts of the two translation directions.",
        "The combined lexicon count Nα(f, e) is now defined as:",
        "The normalization is done in the same way as for the linear interpolation.",
        "The linear interpolation resembles more a union of the two lexica whereas the loglinear interpolation is more similar to an intersection of both lexica.",
        "Thus for the linear interpolation, a word pair (f, e) obtains a large combined count, if the count in at least one direction is large.",
        "For the loglinear interpolation, the combined count is large only if both lexicon counts are large.",
        "In the experiments, we will use the interpolation weight α = 0.5 for both the linear and the loglinear interpolation, i. e. both translation directions are weighted equally."
      ]
    },
    {
      "heading": "3.3 Evidence Trimming",
      "text": [
        "Initially, the lexicon contains all word pairs that cooccur in the bilingual training corpus.",
        "The majority of these word pairs are not translations of each other.",
        "Therefore, we would like to remove those lexicon entries.",
        "Evidence trimming is one way to do this.",
        "The evidence of a word pair (f, e) is the estimated count N(f, e).",
        "Now, we discard a word pair if its evidence is below a certain threshold τ.1 In the case of the symmetric lexicon, we can further refine this method.",
        "For estimating the lexicon in the source-to-target direction ˆp(f Ie), the idea is to keep all entries from this direction and to boost the entries that have a high evidence in the target-to-source direction NTS(f, e).",
        "We obtain the following formula:",
        "The count NST (f, e) is now used to estimate the source-to-target lexicon ˆp(f Ie).",
        "With this method, we do not keep entries in the source-to-target lexicon ˆp(f Ie) if their evidence is low, even if their evidence in the target-to-source 'Actually, there is always implicit evidence trimming caused by the limited machine precision.",
        "direction NTS(f, e) is high.",
        "For the target-to-source direction, we apply this method in a similar way."
      ]
    },
    {
      "heading": "4 Lexicon Smoothing",
      "text": [
        "The lexicon model described so far is based on full-form words.",
        "For highly inflected languages such as German this might cause problems, because many full-form words occur only a few times in the training corpus.",
        "Compared to English, the token/type ratio for German is usually much lower (e.g. Verbmobil: English 99.4, German 56.3).",
        "The information that multiple full-form words share the same base form is not used in the lexicon model.",
        "To take this information into account, we smooth the lexicon model with a backing-off lexicon that is based on word base forms.",
        "The smoothing method we apply is absolute discounting with interpolation: p(f |e) =max {N(f, e) − d, 0} N(e) + α(e) · β(f, ¯e) This method is well known from language modeling (Ney et al., 1997).",
        "Here, e¯ denotes the generalization, i.e. the base form, of the word e. The nonnegative value d is the discounting parameter, α(e) is a normalization constant and β(f, ¯e) is the normalized backing-off distribution.",
        "The formula for α(e) is:",
        "This formula is a generalization of the one typically used in publications on language modeling.",
        "This generalization is necessary, because the lexicon counts may be fractional whereas in language modeling typically integer counts are used.",
        "Additionally, we want to allow for discounting values d greater than one.",
        "The backing-off distribution β(f, ¯e) is estimated using relative frequencies:",
        "Here, N(f, ¯e) denotes the count of the event that the source language word f and the target language base form e¯ occur together.",
        "These counts are computed by summing the lexicon counts N(f, e) over all full-form words e which share the same base form ¯e."
      ]
    },
    {
      "heading": "5 Results",
      "text": []
    },
    {
      "heading": "5.1 Evaluation Criteria",
      "text": [
        "We use the same evaluation criterion as described in (Och and Ney, 2000).",
        "The generated word alignment is compared to a reference alignment which is produced by human experts.",
        "The annotation scheme explicitly takes the ambiguity of the word alignment into account.",
        "There are two different kinds of alignments: sure alignments (S) which are used for alignments that are unambiguous and possible alignments (P) which are used for alignments that might or might not exist.",
        "The P relation is used especially to align words within idiomatic expressions, free translations, and missing function words.",
        "It is guaranteed that the sure alignments are a subset of the possible alignments (S ⊆ P).",
        "The obtained reference alignment may contain many-to-one and one-to-many relationships.",
        "The quality of an alignment A is computed as appropriately redefined precision and recall measures.",
        "Additionally, we use the alignment error rate (AER), which is derived from the well-known F-measure.",
        "With these definitions a recall error can only occur if a S(ure) alignment is not found and a precision error can only occur if a found alignment is not even P(ossible)."
      ]
    },
    {
      "heading": "5.2 Experimental Setup",
      "text": [
        "We evaluated the presented lexicon symmetrization methods on the Verbmobil and the Canadian Hansards task.",
        "The German– English Verbmobil task (Wahlster, 2000) is a speech translation task in the domain of appointment scheduling, travel planning and hotel reservation.",
        "The French–English Canadian Hansards task consists of the debates in the Canadian Parliament.",
        "The corpus statistics are shown in Table 1 and Table 2.",
        "The number of running words and the vocabularies are based on full-form words including punctuation marks.",
        "As in",
        "(Och and Ney, 2003), the first 100 sentences of the test corpus are used as a development corpus to optimize model parameters that are not trained via the EM algorithm, e.g. the discounting parameter for lexicon smoothing.",
        "The remaining part of the test corpus is used to evaluate the models.",
        "We use the same training schemes (model sequences) as presented in (Och and Ney, 2003).",
        "As we use the same training and testing conditions as (Och and Ney, 2003), we will refer to the results presented in that article as the baseline results.",
        "In (Och and Ney, 2003), the alignment quality of statistical models is compared to alternative approaches, e.g. using the Dice coefficient or the competitive linking algorithm.",
        "The statistical approach showed the best performance and therefore we report only the results for the statistical systems."
      ]
    },
    {
      "heading": "5.3 Lexicon Symmetrization",
      "text": [
        "In Table 3 and Table 4, we present the following experiments performed for both the Verbmobil and the Canadian Hansards task:",
        "• Base: the system taken from (Och and Ney, 2003) that we use as baseline system.",
        "• Lin.",
        ": symmetrized lexicon using a linear interpolation of the lexicon counts after each training iteration as described in Section 3.1.",
        "• Log.",
        ": symmetrized lexicon using a loglinear interpolation of the lexicon counts after each training iteration as described in Section 3.2.",
        "ods as a function of the training corpus size for the Verbmobil task (source-to-target direction).",
        "In Table 3, we compare both interpolation variants for the Verbmobil task to (Och and Ney, 2003).",
        "We observe notable improvements in the alignment error rate using the linear interpolation.",
        "For the translation direction from German to English (S→T), an improvement of about 25% relative is achieved from an alignment error rate of 5.7% for the baseline system to 4.3% using the linear interpolation.",
        "Performing the loglinear interpolation, we observe a substantial reduction of the alignment error rate as well.",
        "The two symmetrization methods improve both precision and recall of the resulting Viterbi alignment in both translation directions for the Verbmobil task.",
        "The improvements with the linear interpolation is for both translation directions statistically significant at the 99% level.",
        "For the loglinear interpolation, the target-to-source translation direction is statistically significant at the 99% level.",
        "The statistical significance test were done using boostrap resampling.",
        "We also performed experiments on sub-corpora of different sizes.",
        "For the Verbmobil task, the results are illustrated in Figure 1.",
        "We observe that both symmetrization variants result in improvements for all corpus sizes.",
        "With increasing training corpus size the performance of the linear interpolation becomes superior to the performance of the loglinear interpolation.",
        "In Table 4, we compare the symmetrization methods with the baseline system for the Canadian Hansards task.",
        "Here, the loglinear interpolation performs best.",
        "We achieve a relative improvement over the baseline of more than 30% for both translation directions.",
        "For instance, the alignment error rate for the translation direction from French to English (S-*T) improves from 12.6% for the baseline system to 8.6% for the symmetrized system with loglinear interpolation.",
        "Again, the two symmetrization methods improve both precision and recall of the Viterbi alignment.",
        "For the Canadian Hansards task, all the improvements of the alignment error rate are statistically significant at the 99% level."
      ]
    },
    {
      "heading": "5.4 Generalized Alignments",
      "text": [
        "In (Och and Ney, 2003) generalized alignments are used, thus the final Viterbi alignments of both translation directions are combined using some heuristic.",
        "Experimentally, the best heuristic for the Canadian Hansards task is the intersection.",
        "For the Verbmobil task, the refined method of (Och and Ney, 2003) is used.",
        "The results are summarized in Table 5.",
        "We see that both the linear and the loglinear lexicon symmetrization methods yield an improvement with respect to the alignment error rate.",
        "For the Verbmobil task, the improvement with the loglinear interpolation is statistically significant at the 99% level.",
        "For the Canadian Hansards task, both lexicon symmetrization methods result in statistically significant improvements at the 95% level.",
        "Additionally, we observe that precision and recall are more balanced for the symmetrized lexicon variants, especially for the Canadian Hansards Table 6: Effect of smoothing the lexicon probabilities on the alignment performance for the Verbmobil task (S-*T: source-to-target direction, smooth English; T-*S: target-to-source direction, smooth German; all numbers in percent)."
      ]
    },
    {
      "heading": "5.5 Lexicon Smoothing",
      "text": [
        "In Table 6, we present the results for the lexicon smoothing as described in Section 4 on the Verbmobil corpus 2.",
        "As expected, a notable improvement in the AER is reached if the lexicon smoothing is performed for German (i.e. for the target-to-source direction), because many full-form words with the same base form are present in this language.",
        "These improvements are statistically significant at the 95% level."
      ]
    },
    {
      "heading": "6 Related Work",
      "text": [
        "The popular IBM models for statistical machine translation are described in (Brown et al., 1993).",
        "The HMM-based alignment model was introduced in (Vogel et al., 1996).",
        "A good overview of these models is given in (Och and Ney, 2003).",
        "In that article Model 6 is introduced as the loglinear interpolation of the other models.",
        "Additionally, state-of-the-art results are presented for the Verbmobil task and the Canadian Hansards task for various configurations.",
        "Therefore, we chose them as baseline.",
        "Compared to our work, these publications kept the training of the two translation directions strictly separate whereas we integrate both directions into one symmetrized training.",
        "Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003).",
        "In (Cherry and Lin, 2003) a probability model Pr(a1 If1 , e1) is used, which is symmetric per definition.",
        "Bilingual bracketing methods were used to produce a word alignment in (Wu, 1997).",
        "(Melamed, 2000) uses an alignment model that enforces one-to-one alignments for nonempty words.",
        "In"
      ]
    },
    {
      "heading": "7 Conclusions",
      "text": [
        "We have addressed the task of automatically generating word alignments for bilingual corpora.",
        "This problem is of great importance for many tasks in natural language processing, especially in the field of machine translation.",
        "We have presented lexicon symmetrization methods for statistical alignment models that are trained using the EM algorithm, in particular the five IBM models, the HMM and Model 6.",
        "We have evaluated these methods on the Verbmobil task and the Canadian Hansards task and compared our results to the state-of-the-art system of (Och and Ney, 2003).",
        "We have shown that both the linear and the loglinear interpolation of lexicon counts after each iteration of the EM algorithm result in statistically significant improvements of the alignment quality.",
        "For the Canadian Hansards task, the AER improved by about 30% relative; for the Verbmobil task the improvement was about 25% relative.",
        "Additionally, we have described lexicon smoothing using the word base forms.",
        "Especially for highly inflected languages such as German, this smoothing resulted in statistically significant improvements.",
        "In the future, we plan to optimize the interpolation weights to balance the two translation directions.",
        "We will also investigate the possibility of generating directly an unconstrained alignment based on the symmetrized lexicon probabilities."
      ]
    },
    {
      "heading": "Acknowledgment",
      "text": [
        "This work has been partially funded by the EU project LC-Star, IST-2001-32216."
      ]
    }
  ]
}
