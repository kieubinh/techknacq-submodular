{
  "info": {
    "authors": [
      "Hideki Isozaki",
      "Hideto Kazawa",
      "Tsutomu Hirao"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1040",
    "title": "A Deterministic Word Dependency Analyzer Enhanced With Preference Learning",
    "url": "https://aclweb.org/anthology/C04-1040",
    "year": 2004
  },
  "references": [
    "acl-A00-2018",
    "acl-C02-1054",
    "acl-C96-1058",
    "acl-J93-2004",
    "acl-N01-1025",
    "acl-P02-1034",
    "acl-P02-1043",
    "acl-P03-1005",
    "acl-P03-1029",
    "acl-P97-1003",
    "acl-W02-2016",
    "acl-W03-0402",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Word dependency is important in parsing technology.",
        "Some applications such as Information Extraction from biological documents benefit from word dependency analysis even without phrase labels.",
        "Therefore, we expect an accurate dependency analyzer trainable without using phrase labels is useful.",
        "Although such an English word dependency analyzer was proposed by Yamada and Matsumoto, its accuracy is lower than state-of-the-art phrase structure parsers because of the lack of top-down information given by phrase labels.",
        "This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver.",
        "Experimental results show that these modules based on Preference Learning give better scores than Collins’ Model 3 parser for these subproblems.",
        "We expect this method is also applicable to phrase structure parsers."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": []
    },
    {
      "heading": "1.1 Dependency Analysis",
      "text": [
        "Word dependency is important in parsing technology.",
        "Figure 1 shows a word dependency tree.",
        "Eisner (1996) proposed probabilistic models of dependency parsing.",
        "Collins (1999) used dependency analysis for phrase structure parsing.",
        "It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002).",
        "However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied.",
        "Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels.",
        "(Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM).",
        "They did not use phrase labels by considering annotation of documents in expert domains.",
        "SVM (Vapnik, 1995) has shown good performance in dif",
        "ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002).",
        "Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust.",
        "In Natural Language Processing, we use tens of thousands of words as features.",
        "Therefore, SVM often gives good performance.",
        "However, the accuracy of Yamada’s analyzer is lower than state-of-the-art phrase structure parsers such as Charniak’s Maximum-Entropy-Inspired Parser (MEIP) (Charniak, 2000) and Collins’ Model 3 parser.",
        "One reason is the lack of top-down information that is available in phrase structure parsers.",
        "In this paper, we show that the accuracy of the word dependency parser can be improved by adding a base-NP chunker, a Root-Node Finder, and a Prepositional Phrase (PP) Attachment Resolver.",
        "We introduce the base-NP chunker because base NPs are important components of a sentence and can be easily annotated.",
        "Since most words are contained in a base NP or are adjacent to a base NP, we expect that the introduction of base NPs will improve accuracy.",
        "We introduce the Root-Node Finder because Ya-mada’s root accuracy is not very good.",
        "Each sentence has a root node (word) that does not modify any other words and is modified by all other words directly or indirectly.",
        "Here, the root accuracy is defined as follows.",
        "Root Accuracy (RA) = #correct root nodes / #sentences (= 2,416) We think that the root node is also useful for dependency analysis because it gives global information to each word in the sentence.",
        "Root node finding can be solved by various machine learning methods.",
        "If we use classifiers, however, two or more words in a sentence can be classified as root nodes, and sometimes none of the words in a sentence is classified as a root node.",
        "Practically, this problem is solved by getting a kind of confidence measure from the classifier.",
        "As for SVM, f (x) defined below is used as a confidence measure.",
        "However, f (x) is not necessarily a good confidence measure.",
        "Therefore, we use Preference Learning proposed by Herbrich et al.",
        "(1998) and extended by Joachims (2002).",
        "In this framework, a learning system is trained with samples such as “A is preferable to B” and “C is preferable to D.” Then, the system generalizes the preference relation, and determines whether “X is preferable to Y” for unseen X and Y.",
        "This framework seems better than SVM to select best things.",
        "On the other hand, it is well known that attachment ambiguity of PP is a major problem in parsing.",
        "Therefore, we introduce a PP-Attachment Resolver.",
        "The next sentence has two interpretations.",
        "He saw a girl with a telescope.",
        "1) The preposition ‘with’ modifies ‘saw.’ That is, he has the telescope.",
        "2) ‘With’ modifies ‘girl.’ That is, she has the telescope.",
        "Suppose 1) is the correct interpretation.",
        "Then, “with modifies saw” is preferred to “with modifies girl.” Therefore, we can use Preference Learning again.",
        "Theoretically, it is possible to build a new Dependency Analyzer by fully exploiting Preference Learning, but we do not because its training takes too long."
      ]
    },
    {
      "heading": "1.2 SVM and Preference Learning",
      "text": [
        "Preference Learning is a simple modification of SVM.",
        "Each training example for SVM is a pair (yi, xi), where xi is a vector, yi = +1 means that xi is a positive example, and yi = −1 means that xi is a negative example.",
        "SVM classifies a given test vector x by using a decision function",
        "where {ai} and b are constants and E is the number of training examples.",
        "K(xi, xj) = 0(xi) · O(xj) is a predefined kernel function.",
        "O(x) is a function that maps a vector x into a higher dimensional space.",
        "Training of SVM corresponds to the following quadratic maximization (Cristianini and ShaweTaylor, 2000)",
        "where 0 < ai < C and Ei= 1 aiyi = 0.",
        "C is a soft margin parameter that penalizes misclassification.",
        "On the other hand, each training example for Preference Learning is given by a triplet (yi, xi.",
        "1, xi.2), where xi.1 and xi.2 are vectors.",
        "We use xi.. to represent the pair (xi.1, xi.2).",
        "yi = +1 means that xi.1 is preferable to xi.2.",
        "We can regard their difference O(xi.1) − O(xi.2) as a positive example and O(xi.2) − 0(xi.1) as a negative example.",
        "Symmetrically, yi = −1 means that xi.2 is preferable to xi.1.",
        "Preference of a vector x is given by",
        "If g(x) > g(x') holds, x is preferable to x'.",
        "Since Preference Learning uses the difference 0(xi.1) − O(xi.2) instead of SVM’s O(xi), it corresponds to the following maximization.",
        "where 0 < ai < C and K(xi.",
        "*, xj.",
        "*) = K(xi.1, xj.1) − K(xi.1, xj.2) − K(xi.2, xj.1) + K(xi.2,xj.2).",
        "The above linear constraint",
        "Preference Learning because SVM requires this constraint for the optimal b, but there is no b in g(x).",
        "Although SVMlight (Joachims, 1999) provides an implementation of Preference Learning, we use our own implementation because the current SVMlight implementation does not support non-linear kernels and our implementation is more efficient.",
        "Herbrich’s Support Vector Ordinal Regression (Herbrich et al., 2000) is based on Preference Learning, but it solves an ordered multiclass problem.",
        "Preference Learning does not assume any classes."
      ]
    },
    {
      "heading": "2 Methodology",
      "text": [
        "Instead of building a word dependency corpus from scratch, we use the standard data set for comparison.",
        "That is, we use Penn Treebank’s Wall Street Journal data (Marcus et al., 1993).",
        "Sections 02 through 21 are used as training data (about 40,000 sentences) and section 23 is used as test data (2,416 sentences).",
        "We converted them to word dependency data by using Collins’ head rules (Collins, 1999).",
        "The proposed method uses the following procedures.",
        "• A base NP chunker: We implemented an SVM-based base NP chunker, which is a simplified version of Kudo’s method (Kudo and Matsumoto, 2001).",
        "We use the ‘one vs. all others’ backward parsing method based on an ‘IOB2’ chunking scheme.",
        "By the chunking, each word is tagged as – B: Beginning of abase NP, – I: Other elements of a base NP.",
        "– O: Otherwise.",
        "Please see Kudo’s paper for more details.",
        "• A Root-Node Finder (RNF): We will describe this later.",
        "• A Dependency Analyzer: It works just like Ya-mada’s Dependency Analyzer.",
        "• A PP-Attatchment Resolver (PPAR): This resolver improves the dependency accuracy of prepositions whose part-of-speech tags are IN or TO.",
        "The above procedures require a part-of-speech tagger.",
        "Here, we extract part-of-speech tags from the Collins parser’s output (Collins, 1997) for section 23 instead of reinventing a tagger.",
        "According to the document, it is the output of Ratnaparkhi’s tagger (Ratnaparkhi, 1996).",
        "Figure 2 shows the architecture of the system.",
        "PPAR’s output is used to rewrite the output of the Dependency Analyzer."
      ]
    },
    {
      "heading": "2.1 Finding root nodes",
      "text": [
        "When we use SVM, we regard root-node finding as a classification task: Root nodes are positive examples and other words are negative examples.",
        "For this classification, each word wi in a tagged sentence T = (w1/p1, ... , wi/pi, ... , wN/pN) is characterized by a set of features.",
        "Since the given POS tags are sometimes too specific, we introduce a rough part-of-speech qi defined as follows.",
        "• q = N if p = NN, NNP, NNS, NNPS, PRP, PRP$, POS.",
        "• q = V if p = VBD, VB, VBZ, VBP, VBN.",
        "• q = J if p = JJ, JJR, JJS .",
        "Then, each word is characterized by the following features, and is encoded by a set of boolean variables.",
        "• The word itself wi, its POS tags pi and qi, and its base NP tag bi = B, I, O.",
        "We introduce boolean variables such as currentwordisJohn and cur-rentroughPOSisJ for each of these features.",
        "• Previous word wi_1 and its tags, pi-1, qi_1, and bi_1.",
        "• Next word wi+1 and its tags, pi+1, qi+1, and bi+1.",
        "• The set of left words {w0,... , wi_1}, and their tags, {p0, ... , pi_1 }, {q0, ... , qi_1 }, and {b0, ... , bi_1}.",
        "We use boolean variables such as oneof _theleftwords is -Mary.",
        "• The set of right words {wi+1, ... , wN}, and their POS tags, {pi+1, ... , pN} and {qi+1, ... , qN}.",
        "• Whether the word is the first word or not.",
        "We also add the following boolean features to get more contextual information.",
        "• Existence of verbs or auxiliary verbs (MD) in the sentence.",
        "• The number of words between wi and the nearest left comma.",
        "We use boolean variables such as near-estleftcommaistwowordsaway.",
        "• The number of words between wi and the nearest right comma.",
        "Now, we can encode training data by using these boolean features.",
        "Each sentence is converted to the set of pairs { (yi, xi) } where yi is + 1 when xi corresponds to the root node and yi is −1 otherwise.",
        "For Preference Learning, we make the set of triplets {(yi, xi.",
        "1, xi.2)}, where yi is always +1, xi.1 corresponds to the root node, and xi.2 corresponds to a non-root word in the same sentence.",
        "Such a triplet means that xi.1 is preferable to xi.2 as a root node."
      ]
    },
    {
      "heading": "2.2 Dependency analysis",
      "text": [
        "Our Dependency Analyzer is similar to Ya-mada’s analyzer (Yamada and Matsumoto, 2003).",
        "While scanning a tagged sentence T = (w1/p1, ... , wn/pn) backward from the end of the sentence, each word wi is classified into three categories: Left, Right, and Shift.1 • Right: Right means that wi directly modifies the right word wi+1 and that no word in T modifies wi.",
        "If wi is classified as Right, the analyzer removes wi from T and wi is registered as a left child of wi+1.",
        "• Left: Left means that wi directly modifies the left word wi-1 and that no word in T modifies wi.",
        "If wi is classified as Left, the analyzer removes wi from T and wi is registered as a right child of wi-1.",
        "• Shift: Shift means that wi is not next to its modificand or is modified by another word in T. If wi is classified as Shift, the analyzer does nothing for wi and moves to the left word",
        "wi-1.",
        "This process is repeated until T is reduced to a single word (= root node).",
        "Since this is a three-class problem, we use ‘one vs. rest’ method.",
        "First, we train an SVM classifier for each class.",
        "Then, for each word in T, we compare their values: fLeft (x), fRight (x), and fShift (x) .",
        "If fLeft (x) is the largest, the word is classified as Left.",
        "However, Yamada’s algorithm stops when all words in T are classified as Shift, even when T has two or more words.",
        "In such cases, the analyzer cannot generate complete dependency trees.",
        "Here, we resolve this problem by reclassifying a word in T as Left or Right.",
        "This word is selected in terms of the differences between SVM outputs:",
        "• ALeft (x) = fShift (x) − fLeft (x), • ARight (x) = fShift(x) − fRight (x) .",
        "These values are non-negative because fShift(x) was selected.",
        "For instance, ALeft (x) ^ 0 means that fLeft(x) is almost equal to fShift (x) .",
        "If ALeft (xk) gives the smallest value of these differences, the word corresponding to xk is reclassified as Left.",
        "If",
        "ORight(xk) gives the smallest value, the word corresponding to xk is reclassified as Right.",
        "Then, we can resume the analysis.",
        "We use the following basic features for each word in a sentence.",
        "• The word itself wi and its tags pi, qi, and bi, • Whether wi is on the left of the root node or on the right (or at the root node).",
        "The root node is determined by the Root-Node Finder.",
        "• Whether wi is inside a quotation.",
        "• Whether wi is inside a pair of parentheses.",
        "• wi’s left children {wi1, ... , wik}, which",
        "were removed by the Dependency Analyzer beforehand because they were classified as ‘Right.’ We use boolean variables such as oneof _theleftchildis-Mary.",
        "Symmetrically, wi’s right children {wi1, .",
        ".",
        ".",
        ", wik} are also used.",
        "However, the above features cover only near-sighted information.",
        "If wi is next to a very long base NP or a sequence of base NPs, wi cannot get information beyond the NPs.",
        "Therefore, we add the following features.",
        "• Li, Ri: Li is available when wi immediately follows a base NP sequence.",
        "Li is the word before the sequence.",
        "That is, the sentence looks like: ... Li (abase NP ) wi ... Ri is defined symmetrically.",
        "The following features of neigbors are also used as wi’s features.",
        "• Left words wi-3, .",
        ".",
        ".",
        ", wi-1 and their basic features.",
        "• Right words wi+1, .",
        ".",
        ".",
        ", wi+3 and their basic features.",
        "• The analyzer’s outputs (Left/Right/Shift) for wi+1 , .",
        ".",
        ".",
        ", wi+3.",
        "(This analyzer runs backward from the end of T.)",
        "If we train SVM by using the whole data at once, training will take too long.",
        "Therefore, we split the data into six groups: nouns, verbs, adjectives, prepositions, punctuations, and others."
      ]
    },
    {
      "heading": "2.3 PP attachment",
      "text": [
        "Since we do not have phrase labels, we use all prepositions (except root nodes) as training data.",
        "We use the following features for resolving PP attachment.",
        "• The preposition itself: wi.",
        "• Candidate modificand wj and its POS tag.",
        "• Left words (wi_2, wi-1) and their POS tags.",
        "• Right words (wi+1, wi+2) and their POS tags.",
        "• Previous preposition.",
        "• Ending word of the following base NP and its POS tag (if any).",
        "• i − j, i.e., Number of the words between wi and wj .",
        "• Number of commas between wi and wj .",
        "• Number of verbs between wi and wj .",
        "• Number of prepositions between wi and wj .",
        "• Number of base NPs between wi and wj.",
        "• Number of conjunctions (CCs) between wi and wj .",
        "• Difference of quotation depths between wi and wj.",
        "If wi is not inside of a quotation, its quotation depth is zero.",
        "If wj is in a quotation, its quotation depth is one.",
        "Hence, their difference is one.",
        "• Difference of parenthesis depths between wi and wj .",
        "For each preposition, we make the set of triplets {(yi, xi, 1, xi,2)}, where yi is always +1, xi 1 corresponds to the correct word that is modified by the preposition, and xi,2 corresponds to other words in the sentence."
      ]
    },
    {
      "heading": "3 Results",
      "text": []
    },
    {
      "heading": "3.1 Root-Node Finder",
      "text": [
        "For the Root-Node Finder, we used a quadratic kernel K(xi, xj) = (xi· xj + 1)2 because it was better than the linear kernel in preliminary experiments.",
        "When we used the ‘correct’ POS tags given in the Penn Treebank, and the ‘correct’ base NP tags given by a tool provided by CoNLL 2000 shared task2, RNF’s accuracy was 96.5% for section 23.",
        "When we used Collins’ POS tags and base NP tags based on the POS tags, the accuracy slightly degraded to 95.7%.",
        "According to Yamada’s paper (Yamada and 2 http://cnts.uia.",
        "ac.be/conll200/chunking/ Matsumoto, 2003), this root accuracy is better than Charniak’s MEIP and Collins’ Model 3 parser.",
        "We also conducted an experiment to judge the effectiveness of the base NP chunker.",
        "Here, we used only the first 10,000 sentences (about 1/4) of the training data.",
        "When we used all features described above and the POS tags given in Penn Treebank, the root accuracy was 95.4%.",
        "When we removed the base NP information (bi, Li, Ri), it dropped to 94.9%.",
        "Therefore, the base NP information improves RNF’s performance.",
        "Figure 3 compares SVM and Preference Learning in terms of the root accuracy.",
        "We used the first 10,000 sentences for training again.",
        "According to this graph, Preference Learning is better than SVM, but the difference is small.",
        "(They are better than Maximum Entropy Modeling3 that yielded RA=91.5% for the same data.)",
        "C does not affect the scores very much unless C is too small.",
        "In this experiment, we used Penn’s ‘correct’ POS tags.",
        "When we used Collins’ POS tags, the scores dropped by about one point."
      ]
    },
    {
      "heading": "3.2 Dependency Analyzer and PPAR",
      "text": [
        "As for the dependency learning, we used the same quadratic kernel again because the quadratic kernel gives the best results according to Yamada’s experiments.",
        "The soft margin parameter C is 1 following Yamada’s experiment.",
        "We conducted an experiment to judge the effectiveness of the Root-Node Finder.",
        "We follow Yamada’s definition of accuracy that excludes punctuation marks.",
        "Dependency Accuracy (DA) = #correct parents / #words (= 49,892) Complete Rate (CR) = #completely parsed sentences / #sentences According to Table 1, DA is only slightly improved, but CR is more improved.",
        "in terms of the Dependency Accuracy of prepositions.",
        "SVM’s performance is unstable for this task, and Preference Learning outperforms SVM.",
        "(We could not get scores of Maximum Entropy Modeling because of memory shortage.)",
        "Table 2 shows the improvement given by PPAR.",
        "Since training of PPAR takes a very long time, we used only the first 35,000 sentences of the training data.",
        "We also calculated the Dependency Accuracy of Collins’ Model 3 parser’s output for section 23.",
        "According to this table, PPAR is better than the Model 3 parser.",
        "Now, we use PPAR’s output for each preposition instead of the dependency parser’s output unless the modification makes the dependency tree into a non-tree graph.",
        "Table 3 compares the proposed method with other methods in terms of accuracy.",
        "This data except ‘Proposed’ was cited from Yamada’s paper.",
        "According to this table, the proposed method is close to the phrase structure parsers except Complete Rate.",
        "Without PPAR, DA dropped to 90.9% and CR dropped to 39.7%."
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "We used Preference Learning to improve the SVM-based Dependency Analyzer for root-node finding and PP-attachment resolution.",
        "Preference Learning gave better scores than Collins’ Model 3 parser for these subproblems.",
        "Therefore, we expect that our method is also applicable to phrase structure parsers.",
        "It seems that root-node finding is relatively easy and SVM worked well.",
        "However, PP attachment is more difficult and SVM’s behavior was unstable whereas Preference Learning was more robust.",
        "We want to fully exploit Preference Learning for dependency analysis and parsing, but training takes too long.",
        "(Empirically, it takes O(E2) or more.)",
        "Further study is needed to reduce the computational complexity.",
        "(Since we used Isozaki’s methods (Isozaki and Kazawa, 2002), the runtime complexity is not a problem.)",
        "Kudo and Matsumoto (2002) proposed an SVM-based Dependency Analyzer for Japanese sentences.",
        "Japanese word dependency is simpler because no word modifies a left word.",
        "Collins and Duffy (2002) improved Collins’ Model 2 parser by reranking possible parse trees.",
        "Shen and Joshi (2003) also used the preference kernel 1C(xi.",
        "*, xj.",
        "*) for reranking.",
        "They compare parse trees, but our system compares words."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "Dependency analysis is useful and annotation of word dependency seems easier than annotation of phrase labels.",
        "However, lack of phrase labels makes dependency analysis more difficult than phrase structure parsing.",
        "In this paper, we improved a deterministic dependency analyzer by adding a Root-Node Finder and a PP-Attachment Resolver.",
        "Preference Learning gave better scores than Collins’ Model 3 parser for these subproblems, and the performance of the improved system is close to state-of-the-art phrase structure parsers.",
        "It turned out",
        "that SVM was unstable for PP attachment resolution whereas Preference Learning was not.",
        "We expect this method is also applicable to phrase structure parsers."
      ]
    }
  ]
}
