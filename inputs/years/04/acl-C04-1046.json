{
  "info": {
    "authors": [
      "John Blatz",
      "Erin Fitzgerald",
      "George Foster",
      "Simona Gandrabur",
      "Cyril Goutte",
      "Alex Kulesza",
      "Alberto Sanchis",
      "Nicola Ueffing"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1046",
    "title": "Confidence Estimation for Machine Translation",
    "url": "https://aclweb.org/anthology/C04-1046",
    "year": 2004
  },
  "references": [
    "acl-J04-4002",
    "acl-J93-2003",
    "acl-P00-1016",
    "acl-W03-0413"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a detailed study of confidence estimation for machine translation.",
        "Various methods for determining whether MT output is correct are investigated, for both whole sentences and words.",
        "Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed.",
        "We present results on data from the NIST 2003 Chinese-to-English MT evaluation."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "All current NLP technologies make mistakes.",
        "Applications built on these technologies can cope with mistakes better if they have some reliable indication of when they may have occurred.",
        "For instance, in a speech recognition dialog system, low confidence in the analysis of a user's utterance can lead the system to prompt for a repetition.",
        "This strategy has the potential to significantly improve the system's usability if accurate estimates of correctness can be made.",
        "The binary classification problem of assessing the correctness of an NLP system's output is known as confidence estimation (CE).",
        "It has been extensively studied for speech recognition, but is not well known in other areas.",
        "The motivation for our work was to apply CE techniques to another NLP problem, measure performance, and attempt to draw general conclusions.",
        "We focused on machine translation because it is an important area of NLP, and one where CE has the potential to enable new applications.",
        "In this paper we study confidence estimation for both sentences and words in MT output.",
        "Since most MT systems operate at the sentence level, sentences are natural targets for correctness judgements.",
        "The main challenges in making these judgements are that MT output is rarely correct at the sentence level to begin with, and that there is no satisfactory automatic method for determining whether or not a given output sentence is correct, even if reference translation(s) are available.",
        "Applications for sentence-level CE include filtering translations for human post-editing or information gathering, combining output from different MT systems, and active learning (Ngai and Yarowsky, 2000).",
        "CE for words is relatively unaffected by the problems that apply at the sentence level.",
        "Individual words are more likely to be correct than are whole sentences, and their correctess can be assessed fairly reliably by comparison to reference translations.",
        "On the other hand, what correctness means is less obvious at this level; a word could be correct in some possible translation, but wrong in the current context.",
        "Potential applications here include post-editing, interactive machine translation systems, recombination of multiple sentence-level MT hypotheses, and improved search algorithms (Neti et al., 1997).",
        "In the remainder of the paper, we first give some background on CE in general (s2), then describe our experimental setting (s3), present sentence-level (s4) and word-level (s5) results, and make some concluding remarks (s6)."
      ]
    },
    {
      "heading": "2 Background",
      "text": [
        "The goal of CE is to characterize the behaviour of a base NLP system that produces an output y given an input x.",
        "One way of doing so, which we call weak CE, is to build a classifier that takes x and y as input and returns a correctness score.",
        "Various decisions can then be based on (suitably optimized) thresholding against this score.",
        "When scores are direct estimates of correctness probabilities, they have a somewhat wider range of applications; we refer to this as strong CE.",
        "Both strong and weak approaches are reported in the speech literature; evaluation techniques for each are described in section 3.3 and in (Siu and Gish, 1999).",
        "CE techniques also differ in whether or not they use a separate \"CE layer\" distinct from",
        "the base NLP system.",
        "Many approaches, eg (Wessel et al., 2001), derive confidence scores, such as posterior probabilities P(y\\x), directly from quantities in the base system.",
        "However, methods in which the CE portion is separate predominate.",
        "Although these have the disadvantage of requiring a training corpus of examples labelled for correctness, they are more powerful and modular.",
        "A wide range of machine learning algorithms have been tried in this setting, including naive Bayes (Sanchis et al., 2003), neural nets (Guillevic et al., 2002), and SVMs (Zhang and Rudnicky, 2001).",
        "All previous work on CE for MT has been done by some of us.",
        "Ueffing et al. (Uefling et al., 2003) describe several direct methods, including posterior probabilities, for estimating the correctness of individual words in MT output.",
        "Gandrabur and Foster (Gandrabur and Foster, 2003) describe the use of a neural-net CE layer to sharpen probability estimates for text predictions in an interactive translators' tool."
      ]
    },
    {
      "heading": "3 Experimental Setting 3.1 Corpora",
      "text": [
        "Our corpora consist of Chinese-to-English evaluation sets from NIST MT competitions, as well as a large multi-reference corpus provided by the Linguistic Data Consortium (LDC), cf. table 1.",
        "These were divided into separate train, validation, and test portions.",
        "We obtained ouput from the ISI Alignment Template MT system (Och and Ney, 2004) that participated in the 2003 NIST evaluation (NIST, 2003).",
        "For each source sentence, the system produced an JV-best list of translation candidates, of which we used the top 1,000 for all experiments described in this paper.",
        "Each resulting source-sentence/target-candidate pair was treated as an independent example (many examples for word experiments), whose correctness was established from the reference translation(s) available for the source sentence.",
        "The exact method of defining correctness varied across different experiments, as described below.",
        "Our data can be viewed as a collection of pairs (x,c) in which x is a feature vector and c a correctness indicator.",
        "We explored different ways of capturing the relationship between x and c. For weak CE, we used scores derived directly from x, and also (at the sentence level) multilayer perceptron (MLP) based regression models of MT evaluation scores from which c was derived.",
        "For strong CE, we used naive Bayes and MLP to estimate the probability of correctness P(c = l|x).",
        "Our choice of these learning methods was driven by constraints on the resources required for training on millions of examples: naive Bayes requires only a single pass over the data for parameter estimation; while MLP typically requires only a few passes when using stochastic gradient descent.",
        "Naive Bayes (NB) In a probabilistic setting, the posterior class probability is given by P(c|x) oc P(c)P(x|c).",
        "The Naive Bayes assumption is that features are statistically independent: P(x|c) = Y\\d=i P{xd\\c)i where D is the dimension of the feature vector x. Parameters are estimated using an absolute discounting smoothing of the maximum likelihood solution.",
        "A small constant b E (0,1) is discounted from every positive count and distributed accross all events with null counts.",
        "Denoting JV and N(c) the number of examples in total and in class c, respectively, N(x,i, c) the number of examples in class c with feature value xd, and JV+, JV_ the number of possible values of x(i with N(xd, c) > 0 and with N{xd, c) = 0, respectively, estimates are P(c) = N(c)/N and: For word-level CE, the word class prior probability is considered in the word posterior class estimation: P(c|x, w) oc P(c\\w)P(x.\\c), where P(c\\w) is smoothed as above.",
        "Continuous features are discretised into a fixed number of bins (usually 20) by visual inspection of the histograms of the feature values.",
        "test test As examples typically arise from the same source sentence, they have many similar features and are therefore highly redundant.",
        "Convergence of stochastic gradient descent is guaranteed under certain conditions, in particular examples must be presented in random order.",
        "In our case, we usually have too many examples to first load the training set in memory, then pick examples at random.",
        "We therefore implemented a random caching mechanism, where data are loaded sequentially but unloaded randomly, in order to simulate an indépendant random pick from the entire training set.",
        "Although the examples are only approximately indépendant using this caching system, we observed empirically that when the cache was large enough to hold all the examples corresponding the several source sentences, the final performance was indistinguishable from a model trained using truly independent random examples.",
        "This random cache was implemented in the framework of Torch (Collobert et al., 2002).",
        "As mentioned earlier, we are interested in assessing the performance of CE techniques in two settings: strong CE, requiring accurate probabilities of correctness; and weak CE, requiring only binary classification.",
        "In order to evaluate our models, we use different metrics, all calculated over a test set {(xW.cW)}; the indicator is 1 iff is correct, and 0 otherwise.",
        "We let n\\ and no designate the numbers of correct and incorrect examples.",
        "Strong CE metric: NCE A standard way of measuring the fit between a probabilistic model and a test corpus is negative log-likelihood (or cross entropy): NLL = -^logP(c«|xW)/n.",
        "To remove dependence on the proportion of correct examples in the corpus, we use normalized cross entropy (NCE): The baseline NLL corresponds to assigning fixed probabilities of correctness based on the empirical class frequencies: NLL& = -(n0/n) log(n0/n) - (m/n) log(ni/n).",
        "Weak CE metrics: CER and (I)ROC The metrics we use for weak CE attempt to capture the discriminability of the classification function across the range of all thresholds used to decide correctness.",
        "The simplest metric is the classification error rate (CER): the proportion of examples on which the classifier's output differs from the true correctness indicator.",
        "The values of CER we use are based on thresholds optimized on the test set (for sentence-level experiments), and on the validation set (for word experiments).",
        "The baseline is a classifier which assigns all examples to the most frequent class, for which CERfo = min(no, n\\)/n.",
        "Another common way to assess the discriminability of a classifier is to use the receiver operating characteristic (ROC) curves (Duda et al., 2001).",
        "These plot correct-reject ratio (true negatives/no) vs correct-accept ratio (true positives/rii) for different thresholds.",
        "The ROC curve lies in the unit square, with random choice corresponding to the diagonal and perfect discrimination corresponding to the edges.",
        "A related quantitative measure is the area under the ROC curve or IROC, which gives a global indication of the discriminability over all possible rejection thresholds."
      ]
    },
    {
      "heading": "4 Sentence-Level Experiments",
      "text": [
        "In order to assign a correctness c to each translation hypothesis, we threshold automatic MT evaluation measures.",
        "We use the two measures which correlated best with human judgement in the evaluation exercise described in (Blatz et al., 2003): WERg, the word error rate, normalised by the length of the Levenshtein alignment; and NIST, the sentence-level NIST score (Dod-dington, 2002).",
        "We use two different thresholds for each score, giving four problem settings in total.",
        "The first threshold produces 5% of \"correct\" examples, and is intended to be sufficient for gisting purposes.",
        "The second one tags 30% of examples as correct, which we believe would be enough for applications that require a simple bag-of-words translation, such as cross-language IR.",
        "We used a total of 91 sentence-level features, which we summarize briefly in this section.",
        "A detailed list is given in (Blatz et al., 2003).",
        "Features in this class aimed to capture the translation relation between the source sentence and target hypothesis.",
        "These included IBM model 1 (Brown et al., 1993) probabilities in both directions, word-alignment monotonicity, and various kinds of agreement with other word-level translations in the JV-best list, including a semantic similarity metric based on WordNet.",
        "We compared various MLPs, trained on all features, on the four problem settings described above.",
        "Models used varying numbers of hidden units (from 0 to 20), and either classification or regression.",
        "Table 2 shows the performance of the best configurations for classification and regression.",
        "The number of hidden units for each model is omitted because it has no clear correlation with performance.",
        "However, there is a clear trend in which classification models do better Table 2: Best results for classification (top box) and regression, with 95% confidence bounds.",
        "N and W stand for NIST and WERg in the problem setting column.",
        "Baseline values for CER are 3.21%, 32.5%, 5.65%, and 32.5% for each problem setting, in order.",
        "Note that the results on each line in this table are not necessarily generated by a single model.",
        "than regression models, particularly as measured by IROC.",
        "(This is not completely surprising, given that classification MLPs were specifically trained for the corresponding thresholds.)",
        "Globally, performance is better than the baseline in all cases except CER for the NIST 5% setting.",
        "We compared the contributions of all features, both as individual confidence scores and as part of feature groups used to train MLPs.",
        "To group features, we classified them in two independent ways according to whether they apply to the source sentence, the target hypothesis, or both; and according to whether they depend on the base model or could be calculated without knowledge of it.",
        "We also treated the base-model's scores as group on their own.",
        "All feature experiments were performed only for the NIST 30% setting.",
        "Results are shown in table 3 and figure 1.",
        "The most striking observation is that the MLP trained on only the twelve feature functions from the base model is almost as good as the one trained on all features.",
        "Another pattern is that features that depend on the base model are more useful that those that do not, and features that apply to the target hypothesis are more useful than ones that apply only to the source sentence (as well as, to a much lesser extent, those that apply to a source/target pair).",
        "A final conclusion is that a model that has been trained on labelled data – regardless of the feature set used – is better at discriminating than any single feature on its own."
      ]
    },
    {
      "heading": "5 Word-Level Experiments",
      "text": [
        "It is not intuitively clear how to classify words in MT output as correct or incorrect when comparing the translation to one or several references.",
        "We implemented a number of different measures that were inspired by automatic evaluation metrics like WER and PER.",
        "Pos: This error measure considers a word as correct if it occurs in exactly this target position in the reference translation.",
        "WER: A word is counted as correct if it is Levenshtein-aligned to itself in the reference.",
        "PER: A word is tagged as correct if it occurs in the reference translation.",
        "The word order is completely disregarded, but the number of occurrences is taken into account.",
        "For all error metrics, we determine the reference with minimum distance to the hypothesis according to the metric under consideration and classify the words as correct or incorrect with respect to this reference.",
        "These error metrics behave significantly different with regard to the percentage of words that are labeled as correct.",
        "Pos is very pessimistic with only 15% correct words on the corpora described in section 3.1, whereas WER labels 43% as correct, and PER 64%, respectively.",
        "Note that those figures are not the translation errors for the system output.",
        "They are calculated for every hypothesis in the JV-best list (and not only for the single best translation).",
        "We used 17 features in total which we will describe shortly in this section.",
        "For more details, see (Blatz et al., 2003).",
        "SMT Model Based Features We investigated two features that are based directly on namely the Alignment Template MT model (Och and Ney, 2004).",
        "One gives the identity of the so-called Alignment Template, i.e. the bilingual phrase, that was applied in the translation of the current target word.",
        "Another feature specifies whether the target word was translated by a rule based system or not.",
        "This rule based system was integrated into the translation process for the translation of special phenomena such as dates and time expressions.",
        "IBM Model 1 We implemented one feature that determines the average translation probability of the target word e over the source sentence words according to Modell introduced by IBM in (Brown et al., 1993).",
        "This captures a sort of topic or semantic coherence in translations.",
        "Word Posterior Probabilities and Related Measures We investigated three different features introduced in (Ueffrng et al., 2003) that are calculated rather similarly: relative frequencies, rank weighted frequencies and word posterior probabilities.",
        "Each of them is based on determining those sentences in the JV-best list that contain the word e under consideration in a certain position.",
        "The first variant (called any in table 4) regards all those sentences that contain e at all, whereas the second variant (source) considers all sentences where e occurs as the translation of the same source word(s).",
        "The third variant (target) determines only those target sentences containing e in exactly the same position.",
        "We implemented three different features using the semantic data provided by WordNet.",
        "The first similarity feature is the average semantic similarity from the word in question to the word aligned to the same source position in each of the top three hypotheses.",
        "The two other features come from WordNet's polysemy count, for details see (Blatz et al., 2003).",
        "Two more target language based features were implemented.",
        "One is a basic syntax check that looks to highlight hypotheses with mismatched parentheses and/or quotation marks.",
        "The second feature counts the number of occurences in the sentence for each word in the target sentence.",
        "Using the Naive Bayes classifier, we tested the performance of single features for word confidence estimation.",
        "Additionally, we combined the best 3 and all 17 features with the same classifier.",
        "Table 4 shows the confidence estimation performance of single features in terms of CER and IROC using the error measure PER for labeling words as correct or incorrect.",
        "The features which yield the best results are the word posterior probability, rank weighted frequency, and relative frequency (WPP) with respect to occurrence of the word in any position in the target sentence.",
        "Those three features give a significant improvement over the baseline of more than 5% absolute in CER.",
        "The feature based on Modell also discriminates very well, followed closely by the WPP with regard to the aligned source position(s).",
        "The combination of three of the best performing features (word posterior probabilities with respect to different criteria and the Modell based feature) yields a significant improvement over the performance of any of the single features.",
        "There is no significant change in CER or IROC if more information is added by combining all 17 features.",
        "For word level confidence estimation, we investigated several different MLP architectures, with the number of hidden units ranging from 0 to 20.",
        "We see that the Naive Bayes classifier and the MLP with zero hidden units have a very similar performance.",
        "But as soon as the MLP gets more complex by the addition of more hidden units, the MLP outperforms the Naive Bayes approach significantly.",
        "There is no significant difference between the MLPs consisting of 5 to 20 hidden units.",
        "beginning of the section.",
        "We see that classification according to some of the error measures is easier to learn than according to others.",
        "The highest discriminability is obtained for the most \"relaxed\" measure PER, followed by Pos and WER results are slighly worse."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have reported on the use of various techniques for classifying MT output as correct or not, at both the sentence and word levels.",
        "Both levels present problems for the definition of correctness.",
        "At the sentence level we resolved these problems by using automatic MT evaluation metrics and redefining \"correctness\" to be above a certain threshold (of match to reference translations), which we feel should correspond to usability within different applications.",
        "At the word level we investigated three strategies, differing in strictness, for matching corresponding words in reference translations.",
        "Our main conclusions can be summarized as follows:",
        "• Training a separate layer using machine-learning techniques is better than relying solely on base model scores.",
        "• Features derived from the base model are more valuable than external ones, and should be tried first before investing effort in the implementation of complex external functions.",
        "• Features based on JV-best lists are more valuable than ones based solely on individual hypotheses.",
        "• Features that capture properties of the target text are more valuable than those that do not.",
        "• Multilayer perceptrons (neural nets) outperform naive Bayes models.",
        "MLPs with more hidden units can give better performance than those with fewer.",
        "In future work, we look forward to using the techniques developed here within various applications described in the introduction.",
        "We also intend to continue to refine our definitions of correctness to make them more stable and more broadly applicable."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": []
    }
  ]
}
