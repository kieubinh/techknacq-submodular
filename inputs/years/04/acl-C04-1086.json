{
  "info": {
    "authors": [
      "Slaven Bilac",
      "Hozumi Tanaka"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1086",
    "title": "A Hybrid Back-Transliteration System for Japanese",
    "url": "https://aclweb.org/anthology/C04-1086",
    "year": 2004
  },
  "references": [
    "acl-C02-1099",
    "acl-J01-3002",
    "acl-J98-4003",
    "acl-P00-1037",
    "acl-W02-2017",
    "acl-W98-1005"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Transliterating words and names from one language to another is a frequent and highly productive phenomenon.",
        "Transliteration is information losing since important distinctions are not preserved in the process.",
        "Hence, automatically converting transliter-atcd words back into their original form is a real challenge.",
        "In addition, due to its wide applicability in MT and CLIR, it is an interesting problem from a practical point of view.",
        "In this paper, we propose a new method, combining the transliterated string segmentation module with phoneme-based and grapheme-based transliteration modules in order to enhance the back – transliterations of Japanese words.",
        "Our experiments show significant improvements achieved by the hybrid approach."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "With the advent of technology and increased flow of goods and services, it has become quite common to integrate new words from one language to another.",
        "Whenever a word is adopted into a new language, pronunciation is adjusted to suit the phonetic inventory of the language.",
        "Furthermore, the orthographic form of the word is modified to allow representation in the target language script.",
        "This process of acquisition and assimilation of a new word into an existing writing system is referred to as transliteration (Knight and Graehl, 1998).",
        "For example, the English word cache is transliterated in Japanese as 3� 1 – Y ✓ j \"kyasshu\" .1 Since integration of new words is a very productive process, it often happens that the new",
        "pairs are not recorded in machine or human dictionaries.",
        "Therefore, it is impossible to rely on the dictionary lookup to find the transliteration pairs.",
        "Inability to find a target language equivalent represents a major problem in Machine Translation (MT) since it can cause translation failures.",
        "Furthermore, transliteration represents a serious problem in the area of Cross-Language Information Retrieval (CLIR) where the goal is to retrieve all related documents in two or more languages (Lin and Chen, 2002).",
        "Back-transliteration is the transliteration back into the original language.",
        "It is generally more difficult than transliteration.",
        "Increase in difficulty results from the fact that various distinctions, present in the source language, are not preserved when the word is transliterated into the target language.",
        "For example, Japanese has only five basic vowels and no /0/ or /6/2 sounds.",
        "Non-existent sounds are replaced with their closest equivalents.",
        "Consequently, the following three English words: bass,bath and bus are transliterated as 1� ,7� \"basu\" .3 The system trying to obtain a back-transliteration for I�,7� has therefore three valid choices which cannot be disambiguated in the absence of additional contextual transformation.",
        "While the transliteration commonly reflects the pronunciation of the source language, spelling can also affect it.",
        "For example the first c in eternal is transliterated as \"e\" instead of\"i\" in s:9 – � )L \"etaanaru\".",
        "Transliterated words are normally written in katakana, one of three Japanese writing systems.",
        "While other vocabulary (i.e. animal names or onomatopoeic expressions) can also be written in katakana, the very fact that something is written in katakana is generally a good hint that it might be a transliterated foreign",
        "word or a name.",
        "Thus, unlike Arabic or Korean, where a big part of the back-transliteration problem is identifying candidate transliterations (Stalls and Knight, 1998; Jeong et al., 1999), in Japanese back-transliteration can be directly applied to any katakana strings absent from the bilingual dictionary.",
        "Furthermore, since Japanese does not employ spaces to delimit words, katakana strings are often not individual English words but whole phrases.",
        "For example, �T) �� 5' – ✓ ✓ L '✓ – \"rihabiriteeshonsentaa\" is the transliteration of rehabilitation center.",
        "Abbreviations are common so the rehabilitation center is actually more often transliterated as T) L:f T) � � – \"rihabiri sent aa\".",
        "In this paper we describe a system which first finds the best segmentation of a transliterated string and then obtains back-transliterations using the combined information based on pronunciation and spelling.",
        "Our goal is to demonstrate that by recognizing the weaknesses of existing models and carefully combining them to capitalize on their synergies can significantly improve the overall performance even without substantial modifications to the underlying models.",
        "The reminder of this paper is organized as follows: in Section 2 we review previous research.",
        "Section 3 describes the proposed segmentation method and Section 4 outlines the transliteration model.",
        "Finally, Section 5 gives a short evaluation and a discussion of the results obtained."
      ]
    },
    {
      "heading": "2 Previous research",
      "text": [
        "Previous approaches to (back-)transliteration can be roughly divided into two groups: grapheme and phoneme-based.",
        "These approaches are also referred to as direct and pivot-based methods, respectively."
      ]
    },
    {
      "heading": "2.1 Grapheme-based modeling",
      "text": [
        "In this framework, the English string is not converted into a phonemic representation before its alignment with the transliterated string (Kang and Choi, 2000; Coto et al., 2003).",
        "Brill et al.",
        "(2001) propose a noisy channel model allowing for non-atomics edits (Brill and Moore, 2000).",
        "The input string is broken down into arbitrary substrings, each of which is output independently (and possibly incorrectly).",
        "The best back-transliteration is chosen using a modified edit distance algorithm (Damerau, 1964; Leven-sthein, 1966).",
        "This method fails to generate the correct string in cases where English spelling is not reflected in the pronunciation (e.g --�F � \"maimu\" being incorrectly back-transliterated into maim instead of mime).",
        "Furthermore, since the transliterations are compared to dictionary entries this method does not handle phrases directly."
      ]
    },
    {
      "heading": "2.2 Phoneme-based modeling",
      "text": [
        "In this approach the pronunciation, rather than the spelling of the original string is considered as a basis for transliteration (Jeong et al., 1999; Oh and Choi, 2002).",
        "For Japanese, Knight and Craehl (1998) employ a compositional model combining romaji-to-phoneme, phoneme-to-English and English word probability models.",
        "The combined structure is treated as a graph, and the top ranking strings are found using the k-best path algorithm (Epp-stein, 1994).",
        "A similar model has been applied for Arabic-English back-transliteration (Stalls and Knight, 1998).",
        "However, this model cannot handle cases where the transliteration reflects the original spelling.",
        "Furthermore, even though the system handles phrases, commonly it is the case that even though the correct back-transliterations can be obtained for each of the words in the phrase when handled separately, the system does not output the correct answer when handled as a phrase.",
        "For example, both – \"teepu\" and }} 7 � 5 L� \"sabushisutemu\" are correctly transliterated as tape and subsystem, respectively, but the output for the phrase 5' – 5 \"teepusabushisutemu\" is incorrect."
      ]
    },
    {
      "heading": "3 Input Segmentation",
      "text": [
        "Above we have noted that transliterations are commonly based on English phrases and not only individual words.",
        "Therefore, a system able to determine the proper segmentation of the transliterated string W, has a better chance of getting the correct answer.",
        "Since Japanese does not employ word delimiters (i.e. white spaces), text segmentation is a big part of any NLP system dealing with Japanese.",
        "However, two of the most common morpho-syntactic analysis systems: Juman (Kurohashi et al., 1994) and ChaSen (Matumoto et al., 2002) both employ rule based segmentation which heavily relies on dictionary lookups.",
        "Since the back-transliteration is aiming to handle words that are not contained in the dictionary, such systems cannot be readily applied to segmentation of katakana strings.",
        "Therefore, statistical segmentation methods are preferable.",
        "Furthermore, the transliterated strings make up a small percentage of Japanese texts, thus making it very hard to obtain a large training set.",
        "Thus, the segmentation method must be effective with limited amounts of training data like the algorithms used for word discovery (Brent, 1999; Venkataraman, 2001).",
        "Being designed for word acquisition, these methods penalize longer, new words more than shorter ones, hence they often err by over-segmenting the input.",
        "However, over-segmentation of the string significantly hurts the transliteration performance since sub-word chunks cannot be transformed to correct English words.",
        "In the case of under-segmentation, longer chunks can still be correctly transliterated as phrases (see below).",
        "Thus, for transliteration, a system able to segment strings with little training data and without a tendency to over-segment the input string is preferable."
      ]
    },
    {
      "heading": "3.1 Segmentation model",
      "text": [
        "Here we outline an implementation of a simple segmentation model which favors longer, new words over shorter ones.",
        "Given the string W to be segmented the goal is to insert word delimiters (#) so that the overall score W = u'1#a2# #ak of the string is maximized (Equation 1) if each word ai score is assigned according to Equations 2 and 3. model from the unigram model of Venkatara-man (2001).4"
      ]
    },
    {
      "heading": "4 Transliteration models",
      "text": [
        "After the input is segmented we can proceed to transliterate it back into original language.",
        "As pointed out above, both the pronunciation and spelling of the original influence the transliteration.",
        "This being so, we combine them to achieve higher accuracy.",
        "Given some Japanese word in romaji (J,),5 the goal is to find the English word (phrase) E, that maximizes the probability P(E,,,IJ,,,).",
        "Applying the Bayes' rule and dropping the constant denominator we get P(J� IE,) x P(E,), where P(E,) is the source model and P(J�IE,) is the noisy channel.",
        "We train the channel model as described below, and then reverse it to handle the romaji input."
      ]
    },
    {
      "heading": "4.1 Grapheme-based model (GM)",
      "text": [
        "In this model the English word is directly rewritten as a Japanese romaji string with the probability Pg(J�IE,).",
        "Here, we follow (Brill et al., 2001) to arbitrarily break up the E, string into n parts and output each part independently.",
        "Thus, the resulting probability of outputting J, can be rewritten as in the equation (5).",
        "In these equation C is the occurrence count, N is the count of word types and T is the total token count.",
        "ki is the length of word w,j, ali[j] is the jth letter, r is the relative frequency and L is the average word length.",
        "As can be seen, penalty p is introduced for discouraging segmentations introducing many short, novel words (Equation 4).",
        "The best segmentation is calculated using Dynamic Programming.",
        "Once the best segmentation is determined, the counts are updated as necessary.",
        "The penalty for shorter novel words is what distinguishes this We implement Pg(J �IE,) as a weighted Finite State Transducer (WFST) with E,z as inputs, J,,,z as outputs (Pereira and Riley, 1997; Knight and Graehl, 1998) and transition costs as negative logs of probabilities.",
        "This WFST is then reversed and composed with the source model WFST.S When the source model P(E,) is compiled into a WFST, a null input, word delimiter output transition is added, allowing for multiple words to be output for a single string input.",
        "Hence phrases can also be handled.",
        "The WFST resulting from the composition of the Pg(J�IE,) and P(E,) WFST composition is searched for k-best transliterations using the k-best path algorithm.",
        "A probability Pg (E,,, I J,,, ) is associated with each path obtained."
      ]
    },
    {
      "heading": "4.2 Phoneme-based model (PM)",
      "text": [
        "In this model the channel is broken up into two stages: a) conversion of the English alphabet into English phonemes with some probability P(EpIE,) and b) conversion of the English phonemes into romaji with some probability P(J �lEp).",
        "Consequently, Pp(J� I E,) can be rewritten as equation (6).",
        "Rather than manipulating these two distributions separately, we compute their composition to obtain a unique probability distribution Pp(J, z E, zJ.",
        "Consequently all English alphabet strings can be rewritten directly into romaji without requiring their conversion into intermediate phoneme representation.",
        "This removes the requirement of having a pronunciation dictionary for the back-transliteration.7"
      ]
    },
    {
      "heading": "4.3 Combining the models",
      "text": [
        "After obtaining the back-transliterations E,phon and E, with the respective probabilities of graph Pp(E,�IJ,) and Pg(E,I J,), we can assign the final score of a transliteration S,(E,I J,) as in equation (7) where y and S are set to maximize the accuracy on the training sets Transliteration with the highest score is selected as the best."
      ]
    },
    {
      "heading": "4.4 Training the models",
      "text": [
        "For the GM, we follow (Brill et al., 2001) closely to extract the character-string mappings.",
        "Ro-maji and English alphabet are first aligned using the non-weighted Levensthein distance.",
        "Then, letter-edits are expanded to include up to N edits to the right and to the left.",
        "For example, for the pair (roo,row) we get: r --� r o – o o – w. For N=1, editsro - -�ro, roo – row, oo – ow are also added to the set.",
        "7However, the pronunciation dictionary is still necessary for the training.",
        "SParameters are trained using Golden Section Search (Press et al., 1992).",
        "We collect a complete set of edits ag --� 3g in the training set and assign the probability to each according to equation (8).",
        "Throughout, we distinguish edits that appear at the beginning or the end of the word or neither.",
        "Given the collection of edits ag --� �g for each input word J, we can generate a WFST which contains all possible ways to rewrite the input string.",
        "For the PM, we follow (Knight and Graehl, 1998) to obtain the optimal romaji to English phoneme alignment.",
        "After the EM algorithm selects the optimal alignment, we proceed to expand the set of individual alignments with N adjacent units as above to obtain a set of possible rewrites a,p --� �ja.",
        "This process is repeated to obtain the set of all possible rewrites of English alphabet into phonemes a,a --� 3,p.",
        "p Each input a,, with all its mappings 3,p is converted into a WFST and composed with a WFST encoding the complete set of mappings a,p --� Oja to obtain the set of all possible rewrites of English alphabet strings ap into romaji strings Op based on the PM."
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": [
        "The proposed system is intended for producing transliterations of katakana strings not found in the system dictionary.",
        "Therefore, we evaluate various aspects of the proposed method on sets of novel katakana strings not in the EDICT dictionary.",
        "The first set consists of 150 katakana words extracted from the EDR Japanese corpus (EDR, 1995).",
        "To examine possible uses of transliteration in CLIR, we conducted a second test using the NTCIR-2 test collection (Kando et al., 2001).",
        "All 78 out-of-vocabulary katakana words from the topic section (i.e. queries) were used.",
        "Note that topics section consists of 49 short documents, thus showing that NTCIR-2 collection of scientific texts has a large number of (novel) katakana words.",
        "First, we evaluate the segmentation module of the system.",
        "We use the two sets and compare our system (SEC) with the unigram segmentation model of Venkataraman (2001) (UNI), and the segmentation obtained by ChaSen (CHA).10",
        "For this evaluation we trained the two statistical models on a complete set of about 13,000 all-katakana strings from the EDICT dictionary.",
        "Each string was considered as one word and the unigram and phoneme models were updated after each repetition.",
        "The segmentation standard was segmented by hand.",
        "The results are shown in Table 1.",
        "In this table, recall is calculated as '„ precision as z and F-measure as 2xpreci6iovzxreca11 Here, N is precision+recall the correct number of words (assigned in hand-segmentation), e is the number of words incorrectly identified, c is the number of words correctly identified and n = c + e is the total number of words identified automatically.",
        "We can see that ChaSen segmentation achieves the lowest score of the three methods because it deems most strings as unknown words and leaves them unsegmented.",
        "UNI method performs better, but it assigns a high number of incorrect word boundaries.",
        "The SEG method is highly effective in encouraging selection of longer novel words by penalizing shorter words.",
        "This reduces the number of segmentation errors and significantly increases all three measures.",
        "Next we evaluate the transliteration module of the system.",
        "We extracted a collection of The ChaSen segmentation depends on the sentence context, hence it might be different for different appearances of the string.",
        "about 6000 words in katakana together with the corresponding English translation from the EDICT dictionary.",
        "This set was expanded, so that for each katakana word containing a long vowel or a geminate consonant, we add one with these removed.",
        "The pronunciations for training the PM were obtained from the CMU pronouncing dictionary.\"",
        "When no pronunciations were available the words were excluded from the training.",
        "The parameters were tuned on a different 700+ word subset of the EDICT dictionary.",
        "In the first experiment we used the complete CMU dictionary word set (around 120,000 words) compiled into a language model with word probabilities reflecting the corpus frequencies from the EDR English corpus (EDR, 1995) and look for transliterations of 150 words in the EDR test set.",
        "The transliterations were considered correct, if they matched the English translation, letter-for-letter, in a non-case-sensitive manner.",
        "Table 2 gives the transliteration results respectively for the Phoneme Model without context (PMO), the proposed segmentation with dictionary lookup (SEG), the Grapheme Model (GM), the Phoneme Model (PM) and the combined model (COMB) and the combinations of the latter three with the SEG model.",
        "When the SEG model is used, the input string is first segmented and then a dictionary lookup is performed on each segment.",
        "When this model is combined with other models, the transliteration is produced for each segment regardless of whether dictionary lookup was successful or not.",
        "This is necessary because dictionary information often does not correspond to the desired transliteration (e.g. AIL \"maruchi\" is translated only as multimedia in EDICT, although it would often be better translated as multi).",
        "The PMO was trained only on the directly aligning edits (N=0), and the PM and GM models used a context of two units to the left and to the right (N=2).",
        "We can see that segmenting the string improves the performance of any model it is combined with, and that the GM and PM models achieve similar results when equivalent context is used.",
        "However, the set of correctly handled entries is different for each one of them, hence their combination increases coverage.",
        "Overall, segmentation with the combined grapheme and phoneme models results in the best transliteration accuracy.",
        "Among others, this model successfully handles phrases with abbreviations (e.g. 1i/'� L:f T� L '✓ :9 – \"rehab iri sent aa\" is correctly transliterated into rehabilitation center) provided the abbreviated constituent is recorded in the dictionary.",
        "In the second experiment we created a dictionary model from about 110,000 words and their frequencies as counted in the English part of the NTCIR-2 collection and produced transliterations of the 78 novel katakana words.",
        "We were also interested in evaluating the influence of segmentation accuracy on the transliteration so we give results for two additional segmentation methods: segmentation by ChaSen (CHA) and Venkataraman's unigram model (UNI).",
        "The results are given in Table 3.",
        "We can see that any segmentation method improves the overall performance, but the largest increase in accuracy is achieved by using the proposed segmentation.",
        "Note that the GM + SEG model has best top1 accuracy for this data set, but the best top 10 accuracy is still achieved by COMB + SEG model.",
        "This is due to a high number of scientific terms whose transliteration better reflects original spelling than pronunciation (e.g. �9' T� T� � �\"gurikokarikkusu\" glycocalyx) that are pushed lower in the result set when the transliteration scores are interpolated.",
        "Still, the need to consider both spelling and pronunciation simultaneously is reinforced, since which of the two transliteration models (i.e. PM or GM) performs better depends on the input string."
      ]
    },
    {
      "heading": "5.1 Discussion",
      "text": [
        "Brill et al.",
        "(2001) provide no direct evaluation of their transliteration system.",
        "Instead, they evaluate the ability of their system to extract English-katakana pairs from non-aligned web query logs.",
        "Furthermore, no mention is made of handling English phrases.",
        "On the other hand, Knight and Graehl (1998) give only the accuracy for transliteration of personal names (64% correct, 12% phonetically equivalent), but not for general out-of-vocabulary terms.",
        "Their system does handle English phrases, but uses no context information.",
        "Also, it uses pronunciation only of the most frequent words.",
        "Furthermore, there is no common test set for evaluation of back-transliteration.",
        "All this makes comparison with our system difficult.",
        "Nonetheless, our GM corresponds to Brill's system, augmented with phrase handling ability.",
        "On the other hand, the PMO is similar to Knight's system, except that we do not model romaji to katakana ambiguities.",
        "Also, we combine mappings from English spelling to pronunciation and pronunciation to romaji into a single WFST rather than handling them separately in the transliteration process.",
        "12 The results obtained show the advantages of the proposed approach and there is no reason to believe that improvements in any of the individual modules would not be reflected in the overall accuracy of the combined system.",
        "Based on our observations of the weaknesses of previous systems we were able to make small improvements to each one of the modules.",
        "The resulting system, combining all these modules achieved significant overall improvement.",
        "This shows that, rather than trying to develop a highly complex model able to handle all aspects of the problem at hand, sometimes it is effective enough to combine the strengths of smaller simpler models in order to improve the system's overall performance.",
        "In the future, we would like to explore in more depth the effect of the improved transliteration on CLIR systems.",
        "We would also like to study the benefits of using context available in CLIR queries on the transliteration accuracy."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "Back transliteration is the process of converting transliterated words back into their original form.",
        "Previous models used either only phoneme or grapheme-based information contained in the transliteration.",
        "Furthermore, no segmentation of input strings is performed.",
        "In this paper we show that the performance of back-transliteration can be significantly improved by augmentations of the phoneme-based, grapheme-based transliteration and the segmentation modules and their combination into a hybrid system.",
        "The system evaluation on two sets of transliterated strings, not contained in the system dictionary, shows significant increase in accuracy over singleton models."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank Timothy Baldwin, Tai-ichi Hashimoto, Takenobu Tokunaga, Michael Zock and two anonymous reviewers on valuable comments and help in writing this paper."
      ]
    }
  ]
}
