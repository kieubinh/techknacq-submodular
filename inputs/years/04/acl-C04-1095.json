{
  "info": {
    "authors": [
      "Helmut Schmid",
      "Michaela Atterer"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1095",
    "title": "New Statistical Methods for Phrase Break Prediction",
    "url": "https://aclweb.org/anthology/C04-1095",
    "year": 2004
  },
  "references": [
    "acl-A00-1031",
    "acl-J90-3003",
    "acl-J94-1002",
    "acl-P03-1062",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The paper presents two methods for the prediction of phrase breaks.",
        "The first method uses a standard HMM part-of-speech tagger with variable context length.",
        "The second method directly encodes the distance from the last phrase break in its states.",
        "It combines the probability of a phrase break given the distance from the last phrase break with the probability of a break given the local context consisting of the surrounding words and part of speech tags.",
        "The accuracy of the new tagger is 2 percentage points higher than that of Taylor and Black (1998) on similar data."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The insertion of phrase breaks is an important step on the way from raw text to synthesized speech.",
        "Phrase breaks induce prosodic structure in sentences, thus making them more naturally-sounding and intelligible.",
        "Furthermore, phrase break information is often used by other modules like accent prediction (Hirschberg, 1993, Ross and Ostendorf, 1996) or segment duration assignment (van Santen, 1994).",
        "Correct phrase break prediction is crucial for the quality of synthesized speech because a break at a wrong location can make a whole paragraph unintelligible, whereas errors in other modules (e.g. the duration module) only show local effects.",
        "Usage of break information by other modules amplifies the negative effects of break errors.",
        "Various methods for assigning phrase breaks have been proposed, among them manually developed rules (Bachenko and Fitzpatrick, 1990), decision tree models (Wang and Hirschberg, 1992; Koehn et al., 2000), transformational rule-based learning (Fordyce and Ostendorf, 1998), memory-based learning (Marsi et al., 2003) and Hidden Markov Models (Black and Taylor, 1997, Taylor and Black, 1998).",
        "Most of the literature on prosodic phrasing agrees that, next to syntactic features, the length of the prosodic phrases plays an important role (Nespor and Vogel, 1986; Bachenko and Fitzpatrick, 1990; Ostendorf and Veilleux, 1994).",
        "Prosodic phrases tend to be balanced, such that very short and very long phrases are less likely than phrases of intermediate length.",
        "The probability of a break therefore depends to some extent on the distance from the last break.",
        "In other words, it depends on whether there was a phrase break after the preceding word or after the last but one word and so on.",
        "Modelling such sequences of hidden events is a typical task for Hidden Markov models (HMMs).",
        "In this paper, we discuss earlier work on HMM-based phrase break prediction, describe the implementation of a phrase break tagger by means of a part-of-speech tagger, and propose a new method which directly represents the distance from the last phrase break in the state, and uses more local information (namely words in addition to POS tags).",
        "2 Hidden Markov Models HMM-based approaches transform phrase break prediction into a tagging task.",
        "Each word is either annotated with a break (B) or a no-break (N) tag, indicating whether a break should be placed after the word or not.",
        "This task is similar to part-of-speech tagging.",
        "Statistical POS taggers compute the most likely POS tag sequence ti for a given word sequence w': wn tn) ti = arg max p(t1 �wl) =arg max p 1' 1 ti ti p('wi ) arg max p(wi ti ) tl p(wi) is a constant in the maximisation (unless tokenisation is ambiguous) and therefore ignored.",
        "According to the definition of conditional probabilities, p(w1 ti) is decomposed as follows:",
        "Assuming that the next tag ti only depends on the k preceding tags ti-1 and that the next word – wi depends only on its tag ti, a Hidden Markov model is obtained:",
        "This is the well-known formula for HMM-based part-of-speech tagging.",
        "The best POS sequence for a given word sequence is efficiently computed by the Viterbi algorithm."
      ]
    },
    {
      "heading": "2.1 Model of Taylor and Black",
      "text": [
        "Taylor and Black (1998) describe an HMM tagger which assigns phrase break tags to part-of-speech sequences.",
        "The POS tag ti in Equation 1 is replaced by a phrase break tag and the output word wi is replaced by the POS sequence Ci = ci – M+L (the M tags before and the L – M tags after the potential phrase break, where L is the length of the POS sequence).",
        "To deal with sparse data problems, the output probabilities p(ci-1, ci, ci+1Iti) are smoothed by (i) discounting frequencies with Good-Turing estimates and (ii) a back-off strategy which replaces p(ci-1, ci, ci+1Iti) with p(ci, ci+1 Iti) if the smoothed frequency fGT(ci, ci+1i ti) is below 3.",
        "Taylor and Black (1998) evaluated their tagger on a part of the MARSEC corpus (Knowles et al., 1996; Roach et al., 1994) which they divided into a training corpus (comprising 31,707 words and 6,346 breaks), and a test corpus (7,662 words and 1,404 breaks).",
        "They report a tagging accuracy of 91.6 %.",
        "From the accuracy and the figures for correct breaks, correct junctures and inserted junctures, it is possible to derive an f-score of 75.62 % for the prediction of phrase breaks.",
        "From a theoretical point of view, the model of Taylor and Black (1998) is problematic because",
        "(i) the assumption that the next output symbol Ci only depends on the phrase break tag ti, is violated due to the overlap of the POS sequence Ci with the previous sequence Ci-1, and (ii) the back-off smoothing strategy is incorrect because the bigram probability p(ci, ci+1 Iti) is usually much higher than p(ci – i, ci, ci+1 Iti) and never smaller.",
        "So, replacing the probability of a POS trigram with the probability of a bigram overestimates the probability."
      ]
    },
    {
      "heading": "2.2 Using a POS tagger for phrase",
      "text": [
        "break prediction The similarity of the model of Taylor and Black (1998) to POS tagging models suggests that standard POS taggers could be used for phrase break prediction.",
        "From the many POS taggers available (e.g. Brants, 2000; Ratnaparkhi, 1996), we chose the TreeTagger (Schmid, 1994) because it is based on HMMs and allows larger contexts than trigrams.",
        "The smoothing problem in Taylor and Black (1998) is solved by applying Bayes law to p(Cilti) and exploiting the fact that p(Ci)",
        "strategy which replaces p(ti Ici-1, ci, ci+1) with P(tilci,ci+1) (and potentially with p(tilci+l)), if f (ci-1, ci, ci+1) is 1 or less.",
        "Whether the tagger backs off or not depends only on the POS tags and not on the predicted phrase-break tag.",
        "A backoff factor is therefore not necessary.",
        "The backoff strategy was implemented by means of the hyphenation heuristic of the TreeTagger: The lexical probabilities of an unknown input token are replaced by the lexical probabilities of the largest suffix starting after a hyphen.",
        "If VBD-NN is not in the lexicon, but NN is, the phrase break probabilities of NN are used.",
        "The number of preceding break/no-break tags on which the transition probabilities depend (i.e. the order of the HMM) is variable in this tagging approach.",
        "The input consisted of part-of-speech bigrams.",
        "Because syllables are often assumed to be a better measure of phrase length than orthographic words, we experimented with word-based and syllable-based input representations as illustrated in Figures 1 and 2.",
        "In experiments, this tagger achieved a 2 % gain in f-score compared to Taylor and Black (1998) on similar data (see Sec. 4)."
      ]
    },
    {
      "heading": "3 The New Tagger",
      "text": [
        "Tagging with HMMs of high order is slow because of the large number of states and leads to data sparseness problems.",
        "The number of states and parameters decreases if the most relevant information provided by the preceding phrase break tags, namely the distance from the last",
        "for the TreeTagger.",
        "(The first column was not used for training and testing.)",
        "phrase break, is directly encoded in the state.",
        "The distance is either measured by the number of words, or by the number of syllables.",
        "Adding more information to the local context Ci (e.g. the words around the current tagging position) could improve the accuracy.",
        "These considerations led to the development of a new statistical phrase break tagger.",
        "Its states encode the distance from the last phrase break.",
        "They are numbered 0, 1, ... , D where D is the maximal distance considered.",
        "The \"output\" symbols are tuples consisting e.g. of the two preceding words and POS tags and the following word and POS tag.",
        "The new phrase break tagger computes",
        "where Ci = w-1 1 , i1 t-1 , and d = 1w z+1 z+1 (length of word wi+1) if bi = B (break after wi), and di+1 = di + Jwi+1I if bi = N (no break after wi).",
        "p(Bld) is the probability of a phrase break d syllables (or words) after the previous phrase break, and p(Nld) = 1 – p(Bld) is the probability that no phrase break occurs.",
        "The probability distribution p(.Id) is the same for all distances d > D. The Viterbi probability bid (i.e. the probability of the best phrase break sequence starting at the beginning of the text and ending at position i in state d) is computed as follows",
        "The approach can easily be extended for tagging with more than one type of phrase break tags."
      ]
    },
    {
      "heading": "✡.1 Theoretical Backgro☛nd",
      "text": [
        "A statistical phrase break predictor should compute the most likely phrase break tag sequence bi for a given sequence of words wi which is tagged with part-of-speech tags Ci .",
        "bi = arg max p(bi �wi ci ) bi The probability of the phrase break tag sequence is decomposed into a product of conditional probabilities.",
        "The distance di from the last phrase break tag is a function of b'_1 and wi .",
        "Assuming that the phrase break tag bi only depends on di and the local context Ci consisting of the p preceding and the f following words and part-of-speech tags, the following equation results: n n b' – ') i+✌, i+✌ p(bi l wl Cl b1 ) – p(bi l wi – ✎ Ci – ✎ di) ✑ ✒ ✓ ✕ ✖ ✁ The conditional probability p(bjC, d) is transformed as follows:",
        "the statistical dependence between C and d is identical to the statistical dependence between C and d given b. Computation of this factor on real data indeed often returned values close to 1, but values as high as 3 or as low as 0.2 occurred, as well.",
        "In order to simplify the model and to avoid sparse data problems caused by the huge number of these factors, we neglect them.",
        "This step is also motivated by the similarity of the resulting formula to the POS tagging formula (Eq.",
        "2).",
        "The best sequence of phrase break tags is obtained by the maximisation in Equation 3, which is just the computation performed by the proposed tagger."
      ]
    },
    {
      "heading": "3.2 Syllable Counts",
      "text": [
        "Measuring the phrase length by the number of syllables requires a counting method for syllables.",
        "We approximate the number of syllables with the number of vowels and diphtongs if a word contains vowels, and with the number of characters otherwise.",
        "This simple heuristic produced results virtually as good as those obtained with a sophisticated lexicon-based method."
      ]
    },
    {
      "heading": "3.3 Parameter Estimation",
      "text": [
        "The transition probabilities p(bl d) are directly estimated with relative frequency estimates for distances up to D, where D + 1 is the first value for which the relative frequency estimate is undefined (i.e. the smallest unobserved phrase length).",
        "Smoothing turned out to be unnecessary for these parameters.",
        "The \"lexical' probabilities p(bl ci) are smoothed by adding the weighted backoff probabilities Q p(bl c2) to the frequency counts f (b, cl) (see Eq.",
        "4).",
        "The backoff probabilities have been smoothed in the same way.",
        "The elements of the local context ci are ordered according to increasing relevance (cf. Table 2).",
        "The optimal order was initially guessed and later confirmed in experiments.",
        "The phrase break tagging formula of Equation 3 assigns equal weight to the contextual probabilities p(bld) and the local probabilities p(blC) although the local probabilities seem more important.",
        "In order to apply a weighting to the different factors of our model, we turned it into a log-linear model by means of the equation:",
        "where ai = logp(bil di) + logp(bil Ci) – logp(bi) Multiplying the logarithms of the probabilities with A weights and renormalising the scores by a constant Z, a log-linear model is obtained.",
        "where"
      ]
    },
    {
      "heading": "4 Evaluation",
      "text": [
        "The phrase break predictors were evaluated on the English MARSEC corpus with 52,000 words and two German corpora, a 7,400-word Radio News corpus (RNC) and a 90,000 word newspaper corpus (NPC).",
        "The MARSEC is recordings from the BBC transcribed by two experts on the basis of an auditory analysis (Knowles et al., 1996).",
        "The RNC is recorded radio news from 1995 which was manually annotated with prosodic labels (cf. Mayer, 1995, Rapp, 1998).",
        "The NPC is a subcorpus of the Negra treebank (Skut et al., 1998), which was manually annotated with phrase breaks according to the method described in (Hirschberg and Pri-eto, 1996).",
        "Average phrase lengths were 5.02 (MARSEC), 4.93 (NPC) and 4.53 (RNC) words.",
        "The standard deviation was 2.60 (MARSEC), 2.54 (NPC) and 2.12 (RNC).",
        "We calculated recall (percentage of breaks in the corpus which were predicted), precision (percentage of correctly predicted breaks) and the f-score as 2",
        "The pruning parameter of the TreeTagger was optimized on the test data for each context length.",
        "In the plots, the labelling of the x-axis refers to the syllable-based model.",
        "The values for the word-based model are plotted in the ratio 1:2.",
        "(One word is represented as 2 syllables in the plot.",
        "This approximates the actual word-to-syllable ratio in the corpora.)",
        "Overall, the syllable-based input representation tends to be better than the word-based input representation.",
        "Table 1 compares the TreeTagger results with those obtained with the new statistical phrase break tagger.",
        "The evaluation is based on tenfold cross validation.",
        "The TreeTagger results were obtained with optimal parameter settings, whereas the smoothing parameter of the new tagger was optimised with ninefold cross-validation on the training data inside the tenfold cross validation loop.",
        "Nevertheless, the new",
        "tagger is better on all corpora.",
        "Table 1 also shows baseline results and an upper limit for comparison.",
        "The baseline was obtained by placing phrase breaks at punctuation positions'.",
        "In order to determine an upper limit, we measured how well multiple pronunciations of the same text agreed in the placement of phrase breaks.",
        "The scores were computed in the same way as with automatically tagged data.",
        "Repeated pronunciations were only available for part of the RNC data and there is some uncertainty in the upper limit scores due to the small size of the data (2807 tokens overall).",
        "The MARSEC results were obtained with a version of the corpus that was automatically tagged with POS tags using the TreeTagger2.",
        "'We considered periods, question and exclamation marks, commas, colons, semicolons, parentheses and quotation marks as punctuation.",
        "Other results reported later were obtained with the original POS tags of the MARSEC corpus.",
        "A comparison with the results of (Taylor and Black, 1998) (f-score 75.62 %) is difficult because it is not clear which part of the MARSEC corpus they used and how it was divided into test and training data.",
        "Figure 6 shows how the f-score depends on the smoothing parameter Q.",
        "For Q > 4, the variance of the f-score was small for the MARSEC and the NPC corpus, whereas the smaller RNC corpus shows a higher variance.",
        "for MARSEC and 0.1 % for NPC) with a A2 value of 1.3 (with Al and A3 set to 1).",
        "In all other experiments, the A values were 1.",
        "Finally, we investigated on the NPC corpus how much additional local context contributed to the results.",
        "Table 2 shows precision, recall and f-score values for different contexts.",
        "Adding the following word w+1 to the POS trigram improved the f-score by 0.6 %.",
        "So, adding words indeed helped the tagger.",
        "The TreeTagger processed about 20000 tokens per second on a Sun Blade 1000 with 750 Mhz CPU.",
        "The new tagger was implemented in Perl and processed about 1000 tokens per second.",
        "We would expect a C implementation to have similar speed as the TreeTagger because they are both based on HMMs of similar complexity.",
        "Figure 7 compares word-based and syllable-based distance measures, showing a small, but quite consistent advantage for the syllable-based measure on the MARSEC and the NPC corpus and mixed results on the RNC corpus.",
        "The results of an experiment with log-linear weights (see Sec. 3.4) are summarised in Figure 8.",
        "We achieved a small f-score gain (0.2 % the Penn treebank with tenfold cross validation."
      ]
    },
    {
      "heading": "5 Summary",
      "text": [
        "We improved the HMM-based phrase break tagging method of Taylor and Black (1998) by using a better smoothing technique, larger N-grams and syllable-based input representations.",
        "Furthermore, we presented a new statistical method for phrase-break prediction which directly encodes the distance from the last phrase break in its state and combines two types of conditional probabilities, namely (i) the probability of the next phrase break tag given the distance from the preceding phrase break and (ii) the probability of the next break tag given the surrounding words and part-of-speech tags.",
        "The accuracy on the MARSEC corpus measured by the f-score is more than 2 percentage points higher than that obtained by Taylor and Black (1998) on the same corpus using an unknown splitting into training and test data.",
        "With a German corpus, we were able to show that the tagging accuracy comes close to the upper limit defined by the agreement between different pronunciations of the same text."
      ]
    }
  ]
}
