{
  "info": {
    "authors": [
      "Xiwu Han",
      "Tiejun Zhao",
      "Haoliang Qi",
      "Hao Yu"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1104",
    "title": "Subcategorization Acquisition and Evaluation for Chinese Verbs",
    "url": "https://aclweb.org/anthology/C04-1104",
    "year": 2004
  },
  "references": [
    "acl-A97-1052",
    "acl-C00-2100",
    "acl-J93-2002",
    "acl-P02-1029",
    "acl-P03-1009",
    "acl-P91-1027",
    "acl-W02-0905"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes the technology and an experiment of subcategorization acquisition for Chinese verbs.",
        "The SCF hypotheses are generated by means of linguistic heuristic information and filtered via statistical methods.",
        "Evaluation on the acquisition of 20 multi-pattern verbs shows that our experiment achieved the similar precision and recall with former researches.",
        "Besides, simple application of the acquired lexicon to a PCFG parser indicates great potentialities of subcategorization information in the fields of NLP."
      ]
    },
    {
      "heading": "Credits",
      "text": []
    },
    {
      "heading": "Introduction",
      "text": [
        "Since (Brent 1991) there have been a considerable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics.",
        "As for the former, subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved, e.g. (Korhonen 2001).",
        "And for auto-acquisition and relevant application, researchers have made great achievements not only in English, e.g. (Briscoe and Carroll 1997), (Korhonen 2003), but also in many other languages, such as Germany (Schulte im Walde 2002), Czech (Sarkar and Zeman 2000), and Portuguese (Gamallo et.",
        "al 2002).",
        "However, relevant theoretical researches on Chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and a few papers on manual acquisition or prescriptive designment of syntactic patterns.",
        "Due to irrelevant initial motivations, syntactic and semantic generalizabilities of the consequent outputs are not in such a harmony that satisfies the description granularity for SCF (Han and Zhao 2004).",
        "The only auto-acquisition work for Chinese SCF made by (Han and Zhao 2004) describes the predefinition of 152 general frames for all verbs in Chinese, but that experiment is not based on real corpus.",
        "After observing and analyzing quantity of subcategory phenomena in real Chinese corpus in the People’s Daily (Jan.~June, 1998), we removed from Han & Zhao’s predefinition 15 SCFs that are actually similar derivants of others, and then with this foundation and linguistic rules from (Zhao 2002) as heuristic information we generated SCF hypotheses from the corpus of People’s Daily (Jan.~June, 1998), and statistically filtered the hypotheses into a Chinese verb SCF lexicon.",
        "As far as we know, this is the first attempt of Chinese SCF auto-acquisition based on real corpus.",
        "In the rest of this paper, the second section describes a comprehensive system that builds verb SCF lexicons from large real corpus, the respective operating principles, and the knowledge coded in our SCF.",
        "The third section analyzed the acquired lexicon with two experiments: one evaluated the acquisition results of 20 verbs with multi syntactic patterns against manual gold standard; the other checked the performance of the lexicon when applied in a PCFG parser.",
        "The forth section compares and contrasts this research with related works done by others.",
        "And at last, Section 5 concludes our present achievements, disadvantages and possible future focuses."
      ]
    },
    {
      "heading": "1 SCF Acquisition",
      "text": []
    },
    {
      "heading": "1.1 The Acquisition Method",
      "text": [
        "There are generally 4 steps in the process of our auto-acquisition experiment.",
        "First, the corpus is processed with a cascaded HMM parser; second, every possible local patterns for verbs are abstracted; and then, the verb patterns are classified into SCF hypotheses according to the predefined set; at last, hypotheses are filtered statistically and the respective frequencies are also recorded.",
        "The actual application program consists of 6 parts as shown in the following paragraphs.",
        "a. Segmenting and tagging: The raw corpus is segmented into words and tagged with POS by the comprehensive segmenting and tagging processor developed by MTLAB of Computer Department in Harbin Institute of Technology.",
        "The advantage of the POS definition is that it describes some subsets of nouns and verbs in Chinese.",
        "b. Parsing: The tagged sentences are parsed with a cascaded HMM parser' , developed by MTLAB of HIT, but only the intermediate parsing results are used.",
        "The training set of the parser is 20,000 sentences in the Chinese Tree Bank2 of (Zhao 2002).",
        "c. Error-driven correction: Some key errors occurring in the former two parts are corrected according to manually obtained error-driven rules, which are generally about words or POS in the corpus.",
        "d. Pattern abstraction: Verbs with largest governing ranges are regarded as predicates, then local patterns, previous phrases and respective syntactic tags are abstracted, and isolated parts are combined, generalized or omitted according to basic phrase rules in (Zhao 2002).",
        "e. Hypothesis generation: Based on linguistic restraining rules, e.g. no more than two NP's occurring in a series and no more than three in one pattern, and no PP TP MP occurring with NP before any predicates (Han and Zhao 2004), the patterns are coordinated and classified into the predefined SCF groups.",
        "In this part, about 5% unclassifiable patterns are removed.",
        "f. Hypothesis filtering: According to the statistical reliability of each type of the SCF hypotheses and the linguistic principle that arguments occur more frequently with predicates than adjuncts do, the hypotheses are filtered by means of statistical methods, in this paper which are binomial hypotheses testing (BHT) and maximum likelihood estimation (MLE).",
        "eralizes BNP and NDE as NP, combines the second NP with isolated part \" the minimum number of SCF tokens for a verb is 30, and the maximum is 20,000.",
        "In order to check the acquisition performance of the used system, we evaluated a part of the lexicon against a manual gold standard.",
        "The testing set includes 20 verbs of multi syntactic patterns, and for each verb there are 503-2,000 SCF tokens with the total number of 18,316 (See Table 2).",
        "Table 3 gives the evaluation results for different filtering methods, including non-filtering 5 , BHT, and MLE with thresholds of 0.001, 0.005, 0.008 and 0.01.",
        "We calculated the type precision and recall by the following expressions as (Korhonen 2001) did:",
        "In here, true positives are correct SCF types proposed by the system, false positives are incorrect SCF types proposed by system, and false negatives are correct SCF types not proposed by the system.",
        "IVerbs IEnglish I Tokens I Verbs I English I Tokens I with maximum likelyhood is then regarded as the final choice.",
        "When two or more hypotheses hold the same likelihood, the one with larger or largest PCFG probability will be chosen.",
        "Table 4 shows the phrase-based and sentence-based evaluation results for the parser without and with SCF heuristic information.",
        "There are three cased included: a) The output is one-best; b) The output is 5-best and the best evaluation result is recorded; c) The 5-best output is checked again for the best syntactic tree by means of SCF information.",
        "The phrased-based evaluation follows the popular method for evaluating a parser, while the sentence-based depends on the intersection of the parsed trees and those in the gold standard.",
        "Since the PCFG parser output at least one syntactic tree for every sentence in our testing corpus, the sentence-based precision and recall are equal to each other.",
        "Table 4 shows that SCF information remarkably improved the performance of the PCFG parser: the phrase-based precision increased by 5.36% and recall by 7.1%, while the sentence-based precision and recall both increased by 8.04%.",
        "However, this doesn’t reach the upper limit of the 5-best.",
        "The possible reasons are: a) the our present SCF lexicon remains to be improved; b) our method of applying SCF information to the parser is too simple, e.g. probabilities of PCFG parsing results haven’t been exploited thoroughly."
      ]
    },
    {
      "heading": "3 Related Works",
      "text": [
        "and for the purpose of crosslingual processing, our research is kept in consistency with SCF conventions as much as possible.",
        "Due to linguistic differences, nevertheless, not all theories, methods or experiences could adapt to Chinese.",
        "Generally, there are four aspects that our research differs from those of other languages.",
        "First, the SCF formalization of most former researches follows the Levin style, in which most SCFs omit NP before predicates, while Chinese SCFs need to depict arguments occurring before verbs.",
        "Second, except (Sarkar and Zeman 2000), most former researches are based on manual SCF predefinition, while our predefined SCF set is statistically acquired (See Han and Zhao 2004).",
        "Third, involved parsers of former researches are mostly better than Chinese parsers to some degree.",
        "Forth, our SCF information also includes 5 syntactic morphemes (See also Section 1.1).",
        "Meanwhile, the basic purpose for Chinese SCF acquisition is also to determine the subcategory features for a verb via its argument distributions and then apply the lexicon to NLP tasks.",
        "Therefore, under similar cases the respective evaluations are comparable.",
        "And Table 5 gives the comparison between our research and the best English results without semantic backoff 8 in (Korhonen 2001).",
        "The comparison shows that our nonfiltering result is better than Korhonen’s, both BHT results are similar, while our MLE result is much worse As far as we know, this is the first attempt to automatically acquire SCF information from real Chinese corpus and the first trial to apply SCF lexicon to a Chinese parser.",
        "Our research draws a lot on related works from international researches, 8 Semantic backoff is a method of generating SCF hypotheses according to the semantic classification of the concerned verb.",
        "Note that this paper doesn’t involve verb meanings for generating hypotheses.",
        "Besides, though the evaluation for English SCF acquisition is the best, it’s not the newest.",
        "For the newest, please refer to (Korhonen 2003), in which the precision is 71.8% and recall is 34.5%.",
        "than Korhonen’s.",
        "That means our hypothesis generator performs well but our filtering method remains to be improved.",
        "According to the analysis of relevant corpus, we found the main cause might be that low frequency SCF types account for 32% in our corpus while those in (Korhonen 2001) sum to nearly 21%.",
        "Further more, (Briscoe and Carroll 1997) applied their acquired English SCF lexicon to an intermediate parser, and reported a 7% improvement of both phrase-based precision and recall.",
        "Our application of SCF lexicon to a PCFG parser leads to 5.36% improvement for phrase-based precision, 7.1% for recall, and 8.04% for sentence-based precision and recall."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "This paper for the first time describes a largescale experiment of automatically acquiring SCF lexicon from real Chinese corpus.",
        "Perfor mance evaluation shows that our technology and acquiring program have achieved similar performance compared with former researches of other languages.",
        "And the application of the acquired lexicon to a PCFG parser indicates great potentialities of SCF information in the field of NLP.",
        "However, there is still a large gap between Chinese subcategorization works and those of other languages.",
        "Our future work will focus on the optimization of linguistic heuristic information and filtering methods, the application of semantic backoff, and the exploitation of SCF lexicon for other NLP tasks."
      ]
    }
  ]
}
