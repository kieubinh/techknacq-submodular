{
  "info": {
    "authors": [
      "Fumiyo Fukumoto",
      "Yoshimi Suzuki"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1125",
    "title": "Correcting Category Errors in Text Classification",
    "url": "https://aclweb.org/anthology/C04-1125",
    "year": 2004
  },
  "references": [
    "acl-A00-2020",
    "acl-C02-1101",
    "acl-W99-0606"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We address the problem dealing with category annotation errors which deteriorate the overall performance of text classification.",
        "We use two techniques.",
        "The first is support vectors which are extracted from the training samples by a machine learning technique, Support Vector Machines(SVM).",
        "The second is a loss function which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction.",
        "We apply it to the extracted support vectors, and correct annotation errors.",
        "Experimental results with the RWCP and the Reuters 1996 corpora show that our method achieves high precision in detecting and correcting annotation errors.",
        "Further, results on text classification improves accuracy."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "A large number of tagged corpora are widely used in corpus-based NLP and their application systems.",
        "The performance of these systems greatly depends on the quantity and quality of corpora, since they use some statistics or learning algorithms to train a classifier, given a set of tagged examples.",
        "One key difficulty with the use of tagged corpora is that they are error prone, since tagging must often be done by a human.",
        "For large corpora it is difficult to keep consistency even if tagging has been done by several experts, and thus, problematic for corpus-based NLP with high accuracy.",
        "There are at least two strategies for automatically detecting errors in corpora.",
        "One is to use weight which is assigned to each training example by some learning techniques.",
        "Abney et al.",
        "proposed a method to improve data quality by using boosting.",
        "They applied their technique to part-of-speech tagging and prepositional phrase attachment (Abney et al., 1999).",
        "Nakagawa et al.",
        "used SVM to identify part-of-speech annotation errors in corpora (Nakagawa and Matsumoto, 2002).",
        "Both methods utilize the fact that the training examples with larger values of weights are difficult to classify.",
        "Such training example tends to be an outlier, and be an annotation error.",
        "Abney et al.",
        "conducted error detection in the Penn Treebank WSJ corpus by extracting examples with a large weight.",
        "Nakagawa et al.",
        "tested their method using three different real-world datasets: the Penn Treebank WSJ corpus, the RWCP corpus and the Kyoto University Corpus with high precision.",
        "The other is probabilistic approaches.",
        "Es-kin proposed a method for detecting part-of-speech annotation errors in a corpus using an anomaly detection technique(Eskin, 2000).",
        "The technique is the process of determining when an element of data is an outlier.",
        "Eskin used a `mixture model' with two probability distributions: a majority distribution and an anomalous distribution.",
        "For each element, he measured the likelihood of the distribution under both cases.",
        "The element is detected as an error if the likelihood in the anomalous distribution is sufficiently large.",
        "All of these mentioned above perform well, while they have not applied their methods to correcting annotation errors.",
        "This paper proposes a method to detect and correct category annotation errors which deteriorate the overall performance of text classification.",
        "We use two techniques.",
        "The first is support vectors which are extracted from the training samples by SVM(Vapnik, 1995).",
        "Training SVM is to find the optimal hyperplane which consists of support vectors, and only the support vectors affect the performance.",
        "Thus, if some training sample deteriorates the overall performance of text classification because of an outlier, we can assume that the sample is a support vector.",
        "The second is a loss function which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction.",
        "We apply it to the extracted support vectors, and correct annotation errors.",
        "2 Classifiers We use SVM and NB, mainly for the following reasons.",
        "First, we can obtain only the samples in given training data which matter the overall performance with SVM, since training SVM is to find the optimal hyperplane which consists of support vectors.",
        "Second, NB classifier is a calibrated posterior probability to enable post-processing, i.e. correction of annotation errors.",
        "Third, NB is based on the assumption of word independence in a text, which makes the computation of it far more efficient than SVM."
      ]
    },
    {
      "heading": "2.1 SVM",
      "text": [
        "SVM is introduced by Vapnik(Vapnik, 1995) for solving two-class pattern recognition problems.",
        "Given training samples L = {(x1,81), (x2,82), • • •, (xl,gl)}, (xi E R', yi E {+1, – 1}), SVM finds a hyperplane that best separates a set of positive examples from a set of negative examples.",
        "The optimal hyperplane to separate them is found by solving the following optimization problem:",
        "where C is a parameter that controls the training errors, and becomes upper bound of ai.",
        "Support vectors are those training examples xi with ai > 0 at the solution.",
        "It is known that we can obtain the same decision function even if we remove all training examples except for support vectors.",
        "We use a weight ai to extract outliers.",
        "SVM is basically introduced for solving binary classification, while text classification is a multi-class, multi-label classification problem.",
        "Several methods which were intended for multi-class, multi-label data have been proposed(Weston and Watkins, 1998).",
        "We use One-against-the-Rest version of the SVM model in the work."
      ]
    },
    {
      "heading": "2.2 NB",
      "text": [
        "We use NB for two tasks.",
        "The first task is to assign categories to the extracted training sam-ples(support vectors) with SVM, and extract error candidates from the support vectors.",
        "The second is to calculate the estimated error for each candidate using a loss function.",
        "The basic idea of NB is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document.",
        "There are several versions of the NB classifiers.",
        "Recent studies which is proposed by McCallum et al.",
        "reported high performance over some other commonly used versions of NB on several data collections (McCallum, 1999).",
        "We use the model of NB by McCallum et al.",
        "which is shown in formula (1).",
        "V1 refers to the size of vocabulary, IDIdenotes the number of labeled training documents, and CI shows the number of categories.",
        "I di I denotes document length.",
        "wdik is the word in position h of document di, where the subscript of w, dik indicates an index into the vocabulary.",
        "N(wt, di) denotes the number of times word wt occurs in document di, and P(cj Idi) is defined by P(cj Idi) E {0,1}.",
        "3 Correcting Annotation Errors",
        "Roy et al.",
        "proposed a method of active learning that directly optimizes expected future error by log-loss, using the entropy of the posterior class distribution on a sample of the unlabeled examples(Roy and McCallum, 2001).",
        "We applied their technique to detect and correct category annotation errors.",
        "Figure 1 illustrates an overview of the system design.",
        "It consists of three steps: extracting error candidates, estimating error reduction and correcting annotation errors.",
        "These steps are repeated for each category given a labeled training data."
      ]
    },
    {
      "heading": "3.1 Extracting Error Candidates",
      "text": [
        "Let D* be training data consisting of n samples.",
        "Each sample xi is given a set of label Y, i.e. multiple categories.",
        "x2 E {x1 x2 xTnI be support vectors with a weight greater than zero, i.e. x2 for being a positive sample of the given label ya E Y by training a set of D*.",
        "We remove En 1xj from the training samples D*.",
        "The resulting sample D** is used for training NB, leading to a classification model.",
        "This classification model is tested on each support vector, xz and assigns a set of label, Y*.",
        "If"
      ]
    },
    {
      "heading": "3.2 Estimating Error Reduction",
      "text": [
        "The optimal active learner is one that asks for labels on the examples that, once incorporated into training, will result in the lowest expected error on the test set.",
        "Let P(y I x) be an unknown conditional distribution over inputs, x, and output classes, y E {yi, y2, • • yn}, and let P(x) be the marginal `input' distribution.",
        "The learner is given a labeled training set, D, and estimates a classification function that, given an input x, produces an estimated output distribution PD(ylx).",
        "The expected error of the learner can be defined as follows:",
        "where L is some loss function that measures the degree of our disappointment in any differences between the true distribution, P(y I x) and the learner's prediction, PD (y x).",
        "A log loss which is defined as follows:",
        "The active learning aims to select x', such that when the sample is given label y' and added to the training set, the learner trained on the resulting set (D + (x', y')) has lower error rate than any other x.",
        "We note that the true output distribution P(ylx) in formula (5) is unknown for each sample x. Roy et al.",
        "used bagging to estimate it.",
        "More precisely, from the training samples D, a different training set is created.",
        "The learner then creates a new classifier from the sample, this procedure is repeated m times, and the final class posterior for an instance is taken to be the unweighted average of the class posteriori for each of the classifiers.",
        "We recall that detecting error is done by determining whether the label of an error candidate x2 is ya (E Y*) or not.",
        "Here, YZ* refers to a set of the resulting category label of xz which is estimated by the NB classifier given an input D** = D* - E�lx .",
        "We then use a loss _ function in formula (5) .",
        "Specifically, P (y I x) denotes the true distribution, and PD+(x� y�) (y x) shows the learner's prediction.",
        "D denotes the training samples D* except for the error candidates El__lx .",
        "Here, l is the number of error candidates.",
        "(x',y') in formula (5) refers to (x*, ya), ya E YZ*.",
        "X is a set of test samples, and X I denotes the number of test samples.",
        "Y shows a set of all categories.",
        "For each ya E Y* if the value of E - in formula (5) is suffi",
        "ciently small, the learner's prediction is close to the true distribution.",
        "Like Roy et al's method, we use bagging to reduce variance of the true output distribution P(ylx)."
      ]
    },
    {
      "heading": "3.3 Correcting Annotation Errors",
      "text": [
        "We use formula (5) for each error candidate x2 in order to determine the label assigned to x2 is either ya, or ya E YZ*.",
        "More formally, let D* be training samples, and xi (1 < i < l) be an error candidate.",
        "For each ya of x*, we calculate EPD+(XZ �a� where D refers to D* - El-1x .",
        "We pick up x2 whose loss value is smaller than a certain threshold value, B.",
        "If the label of the selected x2 is ya, we declare the label annotated by humans, ya an error, and its true label is ya.",
        "Otherwise, the label of x2 is ya."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We tested detecting and correcting performance.",
        "Then, we applied the correction results",
        "to text classification.",
        "We use two corpora: the RWCP and the 1996 Reuters(RCiVI) data.",
        "In the following experiments, we use linear SVM and the upper bound value C is set to 1.",
        "Performance is governed by two parameters, the weight ai assigned by SVM and loss value 0 obtained by formula (5).",
        "We thus conducted experiments for various values of ai and 0."
      ]
    },
    {
      "heading": "4.1 The RWCP Corpus",
      "text": [
        "The RWCP corpus(Toyoura et al., 1996) consists of 30,207 documents taken from the Mainichi Shimbun Newspaper in Japanese published in 1994(Mainichi, 1995).",
        "We use ten categories that appeared most often in the corpus.",
        "We select 18,841 documents, each of which has one category to examine a single label classification problem.",
        "We divide these documents into four sets.",
        "Table 1 illustrates each set.",
        "`Training samples' in Table 1 which consists of three sets denotes the samples for detecting and correcting annotation errors.",
        "More precisely, the first fold is used for detecting and correcting annotation errors, and the second(Dev.",
        "training) and the third folds(Dev.",
        "test) are used to estimate the true output distribution P(y I x)1 This process is repeated three times so that each fold serves as the source of the detection and correction data.",
        "`Test samples' refers to the test samples which are used for text classification.",
        "We obtained a vocabulary of 62,709 unique words after stemming by a part-of-speech tag-ger, Chasen(Matsumoto et al., 1997).",
        "Table 2 shows detecting (correcting) performance.",
        "`Sv.'",
        "denotes the total number of support vectors, and `Ec.'",
        "denotes the total number of extracted error candidates across the three folds.",
        "Precision of detection (correction) is the ratio of correct assignments of detect ion (correction) by the system divided by the total number of the system's assignments of de-tection(correction).",
        "The results are examined by hand whether the detected and corrected errors are true errors or not.",
        "The evaluation is made by two humans.",
        "The classification is determined to be correct if two human judges agree.",
        "Precision of Table 2 shows the global accuracy across the three folds.",
        "In Table 2, for example, 0.3 of ai value refers to 0.3 < ai < 1, and 0.20 of 0 stands for 0 < 0.20.",
        "For each 'For bagging, we split Dev.",
        "training samples into 5 sets, and create a new classifier from each set.",
        "This procedure is repeated 10 times.",
        "value of a27 we tested different threshold values, 0(0 < 0 < 0.5)2.",
        "Each value of 0 shows the best result among them.",
        "The best precision score for detection was 0.820, and correction was 0.760.",
        "We expect that no errors are detected by repeating corpus error correction and manual correction of the mislabeled samples by the system.",
        "Figure 2 illustrates the result with 0.1 < ai < 1 and 0 < 0.14.",
        "`Corrected by the system' denotes the number of samples which are corrected by the system.",
        "`Corrected by a human' refers to the number of samples which are manual correction of the mislabeled samples by the system.",
        "We can see that the number of corrected annotation errors decreases rapidly, and no errors are detected in the ninth round.",
        "We applied the best result of correcting annotation errors, i.e. 0 < ai < 1, and 0 < 0.19 to text classification.",
        "`NB' and `SVM' in Table 3 denotes the text classification result obtained by NB, and SVM classifiers, respectively.",
        "The result of NB shows that we use k-per-doc strategy, where k = 1(Field, 1975).",
        "`Baseline' refers to the result using all the training samples, i.e. the first three folds.",
        "`Detecting and Correcting' shows the result of our method.",
        "The overall F values obtained by our method with NB was 2.3% better than the baseline, and that of SVM was 1.7% better than the baseline.",
        "Both results are statistically significant using a micro sign test, P-value < 0.01(Yang and Liu, 1999)."
      ]
    },
    {
      "heading": "4.2 The Reuters 1996 Corpus",
      "text": [
        "The 1996 Reuters (Reuters, 2000) corpus consists of 806,791 documents from 20th Aug. 1996 2 W set the upper value of 0 to 0.5, since the smaller value of 0 is, the more the learner's prediction is close to the true distribution.",
        "to 19th Aug. 1997.",
        "These documents are organized into 126 categories with a four level hierarchy.",
        "The number of categories in each level is 25 top, 33 second, 43 third, and 1 fourth level, respectively.",
        "After eliminating unlabeled documents, we divide these documents into four sets.",
        "Table 4 illustrates each set.",
        "We use 102 categories which have at least one document in each set.",
        "We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-of-speech tagger(Schmid, 1995), and stop word removal.",
        "The number of categories per document is 3.21 on average.",
        "The results are shown in Table 5.",
        "Like the RWCP corpus, for each value of a27 we tested different threshold values, 0(0 < 0 < 0.5).",
        "Each value in Table 5 shows the best results among them.",
        "The best precision for detection was 0.819 and for correction was 0.754.",
        "The results are similar to the result using the RWCP corpus, as the best performance of detection was 0.820, and that of correction was 0.760.",
        "This shows that the method works well even for multi-label data.",
        "In the 1996 Reuters corpus, 2,538 out of 150,000 training samples were error-prone(1,69%).",
        "This ratio indicates that nearly 14,000 samples with error-prone would be included in one year corpus, 806,791 samples.",
        "Detecting these annotation errors by humans is time-consuming.",
        "Our approach detects errors with a high precision (0.819).",
        "Thus, even if we annotate only these samples, we can avoid costly human intervention.",
        "Table 6 illustrates the detecting and correcting samples.",
        "In the examples 3, 5, and 6, our method mislabeled 'Corporate (CCAT)' to these samples.",
        "519 out of 2,862 samples are incorrectly detected, and 705 out of 2,862 are mislabeled with 0 < ai < 1 and 0 < 0.06.",
        "Of these, 70-82% of the samples were labeled `CCAT' incorrectly.",
        "The result is quite reasonable because `CCAT' is assigned to the almost half of the training samples, and thus the category is very general and related to other categories.",
        "Yang has shown that the word-category association measures such as a x2 statistic and information gain criterion are effective for discriminating among categories(Yang and Liu, 1999).",
        "This is definitely worth trying with our method.",
        "Table 7 shows the results obtained by using the best result of correction.",
        "The result of NB shows that we use k-per-doc, where k = 3.",
        "Ta",
        "ble 7 indicates that the result of NB improves 1.7% and that of SVM is 1.1%.",
        "Both results are significantly different than the baseline using a micro sign test, P-value < 0.01.",
        "Performance varies widely across categories.",
        "Table 8 illustrates three categories with the best improvement and the worst drop of F scores obtained by SVM classifiers.",
        "The most significant category was `Ownership changes', and the F score of our method was 6.4% better than the baseline.",
        "Table 8 shows that the accuracy drops when the depth from the top node is large, as the third level categories such as `C152' and `E512' belong to `the drop of F' class.",
        "It might be useful to use category hierarchies, i.e. we employ a hierarchy by learning separate classifiers at each internal node of the tree, and then detecting errors to greedily select sub-branches until a leaf is reached(Dumais and Chen, 2000).",
        "The running cost of SVM depends on the number of features and categories.",
        "Training time for 50,000 samples(102 categories) was more than 6 days using a standard 3.4 GHz Pen",
        "tium IV PC with 2 GB of RAM.",
        "Efficiency can be improved if we can reduce the number of features without sacrificing accuracy."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "The research described in this paper explores the correction of category annotation errors in corpora, based on integrating information from two different classification algorithms: NB and SVM.",
        "We found small advantages in the F score for text classification using the RWCP and the 1996 Reuters corpora, compared with a baseline.",
        "Future work includes feature reduction and investigation of other learning techniques to obtain further advantages in efficiency in the manipulating large corpora(Zhang et al., 2003)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors would like to thank anonymous reviewers for their valuable comments.",
        "This work was supported by the Grant-in-aid for the JSPS,"
      ]
    }
  ]
}
