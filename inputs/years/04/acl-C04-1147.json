{
  "info": {
    "authors": [
      "Egidio L. Terra",
      "Charles L. A. Clarke"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1147",
    "title": "Fast Computation of Lexical Affinity Models",
    "url": "https://aclweb.org/anthology/C04-1147",
    "year": 2004
  },
  "references": [
    "acl-C02-1007",
    "acl-C02-1033",
    "acl-C02-1065",
    "acl-C02-1125",
    "acl-J03-3005",
    "acl-N03-1032",
    "acl-P97-1048",
    "acl-W03-1011"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a framework for the fast computation of lexical affinity models.",
        "The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms, an independence model, and a parametric affinity model.",
        "In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.",
        "The framework is flexible, allowing fast adaptation to applications and it is scalable.",
        "We apply it in combination with a terabyte corpus to answer natural language tests, achieving encouraging results."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Modeling term co-occurrence is important for many natural language applications, such as topic segmentation (Ferret, 2002), query expansion (Vechtomova et al., 2003), machine translation (Tanaka, 2002), language modeling (Dagan et al., 1999; Yuret, 1998), and term weighting (Hisamitsu and Niwa, 2002).",
        "For these applications, we are interested in terms that co-occur in close proximity more often than expected by chance, for example, {“NEW”,“YORK”}, {“ACCURATE”,“EXACT”} and {“GASOLINE”,“CRUDE”}.",
        "These pairs of terms represent distinct lexical-semantic phenomena, and as consequence the terms have an affinity for each other.",
        "Examples of such affinities include synonyms (Terra and Clarke, 2003), verb similarities (Resnik and Diab, 2000) and word associations (Rapp, 2002).",
        "Ideally, a language model would capture the patterns of co-occurrences representing the affinity between terms.",
        "Unfortunately, statistical models used to capture language characteristics often do not take contextual information into account.",
        "Many models incorporating contextual information use only a select group of content words and the end product is a model for sequences of adjacent words (Rosenfeld, 1996; Beeferman et al., 1997; Niesler and Woodland, 1997).",
        "Practical problems exist when modeling text statistically, since we require a reasonably sized corpus in order to overcome sparseness problems, but at the same time we face the difficulty of scaling our algorithms to larger corpora (Rosenfeld, 2000).",
        "Attempts to scale language models to large corpora, in particular to the Web, have often used general-purpose search engines to generate term statistics (Berger and Miller, 1998; Zhu and Rosenfeld, 2001).",
        "However, many researchers are recognizing the limitations of relying on the statistics provided by commercial search engines (Zhu and Rosenfeld, 2001; Keller and Lapata, 2003).",
        "ACL 2004 features a workshop devoted to the problem of scaling human language technologies to terabyte-scale corpora.",
        "Another approach to capturing lexical affinity is through the use of similarity measures (Lee, 2001; Terra and Clarke, 2003).",
        "Turney (2001) used statistics supplied by the Altavista search engine to compute word similarity measures, solving a set of synonym questions taken from a series of practice exams for TOEFL (Test of English as a Foreign Language).",
        "While demonstrating the value of Web data for this application, that work was limited by the types of queries that the search engine supported.",
        "Terra and Clarke (2003) extended Turney’s work, computing different similarity measures over a local collection of Web data using a custom search system.",
        "By gaining better control over search semantics, they were able to vary the techniques used to estimate term co-occurrence frequencies and achieved improved performance on the same question set in a smaller corpus.",
        "The choice of the term co-occurrence frequency estimates had a bigger impact on the results than the actual choice of similarity measure.",
        "For example, in the case of the pointwise mutual information measure (PMI), values for p(blc) are best estimated by counting the number of times the terms b and c appear together within 10-30 words.",
        "This experience suggests that the empirical distribution of distances between adjacent terms may represent a valuable tool for assessing term affinity.",
        "In this paper, we present an novel algorithm for computing these distributions over large corpora and compare them with the expected distribution under an independence assumption.",
        "In section 2, we present an independence model and a parametric affinity model, used to capture term co-occurrence with support for distance information.",
        "In section 3 we describe our algorithm for computing lexical affinity over large corpora.",
        "Using this algorithm, affinity may be computed between terms consisting of individual words or phrases.",
        "Experiments and examples in the paper were generated by applying this algorithm to a terabyte of Web data.",
        "We discuss practical applications of our framework in section 4, which also provides validation of the approach."
      ]
    },
    {
      "heading": "2 Models for Word Co-occurrence",
      "text": [
        "There are two types of models for the co-occurrence of word pairs: functional models and distance models.",
        "Distance models use only positional information to measure co-occurrence frequency (Beeferman et al., 1997; Yuret, 1998; Rosenfeld, 1996).",
        "A special case of the distance model is the n-gram model, where the only distance allowed between pairs of words in the model is one.",
        "Any pair of word represents a parameter in distance models.",
        "Therefore, these models have to deal with combinatorial explosion problems, especially when longer sequences are considered.",
        "Functional models use the underlying syntactic function of words to measure co-occurrence frequency (Weeds and Weir, 2003; Niesler and Woodland, 1997; Grefenstette, 1993).",
        "The need for parsing affects the scalability of these models.",
        "Note that both distance and functional models rely only on pairs of terms comprised of a single word.",
        "Consider the pair of terms “NEW YORK” and “TERRORISM”, or any pair where one of the two items is itself a collocation.",
        "To best of our knowledge, no model tries to estimate composite terms of form P(a, b1c) or P(a, b1c, d) where a,b,c,d are words in the vocabulary, without regard to the distribution function of P. In this work, we use models based on distance information.",
        "The first is an independence model that is used as baseline to determine the strength of the affinity between a pair of terms.",
        "The second is intended to fit the empirical term distribution, reflecting the actual affinity between the terms.",
        "Notation.",
        "Let G be a random variable with range comprising of all the words in the vocabulary.",
        "Also, let us assume that G has multinomial probability distribution function P.. For any pair of terms b and d, let Ab d be a random variable with the distance distribution for the co-occurrence of terms b and d. Let the probability distribution function of the random variable Ab �d be Po (b, d) and the corresponding cumulative be Co (b, d) ."
      ]
    },
    {
      "heading": "2.1 Independence Model",
      "text": [
        "Let b and d be two terms, with occurrence probabilities P. (b) and P. (d).",
        "The chances, under independence, of the pair b and d co-occurring within a specific distance b, Po (b, dl b) is given by a geometric distribution with parameter p, A – Geo(b; p).",
        "This is straightforward since if b and d are independent then P. (bl d) = P9 (b) and similarly P. (dJ b) _",
        "The estimation of p is obtained using the Maximum Likelihood Estimator for the geometric distribution.",
        "Let f a be the number of co-occurrences with distance b, and n be the sample size:",
        "= 00 n r h We make the assumption that multiple occurrences of b do not increase the chances of seeing d and vice-versa.",
        "This assumption implies a different estimation procedure, since we explicitly discard what Befeerman et al.",
        "and Niesler call self-triggers (Beeferman et al., 1997; Niesler and Woodland, 1997).",
        "We consider only those pairs in which the terms are adjacent, with no intervening occurrences of b or d, although other terms may appear between them Figure 1 shows that the geometric distribution fits well the observed distance of independent words"
      ]
    },
    {
      "heading": "DEMOCRACY and WATERMELON. When a de",
      "text": [
        "pendency exists, the geometric model does not fit the data well, as can be seen in Figure 2.",
        "Since the geometric and exponential distributions represent related idea in discrete/continuous spaces it is expected that both have similar results, especially when p < 1."
      ]
    },
    {
      "heading": "2.2 Affinity Model",
      "text": [
        "The model of affinity follows a exponential-like distribution, as in the independence model.",
        "Other researchers also used exponential models for affin",
        "ity (Beeferman et al., 1997; Niesler and Woodland, 1997).",
        "We use the gamma distribution, the generalized version of the exponential distribution to fit the observed data.",
        "Pairs of terms have a skewed distribution, especially when they have affinity for one another, and the gamma distribution is a good choice to model this phenomenon.",
        "where I (a) is the complete gamma function.",
        "The exponential distribution is a special case with a = 1.",
        "Given a set of co-occurrence pairs, estimates for a and � can be calculated using the Maximum Likelihood Estimators given by:",
        "Figure 2 shows the fit of the gamma distribution to the word pair FRUITS and WATERMELON (a = 0.559947)."
      ]
    },
    {
      "heading": "3 Computing the Empirical Distribution",
      "text": [
        "The independence and affinity models depend on a good approximation to y.",
        "We try to reduce the bias of the estimator by using a large corpus.",
        "Therefore, we want to scan the whole corpus efficiently in order to make this framework usable."
      ]
    },
    {
      "heading": "3.1 Corpus",
      "text": [
        "The corpus used in our experiments comprises a terabyte of Web data crawled from the general web in 2001 (Clarke et al., 2002; Terra and Clarke, 2003).",
        "The crawl was conducted using a breadth-first search from a initial seed set of URLs representing the home page of 2392 universities and other educational organizations.",
        "Pages with duplicate content were eliminated.",
        "Overall, the collection contains 53 billion words and 77 million documents."
      ]
    },
    {
      "heading": "3.2 Computing Affinity",
      "text": [
        "Given two terms, b and d, we wish to determine the affinity between them by efficiently examining all the locations in a large corpus where they co-occur.",
        "We treat the corpus as a sequence of terms",
        "pus.",
        "This sequence is generated by concatenating together all the documents in the collection.",
        "Document boundaries are then ignored.",
        "While we are primarily interested in within-document term affinity, ignoring the boundaries simplifies both the algorithm and the model.",
        "Document information need not be maintained and manipulated by the algorithm, and document length normalization need not be considered.",
        "The order of the documents within the sequence is not of major importance.",
        "If the order is random, then our independence assumption holds when a document boundary is crossed and only the within-document affinity can be measured.",
        "If the order is determined by other factors, for example if Web pages from a single site are grouped together in the sequence, then affinity can be measured across these groups of pages.",
        "We are specifically interested in identifying all the locations where b and d co-occur.",
        "Consider a",
        "particular occurrence of b at position k in the sequence (tk = b).",
        "Assume that the next occurrence of b in the sequence is tv, and that the next occurrence of d is tv (ignoring for now the exceptional case where tk is close to the end of the sequence and is not followed by another b and d).",
        "If w > v, then no b or d occurs between tk and tv, and the interval can be counted for this pair.",
        "Otherwise, if w < v let tv, be the last occurrence of b before tv.",
        "No b or d occurs between tv, and tv, and once again the interval containing the terms can be considered.",
        "Our algorithm efficiently computes all locations in a large term sequence where b and d co-occur with no intervening occurrences of either b or d. Two versions of the algorithm are given, an asymmetric version that treats terms in a specific order, and a symmetric version that allows either term to appear before the other.",
        "The algorithm depends on two access functions r and l that return positions in the term sequence tl, ..., tN.",
        "Both take a term t and a position in the term sequence k as arguments and return results as follows: V ifElty=ts.t.k<v and � tv, = t s.t.",
        "k <v' <v N + 1 otherwise u if El tu = t s.t.",
        "k > u and 7tv,,=ts.t.k>u'>u 0 otherwise Informally, the access function r(t, k) returns the position of the first occurrence of the term t located at or after position k in the term sequence.",
        "If there is no occurrence of t at or after position k, then",
        "the term t located at or before position k in the term sequence.",
        "If there is no occurrence of t at or before position k, then l (t, k) returns 0.",
        "These access functions may be efficiently implemented using variants of the standard inverted list data structure.",
        "A very simple approach, suitable for a small corpus, stores all index information in memory.",
        "For a term t, a binary search over a sorted list of the positions where t occurs computes the result of a call to r(t, k) or l (t, k) in O(log ft) < O(log N) time.",
        "Our own implementation uses a two-level index, split between memory and disk, and implements different strategies depending on the relative frequency of a term in the corpus, minimizing disk traffic and skipping portions of the index where no co-occurrence will be found.",
        "A cache and other data structures maintain information from call to call.",
        "The asymmetric version of the algorithm is given below.",
        "Each iteration of the while loop makes three calls to access functions to generate a co-occurrence pair (u, v), representing the interval in the corpus from to to tv where b and d are the start and end of the interval.",
        "The first call (w �-- r(b, k)) finds the first occurrence of b after k, and the second (v �-- r (d, w + 1)) finds the first occurrence of d after that, skipping any occurrences of d between k and w. The third call (u �-- l(b, v – 1)) essentially indexes “backwards” in the corpus to locate last occurrence of b before v, skipping occurrences of b between w and u.",
        "Since each iteration generates a co-occurrence pair, the time complexity of the algorithm depends on M, the number of such pairs, rather than than number of times b and d appear individually in the corpus.",
        "Including the time required by calls to access functions, the algorithm generates all co-occurrence pairs in O(M log N) time.",
        "The symmetric version of the algorithm is given next.",
        "It generates all locations in the term sequence where b and d co-occur with no intervening occurrences of either b or d, regardless of order.",
        "Its operation is similar to that of the asymmetric version.",
        "To demonstrate the performance of the algorithm, we apply it to the 99 word pairs described in Section 4.2 on the corpus described in Section 3.1, distributed over a 17-node cluster-of-workstations.",
        "The terms in the corpus were indexed without stemming.",
        "Table 1 presents the time required to scan all co-occurrences of given pairs of terms.",
        "We report the time for all hosts to return their results."
      ]
    },
    {
      "heading": "4 Evaluation",
      "text": [
        "We use the empirical and the parametric affinity distributions in two applications.",
        "In both, the independence model is used as a baseline."
      ]
    },
    {
      "heading": "4.1 Log-Likelihood Ratio",
      "text": [
        "The co-occurrence distributions assign probabilities for each pair at every distance.",
        "We can compare point estimations from distributions and how unlikely they are by means of log-likelihood ratio test:",
        "where po and pI are the parameters for Po (b, d) under the empirical distribution and independence models, respectively.",
        "It is also possible to use the cumulative Co instead of Po.",
        "Figure 3 show log-likelihood ratios using the asymmetric empirical distribution and Figure 4 depicts log-likelihood ratio using the symmetric distribution.",
        "A set of fill-in-the-blanks questions taken from GRE general tests were answered using the log-likelihood ratio.",
        "For each question a sentence with one or two blanks along with a set of options .A was given, as shown in Figure 5.",
        "The correct alternative maximizes the likelihood",
        "of the complete sentence S:",
        "where bb,d is distance of b and d in the sentence.",
        "Since only the blanks change from one alternative to another, the remaining pairs are treated as constants and can be ignored for the purpose of ranking:",
        "dES,b#d for every b E A.",
        "It is not necessary to compute the likelihood for all pairs in the whole sentence, instead a cut-off for the maximum distance can be specified.",
        "If the cutoff is two, then the resulting behavior will be similar to a word bigram language model (with different estimates).",
        "An increase in the cut-off has two immediate implications.",
        "First, it will incorporate the surroundings of the word as context.",
        "Second, it causes an undirect effect of smoothing, since we use cumulative probabilities to compute the likelihood.",
        "As with any distance model, this approach has the drawback of allowing constructions that are not syntactically valid.",
        "The tests used are from GRE practice tests extracted from the websites: gre .",
        "org (9 questions), Prince tonReview.",
        "com(11 questions), Syvum .",
        "com (15 questions) and Microedu .",
        "com (28 questions).",
        "Table 2 shows the results for a cutoff of seven words.",
        "Every questions has five options, and thus selecting the answer at random gives an expected score of 20%.",
        "Our framework answers 55% of the questions.",
        "The science of seismology has grown just enough so that the first overly bold theories have been .",
        "a) magnetic... accepted b) predictive ... protected c) fledgling... refuted d) exploratory ... recalled"
      ]
    },
    {
      "heading": "4.2 Skew",
      "text": [
        "Our second evaluation uses the parametric affinity model.",
        "We use the skew of the fitted model to evaluate the degree of affinity of two terms.",
        "We validated our hypothesis that a greater positive skew corresponds to more affinity.",
        "A list of pairs from word association norms and a list of randomly picked pairs are used.",
        "Word association is a common test in psychology (Nelson et al., 2000), and it consists of a person providing an answer to a stimulus word by giving an associated one in response.",
        "The set of words used in the test are called “norms”.",
        "Many word association norms are available in psychology literature, we chose the Minnesota word association norms for our experiments (Jenkings, 1970).",
        "It is composed of 100 stimulus words and the most frequent answer given by 1000 individuals who took the test.",
        "We also use 100 word pairs generated by randomly choosing words from a small dictionary.",
        "The skew in the gamma distribution is y = 2/Vfa_ and table 3 shows the normalized skew for the association and the random pair sets.",
        "Note that the set of 100 random pairs include some non-independent ones.",
        "The value of the skew was then tested on a set of TOEFL synonym questions.",
        "Each question in this synonym test set is composed of one target word and a set of four alternatives.",
        "This TOEFL synonym test set has been used by several other researchers.",
        "It was first used in the context of Latent Semantic Analisys(LSA) (Landauer and Du-mais, 1997), where 64.4% of the questions were answered correctly.",
        "Turney (Turney, 2001) and Terra et al.",
        "(Terra and Clarke, 2003) used different sim",
        "ilarity measures and statistical estimates to answer the questions, achieving 73.75% and 81.25% correct answers respectively.",
        "Jarmasz (Jarmasz and Szpakowicz, 2003) used a thesaurus to compute the distance between the alternatives and the target word, answering 78.75% correctly.",
        "Turney (Turney et al., 2003) trained a system to answer the questions with an approach based on combined components, including a module for LSA, PMI, thesaurus and some heuristics based on the patterns of synonyms.",
        "This combined approach answered 97.50% of the questions correctly after being trained over 351 examples.",
        "With the exception of (Turney et al., 2003), all previous approaches were not exclusively designed for the task of answering TOEFL synonym questions.",
        "In order to estimate a and � we compute the empirical distribution.",
        "This distribution provides us with the right hand side of the equation 4 and we can solve for a numerically.",
        "The calculation of, is then straightforward.",
        "Using only skew, we were able to answer 78.75% of the TOEFL questions correctly.",
        "Since skew represents the degree of asymmetry of the affinity model, this result suggests that skew and synonymy are strongly related.",
        "We also used log-likelihood to solve the TOEFL synonym questions.",
        "For each target-alternative pair, we calculated the log-likelihood for every distance in the range four to 750.",
        "The initial cut-off discarded the affinity caused by phrases containing both target and alternative words.",
        "The upper cut-off of 750 represents the average document size in the collection.",
        "The cumulative log-likelihood was then used as the score for each alternative, and we considered the best alternative the one with higher accumulated log-likelihood.",
        "With this approach, we are able to answer 86.25% of questions correctly, which is a substantial improvement over similar methods, which do not require training data."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We presented a framework for the fast and effective computation of lexical affinity models.",
        "Instead of using arbitrary windows to compute word similarity measures, we model lexical affinity using the complete observed distance distribution along with independence and parametric models for this distribution.",
        "Our results shows that, with minimal effort to adapt the models, we achieve good results by applying this framework to simple natural language tasks, such as TOEFL synonym questions and GRE fill-in-the-blanks tests.",
        "This framework allows the use of terabyte-scale corpora by providing a fast algorithm to extract pairs of co-occurrence for the models, thus enabling the use of more precise estimators."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was made possible also in part by PUC/RS and Ministry of Education of Brazil through CAPES agency."
      ]
    }
  ]
}
