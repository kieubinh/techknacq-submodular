{
  "info": {
    "authors": [
      "Namhee Kwon",
      "Michael Fleischman",
      "Eduard Hovy"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1179",
    "title": "FrameNet-Based Semantic Parsing Using Maximum Entropy Models",
    "url": "https://aclweb.org/anthology/C04-1179",
    "year": 2004
  },
  "references": [
    "acl-E03-1055",
    "acl-J02-3001",
    "acl-J96-1002",
    "acl-N03-2009",
    "acl-P97-1003",
    "acl-W03-1007"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "As part of its description of lexico-semantic predicate frames or conceptual structures, the FrameNet project defines a set of semantic roles specific to the core predicate of a sentence.",
        "Recently, researchers have tried to automatically produce semantic interpretations of sentences using this information.",
        "Building on prior work, we describe a new method to perform such interpretations.",
        "We define sentence segmentation first and show how Maximum Entropy re-ranking helps achieve a level of 76.2% F-score (answer among top-five candidates) or 61.5% (correct answer)."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "To produce a semantic analysis has long been a goal of Computational Linguistics.",
        "To do so, however, requires a representation of the semantics of each predicate.",
        "Since each predicate may have a particular collection of semantic roles (agent, theme, etc.)",
        "the first priority is to build a collection of predicate senses with their associated role frames.",
        "This task is being performed in the FrameNet project based on frame semantics (Fillmore, 1976).",
        "Each frame contains a principal lexical item as the target predicate and associated frame-specific roles, such as offender and buyer, called frame elements.",
        "FrameNet I contains 1,462 distinct predicates (927 verbs, 339 nouns, 175 adjectives) in 49,000 annotated sentences with 99,000 annotated frame elements.",
        "Given these, it would be interesting to attempt an automatic sentence interpretation.",
        "We build semantic parsing based on FrameNet, treating it as a classification problem.",
        "We split the problem into three parts: sentence segmentation, frame element identification for each segment, and semantic role tagging for each frame element.",
        "In this paper, we provide a pipeline framework of these three phases, followed by a step of re-ranking from n-best lists of every phase for the final output.",
        "All classification and re-ranking are performed by Maximum Entropy.",
        "The top-five final outputs provide an F-score of 76.2% for the correct frame element identification and semantic role tagging.",
        "The performance of the single best output is 61.5% F-score.",
        "The rest of the paper is organized as follows: we review related work in Section 2, explain Maximum Entropy in Section 3, describe the detailed method in Section 4, show the re-ranking process in Section 5, and conclude in Section 6."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "The first work using FrameNet for semantic parsing was done by Gildea and Jurafsky (G & J, 2002) using conditional probabilistic models.",
        "They divide the problem into two sub-tasks: frame element identification and frame element classification.",
        "Frame element identification identifies the frame element boundaries in a sentence, and frame element classification classifies each frame element into its appropriate semantic role.",
        "The basic assumption is that the frame element (FE) boundaries match the parse constituents, and both identification and classification are then done for each constituent 1.",
        "In addition to the separate two phase model of frame element identification and role classification, they provide an integrated model that exhibits improved performance.",
        "They define a frame element group (FEG) as a set of frame element roles present in a particular sentence.",
        "By integrating FE identification with role labeling, allowing FEG priors and role labeling decision to affect the determination of next FE identification, they accomplish F-score of 71.9% for FE identification and 62.8% for both of FE identification and role labeling.",
        "However, since this integrated approach has an exponential complexity in the number of constituents, they apply a pruning scheme of using only the top m 1 The final output performance measurement is limited to the number of parse constituents matching the frame element boundaries.",
        "hypotheses on the role for each constituent (m = 10).",
        "Fleischman et al.",
        "(FKH, 2003) extend G & J’s work and achieve better performance in role classification for correct frame element boundaries.",
        "Their work improves accuracy from 78.5% to 84.7%.",
        "The main reasons for improvement are first the use of Maximum Entropy and second the use of sentence-wide features such as Syntactic patterns and previously identified frame element roles.",
        "It is not surprising that there is a dependency between each constituent’s role in a sentence and sentence level features reflecting this dependency improve the performance.",
        "In this paper, we extend our previous work (KFH) by adopting sentence level features even for frame element identification."
      ]
    },
    {
      "heading": "3 Maximum Entropy",
      "text": [
        "ME models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence, but otherwise is as uniform as possible (Berger et al.",
        "1996).",
        "We model the probability of a class c given a vector of features x according to the ME formulation below:",
        "feature function which maps each class and vector element to a binary value, n is the total number of feature functions, and λi is a weight for the feature function.",
        "The final classification is just the class with the highest probability given its feature vector and the model.",
        "It is important to note that the feature functions described here are not equivalent to the subset conditional distributions that are used in G & J’s model.",
        "ME models are log-linear models in which feature functions map specific instances of features and classes to binary values.",
        "Thus, ME is not here being used as another way to find weights for an interpolated model.",
        "Rather, the ME approach provides an overarching framework in which the full distribution of classes (semantic roles) given features can be modeled."
      ]
    },
    {
      "heading": "4 Model",
      "text": [
        "We define the problem into three subsequent processes (see Figure 1): 1) sentence segmentation 2) frame element identification, and 3) semantic role tagging for the identified frame elements.",
        "In order to use sentence-wide features for the FE identification, a sentence should have a single non-overlapping constituent sequence instead of all the independent constituents.",
        "Sentence segmentation is applied before FE identification for this purpose.",
        "For each segment the classification into FE or not is performed in the FE identification phase, and from the FE-tagged constituents the semantic role classification is applied in the role tagging phase."
      ]
    },
    {
      "heading": "Input sentence",
      "text": [
        "He got up, bent briefly over her hand."
      ]
    },
    {
      "heading": "4.1 Sentence Segmentation",
      "text": [
        "The advantages of applying sentence segmentation before FE identification are considered in two ways.",
        "First we can utilize sentence-wide features, and second the number of constituents as FE candidates is reduced, which reduces the convergence time in training.",
        "We segment a sentence with parse constituents2.",
        "During training, we split a sentence into true frame elements and the remainder.",
        "After choosing frame elements as segments, we choose the highest level constituents in parse tree for other parts, and then make a complete sentence composed of a sequence of constituent segments.",
        "During testing, we need to consider all combinations of various level constituents.",
        "We know the given target word should be a separate segment because a target word is not a part of other FEs.",
        "Since most frame elements tend to be among the higher levels of a parse tree, we decide to use the highest constituents while separating the target word.",
        "Figure 2 shows an example of the segmentation for 2 We use Michael Collins’s parser : http://www.cis.upenn.edu/~mcollins/",
        "However, this segmentation for testing reduces the FE coverage of constituents, which means our FE classification performance is limited.",
        "Table 1 shows the FE coverage and the number of constituents for our development set.",
        "The FE coverage of individual constituents (86.36%) means the accuracy of the parser.",
        "This limitation and will be discussed in detail in Section 4.4."
      ]
    },
    {
      "heading": "4.2 Frame Element Identification",
      "text": [
        "Frame element identification is executed for the sequence of segments.",
        "For the example sentence in Figure 2, “(He) (got up) (bent) (briefly) (over her hand)”, there are five segments and each segment has its own feature vector.",
        "Maximum Entropy classification into the classes of FE, Target, or None is conducted for each.",
        "Since the target predicate is given we don’t need to classify a target word into a class, but we do not exclude it from the segments because we want to get benefit of using previous segment’s features.",
        "The initial features are adopted from G & J and FKH, and most features are common to both of frame element identification and semantic role classification.",
        "The features are: the feature that is provided by the user.",
        "Although there can be many predicates in a sentence, only one predicate is defined at a time.",
        "• Target identification (tar): The target",
        "identification is a binary value, indicating whether the given constituent is a target or not.",
        "Because we have a target word in a sequence of segments, we provide this information explicitly.",
        "• Constituent path (path): From the syntactic",
        "parse tree of a sentence, we extract the path from each constituent to the target predicate.",
        "The path is represented by the nodes through which one passes while traveling up the tree from the constituent and then down through the governing category to the target word.",
        "For example, “over her hand” in a sentence of Figure 2 has a path PP↑VP↓VBD.",
        "• Phrase Type (pt): The syntactic phrase type (e.g., NP, PP) of each constituent is also extracted from the parse tree.",
        "• Syntactic Head (head): The syntactic head of",
        "each constituent is obtained based on Michael Collins’s heuristic method3.",
        "When the head is a proper noun, “proper-noun” substitutes for the real head.",
        "The decision if the head is proper noun is done by the part of speech tag in a parse tree.",
        "• Logical Function (lf): The logical functions of constituents in a sentence are simplified into three values: external argument, object argument, other.",
        "We follow the links in the parse tree from the constituent to the ancestors until we meet either S or VP.",
        "If the S is found first, we assign external argument to the constituent, and if the VP is found, we assign object argument.",
        "Otherwise, other is assigned.",
        "Generally, a grammatical function of external argument is a subject, and that of object argument is an object.",
        "This feature is applied only to constituents whose phrase type is NP.",
        "• Position (pos): The position indicates whether a constituent appears before or after the target predicate and whether the constituent has the same parent as the target predicate or not.",
        "• Voice (voice): The voice of a sentence (active, passive) is determined by a simple regular expression over the surface form of the sentence.",
        "• Previous class (c_n): The class information of the nth-previous constituent (target, frame element, or none) is used to exploit the dependency between constituents.",
        "During training, this information is provided by simply • Target predicate (target): The target predicate, the principal word in a sentence, is",
        "looking at the true classes of the frame element occurring n-positions before the current element.",
        "During testing, hypothesized classes of the n elements are used and Viterbi search is performed to find the most probable tag sequence for a sentence.",
        "The combination of these features is used in ME classification as feature sets.",
        "The feature sets are optimized by previous work and trial and error experiments.",
        "Table 2 shows the lists of feature sets for “briefly” in a sentence of “He got up, bent briefly over her hand”.",
        "These feature sets contain the previous or next constituent’s features, for example, pt_-1 represents the previous constituent’s phrase type and lf_1 represents the next constituent’s logical function."
      ]
    },
    {
      "heading": "4.3 Semantic Role Classification",
      "text": [
        "The semantic role classification is executed only for the constituents that are classified into FEs in the previous FE identification phase.",
        "Maximum Entropy classification is performed to classify each FE into classes of semantic roles.",
        "Most features from the frame element identification in Section 4.2 are still used, and two additional features are applied.",
        "The feature sets are in Table 3.",
        "• Order (order): The relative position of a frame element in a sentence is given.",
        "For example, in the sentence from Figure 2, there are three frame elements, and the element “He” has order 0, while “over her hand” has order 2.",
        "• Syntactic pattern (pat): The sentence level syntactic pattern is generated from the parse",
        "tree by looking at the phrase type and logical functions of each frame element in a sentence.",
        "For example, in the sentence from Figure 2, “He” is an external argument Noun Phrase, “bent” is a target predicate, and “over her hand” is an external argument Prepositional Phrase.",
        "Thus, the syntactic pattern associated with the sentence is [NP-ext, target, PP-ext].",
        "Feature Sets f(c, target) f(r, head) f(r, target, p t) f(r, head, target) f(r, target, p t, lf) f(r, head, target, p t) f(r, p t, p os, voice) f(r, order, syn) f(r, p t, p os, voice, target) f(r, target, order, syn) f(r, r_-1) f(r,r_-1,r_-2)"
      ]
    },
    {
      "heading": "4.4 Experiments and Results",
      "text": [
        "Since FrameNet II was published during our research, we continued using FrameNet I (120 semantic role categories).",
        "We can, therefore, compare our results with previous research by matching exactly the same data as used in G & J and FKH.",
        "We thank Dan Gildea for providing the following data set: training (36,993 sentences / 75,548 frame elements), development (4,000 sentences / 8,167 frame elements), and held our test sets (3,865 sentences / 7,899 frame elements).",
        "We train the ME models using the GIS algorithm (Darroch and Ratcliff, 1972) as implemented in the YASMET ME package (Och, 2002).",
        "For testing, we use the YASMET MEtagger (Bender et al.",
        "2003) to perform the Viterbi search for choosing the most probable tag sequence for a sentence using the probabilities from training.",
        "Feature weights are smoothed using Gaussian priors with mean 0 (Chen and Rosenfeld, 1999).",
        "The standard deviation of this distribution and the number of GIS iterations for training are optimized on development set for each experiment.",
        "Table 4 shows the performance for test set.",
        "The evaluation is done for individual frame elements.",
        "To segment a sentence before FE identification or role tagging improves the overall performance (from 57.6% to 60.0% in Table 4).",
        "Since the segmentation reduces the FE coverage of segments, we conduct the experiment with the manually chosen segmentation to see how much the segmentation helps the performance.",
        "Here, we extract segments from the parse tree constituents, so the FE coverage is 86% for test set, which maches the parsing accuracy.",
        "Table 5 shows the performance of the frame element identification for test set: F-score is 77.2% that is much better than 71.7% of our automatic segmentation."
      ]
    },
    {
      "heading": "5 n-best Lists and Re-ranking",
      "text": [
        "As stated, the sentence segmentation improves the performance by using sentence-wide features, but it drops the FE coverage of constituents.",
        "In order to determine a good segmentation for a sentence that does not reduce the FE coverage, we perform another experiment by using re-ranking.",
        "We obtain all possible segmentations for a given sentence, and conduct frame element identification and semantic role classification for all segmentations.",
        "During both phases, we get n-best lists with Viterbi search, and finally choose the best output with re-ranking method.",
        "Figure 3 shows the overall framework of this task."
      ]
    },
    {
      "heading": "5.1 Maximum Entropy Re-ranking",
      "text": [
        "We model the probability of output r given candidates’ feature sets {x1 .. xt} where t is the total number of candidates and xj is a feature set of the jth candidate according to the following ME formulation:",
        "where ZX is a normalization factor, fi(r, {x1..xt}) is a feature function which maps each output and all candidates’ feature sets to a binary value, n is the total number of feature functions, and λi is the weight for a given feature function.",
        "The weight λi is associated with only each feature function while the weight in the ME classifier is associated with all possible classes as well as feature functions.",
        "The final decision is r having the highest probability of p(r|{x1..xt}) from t number of candidates.",
        "As a feature set for each candidate, we use the ME classification probability that is calculated during Viterbi search.",
        "These probabilities are conditional probabilities given feature sets and these feature sets depend on the previous output, for example, semantic role tagging is done for the identified FEs in the previous phase.",
        "For this reason, the product of these conditional probabilities is used as a feature set.",
        "p(r |s) = p(seg |s) * p(fe |seg) * p(r |fe) where s is a given sentence, seg is a segmentation, fe is a frame element identification, and r is the final semantic role tagging.",
        "p(fe|seg) and p(r|fe) are produced from the ME classification but p(seg|s) is computed by a heuristic method and a development set optimization experiment.",
        "The adopted p(seg|s) is composed of p(each segment’s part of speech tag |target’s part of speech tag), p(the number of total segments in a sentence |total number of words in a sentence), and the average of each segment’s p(head word of FE |target).",
        "Two additional feature sets other than p(r|s) are applied to get slight improvement for re-ranking performance, which are average of p(parse tree depth of FE |target) and average of p(head word of FE |target)."
      ]
    },
    {
      "heading": "5.2 Experiments and Results",
      "text": [
        "We apply ME re-ranking in YASMET-ME package.",
        "We train re-ranking model with development set after obtaining candidate lists for the set.",
        "For a simple cross validation, the development set is divided into a sub-training set (3,200 sentences) and a sub-development set (800 sentences) by selecting every fifth sentence.",
        "Training for re-ranking is executed with the sub-training set and optimization is done with the sub-development set.",
        "The final test is applied to test set.",
        "The possible number of segmentations is different depending on sentences, but the average number of segmentation lists is 15.24 for the development set.",
        "For these segmentations, we compute 10-best5 lists for the FE identification and 10-best lists for the semantic role classification.",
        "4 To reduce the number of different segmenations while not dropping the FE coverage, the segmentations having too many segments for a long sentence are excluded.",
        "5 The experiment showed 10-best lists outperformed other n-best lists where n is less than 10.",
        "The bigger number was not tested because of huge number of lists.",
        "of segmentations depending on each sentence, (2) has mn number of lists when we obtain m possible segmentations in (1) and we get n-best FE identifications, (3) has mnn number of lists when we get n-best role classifications given mn lists (4) shows finally chosen output.",
        "Table 6 shows the performance of re-ranking.",
        "To evaluate the performance of top-n, the best tagging output for a sentence is chosen among n-lists and the performance is computed for that list.",
        "The top-5 lists show two interesting points: one is that precision is very high, and the other is that F-score including role tagging is not much different from F-score of only FE identification.",
        "In other words, there are a few (not 120) confusing roles for a given frame element, and we have many frame elements that are not identified even in n-best lists.",
        "To improve our re-ranker, more features regarding these problems should be added, and a more principled method to obtain the probability of segmenations, p(seg) in Sectioin 5.1, needs to be investigated.",
        "Table 7 compares the final output with G & J’s best result.",
        "Our model is slightly worse than their integrated model, but it supports much further experimentation in segmentation and re-ranking."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We describe a pipeline framework to analyze sentences into frame elements and semantic roles based on the FrameNet corpus.",
        "The process includes four steps: sentence segmentation, FE identification, role classification, and final re-ranking of the n-best outputs.",
        "In future work, we will investigate ways to reduce the gap between the five-best output performance and the single best output.",
        "More features should be extracted to improve re-ranking accuracy.",
        "Although the segmentation improves the performance, since the final output is dominated by the initial segmentation, we will explore a smart segmentation method, possibly one not even limited to constituents.",
        "In addition to the provided syntactic features, we will apply semantic features using ontology.",
        "Finally, the challenge is to apply this type of work to new predicates, ones not yet treated in FrameNet.",
        "We are searching for methods to achieve this."
      ]
    }
  ]
}
