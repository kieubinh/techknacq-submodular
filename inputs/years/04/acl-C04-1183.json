{
  "info": {
    "authors": [
      "Olivier Kraif",
      "Boxing Chen"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1183",
    "title": "Combining Clues for Lexical Level Aligning Using the Null Hypothesis Approach",
    "url": "https://aclweb.org/anthology/C04-1183",
    "year": 2004
  },
  "references": [
    "acl-C02-1002",
    "acl-E03-1026",
    "acl-J93-1003",
    "acl-P00-1050",
    "acl-P03-1058",
    "acl-W93-0301"
  ],
  "sections": [
    {
      "text": []
    },
    {
      "heading": "Abstract",
      "text": [
        "Various informations can be used to align parallel texts at word level: co-occurrence frequencies, position difference, part-of-speech, graphic resemblance, etc.",
        "This paper proposes a simple method to combine these clues in an efficient way.",
        "The association score is computed from the probabilities of pairing two units under Null hypothesis, assuming that the association is fortuitous.",
        "This approach has been applied to a literary English-French parallel text with good results."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "From the early 1990's, much interest has been given to the research on bilingual parallel text aligning.",
        "Aligning corpora at lexical level proves to be very useful for many applications such as bilingual Lexicography or Terminography, Statistic Machine Translation, cross language information retrieval (Brown, 2000), Computer Assisted Language Learning (Nerbonne, 2000), or even Word Sense Disambiguation (Ng, 2003).",
        "To verify the latter hypothesis, the CARMEL Project aims at gathering literary texts with translations in 4 languages (French, English, Spanish and Italian), and implementing Word Sense Disambiguation and Thematic Identification methods, taking advantage of the multilingual context of each unit.",
        "We assume than given a text, each translation makes explicit additional information about its semantic and referential content.",
        "After the relatively easy task of sentence aligning, we are now implementing lexical level aligning techniques to establish word correspondences between the 4 languages.",
        "Though considerable progress has been made in this field (Dunning 1993, Dagan et al., 1993, Melamed 1998, Tufis 2002), this task remains difficult.",
        "The 75% accuracy of the best system for the translation spotting task, in the last Arcade campaign (Langlais and V�ronis, 2000), showed that there was room for improvement.",
        "In the latest three years, Jin-Xia et al.",
        "(2000) have investigated a linguistic-knowledge-based word similarity to compute the association score of the word pairs, between Chinese and Korean.",
        "Linguistic knowledge was acquired from linguistic comparison of all layers between two languages: word formation, part-of-speech, lexical internal structure and syntax.",
        "Lopes and Mexia (2001) used GenLocalMax algorithm to extract typical contiguous and non-contiguous sequences of characters as cognates, and then used these cognates to extract the word pairs.",
        "Tiedemann (2003) proposed an algorithm to combine several clues for word aligning.",
        "These clues were probabilities, computed from similarity measure or learned from a word-aligned training corpus.",
        "The method we present is somehow similar, because we also combine various clues to take advantage of all the available indices.",
        "But it does not need any word-aligned training corpus.",
        "In section 2, we describe the principle of the Null hypothesis approach.",
        "Section 3 and 4 are devoted to the experiments and evaluation of the results."
      ]
    },
    {
      "heading": "2 The \"Null hypothesis\" approach",
      "text": []
    },
    {
      "heading": "2.1 Aligning algorithm",
      "text": [
        "The general framework of our aligning method is very simple.",
        "Given two aligned sentences, an association score is computed for every possible pairing between units, then the best pairs are selected iteratively.",
        "Let Cand be the set of candidate pairs, and Sel the set of selected pairs.",
        "At initialization Sel< – 0",
        "0.",
        "• (ui,uj) e Cand compute Score(ui,uj).",
        "1.",
        "Sort Cand elements in descending order of the association score.",
        "2. the best scoring pair (us,u't) is removed from Cand and recorded in Sel : Cand < – Cand⁄ {(us,u't)}.",
        "Sel < – Selu{(us,u't)}.",
        "3.",
        "All the competing candidates are removed.",
        "• (us,uj) e Cand, Cand < – Cand⁄ {(us,uj)}.",
        "• (ui,u't) e Cand, Cand < – Cand⁄ {(ui,u't)}.",
        "4.",
        "Return to 2, until Cand = 0",
        "As demonstrated by Melamed (1998), this algorithm approximately establishes the best scoring set of correspondences under the one-to-one assumption.",
        "Moreover, it allows to reduce the effect of indirect association: when two units are strongly linked on the syntagmatic axis, they tend to be associated with the same unit in the translated part.",
        "Because of the one-to-one assumption, units compete with each other to find an association, and the best scoring ones come before."
      ]
    },
    {
      "heading": "2.2 Null Hypothesis",
      "text": [
        "The results of such a simple algorithm strongly depend on the association score.",
        "As we lack word-aligned training corpus, we cannot easily compute empirical distributions for all the interesting clues, in order to estimate the probability for two units to be translational equivalent.",
        "Thus, we just propose to evaluate the probability for two units to be non-equivalent.",
        "We make the following assumption, namely the Null hypothesis: the co-occurrence of two units that are not translational equivalent is a fortuitous event (i.e. bearing no linguistic determination).",
        "Of course, this assumption does not hold strictly, because it does not take into account the syntagmatic associations between words inside each language.",
        "For instance, between the two following sentences (extracted from Flaubert's Madame Bovary): EN: The night was covered with stars, a warm wind blowing in the distance; the dogs were barking.",
        "FR: le ciel etait couvert d'etoiles, un vent chaud passait, au loin des chiens aboyaient.",
        "it appears that the Null hypothesis is verified at various degrees: the co-occurrence of (stars, aboyaient) is fortuitous (from a linguistic point of view), but not the one of (stars, etoiles).",
        "The case of (stars, ciel) is in-between."
      ]
    },
    {
      "heading": "2.3 Association score computing",
      "text": [
        "The probability to observe k independent clues Cl, C2...Ck under the null hypothesis at the same time",
        "where P�(Ci) is the probability to observe the clue Ci under the null hypothesis.",
        "The smaller this probability, the more unlikely the null hypothesis, and the more probable the assumption that units are mutual translation.",
        "Thus, the association score can be built as:",
        "We chose to use the following clues: word distributions, graphic resemblance, word positions, and word parts-of-speech.",
        "To compute an efficient association score, one needs to focus on features that allow to discriminate between fortuitous and non fortuitous correspondence.",
        "For each clue, the computing of probabilities is designed for the best discrimination:",
        "The first association score (Sd) is based on word co-occurrence.",
        "Given two units (ul,u2), given their frequencies nl and n2, it is possible to estimate the probability that they globally co-occur n12 times among n segment pairs', only by chance.",
        "We computed this probability assuming a binomial distribution.",
        "Without simplification, this probability can be expressed by:",
        "This probability is computed as the result of three independent draws, assuming that each unit occurs only once in the same segment pair: Cn1 is the number of different possible draws n for the nl occurrences of ul.",
        "Cn12 is the number of different possible draws nl for the n12 occurrences of u2 that co-occur with ul.",
        "Cn2 nn�2 is the number of different possible – 1 draws for the n2-n12 occurrences of u2 that don't co-occur with ul.",
        "Cn,Cn2 is the total number of possible draws n n without making any assumption on n�2.",
        "The association score based on cognate (Scog) is the log-probability to observe superficial resemblance between two randomly drawn words inside an aligned segment pair.",
        "The event of cognateness is determined by counting the length of the Longest Common Sub-string (LCS).",
        "Two units are considered as potential cognates if the sub-string exceeds a certain proportion (here, 2I3) of the longest unit.",
        "The probability of cognateness Pcog, between two randomly drawn units has been computed from empirical observations on a automatically sentence aligned corpus The score is expressed by the following equation: if l(LCS)>2I3.max(l(u�,u2)) if l(LCS)<2I3.max(l(u�,u2)) ' we call segment a group of aligned sentences",
        " – ",
        "The association score based on word position (SPosi) is the log-probability to observe a small position difference between two randomly drawn words inside an aligned segment pair.",
        "The position difference is computed by: where i is the position of the source word, j is the position of the target word, Is is the length of the source sentence, It is the length of the target sentence.",
        "Three cases are taken into account: log P�osil if D�osi <= 3 log PPosi if 3 < DPosi <= 5 log PPosi3 if 5< D�osi These probabilities can be roughly estimated according to L the average length of the aligned segments (a segment is a group of aligned sentences).",
        "where L is supposed to be higher then 11.",
        "2.3.4 Word part-of-speech The association score based on word part-of-speech (S�os) is the log-probability to observe the same part-of-speech between two randomly drawn words inside an aligned segment pair.",
        "This probability PPos+ can be computed from empirical observations on any sentence aligned corpus.",
        "-log PPos+ if POS are identical log P�os- if POS are different"
      ]
    },
    {
      "heading": "2.3.5 Score combination",
      "text": [
        "The distribution score Sd has a different meaning than S,og, SPosi and S�os, because it is not the result of a random draw of two units inside an aligned segment.",
        "So we propose to combine these scores with different weight:"
      ]
    },
    {
      "heading": "3 Experiment",
      "text": [
        "We implemented this method on Flaubert's novel Madame Bovary and its English translation.",
        "The corpus has been tokenized, lemmatized and POS tagged using XeLDA2.",
        "The parameters P,og+ , P�osil, P�osi2, P�osi3 and PPos+ have been directly computed from the aligned segments of BOVARY.",
        "Sd has been computed from the distributions inside"
      ]
    },
    {
      "heading": "3.1 Results",
      "text": [
        "For evaluation, we created a small gold standard consisting of 149 English and French segment pairs, extracted from the first chapter of the BOVARY.",
        "The manual aligning yielded 1,156 content-word pairs.",
        "Results have been evaluated using a fine-grained metrics for precision and recall (Ahrenberg et al., 2000), and a balanced F-measure.",
        "For the competitive linking algorithm, only content words have been taken into account.",
        "Figure 1 displays the results for various values of k.",
        "Of course the best k depends on the size of the corpus from which Sd is computed.",
        "In the present case, the best results are reached for k=4: P= 91.66% and R=90.31%.",
        "But even without tuning, using k= 1, the results as still good: P=89.10% R=87.71% To highlight the respective role and efficiency of each clue, we have extracted the lexical correspondences for various combinations (see table 2)."
      ]
    },
    {
      "heading": "3.2 Discussion",
      "text": [
        "The results displayed on table 3 shows that the distributional clue is preponderant.",
        "It gives more or less the same results than log-like (Dunning, 1993).",
        "The combination of all other clues gives poor results and shows what can be expected on a small corpus (Sd needs to be computed on a large set of segment pairs).",
        "It is noticeable that all the clues are cumulative: the more the clues we use, the better the results we get.",
        "The efficiency of each clue can be ranked as follows: Spos<Scog<Sposi<Sd It appears that the part-of-speech clue gives not very interesting information.",
        "The results for a non-tagged corpus would be almost the same.",
        "To give a benchmark, we have also implemented the Melamed (1998) method, which bears some similarity with ours, within an iterative framework inspired by EM-algorithm.",
        "For method A, stability was reached after 3 iterations.",
        "For method B, the X+ was set as 0.86 and X as 0.095.",
        "Parameters were stable after 4 iterations.",
        "For comparison's sake, our results are computed using the same data (i.e. Sd has been computed on BOVARY only).",
        "Precision and recall displayed on table 3 shows that, even without tuning, the Null hypothesis approach using four clues is more efficient."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "This experiment shows that it is possible to get good results, with precision and recall around 90%, for bilingual correspondences extraction between content words.",
        "The originality and interest of the Null hypothesis approach is that no training set is required.",
        "In forthcoming experiments, we plan to study the effect of a semantic clue, based on the EuroWordNet interlingual index."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "Thanks to our partners: Marc El-Beze and Gr6goire Moreau de Montcheuil (LIA), Claude Richard and R6gis Meyer (ACCE), SINEQUA, and RIAM which supports the CARMEL Project."
      ]
    }
  ]
}
