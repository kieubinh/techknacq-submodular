{
  "info": {
    "authors": [
      "Ruhi Sarikaya",
      "Yuqing Gao",
      "Michael A. Picheny"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics – Short Papers",
    "id": "acl-N04-4017",
    "title": "A Comparison of Rule-Based and Statistical Methods for Semantic Language Modeling and Confidence Measurement",
    "url": "https://aclweb.org/anthology/N04-4017",
    "year": 2004
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a comparison of a rule-based and a statistical semantic information modeling technique.",
        "For the rule – based method we employ Embedded Grammar (EG) tagging and for the statistical method we use a previously proposed Semantic Structured Language Modeling (SSLM) technique.",
        "Both EG and SSLM achieve around 15% relative improvement in speech recognition performance over the baseline dialog state – based trigram language model in a financial transaction domain.",
        "Combining EG and SSLM using linear interpolation results in further improvement.",
        "We also use the features obtained from EG and SSLM for confidence measurement.",
        "Word level confidence measurement experiments using EG and SSLM – based semantic features combined with posterior probability show over 20% relative improvement in correct acceptance rate (CA) at 5% false alarm (FA) rate over the posterior probability based feature.",
        "In both language model rescoring and confidence measurement experiments SSLM outperforms EG by a small margin."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "There are two main approaches for semantic information modeling: rule – based (or grammar – based) and statistical.",
        "For spoken dialog systems, grammar and statistical methods occupy the opposite sides of the spectrum in terms of the assumptions they make on users and the \"completeness\" of utterances.",
        "In general, grammar – based approaches expect sophisticated users, who can form detailed, grammatical and complete utterances.",
        "On the other side of the spectrum, statistical methods treat speech as an inherently incomplete process, since users in general do not know the system coverage and also they may not always form grammatical sentences (i.e., spontaneous speech).",
        "Both methods have advantages and disadvantages.",
        "Statistical methods require significant amount of annotated data for reliable information modeling.",
        "They usually do not need a priori information about the task, which makes them portable to other tasks as long as there is annotated data for those domains.",
        "However, statistical methods suffer from poor generalizations when data is insufficient.",
        "On the other hand, grammar – based methods do not need annotated training data, but require major effort by experts to hand – code the a priori information into the system.",
        "Grammar – based methods for language modeling are attractive alternatives to statistical models in domains that lack extensive speech corpora (Jurafsky, 1995).",
        "We introduced a set of statistical language modeling techniques that use semantic analysis for spoken dialog systems (Erdogan, 2002).",
        "The motivation was to incorporate the semantic information from the semantic parse tree into language modeling.",
        "The SSLM uses varying levels of lexical and semantic information using maximum entropy (ME) modeling.",
        "Semantic information can also be used for confidence measurement.",
        "Since the speech recognition output is always subject to some level of uncertainty, it is essential to employ a measure that indicates the reliability (of the correctness) of hypothesized words.",
        "There are a number of overlapping speech recognition based features that were exploited in many studies (San-Segundo, 2001; Zhang, 2001; Pao, 1999).",
        "For domain independent large vocabulary speech recognition systems, posterior probability based on a word graph was shown to be the single most useful confidence feature (Wessel, 2000).",
        "In many, if not all, of the previous studies the way semantic information was incorporated into decision process is rather ad hoc.",
        "For example in (Pao, 1999), semantic weights assigned to words are based on heuristics.",
        "Likewise, in (Carpenter, 2001) semantic features such as \"uncovered word percent-age\", \"gap number\", \"slot number\", etc.",
        "were generated experimentally in an effort to incorporate semantic information into confidence metric.",
        "We proposed two methods to obtain semantic information from the parser output to incorporate into the posterior probability (Sarikaya, 2004; Sarikaya, 2003).",
        "In this study, we compare and combine the grammar and statistical methods for language modeling and the features obtained from them for confidence measurement.",
        "The rest of the paper is organized as follows: in Sec",
        "token.",
        "In addition to regular n – gram questions, four more questions are used regarding the semantic structure of the sentence.",
        "These questions are (1) current active parent label (Li), (2) Li and number of words to the left since starting the current concept (Ni), (3) ��, Ni and previous word token, (4) the previous completed constituent (OZ) and number of words to the left since completing O2.",
        "The history given in Eq.",
        "1 consists of answers to these questions.",
        "The language model score for a given word in MELM2 model is conditioned not only on the previous words but also on the labels and the relative coverage of these labels over words.",
        "The SSLM presents an effective statistical method to combine word sequences with semantic parse tree.",
        "Therefore we can use the SSLM score as a feature for confidence measurement."
      ]
    },
    {
      "heading": "4 Experimental Results and Discussions",
      "text": [
        "The experiments are conducted on a financial transaction task.",
        "The SSLM used 28.3K semantically annotated sentences (105K words) as training data.",
        "The ME – based SSLM is trained with the improved iterative scaling algorithm using fuzzy smoothing (Erdogan, 2002; Chen, 2000).",
        "The acoustic data of the SSLM training data is used as confidence measurement training data.",
        "The confidence measurement test data consists of 3152 sentences amounting to 11.4K words.",
        "The confidence training and test data have 27.9% and 28.1% word error rates (WER), respectively.",
        "The speech recognition acoustic models are trained using generic telephony data.",
        "A dialog state – based trigram language model (�S – 3gr) with deleted interpolation is used for the speech recognition to obtain the baseline WER and generate an N – best list.",
        "The baseline �S – 3gr used a separate 194K sentences as training and additional 10K sentences as held – out data from the financial domain.",
        "The N – best list contains an average of 34 alternative hypotheses per sentence with an oracle WER of 16.2%.",
        "The SSLM and EG are used to rescore the N – best list hypothesis.",
        "Table 1 shows the baseline DS – 3gr, EG and the ME – based SSLM results.",
        "The EG achieved a 15.3% relative improvement over the baseline language model.",
        "The SSLM resulted in 15.7% improvement.",
        "These improvements are due to the inclusion of new semantic information that was not part of the original speech recognition system.",
        "Even though individual improvements are similar, linearly interpolating",
        "The posterior probabilities are based on the sausages which are obtained from the word graph (Mangu, 1999).",
        "A sausage is a simplified word graph with a specific topology.",
        "The goal in this conversion is to minimize the WER rather than the sentence error rate.",
        "The technique is named as \"sausage\" since the visual representation of this graph looks like a sausage in its literal sense.",
        "The word graph is converted into a sequence of confusion sets along time.",
        "Each confusion set consists of a group of words, which are competing hypotheses for a certain time interval.",
        "The posterior probability for each word is obtained by summing the probabilities of all the paths going over that word.",
        "A sausage is generated for each sentence in the confidence training and test data.",
        "The best path from the sausage is hypothesized as the speech recognition output.",
        "Each word is labeled as correct (\"1\") or incorrect (\"0\") after aligning the hypothesis with the reference transcript.",
        "All recognition hypotheses are parsed using the statistical semantic parser.",
        "Each sentence is scored with both EG and SSLM to assign semantic probabilities to each word.",
        "The corresponding semantic features are extracted for all the words in the sentence.",
        "All of the positive (correct recognition) and negative (misrecognition) examples are pooled in two sets.",
        "A decision tree is built using the respective features.",
        "The decision tree used the raw scores of each feature.",
        "The tree is grown by partitioning the data recursively at each node until either the node becomes homogeneous or contains too few observations (G 500).",
        "Receiver operating characteristic (ROC) curve is one of the commonly used tools for confidence measurement performance.",
        "The most interesting part of the ROC curve for dialog systems is where the false acceptance rate is low and correct acceptance rate is high, because one needs to accept as many correct words as possible at low False Acceptance (FA) rates.",
        "The FA and CA are calculated using the following formula: # of falsely accepted words"
      ]
    },
    {
      "heading": "X 100",
      "text": []
    },
    {
      "heading": "Total # of negative examples",
      "text": [
        "# of correctly accepted words Total # of positive examples Here, EG refers to EG language model score, SSLM refers to the semantic language model score, and post refers to posterior probability for a given word.",
        "Based on the individual feature performances post outperformed both EG and SSLM for almost all of the FA"
      ]
    }
  ]
}
