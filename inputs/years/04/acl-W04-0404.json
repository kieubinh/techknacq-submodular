{
  "info": {
    "authors": [
      "Timothy Baldwin",
      "Takaaki Tanaka"
    ],
    "book": "Workshop on Multiword Expressions: Integrating Processing",
    "id": "acl-W04-0404",
    "title": "Translation by Machine of Complex Nominals: Getting It Right",
    "url": "https://aclweb.org/anthology/W04-0404",
    "year": 2004
  },
  "references": [
    "acl-C02-1011",
    "acl-N01-1006",
    "acl-P03-1040",
    "acl-P98-1116",
    "acl-W01-1413",
    "acl-W02-2016",
    "acl-W03-1803"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection.",
        "We propose a support vector learning-based method employing target language corpus and bilingual dictionary data, and evaluate it over a English Japanese machine translation task.",
        "We show the proposed method to be superior to previous methods and also robust over low-frequency NN compounds."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Noun-noun (NN) compounds (e.g. web server, kikai hoNyaku “machine translation”,1 the elements of which we will refer to as N and N in linear order of occurrence) are a very real problem for both machine translation (MT) systems and human translators due to: constructional variability in the translations: kikai hoNyaku “machine translation” (N-N) vs. miNkaN kigyou “private company” (Adj-N) vs. kaNkei kaizeN “improvement in relations” (N in N); lexical divergences in Japanese and English: haifu keikaku “distribution schedule” vs. keizai keikaku “economic plan/programme” vs. shuyou keikaku “major project”; semantic underspecification: compounds generally have multiple interpretations, and can only be reliably interpreted in context (Levi, 1978); the existence of non-compositional NN compounds: idobata kaigi “(lit.)",
        "well-side meeting”, which translates most naturally into English as “idle gossip”; high productivity and frequency In order to quantify the high productivity and frequency of NN compounds, we carried out a",
        "basic study of corpus occurrence in English and Japanese.",
        "For English, we based our analysis over: (1) the written portion of the British National Corpus (BNC, 84M words: Burnard (2000)), and (2) the Reuters corpus (108M words: Rose et al.",
        "(2002)).",
        "For Japanese, we focused exclusively on the Mainichi Shimbun Corpus (340M words: Mainichi Newspaper Co. (2001)).",
        "We identified NN compounds in each corpus using the method described in 2.2 below, and from this, derived the statistics of occurrence presented in Table 1.",
        "The token coverage of NN compounds in each corpus refers to the percentage of words which are contained in NN compounds; based on our corpora, we estimate this figure to be as high as 3-5%.",
        "If we then look at the average token frequency of each distinct NN compound type, we see that it is a relatively modest figure given the size of each of the corpora, the reason for which is seen in the huge number of distinct NN compound types.",
        "Combining these observations, we see that a translator or MT system attempting to translate one of these corpora will run across NN compounds with high frequency, but that each individual NN compound will occur only a few times (with around 45-60% occur-ing only once).",
        "The upshot of this for MT systems and translators is that NN compounds are too varied to be able to pre-compile an exhaustive list of translated NN compounds, and must instead be able to deal with novel NN compounds on the fly.",
        "This claim is supported by Tanaka and Baldwin (2003a), who found that static bilingual dictionaries had a type coverage of around 84% and 94% over the top250 most frequent English and Japanese NN compounds, respectively, but only 27% and 60%, respectively, over a random sample of NN compounds occurring more than 10 times in the corpus.",
        "We develop and test a method for translating NN compounds based on Japanese English MT.",
        "The method can act as a standalone module in an MT system, translating NN compounds according to the best-scoring translation candidate produced by the method, and it is primarly in this context that we present and evaluate the method.",
        "This is congruent with the findings of Koehn and Knight (2003) that, in the context of statistical MT, overall translation performance improves when source language noun phrases are prescriptively translated as noun phrases in the target language.",
        "Alternatively, the proposed method can be used to generate a list of plausible translation candidates for each NN compound, for a human translator or MT system to select between based on the full translation context.",
        "In the remainder of the paper, we describe the translation procedure and resources used in this research ( 2), and outline the translation candidate selection method, a benchmark selection method and preprocessors our method relies on ( 3).",
        "We then evaluate the method using a variety of data sources ( 4), and finally compare our method to related research( 5)."
      ]
    },
    {
      "heading": "2 Preliminaries",
      "text": []
    },
    {
      "heading": "2.1 Translation procedure",
      "text": [
        "We translate NN compounds by way of a two-phase procedure, incorporating generation and selection (similarly to Cao and Li (2002) and Langkilde and Knight (1998)).",
        "Generation consists of looking up word-level translations for each word in the NN compound to be translated, and running them through a set of constructional translation templates to generate translation candidates.",
        "In order to translate kaNkei kaizeN “improvement in relations”, for example, possible word-level translations for are relation, connection and relationship, and translations for are improvement and betterment.",
        "✁Constructional templates are of the form [N in N ] (where N indicates that the word is a noun (N) in English ( ) and corresponds to the th-occurring noun in the original Japanese; see Table 3 for further example templates and Kageura et al.",
        "(2004) for discussion of templates of this type).",
        "Each slot in the translation template is indexed for part of speech (POS), and derivational morphology is optionally used to convert a given word-level translation into a form appropriate for a given template.",
        "Example translation candidates for , therefore, are relation improvement, betterment of relationship, improvement connection and relational betterment.",
        "Generation fails in the instance that we are unable to find a word-level translation for N and/or N .",
        "Selection consists of selecting the most likely translation for the original NN compound from the generated translation candidates.",
        "Selection is performed based on a combination of monolingual target language and crosslingual evidence, obtained from corpus or web data.",
        "Ignoring the effects of POS constraints for the moment, the number of generated translations is where and are the fertility of Japanese nouns N✏ and N✏ , respectively, and is the number of translation templates.",
        "As a result, there is often a large number of translation candidates to select between, and the selection method crucially determines the efficacy of the method.",
        "This translation procedure has the obvious advantage that it can generate a translation for any NN compound input assuming that there are word-level translations for each of the component nouns; that is it has high coverage.",
        "It is based on the assumption that NN compounds translate compositional-ity between Japanese and English, which Tanaka and Baldwin (2003a) found to be the case 43.1% of the time for Japanese–English (JE) MT and 48.7% of the time for English –Japanese (EJ) MT.",
        "In this paper, we focus primarily on selecting the correct translation for those NN compounds which can be translated compositionally, but we also investigate what happens when non-compositional NN compounds are translated using a compositional method."
      ]
    },
    {
      "heading": "2.2 Translation data",
      "text": [
        "In order to generate English and Japanese NN compound testdata, we first extracted out all NN bi-grams from the Reuters Corpus and Mainichi Shimbun Corpus.",
        "The Reuters Copus was first tagged and chunked using fnTBL (Ngai and Florian, 2001), and lemmatised using morph (Minnen et al., 2001), while the Mainichi Shimbun was segmented and tagged using ChaSen (Matsumoto et al., 1999).",
        "For both English and Japanese, we took only those NN bigrams adjoined by non-nouns to ensure that they were not part of a larger compound nominal.",
        "We additionally measured the entropy of the left and right contexts for each NN type, and filtered out all compounds where either entropy value was .2 This was done in an attempt to, once again, exclude NNs which were embedded in larger MWEs, such as service department in social service department.",
        "We next calculated the frequency of occurrence of each NN compound type identified in the English and Japanese corpora, and ranked the NN compound types in order of corpus frequency.",
        "Based on this ranking, we split the NN compound types into three partitions of equal token frequency, and from each partition, randomly selected 250 NN compounds.",
        "In doing so, we produced NN compound",
        "data representative of three disjoint frequency bands of equal token size, as detailed in Table 2.",
        "This allows us to analyse the robustness of our method over data of different frequencies.",
        "Our motivation in testing the proposed method over NN compounds according to the three frequency bands is to empirically determine: (a) whether there is any difference in translation-compositionality for NN compounds of different frequency, and (b) whether our method is robust over NN compounds of different frequency.",
        "We return to these questions in 4.1.",
        "In order to evaluate basic translation accuracy over the test data, we generated a unique gold-standard translation for each NN compound to represent its optimally-general default translation.",
        "This was done with reference to two bilingual Japanese-English dictionaries: the ALTDIC dictionary and the on-line EDICT dictionary.",
        "The ALTDIC dictionary was compiled from the ALT-J/E MT system (Ikehara et al., 1991), and has approximately 400,000 entries including more than 200,000 proper nouns; EDICT (Breen, 1995) has approximately 150,000 entries.",
        "The existence of a translation for a given NN compound in one of the dictionaries does not guarantee that we used it as our gold-standard, and 35% of JE translations and 25% of EJ translations were rejected in favour of a manually-generated translation.",
        "In generating the gold-standard translation data, we checked the validity of each of the randomly-extracted NN compounds, and rejected a total of 0.5% of the initial random sample of Japanese strings, and 6.6% of the English strings, on the grounds of: (1) not being NN compounds, (2) being proper nouns, or (3) being part of a larger MWE.",
        "In each case, the rejected string was replaced with an alternate randomly-selected NN compound."
      ]
    },
    {
      "heading": "2.3 Translation templates",
      "text": [
        "The generation phase of translation relies on translation templates to recast the source language NN compound into the target language.",
        "The translation templates were obtained by way of word alignment over the JE and EJ gold-standard translation datasets, generating a total of 28 templates for the JE task and 4 templates for the EJ task.",
        "The reason for the large number of templates in the JE task is that they are used to introduce prepositions and possessive markers, as well as indicating word class conversions (see Table 3)."
      ]
    },
    {
      "heading": "3 Selection methodology",
      "text": [
        "In this section, we describe a benchmark selection method based on monoligual corpus data, and a novel selection method combining monolingual corpus data and crosslingual data derived from bilingual dictionaries.",
        "Each method takes the list of generated translation candidates and scores each, returning the highest-scoring translation candidate as our final translation."
      ]
    },
    {
      "heading": "3.1 Benchmark monolingual method",
      "text": [
        "The monolingual selection method we benchmark ourselves against is the corpus-based translation quality (CTQ) method of Tanaka and Baldwin (2003b).",
        "It rates a given translation candidate according to corpus evidence for both the fully-specified translation and its parts in the context of the translation template in question.",
        "This is calculated as:3 where and are the word-level translations of the source language N★ and N★ , respectively, and is the translation template.4 Each probability is calculated according to a maximum likelihood estimate based on relative corpus occurrence.",
        "The formulation of CTQ is based on linear interpolation over and , where and .",
        "We set to and to throughout evaluation.",
        "The basic intuition behind decomposing the translation candidate into its two parts within the context of the translation template ( and ) is to capture the subcategorisation properties of and relative to .",
        "For example, if and were Bandersnatch and relation, respectively, and for all , we would hope to score relation to (the) Bandersnatch as being more likely than relation on (the) Bandersnatch.",
        "We could hope to achieve this by virtue of the fact that relation occurs in the form relation to ... much more frequently than relation on ..., making the value of greater for the template [N to N ] than [N on N ].",
        "In evaluation, Tanaka and Baldwin (2003b) found the principal failing of this method to be its treatment of all translations contained in the transfer dictionary as being equally likely, where in fact 3 In the original formulation, the product was included as a third term, but Tanaka and Baldwin (2003b) found it to have negligible impact on translation accuracy, so we omit it here.",
        "4 and are assumed to be POS-compatible with .",
        "there is considerable variability in their applicatil-ity.",
        "One example of this is the simplex kiji which is translated as either article or item (in the sense of a newspaper) in ALTDIC, of which the former is clearly the more general translation.",
        "Lacking knowledge of this conditional probability, the method considers the two translations to be equally probable, giving rise to the preferred translation of related item for kaNreN kiji “related article” due to the markedly greater corpus occurrence of related item over related article.",
        "It is this aspect of selection that we focus on in our proposed method."
      ]
    },
    {
      "heading": "3.2 Proposed selection method",
      "text": [
        "The proposed method uses the corpus-based monolingual probability terms of CTQ above, but also mono and crosslingual terms derived from bilingual dictionary data.",
        "In doing so, it attempts to preserve the ability of CTQ to model target language expressional preferences, while incorporating more direct translation preferences at various levels of lexical specification.",
        "For ease of feature expandability, and to avoid interpolation over excessively many terms, the backbone of the method is the TinySVM support vector machine (SVM) learner.",
        "The way we use TinySVM is to take all source language inputs where the gold-standard translation is included among the generated translation candidates, and construct a single feature vector for each translation candidate.",
        "We treat those feature vectors which correspond to the (unique) gold-standard translation as positive exemplars, and all other feature vectors as negative exemplars.",
        "We then run TinySVM over the training exemplars using the ANOVA kernel (the only kernel which was found to converge).",
        "Strictly speaking, SVMs produce a binary classification, by returning a continuous value and determining whether it is closest to (the positive class) or (the negative class).",
        "We treat this value as a translation quality rating, and rank the translation candidates accordingly.",
        "To select the best translation candidate, we simply take the best-scoring exemplar, breaking ties through random selection.",
        "The selection method makes use of three basic feature types in generating a feature vector for each source language–translation candidate pair: corpus-based features, bilingual dictionary-based features and template-based features."
      ]
    },
    {
      "heading": "Corpus-based features",
      "text": [
        "Each source language–translation pair is mapped onto a total of 8 corpus-based feature types, in line with the CTQ formulation above: and ,and is a normalisation parameter used to estimate the frequency of occurrence of mul-tiword expression (MWE) translations from that of the head.",
        "E.g., in generating translations for fudousaNgaisha “real estate company”, we get two word-level translations for : real estate and real property.",
        "In each case, we identify the final word as the head, and calculate the number of times the MWEs (i.e. real estate and real property) occur in the overall corpus as compared to the head (i.e. estate and property, respectively).",
        "In calculating the values of each of the frequency-based features involving these translations, we determine the frequency of the head in the given context, and multiply this by the normalisation parameter.",
        "The reason for doing this is for ease of calculation and, wherever possible, to avoid zero values for frequencies involving MWEs.",
        "The feature is generated by multiplying the MWE parameters for each of and (which are set to 1.0 in the case that the translation is simplex) and intended to model the tendency to prefer simplex translations over MWEs when given a choice.",
        "We construct an additional feature from each of these values, by normalising (by simple division to generate a value in the range ) relative to the maximum value for that feature among the translation candidates generated for a given source language input.",
        "For each corpus, therefore, the total number of corpus-based features is .",
        "In EJ translation, the corpus-based feature values were derived from the Mainichi Shimbun Corpus, whereas in JE translation, we used the BNC and Reuters Corpus, and concatenated the feature values from each."
      ]
    },
    {
      "heading": "Bilingual dictionary-based features",
      "text": [
        "Bilingual dictionary data is used to generate 6 features: and and is the total number of times the given translation candidate occurs as a translation for the source language NN compound across all dictionaries.",
        "While this feature may seem to give our method an unfair advantage over CTQ, it is important to realise that only limited numbers of NN compounds are listed in the dictionaries (12% for English and 28% for Japanese), and that the gold-standard accuracy when the dictionary translation is selected is not as high as one would expect (65% for English and 75% for Japanese).",
        "describes the total occurrences of the translation candidate across all dictionaries (irrespective of the source language expression it translates), and is considered to be an indication of conventionalisation of the candidate.",
        "The remaining features are intended to capture word-level translation probabilities, optionally in the context of the template used in the translation candidate.",
        "Returning to our kaNreN kiji “related article” example from above, of the translations article and item for , article occurs as the translation of for 42% of NN entries with as the N , and within 18% of translations for complex entries involving (irrespective of the form or alignment between article and ).",
        "For item, the respective statistics are 9% and 4%.",
        "From this, we can conclude that article is the more appropriate translation, particularly for the given translation template.",
        "As with the corpus-based features, we additionally construct a normalised variant of each feature value, such that the total number of bilingual dictionary-based features is .",
        "In both JE and EJ translation, we derived bilingual dictionary-based features from the EDICT and ALTDIC dictionaries independently, and concatenated the features derived from each."
      ]
    },
    {
      "heading": "Template-based features",
      "text": [
        "We use a total of two template-based features: the template type and the target language head (N1 or N2).",
        "For template [N N ]J [N N ]E (see 2.3), e.g., the template type is N-N and the target language head is N 1."
      ]
    },
    {
      "heading": "3.3 Corpus data",
      "text": [
        "The corpus frequencies were extracted from the same three corpora as were described in 1: the BNC and Reuters Corpus for English, and Mainichi Shimbun Corpus for Japanese.",
        "We chose to use the BNC and Reuters Corpus because of their complementary nature: the BNC is a balanced corpus and hence has a rounded coverage of NN compounds (see Table 1), whereas the Reuters Corpus contains newswire data which aligns relatively well in content with the newspaper articles in the Mainichi Shimbun Corpus.",
        "We calculated the corpus frequencies based on the tag and dependency output of RASP (Briscoe and Carroll, 2002) for English, and CaboCha (Kudo and Matsumoto, 2002) for Japanese.",
        "RASP is a tag sequence grammar-based stochastic parser which attempts to exhaustively resolve inter-word dependencies in the input.",
        "CaboCha, on the other hand, chunks the input into head-annotated “bunsetsu” or base phrases, and resolves only inter-phrase dependencies.",
        "We thus independently determined the intra-phrasal structure from the CaboCha output based on POS-conditioned templates."
      ]
    },
    {
      "heading": "4 Evaluation",
      "text": [
        "We evaluate the method over both JE and EJ translation selection, using the two sets of 750 NN compounds described in 2.2.",
        "In each case, we first evaluate system performance according to gold-standard accuracy, i.e. the proportion of inputs for which the (unique) gold-standard translation is ranked top amongst the translation candidates.",
        "For the method to have a chance at selecting the gold-standard translation, we clearly must be able to generate it.",
        "The first step is thus to identify inputs which have translation-compositional gold-standard translations, and generate the translation candidates for each.",
        "The translation-compositional data has the distribution given in Table 4.",
        "The overall proportion of translation-compositional inputs is somewhat lower than suggested by Tanaka and Baldwin (2003a), although this is conditional on the coverage of the particular dictionaries we use.",
        "The degree of translation-compositionality appears to be relatively constant across the three frequency bands, a somewhat surprising finding as we had expected the lower frequency NN compounds to be less conventionalised and therefore have more straightforwardly compositional translations.",
        "We use the translation-compositional test data to evaluate the proposed method (SVM ) against CTQ and a simple baseline derived from CTQ, which takes the most probable fully-specified translation",
        ").",
        "We additionally tested the proposed method using just corpus-based features (SVM ) and bilingual dictionary-based features (SVM ) to get a better sense for the relative impact of each on overall performance.",
        "In the case of the proposed method and its derivants, evaluation is according to 10-fold stratified cross-validation, with stratification taking place across the three frequency bands.",
        "The average number of translations generated for the JE dataset was 205.6, and that for the EJ dataset was 847.5.",
        "We were unable to generate any translations for 17 (2.3%) and 57 (7.6%) of the NN compounds in the JE and EJ datasets, respectively, due to there being no word-level translations for N and/or N in the combined ALTDIC/EDICT dictionaries.",
        "The gold-standard accuracies are presented in Table 5, with figures in boldface indicating a statistically significant improvement over both CTQ and the baseline.6 Except for SVM in the EJ task, all evaluated methods surpass the baseline, and all variants of SVM surpassed CTQ.",
        "SVM appears to successfully consolidate on SVM and SVM , indicating that our modelling of target language corpus and crosslingual data is complementary.",
        "Overall, the results for the EJ task are higher than those for the JE task.",
        "Part of the reason for this is that Japanese has less translation variability for a given pair of word translations, as discussed below.",
        "In looking through the examples where a gold-standard translation was not returned by the different methods, we often find that the uniqueness of gold-standard translation has meant that equally good translations (e.g. dollar note vs. the gold-standard translation dollar bill for doru shihei) or marginally lower-quality but perfectly acceptable translations (e.g. territorial issue vs. the gold-standard translation of territorial dispute for ryoudo moNdai) are adjudged incorrect.",
        "To rate the utility of these near-miss translations, we rated each non-gold-standard first-ranking translation according to source language-recoverability (L1-recoverability).",
        "L1-recoverable",
        "translations are defined to be syntactically unmarked, capture the basic semantics of the source language expression and allow the source language expression to be recovered with reasonable confidence.",
        "While evaluation of L1-recoverability is inevitably subjective, we minimise bias towards any given system by performing the L1-recoverability annotation for all methods in a single batch, without giving the annotator any indication of which method selected which translation.",
        "The average number of English and Japanese L1-recoverable translations were 1.9 and 0.94, respectively.",
        "The principle reason for the English data being more forgiving is the existence of possessive and PP-based paraphrases of NN gold-standard translations (e.g. ammendment of rule(s) as an L1-recoverable paraphrase of rule ammendment).",
        "We combine the gold-standard data and L1-recoverable translation data together into a single silver standard translation dataset, based upon which we calculate silver-standard translation accuracy.",
        "The results for the translation-compositional data are given in Table 6.",
        "Once again, we find that the proposed method is superior to the baseline and CTQ, and that the combination of crosslingual and target language corpus data is superior to the individual data sources.",
        "SVM fares particularly badly under silver-standard evaluation as it is unable to capture the target language lexical and constructional preferences as are needed to generate syntactically-unmarked, natural-sounding translations.",
        "Unsurprisingly, the increment between gold-standard accuracy and silver-standard accuracy is greater for English than Japanese."
      ]
    },
    {
      "heading": "4.1 Accuracy over each frequency band",
      "text": [
        "We next analyse the breakdown in gold and silver-standard accuracies across the three frequency bands.",
        "In doing this, we test the hypothesis that training over only translation data from the same frequency band will produce better results than",
        "training over all the translation data.",
        "The results for the JE and EJ translation tasks are presented in Tables 7 and 8, respectively.",
        "The results based on training over data from all frequency bands are labelled All and those based on training over data from only the same frequency band are labelled Local; G is the gold-standard accuracy and S is the silver-standard accuracy.",
        "For each of the methods tested, we find that the gold and silver-standard accuracies drop as we go down through the frequency bands, although the drop off is markedly greater for gold-standard accuracy.",
        "Indeed, silver-standard accuracy is constant between the high and medium bands for the JE task, and the medium and low frequency bands for the EJ task.",
        "SVM appears to be robust over low-frequency data for both tasks, with the absolute difference in silver-standard accuracy between the high and low frequency bands around only 0.",
        "10, and never dropping below 0.70 for either the EJ or JE task.",
        "There was very little difference between training over data from all frequency bands as compared to only the local frequency band, suggesting that there is little to be gained from conditioning training data on the relative frequency of the NN compound we are seeking to translate."
      ]
    },
    {
      "heading": "4.2 Accuracy over non-translation-compositional data",
      "text": [
        "Finally, we evaluate the performance of the methods over the non-translation compositional data.",
        "We are unable to give gold-standard accuracies here as, by definition, the gold-standard translation is not amongst the translation candidates generated for any of the inputs.",
        "We are, however, able to evaluate according to silver-standard accuracy, constructing L1-recoverable translation data as for the translation-compositional case described above.",
        "The classifier is learned from all the translation-compositional data, treating the gold-standard translations as positive exemplars as before.",
        "The results are presented in Table 9.",
        "A large disparity is observable here between the JE and EJ accuracies, which is, once again, a direct result of Japanese being less forgiving when it comes to L1-recoverable translations.",
        "For the translation-compositional data, the EJ task displayed a similarly diminished accuracy increment when the L1-recoverable translation data was incorporated, but this was masked by the higher gold-standard accuracy for the task.",
        "The relative results for the JE task largely mirror those for the translation-compositonal data.",
        "In contrast, SVM actually performs marginally worse than CTQ over the EJ task, despite SVM performing above CTQ.",
        "That is, the addition of dictionary data diminishes overall accuracy, a slightly surprising result given the complementary of corpus and dictionary data in all other aspects of evaluation.",
        "It is possible that we could get better results by treating both L1-recoverable and gold-standard translations in the training data as positive exemplars, which we leave as an item for future research.",
        "Combining the results from Table 9 with those from Table 6, the overall silver-standard accuracy over the JE data is 0.671 for SVM (compared to 0.602 for CTQ), and that over the EJ data is 0.461 (compared to 0.419 for CTQ).",
        "In summary, we have shown our method to be superior to both the baseline and CTQ over EJ and JE translation tasks in terms of both gold and silver-standard accuracy.",
        "We also demonstrated that the method successfully combines crosslingual and target language corpus data, and is relatively robust over low frequency inputs."
      ]
    },
    {
      "heading": "5 Related work",
      "text": [
        "One piece of research relatively closely related to our method is that of Cao and Li (2002), who use bilingual bootstrapping over Chinese and English web data in various forms to translate Chinese NN compounds into English.",
        "While we rely on bilingual dictionaries to determine crosslingual similarity, their method is based on contextual similarity in the two languages, without assuming parallelism or comparability in the corpus data.",
        "They report an impressive F-score of 0.73 over a dataset of 1000 instances, although they also cite a prior-based F-score (equivalent to our Baseline) of 0.70 for the task, such that the particular data set they are dealing with would appear to be less complex than that which we have targeted.",
        "Having said this, contextual similarity is an orthogonal data source to those used in this research, and has the potential to further improve the accuracy of our method.",
        "Nagata et al.",
        "(2001) use “partially bilingual” web",
        "pages, that is web pages which are predominantly Japanese, say, but interspersed with English words, to extract translation pairs.",
        "They do this by accessing web pages containing a given Japanese expression, and looking for the English expression which occurs most reliably in its immediate vicinity.",
        "The method achieves an impressive gold-standard accuracy of 0.62, at a recall of 0.68, over a combination of simplex nouns and compound nominals.",
        "Grefenstette (1999) uses web data to select English translations for compositional German and Spanish noun compounds, and achieves an impressive accuracy of 0.86–0.87.",
        "The translation task Grefenstette targets is intrinsically simpler than that described in this paper, however, in that he considers only those compounds which translate into NN compounds in English.",
        "It is also possible that the historical relatedness of languages has an effect on the difficulty of the translation task, although further research would be required to confirm this prediction.",
        "Having said this, the successful use of web data by a variety of researchers suggests an avenue for future research in comparing our results with those obtained using web data."
      ]
    },
    {
      "heading": "6 Conclusion and future work",
      "text": [
        "We have proposed a method for translating NN compounds which compositionally generates translation candidates and selects among them using a target language model based on corpus statistics and a translation model based on bilingual dictionaries.",
        "Our SVM-based implementation was shown to outperform previous methods and be robust over low-frequency NN compounds for JE and EJ translation tasks."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": []
    }
  ]
}
