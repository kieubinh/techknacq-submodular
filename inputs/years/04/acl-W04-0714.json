{
  "info": {
    "authors": [
      "Ching-Long Yeh",
      "Yi-Chun Chen"
    ],
    "book": "Conference on Reference Resolution and Its Applications",
    "id": "acl-W04-0714",
    "title": "Topic Identification in Chinese Based on Centering Model",
    "url": "https://aclweb.org/anthology/W04-0714",
    "year": 2004
  },
  "references": [
    "acl-C02-1078",
    "acl-C96-1021",
    "acl-C96-2147",
    "acl-J86-3001",
    "acl-J94-2003",
    "acl-J94-4002",
    "acl-J95-2003",
    "acl-P95-1017",
    "acl-P96-1036",
    "acl-P98-1064",
    "acl-P98-2143",
    "acl-W01-0706",
    "acl-W97-1306",
    "acl-W98-1119"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper we are concerned with identifying the topics of sentences in Chinese texts.",
        "The key elements of the centering model of local discourse coherence are employed to identify the topic which is the most salient element in a Chinese sentence.",
        "Due to the phenomenon of zero anaphora occurring in Chinese texts frequently, in addition to the centering model, we further employ the constraint rules to identify the antecedents of zero anaphors.",
        "Unlike most traditional approaches to parsing sentences based on the integration of complex linguistic information and domain knowledge, we work on the output of a part-of-speech tagger and use shallow parsing instead of complex parsing to identify the topics from sentences."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "One of the most striking characteristics in a topic-prominent language like Chinese is the important element, “topic,” in a sentence which can represent what the sentence is about (Li and Thompson, 1981).",
        "That is, if we can identify topics from Chinese sentences, we can obtain the most information embedded in the text.",
        "In this paper, we tend to identify the topic of each utterance within a discourse based on the centering model.",
        "However, in many natural languages, elements that can be easily deduced by the reader are frequently omitted from expressions in texts.",
        "The elimination of anaphoric expressions is termed zero anaphor (ZA) which often occurs in topic position in a Chinese sentence, due to their prominence in discourse.",
        "Accordingly, to accomplish the task of topic identification, we have to solve the problem of zero anaphora resolution.",
        "There are several methods of anaphora resolution.",
        "One method is to integrate different knowledge sources or factors (e.g. gender and number agreement, c-command constraints, semantic information) that discount unlikely candidates until a minimal set of plausible candidates is obtained (Grosz et al., 1995; Lappin and Leass, 1994; Okumura and Tamura, 1996; Walker et al., 1998; Yeh and Chen, 2001).",
        "Anaphoric relations between anaphors and their antecedents are identified based on the integration of linguistic and domain knowledge.",
        "However, it is very labor-intensive and time-consuming to construct a domain knowledge base.",
        "Another method employs statistical models or AI techniques, such as machine learning, to compute the most likely candidate (Aone and Bennett, 1995; Connoly et al., 1994; Ge et al., 1998; Seki et al., 2002).",
        "This method can sort out the above problems.",
        "However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002).",
        "Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as part-of-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003).",
        "The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream.",
        "The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents.",
        "In the following sections we first describe the centering model which including the key elements of the centering model of local discourse coherence.",
        "In Section 3 we describe the details of shallow parsing we employed.",
        "In Section 4 we explain our ZA resolution method based on the centering model and the constraint rules.",
        "The method of topic identification in Chinese sentences is illustrated in Section 5.",
        "In the last section the conclusions are made."
      ]
    },
    {
      "heading": "2 Centering Model",
      "text": [
        "In the centering theory (Grosz and Sidner, 1986; Grosz et al., 1995; Walker et al., 1994; Strube and Hahn, 1996), the 'attentional state' was identified as a basic component of discourse structure that consisted of two levels of focusing: global and local.",
        "For Grosz and Sidner, the centering theory provided a model for monitoring local focus and yielded the centering model which was designed to account for the difference in the perceived coherence of discourses.",
        "In the centering model, each utterance U in a discourse segment has two structures associated with it, called forward-looking centers, Cf(U), and backward-looking center, Cb(U).",
        "The forward-looking centers of Un, Cf(Un), depend only on the expressions that constitute that utterance.",
        "They are not constrained by features of any previous utterance in the discourse segment (DS), and the elements of Cf(Un) are partially ordered to reflect relative prominence in Un.",
        "Grosz et al., in their paper (Grosz et al., 1995), assume that grammatical roles are the major determinant for ranking the forward-looking centers, with the order “Subject > Object(s) > Others”.",
        "The superlative element of Cf(Un) may become the Cb of the following utterance, Cb(Un+1).",
        "In addition to the structures for centers, Cb, and Cf, the centering model specifies a set of constraints and rules (Grosz et al., 1995; Walker et al.",
        "1994)."
      ]
    },
    {
      "heading": "Constraints",
      "text": [
        "For each utterance Ui in a discourse segment consisting of utterances U1, ..., Um:",
        "1.",
        "Ui has exactly one Cb.",
        "2.",
        "Every element of Cf(Ui) must be realized in Ui.",
        "3.",
        "Ranking of elements in Cf(Ui) guides determination of Cb(Ui+1).",
        "4.",
        "The choice of Cb(Ui) is from Cf(Ui-1), and can not be from Cf(Ui-2) or other prior sets of Cf.",
        "Backward-looking centers, Cbs, are often omitted or pronominalized and discourses that continue centering the same entity are more coherent than those that shift from one center to another.",
        "This means that some transitions are preferred over others.",
        "These observations are encapsulated in two rules:"
      ]
    },
    {
      "heading": "Rules",
      "text": [
        "For each utterance Ui in a discourse segment consisting of utterances U1, ..., Um:",
        "I.",
        "If any element of Cf(Ui) is realized by a pronoun in Ui+1 then the Cb(Ui+1) must be realized by a pronoun also.",
        "II.",
        "Sequences of continuation are preferred over sequence of retaining; and sequences of",
        "retaining are to be preferred over sequences of shifting.",
        "Rule I represents one function of pronominal reference: the use of a pronoun to realize the Cb signals the hearer that the speaker is continuing to talk about the same thing.",
        "Psychological research and cross-linguistic research have validated that the Cb is preferentially realized by a pronoun in English and by equivalent forms (i.e. zero anaphora) in other languages (Grosz et al., 1995).",
        "Rule II reflect the intuition that continuation of the center and the use of retentions when possible to produce smooth transitions to a new center provide a basis for local coherence.",
        "For example in (1), the subjects of the utterance (1b) and (1d) are eliminated, and their antecedents are identified as the subjects of the preceding utterances (1a) and (1c) respectively1 according to the centering model.",
        "(1) a. i",
        "Electronics stocki receive USA high-tech stock heavy-fall affect Electronics stocksi were affected by high-tech stocks fallen heavily in America.",
        "(Electronics stocks)i continue fall (Electronics stocks)i continued falling down.",
        "j Securities stocksj also have relative respondence Securities stocksj also had respondence.",
        "d. j (Securities stocks) j continue fall by close.",
        "(Securities stocks) j fell by close one after another."
      ]
    },
    {
      "heading": "3 Shallow Parsing",
      "text": [
        "Shallow (or partial) parsing which is an inexpensive, fast and reliable method does not deliver full syntactic analysis but is limited to parsing smaller constituents such as noun phrases or verb phrases (Abney, 1996; Li and Roth, 2001; Mitkov, 1999).",
        "For example, the sentence (2) can be divided as follows: (2) Hualien became the popular tourist attraction."
      ]
    },
    {
      "heading": "4 [NP ] [VP ] [NP ]",
      "text": [
        "1 We use a b φa to denote a zero anaphor, where the subscript a is the index of the zero anaphor itself and the superscript b is the index of the referent.",
        "A single without any script represents an intrasentential zero anaphor.",
        "Also note that a superscript attached to an NP is used to represent the index of the referent.",
        "[NP Hualien] [VP became] [NP the popular tourist attraction] Given a Chinese sentence, our method of shallow parsing is divided into the following steps: First the sentence is divided into a sequence of POS-tagged words by employing a segmentation program, AUTOTAG, which is a POS tagger developed by CKIP, Academia Sinica (CKIP, 1999).",
        "Second the sequence of words is parsed into smaller constituents such as noun phrases and verb phrases with phrase-level parsing.",
        "Each phrase is represented as a word list.",
        "Then the sequence of word lists is transformed into triples, [S,P,O].",
        "For example in (3), (3b) is the output of sentence (3a) produced by AUTOTAG, and (3c) is the triple representation.",
        "(3) a.",
        "[ (Nc) (VG) (VH) (DE) (VA) (Na)] b.",
        "[NP,[ ]], [VP,[ ]], [NP,[",
        "The definition of triple representation is illustrated in Definition 1.The triple here is a simple representation which consists of three elements: S, P and O which correspond to the Subject (noun phrase), Predicate (verb phrase) and Object (noun phrase) respectively in a clause."
      ]
    },
    {
      "heading": "Definition 1:",
      "text": [
        "A Triple T is characterized by a 3-tuple: T = [S, P, O] where",
        "• S is a list of nouns whose grammatical role is the subject of a clause.",
        "• P is a list of verbs or a preposition whose grammatical role is the predicate of a clause.",
        "• O is a list of nouns whose grammatical",
        "role is the object of a clause.",
        "In the step of triple transformation, the sequence of word lists as shown in (3b) is transformed into triples by employing the Triple Rules.",
        "The Triple Rules is built by referring to the Chinese syntax.",
        "There are four kinds of Triples in the Triple Rules, which corresponds to five basic clauses: subject + transitive verb + object, subject + intransitive verb, subject + preposition + object, and a noun phrase only.",
        "The rules listed below are employed in order: Triple Rules: Triple1(S,P,O) 4 np(S), vtp(P), np(O).",
        "Triple2(S,P,none) 4 np(S), vip(P).",
        "Triple3(S,P,O) 4 np(S), prep(P), np(O).",
        "Triple4(S,none,none) 4 np(S).",
        "The vtp(P) denotes the predicate is a transitive verb phrase, which contains a transitive verb in the rightmost position in the phrase; likewise the vip(P) denotes the predicate is an intransitive verb phrase, which contains an intransitive verb in the rightmost position in the phrase.",
        "In the rule Triple3, the prep(P) denotes the predicate is a preposition.",
        "The Triple4 is employed if only a sentence contains only one noun phrase and no other constituent.",
        "If all the rules in the Triple Rules failed, the ZA Triple Rules are employed to detect zero anaphor (ZA) candidates."
      ]
    },
    {
      "heading": "ZA Triple Rules:",
      "text": [
        "The zero anaphora in Chinese generally occurs in the topic, subject or object position.",
        "The rules Triple1z1, Triple2z1, and Triple3z1 detect the zero anaphora occurring in the topic or subject position.",
        "The rule Triple1z2 detects the zero anaphora in the object position and Triple1z3 detect the zero anaphora occurring in both subject and object positions.",
        "In the Triple4, the co-conj(P) denotes a coordinating conjunction appearing in the initial position of a clause.",
        "For example in (4), there are two triples generated.",
        "In the second triple, zero denotes a zero anaphor according to Triple1z1.",
        "]]] [[[Zhangsan], [enter], [competition]], [[zero], [win], [champion]]]"
      ]
    },
    {
      "heading": "4 Zero Anaphora Resolution 4.1 ZA Resolution Method",
      "text": [
        "The process of analyzing Chinese zero anaphora is different from general pronoun resolution in English because zero anaphors are not expressed in discourse.",
        "Therefore, the ZA resolution method we develop is divided into three phases.",
        "First each sentence of an input document is translated into triples as described in Section 3.",
        "Second is ZA identification that verifies each ZA candidates annotated in triples by employing ZA",
        "identification constraints.",
        "Third is antecedent identification that identifies the antecedent of each detected ZA using rules based on the centering model.",
        "In the ZA detection phase, we use the ZA Triple Rules described in the Section 3 to detect omitted cases as ZA candidates denoted by zero in triples.",
        "Table 1 shows some examples corresponding to the ZA Triple Rules.",
        "After ZA candidates are detected by employing the ZA Triple Rules, the ZA identification constraints are utilized to filter out non-anaphoric cases.",
        "In the ZA identification constraints, the constraint 1 is employed to exclude the exophora3 or cataphora4 which is different from anaphora in 2 We use a Q to denote a question (ma); a ASPECT to denote aspect marker.",
        "texts.",
        "The constraint 2 includes some cases might be incorrectly detected as zero anaphors, such as passive sentences or inverted sentences (Hu, 1995)."
      ]
    },
    {
      "heading": "ZA identification constraints",
      "text": [
        "For each ZA candidate c in a discourse:",
        "1. c can not be in the first utterance in a discourse segment 2.",
        "ZA does not occur in the following case:",
        "In the antecedent identification phase, we employ the concept, ‘backward-looking center’ of centering model to identify the antecedent of each ZA.",
        "First we use noun phrase rules to obtain noun phrases in each utterance, and then the antecedent is identified as the most prominent noun phrase of the preceding utterance (Yeh and Chen, 2001):"
      ]
    },
    {
      "heading": "Antecedent identification rule:",
      "text": [
        "For each zero anaphor z in a discourse segment consisting of utterances U1, ... , Um: If z occurs in Ui, and no zero anaphor occurs in",
        "then choose the noun phrase with the corresponding grammatical role in Ui-1 as the antecedent Else if only one zero anaphor occurs in Ui-1 then choose the antecedent of the zero anaphor in Ui-1 as the antecedent of z Else if more than one zero anaphor occurs in Ui-1 then choose the antecedent of the zero anaphor in Ui-1 as the antecedent of z according to grammatical role criteria: Topic > Subject > Object > Others End if Due to topic-prominence in Chinese (Li and Thompson, 1981), topic is the most salient grammatical role.",
        "In general, if the topic is omitted, the subject will be in the initial position of an utterance.",
        "If the topic and subject are omitted concurrently, the ZA occurs.",
        "Since the antecedent identification rule is corresponding to the concept of centering model."
      ]
    },
    {
      "heading": "4.2 ZA Resolution Experiment",
      "text": [
        "In the experiment of ZA resolution, we use a test corpus which is a collection of 150 news articles contained 998 paragraphs, 4631 utterances, and 40884 Chinese words.",
        "By employing the ZA Triple Rules and ZA identification constraints mentioned previously, zero anaphors occur in topic or subject, and object positions can be detected.",
        "Because the ZA Triple Rules cover each possible topic or subject, and object omission cases, the result shows that the zero anaphors are over detected and the precision rate (PR) is 84% calculated using equation 1.",
        "The main errors of ZA detection occur in the experiment when parsing inverted sentences and non-anaphoric cases (e.g. exophora or cataphora) (Hu, 1995; Mitkov, 2002).",
        "Cataphora is similar to anaphora, the difference being the direction of the reference.",
        "In this paper, we do not deal with the case that the referent of a zero anaphor is in the following utterances, but we can detect about 60% cataphora in the test corpus by employing ZA identification constraint 1.",
        "In the phase of antecedent identification, we take the output of employing the ZA Triple Rules and ZA identification constraints, and further to identify the antecedents of zero anaphors by using antecedent identification rule based on the centering model.",
        "For example, in the discourse segment (5), the zero anaphors are detected in the utterances (5b) and (5c).",
        "According to the antecedent identification rule, the noun phrase, ‘Kee-lung General Hospital,’ whose grammatical role is corresponding to the zero anaphor 01i in (5b) is identified as the antecedent.",
        "Subsequently, the antecedent of the zero anaphor φz in (5c) is identified as the antecedent of 01i in (5b), .",
        "(5) a. i Jilong yiyuan wei kuoda fuwu fanwei Kee-lung hospital for expand service coverage Kee-lung'.",
        "ee-lung' General Hospital aims to increase service coverage."
      ]
    },
    {
      "heading": "b. 0i",
      "text": [
        "jiji tisheng yiliao fuwu pinzhi ji biaozhunhua (Kee-lung General Hospital)i active improve medical-treatment service quality and standardization (Kee-lung General Hospital)i actively improves the service quality of medical treatment and standardization.",
        "c. 02i huo weishengshu renke wei banli wailao tijian yiyuan (Kee-lung General Hospital)i obtain Department-of-Health certify to-be handle foreign-laborer physical-examination hospital (Kee-lung General Hospital)i is certified by Department of Health as a hospital which can handle physical examinations of foreign laborers.",
        "The recall rates (RR) and precision rates (PR) of ZA resolution is 70% and 60.3% respectively calculated using equation 2 and equation 3.",
        "Errors occur in the phase when a zero anaphor refers to an entity other than the corresponding grammatical role or the antecedent of the zero anaphor in the preceding utterance."
      ]
    },
    {
      "heading": "5 Topic Identification",
      "text": [
        "Topic identification is similar to theme identification in (Rambow, 1993).",
        "The theme clearly corresponds to the Cb: the theme, under a general definition, is what the current utterance is about; what utterances are about provides a link to previous discourse, since otherwise the text would be incoherent.",
        "The role of the Cb is precisely to provide such a link.",
        "In our approach, in addition to the centering model, we further employ the antecedent identification rule to identify the topic.",
        "When a ZA occurs in the utterance Ui, the antecedent of the ZA is identified as the topic of Ui.",
        "Otherwise, if the transition relation, center shifting, occurs, topic will not be identified as any of the element in the preceding utterance but the element in the current utterance according to grammatical role criteria.",
        "The topic identification rule is described below:"
      ]
    },
    {
      "heading": "Topic identification rule:",
      "text": [
        "For identifying each topic t in a discourse segment consisting of utterances U1, ... , Um: If at least one ZA occurs in Ui then refer to grammatical role criteria to choose the antecedent of the ZA as the t Else if no ZA occurs in Ui then refer to grammatical role criteria to choose one element of Ui as the t End if We now take the discourse segment (1) as an example to identify each topic of the utterances (1a) to (1d) by employing the topic identification rule.",
        "As illustrated in Table2, the topic of (1a) is ‘Electronics stocks,’ and the topic of (1b) is identified as the antecedent of i, ‘Electronics stocks.’ Similarly, the topic of (1d) is ‘Securities stocks,’ which is the same as the topic of (1c)."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this paper, we propose a method of topic identification in Chinese based on the centering model.",
        "Based on observations on real texts, we found that to identify the topics in Chinese context is much related to the issue of zero anaphora resolution.",
        "We use a zero anaphora resolution method to resolve the problem of ellipsis in Chinese text.",
        "The zero anaphora resolution method works on the output of a part-of-speech tagger and employs a shallow parsing instead of a complex parsing to resolve zero anaphors in Chinese text.",
        "Due to time limit, we have not applied the result of topic identification to applications for evaluation.",
        "We will further continue improving the accuracy of zero anaphora resolution and develop the applications based on topic identification, such as information extraction/retrieval and text categorization."
      ]
    },
    {
      "heading": "7 Acknowledgements",
      "text": [
        "We give our special thanks to CKIP, Academia Sinica for making great efforts in computational linguistics and sharing the Autotag program to academic research."
      ]
    }
  ]
}
