{
  "info": {
    "authors": [
      "Ulrike Baldewein",
      "Katrin Erk",
      "Sebastian Padó",
      "Detlef Prescher"
    ],
    "book": "SENSEVAL International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",
    "id": "acl-W04-0817",
    "title": "Semantic Role Labelling With Similarity-Based Generalization Using EM-Based Clustering",
    "url": "https://aclweb.org/anthology/W04-0817",
    "year": 2004
  },
  "references": [
    "acl-C00-2094",
    "acl-J02-3001",
    "acl-W02-2018",
    "acl-W03-1007",
    "acl-W04-2413"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners.",
        "We focus on generalisation using several similarity measures to increase the amount of training data available and on the use of EM-based clustering to improve role assignment.",
        "Our final score is Precision=73.6%, Recall=59.4% (F=65.7)."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "This paper describes a study in semantic role labelling in the context of the Senseval III task, for which the training and test data were both drawn from the current FrameNet release (Johnson et al., 2002).",
        "We concentrated on two questions: first, whether role assignment can be improved by generalisation over training instances using different similarity measures; and second, the impact of EM-based clustering, both in deriving more informative selectional preference features and in the generalisations mentioned above.",
        "The basis of our experiments was formed by off-the-shelf statistical tools for data processing and modelling.",
        "After listing our data preparation steps (Sec.",
        "2) and features (Sec.",
        "3), we describe our classification procedure and the learners we used (Sec.",
        "4).",
        "Sec.",
        "5 outlines our experiments in similarity-based generalisations, and Section 6 discusses our results."
      ]
    },
    {
      "heading": "2 Data and Instances",
      "text": [
        "Parsing.",
        "To tag and parse the data, we used LoPar (Schmid, 2000), a probabilistic context-free parser, which comes with a Head-Lexicalised Grammar for English (Carroll and Rooth, 1998).",
        "We considered only the most probable parse for each sentence and simplified parse trees by eliminating unary nodes.",
        "The resulting nodes form the instances of our classification.",
        "We used the Stuttgart TreeTagger (Schmid, 1994) to lemmatise constituent heads.",
        "Projection of role labels.",
        "FrameNet provides semantic roles as character offsets.",
        "We labelled those instances (i.e. nodes in the parse tree) with gold standard semantic roles which corresponded to roles’ maximal projections.",
        "13.95% of roles in the training corpus spanned more than one parse tree node.",
        "Figure 1 shows an example sentence for the AWARENESS frame.",
        "The nodes’ respective semantic role labels are given in small caps, and the target predicate is marked in boldface.",
        "Semantic clustering.",
        "We used clustering to generalise over possible fillers of roles.",
        "In a first model, we derived a probability distribution for pairs , where is a target:role combination and is the head lemma of a role filler.",
        "The key idea is that and are mutually independent, but conditioned on an unobserved class .",
        "In this manner, we define the probability of as: Estimation was performed using a variant of the expectation-maximisation algorithm (Prescher et al., 2000).",
        "We used this model both as a feature and in the generalisation described in Sec. 5.",
        "In a second model, we clustered pairs of target:role and the",
        "syntactic properties of the role fillers; the resulting model was only used for generalisation."
      ]
    },
    {
      "heading": "3 Features",
      "text": [
        "Constituent features.",
        "The first group of features represents properties of instances (i.e. constituents).",
        "We used the phrase type and head lemma of each constituent, its preposition, if any (otherwise NONE), its relative position with respect to the target (left, right, overlapping), the phrase type of its mother node, and the simplified path from the target to the constituent: all phrase types encountered on the way, and whether each step was up or down.",
        "Two further features stated whether this path had been seen as a frame element in the training data, and whether the constituent was subcategorised for (determined heuristically).",
        "Sentence level features.",
        "The second type of feature described the context of the current instance: The target word was characterised by its lemma, POS, voice, subcat frame (determined heuristically), and its governing verb; we also compiled a list of all prepositions in the sentence.",
        "Semantic features.",
        "The third type of features made use of EM-based clustering, stating the most probable label assigned to the constituent by the clustering model as well as a confidence score for this decision."
      ]
    },
    {
      "heading": "4 Classification",
      "text": [
        "We first describe our general procedure, then the two different machine learning systems we used.",
        "Classification Procedure.",
        "As the semantic role labels of FrameNet are frame-specific, we decided to train one classifier for each frame.",
        "To cope with the large amount of constituents bearing no role label, we divided the procedure into two steps, distinguishing argument identification and argument labelling.",
        "First, argument identification decides for all constituents whether they are role-bearers or not.",
        "Then, argument labelling assigns semantic roles to those sequences classified as role-bearing.",
        "In our example (Fig.",
        "1), the first step of classification ideally would single out the two NPs as possible role fillers, while the second step would assign the COGNIZER and CONTENT roles.",
        "Maximum Entropy Learning.",
        "Our first classifier was a log-linear model, where the probability of a class given an feature vector is defined as where is a normalisation constant, the value of feature for class , and the weight assigned to .",
        "The model is trained by optimising the weights subject to the maximum entropy constraint which ensures that the least committal optimal model is learnt.",
        "Maximum Entropy (Maxent) models have been successfully applied to semantic role labelling (Fleischman et al., 2003).",
        "We used the estimate software for estimation, which implements the LMVM algorithm (Malouf, 2002) and was kindly provided by Rob Malouf.",
        "Memory-based Learning.",
        "Our second learner implements an instance of a memory-based learning (MBL) algorithm, namely the nearest neighbour algorithm.",
        "This algorithm classifies test instances by assigning them the label of the most similar examples from the training set.",
        "Its parameters are the number of training examples to be considered, the similarity metric, and the feature weighting scheme.",
        "We used the implementation provided by TiMBL (Daelemans et al., 2003) with the default parameters, i.e. =1 and the weighted overlap similarity metric with gain ratio feature weighting."
      ]
    },
    {
      "heading": "5 Similarity-based Generalisation over Training Instances",
      "text": [
        "FrameNet role labels are frame-specific.",
        "This makes it necessary to either train individual classifiers with little training data per frame, or train a large classifier with many sparse classes.",
        "So one important question is whether we can generalise, i.e. exploit similarities between frame elements, to gain more training data.",
        "We experimented with different generalisation methods, all following the same basic idea: If frame element A1 of frame A and frame element B1 of frame B are similar, we reuse A1 training data as B 1 instances.",
        "In this process, we mask out features which might harm learning for A1, such as targets or sentence level features, or semantic features in case of syntactic similarities (and vice versa).",
        "We explored three types of role similarities, two based on symbolic information from the FrameNet database, and one statistical.",
        "Frame Hierarchy.",
        "FrameNet specifies frame-to-frame relations, among them three that order frames hierarchically: Inheritance, the Uses relation of partial inheritance, and the Subframe relation linking larger situation frames to their individual stages.",
        "All three indicate semantic similarity between (at least some) frame elements; in some cases corresponding frame elements are also syntactically similar, e.g. the Victim role of Cause_harm and the Evaluee role of Corporal_punishment are both typically realised as direct objects.",
        "Peripheral frame elements.",
        "FrameNet distinguishes core, extrathematic, and peripheral frame elements.",
        "Peripheral frame elements are frame-independent adjuncts; however the same frame element may be peripheral to one frame and core to another.",
        "So we took a peripheral frame element as similar to the same peripheral frame element in other frames: Given an instance of a peripheral frame element, we used it as training instance for all frames for which it was marked as peripheral in the FrameNet database.",
        "EM-based clustering.",
        "The EM-based clustering methods introduced in Sec. 2 measure the “goodness of fit” between a target word and a potential role filler.",
        "We now say that two frame elements are similar if they are appropriate for some common cluster.",
        "For the head lemma clustering model, we define the appropriateness of a target:role pair for a cluster as follows: where is the total frequency of all head lemmas that have been seen with , weighted by the class-membership probability of in .",
        "This appropriateness measure is built on top of the class-based frequencies rather than on the frequencies or the class-membership probabilities in isolation: For some tasks the combination of lexical and semantic information has been shown to outperform each of the single information sources (Prescher et al., 2000).",
        "Our similarity notion is now formalised as follows: With a threshold as a parameter, two frame elements , count as similar if for some class , and .",
        "In the syntactic clustering model, a role filler was described as a combination of the path from instance to target, the instance’s preposition, and the target voice.",
        "The appropriateness of a target:role pair is defined as for the above model.",
        "For time reasons, only verbal targets were considered.",
        "Figure 2 shows excerpts of two “syntactic” clusters in the form of target:frame.role members.",
        "Group 6 is a very homogeneous group, consisting of roles that are usually realised as subjects.",
        "Group 11 contains roles realised as prepositional phrases, but with very diverse prepositions, including in, at, along, and from."
      ]
    },
    {
      "heading": "6 Results and Discussion",
      "text": [
        "We first give the final results of our systems on the test set according to the official evaluation software.",
        "Then we discuss detailed results on a development set we randomly extracted from the training data."
      ]
    },
    {
      "heading": "6.1 Final Results",
      "text": [
        "We submitted the results of two models.",
        "One was produced using the maximum entropy learner, including all features of Sec. 3 and with the three most helpful generalisation techniques (EM head lemma, EM path, and Peripherals).",
        "For the second model we used the MBL learner trained on all features, with no additional training data1.",
        "The performance of the two models is shown in Table 1."
      ]
    },
    {
      "heading": "6.2 Detailed Results",
      "text": [
        "For a detailed evaluation, we randomly split off 10% of the training data to form development sets.",
        "In this section, we report results of two such splits to take chance variation into account.",
        "For time reasons, this detailed evaluation was performed using our own evaluation software, which is based on our internal constituent-based representation.",
        "This software gives the same tendencies (improvements / deteriorations) as the official software, but absolute values differ; so we restrict ourselves to reporting relative figures.",
        "Basis for Comparison.",
        "All following models are compared against a set of basic models trained on all features of Sec. 3.",
        "Table 2 gives the results for these models, using our own scoring software.",
        "Contribution of Features.",
        "We computed the contribution of individual features by leaving out each feature in turn.",
        "Table 3 shows the results, averaging",
        "over the two splits.",
        "The features that contributed most to the performance were the same for both learners: the label assigned by the EM-based model, the phrase type, and whether the path had been seen to lead to a frame element.",
        "The relative position to the target helped in one MBL and one Maxent run.",
        "Interestingly, the Maxent learner profits from the probability with which the EM-based model assigns its label, while MBL does not.",
        "Generalisation.",
        "To measure the effect of each of the similarity measures listed in Sec. 5, we tested them individually using the Maximum Entropy learner with all features.",
        "As mentioned above, training instances of one frame were generalised and then added to the training instances of another, retaining only part of the features in the generalisation.",
        "Table 4 shows the features retained for each similarity measure, as well as the number of additional instances generated, summed over all frames.",
        "We empirically determined the optimal parameter values as: For FN-h (sem) and FN-h (syn), 1 level in the hierarchy; for EMhead, a weight threshold of , and for EM path, a weight threshold of .",
        "Table 5 gives the improvements made over the baseline through adding data gained by each",
        "generalisation strategy.",
        "Results are shown in points F-score and individually for both training/development splits.",
        "EM-based clustering proved to be helpful, showing both the highest single improvement (EMpath) and the highest consistent improvement (EM head), while all other generalisations show mixed results.",
        "Combining the three most promising generalisation techniques (Peripherals, EM head, and EM path) led to an improvement of 0.7 points F-score for split 1 and 1.1 points F-score for split 2."
      ]
    },
    {
      "heading": "6.3 Discussion.",
      "text": [
        "Feature quality.",
        "The features that improved the learners’ performance most are EM-based label, phrase type and the “path seen as FE”.",
        "The other features did not show much impact for us.",
        "The Maxent learner was negatively affected by sentence-level features such as the subcat frame and “is subcategorised”.",
        "Comparing the learners.",
        "In a comparable basic setting (all features, no generalisation), the Memory-Based learner easily outperforms the Maxent learner, according to our scoring scheme.",
        "However, the official scoring scheme determines the Memory-based learner’s performance at more than",
        "10 points F-score below the Maxent learner.",
        "We intend to run the Memory-based learner with generalisation data for a more comprehensive comparison.",
        "Generalisation.",
        "Gildea and Jurafsky (2002) report an improvement of 1.6% through generalisation, which is roughly comparable to our figures.",
        "The two strategies share the common idea of exploiting role similarities, but the realisations are converse: Gildea and Jurafsky manually compact similar frame elements into 18 abstract, frame-independent roles, whereas we keep the roles frame-specific but augment the training data for each by automatically discovered similarities.",
        "One reason for the disappointing performance of the FrameNet hierarchy-based generalisation strategies may be simply the amount of data, as shown by Table 4: FN-h (sem) and FN-h (syn) each only yield 10,000 additional instances as compared to around 1,000,000 for EM head.",
        "That the reliability of the results roughly seems to go up with the number of additional instances generated (Peripherals: ca.",
        "50,000, EM-Path: ca.",
        "400,000) fits this argumentation well.",
        "The input to the EM path clusters is a tuple of the path, target voice and preposition information.",
        "In the resulting model, generalisation over voice worked well, yielding clusters containing both active and passive alternations of similar frame elements.",
        "However, prepositions were distributed more arbitrarily.",
        "While this may indicate problems of clustering with more structured forms of input, it may also just be a consequence of noisy input, as the preposition feature has not had much impact either on the learners’ performance.",
        "The EM head strategy adds large amounts of head lemma instances, which probably alleviates the sparse data problem that makes the head lemma feature virtually useless.",
        "Another way of capitalising on this type of information would be to use the FN hierarchy generalisation to derive more input for EM-based clustering and see if this indirect use of generalisation still improves semantic role assignment.",
        "Interestingly, the EM head strategy and the EM-based clustering feature, both geared at solving the same sparse data problem, do not cancel each other out.",
        "In future work, we will try to combine the EM head strategy with the FrameNet hierarchy to derive more input for the clustering model to see if this can improve the present generalisation results.",
        "Comparison with CoNLL.",
        "We recently studied semantic role labelling in the context of the CoNLL shared task (Baldewein et al., 2004).",
        "The two key differences to this study were that the semantic roles in question were PropBank roles and that only shallow information was available.",
        "Our system there showed two main differences to the current system: the overall level of accuracy was lower, and EM-based clustering did not improve the performance.",
        "While the performance difference is evidently a consequence of only shallow information being available, it remains an interesting open question why EM-based clustering could improve one system, but not the other."
      ]
    }
  ]
}
