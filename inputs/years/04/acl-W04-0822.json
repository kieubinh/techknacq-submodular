{
  "info": {
    "authors": [
      "Marine Carpuat",
      "Weifeng Su",
      "Dekai Wu"
    ],
    "book": "SENSEVAL International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",
    "id": "acl-W04-0822",
    "title": "Augmenting Ensemble Classification for Word Sense Disambiguation With a Kernel PCA Model",
    "url": "https://aclweb.org/anthology/W04-0822",
    "year": 2004
  },
  "references": [
    "acl-P03-1004",
    "acl-P04-1081",
    "acl-W02-1002",
    "acl-W02-2004",
    "acl-W02-2035",
    "acl-W04-0845",
    "acl-W04-0863"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The HKUST word sense disambiguation systems benefit from a new nonlinear Kernel Principal Component Analysis (KPCA) based disambiguation technique.",
        "We discuss and analyze results from the Senseval-3 English, Chinese, and Multilingual Lexical Sample data sets.",
        "Among an ensemble of four different kinds of voted models, the KPCA-based model, along with the maximum entropy model, outperforms the boosting model and naive Bayes model.",
        "Interestingly, while the KPCA-based model typically achieves close or better accuracy than the maximum entropy model, nevertheless a comparison of predicted classifications shows that it has a significantly different bias.",
        "This characteristic makes it an excellent voter, as confirmed by results showing that removing the KPCA-based model from the ensemble generally degrades performance."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Classifier combination has become a standard architecture for shared task evaluations in word sense disambiguation (WSD), named entity recognition, and similar problems that can naturally be cast as classification problems.",
        "Voting is the most common method of combination, having proven to be remarkably effective yet simple.",
        "A key problem in improving the accuracy of such ensemble classification systems is to find new voting models that (1) exhibit significantly different prediction biases from the models already voting, and yet (2) attain standalone classification accuracies that are as good or better.",
        "When either of these conditions is not met, adding the new voting model typically degrades the accuracy of the ensemble instead of helping it.",
        "In this work, we investigate the potential of one promising new disambiguation model with respect",
        "to augmenting our existing ensemble combining a maximum entropy model, a boosting model, and a naive Bayes model – a combination representing some of the best standalone WSD models currently known.",
        "The new WSD model, proposed by Wu et al.",
        "(2004), is a method for disambiguating word senses that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique.",
        "That the KPCA-based model could potentially be a good candidate for a new voting model is suggested by Wu et al.’s empirical results showing that it yielded higher accuracies on Senseval-2 data sets than other models that included maximum entropy, naive Bayes, and SVM based models.",
        "In the following sections, we begin with a description of the experimental setup, which utilizes a number of individual classifiers in a voting ensemble.",
        "We then describe the KPCA-based model to be added to the baseline ensemble.",
        "The accuracy results of the three submitted models are examined, and also the individual voting models are compared.",
        "Subsequently, we analyze the degree of difference in voting bias of the KPCA-based model from the others, and finally show that this does indeed usually lead to accuracy gains in the voting ensemble."
      ]
    },
    {
      "heading": "2 Experimental setup",
      "text": []
    },
    {
      "heading": "2.1 Tasks evaluated",
      "text": [
        "We performed experiments on the following lexical sample tasks from Senseval-3: English (fine).",
        "The English lexical sample task includes 57 target words (32 verbs, 20 nouns and 5 adjectives).",
        "For each word, training and test instances tagged with WordNet senses are provided.",
        "There are an average of 8.5 senses per target word type, ranging from 3 to 23.",
        "On average, 138 training instances per target word are available.",
        "English (coarse).",
        "This modified evaluation of the preceding task employs a sense map that groups fine-grained sense distinctions into the same coarse-grained sense.",
        "Chinese.",
        "The Chinese lexical sample task includes 21 target words.",
        "For each word, several senses are defined using the HowNet knowledge base.",
        "There are an average of 3.95 senses per target word type, ranging from 2 to 8.",
        "Only about 37 training instances per target word are available.",
        "Multilingual (t).",
        "The Multilingual (t) task is defined similarly to the English lexical sample task, except that the word senses are the translations into Hindi, rather than WordNet senses.",
        "The Multilingual (t) task requires finding the Hindi sense for 31 English target word types.",
        "There are an average of 7.54 senses per target word type, ranging from 3 to 16.",
        "A relatively large training set is provided (more than 260 training instances per word on average).",
        "Multilingual (ts).",
        "The Multilingual (ts) task uses a different data set of 10 target words and provides the correct English sense of the target word for both training and testing.",
        "There are an average of 6.2 senses per target word type, ranging from 3 to 11.",
        "The training set for this subtask was smaller, with about 150 training instances per target word."
      ]
    },
    {
      "heading": "2.2 Ensemble classification",
      "text": [
        "The WSD models presented here consist of ensembles utilizing various combinations of four voting models, as follows.",
        "Some of these component models were also evaluated on other Senseval-3 tasks: the Basque, Catalan, Italian, and Romanian Lexical Sample tasks (Wicentowski et al., 2004), as well as Semantic Role Labeling (Ngai et al., 2004).",
        "The first voting model, a na¨ıve Bayes model, was built as Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data.",
        "The second voting model, a maximum entropy model (Jaynes, 1978), was built as Klein and Manning (2002) found that it yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance.",
        "However, note that a different subset of either Senseval-1 or Senseval-2 English lexical sample data was used.",
        "The third voting model, a boosting model (Fre-und and Schapire, 1997), was built as boosting has consistently turned in very competitive scores on related tasks such as named entity classification (Car-reras et al., 2002)(Wu et al., 2002).",
        "Specifically, we employed an AdaBoost.MH model (Schapire and Singer, 2000), which is a multi-class generalization of the original boosting algorithm, with boosting on top of decision stump classifiers (decision trees of depth one).",
        "The fourth voting model, the KPCA-based model, is described below.",
        "All classifier models were selected for their ability to able to handle large numbers of sparse features, many of which may be irrelevant.",
        "Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent."
      ]
    },
    {
      "heading": "2.3 Controlled feature set",
      "text": [
        "In order to facilitate a controlled comparison across the individual voting models, the same feature set was employed for all classifiers.",
        "The features are as described by Yarowsky and Florian (2002) in their “feature-enhanced naive Bayes model”, with position-sensitive, syntactic, and local collocational features."
      ]
    },
    {
      "heading": "2.4 The KPCA-based WSD model",
      "text": [
        "We briefly summarize the KPCA-based model here; for full details including illustrative examples and graphical interpretation, please refer to Wu et al.",
        "(2004).",
        "Kernel PCA Kernel Principal Component Analysis is a nonlinear kernel method for extracting nonlinear principal components from vector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed, yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors (Sch¨olkopf et al., 1998).",
        "As with other kernel methods, a major advantage of KPCA over other common analysis techniques is that it can inherently take combinations of predictive features into account when optimizing dimensionality reduction.",
        "For WSD and indeed many natural language tasks, significant accuracy gains can often be achieved by generalizing over relevant feature combinations (see, e.g., Kudo and Matsumoto (2003)).",
        "A further advantage of KPCA in the context of the WSD problem is that the dimensionality of the input data is generally very large, a condition where kernel methods excel.",
        "Nonlinear principal components (Diamantaras and Kung, 1996) are defined as follows.",
        "Suppose we are given a training set of M pairs (xt, ct) where the observed vectors xt E Rn in an n-dimensional input space X represent the context of the target word being disambiguated, and the correct class ct represents the sense of the word, for t = 1,.., M. Suppose b is a nonlinear mapping from the input space Rn to the feature space F. Without loss of generality we assume the M vectors are centered vectors in the feature space, i.e., �Mt=1 b (xt) = 0; uncentered vectors can easily be converted to centered vectors (Sch¨olkopf et al., 1998).",
        "We wish to diagonalize the covariance matrix in F:",
        "To do this requires solving the equation Av = Cv for eigenvalues A > 0 and eigenvectors v E Rn\\ {0}.",
        "Because",
        "ˆA2 > ... > ˆAM denote the eigenvalues of Kˆ and ˆa1 ,..., ˆaM denote the corresponding complete set of normalized eigenvectors, such that ˆAt (ˆat - ˆat) = 1 when ˆAt > 0.",
        "Then the lth nonlinear principal component of any test vector xt is defined as",
        "where ˆali is the lth element of ˆal .",
        "See Wu et al.",
        "(2004) for a possible geometric interpretation of the power of the nonlinearity.",
        "WSD using KPCA In order to extract nonlinear principal components efficiently, first note that in both Equations (5) and (6) the explicit form of b (xi) is required only in the form of (-b (xi) - b (xj )), i.e., the dot product of vectors in F. This means that we can calculate the nonlinear principal components by substituting a kernel function k(xi, xj) for (-b( xi) - -b(xj )) in Equations (5) and (6) without knowing the mapping b explicitly; instead, the mapping b is implicitly defined by the kernel function.",
        "It is always possible to construct a mapping into a space where k acts as a dot product so long as k is a continuous kernel of a positive integral operator (Sch¨olkopf et al., 1998).",
        "Thus we train the KPCA model using the following algorithm:",
        "1.",
        "Compute an M x M matrix Kˆ such that ˆKij = k(xi, xj) (7) 2.",
        "Compute the eigenvalues and eigenvectors of matrix Kˆ and normalize the eigenvectors.",
        "Let ˆA1 > ˆA2 > ... > ˆAM denote the eigenvalues and ˆa1,..., ˆaM denote the corresponding complete set of normalized eigenvectors.",
        "To obtain the sense predictions for test instances, we need only transform the corresponding vectors using the trained KPCA model and classify the resultant vectors using nearest neighbors.",
        "For a given test instance vector x, its lth nonlinear principal component is",
        "where ˆali is the ith element of ˆal.",
        "For our disambiguation experiments we employ a polynomial kernel function of the form k(xi, xj) = (xi - xj )d, although other kernel functions such as gaussians could be used as well.",
        "Note that the degenerate case of d = 1 yields the dot product kernel k(xi, xj) = (xi-xj) which covers linear PCA as a special case, which may explain why KPCA always outperforms PCA."
      ]
    },
    {
      "heading": "3 Results and discussion",
      "text": []
    },
    {
      "heading": "3.1 Accuracy",
      "text": [
        "Table 1 summarizes the results of the submitted systems along with the individual voting models.",
        "Since our models attempted to disambiguate all test instances, we report accuracy (precision and recall being equal).",
        "Earlier experiments on Senseval-2 data showed that the KPCA-based model significantly outperformed both naive Bayes and maximum entropy models (Wu et al., 2004).",
        "On the Senseval3 data, the maximum entropy model fares slightly better: it remains significantly worse on the Multilingual (ts) task, but achieves statistically the same accuracy on the English (fine) task and is slightly",
        "and let",
        "more accurate on the Multilingual (t) task.",
        "For unknown reasons – possibly the very small number of training instances per Chinese target word, as mentioned earlier – there is an exception on the Chinese task, where boosting outperforms the KPCA-based model.",
        "We are investigating the possible causes.",
        "The naive Bayes model remains significantly worse under all conditions."
      ]
    },
    {
      "heading": "3.2 Differentiated voting bias",
      "text": [
        "For a new voting model to raise the accuracy of an existing classifier ensemble, it is not only important that the new voting model achieve accuracy comparable to the other voters, as shown above, but also that it provides a significantly differentiated prediction bias than the other voters.",
        "Otherwise, the accuracy is typically hurt rather than helped by the new voting model.",
        "To examine whether the KPCA-based model satisfies this requirement, we compared its predictions against each of the other classifiers (for those tasks where we have been given the answer key).",
        "Table 2 shows nine confusion matrices revealing the percentage of instances where the KPCA-based model votes differently from one of the other voters.",
        "The disagreement between KPCA and the other voting models ranges from 6.03% to 14.63%, as shown by the bold entries in the confusion matrices.",
        "Note that where there is disagreement, the KPCA-based model predicts the correct sense with significantly higher accuracy, in nearly all cases."
      ]
    },
    {
      "heading": "3.3 Voting effectiveness",
      "text": [
        "The KPCA-based model exhibits the accuracy and differentiation characteristics requisite for an effective additional voter, as shown in the foregoing sec",
        "tions.",
        "To verify that adding the KPCA-based model to the voting ensemble indeed improves accuracy, we compared our voting ensemble’s accuracies to that obtained with KPCA removed.",
        "The results, shown in Table 3, confirm that the KPCA-based model generally helps on Senseval-3 Lexical Sample tasks.",
        "The only exception is on Chinese, due to the aforementioned anomaly of boosting outperforming KPCA on that task.",
        "In the Multilingual (t) and (ts) cases, the improvement in accuracy is significant."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "We have described our word sense disambiguation system and its performance on the Senseval-3 English, Chinese, and Multilingual Lexical Sample tasks.",
        "The system consists of an ensemble classifier utilizing combinations of maximum entropy, boosting, na¨ıve Bayes, and a new Kernel PCA based model.",
        "We have demonstrated that our new model based on Kernel PCA is, along with maximum entropy, one of the most accurate standalone models voting in the ensemble, as evaluated under carefully controlled to ensure the same optimized feature set across all models being compared.",
        "Moreover, we have shown that the KPCA model exhibits a significantly different classification bias, a characteristic that makes it a valuable voter in an ensemble.",
        "The results confirm that accuracy is generally improved by the addition of the KPCA-based model."
      ]
    }
  ]
}
