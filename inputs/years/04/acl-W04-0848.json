{
  "info": {
    "authors": [
      "Adrian Novischi",
      "Dan Moldovan",
      "Paul Parker",
      "Adriana Badulescu",
      "Bob Hauser"
    ],
    "book": "SENSEVAL International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",
    "id": "acl-W04-0848",
    "title": "LCC's WSD Systems for Senseval-3",
    "url": "https://aclweb.org/anthology/W04-0848",
    "year": 2004
  },
  "references": [
    "acl-C02-1039",
    "acl-H94-1046",
    "acl-P97-1003",
    "acl-W95-0101",
    "acl-W99-0501"
  ],
  "sections": [
    {
      "text": [
        "SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics SemCor next word - the same as the previous heuristic but this heuristic forms a pair from the target word in a gloss and the next word and searches for this pair in the SemCor corpus.",
        "If the target word has the same sense in all occurrences of the pair, then the heuristic assigns this sense to the target word.",
        "Cross reference - given an ambiguous word W in the synset S, this method looks for a reference to the synset S in all the glosses corresponding to word Wï¿½s senses.",
        "By reference to a word W we denote a word or a part of a compound concept that has the same lemma as the word W. Reversed cross reference - given a word W in the gloss G of the synset S, this method assigns to the word W the sense that contains in its set of synonyms one of the words from the gloss G. Distance among glosses - determines the number of common word lemmas between two glosses.",
        "For an ambiguous word Win a gloss G this method selects the sense with the greatest number of common word lemmas with the gloss G. Common Domain - using the domain labels assigned by (Magnini and Cavaglia, 2000) this method selects the sense of a word in a gloss that has the same domain as the synset of the gloss.",
        "Patterns - exploits the idiosyncratic nature of the WordNet glosses.",
        "The patterns of the form \"N successive words\" and \"M words ... N words\" are extracted offline from the WordNet glosses and manually disambiguated.",
        "This method matches the patterns against glosses and assigns to the words the corresponding sense in the pattern.",
        "First Sense Restricted - this method assigns sense 1 to a noun or verb if this sense has the smallest number of ancestors in the ISA hierarchy from all senses (it is the most general sense).",
        "The method selects sense 1 for an adjective if this sense has the greatest number of similarity pointers compared to all the other senses.",
        "The disambiguation methods in this task can be classified in three types described below: methods based on heuristics, machine learning disambiguation methods, and incremental disambiguation methods.",
        "Heuristics In this category the methods can be further classified in methods that use hand coded rules, methods that use WordNet and methods that use SemCor (Miller et al., 1994).",
        "Some methods applied to Semantic Disambiguation of WordNet glosses were also applied on open text: Semcor Previous Word, Semcor Next Word, Patterns and Lexical Parallelism."
      ]
    },
    {
      "heading": "2.2.2 Machine Learning Disambiguation methods",
      "text": [
        "We used Support Vector Machines (Cortes and Vapnik, 1995) for disambiguating verbs, adjectives, and adverbs, and C5.0 (Quinlan, 2003) rules for disambiguating nouns.",
        "Support Vector Machines (SVM) Method We used the following set of features (Mi-halcea, 2002): current word form and part of speech, contextual features, collocations in a window of (-3,3) words, and keywords and bi-grams in a window of (-3,3) sentences.",
        "For disambiguating verbs we used an additional set of features: Verb mode (which can take 4 values: ACTIVE, INFINITIVE, PAST, GERUND), verb voice (which can take 2 values ACTIVE, PASSIVE), the parent of the current verb in the parse tree (ex: VP, NP), The first ancestor that is not VP in the parse tree (like S, NP, PP, SBAR).",
        "We generated feature values using the Sense-val 2 Lexical Sample training corpus and SemCor corpus combined, and we trained the SVM classifier on those words that had at least 10 training examples."
      ]
    },
    {
      "heading": "C5.0 Rules Method",
      "text": [
        "The second machine learning method, used for word sense disambiguating nouns, is the C5.0 decision tree learning (Quinlan, 2003).",
        "For generating training examples we used the features described in (Mihalcea and Moldovan, 2001b).",
        "We used the SemCor 1.7.1 collection and Senseval 2 All Words collection as training corpus.",
        "The main idea of incremental disambiguation is to disambiguate new words using senses of the previous disambiguated words.",
        "We used some of the procedures presented in (Mihalcea and Moldovan, 2001b) briefly described below: WordNet distance 0 with disambiguated words - find words that are in the same synset with already disambiguated words.",
        "WordNet distance 1 with disambiguated words - find words that are in a hypernymy/hyponymy relation with the words already disambiguated."
      ]
    },
    {
      "heading": "3 Combining Methods using Rules",
      "text": [
        "At this point each WSD system has a pool of disambiguation methods.",
        "An approach using rules for selecting the right sense was described in (Novischi, 2004) and is summarized below.",
        "For a given disambiguated word we create a training example for each of its senses.",
        "Given a manually-disambiguated corpus, training examples are generated for each sense of the tagged words with their correct classification.",
        "Then we can train a machine learning algorithm on this set of training examples.If we use C4.5 or C5.0, where each rule has an associated accuracy value, we can output the single sense that is classified as CORRECT by the rule with the best accuracy.",
        "For Semantic disambiguation of WordNet glosses task, we used a set of rules given by C4.5 program trained on a set of training examples generated from 3196 goldstandard glosses.",
        "For the English All Words task, however, we created the rules manually, each rule corresponding to a disambiguation method and assigned an accuracy value equal to the precision of the disambiguation method.",
        "The rules were tested in the decreasing order of their accuracy."
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "We measured the accuraccy of the LCC WSD systems using precision, recall and coverage.",
        "Precision is defined as the number of correct disambiguated words over the number of attempted words.",
        "Recall is defined as the number of correct disambiguated words over the total number of words for disambiguation.",
        "Coverage is defined as the number of attempted words over the total number of words.",
        "4.1 English All Words In order to illustrate the performance of the system, we examine it from three different perspectives.",
        "First we ran the system without defaulting to first sense and computed the results excluding monosemous words (row a in table 1).",
        "Second we ran the system including defaulting to sense 1, but we still excluded monosemous words when we computed the results (row b in table 1).",
        "Third we included both the first sense for words that were not otherwise disambiguated by the system and monosemous words (row c in table 1).",
        "Table 1 presents the results given by the Python scorer script.",
        "Precision and recall in rows a and b do not include the monosemous words."
      ]
    },
    {
      "heading": "4.2 Semantic Disambiguation of WordNet glosses",
      "text": [
        "In order to illustrate the performance of the system, we examined it from two different perspectives.",
        "First we ran the system including assigning sense 1 for words that were not otherwise disambiguated by the system (row a. in table 2).",
        "Second we ran the system without assigning sense 1 to words that were not otherwise disambiguated by the system (row b. in table 2)."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "The LCC WSD systems for Senseval 3 employ sets of disambiguation methods.",
        "Some methods are based on hand coded rules, some use information in WordNet, and some exploit the statistics of the SemCor corpus.",
        "Others use the senses of previous disambiguated words and still others are based on supervised machine learning algorithms.",
        "All these methods are combined using rules that optimize their output.",
        "The way the WSD systems are designed using multiple methods for disambiguation ensures a greater coverage than a single method by itself.",
        "Combining methods using machine learning algorithms improves the precision of the system."
      ]
    }
  ]
}
