{
  "info": {
    "authors": [
      "Judita Preiss"
    ],
    "book": "SENSEVAL International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",
    "id": "acl-W04-0852",
    "title": "Probabilistic WSD in Senseval-3",
    "url": "https://aclweb.org/anthology/W04-0852",
    "year": 2004
  },
  "references": [
    "acl-A94-1009",
    "acl-J01-3001",
    "acl-W04-0807",
    "acl-W04-0811"
  ],
  "sections": [
    {
      "text": [
        "lustration, we present three frequency distributions from the pos0 module for the word shirt (P(pos0 = NN11shirti), P(pos0 = NN21shirti), and P(pos0 = ��D1shirtq)) in Table 2.",
        "In this table, f (sense n pos) denotes the number of occurrences of w in the given sense with the given PoS tag, and f (sense) is the number of occurrences of shirt in the given sense.",
        "The probability distributions produced by the modules need to be smoothed.",
        "We use Lid-stone's smoothing (e.g., (Manning and Schu�t�e, 1999)), where the optimum smoothing values are empirically determined on a development corpus by an exhaustive search.",
        "Using Lid-stone's smoothing, we can make the smoothing values word and module specific, and so can make the probability distributions generated resemble uniform distributions if we are not very confident in the module for a given word.",
        "The probability distributions produced by the 26 modules are combined using Bayes Rule:"
      ]
    },
    {
      "heading": "P(A n B) 3 Results",
      "text": [
        "There were three versions of the probabilistic WSD system submitted to the English lexical sample (ELS) task (Mihalcea et al., 2004), and to the English all words (EAW) task (Palmer, 2004).",
        "The descriptions of the systems and their training data can be found in Table 1.",
        "For the English all words task, the system was trained on SEMCOR 1.6 converted into 1.7.1 using (a heuristics based) automatic mapping method.",
        "Although the output of our probabilistic system is a probability distribution on senses, this was converted a one sense assignment per instance for evaluation.2 For the English lexical sample task, the \"U\" (unassignable) tag was output whenever our system gave the highest probability to none of the available senses being relevant.",
        "The system also occasionally entirely missed annotating words due to combined errors arising from the morphological decomposition component and the tagger, these were also given the \"U\" tag, resulting in 100% coverage.",
        "In the English all words task, the system always found an available sense.",
        "The lower coverage (97.4%) was due to the errors from the morphological and tagger components.",
        "The official system performances can be found in Table 3."
      ]
    },
    {
      "heading": "4 Discussion",
      "text": [
        "Both Prob2 and Prob5 systems were investigating whether a lower number of modules would yield better performance; when a large number of modules are combined, the difference in probabilities of senses can become quite small.",
        "However, the combination chosen3 was only optimal for the English all words tasks.",
        "Subsequent",
        "The prior distribution comes from the unsupervised PoS and frequency modules, and this is augmented using the remaining modules to produce the best updated estimate in the form of a posterior distribution.",
        "Combining modules using Bayes Rule is the best combination method that was tested, and outperforms the natural combination method based on Dempster – Shafer theory.",
        "no.",
        "of occurrences of w in sense si",
        "to the evaluation taking place, we ran a search through the possible module combinations resulting in a better performance with a combination of the most frequent sense modules, the trigram modules, the window module, and the root of word 3 to the left, 1 to the right and 2 to the right.",
        "The `without U' performance for the English all words task with this module combination is 59.5% precision and 58.0% recall."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We have presented the results of our probabilistic WSD system on the SENSEVAL-3 English lexical sample and the English all words tasks.",
        "In both cases, our system outperformed the baseline for the task.'",
        "We have shown that the system can be further optimized, but even in its raw form it performs well.",
        "task."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "I would like to thank my supervisor, Ted Briscoe, and Joe Hurd for proof reading previous versions of this paper."
      ]
    }
  ]
}
