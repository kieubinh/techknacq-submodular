{
  "info": {
    "authors": [
      "Harold Paredes-Frigolett"
    ],
    "book": "Workshop on Text Meaning and Interpretation",
    "id": "acl-W04-0901",
    "title": "Interpretation in a Cognitive Architecture",
    "url": "https://aclweb.org/anthology/W04-0901",
    "year": 2004
  },
  "references": [
    "acl-P92-1030"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The work reported in this article presents a computational model of interpretation.",
        "The model proposes a cognitive architecture forintelligent agents to reason about competing analyses during interpretation and leverages the positive reinforcement principle."
      ]
    },
    {
      "heading": "1 Motivation",
      "text": [
        "Interpretation of natural language involve the computational effort associated with repeatedly computing, interpreting and deindexinglogical forms for ambiguous parses.",
        "In our viewinterpretation can be construed as a negotiation process wherebylex-ical, structural, semantic, common-sense andworld knowledge information and referential context are used to assign plausibilitiesto competing analyses.",
        "The approach to interpretationtaken here has been motivated by cognitive architectures for intelligent agents in the tradition of SOAR (Laird, Newell, and Rosenbloom, 1987 Laird,1991), ACT-R (Anderson, 1993) and ICARUS (Langley et al., 2003).",
        "In extending cognitive architectures in this tradition to deal with the problem of interpretation, agents carry both the \"meaning\" of competing analyses and the plausibilities associated withthem, which we construe as the reward function ofthe agents.",
        "As more information becomes available from the input string, reward functions are updated and the analysis with the higher plausibility becomes the preferred interpretation.",
        "2 The grammar formalism Consider sentence (1):",
        "(1) The 1a spy 1b watched 1, the 1d cop 1o with 1f the 1g revolver 1h.",
        "1i",
        "The GPSG-like grammar fragment withsemantic annotations in EPISODIC LOGIC (EL), a seman"
      ]
    },
    {
      "heading": "3 Semantic analysis",
      "text": [
        "For sentence (1) the parser computestwoinitial analyses using not only structural, but alsosubcate-gorization and thematic role information assoon as the verb is encountered.",
        "At point g, we could be already predicting several of the possible continuations.",
        "Based onsubcatego-rization and thematic role information forthe verb watch, there is a first analysis that results from applying rule N of our GPSG-like grammar fragment.",
        "At point g, the first analysis is APTT.. 'We refer the reader to (Hwang and Schubert,1993)for",
        "We show here analysis APTT9 and leave APTT09, the analysis in which the prepositional phrase attaches lower to the second NPtothe reader.",
        "As we will see, the cognitive architecture allows structural information, lexicalfactors andse-mantic biases to be used for on-line pruning ofalternative parses, thus keeping thelid onthe explosion of alternatives, and allowing human-likeparsingbehavior.2 For sentence (1) at point g, the semantic interpreter uses the semantic annotations associated with each syntactic rule in the grammar and applies compositional semantic interpretation rulesto come up with the parameterized unscopedlogical form PULF9.",
        "We assume that (i) salient referentsinthe current discourse take wide scope over all other operators in the logical form and that they are scoped within speech act operators, (ii) tense operators aresen-tential operators scope within speech act operators and salient referents, (iii)tense operatorstake wide scope over nonsalient definites, (iv) nonsalientdef-inites take wider scope over operators, and finally (v) existentials are scoped within all other operators in the logical form.",
        "If we propose semantic representationsthat are complete for the partial parse trees duringincre-mental processing, we can use a slightly different version of the algorithm developed by Hwang and Schubert for incremental deindexing.",
        "The incremental deindexer yieldsthe parameterized episodic logical form PELF9.",
        "The relation orients introduced in PELF9 corresponds to a relation to be further particularizedto a temporal, causal or part-of relation between situations.",
        "Nowl corresponds to a term that referstothe speech time of the utterance Speaker and Hearer stand for the speaker and thehearer ofthe utterance, respectively.",
        "eo corresponds to a prior episode described by the utterance situation uo.",
        "uo immediately precedes the utterance situation ul.",
        "el is the situation being described by ul and occurs at about the same time as ul.",
        "That is a sentence nominal-ization operator that takes a sentence as argument and gives rise to a proposition-denotingterm.",
        "The expression [[x I el] (with-instr) z] corresponds to the action of the spy's watching modifiedso astobe performed with something.",
        "The function I is a pairing function applicable to individuals andtuples.",
        "Thus [x I el] is the action performed by x that gives rise to event el.",
        "The operator � is a metalogical operator that corresponds to the operator coextensive-part-of in EL.",
        "The expression e2 � el indicates that situation e2 is coextensive with situation el, that is, eland e2 have the same spatiotemporal location.",
        "Once the parameterized episodiclogical form has been generated, the incremental deindexertrans-forms the lambda expressions that abstract overthe parameters introduced by theincrementalseman-tic interpreter into episodic logical forms.",
        "Tothis end, constants are introducedforthe metalogicalpa-rameters.",
        "These constants stand for parameterized terms and predicates in the resulting episodiclogical form.",
        "A-conversion is then performed for each one of the A-expressions in the parameterized episodic logical form.",
        "Applying this procedure, theincremental deindexer yields the episodic logical form SLT_q.",
        "Applying the same procedure forthe competing analysis, we obtain the following episodiclogical form ELTI_q.",
        "World knowledge in EPILOG, EL's implementation (Schaeffer et al., 1991), is expressed in form of unreliable generalizations using probabilistic conditionals of form 0 – �p,al,,,,,an V, where a✶; ... ;an are controlled variables and p is a statistical probability (Bacchus, 1990) Different choices of controlled variables lead lead to different readings.",
        "An axiom of the form 0 – �p V says that in at least (100) x p % of the situations in which 0 is true, V will also be true.",
        "It is assumed thatin axioms of the form ... the list of controlled variables includes all existentially quantified variables in the antecedent that occur anaphoricallyinthe consequent.",
        "4.1 Meaning postulates about unlocated formulas",
        "MP 1: (del [[[0 n i/)] ** el] _4 [0 n (Ie2:[e2 i el][ ** e2])]]) 4.2 Meaning postulates about seeing objects MP 2: If a person watches a thing, thenthatper-son sees that thing.",
        "4.3 World knowledge axioms about seeing objects WK 1: If someone sees something with a viewing instrument, then she/he probably seesit clearly."
      ]
    },
    {
      "heading": "5 The cognitive architecture",
      "text": [
        "The model of incremental semanticinterpretation, scoping, and deindexing describedinthe previous sections enables us to transform a partially annotated parse tree into an episodic logical formsuitable for inference.",
        "Using the procedure above, we are in a position to integrate syntactic andsemantic information, referential context, and worldknowl-edge in the calculation of plausibility for each analysis.",
        "A naive approach to incrementalinterpretation would consist in exploiting this modelto arrive at an episodic logical form and then consider all alternative equally plausible.",
        "We regardthis alternative as implausible on the grounds of psycholinguistic results on control processes ofinferencein on-line text comprehension (Balota, Flores dArcais, and Rayner, 1990).",
        "As we will see,the model ofincre-mental interpretation proposedis based on a multi-agent cognitive architecture in which agents are assigned competing interpretations.",
        "Central to the architecture isthe concept ofagent reward.",
        "A reward functionis calculated for each agent, each one of them representing an analysis.In general, the alignment between user utilityfunction and agent reward function is one ofthe areasthatisis domain-dependent in this architecture.",
        "We will explore this alignment for our domaininthefollowing sections."
      ]
    },
    {
      "heading": "5.1 Value alignment",
      "text": [
        "In general, the objective of each agentisto maximize its reward function.",
        "How wellthey optimize the user utility function will depend onthe alignment between the user utility function andthe agent reward function.",
        "Notice that, in our domain,there isis a disconnect between the objectives of the agent andthose ofthe user, respectively.",
        "The agent commitsto an analysis and in so doing its fate is already sealed.",
        "How well they end up optimizing the user utilityfunction will depend on variables the agent can only partially control as this process ultimately depends onthein-formation not yet absorbed fromtheinputstring and on the behavior of the other agentsinthe architecture.",
        "The interesting point to note here isthat although this might at first glance appear as an undesirable feature of the architecture, it actuallyleadsto a model in which the different agents cooperateto-wards the ultimate goal of optimizingthe user utility function.",
        "Thus, instead of competing analyses we might as well refer to them as cooperating analyses."
      ]
    },
    {
      "heading": "5.2 Agent reward function",
      "text": [
        "Reward functions for the agents are definedbased on the principle of positive reinforcement.",
        "An analysis is preferred over anothertothe extent that it satisfies the constraints ofitsimmediateref-erential context and to the extentthattheinferences triggered in the knowledgebase are more consistent, more specific and more numerous.",
        "Based on the principle, our model usesthe following sets of heuristics for assigning a reward function to agents:",
        "1.",
        "Give referential context highest precedence; 2.",
        "Give consistency of inferences drawninthe knowledge base precedence over specificity of inferences drawn in the knowledgebase; 3.",
        "Give specificity of inferences drawninthe knowledge base preference over subcatego-rization information; 4.",
        "Give subcategorizationinformation precedence over the amount of inferences drawnin the knowledge base, and 5.",
        "Consider only inferences with a minimum",
        "level of “interestingness”4 The list above is not exhaustive, butit gives us an initial set of heuristics to definethe reward function for the agents.",
        "The choice of some precedences in the heuristics above has been psycholinguisti-cally motivated, as shown in (Altmann and Steed-man, 1988).5 The approach to interpretation followed here is based on the assumptionthatinformation from different sources enterstheinterpretation process at different times and that they concurrently restrain the number of potential analyses, assug-gested in recent psycholinguistictheories ofhuman sentence comprehension (Spivey-Knowlton andSe-divy, 1995).",
        "5.3 Interpretation as learning The process of finding a preferredinterpretation at a given time t, is the result of a process of entropy reversal through information expressedinterms of a set of heuristics that governthe agent rewardin this cognitive architecture The heuristics aboveare a distillation of the information requiredforthis entropy reversal process."
      ]
    },
    {
      "heading": "5.4 An example",
      "text": [
        "Let us illustrate the process of agent-basedinterpretation using our example When processingsentence (1) up to point g, we do have two analyses.",
        "Skolemizing E1/e1, E2/e2, E3/e3, E4/e4, E5/e5, U1/u1, X/y, Y/y and Z/z, the set of inferences drawn at point g is as follows:",
        "Facts Fl through F4 are directly obtained in the knowledge base by asserting SLT.",
        "after splitting conjunctions and top-level skolemizationisis performed on SLT.. El is a situation fully described by the action of the spy watching the cop andbeing modified so as to be performed with \"something.\" Facts F5 and F6 are directly obtained by meaning postulate MP 1.",
        "E2, coextensive with El, is fully described by the action of the spy watchingthe cop.",
        "MP 3 accounts for triggering fact F7, thus setting the expectation in Agentl's discourse model that the incoming referent is a viewinginstrument.",
        "Facts F8 and Fg are obtained using meaning postulate MP 2.",
        "E3, coextensive with El, is fully described by the action of the spy seeing the cop with \"something.\" Notice that using facts F7 and F9 and world knowledge axiom WK 1, we would also be setting the uncertain prediction that the spy sees the cop clearly For our second agent, we wouldhavethe following set of inferences in the knowledgebase:",
        "fully described by the action ofthe spy watching a cop carrying \"something\" 5.4.1 Positive reinforcement: Scenario 1 In this first scenario, we assumethatthe discourse model is initially empty.",
        "Applying our positivere-inforcement principle at point g, Agent] is the most plausible one.",
        "The verb to watch subcategorizes for an instrumental argument andthe analysis pursued by Agent] is initially preferred.",
        "This analysis also leads to more inferences in the knowledgebase,including the certain prediction thattheincoming NP introduces a viewing instrument in Agent1 s discourse model and the uncertain predictionthatthe spy sees the cop clearly.",
        "Referential context does not play a role yet since there were no discourseref-erents introduced initially underthisscenario.",
        "The analysis pursued up to point h by Agent] is shown in PPTh.",
        "Facts Fl to F5 are directly obtained in the knowl- ** el]))] edge base by asserting SLTI.. El is a situation ** u,]) After asserting SLTh and using type-hierarchical knowledge, the following additional facts can be triggered in Agentl's discourse model:",
        "Fact F7 is inconsistent with facts Flo through ❋ll above.",
        "Thus, by a process of \"hierarchy climbing\" in the knowledge base, it turns outthatthe variable z introduced in SLTh is not subsumed by a generic term denoting a viewinginstrumentinthe knowledge base, as expected at point g. By our positive reinforcement heuristics at point h, this analysis is not positively reinforced.",
        "It turns outtobeinconsis-tent with Agentl's prior referential context.",
        "Agent2's discourse model at point h has led to the following discourse model:",
        "Using our heuristics, the analysis preferred under this first scenario turns out to bethe one pursued by the second agent.",
        "Notice that the agent-based cognitive architecture will get \"garden-pathed\" as the analysis initially preferred onthe grounds of subcategorization information andspecificity and interestingness of the inferences drawninthe knowledge base proves anomalous by referential context."
      ]
    },
    {
      "heading": "5.4.2 Positive reinforcement: Scenario 2",
      "text": [
        "In this second scenario, we assumethe discourse model initially consisting of three referents, aspy and two cops.",
        "Applying our positive reinforcement principle at point g for Agent], we have that the verb to watch subcategorizes for an instrumental argument andthe analysis pursued by Agent] would be initially preferred based on subcategorizationinformationfor this verb.",
        "Agent] would also get positively reinforced since its interpretationleadsto moreinferences in the knowledge base andto makethe certain prediction that the incoming NPintroduces a viewing instrument andthe uncertain prediction that the spy sees the cop clearly in Agentl's discourse model, as we have already seen.",
        "But according to our set of heuristics, referential context preempts these preferences, as the needto resolvethe anaphoric reference the cop in the discourse model immediately preceding sentence (1)takes precedence over the other criteria at point g. The analysis pursued by Agent] does not contribute to resolving this anaphoric reference.",
        "On the other hand, the analysis pursued by Agent2 at point g raises the expectation that the cop will be further \"particularized\" so asto resolvethis anaphoric reference.",
        "Giventhe heuristics,thisexpectation takes precedence over Agentl's interpretation and is preferred in this cognitive architecture.",
        "It is interesting to note that inthis secondscenario, subjects are not being led down the garden path when given the referential contextin whichthe need for resolving the anaphoric referenceintroducedby the second NP arses at point g. Our model predicts this behavior accordingly At point i, the analysis pursued by Agent 2 is the preferred one.",
        "The resulting episodiclogical form at point i is SLTi, as shown below."
      ]
    },
    {
      "heading": "6 Discussion",
      "text": [
        "The work presented in this article puts forth an approach to interpretation using a cognitive architecture for intelligent behavior Our work hasso far consisted in defining agent rewardbased onthepos-itive reinforcement principle Fortheinitialimplementation of the principle, we have followed a heuristics-based approach.",
        "Though some of the information usedinthe plausibility computation is probabilistic (Bacchus, 1990), the heuristics are non-probabilisticin our model.",
        "In defining the heuristics, we haveincor-porated recent results in psycholinguisticstudies of human sentence processing In our view ofthe interpretation process, agents contributeto arriving at a \"preferred\" interpretation by maintaining a \"more plausible\" analysis – andits associated discourse model – as the most salient one, while other less plausible analyses are kept in memory for a given period of time by other agents.",
        "By a process of heuristics-based plausibility computation, the \"most plausible analysis\" remain activeinthis architecture and take the lead duringtheinterpretation process.",
        "This cognitive architecture gives a plausible account of some of the issues that pervadehumansentence processing such as garden-path phenomena.",
        "In so doing, we depart from serial first-analysis approaches to sentence comprehensioninthetradition of the garden-path theory of sentence processing (Frazier and Fodor, 1978; Frazier andClifton,1996) and endorse more recent psycholinguistic accounts of this problem which view theinterpretation process as a concurrent negotiation ofinformationfrom syntactic, semantic and pragmatic sources bysev-eral agents (Spivey-Knowlton and Sedivy1995).",
        "We also aim to bridge the gap between models of interpretation in the tradition ofthe garden-path theory, which are related to symbolic approachesto NLP, and subsymbolic approachesinthetraditionof parallel theories of sentence processing.",
        "Ourmodel benefits from the \"niceties\" ofthe former approach to arrive at semantic and knowledge representations for alternative analyses while also leveraging acog-nitive architecture that is suitedtoimplement aparallel approach to interpretation."
      ]
    },
    {
      "heading": "7 Future work",
      "text": [
        "Our future work will focus on studyingtherole agents will have in learning or refining newheuris-tics.",
        "As a matter of fact, we believethatthe architecture is well-suited to minethe context-sensitive information that makes an analysis more plausible than another in a given discourse situation.",
        "Wesee this as a machine learning process by which agents contribute to the common goal of \"entropy rever-sal\" by learning new heuristics and applyingthem during the incremental interpretation process.",
        "Another aspect we will be focusing onin future work is a process we call \"signaling,\" which we shall illustrate using sentence (2): (2) Every ten minutes a man gets muggedinNewYork.",
        "Based on our interpretation algorithm,the most plausible analysis would be the one withthe following representation.",
        "In the absence of any referential contextthat might indicate otherwise, our model does assign narrow scope to the existentially quantified expressionin-troducing \"a man\" in the discourse model.",
        "Applying the heuristics, Agent] carries the most plausible interpretation in which there is an episode e consisting of a collection of periodically, non-overlapping subepisodes e/, each one of them introducing a different individual getting muggedinNewYork, none of whom is salient in the immediate referential context in which sentence (2) is uttered.",
        "Suppose that this fragment continues withsen-tence (3):",
        "(3) We are in New York today tointerviewhim.",
        "As a result, our cognitive architecture gets \"junglepathed\" after processing sentence (3) Inthis case, Agent] is forced to come up with a single salient referent in its discourse model, correspondingtothe poor individual who gets mugged everyten minutes in New York.",
        "Agent] is unable to provide such a referent.",
        "By a process of signaling, agents cannot onlybe leveraged to keep a given analysis and corresponding interpretation active in memory for a given period of time, but also to \"send\" information,including referents, to other agents that might \"request\" this information during their owninterpretationpro-cess.",
        "We will be studying howthissignalingprocess can be used to resolve anaphoric references and ensure discourse coherence.",
        "Our approach will consist in implementing Schubert's dynamicskolemization mechanism using this cognitive architecture."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Many thanks to Pat Langley for discussions on earlier versions of this paper,to Dan Shapiro for discussions on value alignmenttheory andto Len Schubert for his continuous supportteaching me EL."
      ]
    }
  ]
}
