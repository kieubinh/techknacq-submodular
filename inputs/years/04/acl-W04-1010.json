{
  "info": {
    "authors": [
      "Liang Zhou",
      "Eduard Hovy"
    ],
    "book": "Workshop on Text Summarization Branches Out",
    "id": "acl-W04-1010",
    "title": "Template-Filtered Headline Summarization",
    "url": "https://aclweb.org/anthology/W04-1010",
    "year": 2004
  },
  "references": [
    "acl-N03-1020",
    "acl-P00-1041",
    "acl-W03-0501"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Headline summarization is a difficult task because it requires maximizing text content in short summary length while maintaining grammaticality.",
        "This paper describes our first attempt toward solving this problem with a system that generates key headline clusters and fine-tunes them using templates."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Producing headline-length summaries is a challenging summarization problem.",
        "Every word becomes important.",
        "But the need for grammaticality – or at least intelligibility – sometimes requires the inclusion of non-content words.",
        "Forgoing grammaticality, one might compose a “headline” summary by simply listing the most important noun phrases one after another.",
        "At the other extreme, one might pick just one fairly indicative sentence of appropriate length, ignoring all other material.",
        "Ideally, we want to find a balance between including raw information and supporting intelligibility.",
        "We experimented with methods that integrate content-based and form-based criteria.",
        "The process consists two phases.",
        "The keyword-clustering component finds headline phrases in the beginning of the text using a list of globally selected keywords.",
        "The template filter then uses a collection of pre-specified headline templates and subsequently populates them with headline phrases to produce the resulting headline.",
        "In this paper, we describe in Section 2 previous work.",
        "Section 3 describes a study on the use of headline templates.",
        "A discussion on the process of selecting and expanding key headline phrases is in Section 4.",
        "And Section 5 goes back to the idea of templates but with the help of headline phrases.",
        "Future work is discussed in Section 6."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Several previous systems were developed to address the need for headline-style summaries.",
        "A lossy summarizer that ‘translates’ news stories into target summaries using the ‘IBM-style’ statistical machine translation (MT) model was shown in (Banko, et al., 2000).",
        "Conditional probabilities for a limited vocabulary and bigram transition probabilities as headline syntax approximation were incorporated into the translation model.",
        "It was shown to have worked surprisingly well with a standalone evaluation of quantitative analysis on content coverage.",
        "The use of a noisy-channel model and a Viterbi search was shown in another MT-inspired headline summarization system (Zajic, et al., 2002).",
        "The method was automatically evaluated by BiLingual Evaluation Understudy (Bleu) (Papineni, et al., 2001) and scored 0.1886 with its limited length model.",
        "A nonstatistical system, coupled with linguistically motivated heuristics, using a parse-and-trim approach based on parse trees was reported in (Dorr, et al., 2003).",
        "It achieved 0.1341 on Bleu with an average of 8.5 words.",
        "Even though human evaluations were conducted in the past, we still do not have sufficient material to perform a comprehensive comparative evaluation on a large enough scale to claim that one method is superior to others."
      ]
    },
    {
      "heading": "3 First Look at the Headline Templates",
      "text": [
        "It is difficult to formulate a rule set that defines how headlines are written.",
        "However, we may discover how headlines are related to the templates derived from them using a training set of 60933 (headline, text) pairs.",
        "We view each headline in our training corpus as a potential template.",
        "For any new text(s), if we can select an appropriate template from the set and fill it with content words, then we will have a well-structured headline.",
        "An abstract representation of the templates suitable for matching against new material is required.",
        "In our current work, we build templates at the part-of-speech (POS) level."
      ]
    },
    {
      "heading": "3.2 Sequential Recognition of Templates",
      "text": [
        "We tested how well headline templates overlap with the opening sentences of texts by matching POS tags sequentially.",
        "The second column of Table 1 shows the percentage of files whose POS-level headline words appeared sequentially within the context described in the first column."
      ]
    },
    {
      "heading": "3.3 Filling Templates with Key Words",
      "text": [
        "Filling POS templates sequentially using tagging information alone is obviously not the most appropriate way to demonstrate the concept of headline summarization using template abstraction, since it completely ignores the semantic information carried by words themselves.",
        "Therefore, using the same set of POS headline templates, we modified the filling procedure.",
        "Given a new text, each word (not a stop word) is categorized by its POS tag and ranked within each POS category according to its tf.idf weight.",
        "A word with the highest tf.dif weight from that POS category is chosen to fill each placeholder in a template.",
        "If the same tag appears more than once in the template, a subsequent placeholder is filled with a word whose weight is the next highest from the same tag category.",
        "The score for each filled template is calculated as follows: where score _t(i) denotes the final score assigned to template i of up to N placeholders and W; is the tf.idf weight of the word assigned to a placeholder in the template.",
        "This scoring mechanism prefers templates with the most desirable length.",
        "The highest scoring template-filled headline is chosen as the result."
      ]
    },
    {
      "heading": "4 Key Phrase Selection",
      "text": [
        "The headlines generated in Section 3 are grammatical (by virtue of the templates) and reflect some content (by virtue of the tf.idf scores).",
        "But there is no guarantee of semantic accuracy!",
        "This led us to the search of key phrases as the candidates for filling headline templates.",
        "Headline phrases should be expanded from single seed words that are important and uniquely reflect the contents of the text itself.",
        "To select the best seed words for key phrase expansion, we studied several keyword selection models, described below."
      ]
    },
    {
      "heading": "4. 1 Model Selection Bag-of-Words Models",
      "text": [
        "1) Sentence Position Model: Sentence position information has long proven useful in identifying topics of texts (Edmundson, 1969).",
        "We believe this idea also applies to the selection of headline words.",
        "Given a sentence with its position in text, what is the likelihood that it would contain the first appearance of a headline word:",
        "Over all M texts in the collection and over all words from the corresponding M headlines (each has up to N words), Count _Pos records the number of times that sentence position i has the first appearance of any headline word W;.",
        "P(Hk |W;) is a binary feature.",
        "This is computed for all sentence positions from 1 to Q.",
        "Resulting P(Posi) is a table on the tendency of each sentence position contain-ontain ing one or more headlines words (without indicating exact words).",
        "2) Headline Word Position Model: For each headline word Wh, it would most likely first appear at sentence position Posi:",
        "The difference between models 1 and 2 is that for the sentence position model, statistics were collected for each sentence position i; for the headline word position model, information was collected for each headline word Wh.",
        "3) Text Model: This model captures the correlation between words in text and words in headlines (Lin and Hauptmann, 2001):",
        "doc_tf(w,j) denotes the term frequency of word w in the jt4 document of all M documents in the collection.",
        "title_tf(w,j) is the term frequency of word w in the j t4 title.",
        "Hw and Tw are words that appear in both the headline and the text body.",
        "For each instance of Hw and Tw pair, Hw = Tw.",
        "4) Unigram Headline Model: Unigram probabilities on the headline words from the training set.",
        "5) Bigram Headline Model: Bigram probabilities on the headline words from the training set."
      ]
    },
    {
      "heading": "Choice on Model Combinations",
      "text": [
        "Having these five models, we needed to determine which model or model combination is best suited for headline word selection.",
        "The blind data was the DUC2001 test set of 108 texts.",
        "The reference headlines are the original headlines with a total of 808 words (not including stop words).",
        "The evaluation was based on the cumulative unigram overlap between the n top-scoring words and the reference headlines.",
        "The models are numbered as in Section 4.1.",
        "Table 2 shows the effectiveness of each model/model combination on the top 10, 20, 30, 40, and 50 scoring words.",
        "Clearly, for all lengths greater than 10, sentence position (model 1) plays the most important role in selecting headline words.",
        "Selecting the top 50 words solely based on position information means that sentences in the beginning of a text are the most informative.",
        "However, when we are working with a more restricted length requirement, text model (model 3) adds advantage to the position model (highlighted, 7th from the bottom of Table 2).",
        "As a result, the following combination of sentence position and text model was used:"
      ]
    },
    {
      "heading": "4.2 Phrase Candidates to Fill Templates",
      "text": [
        "Section 4.1 explained how we select headline-worthy words.",
        "We now need to expand them into phrases as candidates for filling templates.",
        "As illustrated in Table 2 and stated in (Zajic et al., 2002), headlines from newspaper texts mostly use words from the beginning of the text.",
        "Therefore, we search for n-gram phrases comprising keywords in the first part of the story.",
        "Using the model combination selected in Section 4.1, 10 top-scoring words over the whole story are selected and highlighted in the first 50 words of the text.",
        "The system should have the ability of pulling out the largest window of top-scoring words to form the headline.",
        "To help achieve grammaticality, we produced bigrams surrounding each headline-worthy word (underlined), as shown in Figure 1.",
        "From connecting overlapping bigrams in",
        "t; is a candidate template and h; is a headline phrase.",
        "The top-scoring template is used to filter each headline phrase in composing the final multi phrase headline.",
        "Table 3 shows a random selection of the results produced by the system.",
        "for decades has prided itself on a progressive attitude a reputation The death of that went awry, followed 10 days later by a scuffle between police and..."
      ]
    },
    {
      "heading": "5.2 Evaluation",
      "text": [
        "sequence, one sees interpretable clusters of words forming.",
        "Multiple headline phrases are considered as candidates for template filling.",
        "Using a set of handwritten rules, dangling words were removed from the beginning and end of each headline phrase."
      ]
    },
    {
      "heading": "5 Filling Templates with Phrases",
      "text": []
    },
    {
      "heading": "5.1 Method",
      "text": [
        "Key phrase clustering preserves text content, but lacks the complete and correct representation for structuring phrases.",
        "The phrases need to go through a grammar filter/reconstruction stage to gain grammaticality.",
        "A set of headline-worthy phrases with their corresponding POS tags is presented to the template filter.",
        "All templates in the collection are matched against each candidate headline phrase.",
        "Strict tag matching produces a small number of matching templates.",
        "To circumvent this problem, a more general tag-matching criterion, where tags belonging to the same part-of-speech category can be matched interchangeably, was used.",
        "Headline phrases tend to be longer than most of the templates in the collection.",
        "This results in only partial matches between the phrases and the templates.",
        "A score of fullness on the phrase-template match is computed for each candidate template ft;:",
        "Ideally, the evaluation should show the system’s performance on both content selection and grammaticality.",
        "However, it is hard to measure the level of grammaticality achieved by a system computationally.",
        "Similar to (Banko, et al., 2000), we restricted the evaluation to a quantitative analysis on content only.",
        "Our system was evaluated on previously unseen DUC2003 test data of 615 files.",
        "For each file, headlines generated at various lengths were compared against i) the original headline, and ii) headlines written by four DUC2003 human assessors.",
        "The performance metric was to count term overlaps between the generated headlines and the test standards.",
        "Table 4 shows the human agreement and the performance of the system comparing with the two test standards.",
        "P and R are the precision and recall scores.",
        "The system-generated headlines were also evaluated using the automatic summarization evaluation tool ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin and Hovy,",
        "2003).",
        "The ROUGE score is a measure of n-gram recall between candidate headlines and a set of reference headlines.",
        "Its simplicity and reliability are gaining audience and becoming a standard for performing automatic comparative summarization evaluation.",
        "Table 5 shows the ROUGE performance results for generated headlines with length 12 against headlines written by human assessors."
      ]
    },
    {
      "heading": "6 Conclusion and Future Work",
      "text": [
        "Generating summaries with headline-length restriction is hard because of the difficulty of squeezing a full text into a few words in a readable fashion.",
        "In practice, it often happens in order to achieve the optimal informativeness, grammatical structure is overlooked, and vice versa.",
        "In this paper, we have described a system that was designed to use two methods, individually had exhibited exactly one of the two types of unbalances, and integrated them to yield content and grammaticality.",
        "Structural abstraction at the POS level is shown to be helpful in our current experiment.",
        "However, part-of-speech tags do not generalize well and fail to model issues like subcategorization and other lexical semantic effects.",
        "This problem was seen from the fact that there are half as many templates as the original headlines.",
        "A more refined pattern language, for example taking into account named entity types and verb clusters, will further improve performance.",
        "We intend to incorporate additional natural language processing tools to create a more sophisticated and richer hierarchical structure for headline summarization."
      ]
    }
  ]
}
