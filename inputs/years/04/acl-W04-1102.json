{
  "info": {
    "authors": [
      "Jing-Shin Chang",
      "Yu-Tso Lai"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W04-1102",
    "title": "A Preliminary Study on Probabilistic Models for Chinese Abbreviations",
    "url": "https://aclweb.org/anthology/W04-1102",
    "year": 2004
  },
  "references": [
    "acl-C02-1012",
    "acl-P03-1035"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Chinese abbreviations are widely used in the modern Chinese texts.",
        "They are a special form of unknown words, including many named entities.",
        "This results in difficulty for correct Chinese processing.",
        "In this study, the Chinese abbreviation problem is regarded as an error recovery problem in which the suspect root words are the “errors” to be recovered from a set of candidates.",
        "Such a problem is mapped to an HMM-based generation model for both abbreviation identification and root word recovery, and is integrated as part of a unified word segmentation model when the input extends to a complete sentence.",
        "Two major experiments are conducted to test the abbreviation models.",
        "In the first experiment, an attempt is made to guess the abbreviations of the root words.",
        "An accuracy rate of 72% is observed.",
        "In contrast, a second experiment is conducted to guess the root words from abbreviations.",
        "Some submodels could achieve as high as 51 % accuracy with the simple HMM-based model.",
        "Some quantitative observations against heuristic abbreviation knowledge about Chinese are also observed."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The modern Chinese language is a highly abbreviated one due to the mixed uses of ancient single character words as well as modern multi-character words and compound words.",
        "The abbreviated form and root form are used interchangeably everywhere in the current Chinese articles.",
        "Some news articles may contain about 20% of sentences that have suspect abbreviated words in them (Lai 2003).",
        "Since abbreviations cannot be enumerated in a dictionary, it forms a special class of unknown words, many of which originate from named entities.",
        "Many other open class words are also abbreviatable.",
        "This particular class thus introduces complication for Chinese language processing, including the fundamental word segmentation process (Chiang 1992, Lin 1993, Chang 1997) and many word-based applications.",
        "For instance, a keyword-based information retrieval system may requires the two forms, such as “ A -1� ” and “ A & -J� iR ” (“legislators”), in order not to miss any relevant documents.",
        "The Chinese word segmentation process is also significantly degraded by the existence of unknown words (Chiang 1992), including unknown abbreviations.",
        "There are many heuristics for Chinese abbreviations.",
        "Such heuristics, however, can easily break (Sproat 2002).",
        "Currently, only some quantitative approaches (Huang 1994a, 94b) are available in predicting the presentation of an abbreviation.",
        "Since such formulations regard the word segmentation process and abbreviation identification as two independent processes, they probably cannot optimize the identification process jointly with the word segmentation process, and thus may lose the useful contextual information.",
        "Some class-based segmentation models (Sun 2002, Gao 2003) well integrate the identification of some regular non-lexicalized units (such as named entities).",
        "However, the abbreviation process can be applied to almost all word forms (or classes of words).",
        "Therefore, this particular word formation process may have to be handled as a separate layer in the segmentation process.",
        "To resolve the Chinese abbreviation problems and integrate its identification into the word segmentation process, this study proposes to regard the abbreviation problem in the word segmentation process as an “error recovery” problem in which the suspect root words are the “errors” to be recovered from a set of candidates according to some generation probability criteria.",
        "This idea implies that an HMM-based model for identifying Chinese abbreviations could be effective in either identifying the existence of an abbreviation or the recovery of the root words from an abbreviation.",
        "We therefore start with a unified word segmentation model so that both processes can be handled at the same time, and when the input is reduced to a single abbreviated word, the model can be equally useful for recovering its root.",
        "As a side effect of using HMM-based formulation, we expect that a large abbreviation dictionary could be derived from a large corpus or from web documents through the training process of the unified word segmentation model automatically.",
        "Section 2 will show our HMM models and the three abbreviation problems correspond to the three basic HMM problems.",
        "Section 3 will show the experiment setup.",
        "Section 4 will examine the experiments to guess abbreviations from root or vice versa."
      ]
    },
    {
      "heading": "2 Chinese Abbreviation Models",
      "text": []
    },
    {
      "heading": "2.1 An Error Recovery Paradigm",
      "text": [
        "To resolve the abbreviation problems, first note that the most common action one would take when encountering an abbreviation is to find its candidate roots (probably from a large abbreviation dictionary if available or from an ordinary dictionary with some educated guesses), and then identify the most probable one.",
        "This process is identical to the operation of many spelling correction models, which generate the candidate corrections according to a reversed word formation process, then justify the best candidate.",
        "Such an analogy indicates that we may use an HMM model (Rabiner 1993), which is good at finding the best unseen state sequence, for error recovery.",
        "There will be a direct map between the two paradigms if we regard the observed input character sequence as our “observation sequence”, and regard the unseen word candidates as the underlying “state sequence”.",
        "Given these mappings, we will be able to use many standard processing approaches for HMM when we have to answer some interesting questions (including root word recovery).",
        "Among all interesting questions for an HMM, we have three basic questions to ask the model (Rabiner 1993), namely the output probability of an output sequence, the best underlying state sequence and the best parameters given a training corpus.",
        "If we can ask the HMM for abbreviation the same questions, then we will also be able to answer the question on (1) what is the likelihood that a string is an abbreviation, (2) what are the best underlying root words for an input character string that contains abbreviations, and (3) how to estimate the model parameters automatically given a corpus.",
        "The first question is related to the problem of generating an appropriate abbreviation from a root word; the second question is linked to finding the best underlying roots from an abbreviated string, and the third question have a direct link to the construction of an abbreviation dictionary automatically from a corpus.",
        "For now we will not explore this third question, but leave it to a research that would be launched in the near future.",
        "The most interesting question to ask is, of course, the second question in the Chinese tokenization process.",
        "Therefore, we will start with a unified word segmentation model, which has the capability to handle abbreviation problem jointly with the word segmentation process."
      ]
    },
    {
      "heading": "2.2 HMM-Q2: Unified Word Segmentation Model for Abbreviation Recovery",
      "text": [
        "To integrate the abbreviation process into the word segmentation model, firstly we can regard the segmentation model as finding the best underlying words w; = w1, •, wm (which include only base/root forms), given the surface string of",
        "abbreviated forms of compound words.)",
        "The segmentation process is then equivalent to finding the best word sequence w * such that:",
        "Equation 1.",
        "Unified Word Segmentation Model for Abbreviation Recovery where c�i refers to the surface form of wi, which could be in an abbreviated or non-abbreviated (or any transformed) form of wi .",
        "The last equality assumes that the generation of an abbreviation is independent of context, and the language model is a word-based bigram model.",
        "Such assumptions can be adapted to different submodels for word segmentation (Chiang 1992) as appropriate.",
        "Furthermore, in many cases, the underlying word wi will be a compound word consisting of other constituent words wij (e.g., “ aA )QATRA”).",
        "And, the probability P(ci |wi) is not always 1 or 0, since the constituents may be abbreviated",
        "differently in different context, making the mapping of the compound ambiguous.",
        "For instance, some people may prefer to abbreviate ‘z ���4ff�r ’ (Industrial Technology Research Institute; ITRI) into ‘z4ffr’ (IRI) while other may prefer an abbreviation of ‘z�r’ (ITI).",
        "Notice that, this equation is equivalent to the formulation for an HMM (Hidden Markov Model) (Rabiner 1993) to find the best “state” sequence given the observation symbols.",
        "The parameters P(wi |wi−1) and P(ci |wi) represent the transition probability and the (word-wise) output probability of an HMM, respectively; and, the formulations for P(wj ) and P(c1n |w1m ) are the respective “language model” of the Chinese language and the “generation model” for the abbreviated words (i.e., the “abbreviation model” in the current context).",
        "The “state” sequence in the word segmentation case is characterized by the root forms w1m ≡ w1 , • • • , wm , or the hidden words; and, the “observation symbols” are characterized hereby Cj ≡ c1 , , cn ≡ c1 , • • •, cm , where the surface form ci = Cb�;; is a chunk of characters beginning at the b(i)-th character and ending at the e(i)-th character.",
        "Such an analogy with an HMM enables us to estimate the model parameters using an unsupervised training method that is directly ported from the forward-backward or Baum-Welch re-estimation formula (Rabiner 1993) or a generic EM algorithm (Dempster 1977).",
        "Note also that, while the above formulation is intended for finding root words in a sentence, with the help of contextual words, we can also apply the same formulation to a single abbreviated word (likely to have a compound word as its root in many cases) to find the most likely constituent words, without the help of surrounding words, but with the help of contextual constraints among its constituents."
      ]
    },
    {
      "heading": "2.2.1. Language Model",
      "text": [
        "The word transition probability P( wi |wi−1) used in the language model is used to provide contextual constraints among root words.",
        "It may not be reliably estimated when the language has a large vocabulary and when the training corpus is small.",
        "To resolve this problem, we can back-off the bigram word transition probability to a unigram word probability using Katz’s method (Katz 1987) for rare bigrams.",
        "We can, of course, use other smoothing methods to acquire reliable parameters.",
        "The smoothing issues, however, are not the main focus of this preliminary study.",
        "In the perfect case where all words are lexicalized, rendering all surface forms identical to their “root” forms and all words are known to the system dictionary, we will have P(ci |wi) =1 , ∀i =1, m, and Equation 1 is no more than a word bigram model for word segmentation (Chiang 1992).",
        "In the presence of unknown words (e.g., abbreviations being one of such entities), however, we can no longer ignore the generation probability",
        "For example, if c�i is ‘ pk’ then wi could be the compound word ‘ p 4 k A ’ (Taiwan University) or ‘ p 4 k o l’ (Taiwan Major League).",
        "In this case, the parameters in P(k A |p 4) x P(p|p4) x P(k|kA) and P(kol|p4) x P(p|p 4) x P(k |kol) will indicate how likely ‘ pk’ is an abbreviation, and which of the above two compounds is the root form of the abbreviation.",
        "Therefore, we need a method for estimating the probabilities between the abbreviations and their root forms (many of which are compound words with other constituents)."
      ]
    },
    {
      "heading": "2.3 Applying Abbreviation Models",
      "text": [
        "There are two problems to use the unified model which takes abbreviated words into account.",
        "First of all, since the word lattice is constructed from all possible w1m ≡ w1 , • • •, wm , how can we construct it without really knowing the candidate base forms of c�i in advance?",
        "We don’t really want to randomly combine all possible root forms, which is not affordable in computational cost.",
        "Therefore, we have to make some smarter choices.",
        "Second, how to compute the abbreviation (output/generation) probability P(ci |wi) once the lattice is constructed with candidate root words?"
      ]
    },
    {
      "heading": "2.3.1 Candidate Root Word Generation",
      "text": [
        "The first problem can be resolved if we choose some highly probable constituents w that would generate each individual characters cj in c�i independently, and allow such Top-N candidates to form part of the complete word lattice.",
        "That is, for each individual character cij , we choose its Top-N candidates according to: P(cij |w)•P(w) .",
        "The probability P(cij |w) here represents the character-wise generation probability of a single character from its corresponding root word.",
        "Notice that, after we apply the word segmentation model Equation 1 to the word lattice, some of the above candidates may be preferred and others be discarded, by consulting the neighboring words and their transition probabilities.",
        "This makes the abbreviation model jointly optimized in the word segmentation process, instead of being optimized independent of context.",
        "abbreviation model is to introduce the length and the bit pattern for abbreviation operations as additional features into the abbreviation model.",
        "If this is the case, we will have the following augmented abbreviation model.",
        "The second problem can be resolved using the following equation if wi can be segmented into",
        "In other words, we use the transition probability between constituent words and the character-wise generation probabilities of individual characters from a constituent word to estimate the global generation probability of the abbreviated form."
      ]
    },
    {
      "heading": "2.3.3 Simplified Abbreviation Models",
      "text": [
        "It is sometimes simply not efficient to save all pairs of root compounds and their respective abbreviations in an abbreviation dictionary.",
        "Therefore, it is desirable to simplify the abbreviation probability by using some simpler features for Chinese abbreviation words.",
        "For instance, it is known that many 4-character compound words will be abbreviated as 2-character abbreviations (such as the case for the < a A )C A, a )C> pair.)",
        "It was also known heuristically that many such 4-character words are abbreviated by reserving the first and the third characters, which can be represented by a ‘1010’ bit pattern, where a ‘1’ means to reserve the respective character and a ‘0’ means to delete it.",
        "Therefore, a reasonable simplification for the",
        "w), whichcouldmeans apair of<abbreviation, root> orbe evaluatedas the productoftheper-charactergeneration probabilities andthe sub-constituent transition probabilities as outlinedin Equation 2.",
        "This term can ofcourse be ignoredfromthe above augmentedabbreviation model so thatonlyvery simple lengthandpositionfeatures are usedfor abbreviation handling."
      ]
    },
    {
      "heading": "3 Data and Parameter Estimation",
      "text": [
        "Anabbreviationdictionary containingthe word-abbreviationpairs is requiredto test the proposedmodels.",
        "Unfortunately, alarge Chinese abbreviationdictionaryis notavailable.",
        "Therefore, we have to collectsome ofthe generic abbreviations, andmakeothers manually from some namedentity lists.",
        "Almost halfofour collection comes fromtheMinistryofEducation ofthe ROC.",
        "(http://www.edu.tw/clc/dict/).",
        "(In a future plan, alarge abbreviationdictionarywill be builtautomatically byusingthe proposedmodels.)",
        "Eventually, wegot 1547 root-abbreviationpairs.",
        "Among them, 1235 pairs are consideredsimple and312 pairs are “tough” in the sense thatthey violate some model assumptions.",
        "Forinstance, we requiredthataroot in acompoundwordbe mappedto atleastone characterinits abbreviation (notto anull string), andwe also assumethat the wordcannotbe mappedto acharacterthat is not partofthe word.",
        "(Forexample, AB canbe abbreviatedas AorB butnotC.)",
        "Some tough words will actually map substrings to nullstri ngs; others may be recursively abbreviated; and yet others may change the word order (as in abbreviating “Z – O MUR 1” as “O – 1” instead of “ – O1”.).",
        "As a result, the tough pairs will not be handled correctly with current models.",
        "To simplify the task, only the 1235 simple pairs are tested for evaluation.",
        "They are further divided randomly into a training set of 986 pairs (80%) and a test set of 249 pairs (20%).",
        "Since the corpus size is not large, the compound words are also manually segmented into their constituents in order to know the true alignments between each character of the abbreviation with its root form in the compound word.",
        "Admittedly, such an extremely small training set causes serious data sparseness problem during training.",
        "Therefore, the evaluated performance in this preliminary report will be highly underestimated.",
        "The parameters are estimated in the unsupervised mode using a standard EM algorithm or the re-estimation method as conventional HMM models would do (Rabiner 1993).",
        "In addition, the manually segmented dictionary also allows us to estimate the model parameters in the supervised mode.",
        "The unsupervised training will automatically align each character in the abbreviations to its root form in the full words.",
        "It is observed that 65.5% of the training set dictionary pairs will be aligned correctly.",
        "Other pairs are aligned partially correct.",
        "Note that parameters P(m |n) and P(bit |n) can be estimated using maximum likelihood estimation by directly consulting the abbreviation dictionary since they are only related to word length and character position.",
        "It is interesting, in the first place, to check these types of parameters quantitatively to see if they reveal some abbreviation heuristics recognized by native Chinese speakers.",
        "The high frequency patterns, which are much more frequent than the ones ranked in lower places, are listed in Table 1 and",
        "Table 1 shows how word lengths will change during the abbreviation process, and Table 2 shows which characters will be deleted from the root of a particular length.",
        "The tables quantitatively support some general heuristics for native Chinese speaker.",
        "For instance, most words will be abbreviated by deleting about half the characters in the words, as shown in Table 1.",
        "The data also shows that the first character in a two-character word will be retained in most cases, and the first and the third characters in a 4-character word will be retained in 56% of the cases.",
        "However, the tables also shows that around 50% of the cases cannot be uniquely determined simply by consulting the word length for its abbreviated form.",
        "This does suggest the necessity of an abbreviation model for resolving this kind of unknown words and named entities."
      ]
    },
    {
      "heading": "4 Experiments and Analysis",
      "text": [
        "The unified model can be applied to a whole sentence which contains abbreviations during word segmentation.",
        "When the input is reduced to a single abbreviated word (or compound), it can also be applied to recover the underlying root constituent words (without consulting contextual words).",
        "In this paper, we will only focus on the abbreviation word recovery problems.",
        "Two major experiments are conducted.",
        "The first experiment is to guess the most likely abbreviation form for a word using various feature combinations; the second is to guess the root word from an abbreviation.",
        "The following sections will give more details."
      ]
    },
    {
      "heading": "4.1 Guessing Abbreviations from Roots",
      "text": [
        "The main task of this experiment is to guess the most probable abbreviation forms for the unabbreviated words in a word list.",
        "The abbreviation forms of a word can be enumerated by arbitrarily retaining some characters of this root word and deleting others.",
        "For example, the word “MR,09” has six possible abbreviated forms: “M”, “R”, “�» «M W» \"M OR\" OR”.",
        "In general, �.,, , ,.,, and R,v.",
        "if we have a root word of length L, there could be 2L − 2 possible abbreviations for this root word (excluding the word itself and the null string).",
        "The best possible abbreviation form c* for an input word wi can be determined as the one with the highest generation probability P(ci |wi), i.e.,",
        "probability for a candidate c�i , in turn, can be estimated by summing up all probabilities of alignments between each character cij in c�i and the suspect constituent words wij in wi .",
        "In other words, we have",
        "where PA (ci |wi) is the generation probability for a known alignment A, which can be estimated as in Equation 2.",
        "For simplicity, we assume that each character in c�i will be mapped to a substring wij in wi.",
        "In other words, we assume that the mapping between the constituents is 1-1, and no 1-0 or 0-1 mapping is possible.",
        "(In future works, such a constraint could be removed.)",
        "Also, we will assume that wij should at least contain the character that is aligned to it.",
        "(This is not always true for Chinese abbreviations.",
        "For example, “� �” can be abbreviated with its ancient location name “r4”, which does not appear anywhere in its root.)",
        "There is also a normalization issue in computing the probability of a particular alignment.",
        "In general, a shorter string may be preferred as the best abbreviation simply because it multiplies less probability factors when estimating the alignment probability.",
        "To reduce this effect, we intentionally scale down, by a normalization factor, the generation probabilities for those alignments that map a complete word into a single character.",
        "In fact, there are only about 10% of such alignments, and many of which are mapping a two-character word into a single character (which can be compensated by the large Pr(1|2) factor in the model.",
        "This simple normalization approach actually improves the test set performance greatly.",
        "The following table shows the test set performance for using different features in the abbreviation probability as given in Equation 3.",
        "(The training set performance ranges from 94% to 98%, which suggests a good fit to the training data.)",
        "Abbreviation Generation with Combined Features.",
        "Each column shows the test set performance for a submodel, which is identified by the features used for estimating the probability.",
        "The label ‘1’ (or ‘0’) indicates that the feature at the first column is used (or unused) in the submodel.",
        "For instance, the submodel of the second column (‘1 1 1’) uses all the features, including the word transition probability, word-to-abbreviation probability, probability for mapping n character word to a particular abbreviation bit pattern P(bit|n), and the probability for mapping n-character words into m-character abbreviations.",
        "It is seen that supervised training acquires a little better performance than its unsupervised (EM) counterpart.",
        "Although not shown in this table, it is observed that the word transition probability and word-to-abbreviation probability in general should be used to get better performance.",
        "The table also shows that the other two features based on character positions and word lengths provide additional help.",
        "In particular, P(bit|n) seems to be more helpful than P(m|n) since it contains detailed information for retaining characters at particular positions.",
        "The best performance is about 72% when supervised training is used and all the three types of features are used for estimating the abbreviation probability.",
        "≡ PA"
      ]
    },
    {
      "heading": "4.2 Guessing Roots from Abbreviations",
      "text": [
        "In this experiment, we are given an abbreviation list; the goal is to guess the best root words of the abbreviations in the list.",
        "The parameters used here are acquired from human tagged alignments in a supervised manner.",
        "To find the best root candidates of an abbreviated compound word, we need to find the candidate root words for each input character first.",
        "The candidate root words can be found from the training set whose generation probability",
        "then be picked up as described earlier.",
        "For instance, if we want to find the root words of the abbreviation “A-&”, and the probabilities P(A|A{ ) and P(-&|-&A) are non-zero, then we have the chance to recover the abbreviation “A -&” back to the correct compound word “A{ -& A” , which consists of the candidate root words “A{ ” and “-&A” for the input characters “A” and “-&” respectively.",
        "Unfortunately, the limited abbreviation dictionary we have is highly sparse.",
        "Among the 249 abbreviations in the test set, only 144 (58%) of them have their candidate root words available in the training set.",
        "The other 105 abbreviations (42%) cannot be recovered since each of them has at least one character whose candidate cannot be discovered from the training set.",
        "For this reason, we will limit ourselves to the performance of the “trainable” test set consisting of the 144 abbreviations, in order to factor out the sparseness problem pertaining to the training corpus.",
        "Under such a restricted environment, we have tested various submodels to see how different language models and simple smoothing affect the results of this error recovery process.",
        "The results are summarized in the following table:",
        "The bigram language model uses P(wi |wi−1 ) in the unified HMM model while the unigram model uses P(wi) instead.",
        "Both of them use maximum likelihood estimation over the manually tagged abbreviation-root pairs when smoothing is not applied.",
        "When smoothing is applied, the smoothed bigram probability is acquired by linearly interpolating the unigram and bigram probabilities with an equal weight (0.5).",
        "The above table indicates that using the less complicated unigram model generally improve the test set performance significantly (from 35% to 44%).",
        "If the model parameters are smoothed, the improvement is even greater.",
        "Such results can be well expected in the current environment where the training data is very sparse.",
        "Overall, the best test set performance is about 51% when using a smoothed bigram language model; and this can be achieved by using at most 2 Top-N candidate root words while constructing the underlying word lattice.",
        "This suggests that we don’t really need to wildly enumerate all possible candidate root words for each input character with this model."
      ]
    },
    {
      "heading": "5. Concluding Remarks",
      "text": [
        "Chinese abbreviations, a special form of unknown words and named entities, are widely seen in the modern Chinese texts.",
        "This results in difficulty for correct Chinese processing.",
        "In this preliminary study, the Chinese abbreviation problem is modeled as an error recovery problem in which the suspect root words are to be recovered from a set of candidates.",
        "An HMM-based model is thus used for Chinese in either abbreviation identification, or in the recovery of the root words from an abbreviation.",
        "By extending a simple abbreviation string into a whole text involving abbreviations, it can also be applied to the Chinese word segmentation for identifying abbreviations in a text, or for bootstrapping an abbreviation dictionary from a text corpus.",
        "With the proposed model, the abbreviated forms can be guessed from root words at about 72% correction.",
        "The recovery of the root words from abbreviations is conducted at about 51 % accuracy rate.",
        "Although further improvement is possible, the preliminary results are encouraging.",
        "In the near future, bootstrapping a large abbreviation dictionary from web text by applying the proposed models is planned.",
        "This should partially resolve the data sparseness problems.",
        "Such models will also be integrated into a Chinese word segmentation model to partially resolve the unknown word and named entity identification problems in the tokenization process.",
        "It is expected that more applications will rely on such models for Chinese processing."
      ]
    }
  ]
}
