{
  "info": {
    "authors": [
      "Feng Jiang",
      "Hui Liu",
      "Yu-Quan Chen",
      "Ruzhan Lu"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W04-1105",
    "title": "An Enhanced Model for Chinese Word Segmentation and Part-Of-Speech Tagging",
    "url": "https://aclweb.org/anthology/W04-1105",
    "year": 2004
  },
  "references": [
    "acl-W03-1709"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper will present an enhanced probabilistic model for Chinese word segmentation and part-of-speech (POS) tagging.",
        "The model introduces the information of Chinese word length as one of its features to reach a more accurate result.",
        "And in addition, the model also achieves the integration of segmentation and POS tagging.",
        "After presenting the model, this paper will give a brief discussion on how to solve the problems in statistics and how to further integrate Chinese Named Entity Recognition into the model.",
        "Finally, some figures of experiments and comparisons will be reported, which shows that the accuracy of word segmentation is 97.09%, and the accuracy of POS tagging is 98.77%."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Generally, Chinese Lexical Analysis consists of two phases; one is word segmentation and the other is part-of-speech(POS) tagging.",
        "Rule-based approach and statistic-based approach are two dominant ways in natural language processing, as well as Chinese Lexical Analysis.",
        "This paper will only focus on the later one.",
        "Hence, our model is called a probabilistic model.",
        "Scanning through the researches in this field before, we have just found two points at which the performance of a Chinese word segmentation and POS tagging system could get better.",
        "One is the on the system architecture, and the other is from the Machine Learning theory.",
        "First, the traditional way of Chinese Lexical Analysis simply regards the word segmentation and POS tagging as two separated phases.",
        "Each one of them has its own algorithms and models.",
        "Dividing the whole process into two independent parts can lower the complexity of the design of system, but decrease the performance as well, because the two are fully integrated when a human processing a sentence.",
        "Fortunately, many researchers have already noticed it, and recent projects pay more attention on the integration of word segmentation and POS tagging, such as [Gao Shan, Zhang Yan.",
        "2001]’s pseudo trigram integrated model, [Fu Guohong et al.",
        "2001]’s analyzer which incorporates backward Dynamic Programming and A* algorithm, [Sun Maosong, et al.",
        "2003]’s ‘Divide and Conquer integration’, [Zhang Huaping, et al.",
        "2003]’s hierarchical hidden Markov model and so on.",
        "The experiments given by these papers also showed a great potential of the integrated models.",
        "Besides the system architecture, another point should be noticed.",
        "A probabilistic model of word segmentation and POS tagging can be regarded as an instance of Machine Learning.",
        "In Machine Learning, the feature extraction is the most important aspect, and far more important than a learning algorithm.",
        "In the models nowadays, it seems that the features for Chinese Lexical Analysis are a little too simple.",
        "Most of them take tag sequences, or word frequencies as the distinguishing features and ignore the other useful information that are provided by Chinese itself.",
        "In this paper, we will present an enhanced, not too complex, model for word segmentation and POS tagging, which will not only inherit the merit of an integrated model, but also take a new feature (word length) into account.",
        "The second part of this paper will describe the model, including the input, output, and some assumptions.",
        "The third part will give some brief discussion about the model on some issues like data sparseness and Named Entity Recognition.",
        "In the final part, the results of our experiments will be reported."
      ]
    },
    {
      "heading": "2 The Model",
      "text": [
        "The first step to establish the model is to make a formal description for its input and output.",
        "Here, a Chinese word segmentation and POS tagging system is viewed as with input,",
        "First consider P(C |(L, T)) .",
        "Suppose W is the vertex of words that (L, T) represents(i.e. the segmented word sequence), and the dependency assumption is like the following Bayers Network:",
        "Itis easilyseenthatthe distinctionbetweenthis modelandothermodels isthatthis oneintroduces wordlength.",
        "Infact, wordlengthreallyworks, and affects the performanceofthe systemina great deal, ofwhichourlaterexperiments willapprove.",
        "Themotivationto introducewordlengthinto ourmodelis initiallyfromthe classicalChinese poems.",
        "Whenwe readthese poems, we may spontaneouslyobeysome laws inwhereto have a pause.",
        "Forexample, inmostcases, a7-character-linedJueju(Akindofpoemformat) is readas **/**/***.",
        "Andthe pauses inasentence aremuch relatedto the lengthofwords orchunks.",
        "Evenin modernChinese, wordlengthalso plays apart.",
        "Sometimes we preferto use disyllabic words rather thansingleone, thoughbotharecorrectin",
        "occurrence ofthe wordlengthwillobeysome unwrittenlaws whenhumanwrites orspeaks.",
        "Introducingthewordlengthinto the word segmentationandPOS tagging modelmaybe in accordwiththe needs forprocessing Chinese.",
        "Anothermaincharacteristic ofthe modelis that itis an integrated model, because there is only one tag sequence.",
        "The following text will introduce how the model works.",
        "We will also inherit n-gram assumption in our model.",
        "Our destination is to find a sequence of (L;, T;) pairs that maximizes the probability,",
        "Because W is the segmentation of C , P(C |W) is always 1, and by another assumption that the occurrence of every word is independent to each other, then",
        "where",
        "computed as (the number of “ ” appearing as a verb) / (the number of all 2-charactered verbs).",
        "With 2.3 and 2.4, P(C |(L, T)) is ready.",
        "Then consider P(L, T) , which is easy to retrieve when we apply n gram assumption.",
        "Suppose n is 2, which means that (Li, Ti) only depends on (Li_1, Ti_1).",
        "Here P((Li , Ti) |(Li−1, Ti−1)) means the probability of a Tag Ti with Length Li appearing next to Tag Ti_1 with Length Li_1, which may be computed as (the number of (Li_1, Ti_1)(Li, Ti) appearing in corpus) / (the number of (Li_1, Ti_1) appearing in corpus).",
        "So, P(L,T) is also ready.",
        "Combining formula 2.1, 2.2, 2.3, 2.4 and 2.5, we have,",
        "Now, the enhanced model is complete with 2.6.",
        "When establishing the model, we have made several assumptions.",
        "1. the dependency assumption between tag-length pairs, words and characters like the Bayers network of figure 2.1 2.",
        "Word and word are independent.",
        "3. n gram assumption on (T,L) pairs.",
        "The validation of these assumptions is still somewhat in doubt, but the computational complexity of the model is decreased.",
        "All the resources required to achieve this model are also listed, i.e., a word list with probability P(yv, |CL i � , and an rigram transition Ti network with probability p(⎜LiLi ⎠ (7:._n+1Li-n+1 (Ti-1Li−1"
      ]
    },
    {
      "heading": "3 Discussion",
      "text": [
        "Though the model itself is not difficult to implement as we have presented in last section, there are still some problems thatwe willbe probably encounteredwithinpractice.",
        "Thefirst one is the datasparseness whenwe do the statistics.",
        "Anotheris howto furtherintegrateChineseNamed EntityRecognitioninto the new, word-length",
        "The algorithmto implementthis modelis also rather simple, andusing Dynamic Programming, we couldfinishthe algorithminO(cn), where nis thelengthofinputsentence, andc is aconstan t related to the maximum ambiguity in a position.",
        "how to integrate the existing Name Entity Recognition methods into the new model.",
        "During the integration, more attention should be paid to the structural and probabilistic consistency.",
        "For structural consistency, the original system structure does not need modifying when a new method of Named Entity Recognition is applied.",
        "For probabilistic consistency, the probabilities outputted by the Named Entity Recognition should be compatible with the probabilities of the words in the original word list.",
        "Here, we will take the Human Name Recognition as an example to show how to do the integration.",
        "[Zheng Jiahen, et al.",
        "2000] has presented a probabilistic method for Chinese Human Name Recognition, which is easy to understand and suitable to be borrowed as a demonstration.",
        "That paper defined the probability for a Chinese Human Name as:",
        "Where each one of `i” , `j”, `k” represents a single Chinese characters, `ik” , `ijk” are the strings which may be a human name, `ns” means a single name when `j” is empty, `np” means plural name when `j” is not empty, F(i) is the probability of `i” being a family name, M(j) means the probability of `j” being the middle character of a human name, E(k) means the probability of `k” being the tailing character of a human name, P(ns |ik) is the probability of `ik” being a single name, and P(np | ijk) is the probability of `ijk” being a plural name.",
        "F(i), M(j), and E(k) are easily retrieved from corpus, so P(ns |ik) and P(np |ijk) can be known.",
        "However, P(ns |ik) and P(np |ijk) do not satisfy the requirements of the word length introduced model.",
        "The model needs probabilities like P(w |(l, t)) , where w is a word, t is a word tag, and l is the word length.",
        "Therefore, P(ns |ik) needs to be modified into P(ik |nh, 2), for ik is always a 2 charactered word, and likewise, P(np |ijk) needs to be modified into P(ijk |nh, 3), where `nh” is the word tag for human name in our system.",
        "P(ns |ik) is equivalent to P(nh, 2 |ik) and P(np | ijk) is equivalent to P(nh, 3 |ijk).",
        "P(ns |ik) can be converted into P(ik |nh, 2) through following way,",
        "where `i”, `k” have the same meaning with those in 3.2 and 3.3. and nh is the tag for human name.",
        "In this formula, `i” and `k” are assumed to be independent.",
        "P(nh, 2), P(i), P(k) are easy to retrieve, which represent the probability of a 2 charactered human name, the probability of character `i” and the probability of character `k”.",
        "P(nh, 2 |ik) is computed from 3.2.",
        "Thus, the conversion of P(nk |nh, 2) to P(nh, 2 |ik) is done.",
        "In the same way, P(np |ijk) can be converted into P(ijk |nh, 3) by:",
        "Finally, the Human Name Recognition Module is integrated into the whole system.",
        "The input string C1, C2, ..., Cn first goes through the Human Name Recognition module, and the module outputs a temporary word list, which consists of a column of words that are probably human names and a column of probabilities corresponding to the words, which can be computed by 3.4 and 3.5.",
        "The whole system then merges the temporary word list and the original word list into a new word list, and applies the new word list in segmenting and tagging C1, C2, ..., Cn."
      ]
    },
    {
      "heading": "4 Conclusion & Experiments",
      "text": [
        "This paper has presented an enhanced probabilistic model of Chinese Lexical Analysis, which introduces word length as one of the features and achieves the integration of word segmentation, Named Entity Recognition and POS tagging.",
        "At last, we will briefly give the results of our experiments.",
        "In the previous experiments, we have compared many simple probabilistic models for Chinese word segmentation and POS tagging, and found that the system using maximum word frequency as segmentation strategy and forward trigram Markov model as POS tagging model (MWF + FTMM) reaches the best performance.",
        "Our comparisons will be done between the MWF+FTMM and the enhance model with trigram assumption.",
        "The training corpus is 40MB annotated Chinese text from People’s Daily.",
        "The testing data is about 1MB in size and is from People’s Daily, too.",
        "NOTES: MWF: Maximum Word Frequency, a very simple strategy in word segmentation disambiguation, which chooses the word sequence with max probability as its result.",
        "FTMM: Forward Trr gram Markov Model, a popular model in POS tagging.",
        "MWF+FTMM: A strategy, which chooses the output that makes a balance between the MWF and FTMM as its result.",
        "WSA (by word): Word Segmentation Accuracy, measured by recall, i.e. the number of correct segments divided by the number of segments in corpus.",
        "(In a problem like word segmentation, the result of precision measurement is commonly around that of recall measurement.)",
        "PTA (by word): POS Tagging Accuracy based on correct segmentation, the number of words that are correctly segmented and tagged divided by the number of words that are correctly segmented.",
        "Total (by word): total accuracy of the system, measured by recall, i.e. the number of words that are correctly segmented and tagged divided by the number of words in corpus, or simply WSA * PTA.",
        "WSA (by sentence): the number of correctly segmented sentences divided by the number of sentences in corpus.",
        "A correctly segmented sentence is a sentence whose words are all correctly segmented.",
        "PTA (by sentence): the number of correctly tagged sentences divided by the number of correctly segmented sentences in corpus.",
        "A correctly tagged sentence is a sentence whose words are all correctly segmented and tagged.",
        "Total (by sentence): WSA * PTA.",
        "Named entity considered or not: When named entity is not considered, all the unknown words in corpus are deleted before evaluation.",
        "Otherwise, nothing is done on the corpus.",
        "According to the results above (Table 4.1, Table 4.2, Table 4.3, Table 4.4), the new enhanced model does better than the MWF + FTMM in every field.",
        "Introducing the word length into a Chinese word segmentation and POS tagging system seems effective.",
        "This paper just focuses on the pure probabilistic model for word segmetation and POS tagging.",
        "It can be predicted that, with more disambiguation strategies, such as some rule based approaches, being implemented into the new model to achieve a multrengine system, the performance will be further improved."
      ]
    },
    {
      "heading": "5 Acknowledgements",
      "text": [
        "Thank Fang Hua and Kong Xianglong for their previous work, who have just graduated."
      ]
    }
  ]
}
