{
  "info": {
    "authors": [
      "Chooi-Ling Goh",
      "Masayuki Asahara",
      "Yuji Matsumoto"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W04-1109",
    "title": "Chinese Word Segmentation by Classification of Characters",
    "url": "https://aclweb.org/anthology/W04-1109",
    "year": 2004
  },
  "references": [
    "acl-C02-1049",
    "acl-C02-1055",
    "acl-E99-1023",
    "acl-N01-1025",
    "acl-P00-1042",
    "acl-W02-1815",
    "acl-W02-1817",
    "acl-W03-1701",
    "acl-W03-1705",
    "acl-W03-1719",
    "acl-W03-1720",
    "acl-W03-1730"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "During the process of Chinese word segmentation, two main problems occur: segmentation ambiguities and unknown word occurrences.",
        "This paper describes a method to solve the segmentation problem.",
        "First, we use a dictionary-based approach to segment the text.",
        "We simply apply maximum matching algorithm to segment the text forwardly (FMM) and backwardly (BMM).",
        "Based on the difference between FMM and BMM, and the context, we apply a classification method based on Support Vector Machines to reassign the word boundaries.",
        "By this way, we are using the output of a dictionary-based approach, and then applying a statistics-based approach to solve the segmentation problem.",
        "The experimental results show that our model achieves as high as 99.0 point of F-measure for overall segmentation, given the condition that no unknown word in the text, and 95.1 if unknown words exist."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The first step in Chinese information processing systems is word segmentation.",
        "It is because in written Chinese, all characters are joined together, and there is no separator to mark word boundaries.",
        "A similar problem also occurs in languages like Japanese, but at least in Japanese, there exist three types of characters (hiragana, katakana and kanji), and this could be a clue for finding word boundaries.",
        "For Chinese, as there is only one type of characters (hanzi), more segmentation ambiguities may happen in a text.",
        "During the process of segmentation, two main problems occur: segmentation ambiguities and unknown word occurrences.",
        "This paper focuses on solving the segmentation ambiguity problem, and proposes a sub-model to solve the unknown word problem.",
        "There are basically two types of segmentation ambiguities: covering ambiguity and overlapping ambiguity.",
        "The definitions are given below.",
        "Let x, y, z be some strings in Chinese which could consist of one or more characters.",
        "Subsequently, covering ambiguity is defined as follows: For string w= xy, x E W, y E W and also w E W, where W is a dictionary.",
        "As almost any single character in Chinese can be considered as a word, the definition reflexes only those cases where both word boundaries .../ùû/... and .../ù/û/... can be realized in some sentences.",
        "On the other hand, overlapping ambiguity is defined as follows: For string w = xYz, wl = xy E W and also w2 = yz E W. Although most of the time, the segmentation of one form is more preferred than the other form, but still we need to know where to use the other form.",
        "Both ambiguities depend heavily on the contexts to decide which is the correct segmentation given that particular occurrences in the texts.",
        "(la) and (lb) show examples of covering ambiguity.",
        "Given the string \" – A� \", it is treated as a word in (la), but as two words in (lb).",
        "Hu/ áhiqing/ whole family/ three/ member (The whole three members of Hu Shiqing family) (êb)3/ 5 6 / – / / 8 9 / : / in/ Paris/ one/ company/ magazine/ at/ (At one of the magazine company in Paris) On the other hand, (2a) and (2b) give examples of overlapping ambiguity.",
        "The string \"T, 7",
        "not/ can/ forget/ far away/ hometown/ DE/ parents/ (Cannot forget the parents who are far way at hometown)",
        "(Cannot have the intention to make profit) We intend to solve the ambiguity problems by combining a dictionary-based approach with a statistical model.",
        "By this way, we make use of the information in a dictionary to a statistical approach.",
        "Maximum Matching (MM) algorithm, a very early and simple dictionary-based approach, is used to initially segment the text by referring to a dictionary.",
        "It tries to match the longest possible words found in the dictionary.",
        "We can either parse the sentence forwardly or backwardly.",
        "Normally, the difference between forward and backward parsing will indicate the location where overlapping ambiguities occur.",
        "Then, we use a Support Vector Machine-based (SVM) classifier to decide which output should be the correct answer.",
        "For covering ambiguity, most of the cases, forward and backward MM will give the same outputs, in this case, we will just make use of the contexts to decide whether or not to split a word into two words and etc.",
        "Our results showed that the proposed method could produce the correct answers for overlapping ambiguities up to 92%, and 52% correctly split the words for covering ambiguities."
      ]
    },
    {
      "heading": "2 Previous Work",
      "text": [
        "Solving the ambiguity problems is a fundamental task in Chinese segmentation process.",
        "Although many previous researches have been done for segmentation, only a few have reported on the accuracy to solve ambiguity problems.",
        "(Li et al., 2003) suggest an unsupervised method for training the Naive Bayes classifiers to resolve overlapping ambiguities.",
        "They achieved 94.13% accuracy with 5,759 cases of ambiguity.",
        "A variation form of TF.IDF weighting is proposed for solving covering ambiguity problem in (Luo et al., 2002).",
        "They focus on 90 ambiguous words and achieve an accuracy of 96.58%.",
        "Most of the previous methods reported on the accuracy for overall segmentation.",
        "Recently, many researches are done by combining multiple models.",
        "Furthermore, most people have realized that working on character-based is more efficient than word-based for Chinese word segmentation.",
        "In (Xue and Converse, 2002), two classifiers are combined for Chinese word segmentation.",
        "First, a Maximum Entropy model is used to segment the text, then an error driven transformation model is used to correct the word boundaries.",
        "Similarly, they also use character-based tagging on the position of characters in words.",
        "They achieved an F-measure of 95.17.",
        "Another recent report is by (Fu and Luke, 2003), where hybrid models for integrated segmentation is proposed.",
        "Modified word juncture models and word-formation patterns are used to find the word boundaries and at the same time to identify the unknown words.",
        "They achieved 96.1 F-measure.",
        "As both methods use different corpora for the experiments, it is difficult to tell which method has done better than the other.",
        "Solving unknown word problem is also an important process in word segmentation.",
        "An unknown word is defined as a word not found in a dictionary.",
        "Therefore, they cannot be segmented correctly by simply referring to the dictionary.",
        "Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997).",
        "There are rule-based, statistics-based or even hybrid models.",
        "In other words, we cannot ignore the unknown word problem, as there always exist some unknown words (such as person names, numbers and etc) in the text even if we can get a very large dictionary.",
        "The creation of new words in Chinese is unlimited and is a continuous process.",
        "For example, the name for new diseases, technical terms, new expressions and etc.",
        "The accuracy is better if one focuses only on certain types of unknown words such as person names, place names or transliteration names, with over 80%.",
        "However, for general unknown words such as common nouns, verbs etc, the accuracy ranging from 50% to 70% only."
      ]
    },
    {
      "heading": "3 Proposed Method",
      "text": [
        "The underlying concept of our proposed method is as following.",
        "We regard the problem as a character classification problem.",
        "We believe that each character in Chinese holds its characteristics to appear in a certain position in a word.",
        "In other words, it can be either used at the beginning of a word, in the middle of a word, at the end of a word, or as a single-character word.",
        "By looking at the usage of the characters, we will decide the position tag of the characters using a machine learning based model, which is the Support Vector Machines (Vapnik, 1995).",
        "This method serves as a model to solve ambiguity problem, and at the same time, embeds a model to detect unknown words.",
        "We will now describe the method in more details in the following section."
      ]
    },
    {
      "heading": "3.1 Maximum Matching Algorithm",
      "text": [
        "We intend to solve the ambiguity problem by combining a dictionary-based approach with a statistical model.",
        "Maximum Matching (MM) algorithm is regarded as the simplest dictionary-based word segmentation approach.",
        "It starts from one end of a sentence, and tries to match the first longest word wherever possible.",
        "It is a greedy algorithm, but it has been empirically proved to achieve over 90% accuracy if the dictionary used is large.",
        "However, it cannot solve ambiguity problems and impossible to detect unknown words because only words exist in the dictionary can be segmented correctly.",
        "If we look at the outputs produced by segmenting the sentence forwardly (FMM), from the beginning of the sentence, and backwardly (BMM), from the end of the sentence, we will realize the places where overlapping ambiguities occur.",
        "As an example, FMM will segment the string \"UP a, * ir, H1 \"(when the time comes) into \"UP a, / * ir, / H1/\" (immediately/ come/ when), but BMM will segment it into \"UP / a, / ir, H1 /\" (that/ future/ temporary).",
        "Let Of and Ob be the outputs of FMM and BMM respectively.",
        "According to (Huang, 1997), for overlapping cases: If Of = Ob, then 99% that both the MMs have the correct answer.",
        "If Of =� Ob, then 99% that either Of or Ob has the correct answer.",
        "However, for covering ambiguity cases, even Of = Ob, but both Of and Ob could be correct or both could be wrong.",
        "If there exist unknown words, normally they will be segmented as single characters by both FMM and BMM.",
        "Based on the differences and context created by FMM and BMM, we will apply a machine learning based model to reassign the position tags which indicate the character position in the word."
      ]
    },
    {
      "heading": "3.2 Reclassification of Characters",
      "text": [
        "We plan to reclassify the outputs of FMM and BMM character by character.",
        "First, we will convert the output of the MMs into character-based, where each character will be assigned with a position tag such as described in Table 1.",
        "The BIES tags are as in (Uchimoto et al., 2000) and (Sang and Veenstra, 1999) for named entity extraction.",
        "These tags show the possible character position in a word.",
        "For example, if we look at the character \"* \", it is used as a single character in \" – / * / �s /\" (a book), at the end of a word in \"Poll \" (script), at the beginning of a word in \"* * \" (originally), and at the middle of a word in \"a * _t \" (basically).",
        "Then, based on these features, we will reclassify the tags by using a Support Vector Machine-based (SVM) chunker (Kudo and Matsumoto, 2001).",
        "The solid box in Figure 1 shows the features used to determine the class of the character \"V \" at location i.",
        "Based on the output position tags, finally, we will get the segmentation as \"aLP / Vi V / > 1 /72,� / _L /\" (welcome/ new year/ get-together party/ at/)."
      ]
    },
    {
      "heading": "4 Experiments and Results",
      "text": [
        "We have run our experiments with two datasets, PKU Corpus and SIGHAN Bakeoff data.",
        "The evaluation is done by using the tool provided in SIGHAN Bakeoff (Sproat and Emerson, 2003)."
      ]
    },
    {
      "heading": "4.1 Experiment with PKU Corpus",
      "text": [
        "tagged corpus, but we only use the segmentation information for our experiments.",
        "We divide the corpus randomly into 80% and 20%, for training and testing respectively.",
        "Since our purpose in this experiment is only for solving ambiguity problem, not the unknown word detection, we assume that all words could be found in the dictionary.",
        "We create a dictionary with all words from the corpus, which has 62,030 entries (referred to as Experiment 1).",
        "This experiment intends to show the performance of the method for solving ambiguity problem.",
        "It is sometimes very difficult to determine how many cases of ambiguities appearing in a sentence.",
        "For example, in the sentence in Figure 1, \"� fi \" (welcome the new year), \"fi * \" (new year), \"* 0 \" (a red paper that pasted on the door, written with some greeting words for celebrating new year in China), \"o iv\" (get-together) , \" �iv \" (get-together party), \" -.",
        "\" (at the meeting) and \"-.",
        "\" (at) are all possible words.",
        "How many overlapping cases and covering cases are there?",
        "It is quite impossible to answer.",
        "A word candidate may cause more than one ambiguities with the alternative word candidates.",
        "Therefore, we try to represent the ambiguities by character units since our method is character based.",
        "We group each character into one of these six categories.",
        "Let, Of = Output of FMM Ob = Output of BMM Ans = Correct answer Out = Output from our system",
        "Table 2 shows the conditions for each category.",
        "Category Allcorrect, Correct and Match have correct answers, whereas category Wrong, Mismatch and Allwrong have wrong answers.",
        "We could roughly say that category Correct and Wrong belong to overlapping ambiguities and category Match, Mismatch, and Allwrong belong to covering ambiguities.",
        "We could also say that Match and Mismatch are cases where we need to split the words, and Allwrong are cases where we should not split the words but have been split by the system.",
        "Table 3 shows the results of the method for solving ambiguity problem.",
        "In total, we could obtain about 99.13% that the characters are correctly tagged.",
        "If we only consider the overlapping cases (Correct and Wrong), 92.09% characters are correctly tagged.",
        "For covering cases, if we look at only those cases where we need to split the words (Match and Mismatch), 51.91% have been successfully split.",
        "Table 4 shows the results of word segmentation.",
        "We also compare our method with a Hidden Markov Model-based (HMM) morphological analyzer, where word bi-gram is used to calculate the probability.",
        "The size of the dictionary used for HMM is the same as previous experiment, but with real POS tags.",
        "The HMM does segmentation and POS tagging simultaneously, but we only take the results of segmentation for comparison.",
        "The results show that our proposed method can achieve higher accuracy with over 99.0%.",
        "It means that our method is able to solve ambiguity problem given the information where the ambiguous locations occurred by looking at the output of FMM and BMM."
      ]
    },
    {
      "heading": "4.1.2 Accuracy on Solving Unknown Word Problem",
      "text": [
        "The corpus used is the same as in Section 4.1.1, but the setting is different.",
        "In this round we divide the corpus into three sets, referring to as Set 1, Set 2 and Set 3.",
        "Set 1 plus Set 2 (80%) are used for training and Set 3 (20%) is used for testing, same as the previous experiment.",
        "The difference is the preparation of dictionary.",
        "There are two ways of preparation here.",
        "In the first case, all the words from Set 1 and Set 2 are",
        "Treebank, CHTB 2).",
        "Since our system works only on two-byte codings, some ascii codes in the data have been converted to GB codes before processing, especially numbers and alphabets.",
        "The distribution of the data is as shown in Table 6.",
        "The original dictionaries consist of 55,226 and 19,730 words respectively.",
        "According to these dictionaries, there are 1,189 and 7,216 unknown words in the testing data.",
        "After converting to GB codes, it left only 781 and 7,171 unknown words.",
        "It also means that about 34.3% and 0.6% of the unknown words automatically become known words after the conversion.",
        "The conversion reduced the number of unknown words because for example, if a numeral word \"1 9 9 s \" written in GB code exists in training data, but it is written in ascii code \"1998\" in testing data, it is treated as unknown word at the first place.",
        "After the conversion, it will become known word.",
        "We have set up the experiments similar to Experiment 2 and Experiment 3 above.",
        "For Experiment 2, all the words in the training data are used for creating the dictionary.",
        "For Experiment 3, it is based on our previous experiments where the division of half of the training corpus generated the best result by F-measure.",
        "Therefore, only 50% of the training corpora are used while creating the dictionaries.",
        "As a result, the new dictionaries contain 36,830 and 12,274 words respectively.",
        "Table 6 shows the details for the setting.",
        "For PKU corpus, the best result in the bakeoff 2�e work only on GB code, the standard coding used for simplified Chinese characters.",
        "However, it can be modified easily to suit Big5 coding for traditional Chinese characters.",
        "achieved 95.1 in F-measure (Zhang et al., 2003).",
        "They use hierarchical Hidden Markov Models to segment and POS tag the text.",
        "Although it is a closed test, they have used extra information such as class-based segmentation and role-based tagging models (Zhang et al., 2002), which give better result for unknown word recognition.",
        "Our method has done only slightly worse than theirs, with 94.7.",
        "The recall for unknown words is comparable, with 71.0% while the best one has 72.4%.",
        "Unfortunately, the recall for known words drops a bit, with 97.3%, while the best one is slightly better, with 97.9%, as shown in Table 7.",
        "We also compare with (Asahara et al., 2003), where similar method is used for reassigning the word boundaries, except that the words are first categorized into 5 or 10 classes (which is assumed equivalent to POS tags) using Baum-Welch algorithm, then the sentence is segmented into word sequence by a Hidden Markov Model-based segmenter.",
        "Finally, the same Support Vector Machine-based chunker is trained to correct the errors made by the segmenter.",
        "Our method which is simply a forward and backward maximum matching algorithm, has done a lot better then theirs, where complicated statistical based models are involved.",
        "They have achieved only 92.4 F-measure while we have 94.7.",
        "On the other hand, our results for CHTB corpus are not as comparable as the best result in the bakeoff.",
        "We could only get 84.7 point of F-measure, while the best one has 88.1 (Zhang et al., 2003) and 82.9 by (Asahara et al., 2003).",
        "It may be due to the reason that the training corpus is a lot smaller than the PKU corpus and the testing data contains more unknown words.",
        "However, we still could get quite good recall for unknown words, with 57.7%, while the others have 70.5% and 41.2% respectively.",
        "As a conclusion, our results cannot transcend the best results in the bakeofF for both corpora.",
        "However, our method is simpler.",
        "We only need a dictionary that created from a segmented corpus, FMM and BMM modules, and a classifier, without the intervention of human knowledge.",
        "We get quite comparable results for both known words and also unknown words.",
        "The result is worse when the training corpus is small and there exist a lot of unknown words such as in CHTB testing data.",
        "Therefore, we still need to investigate on the relationship between the size of training corpus and the division of corpus for training of ambiguity problem and unknown word detection.",
        "5 Conclusion Apparently, our proposed method has generated better result than the baseline models, FMM and BMM.",
        "We get nearly 99% accuracy if unknown words do not exist.",
        "However, in the real world, it is impossible that there is no unknown word at all even we could get a very large dictionary.",
        "Therefore, we also embedded a model to detect the unknown words.",
        "Unfortunately, while the accuracy for unknown word detection increased, the performance on solving known word ambiguity drops.",
        "As shown in the experiments with bakeofF data, our model works well only when the training corpus is large enough.",
        "As a conclusion, while our model is suited for solving segmentation ambiguity problem, it can also be used for unknown word detection.",
        "However we still need to find a balance point for solving these two problems simulteneously.",
        "We also need to research on the relationship between training corpus size and the best proportion to divide the corpus for training optimally on solving ambiguity problem and unknown word detection."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "Thanks to Mr. Kudo for his tool on Support Vector Machine-based chunker, Yamcha.",
        "We also thank the reviewers for their invaluable comments, Peking University and SIGHAN for providing the corpora in our experiments."
      ]
    }
  ]
}
