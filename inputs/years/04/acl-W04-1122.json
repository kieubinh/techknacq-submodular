{
  "info": {
    "authors": [
      "Zhiyong Luo",
      "Rou Song"
    ],
    "book": "Workshop on Automatic Alignment and Extraction of Bilingual Domain Ontology for Medical Domain Web Search",
    "id": "acl-W04-1122",
    "title": "An Integrated Method for Chinese Unknown Word Extraction",
    "url": "https://aclweb.org/anthology/W04-1122",
    "year": 2004
  },
  "references": [
    "acl-A94-1030",
    "acl-J90-1003",
    "acl-J93-1003",
    "acl-J93-1007",
    "acl-W03-1805"
  ],
  "sections": [
    {
      "text": [
        "Technology Beijing, PRC 100022 Center for Language Information Processing Beijing Language and Culture University Beijing, PRC 100083 luo_zy@blcu.edu.cn"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "Unknown word recognition is an important problem in Chinese word segmentation systems.",
        "In this paper, we propose an integrated method for Chinese unknown word extraction for off-line corpus processing, in which both context-entropy (on each side) and frequency ratio against background corpus are introduced to evaluate the candidate words.",
        "Both of the measures are computed efficiently on Suffix array with much less space overhead.",
        "Our method can also be reinforced when combined with a basic Segmentor by boundary-verification and arbitrary n-gram words can be extracted by our method.",
        "We test our method on Chinese novel Xiao Ao Jiang Hu, and obtain satisfactory achievements compared to traditional criteria such as Likelihood Ratio."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The unique feature of Chinese writing system is that it is character-based, not word-based.",
        "The fact that there are no delimiters between words poses the well-known problem of word segmentation.",
        "Any Chinese Information Processing (CIP) systems beyond character level, such as information retrieval, automatic proofreading, text classification, text-to-speech conversion, syntactic parser, information extraction and machine translation, etc.",
        "should have a built-in word segmentation block.",
        "Currently, dictionary-based method is the basic and efficient one for word segmentation.",
        "A fixed Chinese electronic dictionary is required for most CIP systems.",
        "Yet there are many unknown words (out of the fixed dictionary) coming into being all the time.",
        "The unknown words are diverse, including proper nouns (person names, place names, organization names,"
      ]
    },
    {
      "heading": "SONG Rou",
      "text": [
        "Center for Language Information Processing Beijing Language and Culture University Beijing, PRC 100083 songrou@blcu.edu.cn etc.",
        "), domain-specific terminological nouns and abbreviations, even author-coined terms, etc.",
        "and they appear frequently in real text.",
        "This may cause ambiguity in Chinese word segmentation and lead to errors in the applications.",
        "Presently, many systems (Tan et al., 1999), (Liu, 2000), (Song, 1993), (Luo et al., 2001) focus on online recognition of proper nouns, and have achieved inspiring results in news-corpus but will be deteriorated in special text, such as spoken corpus, novels.",
        "As to the rests of unknown words types, it is still the obstacle of application systems, although they are really important for specific collections of texts.",
        "For instance, according to our count on Chinese novel Xiao Ao Jiang Hu ( ((Zft�M )) ) (JIN Yong (�J), 1967), there are almost 515 unknown word types (out of our 243,539-item general dictionary) of total 39,404 occurrences and total 112,654 characters, and there are 983,134 characters overall in this novel (that is, about 11.46% characters of the whole novel are occupied by unknown words.).",
        "And most of them, such as “,*,�+%”(person name), “W/W A ice ”(normal noun), “ Q )A 0 t ”(organization name), etc.",
        "can’t be recognized by most current CIP systems.",
        "It is important to note that without efficient unknown word extraction method, most CIP systems can’t obtain satisfactory results."
      ]
    },
    {
      "heading": "2 Relative research works",
      "text": [
        "Offline unknown word extraction can be treated as a special kind of Automatic Term Extraction (ATE).",
        "There are many research works on ATE.",
        "And most successful systems are based on statistics.",
        "Many statistical metrics have been proposed, including point-wise mutual information (MI) (Church et al., 1990), mean and variance, hypothesis testing (t-test, chi-square test, etc.",
        "), log-likelihood ratio (LR) (Dunning, 1993), statistic language model (Tomokiyo, et al., 2003), and so on.",
        "Point-wise MI is often used to find",
        "interesting bigrams (collocations).",
        "However, MI is actually better to think of it as a measure of independence than of dependence (Manning et al., 1999).",
        "LR is one of the most stable methods for ATE so far, and more appropriate for sparse data than other metrics.",
        "However, LR is still biased to two frequent words that are rarely adjacent, such as the pair (the, the) (Pantel et al., 2001).",
        "On the other aspect, MI and LR metrics are difficult to extend to extract multi-word terms.",
        "Relative frequency ratio (RFR) of terms between two different corpora can also be used to discover domain-oriented multi-word terms that are characteristic of a corpus when compared with another (Damerau, 1993).",
        "In this paper, RFR values between source corpus and background one will be used to rank the final candidate-list.",
        "There are also many hybrid methods combined statistical metrics with linguistic knowledge, such as Part-of-Speech filters (Smadja, 1994).",
        "But POS filters are not appropriate for Chinese term extraction.",
        "Since all the terms extraction approaches need to access all the possible patterns and find their frequency of occurrence, a highly efficient data structure based on PAT-tree (Chien, 1997), (Chien, 1998) and (Thian et al., 1999) has been used popularly for this purpose.",
        "However, PAT-tree still has much space overhead, and is very expensive for construction.",
        "Now, we introduce an alternative data structure as Suffix array, with much less space overhead, to commit this task.",
        "In this paper, we propose a four-phase offline unknown word extraction method: (a) Construct the Suffix arrays of source text and background corpus.",
        "In this phase, Suffix arrays, sorted on both left and right sides context for each occurrence of Chinese character, are constructed.",
        "We call them Left-index and Right-index respectively; (b) Extract frequent n-gram candidate terms.",
        "In this phase, firstly we extract n-grams, appearing more than one time in different contexts according to Left-index and Right-index of source text, into Left-list and Right-list respectively.",
        "Then, we combine Left-list with Right-list, and extract n-grams which appear in both of them as candidates (C-list, for short).",
        "We also compute frequency, context-entropy and relative frequency ratio against background corpus for each candidate in this phase; (c) Filter candidates in C-list with context-entropy and boundary-verification coupled with General Purpose Word Segmentation System (GPWS) (Lou et al., 2001).",
        "In this phase, we segment each sentence, where each candidate appears, in the source text with GPWS and eliminate the candidates cross word boundary; (d) Output the final terms on relative frequency ratios.",
        "The remainder of our paper is organized as follows: Section 2 describes the candidate terms extraction approach on Suffix array.",
        "Section 3 describes the candidates’ filter approach on context-entropy and boundary-verification coupled with GPWS.",
        "Section 4 describes the relative frequency ratios and output of the final list.",
        "Section 5 gives our experimental result and Section 6 gives conclusion and future work."
      ]
    },
    {
      "heading": "3 Candidates extraction on Suffix array",
      "text": [
        "Suffix array (also known as String PAT-array)(Manber et al., 1993) is a compact data structure to handle arbitrary-length strings and performs much powerful on-line string search operations such as the ones supported by PAT-tree, but has less space overhead.",
        "Definition 1.",
        "Let X = x0x1x2..xn-1xn as a string of length n. For the sake of left and right context sorting, we have extended X by inserting two unique terminators ($, less than all of the characters) as sentinel symbols at both ends of it, i.e. x0 = xn = $ in X.",
        "Let LSi = xixi-1..x0 (RSi = xixi+1..xn) as the left (right) suffix of X that starts at position i.",
        "The Suffix array Left-index[0..n] (Right-index[0..n]) is an array of indexes of LSi (RSi), where LSLeft-index[i] < LSLeft-index[j] (RSRight-index[i] < RSRight-index[j]), i<j, in lexicological order.",
        "Let LLCP[i] (RLCP[i]), i=0..n-1, as the length of Longest Common Prefix (LCP) between two adjacent suffix strings, LSLeft-index[i] and LSLeft-index[i+1] (RSRight-index[i] and RSRight-index[i+1]).",
        "These arrays on both sides are assistant data structures for speeding string search.",
        "Figure 1 shows a simple Suffix array sorted on left and right context, coupled with the LCP arrays respectively.",
        "We apply the sort-algorithm proposed by (Manber et al., 1993), which takes O(nlogn) in worst cases performance, to construct the Suffix arrays, and sort all the suffix strings in UNICODE order.",
        "Figure 2 shows fragments of Suffix arrays of test corpus Xiao Ao Jiang Hu in readable style.",
        "Sorted suffix arrays have clustered all similar n-grams (of arbitrary length) into continuous blocks and the frequent string patterns, as the longest common prefix (LCP) of adjacent strings, can be extracted by scanning through the suffix arrays sorted on left context and right respectively."
      ]
    },
    {
      "heading": "String “tobeornottobe”",
      "text": [
        "# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 String $ t o b e o r n o t t o b e $"
      ]
    },
    {
      "heading": "Suffix array",
      "text": [
        "# 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Left-index 0 14 3 12 4 13 7 5 8 2 11 6 1 9 10 Right-index 0 14 12 3 13 4 7 11 2 5 8 6 10 1 9 LCP arrays on both sides # 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 LLCP 0 0 3 0 4 0 0 1 1 2 0 0 1 1 / RLCP 0 0 2 0 1 0 0 3 1 1 0 0 4 1 /",
        "As show in Figure 2, on right sorted part which starts at the position of Chinese Character “东”, we can extract the repeated n-grams, such as “东方不败不”, “东方不败为”, “东方不败之”, “东方不败也”, “东方不败仍是”, “东方不败”, etc., in turn and skip many substrings, such as “东 方”, “东方不”, etc., because they are not the LCP of adjacent suffix strings and only appear in the upper string “东方不败” for their all occurrences.",
        "We can apply the same skill on left sorted part which start at the position of Chinese character “败”, and extract “上东方不败”, “与东方不败”, “为东方不败”, “魔教教主东方不败”, “教主东方不败”, “东方不败”, etc., as repeated n-grams and skip many substrings, such as “不 败”, “方不败 ”, etc., for the same reasons.",
        "To extract candidate terms, we can scan through both left and right Suffix arrays and select all repeated n-grams into Left-list and Right-list respectively.",
        "The terms, which appear in both lists, can be treated as candidates (denoted by C-list).",
        "Extraction procedure can be done efficiently by coupled with the arrays of length of LCP on both sides via stack operations.",
        "The length and frequency of candidates can also be computed in this procedure.",
        "For example in Figure 2, term “东方不败” should appear in both Left-list and Right-list, and it is a good candidate.",
        "Yet n-grams “东方不败也” is not a candidate because even though “东方不败也” does appear in Right-list, it does not exist in our final Left-list (It always appears as a substring of direct upper string “连东方不败也” according to right part of Figure 2)."
      ]
    },
    {
      "heading": "4 Filter candidate terms",
      "text": [
        "As what show in Table 1, not all the terms in C-list extracted in Section 3 can be treated as significant terms because of their incomplete lexical boundaries.",
        "There two kinds of incomplete-boundary terms: (1) terms as substring of significant terms; (2) terms overlapping the boundaries of adjacent significant terms.",
        "In this section, we will take measures, including Context-entropy test and boundary-verification with common Segmentor (GPWS) with general lexicon, to eliminate these invalid candidates respectively."
      ]
    },
    {
      "heading": "4.1 Measure on Context-entropy",
      "text": [
        "According to our investigation, significant terms in specific collection of texts can be used frequently and in different contexts.",
        "On the other hand substring of significant term almost locates in its corresponding upper string (that is, in fixed context) even through it occur frequently.",
        "In this part, we propose a metric Context-entropy as a measure of this feature to filter out substrings of significant terms.",
        "Definition 2.",
        "Assume ω as a candidate term which appears n times in corpus X, α = {a1,a2,...,as}(β = {b1,b2,...,bt}) as a set of left (right) side contexts of ω in X.",
        "Left and right Context-entropy of ω in X can be define as:",
        "where n = EC(a;,co) = E Qco, b; ) ,",
        "C(ai, ω) (C(ω ,bi)) is count of concurrence of ai and ω (ω and bi) in X.",
        "Significant terms, which can be used in different context, will get high values of Context-entropy on both sides.",
        "And the substrings, which almost emerge because of their upper strings, will get comparative low values.",
        "The 3rd and 4th columns of Table 1 show the values of Context-entropy on both sides of a list of candidate terms.",
        "Many candidates, which almost emerge because of their direct upper strings, such as “ fk�”(in “ 41fk�”(person name)), “41fk”(in “41fk�”(person name)) , “A-@-1►1”(in “A -@- 1►1 X, ”(organization name)), appear in relatively fixed contexts and should get much lower value(s) of one or both sides of Context-entropy."
      ]
    },
    {
      "heading": "4.2 Boundary-verification with GPWS",
      "text": [
        "The candidate list of terms includes all of the n-grams, which appear in different context on both sides more than ones.",
        "The unique feature of Chinese writing system is that there are no delimiters between words poses a big problem: Many of candidate terms are invalid because of the overlapped factual words’ boundary, i.e. these candidates include several fragments of adjacent words, such as “tu X,”(overlapping the boundary of common word “b tu”(Hua Mountain)), “ �% �”(overlapping the boundary of common word “��”(Sir)), etc.",
        "listed in Table 2.",
        "We eliminate these candidates by verifying boundaries of them with a common Segmentor (GPWS (Lou et al., 2001)) and a general lexicon (with 243,539 words).",
        "GPWS was built as shared framework undertaking different CIP applications.",
        "It has achieved very good performance and great adaptability across different application domains in disambiguation, identification of proper nouns (including Chinese names, Chinese place names, translated names of foreigners, organization and company names, etc.",
        "), identification of high-frequency suffix phrases and numbers.",
        "In this part, we ONLY use the utilities of GPWS to perform the Maximum Match (MM) to find the boundaries of words in lexicon, and all of the unknown words (out of our lexicon) will be segmented into pieces.",
        "Coupled with GPWS, we propose a voting mechanism for boundary-verification as follows: For each candidate term in C-list as term Begin Declare falseNum as integer for the number of invalid boundary-check of term; Declare trueNum as integer for the number of valid boundary-check of term;",
        "Begin Segment sent with GPWS; Compare the term’s position in sent with the segment result of GPWS; If term crosses the adjacent words boundary Set falseNum = falseNum+1; Else Set trueNum = trueNum+1; End If falseNum > trueNum Set boundary-verification flag of term to FALSE; Else Set boundary-verification flag of term to TRUE; End Assistant with the segmentor, we eliminate 38,697 items of total 117,807 in C-list in 96.85% of precision.",
        "Table 2 shows many examples of candidates eliminated by sides-verification with GPWS.",
        "RFR(co ;X,Y) = f(co ,X)/f( co ,Y) RFR of term is based upon the fact that the significant terms will appear frequently in specific collection of text (treated as foreground corpus) but rarely or even not in other quite different corpus (treated as background corpus).",
        "The higher of RFR values of the terms, the more informative of the terms will be in foreground corpus than in background one.",
        "However, selection of background corpus is an important problem.",
        "Degree of difference between foreground and background corpus is rather difficult to measure and it will affect the values of RFR of terms.",
        "Commonly, large and general corpora will be treated as background corpus for comparison.",
        "In this paper, for our foreground corpus (Xiao Ao Jiang Hu), we experientially select a group of novels of the same author excluding Xiao Ao Jiang Hu as compared background corpus for some reasons as follows: (a) Same author wrote all of the novels, including foreground and background.",
        "The unique n-grams in writing style of the author will not emerge on RFR values.",
        "(b) All of the novels are in the same category.",
        "The specific n-grams for this category will not emerge on RFR values.",
        "So, most of the candidate terms with higher RFR values will be more informative and be more significant for the source novel.",
        "On the final phase, we will sort all of the filtered candidate terms on RFR values in desc-order so that the forepart of the final list will get high precision for extraction.",
        "The last column of Table 1 shows the RFR values of many candidates compared with our background corpus.",
        "Many candidates, such as “T�”, “I +”, which are frequent in both foreground and background corpus, will get much lower RFR values and will be eliminated from our final top list."
      ]
    },
    {
      "heading": "6 Experimental result",
      "text": [
        "We use novel Xiao Ao Jiang Hu as foreground corpus compared with the rest of novels of Mr. JIN Yong as background corpus.",
        "The total characters of foreground and background corpus are 983,134 and 7,551,555 respectively.",
        "We read through the novel Xiao Ao Jiang Hu and 5 graduates manually selected 515 new terms (out of our lexicon) with exact meaning in the novel as follows for the final test:",
        "(a) Proper nouns, such as person names: “4_Wt'”, “�)j +�”, “4_W��”, place names: “�SC � ”, “LIU, � �”, “ � Lh � �”, organization names: “ H� O�”, “���r[” etc.",
        "(b) Normal nouns, such as “I1►Iii ”, “ M”, etc.",
        "J (c) Others, such as “M414”, “' ;1V”, etc.",
        "By our method, we extract 117,807 candidates in this novel.",
        "Table 3 shows the result after filtering with Context-entropy on both sides and boundary-verification on different total extracted numbers; We also compared our integrated method to traditional measure LR.",
        "On lower total number levels, LR will overrun our method in unknown-word recall, and in turn overrun by us on higher levels.",
        "As to precision, our method always keeps ahead.",
        "We also notice that both of the methods have much low precision in extraction.",
        "To retrieve terms with much certain, we rank the entire final list on RFR values in final phase.",
        "Most significant terms will comes in the front of ranked list.",
        "Table 3 shows that our method Table 4 shows the top 12 of final list, and Figure 3 shows the performance of our method on different top levels when ranks the final list on RFR values."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "Unknown word recognition is an important problem in CIP systems.",
        "Suffix array based method is an efficient method for exact arbitrary-length frequent terms.",
        "And most of substring of significant terms, which almost appear in fixed contexts, can be eliminated by Context-entropy values.",
        "Large lexicon can help to verify the unknown word doundaris and filter incomplete-boundary n-grams.",
        "Most significant informative candidates list on the top of final list according to RFR values for subsequent manual confirmation, and on the other aspect, RFR also reflects the internal character of the extracted terms."
      ]
    }
  ]
}
