{
  "info": {
    "authors": [
      "Kyo Kageura"
    ],
    "book": "CompuTerm International Workshop on Computational Terminology",
    "id": "acl-W04-1810",
    "title": "Quantitative Portraits of Lexical Elements",
    "url": "https://aclweb.org/anthology/W04-1810",
    "year": 2004
  },
  "references": [
    "acl-C00-1047",
    "acl-J90-1003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper clarifies the basic concepts and theoretical perspectives by and from which quantitative “weighting” of lexical elements are defined, and then draws, quantitative portraits of a few lexical elements in order to exemplify the relevance of the concepts and perspectives examined."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Since Luhn’s pioneering work (Luhn, 1958) in automatic term weighting, many methods have been proposed in the fields of IR (e.g. Spark-Jones, 1973; Harter, 1975) and NLP (e.g. Church et al., 1990).",
        "Some “standard” methods of term weighting such as have been established (Aizawa, 2003; , 1999) and the application range has widened; term weighting has become a mature technology.",
        "Despite this, what has been technically proposed has not been examined from a theoretical point of view, i.e. what kind of weighting scheme reflects what kind of lexical nature within what kind of framework of interpretations in language.",
        "We will clarify this and then illustrate the relevance of this clarification by drawing quantitative portraits of some lexical items using the quantitative measures."
      ]
    },
    {
      "heading": "2 Texts and lexica",
      "text": [
        "Automatic term weighting starts from texts/documents.",
        "To what spheres the weights are attributed can differ.",
        "Figure 1 shows the linguistic spheres of lexica and texts (Kageura, 2002); there are both concrete data spheres and abstract spheres on both the lexical and textual sides.",
        "Within this scheme, three types of relations between lexica and texts can be identified: concrete terms attributed to concrete texts, concrete terms corresponding to discourse, and abstract lexica corresponding to abstract discourse.",
        "We will show below that three major types of automatic term weighting methods correspond to these three types of relations between lexica and texts."
      ]
    },
    {
      "heading": "3 Methods of term weighting",
      "text": []
    },
    {
      "heading": "3.1 Tfidf",
      "text": [
        "is defined as:",
        "where is the total frequency of a term , is the total number of the documents, and is the total number of documents in which the term occurs.",
        "Aizawa (2003) has shown that this can be derived from an information theoretic measure.",
        "Let and be random variables defined over events in a set of documents and a set of different terms Giving probabilities by relative frequencies, and assuming that all the documents have equal size and the frequency of in the documents that contain is equal, this measure becomes ; has an information theoretic meaning within the given set of documents (Figure 2)."
      ]
    },
    {
      "heading": "3.2 Term representativeness",
      "text": [
        "Hisamitsu, et al.",
        "(2000a) proposed a measure of “term representativeness”, in order to overcome the in .",
        "Let denote the frequency of in , the total frequency of ,the total number of running terms in , and the total number of term tokens in .",
        "The “weight” of a term can be given by:",
        "excessive sensitivity of weighting measures to token frequencies.",
        "They hypothesised that, for a term , if the term is representative, (the set of all documents containing ) have some specific characteristic.",
        "They define a measure which calculates the distance between a distributional characteristic of words around and the same distributional characteristic in the whole document set.",
        "In order to remove the factor of data size dependency, Hisamitsu et al.",
        "(2000a) defines the “baseline function,” which indicates the distance between the distribution of words in the original document set and the distribution of words in randomly selected document subsets for each size.",
        "The distance between the distribution of words in the original document set and the distribution of words in the documents which accompany the focal term is normalised by the “baseline function.” Formally, (2) where denotes the set of all documents; the distribution of words in ; a focal term; the set of all documents containing ; distribution of words in ; distribution of words in randomly selected documents whose size equals ; the distance between two distributions of words and .",
        "Log-likelihood ratio was used to measure the distance.",
        "This measure observes the centripetal force of a term vis-`a-vis discourse.",
        "i.e. it captures the characteristic of terms in the general discourse as represented by the given set of documents (Figure 3)."
      ]
    },
    {
      "heading": "3.3 Lexical productivity",
      "text": [
        "Nakagawa (2000) incorporates a factor of lexical productivity of constituent elements of compound Textual sphere / theoretical sphere of discourse",
        "units for complex term extraction.",
        "The method observes in how many different compounds an element is used in a given document set (let us denote this as where indicates the size of the overall document set as counted by the number of word tokens), and used that in the weighting of compounds containing , by taking weighted average.",
        "By explicitly limiting the syntagmatic range of observation of cooccurrence to the unit of compounds, he focused on the lexical productivity as manifasted in texts.",
        "This measure depends on the token occurrence, but we can also think of the theoretical lexical productivity in the lexicological sphere: how many compounds can potentially make” (let us denote this by ).",
        "For that, it is necessary to remove the factor of token occurrence.",
        "This can be done by: This has so far been unexplored.",
        "Potential lexical productivity of an element can be estimated from textual data: Letting be the occurrence probability of in texts, be the token occurrence of in texts, and be the sample space of the distribution of compounds (and simplex word) that contains with probability given to each compound , and assuming the combination of binomial distribution, we have: What is given in the data is the empirical value for , with the empirical distributions of what actually occur in the document set among .",
        "can be estimated by LNRE methods (Baayen, 2001).",
        "Being a measure representing the potential power of a lexical element for constructing compounds, indicates the lexical productivity in the lexicological sphere which correspond to theoretical sphere of discourse as represented by the given document set (Figure 4).",
        "A set of actual texts (targets of IR)"
      ]
    },
    {
      "heading": "4 Portraits of lexical elements",
      "text": [
        "As the three different measures capture three different aspects of lexical elements, they are not competitive 1.",
        "We here use these measures to illustrate characteristics of a few lexical elements.",
        "We used NII morphologically tagged-corpus for observation (Okada et al., 2001), which consists of Japanese abstracts in the field of artificial intelligence.",
        "Table 1 shows the basic quantitative information.",
        "No.",
        "of word tokens word types",
        "We chose the six most frequently occurring nominal element for observation, i.e. (system), (knowledge), (learning), (problem), (model), and (information).",
        "Intuitively, “system”, and “model” are rather general with respect to the domain of artificial intelligence, “knowledge” and “learning” are domain specific, and “information” and “problem” are in between.",
        "Table 2 shows the basic quantitative information for these six lexical elements.",
        "Figure 5 plots and term representativeness for the six elements.",
        "Table 3 shows the estimated value of lexical productivity.",
        "Figure 5 shows “learning” and “knowledge”, intuitively the domain-dependent elements, take high",
        "values, while “information” takes the lowest value.",
        "Term representativeness gives “learning” a high value but the values of “knowledge” is much lower, and about the same as “information”.",
        "Interestingly, the lexical productivity of “knowledge” and “information” is also very close to each other.",
        "It is possible to infer from these values of term representativeness and lexical productivity that both “information” and “knowledge” are, within the discourse of artificial intelligence, not with high centripetal value as both are rather “base” concepts of the domain.",
        "If we observe Table 2, “knowledge” is more often used as it is, while “information” tends to occur as compounds.",
        "From this we might be able to hypothesise that “knowledge” is in itself the “base” concept of artificial intelligence while “information” becomes the “base” concept in combination with other lexical items.",
        "This fits our intuition, as “information” in itself is more a “base” concept of information and computer science, which is a broader domain of which artificial intelligence is a subdomain.",
        "The low value of “information” comes from the low token frequency coupled with relatively high DF, which shows that “information”, as long as it is used, tends to scatter across documents.",
        "This is in accordance with the interpretation that “information” tends to occur in compounds.",
        "Still, however, it is difficult to interpret sensibly the fact that the value of “information” is lower than those of “model” and “system”.",
        "Perhaps it is more sensible to interpret among elements which take the values of term representativeness higher than a certain threshold.",
        "Then we can say that “learning” and “knowledge” represent concepts more “central” to the domain of artificial intelligence than “information”.",
        "The element “learning”, which takes the highest values both in and in term representativeness, is conspicuous in its lexical productivity.",
        "Compared to “knowledge” whose value is also high, and with the three elements “problem”, “information” and “knowledge” whose term representativeness values are relatively high, the order of lexical productivity of “learning” is a million times higher (and similar to “model” or “system”).",
        "Table 2 shows that “learning” does not occur much as it is, nor does it occur much as the head of compounds.",
        "This indicates that “learning” represents an important concept of the given data and in the discourse of artificial intelligence, but only “indirectly” in combination with other elements in compounds where “learning” tend to contribute to as a modifier rather than a head.",
        "The two “general” lexical elements, i.e. “model”",
        "and “system”, take low term representativeness values2.",
        "This is in accordance with our intuition.",
        "The lexical productivity of these two elements are extremely high (practically infinite).",
        "This indicates that these two elements can be widely used in varieties of discoursal contexts, without in itself contributing much to consolidating the content of discourse.",
        "This fits nicely to our intuitive interpretation of the meanings of these two elements, i.e. they are orthogonal to to such domain-dependent elements as “knowledge” or “learning”.",
        "This leaves us with the final element “problem”.",
        "The value of term representativeness is high, second only to “learning” and in between “learning” and “information”/“knowledge”.",
        "The lexical productivity is much closer to “information” and “knowledge” than to the other three.",
        "As such, “problem” can be interpreted as a kind of “base” concept, though it retains stronger centripetal force than “information” and “knowledge”.",
        "If we ignore values of “model” and “system” and only compare “information”, “problem”, “learning” and “knowledge”, it is also sensible to see that “problem” represent a concept more central to the domain than “information” but less central than “learning” and “knowledge”."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "We have shown that different term weighting measures have different spheres of interpretation; on the basis of that we have illustrated that the combination of these can illustrates complex aspects of lexical nature.",
        "Though it can be argued that the present study does not show ways for applications nor “empirical” evaluations within applications, we believe that “empirical” evaluations should be properly founded by the framework of interpretation in order for the results to be generalised in a scientific",
        "way; history of sciences have shown that often reliance on “empirical” evaluations correlates with the lack of theory or scientific wholesomeness."
      ]
    }
  ]
}
