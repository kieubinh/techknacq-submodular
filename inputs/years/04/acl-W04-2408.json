{
  "info": {
    "authors": [
      "Hiroya Takamura",
      "Yuji Matsumoto",
      "Hiroyasu Yamada"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W04-2408",
    "title": "Modeling Category Structures With a Kernel Function",
    "url": "https://aclweb.org/anthology/W04-2408",
    "year": 2004
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization.",
        "In a number of categorization tasks including text categorization, negative examples are usually more common than positive examples and there may be several different types of negative examples.",
        "Therefore, we construct a TOP kernel, regarding the probabilistic model of negative examples as a mixture of several component models respectively corresponding to given categories.",
        "Since each component model of our mixture model is expressed using a one-dimensional Gaussian-type function, the proposed kernel has an advantage in computational time.",
        "We also show that the computational advantage is shared by a more general class of models.",
        "In our experiments, the proposed kernel used with Support Vector Machines outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Recently, Support Vector Machines (SVMs) have been actively studied because of their high generalization ability (Vapnik, 1998).",
        "In the formulation of SVMs, functions which measure the similarity of two examples take an important role.",
        "These functions are called kernelfunc-tions.",
        "The usual dot-product of two vectors respectively corresponding to two examples is often used.",
        "Although some variants to the usual dot-product are sometimes used (for example, higher-order polynomial kernels and RBF kernels), the distribution of examples is not taken into account in such kernels.",
        "However, new types of kernels have more recently been proposed; they are based on the probability distribution of examples.",
        "One is Fisher kernels (Jaakkola and Haussler, 1998).",
        "The other is TOP (Tangent vector Of the Posterior log-odds) kernels (Tsuda et al., 2002).",
        "While Fisher kernels are constructed on the basis of a generative model of data, TOP kernels are based on the class-posterior probability, that is, the probability that the positive class occurs given an example.",
        "However, in order to use those kernels, we have to select a probabilistic model of data.",
        "The selection of a model will affect categorization result.",
        "The present paper provides one solution to this issue.",
        "Specifically, we proposed one type of TOP kernel, because it has been reported that TOP kernels perform better than Fisher kernels in terms of categorization accuracy.",
        "We briefly explain our kernel.",
        "We focus on negative examples in binary classification.",
        "Negative examples are usually more common than positive examples.",
        "There may be several different types of negative examples.",
        "Furthermore, the categories of negative examples are sometimes explicitly given (for example, the situation where we are given documents, each of which has one of three categories “sports”,”politics” and “economics”, and we are to extract documents with “politics”).",
        "In such a situation, the probabilistic model of negative examples can be regarded as a mixture of several component models.",
        "We effectively use this property.",
        "Although many other models can be used, we propose a model based on the separating hyperplanes in the original feature space.",
        "Specifically, a one-dimensional Gaussian-type function normal to a hyperplane corresponds to a category.",
        "The negative class is then expressed as a kind of Gaussian mixture.",
        "The reason for the selection of this model is that the resulting kernel has an advantage in computational time.",
        "The kernel based on this mixture model, what we call Hyperplane-based TOP (HP-TOP) kernel, can be computed efficiently in spite of its high dimensionality.",
        "We later show that the computational advantage is shared by a more general class of models.",
        "In the experiments of text categorization, in which SVMs are used as classifiers, our kernel outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model proposed by Hofmann (2000) in terms of categorization accuracy."
      ]
    },
    {
      "heading": "2 SVMs and Kernel Method",
      "text": [
        "In this section, we explain SVMs and the kernel method, which are the basis of our research.",
        "SVMs have achieved high accuracy in various tasks including text categorization (Joachims, 1998; Dumais et al., 1998).",
        "Suppose a set Dl of ordered pairs consisting of a feature vector and its label",
        "is given.",
        "Dl is called training data.",
        "I is the set of feature indices.",
        "In SVMs, a separating hyperplane (f (x) = w • x – b) with the largest margin (the distance between the hyperplane and its nearest vectors) is constructed.",
        "Skipping the details of SVMs’ formulation, here we just show the conclusion that, using some real numbers a*i (di) and b*, the optimal hyperplane is expressed as follows:",
        "We should note that only dot-products of examples are used in the above expression.",
        "Since SVMs are linear classifiers, their separating ability is limited.",
        "To compensate for this limitation, the kernel method is usually combined with SVMs (Vapnik, 1998).",
        "In the kernel method, the dot-products in (2) are replaced with more general inner-products K(xi, x) (kernel functions).",
        "The polynomial kernel (xi • xj + 1)d (d E N+) and the RBF kernel exp{ – Ilxi – xj �2/2σ2} are often used.",
        "Using the kernel method means that feature vectors are mapped into a (higher dimensional) Hilbert space and linearly separated there.",
        "This mapping structure makes non-linear separation possible, although SVMs are basically linear classifiers.",
        "Another advantage of the kernel method is that although it deals with a high dimensional (possibly infinite) space, explicit computation of high dimensional vectors is not required.",
        "Only the general inner-products of two vectors need to be computed.",
        "This advantage leads to a relatively small computational overhead."
      ]
    },
    {
      "heading": "3 Kernels from Probabilistic Models",
      "text": [
        "Recently new type of kernels which connect generative models of data and discriminative classifiers such as SVMs, have been proposed: the Fisher kernel (Jaakkola and Haussler, 1998) and the TOP (Tangent vector Of the Posterior log-odds) kernel (Tsuda et al., 2002)."
      ]
    },
    {
      "heading": "3.1 Fisher Kernel",
      "text": [
        "Suppose we have a probabilistic generative model p(xlO) of the data (we denote an example by x).",
        "The Fisher score of x is defined as Vo log p(xlO), where Vo means partial differentiation with respect to the parameters O.",
        "The Fisher information matrix is denoted by I(O) (this matrix defines the geometric structure of the model space).",
        "Then, the Fisher kernel at an estimate Oˆ is given by:",
        "The Fisher score of an example approximately indicates how the model will change if the example is added to the training data used in the estimation of the model.",
        "That means, the Fisher kernel between two examples will be large, if the influences of the two examples to the model are similar and large (Tsuda and Kawanabe, 2002).",
        "The matrix I(O) is often approximated by the identity matrix to avoid large computational overhead."
      ]
    },
    {
      "heading": "3.2 TOP Kernel",
      "text": [
        "On the basis of a probabilistic model of the data, TOP kernels are designed to extract feature vectors fˆo which are considered to be useful for categorization with a separating hyperplane.",
        "We begin with the proposition that, between the generalization error R(fˆo) and the expected error of the posterior probability D(fˆo), the relation R(fˆo) – L* ≤ 2D(fˆo) holds, where L* is the Bayes error.",
        "This inequality means that minimizing D (fˆo) leads to reducing the generalization error R(fˆo).",
        "D(fˆo) is expressed, using a logistic function F(t) = 1/(1 + exp( – t)), as",
        "where O* denotes the actual parameters of the model.",
        "The TOP kernel consists of features which can minimize D (fˆo).",
        "In other words, we would like to have feature vectors fˆo that satisfy the following: dx, w • fˆo(x) – b = F-1(P(y = +1lx, O*)).",
        "(5) for certain values of w and b.",
        "For that purpose, we first define a function v(x, O):",
        "and if w and b are properly chosen as",
        "A detailed discussion of the TOP kernel and its theoretical analysis have been given by Tsuda et al. (Tsuda et al., 2002)."
      ]
    },
    {
      "heading": "4 Related Work",
      "text": [
        "Hofmann (2000) applied Fisher kernels to text categorization under the Probabilistic Latent Semantic Indexing (PLSI) model (Hofmann, 1999).",
        "In PLSI, the joint probability of document d and word wis:",
        "where variables zk correspond to latent classes.",
        "After the estimation of the model using the EM algorithm, the Fisher kernel for this model is computed.",
        "The average log-likelihood of document d normalized by the document length is given by",
        "where",
        "They use spherical parameterization (Kass and Vos, 1997) instead of the original parameters in the model.",
        "They define parameters ρjk = 2 VP (wj l zk) and ρk = 2 VP (zk ), and obtained",
        "Thus, the Fisher kernel for this model is obtained as described in Appendix A.",
        "The first term of (31) corresponds to the similarity through latent spaces.",
        "The second term corresponds to the similarity through the distribution of each word.",
        "The number of latent classes zk can affect the value of the kernel function.",
        "In the experiment of (Hofmann, 2000), they computed the kernels with the different numbers (1 to 64) of zk and added them together to make a robust kernel instead of deciding one specific number of latent classes zk.",
        "They concluded that the Fisher kernel based on PLSI is effective when a large amount of unlabeled examples are available for the estimation of the PLSI model."
      ]
    },
    {
      "heading": "5 Hyperplane-based TOP Kernel",
      "text": [
        "In this section, we explain our TOP kernel."
      ]
    },
    {
      "heading": "5.1 Derivation of HP-TOP kernel",
      "text": [
        "Suppose we have obtained the parameters wc and bc of the separating hyperplane for each category c E Ccategory in the original feature space, where Ccategory denotes the set of categories.",
        "We assume that the class-posteriors Pc(+1ld) and",
        "where, for any category x, component function q(dl x) is of Gaussian-type:",
        "with the mean µx of a random variable wx • d – bx and the variance Qx.",
        "Those parameters are estimated with the maximum likelihood estimation, as follows:",
        "We choose the Gaussian-type function as an exam-ple.However, this choice is open to argument, since some other models also have the same computational advantage as described in Section 5.4.",
        "We set Bx1 = µx/Q2x, Bx2 = – 1/2Q2x.",
        "Although Bx1 and Bx2 are not the natural parameters of this model,",
        "we parameterize this model using the parameters 0.,1, 0.,2, w.,, b., and P(x) (dx E Ccategory) for simplicity.",
        "Using this probabilistic model,we compute function v(d, 0) as described in Appendix B (0 denotes �w.,, b.,, 0.,1, 0.,2I x E Ccategory} and w.,i denotes the i-th element of the weight vector w.,).",
        "The partial derivatives of this function with respect to the parameters are in Appendix C. Then we can follow the definition (10) to obtain our version of the TOP kernel.",
        "We call this new kernel a hyperplane-based TOP (HP-TOP) kernel."
      ]
    },
    {
      "heading": "5.2 Properties of HP-TOP kernel",
      "text": [
        "In the derivatives (39), which provide the largest number of features, original features di are accompanied by other factors computed from probability distributions.",
        "This form suggests that two vectors are considered to be more similar, if they have similar distributions over categories.",
        "In other words, an occurrence of a word can have different contribution to the classification result, depending on the context (i.e., the other words in the document).",
        "This property of the HP-TOP kernel can lead to the effect of word sense disambiguation, because “bank” in a financial document is treated differently from “bank” in a document related to a riverside park.",
        "The derivatives (34) and (35) correspond to the first-order differences, respectively for the positive class and the negative class.",
        "Similarly, the derivatives (36) and (37) for the second-order differences.",
        "The derivatives (40) and (41) are for the first-order differences normalized by the variances.",
        "The derivatives other than (38) and (38) directly depend on the distance from a hyperplane, rather than on the value of each feature.",
        "These derivatives enrich the feature set, when there are few active words, by which we mean the words that do not occur in the training data.",
        "For this reason, we expect that the HP-TOP kernel works well for a small training dataset."
      ]
    },
    {
      "heading": "5.3 Computational issue",
      "text": [
        "Computing the kernel in this form is time-consuming, because the number of components of type (39) can be very large:",
        "where I denotes the set of indices for original features.",
        "However, we can avoid this heavy computational cost as follows.",
        "Let us compute the dot-product of derivatives (39) of two vectors d1 and d2, which is shown in Appendix D. The last expression (45) is regarded as the scalar product of two dot-products.",
        "Thus, by preserving vectors d and we can efficiently compute the dot-product in (39); the computational complexity of a kernel function is",
        "on the condition that the original dimension is larger than the number of categories.",
        "Thus, from the viewpoint of computational time, our kernel has an advantage over some other kernels such as the PLSI-based Fisher kernel in Section 4, which requires the computational complexity of O (III x I Ccluster I), where Ccluster denotes the set of clusters.",
        "In the PLSI-based Fisher kernel, each word has a probability distribution over latent classes.",
        "In this sense, the PLSI-based Fisher kernel is more detailed, but detailed models are sometimes suffer overfitting to the training data and have the computational disadvantage as mentioned above.",
        "The PLSI-based Fisher kernel can be extended to a TOP kernel by using given categories as latent classes.",
        "However, the problem of computational time still remains."
      ]
    },
    {
      "heading": "5.4 General statement about the computational advantage",
      "text": [
        "So far, we have discussed the computational time for the kernel constructed on the Gaussian mixture.",
        "However, the computational advantage of the kernel, in fact, is shared by a more general class of models.",
        "We examine the required conditions for the computational advantage.",
        "Suppose the class-posteriors have the mixture form as Equations (16) and (17), but function q(dI x) does not have to be a Gaussian-type function.",
        "Instead, function q(dI x) is supposed to be represented using some function r parametrized by we and b, as:",
        "where f., is a scalar function.",
        "Then, let us obtain the derivative of v (d, 0) with respect to wei, which is the bottleneck of kernel computation:",
        "The first two factors of (25) do not depend on i.",
        "Therefore, if the last factor of (25) is variable-separable with respect to e and i:",
        "where S and T are some function, then the derivative (25) is also variable-separable.",
        "In such cases, the efficient computation described in Section 5.3 is possible by preserving the vectors:",
        "We have now obtained the required conditions for the efficient computation: Equation (24) and the variable-separability.",
        "In case of Gaussian-type functions, function fe and its derivative with respect to wei are",
        "Thus, the conditions are satisfied."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "Through experiments of text categorization, we empirically compare the HP-TOP kernel with the linear kernel and the PLSI-based Fisher kernel.",
        "We use Reuters-21578 dataset2 with ModApte-split (Dumais et al., 1998).",
        "In addition, we delete some texts from the result of ModApte-split, because those texts have no text body.",
        "After the deletion, we obtain 8815 training examples and 3023 test examples.",
        "The words that occur less than five times in the whole training set are excluded from the original feature set.",
        "We do not use all the 8815 training examples.",
        "The size of the actual training data ranges from 1000 to 8000.",
        "For each dataset size, experiments are executed 10 times with different training sets.The result is evaluated with F-measures for the most frequent 10 categories (Table 1).",
        "The total number of categories is actually 116.",
        "However, for small categories, reliable statistics cannot be obtained.",
        "For this reason, we regard the remaining categories other than the 10 most frequent categories as one category.",
        "Therefore, the model for negative examples is a mixture of 10 component models (9 out of the 10 most frequent categories and the new category consisting of the remaining categories).",
        "We assume uniform priors for categories as in (Tsuda et al., 2002).",
        "We computed the Fisher kernels with different numbers (10, 20 and 30) of latent classes and added them together to make a robust kernel (Hofmann, 2000).",
        "After the learning in the original feature space, the parameters for the probability distributions are estimated with",
        "maximum likelihood estimation as in Equations (19) and (20), followed by the learning with the proposed kernel.",
        "We used an SVM package, TinySVM3, for SVM computation.",
        "The soft-margin parameter C was set to 1.0 (other values of C showed no significant changes in results).",
        "The result is shown in Figure 1 (for macro-average) and Figure 2 (for micro-average).",
        "The HP-TOP kernel outperforms the linear kernel and the PLSI-based Fisher kernel for every number of examples.",
        "At each number of examples, we conducted a Wilcoxon Signed Rank test with 5% significance-level, for the HP-TOP kernel and the linear kernel, since these two are better than the other.",
        "The test shows that the difference between the two methods is significant for the training data sizes 1000 to 5000.",
        "The superiority of the HP-TOP kernel for small training datasets supports our expectation that the enrichment of feature set will lead to better performance for few active words.",
        "Although we also expected that the effect of word sense disambiguation would improve accuracy for large training datasets, the experiments do not provide us with an empirical evidence for the expectation.",
        "One possible reason is that Gaussian-type functions do not reflect the actual distribution of data.",
        "We leave its further investigation as future research.",
        "In this experimental setting, the PLSI-based Fisher kernel did not work well in terms of categorization accuracy.",
        "However, this Fisher kernel will perform better when the number of labeled examples is small and a number of unlabeled examples are available, as reported by Hofmann (2000).",
        "We also measured computational time of each method (Figure 3).",
        "The vertical axis indicates the average computational time over 100 runs of experiments (10 runs for each category).",
        "Please note that training time in this fig"
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We proposed a TOP kernel based on separating hyperplanes.",
        "The proposed kernel is created from one-dimensional Gaussians along the normal directions of the hyperplanes.",
        "We showed that the computational advantage that the proposed kernel has is shared by a more general class of models.",
        "We empirically showed that the proposed kernel outperforms the linear kernel in text categorization.",
        "Although the superiority of the proposed method to the 8000 linear kernel was shown, the proposed method has to be further investigated.",
        "Firstly, for large data sizes (namely 7000 and 8000), the proposed method was not significantly better than the linear kernel.",
        "The effectiveness of the proposed method should be confirmed by more experiments and theoretical analysis.",
        "Secondly, we have to compare the proposed method with other kernels in order to check the effectiveness of the kernel function consisting of one-dimensional Gaussians normal to the hyperplanes.",
        "The use of Gaussians is open to argument, because their symmetric form is somewhat against our",
        "intuition.",
        "This model can be extended to incorporate unlabeled examples, for example, using the EM algorithm.",
        "In that sense, the combination of PLSI and the semi-supervised EM algorithm is also one promising model.",
        "When the category structure of the negative examples is not given, the proposed method is not applicable.",
        "We should investigate whether unsupervised clustering can substitute for the category structure."
      ]
    }
  ]
}
