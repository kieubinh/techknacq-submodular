{
  "info": {
    "authors": [
      "Ciprian Chelba",
      "Alex Acero"
    ],
    "book": "SIGDAT Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W04-3237",
    "title": "Adaptation of Maximum Entropy Capitalizer: Little Data Can Help a Lot",
    "url": "https://aclweb.org/anthology/W04-3237",
    "year": 2004
  },
  "references": [
    "acl-H92-1073",
    "acl-J96-1002",
    "acl-N04-1039",
    "acl-P03-1020",
    "acl-W02-1001",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.",
        "The technique is applied to the problem of recovering the correct capitalization of uniformly cased text: a “background” capitalizer trained on 20Mwds of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets – one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text – from 1996.",
        "The “in-domain” performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994.",
        "When evaluating on the mismatched “out-of-domain” test data, the 1-gram baseline is outperformed by 60%; the improvement brought by the adaptation technique using a very small amount of matched BN data – 25-70kwds – is about 20-25% relative.",
        "Overall, automatic capitalization error rate of 1.4% is achieved on BN data."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking.",
        "Capitalization can be also used as a preprocessing step in named entity extraction or machine translation.",
        "We study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data-driven approaches since vast amounts of “training” data are easily obtainable by simply wiping the case information in text.",
        "As in previous approaches, the problem is framed as an instance of the class of sequence labeling problems.",
        "A case frequently encountered in practice is that of using mismatched – out-of-domain, in this particular case we used Broadcast News – test data.",
        "For example, one may wish to use a capitalization engine developed on newswire text for email or office documents.",
        "This typically affects negatively the performance of a given model, and more sophisticated models tend to be more brittle.",
        "In the capitalization case we have studied, the relative performance improvement of the MEMM capitalizer over the 1-gram baseline drops from in-domain – WSJ – performance of 45% to 35-40% when used on the slightly mismatched BN data.",
        "In order to take advantage of the adaptation data in our scenario, a maximum a-posteriori (MAP) adaptation technique for maximum entropy (MaxEnt) models is developed.",
        "The adaptation procedure proves to be quite effective in further reducing the capitalization error of the WSJ MEMM capitalizer on BN test data.",
        "It is also quite general and could improve performance of MaxEnt models in any scenario where model adaptation is desirable.",
        "A further relative improvement of about 20% is obtained by adapting the WSJ model to Broadcast News (BN) text.",
        "Overall, the MEMM capitalizer adapted to BN data achieves 60% relative improvement in accuracy over the 1-gram baseline.",
        "The paper is organized as follows: the next section frames automatic capitalization as a sequence labeling problem, presents previous approaches as well as the widespread and highly suboptimal 1 gram capitalization technique that is used as a baseline in most experiments in this work and others.",
        "The MEMM sequence labeling technique is briefly reviewed in Section 3.",
        "Section 4 describes the MAP adaptation technique used for the capitalization of out-of-domain text.",
        "The detailed mathematical derivation is presented in Appendix A.",
        "The experimental results are presented in Section 5, followed by conclusions and suggestions for future work."
      ]
    },
    {
      "heading": "2 Capitalization as Sequence Tagging",
      "text": [
        "Automatic capitalization can be seen as a sequence tagging problem: each lower-case word receives a tag that describes its capitalization form.",
        "Similar to the work in (Lita et al., 2003), we tag each word in a sentence with one of the tags:",
        "• LOC lowercase • CAP capitalized • MXC mixed case; no further guess is made as to the capitalization of such words.",
        "A possibility is to use the most frequent one encountered in the training data.",
        "• AUC all upper case • PNC punctuation; we decided to have a separate tag for punctuation since it is quite frequent and models well the syntactic context in a parsimonious way",
        "For training a given capitalizer one needs to convert running text into uniform case text accompanied by the above capitalization tags.",
        "For example, PrimeTime continues on ABC .PERIOD Now ,COMMA from Los Angeles ,COMMA Diane Sawyer .PERIOD",
        "The text is assumed to be already segmented into sentences.",
        "Any sequence labeling algorithm can then be trained for tagging lowercase word sequences with capitalization tags.",
        "At test time, the uniform case text to be capitalized is first segmented into sentences1 after which each sentence is tagged."
      ]
    },
    {
      "heading": "2.1 1-gram capitalizer",
      "text": [
        "A widespread algorithm used for capitalization is the 1-gram tagger: for every word in a given vocabulary (usually large, 100kwds or more) use the most frequent tag encountered in a large amount of training data.",
        "As a special case for automatic capitalization, the most frequent tag for the first word in a sentence is overridden by CAP, thus capitalizing on the fact that the first word in a sentence is most likely capitalized2.",
        "Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline.",
        "The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well."
      ]
    },
    {
      "heading": "2.2 Previous Work",
      "text": [
        "We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003).",
        "In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques.",
        "The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.",
        "A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.",
        "Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).",
        "The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach:",
        "• discriminative training of probability model P(T|W) using conditional maximum likelihood is well correlated with tagging accuracy • ability to use a rich set of word-level features in a parsimonious way: sub-word features such as prefixes and suffixes, as well as future words3 are easily incorporated in the probability model • no concept of “out-of-vocabulary” word: sub-word features are very useful in dealing with words not seen in the training data • ability to integrate rich contextual features into the model",
        "More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard part-of-speech tagging task.",
        "In a similar vein, the work",
        "of (Collins, 2002) explores the use of discrimina-tively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.",
        "The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).",
        "We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).",
        "Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.",
        "A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible."
      ]
    },
    {
      "heading": "3 MEMM for Sequence Labeling",
      "text": [
        "A simple approach to sequence labeling is the maximum entropy Markov model.",
        "The model assigns a probability P(T|W) to any possible tag sequence T = t1 ... tn = T1n for a given word sequence W = w1... wn.",
        "The probability assignment is done according to:",
        "where ti is the tag corresponding to word i and xi (W, Ti-1) is the conditioning information at position i in the word sequence on which the probability model is built.",
        "The approach we took is the one in (Ratnaparkhi, 1996), which uses xi(W,Ti-1) = {wi, wi-1, wi+1, ti-1, ti-2}.",
        "We note that the probability model is causal in the sequencing of tags (the probability assignment for ti only depends on previous tags ti-1, ti-2) which allows for efficient algorithms that search for the most likely tag sequence T* (W) = arg maxT P(T |W) as well as ensures a properly normalized conditional probability model P(T|W).",
        "The probability P(ti|xi(W,T1i-1)) is modeled using a maximum entropy model.",
        "The next section briefly describes the training procedure; for details the reader is referred to (Berger et al., 1996)."
      ]
    },
    {
      "heading": "3.1 Maximum Entropy State Transition Model",
      "text": [
        "The sufficient statistics that are extracted from the training data are tuples (y, #, x) = (ti, #, xi(W, Ti-1)) where ti is the tag assigned in context xi(W, T1i-1) = {wi, wi-1, wi+1, ti-1, ti-2} and # denotes the count with which this event has been observed in the training data.",
        "By way of example, the event associated with the first word in the example in Section 2 is (*bdw* denotes a special boundary type): MXC 1 currentword=primetime previousword=*bdw* nextword=continues t1=*bdw* t1,2=*bdw*,*bdw* prefix1=p prefix2=pr prefix3=pri suffix1=e suffix2=me suffix3=ime The maximum entropy probability model P(y|x) uses features which are indicator functions of the type:",
        "Assuming a set of features F whose cardinality is F, the probability assignment is made according to:",
        "where Λ = {λ1... λF} ∈ RF is the set of real-valued model parameters."
      ]
    },
    {
      "heading": "3.1.1 Feature Selection",
      "text": [
        "We used a simple count cut-off feature selection algorithm which counts the number of occurrences of all features in a predefined set after which it discards the features whose count is less than a pre-specified threshold.",
        "The parameter of the feature selection algorithm is the threshold value; a value of 0 will keep all features encountered in the training data.",
        "The model parameters Λ are estimated such that the model assigns maximum log-likelihood to the training data subject to a Gaussian prior centered at 0, Λ ∼ N(0, diag(σ2i )), that ensures smoothing (Chen and Rosenfeld, 2000):",
        "As shown in (Chen and Rosenfeld, 2000) – and re-derived in Appendix A for the non-zero mean case – the update equations are:",
        "In our experiments the variances are tied to σi = σ whose value is determined by line search on development data such that it yields the best tagging accuracy."
      ]
    },
    {
      "heading": "4 MAP Adaptation of Maximum Entropy Models",
      "text": [
        "In the adaptation scenario we already have a MaxEnt model trained on the background data and we wish to make best use of the adaptation data by balancing the two.",
        "A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.",
        "A Gaussian prior for the model parameters Λ has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.",
        "The prior has 0 mean and diagonal covariance: Λ ∼ N(0, diag(σ2 i )).",
        "In the adaptation scenario, the prior distribution used is centered at the parameter values Λ0 estimated from the background data instead of 0: Λ ∼ N(Λ0, diag(σ2i )).",
        "The regularized log-likelihood of the adaptation training data becomes:",
        "The adaptation is performed in stages:",
        "• apply feature selection algorithm on adaptation data and determine set of features Fadapt.",
        "• build new model by taking the union of the background and the adaptation feature sets: F = Fbackground ∪ Fadapt; each of the background features receives the corresponding weight λi determined on the background training data; the new features",
        "Fadapt \\ Fbackground4 introduced in the model receive 0 weight.",
        "The resulting model is thus equivalent with the background model.",
        "• train the model such that the regularized log-likelihood of the adaptation training data is maximized.",
        "The prior mean is set at Λ0 = Λbackground · 0; · denotes concatenation between the parameter vector for the background model and a 0-valued vector of length |Fadapt \\ Fbackground |corresponding to the weights for the new features.",
        "As shown in Appendix A, the update equations are very similar to the 0-mean case:",
        "The effect of the prior is to keep the model parameters λi close to the background ones.",
        "The cost of moving away from the mean for each feature fi is specified by the magnitude of the variance σi: a small variance σi will keep the weight λi close to its mean; a large variance σi will make the regularized log-likelihood (see Eq.",
        "3) insensitive to the prior on λi, allowing the use of the best value λi for modeling the adaptation data.",
        "Another observation is that not only the features observed in the adaptation data get updated: even if E˜p(x,y) [ fi] = 0, the weight λi for feature fi will still get updated if the feature fi triggers for a context x encountered in the adaptation data and some predicted value y – not necessarily present in the adaptation data in context x.",
        "In our experiments the variances were tied to σi = σ whose value was determined by line search on development data drawn from the adaptation data.",
        "The common variance σ will thus balance optimally the log-likelihood of the adaptation data with the Λ0 mean values obtained from the background data.",
        "Other tying schemes are possible: separate values could be used for the Fadapt \\ Fbackground and Fbackground feature sets, respectively.",
        "We did not experiment with various tying schemes although this is a promising research direction."
      ]
    },
    {
      "heading": "4.1 Relationship with Minimum Divergence Training",
      "text": [
        "Another possibility to adapt the background model is to do minimum KL divergence (MinDiv) train",
        "ing (Pietra et al., 1995) between the background exponential model B – assumed fixed – and an exponential model A built using the Fbackground U Fadapt feature set.",
        "It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 – following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models – then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5 .",
        "However, we wish to point out that the equivalence holds only if the feature set for the new model A is Fbackground U Fadapt .",
        "The straightforward application of MinDiv training – by using only the Fadapt feature set for A – will not result in an equivalent procedure to ours.",
        "In fact, the difference in performance between this latter approach and ours could be quite large since the cardinality of Fbackground is typically several orders of magnitude larger than that of Fadapt and our approach also updates the weights corresponding to features in Fbackground \\ Fadapt .",
        "Further experiments are needed to compare the performance of the two approaches."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 – files WS 8 7_{ 0 01-12 6 }.",
        "The in-domain test data used was file WS 94_000 (8.7kwds).",
        "As for the adaptation experiments, two different sets of BN data were used, whose sizes are summarized in Table 1:",
        "1.",
        "BN CNN/NPR data.",
        "The training/development/test partition consisted of a 3-way random split of file BN624BTS.",
        "The resulting sets are denoted CNN-trn/dev/tst, respectively 2.",
        "BN ABC Primetime data.",
        "The training set consisted of file BN623ATS whereas the development/test set consisted of a 2-way random split of file BN624ATS"
      ]
    },
    {
      "heading": "5.1 In-Domain Experiments",
      "text": [
        "We have proceeded building both 1-gram and MEMM capitalizers using various amounts of background training data.",
        "The model sizes for the 1 gram and MEMM capitalizer are presented in Table 2.",
        "Count cut-off feature selection has been used",
        "for the MEMM capitalizer with the threshold set at 5, so the MEMM model size is a function of the training data.",
        "The 1-gram capitalizer used a vocabulary of the most likely 100k wds derived from the training data.",
        "We first evaluated the in-domain and out-of-domain relative performance of the 1-gram and the MEMM capitalizers as a function of the amount of training data.",
        "The results are presented in Table 3.",
        "The MEMM capitalizer performs about 45% better",
        "domain (WSJ-test) and out-of-domain (BN-dev) data for various amounts of training data than the 1-gram one when trained and evaluated on Wall Street Journal text.",
        "The relative performance improvement of the MEMM capitalizer over the 1 gram baseline drops to 35-40% when using out-of-domain Broadcast News data.",
        "Both models benefit from using more training data."
      ]
    },
    {
      "heading": "5.2 Adaptation Experiments",
      "text": [
        "We have then adapted the best MEMM model built on 20Mwds on the two BN data sets (CNN/ABC) and compared performance against the 1-gram and the unadapted MEMM models.",
        "There are a number of parameters to be tuned on development data.",
        "Table 4 presents the variation in model size with different count cut-off values for the feature selection procedure on the adaptation data.",
        "As can be seen, very few features are added to the background model.",
        "Table 5 presents the variation in log-likelihood and capitalization accuracy on the CNN adaptation training and development data, respectively.",
        "The adaptation procedure was found",
        "trn adaptation data; the entry corresponding to the cut-off threshold of 106 represents the number of features in the background model to be insensitive to the number of reestimation iterations, and, more surprisingly, to the number of features added to the background model from the adaptation data, as shown in 5.",
        "The most sensitive parameter is the prior variance Q2, as shown in Figure 1; its value is chosen to maximize classification accuracy on development data.",
        "As expected, low values of Q2 result in no adaptation at all, whereas high values of Q2 fit the training data very well, and result in a dramatic increase of training data log-likelihood and accuracies approaching 100%.",
        "count cut-off and Q2 variance values; log-likelihood and accuracy on adaptation data CNN-trn as well as accuracy on held-out data CNN-dev; the background model results (no new features added) are the entries corresponding to the cut-off threshold of",
        "Finally, Table 6 presents the results on test data for 1-gram, background and adapted MEMM.",
        "As can be seen, the background MEMM outperforms the 1-gram model on both BN test sets by about 35-40% relative.",
        "Adaptation improves performance even further by another 20-25% relative.",
        "Overall, the adapted models achieve 60% relative reduction in capitalization error over the 1-gram baseline on both BN test sets.",
        "An intuitively satisfying result is the fact that the cross-test set performance (CNN",
        "adapted model evaluated on ABC data and the other way around) is worse than the adapted one."
      ]
    },
    {
      "heading": "6 Conclusions and Future Work",
      "text": [
        "The MEMM tagger is very effective in reducing both in-domain and out-of-domain capitalization error by 35%-45% relative over a 1-gram capitalization model.",
        "We have also presented a general technique for adapting MaxEnt probability models.",
        "It was shown to be very effective in adapting a background MEMM capitalization model, improving the accuracy by 20-25% relative.",
        "An overall 50-60% reduction in capitalization error over the standard 1-gram baseline is achieved.",
        "A surprising result is that the adaptation performance gain is not due to adding more, domain-specific features but rather making better use of the background features for modeling the in-domain data.",
        "As expected, adding more background training data improves performance but a very small amount of domain specific data also helps significantly if one can make use of it in an effective way.",
        "The",
        "could be amended by “..., especially if it’s the right data!”.",
        "As future work we plan to investigate the best way to blend increasing amounts of less-specific background training data with specific, in-domain data for this and other problems.",
        "Another interesting research direction is to explore the usefulness of the MAP adaptation of MaxEnt models for other problems among which we wish to include language modeling, part-of-speech tagging, parsing, machine translation, information extraction, text routing."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Special thanks to Adwait Ratnaparkhi for making available the code for his MEMM tagger and MaxEnt trainer."
      ]
    },
    {
      "heading": "References",
      "text": []
    },
    {
      "heading": "Appendix",
      "text": []
    },
    {
      "heading": "A Modified IIS for MaxEnt MAP Adaptation Using a Gaussian Prior",
      "text": [
        "The regularized log-likelihood of the training data – to be maximized by the MAP adaptation training algorithm – is:",
        "where the last equality holds because the argument of the log is independent of y.",
        "The derivation of the updates follows very closely the one in (Chen and Rosenfeld, 2000) for smoothing a MaxEnt model by using a Gaussian prior with 0 mean and diagonal covariance matrix.",
        "At each iteration we seek to find a set of updates for Λ, ∆ = {δi}, that increase the regularized log-likelihood L(Λ) by the largest amount possible.",
        "After a few basic algebraic manipulations, the difference in log-likelihood caused by a ∆ change in the Λ values becomes: the maximum value of A(∆, Λ) – which is non-negative and thus guarantees that the regularized log-likelihood does not decrease at each iteration: L(Λ + ∆∗) − L(Λ) ≥ 0.",
        "Solving for the root of∂AaB ,Λ) = 0 results in the update Eq.",
        "4 and is equivalent to finding the solution to:",
        "Following the same lower bounding technique as in (Chen and Rosenfeld, 2000) by using log x ≤ x −1 and Jensen’s inequality for the U-convexity of the exponential we obtain:",
        "A convenient way to solve this equation is to substi",
        "and then use Newton’s method for finding the solution to ai = f (αi), where f (α) is:",
        "The summation on the right hand side reduces to accumulating the coefficients of a polynomial in α whose maximum degree is the highest possible value of f# (x, y) on any context x encountered in the training data and any allowed predicted value y∈Y.",
        "where f# (x, y) = EFi=1 fi (x, y).",
        "Taking the first and second partial derivative of A(∆, Λ) with respect to δi we obtain, respectively:",
        "and",
        "for the unique root ∆∗ of ∂A(∆,Λ) = 0 we obtain"
      ]
    }
  ]
}
