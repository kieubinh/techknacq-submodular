{
  "info": {
    "authors": [
      "Martin Jansche"
    ],
    "book": "Human Language Technology Conference and Empirical Methods in Natural Language Processing",
    "id": "acl-H05-1087",
    "title": "Maximum Expected F-Measure Training of Logistic Regression Models",
    "url": "https://aclweb.org/anthology/H05-1087",
    "year": 2005
  },
  "references": [
    "acl-H94-1048",
    "acl-J96-1002",
    "acl-N03-2014",
    "acl-P03-1021"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We consider the problem of training logistic regression models for binary classification in information extraction and information retrieval tasks.",
        "Fitting probabilistic models for use with such tasks should take into account the demands of the task-specific utility function, in this case the well-known F-measure, which combines recall and precision into a global measure of utility.",
        "We develop a training procedure based on empirical risk minimization / utility maximization and evaluate it on a simple extraction task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Log-linear models have been used in many areas of Natural Language Processing (NLP) and Information Retrieval (IR).",
        "Scenarios in which log-linear models have been applied often involve simple binary classification decisions or probability assignments, as in the following three examples: Ratnaparkhi et al.",
        "(1994) consider a restricted form of the prepositional phrase attachment problem where attachment decisions are binary; Ittycheriah et al.",
        "(2003) reduce entity mention tracking to the problem of modeling the probability of two mentions being linked; and Greiff and Ponte (2000) develop models of probabilistic information retrieval that involve binary decisions of relevance.",
        "What is common to all three approaches is the application of log-linear models to binary classification tasks.1 As Ratnaparkhi (1998,",
        "p.",
        "27f.)",
        "points out, log-linear models of binary response variables are equivalent to, and in fact mere notational variants of, logistic regression models.",
        "In this paper we focus on binary classification tasks, and in particular on the loss or utility associated with classification decisions.",
        "The three problems mentioned before – prepositional phrase attachment, entity mention linkage, and relevance of a document to a query – differ in one crucial aspect: The first is evaluated in terms of accuracy or, equivalently, symmetric zero–one loss; but the second and third are treated as information extraction/retrieval problems and evaluated in terms of recall and precision.",
        "Recall and precision are combined into a single overall utility function, the well-known F-measure.",
        "It may be desirable to estimate the parameters of a logistic regression model by maximizing F-measure during training.",
        "This is analogous, and in a certain sense equivalent, to empirical risk minimization, which has been used successfully in related areas, such as speech recognition (Rahim and Lee, 1997), language modeling (Paciorek and Rosenfeld, 2000), and machine translation (Och, 2003).",
        "The novel contribution of this paper is a training procedure for (approximately) maximizing the expected F-measure of a probabilistic classifier based on a logistic regression model.",
        "We formulate a vector-valued utility function which has a well-defined expected value; F-measure is then a rational function of this expectation and can be maximized numerically under certain conventional regularizing assumptions.",
        "We begin with a review of logistic regression (Section 2) and then discuss the use of F-measure for evaluation (Section 3).",
        "We reformulate F-measure as a function of an expected utility (Section 4) which is maximized during training (Section 5).",
        "We discuss the differences between our parameter estimation technique and maximum likelihood training on a toy example (Section 6) as well as on a real extraction task (Section 7).",
        "We conclude with a discussion of further applications and generalizations (Section 8)."
      ]
    },
    {
      "heading": "2 Review of Logistic Regression",
      "text": [
        "Bernoulli regression models are conditional probability models of a binary response variable Y given a vector XX of k explanatory variables (X1,...,Xk).",
        "We will use the convention2 that Y takes on a value y E {-1,+1}.",
        "Logistic regression models (Cox, 1958) are perhaps best viewed as instances of generalized linear models (Nelder and Wedderburn, 1972; McCullagh and Nelder, 1989) where the the response variable follows a Bernoulli distribution and the link function is the logit function.",
        "Let us summarize this first, before expanding the relevant definitions:",
        "What this means is that there is an unobserved quantity p, the success probability of the Bernoulli distribution, which we interpret as the probability that Y will take on the value + 1:",
        "The logit function is used to transform a probability, constrained to fall within the interval (0, 1), into a real number ranging over (_∞, +∞).",
        "The inverse function of the logit is the cumulative distribution",
        "function of the standard logistic distribution (also known as the sigmoid or logistic function), which we call g:",
        "We also adopt the usual convention that xx – (1,x1,x2,...,xk), which is a k+ 1-dimensional vector whose first component is always 1 and whose remaining k components are the values of the k explanatory variables.",
        "So the Bernoulli probability can be expressed as",
        "The conditional probability model then takes the following abbreviated form, which will be used throughout the rest of this paper:",
        "A classifier can be constructed from this probability model using the MAP decision rule.",
        "This means predicting the label +1 if Pr(+1 I x, 9) exceeds 1/2, which amounts to the following:",
        "This illustrates the well-known result that a MAP classifier derived from a logistic regression model is equivalent to a (single-layer) perceptron (Rosen-blatt, 1958) or linear threshold unit."
      ]
    },
    {
      "heading": "3 F-Measure",
      "text": [
        "Suppose the parameter vector 0 of a logistic regression model is known.",
        "The performance of the resulting classifier can then be evaluated in terms of the recall (or sensitivity) and precision of the classifier on an evaluation dataset.",
        "Recall (R) and precision (P) are defined in terms of the number of true positives (A), misses (B), and false alarms (C) of the classifier (cf. Table 1):",
        "The Fa measure – familiar from Information Retrieval – combines recall and precision into a single utility criterion by taking their a-weighted harmonic mean:",
        "The Fa measure can be expressed in terms of the triple (A, B, C) as follows: Fa (A, B, C) _ A A+aB+(1 – a)C In order to define A, B, and C formally, we use the notation Q7r� to denote a variant of the Kronecker delta defined like this, where 7G is a Boolean expression:",
        "Given an evaluation dataset (x1,y1), • • •,(.xn,yn) the counts of hits (true positives), misses, and false alarms are, respectively:",
        "Note that F-measure is seemingly a global measure of utility that applies to an evaluation dataset as a whole: while the F-measure of a classifier evaluated on a single supervised instance is well defined, the overall F-measure on a larger dataset is not a function of the F-measure evaluated on each instance in the dataset.",
        "This is in contrast to ordinary loss/ utility, whose grand total (or average) on a dataset can be computed by direct summation."
      ]
    },
    {
      "heading": "4 Relation to Expected Utility",
      "text": [
        "We reformulate F-measure as a scalar-valued rational function composed with a vector-valued utility function.",
        "This allows us to define notions of expected and average utility, setting up the discussion of parameter estimation in terms of empirical risk minimization (or rather, utility maximization).",
        "Define the following vector-valued utility function u, where u(˜y I y) is the utility of choosing the label y˜ if the true label is y:",
        "This function indicates whether a classification decision is a hit, miss, or false alarm.",
        "Correct rejections are not counted.",
        "Expected values are, of course, well-defined for vector-valued functions.",
        "For example, the expected utility is",
        "In empirical risk minimization we approximate the expected utility of a classifier by its average utility US on a given dataset S = (x1,y10,...,(xn,yn):",
        "So US = n-1(A,B,C) where A, B, and C are as defined before.",
        "This means that we can interpret the",
        "F-measure of a classifier as a simple rational function of its empirical average utility (the scaling factor 1/n in (3) can in fact be omitted).",
        "This allows us to approach the parameter estimation task as an empirical risk minimization or utility maximization problem."
      ]
    },
    {
      "heading": "5 Discriminative Parameter Estimation",
      "text": [
        "In the preceding two sections we assumed that the parameter vector 9 was known.",
        "Now we turn to the problem of estimating 0 by maximizing the F-measure formulated in terms of expected utility.",
        "We make the dependence on 9 explicit in the formulation of the optimization task:",
        "where (A(6),B(6),C(d)) = US (6) as defined in (3).",
        "We encounter the usual problem: the basic quantities involved are integers (counts of hits, misses, and false alarms), and the optimization objective is a piecewise-constant functions of the parameter vector 9, due to the fact that 9 occurs exclusively inside Kronecker deltas.",
        "For example: �Ymap(x) _ +1� _ Pr(+ 1 Ix, e) > 0.5� In general, we can set �Pr(+ 1 1.z, 9) > 0.5� Pz� Pr(+1 1x, 9), (4) and in the case of logistic regression this arises as a special case of approximating the limit",
        "with a fixed value of y = 1.",
        "The choice of y does not matter much.",
        "The important point is that we are now dealing with approximate quantities which depend continuously on 9.",
        "In particular A(9) pz� ˜A(9), where",
        "Since the marginal total of positive instances npos (cf. Table 1) does not depend on 9, we use the identi-ties˜B(�e) =npos – ˜A(�e) and ˜mpos(6) =˜A(e)+˜C(e) to rewrite the optimization objective as ˜Fa: ˜Fa( ) = A(e)",
        "Maximization of F˜ as defined in (6) can be carried out numerically using multidimensional optimization techniques like conjugate gradient search (Fletcher and Reeves, 1964) or quasi-Newton methods such as the BFGS algorithm (Broyden, 1967; Fletcher, 1970; Goldfarb, 1970; Shanno,1970).",
        "This requires the evaluation of partial derivatives.",
        "The jth partial derivative of F˜ is as follows:",
        "One can compute the value of ˜F(9) and its gradient ∇˜F(9) simultaneously at a given point 9 in O(nk) time and O(k) space.",
        "Pseudo-code for such an algorithm appears in Figure 1.",
        "In practice, the inner loops on lines 8–9 and 14–18 can be made more efficient by using a sparse representation of the row vectors x[i].",
        "A concrete implementation of this algorithm can then be used as a callback to a multidimensional optimization routine.",
        "We use the BFGS minimizer provided by the GNU Scientific Library (Galassi et al., 2003).",
        "Important caveat: the function F˜ is generally not concave.",
        "We deal with this problem by taking the maximum across several runs of the optimization algorithm starting from random initial values.",
        "The next section illustrates this point further."
      ]
    },
    {
      "heading": "6 Comparison with Maximum Likelihood",
      "text": [
        "A comparison with the method of maximum likelihood illustrates two important properties of discriminative parameter estimation.",
        "Consider the toy dataset in Table 2 consisting of four supervised instances with a single explanatory variable.",
        "Thus the logistic regression model has two parameters and takes the following form:",
        "A surface plot of L is shown in Figure 2.",
        "Observe that L is concave; its global maximum occurs near (00, 01) Pz� (0.35,0.57), and its value is always strictly negative because the toy dataset is not linearly separable.",
        "The classifier resulting from maximum likelihood training predicts the label +1 for all training instances and thus achieves a recall of 3/3 and precision 3/4 on its training data.",
        "The Fa=0.5 measure is 6/7.",
        "Contrast the shape of the log-likelihood function L with the function ˜Fa.",
        "Surface plots of ˜Fa=0.5 and ˜Fa=0.25 appear in Figure 3.",
        "The figures clearly illustrate the first important (but undesirable) property of ˜F, namely the lack of concavity.",
        "They also illustrate a desirable property, namely the ability to take into account certain properties of the loss function during training.",
        "The ˜Fa=0.5 surface in the left panel of Figure 3 achieves its maximum in the right corner for (00,01) – (+∞, +∞).",
        "If we choose (00,01) _ (20,15) the classifier labels every instance of the training data with + 1. fdf(0):",
        "1: m 0 2: A 0 3: for j 0 to k do 4: dm[j] 0 5: dA j 0 6: fori 1tondo 7: p 0 8: for j + – 0 to k do 9: p – p+x[i][j] x 0[j] 10: p <-- 1/(1 +exp( – d)) 11: m <-- m+p 12: ify[i] _ +1 then 13: A+ – A+p 14: for j + – 0 to k do 15: t<--px(1 – p)xx[i][j] 16: dm[j] dm[j] +t 17: ify[i] _ +1 then 18: dA j – dA j +t 19: h+ – 1/(axnpos+(1 – a)xm) 20: F+ – hxA 21: t+ – Fx(1 – a) 22: for j + – 0 to k do 23: dF[j]+ – hx(dA j – txdm[j]) 24: return (F, dF)",
        "corner for (00, 01) – * ( – ∞, +∞).",
        "If we set (00, 01) _ (-20,15) the resulting classifier labels the first two",
        "instances (x = 0 and x = 1) as – 1 and the last two instances (x = 2 and x = 3) as +1.",
        "The classifier trained according to the ˜Fa=0.5 criterion achieves an Fa=0.5 measure of 6/7 Pz� 0.86, compared with 4/5 = 0.80 for the classifier trained according to the ˜Fa=0.25 criterion.",
        "Conversely, that classifier achieves an Fa=0.25 measure of 8/9 Pz� 0.89 compared with 4/5 = 0.80 for the classifier trained according to the ˜Fa=0.5 criterion.",
        "This demonstrates that the training procedure can effectively take information from the utility function into account, producing a classifier that performs well under a given evaluation criterion.",
        "This is the result of optimizing a task-specific utility function during training, not simply a matter of adjusting the decision threshold of a trained classifier."
      ]
    },
    {
      "heading": "7 Evaluation on an Extraction Problem",
      "text": [
        "We evaluated our discriminative training procedure on a real extraction problem arising in broadcast news summarization.",
        "The overall task is to summarize the stories in an audio news broadcast (or in the audio portion of an A/V broadcast).",
        "We assume that story boundaries have been identified and that each story has been broken up into sentence-like units.",
        "A simple way of summarizing a story is then to classify each sentence as either belonging into a summary or not, so that a relevant subset of sentences can be extracted to form the basis of a summary.",
        "What makes the classification task hard, and therefore interesting, is the fact that reliable features are hard to come by.",
        "Existing approaches such as Maskey and Hirschberg 2005 do well only when combining diverse features such as lexical cues, acoustic properties, structural/ positional features, etc.",
        "The task has another property which renders it problematic, and which prompted us to develop the discriminative training procedure described in this paper.",
        "Summarization, by definition, aims for brevity.",
        "This means that in any dataset the number of positive instances will be much smaller than the number of negative instances.",
        "Given enough data, balance could be restored by discarding negative instances.",
        "This, however, was not an option in our case: a moderate amount of manually labeled data had been produced and about one third would have had to be discarded to achieve a balance in the distribution of class labels.",
        "This would have eliminated precious supervised training data, which we were not prepared to do.",
        "The training and test data were prepared by Maskey and Hirschberg (2005), who performed the feature engineering, imputation of missing values, and the training–test split.",
        "We used the data unchanged in order to allow for a comparison between approaches.",
        "The dataset is made up of 30 features, divided into one binary response variable, and one binary explanatory variable plus 28 integer and real-valued explanatory variables.",
        "The training portion consists of 3 535 instances, the test portion of 408 instances.",
        "We fitted logistic regression models in three different ways: by maximum likelihood ML, by ˜Fa=0.5 maximization, and by ˜Fa=0.75 maximization.",
        "Each",
        "call (R), precision (P), Fa=0.5 measure, and Fa=0.75 measure recorded.",
        "The results appear in Table 3.",
        "The row labeled MLt is special: the classifier used here is the logistic regression model fitted by maximum likelihood; what is different is that the threshold for positive predictions was adjusted post hoc to match the number of true positives of the first dis-criminatively trained classifier.",
        "This has the same effect as manually adjusting the threshold parameter e0 based on partial knowledge of the test data (via the performance of another classifier) and is thus not permissible.",
        "It is interesting to note, however, that the ML trained classifier performs worse than the ˜Fa=0.5 trained classifier even when one parameter is adjusted by an oracle with knowledge of the test data and the performance of the other classifier.",
        "Fitting a model based on ˜Fa=0.75, which gives increased weight to recall compared with ˜Fa=0.5, led to higher recall as expected.",
        "However, we also expected that the Fa=0.75 score of the ˜Fa=0.75 trained classifier would be higher than the Fa=0.75 score of the ˜Fa=0.5 trained classifier.",
        "This is not the case, and could be due to the optimization getting stuck in a local maximum, or it may have been an unreasonable expectation to begin with."
      ]
    },
    {
      "heading": "8 Conclusions",
      "text": [
        "We have presented a novel estimation procedure for probabilistic classifiers which we call, by a slight abuse of terminology, maximum expected F-measure training.",
        "We made use of the fact that expected utility computations can be carried out in a vector space, and that an ordering of vectors can be imposed for purposes of maximization which can employ auxiliary functions like the F-measure (2).",
        "This technique is quite general and well suited for working with other quantities that can be expressed in terms of hits, misses, false alarms, correct rejections, etc.",
        "In particular, it could be used to find a point estimate which provides a certain tradeoff between specificity and sensitivity, or operating point.",
        "A more general method would try to optimize several such operating points simultaneously, an issue which we will leave for future research.",
        "The classifiers discussed in this paper are logistic regression models.",
        "However, this choice is not crucial.",
        "The approximation (4) is reasonable for binary decisions in general, and one can use it in conjunction with any well-behaved conditional Bernoulli model or related classifier.",
        "For Support Vector Machines, approximate F-measure maximization was introduced by Musicant et al.",
        "(2003).",
        "Maximizing F-measure during training seems especially well suited for dealing with skewed classes.",
        "This can happen by accident, because of the nature of the problem as in our summarization example above, or by design: for example, one can expect skewed binary classes as the result of the one-vs-all reduction of multi-class classification to binary classification; and in multi-stage classification one may want to alternate between classifiers with high recall and classifiers with high precision.",
        "Finally, the ability to incorporate non-standard tradeoffs between precision and recall at training time is useful in many information extraction and retrieval applications.",
        "Human end-users often create asymmetries between precision and recall, for good reasons: they may prefer to err on the side of caution (e.g., it is less of a problem to let an unwanted spam email reach a user than it is to hold back a legitimate message), or they may be better at some tasks than others (e.g., search engine users are good at filtering out irrelevant documents returned by a query, but are not equipped to crawl the web in order to look for relevant information that was not retrieved).",
        "In the absence of methods that work well for a wide range of operating points, we need training procedures that can be made sensitive to rare cases depending on the particular demands of the application."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "I would like to thank Julia Hirschberg, Sameer Maskey, and the three anonymous reviewers for helpful comments.",
        "I am especially grateful to",
        "Sameer Maskey for allowing me to use his speech summarization dataset for the evaluation in Section 7.",
        "The usual disclaimers apply."
      ]
    }
  ]
}
