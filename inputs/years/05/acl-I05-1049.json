{
  "info": {
    "authors": [
      "Sriram Venkatapathy",
      "Aravind K. Joshi"
    ],
    "book": "Second International Joint Conference on Natural Language Processing: Full Papers",
    "id": "acl-I05-1049",
    "title": "Relative Compositionality of Multi-word Expressions: A Study of Verb-Noun (V-N) Collocations",
    "url": "https://aclweb.org/anthology/I05-1049",
    "year": 2005
  },
  "references": [
    "acl-J93-1003",
    "acl-J93-1007",
    "acl-P01-1025",
    "acl-P89-1010",
    "acl-P98-2210",
    "acl-P99-1041",
    "acl-T75-2013",
    "acl-W01-0513",
    "acl-W03-1809",
    "acl-W03-1810",
    "acl-W03-1812",
    "acl-W04-3224"
  ],
  "sections": [
    {
      "text": [
        "Sriram Venkatapathy'* and Aravind K. Joshi",
        "Language Technologies Research Center, International Institute of Information Technology - Hyderabad, Hyderabad, India sriram@research.iiit.ac.in Department of Computer and Information Science",
        "Abstract.",
        "Recognition of Multi-word Expressions (MWEs) and their relative compositionality are crucial to Natural Language Processing.",
        "Various statistical techniques have been proposed to recognize MWEs.",
        "In this paper, we integrate all the existing statistical features and investigate a range of classifiers for their suitability for recognizing the non-compositional Verb-Noun (V-N) collocations.",
        "In the task of ranking the V-N collocations based on their relative compositionality, we show that the correlation between the ranks computed by the classifier and human ranking is significantly better than the correlation between ranking of individual features and human ranking.",
        "We also show that the properties 'Distributed frequency of object' (as defined in [27]) and 'Nearest Mutual Information' (as adapted from [18]) contribute greatly to the recognition of the non-compositional MWEs of the V-N type and to the ranking of the V-N collocations based on their relative compositionality."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The main goals of the work presented in this paper are (1) To investigate a range of classifiers for their suitability in recognizing the non-compositional V-N collocations, and (2) To examine the relative compositionality of collocations of V-N type.",
        "Measuring the relative compositionality of V-N collocations is extremely helpful in applications such as machine translation where the collocations that are highly non-compositional can be handled in a special way.",
        "Multi-word expressions (MWEs) are those whose structure and meaning cannot be derived from their component words, as they occur independently.",
        "Examples include conjunctions like 'as well as' (meaning 'including'), idioms like",
        "* Part of the work was done at Institute for Research in Cognitive Science, University of Pennsylvania, Philadelphia, PA 19104, USA, when he was visiting IRCS as a visiting Scholar, February to December, 2004.",
        "'kick the bucket' (meaning 'die'), phrasal verbs like 'find out' (meaning 'search') and compounds like 'village community'.",
        "A typical natural language system assumes each word to be a lexical unit, but this assumption does not hold in case of MWEs [6] [12].",
        "They have idiosyncratic interpretations which cross word boundaries and hence are a 'pain in the neck' [23].",
        "They account for a large portion of the language used in day-to-day interactions [25] and so, handling them becomes an important task.",
        "A large number of MWEs have a standard syntactic structure but are non-compositional semantically.",
        "An example of such a subset is the class of non-compositional verb-noun collocations (V-N collocations).",
        "The class of V-N collocations which are non-compositional is important because they are used very frequently.",
        "These include verbal idioms [22], support-verb constructions [1] [2]etc.",
        "The expression 'take place' is a MWE whereas 'take a gift' is not a MWE.",
        "It is well known that one cannot really make a binary distinction between compositional and non-compositional MWEs.",
        "They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes [4].",
        "So, we rate the MWEs (V-N collocations in this paper) on a scale from 1 to6where6denotesacompletely compositional expression, while 1 denotes a completely opaque expression.",
        "But, to address the problem of identification, we still need to do an approximate binary distinction.",
        "We call the expressions with a rating of 4 to 6 compositional and the expressions with rating of 1 to 3 as non-compositional.",
        "(See Section 4 for further details).",
        "Various statistical measures have been suggested for identification of MWEs and ranking expressions based on their compositionality.",
        "Some of these are Frequency, Mutual Information [9], Log-Likelihood [10] and Pearson's x [8].",
        "Integrating all the statistical measures should provide better evidence for recognizing MWEs and ranking the expressions.",
        "We use various Machine Learning Techniques (classifiers) to integrate these statistical features and classify the VN collocations as MWEs or Non-MWEs.",
        "We also use a classifier to rank the V-N collocations according to their compositionality.",
        "We then compare these ranks with the ranks provided by the human judge.",
        "A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features and the ranks of human judges has been done by McCarthy, Keller and Caroll [19] for verb-particle constructions.",
        "(See Section 3 for more details).",
        "Some preliminary work on recognition of V-N collocations was presented in [28].",
        "In the task of classification, we show that the technique of weighted features in distance-weighted nearest-neighbour algorithm performs slightly better than other machine learning techniques.",
        "We also find that the 'distributed frequency of object (as defined by [27])' and 'nearest mutual information (as adapted from [18])' are important indicators of the non-compositionality of MWEs.",
        "In the task of ranking, we show that the ranks assigned by the classifier correlated much better with the human judgement than the ranks assigned by individual statistical measures.",
        "This paper is organised in the following sections (2) Basic Architecture, (3) Related work, (4) Data used for the experiments, (5) Agreement between the Judges, (6) Features, (7) Experiments - Classification, (8) Experiments Ranking and (9) Conclusion."
      ]
    },
    {
      "heading": "2. Basic Architecture",
      "text": [
        "Recognition of MWEs can be regarded as a classification task where every V-N collocation can be classified eitherasaMWE oras a Non-MWE.",
        "Every V-N collocation is represented as a vector of features which are composed largely of various statistical measures.",
        "The values of these features for the V-N collocations are extracted from the British National Corpus.",
        "For example, the V-N collocation 'raise an eyebrow' can be represented as [ Frequency = 271, Mutual Information = 8.43, Log-Likelihood = 1456.29, etc.",
        "].",
        "Now, to recognise the MWEs, the classifier has to do a binary classification of this vector.",
        "So, ideally, the classifier should take the above information and classify 'raise an eyebrow' as an MWE.",
        "The classifier can also be used to rank these vectors according to their relative compositionality."
      ]
    },
    {
      "heading": "3. Related Work",
      "text": [
        "Church and Hanks (1989) proposed a measure of association called Mutual Information [9].",
        "Mutual Information (MI) is the logarithm of the ratio between the probability of the two words occurring together and the product of the probability of each word occurring individually.",
        "The higher the MI, the more likely are the words to be associated with each other.",
        "The usefulness of the statistical approach suggested by Church and Hanks [9] is evaluated for the extraction of V-N collocations from German text Corpora [7].",
        "Several other measures like Log-Likelihood [10], Pearson's x [8], Z-Score [8] , Cubic Association Ratio (MI3), Log-Log [17], etc., have been proposed.",
        "These measures try to quantify the association of the two words but do not talk about quantifying the non-compositionality of MWEs.",
        "Dekang Lin proposes a way to automatically identify the non-compositionality of MWEs [18].",
        "He suggests that a possible way to separate compositional phrases from non-compositional ones is to check the existence and mutual-information values of phrases obtained by replacing one of the words with a similar word.",
        "According to Lin, a phrase is probably non-compositional if such substitutions are not found in the collocations database or their mutual information values are significantly different from that of the phrase.",
        "Another way of determining the non-compositionality of V-N collocations is by using 'distributed frequency of object'(DFO) in V-N collocations [27].",
        "The basic idea in there is that \"if an object appears only with one verb (or few verbs) in a large corpus we expect that it has an idiomatic nature\" [27].",
        "Schone and Jurafsky [24] applied Latent-Semantic Analysis (LSA) to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from the corpus.",
        "An interesting way of quantifying the relative com-positionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows [3].",
        "They use latent semantic analysis (LSA) to determine the similarity between an MWE and its constituent words, and claim that higher similarity indicates great decomposability.",
        "In terms of compositionality, an expression is likely to be relatively more compositional if it is decomposable.",
        "They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Wordnet based decomposibility theory [3].",
        "Evert and Krenn [11] compare some of the existing statistical features for the recognition of MWEs of adjective-noun and preposition-noun-verb types.",
        "Galiano, Valdivia, Santiago and Lopez [14] use five statistical measures to classify generic MWEs using the LVQ (Learning Vector Quantization) algorithm.",
        "In contrast, we do a more detailed and focussed study of V-N collocations and the ability of various classifiers in recognizing MWEs.",
        "We also compare the roles of various features in this task.",
        "McCarthy, Keller and Caroll [19] judge compositionality according to the degree of overlap in the set of most similar words to the verb-particle and head verb.",
        "They showed that the correlation between their measures and the human ranking was better than the correlation between the statistical features and the human ranking.",
        "We have done similar experiments in this paper where we compare the correlation value of the ranks provided by the classifier with the ranks of the individual features for the V-N collocations.",
        "We show that the ranks given by the classifier which integrates all the features provides a significantly better correlation than the individual features."
      ]
    },
    {
      "heading": "4. Data Used for the Experiments",
      "text": [
        "The data used for the experiments is British National Corpus of 81 million words.",
        "The corpus is parsed using Bikel's parser [5] and the Verb-Object Collocations are extracted.",
        "There are 4,775,697 V-N of which 1.2 million were unique.",
        "All the V-N collocations above the frequency of 100 (n=4405) are taken to conduct the experiments so that the evaluation of the system is feasible.",
        "These 4405 V-N collocations were searched in Wordnet, American Heritage Dictionary and SAID dictionary (LDC,2003).",
        "Around 400 were found in at least one of the dictionaries.",
        "Another 400 were extracted from the rest so that the evaluation set has roughly equal number of compositional and non-compositional expressions.",
        "These 800 expressions were annotated with a rating from 1 to 6 by using guidelines independently developed by the authors.",
        "1 denotes the expressions which are totally non-compositional while 6 denotes the expressions which are totally compositional.",
        "The brief explanation of the various rating are (1) No word in the expression has any relation to the actual meaning of the expression.",
        "Example: \"leave a mark\".",
        "(2) Can be replaced by a single verb.",
        "Example : \"take alook\".",
        "(3) Although meanings of both words are involved, at least one of the words is not used in the usual sense.",
        "Example : \"break news\".",
        "(4) Relatively more compositional than (3).",
        "Example : \"prove a point\".",
        "(5) Relatively less compositional than (6).",
        "Example : \"feel safe\".",
        "(6) Completely compositional.",
        "Example : \"drink coffee\".",
        "For the experiments on classification (Section 7), we call the expressions with ratings of 4 to 6 as compositional and the expressions with rating of 1 to 3 as non-compositional.",
        "For the experiments on ranking the expressions based on their relative compositionality, we use all the 6 ratings to represent the relative compositionality of these expressions."
      ]
    },
    {
      "heading": "5. Agreement Between the Judges",
      "text": [
        "The data was annotated by two fluent speakers of English.",
        "For 765 collocations out of 800, both the annotators gave a rating.",
        "For the rest, atleast one of the annotators marked the collocations as \"don't know\".",
        "Table 1 illustrates the details of the annotations provided by the two judges.",
        "Table 1.",
        "Details of the annotations of the two annotators",
        "From the table we see that annotator1 distributed the rating more uniformly among all the collocations while annotator2 observed that a significant proportion of the collocations were completely compositional.",
        "To measure the agreement between the two annotators, we used the Kendall's TAU (t).",
        "t is the correlation between the rankings1 of collocations given by the two annotators.",
        "W ranges between 0 (little agreement) and 1 (full agreement).",
        "W is calculated as below,",
        "T~ vcft-TiXTo^fy",
        "where T0 = n(n - 1)/2, T1 = £ti(ti - 1)/2, T2 = £ui(ui - 1)/2 and where, n is the number of collocations, ti is the number of tied x values of ith group of tied x values and ui is the number of tied y values of ith group of tied y values.",
        "We obtained a t score of 0.61 which is highly significant.",
        "This shows that the annotators were in a good agreement with each other in deciding the rating to be given to the collocations.",
        "We also compare the ranking of the two annotators using Spearman's Rank-Correlation coefficient (rs) (more details in section 8).",
        "We obtained a rs score of 0.71 indicating a good agreement between the annotators.",
        "A couple of examples where the annotators differed are (1) \"perform a task\" was rated 3 by annotator1 while it was rated 6 by annotator2 and (2) \"pay tribute\" was rated 1 by annotator1 while it was rated 4 by annotator2.",
        "The 765 samples annotated by both the annotators were then divided into a training set and a testing set in several possible ways to cross-validate the results of classification and ranking.",
        "Ratings",
        "6",
        "5",
        "4",
        "3",
        "2",
        "1",
        "Compositional",
        "Non-Compositional",
        "(4 to 6)",
        "(1 to 3)",
        "Annotatorl",
        "141",
        "122",
        "127",
        "119",
        "161",
        "95",
        "390",
        "375",
        "Annotator2",
        "303",
        "88",
        "79",
        "101",
        "118",
        "76",
        "470",
        "195",
        "6Features",
        "Each collocation is represented by a vector whose dimensions are the statistical features obtained from the British National Corpus.",
        "This list of features are given in Table 2.2 While conducting the experiments, all features are scaled from 0 to 1 to ensure that all features are represented uniformly.",
        "Table 2.",
        "List of features and their top-3 example collocations"
      ]
    },
    {
      "heading": "7. Experiments - Classification",
      "text": [
        "The evaluation data (765 vectors) is divided randomly into training and testing vectors in 10 ways for cross-validation.",
        "The training data consists of 90% of 786 vectors and the testing data consists of the remaining.",
        "We used various Machine Learning techinques to classify the V-N collocations into MWEs and non-MWEs.",
        "For every classifier, we calculated the average accuracy of all the test sets of each of the annotators.",
        "We then compare the average accuracies of all the classifiers.",
        "We found that the classifier that we used, the technique of weighted features in distance-weighted nearest-algorithm, performs somewhat better than other machine learning techniques.",
        "The following are brief descriptions of the classifiers that we used in this paper.",
        "Feature",
        "Top-3",
        "Feature",
        "Top-3",
        "take place",
        "Mutual Information",
        "shrug shoulder",
        "Frequency",
        "have effect have time",
        "[9]",
        "bridge gap plead guilty",
        "Cubic Association",
        "take place",
        "Log-Log",
        "shake head",
        "Measure",
        "shake head",
        "[17]",
        "commit suicide",
        "(Oakes, 1998)",
        "play role",
        "fall asleep",
        "Log-Likelihood",
        "take place",
        "Pearson's \\",
        "shake head",
        "[10]",
        "shake head play role",
        "[8]",
        "commit suicide fall asleep",
        "T-Score",
        "take place",
        "Z-Score",
        "shake head",
        "[9]",
        "have effect shake head",
        "[26]",
        "commit suicide fall asleep",
        "^-coefficient",
        "bridge gap",
        "Distributed",
        "come true",
        "shrug shoulder",
        "freq.",
        "of object",
        "become difficult",
        "press button",
        "(DFO) [27]",
        "make sure",
        "Nearest MI",
        "Collocations",
        "Whether object",
        "(Binary feature)",
        "(NMI)",
        "with no",
        "can occur",
        "[18]",
        "neigh.",
        "MI",
        "as a verb",
        "Whether object",
        "(Binary feature)",
        "is a nomin.",
        "of some verb",
        "Relative Compositionality of Multi-word Expressions 559 7.1 Nearest-Neighbour Algorithm",
        "This is an instance-based learning technique where the test vector is classified based on its nearest vectors in the training data.",
        "The simple distance between two vectors and Xj is defined as d(xj ,Xj), where",
        "Here, x is an instance of a vector and ar (x) is the value of the rth feature.",
        "One can use K neighbours to judge the class of the test vector.",
        "The test vector is assigned the class of maximum number of neighbours.",
        "This can be furthur modified by calculating the inverse weighted distance between the test vector and the neighbouring training vectors in each of the classes.",
        "The test vector is then assigned the class which has the higher inverse-weighted distance.",
        "One can also use all the training vectors and the weighted-distance principle to classify the test vector.",
        "The average classification accuracy of each of the above methods on the test sets of each of the annotators is shown in Table 3.",
        "Table 3.",
        "Average accuracies of MWE recognition using simple nearest-neighbour algorithms and weighted distance nearest neighbour algorithms",
        "SVMs [15] have been very successful in attaining high accuracy for various machine-learning tasks.",
        "Unlike the error-driven algorithms (Perceptron etc.",
        "), SVM searches for the two distinct classes and maximizes the margin between two classes.",
        "Data of higher dimension can also be classified using the appropriate Kernel.",
        "We used Linear and Polynomial Kernel (degree=2) to test the evaluation data.",
        "We also used the radial-basis network in SVMs to compare the results because of their proximity to the nearest-neigbour algorithms.",
        "Table 4.",
        "Average accuracies of MWE recognition using SVMs (Linear, Polynomial and Radial Basis Function Kernel)",
        "Simple K-Nearest neighbour",
        "Weighted-distance Nearest neighbour",
        "Type",
        "K=l",
        "K=2",
        "K=3",
        "K=l",
        "K=2",
        "K=3",
        "K=A11",
        "Annot.l",
        "62.35",
        "61.31",
        "62.48",
        "62.35",
        "62.35",
        "62.61",
        "66.66",
        "Annot.2",
        "57.64",
        "54.10",
        "60.89",
        "57.64",
        "57.64",
        "60.37",
        "63.52",
        "Linear Ker.",
        "Polynomial Ker.",
        "Radial Basis networks",
        "Parameters",
        "a = 0.5",
        "a = 1.0",
        "a = 1.5",
        "a = 2.0",
        "Annot.l",
        "65.89",
        "65.75",
        "67.06",
        "66.66",
        "66.93",
        "67.06",
        "Annot.2",
        "62.61",
        "65.09",
        "64.17",
        "63.51",
        "62.99",
        "62.99",
        "The average classification accuracy of each of the above methods on the test sets of each of the annotators is shown in Table 4.",
        "7.3 Weighted Features in Distance-Weighted Nearest-Neighbour Algorithm",
        "Among all the features used, only a few might be very relevant to recognizing the non-compositionality of the MWE.",
        "As a result, the distance metric used by the nearest-neighbour algorithm which depends on all the features might be misleading.",
        "The distance between the neighbour will be dominated by large number of irrelevant features.",
        "A way of overcoming this problem is to weight each feature differently when calculating the distance between the two instances.",
        "This also gives us an insight into which features are mainly responsible for recognizing the non-compositional-ity of MWEs.",
        "The jth feature can be multiplied by the weight Zj, where the values of z\\...zn are chosen to minimize the true classification error of the learning algorithm [20].",
        "The distance using these weights is represented as where zr is the weight of the rth feature.",
        "The values of z\\...zn can be determined by cross-validation of the training data.",
        "We use leave-one-out cross-validation [21], in which the set of m training vectors are repeatedly divided into a training set of m-1 and a test set of 1, in all possible ways.",
        "So, each vector in the training data is classified using the remaining vectors.",
        "The classification accuracy is defined as where classify(«)=1, if the ith training example is classified correctly using the distance-weighted nearest neighbour algorithm, otherwise classify(« )=0.",
        "Now, we try to maximize the classification accuracy in the following way, – In every iteration, vary the weights of the features one by one.",
        " – Choose the feature and its weight which brings the maximum increase in the value of Clacc.",
        "One can also choose the feature and its weight such that it brings the minimum increase in the value of Clacc.",
        " – Update the weight of this particular feature and go for the next iteration.",
        " – If thereis noincreaseinclassification accuracy, stop.",
        "When the weights are updated such that there is maximum increase in classification accuracy in every step, the average accuracies are 66.92% and 64.30% on the test sets of the two annotators respectively.",
        "But when the weights are updated such there is a minimum increase in classification accuracy at every",
        "Table 5.",
        "The top three features according to the average weight when there is maximum increase in Clacc at every step",
        "Table 6.",
        "The top three features according to the average weight calculated when there is minimum increase in Clacc at every step",
        "step, the average accuracies are 66.13% and 64.04% on the test sets of the two annotators respectively, which are slightly better than that obtained by the other Machine Learning Techniques.",
        "In the above two methods (Updating weights such that there is maximum or minimum increase in classification accuracy), we add the weights of the features of each of the evaluation sets.",
        "According to the average weights, the top three features (having high average weight) are shown in Tables 5 and 6.",
        "In both the above cases, we find that the properties 'Mutual-Information' and the compositionality oriented feature 'Distributed Frequency of an Object' performed significantly better than the other features."
      ]
    },
    {
      "heading": "8. Experiments - Ranking",
      "text": [
        "All the statistical measures show that the expressions ranked higher according to their decreasing values are more likely to be non-compositional.",
        "We compare these ranks with the average of the ranks given by the annotator (obtained from his rating).",
        "To compare, we use Spearman Rank-Order Correlation Coefficient (rs), defined as where Ri is the rank of ith x value, Si is the rank of ith y value, R is the mean of the Ri values and S is the mean of Si values.",
        "We use an SVM-based ranking system [16] for our training.",
        "Here, we use 10% of the 765 vectors for training and the remaining for testing.",
        "The SVM-based ranking system builds a preference matrix of the training vectors to learn.",
        "It then ranks the test vectors.",
        "The ranking system takes a lot of time to train itself, and hence, we decided to use only a small proportion of the evaluation set for training.",
        "Annotatorl",
        "Weight",
        "Annotator2",
        "Weight",
        "DFO",
        "1.09",
        "MI",
        "1.17",
        "T-Score",
        "1.0",
        "T-Score",
        "1.1",
        "Z-Score",
        "1.0",
        "(^-coefficient",
        "1.0",
        "Annot.l",
        "Weight",
        "Annot.2",
        "Weight",
        "DFO",
        "1.07",
        "MI",
        "2.06",
        "NMI",
        "1.02",
        "T-Score",
        "1.0",
        "Log-Like.",
        "0.97",
        "^-coefficient",
        "1.0",
        "Table 7.",
        "The correlation values of the ranking of individual features and the ranking of classifier with the ranking of human judgements",
        "We also compare our ranks (the average of the ranks suggested by the classifier) with the gold standard using the Spearman Rank-Order Correlation Coefficient.",
        "The results are shown in Table 7.",
        "InTable7,weobservethatthecorrelation between the ranks computed by the classifier and human ranking is better than the correlation between ranking of individual statistical features and human ranking.",
        "We observe that among all the statistical features the ranks based on the properties 'Mutual Information', 'Distributed Frequency of an Object' [27] and 'Nearest mutual information' [18] correlated better with the ranks provided by the annotator.",
        "This is in accordance with the observation we made while describing the classification experiments, where we observed that the properties 'Distributed Frequency of an Object' and 'Mutual Information' contributed much to the classification of the expressions.",
        "When we compare the correlation values of MI, Log-likelihood and x, we see that the Mutual-Information values correlated better.",
        "This result is similar to the observation made by McCarthy, Keller and Caroll [19] for phrasal verbs.",
        "9Conclusion",
        "In this paper, we integrated the statistical features using various classifiers and investigated their suitability for recognising non-compositional MWEs of the VN type.",
        "We also used a classifier to rank the V-N collocations according to their relative compositionality.",
        "This type of MWEs constitutes a very large percentage of all MWEs and are crucial for NLP applications, especially for Machine Translation.",
        "Our main results are as follows.",
        " – The technique of weighted features in distance-weighted nearest neighbour algorithm performs better than other Machine Learning Techniques in the task of recognition of MWEs of V-N type.",
        " – We show that the correlation between the ranks computed by the classifier and human ranking is significantly better than the correlation between ranking of individual features and human ranking.",
        " – The properties 'Distributed frequency of object' and 'Nearest MI' contribute greatly to the recognition of the non-compositional MWEs of the V-N type and to the ranking of the V-N collocations based on their relative composi-tionality.",
        "MI",
        "-0.125",
        "Z-Score",
        "-0.059",
        "MI3",
        "0.001",
        "0-coeff",
        "-0.102",
        "Log-Log",
        "-0.086",
        "DFO",
        "-0.113",
        "Log-Likelihood",
        "0.005",
        "NMI",
        "-0.167",
        "xA",
        "-0.056",
        "Class.",
        "0.388",
        "T-Score",
        "0.045",
        "Our future work will consist of the following tasks",
        " – Evaluate the effectiveness of the techniques developed in this paper for applications like Machine Translation.",
        " – Improve our annotation guidelines and create more annotated data.",
        " – Extend our approach to other types of MWEs."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We want to thank Libin Shen and Nikhil Dinesh for their help in clarifying various aspects of Machine Learning Techniques.",
        "We would like to thank Roderick Saxey and Pranesh Bhargava for annotating the data and Mark Mandel for considerable editorial help."
      ]
    }
  ]
}
