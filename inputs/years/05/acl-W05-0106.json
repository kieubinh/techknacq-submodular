{
  "info": {
    "authors": [
      "Nashira Lincoln",
      "Marc Light"
    ],
    "book": "Workshop on Effective Tools and Methodologies for Teaching NLP and CL",
    "id": "acl-W05-0106",
    "title": "Making Hidden Markov Models More Transparent",
    "url": "https://aclweb.org/anthology/W05-0106",
    "year": 2005
  },
  "references": [
    "acl-J93-2004",
    "acl-W02-0102"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Understanding the decoding algorithm for hidden Markov models is a difficult task for many students.",
        "A comprehensive understanding is difficult to gain from static state transition diagrams and tables of observation production probabilities.",
        "We have built a number of visualizations depicting a hidden Markov model for part-of-speech tagging and the operation of the Viterbi algorithm.",
        "The visualizations are designed to help students grasp the operation of the HMM.",
        "In addition, we have found that the displays are useful as debugging tools for experienced researchers."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Hidden Markov Models (HMMs) are an important part of the natural language processing toolkit and are often one of the first stochastic generation models that students1 encounter.",
        "The corresponding Viterbi algorithm is also often the first example of dynamic programming that students encounter.",
        "Thus, HMMs provide an opportunity to start students on the correct path of understanding stochastic models, not simply treating them as black boxes.",
        "Unfortunately, static state transition diagrams, tables of probability values, and lattice diagrams are not enough for many students.",
        "They have a general idea of how a HMM works but often have common",
        "misconceptions.",
        "For example, we have found that students often believe that as the Viterbi algorithm calculates joint state sequence observation sequence probabilities, the best state sequence so far is always a prefix of global best path.",
        "This is of course false.",
        "Working a long example to show this is very tedious and thus text books seldom provide such examples.",
        "Even for practitioners, HMMs are often opaque in that the cause of a mis-tagging error is often left uncharacterized.",
        "A display would be helpful to pinpoint why an HMM chose an incorrect state sequence instead of the correct one.",
        "Below we describe two displays that attempt to remedy the above mentioned problems and we discuss a Java implementation of these displays in the context of a part-of-speech tagging HMM (Kupiec, 1992).",
        "The system is freely available and has an XML model specification that allows models calculated by other methods to be viewed.",
        "(A standard maximum likelihood estimation was implemented and can be used to create models from tagged data.",
        "A model is also provided.)"
      ]
    },
    {
      "heading": "2 Displays",
      "text": [
        "Figure 1 shows a snapshot of our first display.",
        "It contains three kinds of information: most likely path for input, transition probabilities, and history of most likely prefixes for each observation index in the Viterbi lattice.",
        "The user can input text at the bottom of the display, e.g., Pelham pointed out that Georgia voters rejected the bill.",
        "The system then runs Viterbi and animates the search through all possible state sequences and displays the best state sequence prefix as it works its way through the observation",
        "most likely path for “Pelman pointed out that Georgia voters ...”; Middle pane: a mouse-over-triggered bar graph of out transition probabilities for a state; Bottom pane: a history of most likely prefixes for each observation index in the Viterbi lattice.",
        "Below the panes is the input text field.",
        "quence and by looking at the prefix history, a student Students (and researchers) need to understand has a good chance of dispelling the false belief of HMMs.",
        "We have built a display that allow users monotonicity.",
        "to probe different aspects of an HMM and watch The second display allows the user to contrast two Viterbi in action.",
        "In addition, our system provides state sequences for the same observation sequence.",
        "a display that allows users to contrast state sequence See Figure 2.",
        "For each contrasting state pairs, it probabilities.",
        "To drive these displays, we have built shows the ratio of the corresponding transition to a standard HMM system including parameter esti-each state and it shows the ratio of the generation of mating and decoding and provide a part-of-speech the observation conditioned on each state.",
        "For exam model trained on UPenn Treebank data.",
        "The system ple, in Figure 2 the transition DT – *JJ is less likely can also read in models constructed by other sys-than DT – *NNP.",
        "The real culprit is generation proba- tems.",
        "bility P(Equal IJJ) which is almost 7 times larger than This system was built during this year’s offering P(EqualI NNP).",
        "Later in the sequence we see a simi- of Introduction to Computational Linguistics at the lar problem with generating opportunity from a NNP University of Iowa.",
        "In the Spring of 2006 it will be state.",
        "These generation probabilities seem to drown deployed in the classroom for the first time.",
        "We plan out any gains made by the likelihood of NNP runs.",
        "on giving a demonstration of the system during a To use this display, the user types in a sentence lecture on HMMs and part-of-speech tagging.",
        "A rein the box above the graph and presses enter.",
        "The lated problem set using the system will be assigned.",
        "HMM is used to tag the input.",
        "The user then modi The students will be given several mis-tagged sen-fies (e.g., corrects) the tag sequence and presses en tences and asked to analyze the errors and report ter and the ratio bars then appear.",
        "on precisely why they occurred.",
        "A survey will be Let us consider another example: in Figure 2, the administered at the end and improvements will be mis-tagging of raises as a verb instead of a noun at made to the system based on the feedback provided.",
        "the end of the sentence.",
        "The display shows us that In the future we plan to implement Good-Turing although NN – *NNS is more likely than NN – *VBZ, smoothing and a method for dealing with unknown the generation probability for raises as a verb is words.",
        "We also plan to provide an additional display over twice as high as a noun.",
        "(If this pattern of that shows the traditional Viterbi lattice figure, i.e., mis-taggings caused by high generation probabil- observations listed left-to-right, possible states listed ity ratios was found repeatedly, we might consider from top-to-bottom, and lines from left-to-right con-smoothing these distributions more aggressively.)",
        "necting states at observation index i with the previ3 Implementation ous states, i-1, that are part of the most likely state The HMM part-of-speech tagging model and sequence to i.",
        "Finally, we would like to incorpo-corresponding Viterbi algorithm were implemented rate an additional display that will provide a visual-based on their description in the updated version, ization of EM HMM training.",
        "We will use (Eisner, http://www.cs.colorado.edu/˜martin/ 2002) as a starting point.",
        "SLP/updated.html , of chapter 8 of (Jurafsky 35"
      ]
    }
  ]
}
