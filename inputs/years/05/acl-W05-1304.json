{
  "info": {
    "authors": [
      "Yoshimasa Tsuruoka",
      "Sophia Ananiadou",
      "Jun'ichi Tsujii"
    ],
    "book": "Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics",
    "id": "acl-W05-1304",
    "title": "A Machine Learning Approach to Acronym Generation",
    "url": "https://aclweb.org/anthology/W05-1304",
    "year": 2005
  },
  "references": [
    "acl-J96-1002",
    "acl-W03-1018"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a machine learning approach to acronym generation.",
        "We formalize the generation process as a sequence labeling problem on the letters in the definition (expanded form) so that a variety of Markov modeling approaches can be applied to this task.",
        "To construct the data for training and testing, we extracted acronym-definition pairs from MEDLINE abstracts and manually annotated each pair with positional information about the letters in the acronym.",
        "We have built an MEMM-based tagger using this training data set and evaluated the performance of acronym generation.",
        "Experimental results show that our machine learning method gives significantly better performance than that achieved by the standard heuristic rule for acronym generation and enables us to obtain multiple candidate acronyms together with their likelihoods represented in probability values."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Technical terms and named-entities play important roles in knowledge integration and information retrieval in the biomedical domain.",
        "However, spelling variations make it difficult to identify the terms conveying the same concept because they are written in different manners.",
        "Acronyms constitute a major part of spelling variations (Nenadic et al., 2002), so proper management of acronyms leads to improved performance of the information systems in this domain.",
        "As for the methods for recognizing acronym-definition pairs from running text, there are many studies reporting high performance (e.g. over 96% accuracy and 82% recall) (Yoshida et al., 2000; Nenadic et al., 2002; Schwartz and Hearst, 2003; Zahariev, 2003; Adar, 2004).",
        "However, another aspect that we have to consider for efficient acronym management is to generate acronyms from the given definition (expanded form).",
        "One obvious application of acronym generation is to expand the keywords in information retrieval.",
        "As reported in (Wren et al., 2005), for example, you can retrieve only 25% of the documents concerning the concept of “JNK” by using the keyword “c jun N-terminal kinase”.",
        "In more than 33% of the documents the concept is written with its acronym “JNK”.",
        "To alleviate this problem, some research efforts have been devoted to constructing a database containing a large number of acronym-definition pairs from running text of biomedical documents (Adar, 2004).",
        "However, the major problem of this database-building approach is that building the database offering complete coverage is nearly impossible because not all the biomedical documents are publicly available.",
        "Although most of the abstracts of biomedical papers are publicly available on MEDLINE, there is still a large number of full-papers which are not available.",
        "In this paper, we propose an alternative approach",
        "Proceedings of the ACL-ISMB Workshop on Linking Biolo ical Literature, Ontologies and Databases: Mining Biological Semantics, pages 25–31, Detroit, June 2005.",
        "@0 2005 Association for Computational Linguistics to providing acronyms from their definitions so that we can obtain acronyms without consulting acronym-definition databases.",
        "One of the simplest way to generate acronyms from definitions would be to choose the letters at the beginning of each word and capitalize them.",
        "However, there are a lot of exceptions in the acronyms appearing in biomedical documents.",
        "The followings are some real examples of the definition-acronym pairs that cannot be created with the simple heuristic method.",
        "RNA polymerase (RNAP) antithrombin (AT) melanoma cell adhesion molecule (Mel-CAM) the xenoestrogen 4-tert-octylphenol (t-OP) In this paper we present a machine learning approach to automatic generation of acronyms in order to capture a variety of mechanisms of acronym generation.",
        "We formalize this problem as a sequence labeling task such as part-of-speech tagging, chunking and other natural language tagging tasks so that common Markov modeling approaches can be applied to this task."
      ]
    },
    {
      "heading": "2 Acronym Generation as a Sequence",
      "text": []
    },
    {
      "heading": "Labeling Problem",
      "text": [
        "Given the definition (expanded form), the mechanism of acronym generation can be regarded as the task of selecting the appropriate action on each letter in the definition.",
        "Figure 1 illustrates an example, where the definition is “Duck interferon gamma” and the generated acronym is “DuIFN-gamma”.",
        "The generation proceeds as follows: The acronym generator outputs the first two letters unchanged and skips the following three letters.",
        "Then the generator capitalizes ‘i’ and skip the following four letters... By assuming that an acronym is made up of alphanumeric letters, spaces and hyphens, the actions being taken by the generator are classified into the following five classes."
      ]
    },
    {
      "heading": "SKIP",
      "text": [
        "The generator skips the letter."
      ]
    },
    {
      "heading": "UPPER",
      "text": [
        "If the target letter is uppercase, the generator outputs the same letter.",
        "If the target letter is lowercase, the generator coverts the letter into the corresponding upper letter.",
        "LOWER If the target letter is lowercase, the generator outputs the same letter.",
        "If the target letter is uppercase, the generator coverts the letter into the corresponding lowercase letter."
      ]
    },
    {
      "heading": "SPACE",
      "text": [
        "The generator convert the letter into a space.",
        "HYPHEN The generator convert the letter into a hyphen.",
        "From the probabilistic modeling point of view, this task is to find the sequence of actions that maximizes the following probability given the observation",
        "Observations are the letters in the definition and various types of features derived from them.",
        "We decompose the probability in a left-to-right manner.",
        "(2) By making a first-order markov assumption, the equation becomes (3) If we have the training data containing a large number of definition-acronym pairs where the definition is annotated with the labels for actions, we can estimate the parameters of this probabilistic model and the best action sequence can be efficiently computed by using a Viterbi decoding algorithm.",
        "In this paper we adopt a maximum entropy model (Berger et al., 1996) to estimate the local probabilities since it can incorporate diverse types of features with reasonable computational cost.",
        "This modeling, as a whole, is called Maximum Entropy Markov Modeling (MEMM).",
        "and the acronym is “DuIFN-gamma”.",
        "Each letter in the acronym is generated from a letter in the definition following the action for the letter.",
        "Regularization is important in maximum entropy modeling to avoid overfitting to the training data.",
        "For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama and Tsujii, 2003).",
        "The model gives equally good performance as the maximum entropy modeling with Gaussian priors (Chen and Rosenfeld, 1999), and the size of the resulting model is much smaller than that of Gaussian priors because most of the parameters become zero.",
        "This characteristic enables us to easily handle the model data and carry out quick decoding, which is convenient when we repetitively perform experiments.",
        "This modeling has one parameter to tune, which is called width factor.",
        "We set this parameter to be 1.0 throughout the experiments."
      ]
    },
    {
      "heading": "3 The Data for Training and Testing",
      "text": [
        "Since there is no training data available for the machine learning task described in the previous section, we manually created the data.",
        "First, we extracted definition-acronym pairs from MEDLINE abstracts using the acronym acquisition method proposed by (Schwartz and Hearst, 2003).",
        "The abstracts used for constructing the data were randomly selected from the abstracts published in the year of 2001.",
        "Duplicated pairs were removed from the set.",
        "In acquiring the pairs from the documents, we focused only on the pairs that appear in the form of ... expanded form (acronym) ... We then manually removed misrecognized pairs and annotated each pair with positional information.",
        "The positional information tells which letter in the definition should correspond to a letter in the acronym.",
        "Table 1 lists a portion of the data.",
        "For example, the positional information in the first pair indicates that the first letter ‘i’ in the definition corresponds to ‘I’ in the acronym, and the 12th letter ‘m’ corresponds to ‘M’.",
        "With this positional information, we can create the training data for the sequence labeling task because there is one-to-one correspondence between the sequence labels and the data with positional information.",
        "In other words, we can determine the ap",
        "propriate action for each letter in the definition by comparing the letter with the corresponding letter in the acronym."
      ]
    },
    {
      "heading": "4 Features",
      "text": [
        "Maximum entropy modeling allows us to incorporate diverse types of features.",
        "In this paper we use the following types of features in local classification.",
        "As an example, consider the situation where we are going to determine the action at the letter ‘f’ in the definition “Duck interferon gamma”.",
        "Letter unigram (UNI) The unigrams of the neighboring letters.",
        "(i.e. “ r”, “ f”, and “ e”) Letter bigram (BI) The bigrams of the neighboring letters.",
        "(i.e. Letter trigram (TRI) The trigrams of the neighboring letters.",
        "(i.e. “ter”, “erf”, “rfe”, “ fer”, and “ ero”) Action history (HIS) The preceding action (i.e.",
        "SKIP) Orthographic features (ORT) Whether the target letter is uppercase or not (i.e. false)",
        "The number of the words in the definition (i.e. “len=3 ”) Letter sequence (SEQ)",
        "1.",
        "The sequence of the letters ranging from the beginning of the word to the target letter.",
        "(i.e. “ interf”) 2.",
        "The sequence of the letters ranging from the target letter to the end of the word.",
        "(i.e. “ feron”) 3.",
        "The word containing the target letter.",
        "(i.e. “ interferon”) Distance (DIS) 1.",
        "The distance between the target letter and the beginning of the word.",
        "(i.e. “ 6”) 2.",
        "The distance between the target letter and the tail of the word.",
        "(i.e. “ 5”)"
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "To evaluate the performance of the acronym generation method presented in the previous section, we ran fivefold cross validation experiments using the manually curated data set.",
        "The data set consists of 1,901 definition-acronym pairs.",
        "For comparison, we also tested the performance of the popular heuristics for acronym generation in which we choose the letters at the beginning of each word in the definition and capitalize them."
      ]
    },
    {
      "heading": "5.1 Generated Acronyms",
      "text": [
        "Tables 2 to 5 show some examples of generated acronyms together with their probabilities.",
        "They are sorted with their probabilities and the top ten acronyms are shown.",
        "The correct acronym given in the training data is described in the bottom row in each table.",
        "In Table 2, the definition is “traumatic brain injury” and the correct acronym is “TBI”.",
        "This is the simplest case in acronym generation, where the first letter of each word in the definition is to be capitalized.",
        "Our acronym generator gives a high probability to the correct acronym and it is ranked at the top.",
        "Table 3 shows a slightly more complex case, where the generator needs to convert the space be",
        "tween ‘F’ and ‘1’ into a hyphen.",
        "The correct answer is located at the third rank.",
        "The definition in Table 4 is “RNA polymerase” and the correct acronym is “RNAP”, so the generator needs to the first three letters unchanged.",
        "The correct answer is located at the fourth rank, and the probability given the correct answer does not have a large gap with the top-ranked acronym.",
        "Table 5 shows a more difficult case, where you need to output the first letter in lowercase and choose appropriate letters from the string having no delimiters (e.g. spaces and hyphens).",
        "Our acronym generator outputs the correct acronym at the nine-th rank but the probability given this acronym is very low compared to that given to the top-ranked string.",
        "Table 6 shows a similar case.",
        "The probability given to the correct acronym is very low."
      ]
    },
    {
      "heading": "5.2 Coverage",
      "text": [
        "Table 7 shows how much percentage of the correct acronyms are covered if we take top N candidates from the outputs of the acronym generator.",
        "The bottom line (BASELINE) shows the coverage achieved by generating one acronym using the standard heuristic rule for acronym generation.",
        "Note that the coverage achieved with a single candidate (Rank 1) is better that of BASELINE.",
        "If we take top five candidates, we can have a coverage of 75.4%, which is considerably better than that achieved by the heuristic rule.",
        "This suggests that the acronym generator could be used to significantly improve the performance of the systems for information retrieval and information integration."
      ]
    },
    {
      "heading": "5.3 Features",
      "text": [
        "To evaluate how much individual types of features affect the generation performance, we ran experiments using different feature types.",
        "Table 8 shows the results.",
        "Overall, the results show that various types of features have been successfully incorporated in the MEMM modeling and individual types of features contribute to improving performance.",
        "The performance achieved with only unigram features is almost the same as that achieved by the heuristic rule.",
        "Note that the features on the previous state improve the performance, which suggests that our selection of the states in the Markov modeling is a reasonable choice for this task."
      ]
    },
    {
      "heading": "5.4 Learning Curve",
      "text": [
        "Figure 2 shows a learning curve of our acronym generator, which shows the relationship between the number of the training samples and the performance of the system.",
        "The graph clearly indicates that the performance consistently improves as the training data increases and still continues to improve even when the size of the training data reaches the maximum.",
        "This suggests that we can achieve improved performance by increasing the annotated data for training."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We presented a machine learning approach to acronym generation.",
        "In this approach, we regarded the generation process as a sequence labeling problem, and we manually created the data for training and testing.",
        "Experimental results using 1901 definition-acronym pairs, we achieved a coverage of 55.1%, which is significantly bettern than that achieved by the standard heuristic rule for acronym generation.",
        "The algorithm also enables us to have other acronym candidates together with the probabilities representing their likelihood.",
        "6.1 Future work In this paper we did not consider the generation mechanisms where the letters in the acronym appear in a different order in the definition.",
        "Since about 3% of acronyms reportedly involve this types of generation mechanism (Schwartz and Hearst, 2003), we",
        "might further improve performance by considering such permutation of letters.",
        "As the learning curve (Fig 2) suggested, one obvious way to improve the performance is to increase the training data.",
        "The size of the training data used in the experiments is fairly small compared to those in other sequence tagging tasks such POS tagging and chunking.",
        "We plan to increase the size of the training data with a semi-automatic way that could reduce the human effort for annotation."
      ]
    }
  ]
}
