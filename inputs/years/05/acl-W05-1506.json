{
  "info": {
    "authors": [
      "Liang Huang",
      "David Chiang"
    ],
    "book": "International Workshop on Parsing Technology",
    "id": "acl-W05-1506",
    "title": "Better K-Best Parsing",
    "url": "https://aclweb.org/anthology/W05-1506",
    "year": 2005
  },
  "references": [
    "acl-A00-2018",
    "acl-C00-1011",
    "acl-J02-3001",
    "acl-J03-1006",
    "acl-J03-4003",
    "acl-J04-4002",
    "acl-J04-4004",
    "acl-J93-2004",
    "acl-J99-4004",
    "acl-N04-1022",
    "acl-N04-1023",
    "acl-P03-1021",
    "acl-P05-1012",
    "acl-P05-1022",
    "acl-P05-1033",
    "acl-W04-3201",
    "acl-W05-0636",
    "acl-W97-0301"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing.",
        "To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel’s implementation of Collins’ lexicalized PCFG model, and on Chiang’s CFG-based decoder for hierarchical phrase-based translation.",
        "We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many problems in natural language processing (NLP) involve optimizing some objective function over a set of possible analyses of an input string.",
        "This set is often exponential-sized but can be compactly represented by merging equivalent subanalyses.",
        "If the objective function is compatible with a packed representation, then it can be optimized efficiently by dynamic programming.",
        "For example, the distribution of parse trees for a given sentence under a PCFG can be represented as a packed forest from which the highest-probability tree can be easily extracted.",
        "However, when the objective function f has no compatible packed representation, exact inference would be intractable.",
        "To alleviate this problem, one common approach from machine learning is loopy belief propagation (Pearl, 1988).",
        "Another solution (which is popular in NLP) is to split the computation into two phases: in the first phase, use some compatible objective function f' to produce a k-best list (the top k candidates under f'), which serves as an approximation to the full set.",
        "Then, in the second phase, optimize f over all the analyses in the k-best list.",
        "A typical example is discriminative reranking on k-best lists from a generative module, such as (Collins, 2000) for parsing and (Shen et al., 2004) for translation, where the reranking model has nonlocal features that cannot be computed during parsing proper.",
        "Another example is minimum-Bayes-risk decoding (Kumar and Byrne, 2004; Goodman, 1998),where, assuming f' defines a probability distribution over all candidates, one seeks the candidate with the highest expected score according to an arbitrary metric (e.g., PARSEVAL or BLEU); since in general the metric will not be compatible with the parsing algorithm, the k-best lists can be used to approximate the full distribution f'.",
        "A similar situation occurs when the parser can produce multiple derivations that are regarded as equivalent (e.g., multiple lexicalized parse trees corresponding to the same unlexi-calized parse tree); if we want the maximum a posteriori parse, we have to sum over equivalent derivations.",
        "Again, the equivalence relation will in general not be compatible with the parsing algorithm, so the k-best lists can be used to approximate f', as in Data Oriented Parsing (Bod, 2000) and in speech recognition (Mohri and Riley, 2002).",
        "Another instance of this k-best approach is cascaded optimization.",
        "NLP systems are often cascades of modules, where we want to optimize the modules’ objective functions jointly.",
        "However, often a module is incompatible with the packed representation of the previous module due to factors like non-local dependencies.",
        "So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and coref-erence resolution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999).",
        "Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation.",
        "For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on",
        "Vancouver, October 2005. c�2005 Association for Computational Linguistics the data.",
        "Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU.",
        "For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finite-state methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002).",
        "This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders.",
        "Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class.",
        "In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005)."
      ]
    },
    {
      "heading": "2 Previous Work",
      "text": [
        "As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming.",
        "The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004).",
        "But this approach is prohibitively slow, and produces rather low-quality k-best lists (see Sec. 5.1.2).",
        "Gildea and Jurafsky (2002) described an O(k2)-overhead extension for the CKY algorithm and reimplemented Collins’ Model 1 to obtain k-best parses with an average of 14.9 parses per sentence.",
        "Their algorithm turns out to be a special case of our Algorithm 0 (Sec.",
        "4.1), and is reported to also be prohibitively slow.",
        "Since the original design of the algorithm described below, we have become aware of two efforts that are very closely related to ours, one by Jim´enez and Marzal (2000) and another done in parallel to ours by Charniak and Johnson (2005).",
        "Jim´enez and Marzal present an algorithm very similar to our Algorithm 3 (Sec.",
        "4.4) while Charniak and Johnson propose using an algorithm similar to our Algorithm 0, but with multiple passes to improve efficiency.",
        "They apply this method to the Charniak (2000) parser to get 50-best lists for reranking, yielding an improvement in parsing accuracy.",
        "Our work differs from Jim´enez and Marzal’s in the following three respects.",
        "First, we formulate the parsing problem in the more general framework of hypergraphs (Klein and Manning, 2001), making it applicable to a very wide variety of parsing algorithms, whereas Jim´enez and Marzal define their algorithm as an extension of CKY, for CFGs in Chomsky Normal Form (CNF) only.",
        "This generalization is not only of theoretical importance, but also critical in the application to state-of-the-art parsers such as (Collins, 2003) and (Charniak, 2000).",
        "In Collins’ parsing model, for instance, the rules are dynamically generated and include unary productions, making it very hard to convert to CNF by preprocessing, whereas our algorithms can be applied directly to these parsers.",
        "Second, our Algorithm 3 has an improvement over Jim´enez and Marzal which leads to a slight theoretical and empirical speedup.",
        "Third, we have implemented our algorithms on top of state-of-the-art, large-scale statistical parser/decoders and report extensive experimental results while Jim´enez and Marzal’s was tested on relatively small grammars.",
        "On the other hand, our algorithms are more scalable and much more general than the coarse-to-fine approach of Charniak and Johnson.",
        "In our experiments, we can obtain 10000-best lists nearly as fast as 1-best parsing, with very modest use of memory.",
        "Indeed, Charniak (p.c.)",
        "has adopted our Algorithm 3 into his own parser implementation and confirmed our findings.",
        "In the literature of k shortest-path problems, Minieka (1974) generalized the Floyd algorithm in a way very similar to our Algorithm 0 and Lawler (1977) improved it using an idea similar to but a little slower than the binary branching case of our Algorithm 1.",
        "For hypergraphs, Gallo et al.",
        "(1993) study the shortest hyperpath problem and Nielsen et al.",
        "(2005) extend it to k shortest hyperpath.",
        "Our work differes from (Nielsen et al., 2005) in two aspects.",
        "First, we solve the problem of k-best derivations (i.e., trees), not the k-best hyperpaths, although in many cases they coincide (see Sec. 3 for further discussions).",
        "Second, their work assumes non-negative costs (or probabilities <_ 1) so that they can apply Dijkstra-like algorithms.",
        "Although generative models, being probability-based, do not suffer from this problem, more general models (e.g., log-linear models) may require negative edge costs (McDonald et al., 2005; Taskar et al., 2004).",
        "Our work, based on the Viterbi algorithm, is still applicable as long as the hypergraph is acyclic, and is used by McDonald et al.",
        "(2005) to get the k-best parses."
      ]
    },
    {
      "heading": "3 Formulation",
      "text": [
        "Following Klein and Manning (2001), we use weighted directed hypergraphs (Gallo et al., 1993) as an abstraction of the probabilistic parsing problem.",
        "set of vertices, E is a finite set of hyperarcs, and R is the set of weights.",
        "Each hyperarc e E E is a triple",
        "tion from RlT(e)l to R. t E V is a distinguished vertex called target vertex.",
        "Note that our definition is different from those in previous work in the sense that the tails are now vectors rather than sets, so that we can allow multiple occurrences of the same vertex in a tail and there is an ordering among the components of a tail.",
        "Definition 2.",
        "A hypergraph H is said to be monotonic if there is a total ordering < on R such that every weight function f in H is monotonic in each of its arguments according to <, i.e., if f : Rm H R, then V1 <_ i <_ m, if ai < a'i, then f(a1,... ,ai,... ,am) < f(a1,... ,a'i,...,am).",
        "We also define the comparison function min� (a, b) to output a if a < b, or b if otherwise.",
        "In this paper we will assume this monotonicity, which corresponds to the optimal substructure property in dynamic programming (Cormen et al., 2001).",
        "Definition 3.",
        "We denote lel =lT(e)l to be the arity of the hyperarc.",
        "If lel = 0, then f(e) E R is a constant and we call h(e) a source vertex.",
        "We define the arity of a hypergraph to be the maximum arity of its hyperarcs.",
        "Definition 4.",
        "The backward-star BS(v) of a vertex v is the set of incoming hyperarcs {e E E l h(e) = v}.",
        "The in-degree of v is lBS (v)l. Definition 5.",
        "A derivation D of a vertex v in a hypergraph H, its size lDl and its weight w(D) are recursively defined as follows:",
        "• If e E BS (v) with lel = 0, then D = (e, e) is a derivation of v, its size lDl = 1, and its weight w(D) = f(e)().",
        "• If e E BS(v) where lel > 0 and Di is a derivation of Ti(e) for 1 <_ i <_ lel, then D = (e, D1 .",
        ".",
        ".",
        "Dlel) is",
        "a derivation of v, its size lDl = 1 + �lel i=1 lDil and its weight w(D) = f(e)(w(D1), ... , w(Dlel)).",
        "The ordering on weights in R induces an ordering on derivations: D< D' iff w(D) < w(D').",
        "Definition 6.",
        "Define Di(v) to be the irh-best derivation of v. We can think of D1(v), ... , Dk(v) as the components of a vector we shall denote by D(v).",
        "The k-best derivations problem for hypergraphs, then, is to find D(t) given a hypergraph (V, E, t, R).",
        "With the derivations thus ranked, we can introduce a nonrecursive representation for derivations that is analogous to the use of back pointers in parser implementation.",
        "Definition 7.",
        "A derivation with back pointers (dbp) Dˆ of v is a tuple (e, j) such that e E BS(v), and j E {1, 2, ... , k}lel.",
        "There is a one-to-one correspondence – between dbps of v and derivations of v:",
        "Accordingly, we extend the weight function w to dbps: w(D) = w(D) if D – D. This in turn induces an ordering on dbps: D < D' iff w(D) < w(D').",
        "Let Di(v) denote the irh-best dbp of v. Where no confusion will arise, we use the terms ‘derivation’ and ‘dbp’ interchangeably.",
        "Computationally, then, the k-best problem can be stated as follows: given a hypergraph H with arity a, compute ˆD1(t), .",
        ".",
        ".",
        ", ˆDk(t).1 As shown by Klein and Manning (2001), hypergraphs can be used to represent the search space of most parsers (just as graphs, also known as trellises or lattices, can represent the search space of finite-state automata or IIMMs).",
        "More generally, hypergraphs can be used to represent the search space of most weighted deductive system (Nederhof, 2003).",
        "For example, the weighted CKY algorithm given a context-free grammar G = (N, T, P, S) in Chomsky Normal Form (CNF) and an input string w can be represented as a hypergraph of arity 2 as follows.",
        "Each item [X, i, j] is represented as a vertex v, corresponding to the recognition of nonterminal X spanning w from positions i + 1 through j.",
        "For each production rule X – � YZ in P and three free indices i < j < k, we have a hyperarc (((Y, i, k), (Z, k, j)), (X, i, k), f) corresponding to the instantiation of the inference rule COMPLETE in the deductive system of (Shieber et al., 1995), and the weight function f is defined as f(a, b) = ab .",
        "Pr(X – � YZ), which is the same as in (Nederhof, 2003).",
        "In this sense, hypergraphs can be thought of as compiled or instantiated versions of weighted deductive systems.",
        "A parser does nothing more than traverse this hypergraph.",
        "In order that derivation values be computed correctly, however, we need to traverse the hypergraph in a particular order: Definition 8.",
        "The graph projection of a hypergraph H = (V, E, t, R) is a directed graph G = (V, E') where E' = {(u, v) l�e E BS (v), u E T(e)}.",
        "A hypergraph H is said to be acyclic if its graph projection G is a directed acyclic graph; then a topological ordering of H is an ordering of V that is a topological ordering in G (from sources to target).",
        "We assume the input hypergraph is acyclic so that we can use its topological ordering to traverse it.",
        "In practice the hypergraph is typically not known in advance, but the 'Note that although we have defined the weight of a derivation as a function on derivations, in practice one would store a derivation’s weight inside the dbp itself, to avoid recomputing it over and over.",
        "topological ordering often is, so that the (dynamic) hypergraph can be generated in that order.",
        "For example, for CKY it is sufficient to generate all items [X, i, j] before all items [Y, i', j'] when j' – i' > j – i (X and Y are arbitrary nonterminals)."
      ]
    },
    {
      "heading": "Excursus: Derivations and Hyperpaths",
      "text": [
        "The work of Klein and Manning (2001) introduces a correspondence between hyperpaths and derivations.",
        "When extended to the k-best case, however, that correspondence no longer holds.",
        "Definition 9.",
        "(Nielsen et al., 2005) Given a hypergraph H = (V, E, t, R), a hyperpath erv of destination v E V is an acyclic minimal hypergraph H, = (V,, E,, v, R) such that 1.E,E_E",
        "2. v E V, = SeEE, (T(e) u fh(e)}) 3.",
        "Vu E V,, u is either a source vertex or connected to",
        "a source vertex in H,.",
        "As illustrated by Figure 1, derivations (as trees) are different from hyperpaths (as minimal hypergraphs) in the sense that in a derivation the same vertex can appear more than once with possibly different sub-derivations while it is represented at most once in a hyperpath.",
        "Thus, the k-best derivations problem we solve in this paper is very different in nature from the k-shortest hyperpaths problem in (Nielsen et al., 2005).",
        "However, the two problems do coincide when k = 1 (since all the sub-derivations must be optimal) and for this reason the 1-best hyperpath algorithm in (Klein and Manning, 2001) is very similar to the 1-best tree algorithm in (Knuth, 1977).",
        "For k-best case (k > 1), they also coincide when the hypergraph is isomorphic to a Case-Factor Diagram (CFD) (McAllester et al., 2004) (proof omitted).",
        "The derivation forest of CFG parsing under the CKY algorithm, for instance, can be represented as a CFD while the forest of Earley algorithm can not.",
        "An",
        "1: procedure Vi�ERSi(k) 2: for v E V in topological order do 3: for e E BS(v) do D for all incoming hyperarcs 4: ˆD1 (v) <-- min, (ˆD1(v), (e, 1)) D update",
        "item (or equivalently, a vertex in hypergraph) can appear twice in an Earley derivation because of the prediction rule (see Figure 2 for an example).",
        "The k-best derivations problem has potentially more applications in tree generation (Knight and Graehl, 2005), which can not be modeled by hyperpaths.",
        "But detailed discussions along this line are out of the scope of this paper."
      ]
    },
    {
      "heading": "4 Algorithms",
      "text": [
        "The traditional 1-best Viterbi algorithm traverses the hypergraph in topological order and for each vertex v, calculates its 1-best derivation D1 (v) using all incoming hyperarcs e E BS(v) (see Figure 3).",
        "If we take the arity of the hypergraph to be constant, then the overall time complexity of this algorithm is O(JEJ)."
      ]
    },
    {
      "heading": "4.1 Algorithm 0: naive",
      "text": [
        "Following (Goodman, 1999; Mohri, 2002), we isolate two basic operations in line 4 of the 1-best algorithm that",
        "can be generalized in order to extend the algorithm: first, the formation of the derivation (e, 1) out of lel best sub-derivations (this is a generalization of the binary operator (9 in a semiring); second, min1, which chooses the better of two derivations (same as the ® operator in an idempotent semiring (Mohri, 2002)).",
        "We now generalize these two operations to operate on k-best lists.",
        "Let r = lel.",
        "The new multiplication operation, mult1k(e), is performed in three steps:",
        "1. enumerate the k' derivations {(e, j1 • • • j') l Vi, 1 < ji < k}.",
        "Time: O(k').",
        "2. sort these k' derivations (according to weight).",
        "Time: O(k' log(k')) = O(rk' log k).",
        "3. select the first k elements from the sorted list of k' elements.",
        "Time: O(k).",
        "So the overall time complexity of mult1k is O(rk' log k).",
        "We also have to extend min1 to merge1k, which takes two vectors of length k (or fewer) as input and outputs the top k (in sorted order) of the 2k elements.",
        "This is similar to merge-sort (Cormen et al., 2001) and can be done in linear time O(k).",
        "Then, we only need to rewrite line 4 of the Viterbi algorithm (Figure 3) to extend it to the k-best case: 4: ˆD(v) <-- merge1k(ˆD(v), mult1k(e)) and the time complexity for this line is O(lelk�e�log k), making the overall complexity O(lElka log k) if we consider the arity a of the hypergraph to be constant.",
        "The overall space complexity is O(lVlk) since for each vertex we need to store a vector of length k. In the context of CKY parsing for CFG, the 1-best Viterbi algorithm has complexity O(n3l Pl) while the k-best version is O(n3lPlk2 log k), which is slower by a factor of O(k2 log k)."
      ]
    },
    {
      "heading": "4.2 Algorithm 1: speed up mult1k",
      "text": [
        "First we seek to exploit the fact that input vectors are all sorted and the function f is monotonic; moreover, we are only interested in the top k elements of the klel possibilities.",
        "Define 1 to be the vector whose elements are all 1; define bi to be the vector whose elements are all 0 except bi = 1. i As we compute pe = mult1k(e), we maintain a candidate set C of derivations that have the potential to be the next best derivation in the list.",
        "If we picture the input as an lel-dimensional space, C contains those derivations that 2Actually, we do not need to sort all kjej elements in order to extract the top k among them; there is an efficient algorithm (Cormen et al., 2001) that can select the kth best element from the kjej elements in time O(kjej).",
        "So we can improve the overhead to O(ka).",
        "have not yet been included in pe, but are on the boundary with those which have.",
        "It is initialized to {(e,1)}.",
        "At each step, we extract the best derivation from C – call it (e, j) – and append it to pe.",
        "Then (e, j) must be replaced in C by its neighbors, {(e,j +bl)l 1 < l< lel} (see Figure 4.2 for an illustration).",
        "We implement C as a priority queue (Cormen et al., 2001) to make the extraction of its best derivation efficient.",
        "At each iteration, there are one EXTRACT-MiN and lel INSERT operations.",
        "If we use a binary-heap implementation for priority queues, we get O(lellog klel) time complexity for each iteration.3 Since we are only interested in the top k elements, there are k iterations and the time complexity for a single mult1k is O(kl el log kl el ), yielding an overall time complexity of O(lElk log k) and reducing the multiplicative overhead by a factor of O(ka�1) (again, assuming a is constant).",
        "In the context of CKY parsing, this reduces the overhead to O(klog k).",
        "Figure 5 shows the additional pseudocode needed for this algorithm.",
        "It is integrated into the Viterbi algorithm (Figure 3) simply by rewriting line 4 of to invoke the function MuLT(e, k):",
        "4: ˆD(v) <-- merge1k(ˆD(v), MuLT(e, k)) 4.3 Algorithm 2: combine merge1k into mult1k",
        "We can further speed up both merge1k and mult1k by a similar idea.",
        "Instead of letting each mult1k generate a full k derivations for each hyperarc e and only then applying merge1k to the results, we can combine the candidate sets for all the hyperarcs into a single candidate set.",
        "That is, we initialize C to {(e,1) l e E BS(v)}, the set of all the top parses from each incoming hyperarc (cf. Algorithm 1).",
        "Indeed, it suffices to keep only the top k out of the lBS (v)l candidates in C, which would lead to a significant speedup in the case where lBS(v)l >> k. 4 Now the top derivation in C is the top derivation for v. Then, whenever we remove an element (e, j) from C, we replace it with the lel elements {(e, j + bl) l1 1 < l < lel} (again, as in Algorithm 1).",
        "The full pseudocode for this algorithm is shown in Figure 6."
      ]
    },
    {
      "heading": "4.4 Algorithm 3: compute mult1k lazily",
      "text": [
        "Algorithm 2 exploited the idea of lazy computation: performing mult1k only as many times as necessary.",
        "But this algorithm still calculates a full k-best list for every vertex in the hypergraph, whereas we are only interested in",
        "function f is defined as f (a, b) = a + b. Italic numbers on the x and y axes are ai’s and bj’s, respectively.",
        "We want to compute the top 3 results from f(ai, b j) with 1 < i, j < 3.",
        "In each iteration the current frontier is shown in oval boxes, with the boldface denoting the best element among them.",
        "That element will be extracted and replaced by its two neighbors (' and ==>) in the next iteration.",
        "1: function MuLT(e, k) 2: cand <-- {(e,1)} D initialize the heap 3: p <-- empty list D the result of mult-<k 4: while 1p1 < k and 1cand1 > 0 do 5: APPENUNExT(cand, p, k) 6: return p 8: procedure APPENUNExT(cand, p) 9: (e, j) <-- ExTR�CT-M�N(cand) 10: append (e, j) to p 11: for i <-- 1...1e1 do D add the 1e1 neighbors 12: j'<--j+bi 13: if j'i < 1 ˆD(Ti(e))1 and (e, j') V cand then 14: INSERT(cand, (e, j')) D add to heap",
        "1: procedure F�NUALLKBEST(k) 2: for v E V in topological order do 3: F�NUKBEST(v, k) 5: procedure FrNUKBEST(v, k) 6: GETC�NU�U�TES(v, k) D initialize the heap 7: while 1ˆD(v)1 < k and 1cand[v]1 > 0 do 8: APPENUNExT(cand[v], ˆD(v)) 10: procedure",
        "D initialize the heap",
        "6: append ExTR�CT-M�N(cand[v]) to 8: (e,j) <-- DIˆD(v)I(v)D last derivation 9: LnzYNExT(cand[v], e, j, k') D update the heap, adding the successors of last derivation ˆD(v)D get the next best derivation an d delete it from the heap 13: for i <-- 1...1e1 do D add the 1e1 neighbors j'i, k') D recursively solve asub-problem 16: if j'i < 1 1 ˆD(Ti(e))1 and (e, j') V cand then D if it exists an d is not in heap yet 9:",
        "the k-best derivations of the target vertex (goal item).",
        "We can therefore take laziness to an extreme by delaying the whole k-best calculation until after parsing.",
        "Algorithm 3 assumes an initial parsing phase that generates the hypergraph and finds the 1-best derivation of each item; then in the second phase, it proceeds as in Algorithm 2, but starts at the goal item and calls itself recursively only as necessary.",
        "The pseudocode for this algorithm is shown in Figure 7.",
        "As a side note, this second phase should be applicable also to a cyclic hypergraph as long as its derivation weights are bounded.",
        "Algorithm 2 has an overall complexity of O(IEI + I V I k log k) and Algorithm 3 is O(IEI + IDmax Ik log k) where IDmaxI is the size of the longest among all top k derivations (for CFG in CNF, IDI = 2n −1 for all D, so IDmaxI I is O(n)).",
        "These are significant improvements against Algorithms 0 and 1 since it turns the multiplicative overhead into an additive overhead.",
        "In practice, IEI usually dominates, as in CKY parsing of CFG.",
        "So theoretically the running times grow very slowly as k increases, which is exactly demonstrated by our experiments below."
      ]
    },
    {
      "heading": "4.5 Summary and Discussion of Algorithms",
      "text": [
        "The four algorithms, along with the 1-best Viterbi algorithm and the generalized Jim´enez and Marzal algorithm, are compared in Table 1.",
        "The key difference between our Algorithm 3 and Jim´enez and Marzal’s algorithm is the restriction of top k candidates before making heaps (line 11 in Figure 6, see also Sec. 4.3).",
        "Without this line Algorithm 3 could be considered as a generalization of the Jim´enez and Marzal algorithm to the case of acyclic monotonic hypergraphs.",
        "This line is also responsible for improving the time complexity from O(IEI + IDmaxIklog(d + k)) (generalized Jim´enez and Marzal algorithm) to O(IEI + IDmaxIklog k), where d = max„ IBS (v)I is the maximum in-degree among all vertices.",
        "So in case k < d, our algorithm outperforms Jim´enez and Marzal’s."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We report results from two sets of experiments.",
        "For probabilistic parsing, we implemented Algorithms 0, 1, and 3 on top of a widely-used parser (Bikel, 2004) and conducted experiments on parsing efficiency and the quality of the k-best-lists.",
        "We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed."
      ]
    },
    {
      "heading": "5.1 Experiment 1: Bikel Parser",
      "text": [
        "Bikel’s parser (2004) is a state-of-the-art multilingual parser based on lexicalized context-free models (Collins, 2003; Eisner, 2000).",
        "It does support k-best parsing, but, following Collins’ parse-reranking work (Collins, 2000) (see also Section 5.1.2), it accomplishes this by simply abandoning dynamic programming, i.e., no items are considered equivalent (Charniak and Johnson, 2005).",
        "Theoretically, the time complexity is exponential in n (the input sentence length) and constant in k, since, without merging of equivalent items, there is no limit on the number of items in the chart.",
        "In practice, beam search is used to reduce the observed time.5 But with the standard beam width of 10-4, this method becomes prohibitively expensive for n >- 25 on Bikel’s parser.",
        "Collins (2000) used a narrower 10-3 beam and further applied a cell limit of 100,6 but, as we will show below, this has a detrimental effect on the quality of the output.",
        "We therefore omit this method from our speed comparisons, and use our implementation of Algorithm 0 (naive) as the baseline.",
        "We implemented our k-best Algorithms 0, 1, and 3 on top of Bikel’s parser and conducted experiments on a 2.4 GHz 64-bit AMD Opteron with 32 GB memory.",
        "The program is written in Java 1.5 running on the Sun JVM in server mode with a maximum heap size of 5 GB.",
        "For this experiment, we used sections 02–21 of the Penn Treebank (PTB) (Marcus et al., 1993) as the training data and section 23 (2416 sentences) for evaluation, as is now standard.",
        "We ran Bikel’s parser using its settings to emulate Model 2 of (Collins, 2003).",
        "We tested our algorithms under various conditions.",
        "We first did a comparison of the average parsing time per sentence of Algorithms 0, 1, and 3 on section 23, with k <- 10000 for the standard beam of width 10-4.",
        "Figure 8(a) shows that the parsing speed of Algorithm 3 improved dramatically against the other algorithms and is nearly constant in k, which exactly matches the complexity analysis.",
        "Algorithm 1 (k log k) also significantly outperforms the baseline naive algorithm (k2 log k).",
        "We also did a comparison between our Algorithm 3 and the Jim´enez and Marzal algorithm in terms of average",
        "heap size.",
        "Figure 8(b) shows that for larger k, the two algorithms have the same average heap size, but for smaller k, our Algorithm 3 has a considerably smaller average heap size.",
        "This difference is useful in applications where only short k-best lists are needed.",
        "For example, McDonald et al.",
        "(2005) find that k = 5 gives optimal parsing accuracy."
      ]
    },
    {
      "heading": "5.1.2 Accuracy",
      "text": [
        "Our efficient k-best algorithms enable us to search over a larger portion of the whole search space (e.g. by less aggressive pruning), thus producing k-best lists with better quality than previous methods.",
        "We demonstrate this by comparing our k-best lists to those in (Ratnaparkhi, 1997), (Collins, 2000) and the parallel work by Charniak and Johnson (2005) in several ways, including oracle reranking and average number of found parses.",
        "Ratnaparkhi (1997) introduced the idea of oracle reranking: suppose there exists a perfect reranking scheme that magically picks the best parse that has the highest F-score among the top k parses for each sentence.",
        "Then the performance of this oracle reranking scheme is the upper bound of any actual reranking system like (Collins, 2000).As k increases, the F-score is nondecreas-ing, and there is some k (which might be very large) at which the F-score converges.",
        "Ratnaparkhi reports experiments using oracle reranking with his statistical parser MXPARSE, which can compute its k-best parses (in his experiments, k = 20).",
        "Collins (2000), in his parse-reranking experiments, used his Model 2 parser (Collins, 2003) with a beam width of 10-3 together with a cell limit of 100 to obtain k-best lists; the average number of parses obtained per sentence was 29.2, the maximum, 101.7 Charniak and Johnson (2005) use coarse-to-fine parsing on top of the Charniak (2000) parser and get 50-best lists for section 23.",
        "Figure 9(a) compares the results of oracle reranking.",
        "Collins’ curve converges at around k = 50 while ours continues to increase.",
        "With a beam width of 10-4 and k = 100, our parser plus oracle reaches an F-score of 96.4%, compared to Collins’ 94.9%.",
        "Charniak and John-son’s work, however, is based on a completely different parser whose 1-best F-score is 1.5 points higher than the 1-bests of ours and Collins’, making it difficult to compare in absolute numbers.",
        "So we instead compared the relative improvement over 1-best.",
        "Figure 9(b) shows that our work has the largest percentage of improvement in terms of F-score when k > 20.",
        "To further explore the impact of Collins’ cell limit on the quality of k-best lists, we plotted average number of parses for a given sentence length (Figure 10).",
        "Generally speaking, as input sentences get longer, the number of parses grows (exponentially).",
        "But we see that the curve for Collins’ k-best list goes down for large k (> 40).",
        "We suspect this is due to the cell limit of 100 pruning away potentially good parses too early in the chart.",
        "As sentences get longer, it is more likely that a lower-probability parse might contribute eventually to the k-best parses.",
        "So we infer that Collins’ k-best lists have limited quality for large k, and this is demonstrated by the early convergence of its oracle-reranking score.",
        "By comparison, our curves of both beam widths continue to grow with k = 100.",
        "All these experiments suggest that our k-best parses are of better quality than those from previous k-best parsers,",
        "and similar quality to those from (Charniak and Johnson, 2005) which has so far the highest F-score after reranking, and this might lead to better results in real parse reranking."
      ]
    },
    {
      "heading": "5.2 Experiment 2: MT decoder",
      "text": [
        "Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004).",
        "We implemented Algorithms 2 and 3 to compute k-best English translations of Mandarin sentences.",
        "Because the CFG used in this system is large to begin with (millions of rules), and then effectively intersected with a finite-state machine on the English side (the language model), the grammar constant for this system is quite large.",
        "The decoder uses a relatively narrow beam search for efficiency.",
        "We ran the decoder on a 2.8 GHz Xeon with 4 GB of memory, on 331 sentences from the 2002 NIST MTEval test set.",
        "We tested Algorithm 2 for k = 2', 3 :5 i :5 10, and Algorithm 3 (offline algorithm) for k = 2', 3 :5 i :5 20.",
        "For each sentence, we measured the time to calculate the k-best list, not including the initial 1-best parsing phase.",
        "We then averaged the times over our test set to produce the graph of Figure 11, which shows that Algorithm 3 runs an average of about 300 times faster than Algorithm 2.",
        "Furthermore, we were able to test Algorithm 3 up to k = 106 in a reasonable amount of time.8 'The curvature in the plot for Algorithm 3 for k < 1000 may be due to lack of resolution in the timing function for short times."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "The problem of k-best parsing and the effect of k-best list size and quality on applications are subjects of increasing interest for NLP research.",
        "We have presented here a general-purpose algorithm for k-best parsing and applied it to two state-of-the-art, large-scale NLP systems: Bikel’s implementation of Collins’ lexicalized PCFG model (Bikel, 2004; Collins, 2003) and Chiang’s synchronous CFG based decoder (Chiang, 2005) for machine translation.",
        "We hope that this work will encourage further investigation into whether larger and better k-best lists will improve performance in NLP applications, questions which we ourselves intend to pursue as well."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank one of the anonymous reviewers of a previous version of this paper for pointing out the work by Jim´enez and Marzal, and Eugene Charniak and Mark Johnson for providing an early draft of their paper and very useful comments.",
        "We are also extremely grateful to Dan Bikel for the help in experiments, and Michael Collins for providing the data in his paper.",
        "Our thanks also go to Dan Gildea, Jonathan Graehl, Julia Hock-enmaier, Aravind Joshi, Kevin Knight, Daniel Marcu, Mitch Marcus, Ryan McDonald, Fernando Pereira, Giorgio Satta, Libin Shen, and Hao Zhang."
      ]
    }
  ]
}
