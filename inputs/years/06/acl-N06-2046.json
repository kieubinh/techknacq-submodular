{
  "info": {
    "authors": [
      "Xiaojun Wan",
      "Jianwu Yang"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics – Short Papers",
    "id": "acl-N06-2046",
    "title": "Improved Affinity Graph Based Multi-Document Summarization",
    "url": "https://aclweb.org/anthology/N06-2046",
    "year": 2006
  },
  "references": [
    "acl-I05-2004",
    "acl-N03-1020",
    "acl-P02-1058",
    "acl-W04-3247"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes an affinity graph based approach to multi-document summarization.",
        "We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences.",
        "A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary.",
        "Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-the-art systems."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Automated multi-document summarization has drawn much attention in recent years.",
        "Multi-document summary is usually used to provide concise topic description about a cluster of documents and facilitate the users to browse the document cluster.",
        "A particular challenge for multi-document summarization is that the information stored in different documents inevitably overlaps with each other, and hence we need effective summarization methods to merge information stored in different documents, and if possible, contrast their differences.",
        "A variety of multi-document summarization methods have been developed recently.",
        "In this study, we focus on extractive summarization, which involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting t e sentences with highest scores.",
        "MEAD is an implementation of the centroid-based method (Radev et al., 2004) that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TF*IDF, etc.",
        "NeATS (Lin and Hovy, 2002) selects important content using entence position, term frequency, topic signature and term clustering, and then uses MMR (Goldstein et al., 1999) to remove redundancy.",
        "XDoX (Hardy et al., 1998) identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes.",
        "Harabagiu and Lacatusu (2005) investigate different topic representations and extraction methods.",
        "Graph-based methods have been proposed to rank sentences or passages.",
        "Websumm (Mani and Bloedorn, 2000) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information.",
        "LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality.",
        "Mihalcea and Tarau (2005) also propose similar algorithms based on PageR-ank and HITS to compute sentence importance for document summarization.",
        "In this study, we extend the above graph-based works by proposing an integrated framework for considering both information richness and information novelty of a sentence based on sentence affinity graph.",
        "First, a diffusion process is imposed on sentence affinity graph in order to make the affinity graph reflect true semantic relationships between sentences.",
        "Second, intra-document links and inter-document links between sentences are differentiated to attach more importance to inter-document links for sentence information richness computation.",
        "Lastly, a diversity penalty process is imposed on sentences to penalize redundant sentences.",
        "Experiments on DUC 2002 and DUC 2004 data are performed and we obtain encouraging results and conclusions."
      ]
    },
    {
      "heading": "2 The Affinity Graph Based Approach",
      "text": [
        "The proposed affinity graph based summarization method consists of three steps: (1) an affinity graph is built to reflect the semantic relationship between sentences in the document set; (2) information richness of each sentence is computed based on the affinity graph; (3) based on the affinity graph and the information richness scores, diversity penalty is imposed to sentences and the affinity rank score for each sentence is obtained to reflect both information richness and information novelty of the sentence.",
        "The sentences with high affinity rank scores are chosen to produce the summary."
      ]
    },
    {
      "heading": "2.1 Affinity Graph Building",
      "text": [
        "Given a sentence collection S= {si � 1 i n}, the affinity weight aff(si, sj) between a sentence pair of si and sj is calculated using the cosine measure.",
        "The weight associated with term t is calculated with the tft*isft formula, where tft is the frequency of term t in the corresponding sentence and isft is the inverse sentence frequency of term t, i.e. 1+log(N/nt), where N is the total number of sentences and nt is the number of sentences containing term t. If sentences are considered as nodes, the sentence collection can be modeled as an undirected graph by generating the link between two sentences if their affinity weight exceeds 0, i.e. an undirected link between si and sj (i j) with affinity weight aff(si,sj) is constructed if aff(si,sj)>0; otherwise no link is constructed.",
        "Thus, we construct an undirected graph G reflecting the semantic relationship between sentences by their content similarity.",
        "The graph is called as Affinity Graph.",
        "We use an adjacency (affinity) matrix M to describe the affinity graph with each entry corresponding to the weight of a link in the graph.",
        "M = (Mi,j)n�n is defined as follows:",
        "Then M is normalized to make the sum of each row equal to 1.",
        "Note that we use the same notation to denote a matrix and its normalized matrix.",
        "However, the affinity weight between two sentences in the affinity graph is currently computed simply based on their own content similarity and ignore the affinity diffusion process on the graph.",
        "Other than the direct link between two sentences, the possible paths with more than two steps between the sentences in the graph also convey more or less semantic relationship.",
        "In order to acquire the implicit semantic relationship between sentences, we apply a diffusion process Kandola et al., 2002 on the graph to obtain a more appropriate affinity matrix.",
        "Though the number of possible paths between any two given nodes can grow exponentially, recent spectral graph theory (Kondor and Lafferty, 2002) shows that it is possible to compute the affinity between any two given nodes efficiently without examining all possible paths.",
        "The diffusion process on the graph is as follows:",
        "where (0< <1) is the decay factor set to 0.9. is the t-th power of the initial affinity matrix and the entry in it is given by",
        "that is the sum of the products of the weights over all paths of length t that start at node i and finish at node j in the graph on the examples.",
        "If the entries satisfy that they are all positive and for each node the sum of the connections is 1, we can view the entry as the probability that a random walk beginning at node i reaches node j after t steps.",
        "The matrix M is normalized to make the sum of each row equal to 1. t is limited to 5 in this study."
      ]
    },
    {
      "heading": "2.2 Information Richness Computation",
      "text": [
        "The computation of information richness of sentences is based on the following the following three intuitions: 1) the more neighbors a sentence has, the more informative it is; 2) the more informative a sentence's neighbors are, the more informative it is; 3) the more heavily a sentence is linked with other informative sentences, the more informative it is.",
        "Based on the above intuitions, the information richness score InfoRich(si) for a sentence si can be deduced from those of all other sentences linked with it and it can be formulated in as follows:",
        "After the affinity rank scores are obtained for all where O � InfoRich (s i )� n u 1 is the eigenvector of M&e �Tsentences, the sentenceswith highest affinity rank .",
        "is a unit vector with all elements equaling scores are chosen to produce the summary accord-to 1. d is the damping factor set to 0.85. ing to the summary length limit.",
        "Note that given a link between a sentence pair of si and sj, if si and sj comes from the same document, 3 Experiments and Results the link is an intra-document link; and if si and sj comes from different documents, the link is an in We compare our system with top 3 performing ter-document link.",
        "We believe that inter-document systems and two baseline systems on task 2 of links are more important than intra-document links DUC 2002 and task 4 of DUC 2004 respectively.",
        "for information richness computation� Different ROUGE (Lin and �ovy, 2003) metrics is used for evaluation1 and we mainly concern about ROUGE-weights are assigned to intra-document links and inter-document links respectively, and the new af-1.",
        "The parameters of our system are tuned on DUC finity matrix is: 2001 as follows: and",
        "where intra M is the affinity matrix containing only systems on both DUC 2002 and DUC 2004 tasks over all three metrics.",
        "The performance improve-the intra-document links (the entries of inter� ment achieved by our system results from three document links are set to 0) and inter M is the affin- factors: diversity penalty imposition, intra-ity matrix containing only the inter-document links (the entries of intra-document links are set to 0).",
        ", are weighting parameters and we let 0 , 1."
      ]
    },
    {
      "heading": "2.3 Diversity Penalty Imposition",
      "text": [
        "Based on the affinity graph and obtained information richness scores, a greedy algorithm is applied to impose the diversity penalty and compute the final affinity rank scores of sentences as follows:",
        "1.",
        "Initialize two sets A=O, B={si �I i=1,2,...,n}, and each sentence's affinity rank score is initialized to its information richness score, i.e. ARScore(si) = InfoRich(si), i=1,2 ,... n. 2.",
        "Sort the sentences in B by their current affinity rank scores in descending order.",
        "3.",
        "Suppose si is the highest ranked sentence, i.e. the first sentence in the ranked list.",
        "Move sentence si from B to A, and then a diversity penalty is imposed to the affinity rank score of each sentence linked with si as follows:",
        "where >0 is the penalty degree factor.",
        "The larger is, the greater penalty is imposed to the affinity rank score.",
        "If =0, no diversity penalty is imposed at all.",
        "Figures 1-4 show the influence of the parameters in our system.",
        "Note that : denotes the real values and are set to.",
        "\"w/ diffusion\" is the system with the diffusion process (our system) and \"w/o diffusion\" is the system without the diffusion proc",
        "ess.",
        "The observations demonstrate that \"w/ diffu-sion\" performs better than \"w/o diffusion\" for most parameter settings.",
        "Meanwhile, \"w/ diffusion\" is more robust than \"w/o diffusion\" because the ROUGE-1 value of \"w/ diffusion\" changes less when the parameter values vary.",
        "Note that in Figures 3 and 4 the performance decreases sharply with the decrease of the weight of inter-document links and it is the worst case when inter-document links are not taken into account (i.e. : =1:0), while if intra-document links are not taken into account (i.e. : =0:1), the performance is still good, which demonstrates the great importance of inter-document links."
      ]
    }
  ]
}
