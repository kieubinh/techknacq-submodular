{
  "info": {
    "authors": [
      "Gary Kacmarcik",
      "Michael Gamon"
    ],
    "book": "International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics – Poster Sessions",
    "id": "acl-P06-2058",
    "title": "Obfuscating Document Stylometry to Preserve Author Anonymity",
    "url": "https://aclweb.org/anthology/P06-2058",
    "year": 2006
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper explores techniques for reducing the effectiveness of standard authorship attribution techniques so that an author A can preserve anonymity for a particular document D. We discuss feature selection and adjustment and show how this information can be fed back to the author to create a new document D’ for which the calculated attribution moves away from A.",
        "Since it can be labor intensive to adjust the document in this fashion, we attempt to quantify the amount of effort required to produce the ano-nymized document and introduce two levels of anonymization: shallow and deep.",
        "In our test set, we show that shallow anonymization can be achieved by making 14 changes per 1000 words to reduce the likelihood of identifying A as the author by an average of more than 83%.",
        "For deep anonymization, we adapt the unmasking work of Koppel and Schler to provide feedback that allows the author to choose the level of anonymization."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Authorship identification has been a long standing topic in the field of stylometry, the analysis of literary style (Holmes 1998).",
        "Issues of style, genre, and authorship are an interesting sub-area of text categorization.",
        "In authorship detection it is not the topic of a text but rather the stylistic properties that are of interest.",
        "The writing style of a particular author can be identified by analyzing the form of the writing, rather than the content.",
        "The analysis of style therefore needs to abstract away from the content and focus on the content-independent form of the linguistic expressions in a text.",
        "Advances in authorship attribution have raised concerns about whether or not authors can truly maintain their anonymity (Rao and Rohatgi 2000).",
        "While there are clearly many reasons for wanting to unmask an anonymous author, notably law enforcement and historical scholarship, there are also many legitimate reasons for an author to wish to remain anonymous, chief among them the desire to avoid retribution from an employer or government agency.",
        "Beyond the issue of personal privacy, the public good is often served by whistle-blowers who expose wrongdoing in corporations and governments.",
        "The loss of an expectation of privacy can result in a chilling effect where individuals are too afraid to draw attention to a problem, because they fear being discovered and punished for their actions.",
        "It is for this reason that we set out to investigate the feasibility of creating a tool to support anonymizing a particular document, given the assumption that the author is willing to expend a reasonable amount of effort in the process.",
        "More generally, we sought to investigate the sensitivity of current attribution techniques to manipulation.",
        "For our experiments, we chose a standard data set, the Federalist Papers, since the variety of published results allows us to simulate authorship attribution “attacks” on the obfuscated document.",
        "This is important since there is no clear consensus as to which features should be used for authorship attribution."
      ]
    },
    {
      "heading": "2 Document Obfuscation",
      "text": [
        "Our approach to document obfuscation is to identify the features that a typical authorship attribution technique will use as markers and then adjust the frequencies of these terms to render them less effective on the target document.",
        "While it is obvious that one can affect the attribution result by adjusting feature values, we were concerned with:",
        "• How easy is it to identify and present the required changes to the author?",
        "• How resilient are the current authorship detection techniques to obfuscation?",
        "• How much work is involved for the au",
        "thor in the obfuscation process?",
        "The only related work that we are aware of is (Rao and Rohatgi 2000) who identify the problem and suggest (somewhat facetiously, they admit) using a round-trip machine translation (MT) process (e.g., English --* French --* English) to obscure any traces of the original author’s style.",
        "They note that the current quality of MT would be problematic, but this approach might serve as a useful starting point for someone who wants to scramble the words a bit before hand-correcting egregious errors (taking care not to reintroduce their style)."
      ]
    },
    {
      "heading": "2.1 The Federalist Papers",
      "text": [
        "One of the standard document sets used in authorship attribution is the Federalist Papers, a collection of 85 documents initially published anonymously, but now known to have been written by 3 authors: Alexander Hamilton, John Madison and John Jay.",
        "Due to illness, Jay only wrote 5 of the papers, and most of the remaining papers are of established authorship (Hamilton = 51; Madison = 14; and 3 of joint authorship between Hamilton and Madison).",
        "The 12 remaining papers are disputed between Hamilton and Madison.",
        "In this work we limit ourselves to the 65 known single-author papers and the 12 disputed papers.",
        "While we refer to these 12 test documents as “disputed”, it is generally agreed (since the work of Mosteller and Wallace (1964)) that all of the disputed papers were authored by Madison.",
        "In our model, we accept that Madison is the author of these papers and adopt the fiction that he is interested in obscuring his role in their creation."
      ]
    },
    {
      "heading": "2.2 Problem Statement",
      "text": [
        "A more formal problem statement is as follows: We assume that an author A (in our case, Madison) has created a document D that needs to be anonymized.",
        "The author self-selects a set K of N authors (where A ∈ K) that some future agent (the “attacker” following the convention used in cryptography) will attempt to select between.",
        "The goal is to use authorship attribution techniques to create a new document D’ based on D but with features that identify A as the author suppressed."
      ]
    },
    {
      "heading": "3 Document Preparation",
      "text": [
        "Before we can begin with the process of obfuscating the author style in D, we need to gather a training corpus and normalize all of the documents."
      ]
    },
    {
      "heading": "3.1 Training Corpus",
      "text": [
        "While the training corpus for our example is trivially obtained, authors wishing to anonymize their documents would need to gather their own corpus specific for their use.",
        "The first step is to identify the set of authors K (including A) that could have possibly written the document.",
        "This can be a set of co-workers or a set of authors who have published on the topic.",
        "Once the authors have been selected, a suitable corpus for each author needs to be gathered.",
        "This can be emails or newsgroup postings or other documents.",
        "In our experiments, we did not include D in the corpus for A, although it does not seem unreasonable to do so.",
        "For our example of the Federalist Papers, K is known to be {Hamilton, Madison} and it is already neatly divided into separate documents of comparable length."
      ]
    },
    {
      "heading": "3.2 Document Cleanup",
      "text": [
        "Traditional authorship attribution techniques rely primarily on associating idiosyncratic formatting, language usage and spelling (misspellings, typos, or region-specific spelling) with each author in the study.",
        "Rao and Rohatgi (2000) and Koppel and Schler (2003) both report that these words serve as powerful discriminators for author attribution.",
        "Thus, an important part of any obfuscation effort is to identify these idiosyncratic usage patterns and normalize them in the text.",
        "Koppel and Schler (2003) also note that many of these patterns can be identified using the basic spelling and grammar checking tools available in most word processing applications.",
        "Correcting the issues identified by these tools is an easy first step in ensuring the document conforms to conventional norms.",
        "This is especially important for work that will not be reviewed or edited since these idiosyncrasies are more likely to go unnoticed.",
        "However, there are distinctive usage patterns that are not simple grammar or spelling errors that also need to be identified.",
        "A well-known example of this is the usage of while/whilst by the authors of the Federalist Papers.",
        "In the disputed papers, “whilst” occurs in 6 of the documents (9 times total) and “while” occurs in none.",
        "To properly anonymize the disputed documents, “whilst” would need to be eliminated or normalized.",
        "This is similar to the problem with idiosyncratic spelling in that there are two ways to apply this information.",
        "The first is to simply correct the term to conform to the norms as defined by the authors in K. The second approach is to incorporate characteristic forms associated with a particular author.",
        "While both approaches can serve to reduce the author’s stylometric fingerprint, the latter approach carries the risk of attempted style forgery and if applied indiscriminately may also provide clues that the document has been anonymized (if strong characteristics of multiple authors can be detected).",
        "For our experiments, we opted to leave these markers in place to see how they were handled by the system.",
        "We did, however, need to normalize the paragraph formatting, remove all capitalization and convert all footnote references to use square brackets (which are otherwise unused in the corpus)."
      ]
    },
    {
      "heading": "3.3 Tokenization",
      "text": [
        "To tokenize the documents, we separated sequences of letters using spaces, newlines and the following punctuation marks: .,()-:;`'?![].",
        "No stemming or morphological analysis was performed.",
        "This process resulted in 8674 unique tokens for the 65 documents in the training set."
      ]
    },
    {
      "heading": "4 Feature Selection",
      "text": [
        "The process of feature selection is one of the most crucial aspects of authorship attribution.",
        "By far the most common approach is to make use of the frequencies of common function words that are content neutral, but practitioners have also made use of other features such as letter metrics (e.g., bi-grams), word and sentence length metrics, word tags and parser rewrite rules.",
        "For this work, we opted to limit our study to word frequencies since these features are generally acknowledged to be effective for authorship attribution and are transparent, which allows the author to easily incorporate the information for document modification purposes.",
        "We wanted to avoid depending on an initial list of candidate features since there is no guarantee that the attackers will limit themselves to any of the commonly used lists.",
        "Avoiding these lists makes this work more readily useful for non-English texts (although morphology or stemming may be required).",
        "We desire two things from our feature selection process beyond the actual features.",
        "First, we need a ranking of the features so that the author can focus efforts on the most important features.",
        "The second requirement is that we need a threshold value so that the author knows how much the feature frequency needs to be adjusted.",
        "To rank and threshold the features, we used decision trees (DTs) and made use of the readily available WinMine toolkit (Chickering 2002).",
        "DTs produced by WinMine for continuously valued features such as frequencies are useful since each node in the tree provides the required threshold value.",
        "For term-ranking, we created a Decision Tree Root (DTR) ranking metric to order the terms based on how discriminating they are.",
        "DTR Rank is computed by creating a series of DTs where we remove the root feature, i.e. the most discriminating feature, before creating the next DT.",
        "In this fashion we create a ranking based on the order in which the DT algorithm determined that the term was most discriminatory.",
        "The DTR ranking algorithm is as follows:",
        "1) Start with a set of features 2) Build DT and record root feature 3) Remove root feature from list of features 4) Repeat from step 2",
        "It is worth noting that the entire DT need not be calculated since only the root is of interest.",
        "The off-the-shelf DT toolkit could be replaced with a custom implementation1 that returned only the root (also known as a decision stump).",
        "Since",
        "our work is exploratory, we did not pursue optimizations along these lines.",
        "For our first set of experiments, we applied DTR ranking starting with all of the features (8674 tokens from the training set) and repeated until the DT was unable to create a tree that performed better than the baseline of p(Hamilton) = 78.46%.",
        "In this fashion, we obtained an ordered list of 2477 terms, the top 10 of which are shown in Table 2, along with the threshold and bias.",
        "The threshold value is read directly from the DT root node and the bias (which indicates whether we desire the feature value to be above or below the threshold) is determined by selecting the branch of the DT which has the highest ratio of non A to A documents.",
        "Initially, this list looks promising, especially since known discriminating words like “upon” and “whilst” are the top two ranked terms.",
        "However, when we applied the changes to our base",
        "ing the authorship to Madison for each disputed document as each feature is adjusted.",
        "We expect the confidence to start high on the left side and move downward as more features are adjusted.",
        "After adjusting all of the identified features, half of the documents were still assigned to Madison (i.e., confidence > 0.50).",
        "Choosing just the high-frequency terms was also problematic since most of them were not considered to be discriminating by DTR ranking (see Table 3).",
        "The lack of DTR rank not only means that these are poor discriminators, but it also means that we do not have a threshold value to drive the feature adjustment process.",
        "We next combined the DTR and the term frequency approaches by computing DTR one the set of features whose frequency exceeds a specified threshold for any one of the authors.",
        "Selecting a frequency of 0.001 produces a list of 35 terms, the first 14 of which are shown in Table 4.",
        "Results for this list were much more promising and are shown in Figure 2.",
        "The confidence of attributing authorship to Madison is reduced by an average of 84.42% (6 = 12.51 %) and all of the documents are now correctly misclassified as being written by Hamilton."
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": [
        "Evaluating the effectiveness of any authorship obfuscation approach is made difficult by the fact that it is crucially dependent on the authorship detection method that is being utilized.",
        "An advantage of using the Federalist Papers as the test data set is that there are numerous papers documenting various methods that researchers have used to identify the authors of the disputed papers.",
        "However, because of differences in the exact data set2 and machine learning algorithm used, it is not reasonable to create an exact and complete implementation of each system.",
        "For our experiments, we used only the standard Federalist Papers documents and tested each feature set using linear-kernel SVMs, which have been shown to be effective in text categorization (Joachims 1998).",
        "To train our SVMs we used a sequential minimal optimization (SMO) implementation described in (Platt 1999).",
        "The SVM feature sets that we used for the evaluation are summarized in Table 5.",
        "For the early experiments described in the previous section we used SVM30, which incorporates the final set of 30 terms that Mosteller & Wallace used for their study.",
        "As noted earlier, they made use of a different data set than we did, so we did expect to see some differences in the results.",
        "The baseline model (plotted as the leftmost column of points in Figure 1 and Figure 2) assigned all of the disputed papers to Madison except one3."
      ]
    },
    {
      "heading": "5.1 Feature Modification",
      "text": [
        "Rather than applying the suggested modifications to the original documents and regenerating the document feature vectors from scratch each time, we simplified the evaluation process by adjusting the feature vector directly and ignoring the impact of the edits on the overall document probabilities.",
        "The combination of insertions and deletions results in the total number of words in the document being increased by an average of 19.58 words (a = 7.79), which is less than 0.5% of the document size.",
        "We considered this value to be small enough that we could safely ignore its impact.",
        "Modifying the feature vector directly also allows us to consider each feature in isolation, without concern for how they might interact with each other (e.g. converting whilst while or rewriting an entire sentence).",
        "It also allows us to avoid the problem of introducing rewrites into the document with our distinctive stylometric signature instead of a hypothetical Madison rewrite."
      ]
    },
    {
      "heading": "5.2 Experiments",
      "text": [
        "We built SVMs for each feature set listed in Table 5 and applied the obfuscation technique described above by adjusting the values in the feature vector by increments of the single-word probability for each document.",
        "The results that we obtained were the same as observed with our test model – all of the models were coerced to prefer Hamilton for each of the disputed documents.",
        "Federalists [ ... ] with the possible exception of No.",
        "55.",
        "For No.",
        "55 our evidence is relatively weak [...].” (Mosteller & Wallace 1964) p.263.",
        "Figure 3 shows the graph for SVM70, the model that was most resilient to our obfuscation techniques.",
        "The results for all models are summarized in Table 6.",
        "The overall reduction achieved across all models is 86.86%.",
        "of assigning the disputed papers to Madison for each of the tested feature sets.",
        "Of particular note in the results are those for SVM03, which proved to be the most fragile model because of its low dimension.",
        "If we consider this case an outlier and remove it from study, our overall reduction becomes 83.82%."
      ]
    },
    {
      "heading": "5.3 Feature Changes",
      "text": [
        "As stated earlier, an important aspect of any obfuscation approach is the number of changes required to effect the mis-attribution.",
        "Table 7 summarizes the absolute number of changes (both insertions and deletions) and also expresses this value related to the original document size.",
        "The average number of changes required per 1000 words in the document is 14.2.",
        "While it is difficult to evaluate how much effort would be required to make each of these individual changes, this value seems to be within the range that a motivated person could reasonably undertake.",
        "More detailed summaries of the number of feature changes required for single document (#49) are given in Table 2 and Table 4.",
        "By calculating the overall number of changes required, we implicitly consider insertions and deletions to be equally weighted.",
        "However, while deletion sites in the document are easy to identify,",
        "proposing insertion sites can be more problematic.",
        "We do not address this difference in this paper, although it is clear that more investigation is required in this area."
      ]
    },
    {
      "heading": "6 Deep Obfuscation",
      "text": [
        "The techniques described above result in what we term shallow obfuscation since they focus on a small number of features and are only useful as a defense against standard attribution attacks.",
        "More advanced attribution techniques, such as that described in (Koppel and Schler 2004) look deeper into the author’s stylometric profile and can identify documents that have been obfuscated in this manner.",
        "Koppel and Schler introduce an approach they term “unmasking” which involves training a series of SVM classifiers where the most strongly weighted features are removed after each iteration.",
        "Their hypothesis is that two texts from different authors will result in a steady and relatively slow decline of classification accuracy as features are being removed.",
        "In contrast, two texts from the same author will produce a relatively fast decline in accuracy.",
        "According to the authors, a slow decline indicates deep and fundamental stylistic differences in style - beyond the “obvious” differences in the usage of a few frequent words.",
        "A fast decline indicates that there is an underlying similarity once the impact of a few superficial distinguishing markers has been removed.",
        "We repeated their experiments using 3-fold cross-validation to compare Hamilton and Madison with each other and the original (D) and obfuscated (D’) documents.",
        "The small number of documents required that we train the SVM using the 50 most frequent words.",
        "Using a larger pool of feature words resulted in unstable models, especially when comparing Madison (14 documents) with D and D’ (12 documents).",
        "The results of this comparison are shown in Figure 4.",
        "In this graph, the comparison of Hamilton and the modified document (MvD’) exhibits the characteristic curve described by Koppel and Schler, which indicates that the original author can still be detected.",
        "However, the curve has been raised above the curve for the original document which suggests that our approach does help insulate against attacks that identify deep stylometric features.",
        "Modifying additional features continues this trend and raises the curve further.",
        "Figure 5 summarizes this difference by plotting the difference between the accuracy of the HvD’ and MvD’ curves for documents at different levels of feature modification.",
        "An ideal curve in this graph would be one that hugged the x-axis since this would indicate that it was as difficult to train a classifier to distinguish between M and D’ as it is to distinguish between H and D’.",
        "In this graph, the “0” curve corresponds to the original document, and the “14” curve to the modified document shown in Figure 4.",
        "The “35” curve uses all of the DTR(0.001) features.",
        "This graph demonstrates that using DTR ranking to drive feature adjustment can produce documents that are increasingly harder to detect as being written by the author.",
        "While it is unsurprising that a deep level of obfuscation is not achieved when only a minimal number of features are modified, this graph can be used to measure progress so that the author can determine enough features have been modified to achieve the desired level of anonymization.",
        "Equally unsurprising is that this increased anonymization comes at an additional cost, summarized in Table 8.",
        "While in this work we limited ourselves to the 35 DTR(0.001) features, further document modification can be driven by lowering the DTR probability threshold to identify additional terms in an orderly fashion."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "In this paper, we have shown that the standard approaches to authorship attribution can be confounded by directing the author to selectively edit the test document.",
        "We have proposed a technique to automatically identify distinctive features and their frequency thresholds.",
        "By using a list of features that are both frequent and highly ranked according to this automatic technique, the amount of effort required to achieve reasonable authorship obfuscation seems to be well within the realm of a motivated author.",
        "While we make no claim that this is an easy task, and we make the assumption that the author has undertaken basic preventative measures (like spellchecking and grammar checking), it does not seem to be an onerous task for a motivated individual.",
        "It not surprising that we can change the outcome by adjusting the values of features used in authorship detection.",
        "Our contribution, however, is that many of the important features can be determined by simultaneously considering term-frequency and DTR rank, and that this process results in a set of features and threshold values that are transparent and easy to control.",
        "Given this result, it is not unreasonable to expect that a tool could be created to provide feedback to an author who desires to publish a document anonymously.",
        "A sophisticated paraphrase tool could theoretically use the function word change information to suggest rewrites that worked toward the desired term frequency in the document.",
        "For our experiments, we used a simplified model of the document rewrite process by evaluating the impact of each term modification in isolation.",
        "However, modifying the document to increase or decrease the frequency of a term will necessarily impact the frequencies of other terms and thus affect the document's stylometric signature.",
        "Further experimentation is clearly needed in this area needs to address the impact of this interdependency.",
        "One limitation to this approach is that it applies primarily to authors that have a reasonably-sized corpus readily available (or easily created).",
        "However, for situations where a large corpus is not available, automated authorship attribution techniques are likely to be less effective (and thus obfuscation is less necessary) since the number of possible features can easily exceed the number of available documents.",
        "An interesting experiment would be to explore how this approach applies to different types of corpora like email messages.",
        "We also recognize that these techniques could be used to attempt to imitate another author’s style.",
        "We do not address this issue other than to say that our thresholding approach is intended to push feature values just barely across the threshold away from A rather than to mimic any one particular author.",
        "Finally, in these results, there is a message for those involved in authorship attribution: simple SVMs and low-dimensional models (like SVM03) may appear to work well, but are far less resilient to obfuscation attempts than Koppel and Schler’s unmasking approach.",
        "Creating classifiers with the minimum number of features produces a model that is brittle and more susceptible to even simplistic obfuscation attempts."
      ]
    },
    {
      "heading": "8 Acknowledgements",
      "text": [
        "Thanks are in order to the reviewers of earlier drafts of this document, notably Chris Brockett and our anonymous reviewers.",
        "In addition, Max Chickering provided useful information regarding his implementation of DTs in the WinMine toolkit."
      ]
    }
  ]
}
