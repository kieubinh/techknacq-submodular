{
  "info": {
    "authors": [
      "Guergana Savova",
      "Terry Therneau",
      "Christopher G. Chute"
    ],
    "book": "Workshop on Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together",
    "id": "acl-W06-2502",
    "title": "Cluster Stopping Rules for Word Sense Discrimination",
    "url": "https://aclweb.org/anthology/W06-2502",
    "year": 2006
  },
  "references": [
    "acl-W04-2406"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "As text data becomes plentiful, unsupervised methods for Word Sense Disambiguation (WSD) become more viable.",
        "A problem encountered in applying WSD methods is finding the exact number of senses an ambiguity has in a training corpus collected in an automated manner.",
        "That number is not known a priori; rather it needs to be determined based on the data itself.",
        "We address that problem using cluster stopping methods.",
        "Such techniques have not previously applied to WSD.",
        "We implement the methods of Calinski and Harabasz (1975) and Harti-gan (1975) and our adaptation of the Gap statistic (Tibshirani, Walter and Hastie, 2001).",
        "For evaluation, we use the WSD Test Set from the National Library of Medicine, whose sense inventory is the Unified Medical Language System.",
        "The best accuracy for selecting the correct number of clusters is 0.60 with the C&H method.",
        "Our error analysis shows that the cluster stopping methods make finer-grained sense distinctions by creating additional clusters.",
        "The highest F-scores (82.89), indicative of the quality of cluster membership assignment, are comparable to the baseline majority sense (82.63) and point to a path towards accuracy improvement via additional cluster pruning.",
        "The importance and significance of the current work is in applying cluster stopping rules to WSD."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The dominant approach in word sense disambiguation (WSD) is based on supervised learning from manually sense-tagged text.",
        "While this is effective, it is quite difficult to get a sufficient number of manually sense-tagged examples to train a system.",
        "Mihalcea (2003) estimates that 80-person years of annotation would be needed to create training corpora for 20,000 ambiguous English words, given 500 instances per word.",
        "For that reason, we are developing unsupervised knowledge-lean methods that avoid the bottlenecks created by sense-tagged text.",
        "Unsupervised clustering methods utilize only raw corpora as their source of information, and there are growing amounts of general and specialized domain corpora available, e.g. biomedical domain corpora.",
        "Improvements in WSD methods would be of immediate value in indexing and retrievals of biomedical text given the explosion of biomedical literature as well as the rapid deployment of electronic medical records.",
        "Semantic/conceptual indexing and retrieval in that domain is often done in regard to the Unified Medical Language System (UMLS) developed at the National Library of Medicine (NLM) at the United States National Institutes of Health (NIH)1.",
        "It is important to understand that the UMLS is significantly different than a dictionary, which is often the source of the sense inventory.",
        "Rather, the UMLS integrates more than 100 medical domain controlled vocabularies such as SNOMED-CT2 and the International Classification of Diseases (ICD)3.",
        "UMLS has three main components.",
        "The first component, the Metathesaurus, includes all terms from the controlled vocabularies and is organized by concept, which is a cluster of terms representing the same meaning.",
        "Each concept is assigned a concept unique identifier (CUI), which is inherited by each term in the cluster.",
        "UMLS-based semantic indexing is based on CUI assignments.",
        "The second component, the Semantic Network, groups the concepts into 134 types of categories and indicates the relationships between them.",
        "The Semantic Network is a coarse ontology of the concepts.",
        "The third component, the SPECIALIST lexicon, contains syntactic information for the Metathesaurus terms.",
        "MeSH, an ontology within UMLS, is heavily used for indexing biomedical scientific publications, e.g. Medline4.",
        "Hospitals, medical practices and biomedical research increasingly rely on the UMLS, or a subset ontology within it, to index and retrieve relevant information.",
        "It is estimated that approximately 7400 UMLS terms map to multiple concepts which creates ambiguity (Weeber, Mork and Aronson., 2001).",
        "Term ambiguity has been pointed out to be one of the major challenges for UMLS-based semantic indexing and retrieval (Weeber et al., 2001).",
        "For example, “cold” has the following six UMLS meanings, each with its own UMLS CUI: cold temperature, common cold, cold sensation, cold therapy, chronic obstructive lung disease (COLD), and Cold brand of chlorpheniramine-phenylpropanolamine.",
        "The problem we are addressing in this paper is discovering the number of senses an ambiguous word has in a given corpus, which is a component within a completely unsupervised WSD system.",
        "For example, if a corpus of 1000 instances containing the word “cold” has been compiled from patients medical records, how many “cold“ senses are in that corpus?",
        "This is a challenge any NLP system implementing WSD faces.",
        "To address this problem, we apply cluster stopping rules in an automated way.",
        "The paper is organized as follows.",
        "Section 2 overviews the related work on cluster stopping rules.",
        "Section 3 outlines our methods, tools, features selection, test set and evaluation metrics.",
        "Section 4 presents the results and discusses them.",
        "Section 5 is the conclusions."
      ]
    },
    {
      "heading": "2 Background and Related Work",
      "text": [
        "Our work is based on cluster analysis.",
        "Cluster analysis is often performed to discover the groups that the data naturally fall into.",
        "The number of groups is not known a priori; rather, it needs to be determined based on the data itself.",
        "Such methods or “cluster stopping rules” usually rely on within-cluster dissimilarity/error (W(k)) metrics which in general exhibit a decline when the number of clusters increases.",
        "Splitting a natural group into subgroups reduces the criterion less than when well-separated clusters are discovered.",
        "In those cases, the W(k) will not have a sharp decline as the instances are close.",
        "This phenomenon has been described in statistical literature as the “elbow” effect as illustrated in",
        "been the goal of many research studies (Hartigan, 1975; Calinski and Harabasz, 1975; Milligan and Cooper, 1987; Tibshirani, Walter and Hastie, 2001 among many).",
        "Milligan and Cooper (1985) offer the most comprehensive comparative study of the performance of 30 stopping rules.",
        "They carry out their study on “mildly truncated data from multivariate normal distributions, and one would not expect their ranking of the set of stopping rules to be reproduced exactly if a different cluster-generating strategy were adopted.” (Gordon, 1999, p. 61).",
        "The five rules which were the top performance in the Milligan and Cooper study are Calinski and Harabasz (1974) a.k.a.",
        "C&H, Goodman and Kruskal (1954), C index (Hubert and Schultz, 1976), Duda and Hart (1973) and Beale (1969).",
        "Tibshirani et al.",
        "(2001) introduce the Gap statistic and compare its performance to the methods of Calinski and Harabasz (1974), Krzanowski and Lai (1985), Hartigan (1975), and the Silhouette method (Kaufman and Rousseeuw, 1990).",
        "On the simulated and DNA microarray data Tibshirani and colleagues used for their experiments, the Gap statistic yields the best result.",
        "In general, stopping rules fall into two categories – global and local (Gordon, 1999; Tibshirani et al., 2001).",
        "Global rules take into account a combination of within-cluster and between-cluster similarity measures over the entire data.",
        "Global rules choose such k where that combined metric is optimal.",
        "Global rules, however, in most cases do not work for k=1, that is they do not make predictions of when the data should not be partitioned at all.",
        "Global rules look at the entire data over k number of clusters.",
        "Local rules, on the other hand, are based only on a given k solution or individual pairs of clusters and test whether they should be grouped together.",
        "They need a threshold value or a significance level, which depends on the specific data and in most cases have to be empirically determined."
      ]
    },
    {
      "heading": "3 Methodology",
      "text": []
    },
    {
      "heading": "3.1 Overview",
      "text": [
        "In this study, we explore three cluster stopping methods as applied to unsupervised WSD – Hartigan (1975), Calinski and Harabasz (1974), and the Gap statistic (Tibshirani et al., 2001).",
        "The data to be clustered is instances of context surrounding each ambiguity.",
        "Each instance is converted into a feature vector where the features are",
        "ngrams (unigrams or bigrams) and each cell is the frequency of occurrence of a unigram or bi-gram or the log-likelihood of a bigram occurring in that particular instance after applying a feature selection method.",
        "The clustering algorithm for this set of experiments is agglomerative clustering (see Section 3.5 for a more detailed description).",
        "Our goal is to group contexts into separate clusters based on the underlying sense of the ambiguous word.",
        "Thus, the observations are contexts and the features are the identified lexical features (i.e. significant word(s)) that represent the contexts.",
        "Our observed data matrix generally shows the following characteristics –",
        "1) it is discrete 2) it is high dimensional/multivariate 3) it can be real valued or integer, or binary 4) it is sparse; while the number of features can be in few hundreds, contexts have a length limit (ignoring the commonly occurring “closed class words” like “the”, “an”, “on” etc.)",
        "5) it represents a distribution of contexts that is generally skewed.",
        "Following is our motivation for choosing the three cluster stopping rules.",
        "Hartigan (1975) and Calinski and Harabasz (1974) have been consistently used as baselines in a number of studies, e.g. Tibshirani et al.",
        "(2001).",
        "The Hartigan method is computationally simple and efficient and unlike C&H, it is defined for k=1.",
        "The C&H method was ranked the top among 30 stopping rules in the comprehensive study conducted by Milligan and Cooper (1975).",
        "The Gap statistic (Tibshirani et al., 2001) is a fairly recent method that has gained popularity by showing excellent results when applied to the bio domain, e.g. clustering DNA mircoarray data.",
        "None of the methods, however, have been applied or adapted to WSD."
      ]
    },
    {
      "heading": "3.2 Calinski and Harabasz (1975) Method",
      "text": [
        "The C&H method is reported to perform the best among 30 stopping rules (Milligan and Cooper, 1985).",
        "C&H is a global method.",
        "The Variance Ratio Criteria C&H uses is where BGSS (between group sum of squares) is the sum of the dispersions between the k cluster centroids (within-gr cluster’s (measured tween eac weighed b the numbe instances.",
        "tance.",
        "As “is analog analysis” i C&H seek"
      ]
    },
    {
      "heading": "3.3 H",
      "text": [
        "where n i clustered, WGSS(k) i cluster me clusters wh H(k) is needed rat proximates H(k) woul is warrant rule of th creasing th (Hartigan, smallest k can return tigan (197"
      ]
    },
    {
      "heading": "3.4 G",
      "text": [
        "In general ference/ga sion meas that for a data.",
        "Tibsh 2001) start ter null mo a k-compo supports it form distr data to sta the W(k) o is the po around th Euclidean the least likely to",
        "Tibshirani and colleagues also point out that the choice of an appropriate null distribution depends on the data.",
        "Tibshirani and colleagues compare the curve of log(W(k)) to the log(W*(k)) curve obtained from the reference uniformly distributed over the data.",
        "The estimated optimal number of clusters is the k value where the gap between the two curves is the largest.",
        "Figure 1 is an example of log(W(k)) to the log(W*(k)) curves used in the computation of the Gap statistic.",
        "The two main advantages of the Gap statistic over various previously proposed “stopping rules” are its ability to work with data created by almost any type of clustering and its ability to accurately estimate the optimal number of clusters even for data that naturally falls into just one cluster.",
        "The Gap statistic is an application of parametric bootstrap methods to the clustering problem.",
        "Unlike non-parametric methods, parametric techniques represent the observed data distribution.",
        "The basic strategy is to create multiple random data sets over the observed distribution for which there are no clusters, apply the chosen clustering method to them, and tabulate the apparent decrease in within-cluster variation that ensues.",
        "This gives a measure of optimism with which to compare the clustering of the observed data.",
        "The complete methodology can be broadly classified into two important components namely the reference distribution and the algorithm which uses the reference distribution.",
        "We describe each of the two components below."
      ]
    },
    {
      "heading": "Reference Distribution Generation for an NLP Task",
      "text": [
        "Here, we describe how we extend the generation of the reference distribution over the observed data to retain the characteristics mentioned at end of section 3.1.",
        "We will use the observed data shown in Table 1 as a running example.",
        "To simulate the structure of the observed data, the following features are to be emulated: (a) Context length is the number of features that can occur in a context.",
        "Contexts can be sentences, paragraphs, entire documents or just any specified window size.",
        "In general, the number of available features will be at least in the hundreds, however, only a few might occur in a given context, especially if the context is limited to the sentence the target ambiguity occurs in.",
        "Additionally, context length is influenced by the feature selection method – if only very frequent lexical units are retained as features, then only those units will represent the context.",
        "Thus, a context length could be very small compared to the size of the feature set.",
        "In the example from Table 1, context length is captured by the row marginals, e.g. the context length for Context1 is 3, which means that overall there are only three features for that context.",
        "(b) Sparsity is a consequence of relatively small context length.",
        "Currently, our assumption is that contexts are derived from small discourse units (sentences or abstracts at the most).",
        "For bigger discourse units, e.g. several paragraphs or entire documents, our proposed generation of the reference distribution should be modified to reflect feature occurrences over those units.",
        "In the example from Table 1, for instance in Context1, there are 3 features that are present – Feature1, Feature4 and Feature5 – the rest are absent.",
        "Sparsity can be viewed as the number of absent/zero-valued features for each row.",
        "(c) Feature distribution is the frequency of occurrence of each feature across all contexts.",
        "It is captured by the column marginals of the observed data matrix.",
        "For example, in Table 1 Feature 1 occurs twice over the entire data; similarly Feature2 occurs twice and so on.",
        "Feature distribution can be viewed as the number of occurrences of each feature in the entire corpus.",
        "Now we describe how we do the reference generation to stay faithful to the characteristics described above.",
        "We use the uniform and the proportional methods.",
        "The uniform method generates data that realizes (a) and (b) characteristics of the data and is the used originally in Tibshirani et al.",
        "(2001).",
        "The proportional method captures (a), (b), and (c) and is our adaptation of the Gap method.",
        "The data is constructed as follows.",
        "To retain the context lengths of the observed data in the",
        "reference data, the row marginals of the reference data are fixed to be equal to those of the observed data.",
        "In Table 1, the row marginals for the reference data will be {3, 3, 2, 4}.",
        "Carrying the observed marginals to the reference data applies to both the uniform and proportional methods.",
        "Note that currently we fix only the row marginals.",
        "Due to the current assumption of binary feature frequency, the generated reference data is binary too and this is true for both methods.",
        "The main difference between the uniform and proportional methods lies in whether the feature distribution is maintained in the simulation.",
        "The uniform method does not weigh the features; rather, all features are given equal probability of occurring in the generated data.",
        "A uniform random number r over the range [1, featureSetSize] is drawn.",
        "The cell corresponding to the rth column (i.e. feature) in the current row under consideration (i.e. context) is assigned “1”.",
        "For example, in our running example let’s say we are generating reference data for the 3rd row from Table 1.",
        "We first generate a random number over the range [1, p].",
        "Let’s assume that the generated number is 4.",
        "Then, the cell [3, 4] is assigned value “1”.",
        "This procedure is repeated twice since the row marginal for this row of the reference data is 2.",
        "The proportional method factors in the distribution of the column marginals of the observed data while generating the random data.",
        "Unlike the uniform method, it takes into account the weight of each feature.",
        "In other words, the features by their frequency assign themselves a range.",
        "For example, the features in the Table 1 will be assigned the following ranges: Feature1 - [1, 2]; Feature2 - [3, 4]; Feature3 – [5, 6]; Feature4 – [7, 8]; Feature5 – [9, 12].",
        "A random number is generated over the range [1, total number of feature occurrences].",
        "For the data in Table 1, a random number is generated over the range [1, 12].",
        "The feature corresponding to the range in which the random number falls is assigned “1”.",
        "For example, if we are generating the reference for Context3 and the generated random number over the range [1, 12] is 5, then a lookup determines that 5 falls in the range for Feature3.",
        "Hence, the cell in Context3 corresponding to Feature3 is assigned “1”.",
        "Similar to the uniform method we would repeat this procedure twice to achieve the row marginal total of 2.",
        "Currently we proceed with the binary reference data created by the procedure described above.",
        "Note that this binary reference matrix can be converted to a strength-of-association matrix by multiplying it with a diagonal matrix that contains the strength-of-association scores, e.g. log likelihood ratio, Mutual Information, Pointwise mutual information, Chi-squared to name a few."
      ]
    },
    {
      "heading": "Algorithm",
      "text": [
        "The complete algorithm of the Gap Statistics which the reference distribution is a part of is:",
        "1.",
        "Cluster the observed data, varying the total number of clusters from k = 1, 2, ...., K, giving within dispersion measures W(k), k = 1, 2, ....K. 2.",
        "Generate B reference datasets using the uni",
        "form or the proportional methods as described above, and cluster each one giving within dispersion measures W*(kb), b = 1, 2, ... B, k = 1, 2,... K. Compute the estimated Gap statistic:",
        "The final step is the criterion for selecting the optimal k value.",
        "It says to choose the smallest k value for which the gap is greater than the gap for the earlier k value by the significance test of “one standard error”.",
        "The “one standard error” calculations are modified to account for the simulation error.",
        "Tibshirani and colleagues also advise to use a multiplier to the s(k) for better rejection of the null hypothesis."
      ]
    },
    {
      "heading": "3.5 Tools, Feature Selection and Method Parameters",
      "text": [
        "For feature representation, selection, context representation and clustering, we used Sense-Clusters0.69 (http://senseclusters.sourceforge.net).",
        "It offers a variety of lexical features (ngrams, collocations, etc.)",
        "and feature selection methods (frequency, log likelihood, etc.).",
        "The contexts can then be represented with those features in vector space using first or second order vectors which are then clustered.",
        "A detailed description can be found in Purandare and Pedersen (2004) and http://www.d.umn.edu/~tpederse/senseclusters.html.",
        "SenseClusters links to CLUTO for the clustering part (http://www-users.cs.umn.edu/~karypis/ cluto/download.html).",
        "CLUTO implements in a fast and efficient way the main clustering algorithms – agglomerative, partitional and repeated bisections.",
        "We chose the following methods for feature representation and selection.",
        "Method1 uses bigrams as features, average link clustering in similarity space and the abstract as the context to derive the features from.",
        "The method is described in Purandare and Pedersen (2004).",
        "It is based on first order context vectors, which represent features that occur in that context.",
        "A similarity matrix is clustered using the average link agglomerative method.",
        "Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts).",
        "The application of that method to the biomedical domain is described in a technical report (Savova, Pedersen, Kulkarni and Purandare, 2005).",
        "Method2 uses unigrams which occur at least 5 times in the corpus.",
        "The context is the abstract.",
        "The choice of those features is motivated by Joshi, Pedersen and Maclin (2005) study which achieves best results with unigram features.",
        "For the Hartigan cluster stopping method, the threshold is set to 10 which is the recommendation in the original algorithm.",
        "For the Gap cluster stopping method, we experiment with B=100, and the uniform and proportional reference generation methods."
      ]
    },
    {
      "heading": "3.6 Test Set",
      "text": [
        "Our test set is the NLM WSD5 set which comprises 5000 disambiguated instances for 50 highly frequent ambiguous UMLS Metathesau",
        "rus strings (Weeber et al., 2001).",
        "Each ambiguity has 100 manually sense-tagged instances.",
        "All instances were randomly chosen from Medline abstracts.",
        "Each ambiguity instance is provided with the sentence it occurred in and the Medline abstract text it was derived from.",
        "The senses for every ambiguity are the UMLS senses plus a “none of the above” category which captures all instances not fitting the available UMLS senses.",
        "For the current study, we modified the NLM WSD by excluding instances sense-tagged with the “none of the above” category.",
        "This is motivated by the fact that that category is a catch-all category for all senses that do not fit the current UMLS inventory.",
        "First, we excluded words whose majority category was “none of the above”.",
        "Secondly, from the instances of the remaining words, we removed those marked with “none of the above”.",
        "That subset of the original NLM WSD set we refer to as the “modified NLM WSD set” (Table 2)."
      ]
    },
    {
      "heading": "3.7 Evaluation",
      "text": [
        "Our evaluation of the performance of the cluster stopping rules is twofold.",
        "Accuracy is a direct evaluation measuring the correctly recognized number of senses: words with correctly predicted number of senses all words Accuracy evaluates how well the methods discover the exact number of senses in the test corpus.",
        "The F-score of the WSD is an indirect evaluation for the quality of the cluster assignment:",
        "Precision is the number of correctly clu instances divided by the number of cluster stances; Recall is the number of correctly tered instances divided by all instances.",
        "may be some number of contexts that the cling algorithm declines to process, which le the difference in precision and recall.",
        "Our baseline is a simple clustering al assigns all instances of a target word cluster."
      ]
    },
    {
      "heading": "4 Results and Discussion",
      "text": [
        "Table 3 presents the results for th",
        "In terms of accuracy (Table 3, column 3), the C&H method has the best results (p<0.01 with t-test).",
        "Note that the modified NLM WSD set contains seven words with one sense – depression, pressure, determination, fluid, frequency, scale, secretion – for which the C&H method is at a disadvantage as it cannot return one cluster solution.",
        "In terms of predicted number of senses (Table 3, column 5), the Hartigan method tends to underestimate the number of senses (overcluster), thus making coarser sense distinctions.",
        "The adapted Gap and C&H methods tend to overestimate them (undercluster), thus making finer grained sense distinctions.",
        "In terms of cluster member assignment as demonstrated by the F-scores (Table 3, column 4), our adapted Gap method and the Hartigan method perform better than the C&H method (p<0.05 with t-test).",
        "The Hartigan method F-scores along with Gap uniform with Method 1 feature selection are not significantly different from the baseline (p>0.05 with t-test); the rest are significantly lower than the majority sense baseline (p<0.05 with t-test).",
        "The high F-scores point to a path for improving accuracy results.",
        "Singleton clusters could be pruned as they could be insignificant to sense discrimination.",
        "As it was pointed out, the best performing algorithms (C&H and Gap proportional) tend to create too many clusters (Table 3, column 5).",
        "Another way of dealing with singleton or smaller clusters is to present them for human review as they might represent new sense distinctions not included in the sense inventory.",
        "One explanation for the performance of the stopping rules (overclustering in particular) might be that some senses are very similar, e.g. “cold temperature” and “cold sensation” for the “cold” ambiguity in instances like “Her feet are cold.” Another explanation is that the stopping rules rely on the clustering algorithm used.",
        "In our current study, the experiments were run with only agglomerative clustering as implemented in CLUTO.",
        "The distance measure that we used is Euclidean distance, which is only one of many choices.",
        "Yet another explanation is in the feature sets we experimented with.",
        "They performed very similarly on both the accuracy and F-scores.",
        "Future work we plan to do is aimed at experimenting with different features, clustering algorithms, distance measures as well as applying Singular Value Decomposition (SVD) to the reference distribution matrix for our adapted Gap method.",
        "We are actively pursuing reference generation with fixed column and row marginals.",
        "The work of Pedersen, Kayaalp and Bruce (1996) uses this technique to find significant lexical relationships.",
        "They use the CoCo (Badsberg, 1995) package which implements the Patefield (1981) algorithm for I x J tables.",
        "Another venue is in the combination of several stopping rules which will take advantage of each rule’s strengths.",
        "Yet another component that needs to be addressed towards the path of completely automated WSD is cluster labeling."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "In this work, we explored the problem of discovering the number of the senses in a given target ambiguity corpus by studying three cluster stopping rules.",
        "We implemented the original algorithms of Calinski and Harabasz (1975) and Hartigan (1975) and adapted the reference generation of the Gap algorithm (Tibshirani et al., 2001) to our task.",
        "The best accuracy for selecting the correct number of clusters is 0.60 with the",
        "C&H method.",
        "Our error analysis shows that the cluster stopping methods make finer-grained sense distinctions by creating additional singleton clusters.",
        "The F-scores, indicative of the quality of cluster membership assignment, are in the 80’s and point to a path towards accuracy improvement via additional cluster pruning."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The Perl modules of our implementations of the algorithms can be downloaded from http://search.cpan.org/dist/Statistics-CalinskiHarabasz/, http://search.cpan.org/dist/Statistics-Hartigan/, http://search.cpan.org/dist/Statistics-Gap/.",
        "We are greatly indebted to Anagha Kulkarni and Ted Pedersen for their participation in this research.",
        "We would also like to thank Patrick Duffy, James Buntrock and Philip Ogren for their support and collegial feedback, and the Mayo Clinic for funding the work."
      ]
    }
  ]
}
