{
  "info": {
    "authors": [
      "Ping Li",
      "Kenneth Ward Church"
    ],
    "book": "Computational Linguistics",
    "id": "acl-J07-3003",
    "title": "A Sketch Algorithm for Estimating Two-Way and Multi-Way Associations",
    "url": "https://aclweb.org/anthology/J07-3003",
    "year": 2007
  },
  "references": [
    "acl-H05-1089",
    "acl-J93-1003",
    "acl-P05-1077",
    "acl-P89-1010",
    "acl-W04-3243"
  ],
  "sections": [
    {
      "text": [
        "Ping Li*",
        "Stanford University",
        "Kenneth W. Church** Microsoft Corporation",
        "We should not have to look at the entire corpus (e.g., the Web) to know if two (or more) words are strongly associated or not.",
        "One can often obtain estimates of associations from a small sample.",
        "We develop a sketch-based algorithm that constructs a contingency table for a sample.",
        "One can estimate the contingency table for the entire population using straightforward scaling.",
        "However, one can do better by taking advantage of the margins (also known as document frequencies).",
        "The proposed method cuts the errors roughly in half over Broder's sketches."
      ]
    },
    {
      "heading": "1.. Introduction",
      "text": [
        "We develop an algorithm for efficiently computing associations, for example, word associations.",
        "Word associations (co-occurrences, or joint frequencies) have a wide range of applications including: speech recognition, optical character recognition, and information retrieval (IR) (Salton 1989; Church and Hanks 1991; Dunning 1993; Baeza-Yates and Ribeiro-Neto 1999; Manning and Schütze 1999).",
        "The Know-It-All project computes such associations at Web scale (Etzioni et al.",
        "2004).",
        "It is easy to compute a few association scores for a small corpus, but more challenging to compute lots of scores for lots of data (e.g., the Web), with billions of Web pages (D) and millions of word types.",
        "Web search engines produce estimates of page hits, as illustrated in Tables 13.",
        "Table 1 shows hits for two high frequency words, a and the, suggesting that the total number of English documents is roughly D « 10.",
        "In addition to the two high-frequency words, there are three low-frequency words selected from The New Oxford Dictionary of English (Pearsall 1998).",
        "The low-frequency words demonstrate that there are many hits, even for relatively rare words.",
        "How many page hits do \"ordinary\" words have?",
        "To address this question, we randomly picked 15 pages from a learners' dictionary (Hornby 1989), and selected the first entry on each page.",
        "According to Google, there are 10 million pages/word (median value, aggregated over the 15 words).",
        "To compute all two-way associations for the 57,100 entries in this dictionary would probably be infeasible, let alone all multi-way associations.",
        "* Department of Statistical Science, Cornell University, Ithaca, NY 14853.",
        "E-mail: pl332@cornell.edu.",
        "** Microsoft Research, Microsoft Corp., Redmond, WA 98052.",
        "E-mail: church@microsoft.com.",
        "1 This paper considers boolean (0/1) data.",
        "See Li, Church, and Hastie (2006, 2007) for generalizations to real-valued data (and lp distances).",
        "2 All experiments with MSN.com and Google were conducted in August 2005.",
        "Submission received: 6 December 2005; revised submission received: 5 September 2006; accepted for publication: 7 December 2006.",
        "Estimates of page hits are not always consistent.",
        "Joint frequencies ought to decrease monotonically as we add terms to the query, but estimates produced by current state-of-the-art search engines sometimes violate this invariant.",
        "This table illustrates the usefulness of joint counts in query planning for databases.",
        "To minimize intermediate writes, the optimal order of joins is: ((\"Schwarzenegger\" n \"Austria\") n \"Terminator\") n \"Governor,\" with 136,000 intermediate results.",
        "The standard practice starts with the least frequent terms, namely, ((\"Schwarzenegger\" n \"Terminator\") n \"Governor\") n \"Austria,\" with 579,100 intermediate results.",
        "Query Hits (Google)",
        "Estimates are often good enough.",
        "We should not have to look at every document to determine whether two words are strongly associated or not.",
        "One could use the estimated co-occurrences from a small sample to compute the test statistics, most commonly Pearson's chi-squared test, the likelihood ratio test, Fisher's exact test, cosine similarity, or resemblance (Jaccard coefficient) (Dunning 1993; Manning and Schütze 1999; Agresti 2002; Moore 2004).",
        "Page hits for a",
        "few high-frequency words and a few low-frequency words (as of August 2005).",
        "Query",
        "Hits (MSN.com)",
        "Hits (Google)",
        "A",
        "2,452,759,266",
        "3,160,000,000",
        "The",
        "2,304,929,841",
        "3,360,000,000",
        "Kalevala",
        "159,937",
        "214,000",
        "Griseofulvin",
        "105,326",
        "149,000",
        "Saccade",
        "38,202",
        "147,000",
        "Query",
        "Hits (MSN.com)",
        "Hits (Google)",
        "America",
        "150,731,182",
        "393,000,000",
        "America, China",
        "15,240,116",
        "66,000,000",
        "America, China, Britain",
        "235,111",
        "6,090,000",
        "America, China, Britain, Japan",
        "154,444",
        "23,300,000",
        "Austria",
        "88,200,000",
        "Governor",
        "37,300,000",
        "One-way",
        "Schwarzenegger",
        "4,030,000",
        "Terminator",
        "3,480,000",
        "Governor, Schwarzenegger",
        "1,220,000",
        "Governor, Austria",
        "708,000",
        "Schwarzenegger, Terminator",
        "504,000",
        "Two-way",
        "Terminator, Austria",
        "171,000",
        "Governor, Terminator",
        "132,000",
        "Schwarzenegger, Austria",
        "120,000",
        "Governor, Schwarzenegger, Terminator",
        "75,100",
        "Three-way",
        "Governor, Schwarzenegger, Austria",
        "46,100",
        "Schwarzenegger, Terminator, Austria",
        "16,000",
        "Governor, Terminator, Austria",
        "11,500",
        "Four-way",
        "Governor, Schwarzenegger, Terminator, Austria",
        "6,930",
        "Sketch for Estimating Associations",
        "Sampling can make it possible to work in physical memory, avoiding disk accesses.",
        "Brin and Page (1998) reported an inverted index of 37.2 GBs for 24 million pages.",
        "By extrapolation, we should expect the size of the inverted indexes for current Web scale to be 1.5 TBs/billion pages, probably too large for physical memory.",
        "A sample is more manageable.",
        "When estimating associations, it is desirable that the estimates be consistent.",
        "Joint frequencies ought to decrease monotonically as we add terms to the query.",
        "Table 2 shows that estimates produced by current search engines are not always consistent.",
        "1.1 The Data Matrix, Postings, and Contingency Tables",
        "We assume a term-by-document matrix, A,with n rows (words) and D columns (documents).",
        "Because we consider boolean (0/1) data, the (i, j)th entry of A is 1 if word i occurs in document j and 0 otherwise.",
        "Computing all pairwise associations of A is a matrix multiplication, AAT.",
        "Because word distributions have long tails, the term-by-document matrix is highly sparse.",
        "It is common practice to avoid materializing the zeros in A, by storing the matrix in adjacency format, also known as postings, and an inverted index (Witten, Moffat, and Bell 1999, Section 3.2).",
        "For each word W, the postings list, P, contains a sorted list of document IDs, one for each document containing W.",
        "Figure 1(a) shows a contingency table.",
        "The contingency table for words W1 and W2 can be expressed as intersections (and complements) of their postings P1 and P2 in the obvious way:",
        "c = |P21.",
        "For larger corpora, it is natural to introduce sampling.",
        "For example, we can randomly sample Ds (out of D) documents, as illustrated in Figure 1(b).",
        "This sampling scheme, which we call sampling over documents, is simple and easy to describe – but we can do better, as we will see in the next subsection.",
        "w.",
        "Figure 1 (a) A contingency table for word W1 and word W2.Cell a is the number of documents that contain both W1 and W2, b is the number that contain W1 but not W2, c is the number that contain W2 but not W1,and d is the number that contain neither.",
        "The margins, f1 = a + b and f2 = a + c are known as document frequencies in IR.",
        "D = a + b + c + d is the total number of documents in the collection.",
        "For consistency with the notation we use for multi-way associations, a, b, c,and d are also denoted, in parentheses, by x1, x2, x3,and x4, respectively.",
        "(b) A sample contingency table (as, bs, cs, ds), where the subscript s indicates the sample space.",
        "The cells are also numbered as (s1, s2, s3, s4).",
        "a (xi)",
        "b(x2)",
        "c (x3)",
        "d (x4)",
        "k (s2)",
        "cs(s3)",
        "ds (54)",
        "1.2 Sampling Over Documents and Sampling Over Postings",
        "Sampling over documents selects Ds documents randomly from a collection of D documents, as illustrated in Figure 1.",
        "The task of computing associations is broken down into three subtasks:",
        "1.",
        "Compute sample contingency table.",
        "2.",
        "Estimate contingency table for population from sample.",
        "3.",
        "Summarize contingency table to produce desired measure of association: cosine, resemblance, mutual information, correlation, and so on.",
        "Sampling over documents is simple and well understood.",
        "The estimation task is straightforward if we ignore the margins.",
        "That is, we simply scale up the sample in the obvious way: aMF = .",
        "We refer to these estimates as the \"margin-free\" baseline.",
        "However, we can do better when we know the margins, f1 = a + b andf2 = a + c (called document frequencies in IR), using a maximum likelihood estimator (MLE) with fixed margin constraints.",
        "Rare words can be a challenge for sampling over documents.",
        "In terms of the term-by-document matrix A, sampling over documents randomly picks a fraction (^)of columns from A.",
        "This is a serious drawback because A is highly sparse (as word distributions have long tails) with a few high-frequency words and many low-frequency words.",
        "The jointly non-zero entries in A are unlikely to be sampled unless the sampling rate D is high.",
        "Moreover, the word sparsity differs drastically from one word to another; it is thus desirable to have a sampling mechanism that can adapt to the data sparsity with flexible sample sizes.",
        "One size does not fit all.",
        "\"Sampling over postings\" is an interesting alternative to sampling over documents.",
        "Unfortunately, it doesn't work out all that well either (at least using a simple straightforward implementation), but we present it here nevertheless, because it provides a convenient segue between sampling over documents and our sketch-based recommendation.",
        "\"Naive sampling over postings\" obtains a random sample of size k\\ from P1,de-notedasZ1, and a random sample Z2 of size k2 from P2.",
        "Also, we denote aN = |Z1 n Z2|.",
        "We then use aN to infer a.",
        "For simplicity, assume k1 = k2 = k and f1 = f2 = f. It follows that E ^%- j = f^l.",
        "In other words, under naive sampling over postings, one could estimate the associations by 12",
        "3 Suppose there are m defectives among N objects.",
        "We randomly pick k objects (without replacement) and obtain x defectives.",
        "Then x follows a hypergeometric distribution, x ~ HG(N, m, k).",
        "It is known that E(x) =",
        "mmk.",
        "In our setting, suppose we know that among Zj (of size kj), there are af samples that belong to the original intersection Pj n P2.",
        "Similarly, suppose we know that there are af2 samples among Z2 (of size k2) that belong to Pj n P2.Then aN = |Zj n Z2I ~ HG(a, as , af2).",
        "Therefore E (aN) = Jas af2.",
        "Because as and af2 are both random, we should use conditional expectations: E (aN) = E ^a^a^, af2jj = E (iafl af2%) = J E (afl^ E (af2Y (Recall that Zj andZ2 are independent.)",
        "Note that afl ~ HG(fj, a, kj)",
        "and af2 ~ HG(f2,a,k2), that is, E (a^) = fjkj and E (af2) = f2k2.",
        "Therefore, E (asN) = ifjkjf2k2, namely,E (a0Pj = kj^.",
        "Sketch for Estimating Associations",
        "The proposed sketch method (solid curve) produces larger counts (as) with less work (k).",
        "With \"naive sampling over postings,\" there is an undesirable quadratic: E ^= j§ (dashed curve), whereas with sketches, E (^) & k. These results were generated by simulation, withf1 = f2 = f = 0.2D, D = 10 and a = 0.22, 0.38, 0.65, 0.80, 0.85f.",
        "There is only one dashed curve across all values of a.",
        "There are different (but indistinguishable) solid curves depending on a.",
        "Of course, the quadratic relation, E ^ atj = f§, is undesirable; !% effort returns only 0.0!% useful information.",
        "Ideally, to maximize the signal, we'd like to see large counts in a small sample, not small counts in a large sample.",
        "The crux is as, which tends to have the smallest counts.",
        "We'd like as to be as large as possible, but we'd also like to do as little work (k) as possible.",
        "The next subsection on sketches proposes an improvement, where j% effort returns roughly j% useful information, as illustrated in Figure 2.",
        "1.3 An Improvement Based on Sketches",
        "A sketch is simply the front of the postings (after a random permutation).",
        "We find it helpful, as an informal practical metaphor, to imagine a virtual machine architecture where sketches (Broder V991), the front of the postings, reside in physical memory, and the rest of the postings are stored on disk.",
        "More formally, the sketch, K = MINk (n(P)), contains the k smallest postings, after applying a random permutation n to document IDs, M = {j, 2,3,..., D}, to eliminate whatever structure there might be.",
        "Given two words, Wj and W2, we have two sets of postings, Pj and P2,and two sketches, K = MINkj(n(Pj)) and K2 = MINk2(n(P2)).",
        "We construct a sample contingency table from the two sketches.",
        "Let Ms = {j, 2,3,..., Ds} be the sample space, where Ds is set to min(max(Kj),max(K2)).",
        "With this choice of Ds, all the document IDs in the sample space, Qs, can be assigned to the appropriate cell in the sample contingency table without looking outside the sketch.",
        "One could use a smaller Ds, but doing so would throw out data points unnecessarily.",
        "The sample contingency table is constructed from K and K2 in O(kj + k2)time, using a straightforward linear pass over the two sketches:",
        "The final step is an estimation task.",
        "The margin-free (MF) estimator recovers the original contingency table by a simple scaling.",
        "For better accuracy, one could take advantage of the margins by using a maximum likelihood estimator (MLE).",
        "With \"sampling over documents,\" it is convenient to express the sampling rate in terms of Ds and D, whereas with sketches, it is convenient to express the sampling rate in terms of k and f. The following two approximations allow us to flip back and forth between the two views:",
        "In other words, using sketches with size k, the corresponding sample size Ds in \"sampling over documents\" would be Ds & where D represents the data sparsity.",
        "Because the estimation errors (variances) are inversely proportional to sample size, we know the proposed algorithm improves \"sampling over documents\" by a factor proportional to the data sparsity.",
        "1.4 Improving Estimates Using Margins",
        "When we know the margins, we ought to use them.",
        "The basic idea is to maximize the likelihood of the sample contingency table under margin constraints.",
        "In the pairwise case, we will show that the resultant maximum likelihood estimator is the solution to a cubic equation, which has a remarkably accurate quadratic approximation.",
        "The use of margins for estimating contingency tables was suggested in the 1940s (Deming and Stephan 1940; Stephan 1942) for a census application.",
        "They developed a straightforward iterative estimation method called iterative proportional scaling, which was an approximation to the maximum likelihood estimator.",
        "Computing margins is usually much easier than computing interactions.",
        "For a data matrix A of n rows and D columns, computing all marginal l2 norms costs only O(nD), whereas computing all pairwise associations (or l2 distances) costs O(nD).",
        "One could compute the margins in a separate prepass over the data, without increasing the time and space complexity, though we suggest computing the margins while applying the random permutation n to all the document IDs on all the postings.",
        "Let's start with conventional random sampling over documents, using a running example in Figure 3.",
        "We choose a sample of Ds = 18 documents randomly out of a collection of D = 36.",
        "After applying the random permutation, document IDs will be uniformly random.",
        "Thus, we can construct the random sample by picking any Ds documents.",
        "For convenience, we pick the first Ds.",
        "The sample contingency table is then constructed, as illustrated in Figure 3.",
        "The recommended procedure is illustrated in Figure 4.",
        "The two sketches, K1 and K2, are highlighted in the large box.",
        "We find it convenient, as an informal practical metaphor, to think of the large box as physical memory.",
        "Thus, the sketches reside in physical memory, and the rest are paged out to disk.",
        "We choose Ds to be min(max(K1), max(K2)) = min(18,21) = 18, so that we can compute the sample contin-",
        "Sketch for Estimating Associations",
        "Figure 3",
        "In this example, the corpus contains D = 36 documents.",
        "The population is: 0 = {1,2,..., D}.",
        "The sample space is 0s = {1,2,..., Ds },where Ds = 18.",
        "Circles denote documents containing W1, and squares denote documents containing W2.",
        "The sample contingency table is: as = |{4,15}| = 2, bs = |{3,7,9,10,18}| = 5, cs = |{2,5,8}| = 3, ds = 6,11,12,13,14,16,17}| = 8.",
        "Figure 4",
        "This procedure, which we recommend, produces the same sample contingency table as in Figure 3: as = 2, bs = 5, cs = 3, and ds = 8.",
        "The two sketches, K and K2 (larger shaded box), reside in physical memory, and the rest of the postings are paged out to disk.",
        "K contains of the first k1 = 7 document IDs in P1 and K2 contains of the first k2 = 7IDs in P2.We assume P1 and P2 are already permuted, otherwise we should write n(P:)and n(P2) instead.",
        "Ds = min(max(K!",
        "),max(K2))= min(18,21) = 18.",
        "The sample contingency table is computed from the sketches (large box) in time k1 + k2, but documents exceeding Ds are excluded from Qs(small box), because we can't tell if they are in the intersection or not, without looking outside the sketch.",
        "As it turns out, 19 is in the intersection and 21 is not.",
        "gency table for Qs = {1,2,3,..., Ds} in physical memory in time O (k1 + k2) from K1 and K2.",
        "In this example, documents 19 and 21 (highlighted in the smaller box) are excluded from Qs.",
        "It turns out that 19 is part of the intersection, and 21 is not, but we would have to look outside the sketches (and suffer a page fault) to determine that.",
        "The resulting sample contingency table is the same as in Figure 3:",
        "1.6 A Five-Word Example",
        "Figure 5 shows an example with more than two words.",
        "There are D = 15 documents in the collection.",
        "We generate a random permutation n as shown in Figure 5(b).",
        "For every ID in postings Pz in Figure 5(a), we apply the random permutation n, but we only store the k smallest IDs as a sketch K,thatis,K = MIN^ (n(Pz)).",
        "In this example, we choose k1 = 4, k2 = 4, k3 = 4, k4 = 3, k5 = 6.",
        "The sketches are stored in Figure 5(c).",
        "In addition, because 7t(Pz) operates on every ID in Pz, we know the total number of non-zeros in Pz, denoted by f = |Pf |.",
        "The estimation procedure is straightforward if we ignore the margins.",
        "For example, suppose we need to estimate the number of documents containing the first two words.",
        "In other words, we need to estimate the inner product between P1 and P2, denoted by a(1,2).",
        "(We have to use the additional subscript (1,2) because we have more than (a) Postings",
        "3",
        "4 7 9",
        "10",
        "15",
        "18",
        "2",
        "4 5 8",
        "15",
        "19",
        "21",
        "(b) Permutation -",
        "(c) Sketches",
        "Figure 5",
        "The original postings sets are given in (a).",
        "There are D = 15 documents in the collection.",
        "We generate a random permutation n as shownin(b).Weapply n to the postings Pz and store the sketch K = MINk.",
        "(n(Pf)).",
        "For example, n(P1) = {11,13,1,12,15,6,8}.",
        "We choose k1 = 4;and hence the four smallest IDs in n(P1)are K1 = {1,6,8,11}.",
        "We choose k2 = 4, k3 = 4, k4 = 3, and k5 = 6.",
        "just two words in the vocabulary.)",
        "We calculate, from sketches K1 and K2, the sample inner product as (12) = |{6}| = 1, and the corresponding corpus sample size, denoted by Ds (12) = min(max(K1),max(K2)) = min(11,12) = 11.",
        "Therefore, the \"margin-free\" estimate of a(12) is simply as (12) ^2) = = 1.4.",
        "This estimate can be compared to the \"truth,\" which is obtained from the complete postings list, as opposed to the sketch.",
        "In this case, P1 and P2 have 4 documents in common.",
        "And therefore, the estimation error is 4 - 1.4 or 2.6 documents.",
        "Similarly, for P1 and P5, Ds (15) = min(11,6) = 6, as (15) = 2.",
        "Hence, the \"margin-free\" estimate of a(15) is simply 2 = 5.0.",
        "In this case, the estimate matches the \"truth\" perfectly.",
        "The procedure can be easily extended to more than two rows.",
        "Suppose we would like to estimate the three-way inner product (three-way joins) among P1, P4,andP5, denoted by a(1,4,5).",
        "We calculate the three-way sample inner product from K1,K4,and K5, as(145) = |{6}| = 1, and the corpus sample size Ds(145) = min(max(K1),max(K4),max(K5)) = min(11,12,6) = 6.",
        "Then the \"margin-free\" estimate of a(1,4,5) is 1 = 2.5.",
        "Of course, we can improve these estimates by taking advantage of the margins.",
        "2.",
        "Applications",
        "There is a large literature on sketching techniques (e.g., Alon, Matias, and Szegedy 1996; Broder 1997; Vempala 2004).",
        "Such techniques have applications in information retrieval, databases, and data mining (Broder et al.",
        "1997; Haveliwala, Gionis, and Indyk 2000;",
        "Haveliwala et al.",
        "2002).",
        "Broder's sketches (Broder 1997) were originally introduced to detect duplicate documents in Web crawls.",
        "Many URLs point to the same (or nearly the same) HTML blobs.",
        "Approximate answers are often good enough.",
        "We don't need to find all such pairs, but it is handy to find many of them, without spending more than it is worth on computational resources.",
        "In IR applications, physical memory is often a bottleneck, because the Web collection is too large for memory, but we want to minimize seeking data in the disk as the query response time is critical (Brin and Page 1998).",
        "As a space saving device, dimension reduction techniques use a compact representation to produce approximate answers in physical memory.",
        "11",
        "9",
        "^5",
        "3",
        "10",
        "^14",
        "9",
        "11",
        "^15",
        "13",
        "12",
        "1",
        "13",
        "-+6",
        "7",
        "14",
        "12",
        "15",
        "^8",
        "10",
        "Ki",
        "1",
        "6",
        "8",
        "11",
        "K2",
        "3",
        "6",
        "10",
        "12",
        "K3",
        "2",
        "5",
        "7",
        "9",
        "K4",
        "3",
        "6",
        "12",
        "K5",
        "1",
        "2",
        "3",
        "4 5 6",
        "Sketch for Estimating Associations",
        "Section 1 mentioned page hit estimation.",
        "If we have a two-word query, we'd like to know how many pages mention both words.",
        "We assume that pre-computing and storing page hits is infeasible, at least not for infrequent pairs of words (and multi-word sequences).",
        "It is customary in information retrieval to start with a large boolean term-by-document matrix.",
        "The boolean values indicate the presence or absence of a term in a document.",
        "We assume that these matrices are too large to store in physical memory.",
        "Depending on the specific applications, we can construct an inverted index and store sketches either for terms (to estimate word association) or for documents (to estimate document similarity).",
        "2.1 Association Rule Mining \"Market-basket\" analysis and association rules (Agrawal, Imielinski, and Swami 1993; Agrawal and Srikant 1994; Agrawal et al.",
        "1996; Hastie, Tibshirani, and Friedman 2001, Chapter 14.2) are useful tools for mining commercial databases.",
        "Commercial databases tend to be large and sparse (Aggarwal and Wolf 1999; Strehl and Ghosh 2000).",
        "Various sampling algorithms have been proposed (Toivonen 1996; Chen, Haas, and Scheuermann 2002).",
        "The proposed algorithm scales better than traditional random sampling (i.e., a fixed sample of columns of the data matrix) for reasons mentioned earlier.",
        "In addition, the proposed algorithm makes it possible to estimate association rules on-line, which may have some advantage in certain applications (Hidber 1999).",
        "2.2 All Pair-Wise Associations (Distances)",
        "In many applications, including distance-based classification or clustering and bi-gram language modeling (Church and Hanks 1991), we need to compute all pairwise associations (or distances).",
        "Given a data matrix A of n rows and D columns, brute force computation of AAT would cost O(nD), or more efficiently, O(nf), where f is the average number of non-zeros among all rows of A. Brute force could be very time-consuming.",
        "In addition, when the data matrix is too large to fit in the physical memory, the computation may become especially inefficient.",
        "Using our proposed algorithm, the cost of computing AAT can be reduced to O(nf) + O(nk), where k is the average sketch size.",
        "It costs O(nf) for constructing sketches and O(nk) for computing all pairwise associations.",
        "The savings would be significant when k << f .Notethat AAT is called \"Gram Matrix\" in machine learning; and various algorithms have been proposed for speeding up the computation (e.g., Drineas and Mahoney 2005).",
        "Ravichandran, Pantel, and Hovy (2005) computed pairwise word associations (boolean data) among n « 0.6 million nouns in D « 70 million Web pages, using random projections.",
        "We have discovered that in boolean data, our method exhibits (much) smaller errors (variances); but we will present the detail in other papers (Li, Church, and Hastie 2006, 2007).",
        "For applications which are mostly interested in finding the strongly associated pairs, the n might appear to be a show stopper.",
        "But actually, in a practical application, we implemented an inverted index on top of the sketches, which made it possible to find many of the most interesting associations quickly.",
        "2.3 Database Query Optimization",
        "In databases, an important task is to determine the order of joins, which has a large impact on the system performance (Garcia-Molina, Ullman, and Widom 2002, Chapter 16).",
        "Based on the estimates of two-way, three-way, and even higher-order join sizes, query optimizers construct a plan to minimize a cost function (e.g., intermediate writes).",
        "Efficiency is critical as we certainly do not want to spend more time optimizing the plan than executing it.",
        "We use an example (called Governator) to illustrate that estimates of two-way and multi-way association can help the query optimizer.",
        "Table 3 shows estimates of hits for four words and their two-way, three-way, and four-way combinations.",
        "Suppose the optimizer wants to construct a plan for the query: \"Governor, Schwarzenegger, Terminator, Austria.\"",
        "The standard solution starts with the least frequent terms: ((\"Schwarzenegger\" n \"Terminator\") n \"Governor\") n \"Austria.\"",
        "That plan generates 579,100 intermediate writes after the first and second joins.",
        "An improvement would be ((\"Schwarzenegger\" n \"Austria\") n \"Terminator\") n \"Governor,\" reducing the 579,100 down to 136,000.",
        "3.",
        "Outline of Two-Way Association Results",
        "To approximate the associations between words W1 and W2, we work with sketches K1 and K2.",
        "We first determine Ds = min(max(K1),max(K2)) and then construct the sample contingency table on Qs = {1,2,..., Ds}.",
        "The contingency table for the entire document collection, fi = {1,2,..., D}, is estimated using a maximum likelihood estimator (MLE):",
        "Instead of solving a cubic equation, we recommend a convenient and accurate quadratic approximation:",
        "We will compare the proposed MLE to two baselines: the independence baseline, aind, and the margin-free baseline, aMF:",
        "The margin-free baseline has smaller errors than the independence baseline, but we can do even better if we know the margins, as is common in practice.",
        "As expected, computational work and statistical accuracy (variance or errors) depend on sampling rate.",
        "The larger the sample, the better the estimate, but the more work we have to do.",
        "Sketch for Estimating Associations",
        "These results are demonstrated both empirically and theoretically.",
        "In our field, it is customary to end with a large empirical evaluation.",
        "But there are always lingering questions.",
        "Do the results generalize to other collections with more documents or different documents?",
        "This paper attempts to put such questions to rest by deriving closed-form expressions for the variances.",
        "Var (âmle )",
        "Var (Ûmf )",
        "These formulas establish the superiority of the proposed method over the alternatives, not just for a particular data set, but more generally.",
        "These formulas will also be used to determine stopping rules.",
        "How many samples do we need?",
        "We will use such an argument to suggest that a sampling rate of 10~ may be sufficient for certain Web applications.",
        "The proposed method generalizes naturally to multi-way associations, as presented in Section 6.",
        "Section 7 describes Broder's sketches, which were designed for estimating resemblance, a particular association statistic.",
        "It will be shown, both theoretically and empirically, that our proposed method reduces the mean square error (MSE) by about 50%.",
        "In other words, the proposed method achieves the same accuracy with about half the sample size (work).",
        "4.",
        "Evaluation of Two-Way Associations",
        "We evaluated our two-way association sampling/estimation algorithm with a chunk of Web crawls (D = 2) produced by the crawler for MSN.com.",
        "We collected two sets of English words which we will refer to as the small data set and the large data set.",
        "The small data set contains just four high frequency words: THIS, HAVE, HELP and PROGRAM (see Table 4), whereas the large data set contains 968 words (i.e., 468,028 pairs).",
        "The large data set was constructed by taking a random sample of English words that appeared in at least 20 documents in the collection.",
        "The histograms of the margins and co-occurrences have long tails, as expected (see Figure 6).",
        "For the small data set, we applied 10 independent random permutations to the D = 2 document IDs, Q = {1,2,D}.",
        "High-frequency words were selected so we could study a large range of sampling rates (|), from 0.002 to 0.95.",
        "A pair of sketches was constructed for each of the 6 pairs of words in Table 4, each of the 10 permutations and each sampling rate.",
        "The sketches were then used to compute a sample contingency table, leading to an estimate of co-occurrence, a.",
        "An error was computed by comparing this estimate, a, to the appropriate gold standard value for a in Table 4.",
        "Mean square errors (MSE = E(a – a)) and other statistics were computed by aggregating over the 10",
        "Small dataset: co-occurrences and margins for the population.",
        "The task is to estimate these values, which will be referred to as the gold standard, from a sample.",
        "Monte Carlo trials.",
        "In this way, the small data set experiment made it possible to verify our theoretical results, including the approximations in the variance formulas.",
        "The larger experiment contains many words with a large range of frequencies; and hence the experiment was repeated just six times (i.e., six different permutations).",
        "With such a large range of frequencies and sampling rates, there is a danger that some samples would be too small, especially for very rare words and very low sampling rates.",
        "A floor was imposed to make sure that every sample contains at least 20 documents.",
        "4.1 Results from Large Monte Carlo Experiment",
        "Figure 7 shows that the proposed methods (solid lines) are better than the baselines (dashed lines), in terms of MSE, estimated by the large Monte Carlo experiment over the small data set, as described herein.",
        "Note that errors generally decrease with sampling rate, as one would expect, at least for the methods that take advantage of the sample.",
        "The independence baseline (aIND), which does not take advantage of the sample, has very large errors.",
        "The sample is a very useful source of information; even a small sample is much better than no sample.",
        "The recommended quadratic approximation, aMLEA, is remarkably close to the exact MLE solution.",
        "Both of the proposed methods, aMLEa and aMLE (solid lines), have",
        "Figure 6",
        "Large data set: histograms of document frequencies, df (left), and co-occurrences, a (right).",
        "Left: max document frequency df = 42,564, median = 1135, mean = 2135, standard deviation = 3628.",
        "Right: max co-occurrence a = 33,045, mean = 188, median = 74, standard deviation = 459.",
        "Case #",
        "Words",
        "Co-occurrence (a)",
        "Margin f",
        "Margin (f>)",
        "Case 2-1",
        "THIS, HAVE",
        "13,517",
        "27,633",
        "17,369",
        "Case 2-2",
        "THIS, HELP",
        "7,221",
        "27,633",
        "10,791",
        "Case 2-3",
        "THIS, PROGRAM",
        "3,682",
        "27,633",
        "5,327",
        "Case 2-4",
        "HAVE, HELP",
        "5,781",
        "17,369",
        "10,791",
        "Case 2-5",
        "HAVE, PROGRAM",
        "3,029",
        "17,369",
        "5,327",
        "Case 2-6",
        "HELP, PROGRAM",
        "1,949",
        "17,369",
        "5,327",
        "Sketch for Estimating Associations",
        "much smaller MSE than the margin-free baseline aMF (dashed lines), especially at low sampling rates.",
        "When we know the margins, we ought to use them.",
        "4.1.1 Margin Constraints Improve Smoothing.",
        "Though not a major emphasis of this paper, Figure 8 shows that smoothing is effective at low sampling rates, but only for those methods that take advantage of the margin constraints (solid lines as opposed to dashed lines).",
        "Figure 8 compares smoothed estimates (aMLE, aMLEa,and aMF) with their un-smoothed counterparts.",
        "The y-axis reports percentage improvement of the MSE due to smoothing.",
        "Smoothing helps the proposed methods (solid lines) for all six word pairs, and hurts the baseline methods (dashed lines), for most of the six word pairs.",
        "We believe margin constraints keep the smoother from wandering too far astray; without margin constraints, smoothing can easily do more harm than good, especially when the smoother isn't very good.",
        "In this experiment, we used the simple \"add-one\" smoother that replaces as, bs, cs ,and ds with as + 1, bs + 1, cs + 1, and ds + 1, respectively.",
        "We could have used a more sophisticated smoother (e.g., Good-Turing), but if we had done so, it would have been harder to see how the margin constraints keep the smoother from wandering too far astray.",
        "4.1.2 Monte Carlo Verification of Variance Formula.",
        "How accurate is the approximation of the variance in Equations (9) and (11)?",
        "Figure 9 shows that the Monte Carlo simulation is remarkably close to the theoretical formula (9).",
        "Formula (11) is the same as (9), except that e(D*-) is replaced with the approximation",
        "Figure 7",
        "The proposed estimator, aMLE, outperforms the margin-free baseline, aMF,interms of .",
        "The quadratic approximation, aMLEa,isclose to aMLE.",
        "All methods are better than assuming independence (IND).",
        "Figure 8",
        "Smoothing improves the proposed MLE estimators but hurts the margin-free estimator in most cases.",
        "The vertical axis is the percentage of relative improvement in VMSEofeachsmoothed estimator with respect to its un-smoothed version.",
        "Sampling rates Sampling rates",
        "Figure 9",
        "Normalized standard error, ^0^, for the MLE.",
        "The theoretical variance formula (9) fits the simulation results so well that the curves are indistinguishable.",
        "Also, smoothing is effective in reducing variance, especially at low sampling rates.",
        "max ^ktx, %).",
        "Theoretically, we expect max ^k[, HJ – EyD!sJ • Figure 10 verifies the inequality, and shows that the inequality is not too far from an equality.",
        "We will use (11) instead of (9), because the differences are not too large, and (11) is more convenient.",
        "4.1.3 Monte Carlo Estimate of Bias.",
        "Finally, we also compare the biases in Figure 11 for Case 2-5 and Case 2-6.",
        "The figure shows that the MLE estimator is essentially unbiased.",
        "Sketch for Estimating Associations",
        "Sampling rates",
        "For all 6 cases, the ratios max ^j^, /E (D^) are close to 1, and the differences roughly monotonically decrease with increasing sampling rates.",
        "When the sampling rates > 0.005 (roughly the sketch sizes > 20), max f |l, j^ is an accurate approximation of E (D^).",
        "Biases in terms of \\E(aa~a\\.",
        "aMLE is practically unbiased.",
        "Smoothing increases bias slightly.",
        "4.2 Results from Large Data Set Experiment",
        "In Figure 12, the large data set experiment confirms the findings of the large Monte Carlo experiment: The proposed MLE method is better than the margin-free and independence baselines.",
        "The recommended quadratic approximation, aMLEa, is close to the exact solution, aMLE.",
        "4.3 Rank Retrieval by Cosine",
        "We are often interested in finding top ranking pairs according to some measure of similarity such as cosine.",
        "Performance improves with sampling rate for this task (as well as almost any other task; there is no data like more data), but nevertheless, Figure 13 shows that we can find many of the top ranking pairs, even at low sampling rates.",
        "Note that the estimate of cosine, - – =, depends solely on the estimate of a, because we know the margins, f1 and f2.",
        "If we sort word pairs by their cosines, using estimates of a based on a small sample, the rankings will hopefully be close to what we would",
        "(a) The proposed MLE methods (solid lines) have smaller errors than the baselines (dashed lines).",
        "We report the mean absolute errors (normalized by the mean co-occurrences, 188).",
        "All curves are averaged over six permutations.",
        "The two solid lines, the proposed MLE and the recommended quadratic approximation, are close to one another.",
        "Both are well below the margin-free (MF) baseline and the independence (IND) baseline.",
        "(b) Percentage of improvement due to smoothing.",
        "Smoothing helps MLE, but hurts MF.",
        "We can find many of the most obvious associations with very little work.",
        "Two sets of cosine scores were computed for the 468,028 pairs in the large dataset experiment.",
        "The gold standard scores were computed over the entire dataset, whereas sample scores were computed over a sample of the data set.",
        "The plots show the percentage of agreement between these two lists, as a function of S. As expected, agreement rates are high 100%) at high sampling rates (0.5).",
        "But it is reassuring that agreement rates remain pretty high 70%) even when we crank the sampling rate way down (0.003).",
        "obtain if we used the entire data set.",
        "This section will compare the rankings based on a small sample to a gold standard, the rankings based on the entire data set.",
        "How should we evaluate rankings?",
        "We follow the suggestion in Ravichandran, Pantel, and Hovy (2005) of reporting the percentage of agreements in the top-S. That is, we compare the top-S pairs based on a sample with the top-S pairs based on the entire data set.",
        "We report the intersection of the two lists, normalized by S. Figure 13(a) emphasizes high precision region (3 – S – 200), whereas Figure 13(b) emphasizes higher recall, extending S to cover all 468,028 pairs in the large dataset experiment.",
        "Of course, agreement rates are high at high sampling rates.",
        "For example, we have nearly « 100% agreement at a sampling rate of 0.5.",
        "It is reassuring that agreement rates remain fairly high 70%), even when we push the sampling rate way down",
        "Sketch for Estimating Associations",
        "(0.003).",
        "In other words, we can find many of the most obvious associations with very little work.",
        "The same comparisons can be evaluated in terms of precision and recall, by fixing the top-LG gold standard list but varying the length of the sample list LS.More precisely, recall = relevant/LG, and precision = relevant/LS, where \"relevant\" means the retrieved pairs in the gold standard list.",
        "Figure 14 gives a graphical representation of this evaluation scheme, using notation in Manning and Schütze (1999), Chapter 8.1.",
        "Figure 15 presents the precision-recall curves for LG = 1%L and 10%L, where L = 468,028.",
        "For each LG, there is one precision-recall curve corresponding to each sampling rate.",
        "All curves indicate the precision-recall trade-off and that the only way to improve both precision and recall simultaneously is to increase the sampling rate.",
        "To summarize the main results of the large and small data set experiments, we found that the proposed MLE (and the recommended quadratic approximation) have smaller",
        "Gold Standard Word-pair List",
        "Reconstructed Word-pair List",
        "Similarity scores",
        "Definitions of recall and precision.",
        "L = total numberofpairs.",
        "LG = number of pairs from the top of the gold standard similarity list.",
        "LS = number of pairs from the top of the reconstructed similarity list.",
        "Relevant",
        "Irrelevant",
        "Retrieved",
        "TP(a)",
        "FP(b)",
        "Not Retrieved",
        "FN(c)",
        "TN (d)",
        "Precision",
        "TP",
        "_ a",
        "TP + FP a + b",
        "Recall",
        "TP",
        "_ a",
        "TP + FN a + c",
        "Sampling rates",
        "errors than the two baselines (the MF baseline and the independence (IND) baseline).",
        "Margin constraints improve smoothing, because the margin constraints keep the smoother from wandering too far astray.",
        "Monte Carlo simulations verified the variance formulas (9) and (11), and showed that the proposed MLE method is essentially unbiased.",
        "The ranking experiment showed that we can find many of the most obvious associations with very little work.",
        "5.",
        "The Maximum Likelihood Estimator (MLE)",
        "Section 4 evaluated the proposed method empirically; this section will explore the statistical theory behind the method.",
        "The task is to estimate the contingency table (a, b, c, d) from the sample contingency table (as, bs, cs, ds), the margins, and D.",
        "We can factor the (full) likelihood (probability mass function, PMF) Pr(as, bs, cs, ds; a)",
        "Pr(as, bs, Cs, ds; a) = Prfe, bs, Cs, ds Ds; a) x Pr(Ds; a) (12) We seek the a that maximizes the partial likelihood Pr(as, bs, cs,ds \\Ds;a), that is,",
        "Pr(as, bs,cs,ds\\Ds;a) is just the PMF of a two-way sample contingency table.",
        "That is relatively straightforward, but Pr(Ds; a) is difficult.",
        "As illustrated in Figure 16, there is no strong dependency of Ds on a, and therefore, we can focus on the easy part.",
        "Before we delve into maximizing Pr(as, bs, cs,ds\\Ds;a) under margin constraints, we will first consider two simplifications, which lead to two baseline estimators.",
        "The independence baseline does not use any samples, whereas the margin-free baseline does not take advantage of the margins.",
        "This experiment shows that E(Ds) is not sensitive to a.",
        "D = 2 x 10,/1 = D/20,/2 = f1 /2.",
        "The different curves correspond to a = 0, 0.05, 0.2, 0.5, and 0.9f2.",
        "These curves are almost indistinguishable except at very low sampling rates.",
        "Note that, at sampling rate = 10-5, the sample size j2 = 5only.",
        "d = 2x\\0",
        "^ = 0.05x1)",
        "f2 = 0.5xfx",
        "Sketch for Estimating Associations",
        "5.1 The Independence Baseline",
        "Independence assumptions are often made in databases (Garcia-Molina, Ullman, and Widom 2002, Chapter 16.4) and NLP (Manning and Schütze 1999, Chapter 13.3).",
        "When two words W1 and W2 are independent, the size of intersections, a, follows a hypergeo-metric distribution, where (m) = m!(n – m)!.",
        "This distribution suggests an estimator",
        "Note that(14) is also a common null-hypothesis distributionintestingtheindepen-dence of a two-way contingency table, that is, the so-called Fisher's exact test (Agresti 2002, Section 3.5.1).",
        "5.2 The Margin-Free Baseline",
        "Conditional on Ds, the sample contingency table (as, bs, cs, ds) follows the multivariate hypergeometric distribution with moments",
        "where the term DD_D1s « 1 – %■, is known as the \"finite population correction factor.\"",
        "An unbiased estimator and its variance would be",
        "We refer to this estimator as \"margin-free\" because it does not take advantage of the margins.",
        "The multivariate hypergeometric distribution can be simplified to a multinomial assuming \"sample-with-replacement,\" which is often a good approximation when Dis small.",
        "According to the multinomial model, an estimator and its variance would be:",
        "That is, for the margin-free model, the \"sample-with-replacement\" simplification still results in the same estimator but slightly overestimates the variance.",
        "4 http://www.ds.unifi.it/VL/VL_EN/urn/urn4.html.",
        "Note that these expectations in (16) hold both when the margins are known, as well as when they are not known, because the samples (as, bs, cs, ds) are obtained randomly without consulting the margins.",
        "Of course, when we know the margins, we can do better than when we don't.",
        "5.3 The Exact MLE with Margin Constraints",
        "Considering the margin constraints, the partial likelihood Pr (as,bs,cs,ds\\Ds;a) can be expressed as a function of a single unknown parameter, a:",
        "where the multiplicative terms not mentioning a are discarded, because they do not contribute to the MLE.",
        "Let aMLE be the value of a that maximizes the partial likelihood (19), or equivalently, maximizes the log likelihood, log Pr (as, bs, cs, ds\\Ds; a):",
        "whose first derivative, 9log,dslDs;a),is",
        "Because the second derivative, 9logPrfeA,cs,ds1Ds;U), is negative, the log likelihood function is concave, and therefore, there is a unique maximum.",
        "One could solve (20) for 9log^feA^A\\Dsa) = o numerically, but it turns out there is a more direct solution using the updating formula from (19):",
        "Sketch for Estimating Associations",
        "Because we know that the MLE exists and is unique, it suffices to find the a such that g(a) = 1, which is cubic in a (because the fourth term vanishes).",
        "We recommend a straightforward numerical procedure for solving g(a) = 1.",
        "Note that g(a) = 1 is equivalent to q(a) = logg(a) = 0.",
        "The first derivative of ^(a)is",
        "We can solve for q(a) = 0 iteratively using Newton's method: a(new) = a(old) - ^^^m^.See Appendix 1 for a C code implementation.",
        "5.4 The \"Sample-with-Replacement\" Simplification",
        "Under the \"sample-with-replacement\" assumption, the likelihood function is slightly simpler:",
        "« aas (f1 - a)bs (fi - a)Cs (D - fx - f2 + a)ds (i3) Setting the first derivative of the log likelihood to be zero yields a cubic equation:",
        "As shown in Section 5.i, using the margin-free model, the \"sample-with-replacement\" assumption amplifies the variance but does not change the estimation.",
        "With our proposed MLE, the \"sample-with-replacement\" assumption will change the estimation, although in general we do not expect the differences to be large.",
        "Figure 17 gives an (exaggerated) example, to show the concavity of the log likelihood and the difference caused by assuming \"sample-with-replacement.\"",
        "5.5 A Convenient Practical Quadratic Approximation",
        "Solving a cubic equation for the exact MLE may be so inconvenient that one may prefer the less accurate margin-free baseline because of its simplicity.",
        "This section derives a convenient closed-form quadratic approximation to the exact MLE.",
        "The idea is to assume \"sample-with-replacement\" and that one can identify as from",
        "K1 without knowledge of K2.",
        "In other words, we assume a, ~ Binomial (as + bs, f-\\",
        "An example: as = 20, bs = 40, cs = 40, ds = 800, f1 = f2 = 100, D = 1000.",
        "The estimated a = 43for \"sample-with-replacement,\" and a = 51 for \"sample-without-replacement.\"",
        "(a) The likelihood profile, normalized to have a maximum = 1.",
        "(b) The log likelihood profile, normalized to have a maximum = 0.",
        "af ~ Binomial (as + cs, f-J,and a(1) and a(2) are independent with a(1) = a(2) = as.",
        "The PMF of (aS^,af^ is a product of two binomials:",
        "(as + 0 00 (^TT) _ * [(asf cJ (f) ) .",
        "a f1 - a f2 - a which is quadratic in a and has a convenient closed-form solution:",
        "The second root can be ignored because it is always out of range:",
        "The evaluation in Section 4 showed that aMLEa is close to aMLE.",
        "Setting the first derivative of the logarithm of (25) to be zero, we obtain",
        "Sketch for Estimating Associations",
        "5.6 The Conditional Variance and Bias",
        "Usually, a maximum likelihood estimator is nearly unbiased.",
        "Furthermore, assuming \"sample-with-replacement,\" we can apply the large sample theory (Lehmann and Casella 1998, Theorem 6.3.10), which says that aMLE is asymptotically unbiased and converges in distribution to a Normal with mean a and variance j^Ly, where I(a), the expected Fisher Information, is where we evaluate E(as\\Ds), E(bs\\Ds), E(cs\\Ds), E(ds\\Ds) by (16).",
        "For \"sampling-without-replacement,\" we correct the asymptotic variance by multiplying by the finite population correction factor 1",
        "Comparing (17) with (29), we know that Var (aMLE\\Ds) < Var (aMF\\Ds),and thedif-ference could be substantial.",
        "In other words, when we know the margins, we ought to use them.",
        "5.7 The Unconditional Variance and Bias",
        "Errors are a combination of variance and bias.",
        "Fortunately, we don't need to be concerned about bias, at least asymptotically:",
        "The unconditional variance can be computed using the conditional variance formula:",
        "5 See Rosen (i97ia, i97ib) for the rigorous regularity conditions that ensure convergence in the case of \"sample-without-replacement.\"",
        "because E (aMLE\\D^) – a, which is a constant.",
        "Hence Var (E (aMLE\\Ds)) – 0.",
        "To evaluate E (%) exactly, we need PMF Pr(Ds; a), which is unavailable.",
        "Even if it were available, E (probably wouldn't have a convenient closed-form.",
        "Here we recommend the approximations, (3) and (4), mentioned previously.",
        "To derive these approximations, recall that Ds = min (max(Kj),max(K2)).",
        "Using the discrete order statistics distribution (David 1981, Exercise 2.1.4), we obtain:",
        "The min function can be considered to be concave.",
        "By Jensen's inequality (see Cover and Thomas 1991, Theorem 2.6.2), we know that",
        "The reciprocal function is convex.",
        "Again by Jensen's inequality, we have",
        "By replacing the inequalities with equalities, we obtain (35) and (36):",
        "In our experiments, when the sample size is reasonably large (Ds – 20), the errors in (35) and (36) are usually within 5%.",
        "Approximations (35) and (36) provide an intuitive relationship between two views of the sampling rate: (a) %, which depends on corpus size and (b) |, which depends on the size of the postings.",
        "The difference between these two views is important when the term-by-document matrix is sparse, which is often the case in practice.",
        "Using (36), we obtain the following approximation for the unconditional variance:",
        "6Also, see http://uuu.ds.unifi.it/VL/VLJ_N/urn/urn5.html.",
        "Sketch for Estimating Associations",
        "5.8 The Variance of h$MLE)",
        "We can estimate any function h(a)by h(aMLE).",
        "In practical applications, h could be any measure of association including cosine, resemblance, mutual information, etc.",
        "When h(a) is a nonlinear function of a, h(aMLE) will be biased.",
        "One can remove the bias to some extent using Taylor expansions.",
        "See some examples in Li and Church (2005).",
        "Bias correction is important for small samples and highly nonlinear h's (e.g., the log likelihood ratio, LLR).",
        "The bias of h(aMLE) decreases with sample size.",
        "Precisely, the delta method (Agresti 2002, Chapter 3.1.5) says that h(aMLE ) is asymptotically unbiased and the variance of h(aMLE )is provided hh(a) exists and is non-zero.",
        "Non-asymptotically, it is easy to show that 5.9 How Many Samples Are Sufficient?",
        "The answer depends on the trade-off between computational costs (time and space) and estimation errors.",
        "For very infrequent words, we might afford to sample 100%.",
        "In general, a reasonable criterion is the coefficient of variation, cv = SE ^Var(S).",
        "We consider the estimate is accurate if the cv is below some threshold p0 (e.g., p0 = 0.1).",
        "The cv can be expressed as",
        "Figure 18(a) plots the required sampling rate min ^^, ^jcomputed from (41).",
        "The figure shows that at Web scale (i.e., D « 10 billion), a sampling rate as low as 10~ may suffice for \"ordinary\" words (i.e., /i « 10 = 0.001D).",
        "Figure 18(b) plots the required sample size ~k\\, for the same experiment in Figure 18(a), where for simplicity, we assume A = A.",
        "The figure shows that, after D is large enough, the required sample size does not increase as much.",
        "To apply (41) to the real data, Table 5 presents the critical sampling rates and sample sizes for all pairwise combinations of the four-word query Governor, Schwarzenegger, Terminator, Austria.",
        "Here we assume the estimates in Table 3 are exact.",
        "The table verifies that only a very small sample may suffice to achieve a reasonable cv.",
        "5.10 Tail Bound and Multiple Comparisons Effect",
        "To choose the sample size, it is often necessary to consider the effect of multiple comparisons.",
        "For example, when we estimate all pairwise associations among n data points,",
        "(a) An analysis based on cv = ^ =0.1 suggests that we can get away with very low sampling rates.",
        "The three curves plot the critical value for the sampling rate, min ^as a function of",
        "a = /2/20.",
        "(b) The critical sample size k1 (assuming ^ = /O, corresponding to the sampling rates",
        "in (a).",
        "The critical sampling rates and sample sizes (for cv = 0.1) are computed for all two-way combinations among the four words Governor, Schwarzenegger, Terminator, Austria, assuming the estimated document frequencies and two-way associations in Table 3 are exact.",
        "The required sampling rates are all very small, verifying our claim that for \"ordinary\" words, a sampling rate as low as 10-3 may suffice.",
        "In these computations, we used D = 5 x 10 for the number of English documents in the collection.",
        "Query Critical Sampling Rate",
        "Governor, Schwarzenegger Governor, Terminator Governor, Austria Schwarzenegger, Terminator Schwarzenegger, Austria Terminator, Austria",
        "we are estimating n(n2) pairs simultaneously.",
        "A convenient approach is to bound the tail probability where 6 (e.g., 0.05) is the level of significance, e is the specified accuracy (e.g., e< 0.5), and p is the correction factor for multiple comparisons.",
        "The most conservative choice is p = ^J, known as the Bonferroni Correction.",
        "But often it is reasonable to let p be much smaller (e.g., p = 100).",
        "We can gain some insight from (42).",
        "In particular, our previous argument based on coefficient of variations (cv) is closely related to (42).",
        "5.6x",
        "10",
        "5",
        "7.2x",
        "10",
        "4",
        "1.4x",
        "10",
        "4",
        "1.5x",
        "10",
        "4",
        "8.",
        "1 x",
        "10",
        "4",
        "5.5",
        "10",
        "4",
        "Sketch for Estimating Associations",
        "combined with (42), leads to the following criterion on cv",
        "For example, if we let 6 = 0.05, p = 100, and e = 0.4, then (44) will output cv « 0.1.",
        "5.11 Sample Size Selection Based on Storage Constraints",
        "Suppose we can compute the maximum allowed total samples, T, for example, based on the available memory.",
        "That is, n=1 k = T, where n is the total number of words.",
        "We could allocate T according to document frequencies fj,thatis,",
        "Usually, we will need to define a lower bound ki and an upper bound k„, which have to be selected from engineering experience, depending on the specific applications.",
        "We will truncate the computed kj if it is outside [ki, k„].",
        "Equation (45) implies a uniform corpus sampling rate, which may not be always desirable, but the confinement by [ki, k„] can effectively vary the sampling rates.",
        "More carefully, we can minimize the total number of \"unused\" samples.",
        "For a pair,",
        "Wi and Wj,if ^ > ft, then on average, there are – jr^jfi samples unused in Ki.This is the basic idea behind the following linear program for choosing the \"optimal\" sample sizes:",
        "where (z)+ = max(0, z), is the positive part of z.",
        "This program can be modified (possibly no longer a linear program) to consider other factors in different applications.",
        "For example, some applications may care more about the very rare words, so we would weight the rare words more.",
        "5.12 When Will Sketches Not Perform Well?",
        "We consider three scenarios.",
        "(A)fi andf2 are both large; (B)fi andf2 are both small; (C) f1 is very large but f2 is very small.",
        "Conventional sampling over documents can handle situation (A), but will perform poorly on (B) because there is a good chance that the sample will miss the rare words.",
        "The sketch algorithm can handle both (A) and (B) well.",
        "In fact, it will do very well when both words are rare because the equivalent sampling rate D « min (^, ^ can be high, even 100%.",
        "When f2 <^ f1, no sampling method can work well unless we are willing to sample P1 with a sufficiently large sample.",
        "Otherwise even if we let ^ = 100%, the corpus sampling rate, D « will be low.",
        "For example, Google estimates 14,000,000 hits for Holmes, 37,500 hits for Diaconis, and 892 joint hits.",
        "Assuming D = 5 x 10 and cv = 0.1, the critical sample size for Holmes would have to be 1.4 x 10, probably too large as a sample.",
        "6.",
        "Extension to Multi-Way Associations",
        "Many applications involve multi-way associations, for example, association rules, databases, and Web search.",
        "The \"Governator\" example in Table 3, for example, made use of both two-way and three-way associations.",
        "Fortunately, our sketch construction and estimation algorithm can be naturally extended to multi-way associations.",
        "We have already presented an example of estimating multi-way associations in Section 1.6.",
        "When we do not consider the margins, the estimation task is as simple as in the pairwise case.",
        "When we do take advantage of margins, estimating multi-way associations amounts to a convex program.",
        "We will also analyze the theoretical variances.",
        "6.1 Multi-Way Sketches",
        "Suppose we are interested in the associations among m words, denoted by W1, W2,Wm.",
        "The document frequencies are f1,f2,... ,andfm, which are also the lengths of the postings P1,P2,... ,Pm.",
        "There are N = 2m combinations of associations, denoted by x1, x2, ... , xN.",
        "For example, which can be directly corresponded to the binary representation of integers.",
        "Using the vector and matrix notation, X = [x1,x2,...,xN]T, F = [ f1,f2,...,fm,D]T, where the superscript \"T\" stands for \"transpose\", that is, we always work with column vectors.",
        "We can write down the margin constraints in terms of a linear matrix equation as",
        "7 Readers familiar with random projections can verify that in this case we need k = 6.6 x 10 projections in order to achieve cv = 0.1.",
        "See Li, Hastie, and Church (2006a, 2006b) for the variance formula of random projections.",
        "Sketch for Estimating Associations",
        "where A is the constraint matrix.",
        "If necessary, we can use A(m) to identify A for different m values.",
        "For example, when m = 2or m = 3,",
        "For each word W,, we sample the ki smallest elements from its permuted postings, 7t(P;), to form a sketch, K,.",
        "Recall n is a random permutation on Q = (1,2,..., D}.We compute",
        "After removing the elements in all m K/s that are larger than Ds, we intersect these m trimmed sketches to generate the sample table counts.",
        "The samples are denoted as",
        "S = S2, ..., sn ]t.",
        "Conditional on Ds, the samples S are statistically equivalent to Ds random samples over documents from the corpus.",
        "The corresponding conditional PMF and log PMF would be",
        "The log PMF is concave, as in two-way associations.",
        "A partial likelihood MLE solution, namely, the X that maximizes log Pr(S|Ds; X), will again be adopted, which leads to a convex optimization problem.",
        "But first, we shall discuss two baseline estimators.",
        "6.2 Baseline Independence Estimator",
        "Assuming independence, an estimator of x1 would be which can be easily proved using a conditional expectation argument.",
        "By the property of the hypergeometric distribution, E(|P, n Pj\\) = D. Therefore, 6.3 Baseline Margin-Free Estimator",
        "The conditional PMF Pr(S|Ds; X) is a multivariate hypergeometric distribution, based on which we can derive the margin-free estimator:",
        "We can see that the margin-free estimator remains its simplicity in the multi-way case.",
        "The exact MLE can be formulated as a standard convex optimization problem, where X y S is a compact representation for xi > si,1 < i < N.",
        "This optimization problem can be solved by a variety of standard methods such as Newton's method (Boyd and Vandenberghe 2004, Chapter 10.2).",
        "Note that we can ignore the implicit inequality constraints, X y S, if we start with a feasible initial guess.",
        "It turns out that the formulation in (56) will encounter numerical difficulty due to the inner summation in the objective function Q. Smoothing will bring in more numerical issues.",
        "Recall that in estimating two-way associations we do not have this problem, because we have eliminated the summation in the objective function, using an (integer) updating formula.",
        "In multi-way associations, it seems not easy to reformulate the objective function Q in a similar form.",
        "To avoid the numerical problems, a simple solution is to assume \"sample-with-replacement,\" under which the conditional likelihood and log likelihood become",
        "Our MLE problem can then be reformulated as minimize - Q = - Si logxi",
        "subject to AX = F,and X y S (59) which is again a convex program.",
        "To simplify the notation, we neglect the subscript \"r.\"",
        "Sketch for Estimating Associations",
        "We can compute the gradient (vQ) and Hessian (vQ).",
        "The gradient is a vector of the first derivatives of Q with respect to x^for 1 < i < N,",
        "The Hessian is a matrix whose (i,j)th entry is the partial derivative jy^, thatis,",
        "The Hessian has a very simple diagonal form, implying that Newton's method will be a good algorithm for solving this optimization problem.",
        "We implement, in Appendix 2, the equality constrained Newton's method with feasible start and backtracking line search (Boyd and Vandenberghe 2004, Algorithm 10.1).",
        "A key step is to solve for Newton's step, AXnt:",
        "Because the Hessian vQ is a diagonal matrix, solving for Newton's step in (62) can be sped up substantially (e.g., using the block matrix inverse formula).",
        "6.5 The Covariance Matrix",
        "We apply the large sample theory to estimate the covariance matrix of the MLE.",
        "Recall that we have N = 2m variables and m + 1 constraints.",
        "The effective number of variables would be 2m – (m + 1), which is also the dimension of the covariance matrix.",
        "We seek a partition of A = [A1, A2], such that A2 is invertible.",
        "We may have to switch some columns of A in order to find an invertible A2.",
        "In our construction, the .th column of A2 is the column of A such that last entry of the .th row of A is 1.",
        "An example for m = 3 would be where a13) is the [1 2 3 5] columns of A(3) and a23) is the [4 6 7 8] columns of A(3).",
        "We can see that A2 constructed this way is always invertible because its determinant is always one.",
        "1110",
        "1000",
        "1101",
        "a23) =",
        "0100",
        "1011",
        "0010",
        "1111",
        "1111",
        "The log likelihood function Q, which is separable, can then be expressed as",
        "Q(X) = Q1(X1) + Q2(X2) (65) By the matrix derivative chain rule, the Hessian of Q with respect to X1 would be where we use v1 and v2 to indicate the Hessians are with respect to X1 and X2, respectively.",
        "Conditional on Ds, the Expected Fisher Information of X1 is",
        "By the large sample theory, and also considering the finite population correction factor, we can approximate the (conditional) covariance matrix of X1 to be",
        "For a sanity check, we verify that this approach recovers the same variance formula in the two-way association case.",
        "Recall that, when m = 2, we have",
        "1100",
        "1",
        "100",
        "a(2) =",
        "1010",
        ", a12) =",
        "1",
        ", a22) =",
        "010",
        "1111",
        "1",
        "111",
        "Sketch for Estimating Associations",
        "which leads to the same Fisher Information for the two-way association as we have derived.",
        "6.6 The Unconditional Covariance Matrix",
        "Similar to two-way associations, the unconditional variance of the proposed MLE can be estimated by replacing D in (70) with E (Dl^j, namely,",
        "Again, the approximation (76) will overestimate E y-jfj and (77) will underestimate E (t) hence also underestimating the unconditional variance.",
        "6.7 Empirical Evaluation",
        "We use the same four words as in Table 4 to evaluate the multi-way association algorithm, as merely a sanity check.",
        "There are four different combinations of three-way associations and one four-way association, as listed in Table 6.",
        "We present results for x1 (i.e., a in two-way associations) for all cases.",
        "The evaluations for four three-way cases are presented in Figures 19, 20 and 21.",
        "From these figures, we see that the proposed MLE has lower MSE than the MF.",
        "As in the two-way case, smoothing helps MLE but still hurts MF in most cases.",
        "Also, the experiments verify that our approximate variance formulas are fairly accurate.",
        "Figure 22 presents the evaluation results for the four-way association case, including MSE, smoothing, and variance.",
        "The results are similar to the three-way case.",
        "The same four words as in Table 4 are used for evaluating multi-way associations.",
        "There are in total four three-way combinations and one four-way combination.",
        "Case No.",
        "Words Co-occurrences",
        "We have used the empirical E y j^j to compute the unconditional variance.",
        "Figure 23 plots max ^ j^, ||,/ iys for all cases.",
        "The figure indicates that using max ^km^j to estimate E ^is still fairly accurate when the sample size is reasonable.",
        "Combining the results of two-way associations for the same four words, we can study the trend how the proposed MLE improve the MF baseline.",
        "Figure 24(a) sug-",
        "Case 3-1",
        "THIS, HAVE, HELP",
        "4940",
        "Case 3-2",
        "THIS, HAVE, PROGRAM",
        "2575",
        "Case 3-3",
        "THIS, HELP, PROGRAM",
        "1626",
        "Case 3-4",
        "HAVE, HELP, PROGRAM",
        "1460",
        "Case 4",
        "THIS, HAVE, HELP, PROGRAM",
        "1316",
        "Sketch for Estimating Associations",
        "Sampling rates Sampling rates",
        "The simple \"add-one\" smoothing improves the estimation accuracies for the proposed MLE.",
        "Smoothing, however, in all cases except Case 3-1 hurts the margin-free estimator.",
        "gests that the proposed MLE is a big improvement over the MF baseline for two-way associations, but the improvement becomes less and less noticeable with higher order associations.",
        "This observation is not surprising, because the number of degrees of freedom, 2m – (m + 1), increases exponentially with m. In order words, the margin constraints are most effective for small m, but the effectiveness decreases rapidly with m.",
        "On the other hand, smoothing becomes more and more important as m increases, as shown in Figure 24(b), partly because of the data sparsity in high order associations.",
        "7.",
        "Related Work: Comparison with Broder's Sketches",
        "Broder's sketches (Broder 1997), originally introduced for removing duplicates in the AltaVista index, have been applied to a variety of applications (Broder et al.",
        "1997; Haveliwala, Gionis, and Indyk 2000; Haveliwala et al.",
        "2002).",
        "Broder et al.",
        "(1998, 2000) presented some theoretical aspects of the sketch algorithm.",
        "There has been considerable exciting work following up on this line of research including Indyk (2001), Charikar (2002), and Itoh, Takei, and Tarui (2003).",
        "Broder and his colleagues introduced two algorithms, which we will refer to as the \"original sketch\" and the \"minwise sketch\" for estimating resemblance, R = jP^P^ .",
        "The original sketch uses a single random permutation on Q = {1,2,3,D},and the minwise sketch uses random permutations.",
        "Both algorithms have similar estimation accuracies, as will see.",
        "In terms of ^Xp^, the theoretical variance of MLE fits the empirical values very well.",
        "At low sampling rates, smoothing effectively reduces the variance.",
        "Note that we plug in the empirical E (j^) into (75) to estimate the unconditional variance.",
        "The errors due to this approximation are presented in Figure 23.",
        "Sampling rates Sampling rates Sampling rates",
        "(a) MSE (b) Smoothing (c) Variance",
        "Four-way associations (Case 4).",
        "(a) The proposed MLE has smaller MSE than the margin-free (MF) baseline, which has smaller MSE than the independence baseline.",
        "(b) Smoothing considerably improves the accuracy for MLE and also slightly improves MF.",
        "(c) For the proposed MLE, the theoretical prediction fits the empirical variance very well.",
        "Smoothing considerably reduces variance.",
        "Our proposed sketch algorithm is closer to Broder's original sketch, with a few important differences.",
        "A key difference is that Broder's original sketch throws out half of the sample, whereas we throw out less.",
        "In addition, the sketch sizes are fixed over all words for Broder, whereas we allow different sizes for different words.",
        "Broder's method was designed for a single statistic (resemblance), whereas we generalize the method to",
        "Sketch for Estimating Associations (a) Combining the three-way, four-way, and two-way association results for the four words in the evaluations, the average relative improvements of VMSE suggests that the proposed MLE is consistently better than the MF baseline but the improvement decreases monotonically as the order of associations increases.",
        "(b) Average VMSE improvements due to smoothing imply that smoothing becomes more and more important as the order of association increases.",
        "compute contingency tables (and summaries thereof).",
        "Broder's method was designed for pairwise associations, whereas our method generalizes to multi-way associations.",
        "Finally, Broder's method was designed for boolean data, whereas our method generalizes to reals.",
        "7.1 Broder's Minwise Sketch",
        "Suppose a random permutation n1 is performed on the document IDs.",
        "We denote the smallest IDs in the postings and P2,bymin(7t1(P1)) and min(ni(P2)), respectively.",
        "Obviously,",
        "After k minwise independent permutations, denoted as nj, n2,n^, we can estimate R without bias, as a binomial probability, namely, 7.2 Broder's Original Sketch",
        "A single random permutation n is applied to the document IDs.",
        "Two sketches are constructed: K1 = MINki(n(P1)), K2 = MINk2(n(P2)).",
        "Broder (1997) proposed an unbiased estimator for the resemblance:",
        "Note that intersecting by MINk(K1 u K2) throws out half the samples, which can be undesirable (and unnecessary).",
        "The following explanation for (80) is slightly different from Broder (1997).",
        "We can divide the set P1 U P2 (of size a + b + c = f1 + f2 – a) into two disjoint sets: P1 n P2and P1 U P2 – P1 n P2.",
        "Within the set MINk(K1 U K2) (of size k), the document IDs that belong to P1 n P2 would be MINk(K1 U K2) n K1 n K2, whose size is denoted by asB.This way, we have a hypergeometric sample, that is, we sample k document IDs from P1 U P2randomly without replacement and obtain aB IDs that belong to P1 n P2.",
        "By the property of the hypergeometric distribution, the expectation of aB would be",
        "The variance of R*, according to the hypergeometric distribution, is:",
        "where the term f~--X is the \"finite population correction factor.\"",
        "The minwise sketch can be considered as a \"sample-with-replacement\" variate of the original sketch.",
        "The analysis of minwise sketch is slightly simpler mathematically whereas the original sketch is more efficient.",
        "The original sketch requires only one random permutation and has slightly smaller variance than the minwise sketch, that is, Var (R*^) > Var (RB).",
        "When k is reasonably small, as is common in practice, two sketch algorithms have similar errors.",
        "7.3 Why Our Algorithm Improves Broders's Sketch",
        "Our proposed sketch algorithm starts with Broder's original (one permutation) sketch; but our estimation method differs in two important aspects.",
        "8 Actually, the method required fixing sketch sizes: k1 = k2 = k, a restriction that we find convenient to relax.",
        "Sketch for Estimating Associations",
        "Firstly, Broder's estimator (80) uses k out of 2 x k samples.",
        "In particular, it uses only aB = |MINk(Kj U K2) n Kin K2| intersections, which is always smaller than as = |Kj n K2| available in the samples.",
        "In contrast, our algorithm takes advantage of all useful samples up to Ds = min(max(Kj), max(K2)), particularly all as intersections.",
        "If ^ = f,, that is, if we sample proportionally to the margins:",
        "it is expected that almost all samples will be utilized.",
        "Secondly, Broder's estimator (80) considers a two-cell hypergeometric model (a, b + c) whereas the two-way association is a four-cell model (a, b, c, d), which is used in our proposed estimator.",
        "Simpler data models often result in simpler estimation methods but with larger errors.",
        "Therefore, it is obvious that our proposed method has smaller estimator errors.",
        "Next, we compare our estimator with Broder's sketches in terms of the theoretical variances.",
        "7.4 Comparison of Variances",
        "Broder's method was designed to estimate resemblance.",
        "Thus, this section will compare the proposed method with Broder's sketches in terms of resemblance, R. We can compute R from our estimated association flMLE:",
        "RMLE is slightly biased.",
        "However, because the second derivative R\"(a)",
        "is small (i.e., the nonlinearity is weak), it is unlikely that the bias will be noticeable in practice.",
        "By the delta method as described in Section 5.8, the variance of RMLE is approximately",
        "conservatively ignoring the \"finite population correction factor,\" for convenience.",
        "Define the ratio of the variances to be VB = Va\"(RMLE), then",
        "To help our intuitions, let us consider some reasonable simplifications to VB .Assuming a << min(hi,h2) < max(hi,h2) << D, then approximately k max( h b which indicates that the proposed method is a considerable improvement over Broder's sketches.",
        "In order to achieve the same accuracy, our method requires only half as many samples.",
        "Figure 25 plots the VB in (87) for the whole range of /i, h2,and a, assuming equal samples: ki = k2 = k. We can see that VB < i always holds and VB = i only when hi = h2 = a.",
        "There is also the possibility that VB is close to zero.",
        "Proportional samples further reduce VB, as shown in Figure 26.",
        "are max+12h2).",
        "We can see that for all cases, VB < 1 holds.",
        "VB = lwhenh1 = h2 = a, a trivial case.",
        "When a/h2 is small, VB – holds well.",
        "It is also possible that VB is very close to zero.",
        "Sketch for Estimating Associations",
        "We can show algebraically that VB in (87) is always less than unity unlessh1 = h2 = a.",
        "For convenience, we use the notion a, b, c, d in (87).",
        "Assuming kL = k2 = k and f1 > h2, we obtain",
        "To show VB < 1, it suffices to show which is equivalent to following true statement:",
        "7.5 Empirical Evaluations",
        "We have theoretically shown that our proposed method is a considerable improvement over Broder's sketch.",
        "Next, we would like to evaluate these theoretical results using the same experiment data as in evaluating two-way associations (i.e., Table 4).",
        "Figure 27 compares the MSE.",
        "Here we assume equal samples and later we will show that proportional samples could further improve the results.",
        "The figure shows that our MLE estimator is consistently better than Broder's sketch.",
        "In addition, the approximate MLE aMLEa still gives very close answers to the exact MLE, and the simple \"add-one\" smoothing improves the estimations at low sampling rates, quite substantially.",
        "Figure 28 illustrates the bias.",
        "As expected, estimating resemblance from flMLE introduces a small bias.",
        "This bias will be ignored since it is small compared to the MSE.",
        "Figure 29 verifies that the variance of our estimator is always smaller than Broder's sketch.",
        "Our theoretical variance in (86) underestimates the true variances because the approximation E f D~) = max f j^, k-) underestimates the variance.",
        "In addition, because",
        "When estimating the resemblance, our algorithm gives consistently more accurate answers than Broder's sketch.",
        "In our experiments, Broder's \"minwise\" construction gives almost the same answers as the \"original\" sketch, thus only the \"minwise\" results are presented here.",
        "The approximate MLE again gives very close answers to the exact MLE.",
        "Also, smoothing improves at low sampling rates.",
        "the resemblance R(a) is a convex function of a, the delta method also underestimates the variance.",
        "However, Figure 29 shows that the errors are not very large, and become negligible with reasonably large sample sizes (e.g., 50).",
        "This evidence suggests that the variance formula (86) is reliable.",
        "Our proposed MLE has higher bias than the \"minwise\" estimator because of the non-linearity of resemblance.",
        "However, the bias is very small compared with the MSE.",
        "Sketch for Estimating Associations",
        "Our proposed estimator has consistently smaller variances than Broder's sketch.",
        "The theoretical variance, computed by (86), slightly underestimates the true variance with small samples.",
        "Here we did not plot the theoretical variance for Broder's sketch because it is very close to the empirical curve.",
        "Finally, in Figure 30, we show that with proportional samples, our algorithm further improves the estimates in terms of MSE.",
        "With equal samples, our estimators improve Broder's sketch by 30-50%.",
        "With proportional samples, improvements become 40-80%.",
        "Note that the maximum possible improvement is 100%.",
        "8.",
        "Conclusion",
        "In databases, data mining, and information retrieval, there has been considerable interest in sampling and sketching techniques (Chaudhuri, Motwani, and Narasayya 1998; Indyk and Motwani 1998; Manku, Rajagopalan, and Lindsay 1999; Charikar 2002; Achlioptas 2003; Gilbert et al.",
        "2003; Li, Hastie, and Church 2007; Li 2006), which are useful for numerous applications such as association rules (Brin et al.",
        "1997; Brin, Motwani, and Silverstein 1997), clustering (Guha, Rastogi, and Shim 1998; Broder 1998; Aggarwal et al.",
        "1999; Haveliwala, Gionis, and Indyk 2000; Haveliwala et al.",
        "2002), query optimization (Matias, Vitter, and Wang 1998; Chaudhuri, Motwani, and Narasayya 1999), duplicate detection (Broder 1997; Brin, Davis, and Garcia-Molina 1995), and more.",
        "Sampling methods become more and more important with larger and larger collections.",
        "The proposed method generates random sample contingency tables directly from the sketch, the front of the inverted index.",
        "Because the term-by-document matrix is extremely sparse, it is possible for a relatively small sketch, k, to characterize a large sample of Ds documents.",
        "The front of the inverted index not only tells us about the presence of the word in the first k documents, but it also tells us about the absence of the word in the remaining Ds – k documents.",
        "This observation becomes increasingly important with larger Web collections (with ever increasing sparsity).",
        "Typically, Ds > k.",
        "Compared with Broder's sketch, the relative MSE improvement should be, approximately, mij^++1f2f2^ with equal samples, and 2 with proportional samples.",
        "The two horizontal lines in each figure correspond to these two approximates.",
        "The actual improvements could be lower or higher.",
        "The figure verifies that proportional samples can considerably improve the accuracies.",
        "To estimate the contingency table for the entire population, one can use the \"margin-free\" baseline, which simply multiplies the sample contingency table by the appropriate scaling factor.",
        "However, we recommend taking advantage of the margins (also known as document frequencies).",
        "The maximum likelihood solution under margin constraints is a cubic equation, which has a remarkably accurate quadratic approximation.",
        "The proposed MLE methods were compared empirically and theoretically to the MF baseline, finding large improvements.",
        "When we know the margins, we ought to use them.",
        "Our proposed method differs from Broder's sketches in important aspects.",
        "(1) Our sketch construction allows more flexibility in that the sketch size can be different from one word to the next.",
        "(2) Our estimation is more accurate.",
        "The estimator in Broder's sketches uses one half of the samples whereas our method always uses more.",
        "More samples lead to smaller errors.",
        "(3) Broder's method considers a two-cell model whereas our method works with a more refined (hence more accurate) four-cell contingency table model.",
        "(4) Our method extends naturally to estimating multi-way associations.",
        "(5) Although this paper only considers boolean (0/1) data, our method extends naturally to general real-valued data; see Li, Church, and Hastie (2006, 2007).",
        "Although we have used \"word associations\" for explaining the algorithm, the method is a general sampling technique, with potential applications in Web search, databases, association rules, recommendation systems, nearest neighbors, and machine learning such as clustering."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors thank Trevor Hastie, Chris Meek, David Heckerman, Mark Manasse, David Siegmund, Art Owen, Robert",
        "Tibshirani, Bradley Efron, Andrew Ng, and Tze Leung Lai.",
        "Much of the work was conducted at Microsoft while the first author was an intern during the summers of 2004 and 2005.",
        "Sketch for Estimating Associations",
        "#include <stdio.h> #include <math.h> int est_a_appr(int as,int bs,int cs, int fl, int est_a_mle(int as,int bs, int cs, int ds, int main(void) { // The approximate MLE is the solution to a quadratic equation // Newton's method to solve for the exact MLE",
        "function test_program % A short program for testing the multi-way association algorithm.",
        "% First generate a random gold standard dataset.",
        "Then construct % sketches by sampling a certain portion of the postings.",
        "Associations % are estimated by the exact MLE as well as the margin-free (MF) method.",
        "clear all; % The postings of words 2 to m are randomly generated.",
        "30% are % sampled from the postings of word 1. for i = 2:m",
        "X= compute_intersection(P,D); % Gold standard associations pc = 1; % Pseudo-count(pc), pc=0 for no smoothing, pc=1 for \"add-one\".",
        "% Estimate the associations and covariance matrices [X_MLE, X_MF, Var_c, Var_o] = multi_way_est(K,f,D,pc); % Display the estimations of associations % Matlab code for estimating multi-way associations % f: Document frequencies, a column vector % D: Total number of documents % pc: Pseudo-count for smoothing.",
        "% X_MLE: Maximum likelihood estimator (MLE), a column vector % X_MF : Margin-free (MF) estimator, a column vector % Var_c: Conditional (on Ds) covariance matrix, using the estimated X, % Var_o: Covariance computed using the observed Fisher information",
        "S = compute_intersection(K,Ds); % Intersect the sketches to get samples [X_MLE, X_MF] = newton_est(pc,S,Ds,D,A,f); % Estimate X % Conditional variance % Estimate multi-way associations by solving a convex % optimization problem using the Newton's method.",
        "NEWTON_ERR = 0.001; % Threshold for termination.",
        "MAX_ITER = 50; % Maximum allowed iteration.",
        "N = length(S); m = length(f); F = [f;D]; pc = min(pc,(D-Ds)/N); % Adjust pc, if Ds is close to D.",
        "% Solve a quadratic programming problem to find an initial",
        "Sketch for Estimating Associations",
        "% guess of the MLE that minimizes the 2-norm with respect to % the MF estimation and satisfies the constraints.",
        "X_MF = (S+pc)./(Ds+N*pc)*D; % Margin-free estimations.",
        "pc = pc/2; % Occasionally need reduce pc for a feasible solution.",
        "% Solve a linear system of equations for the Newton's step.",
        "function S = compute_intersection(K,Ds); % Compute the intersections to generate a table with N = 2\"m % cells.",
        "The cells are ordered in terms of the binary representation % of integers from 0 to 2\"m-1, where m is the number of words.",
        "% Generate the margin constraint matrix and compute its decompositions % for analyzing the covariance matrix"
      ]
    }
  ]
}
