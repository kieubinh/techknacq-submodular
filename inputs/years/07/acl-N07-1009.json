{
  "info": {
    "authors": [
      "Yejin Choi",
      "Claire Cardie"
    ],
    "book": "Human Language Technologies 2007: the Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
    "id": "acl-N07-1009",
    "title": "Structured Local Training and Biased Potential Functions for Conditional Random Fields with Application to Coreference Resolution",
    "url": "https://aclweb.org/anthology/N07-1009",
    "year": 2007
  },
  "references": [
    "acl-J96-1002",
    "acl-P02-1014",
    "acl-P05-1045",
    "acl-W02-1001",
    "acl-W06-1640"
  ],
  "sections": [
    {
      "text": [
        "Yejin Choi and Claire Cardie",
        "Conditional Random Fields (CRFs) have shown great success for problems involving structured output variables.",
        "However, for many real-world NLP applications, exact maximum-likelihood training is intractable because computing the global normalization factor even approximately can be extremely hard.",
        "In addition, optimizing likelihood often does not correlate with maximizing task-specific evaluation measures.",
        "In this paper, we present a novel training procedure, structured local training, that maximizes likelihood while exploiting the benefits of global inference during training: hidden variables are used to capture interactions between local inference and global inference.",
        "Furthermore, we introduce biased potential functions that empirically drive CRFs towards performance improvements w.r.t.",
        "the preferred evaluation measure for the learning task.",
        "We report promising experimental results on two coreference data sets using two task-specific evaluation measures."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Undirected graphical models such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) have shown great success for problems involving structured output variables (e.g. Wellner et al.",
        "(2004), Finkel et al.",
        "(2005)).",
        "For many real-world NLP applications, however, the required graph structure can be very complex, and computing the global normalization factor even approximately can be extremely hard.",
        "Previous approaches for training CRFs have either (1) opted for a training method that no longer maximizes the likelihood, (e.g. McCallum and Wellner (2004), Roth and Yih (2005)) or (2) opted for a",
        "'Both McCallum and Wellner (2004) and Roth and Yih (2005) used the voted perceptron algorithm (Collins, 2002) to train intractable CRFs.",
        "simplified graph structure to avoid intractable global normalization (e.g. Roth and Yih (2005), Wellner et al.",
        "(2004)).",
        "Solutions of the first type replace the computation of the global normalization factor J2y p(y\\x) with argmaxy p(y|x) during training, since finding an argmax of a probability distribution is often an easier problem than finding the entire probability distribution.",
        "Training via the voted perceptron algorithm (Collins, 2002) or using a max-margin criterion also correspond to the first option (e.g. McCallum and Wellner (2004), Finley and Joachims (2005)).",
        "But without the global normalization, the maximum-likelihood criterion motivated by the maximum entropy principle (Berger et al., 1996) is no longer a feasible option as an optimization criterion.",
        "The second solution simplifies the graph structure for training, and applies complex global inference only for testing.",
        "In spite of the discrepancy between the training model and the testing model, it has been empirically shown that (1) performing global inference only during testing can improve performance (e.g. Finkel et al.",
        "(2005), Roth and Yih (2005)), and (2) full-blown global training can often perform worse due to insufficient training data (e.g. Punyakanok et al.",
        "(2005)).",
        "Importantly, however, attempts to reduce the discrepancy between the training and test models – by judiciously adding the effect of global inference to the training – have produced substantial performance improvements over locally trained models (e.g. Cohen and Carvalho (2005), Sutton and McCallum (2005a)).",
        "In this paper, we present structured local training, a novel training procedure for maximum-likelihood training of undirected graphical models, such as CRFs.",
        "The procedure maximizes likelihood while exploiting the benefits of global inference during training by capturing the interactions between local inference and global inference via hidden variables.",
        "Furthermore, we introduce biased potential functions that redefine the likelihood for CRFs so that the performance of CRFs trained under the maximum likelihood criterion correlates better empirically with the preferred evaluation measures such as F-score and MUC-score.",
        "We focus on the problem of coreference resolution; however, our approaches are general and can be extended to other NLP applications with structured output.",
        "Our approaches also extend to non-conditional graphical models such as Markov Random Fields.",
        "In experiments on two coreference data sets, structured local training reduces the error rate significantly (3.5%) for one coreference data set and minimally (< 1%) for the other.",
        "Experiments using biased potential functions increase recall uniformly and significantly for both data sets and both task-specific evaluation measures.",
        "Results for the combination of the two techniques are promising, but mixed: pairwise F1 increases by 0.8-5.5% for both data sets; MUC F1 increases by 3.5% for one data set, but slightly hurts performance for the second data set.",
        "In §2, we describe structured local training, and follow with experimental results in §3.",
        "In §4, we describe biased potential functions and follow with experimental results in §5.",
        "We discuss related work in §6."
      ]
    },
    {
      "heading": "2. Structured Local Training 2.1 Definitions",
      "text": [
        "For clarity, we define the following terms that we will use throughout the paper.",
        "• local inference: Inference factored into smaller independent pieces, without considering the structure of the output space.",
        "• global inference: Inference applied on the entire set of output variables, considering the structure of the output space.",
        "• local training: Training that does not invoke global inference at each iteration.",
        "• global training: Training that does invoke global inference at each iteration.",
        "In this section, we present an example of the coref-erence resolution problem to motivate our approach.",
        "It has been shown that global inference-based training for coreference resolution outperforms training with local inference only (e.g. Finley and Joachims (2005), McCallum and Wellner (2004)).",
        "In particular, the output of coreference resolution must obey equivalence relations, and exploiting such structural constraints on the output space during training can improve performance.",
        "Consider the coreference resolution task for the following text.",
        "It was after the passage of this act, that Mary^- 's attitude towards Elizabeth^ became overtly hostile.",
        "The deliberations surrounding the act seem to have revived all Mary's memories of the humiliations she had suffered at the hands of Anne Boleyn.",
        "At the same time, Elizabeth^ 's continuing prevarications over religion confirmed that she was indeed her mother's daughter.",
        "In the above text, the \"she\" in the last sentence is coreferent with both mentions of \"Elizabeth\".",
        "However, when we consider \"she\" and \"Elizabeth\" in isolation from the remaining coreference chain, it can be difficult for a machine learning method to determine whether the pair is coreferent or not.",
        "Indeed, such a pair may not look very different from the pair \"she\" and \"Mary(l\" in terms of feature vectors.",
        "It is much easier, however, to determine that \"she\" and \"Elizabeth\" are coreferent, or that \"Elizabeth^ \" and \"Elizabeth \" are coreferent.",
        "Only by taking the transitive closure of these pairwise coreference relations does it become clear that \"she\" and \"Elizabeth\" are coreferent.",
        "In other words, global training might handle potentially confusing coreference cases better because it allows parameter learning (for each pairwise coreference decision) to be informed by global inference.",
        "We argue that, with appropriate modification to the learning instances, local training is adequate for the coreference resolution task.",
        "Specifically, we propose that confusing pairs in the training data – such as \"she\" and \"Elizabeth \" – be learned as not-coreferent, so long as the global inference step can fix this error by exploiting the structure of the output space, i.e. by exploiting the equivalence relations.",
        "This is the key idea of structured local training, which we elaborate formally in the following section.",
        "In this section, we present a general description of structured local training.",
        "Let y be a vector of output variables for structured output, and let x be a vector of input variables.",
        "In order to capture the interactions between global inference and local inference, we introduce hidden variables h, |h| = |y|, so that the global inference for p(y, h|x) can be factored into two components using the product rule, as follows:",
        "The second component p(h|x) on the right hand side corresponds to the local model, for which the inference factorizes into smaller independent pieces, e.g. argmaxhp(h|x) = {argmax^.0(hj, x)}.",
        "And the first component p(y|h, x) on the right hand side corresponds to the global model, whose inference may not factorize nicely.",
        "Further, we assume that y is independent of x given h, so that p(y|h, x)= p(y|h).",
        "That is to say, h captures sufficient information from x, so that given h, global inference of y only depends on h. The quantity of p(y|x) then is given by marginalizing out h as follows:",
        "Intuitively, the hidden variables h represent the local decisions that can lead to a good y after global inference is applied.",
        "In the case of coreference resolution, one natural factorization would be that global inference is a clustering algorithm, and local inference is a classification decision on each pair of noun phrases (or mentions).",
        "In this paper, we assume that we only parameterize the local model p(h|x), although it would be possible to extend the parameterization to the global model as well, depending on the particular application under consideration.",
        "The similarity between a pair of mentions is parameterized via log-linear models.",
        "However, once we have the similarity scores extracted via local inference, the clustering algorithm does not require further parameterization.",
        "For training, we apply the standard Expectation-Maximization (EM) algorithm (Dempster et al., 1977) as follows:",
        "• E Step: Compute a distribution",
        "By repeatedly applying the above two steps for t = 1,2,..., the value of 6 converges to the local maxima of the conditional log likelihood L(6) = logP (y|x,6).",
        "For yi G y (and hi G h) in the coreference resolution task, yi = 1 (and hi = 1) corresponds to ith pair of mentions being coreferent, and yi = 0 (and hi = 0) corresponds to ith pair being not coreferent.",
        "[Local Model P(h|x)] For the local model, we define cliques as individual nodes, and parameterize each clique potential as this model,",
        "Notice that in finding argmaxhP(h|x corresponds to simply finding argmax^.^(hi,x.Ndependently for each hi G h.",
        "Algorithm-1 input: x, true labeling y*, current local model P(h|x) goal: Find the highest confidence labeling ysuch that y* = single-link-clustering(y')",
        "else end for return y'",
        "Fi gure 1: Algorithm to find the highest confidence labeling y that can be clustered to the true labeling y* [Global Model P (y|h)] For the global model, we assume a deterministic clustering algorithm is given.",
        "In particular, we focus on single-link clustering, as it has been shown to be effective for coreference resolution (e.g. Ng and Cardie (2002)).",
        "With single-link clustering, P(y|h) = 1 if h can be clustered to y, and P(y|h) = 0 if h cannot be clustered to y.",
        "[Computation of the E-step] The E-step requires computation of the distribution of P(h|y, x,6t-1)), which we will simply denote as P(h|y, x), since all our distributions are implicitly conditioned on the model parameters 6.",
        "Notice that when computing P(h|y, x), the denominator P(y|x) stays as a constant for different values ofh.",
        "The E-step requires enumeration ofall possible values of h, but it is intractable with our formulation, because inference for the global model P(y| h) does not factor out nicely.",
        "Therefore, we must resort to an",
        "Algorithm-2 input: x, true labeling y*, current local model P(h|x) goal: Find a high confidence labeling y that is close to the true labeling y* h* – argmaxjjP (h|x) h' – single-link-clustering (h*) for each h'i e h'",
        "Figure 2: Algorithm to find a high confidence labeling y that is close to the true labeling y* approximation method.",
        "Neal and Hinton (1998) analyze and motivate various approximate EM training methods.",
        "One popular choice in practice is called \"Viterbi training\", a variant of the EM algorithm, which has been shown effective in many NLp applications.",
        "Viterbi training approximates the distribution by assigning all probability mass to a single best assignment.",
        "The algorithm for this is shown in Figure 1.",
        "We propose another approximation option for the E-step that is given by Figure 2.",
        "Intuitively, when the current local model misses positive coreference decisions, the first algorithm constructs a y that is closest to h' for single-link clustering to recover the true labeling y*, while the second algorithm constructs a y that is closer to y* by preserving all of the missing positive coreference decisions.",
        "[Computation of M-step] Because P(y|h) is not parameterized, finding argmax^ P(y, h|x) reduces to finding argmax^ P(h|x), which is standard CRF training.",
        "In order to speed up the training, we start convex optimization for CRFs using the parameter values 6(t-1 from the previous M-step.",
        "For the very first iteration of EM, we start by setting P(y*|x) = 1 for E-step, so that the first M-step will finds argmaxg P(y*|x).",
        "worse.",
        "yi – h* else ifh'i e M",
        "[Inference on the test data] It is intractable to marginalize out h from P(y, h|x).",
        "Therefore, similar to the Viterbi-training in the E-step, we approximate the distribution of h by argmaxjjP(h|X)."
      ]
    },
    {
      "heading": "3. Experiments-I",
      "text": [
        "Data set: We evaluate our approach with two coreference data sets: MUC6 (MUC-6, 1995) and MPQA(Wiebe et al., 2005).",
        "For the MUC6 data set, we extract noun phrases (mentions) automatically, but for MPQA, we assume mentions for corefer-ence resolution are given as in Stoyanov and Cardie (2006).",
        "For MUC6, we use the standard training/test data split.",
        "For MPQA, we use 150 documents for training, and 50 documents for testing.",
        "Configuration: We follow Ng and Cardie (2002) for feature vector construction for each pair of mentions, and Finley and Joachims (2005) for constructing a training/testing instance for each document: a training/testing instance consists of all pairs of mentions in a document.",
        "Then, a single pair of mentions is a sub-instance.",
        "We use the Mallet implementation of CRFs, and set a Gaussian prior of 1 .",
        "0 for all experiments.",
        "At each M-step, we train CRFs starting from the parameters from the previous M-step.",
        "We train CRFs up to 200 iterations, but because we start training CRFs from the previous parameters, the convergence from the second M-step becomes much faster.",
        "We apply up to 5 EM iterations, and choose best performing 6^, 2 < t < 5 based on the performance on the training data.",
        "Hypothesis: For the baseline (BASE) we employ the locally trained model for pairwise decisions without global inference.",
        "Clustering is applied only at test time, in order to make the assignment on the output variables coherent.",
        "We hypothesize that for the baseline, maximizing the likelihood for training will correlate more with the pairwise accuracy of the",
        "Table 1: performance of Structured Local Training: SLT reduces error rate (e %) after applying single-link clustering.",
        "incoherent decisions before clustering than the pairwise accuracy ofthe coherent decisions aftercluster-ing.",
        "We also hypothesize that by performing structured local training (SLT), maximizing the likelihood will correlate more with the pairwise accuracy after clustering.",
        "Results: Experimental results are shown in Table 1.",
        "We report error rate (error rate = 100 – accuracy) on the pairwise decisions (e %), and F1-score (F %) on the coreferent pairs.",
        "For comparison, we show numbers from both after and before single-link clustering is applied.",
        "As hypothesized, the error rate of BASE increases after clustering, while the error rate of SLT decreases after clustering.",
        "Moreover, the error rate of SLT is considerably lower than that of BASE after clustering.",
        "However, the F1-score does not correlate with the error rate.",
        "That is, a lower error rate does not always lead to a higher F1-score, which motivates the Biased Potential Functions that we introduce in the next section.",
        "Notice that when we compare the precision/recall breakdown after clustering, SLT has higher precision and lower recall than BASE."
      ]
    },
    {
      "heading": "4. Biased Potential Functions",
      "text": [
        "We introduce biased potential functions for training CRFs to empirically favor preferred evaluation measures for the learning task, such as F-score and MuC-score that have been considered hard for traditional likelihood-based methods to optimize for.",
        "Intuitively, biased potential functions emphasize those subcomponents of an instance that can be of greater importance than the rest of an instance.",
        "after clustering",
        "before clustering",
        "e%",
        "R% P% F%",
        "e%",
        "R% P% F%",
        "base",
        "1.50",
        "59.2 56.2 57.7",
        "1.18",
        "38.0 85.6 52.6",
        "slt",
        "1.28",
        "49.8 67.3 57.2",
        "1.35",
        "26.4 84.3 40.2",
        "MPQA",
        "after clustering",
        "before clustering",
        "e%",
        "R% P% F%",
        "e%",
        "R% P% F%",
        "base",
        "9.83",
        "75.8 57.0 65.1",
        "7.05",
        "52.1 83.4 64.1",
        "slt",
        "6.39",
        "62.1 80.6 70.2",
        "7.39",
        "43.7 90.1 58.9",
        "The conditional probability of P(y|x) for CRFs is given by (Lafferty et al., 2001) where <fi(Ci, x) is a potential function defined over each clique C Potential functions are typically parameterized in an exponential form as follows.",
        "where Xk are the parameters and fk(•) are feature indicator functions.",
        "Because the Hammersley-Clifford theorem (1971) for undirected graphical models holds for any non-negative potential functions, we propose alternative potential functions as follows.",
        "where [3 is a non-negative bias factor, and [i(Ci, x) is a predicate (or an indicator function) to check certain properties on (Cj, x).",
        "Examples of possible //(•) would be whether the true assignment for Cj in the training data contains certain class values, or whether the current observation indexed by Cj has particular characteristics.",
        "More specific details will be given in §4.2.",
        "Training and testing with biased potential functions is mostly identical to the traditional log-linear formulations by </>(•) as defined above, except for small and straightforward modifications to the computation of the likelihood and the derivative of the likelihood.",
        "The key idea for biased potential functions is nothing new, as it is conceptually similar to instance weighting for problems with non-structured output (e.g. Aha and Goldstone (1992), Cardie et al.",
        "(1997)).",
        "However, biased potential functions differ technically in that they emphasize desired subcomponents without altering the i.i.d.",
        "assumption, and still weight each instance alike.",
        "Despite the conceptual simplicity, we are not aware of any previous work that explored biased potential functions for problems with structured output.",
        "[Bias on Coreferent Pairs] For coreference resolution, pairs that are coreferent are in a minority class, and biased potential functions can mitigate this skewed data problem, by amplifying the clique potentials that correspond to coreferent pairs.",
        "We define /j,(iji,Xi) to be true if and only if the true assignment for yi in the training data is 'coreferent'.",
        "Notice that ) does not depend on what particular value yj might take, but only depends on the true value of yi in the training data.",
        "For testing, [i(yi,Xi) will be always false.",
        "[Bias on Closer Coreferent Pairs] For corefer-ence resolution, we hypothesize that coreferent pairs for closer mentions have more significance, because they tend to have clearer linguistic clues to determine coreference.",
        "We further hypothesize that by emphasizing only close coreferent pairs, we can have our model favor the MuC score.",
        "For this, we define [i(yi,Xi) to be true if and only if Xi is for a pair of mentions that are the closest coreferent pair."
      ]
    },
    {
      "heading": "5. Experiments-II",
      "text": [
        "Data sets and configurations for experiments are identical to those used in §3.",
        "Hypothesis: We hypothesize that using biased potential functions, maximizing the likelihood for training can correlate better with F1-score or MuC-score than the pairwise accuracy.",
        "In particular,",
        "Table 2: Performance of Biased Potential Functions: pairwise scores are taken before single-link-clustering is applied.",
        "we hypothesize that biasing on every coreferent pair will correlate more with F1-score, and biasing on close coreferent pairs will correlate more with MUC-score.",
        "In general, we expect that biasing on coreferent pairs will boost recall, potentially decreasing precision.",
        "Results [BPF]: Experimental results for biased potential functions, without structured local training, are shown in Table 2. basic-p1 denotes local training with biased potential on the closest corefer-ent pairs with bias factor [, and basic-po denotes local training with biased potential on the all coref-erent pairs with bias factor [, where [ = 1.5 or 3.0.",
        "For brevity, we only show pairwise numbers before applying single-link-clustering.",
        "As hypothesized, biased potential functions in general boost recall at the cost of precision.",
        "Also, for a fixed value of 3, basic-p1 gives better MUC-F1 than basic-po, and basic-po gives better pairwise-F1 than basic-p1 for both data sets.",
        "Results [SLT+BPF]: Experimental results that combine SLT and BPF are shown in Table 3.",
        "Similarly as before, slt-px denotes SLT with biased potential scheme Px, with bias factor [.",
        "For brevity,",
        "Table 3: Performance of Biased Potential Functions with Structured Local Training: All numbers are taken after singlelink clustering.",
        "we only show numbers after applying single-link-clustering.",
        "unlike the results shown in Table 2, for a fixed value of [, slt-pi correlates better with pairwise-F1, and SLT-Pa' correlates better with MuC-F1.",
        "This indicates that when biased potential functions are used in conjunction with SLT, the effect of biased potential functions can be different from the case without SLT.",
        "Comparing F1-scores in Table 2 and Table 3, we see that the combination of biased potential functions with SLT improves performance in general.",
        "In particular, SLT-Pl and SLT-Pa consistently improve performance over BASE on both data sets, for both pairwise-F1 and MuC-F1.",
        "We present performance scores for all variations of configurations for reference, but we also mark the particular configuration SLT-Px (by '*' on F1-scores) that is chosen when selecting the configuration based on the performance on the training data for each performance measure.",
        "To conclude, structured local training with biased potential functions bring a substantial improvement for MUC-F1 score, from 66.4% to 69.9% for MUC6 data set.",
        "For pairwise-F1, the performance increase from 57.7% to 58.5% for MUC6, and from 65.1% to 70.6% for MPQA.",
        "\"Performance on the MPQA data for MUC-F1 is slightly",
        "pairwise",
        "MUC",
        "e%",
        "R% P% F%",
        "R% P% F%",
        "BASE",
        "1.18",
        "38.0 85.6 52.6",
        "59.0 75.8 66.4",
        "BASIC-Pl",
        "1.20",
        "38.9 82.1 52.8",
        "64.2 71.8 67.8",
        "BASIC-Pl",
        "1.32",
        "46.9 71.3 56.6",
        "68.9 64.3 66.5",
        "BASIC-Pa'",
        "1.15",
        "44.2 79.9 56.9",
        "62.1 68.7 65.2",
        "BASIC-Pa'",
        "1.44",
        "52.5 62.9 57.2",
        "70.9 60.5 65.3",
        "MPQA",
        "pairwise",
        "MUC",
        "e%",
        "R% P% F%",
        "R% P% F%",
        "BASE",
        "7.05",
        "52.1 83.4 64.1",
        "75.6 81.5 78.4",
        "BASIC-Pl",
        "7.18",
        "54.6 79.6 64.8",
        "77.7 76.5 77.1",
        "BASIC-Pl",
        "7.22",
        "59.9 75.4 66.8",
        "83.3 71.7 77.1",
        "BASIC-Pa'",
        "7.65",
        "59.7 72.2 65.4",
        "79.8 73.2 76.4",
        "BASIC-Pa'",
        "8.22",
        "69.2 65.1 67.1",
        "85.8 67.8 75.7",
        "pairwise",
        "MUC",
        "e%",
        "R% P% F%",
        "R% P% F%",
        "BASE",
        "1.50",
        "59.2 56.2 57.7",
        "59.0 75.8 66.4",
        "SLT",
        "1.28",
        "49.8 67.3 57.2",
        "56.3 77.8 65.3",
        "SLT-Pl'",
        "1.19",
        "52.8 70.6 60.4",
        "59.3 74.6 66.1",
        "SLT-Pl",
        "1.42",
        "63.5 57.9 60.6",
        "67.5 70.7 69.1",
        "SLT-Pa-",
        "1.43",
        "58.6 58.5 58.5*",
        "64.0 73.6 68.5",
        "3 n",
        "SLT-Pa",
        "1.71",
        "65.2 50.3 56.8",
        "70.5 69.3 69.9*",
        "MPQA",
        "pairwise",
        "MUC",
        "e%",
        "R% P% F%",
        "R% P% F%",
        "BASE",
        "9.83",
        "75.8 57.0 65.1",
        "75.6 81.5 78.4",
        "SLT",
        "6.39",
        "62.1 80.6 70.2",
        "69.1 88.2 77.5",
        "SLT-Pl",
        "6.54",
        "64.9 77.4 70.6*",
        "72.2 84.5 77.9*",
        "SLT-Pl",
        "9.09",
        "77.2 59.6 67.3",
        "78.4 79.5 78.9",
        "SLT-Pa'",
        "6.74",
        "65.2 75.7 70.1",
        "72.4 87.2 79.1",
        "^ n",
        "SLT-Pa",
        "14.71",
        "78.2 43.9 56.2",
        "80.5 73.8 77.0"
      ]
    },
    {
      "heading": "6. Related Work",
      "text": [
        "Structured local training is motivated by recent research that has shown that reducing the discrepancy between the training model and testing model can improve the performance without incurring the heavy computational overhead of full-blown global inference-based training.",
        "(e.g. Cohen and Car-valho (2005), Sutton and McCallum (2005a), Sutton and McCallum (2005b)).",
        "Our work differs in that (1) we use hidden variables to capture the interactions between local inference and global inference, (2) we present an application to coreference resolution, while previous work has shown applications for variants of sequence tagging.",
        "McCallum and Wellner (2004) showed a global training approach with CRFs for coreference resolution, but they used the voted perceptron algorithm for training, which no longer maximizes the likelihood.",
        "In addition, they assume that all and only those noun phrases involved in coreference resolution are given.",
        "The performance of our system on MUC6 data set is comparable to previously reported systems.",
        "Using the same feature set, Ng and Cardie (2002) reports 64.5% of MUC-score, while our system achieved 69.9%.",
        "Ng and Cardie (2002) reports 70.4% of MUC-score using hand-selected features.",
        "With an additional feature selection or feature induction step, the performance of our system might further improve.",
        "McCallum and Wellner (2004) reports 73.42% of MUC-score on MUC6 data set, but their experiments assumed perfect identification of all and only those noun phrases involved in a coreference relation, thus substantially simplifying the task."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "We present a novel training procedure, structured local training, that maximizes likelihood while exploiting the benefits of global inference during training.",
        "This is achieved by incorporating hidden variables to capture the interactions between local",
        "MPQA baseline are already quite high to begin with.",
        "inference and global inference.",
        "In addition, we introduce biased potential functions that allow CRFs to empirically favor performance measures such as F1-score or MUC-score.",
        "We focused on the application of coreference resolution in this paper, but the key ideas of our approaches can be extended to other applications, and other machine learning techniques motivated by Markov networks.",
        "Acknowledgments We thank the reviewers as well as Eric Breck and Ves Stoyanov for their many helpful comments.",
        "This work was supported by the Advanced Research and",
        "Development Activity (ARDA), by NSF Grants BCS-0624277,",
        "IIS-0535099, and IIS-0208028, and by gifts from Google and the Xerox Foundation."
      ]
    }
  ]
}
