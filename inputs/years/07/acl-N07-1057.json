{
  "info": {
    "authors": [
      "Fei Xia",
      "William H. Lewis"
    ],
    "book": "Human Language Technologies 2007: the Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
    "id": "acl-N07-1057",
    "title": "Multilingual Structural Projection across Interlinear Text",
    "url": "https://aclweb.org/anthology/N07-1057",
    "year": 2007
  },
  "references": [
    "acl-H05-1107",
    "acl-H94-1020",
    "acl-J93-2003",
    "acl-J94-4004",
    "acl-N01-1026",
    "acl-N06-1041",
    "acl-P00-1056",
    "acl-P02-1050",
    "acl-P05-1034",
    "acl-P06-1123",
    "acl-P95-1037",
    "acl-W02-1039",
    "acl-W06-1608"
  ],
  "sections": [
    {
      "text": [
        "fxiaQu.washington.edu",
        "William D. Lewis",
        "Seattle, WA 98195",
        "This paper explores the potential for annotating and enriching data for low-density languages via the alignment and projection of syntactic structure from parsed data for resource-rich languages such as English.",
        "We seek to develop enriched resources for a large number of the world's languages, most of which have no significant digital presence.",
        "We do this by tapping the body of Web-based linguistic data, most of which exists in small, analyzed chunks embedded in scholarly papers, journal articles, Web pages, and other online documents.",
        "By harvesting and enriching these data, we can provide the means for knowledge discovery across the resulting corpus that can lead to building computational resources such as grammars and transfer rules, which, in turn, can be used as bootstraps for building additional tools and resources for the languages represented."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Developing natural language applications is generally dependent on the availability of annotated corpora.",
        "Building annotated resources, however, is a significantly time consuming process involving considerable human effort.",
        "Although a number of projects have been undertaken to develop annotated resources for non-English languages, e.g., treebanks, the development of these resources has been no small feat, and to date have been limited to a very small number of the world's languages (e.g., Chinese, German, Arabic, Korean, etc.).",
        "Some notable efforts have been undertaken to develop automated means for creating annotated corpora through the projection of annotations (Yarowksy and Ngai, 2001; Xi and Hwa, 2005).",
        "The resulting methods, however, can only be applied to a small number of language pairs due mostly to the need for sizeable parallel corpora.",
        "Unfortunately, most languages do not have parallel corpora of sufficient size, making these methods inapplicable for the vast majority of the world's languages.",
        "We describe a method for bootstrapping resource creation by tapping the wealth of multilingual data on the Web that has been created by linguists.",
        "Of particular note is the linguistic presentation format of \"interlinear text\", a common format used for presenting language data and analysis relevant to a particular argument or investigation.",
        "Since interlinear examples consist of orthographically or phonetically encoded language data aligned with an English translation, the \"database\" of interlinear examples found on the Web, when taken together, constitute a significant multilingual, parallel corpus covering hundreds to thousands of the world's languages.",
        "We do not propose that a database of interlinear text alone is sufficient to create NLP resources and tools, but rather that it may act as a means for more rapidly developing such tools using less data.",
        "We contend that such a resource allows one to develop computational artifacts, such as grammars and transfer rules, which can be used as \"seed\" knowledge for building larger resources.",
        "In particular, knowing a little about the structure of a language can help in developing annotated corpora and tools, since a little knowledge can go a long way in inducing accurate structure and annotations (Haghighi and Klein, 2006).",
        "Of particular relevance to MT is the issue of structural divergence (Dorr, 1994).",
        "Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in (Hwa et al., 2002).",
        "However, to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data (Fox, 2002; Hwa et al., 2002; Wellington et al., 2006).",
        "A larger sample of typologically diverse language data can help test the assumption for hundreds of languages.",
        "We contend that the knowledge garnered from structural projections applied to interlinear text can bootstrap the development of resources and tools across parallel corpora, where such corpora could be of smaller size and the resulting tools more robust, opening the door to the development of tools and resources for a larger number of the world's languages.",
        "Given the imminent death of half of the world's 6,000 languages (Krauss, 1992), the development of any language specific tools for a larger percentage of the world's languages than is currently possible can aid in both their documentation and preservation."
      ]
    },
    {
      "heading": "2. Background",
      "text": [
        "The practice of presenting language data in interlinear form has a long history in the field of linguistics, going back at least to the time of the structuralists (see (Swanton, 1912) for early examples).",
        "The modern form of interlinear data presentation started to gel in the mid-1960s, resulting in the canonical three line form shown in Ex (1), which we will refer to as Interlinear Glossed Text, or IGT.",
        "The canonical form consists of three lines: a line for the language in question (often a sentence, which we will refer to here as the source sentence), an English gloss line, and an English translation.",
        "gave-3sg the teacher book to-the boy yesterday \"The teacher gave a book to the boy yesterday\" (Bailyn, 2001)",
        "Although IGT is usually embedded in linguistics documents as part of a larger analysis, in and of itself it contains analysis and interesting information about the source language.",
        "In particular, the gloss line, which is word and morpheme aligned with the source, contains word and morpheme translations for the source language data, and can even contain grammatically salient annotations (e.g., 3sg for Third Person Singular).",
        "Further, the reader will note that many words are shared between the gloss and translation lines, allowing for the alignment between these two lines as a intermediate step in the alignment between the translation and the source.",
        "An effort is underway to collect these interlinear snippets into an online searchable database, the primary purpose of which is to help linguists find analyzed data for languages they are interested in.",
        "We use this resource, called ODIN, the Online Database of INterlinear text (Lewis, 2006), as our primary data source.",
        "At the time of this writing, ODIN contains 36,439 instances of interlinear data for 725 of the world's languages."
      ]
    },
    {
      "heading": "3. The Enrichment Algorithm",
      "text": [
        "Our algorithm enriches the original IGT examples by building syntactic structures over the English data and then projects these onto the source language data via word alignment.",
        "The term syntactic structure in this paper refers to both phrase structure (PS) and dependency structure (DS).",
        "The enrichment process has three steps:",
        "1.",
        "Parse the English translation using an off-the-shelf parser.",
        "2.",
        "Align the source sentence and English translation with the help of the gloss line.",
        "3.",
        "Project the English syntactic structures to obtain the source syntactic structures using word alignment.",
        "There are many English parsers available to the public, and in this experiment we used Charniak's parser (Charniak, 1997), which was trained on the English Penn Treebank (Marcus et al., 1994).",
        "Figure 1(a) shows a parse tree (in the Penn Treebank style) for the English translation in Ex (1).",
        "Given a parse tree, we use a head percolation table (Magerman, 1995) to create the corresponding dependency structure.",
        "Figure 2(a) shows the dependency structure derived from the parse tree in Figure 1(a).",
        "Because most of the 700+ languages in ODIN are low-density languages with no on-line bilingual dictionaries or large parallel corpora, aligning the source sentence and its English translation directly would not work well.",
        "To take advantage of the unique layout of IGT examples, we propose using the gloss line as a bridge between the other two lines; that is, we first align the source sentence and the gloss line, and then align the gloss line and the English translation.",
        "The process is illustrated in Figure 3.",
        "(a) English PS (b) Source PS after Step 2 (c) Final source PS",
        "yr I rhoddodd I in 1^1 (the)athro (gave) NN I y\\",
        "J I yesterday the boy",
        "(teacher) lyfr (to-the)l (book)",
        "NP PP NP bachogen (boy) rhoddodd",
        "(a) English DS gave",
        "teacher book to yesterday the a boy (b) Source DS after Step 2 Rhoddodd (c) Final source DS Rhoddodd",
        "bachgen I",
        "Gloss.",
        "gave-3sg the teacher book to-the boy yesterday",
        "Transatlion.",
        "The teacher gave a book to the boy yesterday",
        "The alignment between the source sentence and the gloss line is trivial and our preliminary experiments showed that simply using whitespace and dashes as delimiters, and assuming a one-to-one alignment produces almost perfect results.",
        "In contrast, the alignment between the gloss line and the English translation is more complicated since alignment links can cross and words on one side can link to zero or more words on the other side.",
        "We built two aligners for this stage, as described below.",
        "We create a parallel corpus by using the gloss lines and the translation lines of all the IGT examples for all the languages in ODIN.",
        "We then train IBM models (Brown et al., 1993) using the GIZA++ package (Och and Ney, 2000).",
        "In addition to the common practice of lowercasing words and combining word alignments from both directions, we adopt the following strategies to improve word alignment:",
        "Breaking words into morphemes: Since a multi-morpheme word in a gloss line often corresponds to multiple words in the translation line, we split each word on the gloss line into morphemes using the standard IGT morpheme delimiters (e.g., \"-\").",
        "For instance, the seven words in the gloss line of Ex (1) become nine morphemes.",
        "Adding (x,x) pairs: If a word x appears in the gloss and the translation lines of the same IGT example, it is highly likely that the two copies of the same word should be aligned to each other.",
        "To help GIZA++ recognize this property, we first identify and collect all such words and then add single word pairs (x,x) to the training data.",
        "For instance, from Ex (1), we would add a sentence pair for each morpheme (excepting -3sg which does not appear in the translation line).",
        "Our second word aligner is based on the assumption that if two words (one on the gloss line, the other on the translation line) have the same root form, they are likely to be aligned to one other.We built a simple English morphological analyzer and ran it on the two lines, and then linked the words with the same root form.",
        "We designed two projection algorithms: one which projects PS and the other which projects DS, both from the English to the source language.",
        "Our DS projection algorithm is similar to the projection algorithms described in (Hwa et al., 2002) and (Quirk et al., 2005).",
        "It has four steps: First, we copy the English DS, and remove all the unaligned English words from the DS.",
        "Second, we replace each English word in the DS with the corresponding source words.",
        "If an English word x aligns to several source words, we will make several copies of the node for x, one copy for each such source word.",
        "The copies will all be siblings in the DS.",
        "If a source word aligns to multiple English words, after Step 2 the source word will have several copies in the resulting DS.",
        "In the third step, we keep only the copy that is closest to the root and remove all the other copies.",
        "In Step 4, we attach unaligned source words to the DS using the heuristics described in (Quirk et al., 2005).",
        "Figure 2 shows the English DS, the source DS after Step 2, and the final DS.",
        "Our PS projection algorithm also has four steps, the first two being the same as those for projecting DS.",
        "In the third step, starting from the root of the current source PS and for each node x with more than one child, we reorder each pair of x s children until they are in the same order as dictated by the source sentence.",
        "Let yi and yj be two children of x, and their spans be Si = [ai, bi] and Sj = [aj ,bj ].",
        "When we reorder yi and yj, there are four possible scenarios:",
        "(1) Si and Sj don't overlap: we put yi before yj if a,: < aj or the opposity if aj > aj.",
        "(2) Si is a strict subset of Sj: we remove yj from the PS and promote its children: yj's children will become children of yj s parent.",
        "(3) Sj is a strict subset of Si: we remove yi and promote its children.",
        "(4) Si and Sj overlap but neither is a strict subset of the other: we remove both yi and yj and promote their children.",
        "If both yi and yj are leaf nodes with the same span, we will merge the two nodes.",
        "The last step is to insert unaligned source words into the source PS.",
        "For each unaligned source word x, we will find its closest left and right neighbors that are aligned to some English words, and then attach x to the lowest common ancestor of the two neighbors.",
        "Figure 1 shows the English PS, the source PS after Step 2, and the final source PS.",
        "The three boxes in 1(b) mark the nodes that are removed in Step 3."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We tested the feasibility of our approach on a small set of IGT examples for seven languages: German (GER), Korean (KKN), Hausa (HUA), Malagasy (MEX), Welsh (WLS), Irish (GLI), and Yaqui (YAQ).",
        "This set of languages was chosen because of its typological diversity: GER and HUA are SVO languages, KKN and YAQ are SOV, GLI and WLS are VSO, and MEX is VOS.",
        "In addition, while German and Korean are well-studied and have readily accessible resources that we could use to test the effectiveness and accuracy of our methods, Yaqui, with about 16,000 speakers, is a highly endangered language and serves as a demonstration of our methods for resource-poor and endangered languages.",
        "The number of IGT examples in ODIN varies greatly across the seven languages, ranging from less than one hundred for Welsh to over seventeen hundred for German.",
        "For each language, we randomly picked 50150 IGT examples from the available examples whose English translations had at least five words.",
        "The examples were manually checked and corrupted examples were thrown away.",
        "The remaining examples formed our test data.",
        "Table 1 shows the size and average sentence lengths of the test data by language.The languages are sorted by number of speakers (as derived from the Ethnologue (Gordon, 2005)).",
        "We ran our algorithm on the test data, and the system produced the following: an English PS, English DS, word alignment, projected source PS, and projected source DS.",
        "We asked human annotators to manually check the output and correct the English DS, word alignments and projected DS structures where necessary.",
        "In order to calculate inter-annotator agreement, the Yaqui data and half of the German data were each checked by two annotators, and the disagreement between the annotators was adjudicated and a gold standard was created.",
        "The inter-annotator agreement (a.k.a.",
        "the F-measure of dependency or alignment links) on English DS, glosstranslation alignment, and projected source DS are 96.34%, 96.35%, and 91.09%, respectively.",
        "The rest of the data were annotated by one annotator.",
        "We tested our word aligners on 70% (374 examples) of the whole test set (538 examples), while reserving the remaining 30% for future use.",
        "As indicated earlier, the ODIN database contains 36,439 IGT examples.",
        "We removed duplicates and examples with missing lines, and used the remaining 28,902 examples for GIZA++ training.",
        "Table 2 shows the statistics of the training data with all words lowercased.",
        "Tables 3-5 show the performance of the word aligner under three settings:",
        "(1) : Not splitting words in the gloss lines into morphemes.",
        "(2) : Splitting words in gloss lines into morphemes.",
        "the training data, where x is a word that appears in both the gloss and translation lines of the same IGT example.",
        "For each setting, we trained in both directions and combined the two alignments by taking the intersection, union, and refined as defined in (Och and Ney, 2000).",
        "The best F-score for each setting is in boldface.",
        "From the tables, it is clear that the third setting works the best, and combining the alignments from both directions works better than either direction alone.",
        "GER",
        "KKN",
        "HUA",
        "MEX",
        "WLS",
        "GLI",
        "YAQ",
        "Total",
        "# of IGT examples",
        "104",
        "103",
        "77",
        "87",
        "53",
        "46",
        "68",
        "538",
        "# of src words Ave src sent leng",
        "739 7.11",
        "526 5.11",
        "441 5.73",
        "498 5.72",
        "313 5.91",
        "252 5.48",
        "404 5.94",
        "3173 5.90",
        "# of Eng words Ave Eng sent leng",
        "711 7.41",
        "735 7.14",
        "520 6.75",
        "646 7.43",
        "329 6.21",
        "278 6.04",
        "544 8.01",
        "3823 7.11",
        "# of speakers",
        "128M",
        "78M",
        "39M",
        "9.4M",
        "580K",
        "260K",
        "16K",
        "255.3M",
        "# of sentences",
        "28,902",
        "# of words in gloss lines",
        "# of morphemes in gloss lines",
        "174,765 251,465",
        "# of words in translation lines",
        "217,022",
        "Size of gloss word vocabulary Size of gloss morpheme vocabulary",
        "16360 14050",
        "Size of translation word vocabulary",
        "14029",
        "Precision",
        "Recall",
        "F-measure",
        "Gloss – > trans",
        "0.674",
        "0.689",
        "0.681",
        "Trans – > gloss",
        "0.721",
        "0.823",
        "0.769",
        "Intersection",
        "0.948",
        "0.620",
        "0.750",
        "Union",
        "0.590",
        "0.892",
        "0.711",
        "Refined",
        "0.846",
        "0.780",
        "0.812",
        "The word aligner has two settings.",
        "In the first one, the aligner aligns two words if and only if they have the same orthographic form.",
        "In the second, it aligns two words if and only if they have the same root form.",
        "The results are shown in the first and second rows of Table 6.",
        "We experimented with various methods of combining the two aligners, and the best one is an augmented heuristic word aligner which links two words if and only if they have the same root form or they are good translations of each other according to the translation model built by GIZA++.",
        "The result is shown in the last row of Table 6.",
        "We used this aligner for the structural projection experiment.",
        "We evaluated the results of the major steps in our algorithm: the English DS derived from the parse trees produced by the English parser, the word alignment between the gloss and translation lines, and the projected source DS.",
        "We calculated the precision, recall, and F-score of the dependency links and word alignment links.",
        "The F-scores are shown in Table 7.",
        "Both the English parser and the word aligner work reasonably well with most F-scores well above 90%.",
        "The F-scores for dependency links in the source DS are lower partly due to errors in early parts of the process (e.g., English DS and word alignment), which propagates to this step.",
        "When we replace the automatically generated English DS and word alignment with the ones in gold standard, the F-measure of source DS increases significantly, as shown in Table 8.",
        "To identify the causes of the remaining errors in the oracle results, we manually checked and classified one third of the errors in the German data.",
        "Among the 43 errors in the source DS, 26 (60.5%) are due to language divergence (e.g., head switching), eight (18.6%) are errors made by the projection heuristics, and nine (20.9%) are due to non-exact translations such as the one shown in Ex (2).",
        "Because language divergence can reveal interesting typological distinctions between languages, the first type of error may, in fact, identify examples that could be of great value to linguists and computational linguists.",
        "(2) der Antrag des oder der Dozenten the petition of-the.SG or of-the.PL docent.MSC \"the petition of the docent.\"",
        "(Daniels, 2001)"
      ]
    },
    {
      "heading": "5. Discussion",
      "text": [
        "From the enriched data, various kinds of information can be extracted, such as grammars and transfer rules.",
        "We extracted CFGs for the seven languages by reading off the context-free rules from the projected source PS.",
        "The numbers of rule types and rule tokens for four of the languages are listed in Table 9.",
        "Precision",
        "Recall",
        "F-measure",
        "Gloss – > trans",
        "0.746",
        "0.889",
        "0.811",
        "Trans – > gloss",
        "0.797",
        "0.863",
        "0.829",
        "Intersection",
        "0.958",
        "0.811",
        "0.878",
        "Union",
        "0.659",
        "0.941",
        "0.775",
        "Refined",
        "0.918",
        "0.900",
        "0.909",
        "Precision",
        "Recall",
        "F-measure",
        "Gloss – > trans",
        "0.759",
        "0.922",
        "0.833",
        "Trans – > gloss",
        "0.801",
        "0.924",
        "0.858",
        "Intersection",
        "0.956",
        "0.885",
        "0.919",
        "Union",
        "0.666",
        "0.961",
        "0.787",
        "Refined",
        "0.908",
        "0.921",
        "0.915",
        "Precision",
        "Recall",
        "F-measure",
        "No morphing",
        "0.983",
        "0.742",
        "0.846",
        "With morphing",
        "0.983",
        "0.854",
        "0.914",
        "Augmented aligner",
        "0.981",
        "0.881",
        "0.928",
        "It is important to note that IGT data is somewhat biased: examples tend to be short and are selected for the purposes of a particular rhetorical context.",
        "They, therefore, deviate from the \"normal\" usage that one might normally expect to find in a corpus of language data.",
        "As such, one might question whether the information extracted from IGT would also be skewed due to these biases.",
        "To test the usefulness of the data for answering typological questions, we wrote a tool that predicted the canonical word order (e.g., SOV, SVO) of a language using simple heuristics.",
        "It was able to produce the correct answers for all seven languages in our sample.",
        "We suspect that the number of IGT instances and their diversity (i.e., from multiple documents) is crucial to overcoming the IGT bias, and feel that the same heuristics could be applied to a much larger sample of languages.",
        "These could be further adapted to additional typological parameters beyond word order (e.g., orders of heads and modifiers in PS).",
        "We leave this to future work.",
        "Given syntactically enriched data, it is also possible to search for patterns that are linguistically interesting.",
        "For instance, we wrote a piece of code that automatically identified examples with crossing dependencies (i.e., the ones whose DS have crossing links).",
        "One such example from the Yaqui data is in Ex (3), where the coordinated noun phrase kow-ta into mis-ta \"the pig and the cat\" is separated by the verb bwuise-k \"grasp\".",
        "Note that the crossing dependencies can only be discovered in the Yaqui data and not in the English since none exist in the English.",
        "1SG pig-NNOM.SG grasp-PST and cat-NNOM.SG \"I caught the pig and the cat.\"",
        "(Martinez Fabian, 2006)",
        "So far, we have examined linguistically interesting information in the source.",
        "In the future, we plan to examine structures in both the source and English.",
        "For instance, we plan to extract transfer rules from the aligned source and English structures and also calculate head/modifier crossings between languages similar to those described in (Fox, 2002).",
        "The information that we discover about a language can help with the development of tools for the language.",
        "The order of constituents, for instance, can be used to inform prototype-driven learning strategies (Haghighi and Klein, 2006), which can then be applied to raw corpora.",
        "It is also possible that small samples of data showing the alignment interactions between source language structures and those of English can provide essential bootstrap information for informing machine translation systems (cf (Quirk and Corston-Oliver, 2006)).",
        "Proof of the utility of an enriched corpus built over ODIN will depend crucially on its evaluation, and we feel that an important part of our future work will be the development of parsers that have been trained on projected structures.",
        "These parsers can be evaluated against human built corpora such as treebanks (obviously, only for those languages that have treebanks).",
        "Proof will also come from linguists who will be able to use the corpus to search for constructions of interest (e.g., passives, relative clauses, etc.",
        "), and will likely be able to do so using standard tools such as tgrep.",
        "Crucially, linguists would be able to conduct such searches over a very large number of languages.",
        "GER",
        "KKN",
        "HUA",
        "MEX",
        "WLS",
        "GLI",
        "YAQ",
        "Total",
        "English DS Word alignment Source DS",
        "94.25 94.91 78.14",
        "89.78 94.20 82.16",
        "96.15 94.71 84.71",
        "95.51 94.26 84.22",
        "91.49 95.65 84.39",
        "93.53 88.11 78.17",
        "93.57 93.64 79.36",
        "93.48 94.03 81.45",
        "GER",
        "KKN",
        "HUA",
        "MEX",
        "WLS",
        "GLI",
        "YAQ",
        "Total",
        "With gold Eng DS With gold alignment With both",
        "82.21 85.77 91.21",
        "87.67 86.15 91.67",
        "88.46 86.07 89.82",
        "85.23 88.44 89.65",
        "91.72 84.98 94.25",
        "80.16 82.40 85.77",
        "83.81 86.27 90.68",
        "85.42 86.00 90.64",
        "HUA",
        "MEX",
        "GLI",
        "YAQ",
        "Word order",
        "SVO",
        "VOS",
        "VSO",
        "SOV",
        "# of rule types",
        "102",
        "129",
        "86",
        "115",
        "# of rule tokens",
        "384",
        "466",
        "202",
        "295"
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "In this paper we demonstrate a methodology for projecting structure from annotated English data onto source language data.",
        "Because each IGT instance provides an English translation and an intermediary gloss line, we are able to project full syntactic structures from the automatically parsed translation.",
        "The fact that our basic methodology and code were applied to a typologically diverse sample of seven languages without modification suggests the potential for application to a much larger sample, perhaps numbering into the hundreds of languages.",
        "The resulting enriched structures could be of great importance to the fields of linguistics and computational linguistics.",
        "For the former, search facilities could be built over the data that would allow linguists to find syntactically marked up data for a large variety of languages, and could even accommodate cross-linguistic comparisons and analyses.",
        "For the latter, we could automatically discern grammars and transfer rules from the aligned and marked up data, where these computational artifacts could act as bootstraps for the development of additional tools and resources."
      ]
    }
  ]
}
