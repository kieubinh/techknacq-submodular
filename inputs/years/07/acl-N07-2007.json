{
  "info": {
    "authors": [
      "Jakob Elming",
      "Nizar Habash"
    ],
    "book": "Human Language Technologies 2007: the Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",
    "id": "acl-N07-2007",
    "title": "Combination of Statistical Word Alignments Based on Multiple Preprocessing Schemes",
    "url": "https://aclweb.org/anthology/N07-2007",
    "year": 2007
  },
  "references": [
    "acl-H01-1035",
    "acl-H05-1010",
    "acl-H05-1012",
    "acl-J03-1002",
    "acl-N03-1017",
    "acl-N04-4015",
    "acl-N06-1060",
    "acl-N06-2013",
    "acl-P00-1056",
    "acl-P05-1057",
    "acl-P05-1071",
    "acl-P06-1001",
    "acl-P06-1097",
    "acl-P97-1037"
  ],
  "sections": [
    {
      "text": [
        "Center for Comp.",
        "Modeling of Language Copenhagen Business School je.id@cbs.dk",
        "Center for Comp.",
        "We present an approach to using multiple preprocessing schemes to improve statistical word alignments.",
        "We show a relative reduction of alignment error rate of about 38%."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Word alignments over parallel corpora have become an essential supporting technology to a variety of natural language processing (NLP) applications most prominent among which is statistical machine translation (SMT).",
        "Although phrase-based approaches to SMT tend to be robust to word-alignment errors (Lopez and Resnik, 2006), improving word-alignment is still useful for other NLP research that is more sensitive to alignment quality, e.g., projection of information across parallel corpora (Yarowsky et al., 2001).",
        "In this paper, we present a novel approach to using and combining multiple preprocessing (tok-enization) schemes to improve word alignment.",
        "The intuition here is similar to the combination of different preprocessing schemes for a morphologically rich language as part of SMT (Sadat and Habash, 2006) except that the focus is on improving the alignment quality.",
        "The language pair we work with is Arabic-English.",
        "In the following two sections, we present related work and Arabic preprocessing schemes.",
        "Section 4 and 5 present our approach to alignment preprocessing and combination, respectively.",
        "Results are presented in Section 6.",
        "'The second author was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
        "HR0011-06-C-0023.",
        "Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA.",
        "We thank Necip Ayan, Mona Diab, Bonnie Dorr, Abe Ittycheriah, Martin Jansche and Owen Rambow for helpful discussions."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Recently, several successful attempts have been made at using supervised machine learning for word alignment (Liu et al., 2005; Taskar et al., 2005; Itty-cheriah and Roukos, 2005; Fraser and Marcu, 2006).",
        "In contrast to generative models, this framework is easier to extend with new features.",
        "With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features.",
        "We extend on this approach by including alignment information based on multiple preprocessing schemes in the alignment process.",
        "In other related work, Tillmann et al.",
        "(1997) use several preprocessing strategies on both source and target language to make them more alike with regards to sentence length and word order.",
        "Lee (2004) only changes the word segmentation of the morphologically complex language (Arabic) to induce morphological and syntactic symmetry between the parallel sentences.",
        "We differ from these two in that we do not decide on a certain scheme to make source and target sentences more symmetrical.",
        "Instead, it is left to the alignment algorithm to decide under which circumstances alignment information based on a specific scheme is more likely to be correct than information based on other schemes."
      ]
    },
    {
      "heading": "3. Arabic Preprocessing Schemes",
      "text": [
        "Arabic is a morphologically complex language with a large set of morphological features.",
        "As such, the set of possible preprocessing schemes is rather large (Habash and Sadat, 2006).",
        "We focus here on a subset of schemes pertaining to Arabic attachable clitics.",
        "There are three degrees of cliticization that apply to a word BASE: ( [CONJ+ [PART+ [Al+ BASE +PRON]]]).",
        "At the deepest level, the BASE can have a definite article +JI (Al+ the) or a member of the class of pronominal clitics, +PRON, (e.g., U+ +hA her/it/its).",
        "Next comes the class of particles (PART+), (e.g., s+ will [future]).",
        "Most shallow is the class of conjunctions (CONJ+), (e.g., +j w+ and).",
        "We use the following five schemes: AR, Dl, D2, D3 and TB.",
        "Definitions and contrastive examples of these schemes are presented in Table 1.",
        "To create these schemes, we use Mada, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and Tokan, a general Arabic tokenizer (Habash and",
        "Sadat, 2006)."
      ]
    },
    {
      "heading": "4. Preprocessing Schemes for Alignment",
      "text": [
        "Using a preprocessing scheme for word alignment breaks the process of applying Giza++ (Och and Ney, 2003) on some parallel text into three steps: preprocessing, alignment and remapping.",
        "In preprocessing, the words are tokenized into smaller units.",
        "Then, they are passed along to Giza++ for alignment (default settings).",
        "Finally, the Giza++ alignments are mapped back (remapped) to the original word form which is AR tokens in this work.",
        "For instance, take the first word in Table 1, wsyktbhA; if the D3 preprocesssing scheme is applied to it before alignment, it is turned into four tokens (w+ s+ yktb +hA).",
        "Giza++ will link these tokens to different words on the English side.",
        "In the remapping step, the union of these links is assigned to the original word wsyk-tbhA.",
        "We refer to such alignments as remappings."
      ]
    },
    {
      "heading": "5. Alignment Combination",
      "text": [
        "After creating the multiple remappings, we pass them as features into an alignment combiner.",
        "The combiner is also given a variety of additional features, which we discuss later in this section.",
        "The combiner is simply a binary classifier that determines for each source-target pair whether they are linked or not.",
        "Given the large size of the data used, we use a simplifying heuristic that allows us to minimize the number of source-target pairs used in training.",
        "Only links evidenced by at least one of the initial alignments and their immediate neighbors are included.",
        "All other links are considered non-existent.",
        "The combiner we use here is implemented using a rule-based classifier, Ripper (Cohen, 1996).",
        "The reasons we use Ripper as opposed other machine learning approaches are: (a) Ripper produces human readable rules that allow better understanding of the kind of decisions being made; and (b) Ripper is relatively fast compared to other machine learning approaches we examined given the very large nature of the training data we use.",
        "The combiner is trained using supervised data (human annotated alignments), which we discuss in Section 6.1.",
        "In the rest of this section we describe the different machine learning features given to the combiner.",
        "We break the combination features in two types: word/sentence level and remapping features.",
        "Word/Sentence Features:",
        "• Word Form: The source and target word forms.",
        "• POS: The source and target part-of-speech tags.",
        "• Location: The source and target relative sentence position (the ratio of absolute position to sentence length).",
        "We also use the difference between these values for both source and target.",
        "• Frequency: The source and target word frequency computed as the number of occurrences of the word form in training data.",
        "We also use the ratio of source to target frequency.",
        "Similarity: This feature is motivated by the fact that proper nouns in different languages often resemble each other, e.g. Cfr*s> ^.\\x*& 'SdAm Hsyn' and 'saddam hussein'.",
        "We use the equivalence classes proposed by Freeman et al.",
        "(2006) to normalize Arabic and English word forms.",
        "Then, we employ the longest common substring as a similarity measure.",
        "Remapping Features:",
        "• Link: for each source-target link, we include (a) a binary value indicating whether the link exists according to each remapping; (b) a cumulative sum of the different remappings supporting this link; and (c) co-occurrence information for this link.",
        "This last value is calculated for each source-target word pair as a weighted average of the product of the relative frequency of co-occurrence in both directions for each remapping.",
        "The weight assigned to each",
        "Preprocessing Scheme",
        "Example",
        "AR",
        "simple",
        "wsyktbhA",
        "Dl",
        "split conj",
        "w+ syktbhA",
        "D2",
        "split conj, PART",
        "w+ s+ yktbhA",
        "TB",
        "Arabic Treebank",
        "w+ syktb +hA",
        "D3",
        "split all clitics",
        "w+ s+ yktb +hA",
        "remapping is computed empirically.• Neighbor: The same information as Link, but for each of the immediate neighbors of the current link.",
        "• Cross: These include (a) the number of source words linked to the current target word, the same for target to source, and the number of words linked to either of the current words; and (b) the ratio of the co-occurrence mass placed in this link to the total mass assigned to the source word, the same for the target word and the union of both."
      ]
    },
    {
      "heading": "6. Evaluation",
      "text": [
        "The gold standard alignments we use here are part of the IBM Arabic-English aligned corpus (IBMAC)(Ittycheriah and Roukos, 2005).",
        "We only use 8.8K sentences from IBMAC because the rest (smaller portion) of the corpus uses different normalizations for numerals that make the two sets incompatible.",
        "We break this data into 6.6K sentences for training and 2.2K sentences for development.",
        "As for test data, we use the IBMAC's test set: NIST MTEval 2003 (663 Arabic sentences each human aligned to four English references).",
        "To get initial Giza++ alignments, we use a larger parallel corpus together with the annotated set.",
        "The Arabic-English parallel corpus has about 5 million words.",
        "The Arabic text in IBMAC is preprocessed in the AR preprocessing scheme with some additional character normalizations.",
        "We match the preprocessing and normalizations on our additional data to that of IBMAC's Arabic and English preprocessing (Ittycheriah and Roukos, 2005).",
        "The standard evaluation metric within word alignment is the Alignment Error Rate (AER) (Och and Ney, 2000), which requires gold alignments that are marked as 'sure' or 'probable'.",
        "Since the IBMAC gold alignments we use are not marked as such, AER reduces to 1 - F-score (Ittycheriah and Roukos, where A links are proposed and S links are gold.",
        "NULL links are not included in the evaluation (Ayan, 2005; Ittycheriah and Roukos, 2005).",
        "We conducted three experiments on our development data: (a) to assess the contribution of alignment remapping, (b) to assess the contribution of combination features for a single alignment (i.e., independent of the combination task) and (c) to determine the best performing combination of alignment remappings.",
        "Experiments (b) and (c) used only 2.2K of the gold alignment training data to minimize computation time.",
        "As for our test data experiment, we use our best system with all ofthe available data.",
        "We also present an error analysis of our best system.",
        "The baseline we measure against in all of these experiments is the state-of-the-art grow-diag-final (gdf) alignment refinement heuristic commonly used in phrase-based SMT (Koehn et al., 2003).",
        "This heuristic adds links to the intersection of two asymmetrical statistical alignments in an attempt to assign every word a link.",
        "The AER of this baseline is 24.77%.",
        "The Contribution of Alignment Remapping We experimented with five alignment remappings in two directions: dir (Ar-En) and inv (En-Ar).",
        "We also constructed their corresponding gdfalignment.",
        "The more verbose a preprocessing scheme, the lower the AER for either direction and for gdfof the corresponding remapping.",
        "The order of the schemes from worst to best is AR, Dl, D2, TB and D3.",
        "The best result we obtained through remapping is that of D3gclf which had a 20.45% AER (17.4% relative decrease from the baseline).",
        "The Contribution of Combination Features For each of the basic ten (non gdf) alignment remap-pings, we trained a version of the combiner that uses all the relevant features but has access to one alignment at a time.",
        "We saw a substantial improvement for all alignment remappings averaging 29.9% relative decrease in AER against the basic remapped version.",
        "The range of AER values is from 14.5% (D3dir) to 20.79% (ARmv).",
        "Alignment Combination Experiments To determine the best subset of alignment remappings to combine, we ordered the alignments given their AER performance in the last experiment described (using combination features).",
        "Starting with the best performer (D3dir), we continued adding alignments in the order of their performance so long the combination's AER score is decreased.",
        "Our best combination results are listed in Table 2.",
        "All additional alignments not listed in this table caused an increase in AER.",
        "The best alignment combination used alignments from four different schemes which confirms our intuition that such combination is useful.",
        "Test Set Evaluation We ran our best system trained on all of the IBMAC data (training & development), on all the unseen IBMAC test set.",
        "On this data we achieve a substantial relative improvement of 38.3% from an AER of 22.99 to 14.19.",
        "Ittycheriah and Roukos (2005) used only the top 50 sentences in IBMAC test data.",
        "Our best AER result on their test set is 14.02% (baseline is 22.48%) which is higher than their reported result (12.2% with 20.5% baseline (unrefined GIZA++)).",
        "The two results are not comparable because: (a) Ittycheriah and Roukos (2005) used additional gold aligned data that was not released and (b) they use an additional 500K sentences from the LDC UN corpus for Giza training that was created by adapting to the source side of the test set - the details of such adaptation were not provided and thus it is not clear how to replicate them to compare fairly.",
        "Clearly this additional data is helpful since even their baseline is higher than ours.",
        "Error Analysis We conducted error analysis on 50 sentences from our development set.",
        "The majority of the errors involved high frequency closed-class words (54%) and complex phrases (non-compositional or divergent translations) (23%).",
        "Both kinds of errors could be partly addressed by introducing phrasal constraints which are currently lacking in our system.",
        "Orthogonally, about 18% of all errors involved gold-standard inconsistencies and errors.",
        "These gold errors are split equally between closed-class and complex-phrase errors."
      ]
    },
    {
      "heading": "7. Conclusion and Future Plans",
      "text": [
        "We have presented an approach for using and combining multiple alignments created using different preprocessing schemes.",
        "We have shown a relative reduction of AER of about 38% on a blind test set.",
        "In the future, we plan to extend our system with additional models at the phrase and multi-word levels for both alignment and alignment combination improvement.",
        "We plan to use more sophisticated machine learning models such as support vector machines for combination and make use of more available parallel data.",
        "We also plan to evaluate the influence of our alignment improvement on MT quality.",
        "Alignment Remapping combination",
        "AER",
        "14.50",
        "14.12",
        "D^dir Dldir D'iinv",
        "12.81",
        "D^dirD^dirD^i^DXdir",
        "12.75",
        "-D 3 dir D 2 dir -D3inv D1 dir ^ira",
        "12.69"
      ]
    }
  ]
}
