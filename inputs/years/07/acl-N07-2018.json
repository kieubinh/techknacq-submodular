{
  "info": {
    "authors": [
      "Fang Huang",
      "Yorick Wilks"
    ],
    "book": "Human Language Technologies 2007: the Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",
    "id": "acl-N07-2018",
    "title": "Clustered Sub-Matrix Singular Value Decomposition",
    "url": "https://aclweb.org/anthology/N07-2018",
    "year": 2007
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Clustered Sub-matrix Singular Value Decomposition",
        "This paper presents an alternative algorithm based on the singular value decomposition (SVD) that creates vector representation for linguistic units with reduced dimensionality.",
        "The work was motivated by an application aimed to represent text segments for further processing in a multi-document summarization system.",
        "The algorithm tries to compensate for SVD's bias towards dominant-topic documents.",
        "Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the baseline algorithms - the SVD and the vector space model."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "We present, in this paper, an alternative algorithm called Clustered Sub-matrix Singular Value Decom-position(CSSVD) algorithm, which applied clustering techniques before basis vector calculation in SVD (Golub and Loan, 1996).",
        "The work was motivated by an application aimed to provide vector representation for terms and text segments in a document collection.",
        "These vector representations were then used for further preprocessing in a multi-document summarization system.",
        "The SVD is an orthogonal decomposition technique closely related to eigenvector decomposition and factor analysis.",
        "It is commonly used in information retrieval as well as language analysis applications.",
        "In SVD, a real m-by-n matrix A is decomposed into three matrices, A = U ^ VT. Y is an m-by-n matrix such that the singular value ai=Yii is the square root of the ith largest eigenvalue of AAT, and Yij =0 for i = j.",
        "Columns of orthogonal matrices U and V define the orthonormal eigenvectors associated with eigenvalues of AAT and ATA, respectively.",
        "Zeroing out all but the k, k < rank(A), largest singular values yields Ak = Yi=\\ (JiuivlT, which is the closest rank-k matrix to A.",
        "Let A be a term-document matrix.",
        "Applications such as latent semantic indexing (Deerwester et al., 1990) apply the rank-k approximation to the original matrix A, which corresponds to projecting A onto the k-dimension subspace spanned by u\\,u2, ...,Uk.",
        "Because k << m, in this k-dimension space, minor terms are ignored, so that terms are not independent as they are in the traditional vector space model.",
        "This allows semantically related documents to be related to each other even though they may not share terms.",
        "However, SVD tends to wipe out outlier (minority-class) documents as well as minor terms (Ando, 2000).",
        "Consequently, topics underlying outlier documents tend to be lost.",
        "In applications such as multi-document summarization, a set of related documents are used as the information source.",
        "Typically, the documents describe one broad topic from several different view points or sub-topics.",
        "It is important for each of the subtopics underlying the document collection to be represented well.",
        "Based on the above consideration, we propose the CSSVD algorithm with the intention of compensating for SVD's tendency to wipe out minor topics.",
        "The basic idea is to group the documents into a set of clusters using clustering algorithms.",
        "The SVD is then applied on each of the document clusters.",
        "The algorithm thus selects basis vectors by treating equally each of the topics.",
        "Our experiments on measuring document similarities have shown that the algorithm achieves higher average precision with lower number of dimensions than the SVD."
      ]
    },
    {
      "heading": "2. the Algorithm",
      "text": [
        "The input to the CSSVD algorithm is an mxn term-document matrix A.",
        "Documents in matrix A are grouped into a set of document clusters.",
        "Here, we adopt single-link algorithm to develop the initial clusters, then use K-means method to refine the clusters.",
        "After clustering, columns in matrix A are partitioned and regrouped into a set of sub-matrices Aq.",
        "Each of these matrices represents a document cluster.",
        "Assume Aj, 1 < i < q, is an m x nj matrix, these sub-matrices are ranked in decreasing order of their sizes, i.e., n1 > n2 > ... > nq, then n1 + n2 + ... + nq = n.",
        "The algorithm computes basis vectors as follows: the first basis vector u1 is computed from A1, i.e., the first left singular vector of A1 is selected.",
        "In order to ensure that the basis vectors are orthogonal, singular vectors are actually computed on residual matrices.",
        "Rj, the residual matrix of Ai after the selection of basis vectors u1, u2,..., Uj, is defined as",
        "proj (Ajj) j = 0 otherwise",
        "where, proj(Aij) is the orthogonal projection of the document vectors in Ai onto the span ofu1,u2,...,uj, i.e.,",
        "proj(Ajj) = Uk ul Aj k=1",
        "the residual matrix of Ai describes how much the document vectors in Ai are excluded from the proposed basis vectors u1, u2,..., uj.",
        "For the first basis vector computation, residual matrices are initialized as original sub-matrices.",
        "The computation of the residual matrix makes the remaining vectors perpendicular to the previous basis vectors, thus ensures that the basis vectors are orthogonal, as the eigenvector computed next is a linear combination of the remaining vectors.",
        "After calculating a basis vector, the algorithm judges whether the sub-matrices have been well represented by the derived basis vectors.",
        "The residual ratio was defined as a criterion for this judgement, where Rij is the residual matrix of Ai after j basis vectors have been selected; nj is the number of the documents in matrix Aj; kj is the number of singular vectors that have been selected from matrix Aj.",
        "Residual ratios of each sub-matrix are calculated.",
        "The sub-matrix with the largest residual ratio is assumed to be the one that contains the most information that has not been represented by the previous chosen basis vectors.",
        "The first left singular vector of this sub-matrix is computed and selected as the next basis vector.",
        "As described above, the computation of a basis vector uses the corresponding residual matrix.",
        "Once a basis vector is selected, its influence from each sub-matrix is subtracted.",
        "The procedure is repeated until an expected number of basis vectors have been chosen.",
        "The pseudo-code of the algorithm for semantic space construction is shown as follows: 1.",
        "Partition A into matrices A1,...,Aq corresponding to document clusters, where Aj , 1 < i < q, is an",
        "m x nj (n1 > n2 > ... > nq) matrix."
      ]
    },
    {
      "heading": "4.. u r = the first unit eigenvector of R j R l ;",
      "text": [
        "9.",
        "If rrj < threshold then stop else goto step 4.",
        "For the single-link algorithm used in the CSSVD, we use a threshold 0.2 and cosine measure to calculate the similarity between two clusters in our experiments.",
        "The performance of the CSSVD is also relative to the number of dimensions of the created subspace.",
        "As described above, the algorithm uses the residual ratio as a stopping criterion for the basis vector computation.",
        "In each iteration, after a basis vector is created, the residual ratio is compared to a threshold.",
        "Once the residual ratio of each sub-matrix fell below a certain threshold, the process of basis-vector selection is finished.",
        "In our experiments, the threshold was trained on corpus.",
        "After all the k basis vectors are chosen, a term-document vector di can be converted to dk, a vector in the k-dimensional space, by multiplying the matrix of basis vectors following the standard method of orthogonal transformation,i.e., dk =",
        "[Ul ,U2, ...,Uk ]T di."
      ]
    },
    {
      "heading": "3. Evaluation",
      "text": [
        "For the evaluation of the algorithm, 38 topics from the Text REtrieval Conference (TREC) collections were used in our experiments.",
        "These topics include foreign minorities, behavioral genetics, steel production, etc.",
        "We deleted documents relevant to more than one topic so that each document is related only to one topic.",
        "The total number of documents used was 2962.",
        "These documents were split into two disjoint groups, called 'pool 1' and 'pool 2'.",
        "The number of documents in 'pool 1' and 'pool 2' were 1453 and 1509, respectively.",
        "Each of the two groups used 19 topics.",
        "We generated training and testing data by simulating the result obtained by a query search.",
        "This simulation is further simplified by selecting documents containing same keywords from each document group.",
        "Thirty document sets were generated from each of the two document groups, i.e. 60 document sets in total.",
        "The number of documents for each set ranges from 51 to 582 with an average of 128; the number of topics ranges from 5 to 19 with an average of 12.",
        "Due to the limited number of the document sets we created, these sets were used both for training and evaluation.",
        "For the evaluation of the documents sets from 'pool 1', 'pool 2' was used for training, and vice versa.",
        "To construct the original term-document matrix, the following operations were performed on each of the documents: 1) filtering out all non-text tags in the documents; 2) converting all the characters into lower case; 3) removing stop words - a stoplist containing 319 words was used; and 4) term indexing - the tf.idf scheme was used to calculate a term's weight in a document.",
        "Finally, a document set is represented as a matrix A = [ajj], where ajj denotes the normalized weight assigned to term i in document j.",
        "Our algorithm was motivated by a multi-document summarization application which is mainly based on measuring the similarities and differences among text segments.",
        "Therefore, the basic requisite is to accurately measure similarities among texts.",
        "Based on this consideration, we used the CSSVD algorithm to create the document vectors in a reduced space for each of the document sets; cosine similarities among these document vectors were computed; and the results were then compared with the TREC relevance judgments.",
        "As each of the TREC documents we used has one specific topic.",
        "Assume that similarity should be higher for any document pair relevant to the same topic than for any pair relevant to different topics.",
        "The algorithm's accuracy for measuring the similarities among documents was evaluated using average precision taken at various recall levels (Har-man, 1995).",
        "Let pj denote the document pair that has the ith largest similarity value among all pairs of documents in the document set.",
        "The precision for an intra-topic pair pk is calculated by where pj is an intra-topic pair.",
        "The average of the precision values over all intra-topic pairs is computed as the average precision.",
        "The algorithms are evaluated by the average precision over 60 document sets.",
        "In order to make a comparison, two baseline algorithms besides CSSVD are evaluated.",
        "One is the vector space model (VSM) without dimension reduction.",
        "The other is SVD taking the left singular vectors as the basis vectors.",
        "To treat the selection of dimensions as a separate issue, we first evaluate the algorithms in terms of the best average precision.",
        "The 'best average precision' means the best over all the possible numbers of dimensions.",
        "The second row of Table 1 shows the best average precision of our algorithm, VSM, and SVD.",
        "The best average precision on average over 60 document sets of CSSVD is 69.6%, which is 11.5% higher than VSM and 6.1% higher than SVD.",
        "In the experiments, we observed that the CSSVD algorithm obtained its best performance with the number of dimensions lower than that of SVD.",
        "The Dimensional Ratio (DR) is defined as the number of dimensions of the derived subspace compared with the dimension number of the original space, i.e.,",
        "# of dimensions in derived space # of dimensions in original space",
        "The average dimensional ratio is calculated over all the 60 document sets.",
        "As the algorithms' computational efficiency is dependent on the number of dimensions computed, our interest is in getting good performance with an average dimensional ratio as low as possible.",
        "The third row of Table 1 shows the average dimensional ratio that yielded the best average precision.",
        "The average dimensional ratio that CSSVD yielded the best average precision is 32.1%, which is 22.3% lower than that of SVD.",
        "Thus, our algorithm has the advantage of being computationally inexpensive, assuming that we can find the optimal number of dimensions.",
        "The bottom row of Table 1 shows the average precision of the algorithms.",
        "The threshold used in CSSVD algorithm was trained on corpus.",
        "Let p be the threshold on residual ratio that yielded the best average precision on the training data.",
        "The value of p is then used as the threshold on the evaluation data.",
        "For the SVD algorithm, the average dimensional ratio that yielded the best average precision on training data was used as the dimensional ratio to determine the subspace dimensionality in evaluation.",
        "The performance shown here are the average of average precision over 60 document sets.",
        "Again, the CSSVD achieves the best performance, which is 7.3% higher than the performance of SVD and 8.7% higher than VSM."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "We have presented an alternative algorithm, the CSSVD, that creates vector representation for linguistic units with reduced dimensionality.",
        "The algorithm aims to compensate for SVD's bias towards dominant-topic documents by grouping documents into clusters and selecting basis vectors from each of the clusters.",
        "It introduces a threshold on the residual ratio of clusters as a stopping criterion of basis vector selection.",
        "It thus treats each topic underlying the document collection equally while focuses on the dominant documents in each topic.",
        "The preliminary experiments on measuring document similarities have shown that the CSSVD achieves higher average precision with lower number of dimensions than the baseline algorithms.",
        "Motivated by a multi-document summarization application, the CSSVD algorithm's emphasis on topics and dominant information within each topic meets the general demand of summarization.",
        "We expect that the algorithm fits the task of summarization better than SVD.",
        "Our future work will focus on more thorough evaluation of the algorithm and integrating it into a summarization system."
      ]
    },
    {
      "heading": "5. Acknowledgments",
      "text": [
        "We would like to thank Mark Sanderson, Horacio Saggion, and Robert Gaizauskas for helpful comments at the beginning of this research.",
        "measure",
        "VSM",
        "SVD",
        "CSSVD",
        "best average precision (%)",
        "58.1",
        "63.5",
        "69.6",
        "average DR (%)",
        "N/A",
        "54.4",
        "32.1",
        "average precision (%)",
        "58.1",
        "59.5",
        "66.8"
      ]
    }
  ]
}
