{
  "info": {
    "authors": [
      "Sarvnaz Karimi",
      "Falk Scholer",
      "Andrew Turpin"
    ],
    "book": "45th Annual Meeting of the Association of Computational Linguistics",
    "id": "acl-P07-1082",
    "title": "Collapsed Consonant and Vowel Models: New Approaches for English-Persian Transliteration and Back-Transliteration",
    "url": "https://aclweb.org/anthology/P07-1082",
    "year": 2007
  },
  "references": [
    "acl-C00-1056",
    "acl-C02-1099",
    "acl-J03-1002",
    "acl-J93-2003",
    "acl-J96-4002",
    "acl-J98-4003",
    "acl-P04-1021",
    "acl-P06-1103",
    "acl-W03-1508",
    "acl-W06-1672"
  ],
  "sections": [
    {
      "text": [
        "Sarvnaz Karimi Falk Scholer Andrew Turpin",
        "We propose a novel algorithm for English to Persian transliteration.",
        "Previous methods proposed for this language pair apply a word alignment tool for training.",
        "By contrast, we introduce an alignment algorithm particularly designed for transliteration.",
        "Our new model improves the English to Persian transliteration accuracy by 14% over an n-gram baseline.",
        "We also propose a novel back-transliteration method for this language pair, a previously unstudied problem.",
        "Experimental results demonstrate that our algorithm leads to an absolute improvement of 25% over standard transliteration approaches."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Translation of a text from a source language to a target language requires dealing with technical terms and proper names.",
        "These occur in almost any text, but rarely appear in bilingual dictionaries.",
        "The solution is the transliteration of such out-of-dictionary terms: a word from the source language is transformed to a word in the target language, preserving its pronunciation.",
        "Recovering the original word from the transliterated target is called backtransliteration.",
        "Automatic transliteration is important for many different applications, including machine translation, cross-lingual information retrieval and cross-lingual question answering.",
        "Transliteration methods can be categorized into grapheme-based (AbdulJaleel and Larkey, 2003; Li et al., 2004), phoneme-based (Knight and Graehl, 1998; Jung et al., 2000), and combined (Bilac and Tanaka, 2005) approaches.",
        "Grapheme-based methods perform a direct orthographical mapping between source and target words, while phoneme-based approaches use an intermediate phonetic representation.",
        "Both grapheme-or phoneme-based methods usually begin by breaking the source word into segments, and then use a source segment to target segment mapping to generate the target word.",
        "The rules of this mapping are obtained by aligning already available transliterated word pairs (training data); alternatively, such rules can be handcrafted.",
        "From this perspective, past work is roughly divided into those methods which apply a word alignment tool such as giza++ (Och and Ney, 2003), and approaches that combine the alignment step into their main transliteration process.",
        "Transliteration is language dependent, and methods that are effective for one language pair may not work as well for another.",
        "In this paper, we investigate the English-Persian transliteration problem.",
        "Persian (Farsi) is an Indo-European language, written in Arabic script from right to left, but with an extended alphabet and different pronunciation from Arabic.",
        "Our previous approach to English-Persian transliteration introduced the grapheme-based collapsed-vowel method, employing giza++ for source to target alignment (Karimi et al., 2006).",
        "We propose a new transliteration approach that extends the collapsed-vowel method.",
        "To meet Persian language transliteration requirements, we also propose a novel alignment algorithm in our training stage, which makes use of statistical information of the corpus, transliteration specifications, and simple language properties.",
        "This approach handles possible consequences of elision (omission of sounds to make the word easier to read) and epenthesis (adding extra sounds to a word to make it fluent) in written target words that happen due to the change of language.",
        "Our method shows an absolute accuracy improvement of 14.2% over an n-gram baseline.",
        "In addition, we investigate the problem of backtransliteration from Persian to English.",
        "To our knowledge, this is the first report of such a study.",
        "There are two challenges in Persian to English transliteration that makes it particularly difficult.",
        "First, written Persian omits short vowels, while only long vowels appear in texts.",
        "Second, monophthon-gization (changing diphthongs to monophthongs) is popular among Persian speakers when adapting foreign words into their language.",
        "To take these into account, we propose a novel method to form transformation rules by changing the normal segmentation algorithm.",
        "We find that this method significantly improves the Persian to English transliteration effectiveness, demonstrating an absolute performance gain of 25.1% over standard transliteration approaches."
      ]
    },
    {
      "heading": "2. Background",
      "text": [
        "In general, transliteration consists ofa training stage (running on a bilingual training corpus), and a generation - also called testing - stage.",
        "The training step of a transliteration develops transformation rules mapping characters in the source to characters in the target language using knowledge of corresponding characters in transliterated pairs provided by an alignment.",
        "For example, for the source-target word pair (pat, ), an alignment may map \"p\" to \">_>\" and \"a\" to \"!",
        "\", and the training stage may develop the rule pa – !, with \"!\"",
        "as the transliteration of \"a\" in the context of \"pa\".",
        "The generation stage applies these rules on a segmented source word, transforming it to a word in the target language.",
        "Previous work on transliteration either employs a word alignment tool (usually giza++), or develops specific alignment strategies.",
        "Transliteration methods that use giza++ as their word pair aligner (Ab-dulJaleel and Larkey, 2003; Virga and Khudanpur, 2003; Karimi et al., 2006) have based their work on the assumption that the provided alignments are reliable.",
        "Gao et al.",
        "(2004) argue that precise alignment can improve transliteration effectiveness, experimenting on English-Chinese data and comparing IBM models (Brown et al., 1993) with phoneme-based alignments using direct probabilities.",
        "Other transliteration systems focus on alignment for transliteration, for example the joint source-channel model suggested by Li et al.",
        "(2004).",
        "Their method outperforms the noisy channel model in direct orthographical mapping for English-Chinese transliteration.",
        "Li et al.",
        "also find that grapheme-based methods that use the joint source-channel model are more effective than phoneme-based methods due to removing the intermediate phonetic transformation step.",
        "Alignment has also been investigated for transliteration by adopting Coving-ton's algorithm on cognate identification (Coving-ton, 1996); this is a character alignment algorithm based on matching or skipping of characters, with a manually assigned cost of association.",
        "Coving-ton considers consonant to consonant and vowel to vowel correspondence more valid than consonant to vowel.",
        "Kang and Choi (2000) revise this method for transliteration where a skip is defined as inserting a null in the target string when two characters do not match based on their phonetic similarities or their consonant and vowel nature.",
        "Oh and Choi (2002) revise this method by introducing binding, in which many to many correspondences are allowed.",
        "However, all of these approaches rely on the manually assigned penalties that need to be defined for each possible matching.",
        "In addition, some recent studies investigate discriminative transliteration methods (Klementiev and Roth, 2006; Zelenko and Aone, 2006) in which each segment of the source can be aligned to each segment of the target, where some restrictive conditions based on the distance of the segments and phonetic similarities are applied."
      ]
    },
    {
      "heading": "3. The Proposed Alignment Approach",
      "text": [
        "We propose an alignment method based on segment occurrence frequencies, thereby avoiding predefined matching patterns and penalty assignments.",
        "We also apply the observed tendency of aligning consonants to consonants, and vowels to vowels, as a substitute for phonetic similarities.",
        "Many to many, one to many, one to null and many to one alignments can be generated.",
        "Our alignment approach consists of two steps: the first is based on the consonant and vowel nature of the word's letters, while the second uses a frequency-based sequential search.",
        "Definition 1 A bilingual corpus B is the set {(S,T)}, where S = s\\..s£, T = t\\..tm, Si is a letter in the source language alphabet, and tj is a letter in the target language alphabet.",
        "Definition 2 Given some word, w, the consonant-vowel sequence p = (C\\V)+for w is obtained by replacing each consonant with C and each vowel with V.",
        "Definition 3 Given some consonant-vowel sequence, p, a reduced consonant-vowel sequence q replaces all runs of C's with C, and all runs of V's with V; hence q = q'\\q\", q' = V(CV)*(C\\e) and q\" = C(VC)*(V\\e).",
        "Foreachnatural language word, we candetermine the consonant-vowel sequence (p) from which the reduced consonant-vowel sequence (q) can be derived, giving a common notation between two different languages, no matter which script either of them use.",
        "To simplify, semi-vowels and approxi-mants (sounds intermediate between consonants and vowels, such as \"w\" and \"y\" in English) are treated according to their target language counterparts.",
        "In general, for all the word pairs (S, T) in a corpus B, an alignment can be achieved using the function f : B – A; (S,T) – (S,T,r).",
        "The function f maps the word pair (S, T) e B to the triple (S, T,r) e A where S and T are substrings of S and T respectively.",
        "The frequency of this correspondence is denoted by r. A represents a set of substring alignments, and we use a per word alignment notation of ae2p when aligning English to Persian and ap2e for Persian to English.",
        "Our algorithm consists of two steps.",
        "Step 1 (Consonant-Vowel based)",
        "For any word pair (S, T) e B, the corresponding reduced consonant-vowel sequences, qS and qT, are generated.",
        "If the sequences match, then the aligned consonant clusters and vowel sequences are added to the alignment set A.",
        "If qS does not match with qT, the word pair remains unaligned in Step 1.",
        "The assumption in this step is that transliteration of each vowel sequence of the source is a vowel sequence in the target language, and similarly for consonants.",
        "However, consonants do not always map to consonants, or vowels to vowels (for example, the English letter \"s\" may be written as \" \" in Persian which consists of one vowel and one consonant).",
        "Alternatively, they might be omitted altogether, which can be specified as the null string, e. We therefore require a second step.",
        "Step 2 (Frequency based)",
        "For most natural languages, the maximum length of corresponding phonemes of each grapheme is a digraph (two letters) or at most a trigraph.",
        "Hence, alignment can be defined as a search problem that seeks for units with a maximum length of two or three in both strings that need to be aligned.",
        "In our approach, we search based on statistical occurrence data available from Step 1.",
        "In Step 2, only those words that remain unaligned at the end of Step 1 need to be considered.",
        "For each pair of words (S, T), matching proceeds from left to right, examining one of the three possible options of transliteration: single letter to single letter, digraph to single letter and single letter to digraph.",
        "Trigraphs are unnecessary in alignment as they can be effectively captured during transliteration generation, as we explain below.",
        "We define four different valid alignments for the source (S = sxs2 ...si ...st) and target (T = ti t2 ...tj ...tm) strings: (s4 ,tj, r), (s4 si+i, tj, r), (si ,tj tj+1}r) and (st ,e, r).",
        "These four options are considered as the only possible valid alignments, and the most frequently occurring alignment (highest r) is chosen.",
        "These frequencies are dynamically updated after successfully aligning a pair.",
        "For exceptional situations, where there is no character in the target string to match with the source character si, it is aligned with the empty string.",
        "It is possible that none of the four valid alignment options have occurred previously (that is, r = 0 for each).",
        "This situation can arise in two ways: first, such a tuple may simply not have occurred in the training data; and, second, the previous alignment in the current string pair may have been incorrect.",
        "To account for this second possibility, a partial backtracking is considered.",
        "Most misalignments are derived from the simultaneous comparison of alignment possibilities, giving the highest priority to the most frequent.",
        "For example if S=bbc, T=i_jkj* and A = {(b,v,100),(bb,sj,40),(c,tJ*,60)}, starting from the initial position s1 and t1, the first alignment choice is (b,>_>,101).",
        "However immediately after, we face the problem of aligning the second \"b\".",
        "There are two solutions: inserting e and adding the triple (b,e,1), or backtracking the previous alignment and substituting that with the less frequent but possible alignment of (bb, ,41).",
        "The second solution is a better choice as it adds less ambiguous alignments containing e. At the end, the alignment set is updated as A = {(b,v,100),(bb,v,41),(c,u*,61)}.",
        "In case of equal frequencies, we check possible subsequent alignments to decide on which alignment should be chosen.",
        "For example, if (b, ,100) and (bb, ,100) both exist as possible options, we consider if choosing the former leads to a subsequent e insertion.",
        "If so, we opt for the latter.",
        "At the end of a string, if just one character in the target string remains unaligned while the last alignment is a e insertion, that final alignment will be substituted for e. This usually happens when the alignment of final characters is not yet registered in the alignment set, mainly because Persian speakers tend to transliterate the final vowels to consonants to preserve their existence in the word.",
        "For example, in the word \"Jose\" the final \"e\" might be transliterated to \" \" which is a consonant (\"h\") and therefore is not captured in Step 1.",
        "Backparsing",
        "The process of aligning words explained above can handle words with already known components in the alignment set A (the frequency of occurrence is greater than zero).",
        "However, when this is not the case, the system may repeatedly insert e while part or all of the target characters are left intact (unsuccessful alignment).",
        "In such cases, processing the source and target backwards helps to find the problematic substrings: backparsing.",
        "The poorly aligned substrings of the source and target are taken as new pairs of strings, which are then reintroduced into the system as new entries.",
        "Note that they themselves are not subject to back-parsing.",
        "Most strings of repeating nulls can be broken up this way, and in the worst case will remain as one tuple in the alignment set.",
        "To clarify, consider the example given in Figure 1.",
        "For the word pair (patricia, ), where an association between \"c\" and \" \" is not yet registered.",
        "Forward parsing, as shown in the figure, does not resolve all target characters; after the incorrect alignment of \"c\" with \"e\", subsequent characters are also aligned with null, and the substring \" \" remains intact.",
        "Backward parsing, shown in the next line of the figure, is also not successful.",
        "It is able to correctly align the last two characters of the string, before generating repeated null alignments.",
        "Therefore, the central region – substrings of the source and target which remained unaligned plus one extra aligned segment to the left and right – is entered as a new pair to the system (ici, ), as shown in the line labelled Input 2 in the figure.",
        "This new input meets Step 1 requirements, and is aligned successfully.",
        "The resulting tuples are then merged with the alignment set A.",
        "An advantage of our backparsing strategy is that it takes care of casual transliterations happening due to elision and epenthesis (adding or removing extra sounds).",
        "It is not only in translation that people may add extra words to make fluent target text; for transliteration also, it is possible that spurious characters are introduced for fluency.",
        "However, this often follows patterns, such as adding vowels to the target form.",
        "These irregularities are consistently covered in the backparsing strategy, where they remain connected to their previous character."
      ]
    },
    {
      "heading": "4. Transliteration Method",
      "text": [
        "Transliteration algorithms use aligned data (the output from the alignment process, ae2p or ap2e alignment tuples) for training to derive transformation rules.",
        "These rules are then used to generate a target word T given a new input source word S.",
        "Initial alignment set:",
        "Updated alignment set:",
        "Figure 1: A backparsing example.",
        "Note middle tuples in forward and backward parsings are not merged in A till the alignment is successfully completed.",
        "Figure 2: An example of transliteration for the word pair (shelley,^J^).",
        "Underlined characters are actually transliterated for each segment.",
        "Most transliteration methods reported in the literature – either grapheme-or phoneme-based – use n-grams (AbdulJaleel and Larkey, 2003; Jung et al., 2000).",
        "The n-gram-based methods differ mainly in the way that words are segmented, both for training and transliteration generation.",
        "A simple n-gram based method works only on single characters (unigram) and transformation rules are defined as si – tj, while an advanced method may take the surrounding context into account (Jung et al., 2000).",
        "We found that using one past symbol (bigram model) works better than other n-gram based methods for English to Persian transliteration (Karimi et al., 2006).",
        "Our collapsed-vowel methods consider language knowledge to improve the string segmentation of n-gram techniques (Karimi et al., 2006).",
        "The process begins by generating the consonant-vowel sequence (Definition 2) of a source word.",
        "For example, the word \"shelley\" is represented by the sequence p = CCVCCVV.",
        "Then, following the collapsed vowel concept (Definition 3), this sequence becomes \"CCVCCV\".",
        "These approaches, which we refer to as cv-model1 and cv-model2 respectively, partition these sequences using basic patterns (C and V) and main patterns (CC, CVC, VC and CV).",
        "In the training phase, transliteration rules are formed according to the boundaries of the defined patterns and their aligned counterparts (based on ae2p or ap2e) in the target language word T. Similar segmentation is applied during the transliteration generation stage.",
        "The restriction on the context length of consonants imposed by cv-model1 and cv-model2 makes the transliteration of consecutive consonants mapping to a particular character in the target language difficult.",
        "For example, \"ght\" in English maps to only one character in Persian: \" \".",
        "Dealing with languages which have different alphabets, and for which the number of characters in their alphabets also differs (such as 26 and 32 for English and Persian), increases the possibility of facing these cases, especially when moving from the language with smaller alphabet size to the one with a larger size.",
        "To more effectively address this, we propose a collapsed consonant and vowel method (cv-model3) which uses the full reduced sequence (Definition 3), rather than simply reduced vowel sequences.",
        "Although recognition of consonant segments is based on the vowel positions, consonants are considered as independent blocks in each string.",
        "Conversely, vowels are transliterated in the context of surrounding consonants, as demonstrated in the example below.",
        "Input:",
        "(patricia^Oj^^^t) qS = CVCVCV qT",
        "= CVCV",
        "Step 1:",
        "qS = qT",
        "Forward alignment:",
        "(p,y,43), (a,e,100), (t,o,52), (r,j,201), (i,^,61), (c",
        ",e,l), (i,e,6), (a,e,100)",
        "Backward alignment:",
        "(a, 1,321), (i,j,6\\\\ (c,e,l), (i,e,6), (r,e,l), (t,e,l), (a,",
        ",e,100), (p,e,l)",
        "Input 2:",
        "(ici,jjj) qs = vcv qT = VCV",
        "Step I:",
        "(i,^,61),(c,ai,l), (1,^,61)",
        "Method",
        "Intermediate Sequence",
        "Segment(Pattern)",
        "Backoff",
        "Bigram",
        "N/A",
        "#s, sh, he, el, 11, le, ey",
        "s,h,e,l,e,y",
        "CV-MODEL1",
        "CCVCCV",
        "shfCC), hel(CVC)7ll(CC), lley(CV)",
        "s(C), h(C), e(V), l(C),e(V),y(V)",
        "CV-MODEL2",
        "CCVCCV",
        "sh(CC), e(CVC), U(CC), ey(CV)",
        "As Above.",
        "CV-MODEL3",
        "CVCV",
        "#sh(C), e(CVC), 11(C), ey(CV)",
        "sh(C), s(C), h(C), e(V), 1(C), e(V), y(V)",
        "A special symbol is used to indicate the start and/or end of each word if the beginning and end of the word is a consonant respectively.",
        "Therefore, for the words starting or ending with consonants, the symbol \"#\" is added, which is treated as a consonant and therefore grouped in the consonant segment.",
        "An example of applying this technique is shown in Figure 2 for the string \"shelley\".",
        "In this example, \"sh\" and \"ll\" are treated as two consonant segments, where the transliteration of individual characters inside a segment is dependent on the other members but not the surrounding segments.",
        "However, this is not the case for vowel sequences which incorporate a level of knowledge about any segment neighbours.",
        "Therefore, for the example \"shelley\", the first segment is \"sh\" which belongs to C pattern.",
        "During transliteration, if \"#sh\" does not appear in any existing rules, a backoff splits the segment to smaller segments: \"#\" and \"sh\", or \"s\"and \"h\".",
        "The second segment contains the vowel \"e\".",
        "Since this vowel is surrounded by consonants, the segment pattern is CVC.",
        "In this case, backoff only applies for vowels as consonants are supposed to be part of their own independent segments.",
        "That is, if search in the rules of pattern CVC was unsuccessful, it looks for \"e\" in V pattern.",
        "Similarly, segmentation for this word continues with \"ll\" in C pattern and \"ey\" in CV pattern (\"y\" is an approximant, and therefore considered as a vowel when transliterating English to Persian).",
        "Written Persian ignores short vowels, and only long vowels appear in text.",
        "This causes most English vowels to disappear when transliterating from English to Persian; hence, these vowels must be restored during back-transliteration.",
        "When the initial transliteration happens from English to Persian, the transliterator (whether human or machine) uses the rules of transliterating from English as the source language.",
        "Therefore, transliterating back to the original language should consider the original process, to avoid losing essential information.",
        "In terms of segmentation in collapsed-vowel models, different patterns define segment boundaries in which vowels are necessary clues.",
        "Although we do not have most of these vowels in the transliteration generation phase, it is possible to benefit from their existence in the training phase.",
        "For example, using cv-model3, the pair (^jd^merkel) with qS =C and ap2e=((f,me),(j,r),(J^e),(J,l)), produces just one transformation rule \" – merkel\" based on a C pattern.",
        "That is, the Persian string contains no vowel characters.",
        "If, during the transliteration generation phase, a source word \"JS^r*\" (S=^jJ\"i}) is entered, there would be one and only one output of \"merkel\", while an alternative such as \"mercle\" might be required instead.",
        "To avoid overfitting the system by long consonant clusters, we perform segmentation based on the English q sequence, but categorise the rules based on their Persian segment counterparts.",
        "That is, for the pair ( ,merkel) with – merk (C), – rkel (C).",
        "We call the suggested training approach reverse segmentation.",
        "Reverse segmentation avoids clustering all the consonants in one rule, since many English words might be transliterated to all-consonant Persian words.",
        "In the transliteration generation stage, the source word is segmented following the same process of segmenting words in training stage, and a probability is computed for each generated target word:",
        "where \\K| is the number of distinct source segments.",
        "P(Tk\\Sk) is the probability of the Sk-^Tktransformation rule, as obtained from the training stage:",
        "frequency of Sk – ► TTkfrequency of Sk",
        "where frequency of Skis the number of its occurrence in the transformation rules.",
        "We apply a tree structure, following Dijkstra's a-shortest path, to generate the a highest scoring (most probable) transliterations, ranked based on their probabilities."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "To investigate the effectiveness of cv-model3 and the new alignment approach on transliteration, we first compare cv-model3 with baseline systems, employing giza++ for alignment generation during system training.",
        "We then evaluate the same systems, using our new alignment approach.",
        "Backtransliteration is also investigated, applying both alignment systems and reverse segmentation.",
        "In all our experiments, we used tenfold cross-validation.",
        "The statistical significance of different performance levels are evaluated using a paired t-test.",
        "The notation top-X indicates the first X transliterations prodcued by the automatic methods.",
        "We used two corpora of word pairs in English and Persian: the first, called Large, contains 16,670 word pairs; the second, Small, contains 1,857 word pairs, and are described fully in our previous paper (Karimi et al., 2006).",
        "The results oftransliteration experiments are evaluated using word accuracy (Kang and Choi, 2000) which measures the proportion of transliterations that are correct out of the test corpus.",
        "The results of our experiments for transliterating English to Persian, using giza++ for alignment generation, are shown in Table 1. cv-model3 outperforms all three baseline systems significantly in top-1 and top-5 results, for both Persian corpora.",
        "top-1 results were improved by 9.2% to 16.2% (p<0.0001, paired t-test) relative to the baseline systems for the Small corpus.",
        "For the Large corpus, cv-model3 was 9.3% to 17.2% (p<0.0001) more accurate relative to the baseline systems.",
        "The results of applying our new alignment algorithm are presented in the last column of Table 1, comparing word accuracy of cv-model3 using giza++ and the new alignment for English to Persian transliteration.",
        "Transliteration accuracy increases in top 1 for both corpora (a relative increase of 7.1% (p=Q.QQ2) for the Small corpus and 8.1% (p< 0.0001) for the Large corpus).",
        "The top-10 results of the Large corpus again show a relative increase of 3.5% (p=0.004).",
        "Although the new alignment also increases the performance for top-5 and top-10 of the Small corpus, these increases are not statistically significant.",
        "The results of back-transliteration are shown in Table 2.",
        "We first consider performance improvements gained from using cv-model3: cv-model3 using giza++ outperforms Bigram, cv-model1 and cv-model2 by 12.8% to 40.7% (p<0.0001) in top-1 for the Small corpus.",
        "The corresponding improvement for the Large corpus is 12.8% to 74.2% (p<0.0001).",
        "The fifth column of the table shows the performance increase when using cv-model3 with the new alignment algorithm: for the Large corpus, the new alignment approach gives a relative increase in accuracy of 15.5% for top-5 (p<0.0001) and 10% for top-10 (p=Q.QQ5).",
        "The new alignment method does not show a significant difference using cv-model3 for the Small corpus.",
        "The final column of Table 2 shows the performance of the cv-model3 with the new reverse segmentation approach.",
        "Reverse segmentation leads to a significant improvement over the new alignment approach in top-1 results for the Small corpus by 40.1% (p<0.0001), and 49.4% (p<0.0001) for the Large corpus.",
        "corpus",
        "Baseline",
        "CV-MODEL3",
        "Bigram",
        "CV-MODELl",
        "CV-MODEL2",
        "GIZA++",
        "New Alignment",
        "Small corpus",
        "TOP-1",
        "58.0(2.2)",
        "61.7(3.0)",
        "60.0 (3.9)",
        "67.4(5.5)",
        "72.2 (2.2)",
        "TOP-5",
        "85 6 (3 4)",
        "80.9 (2.2)",
        "86.0 (2.8)",
        "90 9 (2 1)",
        "92.9 (1.6)",
        "TOP-10",
        "89.4(2.9)",
        "82.0 (2.1)",
        "91.2 (2.5)",
        "93.8 (2.1)",
        "93.5 (1.7)",
        "Large corpus",
        "TOP-1",
        "47 2 (1 0)",
        "50.6 (2.5)",
        "47.4(1.0)",
        "55 3 (0 8)",
        "59.8 (1.1)",
        "TOP-5",
        "77.6(l.4)",
        "79.8 (3.4)",
        "79 2 (1 0)",
        "84.5 (0.7)",
        "85.4(0.8)",
        "TOP-10",
        "83.3 (1.5)",
        "84.9 (3.1)",
        "87.0 (0.9)",
        "89.5 (0.4)",
        "92.6 (0.7)",
        "Table 2: comparison of mean (standard deviation) word accuracy (%) for Persian to English transliteration."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "We have presented a new algorithm for English to Persian transliteration, and a novel alignment algorithm applicable for transliteration.",
        "Our new transliteration method (cv-model3) outperforms the previous approaches for English to Persian, increasing word accuracy by a relative 9.2% to 17.2% (top- 1), when using giza++ for alignment in training.",
        "This method shows further 7.1% to 8.1% increase in word accuracy (top- 1) with our new alignment algorithm.",
        "Persian to English back-transliteration is also investigated, with cv-model3 significantly outperforming other methods.",
        "Enriching this model with a new reverse segmentation algorithm gives rise to further accuracy gains in comparison to directly applying English to Persian methods.",
        "In future work we will investigate whether phonetic information can help refine our cv-model3, and experiment with manually constructed rules as a baseline system."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported in part by the Australian government IPRS program (SK) and an ARc Discovery Project Grant (AT).",
        "corpus",
        "Bigram",
        "cv-MODEL1",
        "cv-MODEL2",
        "GIZA++",
        "CV-MODEL3",
        "New Alignment",
        "Reverse",
        "Small corpus",
        "TOP-1",
        "23.1 (2.0)",
        "28.8 (4.6)",
        "24.9 (2.8)",
        "32.5 (3.6)",
        "34.4(3.8)",
        "48.2 (2.9)",
        "TOP-5",
        "40 8 (3 1)",
        "51 0 (4 8)",
        "52.9 (3.4)",
        "56.0 (3.5)",
        "54 8 (3 7)",
        "68.1 (4.9)",
        "TOP-10",
        "50.1 (4.l)",
        "58.2(5.3)",
        "63.2 (3.l)",
        "64.2 (3.2)",
        "63.8 (3.6)",
        "75.7(4.2)",
        "Large corpus",
        "TOP-1",
        "10.1 (0.6)",
        "15.6(1.0)",
        "12.0 (1.0)",
        "17.6 (0.8)",
        "18.0(1.2)",
        "26.9 (0.7)",
        "TOP-5",
        "20.6(l.2)",
        "31 7 (0 9)",
        "28.0 (0.7)",
        "36.2 (0.5)",
        "41 8 (1 2)",
        "41.3 (l.7)",
        "TOP-10",
        "27.2(1.0)",
        "40.1 (1.1)",
        "37.4(0.8)",
        "46.0 (0.8)",
        "50.6(1.1)",
        "49.3 (1.6)"
      ]
    }
  ]
}
