{
  "info": {
    "authors": [
      "Surabhi Gupta",
      "Matthew Purver",
      "Daniel Jurafsky"
    ],
    "book": "45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    "id": "acl-P07-2027",
    "title": "Disambiguating Between Generic and Referential \"You\" in Dialog",
    "url": "https://aclweb.org/anthology/P07-2027",
    "year": 2007
  },
  "references": [
    "acl-E06-1007",
    "acl-E06-1022",
    "acl-H94-1020",
    "acl-J00-3003",
    "acl-W04-2319"
  ],
  "sections": [
    {
      "text": [
        "Disambiguating Between Generic and Referential \"You\" in Dialog*",
        "Surabhi Gupta Matthew Purver Dan Jurafsky",
        "Stanford University of Language and Information Stanford University",
        "Stanford, CA 94305, US Stanford University Stanford, CA 94305, US",
        "We describe an algorithm for a novel task: disambiguating the pronoun you in conversation.",
        "You can be generic or referential; finding referential you is important for tasks such as addressee identification or extracting 'owners' of action items.",
        "Our classifier achieves 84% accuracy in two-person conversations; an initial study shows promising performance even on more complex multi-party meetings."
      ]
    },
    {
      "heading": "1. Introduction and Background",
      "text": [
        "This paper describes an algorithm for disambiguating the generic and referential senses of the pronoun you.",
        "Our overall aim is the extraction of action items from multi-party human-human conversations, concrete decisions in which one (or more) individuals take on a group commitment to perform a given task (Purver et al., 2006).",
        "Besides identifying the task itself, it is crucial to determine the owner, or person responsible.",
        "Occasionally, the name of the responsible party is mentioned explicitly.",
        "More usually, the owner is addressed directly and therefore referred to using a second-person pronoun, as in example (1).",
        "It can also be important to distinguish between singular and plural reference, as in example (2) where the task is assigned to more than one person:",
        "A: So y-so you guys will send to the rest of us um a",
        "version of um, this, and - the - uh, description -B: With sugge- yeah, suggested improvements and -",
        "Use of \"you\" might therefore help us both in de-",
        "*This work was supported by the CALO project (DARPA grant NBCH-D-03-0010) and ONR (MURI award N000140510388).",
        "The authors also thank John Niekrasz for annotating our test data.",
        "'(1,2) are taken from the ICSI Meeting Corpus (Shriberg et al., 2004); (3,4) from Switchboard (Godfrey et al., 1992).",
        "tecting the fact that a task is being assigned, and in identifying the owner.",
        "While there is an increasing body of work concerning addressee identification (Katzenmaier et al., 2004; Jovanovic et al., 2006), there is very little investigating the problem of second-person pronoun resolution, and it is this that we address here.",
        "Most cases of \"you\" do not in fact refer to the addressee but are generic, as in example (3); automatic referentiality classification is therefore very important.",
        "B: Well, usually what you do is just wait until you think it's stopped, and then you patch them up."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Previous linguistic work has recognized that \"you\" is not always addressee-referring, differentiating between generic and referential uses (Holmes, 1998; Meyers, 1990) as well as idiomatic cases of \"you know\".",
        "For example, (Jurafsky et al., 2002) found that \"you know\" covered 47% of cases, the referential class 22%, and the generic class 27%, with no significant differences in surface form (duration or vowel reduction) between the different cases.",
        "While there seems to be no previous work investigating automatic classification, there is related work on classifying \"it\" , which also takes various referential and non-referential readings: (Muller, 2006) use lexical and syntactic features in a rule-based classifier to detect non-referential uses, achieving raw accuracies around 74-80% and F-scores 63-69%.",
        "We used the Switchboard corpus of two-party telephone conversations (Godfrey et al., 1992), and annotated the data with four classes: generic, referential singular, referential plural and a reported referential class, for mention in reported speech of an",
        "A: and um if you can get that binding point also maybe with a nice example that would be helpful for Johno and me.",
        "| B: OhyeahuhOJC.",
        "of \"you\".",
        "We also investigated using the presence of a question mark in the transcription as a feature, as a possible replacement for some dialog act features.",
        "Table 2 presents our features in detail.",
        "originally referential use (as the original addressee may not be the current addressee - see example (4)).",
        "We allowed a separate class for genuinely ambiguous cases.",
        "Switchboard explicitly tags \"you know\" when used as a discourse marker; as this (generic) case is common and seems trivial we removed it from our data.",
        "To test inter-annotator agreement, two people annotated 4 conversations, yielding 85 utterances containing \"you\" ; the task was reported to be easy, and the kappa was 100%.",
        "We then annotated a total of 42 conversations for training and 13 for testing.",
        "Different labelers annotated the training and test sets; none of the authors were involved in labeling the test set.",
        "Table 1 presents information about the number of instances of each of these classes found."
      ]
    },
    {
      "heading": "4. Features",
      "text": [
        "All features used for classifier experiments were extracted from the Switchboard LDC Treebank 3 release, which includes transcripts, part of speech information using the Penn tagset (Marcus et al., 1994) and dialog act tags (Jurafsky et al., 1997).",
        "Features fell into four main categories: sentential features which capture lexical features of the utterance itself; part-of-speech features which capture shallow syntactic patterns; dialog act features capturing the discourse function of the current utterance and surrounding context; and context features which give oracle information (i.e., the correct generic/referential label) about preceding uses",
        "Features Sentential Features (Sent) you, you know, you guys number of you, your, yourself you (say| said|tell |told| mention(ed)| mean(t)| sound(ed)) you (hear heard) (do|does|did|have|has|had|are|could|should|n't) you \"if you\" (which| what| where| when| how) you Part of Speech Features (POS) Dialog Act Features (DA)",
        "DA tag of current utterance i",
        "DA tag of previous utterance i – 1",
        "DA tag of utterance i – 2",
        "Presence of any question DA tag (Q_DA)",
        "Presence of elaboration DA tag Oracle Context Features (Ctxt) Class of utterance i – 1 Class of utterance i – 2 Class of previous utterance by same speaker Class of previous labeled utterance Other Features (QM)",
        "Question mark",
        "Table 2: Features investigated.",
        "N indicates the number of possible values (there are 46 DA tags; context features can be generic, referential or N/A)."
      ]
    },
    {
      "heading": "5. Experiments and Results",
      "text": [
        "As Table 1 shows, there are very few occurrences of the referential plural, reported referential and ambiguous classes.",
        "We therefore decided to model our problem as a two way classification task, predicting generic versus referential (collapsing referential singular and plural as one category).",
        "Note that we expect this to be the major useful distinction for our overall action-item detection task.",
        "Baseline A simple baseline involves predicting the dominant class (in the test set, referential).",
        "This gives 54.59% accuracy (see Table 1).",
        "SVM Results We used LIBSVM (Chang and Lin, 2001), a support vector machine classifier trained using an RBF kernel.",
        "Table 3 presents results for",
        "B: Well, uh, I guess probably the last one I went to I met so many people that I had not seen in proba bly ten, over ten years.",
        "It was like, don't you remember me.",
        "And I am like no.",
        "A: Am I related to you?",
        "Training",
        "Testing",
        "Generic",
        "360",
        "79",
        "Referential singular",
        "287",
        "92",
        "Referential plural",
        "17",
        "3",
        "Reported referential",
        "5",
        "1",
        "Ambiguous",
        "4",
        "1",
        "Total",
        "673",
        "176",
        "various selected sets offeatures.",
        "The bestsetoffea-tures gave accuracy of84.39% and f-score 84.21%.",
        "Discussion Overall performance is respectable; precision was consistently high (94% for the highest-accuracy result).",
        "Perhaps surprisingly, none of the context or part-of-speech features were found to be useful; however, dialog act features proved very useful - using these features alone give us an accuracy of 80.92% - with the referential class strongly associated with question dialog acts.",
        "We used manually produced dialog act tags, and automatic labeling accuracy with this fine-grained tagset will be low; we would therefore prefer to use more robust features if possible.",
        "We found that one such heuristic feature, the presence of question mark, cannot entirely substitute: accuracy is reduced to 76.3%.",
        "However, using only the binary Q_DA feature (which clusters together all the different kinds of question DAs) does better (79.19%).",
        "Although worse than performance with a full tagset, this gives hope that using a coarse-grained set of tags might allow reasonable results.",
        "As (Stolcke et al., 2000) report good accuracy (87%) for statement vs. question classification on manual Switchboard transcripts, such coarse-grained information might be reliably available.",
        "Surprisingly, using the oracle context features (the correct classification for the previous you) alone performs worse than the baseline; and adding these features to sentential features gives no improvement.",
        "This suggests that the generic/referential status of each you may be independent of previous yous."
      ]
    },
    {
      "heading": "6. Prosodic Features",
      "text": [
        "We next checked a set of prosodic features, testing the hypothesis that generics are prosodically reduced.",
        "Mean pitch, intensity and duration were extracted using Praat, both averaged over the entire utterance and just for the word \"you\".",
        "Classification results are shown in Table 4.",
        "Using only prosodic features performs below the baseline; including prosodic features with the best-performing feature set from Table 3 gives identical performance to that with lexical and contextual features alone.",
        "To see why the prosodic features did not help, we examined the difference between the average pitch, intensity and duration for referential versus generic cases (Table 5).",
        "A one-sided t-test shows no significant differences between the average intensity and duration (confirming the results of (Jurafsky et al., 2002), who found no significant change in duration).",
        "The difference in the average pitch was found to be significant (p=0.2) - but not enough for this feature alone to cause an increase in overall accuracy."
      ]
    },
    {
      "heading": "7. Error Analysis",
      "text": [
        "We performed an error analysis on our best classifier output on the training set; accuracy was 94.53%, giving a total of 36 errors.",
        "Half of the errors (18 of 36) were ambiguous even for humans (the authors), if looking at the sentence alone without the neighboring context from the actual conversation - see (5a).",
        "Treating these examples thus needs a detailed model of dialog context.",
        "The other major class of errors requires detailed knowledge about sentential semantics and/or the world - see e.g. (5b,c), which we can tell are referential because they predicate interpersonal comparison or communication.",
        "Features",
        "Accuracy",
        "F-Score",
        "Ctxt",
        "45.66%",
        "0%",
        "Baseline",
        "54.59%",
        "70.63%",
        "Sent",
        "67.05%",
        "57.14%",
        "Sent + Ctxt + POS",
        "67.05%",
        "57.14%",
        "Sent + Ctxt + POS + QM",
        "76.30%",
        "72.84%",
        "Sent + Ctxt + POS + Q_DA",
        "79.19%",
        "77.50%",
        "DA",
        "80.92%",
        "79.75%",
        "Sent + Ctxt + POS + QM + DA",
        "84.39%",
        "84.21%",
        "Features",
        "Accuracy",
        "F-Score",
        "Prosodic only",
        "46.66%",
        "44.31%",
        "Baseline",
        "54.59%",
        "70.63%",
        "Sent + Ctxt + POS + QM + DA + Prosodic",
        "84.39%",
        "84.21%",
        "Category",
        "Referential",
        "Generic",
        "Count",
        "294",
        "340",
        "Pitch (Hz)",
        "156.18",
        "143.98",
        "Intensity (dB)",
        "60.06",
        "59.41",
        "Duration (msec)",
        "139.50",
        "136.84",
        "In addition, as questions are such a useful feature (see above), the classifier tends to label all question cases as referential.",
        "However, generic uses do occur within questions (5d), especially if rhetorical (5e):",
        "(5) a. so uh and if you don't have the money then use a credit card b. I'm probably older than you",
        "c. although uh I will personally tell you I used to work at a bank d. Do they survive longer if you plant them in the winter e. my question I guess are they really your peers?"
      ]
    },
    {
      "heading": "8. Initial Multi-Party Experiments",
      "text": [
        "The experiments above used two-person dialog data: we expect that multi-party data is more complex.",
        "We performed an initial exploratory study, applying the same classes and features to multi-party meetings.",
        "Two annotators labeled one meeting from the AMI corpus (Carletta et al., 2006), giving a total of 52 utterances containing \"you\" on which to assess agreement: kappa was 87.18% for two way classification of generic versus referential.",
        "One of the authors then labeled a testing set of 203 utterances; 104 are generic and 99 referential, giving a baseline accuracy of 51.23% (and F-score of 67.65%).",
        "We performed experiments for the same task: detecting generic versus referential uses.",
        "Due to the small amount of data, we trained the classifier on the Switchboard training set from section 3 (i.e. on two-party rather than multi-party data).",
        "Lacking part-of-speech or dialog act features (since the dialog act tagset differs from the Switchboard tagset), we used only the sentential, context and question mark features described in Table 2.",
        "However, the classifier still achieves an accuracy of 73.89% and F-score of 74.15%, comparable to the results on Switchboard without dialog act features (accuracy 76.30%).",
        "Precision is lower, though (both precision and recall are 73-75%)."
      ]
    },
    {
      "heading": "9. Conclusions",
      "text": [
        "We have presented results on two person and multiparty data for the task of generic versus referential \"you\" detection.",
        "We have seen that the problem is a real one: in both datasets the distribution of the classes is approximately 50/50, and baseline accuracy is low.",
        "Classifier accuracy on two-party data is reasonable, and we see promising results on multiparty data with a basic set of features.",
        "We expect the accuracy to go up once we train and test on samegenre data and also add features that are more specific to multi-party data."
      ]
    }
  ]
}
