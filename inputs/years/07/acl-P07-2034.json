{
  "info": {
    "authors": [
      "Changhua Yang",
      "Kevin Hsin-Yih Lin",
      "Hsin-Hsi Chen"
    ],
    "book": "45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    "id": "acl-P07-2034",
    "title": "Building Emotion Lexicon from Weblog Corpora",
    "url": "https://aclweb.org/anthology/P07-2034",
    "year": 2007
  },
  "references": [
    "acl-P05-2008"
  ],
  "sections": [
    {
      "text": [
        "Changhua Yang Kevin Hsin-Yih Lin Hsin-Hsi Chen",
        "Sec.",
        "4, Taipei, Taiwan 106",
        "1",
        "An emotion lexicon is an indispensable resource for emotion analysis.",
        "This paper aims to mine the relationships between words and emotions using weblog corpora.",
        "A collocation model is proposed to learn emotion lexicons from weblog articles.",
        "Emotion classification at sentence level is experimented by using the mined lexicons to demonstrate their usefulness."
      ]
    },
    {
      "heading": "Introduction",
      "text": [
        "Weblog (blog) is one of the most widely used cybermedia in our internet lives that captures and shares moments of our day-to-day experiences, anytime and anywhere.",
        "Blogs are web sites that timestamp posts from an individual or a group of people, called bloggers.",
        "Bloggers may not follow formal writing styles to express emotional states.",
        "In some cases, they must post in pure text, so they add printable characters, such as \":-)\" (happy) and \":-(\" (sad), to express their feelings.",
        "In other cases, they type sentences with an internet messenger-style interface, where they can attach a special set of graphic icons, or emoticons.",
        "Different kinds of emoticons are introduced into text expressions to convey bloggers' emotions.",
        "Since thousands of blog articles are created everyday, emotional expressions can be collected to form a large-scale corpus which guides us to build vocabularies that are more emotionally expressive.",
        "Our approach can create an emotion lexicon free of laborious efforts of the experts who must be familiar with both linguistic and psychological knowledge.",
        "Related Works",
        "Some previous works considered emoticons from weblogs as categories for text classification.",
        "Mishne (2005), and Yang and Chen (2006) used emoticons as tags to train SVM (Cortes and Vap-nik, 1995) classifiers at document or sentence level.",
        "In their studies, emoticons were taken as moods or emotion tags, and textual keywords were taken as features.",
        "Wu et al.",
        "(2006) proposed a sentence-level emotion recognition method using dialogs as their corpus.",
        "\"Happy, \"Unhappy\", or \"Neutral\" was assigned to each sentence as its emotion category.",
        "Yang et al.",
        "(2006) adopted Thayer's model (1989) to classify music emotions.",
        "Each music segment can be classified into four classes of moods.",
        "In sentiment analysis research, Read (2005) used emoticons in newsgroup articles to extract instances relevant for training polarity classifiers."
      ]
    },
    {
      "heading": "3. Training and Testing Blog Corpora",
      "text": [
        "We select Yahoo!",
        "Kimo Blog posts as our source of emotional expressions.",
        "Yahoo!",
        "Kimo Blog service has 40 emoticons which are shown in Table 1.",
        "When an editing article, a blogger can insert an emoticon by either choosing it or typing in the corresponding codes.",
        "However, not all articles contain emoticons.",
        "That is, users can decide whether to insert emoticons into articles/sentences or not.",
        "In this paper, we treat these icons as emotion categories and taggings on the corresponding text expressions.",
        "The dataset we adopt consists of 5,422,420 blog articles published at Yahoo!",
        "Kimo Blog from January to July, 2006, spanning a period of 212 days.",
        "In total, 336,161 bloggers' articles were collected.",
        "Each blogger posts 16 articles on average.",
        "We used the articles from January to June as the training set and the articles in July as the testing set.",
        "Table 2 shows the statistics of each set.",
        "On average, 14.10% of the articles contain emotion-tagged expressions.",
        "The average length of articles with tagged emotions, i.e., 272.58 characters, is shorter",
        "Blog Articles",
        "Extraction",
        "Lexicon Construction",
        "Emotion Lexicon",
        "Training Set",
        "Testing Set",
        "than that of articles without tagging, i.e., 465.37 characters.",
        "It seems that people tend to use emoti-cons to replace certain amount of text expressions to make their articles more succinct.",
        "Figure 1 shows the three phases for the construction and evaluation of emotion lexicons.",
        "In phase 1, 1,185,131 sentences containing only one emoticon are extracted to form a training set to build emotion lexicons.",
        "In phase 2, sentence-level emotion classifiers are constructed using the mined lexicons.",
        "In phase 3, a testing set consisting of 307,751 sentences is used to evaluate the classifiers."
      ]
    },
    {
      "heading": "4. Emotion Lexicon Construction",
      "text": [
        "The blog corpus contains a collection of bloggers' emotional expressions which can be analyzed to construct an emotion lexicon consisting of words that collocate with emoticons.",
        "We adopt a variation of pointwise mutual information (Manning and Schiitze, 1999) to measure the collocation strength co(e,w) between an emotion e and a word w:",
        "where P(e,w)=c(e,w)/N, P(e)=c(e)/N, P(w)=c(w)/N, c(e) and c(w) are the total occurrences of emoticon e and word w in a tagged corpus, respectively, c(e,w) is total co-occurrences of e and w, and N denotes the total word occurrences.",
        "A word entry of a lexicon may contain several emotion senses.",
        "They are ordered by the collocation strength co.",
        "Figure 2 shows two Chinese example words, \" \" (ha1ha1) and \" ^ \" (ke3wu4).",
        "The former collocates with \"laughing\" and \"big grin\" emoticons with collocation strength 25154.50 and 2667.11, respectively.",
        "Similarly, the latter collocates with \"angry\" and \"phbbbbt\".",
        "When all collocations (i.e., word-emotion pairs) are listed in a descending order of co, we can choose top n collocations to build an emotion lexicon.",
        "In this paper, two lexicons (Lexicons A and B) are extracted by setting n to 25k and 50k.",
        "Lexicon A contains 4,776 entries with 25,000 sense pairs and Lexicon B contains 11,243 entries and 50,000 sense pairs."
      ]
    },
    {
      "heading": "5. Emotion Classification",
      "text": [
        "Suppose a sentence S to be classified consists of n emotion words.",
        "The emotion of S is derived by a mapping from a set of n emotion words to m emotion categories as follows:",
        "ID",
        "Emoticon",
        "Code",
        "Description",
        "ID",
        "Emoticon",
        "Code",
        "Description",
        "ID",
        "Emoticon",
        "Code",
        "Description",
        "ID",
        "Emoticon",
        "Code",
        "Description",
        "1",
        "■)",
        "happy",
        "11",
        "©",
        "■O",
        "surprise",
        "21",
        "®",
        "00",
        "angel",
        "Sl",
        "©",
        "(■I",
        "yawn",
        "2",
        "©",
        "■(",
        "sad",
        "12",
        "X-(",
        "angry",
        "22",
        "■-B",
        "nerd",
        "S2",
        "=P~",
        "drooling",
        "S",
        ";)",
        "winking",
        "1S",
        "■>",
        "smug",
        "2S",
        "=;",
        "talk to the hand",
        "SS",
        ":-?",
        "thinking",
        "4",
        "--",
        "■D",
        "big grin",
        "14",
        "B-)",
        "cool",
        "24",
        "&",
        "I-)",
        "asleep",
        "S4",
        "\"S",
        ";))",
        "hee hee",
        "5",
        ";;)",
        "batting eyelashes",
        "15",
        "©",
        "■-S",
        "worried",
        "25",
        "©",
        "S-)",
        "rolling eyes",
        "S5",
        "m",
        "=D>",
        "applause",
        "6",
        "■-/",
        "confused",
        "16",
        "m",
        ">■)",
        "devil",
        "26",
        "©",
        "■-&",
        "sick",
        "S6",
        "[-o<",
        "praying",
        "7",
        "■x",
        "love struck",
        "17",
        "■((",
        "crying",
        "27",
        "■-S",
        "don't tell anyone",
        "S7",
        "■-<",
        "sigh",
        "S",
        "©",
        "■\">",
        "blushing",
        "1S",
        "■))",
        "laughing",
        "28",
        "%",
        "[-(",
        "not talking",
        "SS",
        "©",
        "xP",
        "phbbbbt",
        "9",
        ":p",
        "tongue",
        "19",
        "■ I",
        "straight face",
        "29",
        "w",
        "■o)",
        "clown",
        "S9",
        "@};-",
        "rose",
        "10",
        "®",
        "^*",
        "kiss",
        "20",
        "raised eyebrow",
        "S0",
        "@-)",
        "hypnotized",
        "40",
        "©",
        "■ @)",
        "Dataset Article #",
        "Tagged # Percentage Tagged Len.",
        "Untagged L.",
        "Training 4,1S7,7S7",
        "575,009",
        "1S.S6% 269.77 chrs.",
        "46S.14 chrs.",
        "Testing 1,2S4,6SS",
        "1S2,999",
        "14.92% 2S1.42 chrs.",
        "455.S2 chrs.",
        "Total 5,422,420",
        "764,7SS",
        "14.10% 272.5S chrs.",
        "465.S7 chrs.",
        "r",
        "Classifiers",
        " – a r",
        "Evaluation",
        "\"What's the hacker doing... darn Sense 2.",
        "©(phbbbbt) - co: 619.24 e.g., ^ri^^IA...© \"Damn those aliens©\" Figure 2.",
        "Some Example Words in a Lexicon.",
        "For each emotion word ew{, we may find several emotion senses with the corresponding collocation strength co by looking up the lexicon.",
        "Three alternatives are proposed as follows to label a sentence S with an emotion:",
        "(1) Consider all senses of ewi as votes.",
        "Label S with the emotion that receives the most votes.",
        "(2) If more than two emotions get the same number of votes, then label S with the emotion that has the maximum co.",
        "Collect emotion senses from all ewi.",
        "Label S with the emotion that has the maximum co.",
        "The same as Method 1 except that each ewi votes only one sense that has the maximum co.",
        "In past research, the approach used by Yang et al.",
        "(2006) was based on the Thayer's model (1989), which divided emotions into 4 categories.",
        "In sentiment analysis research, such as Read's study (2006), a polarity classifier separated instances into positive and negative classes.",
        "In our experiments, we not only adopt fine-grain classification, but also coarse-grain classification.",
        "We first select 40 emoticons as a category set, and also adopt the Thayer's model to divide the emoticons into 4 quadrants of the emotion space.",
        "As shown in Figure 3, the top-right side collects the emotions that are more positive and energetic and the bottom-left side is more negative and silent.",
        "A polarity classi-",
        "Arousal (energetic) (negative) (positive) (silent)",
        "fier uses the right side as positive and the left side as negative."
      ]
    },
    {
      "heading": "6. Evaluation",
      "text": [
        "Table 3 shows the performance under various combinations of lexicons, emotion categories and classification methods.",
        "\"Hit #\" stands for the number of correctly-answered instances.",
        "The baseline represents the precision of predicting the majority category, such as \"happy\" or \"positive\", as the answer.",
        "The baseline method's precision increases as the number of emotion classes decreases.",
        "The upper bound recall indicates the upper limit on the fraction of the 307,751 instances solvable by the corresponding method and thus reflects the limitation of the method.",
        "The closer a method's actual recall is to the upper bound recall, the better the method.",
        "For example, at most 40,855 instances (14.90%) can be answered using Method 1 in combination with Lexicon A.",
        "But the actual recall is 4.55% only, meaning that Method 1's recall is more than 10% behind its upper bound.",
        "Methods which have a larger set of candidate answers have higher upper bound recalls, because the probability that the correct answer is in their set of candidate answers is greater.",
        "Experiment results show that all methods utilizing Lexicon A have performance figures lower than the baseline, so Lexicon A is not useful.",
        "In contrast, Lexicon B, which provides a larger collection of vocabularies and emotion senses, outperforms Lexicon A and the baseline.",
        "Although Method 3 has the smallest candidate answer set and thus has the smallest upper bound recall, it outperforms the other two methods in most cases.",
        "Method 2 achieves better precisions when using",
        "Upp.",
        "R. - upper bound recall; Prec.",
        "- precision; Reca.",
        "- recall",
        "F = 2x(PrecisionxRecall)/(Precision+Recall)",
        "Thayer's emotion categories.",
        "Method 1 treats the vote to every sense equally.",
        "Hence, it loses some differentiation abilities.",
        "Method 1 performs the best in the first case (Lexicon A, 40 classes).",
        "We can also apply machine learning to the dataset to train a high-precision classification model.",
        "To experiment with this idea, we adopt LIBSVM (Fan et al., 2005) as the SVM kernel to deal with the binary polarity classification problem.",
        "The SVM classifier chooses top k (k = 25, 50, 75, and 100) emotion words as features.",
        "Since the SVM classifier uses a small feature set, there are testing instances which do not contain any features seen previously by the SVM classifier.",
        "To deal with this problem, we use the class prediction from Method 3 for any testing instances without any features that the SVM classifier can recognize.",
        "In Table 4, the SVM classifier employing 25 features has the highest precision.",
        "On the other hand, the SVM classifier employing 50 features has the highest F measure when used in conjunction with",
        "Method 3.",
        "Conclusion and Future Work",
        "Our methods for building an emotional lexicon utilize emoticons from blog articles collaboratively contributed by bloggers.",
        "Since thousands of blog articles are created everyday, we expect the set of emotional expressions to keep expanding.",
        "In the experiments, the method of employing each emotion word to vote only one emotion category achieves the best performance in both fine-grain and coarse-grain classification.",
        "Acknowledgment",
        "Research of this paper was partially supported by Excellent Research Projects of National Taiwan University, under the contract of 95R0062-AE00-02.",
        "We thank Yahoo!",
        "Taiwan Inc. for providing the dataset for researches.",
        "Baseline",
        "Method 1 (M1)",
        "Method 2 (M2)",
        "Method S (MS)",
        "Upp.",
        "R.",
        "Hit t",
        "Prec.",
        "Reca.",
        "Upp.",
        "R.",
        "Hit t",
        "Prec.",
        "Reca.",
        "Upp.",
        "R.",
        "Hit t",
        "Prec.",
        "Reca.",
        "Lexicon A 40 classes",
        "8.04%",
        "14.90%",
        "14,009",
        "4.86%",
        "4.55%",
        "14.90%",
        "9,S92",
        "S.26%",
        "S.05%",
        "6.49%",
        "1S,929",
        "4.8S%",
        "4.52%",
        "Lexicon A Thayer",
        "38.38%",
        "48.70%",
        "90^2",
        "S2.46%",
        "2935%",
        "48.70%",
        "64,689",
        "2S.25%",
        "21.02%",
        "S5.94%",
        "9S,285",
        "SS.5S%",
        "S0.S1%",
        "Lexicon A Polarity",
        "63.49%",
        "60.74%",
        "150,946",
        "54.25%",
        "49.05%",
        "60.74%",
        "120,2S7",
        "4S.21%",
        "S9.07%",
        "54.97%",
        "15S,292",
        "55.09%",
        "49.81%",
        "Lexicon B 40 classes",
        "8.04%",
        "7S.18%",
        "45,075",
        "15.65%",
        "14.65%",
        "7S.18%",
        "4S,6S7",
        "15.15%",
        "14.18%",
        "27.89%",
        "45,604",
        "15.83%",
        "14.81%",
        "Lexicon B Thayer",
        "S838%",
        "89.11%",
        "104,094",
        "S7.40%",
        "SS.82%",
        "89.11%",
        "118392",
        "42.55%",
        "S8.47%",
        "6S.74%",
        "110,904",
        "S9.86%",
        "S6.04%",
        "Lexicon B Polarity",
        "6S.49%",
        "91.12%",
        "192,65S",
        "69.24%",
        "62.60%",
        "91.12%",
        "188/34",
        "67.72%",
        "61.2S%",
        "81.92%",
        "195,190",
        "70.15%",
        "6S.42%",
        "Method",
        "Upp.",
        "R.",
        "Hit #",
        "Prec.",
        "Reca.",
        "F",
        "Lexicon B M3",
        "81.92%",
        "195,190 1 70.15%",
        "6S.42%",
        "66.62%",
        "SVM 25 features",
        "15.80%",
        "S8,651",
        "79.49% 1 12.56%",
        "21.69%",
        "SVM 50 features",
        "26.27%",
        "62,999",
        "77.9S%",
        "20.47%",
        "S2.42%",
        "SVM 75 features",
        "S6.74%",
        "84,6S8",
        "74.86%",
        "27.50%",
        "40.2S%",
        "SVM 100 features",
        "45.49%",
        "101,9S4",
        "72.81%",
        "SS.12%",
        "45.5S%",
        "(Svm-25 + M3)",
        "90.41%",
        "196,147",
        "70.05%",
        "6S.7S%",
        "66.74%",
        "(Svm-50 + M3)",
        "90.41%",
        "195,8S5",
        "7037%",
        "6S.64%",
        "66.8S%",
        "(Svm-75 + M3)",
        "90.41%",
        "195,229",
        "70.16%",
        "6S.44%",
        "66.6S%",
        "(Svm-100 + M3)",
        "90.41%",
        "195,054",
        "70.01%",
        "6S.S8%",
        "66.5S%"
      ]
    }
  ]
}
