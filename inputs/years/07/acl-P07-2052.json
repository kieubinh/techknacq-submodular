{
  "info": {
    "authors": [
      "Daisuke Kawahara",
      "Kiyotaka Uchimoto"
    ],
    "book": "45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
    "id": "acl-P07-2052",
    "title": "Minimally Lexicalized Dependency Parsing",
    "url": "https://aclweb.org/anthology/P07-2052",
    "year": 2007
  },
  "references": [
    "acl-A00-2018",
    "acl-C04-1010",
    "acl-C04-1040",
    "acl-E06-1011",
    "acl-H05-1059",
    "acl-J04-4004",
    "acl-N01-1025",
    "acl-N03-1027",
    "acl-N06-1021",
    "acl-P03-1054",
    "acl-P04-1054",
    "acl-P05-1010",
    "acl-P05-1012",
    "acl-P05-1067",
    "acl-P06-1055"
  ],
  "sections": [
    {
      "text": [
        "Daisuke Kawahara and Kiyotaka Uchimoto",
        "National Institute of Information and Communications Technology, 3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan {dk, uchimoto}@nict.go.jp",
        "Dependency structures do not have the information of phrase categories in phrase structure grammar.",
        "Thus, dependency parsing relies heavily on the lexical information of words.",
        "This paper discusses our investigation into the effectiveness of lexicalization in dependency parsing.",
        "Specifically, by restricting the degree of lexicalization in the training phase of a parser, we examine the change in the accuracy of dependency relations.",
        "Experimental results indicate that minimal or low lexicalization is sufficient for parsing accuracy."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In recent years, many accurate phrase-structure parsers have been developed (e.g., (Collins, 1999; Charniak, 2000)).",
        "Since one of the characteristics of these parsers is the use of lexical information in the tagged corpus, they are called \"lexicalized parsers\".",
        "Unlexicalized parsers, on the other hand, achieved accuracies almost equivalent to those of lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006).",
        "Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem.",
        "Bikel also indicated that Collins' parser can use bilexical dependencies only 1.49% of the time; the rest of the time, it backs off to condition one word on just phrasal and part-of-speech categories (Bikel, 2004).",
        "This paper describes our investigation into the effectiveness of lexicalization in dependency parsing instead of phrase-structure parsing.",
        "Usual dependency parsing cannot utilize phrase categories, and thus relies on word information like parts of speech and lexicalized words.",
        "Therefore, we want to know the performance of dependency parsers that have minimal or low lexicalization.",
        "Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005).",
        "For such applications, a fast, efficient and accurate dependency parser is required to obtain dependency trees from a large corpus.",
        "From this point of view, minimally lexicalized parsers have advantages over fully lexicalized ones in parsing speed and memory consumption.",
        "We examined the change in performance of dependency parsing by varying the degree of lexical-ization.",
        "The degree of lexicalization is specified by giving a list of words to be lexicalized, which appear in a training corpus.",
        "For minimal lexicalization, we used a short list that consists of only high-frequency words, and for maximal lexicalization, the whole list was used.",
        "Consequently, minimally or low lexical-ization is sufficient for dependency accuracy."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Klein and Manning presented an unlexicalized PCFG parser that eliminated all the lexicalized parameters (Klein and Manning, 2003).",
        "They manually split category tags from a linguistic view.",
        "This corresponds to determining the degree of lexicalization by hand.",
        "Their parser achieved an Fof 85.7% for section 23 of the Penn Treebank.",
        "Matsuzaki et al.",
        "and Petrov et al.",
        "proposed an automatic approach to",
        "Dependency accuracy (DA) Proportions of words, except punctuation marks, that are assigned the correct heads.",
        "Root accuracy (RA) Proportions of root words that are correctly detected.",
        "Complete rate (CR) Proportions of sentences whose dependency structures are completely correct.",
        "splitting tags (Matsuzaki et al., 2005; Petrov et al., 2006).",
        "In particular, Petrov et al.",
        "reported an Fi of 90.2%, which is equivalent to that of state-of-the-art lexicalized parsers.",
        "Dependency parsing has been actively studied in recent years (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Isozaki et al., 2004; McDonald et al., 2005; McDonald and Pereira, 2006;",
        "Corston-Oliver et al., 2006).",
        "For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004).",
        "McDonald et al.",
        "proposed an online large-margin method for training dependency parsers (McDonald et al., 2005).",
        "All of them performed experiments using section 23 of the Penn Treebank.",
        "Table 2 summarizes their dependency accuracies based on three evaluation criteria shown in Table 1.",
        "These parsers believed in the generalization ability of machine learners and did not pay attention to the issue of lexicalization."
      ]
    },
    {
      "heading": "3. Minimally Lexicalized Dependency Parsing",
      "text": [
        "We present a simple method for changing the degree of lexicalization in dependency parsing.",
        "This method restricts the use of lexicalized words, so it is the opposite to tag splitting in phrase-structure parsing.",
        "In the remainder of this section, we first describe a base dependency parser and then report experimental results.",
        "We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser.",
        "We adopted this algorithm because of its linear-time complexity.",
        "In the algorithm, parsing states are represented by triples (S, I, A), where S is the stack that keeps the words being under consideration, I is the list of remaining input words, and A is the list of determined dependencies.",
        "Given an input word sequence, W, the parser is first initialized to the triple (nil, W, 4>).",
        "The parser estimates a dependency relation between two words (the top elements of stacks S and I).",
        "The algorithm iterates until the list I is empty.",
        "There are four possible operations for a parsing state (where t is the word on top of S, n is the next input word in I, and w is any word):",
        "Left In a state (t\\S,n\\I, A), if there is no dependency relation (t – w) in A, add the new dependency relation (t – n) into A and pop S (remove t), giving the state (S,n\\I,A U (t – n)).",
        "Right In a state ( t\\ S, n\\ I, A) , if there is no dependency relation (n – w) in A, add the new dependency relation (n – t) into A and push n onto S, giving the state (n\\t\\S, I, AU(n – t)).",
        "Reduce In a state (t\\S,I,A), if there is a dependency relation (t – w) in A, pop S, giving the state (S, I, A).",
        "Shift In a state (S,n\\I, A), push n onto S, giving the state (n\\S,I,A).",
        "In this work, we used Support Vector Machines (SVMs) to predict the operation given a parsing state.",
        "Since SVMs are binary classifiers, we used the pairwise method to extend them in order to classify our four-class task.",
        "The features of a node are the word's lemma, the POS/chunk tag and the information of its child node(s).",
        "The lemma is obtained from the word form using a lemmatizer, except for numbers, which are replaced by \"(num)\".",
        "The context features are the two preceding nodes of node t (and t itself), the two succeeding nodes of node n (and n itself), and their child nodes (lemmas and POS tags).",
        "The distance between nodes n and t is also used as a feature.",
        "DA",
        "RA",
        "CR",
        "(Yamada and Matsumoto, 2003)",
        "90.3",
        "91.6",
        "38.4",
        "(Nivre and Scholz, 2004)",
        "87.3",
        "84.3",
        "30.4",
        "(Isozaki et al., 2004)",
        "91.2",
        "95.7",
        "40.7",
        "(McDonald et al., 2005)",
        "90.9",
        "94.2",
        "37.5",
        "(McDonald and Pereira, 2006)",
        "91.5",
        "N/A",
        "42.1",
        "(Corston-Oliver et al., 2006)",
        "90.8",
        "93.7",
        "37.6",
        "Our Base Parser",
        "90.9",
        "92.6",
        "39.2",
        "We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank.",
        "We used section 23 as the test set.",
        "Since the original treebank is based on phrase structure, we converted the treebank to dependencies using the head rules provided by Yamada .",
        "During the training phase, we used intact POS and chunk tags.",
        "During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka's tagger(Tsuruoka and Tsujii, 2005) and YamCha chunker(Kudo and Matsumoto, 2001).",
        "We used an SVMs package, TinySVM,and trained the SVMs classifiers using a third-order polynomial kernel.",
        "The other parameters are set to default.",
        "The last row in Table 2 shows the accuracies of our base dependency parser.",
        "The degree of lexicalization is specified by giving a list of words to be lexicalized, which appear in a training corpus.",
        "For minimal lexicalization, we used a short list that consists of only high-frequency words, and for maximal lexicalization, the whole list was used.",
        "To conduct the experiments efficiently, we trained our models using the first 10,000 sentences in sections 2-21 of the WSJ portion of the Penn Treebank.",
        "We used section 24, which is usually used as the development set, to measure the change in performance based on the degree of lexicalization.",
        "We counted word (lemma) frequencies in the training corpus and made a word list in descending order of their frequencies.",
        "The resultant list consists of 13,729 words, and the most frequent word is \"the\", which occurs 13,252 times, as shown in Table 3.",
        "We define the degree of lexicalization as a threshold of the word list.",
        "If, for example, this threshold is set to 1,000, the top 1,000 most frequently occurring words are lexicalized.",
        "We evaluated dependency accuracies while changing the threshold of lexicalization.",
        "Figure 1 shows the result.",
        "The dotted line (88.23%) represents the dependency accuracy of the maximal lex-icalization, that is, using the whole word list.",
        "We can see that the decrease in accuracy is less than 1% at the minimal lexicalization (degree=100) and the accuracy of more than 3,000 degree slightly exceeds that of the maximal lexicalization.",
        "The best accuracy (88.34%) was achieved at 4,500 degree and significantly outperformed the accuracy (88.23%) of the maximal lexicalization (McNemar's test; p = 0.017 < 0.05).",
        "These results indicate that maximal lexicalization is not so effective for obtaining accurate dependency relations.",
        "We also applied the same trained models to the Brown Corpus as an experiment of parser adaptation.",
        "We first split the Brown Corpus portion of the Penn Treebank into training and testing parts in the same way as (Roark and Bacchiani, 2003).",
        "We further extracted 2,425 sentences at regular intervals from the training part and used them to measure the change in performance while varying the degree of lexicalization.",
        "Figure 2 shows the result.",
        "The dotted line (84.75%) represents the accuracy of maximal lexicalization.",
        "The resultant curve is similar to that of the WSJ experiment.",
        "We can say that our claim is true even if the testing corpus is outside the domain.",
        "We have presented a minimally or lowly lexical-ized dependency parser.",
        "Its dependency accuracy is close or almost equivalent to that of fully lexicalized parsers, despite the lexicalization restriction.",
        "Furthermore, the restriction reduces the time and space complexity.",
        "The minimally lexicalized parser (de-gree=100) took 12m46s to parse the WSJ development set and required 111 MB memory.",
        "These are 36% of time and 45% of memory reduction, compared to the fully lexicalized one.",
        "The experimental results imply that training corpora are too small to demonstrate the full potential of lexicalization.",
        "We should consider unsupervised or semi-supervised ways to make lexicalized parsers more effective and accurate.",
        "Acknowledgment",
        "This research is partially supported by special coordination funds for promoting science and technology._",
        "rank",
        "word",
        "freq.",
        "rank",
        "word",
        "freq.",
        "1",
        "the",
        "13,252",
        "1,000",
        "watch",
        "29",
        "2",
        ",",
        "12,858",
        "2,000",
        "healthvest",
        "12",
        "100",
        "week",
        "261",
        "3,000",
        "whoop",
        "7",
        "500",
        "estate",
        "64"
      ]
    }
  ]
}
