{
  "info": {
    "authors": [
      "Bruno Jedynak",
      "Damianos Karakos"
    ],
    "book": "Workshop on TextGraphs: Graph-Based Algorithms for Natural Language Processing",
    "id": "acl-W07-0205",
    "title": "Unigram Language Models using Diffusion Smoothing over Graphs",
    "url": "https://aclweb.org/anthology/W07-0205",
    "year": 2007
  },
  "references": [],
  "sections": [
    {
      "text": [
        "of Appl.",
        "of Electrical and Computer Engineering Center for Language and Speech Processing Johns Hopkins University",
        "Baltimore, MD 21218-2686",
        "We propose to use graph-based diffusion techniques with data-dependent kernels to build unigram language models.",
        "Our approach entails building graphs, where each vertex corresponds uniquely to a word from a closed vocabulary, and the existence of an edge (with an appropriate weight) between two words indicates some form of similarity between them.",
        "In one of our constructions, we place an edge between two words if the number of times these words were seen in a training set differs by at most one count.",
        "This graph construction results in a similarity matrix with small intrinsic dimension, since words with the same counts have the same neighbors.",
        "Experimental results from a benchmark task from language modeling show that our method is competitive with the Good-Turing estimator."
      ]
    },
    {
      "heading": "1. Diffusion over Graphs 1.1 Notation",
      "text": [
        "Let G = (V, E) be an undirected graph, where V is a finite set of vertices, and E c V x V is the set of edges.",
        "Also, let V be a vocabulary of words, whose probabilities we want to estimate.",
        "Each vertex corresponds uniquely to a word, i.e., there is a one-to-one mapping between V and V. Without loss of generality, we will use V to denote both the set of words and the set of vertices.",
        "Moreover, to simplify notation, we assume that the letters x, y, z will always denote vertices ofG.",
        "The existence of an edge between x, y will be denoted by x ~ y.",
        "We assume that the graph is strongly connected (i.e., there is a path between any two vertices).",
        "Furthermore, we define a non-negative real valued function w over V x V, which plays the role of the similarity between two words (the higher the value of w(x,y), the more similar words x, y are).",
        "In the experimental results section, we will compare different measures of similarity between words which will result in different smoothing algorithms.",
        "The degree of a vertex is defined as",
        "We assume that for any vertex x, d(x) > 0; that is, every word is similar to at least some other word.",
        "The setting described here was introduced in (Szlam et al., 2006).",
        "First, we define a Markov chain {Xt}, which corresponds to a random walk over the graph G. Its initial value is equal to X0, which has distribution no.",
        "(Although n0 can be chosen arbitrarily, we assume in this paper that it is equal to the empirical, unsmoothed, distribution of words over a training set.)",
        "We then define the transition matrix as follows:",
        "T (x,y) = P (Xi = y|Xo = x) = d_1(x)w(x, y).",
        "This transition matrix, together with n0, induces a distribution over V, which is equal to the distribu-",
        "This distribution can be construed as a smoothed version of n0, since the n1 probability of an unseen word will always be non-zero, if it has a nonzero similarity to a seen word.",
        "In the same way, a whole sequence of distributions n2, n3,... can be computed; we only consider n1 as our smoothed estimate in this paper.",
        "(One may wonder whether the stationary distribution of this Markov chain, i.e., the limiting distribution of Xt, as t – oo, has any significance; we do not address this question here, as this limiting distribution may have very little dependence on n0 in the Markov chain cases under consideration.)",
        "We assume here that for any vertex x, w(x, x) = 0 and that w is symmetric.",
        "Following (Kondor and Lafferty, 2002), we define the following matrix over where £(u) is the delta function which takes the value 1 if property u is true, and 0 otherwise.",
        "The negative of the matrix H is called the Laplacian of the graph and plays a central role in spectral graph theory (Chung, 1997).",
        "We further define the heat equation over the graph G as with initial condition K0 = I, where Kt is a time-dependent square matrix of same dimension as H, and I is the identity matrix.",
        "Kt(x, y) can be interpreted as the amount of heat that reaches vertex x at time t, when starting with a unit amount of heat concentrated at y.",
        "Using (1) and (4), the right hand side of (5) expands to",
        "HKt(x,y)= J2 w(x,z)(Kt(z,y) - Kt(x,y)).",
        "From this equation, we see that the amount of heat at x will increase (resp.",
        "decrease) if the current amount of heat at x (namely Kt(x,y)) is smaller (resp.",
        "larger) than the weighted average amount of heat at the neighbors of x, thus causing the system to reach a steady state.",
        "The heat equation (5) has a unique solution which is the matrix exponential Kt = exp(tH), (see (Kondor and Lafferty, 2002)) and which can be defined equivalently as",
        "Moreover, if the initial condition is replaced by K0(x,y) = n0(x)5(x = y) then the solution of the heat equation is given by the matrix product n1 = Ktn0.",
        "In the following, n0 will be the empirical distribution over the training set and t will be chosen by trial and error.",
        "As before, n1 will provide a smoothed version of n0."
      ]
    },
    {
      "heading": "2. Unigram Language Models",
      "text": [
        "Let Tr be a training set of n tokens, and T a separate test set of m tokens.",
        "We denote by n(x), m(x) the number of times the word x has been seen in the training and test set, respectively.",
        "We assume a closed vocabulary V containing K words.",
        "A unigram model is a probability distribution n over the vocabulary V. We measure its performace using the average code length (Cover and Thomas, 1991) measured on the test set:",
        "The empirical distribution over the training set is",
        "This estimate assigns a probability 0 to all unseen words, which is undesirable, as it leads to zero probability of word sequences which can actually be observed in practice.",
        "A simple way to smooth such estimates is to add a small, not necessarily integer, count to each word leading to the so-called add-/// estimate , defined as",
        "^ (x) = 7n+PK.",
        "One may observe that",
        "Hence add-/ estimators perform a linear interpolation between n0 and the uniform distribution over the entire vocabulary.",
        "In practice, a much more efficient smoothing method is the so-called Good-Turing (Orlitsky et al., 2003; McAllester and Schapire, 2000).",
        "The Good-Turing estimate is defined as",
        "an0(x), otherwise,",
        "where rj is the number of distinct words seen j times in the training set, and a is such that nOT sums up to 1 over the vocabulary.",
        "The threshold M is empirically chosen, and usually lies between 5 and 10.",
        "(Choosing a much larger M decreases the performance considerably.)",
        "The Good-Turing estimator is used frequently in practice, and we will compare our results against it.",
        "The add-/ will provide a baseline, as well as an idea of the variation between different smoothers."
      ]
    },
    {
      "heading": "3. Graphs over sets of words",
      "text": [
        "Our objective, in this section, is to show how to design various graphs on words; different choices for the edges and for the weight function w lead to different smoothings.",
        "The simplest possible choice is the complete graph, where all vertices are pairwise connected.",
        "In the case of normalized diffusion, choosing with a = 0 leads to the add-/ smoother with parameter // = a-1n.",
        "In the case of kernel smoothing with the complete graph and w = 1, one can show, see (Kondor and = K – 1 (1 - e – Kt) if x = y.",
        "A more interesting way ofdesigning the word graph is through a similarity function which is based on the training set.",
        "For the normalized diffusion case, we propose the following",
        "That is, 2 words are \"similar\" if they have been seen a number of times which differs by at most one.",
        "The obtained estimator is denoted by nND.",
        "After some algebraic manipulations, we obtain",
        "This estimator has a Good-Turing \"flavor\".",
        "For example, the total mass associated with the unseen words is",
        "This leads to another add-/ smoother.",
        "Note that the estimate of the unseen mass, in the case of the Good-Turing estimator, is equal to n – 1r1, which is very close to the above when the vocabulary is large compared to the size of the training set (as is usually the case in practice).",
        "Similarly, in the case of kernel diffusion, we choose w = 1 and",
        "The time t is chosen to be |V| – 1.",
        "The smoothercan-not be computed in closed form.",
        "We used the formula (7) with n = 3 in the experiments.",
        "Larger values of n did not improve the results."
      ]
    },
    {
      "heading": "4. Experimental Results",
      "text": [
        "In our experiments, we used Sections 00-22 (consisting of – 10 words) of the UPenn Treebank corpus for training, and Sections 23-24 (consisting of – 10 words) for testing.",
        "We split the training set into 10 subsets, leading to 10 datasets of size – 10tokens each.",
        "The first of these sets was further split in subsets of size – 10 tokens each.",
        "Averaged results are presented in the tables below for various choices of the training set size.",
        "We show the mean code-length, as well as the standard deviation (when available).",
        "In all cases, we chose K = 10 as the fixed size of our vocabulary.",
        "The results show that nND, the estimate obtained with the Normalized Diffusion, is competitive with the Good-Turing nOT.",
        "We performed a Kolmogorov-Smirnov test in order to determine if the code-lengths obtained with nND and nOT in Table 1 differ significantly.",
        "The result is negative (P-value = .65), and the same holds for the larger training set in Table 2 (P-value=.95).",
        "On the other hand, nKD (obtained with Kernel Diffusion) is not as efficient, but still better than add-/ with / = 1."
      ]
    },
    {
      "heading": "5. Concluding Remarks",
      "text": [
        "We showed that diffusions on graphs can be useful for language modeling.",
        "They yield naturally smooth estimates, and, under a particular choice of the \"similarity\" function between words, they are competitive with the Good-Turing estimator, which is considered to be the state-of-the-art in unigram language modeling.",
        "We plan to perform more experiments with other definitions of similarity between words.",
        "For example, we expect similarities based on co-occurence in documents, or based on notions of semantic closeness (computed, for instance, using the WordNet hierarchy) to yield significant improvements over estimators which are only based on word counts.",
        "mean code length",
        "std",
        "12.94",
        "0.05",
        "TÏGT",
        "11.40",
        "0.08",
        "nND",
        "11.42",
        "0.08",
        "nKD",
        "11.51",
        "0.08",
        "le 1: Results with training set of size ~",
        "mean code length",
        "std",
        "nfi ,p =",
        "11.10",
        "0.03",
        "nGT",
        "10.68",
        "0.06",
        "nND",
        "10.69",
        "0.06",
        "10.74",
        "0.08",
        "mean code length",
        "n/3, P =",
        "10.34",
        "7TGT",
        "10.30",
        "10.30",
        "10.31"
      ]
    }
  ]
}
