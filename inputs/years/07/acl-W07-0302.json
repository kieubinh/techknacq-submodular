{
  "info": {
    "authors": [
      "Blaise Thomson",
      "Jost Schatzmann",
      "Karl Weilhammer",
      "Hui Ye",
      "Steve Young"
    ],
    "book": "Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies",
    "id": "acl-W07-0302",
    "title": "Training a real-world POMDP-based Dialog System",
    "url": "https://aclweb.org/anthology/W07-0302",
    "year": 2007
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Training a real-world POMDP-based Dialogue System",
        "Blaise Thomson, Jost Schatzmann, Karl Weilhammer, Hui Ye and Steve Young",
        "Cambridge University Engineering Department Trumpington Street, Cambridge, CB21PZ, United Kingdom",
        "Partially Observable Markov Decision Processes provide a principled way to model uncertainty in dialogues.",
        "However, traditional algorithms for optimising policies are intractable except for cases with very few states.",
        "This paper discusses a new approach to policy optimisation based on grid-based Q-learning with a summary of belief space.",
        "We also present a technique for bootstrapping the system using a novel agenda-based user model.",
        "An implementation of a policy trained using this system was tested with human subjects in an extensive trial.",
        "The policy gave highly competitive results, with a 90.6% task completion rate."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recent work on statistical models for dialogue systems has argued that Partially Observable Markov Decision Processes (POMDPs) provide a principled mathematical framework for modeling the uncertainty inherent in human-machine dialogue (Young, 2006).",
        "Briefly speaking, POMDPs extend the traditional (fully-observable) Markov Decision Process (MDP) framework by maintaining a belief state, ie.",
        "a probability distribution over dialogue states.",
        "This enables the dialogue manager to avoid and recover from recognition errors by sharing and shifting probability mass between multiple hypotheses of the current dialogue state.",
        "The framework also naturally incorporates n-best lists of multiple recognition hypotheses coming from the speech recogniser.",
        "Due to the vast space of possible belief states, however, the use of POMDPs for any practical system is far from straightforward.",
        "Exact algorithms for solving POMDPs do exist, but have been shown to be intractable except for domains limited to a few states (Kaelbling et al., 1998).",
        "In a practical dialogue system the minimum number of dialogue states is typically determined by the number of possible user goals, and this number usually far exceeds the limits of exact solution algorithms.",
        "Approximate algorithms have been developed to overcome the intractibility of exact algorithms but even the most efficient of these techniques such as Point Based Value Iteration (PBVI) cannot scale to the many thousand states required by a statistical dialogue manager (Williams, 2006; Pineau et al., 2003).",
        "Previous work by Williams and Young (2006) on Composite Summary Point Based Value Iteration (CSPBVI) has suggested the use of a small summary space for each slot where PBVI policy optimisation can be applied.",
        "This has shown to give good results on synthetic data but remains untested within real dialogue systems.",
        "One potential problem with the CSPBVI technique is that policy learning can only be performed offline, ie.",
        "at design time, because policy training requires an existing accurate model of user behaviour.",
        "In this paper, an alternative technique for online training based on Q-learning is presented.",
        "Online training allows the system to adapt to real users as new dialogues are recorded.",
        "The learning algorithm presented here does not require any model of user behaviour so initial dialogues may well be incoherent.",
        "In fact, the system requires several thousand dialogues before convergence to a suitable policy begins.",
        "This means that in practice the model needs to be bootstrapped via a user simulator.",
        "Further adapatation can then be done with real users.",
        "The paper is organised as follows.",
        "Section 2 provides an introduction to the POMDP model and explains the Summary POMDP framework that is used in the remainder of the paper.",
        "A new online method for policy optimisation is presented in Section 3 and a novel agenda-based user model for bootstrapping the system is introduced in Section 4.",
        "Section 5 discusses an evaluation of a sample implementation built for a Tourist Information System and tested with human subjects.",
        "The system performed competitively with 90.6% of tasks successfully completed despite a mix of native and non-native speakers.",
        "The paper concludes with a summary and some directions for future work."
      ]
    },
    {
      "heading": "2. Background 2.1 POMDP Basics",
      "text": [
        "A POMDP is defined in much the same way as an MDP, except that the states are not observable and instead have to be estimated from observations.",
        "Formally, a POMDP is a tuple {Sm, Am, T, R, O.",
        "Z, A, bo} where:",
        "• Sm is a set of machine states",
        "• Am is a set of actions that the machine may take",
        "• O is a set of possible observations",
        "• T defines the transition probability such that T(sm,am,a'm) = P(8'm\\sm,am)",
        "• R defines the immediate reward obtained from taking a particular action in a particular state to be",
        "• Z defines the probability of a particular observation given the state and machine action P(o'\\s'm,am)",
        "• bo is an initial belief state.",
        "When the POMDP operates, it also makes use of a policy 7T : 11(5) – > Am that chooses an action given a point in belief space.",
        "Here 11(5) is the set of all possible probability distributions over Srn (an |5m| – 1 dimensional simplex).",
        "77(b) gives the action to take when the POMDP is in belief state b.",
        "The sequence of events in the POMDP follows a cycle.",
        "At each time step, the machine is in some unobserved state sm e Sm.",
        "Since the true state is unknown, the machine maintains a probability distribution over the states b, which is called the belief state.",
        "Based on this belief state and the policy tt being followed, the system takes an action am – 7r(6).",
        "The machine is rewarded with r(sm, am) and the state transtitions to a new unobserved state s'm with probability T(sm,am,s'm).",
        "The machine then receives an observation o' £ O, with probability dependent only on the new state s'm and the machine action am.",
        "The belief state is updated based on the events of the turn and the cycle repeats.",
        "The belief state update is computed as where k is a normalisation constant(Kaelbling et al., 1998).",
        "Maintaining this belief state as the dialog evolves is called belief monitoring.",
        "Figure 1 shows a graphical representation of a POMDP based dialogue system.",
        "When the user utters a user act au",
        "Real user/ Simulator",
        "Belief Estimator",
        "Dialogue Policy expected reward",
        "Reinforcement Learning",
        "Optimise",
        "Figure 1 : Training a POMDP with a simulated user it is transmitted via speech to the dialogue system.",
        "From the machine's point of view, the speech acts as a noisy channel so that the observation received, o, is not necessarily the true dialogue act.",
        "Instead it typically describes an n-best list of hypothesised user acts.",
        "Based on this, the POMDP belief state is updated and a machine dialogue act, am, is selected.",
        "As can be seen from the diagram, it is quite trivial to replace a real user with a simulated one so that training can be performed less laboriously.",
        "Given a particular policy, the infinite horizon expected reward as a function of belief state is called the value function.",
        "It is calculated as:",
        "The goal of POMDP policy optimisation is to find the policy that maximises the value function at every point b.",
        "It can be shown that such a function always exists and is both continuous and convex.",
        "In the context of policy optimisation it is also useful to define the concept of a Q function(Sutton and Barto, 1998).",
        "This is a function of both belief state b and action am and is simply the expected reward obtained by first taking action am and then following the policy tt.",
        "The Bellman Optimality Equation states that a policy is optimal if and only if",
        "As discussed in the introduction, directly optimising POMDPs for dialogue systems is completely impractical.",
        "Instead, the belief state and actions are mapped down to a summarised form where optimisation becomes tractable.",
        "In this context, the original belief space and actions are",
        "(Master Space) (Master Actions) (Summary Space) (Summary Actions)",
        "called master space and master actions, while the summarised versions are called summary space and summary actions.",
        "Action selection in the full model would be a mapping from a belief state b g H(Sm) to an action am € Am.",
        "The summary POMDP splits this up as follows.",
        "The model initially extends the standard POMDP with a set of summary actions Am and a mapping from summary actions to master actions F. This function should be allowed access to the master belief space so that the summary can be as brief as possible (formally, F : Am x n(5m) – > Am).",
        "Next, a summarising function / is defined from master belief space H(Sm) to summary belief space Rk.",
        "Finally, a summary policy tt is defined as a mapping from summary space Rk to summary actions Am.",
        "A policy in master space is composed from the above three functions by first mapping to summary space via /, using policy tt to find an appropriate summary action am and then obtaining a master action with F. The full process is shown graphically in Figure 2.",
        "Algebraically the master policy is defined by:",
        "Further explanation of the Summary POMDP method can be found in (Williams and Young, 2005).",
        "Note that the formalism introduced above may be used for both the summary methods previously used for dialogue systems as well as belief compression techniques used in a more general setting (Roy et al., 2005)",
        "At the summary level, policies may not have enough information to act truly optimally.",
        "Hence defining an optimal summary policy is not so obvious.",
        "If / is chosen well, however, then one could hope that the optimal action is dependent only on /(&).",
        "If this is true then a summary policy is called optimal when the following equation holds for every b such that /(&) = (cc):"
      ]
    },
    {
      "heading": "3. Summarised Q-learning",
      "text": [
        "Q-learning is a technique for online learning traditionally used in an MDP framework.",
        "It is an iterative Monte-Carlo style algorithm where a sequence of sample dialogues are used to estimate the Q functions for each state and action.",
        "Inspired by grid-based methods, the summarised Q-learning algorithm discretises summary space and uses Q-learning on the resulting MDP-like grid.",
        "Operation of the algorithm proceeds by simply engaging the dialogue manager with either a real user or a user model.",
        "At each point where the system must choose an action, the master belief space is mapped down to the summary level as described in Section 2.2.",
        "The nearest summary point in the grid is found and the optimal summary action given by that point is chosen.",
        "At the end of the dialogue, the discounted future reward is known for each stage where a choice was taken.",
        "This value is recorded along with the grid point where the decision was made, and the action chosen.",
        "This is a sample of the discounted future reward obtained by taking the particular action and then following the current policy - i.e. the Q-function evaluated at this grid point.",
        "If sufficient dialogues are done the mean of these values will give a good estimate of the true Q-value.",
        "In order to enable learning, an exploration paramater e is selected so that a random summary act will be chosen with probability e. After a batch of dialogues have been completed, the estimates of the Q-functions are updated with the new dialogue scores.",
        "The optimal action is then chosen for each point p by",
        "The selection of which points to put into the grid is a crucial part of the algorithm as one would like the most accuracy at points that will be visited often.",
        "As a result, this algorithm uses a variable grid method (Brafman, 1997; Bonet, 2002).",
        "During operation, when a point is reached that is further away from any other point than some threshold paramater, the point is added to the grid.",
        "This ensures that points are only included in the grid if needed.",
        "Grid-based methods are often criticised because they do not scale well to large state spaces (Pineau et al., 2003).",
        "However, when using the Summary POMDP method the state space is reduced significantly before the grid is applied.",
        "Although there are no convergence guarantees for this method in the context of Summary POMDPs, Q-learning does guarantee convergence to the optimal policy for standard MDPs.",
        "As can be seen from Figure 4, in practice the method does converge to a high performing policy after several thousand dialogues."
      ]
    },
    {
      "heading": "4. Agenda-Based Simulation",
      "text": [
        "As described in the introduction to this paper, online methods for training statistical dialogue managers allow the dialogue policy to be adapted and improved at runtime, ie.",
        "through interaction with real users.",
        "During the initial development phase however, many thousand training dialogues are needed to bootstrap the dialogue policy, and this is generally too time-consuming and expensive to be done with real users.",
        "A number of research groups (Levin et al., 2000; Scheffler and Young, 2002; Pietquin and Dutoit, 2005; Georgila et al., 2005; Rieser and Lemon, 2006) have thus investigated the use of user simulation tools for training the dialogue manager (DM).",
        "The simulation-based approach typically involves two steps.",
        "Firstly, a statistical user model (such as an n-gram or a graphical model) is trained on a limited amount of dialogue data.",
        "The model is then used to simulate dialogues with the interactively learning DM (see Schatzmann et al.",
        "(2006) for a literature review).",
        "Simulation is usually done at a semantic dialogue act level to avoid having to reproduce the variety of user utterances at the word-or acoustic level.",
        "The simulation-based approach assumes the presence of a small corpus of suitably annotated in-domain dialogues (Lemon et al., 2006).",
        "For the experiments presented in this paper, no such data was available for training the user model.",
        "Hence, it was necessary to develop a model which was simple enough for the model parameters to be handcrafted and yet capable of producing user behaviour realistic enough for training a prototype system.",
        "A similar approach has been previously taken by (Levin et al., 2000; Pietquin and Dutoit, 2005) but the performance of the learned dialogue policies was not evaluated using real users.",
        "Human-machine dialogue can be formalised on a semantic level as a sequence of state transitions and dialogue acts.",
        "At any time t, the user is in a state su, takes action au, transitions into the intermediate state s'u, receives machine action am, and transitions into the next state s\" where the cycle restarts.",
        "Assuming a Markovian state representation, user behaviour can be decomposed into three models: P(au\\su) for action selection, P(s'u\\au, su) for the state transition into s'u, and P(s't[\\am, s'u) for the transition into s\".",
        "'in this paper, the terms dialogue act and dialogue action are used interchangeably.",
        "The notation act(a=x, b=y,...) is used to represent a dialogue act of a given type act (such as inform or request with items a = x, b = y, etc.",
        "Inspired by agenda-based methods to dialogue management (Wei and Rudnicky, 1999) the approach described here factors the user state into an agenda A and a goal G.",
        "During the course of the dialogue, the goal G ensures that the user behaves in a consistent, goal-directed manner.",
        "G consists of constraints C which specify the required venue, eg.",
        "a centrally located bar serving beer, and requests R which specify the desired pieces of information, eg.",
        "the name, address and phone number (cf.",
        "Fig.",
        "3).",
        "The user agenda A is a stack-like structure containing the pending user dialogue acts that are needed to elicit the information specified in the goal.",
        "At the start of the dialogue a new goal is randomly generated using the system database and the agenda is populated by converting all goal constraints into inform acts and all goal requests into request acts.",
        "A bye act is added at the bottom of the agenda to close the dialogue.",
        "As the dialogue progresses the agenda is dynamically updated and acts are selected from the top of the agenda to form user acts au.",
        "In response to incoming machine acts am, new user acts are pushed onto the agenda and no longer relevant ones are removed.",
        "The agenda thus serves as a convenient way of tracking the progress of the dialogue as well as encoding the relevant dialogue history.",
        "As can be seen in Fig. 3 (turns 1-3), user acts can also be temporarily stored when actions of higher priority need to be issued first, hence providing the simulator with a simple model of user memory.",
        "At any time during the dialogue, the updated agenda of length N contains all dialogue acts the user intends to convey to the system.",
        "Since the agenda is ordered according to priority, with A\\N] denoting the top and A[l] denoting the bottom item, selecting the next user act simplifies to popping n items off the top of the stack.",
        "Hence, letting au[i] denote the ith item in the user act au and the action selection model becomes a Dirac delta function where A[N – n + 1..N] is a Matlab-like shorthand notation for the top n items on A and the random variable n corresponds to the level of initiative taken by the simulated user.",
        "In a statistical model the probability distribution over integer values for n should be conditioned on A and learned from dialogue data.",
        "For the purposes of",
        "Ro = SysO",
        "Usr 1 Sys 1 type = bar drinks – beer area = central",
        "Hello, how may I help you?",
        "inform(type = bar) inform(drinks = beer) inform(area = central) request(name) rcquest(addr) request(phone) bye{)",
        "I'm looking for a fine beer bar.",
        "Ok, a wine bar.",
        "What pricerange?",
        "ne.gate(drinks = beer) inform(price = cheap) inform(area = central) request(na,me) request(addr) request(j)hone) bye()",
        "No, beer please!",
        "You are looking for a beer bar, correct?",
        "Yeah something cheap in the town centre.",
        "Murphy's on Main Square serves cheap beer.",
        "request(phone) byeQ",
        "= cheap) central)",
        "bootstrapping the system, n can be assumed independent of A and any distribution P(n) that places the majority of its probability mass on small values of n can be used.",
        "The factorisation of su into A and G can now be applied to the state transition models P(s'u\\au,su) and P(s\"\\am.",
        "s'u).",
        "Letting A' denote the agenda after selecting au (as explained in the previous subsection) and using",
        "Using this definition of A' and assuming that the goal remains constant when the user executes au, the first state transition depending on au simplifies to",
        "Using su = (A, G), the chain rule of probability, and reasonable conditional independence assumptions, the second state transition based on am can be decomposed into goal update and agenda update modules:",
        "agenda update goal update",
        "When no restrictions are placed on A\" and G\", the space of possible state transitions is vast.",
        "The model parameter set is too large to be handcrafted and even substantial amounts of training data would be insufficient to obtain reliable estimates.",
        "It can however be assumed that A\" is derived from A' and that G\" is derived from G' and that in each case the transition entails only a limited number of well-defined atomic operations.",
        "The agenda transition from A' to A\" can be viewed as a sequence of push-operations in which dialogue acts are added to the top of the agenda.",
        "In a second \"clean-up\" step, duplicate dialogue acts, null() acts, and unnecessary request() acts for already filled goal request slots must be removed but this is a deterministic procedure so that it can be excluded in the following derivation for simplicity.",
        "Considering only the push-operations, the items 1 to N' at the bottom of the agenda remain fixed and the update model can be rewritten as follows:",
        "The first term on the RHS of Eq.",
        "17 can now be further simplified by assuming that every dialogue act item in am triggers one push-operation.",
        "This assumption can be made without loss of generality, because it is possible to push a null() act (which is later removed) or to push an act with more than one item.",
        "The advantage of this assumption is that the known number M of items in am",
        "inform(pricc = inform(area = request(na,me) request(addr) request(phone) bye()",
        "now determines the number of push-operations.",
        "Hence",
        "The expression in Eq.",
        "19 shows that each item am[i] in the system act triggers one push operation, and that this operation is conditioned on the goal.",
        "This model is now simple enough to be handcrafted using heuristics.",
        "For example, the model parameters can be set so that when the item x=y in am[i] violates the constraints in G\", one of the following is pushed onto A\": negate(), inform(x=z), deny(x=y, x=z), etc.",
        "The goal update model P(G\"\\am, G') describes how the user constraints C' and requests R' change with a given machine action am.",
        "Assuming that R\" is conditionally independent of C' given C\" it can be shown that",
        "To restrict the space of transitions from R' to R\" it can be assumed that each request slot (ag.",
        "addr,phone,etc.)",
        "is either filled using information in am or left unchanged.",
        "One can further assume that the value of any slot depends on its value at the previous time step, the value provided by am and that the transition needs to be conditioned on whether the information given in am matches the goal constraints.",
        "Using R[k] to denote the fc'th request slot we can approximate",
        "To simplify P(C\"\\am,R', C') we assume that C\" is derived from C' by either adding a new constraint, setting an existing constraint slot to a different value (eg.",
        "drinks=dontcare), or by simply changing nothing.",
        "The choice of transition does not need to be conditioned on the full space of possible am, R!",
        "and C'.",
        "Instead it can be conditioned on simple boolean flags such as \"Does amask for a slot in the constraint set?",
        "\", \"Does am signal that no item in the database matches the given constraints?",
        "\", etc.",
        "The model parameter set is then sufficiently small for handcrafted values to be assigned to the probabilities."
      ]
    },
    {
      "heading": "5. Evaluation",
      "text": [
        "The summary Q-learning algorithm and agenda-based user model were tested by implementing a POMDPbased dialogue system for a Tourist Information Domain.",
        "Users are assumed to have arrived in a town unknown to them and must find a bar, a hotel or a restaurant in the town subject to some constraints (eg.",
        "a cheap, Chinese restaurant in the centre of town).",
        "The town used was fictitious so that users could not know any of the venues.",
        "The speech recognition was implemented using the Application Toolkit for HTK (ATK) with a vocabulary of about 2000 words.",
        "A simple keyword-spotting semantic decoder was used to extract meaning representations (dialogue acts) from the output of the recogniser.",
        "The dialogue manager is based on the Hidden Information State (HIS) model (Young et al., 2007), which gives an efficient way of implementing the belief state update in a POMDP-based dialogue system.",
        "In the implementation used for testing, the town included approximately 40 possible venues.",
        "Eight different variables are used by the system in deciding which venue to recommend: type of venue; pricerange; area; proximity to a particular place; stars; drinks; food and music.",
        "Additionally, the user could ask for the average price, the phone number, the address or a comment on a particular venue.",
        "The model allows for a rich structure in possible user goals via simple ontology rules.",
        "For example, venues can only have a food concept if their type is restaurant.",
        "Hence, one would expect that most information retrieval type dialogues could be modeled in a similar manner.",
        "The HIS manager factors the machine state of the POMDP into three parts: the user's goal, the dialogue state and the last machine act.",
        "An important feature of the system is that indistinguishable user goals are grouped together into partitions.",
        "For example, if the user is trying to obtain information about restaurants and has not mentioned what type of food they would like, then restaurants will be grouped together regardless of the type of food they serve.",
        "In the HIS model, a hypothesis refers to the grouping of a partition with a dialog state.",
        "The splitting of the machine state into separate hypotheses provides for a simple mapping to summary state for the Q-learning algorithm, where only information from the top two hypotheses is included.",
        "The summary state used has five components: the probabilities of the two most probable hypotheses along with three summary features.",
        "These enumerate the possibilities for how many database items fall into the partition, a summary of the dialog state and the type of the last dialog act.",
        "Rewards were given based on task completion and the number of turns in the dialogue.",
        "The system was given 20 points for a successful dialogue and 0 for an unsuccessful one.",
        "One point was subtracted for each dialogue turn.",
        "This encourages the system to be sure of the user's goal, while penalising inefficient system behaviour.",
        "Training was done using the agenda-based user model described in Section 4.",
        "Initially, batches of 1000 dialogues were performed with no error-modeling, updating Q-values and optimal actions at the end of each batch.",
        "Figure 4 shows the average reward obtained from each of these policies over 1000 sample dialogues.",
        "The policy score converged after approximately 25 000 dialogues and reached an average score of around 14.",
        "This illustrates that the Q-learning approach described above does in fact converge in practice.",
        "Further training was then done including simulated errors by changing random concept words with a probability of 0.05.",
        "The prototype dialogue system described above was evaluated as part of the EU FP6 Talk Project.",
        "No online learning was performed during the evaluation.",
        "40 human subjects took part, each of whom completed 4 dialogues.",
        "In total, 160 dialogues were recorded, with 6452 turns and 21667 words.",
        "Subjects were selected to include a mix of native and non-native speakers and the average Word-Error-Rate was 29.8%.",
        "The performance of the system was measured based on the recommendation of a correct venue, ie.",
        "a venue matching all constraints specified in the given task (all tasks were designed to have exactly one solution).",
        "Based on this definition, 145 out of 160 dialogues (90.6%) were completed successfully, and the average number of turns to completion was 5.59 (if no correct venue was offered the full number of turns was counted).",
        "An extract from an example dialogue is given as an illustration in Fig. 5."
      ]
    },
    {
      "heading": "6. Summary and Future Work",
      "text": [
        "This paper has investigated a new online method for training a real-world POMDP-based dialogue system.",
        "A novel agenda-based user simulation technique was also presented for bootstrapping the dialogue manager without access to training data.",
        "Although there is no guarantee of convergence, evaluation results suggest that in practice the training algorithm does converge to a suitable policy.",
        "The results also show that the learned policy works well",
        "Figure 5: Sample test dialogue along with most likely output from the automatic speech recognition (ASR) and decoded dialogue act (DA).",
        "The dialogue act format is described in (Young et al., 2005).",
        "for human users.",
        "Future work will focus on finding better mappings to summary space, on training the simulated user with recorded data and on investigating the effectiveness of online training with real users.",
        "Turn 1:",
        "System:",
        "Hello and welcome to our dialogue system!",
        "Are you looking for a hotel, a restaurant",
        "or a bar?",
        "User:",
        "I'm looking for a five star hotel, preferably",
        "in the East side of town, near the park",
        "ASR:",
        "I'm looking for a five star hotel preferably",
        "use side of town near the park",
        "DA:",
        "request(hotel,stars=5 ,near=Park)",
        "Turn 2:",
        "System:",
        "What part of town do you have in mind?",
        "For example something in the centre of",
        "town or in the north or on the riverside",
        "User:",
        "In the East side of town",
        "ASR:",
        "In the East side of town",
        "DA:",
        "inform(area=east)",
        "Turn 3:",
        "System:",
        "Did you say you are looking for a hotel",
        "in the east part of town?",
        "User:",
        "Yes",
        "ASR:",
        "Yes",
        "DA:",
        "affirmO",
        "Turn 4:",
        "System:",
        "The Royal Hotel is a great hotel in the",
        "east part of town",
        "User:",
        "I was wondering whether I could get the",
        "telephone number for that town...",
        "for that hotel?",
        "ASR:",
        "was wondering about could get the",
        "telephone number of the town for a hotel",
        "DA:",
        "request(phone,hotel)",
        "Turn 5:",
        "System:",
        "The Royal Hotel has 5 stars and is near",
        "the Park",
        "User:",
        "Could I have the phone number?",
        "ASR:",
        "I have the phone number",
        "DA:",
        "request (phone)",
        "Turn 6:",
        "System:",
        "No problem the phone number of the Royal",
        "Hotel is 7027003"
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This research was partly funded by the EU FP6 TALK Project (http://www.talk-project.org).",
        "The system evaluation was conducted in collaboration with Oliver Lemon, Kalliroi Georgila and Jamie Henderson at Edinburgh University and their work is gratefully acknowledged.",
        "We would also like to thank the anonymous referees for their helpful comments."
      ]
    }
  ]
}
