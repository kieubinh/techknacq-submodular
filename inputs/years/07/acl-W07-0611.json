{
  "info": {
    "authors": [
      "Tamas Biro"
    ],
    "book": "Workshop on Cognitive Aspects of Computational Language Acquisition",
    "id": "acl-W07-0611",
    "title": "The Benefits of Errors: Learning an OT Grammar with a Structured Candidate Set",
    "url": "https://aclweb.org/anthology/W07-0611",
    "year": 2007
  },
  "references": [
    "acl-P00-1046",
    "acl-P97-1040"
  ],
  "sections": [
    {
      "text": [
        "ACLC, Universiteit van Amsterdam Spuistraat 210 Amsterdam, The Netherlands t.s.biro@uva.nl",
        "We compare three recent proposals adding a topology to OT: McCarthy's Persistent OT, Smolensky's ICS and Biro's SA-OT.",
        "To test their learnability, constraint rankings are learnt from SA-OT's output.",
        "The errors in the output, being more than mere noise, follow from the topology.",
        "Thus, the learner has to reconstructs her competence having access only to the teacher's performance."
      ]
    },
    {
      "heading": "1. Introduction: topology and OT",
      "text": [
        "The year 2006 witnessed the publication of several novel approaches within Optimality Theory (OT) (Prince and Smolensky, 1993 aka 2004) introducing some sort of neighbourhood structure (topology, geometry) on the candidate set.",
        "This idea has been already present since the beginnings of OT but its potentialities had never been really developed until recently.",
        "The present paper examines the learn-ability of such an enriched OT architecture.",
        "Traditional Optimality Theory's GEN function generates a huge candidate set from the underlying form (UF) and then EVAL finds the candidate w that optimises the Harmony function H (w) on this unrestricted candidate set.",
        "H(w) is derived from the violation marks assigned by a ranked set of constraints to w. The surface form SF corresponding to UF is the (globally) optimal element of GEN(UF):",
        "Proceedings of the Workshop on Cognitive Aspects of Prague, Czech Republic, June 2007 ©2007",
        "restricting GEN, creating an alternative closer to standard derivations.",
        "Based the iterative syllabification in Imdlawn Tashlhiyt Berber, they suggest: \"some general procedure (Do-a) is allowed to make a certain single modification to the input, producing thecandidate set ofallpossible outcomes of such modification.\"",
        "The outputs of Do-a are \"neighbours\" of its input, so Do-a defines a topology.",
        "Subsequently, EVAL finds the most harmonic element of this restricted candidate set, which then serves again as the input of Do-a.",
        "Repeating this procedure again and again produces a sequence of neighbouring candidates with increasing Harmony, which converges toward the surface form.",
        "Calling Do-a a restricted GEN, as opposed to the freedom of analysis offered by the traditional GEN, McCarthy (2006) develops this idea into the Persistent OT architecture (aka.",
        "harmonic serialism, cf. references in McCarthy 2006).",
        "He demonstrates on concrete examples how repeating the GEN – EVAL – GEN – EVAL – ... cycle until reaching some local optimum will produce a more restrictive language typology that conforms rather well to observation.",
        "Importantly for our topic, learnabil-ity, he claims that Persistent OT \"canimpose stricter ranking requirements thanclassic OTbecause ofthe need to ensure harmonic improvement in the intermediate forms as well as the ultimate output\".",
        "In two very different approaches, both based on the traditional concept of GEN, Smolensky's Integrated Connectionist/Symbolic (ICS) Cognitive Architecture (Smolensky and Legendre, 2006) and the strictly symbolic Simulated Annealing for Optimality Theory Algorithm (SA-OT) proposed by",
        "Computational Language Acquisition, pages 81-88, Association for Computational Linguistics",
        "Biro (2005a; 2005b; 2006a), use simulated annealing to find the best candidate w in equation (1).",
        "Simulated annealing performs a random walk on the search space, moving to a similar (neighbouring) element in each step.",
        "Hence, it requires a topology on the search space.",
        "In SA-OT this topology is directly introduced on the candidate set, based on a linguistically motivated symbolic representation.",
        "At the same time, connectionist OT makes small changes in the state of the network; so, to the extent that states correspond to candidates, we obtain again a neighbourhood relation on the candidate set.",
        "Whoever introduces a neighbourhood structure (or a restricted GEN) also introduces local optima: candidates more harmonic than all their neighbours, independently of whether they are globally optimal.",
        "Importantly, each proposal is prone to be stuck in local optima.",
        "McCarthy s model repeats the generation-evaluation cycle as long as the first local optimum is not reached; whereas simulated annealing is a heuristic optimisation algorithm that sometimes fails to find the global optimum and returns another local optimum.",
        "How do these proposals influence the OT \"philosophy\"?",
        "For McCarthy, the first local optimum reached from UF is the grammatical form (the surface form predicted by the linguistic competence model), so he rejects equation (1).",
        "Yet, Smolensky and Biro keep the basic idea of OT as in (1), and Biro (2005b; 2006a) shows the errors made by simulated annealing can mimic performance errors (such as stress shift in fast speech).",
        "So mainstream Optimality Theory remains the model oflinguistic competence, whereas its cognitively motivated, though imperfect implementation with simulated annealing becomes a model of linguistic performance.",
        "Or, as Biro puts it, a model of the dynamic language production process in the brain.",
        "(See also Smolensky and Legendre (2006), vol.",
        "1, pp.",
        "227-229.)",
        "In the present paper we test the learnability of an OT grammar enriched with a neighbourhood structure.",
        "To be more precise, we focus on the latter approaches: how can a learner acquire a grammar, that is, the constraint hierarchy defining the Harmony function H (w), if the learning data are produced by a performance model prone to make errors?",
        "What is the consequence of seeing errors not simply as mere noise, but as the result of a specific mechanism?"
      ]
    },
    {
      "heading": "2. Walking in the candidate set",
      "text": [
        "First, we introduce the production algorithms (section 2) and a toy grammar (section 3), before we can run the learning algorithms (section 4).",
        "Equation (1) defines Optimality Theory as an optimisation problem, but finding the optimal candidate can be NP-hard (Eisner, 1997).",
        "Past solutions – chart parsing (Tesar and Smolensky, 2000; Kuhn, 2000) and finite state OT (see Biro (2006b) for an overview) – require conditions met by several, but not by all linguistic models.",
        "They are also \"too perfect\", not leaving room for performance errors and computationally too demanding, hence cognitively not plausible.",
        "Alternative approaches are heuristic optimization techniques: genetic algorithms and simulated annealing.",
        "These heuristic algorithms do not always find the (globally) optimal candidate, but are simple and still efficient because they exploit the structure of the candidate set.",
        "This structure is realized by a neighbourhood relation: for each candidate w there exists a set Neighbours(w), the set of the neighbours of w. It is often supposed that neighbours differ only minimally, whatever this means.",
        "The neighbourhood relation is usually symmetric, irreflexive and results in a connected structure (any two candidates are connected by a finite chain of neighbours).",
        "The topology (neighbourhood structure) opens the possibility to a (random) walk on the candidate set: a series wo, w1;w2,...,wl such that for all 0 < i < L, candidate wj+1 is wj or a neighbour of wj.",
        "(Candidate w0 will be called winit, and wL will be wfinai, henceforth.)",
        "Genetic algorithms start with a random population of winit s, and employ OT s EVAL function to reach a population of wfinai's dominated by the (globally) optimal candi-date(s) (Turkel, 1994).",
        "In what follows, however, we focus on algorithms using a single walk only.",
        "The simplest algorithm, gradient descent, comes in two flavours.",
        "The version on Fig. 1 defines wj+1as the best element of set |wi}UNeighbours(wi).",
        "It runs as long as wj+1 differs from wj, and is deterministic for each winit.",
        "Prince and Smolensky's and McCarthy s serial evaluation does exactly this: winit is the underlying form, Do-a (the restricted GEN) creates the set {w} UNeighbours(w), and EVAL finds its best element.",
        "ALGORITHM Gradient Descent: OT with restricted GEN w := w_init; repeat w := most_harmonic_element( {w_prev} U Neighbours(w_prev) ); until w = w_prev return w # w is an approximation to the optimal solution",
        "ALGORITHM Randomized Gradient Descent w := w_init ; repeat",
        "The second version of gradient descent is stochastic (Figure 2).",
        "In step i, a random w' G Neighbours(wj) is chosen using some predefined probability distribution on Neighbours(wj) (often a constant function).",
        "If neighbour w' is not worse than wj, then the next element wj+1 of the random walk will be w'; otherwise, wj+1 is wj.",
        "The stopping condition requires the number of iterations reach some value, or the average improvement of the target function in the last few steps drop below a threshold.",
        "The output is wfinai, a local optimum if the walk is long enough.",
        "Simulated annealing (Fig.",
        "3) plays with this second theme to increase the chance of finding the global optimum and avoid unwanted local optima.",
        "The idea is the same, but ifw' is worse than wj, then there is still a chance to move to w'.",
        "The transition probability of moving to w' depends on the target function E at wj and w', and on 'temperature' T: P{Wl w'\\T) = exp (-%,)-%)).",
        "Using a random r, we move to w' iff r < P(wj – w'|T).",
        "Temperature T is gradually decreased following the cooling schedule.",
        "Initially the system easily climbs larger hills, but later it can only descend valleys.",
        "Importantly, the probability wj}nai is globally optimal converges to 1 as the number of iterations grows.",
        "But the target function is not real-valued in Optimality Theory, so how can we calculate the transition probability?",
        "ICS (Smolensky and Legendre, 2006) approximates OT s harmony function with a real-valued target function, while Biro (2006a) introduces a novel algorithm (SA-OT, Figure 4) to guarantee the principle of strict domination in the constraint ranking.",
        "The latter stays on the purely symbolic level familiar to the linguist, but does not always display the convergence property of traditional simulated annealing.",
        "Temperature in the SA-OT Algorithm is a pair (K, t) with t > 0, and is diminished in two, embedded loops.",
        "Similarly, the difference in the target function (Harmony) is not a single real number but a pair (C, d).",
        "Here C is the fatal constraint, the highest ranked constraint by which wj and w' behave differently, while d is the difference of the violations of this constraint.",
        "(For H(wj) = H(w') let the difference be (0,0).)",
        "Each constraint is assigned a real-valued rank (most often an integer; we shall call it a K-value) such that a higher ranked constraint has a higher K-value than a lower ranked constraint (hierarchies are fully ranked).",
        "The K-value of the fatal constraint corresponds to the first component of the temperature, and the second component of the difference in the target function corresponds to the second component of the temperature.",
        "The transition probability from wj to its neighbour w' is 1 if w' is not less harmonic than wj; otherwise, the originally exponential transition probability becomes",
        "fl if K-value of C< K",
        "ALGORITHM Simulated Annealing w := w_init ; T := T_max ;",
        "CHOOSE random w' in Neighbours(w); else # move to w' with transition probability P(Delta;T) = exp(-Delta/T):",
        "generate random r uniformly in range (0,1); until stopping condition = true return w # w is an approximation to the minimal solution",
        "Figure 3: Minimizing areal-valued energy function E(w) with simulated annealing.",
        "Again, wi+1 is w' if the random number r generated between 0 and 1 is less than this transition probability; otherwise wi+1 = w».",
        "Biro (2006a, Chapt.",
        "2-3) argues that this definition fits best the underlying idea behind both OT and simulated annealing.",
        "In the next part of the paper we focus on SA-OT, and return to the other algorithms afterwards only."
      ]
    },
    {
      "heading": "3. A string grammar",
      "text": [
        "To experiment with, we now introduce an abstract grammar that mimics real phonological ones.",
        "Let the set of candidates generated by GEN for any input be {0,1,P – 1}L, the set of strings of length L over an alphabet of P phonemes.",
        "We shall use L = P = 4.",
        "Candidate w' is a neighbour of candidate w if and only if a single minimal operation (a basic step) transforms w into w'.",
        "A minimal operation naturally fitting the structure of the candidates is to change one phoneme only.",
        "In order to obtain a more interesting search space and in order to meet some general principles – the neighbourhood relation should be symmetric, yielding a connected graph but be minimal – a basic step can only change the value of a phoneme by 1 modulo P. For instance, in the L = P = 4 case, neighbours of 0123 are among others 1123, 3123, 0133 and 0120, but not 1223, 2123 or 0323.",
        "If the four phonemes are represented as a pair of binary features (0 = [ – ], 1 = [+ – ], 2 = [++] and 3 = [ – +]), then this basic step alters exactly one feature.",
        "We also need constraints.",
        "Constraint Non counts the occurrences of phoneme n (0 < n < P) in the candidate (i.e., assigns one violation mark per phoneme n).",
        "Constraint No-initial-n punishes phoneme n word initially only, whereas No-final-n does the same word finally.",
        "Two more constraints sum up the number of dissimilar and similar pairs of adjacent phonemes.",
        "Let w^) be the ith phoneme in string w, and let [b] = 1 if b is true and [b] =0 if b is false; then we have 3P + 2 markedness constraints:",
        "Grammars also include faithfulness constraints punishing divergences from a reference string <r, usually the input.",
        "Ours sums up the distance of the phonemes in w from the corresponding ones in <r:",
        "where d(a,b) = min((a – b) mod P, (b – a) mod P)) is the minimal number of basic steps transforming phoneme a into b.",
        "In our case, faithfulness is also the number of differing binary features.",
        "To illustrate SA-OT, we shall use grammar H:",
        "A quick check proves that the global optimum is candidate 3333, but there are many other local optima: 1111, 2222, 3311, 1333, etc.",
        "Table 1 shows the frequencies of the outputs as a function of t_step, all other parameters kept unchanged.",
        "Several characteristics of SA-OT can be observed.",
        "For high t_step, the thirteen local optima ({1, 3}and 2222) are all produced, but as the number of",
        "No-n:",
        "non(w)",
        "= £i=o [w(i)",
        "= n]",
        "No-initial-n:",
        "nin(w)",
        "= [w(o) = n]",
        "No-final-n:",
        "nfn(w)",
        "= £ i=o2[w(i)",
        "n]",
        "Assimilate:",
        "ass(w)",
        "+ W(i+i)]",
        "Dissimilate:",
        "dis(iy)",
        "= £i=o Ri)",
        "= W(i+i)]",
        "return w # w is an approximation to the optimal solution",
        "ALGORITHM Simulated Annealing for Optimality Theory w := w_init ; for K = K_max to K_min step K_step for t = t_max to t_min step t_step else w := w' with transition probability",
        "end-for end-for",
        "iterations increases (parameter t_step drops), the probability of finding the globally optimal candidate grows.",
        "In many grammars (e.g., nil and ni3 moved to between no0 and ass in H), the global optimum is the only output for small t_step values.",
        "Yet, H also yields irregular forms: 1111 and 2222 are not globally optimal but their frequencies grow together with the frequency of 3333."
      ]
    },
    {
      "heading": "4. Learning grammar from performance",
      "text": [
        "To summarise, given a grammar, that is, a constraint hierarchy, the SA-OT Algorithm produces performance forms, including the grammatical one (the global optimum), but possibly also irregular forms and performance errors.",
        "The exact distribution depends on the parameters of the algorithm, which are not part of the grammar, but related to external (physical, biological, pragmatic or sociolinguistic) factors, for instance, to speech rate.",
        "Our task of learning a grammar can be formulated thus: given the output distribution of SA-OT based on the target OT hierarchy (the target grammar), the learner seeks a hierarchy that produces a similar performance distribution using the same SA-OT Algorithm.",
        "(See Yang (2002) on grammar learning as parameter setting in general.)",
        "Without any information on grammaticality, her goal is not to mimic competence, not to find a hierarchy with the same global optima.",
        "The grammar learnt can diverge from the target hierarchy, as long as their performance is comparable (see also Apoussidou (2007), p. 203).",
        "For instance, if nil and ni3 change place in grammar H, the grammaticality of 1111 and 3333 are reversed, but the performance stays the same.",
        "This resembles two native speakers whose divergent grammars are revealed only when they judge differently forms otherwise produced by both.",
        "We suppose that the learner employs the same SA-OT parameter setting.",
        "The acquisition of the parameters is deferred to future work, because this task is not part of language acquisition but of social acculturation: given a grammar, how can one learn which situation requires what speed rate or what level of care in production?",
        "Consequently, fine-tuning the output frequencies, which can be done by fine-tuning the parameters (such as t_step) and not the grammar, is not our goal here.",
        "But language learners do not seem to do it, either.",
        "Learning algorithms in Optimality Theory belong to two families: off-line and on-line algorithms.",
        "Offline algorithms, the prototype of which is Recursive Constraint Demotion (RCD) (Tesar, 1995; Tesar and Smolensky, 2000), first collect the data and then attempt to build a hierarchy consistent with them.",
        "On-line algorithms, such as Error Driven Constraint Demotion (ECDC) (Tesar, l995; Tesar and Smolen-sky, 2000) and Gradual Learning Algorithm (GLA) (Boersma, l997; Boersma and Hayes, 200l), start with an initial hierarchy and gradually alter it based on discrepancies between the learning data and the data produced by the learner's current hierarchy.",
        "Since infants gather statistical data on their mother tongue-to-be already in pre-linguistic stages (Saffran et al., 1996; Gervain et al., submitted), an off-line algorithm created our initial grammar.",
        "Then, on-line learning refined it, modelling child language",
        "Table 1: Outputs of SA-OT for hierarchy H. \"Others\" are twelve forms, each with a frequency between 2% and 8% for t_step = 1, and lower than 4.5% for t_step = 0.1.",
        "(Forms produced in 8% of the cases at t_step = 1 are not produced if t_step = 0.01!)",
        "An experiment consisted of running 4096 simulations and counting relative frequencies; each cell contains the mean and standard deviation of three experiments.",
        "development.",
        "(Although on-line algorithms require virtual production only, not necessarily uttered in communication, we suppose the two go together.)",
        "We defer for future work issues as parsing hidden structures, learning underlying forms and biases for ranking markedness above faithfulness.",
        "We first implemented Recursive Constraint Demotion with SA-OT.",
        "To begin with, RCD creates a winner/loser table, in which rows correspond to pairs (w, l) such that winner w is a learning datum, and loser l is less harmonic than w. Column winner marks contains the constraints that are more severely violated by the winner than by the loser, and vice-versa for column loser marks.",
        "Subsequently, RCD builds the hierarchy from top.",
        "It repeatedly collects the constraints not yet ranked that do not occur as winner marks.",
        "If no such constraint exists, then the learning data are inconsistent.",
        "These constraints are then added to the next stratum of the hierarchy in a random order, while the rows in the table containing them as loser marks are deleted (because these rows have been accounted for by the hierarchy).",
        "Given the complexity of the learning data produced by SA-OT, it is an advantage of RCD that it recognises inconsistent data.",
        "But how to collect the winner-loser pairs for the table?",
        "The learner has no information concerning the grammaticality ofthe learning data, and only knows that the forms produced are local optima for the target (unknown) hierarchy and the universal (hence, known) topology.",
        "Thus, we constructed the winner-loser table from all pairs (w, l) such that w was an observed form, and l was a neighbour of w. To avoid the noise present in real-life data, we considered only w's with a frequency higher than VN, where N was the number of learning data.",
        "Applying then RCD resulted in a hierarchy that produced the observed local optima – and most often also many others, depending on the random constraint ranking in a stratum.",
        "These unwanted local optima suggest a new explanation of some \"child speech forms\".",
        "Therefore, more information is necessary to find the target hierarchy.",
        "As learners do not use negative evidence (Pinker, 1984), we did not try to remove extra local optima directly.",
        "Yet, the learners do collect statistical information.",
        "Accordingly, we enriched the winner/loser table with pairs (w, l) such that w was a form observed significantly more frequently than l; 1's were observed forms and the extra local optima.",
        "(A difference in frequency was significant if it was higher than V^V.)",
        "The assumption that frequency reflects harmony is based on the heuristics of SA-OT, but is far not always true.",
        "So RCD recognised this new table often to be inconsistent.",
        "Enriching the table could also be done gradually, adding a new pair only if enough errors have supported it (Error-Selective Learning, Tessier (2007).",
        "The pair is then removed if it proves inconsistent with stronger pairs (pairs supported by more errors, or pairs of observed forms and their neighbours).",
        "Yet, we instead turned to real on-line algorithms, namely to Boersma's Gradual Learning Algorithm (GLA) (Boersma, 1997).",
        "(Error Driven Constraint Demotion is not robust, and gets stuck for inconsistent data.)",
        "Similarly to Error-Selective Learning, GLA accumulates gradually the arguments for reranking two constraints.",
        "The GLA Algorithm assigns a real-valued rank r to each constraint, so that a higher ranked constraint has a higher r. Then, in each learning step the learning datum (the winner) is compared to the output produced by the learner's actual hierarchy (the loser).",
        "Every constraint's rank is decreased by a small value (the plasticity) if the winner violates it more than the loser, and it is increased by the same value if the loser has more violations than the winner.",
        "Often – still, not always (Pater, 2005) – these small steps accumulate to converge towards the correct constraint ranking.",
        "output",
        "t_step = 1",
        "t_step = 0.1",
        "t_step = 0.01",
        "t_step = 0.001",
        "3333",
        "0.1174 ± 0.0016",
        "0.2074 ± 0.0108",
        "0.2715 ± 0.0077",
        "0.3107 ± 0.0032",
        "1111",
        "0.1163 ± 0.0021",
        "0.2184 ± 0.0067",
        "0.2821 ± 0.0058",
        "0.3068 ± 0.0058",
        "2222",
        "0.1153 ± 0.0024",
        "0.2993 ± 0.0092",
        "0.3787 ± 0.0045",
        "0.3602 ± 0.0091",
        "1133",
        "0.0453 ± 0.0018",
        "0.0485 ± 0.0038",
        "0.0328 ± 0.0006",
        "0.0105 ± 0.0014",
        "3311",
        "0.0436 ± 0.0035",
        "0.0474 ± 0.0054",
        "0.0344 ± 0.0021",
        "0.0114 ± 0.0016",
        "others",
        "0.5608",
        "0.1776",
        "< 0.0002",
        "-",
        "When producing an output (the winner) for the target hierarchy and another one (the loser) for the learner's hierarchy, Boersma uses Stochastic OT (Boersma, 1997).",
        "But one can also employ traditional OT evaluation, whereas we used SA-OT with t_step = 0.1.",
        "The learner's actual hierarchy in GLA is stored by the real-valued ranks r. So the fatal constraint in the core of SA-OT (Fig.",
        "4) is the constraint that has the highest r among the constraints assigning different violations to w and w'.",
        "(A random one ofthem, ifmore constraints have the same r-values, but this is very rare.).",
        "The K-values were the floor of the r-values.",
        "(Note the possibility of more constraints having the same K-value.)",
        "The r-values could also be directly the K-values; but since parameters K_max, K_min and K_step are integers, this would cause the temperature not enter the domains of the constraints, which would skip an important part of simulated annealing.",
        "Similarly to Stochastic OT, our model also displayed different convergence properties of GLA.",
        "Quite often, GLA reranked its initial hierarchy (the output of RCD) into a hierarchy yielding the same or a similar output distribution to that produced by the target hierarchy.",
        "The simulated child's performance converged towards the parent's performance, and \"child speech forms\" were dropped gradually.",
        "In other cases, however, the GLA algorithm turned the performance worse.",
        "The reason for that might be more than the fact that GLA does not always converge.",
        "Increasing or decreasing the constraints' rank by a plasticity in GLA is done in order to make the winners gradually better and the losers worse.",
        "But in SA-OT the learner's hierarchy can produce a form that is indeed more harmonic (but not a local optimum) for the target ranking than the learning datum; then the constraint promotions and demotions miss the point.",
        "Moreover, unlike in Stochastic OT, these misguided moves might be more frequent than the opposite moves.",
        "Still, the system performed well with our grammar H. Although the initial grammars returned by RCD included local optima (\"child speech forms\", e.g., 0000), learning with GLA brought the learner's performance most often closer to the teacher's.",
        "Still, final hierarchies could be very diverse, with different global optima and frequency distributions.",
        "In another experiment the initial ranking was the target hierarchy.",
        "Then, 13 runs returned the target distribution with some small changes in the hierarchy; in five cases the frequencies changed slightly, but twice the distribution became qualitatively different (e.g., 2222 not appearing).",
        "Learning in the ICS architecture involves similar problems to those encountered with SA-OT.",
        "The learner is faced again with performance forms that are local optima and not always better than unattested forms.",
        "The learning differs exclusively as a consequence of the connectionist implementation.",
        "In McCarthy's Persistent OT, the learner only knows that the observed form is a local optimum, i. e., it is better than all its neighbours.",
        "Then, she has to find a path backwards, from the surface form to the underlying form, such that in each step the candidate closer to the SF is better than all other neighbours of the candidate closer to the UF.",
        "Hence, the problem is more complex, but it results in a similar winner/loser table of locally close candidates."
      ]
    },
    {
      "heading": "5. Conclusion and future work",
      "text": [
        "We have tested the learnability of an OT grammar enriched with a neighbourhood structure.",
        "The learning data were produced by a performance model (viz., SA-OT), so the learner only had access to the teacher's performance.",
        "But by knowing the mechanism distorting production, she still could learn the target competence more or less.",
        "(Minor differences in competence are possible, as long as the performance is very similar.)",
        "She made use of the structure (the topology) of the candidate set, but also of the observed error patterns.",
        "Future work may exploit the fact that different parameter settings of SA-OT yield different distributions.",
        "Not correctly reconstructed grammars often lead to different grammaticality judgements, but also to quantitative differences in the performance distribution, despite the qualitative similarity.",
        "This fact can explain diachronic changes and why some grammars are evolutionarily more stable than others.",
        "Inaccurate reconstruction, as opposed to exact learning, is similar to what Dan Sperber and others said about symbolic-cultural systems: \"Thetacit knowledge of a participant in a symbolic-cultural system is neither taught nor learned by rote.",
        "Rather each new participant [...] reconstructs the rules which govern the symbolic-cultural system in question.",
        "These reconstructions maydiffer considerably, depending upon such factors as the personal history ofthe individual in question.",
        "Consequently, the products of each individual's symbolic mechanism are idiosyncratic tosome extent.\"",
        "(Lawson and McCauley, 1990, p. 68, italics are original).",
        "This observation has been used to argue that cultural learning is different from language learning; now we turn the table and claim that acquiring a language is indeed similar in this respect to learning a culture."
      ]
    }
  ]
}
