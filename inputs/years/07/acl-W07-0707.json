{
  "info": {
    "authors": [
      "Maja Popović",
      "Hermann Ney"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W07-0707",
    "title": "Word Error Rates: Decomposition over POS classes and Applications for Error Analysis",
    "url": "https://aclweb.org/anthology/W07-0707",
    "year": 2007
  },
  "references": [
    "acl-C00-2162",
    "acl-E06-1031",
    "acl-H05-1085",
    "acl-P02-1040",
    "acl-P04-1079",
    "acl-W05-0903",
    "acl-W05-0909",
    "acl-W06-3101"
  ],
  "sections": [
    {
      "text": [
        "Word Error Rates: Decomposition over Pos Classes and Applications for",
        "Error Analysis",
        "Maja PopoviC Hermann Ney",
        "Lehrstuhl für Informatik 6 Lehrstuhl für Informatik 6",
        "RWTH Aachen University RWTH Aachen University",
        "Aachen, Germany Aachen, Germany",
        "popovic@cs.rwth-aachen.de ney@cs.rwth-aachen.de",
        "Evaluation and error analysis of machine translation output are important but difficult tasks.",
        "In this work, we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of Word Error Rate (Wer) and Position independent word Error Rate (Per) over different Part-of-Speech (Pos) classes.",
        "Furthermore, we investigate two possible aspects of the use of these decompositions for automatic error analysis: estimation of inflectional errors and distribution of missing words over Pos classes.",
        "The obtained results are shown to correspond to the results of a human error analysis.",
        "The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Evaluation of machine translation output is a very important but difficult task.",
        "Human evaluation is expensive and time consuming.",
        "Therefore a variety of automatic evaluation measures have been studied over the last years.",
        "The most widely used are Word Error Rate (Wer), Position independent word Error Rate (Per), the Bleu score (Papineni et al., 2002) and the Nist score (Doddington, 2002).",
        "These measures have shown to be valuable tools for comparing different systems as well as for evaluating improvements within one system.",
        "However, these measures do not give any details about the nature oftranslation errors.",
        "Therefore some more detailed analysis ofthe generated output is needed in order to identify the main problems and to focus the research efforts.",
        "A framework for human error analysis has been proposed in (Vilar et al., 2006), but as every human evaluation, this is also a time consuming task.",
        "This article presents a framework for calculating the decomposition of Wer and Per over different Pos classes, i.e. for estimating the contribution of each Pos class to the overall word error rate.",
        "Although this work focuses on Pos classes, the method can be easily extended to other types of linguistic information.",
        "In addition, two methods for error analysis using the Wer and Per decompositons together with base forms are proposed: estimation of inflectional errors and distribution of missing words over Pos classes.",
        "The translation corpus used for our error analysis is built in the framework of the Tc-Star project (tcs, 2005) and contains the transcriptions of the European Parliament Plenary Sessions (Epps) in Spanish and English.",
        "The translation system used is the phrase-based statistical machine translation system described in (Vilar et al., 2005; Matusov et al., 2006)."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Automatic evaluation measures for machine translation output are receiving more and more attention in the last years.",
        "The Bleu metric (Papineni et al., 2002) and the closely related Nist metric (Doddington, 2002) along with Wer and Per have been widely used by many machine translation researchers.",
        "An extended version of Bleu which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004).",
        "(Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures Wer, Per, Bleu andNiST.",
        "The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output).",
        "A new automatic metric Meteor (Banerjee and Lavie, 2005) uses stems and synonyms of the words.",
        "This measure counts the number of exact word matches between the output and the reference.",
        "In a second step, unmatched words are converted into stems or synonyms and then matched.",
        "The Ter metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference.",
        "The CDer measure (Leusch et al., 2006) is based on edit distance, such as the well-known Wer, but allows reordering of blocks.",
        "Nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc.",
        "A framework for human error analysis has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out.",
        "However, human error analysis, like any human evaluation, is a time consuming task.",
        "Whereas the use of linguistic knowledge for improving the performance of a statistical machine translation system is investigated in many publications for various language pairs (like for example (NieBen and Ney, 2000), (Goldwater and Mc-Closky, 2005)), its use for the analysis of translation errors is still a rather unexplored area.",
        "Some automatic methods for error analysis using base forms and Pos tags are proposed in (Popovic et al., 2006; Popovic and Ney, 2006).",
        "These measures are based on differences between Wer and Per which are calculated separately for each Pos class using subsets extracted from the original texts.",
        "Standard overall Wer and Per of the original texts are not at all taken into account.",
        "In this work, the standard Wer and Per are decomposed and analysed."
      ]
    },
    {
      "heading": "3. Decomposition of Wer and Per over Pos classes",
      "text": [
        "The standard procedure for evaluating machine translation output is done by comparing the hypothesis document hyp with given reference translations ref, each one consisting of K sentences (or segments).",
        "The reference document ref consists of R reference translations for each sentence.",
        "Let the length of the hypothesis sentence hypk be denoted as NhyPk, and the reference lengths of each sentence Nref k r. Then, the total hypothesis length of the document is Nhyp = Y^k Nhypk, and the total reference length is Nre/ = Ylk N*e/ where N*ef is defined as the length of the reference sentence with the lowest sentence-level error rate as shown to be optimal in (Leusch et al., 2005).",
        "The word error rate (Wer) is based on the Lev-enshtein distance (Levenshtein, 1966) - the minimum number of substitutions, deletions and insertions that have to be performed to convert the generated text hyp into the reference text ref.",
        "A shortcoming of the Wer is the fact that it does not allow reorderings of words, whereas the word order ofthe hypothesis can be different from word order of the reference even though it is correct translation.",
        "In order to overcome this problem, the position independent word error rate (Per) compares the words in the two sentences without taking the word order into account.",
        "The PER is always lower than or equal to the Wer.",
        "On the other hand, shortcoming of the Per is the fact that the word order can be important in some cases.",
        "Therefore the best solution is to calculate both word error rates.",
        "Calculation of Wer: The Wer of the hypothesis hyp with respect to the reference ref is calculated as:",
        "where dL (ref k r, hypk) is the Levenshtein distance between the reference sentence ref k r and the hypothesis sentence hypk.",
        "The calculation of Wer is performed using a dynamic programming algorithm.",
        "Calculation of Per: The Per can be calculated using the counts n(e, hypk) and n(e, ref k r) of a word e in the hypothesis sentence hypk and the reference sentence ref k r respectively:",
        "min dPER(ref ^, hypk)",
        "The dynamic programming algorithm for Wer enables a simple and straightforward identification of each erroneous word which actually contributes to Wer.",
        "Let errk denote the set of erroneous words in sentence k with respect to the best reference and p be a Pos class.",
        "Then n(p, errk) is the number of errors in errk produced by words with Pos class p. It should be noted that for the substitution errors, the Pos class of the involved reference word is taken into account.",
        "Pos tags of the reference words are also used for the deletion errors, and for the insertion errors the Pos class of the hypothesis word is taken.",
        "The Wer for the word class p can be calculated as:",
        "The sum over all classes is equal to the standard overall Wer.",
        "An example of a reference sentence and hypothesis sentence along with the corresponding Pos tags is shown in Table 1.",
        "The Wer errors, i.e. actual words participating in Wer together with their Pos classes can be seen in Table 2.",
        "The reference words involved in Wer are denoted as reference errors, and hypothesis errors refer to the hypothesis words participating in Wer.",
        "Standard Wer of the whole sentence is equal to 4/12 = 33.3%.",
        "The contribution of nouns is reference:",
        "Mister#N Commissioner#N ,#Pun twenty-four#Num hours#N sometimes#Adv can#V be#V too#Adv much#Pron time#N .#Pun hypothesis:",
        "Mrs#N Commissioner#N ,#Pun twenty-four#Num hours#N is#V sometimes#Adv too#Adv much#Pron time#N .#Pun",
        "Table 1: Example for illustration of actual errors: a Pos tagged reference sentence and a corresponding hypothesis sentence",
        "Table 2: Wer errors: actual words which are participating in the word error rate and their corresponding Pos classes",
        "In contrast to Wer, standard efficient algorithms for the calculation of Per do not give precise information about contributing words.",
        "However, it is possible to identify all words in the hypothesis which do not have a counterpart in the reference, and vice versa.",
        "These words will be referred to as Per errors.",
        "Table 3: Per errors: actual words which are participating in the position independent word error rate and their corresponding Pos classes",
        "An illustration of Per errors is given in Table 3.",
        "reference errors",
        "hypothesis errors",
        "error type",
        "Mister#N sometime s#A dv can#V be#V",
        "Mrs#N is#V",
        "sometimes#Adv",
        "substitution substitution deletion substitution",
        "reference errors",
        "hypothesis errors",
        "Mister#N",
        "be#V",
        "can#V",
        "Mrs#N is#V",
        "The number of errors contributing to the standard Per according to the algorithm described in 3.1 is 3 - there are two substitutions and one deletion.",
        "The problem with standard PER is that it is not possible to detect which words are the deletion errors, which are the insertion errors, and which words are the substitution errors.",
        "Therefore we introduce an alternative Per based measure which corresponds to the F-measure.",
        "Let herr k refer to the set of words in the hypothesis sentence k which do not appear in the reference sentence k (referred to as hypothesis errors).",
        "Analogously, let rerr k denote the set of words in the reference sentence k which do not appear in the hypothesis sentence k (referred to as reference errors).",
        "Then the following measures can be calculated:",
        "• reference Per (RPer) (similar to recall):",
        "hypothesis Per (HPer) (similar to precision):",
        "F-based Per (FPer):",
        "Since we are basically interested in all words without a counterpart, both in the reference and in the hypothesis, this work will be focused on FPER.",
        "The sum of FPer over all Pos classes is equal to the overall FPer, and the latter is always less or equal to the standard Per.",
        "For the example sentence presented in Table 1, the number of hypothesis errors n(e, herr k) is 2 and the number of reference errors n(e, rerrk) is 3 where e denotes the word.",
        "The number oferrors contributing to the standard Per is 3, since |Nre/ – Nhyp | = 1 and Ye |n(e, ref k) – n(e, hypk)| = 5.",
        "The standard Per is normalised over the reference length",
        "Nre/ = 12 thus being equal to 25%.",
        "TheFPeristhe sum of hypothesis and reference errors divided by the sum of hypothesis and reference length: FPer = (2 + 3)/(11 + 12) = 5/23 = 21.7%.",
        "The contribution of nouns is FPer(N) = 2/23 = 8.7% and the contribution of verbs is FPer(V) = 3/23 = 13%."
      ]
    },
    {
      "heading": "4. Applications for error analysis",
      "text": [
        "The decomposed error rates described in Section 3.2 and Section 3.3 contain more details than the standard error rates.",
        "However, for more precise information about certain phenomena some kind of further analysis is required.",
        "In this work, we investigate two possible aspects for error analysis:",
        "• estimation of inflectional errors by the use of FPer errors and base forms",
        "• extracting the distribution of missing words over Pos classes using Wer errors, FPer errors and base forms.",
        "Inflectional errors can be estimated using FPer errors and base forms.",
        "From each reference-hypothesis sentence pair, only erroneous words which have the common base forms are taken into account.",
        "The inflectional error rate of each Pos class is then calculated in the same way as FPer.",
        "For example, from the Per errors presented in Table 3, the words \"is\" and \"be\" are candidates for an inflectional error because they are sharing the same base form \"be\".",
        "Inflectional error rate in this example is present only for the verbs, and is calculated in the same way as FPer, i.e. iFPer(V) = 2/23 = 8.7%.",
        "Distribution of missing words over Pos classes can be extracted from the Wer and FPer errors in the following way: the words considered as missing are those which occur as deletions in Wer errors and at the same time occur only as reference Per errors without sharing the base form with any hypothesis error.",
        "The use of both Wer and Per errors is much more reliable than using only the Wer deletion er-ros because not all deletion errors are produced by missing words: a number of Wer deletions appears due to reordering errors.",
        "The information about the base form is used in order to eliminate inflectional errors.",
        "The number ofmissing words is extracted for each word class and then normalised over the sum of all classes.",
        "For the example sentence pair presented in Table 1, from the Wer errors in Table 2 and the Per errors in Table 3 the word \"can\" will be identified as missing."
      ]
    },
    {
      "heading": "5. Experimental settings",
      "text": [
        "The machine translation system used in this work is based on the statistical aproach.",
        "It is built as a log-linear combination of seven different statistical models: phrase based models in both directions, I bm1 models at the phrase level in both directions, as well as target language model, phrase penalty and length penalty are used.",
        "A detailed description ofthe system can be found in (Vilar et al., 2005; Matusov et al., 2006).",
        "The corpus analysed in this work is built in the framework of the Tc-Star project.",
        "The training corpus contains more than one million sentences and about 35 million running words of the European Parliament Plenary Sessions (Epps) in Spanish and English.",
        "The test corpus contains about 1 000 sentences and 28 000 running words.",
        "The OOV rates are low, about 0.5% of the running words for Spanish and 0.2% for English.",
        "The corpus statistics can be seen in Table 4.",
        "More details about the Epps data can be found in (Vilar et al.,2005).",
        "Table 4: Statistics of the training and test corpora of the Tc-Star Epps Spanish-English task.",
        "Test corpus is provided with two references."
      ]
    },
    {
      "heading": "6. Error analysis",
      "text": [
        "The translation is performed in both directions (Spanish to English and English to Spanish) and the error analysis is done on both the English and the Spanish output.",
        "Morpho-syntactic annotation of the English references and hypotheses is performed using the constraint grammar parser EngCg (Vouti-lainen, 1995), and the Spanish texts are annotated using the FreeLing analyser (Carreras et al., 2004).",
        "In this way, all references and hypotheses are provided with Pos tags and base forms.",
        "The decomposition of Wer and FPer is done over the ten main Pos classes: nouns (N), verbs (V), adjectives (A), adverbs (Adv), pronouns (Pron), determiners (Det), prepositions (Prep), conjunctions (Con), numerals (Num) and punctuation marks (Pun).",
        "Inflectional error rates are also estimated for each Pos class using FPer counts and base forms.",
        "Additionally, details about the verb tense and person inflections for both languages as well as about the adjective gender and person inflections for the Spanish output are extracted.",
        "Apart from that, the distribution of missing words over the ten Pos classes is estimated using the Wer and FPer errors.",
        "Figure 1 presents the decompositions of Wer and FPer over the ten basic Pos classes for both languages.",
        "The largest part of both word error rates comes from the two most important word classes, namely nouns and verbs, and that the least critical classes are punctuations, conjunctions and numbers.",
        "Adjectives, determiners and prepositions are significantly worse in the Spanish output.",
        "This is partly due to the richer morphology of the Spanish language.",
        "Furthermore, the histograms indicate that the number of erroneus nouns and pronouns is higher in the English output.",
        "As for verbs, Wer is higher for English and FPer for Spanish.",
        "This indicates that there are more problems with word order in the English output, and more problems with the correct verb or verb form in the Spanish output.",
        "In addition, the decomposed error rates give an idea of where to put efforts for possible improvements of the system.",
        "For example, working on improvements of verb translations could reduce up to about 10% Wer and 7% FPer, working on nouns",
        "Train",
        "Spanish English",
        "Sentences",
        "1 167 627",
        "Running words",
        "35 320 646",
        "33 945 468",
        "Vocabulary",
        "159080",
        "110 636",
        "Test",
        "Sentences",
        "894",
        "1 117",
        "Running words",
        "28 591",
        "28 492",
        "OOVs",
        "0.52%",
        "0.25%",
        "Figure 1: Decomposition of Wer and FPer [%] over the ten basic Pos classes for English and Spanish output up to 8% Wer and 5% FPer, whereas there is no reason to put too much efforts on e.g. adverbs since this could lead only to about 2% of Wer and FPer reduction.",
        "Inflectional error rates for the ten Pos classes are presented in Figure 2.",
        "For the English language, these errors are significant only for two Pos classes: nouns and verbs.",
        "The verbs are the most problematic category in both languages, for Spanish having almost two times higher error rate than for English.",
        "This is due to the very rich morphology of Spanish verbs - one base form might have up to about fourty different inflections.",
        "Nouns have a higher error rate for English than for Spanish.",
        "The reason for this difference is not clear, since the noun morphology of neither of the languages is particularly rich - there is only distinction between singular and plural.",
        "one possible explanation might be the numerous occurences of different variants of the same word, like for example \"Mr\" and \"Mister\".",
        "In the Spanish output, two additional Pos classes are showing significant error rate: determiners and adjectives.",
        "This is due to the gender and number inflections of those classes which do not exist in the English language - for each determiner or adjective, there are four variants in Spanish and only one in English.",
        "Working on inflections of Spanish verbs might reduce approximately 2% of FPer, on English verbs about 1%.",
        "Improvements of Spanish determiners could lead up to about 2% of improvements.",
        "The results obtained for inflectional errors are comparable with the results of a human error analysis carried out in (Vilar et al., 2006).",
        "Although it is difficult to compare all the numbers directly, the overall tendencies are the same: the largest number of translation errors are caused by Spanish verbs, and much less but still a large number of errors by English verbs.",
        "A much smaller but still significant number of errors is due to Spanish adjectives, and only a few errors of English adjectives are present.",
        "Human analysis was done also for the tense and person of verbs, as well as for the number and gender of adjectives.",
        "We use more detailed Pos tags in order to extract this additional information and calculate inflectional error rates for such tags.",
        "It should be noted that in contrast to all previous error rates, these error rates are not disjunct but overlapping: many words are contributing to both.",
        "The results are shown in Figure 3, and the tendencies are again the same as those reported in (Vilar et al., 2006).",
        "As for verbs, tense errors are much more frequent than person errors for both languages.",
        "Adjective inflections cause certain amount of errors only in the Spanish output.",
        "Contributions of gender and of number are aproximately equal.",
        "Figure 3: More details about inflections: verb tense and person error rates and adjective gender and number error rates [%]",
        "Figure 4 presents the distribution of missing words over Pos classes.",
        "This distribution has a same behaviour as the one obtained by human error analysis.",
        "Most missing words for both languages are verbs.",
        "For English, the percentage of missing verbs is significantly higher than for Spanish.",
        "The same thing happens for pronouns.",
        "The probable reason for this is the nature of Spanish verbs.",
        "Since person and tense are contained in the suffix, Spanish pronouns are often omitted, and auxiliary verbs do not exist for all tenses.",
        "This could be problematic for a translation system, because it processes only one Spanish word which actually contains two (or more) English words.",
        "Prepositions are more often missing in Spanish than in English, as well as determiners.",
        "A probable reason is the disproportion of the number of occurrences for those classes between two languages."
      ]
    },
    {
      "heading": "7. Conclusions",
      "text": [
        "This work presents a framework for extraction oflin-guistic details from standard word error rates Wer and Per and their use for an automatic error analysis.",
        "We presented a method for the decomposition of standard word error rates Wer and Per over ten basic Pos classes.",
        "We also carried out a detailed analysis of inflectional errors which has shown that the results obtained by our method correspond to those obtained by a human error analysis.",
        "In addition, we proposed a method for analysing missing word errors.",
        "We plan to extend the proposed methods in order to carry out a more detailed error analysis, for example examining different types of verb inflections.",
        "We also plan to examine other types of translation errors like for example errors caused by word order."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was partly funded by the European Union under the integrated project Tc-Star-Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738)."
      ]
    }
  ]
}
