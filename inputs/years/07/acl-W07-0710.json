{
  "info": {
    "authors": [
      "Patrick Nguyen",
      "Milind Mahajan",
      "Xiaodong He"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W07-0710",
    "title": "Training Non-Parametric Features for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/W07-0710",
    "year": 2007
  },
  "references": [
    "acl-J96-1002",
    "acl-N04-1021",
    "acl-P02-1038",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P05-1034",
    "acl-P06-2101"
  ],
  "sections": [
    {
      "text": [
        "Patrick Nguyen, Milind Mahajan and Xiaodong He",
        "Microsoft Corporation",
        "Modern statistical machine translation systems may be seen as using two components: feature extraction, that summarizes information about the translation, and a log-linear framework to combine features.",
        "In this paper, we propose to relax the linearity constraints on the combination, and hence relaxing constraints of monotonicity and independence of feature functions.",
        "We expand features into a non-parametric, non-linear, and high-dimensional space.",
        "We extend empirical Bayes reward training of model parameters to meta parameters of feature generation.",
        "In effect, this allows us to trade away some human expert feature design for data.",
        "Preliminary results on a standard task show an encouraging improvement."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In recent years, statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation (Papineni et al., 2002) and error-based optimization (Och, 2003).",
        "The conditional log-linear feature combination framework (Berger, Della Pietra and Della Pietra, 1996) is remarkably simple and effective in practice.",
        "Therefore, recent efforts (Och et al., 2004) have concentrated on feature design - wherein more intelligent features may be added.",
        "Because of their simplicity, however, log-linear models impose some constraints on how new information may be inserted into the system to achieve the best results.",
        "In other words, new information needs to be parameterized carefully into one or more real valued feature functions.",
        "Therefore, that requires some human knowledge and understanding.",
        "When not readily available, this is typically replaced with painstaking experimentation.",
        "We propose to replace that step with automatic training of non-parametric agnostic features instead, hopefully relieving the burden of finding the optimal parameterization.",
        "First, we define the model and the objective function training framework, then we describe our new non-parametric features."
      ]
    },
    {
      "heading": "2. Model",
      "text": [
        "In this section, we describe the general log-linear model used for statistical machine translation, as well as a training objective function and algorithm.",
        "The goal is to translate a French (source) sentence indexed by t, with surface string ft.",
        "Among a set of Kt outcomes, we denote an English (target) a hypothesis with surface string ef indexed by k.",
        "The prevalent translation model in modern systems is a conditional log-linear model (Och and Ney, 2002).",
        "From a hypothesis ej?, we extract features ±±fc , abbreviated hk, as a function of ejjf^ and ft.",
        "The conditional probability of a hypothesis ejjf^ given a source sentence ft is:",
        "where the partition function Zft-\\ is given by: Zft;A = ^exptA ' hI\"",
        "The vector of parameters of the model A, gives a relative importance to each feature function component.",
        "In this section, we quickly review how to adjust A to get better translation results.",
        "First, let us define the figure of merit used for evaluation of translation quality.",
        "The BLEU score (Papineni et al., 2002) was defined to measure overlap between a hypothesized translation and a set of human references.",
        "n-gram overlap counts {cn}^=i are computed over the test set sentences, and compared to the total counts of n-grams in the hypothesis:",
        "a # of n-grams in hypothesis",
        "Those quantities are abbreviated ck and ak to simplify the notation.",
        "The precision ratio Pn for an n-gram order n is:",
        "A brevity penalty BP is also taken into account, to avoid favoring overly short sentences:",
        "where r is the average length of the shortest sentence, and a is the average length of hypotheses.",
        "The BLEU score the set of hypotheses {ejj*'>} is:",
        "'As implemented by NIST mteval -v11b.",
        "pl.",
        "Oracle BLEU hypothesis: There is no easy way to pick the set hypotheses from an n-best list that will maximize the overall BLEU score.",
        "Instead, to compute oracle BLEU hypotheses, we chose, for each sentence independently, the hypothesis with the highest BLEU score computed for a sentence itself.",
        "We believe that it is a relatively tight lower bound and equal for practical purposes to the true oracle",
        "BLEU.",
        "Used in earlier models (Och and Ney, 2002), the likelihood criterion is defined as the likelihood of an oracle hypothesis e^i, typically a single reference translation, or alternatively the closest match which was decoded.",
        "When the model is correct and infinite amounts of data are available, this method will converge to the Bayes error (minimum achievable error), where we define a classification task of selecting k* against all others.",
        "One can convert a maximum likelihood problem into maximum a posteriori using Bayes' rule:",
        "where p0( ) is the prior distribution of A.",
        "The most frequently used prior in practice is the normal prior (Chen and Rosenfeld, 2000):",
        "log po(A) A",
        "where a > 0 is the variance.",
        "It can be thought of as the inverse of a Lagrange multiplier when working with constrained optimization on the Euclidean norm of A.",
        "When not interpolated with the likelihood, the prior can be thought of as a penalty term.",
        "The entropy penalty may also be used:",
        "J2J2Pk log Pk.",
        "Unlike the Gaussian prior, the entropy is independent of parameterization (i.e., it does not depend on how features are expressed).",
        "A good way oftraining A is to minimize empirical top-1 error on training data (Och, 2003).",
        "Compared to maximum-likelihood, we now give partial credit for sentences which are only partially correct.",
        "The criterion is:",
        "arg max pj.",
        "We optimize the A so that the BLEU score of the most likely hypotheses is improved.",
        "For that reason, we call this criterion BLEU max.",
        "This function is not convex and there is no known exact efficient optimization for it.",
        "However, there exists a linear-time algorithm for exact line search against that objective.",
        "The method is often used in conjunction with coordinate projection to great success.",
        "The algorithm may be improved by giving partial credit for confidence pk of the model to partially correct hypotheses outside of the most likely hypothesis (Smith and Eisner, 2006):",
        "£J>k log B ({ek (t)}).",
        "Instead of the BLEU score, we use its logrithm, because we think it is exponentially hard to improve BLEU.",
        "This model is equivalent to the previous model when pk give all the probability mass to the top-1.",
        "That can be reached, for instance, when A has a very large norm.",
        "There is no known method to train against this objective directly, however, efficient approximations have been developed.",
        "Again, it is not convex.",
        "It is hoped that this criterion is better suited for high-dimensional feature spaces.",
        "That is our main motivation for using this objective function throughout this paper.",
        "With baseline features and on our data set, this criterion also seemed to lead to results similar to Minimum Error Rate Training.",
        "We can normalize B to a probability measure b({etf>}).",
        "The empirical Bayes reward also coincides with a divergence",
        "We train our model using a gradient ascent method over an approximation of the empirical Bayes reward function.",
        "Because the empirical Bayes reward is defined over a set of sentences, it may not be decomposed sentence by sentence.",
        "This is computationally burdensome.",
        "Its sufficient statistics are r, t ck and J2t ak.",
        "The function may be reconstructed in a firstorder approximation with respect to each of these statistics.",
        "In practice this has the effect of commuting the expectation inside of the functional, and for that reason we call this criterion BLEU soft.",
        "This approximation is called linearization (Smith and Eisner, 2006).",
        "We used a first-order approximation for speed, and ease of interpretation of the derivations.",
        "The new objective function is:",
        "jAlogBP + ^llogEtEC",
        "where the average bleu penalty is:",
        "}.",
        "The expectation is understood to be under the current estimate of our log-linear model.",
        "Because B P is not differentiable, we replace the hard min function with a sigmoid, yielding:",
        "log bp w u(r - Ek,tak",
        "with the sigmoid function u(x) defines a soft step function:",
        "1 + e-TXwith a parameter t » 1.",
        "We can obtain the gradients of the objective function using the chain rule by first differentiating with respect to the probability.",
        "First, let us decompose the log-precision of the expected counts:",
        "log Pn = log Ecn'(t) log Ean'(t).",
        "Each n-gram precision may be treated separately.",
        "For each n-gram order, let us define sufficient statistics p for the precision:",
        "V>a A (vAPfc)ck; Pa A Y(vAPfc)ak,",
        "where the gradient of the probabilities is given by:",
        "The derivative of the precision Pn is:",
        "VAlogPra = -",
        "T |_Eck Eak_",
        "For the length, the derivative of log Bp is:",
        "where p^ is the 1-gram component of .",
        "Finally, the derivative of the entropy is:",
        "For all our experiments, we chose RProp (Riedmiller and Braun, 1992) as the gradient ascent algorithm.",
        "Unlike other gradient algorithms, it is only based onthe sign ofthe gradient components ateach iteration.",
        "It is relatively robust to the objective function, requires little memory, does not require meta parameters to be tuned, and is simple to implement.",
        "On the other hand, it typically requires more iterations than stochastic gradient (Kushner and Yin, 1997) or L-BFGS (Nocedal and Wright, 1999).",
        "Using fairly conservative stopping criteria, we observed that RProp was about 6 times faster than Minimum Error Rate Training."
      ]
    },
    {
      "heading": "3. Adding Features",
      "text": [
        "The log-linear model is relatively simple, and is usually found to yield good performance in practice.",
        "With these considerations in mind, feature engineering is an active area of research (Och et al., 2004).",
        "Because the model is fairly simple, some of the intelligence must be shifted to feature design.",
        "After having decided what new information should go in the overall score, there is an extra effort involved in expressing or parameterizing features in a way which will be easiest for the model learn.",
        "Experimentation is usually required to find the best configuration.",
        "By adding non-parametric features, we propose to mitigate the parameterization problem.",
        "We will not add new information, but rather, propose a way to insulate research from the parameterization.",
        "The system should perform equivalently invariant ofany continuous invertible transformation of the original input.",
        "The baseline system is a syntax based machine translation system as described in (Quirk, Menezes and Cherry, 2005).",
        "Our existing feature set includes 11 features, among which the following:",
        "• Target hypothesis word count.",
        "• Treelet count used to construct the candidate.",
        "• Target language models, based on the Giga-word corpus (5-gram) and target side of parallel training data (3-gram).",
        "• Order models, which assign a probability to the position of each target node relative to its head.",
        "• Treelet translation model.",
        "• Dependency-based bigram language models.",
        "Our algorithm works in a re-ranking framework.",
        "In particular, we are adding features which are not causal or additive.",
        "Features for a hypothesis may not be accumulating by looking at the English (target) surface string words from the left to the right and adding a contribution per word.",
        "Word count, for instance, is causal and additive.",
        "This property is typically required for efficient first-pass decoding.",
        "Instead, we look at a hypothesis sentence as a whole.",
        "Furthermore, we assume that the Kt-best list provided to us contains the entire probability space.",
        "In particular, the computation of the partition function is performed over all Kt-best hypotheses.",
        "This is clearly not correct, and is the subject of further study.",
        "We use the n-best generation scheme interleaved with A optimization as described in (Och, 2003).",
        "As alluded to earlier, when designing a new feature in the log-linear model, one has to be careful to find the best embodiment.",
        "In general, a set of features must satisfy the following properties, ranked from strict to lax:",
        "• Linearity (warping)",
        "• Monotonicity",
        "• Independence (conjunction)",
        "Firstly, a feature should be linearly correlated with performance.",
        "There should be no region were it matters less than other regions.",
        "For instance, instead of a word count, one might consider adding its logarithm instead.",
        "Secondly, the \"goodness\" of a hypothesis associated with a feature must be monotonic.",
        "For instance, using the signed difference between word count in the French (source) and English (target) does not satisfy this.",
        "(In that case, one would use the absolute value instead.)",
        "Lastly, there should be no interdependence between features.",
        "As an example, we can consider adding multiple language model scores.",
        "Whether we should consider ratios those of, globally linearly or log-linearly interpolating them, is open to debate.",
        "When features interact across dimensions, it becomes unclear what the best embodiment should be.",
        "A generic solution may be sought in non-parametric processing.",
        "Our method can be derived from a quantized Parzen estimate of the feature density function.",
        "The Parzen window is an early empirical kernel method (Duda and Hart, 1973).",
        "For an observation hm, we extrapolate probability mass around it with a smoothing window $(•).",
        "The density function is:",
        "assuming $(•) is a density function.",
        "Parzen windows converge to the true density estimate, albeit slowly, under weak assumptions.",
        "One popular way of using continuous features in log-linear models is to convert a single continuous feature into multiple \"bin\" features.",
        "Each bin feature is defined as the indicator function of whether the original continuous feature was in a certain range.",
        "The bins were selected so that each bin collects an equal share of the probability mass.",
        "This is equivalent to the maximum likelihood estimate of the density function subject to a fixed number of rectangular density kernels.",
        "Since that mapping is not differentiable with respect to the original features, one may use sigmoids to soften the boundaries.",
        "Bin features are useful to relax the requirements of linearity and monotonicity.",
        "However, because they work on each feature individually, they do not address the problem of interdependence between features.",
        "Bin features may be generalized to multidimensional kernels by using a Gaussian smoothing window instead of a rectangular window.",
        "The direct analogy is vector quantization.",
        "The idea is to weight specific regions of the feature space differently.",
        "Assuming that we have M Gaussians each with mean vector /xm and diagonal covariance matrix Cm, and prior weight wm.",
        "We will add m new features, each defined as the posterior in the mixture model:",
        "It is believed that any reasonable choice of kernels will yield roughly equivalent results (Povey et al., 2004), ifthe amount oftraining data and the number of kernels are both sufficiently large.",
        "We show two methods for obtaining clusters.",
        "In contrast with bins, lossless representation becomes rapidly impossible.",
        "ML kernels: The canonical way ofobtaining cluster is to use the standard Gaussian mixture training.",
        "First, a single Gaussian is trained on the whole data set.",
        "Then, the Gaussian is split into two Gaussians, with each mean vector perturbed, and the Gaus-sians are retrained using maximum-likelihood in an expectation-maximization framework (Rabiner and Huang, 1993).",
        "The number ofGaussians is typically increased exponentially.",
        "Perceptron kernels: We also experimented with another quicker way of obtaining kernels.",
        "We chose an equal prior and a global covariance matrix.",
        "Means were obtained as follows: for each sentence in the training set, if the top-1 candidate was different from the approximate maximum oracle BLEU hypothesis, both were inserted.",
        "It is a quick way to bootstrap and may reach the oracle BLEU score quickly.",
        "In the limit, GMMs will converge to the oracle BLEU.",
        "In the next section, we show how to reestimate these kernels ifneeded.",
        "Features may also be trained using the same empirical maximum Bayes reward.",
        "Let 6 be the hyperparameter vector used to generate features.",
        "In the case of language models, for instance, this could be backoff weights.",
        "Let us further assume that the feature values are differentiable with respect to 6.",
        "Gradient ascent may be applied again but this time with respect to 6.",
        "Using the chain rule:",
        "with Vhpk = pk(1 – pk)A.",
        "Let us take the example of re-estimating the mean of a Gaussian kernel /tm:",
        "for its own feature, and for other posteriors r = m:",
        "which is typically close to zero if no two Gaussians fire simultaneously."
      ]
    },
    {
      "heading": "4. Experimental Results",
      "text": [
        "For our experiments, we used the standard NIST MT-02 data set to evaluate our system.",
        "A relatively simple baseline was used for our experiments.",
        "The system is syntactically-driven (Quirk, Menezes and Cherry, 2005).",
        "The system was trained on 175k sentences which were selected from the NIST training data (NIST, 2006) to cover words in source language sentences of the MT02 development and evaluation sets.",
        "The 5-gram target language model was trained on the Gigaword monolingual data using absolute discounting smoothing.",
        "In a single decoding, the system generated 1000 hypotheses per sentence whenever possible.",
        "In order to have enough data for training, we generated our n-best lists using 10-fold leave-one-out training: base feature extraction models were trained on 9/10th of the data, then used for decoding the held-out set.",
        "The process was repeated for all 10 parts.",
        "A single A was then optimized on the combined lists of all systems.",
        "That A was used for another round of 10 decodings.",
        "The process was repeated until it reached convergence after 7 iterations.",
        "Each decoding generated about 100 hypotheses, and there was relatively little overlap across decodings.",
        "Therefore, there were about 1M hypotheses in total.",
        "The combined list of all iterations was used for all subsequent experiments of feature expansion.",
        "We tried training systems under the empirical Bayes reward criterion, and appending either bin or GMM features.",
        "We will find that bin features are essentially ineffective while GMM features show a modest improvement.",
        "We did not retrain hyperparameters.",
        "The first question to ask is how many local optima does the cost surface have using the standard features.",
        "A complex cost surface indicates that some gain may be had with non-linear features, but it also shows that special care should be taken during optimization.",
        "Non-convexity is revealed by sensitivity to initialization points.",
        "Thus, we decided to initialize from all vertices of the unit hypercube, and since we had 11 features, we ran 2 experiments.",
        "The histogram of BLEU scores on dev data after convergence is shown on Figure 1.",
        "We also plotted the histogram of an example dimension in Figure 2.",
        "The range of BLEU scores and lambdas is reasonably narrow.",
        "Even though A seems to be bimodal, we see that this does not seriously affect the BLEU score.",
        "This is not definitive evidence but we provisionally pretend that the cost surface is almost convex for practical purposes.",
        "bins and re-trained.",
        "On Figure 3, we show that relaxing the monotonicity constraint leads to rough values for A.",
        "Surprisingly, the BLEU score and objective on the training set only increases marginally.",
        "Starting from A = 0, we obtained nearly exactly the same training objective value.",
        "By varying the number of bins (20-50), we observed similar behavior as well.",
        "d e in rai",
        "A log-linear model can be converted into a bin feature model nearly exactly by setting A values in such a way that scores will be equal.",
        "Equivalent weights (marked as 'original' in Figure 3) have the shape of an error function (erf): this is because the input feature is a cummulative random variable, which quickly converges to a Gaussian (by the central limit theorem).",
        "After training the A weights for the log-linear model, weights may be converted into",
        "Figure 3: Values before and after training bin features.",
        "Monotonicity constraint has been relaxed.",
        "BLEU score is virtually unchanged.",
        "Experiments were carried out with GMM features.",
        "The summary is shown on Table 1.",
        "The baseline was the log-linear model trained with the baseline features.",
        "The baseline features are included in all systems.",
        "We trained GMM models using the iterative mixture splitting interleaved with EM reestimation, split up to 1024 and 16384 Gaussians, which we call GMM-ML-1k and GMM-ML-16k respectively.",
        "We also used the \"perceptron\" selection features on the training set to bootstrap quickly same algorithm on the development set (GMMfeatures, and was trained on 175k sentences (each with about 700 hypotheses).",
        "For all experiments but \"unreg\" (unregularized), we chose a prior Gaussian prior with variance empirically by looking at the development set.",
        "For all but GMM-PCP-300k, regularization did not seem to have a noticeably positive effect on development BLEU scores.",
        "All systems were seeded with the baseline log-linear model, and all additional weights set to zero, and then trained with about 50 iterations, but convergence in BLEU score, empirical reward, and development BLEU score occurred after about 30 iterations.",
        "In that setting, we found that regularized empirical Bayes reward, BLEU score on training data, and BLEU score on development and evaluation to be well correlated.",
        "Cursory experiments revealed that using multiple initializations did not significantly alter the final BLEU score.",
        "\\",
        "1",
        "A''''",
        "( V",
        "•",
        "original weights – trained weights",
        "Table 1: BLEU scores for GMM features vs the linear baseline, using different selection methods and number of kernels.",
        "Perceptron kernels based on the training set improved the baseline by 0.5 BLEU points.",
        "We measured significance with the Wilcoxon signed rank test, by batching 10 sentences at a time to produce an observation.",
        "The difference was found to be significant at a 0.9-confidence level.",
        "The improvement may be limited due to local optima or the fact that original feature are well-suited for log-linear models."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, we have introduced a non-parametric feature expansion, which guarantees invariance to the specific embodiment of the original features.",
        "Feature generation models, including feature expansion, may be trained using maximum regularized empirical Bayes reward.",
        "This may be used as an end-to-end framework to train all parameters of the machine translation system.",
        "Experimentally, we found that Gaussian mixture model (GMM) features yielded a 0.5 BLEU improvement.",
        "Although this is an encouraging result, further study is required on hyper-parameter re-estimation, presence of local optima, use of complex original features to test the effectiveness of the parameterization invariance, and evaluation on a more competitive baseline.",
        "System",
        "Train",
        "Dev",
        "Eval",
        "Oracle",
        "14.10",
        "N/A",
        "N/A",
        "Baseline",
        "10.95",
        "35.15",
        "25.95",
        "GMM-ML-lk",
        "10.95",
        "35.15",
        "25.95",
        "GMM-ML-16k",
        "11.09",
        "35.25",
        "25.89",
        "GMM-PCP-2k",
        "10.95",
        "35.15",
        "25.95",
        "GMM-PCP-300k-unreg",
        "13.00",
        "N/A",
        "N/A",
        "GMM-PCP-300k",
        "12.11",
        "35.74",
        "26.42"
      ]
    }
  ]
}
