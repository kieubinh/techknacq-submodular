{
  "info": {
    "authors": [
      "Andrew Hickl",
      "Jeremy Bensley"
    ],
    "book": "ACL-PASCAL Workshop on Textual Entailment and Paraphrasing",
    "id": "acl-W07-1428",
    "title": "A Discourse Commitment-Based Framework for Recognizing Textual Entailment",
    "url": "https://aclweb.org/anthology/W07-1428",
    "year": 2007
  },
  "references": [
    "acl-H05-1010",
    "acl-H05-1049",
    "acl-J05-1004",
    "acl-P04-1018",
    "acl-W05-1208"
  ],
  "sections": [
    {
      "text": [
        "A Discourse Commitment-Based Framework for Recognizing Textual",
        "Entailment",
        "Andrew Hickl and Jeremy Bensley",
        "Language Computer Corporation 1701 North Collins Boulevard Richardson, Texas 75080 USA",
        "In this paper, we introduce a new framework for recognizing textual entailment which depends on extraction of the set of publicly-held beliefs - known as discourse commitments - that can be ascribed to the author of a text or a hypothesis.",
        "Once a set of commitments have been extracted from a t-h pair, the task of recognizing textual entailment is reduced to the identification of the commitments from a t which support the inference of the h. Promising results were achieved: our system correctly identified more than 80% of examples from the RTE-3 Test Set correctly, without the need for additional sources of training data or other web-based resources."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Systems participating in the previous two PASCAL Recognizing Textual Entailment (RTE) Challenges (Bar-Haim et al., 2006) have successfully employed a variety of \"shallow\" techniques in order to recognize instances of textual entailment, including methods based on: (1) sets of heuristics (Vander-wende et al., 2006), (2) measures of term overlap (Jijkoun and de Rijke, 2005), (3) the alignment of graphs created from syntactic or semantic dependencies (Haghighi et al., 2005), or (4) statistical classifiers which leverage a wide range of features, including the output of paraphrase generation (Hickl et al., 2006) or model building systems (Bos and Markert, 2006).",
        "While relatively \"shallow\" approaches have shown much promise in RTE for entailment pairs where the text and hypothesis remain short, we expect that performance of these types of systems will ultimately degrade as longer and more syntactically complex entailment pairs are considered.",
        "In order to remain effective as texts get longer, we believe that RTE systems will need to employ techniques that will enable them to enumerate the set of propositions which are inferable - whether asserted, presupposed, or conventionally or conversationally implicated - from a text-hypothesis pair.",
        "In this paper, we introduce a new framework for recognizing textual entailment which depends on extraction of the set of publicly-held beliefs - or discourse commitments - that can be ascribed to the author of a text or a hypothesis.",
        "We show that once a set of discourse commitments have been extracted from a text-hypothesis pair, the task of recognizing textual entailment can be reduced to the identification of the one (or more) commitments from the text which are most likely to support the inference of each commitment extracted from the hypothesis.",
        "More formally, we assume that given a commitment set {ct} consisting of the set of discourse commitments inferable from a text t and a hypothesis h, we define the task of RTE as a search for the commitment c â‚¬ {ct} which maximizes the likelihood that c textually entails h.",
        "The rest of this paper is organized in the following way.",
        "Section 2 provides a sketch of the system we used in the PASCAL RTE-3 Challenge.",
        "Sections 3, 4, and 5 describe details of our systems for Commitment Extraction, Commitment Se-",
        "Preprocessing",
        "Commitment Extraction",
        "Extracted Knowledge",
        "Commitment Selection",
        "Lexical Alignment",
        "Entailment Classification",
        "Contradiction Detection",
        "Entailed Knowledge",
        "YES J - Contradiction",
        "lection, and Entailment Classification, respectively.",
        "Finally, Section 6 discusses results from this year's evaluation, and Section 7 provides our conclusions."
      ]
    },
    {
      "heading": "2. System Overview",
      "text": [
        "The architecture of our system for recognizing textual entailment (RTE) is presented in Figure 1.",
        "In our system, text-hypothesis (t-h) pairs are initially submitted to a Preprocessing module which (1) syntactic parses each passage (using an implementation of the (Collins, 1999) parser), (2) identifies semantic dependencies (using a semantic dependency parser trained on PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004)), (3) annotates named entities (using LCC's Cicero-Lite named entity recognition system), (4) resolves instances of pronominal and nominal coreference (using a system based on (Luo et al., 2004)), and (5) normalizes temporal and spatial expressions to fully-resolved instances (using a technique first introduced in (Aarseth et al., 2006)).",
        "Annotated passages are then sent to a Commitment Extraction module, which uses a series of extraction heuristics in order to enumerate a subset of the discourse commitments that are inferable from either the text or hypothesis.",
        "Following (Gunlog-son, 2001; Stalnaker, 1979), we assume that a discourse commitment (c) represents the any of the set of propositions that can necessarily be inferred to be true, given a conventional reading of a text passage.",
        "The complete list of commitments that our system is able to extract from from the t used in examples 34 and 36 from the RTE-3 Test Set is presented in Figure 2.",
        "(Details of our commitment extraction approach are presented in Section 3.)",
        "Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al., 2005b) in order to identify the commitment from the t which features the best alignment for each commitment extracted from the h. The commitment pairs identified for the hypotheses from 34 and 36 are highlighted in Figure 2.",
        "(Details of our method for selecting and aligning commitments are provided in Section 4.)",
        "Each pair of commitments are then considered in turn by an Entailment Classification module, which in using a decision tree classifier in order to compute the likelihood that a commitment extracted from a t textually entails a commitment extracted from an h.",
        "If a commitment pair is judged to be a positive instance of TE, it is sent to an Entailment Validation module, which uses a system for recognizing instances of textual contradiction (RTC) based on (Harabagiu et al., 2006) in order to determine whether the (presumably) entailed hypothesis is contradicted by any of other commitments extracted from the t during commitment extraction.",
        "If no text commitment can be identified which contradicts the hypothesis, it is presumed to be textually entailed, and a judgment of YES is returned.",
        "Alternatively, if the entailed h is textually contradicted by one (or more) of the commitments extracted from the t, the h is considered to be contradicted by the t, the entailment pair is classified as a negative instance of TE, and a judgment of NO is returned.",
        "In contrast, when commitment pairs are judged to be negative instances of TE by the Entailment Classifier, the current pair is removed from further consideration by the system, and the next most likely commitment pair is considered.",
        "Commitment pairs are considered in decreasing order of the probability output by the Commitment Selection module until a positive instance of TE is identified - or until there are no more commitment pairs with a selection probability greater than a predefined threshold.",
        "Text: A Revenue Cutter, the ship was named for Harriet Lane, niece of President James Buchanan, who served as Buchanan's White House hostess.",
        "T1.",
        "A Revenue Cutter is a ship.",
        "T2.",
        "The ship was named for Harriet Lane.",
        "T3.",
        "Harriet Lane was the niece of President James Buchanan.",
        "T4.",
        "The niece of Buchanan served as Buchanan's White House hostess.",
        "T5.",
        "A Revenue Cutter was named for Harriet Lane.",
        "T6.",
        "A Revenue Cutter was named for the niece of President James Buchanan.",
        "T7.",
        "A Revenue Cutter was named for Buchanan's White House hostess.",
        "T8.",
        "A Revenue Cutter was named for a White House hostess.",
        "T9.",
        "A Revenue Cutter was named for a hostess.",
        "T10.",
        "The niece of a President served as Buchanan's White House hostess.",
        "T11.",
        "The niece of a President served as Buchanan's hostess.",
        "T12.",
        "The niece of a President served as a White House hostess.",
        "T13.",
        "The niece of a President served at the White House.",
        "T14.",
        "The niece of a President had occupation hostess.",
        "T15.",
        "The niece of a President served as a hostess.",
        "T16.",
        "Harriet Lane was related to President James Buchanan.",
        "T17.",
        "Harriet Lane was the niece of a President.",
        "T18.",
        "Harriet Lane was related to a President.",
        "T19.",
        "Harriet Lane was related to James Buchanan.",
        "T20.",
        "James Buchanan had title of President.",
        "T21.",
        "James Buchanan had a White House hostess.",
        "T22.",
        "James Buchanan had a hostess.",
        "T23.",
        "James Buchanan was associated with the White House.",
        "T24.",
        "James Buchanan had a niece.",
        "T25.",
        "Harriet Lane served as Buchanan's White House hostess.",
        "T26.",
        "Harriet Lane served as Buchanan's hostess.",
        "T27.",
        "Harriet Lane served as a White House hostess.",
        "T28.",
        "Harriet Lane served at the White House.-",
        "T29.",
        "Harriet Lane had occupation hostess.",
        "T30.",
        "Harriet Lane served as a hostess..",
        "Hyp(34): Harriet Lane owned a Revenue Cutter.",
        "Hyp(36): Harriet Lane worked at the White House.",
        "Positive Instance of Textual Entailment"
      ]
    },
    {
      "heading": "3. Extracting Discourse Commitments",
      "text": [
        "Following Preprocessing, our system for RTE leverages a series of heuristics in order to extract a subset of the discourse commitments available from a text-hypothesis pair.",
        "In this section, we outline the five classes of heuristics we used to extract commitments for the RTE-3 Challenge.",
        "Sentence Segmentation: We use a sentence segmenter to break text passages into sets of individual sentences; commitments are then extracted from each sentence independently.",
        "Syntactic Decomposition: We use heuristics to syntactically decompose sentences featuring coordination and lists into well-formed sentences that only include a single conjunct or list element.",
        "Supplemental Expressions: Recent work has demonstrated that the class of supplemental expressions - including appositives, as-clauses, parentheticals, parenthetical adverbs, non-restrictive relative clauses, and epithets - trigger conventional implicatures (CI) whose truth is necessarily presupposed, even if the truth conditions of a sentence are not satisfied.",
        "In our current system, heuristics were used to extract supplemental expressions from each sentence under consideration and to create new sentences which specify the CI conveyed by the expression.",
        "Relation Extraction: We used an in-house relation extraction system to recognize six types of semantic relations between named entities, including: (1) artifact (e.g. OWNER-OF), (2) general affiliation (e.g. LOCATION-OF), (3) organization affilia- tion (e.g. EMPLOYEE-OF), (4) part-whole, (5) social affiliation (e.g. RELATED-TO), and (6) physical location (e.g. LOCATED-NEAR) relations.",
        "Again, as with supplemental expressions, heuristics were used to generate new commitments which expressed the semantics conveyed by these nominal relations.",
        "Coreference Resolution: We used systems for resolving pronominal and nominal coreference in order to expand the number of commitments available to the system.",
        "After a set of co-referential entity mentions were detected (e.g. Harriet Lane, the niece, Buchanan's White House hostess), new commitments were generated from the existing set of commitments which incorporated each co-referential mention."
      ]
    },
    {
      "heading": "4. Commitment Selection",
      "text": [
        "Following Commitment Extraction, we used an word alignment technique first introduced in (Taskar et al., 2005b) in order to select the commitment extracted from t (henceforth, ct) which represents the best alignment for each of the commitments extracted from h (henceforth, ch).",
        "We assume that the alignment of two discourse commitments can be cast as a maximum weighted matching problem in which each pair of words (ti,hj) in an commitment pair (ct,ch) is assigned a score Sij (t, h) corresponding to the likelihood that ti is aligned to hj.",
        "As with (Taskar et al., 2005b), we use the large-margin structured prediction model",
        "or zero words from the t.",
        "introduced in (Taskar et al., 2005a) in order to compute a set of parameters w (computed with respect to a set of features f) which maximize the number of correct alignment predictions (j/j) made given a set of training examples (x), as in Equation (1).",
        "We used three sets of features in our model: (1) string features (including Levenshtein edit distance, string equality, and stemmed string equality), (2) lexico-semantic features (including WordNet Similarity (Pedersen et al., 2004) and named entity similarity equality), and (3) word association features (computed using the Dice coefficient (Dice, 1945)).",
        "In order to provide a training set which most closely resembled the RTE-3 Test Set, we hand-annotated token alignments for each of the 800 entailment pairs included in the Development Set.",
        "Following alignment, we used the sum of the edge scores (J2i,j=i sij ($%, hj)) computed for each of the possible (ct, ch) pairs in order to search for the ctwhich represented the reciprocal best hit (Mushe-gian and Koonin, 2005) of each ch extracted from the hypothesis.",
        "This was performed by selecting a commitment pair (ct, ch) where ct was the top-scoring alignment candidate for ch and ch was the top-scoring alignment candidate for ct.",
        "If no reciprocal best-hit could be found for any of the commitments extracted from the h, the system automatically returned a TE judgment of NO.",
        "We compared the performance of our word alignment and commitment selection algorithms against an implementation of the lexical alignment classifier described in (Hickl et al., 2006) on commitments extracted from the entailment pairs from the RTE-2 Test Set.",
        "Table 1 presents results from evaluations of these two models on the token alignment and commitment selection tasks.",
        "(Gold standard annotations for each task were created by hand by a team of 3 annotators following the RTE-3 evaluations.)"
      ]
    },
    {
      "heading": "5. Entailment Classification",
      "text": [
        "Following work done by (Bos and Markert, 2006; Hickl et al., 2006) for the RTE-2 Challenge, we used a decision tree (C5.0 (Quinlan, 1998)) to estimate the likelihood that a commitment pair represented a valid instance of textual entailment.",
        "Confidence values associated with each leaf node (i.e. yes or NO) were normalized and used to rank examples for the official submission.",
        "In a departure from previous work (such as (Hickl et al., 2006)) which leveraged large corpora of entailment pairs to train an entailment classifier, our model was only trained on the 800 text-hypothesis pairs found in the RTE-3 Development Set (DevSet).",
        "Features were selected manually by performing tenfold cross validation on the DevSet.",
        "Maximum performance of the entailment classifier on the DevSet is provided in Table 2.",
        "Table 2: Entailment Classifier Performance.",
        "A partial list of the features used in the Entailment Classifier used in our official submission is provided in Figure 3."
      ]
    },
    {
      "heading": "6. Experiments and Results",
      "text": [
        "We submitted one ranked run in our official submission for this year's evaluation.",
        "Official results from the RTE-3 Test Set are presented in Table 3.",
        "Accuracy and average precision varied significantly (p < 0.05) across each of the four tasks.",
        "Performance (in terms of accuracy and average precision) was highest on the QA set (90.0% precision) and lowest on the IE set (67.5%).",
        "The length of the text (either short or long) did not significantly impact performance, however; in fact,",
        "Task",
        "Measurement",
        "Current Work",
        "Hickl et al.",
        "Token Alignment",
        "Precision",
        "94.55%",
        "92.22%",
        "Token Alignment",
        "MRR",
        "0.9219",
        "0.8797",
        "Commitment Selection",
        "Precision",
        "89.50%",
        "72.50%",
        "Commitment Selection",
        "MRR",
        "0.8853",
        "0.7410",
        "IE",
        "IR",
        "QA",
        "SUM",
        "Total",
        "Accuracy",
        "0.8450",
        "0.8750",
        "0.8850",
        "0.8600",
        "0.8663",
        "Average Precision",
        "0.8522",
        "0.8953",
        "0.9005",
        "0.8959",
        "0.8860",
        "IE",
        "IR",
        "QA",
        "SUM",
        "Total",
        "Accuracy",
        "0.6750",
        "0.8000",
        "0.9000",
        "0.8400",
        "0.8038",
        "Average Precision",
        "0.7760",
        "0.8133",
        "0.9308",
        "0.8974",
        "0.8815",
        "ALIGNMENT FEATURES: Derived from the results of the alignment of each pair of commitments performed during Commitment Selection.",
        "olo Longest Common String: This feature represents the longest contiguous string common to both texts.",
        "o2o Unaligned Chunk: This feature represents the number of chunks in one text that are not aligned with a chunk from the other o3o Lexical Entailment Probability: Defined as in (Glickman and Dagan, 2005).",
        "DEPENDENCY FEATURES: Computed from the semantic dependencies identified by the PropBank- and NomBank-based semantic parsers.",
        "olo Entity-Arg Match: This is a boolean feature which fires when aligned entities were assigned the same argument role label.",
        "o2o Entity-Near-Arg Match: This feature is collapsing the arguments Argi and Arg2 (as well as the ArgM subtypes) into single categories for the purpose of counting matches.",
        "o3o Predicate-Arg Match: This boolean feature is flagged when at least two aligned arguments have the same role.",
        "o4o Predicate-Near-Arg Match: This feature is collapsing the arguments Arg1 and Arg2 (as well as the ArgM subtypes) into single categories for the purpose of counting matches.",
        "SEMANTIC/PRAGMATIC FEATURES: Extracted during preprocessing.",
        "olo Named Entity Class: This feature has a different value for each of the 150 named entity classes.",
        "o2o Temporal Normalization: This boolean feature is flagged when the temporal expressions are normalized to the same ISO 9000 equivalents.",
        "o3o Modality Marker: This boolean feature is flagged when the two texts use the same modal verbs.",
        "o4o Speech-Act: This boolean feature is flagged when the lexicons indicate the same speech act in both texts.",
        "o5o Factivity Marker: This boolean feature is flagged when the factivity markers indicate either TRUE or FALSE in both texts simultaneously.",
        "o6o Belief Marker: This boolean feature is set when the belief markers indicate either TRUE or FALSE in both texts simultaneously.",
        "as can be seen in Table 4, total accuracy was nearly the same for examples featuring short or long texts.",
        "In experiments conducted following the RTE-3 submission deadline, we found that using a system for recognizing textual contradiction to validate judgments output by the entailment classifier had only a slight positive impact on the overall performance of our system.",
        "Table 5 compares performance of our RTE system when four different configurations of our system for recognizing textual contradiction was used.",
        "When used with its default threshold (A = 0.85), we discovered that using textual contradiction enabled us to identify 17 additional examples (2.13% overall) that were not available when using our system for RTE alone.",
        "When we hand-tuned A to maximize performance on the RTE-3 Test Set, we found that accuracy could be increased by 3.0% over the baseline (to 81.25% overall).",
        "Despite its limited effectiveness on this year's Test Set, we believe that net positive effect of using textual contradiction to validate textual entailment judgments suggests that this technique has merit and should be explored in future evaluations.",
        "in a second post hoc experiment, we sought to quantify the impact that additional sources of training data could have on the performance of our RTE system.",
        "Although our official submission was only trained on the 800 t-h pairs found in the RTE-3 Development Set, we followed (Hickl et al., 2006) in using a large, hand-crafted training set of 100,000 text-hypothesis pairs in order to train our entailment classifier.",
        "Even though previous work has shown that RTE accuracy increased with the size of the training set, our experiments showed no correlation between the size of the training corpus and the overall accuracy of the system.",
        "Table 6 summarizes the performance of our RTE system when trained on increasing amounts of training data.",
        "While increasing the training data to approximately 10,000 training examples did positively impact performance, we discovered that using a training corpus of a size equal to (Hickl et al., 2006)'s had nearly no measurable impact on the observed performance of our system.",
        "While large training corpora (like (Hickl et al., 2006)'s or the one compiled for this work) may provide an important source of lexico-semantic information that can be leveraged in performing an entailment classification, these results suggest that our approach based on commitment extraction may nullify the gains in performance seen by these approaches.",
        "7 Conclusions",
        "Validation?",
        "A",
        "IE",
        "IR",
        "QA",
        "SUM",
        "Total",
        "Yes (RTE-3)",
        "0.85",
        "0.6750",
        "0.8000",
        "0.9000",
        "0.8400",
        "0.8038",
        "Yes",
        "0.75",
        "0.6900",
        "0.8100",
        "0.8850",
        "0.8650",
        "0.8125",
        "Yes",
        "0.65",
        "0.6550",
        "0.8000",
        "0.8850",
        "0.8250",
        "0.7913",
        "No",
        "-",
        "0.6550",
        "0.8000",
        "0.8650",
        "0.8250",
        "0.7865",
        "Short",
        "Long",
        "n",
        "Accuracy",
        "n",
        "Accuracy",
        "IE",
        "181",
        "0.6685",
        "19",
        "0.7368",
        "IR",
        "146",
        "0.8082",
        "54",
        "0.7778",
        "QA",
        "165",
        "0.8909",
        "35",
        "0.9429",
        "SUM",
        "191",
        "0.8482",
        "9",
        "0.6667",
        "Total",
        "683",
        "0.8023",
        "117",
        "0.8120",
        "Training Corpus",
        "Accuracy",
        "Average Precision",
        "800 pairs (RTE-3 Dev)",
        "0.8038",
        "0.8815",
        "10,000 pairs",
        "0.8150",
        "0.8939",
        "25,000 pairs",
        "0.8225",
        "0.8834",
        "50,000 pairs",
        "0.8125",
        "0.8355",
        "100,000 pairs",
        "0.8050",
        "0.8003",
        "This paper introduced a new framework for recognizing textual entailment which depends on the extraction of the discourse commitments that can be inferred from a conventional interpretation of a text passage.",
        "By explicitly enumerating the set of inferences that can be drawn from a t or h, our approach is able to reduce the task of RTE to the identification of the set of commitments that support the inference of each corresponding commitment extracted from a hypothesis.",
        "in our current work, we show that this approach can be used to correctly classify more than 80% of examples from the RTE-3 Test Set, without the need for additional sources of training data or web-based resources."
      ]
    }
  ]
}
