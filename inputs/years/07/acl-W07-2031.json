{
  "info": {
    "authors": [
      "Johannes Leveling"
    ],
    "book": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)",
    "id": "acl-W07-2031",
    "title": "FUH (FernUniversit\\\"at in Hagen): Metonymy Recognition Using Different Kinds of Context for a Memory-Based Learner",
    "url": "https://aclweb.org/anthology/W07-2031",
    "year": 2007
  },
  "references": [
    "acl-C92-3145",
    "acl-E06-3009",
    "acl-P03-1008",
    "acl-P93-1012",
    "acl-W07-2007"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 153?156, Prague, June 2007. c?2007 Association for Computational Linguistics FUH (FernUniversita?t in Hagen):"
      ]
    },
    {
      "heading": "Metonymy Recognition Using Different Kinds of Context for a Memory-Based Learner Johannes Leveling Intelligent Information and Communication Systems (IICS)",
      "text": [
        "FernUniversita?t in Hagen (University of Hagen) johannes.leveling@fernuni-hagen.de"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "For the metonymy resolution task at SemEval-2007, the use of a memory-based learner to train classifiers for the identification of metonymic location names is investigated.",
        "Metonymy is resolved on different levels of granularity, differentiating between literal and non-literal readings on the coarse level; literal, metonymic, and mixed readings on the medium level; and a number of classes covering regular cases of metonymy on a fine level.",
        "Different kinds of context are employed to obtain different features: 1) a sequence of n1 synset IDs representing subordination information for nouns and for verbs, 2) n2 prepositions, articles, modal, and main verbs in the same sentence, and 3) properties of n3 tokens in a context window to the left and to the right of the location name.",
        "Different classifiers were trained on the Mascara data set to determine which values for the context sizes n1, n2, and n3 yield the highest accuracy (n1 = 4, n2 = 3, and n3 = 7, determined with the leave-one-out method).",
        "Results from these classifiers served as features for a combined classifier.",
        "In the training phase, the combined classifier achieved a considerably higher precision for the Mascara data.",
        "In the SemEval submission, an accuracy of 79.8% on the coarse, 79.5% on the medium, and 78.5% on the fine level is achieved (the baseline accuracy is 79.4%)."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Metonymy is typically defined as a figure of speech in which a speaker uses one entity to refer to another that is related to it (Lakoff and Johnson, 1980).",
        "The identification of metonymy becomes important for NLP tasks such as question answering (Stallard, 1993) or geographic information retrieval (Leveling and Hartrumpf, 2006).",
        "For regular cases of metonymy for locations and organizations, Markert and Nissim have proposed a set of metonymy classes.",
        "Annotating a subset of the BNC (British National Corpus), they extracted a set of metonymic proper nouns from two categories: country names (Markert and Nissim, 2002) and organization names (Nissim and Markert, 2003).",
        "In the metonymy resolution task at SemEval2007, the goal was to identify metonymic names in a subset of the BNC.",
        "The task consists of two subtasks for company and country names, which are further divided into classification on a coarse level (recognizing literal and non-literal readings), on a medium level (differentiating non-literal readings into mixed and metonymic readings), and on a fine level (identifying classes of regular metonymy, such as a name referring to the population, place-for-people).",
        "The task is described in more detail by Markert and Nissim (2007)."
      ]
    },
    {
      "heading": "2 System Description",
      "text": []
    },
    {
      "heading": "2.1 Tools and Resources",
      "text": [
        "The following tools and resources are used for the metonymy classification: ?",
        "TiMBL 5.1 (Daelemans et al., 2004), a memory-based learner for classification is em",
        "ployed for training the classifiers (supervised learning).1 ?",
        "Mascara 2.0 ?",
        "Metonymy Annotation Scheme And Robust Analysis (Markert and Nissim, 2003; Nissim and Markert, 2003; Markert and Nissim, 2002) contains annotated data for metonymic names from a subset of the the BNC.",
        "?",
        "WordNet 2.0 (Fellbaum, 1998) serves as a linguistic resource for assigning synset IDs and for looking up subordination information and frequency of readings.",
        "?",
        "The TreeTagger (Schmid, 1994) is utilized for sentence boundary detection, lemmatization, and part-of-speech tagging.",
        "The English tag-ger was trained on the PENN treebank and uses the English morphological database from the XTAG project (Karp et al., 1992).",
        "The parameter files were obtained from the web site.2"
      ]
    },
    {
      "heading": "2.2 Different Kinds of Context",
      "text": [
        "Following the assumption that metonymic location names can be identified from the context, there are different kinds of context to consider.",
        "At most, the context comprises a single sentence in this setup.",
        "Three kinds of context were employed to extract features for the memory-based learner TiMBL:",
        "?",
        "C1: Subordination (hyponymy) information for nouns and verbs from the left and right context of the possibly metonymic name.",
        "?",
        "C2: The sentence context for modal verbs, main verbs, prepositions, and articles.",
        "?",
        "C3: A context window of tokens left and right of the location name.",
        "The trial data provided (a subset of the Mascara data) contained 188 non-literal location names (of 925 samples total).",
        "For a supervised learning approach, this is too few data.",
        "Therefore, the full",
        "notations (of 2797 samples total).",
        "Some cases in the Mascara corpus are filtered during processing, including cases annotated as homonyms and cases whose metonymy class could not be agreed upon.",
        "The test data had a majority baseline of 82.8% accuracy for country names."
      ]
    },
    {
      "heading": "2.3 Features",
      "text": [
        "The Mascara data was processed to extract the following features (no hand-annotated data from Mascara was employed for feature values, i.e. no grammatical roles): ?",
        "For C1 (WordNet context): From a context of n1 verbs and nouns in the same sentence, their distance to the location name is calculated.",
        "A sequence of eight feature values of WordNet synset IDs is obtained by iteratively looking up the most frequent reading for a lemma inWord-Net and determining its synset ID.",
        "Subordination information between synsets is used to find a parent synset.",
        "This process is repeated until a top-level parent synset is reached.",
        "No actual word sense disambiguation is employed.",
        "?",
        "For C2 (sentence context): Sentence boundaries, part-of-speech tags, and lemmatization are determined from the TreeTagger output.",
        "From a context window of n2 tokens, lemma and distance are encoded as feature values for prepositions, articles, modal, and main verbs ?",
        "For C3 (word context): From a context of n3 tokens to the left and to the right, the distance between token and location name, three prefix characters, three suffix characters, part-of-speech tag, case information (U=upper case, L=lower case, N=numeric, O=other), and word length are used as feature values.",
        "Table 1 and Table 2 show results for memory based learners trained with TiMBL.",
        "Performance measures were obtained with the leave-one-out method.",
        "The classifiers were trained on features for different context sizes (ni ranging from 2 to 7) to determine the setting for which the highest accuracy is achieved (e.g. 1c, 2c, and 3c).",
        "In the next step, classifiers with a combined context were",
        "coarse location name classes (2797 instances, 509 non-literal, leave-one-out) for the Mascara data (P = precision, R = recall, F = F-score).",
        "ID n1,n2,n3 coarse class P R F",
        "trained, selecting the setting with the highest accuracy for a single context for the combination (e.g. 4c, 5c, 6c, and 7c).",
        "As an additional experiment, a classifier was trained on classification results of the classifiers described above (combination of 1?7, e.g. 8c).",
        "It was expected that the combination of features from different kinds of context would increase performance, and that the combination of classifier results would increase performance."
      ]
    },
    {
      "heading": "3 Evaluation Results",
      "text": [
        "Table 3 shows results for the official submission.",
        "Compared to results from the training phase on the Mascara data (tested with the leave-one-out method), performance is considerably lower.",
        "For this data, the combined classifier achieved a considerably higher precision (63.9% for non-literal readings; 57.3% for the fine class place-for-people and even 83.3% for the rare class place-for-event).",
        "Performance may be affected by several reasons: A number of problems were encountered while processing the data.",
        "The TreeTagger automatically tokenizes its input and applies sentence boundary detection.",
        "In some cases, the sentence boundary detection did not work well, returning sentences of more than 170 words.",
        "Furthermore, the tagger output had to be aligned with the test data again, as multi-word",
        "sifiers on the fine location name classes (2797 instances, leave-one-out) for the Mascara data.",
        "ID n1,n2,n3 fine class P R F",
        "names (e.g. New York) were split into different tokens.",
        "In addition, the tag set of the tagger differs somewhat from the official PENN tag set and includes additional tags for verbs.",
        "In earlier experiments on metonymy classification on a German corpus (Leveling and Hartrumpf, 2006), the data was nearly evenly distributed between literal and metonymic readings.",
        "This seems to make a classification task easier because there is no hidden bias in the classifier (i.e. the baseline of always selecting the literal readings is about 50%).",
        "Features are obtained by shallow NLP methods only, not making use of a parser or chunker.",
        "Thus, important syntactic or semantic information to decide on metonymy might be missing in the features.",
        "However, semantic features are more difficult to determine, because reliable automatic tools for semantic annotation are still missing.",
        "This is also indicated by the fact that the grammatical roles (comprising syntactic features) in Mascara data are hand-annotated.",
        "However, some linguistic phenomena are already implicitly represented by shallower features from",
        "the surface level (given enough training instances).",
        "For instance, active/passive voice may be encoded by a combination of features for main verb/modal verbs.",
        "If only a small training corpus is available, overall performance will be higher when utilizing explicit syntactic or semantic features.",
        "Finally, the data may be too sparse for a supervised memory-based learning approach.",
        "The identification of rare classes of metonymy (e.g. place-for-event) would greatly benefit from a larger corpus covering these classes."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "Evaluation results on the training data were very promising, indicating a boost of precision by combining classification results.",
        "In the training phase, an accuracy of 83.7% was achieved on the coarse level, compared to the majority baseline accuracy of 81.8%.",
        "For the submission for the metonymy resolution task at SemEval-2007, accuracy is close to the majority baseline (79.4%) on the coarse (79.8%), medium (79.5%), and fine (78.5%) level.",
        "In summary, using different context sizes for different kinds of context and combining results of different classifiers for metonymy resolution increases performance.",
        "The general approach would profit from combining results of more diverse classifiers, i.e. classifiers employing features extracted from the surface, syntactic, and semantic context of a location name."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The research described was in part funded by the DFG (Deutsche Forschungsgemeinschaft) in the project IRSAW (Intelligent Information Retrieval on the Basis of a Semantically Annotated Web)."
      ]
    }
  ]
}
