{
  "info": {
    "authors": [
      "Peng Jin",
      "Danqing Zhu",
      "Fuxin Li",
      "Yunfang Wu"
    ],
    "book": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)",
    "id": "acl-W07-2056",
    "title": "PKU: Combining Supervised Classifiers with Features Selection",
    "url": "https://aclweb.org/anthology/W07-2056",
    "year": 2007
  },
  "references": [
    "acl-C02-1168",
    "acl-W04-0834"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 261?263, Prague, June 2007. c?2007 Association for Computational Linguistics"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper presents the word sense disambiguation system of Peking University which was designed for the SemEval-2007 competition.",
        "The system participated in the Web track of task 11 ?English Lexical Sample Task via English-Chinese Parallel Text?.",
        "The system is a hybrid model by combining two supervised learning algorithms SVM and ME.",
        "And the method of entropy-based feature chosen was experimented.",
        "We obtained precision (and recall) of 81.5%."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The PKU system participated in the web track of task 11.",
        "In this task, the organizers propose an English lexical sample task for word sense disambiguation (WSD), where the sense-annotated examples are (semi)-automatically gathered from word-aligned English-Chinese parallel texts.",
        "After assigning appropriate Chinese translations to each sense of an English word, the English side of the parallel texts can then serve as the training data, as they are considered to have been disambiguated and \"sense-annotated\" by the appropriate Chinese translations.",
        "This proposed task is thus similar to the multilingual lexical sample task in Senseval3, except that the training and test examples are collected without manually annotating each individual ambiguous word occurrence.",
        "The system consists of two supervised learning classifiers, support vector machines (SVM) and maximum entropy (ME).",
        "A method of entropy-based feature chosen was experimented to reduce the feature dimensions.",
        "The training data was limited to the labeled data provided by the task, and a PoS-tagger (tree-tagger) was used to get more features."
      ]
    },
    {
      "heading": "2 Features Selection",
      "text": [
        "We used tree-tagger to PoS-tag the texts before the feature extractor.",
        "No other resource is used in the system.",
        "The window size of the context is set to 5 around the ambiguous word.",
        "Only the following features are used in the system:"
      ]
    },
    {
      "heading": "Local words Local PoSs Bag-of-words Local collocations",
      "text": [
        "Here local collocation means any two words which fall into the context window to form collocation pair.",
        "Two methods are used to reduce the dimensions of feature space.",
        "One comes from the linguistic knowledge, some words whose PoSs are IN, DT, SYM, POS, CC or ?``?",
        "are not included as the features.",
        "The second method is based on entropy.",
        "To each word, the training data was split to two parts for parameter estimation.",
        "One (usually consist of 30 ?",
        "50 instances) as the simultaneous test and the rest instances form the other part.",
        "First the entropy of each feature was calculated.",
        "For example, the target word ?work?, it has two senses and the dimensions of its feature space is N. For feature , if it appears in m instances belonging to sense A and n instances in sense B.",
        "So the if",
        "We rank all the features according to their entropy from small to big.",
        "And then first percent lambda features are chosen as the final feature set.",
        "Using this smaller feature set, we use the classifier to make a new prediction.",
        "The parameter ?",
        "is estimated by comparing the system performance on the simultaneous test.",
        "In our system, .68 is chosen.",
        "It means that 68% original features used to form the new feature space.",
        "The same classifier was tried on different feature sets to get different outputs and then were combined."
      ]
    },
    {
      "heading": "3 Classifiers",
      "text": [
        "The Support Vector Machines (SVM) are a group of supervised learning methods that can be applied to classification or regression.",
        "It is developed by Vapnik and has been applied into WSD (Lee et al., 2004).",
        "Since most of the target words have more than two senses, we used the implementation of SVM that includes lib-svm (Chang and Lin, 2001) and svm-multiclass (Joachims, 2004).",
        "To lib-svm, the parameter of ?b?",
        "which is used to obtain probability information after training is set 0 or 1 individually to form different classifiers.",
        "The default linear kernel is used.",
        "Each vector dimension represents a feature.",
        "The numerical value of a vector entry is the numerical value of the corresponding feature.",
        "In our system, we use binary features.",
        "If the context of an instance has a particular feature, then the feature value is set to 1, otherwise the value is set to 0.",
        "ME modeling provides a framework for integrating information for classification from many heterogeneous information sources.",
        "The intuition behind the maximum entropy principle is: given a set of training data, model what is known and assume no further knowledge about the unknown by assigning them equal probability (entropy is maximum).",
        "There are also some researchers using ME to WSD (Chao and Dyer, 2002).",
        "Dekang Lin's implementation of ME was used.",
        "He used Generalized Iterative Scaling (GIS) algorithm."
      ]
    },
    {
      "heading": "4 Development",
      "text": [
        "Because of time constraints, we could not experiment all the training data by cross-validation.",
        "To each target word, we extract first 50 training instances as the test.",
        "For some adjectives, we just extract first 30 because the training data is small.",
        "For ten of adjectives, the training data is too small, we directly use the lib-svm (with probability output) as the final classifier.",
        "Both SVM and ME could output the probability for each instance to each class.",
        "So we try to combine them to improve the performance.",
        "Several methods of combining classifiers have been investigated (Radu et al., 2002).",
        "The enhanced Counted-based Voting (CBV) and Rank-Based Voting, Probability Mixture Model, and best single Classifier are experimented in the training data.",
        "Table 1 and Table 2 indicate the results of nouns and adjectives individually, which were achieved with each of the different methods.",
        "In these tables, \"Orig F.S.\" and \"Red.",
        "F.S.\" mean original feature set and reduced feature set.",
        "\"Prob.",
        "output\" and \"Non Prob.",
        "output\" are two implementation of lib-svm.",
        "The former output the probability of each instance belonging to each class, otherwise the latter not.",
        "Different from the results of Radu, choosing the best single classifier get the better performance than any kinds of combination.",
        "In this paper, we did not list the performances of combining.",
        "According to Table 1 and Table 2, the particular classifier chosen for that word was the one with the highest score in the training data.",
        "Two parameters are different from these two SVMs.",
        "One is the ?-c?, which is the tradeoff between training error and margin.",
        "In lib-svm the value of ?-c?",
        "is set 1; but in svm-multiclass is 0.01.",
        "The other is the strategy of how to utility binary-classification to resolve multi-class.",
        "In svm-multiclass, no strategy is needed since the algorithm in (Crammer and Singer, 2001) solves the multi-class problem directly.",
        "In lib-svm, we use the one-against-all approach which is the default in lib-svm.",
        "Down-sampling is used if some result is trivial classification.",
        "The reason is that the unbalanced distribution of training data.",
        "We compared selecting support vectors and down-sampling.",
        "The latter is better."
      ]
    },
    {
      "heading": "5 Results",
      "text": [
        "We participated in the subtask of SemEval-2007 English lexical sample task via English-Chinese parallel text.",
        "The organizers make use of English-Chinese documents gathered from the URL pairs given by the STRAND Bilingual Databases.",
        "They used this corpus for the evaluation of 40 English words (20 nouns and 20 adjectives).",
        "Our system gives exactly one sense for each test example.",
        "So the recall is always the same as precision.",
        "Micro-average precision is 81.5%.",
        "According to the task organizers, the recall of the best participating in this subtask is 81.9%.",
        "So the performance of our system compares favorably with the best participating system."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "This research is supported by Humanity and Social Science Research Project of China State Education Ministry (No.",
        "06JC740001) and National Basic Research Program of China (No.",
        "2004CB318102).",
        "We are indebted to Helmut Schmid, IMS, University of Stuttgart, for making Tree-Tagger available free of charge.",
        "Finally, the authors thank the organizers Hwee Tou Ng and Yee Seng Chan, for their hard work to collect the training and test data."
      ]
    }
  ]
}
