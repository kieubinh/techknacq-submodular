{
  "info": {
    "authors": [
      "Saif Mohammad",
      "Graeme Hirst",
      "Philip Resnik"
    ],
    "book": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)",
    "id": "acl-W07-2071",
    "title": "Tor, TorMd: Distributional Profiles of Concepts for Unsupervised Word Sense Disambiguation",
    "url": "https://aclweb.org/anthology/W07-2071",
    "year": 2007
  },
  "references": [
    "acl-C92-2070",
    "acl-D07-1060",
    "acl-E06-1016",
    "acl-N06-2015",
    "acl-P04-1036",
    "acl-W06-1605",
    "acl-W07-2004",
    "acl-W07-2009"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Words in the context of a target word have long been used as features by supervised word-sense classifiers.",
        "Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words?the distributional profile of a concept (DPC)?without the use of manually annotated data.",
        "We implemented an unsupervised na?",
        "?ve Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese?",
        "English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17).",
        "We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its performance was poor."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Determining the intended sense of a word is potentially useful in many natural language tasks including machine translation and information retrieval.",
        "The best approaches for word sense disambiguation are supervised and they use words that co-occur with the target as features.",
        "These systems rely on sense-annotated data to identify words that are indicative of the use of the target in each of its senses.",
        "However, only limited amounts of sense-annotated data exist and it is expensive to create.",
        "In our previous work (Mohammad and Hirst, 2006a), we proposed an unsupervised approach to determine the strength of association between a sense or concept and its co-occurring words?the distributional profile of a concept (DPC)?relying simply on raw text and a published thesaurus.",
        "The categories in a published thesaurus were used as coarse senses or concepts (Yarowsky, 1992).",
        "We now show how distributional profiles of concepts can be used to create an unsupervised na?",
        "?ve Bayes word-sense classifier.",
        "We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words.",
        "These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese?",
        "English Lexical Sample Task (task #5).",
        "The English Lexical Sample Task (Pradhan et al., 2007) is a traditional word sense disambiguation task wherein the intended (WordNet) sense of a target word is to be determined from its context.",
        "We manually mapped the WordNet senses to the categories in a thesaurus and the DPC-based na?",
        "?ve Bayes classifier was used to identify the intended sense (category) of the target words.",
        "The object of the Lexical Substitution Task (McCarthy and Navigli, 2007) is to replace a target word in a sentence with a suitable substitute that preserves the meaning of the utterance.",
        "The list of possible substitutes for a given target word is usually contingent on its intended sense.",
        "Therefore, word sense disambiguation is expected to be useful in lexical substitution.",
        "We used the PMI-based classier to determine the intended sense.",
        "The objective of the Multilingual Chinese?",
        "English Lexical Sample Task (Jin et al., 2007) is to select from a given list a suitable English translation of a Chinese target word in context.",
        "Mohammad et al.",
        "(2007) proposed a way to create cross-lingual distributional profiles of a concepts (CL-DPCs)?",
        "the strengths of association between the concepts of one language and words of another.",
        "For this task, we mapped the list of English translations to appropriate thesaurus categories and used an implementation of a CL-DPC?based unsupervised na?",
        "?ve Bayes classifier to identify the intended senses (and thereby the English translations) of target Chinese words."
      ]
    },
    {
      "heading": "2 Distributional profiles of concepts",
      "text": [
        "In order to determine the strength of association between a sense of the target word and its co-occurring words, we need to determine their individual and joint occurrence counts in a corpus.",
        "Mohammad and Hirst (2006a) and Mohammad et al. (2007) proposed ways to determine these counts in a monolingual and cross-lingual framework without the use of sense-annotated data.",
        "We summarize the ideas in this section; the original papers give more details."
      ]
    },
    {
      "heading": "2.1 Word?category co-occurrence matrix",
      "text": [
        "We create a word?category co-occurrence matrix (WCCM) having English word types wen as one dimension and English thesaurus categories cen as another.",
        "We used the Macquarie Thesaurus (Bernard, 1986) both as a very coarse-grained sense inventory and a source of words that together represent each category (concept).",
        "The WCCM is populated with co-occurrence counts from a large English corpus (we used the British National Corpus (BNC)).",
        "A particular cell mi j, corresponding to word weni and concept cenj , is populated with the number of times weni co-occurs with any word that has cenj as one of its senses (i.e., weni co-occurs with any word listed under concept cenj in the thesaurus).",
        "A particular cell mi j, corresponding to word weni and concept cenj , is populated with the number of times weni co-occurs with any word that has cenj as one of its senses (i.e., weni co-occurs with any word listed under concept cenj in the thesaurus).",
        "This matrix, created after a first pass of the corpus, is the base word?category co-occurrence matrix (base WCCM) and it captures strong associations between a sense and co-occurring words (see discussion of the general principle in Resnik (1998)).",
        "From the base WCCM we can determine the number of times a word w and concept c co-occur, the number of times w co-occurs with any concept, and the number of times c co-occurs with any word.",
        "A statistic such as PMI can then give the strength of association between w and c. This is similar to how Yarowsky (1992) identifies words that are indicative of a particular sense of the target word.",
        "Words that occur close to a target word tend to be good indicators of its intended sense.",
        "Therefore, we make a second pass of the corpus, using the base WCCM to roughly disambiguate the words in it.",
        "For each word, the strength of association of each of the words in its context (?5 words) with each of its senses is summed.",
        "The sense that has the highest cumulative association is chosen as the intended sense.",
        "A new bootstrapped WCCM is created such that each cell mi j, corresponding to word weni and concept cenj , is populated with the number of times weni co-occurs with any word used in sense cenj .",
        "Mohammad and Hirst (2006a) used the DPCs created from the bootstrapped WCCM to attain near-upper-bound results in the task of determining word sense dominance.",
        "Unlike the McCarthy et al. (2004) dominance system, this approach can be applied to much smaller target texts (a few hundred sentences) without the need for a large similarly-sense-distributed text1.",
        "Mohammad and Hirst (2006b) used the DPC-based monolingual distributional measures of concept-distance to rank word pairs by their semantic similarity and to correct real-word spelling errors, attaining markedly better results than monolingual distributional measures of word-distance.",
        "In the spelling correction task, the",
        "ate a distributional thesaurus from the target text (if it is large enough?a few million words) or from another large text with a distribution of senses similar to the target text.",
        "nese words and .",
        "distributional concept-distance measures performed better than all WordNet-based measures as well, except for the Jiang and Conrath (1997) measure."
      ]
    },
    {
      "heading": "2.2 Cross-lingual word?category",
      "text": [
        "co-occurrence matrix Given a Chinese word wch in context, we use a Chinese?English bilingual lexicon to determine its different possible English translations.",
        "Each English translation wen may have one or more possible coarse senses, as listed in an English thesaurus.",
        "These English thesaurus concepts (cen) will be referred to as cross-lingual candidate senses of the Chinese word wch.2 Figure 1 depicts examples.",
        "We create a cross-lingual word?category co-occurrence matrix (CL-WCCM) with Chinese word types wch as one dimension and English thesaurus concepts cen as another.",
        "The matrix is populated with co-occurrence counts from a large Chinese corpus; we used a collection of LDC-distributed corpora3?Chinese Treebank English Parallel Corpus, FBIS data, Xinhua Chinese?",
        "English Parallel News Text Version 1.0 beta 2, Chi",
        "one of their cross-lingual candidate senses.",
        "News Translation Text Part 1, and Hong Kong Parallel Text.",
        "A particular cell mi j, corresponding to word wchi and concept cenj , is populated with the number of times the Chinese word wchi co-occurs with any Chinese word having cenj as one of its cross-lingual candidate senses.",
        "For example, the cell for (?space?)",
        "and ?celestial body?",
        "will have the sum of the number of times co-occurs with , , , , , and so on (see Figure 2).",
        "We used the Macquarie Thesaurus (Bernard, 1986) (about 98,000 words).",
        "The possible Chinese translations of an English word were taken from the Chinese?",
        "English Translation Lexicon version 3.0 (Huang and",
        "Graff, 2002) (about 54,000 entries).",
        "This base word?category co-occurrence matrix (base WCCM), created after a first pass of the corpus, captures strong associations between a category (concept) and co-occurring words.",
        "For example, even though we increment counts for both ?",
        "?celestial body?",
        "and ??celebrity?",
        "for a particular instance where co-occurs with , will co-occur with a number of words such as , , and that each have the sense of celestial body in common (see Figure 2), whereas all their other senses are likely different and distributed across the set of concepts.",
        "Therefore, the co-occurrence count of and ?celestial body?",
        "will be relatively higher than that of and ?celebrity?.",
        "As in the monolingual case, a second pass of the corpus is made to disambiguate the (Chinese) words in it.",
        "For each word, the strength of association of each of the words in its context (?5 words) with each of its cross-lingual candidate senses is summed.",
        "The sense that has the highest cumulative association with co-occurring words is chosen as the intended sense.",
        "A new bootstrapped WCCM is created by populating each cell mi j, corresponding to word wchi and concept cenj , with the number of times the Chinese word wchi co-occurs with any Chi",
        "nese word used in cross-lingual sense cenj .",
        "A statistic such as PMI is then applied to these counts to determine the strengths of association between a target concept and co-occurring words, giving the distributional profile of the concept.",
        "Mohammad et al. (2007) combined German text with an English thesaurus using a German?English bilingual lexicon to create German?English DPCs.",
        "These DPCs were used to determine semantic distance between German words, showing that state-of-the-art accuracies for one language can be achieved using a knowledge source (thesaurus) from another.",
        "Given that a published thesaurus has about 1000 categories and the size of the vocabulary N is at least 100,000, the CL-WCCM and the WCCM are much smaller matrices (about 1000?N) than the traditional word?word co-occurrence matrix (N ?N).",
        "Therefore the WCCMs are relatively inexpensive both in terms of memory and computation."
      ]
    },
    {
      "heading": "3 Classification",
      "text": [
        "We implemented two unsupervised classifiers.",
        "The words in context were used as features."
      ]
    },
    {
      "heading": "3.1 Unsupervised Na??ve Bayes Classifier",
      "text": [
        "The na?",
        "?ve Bayes classifier has the following formula to determine the intended sense cnb:",
        "where C is the set of possible senses (as listed in the Macquarie Thesaurus) and W is the set of words that co-occur with the target (we used a window of ?5 words).",
        "Traditionally, prior probabilities of the senses (P(c j)) and the conditional probabilities in the likelihood (?wi?W P(wi|c j)) are determined by simple counts in sense-annotated data.",
        "We approximate these probabilities using counts from the word?category co-occurrence matrix (monolingual or cross-lingual), thereby obviating the need for manually-annotated data.",
        "For the English Lexical Task, mi j is the number of times the English word wi co-occurs with the English category c j?as listed in the word?category co-occurrence matrix (WCCM).",
        "For the Multilingual Chinese?English Lexical Task, mi j is the number of times the Chinese word wi co-occurs with the English category c j?as listed in the cross-lingual word?category co-occurrence matrix (CL-WCCM)."
      ]
    },
    {
      "heading": "3.2 PMI-based classifier",
      "text": [
        "We calculate the pointwise mutual information between a sense of the target word and a co-occurring word using the following formula:",
        "mi j is the count in the WCCM or CL-WCCM (as described in the previous subsection).",
        "For each sense of the target, the sum of the strength of association (PMI) between it and each of the co-occurring words (in a window of ?5 words) is calculated.",
        "The sense with the highest sum is chosen as the intended sense.",
        "Note that this PMI-based classifier does not capitalize on prior probabilities of the different senses."
      ]
    },
    {
      "heading": "4 Data",
      "text": []
    },
    {
      "heading": "4.1 English Lexical Sample Task",
      "text": [
        "The English Lexical Sample Task training and test data (Pradhan et al., 2007) have 22281 and 4851 instances respectively for 100 target words (50 nouns and 50 verbs).",
        "WordNet 2.1 is used as the sense inventory for most of the target words, but certain words have one or more senses from OntoNotes (Hovy et al., 2006).",
        "Many of the fine-grained senses are grouped into coarser senses.",
        "Our approach relies on representing a sense with a number of near-synonymous words, for which a thesaurus is a natural source.",
        "Even though the approach can be ported to WordNet4, there was no easy",
        "and the na?",
        "?ve Bayes classifier on both training and test data way of representing OntoNotes senses with near-synonymous words.",
        "Therefore, we asked four native speakers of English to map the WordNet and OntoNotes senses of the 100 target words to the Macquarie Thesaurus and use it as our sense inventory.",
        "We also wanted to examine the effect of using a very coarse sense inventory such as the categories in a published thesaurus (811 in all).",
        "The annotators were presented with a target word, its WordNet/OntoNotes senses, and the Macquarie senses.",
        "WordNet senses were represented by synonyms, gloss, and example usages.",
        "The OntoNotes senses were described through syntactic patterns and example usages (provided by the task organizers).",
        "The Macquarie senses (categories) were described by the category head (a representative word for the category) and five other words in the category.",
        "Specifically, words in the same semicolon group5 as the target were chosen.",
        "Annotators 1 and 2 labeled each WordNet/OntoNotes sense of the first 50 target words with one or more appropriate Macquarie categories.",
        "Annotators 3 and 4 labeled the senses of the other 50 words.",
        "We combined all four annotations into a WordNet?Macquarie mapping file by taking, for each target word, the union of categories chosen by the two annotators."
      ]
    },
    {
      "heading": "4.2 English Lexical Substitution Task",
      "text": [
        "The English Lexical Substitution Task has 1710 test instances for 171 target words (nouns, verbs, adjectives, and adverbs) (McCarthy and Navigli, 2007).",
        "Some instances were randomly extracted from an Internet corpus, whereas others were selected manually from it.",
        "The target word might or might not be part of a multiword expression.",
        "The task is not tied to any particular sense inventory.",
        "5Words within a semicolon group of a thesaurus tend to be more closely related than words across groups."
      ]
    },
    {
      "heading": "4.3 Multilingual Chinese?English Lexical Sample Task The Multilingual Chinese?English Lexical Sample",
      "text": [
        "Task training and test data (Jin et al., 2007) have 2686 and 935 instances respectively for 40 target words (19 nouns and 21 verbs).",
        "The instances are taken from a corpus of People's Daily News.",
        "The organizers used the Chinese Semantic Dictionary (CSD), developed by the Institute of Computational Linguistics, Peking University, both as a sense inventory and bilingual lexicon (to extract a suitable English translation of the target word once the intended Chinese sense is determined).",
        "In order to determine the English translations of Chinese words in context, our system relies on Chinese text and an English thesaurus.",
        "As the thesaurus is used as our sense inventory, the first author and a native speaker of Chinese mapped the English translations of the target to appropriate Macquarie categories.",
        "We used three examples (from the training data) per English translation for this purpose."
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": []
    },
    {
      "heading": "5.1 English Lexical Sample Task",
      "text": [
        "Both the na?",
        "?ve Bayes classifier and the PMI-based one were applied to the training data.",
        "For each instance, the Macquarie category c that best captures the intended sense of the target was determined.",
        "The instance was labeled with all the WordNet senses that are mapped to c in the WordNet?Macquarie mapping file (described earlier in Section 4.1)."
      ]
    },
    {
      "heading": "5.1.1 Results",
      "text": [
        "Table 1 shows the performances of the two classifiers.",
        "The system attempted to label all instances and so we report accuracy values instead of precision and recall.",
        "The na?",
        "?ve Bayes classifier performed markedly better in training than the PMI",
        "based one and so was applied to the test data.",
        "The table also lists baseline results obtained when a system randomly guesses one of the possible senses for each target word.",
        "Note that since this is a completely unsupervised system, it is not privy to the dominant sense of the target words.",
        "We do not rely on the ranking of senses in WordNet as that would be an implicit use of the sense-tagged SemCor corpus.",
        "Therefore, the most-frequent-sense baseline does not apply.",
        "Table 1 also shows results obtained using just the prior probability and likelihood components of the na?",
        "?ve Bayes formula.",
        "Note that the combined accuracy is higher than individual components for nouns but not for verbs.",
        "The na?",
        "?ve Bayes classifier's accuracy is only about one percentage point lower than that of the best unsupervised system taking part in the task (Pradhan et al., 2007).",
        "One reason that it does better than the PMI-based one is that it takes into account prior probabilities of the categories.",
        "However, using just the likelihood also outperforms the PMI classifier.",
        "This may be because of known problems of using PMI with low frequencies (Manning and Schu?tze, 1999).",
        "In case of verbs, lower combined accuracies compared to when using just prior probabilities suggests that the bag-of-words type features are not very useful.",
        "It is expected that more syntactically oriented features will give better results.",
        "Using window sizes (?1,?2, and ?10) on the training data resulted in lower accuracies than that obtained using a window of ?5 words.",
        "A smaller window size is probably missing useful co-occurring words, whereas a larger window size is adding words that are not indicative of the target's intended sense.",
        "The use of a sense inventory (Macquarie Thesaurus) different from that used to label the data (WordNet) clearly will have a negative impact on the results.",
        "The mapping from WordNet/OntoNotes to Macquarie is likely to have some errors.",
        "Further, for 19 WordNet/OntoNotes senses, none of the annotators found a thesaurus category close enough in meaning.",
        "This meant that our system had no way of correctly disambiguating instances with these senses.",
        "Also impacting accuracy is the significantly fine-grained nature of WordNet compared to the thesaurus.",
        "For example, following are the three coarse",
        "obtained using the PMI-based classifier senses for the noun president in WordNet: (1) executive officer of a firm or college, (2) the chief executive of a republic, and (3) President of the United States.",
        "The last two senses will fall into just one category for most, if not all, thesauri."
      ]
    },
    {
      "heading": "5.2 English Lexical Substitution Task",
      "text": [
        "We used the PMI-based classifier6 for the English Lexical Substitution Task.",
        "Once it identifies a suitable thesaurus category as the intended sense for a target, ten candidate substitutes are chosen from that category.",
        "Specifically, the category head word and up to nine words in the same semicolon group as the target are selected (words within a semicolon group are closer in meaning).",
        "Of the ten candidates, the single-word expression that is most frequent in the BNC is chosen as the best substitute; the motivation is that the annotators, who created the gold standard, were instructed to give preference to single words over multiword expressions as substitutes."
      ]
    },
    {
      "heading": "5.2.1 Results",
      "text": [
        "The system was evaluated not only on the best substitute (BEST) but also on how good the top ten candidate substitutes are (OOT).",
        "Table 2 presents the results.",
        "The system attempted all instances.",
        "The table also lists performances of the system on instances where the target is not part of a multiword expression (NMWT), on instances where the substitute is not a multiword expression (NMWS), on instances randomly extracted from the corpus (RAND), and on instances manually selected (MAN).",
        "Competitive performance of our DPC-based system on the English Lexical Sample Task and the Chinese?English Lexical Sample Task (see next subsection) suggests that DPCs are useful for sense disambiguation.",
        "Poor results on the substitution task can be ascribed to several factors.",
        "First, we used the PMI-based classifier that we found later to be markedly less accurate than the na?",
        "?ve Bayes classifier in the other two tasks.",
        "Second, the words in the thesaurus categories may not always be near-synonyms; they might just be strongly related.",
        "Such words will be poor substitutes for the target.",
        "Also, we chose as the best substitute simply the most frequent of the ten candidates.",
        "This simple technique is probably not accurate enough.",
        "On the other hand, because we chose the candidates without any regard to frequency in a corpus, the system chose certain infrequent words such as wellnigh and ecchymosed, which were not good candidate substitutes."
      ]
    },
    {
      "heading": "5.3 Multilingual Chinese?English Lexical Sample Task",
      "text": [
        "In the Multilingual Chinese?English Lexical Sample Task, both the na?",
        "?ve Bayes classifier and the PMI-based classifier were applied to the training data.",
        "For each instance, the Macquarie category, say c, that best captures the intended sense of the target word is determined.",
        "Then the instance is labeled with all the English translations that are mapped to c in the English translations?Macquarie mapping file (described earlier in Section 4.3)."
      ]
    },
    {
      "heading": "5.3.1 Results",
      "text": [
        "Table 3 shows accuracies of the two classifiers.",
        "Macro average is the ratio of number of instances correctly disambiguated to the total, whereas micro average is the average of the accuracies achieved on each target word.",
        "As in the English Lexical Sample Task, both classifiers, especially the na?",
        "?ve Bayes classifier, perform well above the random baseline.",
        "Since the na?",
        "?ve Bayes classifier also performed markedly better than the PMI-based one in training, it was applied to the test data.",
        "Table 3 also shows results obtained using just the likelihood and prior probability components of the na?",
        "?ve Bayes classifier on the test data."
      ]
    },
    {
      "heading": "5.3.2 Discussion",
      "text": [
        "Our na?",
        "?ve Bayes classifier scored highest of all unsupervised systems taking part in the task (Jin et al., 2007).",
        "As in the English Lexical Sample Task, using just the likelihood again outperforms the PMI classifier on the training data.",
        "The use of a sense inventory different from that used to label the data again will have a negative impact on the results as the mapping may have a few errors.",
        "The annotator believed none of the given Macquarie categories could be mapped to two Chinese Semantic Dictionary senses.",
        "This meant that our system had no way of correctly disambiguating instances with these senses.",
        "There were also a number of cases where more than one CSD sense of a word was mapped to the same Macquarie category.",
        "This occurred for two reasons: First, the categories of the Macquarie Thesaurus act as very coarse senses.",
        "Second, for certain target words, the two CSD senses may be different in terms of their syntactic behavior, yet semantically very close (for example, the ?be shocked?",
        "and ?shocked?",
        "senses of ).",
        "This many-to-one mapping meant that for a number of instances more than one English translation was chosen.",
        "Since the task required us to provide exactly one answer (and there was no partial credit in case of multiple answers), a category was chosen at random."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We implemented a system that uses distributional profiles of concepts (DPCs) for unsupervised word sense disambiguation.",
        "We used words in the context as features.",
        "Specifically, we used the DPCs to create a na?",
        "?ve Bayes word-sense classifier and a simple PMI-based classifier.",
        "Our system attempted three SemEval-2007 tasks.",
        "On the training data of the English Lexical Sample Task (task #17) and the Multilingual Chinese?English Lexical Sample Task (task #5), the na?",
        "?ve Bayes classifier achieved markedly better results than the PMI-based classifier and so was applied to the respective test data.",
        "On both test and training data of both tasks, the system achieved accuracies well above the random baseline.",
        "Further, our system placed best or close to one percentage point from the best among the unsupervised systems.",
        "In the English Lexical Substitution Task (task #10), for which there was no training data, we used the PMI-based classifier.",
        "The system performed poorly, which is probably a result of using the weaker classifier and a simple brute force method for identifying the substitute among the words in a thesaurus category.",
        "Markedly higher-than-baseline performance of the na?",
        "?ve Bayes classifier on task #17 and task #5 suggests that the DPCs are useful for word sense disambiguation."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": []
    }
  ]
}
