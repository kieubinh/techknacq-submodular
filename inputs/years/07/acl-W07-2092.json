{
  "info": {
    "authors": [
      "Ioannis P. Klapaftis",
      "Suresh Manandhar"
    ],
    "book": "Fourth International Workshop on Semantic Evaluations (SemEval-2007)",
    "id": "acl-W07-2092",
    "title": "UOY: A Hypergraph Model for Word Sense Induction \\& Disambiguation",
    "url": "https://aclweb.org/anthology/W07-2092",
    "year": 2007
  },
  "references": [
    "acl-J98-1004",
    "acl-W04-0807",
    "acl-W04-2406",
    "acl-W06-1669",
    "acl-W07-2002"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper is an outcome of ongoing research and presents an unsupervised method for automatic word sense induction (WSI) and disambiguation (WSD).",
        "The induction algorithm is based on modeling the co-occurrences of two or more words using hypergraphs.",
        "WSI takes place by detecting high-density components in the co-occurrence hypergraphs.",
        "WSD assigns to each induced cluster a score equal to the sum of weights of its hyperedges found in the local context of the target word.",
        "Our system participates in SemEval-2007 word sense induction and discrimination task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The majority of both supervised and unsupervised approaches to WSD is based on the ?fixed-list?",
        "of senses paradigm where the senses of a target word is a closed list of definitions coming from a standard dictionary (Agirre et al., 2006).",
        "Lexicographers have long warned about the problems of such an approach, since dictionaries are not suited to this task; they often contain general definitions, they suffer from the lack of explicit semantic and topical relations or interconnections, and they often do not reflect the exact content of the context, in which the target word appears (Veronis, 2004).",
        "To overcome this limitation, unsupervised WSD has moved towards inducing the senses of a target word directly from a corpus, and then disambiguating each instance of it.",
        "Most of the work in WSI is based on the vector space model, where the context of each instance of a target word is represented as a vector of features (e.g second-order word co-occurrences) (SchÃ¼tze, 1998; Purandare and Pedersen, 2004).",
        "These vectors are clustered and the resulting clusters represent the induced senses.",
        "However, as shown experimentally in (Veronis, 2004), vector-based techniques are unable to detect low-frequency senses of a target word.",
        "Recently, graph-based methods were employed in WSI to isolate highly infrequent senses of a target word.",
        "HyperLex (Veronis, 2004) and the adaptation of PageRank (Brin and Page, 1998) in (Agirre et al., 2006) have been shown to outperform the most frequent sense (MFS) baseline in terms of supervised recall, but they still fall short of supervised WSD systems.",
        "Graph-based approaches operate on a 2 dimensional space, assuming a one-to-one relationship between co-occurring words.",
        "However, this assumption is insufficient, taking into account the fact that two or more words are usually combined to form a relationship of concepts in the context.",
        "Additionally, graph-based approaches fail to model and exploit the existence of collocations or terms consisting of more than two words.",
        "This paper proposes a method for WSI, which is based on a hypergraph model operating on a n-dimensional space.",
        "In such a model, co-occurrences of two or more words are represented using weighted hyperedges.",
        "A hyperedge is a more expressive representation than a simple edge, because it is able to capture the information shared by two or more words.",
        "Our system participates in",
        "SemEval-2007 word sense induction and discrimination task (SWSID) (Agirre and Soroa, 2007)."
      ]
    },
    {
      "heading": "2 Sense Induction & Disambiguation",
      "text": [
        "This section presents the induction and disambiguation algorithms."
      ]
    },
    {
      "heading": "2.1 Sense Induction",
      "text": [
        "A hypergraph H = (V, F ) is a generalization of a graph, which consists of a set of vertices V and a set of hyperedges F ; each hyperedge is a subset of vertices.",
        "While an edge relates 2 vertices, a hyperedge relates n vertices (where n ?",
        "1).",
        "In our problem, we represent each word by a vertex and any set of co-occurring related words by a hyperedge.",
        "In our approach, we restrict hyperedges to 2, 3 or 4 words.",
        "Figure 1 shows an example of an abstract hypergraph model 1.",
        "The degree of a vertex is the number of hyperedges it belongs to, and the degree of a hyperedge is the number of vertices it contains.",
        "A path in the hypergraph model is a sequence of vertices and hyperedges such as v1, f1, ..., vi?1, fi?1, vi, where vk are vertices, fk are hyperedges, each hyperedge fk contains vertices to its left and right in the path and no hyperedge or vertex is repeated.",
        "The length of a path is the number of hyperedges it contains, the distance between two vertices is the shortest path between them and the distance between two hyperedges is the minimum distance of all the pairs of their vertices.",
        "Let bp be the base corpus from which we induce the senses of a target word tw.",
        "Our bp consists of BNC and all the SWSID paragraphs containing the 1Image was taken from Wikipedia (Rocchini, 2006) target word.",
        "The total size of bp is 2000 paragraphs.",
        "Note that if SWSID paragraphs of tw are more than 2000, BNC is not used.",
        "In order to build the hypergraph, tw is removed from bp and each paragraph pi is POS-tagged.",
        "Following the example in (Agirre et al., 2006), only nouns are kept and lemmatised.",
        "We apply two filtering heuristics.",
        "The first one is the minimum frequency of nouns (parameter p1), and the second one is the minimum size of a paragraph (parameter p2).",
        "A key problem at this stage is the determination of related vertices (nouns), which can be grouped into hyperedges and the weighting of each such hyperedge.",
        "We deal with this problem by using association rules (Agrawal and Srikant, 1994).",
        "Frequent hyperedges are detected by calculating support, which should exceed a user-defined threshold (parameter p3).",
        "Let f be a candidate hyperedge and a, b, c its vertices.",
        "Then freq(a, b, c) is the number of paragraphs in bp, which contain all the vertices of f , and n is the total size of bp.",
        "Support of f is shown in Equation 1.",
        "The weight assigned to each collected hyperedge, f , is the average of m calculated confidences, where m is the size of f .",
        "Let f be a hyperedge containing the vertices a, b, c. The confidence for the rule r0 = {a, b} => {c} is defined in Equation 2.",
        "(2) Since there is a three-way relationship among a, b and c, we have two more rules r1 = {a, c} => {b} and r2 = {b, c} => {a}.",
        "Hence, the weighting of f is the average of the 3 calculated confidences.",
        "We apply a filtering heuristic (parameter p4) to remove hyperedges with low weights from the hypergraph.",
        "At the end of this stage, the constructed hypergraph is reduced, so that our hypergraph model agrees with the one described in subsection 2.1.1.",
        "are small-world networks, since they exhibited a high clustering coefficient and a small average path length.",
        "Furthermore, the frequency of vertices with a given degree plotted against the degree showed that our hypergraphs satisfy a power-law distribution P (d) = c ?",
        "d?",
        "?, where d is the vertex degree, P (d) is the frequency of vertices with degree d. Figure 2 shows the log-log plot for the noun difference of S3LS.",
        "In order to extract the senses of the target word, we modify the HyperLex algorithm (Veronis, 2004) for selecting the root hubs of the hypergraph as follows.",
        "At each step, the algorithm finds the vertex vi with the highest degree, which is selected as a root hub, according to two criteria.",
        "The first one is the minimum number of hyperedges it belongs to (parameter p5), and the second is the average weight of the first p5 hyperedges (parameter p6) 2.",
        "If these criteria are satisfied, then hyperedges containing vi are grouped to a single cluster cj (new sense) with a 0 distance from vi, and removed from the hypergraph.",
        "The process stops, when there is no vertex eligible to be a root hub.",
        "Each remaining hyperedge, fk, is assigned to the cluster, cj , closest to it, by calculating the minimum distance between fk and each hyperedge of cj as defined in subsection 2.1.1.",
        "The weight assigned to fk is inversely proportional to its distance from cj ."
      ]
    },
    {
      "heading": "2.2 Word Sense Disambiguation",
      "text": [
        "Given an instance of the target word, tw, paragraph pi containing tw is POS-tagged, nouns are kept and 2Hyperedges are sorted in decreasing order of weight lemmatised.",
        "Next, each induced cluster cj is assigned a score equal to the sum of weights of its hyperedges found in pi."
      ]
    },
    {
      "heading": "3 Evaluation",
      "text": []
    },
    {
      "heading": "3.1 Preliminary Experiments",
      "text": [
        "This method is an outcome of ongoing research.",
        "Due to time restrictions we were able to test and tune (Table 1), but not optimize, our system only on a very small set of nouns of S3LS targeting at a high supervised recall.",
        "Our supervised recall on the 10 first nouns of S3LS was 66.8%, 9.8% points above the MFS baseline."
      ]
    },
    {
      "heading": "Parameter Value",
      "text": []
    },
    {
      "heading": "3.2 SemEval-2007 Results",
      "text": [
        "Tables 2 and 3 show the average supervised recall, FScore, entropy and purity of our system on nouns and verbs of the test data respectively.",
        "The submitted answer consisted only of the winning cluster per instance of a target word, in effect assigning it with weight 1 (default).",
        "Entropy measures how well the various gold standard senses are distributed within each cluster, while purity measures how pure a cluster is, containing objects from primarily one class.",
        "In general, the lower the entropy and the larger the purity values, the better the clustering algorithm performs.",
        "For nouns our system achieves a low entropy and a high purity outperforming the MFS baseline, but a lower FScore.",
        "This can be explained by the fact that the average number of clusters we produce for nouns is 11, while the gold standard average of senses is around 2.8.",
        "For verbs the performance of our system",
        "is worse than for nouns, although entropy and purity still outperform the MFS baseline.",
        "FScore is very low, despite that the average number of clusters we produce for verbs (around 8) is less than the number of clusters we produce for nouns.",
        "This means that for verbs the senses of gold standard are much more spread among induced clusters than for nouns, causing a low unsupervised recall.",
        "Overall, FScore results are in accordance with the idea of microsenses mentioned in (Agirre et al., 2006).",
        "FScore is biased towards clusters similar to the gold standard senses and cannot capture that theory.",
        "Our supervised recall for verbs is 73.3%, and below the MFS baseline (76.2%), which no system managed to outperform.",
        "For nouns our supervised recall is 81.6%, which is around 0.7% above the MFS baseline.",
        "In order to fully examine the performance of our system we applied a second evaluation of our methodology using the SWSID official software.",
        "The solution per target word instance included the entire set of clusters with their associated weights (Table 4).",
        "Results show that the submitted answer (instance - winning cluster), was degrading seriously our performance both for verbs and nouns due to the loss of information in the mapping step.",
        "Our supervised recall for nouns has outperformed the MFS baseline by 3.4% with the best system achieving 86.8%.",
        "Performance for verbs is 75.6%, 0.6% below the best system and MFS."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "We have presented a hypergraph model for word sense induction and disambiguation.",
        "Preliminary experiments suggested that our reduced hypergraphs are small-world networks.",
        "WSI identifies the highly connected components (hubs) in the hypergraph, while WSD assigns to each cluster a score equal to the sum of weights of its hyperedges found in the local context of a target word.",
        "Results show that our system achieves high entropy and purity performance outperforming the MFS baseline.",
        "Our methodology achieves a low FScore producing clusters that are dissimilar to the gold standard senses.",
        "Our supervised recall for nouns is 3.4% above the MFS baseline.",
        "For verbs, our supervised recall is below the MFS baseline, which no system managed to outperform."
      ]
    }
  ]
}
