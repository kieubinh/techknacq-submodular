{
  "info": {
    "authors": [
      "Takashi Ninomiya",
      "Takuya Matsuzaki",
      "Yusuke Miyao",
      "Jun'ichi Tsujii"
    ],
    "book": "Tenth International Conference on Parsing Technologies",
    "id": "acl-W07-2208",
    "title": "A log-linear model with an n-gram reference distribution for accurate HPSG parsing",
    "url": "https://aclweb.org/anthology/W07-2208",
    "year": 2007
  },
  "references": [
    "acl-A00-2021",
    "acl-C04-1041",
    "acl-H05-1059",
    "acl-J93-2004",
    "acl-J96-1002",
    "acl-J97-4005",
    "acl-J99-2004",
    "acl-N04-1013",
    "acl-P00-1061",
    "acl-P02-1036",
    "acl-P03-1046",
    "acl-P04-1014",
    "acl-P05-1011",
    "acl-P06-1037",
    "acl-P06-1041",
    "acl-P99-1069",
    "acl-W02-2018",
    "acl-W04-0307",
    "acl-W05-1511",
    "acl-W06-1619",
    "acl-W97-0302"
  ],
  "sections": [
    {
      "text": [
        "A log-linear model with an n-gram reference distribution for accurate HPSG",
        "parsing",
        "Takashi Ninomiya Takuya Matsuzaki",
        "Information Technology Center Department of Computer Science",
        "Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan",
        "This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing.",
        "In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging.",
        "Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but su-pertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined.",
        "We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG.",
        "This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "For the last decade, fast, accurate and wide-coverage parsing for real-world text has been pursued in sophisticated grammar formalisms, such as head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994), combinatory categorial grammar (CCG) (Steedman, 2000) and lexical function grammar (LFG) (Bresnan, 1982).",
        "They are preferred because they give precise and in-depth analyses for explaining linguistic phenomena, such as pas-sivization, control verbs and relative clauses.",
        "The main difficulty of developing parsers in these formalisms was how to model a well-defined probabilistic model for graph structures such as feature structures.",
        "This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Rie-zler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005).",
        "Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; Ninomiya et al., 2005).",
        "An interesting approach to the problem ofparsing efficiency was using supertagging (Clark and Cur-",
        "Proceedings of the 10th Conference on Parsing Technologies, pages 6G-68, Prague, Czech Republic, June 2GG7.",
        "(c 2GG7 Association for Computational Linguistics",
        "Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999).",
        "Supertagging is a process where words in an input sentence are tagged with 'supertags,' which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG.",
        "The concept of supertagging is simple and interesting, and the effects of this were recently demonstrated in the case of a CCG parser (Clark and Curran, 2004a) with the result of a drastic improvement in the parsing speed.",
        "Wang and Harper (2004) also demonstrated the effects of supertagging with a statistical constraint dependency grammar (CDG) parser by showing accuracy as high as the state-of-the-art parsers, and Foth et al.",
        "(2006) and Foth and Menzel (2006) reported that accuracy was significantly improved by incorporating the supertagging probabilities into manually tuned Weighted CDG.",
        "Ninomiya et al.",
        "(2006) showed the parsing model using only supertagging probabilities could achieve accuracy as high as the probabilistic model for phrase structures.",
        "This means that syntactic structures are almost determined by supertags as is claimed by Bangalore and Joshi (1999).",
        "However, supertaggers themselves were heuristically used as an external tagger.",
        "They filter out unlikely lexical entries just to help parsing (Clark and Curran, 2004a), or the probabilistic models for phrase structures were trained independently of the supertagger's probabilistic models (Wang and Harper, 2004; Ninomiya et al., 2006).",
        "In the case of supertagging of Weighted CDG (Foth et al., 2006), parameters for Weighted CDG are manually tuned, i.e., their model is not a well-defined probabilistic model.",
        "We propose a log-linear model for probabilistic HPSG parsing in which the supertagging probabilities are introduced as a reference distribution for the probabilistic HPSG.",
        "The reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and part-of-speech (POS) n-gram as defined in the CCG/HPSG/CDG supertagging.",
        "This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model.",
        "We compared our model with the probabilistic model for phrase structures (Miyao and Tsu-jii, 2005).",
        "This model uses word and POS unigram for its reference distribution, i.e., the probabilities of unigram supertagging.",
        "Our model can be regarded as an extension of a unigram reference distribution to an n-gram reference distribution with features that are used in supertagging.",
        "We also compared with a probabilistic model in (Ninomiya et al., 2006).",
        "The probabilities of their model are defined as the product of probabilities of supertagging and probabilities of the probabilistic model for phrase structures, but their model was trained independently of supertag-ging probabilities, i.e., the supertagging probabilities are not used for reference distributions."
      ]
    },
    {
      "heading": "2. HPSG and probabilistic models",
      "text": [
        "HPSG (Pollard and Sag, 1994) is a syntactic theory based on lexicalized grammar formalism.",
        "In HPSG, a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics.",
        "The structures of sentences are explained using combinations of schemata and lexical entries.",
        "Both schemata and lexical entries are represented by typed feature structures, and constraints represented by feature structures are checked with unification.",
        "An example of HPSG parsing of the sentence \"Spring has come\" is shown in Figure 1.",
        "First, each of the lexical entries for \"has\" and \"come\" is unified with a daughter feature structure of the Head-Complement Schema.",
        "Unification provides the phrasal sign of the mother.",
        "The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs.",
        "Finally, the parse result is output as a phrasal sign that dominates the sentence.",
        "Given a set W of words and a set F of feature structures, an HPSG is formulated as a tuple, G = (L, R), where lexical entries, and R is a set of schemata; i.e., r G R is a partial function: F x F â€“ F.",
        "Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of",
        "HEAD verb",
        "I head-comp l_",
        "has HEAD verb SUBJ < > CGMPS < >",
        "Subject-head head-comp",
        "parsing.",
        "Note that HPSG is one of the lexicalized grammar formalisms, in which lexical entries determine the dominant syntactic structures.",
        "defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996).",
        "The probability that a parse result T is assigned to a given sentence w = (w1}... ,wn) is",
        "(Probabilistic HPSG)",
        "phpsg(T |w)",
        "Xufu(T)j",
        "where \\u is a model parameter, fu is a feature function that represents a characteristic of parse tree T, and Zw is the sum over the set of all possible parse trees for the sentence.",
        "Intuitively, the probability is defined as the normalized product of the weights exp(Au) when a characteristic corresponding to fuappears in parse result T. The model parameters, Au, are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data.",
        "However, the above model cannot be easily estimated because the estimation requires the computation of p(T|w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.",
        "To make the model estimation tractable, German and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T|w).",
        "Miyao and Tsujii (2005) also introduced a preliminary probabilistic model p0(T |w) whose estimation does not require the parsing of a treebank.",
        "This model is introduced as a reference distribution (Jelinek, 1998; Johnson and Riezler, 2000) of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage (Miyao and Tsujii, 2005), or a probabilistic model can be augmented by several distributions estimated from the larger and simpler corpus (Johnson and Riezler, 2000).",
        "In (Miyao and Tsujii, 2005), p0(T|w) is defined as the product of probabilities of selecting lexical entries with word and POS unigram features:",
        "where U is a lexical entry assigned to word Wi in T and p(lilwi) is the probability of selecting lexical entry li for wi.",
        "In the experiments, we compared our model with other two types of probabilistic models using a supertagger (Ninomiya et al., 2006).",
        "The first one is the simplest probabilistic model, which is defined with only the probabilities of lexical entry selection.",
        "It is defined simply as the product of the probabilities of selecting all lexical entries in the sentence; i.e., the model does not use the probabilities of phrase structures like the probabilistic models explained above.",
        "Given a set of lexical entries, L, a sentence, w = (wi,..., wn), and the probabilistic model of lexical entry selection, p(li Â£ L|w, i), the first model is formally defined as follows:",
        "l, VP, come, VBN, HEAD <Np> COMPS <>",
        "come/VBN",
        "where li is a lexical entry assigned to word wi in T and p(lilw, i) is the probability of selecting lexical entry li for wi.",
        "The probabilities of lexical entry selection, p(lilw, i), are defined as follows:",
        "(Probabilistic model of lexical entry selection)",
        "where Zw is the sum over all possible lexical entries for the word wi.",
        "The second model is a hybrid model of supertag-ging and the probabilistic HPSG.",
        "The probabilities are given as the product of Ninomiya et al.",
        "(2006)'s model 1 and the probabilistic HPSG.",
        "Pmodei3(T w) = Pmodeil(T ^Phpsg^ w",
        "In the experiments, we compared our model with Miyao and Tsujii (2005)'s model and Ninomiya et",
        "funary froot fiex",
        "sPi, syi, hwi, hPi, hli, sPr, syr, hwr, hPr, hlr name of the applied schema distance between the head words ofthe daughters whether a comma exists between daughters",
        "and/or inside daughter phrases number ofwords dominated by the phrase symbol ofthe phrasal category surface form of the head word part-of-speech of the head word lexical entry assigned to the head word i-th word part-of-speech for wilexical entry for wi",
        "al.",
        "(2006)'s model 1 and 3.",
        "The features used in our model and their model are combinations of the feature templates listed in Table 1 and Table 2.",
        "The feature templates fbinary and funary are defined for constituents at binary and unary branches, froot is a feature template set for the root nodes of parse trees.",
        "fiex is a feature template set for calculating the unigram reference distribution and is used in Miyao and Tsujii (2005)'s model.",
        "fsptag is a feature template set for calculating the probabilities of selecting lexical entries in Ninomiya et al.",
        "(2006)'s model 1 and 3.",
        "The feature templates in fsptag are word trigrams and POS 5-grams.",
        "An example of features applied to the parse tree for the sentence \"Spring has come\" is shown in Figure 2.",
        "S Probabilistic HPSG with an n-gram reference distribution",
        "In this section, we propose a probabilistic model with an n-gram reference distribution for probabilistic HPSG parsing.",
        "This is an extension of Miyao and Tsujii (2005)'s model by replacing the unigram reference distribution with an n-gram reference distribution.",
        "Our model is formally defined as follows:",
        "expf Xufuili, w,i) combinations of feature templates for funary combinations of feature templates for f\\ex combinations of feature templates for fsptag (Probabilistic HPSG with an n-gram reference distribution) Pnref (T w",
        "HEAD noun",
        "SUBJ <>",
        "COMPS <>",
        "Znref Znref â€“ ",
        "Pmodell(T |w)exp > Xu fu(T)",
        "In our model, Ninomiya et al.",
        "(2006)'s model 1 is used as a reference distribution.",
        "The probabilistic model of lexical entry selection and its feature templates are the same as defined in Ninomiya et al.",
        "(2006)'s model 1.",
        "The formula of our model is the same as Ni-nomiya et al.",
        "(2006)'s model 3.",
        "But, their model is not a probabilistic model with a reference distribution.",
        "Both our model and their model consist of the probabilities for lexical entries (= pmodeii(T|w)) and the probabilities for phrase structures (= the rest of each formula).",
        "The only difference between our model and their model is the way of how to train model parameters for phrase structures.",
        "In both our model and their model, the parameters for lexical entries (= the parameters of pmodei1(T|w)) are first estimated from the word and POS sequences independently of the parameters for phrase structures.",
        "That is, the estimated parameters for lexical entries are the same in both models, and hence the probabilities of pmodei1(T |w) of both models are the same.",
        "Note that the parameters for lexical entries will never be updated after this estimation stage; i.e., the parameters for lexical entries are not estimated in the same time with the parameters for phrase structures.",
        "The difference of our model and their model is the estimation of parameters for phrase structures.",
        "In our model, given the probabilities for lexical entries, the parameters for phrase structures are estimated so as to maximize the entire probabilistic model (= the product of the probabilities for lexical entries and the probabilities for phrase structures) in the training corpus.",
        "In their model, the parameters for phrase structures are trained without using the probabilities for lexical entries, i.e., the parameters for phrase structures are estimated so as to maximize the probabilities for phrase structures only.",
        "That is, the parameters for lexical entries and the parameters for phrase structures are trained independently in their model.",
        "Miyao and Tsujii (2005)'s model also uses a reference distribution, but with word and POS unigram features, as is explained in the previous section.",
        "The only difference between our model and Miyao and Tsujii (2005)'s model is that our model uses sequences of word and POS tags as n-gram features for selecting lexical entries in the same way as su-pertagging does."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We evaluated the speed and accuracy of parsing by using Enju 2.1, the HPSG grammar for English",
        "No.",
        "of tested sentences Total No.",
        "of sentences Avg.",
        "length of tested sentences",
        "Section 23 (Gold POSs)",
        "abilistic models were trained using the same portion of the treebank.",
        "We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and quick check (Maloufetal., 2000).",
        "We measured the accuracy of the predicate-argument relations output of the parser.",
        "A predicate-argument relation is defined as a tuple (a,Wh,a,wa), where a is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ARG4), and wa is the head word of the argument.",
        "Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser.",
        "Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label.",
        "This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark",
        "The HPSG treebank is used for training the probabilistic model for lexical entry selection, and hence, those lexical entries that do not appear in the treebank are rarely selected by the probabilistic model.",
        "The 'effective' tag set size, therefore, is around 1,361, the number of lexical entries without those never-seen lexical entries.",
        "and Curran, 2004b; Miyao and Tsujii, 2005).",
        "The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU.",
        "Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of < 100 words in Section 23.",
        "The performance of each model was analyzed using the sentences in Section 24 of < 100 words.",
        "Table 3 details the numbers and average lengths of the tested sentences of < 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24.",
        "The parsing performance for Section 23 is shown in Table 4.",
        "The upper halfofthe table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger (Tsuruoka and Tsujii, 2005).",
        "LF and UF in the figure are labeled F-score and unlabeled F-score.",
        "F-score is the harmonic mean of precision and recall.",
        "We evaluated our model in two settings.",
        "One is implemented with a narrow beam width ('our model 1' in the figure), and the other is implemented with a wider beam width ('our model 2' in the figure).",
        "'our model 1' was introduced to measure the performance with balanced F-score and speed, which we think appropriate for practical use.",
        "'our model 2' was introduced to measure how high the precision and recall could reach by sacrificing speed.",
        "Our models increased the parsing accuracy.",
        "'our model 1' was around 2.6 times faster and had around 2.65 points higher F-score than Miyao and Tsujii (2005)'s model.",
        "'our model 2' was around 2.3 times slower but had around 2.9 points higher F-score than Miyao and Tsujii (2005)'s model.",
        "We must admit that the difference between our models and Ninomiya et al.",
        "(2006)'s model 3 was not as great as the difference from Miyao and Tsujii (2005)'s model, but 'our model 1' achieved 0.56 points higher F-score, and 'our model 2' achieved 0.8 points higher F-score.",
        "When the automatic POS tagger was introduced, F-score dropped by around 2.4 points for all models.",
        "LP",
        "LR",
        "LF",
        "UP",
        "UR",
        "UF",
        "Avg.",
        "time",
        "(%)",
        "(%)",
        "(%)",
        "(%)",
        "(%)",
        "(%)",
        "(ms)",
        "Miyao and Tsujii (2005)'s model",
        "87.26",
        "86.50",
        "86.88",
        "90.73",
        "89.93",
        "90.33",
        "604",
        "Ninomiya et al.",
        "(2006)'s model 1",
        "87.23",
        "86.47",
        "86.85",
        "90.05",
        "89.27",
        "89.66",
        "129",
        "Ninomiya et al.",
        "(2006)'s model 3",
        "89.48",
        "88.58",
        "89.02",
        "92.33",
        "91.40",
        "91.86",
        "152",
        "our model 1",
        "89.78",
        "89.28",
        "89.53",
        "92.58",
        "92.07",
        "92.32",
        "234",
        "our model 2",
        "90.03",
        "89.60",
        "89.82",
        "92.82",
        "92.37",
        "92.60",
        "1379",
        "Section 23 (POS tagger)",
        "LP",
        "LR",
        "LF",
        "UP",
        "UR",
        "UF",
        "Avg.",
        "time",
        "(%)",
        "(%)",
        "(%)",
        "(%)",
        "(%)",
        "(%)",
        "(ms)",
        "Miyao and Tsujii (2005)'s model",
        "84.96",
        "84.25",
        "84.60",
        "89.55",
        "88.80",
        "89.17",
        "674",
        "Ninomiya et al.",
        "(2006)'s model 1",
        "85.00",
        "84.01",
        "84.50",
        "88.85",
        "87.82",
        "88.33",
        "154",
        "Ninomiya et al.",
        "(2006)'s model 3",
        "87.35",
        "86.29",
        "86.82",
        "91.24",
        "90.13",
        "90.68",
        "183",
        "Matsuzaki et al.",
        "(2007)'s model",
        "86.93",
        "86.47",
        "86.70",
        "-",
        "-",
        "-",
        "30",
        "our model 1",
        "87.28",
        "87.05",
        "87.17",
        "91.62",
        "91.38",
        "91.50",
        "260",
        "our model 2",
        "87.56",
        "87.46",
        "87.51",
        "91.88",
        "91.77",
        "91.82",
        "1821",
        "The terms k and 5 are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell.",
        "The terms a and ÃŸ are the thresholds of the number and the beam width of lexical entries, and 0 is the beam width for global thresholding (Goodman, 1997).",
        "The terms with suffixes 0 are the initial values.",
        "The parser iterates parsing until it succeeds to generate a parse tree.",
        "The parameters increase for each iteration by the terms prefixed by A, and parsing finishes when the parameters reach the terms with suffixes last.",
        "Details of the parameters are written in (Ninomiya et al., 2005).",
        "The beam thresholding parameters for 'our model 2' are ao = 18, Aa = 6,alast = 42, ÃŸo = 9.0, AÃŸ = 3.0,ÃŸlast = 21.0,5o = 18, A5 = 6,5last = 42, k0 = 9.0, Ak = 3.0,Klast = 21.0.",
        "In 'our model 2', the global thresholding was not used.",
        "posed a technique for efficient HPSG parsing with supertagging and CFG filtering.",
        "Their results with the same grammar and servers are also listed in the lower half of Table 4.",
        "They achieved drastic improvement in efficiency.",
        "Their parser ran around 6 times faster than Ninomiya et al.",
        "(2006)'s model 3, 9 times faster than 'our model 1' and 60 times faster than 'our model 2.'",
        "Instead, our models achieved better accuracy.",
        "'our model 1' had around 0.5 higher F-score, and 'our model 2' had around 0.8 points higher F-score.",
        "Their efficiency is mainly due to elimination of ungrammatical lexical entries by the CFG filtering.",
        "They first parse a sentence with a CFG grammar compiled from an HPSG grammar, and then eliminate lexical entries that are not in the parsed CFG trees.",
        "Obviously, this technique can also be applied to the HPSG parsing of our models.",
        "We think that efficiency of HPSG parsing with our models will be drastically improved by applying this technique.",
        "The average parsing time and labeled F-score curves of each probabilistic model for the sentences in Section 24 of < 100 words are graphed in Figure 3.",
        "The graph clearly shows the difference of our model and other models.",
        "As seen in the graph, our model achieved higher F-score than other model when beam threshold was widen.",
        "This implies that other models were probably difficult to reach the F-score of 'our model 1' and 'our model 2' for Section 23 even if we changed the beam thresholding parameters.",
        "However, F-score of our model dropped easily when we narrow down the beam threshold, compared to other models.",
        "We think that this is mainly due to its bad implementation of parser interface.",
        "The n-gram reference distribution is incorporated into the kernel of the parser, but the n-gram features and a maximum entropy estimator are defined in other modules; n-gram features are defined in a grammar module, and a maximum entropy estimator for the n-gram reference distribution is implemented with a general-purpose maximum entropy estimator module.",
        "Consequently, strings that represent the n-gram information are very frequently changed into feature structures and vice versa when they go in and out of the kernel of the parser.",
        "On the other hand, Ninomiya et al.",
        "(2006)'s model 3 uses the supertagger as an external module.",
        "Once the parser acquires the supertagger's outputs, the n-gram information never goes in and out of the kernel.",
        "This advantage of Ni-nomiya et al.",
        "(2006)'s model can apparently be implemented in our model, but this requires many parts of rewriting of the implemented parser.",
        "We estimate that the overhead of the interface is around from 50 to 80 ms/sentence.",
        "We think that reimplementation of the parser will improve the parsing speed as estimated.",
        "In Figure 3, the line of our model crosses the line of Ninomiya et al.",
        "(2006)'s model.",
        "If the estimation is correct, our model will be faster and more accurate so that the lines in the figure do not cross.",
        "Speed-up in our model is left as a future work."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We proposed a probabilistic model in which su-pertagging is consistently integrated into the probabilistic model for HPSG.",
        "In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertag-ging.",
        "We conducted experiments on the Penn Treebank with a wide-coverage HPSG parser.",
        "In the experiments, we compared our model with the probabilistic HPSG with a unigram reference distribution (Miyao and Tsujii, 2005) and the probabilistic HPSG with supertagging (Ninomiya et al., 2006).",
        "Though our model was not as fast as Ninomiya et al.",
        "(2006)'s models, it achieved the highest accuracy among them.",
        "Our model had around 2.65 points higher F-score than Miyao and Tsujii (2005)'s model and around 0.56 points higher F-score than the Ninomiya et al.",
        "(2006)'s model 3.",
        "When we sacrifice parsing speed, our model achieved around 2.9 points higher F-score than Miyao and Tsujii (2005)'s model and around 0.8 points higher F-score than Ninomiya et al.",
        "(2006)'s model 3.",
        "Our model achieved higher F-score because parameters for phrase structures in our model are trained with the supertagging probabilities, which are not in other models."
      ]
    }
  ]
}
