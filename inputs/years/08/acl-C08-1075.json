{
  "info": {
    "authors": [
      "Erwan Moreau",
      "François Yvon",
      "Olivier Cappé"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C08-1075",
    "title": "Robust Similarity Measures for Named Entities Matching",
    "url": "https://aclweb.org/anthology/C08-1075",
    "year": 2008
  },
  "references": [
    "acl-N06-1060"
  ],
  "sections": [
    {
      "text": [
        "Matching coreferent named entities without prior knowledge requires good similarity measures.",
        "Soft-TFIDF is a fine-grained measure which performs well in this task.",
        "We propose to enhance this kind of metrics, through a generic model in which measures may be mixed, and show experimentally the relevance of this approach."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In this paper, we study the problem of matching coreferent named entities (NE in short) in text collections, focusing primarily on orthographic variations in nominal groups (we do not handle the case of pronominal references).",
        "Identifying textual variations in entities is useful in many text mining and/or information retrieval tasks (see for example (Pouliquen et al., 2006)).",
        "As described in the literature (e.g. (Christen, 2006)), textual differences between entities are due to various reasons: typographical errors, names written in different ways (with/without first name/title, etc.",
        "), abbreviations, lack of precision in organization names, transliterations, etc.",
        "For example, one wants \"Mr. Rumyantsev\" to match with \"Alexander Rumyanstev\" but not with \"Mr. Ryabev\".",
        "Here we do not address the related problem of disambiguation (e.g. knowing whether a given occurrence of \"George Bush\" refers to the 41st or 43rd president of the USA), because it is technically very different from the matching problem.",
        "There are different ways to tackle the problem of NE matching: the first and certainly most reliable one consists in studying the specific features of the data, and then use any available tool to design a specialized method for the matching task.",
        "This approach will generally take advantage of language-specific (e.g. in (Freeman et al., 2006)) and domain-specific knowledge, of any external resources (e.g. database, names dictionaries, etc.",
        "), and of any information about the entities to process, e.g. their type (person name, organization, etc.",
        "), or internal structure (e.g. in (Prager et al., 2007)).",
        "In such an in-depth approach, supervised learning is helpful: it has been used for example in a database context in (Bilenko et al., 2003), but this approach requires labeled data which is usually costly.",
        "All those data specific appproaches would necessitate some sort of human expertise.",
        "The second approach is the robust one: we propose here to try to match any kind of NE, extracted from \"real world\" (potentially noisy) sources, without any kind of prior knowledge.",
        "One looks for coreferent NE, whatever their type, source, language or quality.",
        "Such robust similarity methods may be useful for a lot of generic tasks, in which maximum accuracy is not the main criterion, or simply where the required resources are not available.",
        "The literature on string comparison metrics is abundant, containing both general techniques and",
        "'Actually we have only studied English and French (our approach is neither \"multilingual\", in the sense that it is not specific to multilingual documents).",
        "more linguistically motivated measures, see e.g. (Cohen et al., 2003) for a review.",
        "From a bird's eye view, these measures can be sorted in two classes: \"Sequential character-based methods\" and \"Bag-of-words methods\".",
        "Both classes show relevant results, but do not capture the same kind of similarity.",
        "In a robust approach for NE matching, one needs a more fine-grained method, which performs at least as well as bag-of-words methods, without ignoring coreferent pairs that such methods miss.",
        "A first attempt in this direction was introduced in (Cohen et al., 2003), in the form of a measure called Soft-TFIDF.",
        "We will show that this measure has theoretical pitfalls and a few practical drawbacks.",
        "Nevertheless, Soft-TFIDF outperforms the better standard string similarity measures in the NE matching task.",
        "That is why we propose to generalize and improve its principle, and show experimentally that this approach is relevant.",
        "In section 2 we introduce standard similarity measures and enhance the definition of Soft-TFIDF.",
        "Then we define a generic model in which similarity measures may be combined (section 3).",
        "Finally, section 4 shows that experiments with two different corpora validate our approach."
      ]
    },
    {
      "heading": "2. Approximate matching methods",
      "text": [
        "We present below some of the main string similarity measures used to match named entities (Christen, 2006; Cohen et al., 2003; Bilenko et al., 2003).",
        "Levenshtein edit distance.",
        "This well-known distance metric d represents the minimum number of insertions, deletions or substitutions needed to transform a string x into another string y.",
        "For example, d(kitten, sitting) = 3 (k s, e i – > i, e i – > g).",
        "The corresponding normalized similarity measure is defined as s = 1 – d/max(\\x\\, \\y\\).",
        "A lot of variants and/or improvements exist (Navarro, 2001), among which:",
        "• Damerau.",
        "One basic edit operation is added: a transposition consists in swapping two characters;",
        "• Needleman-Wunch.",
        "Basic edit operation costs are parameterized: G is the cost of a gap",
        "(insertion or deletion), and there is a function cost(c, d) which gives the cost of substituting c with d for any pair of characters (c, d).",
        "Jam metric (Winkler, 1999).",
        "This measure is based on the number and the order of common characters.",
        "Given two strings and y = b\\... bm, let H = min(n, m)/2: a» is in common with y if there exists bj in y such that a» = bj and i – H < j < i + H. Let x' = a[ ... a'n, (resp.",
        "y' = b'i... b'm,) be the sequence of characters from x (resp.",
        "y) in common with y (resp.",
        "x), in the order they appear in x (resp.",
        "y).",
        "Any position i such that a[ / b[ is called a transposition.",
        "Let T be the number of transpositions between x' and y divided by 2:",
        "With these methods, each NE is represented as a set of features (generally words or characters n-grams).",
        "LetX = {xi}i<i<n and Y = {yi}i<i<mbe the sets representing the entities x, y.",
        "Simplest measures only count the number of elements in common, e.g:",
        "Overlap(x, y)",
        "min(\\X\\,\\Y\\) Some more subtle techniques are based on a vector representation of entities x and y, which may take into account parameters that are are not included in the sets themselves.",
        "Let A = (ai,..., a|S|) and B = (b\\,..., be such vectors, the widely used cosine similarity is:",
        "Traditionally, TF-IDF weights are used in vectors (Term Frequency-Inverse Document Frequency).",
        "In the NE case, this value represents the importance each feature w (e.g. word) has for an entity x belonging to the set E of entities:",
        "tfidf(w, x) = ti(w,x) x idf(w).",
        "with nw>x the number of times w appears in x.",
        "Thus the similarity score is CosTFIDF(a;, y) = Cos(A, B), where each (resp.",
        "bA in A (resp.",
        "in B) is tfidf (wj, x) (resp.",
        "tfidf (wj, y)).",
        "Experiments show that sequential character-based measures catch mainly coreferent pairs of long NE that differ only by a few characters.",
        "Bag-of-words methods suit better to the NE matching problem, since they are more flexible about word order and position.",
        "But a lot of coreferent pairs can not be identified by such measures, because of small differences between words: for example, \"Director ElBaradei \" and \"Director-General ElBareidi \" is out of reach for such methods.",
        "That is why \"second level\" measures are relevant: their principle is to apply a sub-measure sim' to all pairs of words between the two NE and to compute a final score based on these values.",
        "This approach is possible because NE generally contain only a few words.",
        "Monge-Elkan measure belongs to this category: it simply computes the average of the better pairs of words according to the sub-measure:",
        "But experiments show that Monge-Elkan does not perform well.",
        "Actually, its very simple behavior favors too much short entities, because averaging penalizes a lot every non-matching word.",
        "A more elaborated measure is proposed in (Cohen et al., 2003): Soft-TFIDF is intended precisely to take advantage of the good results obtained with Cosine/TFIDF, without automatically discarding words which are not strictly identical.",
        "The original definition is the following: let CLOSER, X,F) be the set of words w G X such that there exists a word v G Y such that sim'(w, v) > 0.",
        "Let N(w,Y) = max({sim'(w, v)\\v G Y}).",
        "For any w G CLOSER, X, F),let",
        "This definition is not entirely correct, because weight(w,Y) = 0 if w £ Y (in other words, w must appear in both X and Y, thus SoftTFIDF(X, Y) would always be equal to CosTFIDF(X, Y)).",
        "We propose instead the following corrected definition, which corresponds to the implementation the authors provided in the package SecondString:",
        "Let CLOSEST^, w,Z) = {v G Z \\W G Z : sim'(w,v) > sim'(w,v') A sim'(w, v) > 0}.",
        "SoftTFiDF(X, Y) = ^ weight(w,X) ■ aw>Y.",
        "where aW)Z = 0 if CLOSEST(#, w, Z) = 0, and = weight(w',Z) ■ sim'(w,w') otherwise, with w' G CLOSEST(#,w,Z).",
        "As one may see, SoftTFIDF relies on the same principle than Monge-Elkan: for each word Xi in the first entity, find a word yj in the second one that maximizes sim'(xi, yj).",
        "Therefore, these measures have both the drawback not to be symmetric.",
        "Furthermore, there is another theoretical pitfall with SoftTFIDF: in Monge-Elkan, the final score is simply normalized in [0,1] using the average among words of the first entity.",
        "According to the principle of the Cosine angle of TF-IDF-weighted vectors, SoftTFIDF uses both vectors norms.",
        "However the way words are \"approximately matched\" does not forbid the matching of a given word in the second entity twice: in this case, normalization is wrong because this word is counted only once in the norm of the second vector.",
        "Consequently there is a potential overflow: actually it is not hard to find simple examples where the final score is greater than 1, even if this case is unlikely with real NE and a high threshold 6."
      ]
    },
    {
      "heading": "3. Generalizing Soft-TFIDF",
      "text": [
        "We propose to formalize similarity measures in the generic model below.",
        "This model is intended to define, compare and possibly mix different kinds of measures.",
        "The underlying idea is simply that most measures may be viewed as a process following different steps: representation as a sequence of features (e.g. tokenization), alignment and a way to compute the final score.",
        "We propose to define a similarity measure sim through these three steps, each of them is modeled as a function:",
        "Representation.",
        "Given a set F of features, let features(e) = (a\\,..., an) be a function that assigns an (ordered) sequence of features to any entity e (a,i G F for any i).",
        "Features may be of any kind (e.g. characters, words, n-grams, or even contextual elements of the entity); Alignment.",
        "Given a function simF : F i – > R which defines similarity between any pair of features, let align((ai,..., an), (a[,..., a'n,)) = G be a function which assigns a graph G to any pair of features sequences.",
        "G = (V, E) is a bipartite weighted graph where:",
        "• The set of vertices is V = A U A', where A and A' are the partitions defined as A = {vi,..., vn} and A' = {v[,..., v'n,}.",
        "Each Vi (resp.",
        "v'A represents (the position of) the corresponding feature a» (resp.",
        "a'A ;",
        "• The set of weighted edges is E =",
        "v'., G A'.",
        "Weights Sj generally depend on simF(aij,a'i,).",
        "Scoring.",
        "Finally sim = score(G), where score assigns a real value (possibly normalized in [0,1]) to the alignment G.",
        "The representation step is not particularly original, since different kinds of representation have already been used both with sequential methods and \"bag-of-features\" methods.",
        "However our model also entails an alignment step, which does not exist with bag-of-features methods.",
        "Actually, the alignment is implicit with such methods, and we will show that making it visible is essential in the case of NE matching.",
        "In the remaining of this paper we will only consider normalized metrics (scores belong to [0,1]).",
        "Measures presented in section 2 may be defined within the model presented above.",
        "This mod-elization is only intended to provide a theoretical viewpoint on the measures: for all practical purposes, standard implementations are clearly more efficient.",
        "Below we do not detail the representation step, because there is no difficulty with it, and also because it is interesting to consider that any measure may be used with different kinds of features, as we will show in the next section.",
        "Let S = (a\\,..., an) = features(e) and S' = (a[,..., a'n,) = features(e') for any pair of entities (e, e').",
        "The function aligriiev(S, S') is defined in the following way: let Qiev be the set of all graphs G = (V,E) such that any pair of edges ik A i'j < i'k) V (ij > ik A % > i'k).",
        "This constraint ensures that the sequential order of features is respected, and that no feature may be matched twice.",
        "In the simplest form of Leven-shtein, simF(a, b) = 1 if a = b and 0 otherwise: for any (vi^v^Sj) G E, Sj = simF'(a^.,^,).",
        "Let where M = max(n, n') and ng is the number of vertices that are not connected (i.e. the number of inserted or deleted words).",
        "costg = 1 in the simple Levenshtein form, but may be a parameter in the Needleman-Wunch variant (gap cost).",
        "In brief, the principle in this definition is to count the positions where no edit operation is needed: thus maximizing sim(G) is equivalent to minimizing the cost of an alignment:",
        "Finally, the function scoreiev is simply defined as scoreiev(G) = sim(G)/max(n, n').",
        "It is not hard to see that this definition is equivalent to the usual one (see section 2): basically, the graph represents the concept called trace in (Wagner and Fischer, 1974), except that the cost function is \"reversed\" to become a similarity function.",
        "i Suppose costg = 1:",
        "n - n scorelev{G) = 4/7.",
        "For all simple measures using only sets of features, the function aligribag(S, S') is defined in the following way: let Q be the set of all graphs",
        "''Constraints are a bit more complex for Damerau.",
        "In the Needleman-Wunch variant, simF should depend on the cost function, e.g.: simF (a, b) = 1 – cost (a, b).",
        "aij = a'ir (equivalently simF(aij,a'i,) = 1).",
        "Now",
        "satisfies ij / ik A i'j / i'k (at most one match for each feature), and / a»fc (a feature occurring several times is matched only once).",
        "Let sim(G) = Sj for any G = (V,E).",
        "alignbag(S, S') = G, where G is any graph such that sim(G) = max({sim(G\") | G G once(Q)}).",
        "Since all weights are equal to 1, one may show that sim(G) = \\S n S'\\ for any G G once(Q).",
        "Thus the score function is simply used for normalization, depending on the given measure: for example, scoreoverlap(G) = – – ---.",
        "The case of Cosine measure with TFIDF weighted vectors is a bit different.",
        "Here we define the SoftTFIDF version: let alignsoft(S, S') be the graph G = (V, E) defined as (v^, v\\, ,sj) e E if and only if ^ = seZeci(CLOSEST(0, aip S')), where CLOSEST is the function defined in section 2 and select(E) is a function returning the first element in E if \\E\\ > 0, and is undefined otherwise.",
        "For any such edge, the weight Sj is",
        "Sj = stm (aivait)",
        "Although it is not explicitly used in this definition, term frequency is taken into account through the number of edges: suppose a given term t appears m times in S and m!",
        "times in S', all m vertices corresponding to t in A (the partition representing S) will be connected to all m!",
        "vertices corresponding to t in A'.",
        "Thus there will be m x m!",
        "edges, which is exactly the unnormalized product",
        "all identical features (and only they) are connected.",
        "of term frequencies tf (t, S) ■ ti(t, S') - n n. Thus summing m x m!",
        "times idi(t)/n ■ idf(i)/n' in sim(G) is equal to tfidf (t, S) ■ tfidf (t, S') (normalization is computed in the same way).",
        "We have shown in part 2.2 that there are some pitfalls in Soft-TFIDF, especially in the way the alignment is computed: no symmetry, possible score overflow.",
        "But experiments show that taking words IDF into account increases performance, and that Soft-TFIDF, i.e. the possible matching of words that are not strictly identical, increases performance (see section 4).",
        "That is why improving this kind of measure is interesting.",
        "Following the model we proposed above, we propose to mix the cosine-like similarity used in Soft-TFIDF with a Levenshtein-like alignment.",
        "The following measure, called Meta-Levenshtein (ML for short), takes IDFs into account but is not a bag-of-features metrics.",
        "Let us define aligriML in the following way: let Qml be defined exactly as the set of graphs Q\\ev(see part 3.2.1), except that weights are defined as in the case of Soft-TFIDF: for any G = (V, E) e aligriML{S,S') = G, where G is such that sim(G) = max({sim(G/) | G' G Qml})- Finally, scoreML(G) = sim(G)/(\\\\S\\\\■ \\\\S'\\\\).",
        "Compared to Soft-TFIDF, ML solves the problem of symmetry (ML(S, S') = ML(S', S)), and also the potential overflow, because no feature may be matched twice (see fig. 2).",
        "Of course, the alignment is less flexible in ML, since it must satisfy the sequential order of features.",
        "Practically, this measure may be efficiently implemented in the same way as Levenshtein similarity, including optionally the Damerau extension for transpositions.",
        "We have also tested a simple variant with possible extended transpositions, i.e. cases like ABC compared to CA, where both C and A are matched.",
        "One of the points we want to emphasize through the generic framework presented above is the mod-",
        "Soft-TFIDF ularity of similarity measures.",
        "Our viewpoint is that traditional measures may be seen not only in their original context, but also as modular parameterized functions.",
        "The first application of such a definition is already in use in the form of measures like Monge-Elkan or Soft-TFIDF, which rely on some sub-measure to compare words inside NEs.",
        "But we will show that modularity is also useful at a lower level: measures concerning words may rely on similarity between (for example) n-grams, and even at this restricted level numerous possible kinds of similarity may be used.",
        "Moreover, from the viewpoint of applications it is not very costly to compute similarities between n-grams and even between words.",
        "The number of n-grams is clearly bounded, and the number of words is not so high because there are only about 2 words by entity in average, and overall some words appear very often in entities."
      ]
    },
    {
      "heading": "4. Experiments 4.1 Data",
      "text": [
        "Two corpora were used.",
        "Both contain mainly news and press articles, collected from various international sources.",
        "The first one, called \"Iran Nuclear Threat\" (INT in short), is in English and was extracted from the NTI (Nuclear Threat Initiative) web site.",
        "It is 236,000 words long.",
        "Our second corpus, called \"French Speaking Medias\" (FSM in short), is 856,000 words long.",
        "It was extracted from a regular crawling of a set of French-speaking international newspapers web sites during a short time-frame (in July 2007).",
        "GATEwas used as the named entities recognizer for INT, whereas Arisem performed the tagging of NEs for FSM.",
        "Recognition errors appear in both corpora, but significantly less in FSM.",
        "We restricted the sets of NEs to those recognized as locations, organizations and persons, and decided to work only on entities appearing at least twice.",
        "Finally for INT (resp.",
        "FSM) we obtain 1,588 distinct NE (resp.",
        "3,278) accounting altogether for 33,147 (resp.",
        "23,725) occurrences.",
        "Of course, it would be too costly to manually label as match (positive) or non-match (negative) the whole set containing n x (n – l)/2 pairs, for the observed values of n. The approach consisting in labeling only a randomly chosen subset of pairs is ineffective, because of the disproportion between the number of negative and positive pairs (less than 0.1%).",
        "Therefore we tried to find all positive pairs, assuming the remaining lot are negative.",
        "Practically, the labeling step was based only on the best pairs as identified by a large set of measures.",
        "The guidelines we used for labeling are the following: any incomplete, over-tagged or simply wrongly recognized NE is discarded.",
        "Then remaining pairs are classified as positive (corefer-ent), negative (non-coreferent), or \"don't know\"",
        "According to our initial hypotheses, all non-tagged pairs are considered as negative in the experiments below.",
        "\"Don't know\" pairs are ignored.",
        "As a further note, about 20% of the pairs are not orthographically similar (e.g. acronyms and their expansion): these pairs are out of reach of our techniques, and would require additional knowledge.",
        "To evaluate the contribution of IDF in scoring the coreference degree between NE, let us ob-",
        "http://www.nti.org 'http://gate.ac.uk http://www.ari sem.com",
        "Corpus",
        "Discarded",
        "Pos.",
        "Neg.",
        "Don't know",
        "INT",
        "416/1,588",
        "764",
        "2,821",
        "302",
        "FSM",
        "745 / 3,278",
        "741",
        "32,348",
        "419",
        "serve the differences among best scored pairs for measures Bag-of-words Cosine and Cosine over TFIDF weighted vectors.",
        "For example, the former will assign 0.5 to pair \"Prime Minister Tony Blair \"/\"Blair\" (from corpus INT), whereas the latter gives 0.61.",
        "As expected, IDF weights lighten the effect of non-informative words and strengthen important words.",
        "In both corpora, The Fl measure for TFIDF Cosine is about 10 points (in average) better than for Bag-of-words Cosine (see fig. 3).",
        "4.2.2 Soft-TFIDF problems: normalization, threshold and sub-measure",
        "As we have explained in section 2.2, the Soft-TFIDF measure (Cohen et al., 2003) may suffer from normalization problems.",
        "This is probably the reason why the authors seem to use it parsimoniously, i.e. only in the case words are very close (which is verified using a high threshold 9).",
        "Indeed, problems occur when the sub-measure and/or the threshold are not carefully chosen, causing performances drop: using Jaro measure with a very low threshold (0.2 here), performances are even worst than Bag-of-words cosine (see fig. 3).",
        "This is due to the double matching problem: for example, pair \"Tehran Times (Tehran) \"/\"Inter Press Service \" (from INT) is scored more than 1.0 because \"Tehran\" matches \"Inter\" twice: even with a low score as a coefficient, \"Inter\" has a high IDF compared to \"Press \" and \"Service \", so counting it twice makes normalization wrong.",
        "However, this problem may be solved by choosing a more adequate sub-measure: experiments show that using the CosTFIDF measure with bi-grams or trigrams outperforms standard CosTFIDF.",
        "Of course, there are some positive pairs that are found \"later\" by Soft-TFIDF, since it may only increase score.",
        "But the \"soft\" comparison brings back to the top ranked pairs a lot of positive ones.",
        "In both corpora, the best sub-measure found is CosTFIDF with trigrams.",
        "\"Mohamed ElBa-radei \"/\"Director Mohammad ElBaradei\" (INT) or \"Chine\"/\"China\" (FSM) are typical positive pairs found by this measure but not by standard CosTFIDF.",
        "Here no threshold is needed anymore because the sub-measure has been chosen with care, depending on the data, in order to avoid the normalization problem.",
        "This is clearly a drawback for Soft-TFIDF: it may perform well, but only with hand-tuning sub-measure and/or threshold.",
        "In the FSM corpus, replacing Soft-TFIDF with (simple) Meta-Levenshtein at the word level does not decrease performance, even though the alignment is more constrained in the latter case.",
        "Using the same sub-measure to compare words (trigrams CosTFIDF), it does neither increase performance.",
        "A few positive pairs are missed in the INT corpus, due to the more flexible word order in English: \"U.S. State Department\"/\"US Department of State\" is such an example (12 among 764 are concerned).",
        "This problem is easily solved with the ML variant with extended transposition (see part 3.3): in both corpora, there are no positive pairs requiring more than a gap of one word in the alignment.",
        "Thus this measure is not only performant but also robust, since it does not need any hand-tuning.",
        "As a second step, we want to improve results by selecting a more fine-grained sub-measure.",
        "We have tried several ideas, such as using different kinds of n-grams similarity inside the words similarity measure.",
        "Firstly, trigrams performed better than bigrams or simple characters.",
        "Secondly, the best trigrams similarity method found is actually very simple: it consists in using CosTFIDF computed on the trigrams contexts, i.e. the set of closest trigrams of all occurrences of the given trigram.",
        "Unsurprisingly, good scores are generally obtained for pairs of trigrams that have common characters.",
        "But it seems that this approach also enhances robustness, because it finds similarities between \"close characters\": in the French corpus, one observes quite good scores between trigrams containing an accentuated version and the non accentuated version of the same character.",
        "Furthermore, some character encoding errors are somehow corrected this way.",
        "This is possibly the reason why the improvement of results is better in FSM than in INT (see table 1).",
        "Finally, using also ML to compute similarity between words yields the best results.",
        "This means that compared to the simple CosTFIDF sub-measure, one does not compare bags of trigrams but ordered sequences of trigrams.",
        "Bag of words Cosine Cosine TFIDF (words) Soft-TFIDF (Jaro) Soft-TFIDF (TFIDF 3g)",
        "ML (ML/contexts 3g)",
        "n best scored pairs (considered as positive) Example: for Cosine TFIDF with words, if the threshold is",
        "set in such a way that (only) the 1000 top ranked pairs are classified as positive, then the Fl-measure is around 60%.",
        "P/R: Corresponding Precision/Recall.",
        "Results are synthesized in table 1, which is based on the maximum Fl-measure for each measure.",
        "One observes that Fl-measure is 3 to 6 points better for Soft-TFIDF than for standard TF-IDF, and that our measure still increases Fl-measure by 2 (INT) to 4 points (FSM).",
        "Results show that its contribution consists mainly in improving the recall, which means that our measure is able to catch more positive pairs than Soft-TFIDF: for example, the pair \"Fatah Al Islam \"/ \"Fateh el-Islam \" (FSM) is scored 0.54 by SoftTFIDF and 0.70 by ML.",
        "Our measure remains the best for all values of n in fig. 3, and results are similar for F0.5-measure and F2-measure: thus, irrespective of specific application needs which may favor precision or recall, ML seems preferable."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In conclusion, we have proposed a generic model to show that similarity measures may be combined in numerous ways.",
        "We have tested such a combination, based on Soft-TFIDF, which performs bet-",
        "the \"right\" use of the trigrams sub-measure which is responsible for the improvement, since the only possible comparison at this level is Soft-TFIDF.",
        "ter than all existing similarity metrics on two corpora.",
        "Our measure is robust, since it does not rely on any kind of prior knowledge.",
        "Thus it may be easily used, in particular in applications where NE matching is useful but is not the essential task."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work has been funded by the National Project Cap Digital - Infom@gic.",
        "We thank Lois Rigouste (Pertimm) and Nicolas Dessaigne and Aurelie Mi-geotte (Arisem) for providing us with the annotated French corpus.",
        "Measure",
        "INT Fl P R",
        "FSM Fl P R",
        "Cosine CosTFIDF Soft TFIDF/3g ML/ML-context",
        "51.6 63.2 43.6 62.6 71.7 55.6 68.6 74.2 63.9 70.6 72.6 68.7",
        "59.5 76.2 48.7 69.9 84.2 59.8 73.1 79.8 67.6 77.0 82.5 72.2"
      ]
    }
  ]
}
