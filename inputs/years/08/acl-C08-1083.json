{
  "info": {
    "authors": [
      "Naoaki Okazaki",
      "Sophia Ananiadou",
      "Jun'ichi Tsujii"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C08-1083",
    "title": "A Discriminative Alignment Model for Abbreviation Recognition",
    "url": "https://aclweb.org/anthology/C08-1083",
    "year": 2008
  },
  "references": [
    "acl-D07-1064",
    "acl-J96-1002",
    "acl-P02-1021",
    "acl-P02-1038",
    "acl-P05-1057",
    "acl-P06-1009",
    "acl-W01-0516",
    "acl-W02-0312"
  ],
  "sections": [
    {
      "text": [
        "This paper presents a discriminative alignment model for extracting abbreviations and their full forms appearing in actual text.",
        "The task of abbreviation recognition is formalized as a sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form).",
        "We design a large amount of finegrained features that directly express the events where letters produce or do not produce abbreviations.",
        "We obtain the optimal combination of features on an aligned abbreviation corpus by using the maximum entropy framework.",
        "The experimental results show the usefulness of the alignment model and corpus for improving abbreviation recognition."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Abbreviations present two major challenges in natural language processing: term variation and ambiguity.",
        "Abbreviations substitute for expanded terms (e.g., dynamic programming) through the use of shortened term-forms (e.g., DP).",
        "At the same time, the abbreviation DP appearing alone in text is ambiguous, in that it may refer to different concepts, e.g., data processing, dirichlet process, differential probability.",
        "Associating abbreviations and their full forms is useful for various applications including named entity recognition, information retrieval, and question answering.",
        "The task of abbreviation recognition, in which abbreviations and their expanded forms appearing in actual text are extracted, addresses the term variation problem caused by the increase in the number of abbreviations (Chang and Schiitze, 2006).",
        "Furthermore, abbreviation recognition is also crucial for disambiguating abbreviations (Pakhomov, 2002; Gaudan et al., 2005; Yu et al., 2006), providing sense inventories (lists of abbreviation definitions), training corpora (context information of full forms), and local definitions of abbreviations.",
        "Hence, abbreviation recognition plays a key role in abbreviation management.",
        "Numerous researchers have proposed a variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (Park and Byrd, 2001; Wren and Garner, 2002; Liu and Friedman, 2003; Okazaki and Ananiadou, 2006; Zhou et al., 2006; Jain et al., 2007).",
        "Schwartz and Hearst (2003) implemented a simple algorithm that finds the shortest expression containing all alphanumerical letters of an abbreviation.",
        "Adar (2004) presented four scoring rules to choose the most likely expanded form in multiple candidates.",
        "Ao and Takagi (2005) designed more detailed conditions for accepting or discarding candidates of abbreviation definitions.",
        "However, these studies have limitations in discovering an optimal combination of heuristic rules from manual observations of a corpus.",
        "For example, when expressions transcription factor 7_ and thyroid transcription factor 1 are full-form candidates for the abbreviation TTF-1, an algorithm should choose the latter expression over the shorter expression (former).",
        "Previous studies hardly handle abbreviation definitions where full forms (e.g., water activity) shuffle their abbreviation letters (e.g., AW).",
        "It is also difficult to reject 'negative' definitions in a text; for example, an algorithm should not extract an abbreviation definition from the text, \"the replicon encodes a large replication protein (RepA),\" since RepA provides a description of the protein rather than an abbreviation.",
        "In order to acquire the optimal rules from the corpora, several researchers applied machine learning methods.",
        "Chang and Schiitze (2006) applied logistic regression to combine nine features.",
        "Nadeau and Turney (2005) also designed seventeen features to classify candidates of abbreviation definitions into positive or negative instances by using the Support Vector Machine (SVM).",
        "Notwithstanding, contrary to our expectations, the machine-learning approach could not report better results than those with hand-crafted rules.",
        "We identify the major problem in the previous machine-learning approach: these studies did not model associations between abbreviation letters and their origins, but focused only on indirect features such as the number of abbreviation letters that appear at the head of a full form.",
        "This was probably because the training corpus did not include annotations on the exact origins of abbreviation letters but only pairs of abbreviations and full forms.",
        "It was thus difficult to design effective features for abbreviation recognition and to reuse the knowledge obtained from the training processes.",
        "In this paper, we formalize the task of abbreviation recognition as a sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form).",
        "We design a large amount of features that directly express the events where letters produce or do not produce abbreviations.",
        "Preparing an aligned abbreviation corpus, we obtain the optimal combination of the features by using the maximum entropy framework (Berger et al., 1996).",
        "We report the remarkable improvements and conclude this paper."
      ]
    },
    {
      "heading": "2. Proposed method",
      "text": [
        "We express a sentence a; as a sequence of letters (xi,xjf), and an abbreviation candidate y in the sentence as a sequence of letters (yi,Vm)- We define a letter mapping a = (i,j) to indicate that the abbreviation letter tjj is produced by the letter in the full form Xi.",
        "A null mapping a = (i, 0) indicates that the letter in the sentence x% is unused to form the abbreviation.",
        "Similarly, a null mapping a = (0,j) indicates that the abbreviation letter yj does not originate from any letter in x.",
        "We define a^) and in order to represent the first and second elements of the letter mapping a.",
        "In other words, a^) and are equal to i and j respectively, when a = Finally, an abbreviation alignment a is defined as a sequence of letter mappings, a = (ai,...,ax), where T represents the number of mappings in the alignment.",
        "Let us consider the following example sentence:",
        "We investigate the effect of thyroid transcription factor 1 (TTF-1).",
        "This sentence contains an abbreviation candidate TTF-1 in parentheses.",
        "Figure 1 illustrates the correct alignment a (bottom line) and its two-dimensional representation for the example sentence; the abbreviation letters't,' 't,' 'f,' and '1' originate from a;30, £38, X52, nowhere (null mapping), and £59 respectively.",
        "We directly model the conditional probability of the alignment a, given x and y, using the maximum entropy framework (Berger et al., 1996),",
        "D/ 1 x exp{A- F(a,x,y)} P(a\\x,y) = – – -.",
        "In Formula \\ ,F = {/1, ...,/#} is a global feature vector whose elements present K feature functions, A = {Ai,Xk} denotes a weight vector for the feature functions, and C(x,y) yields a set of possible alignments for the given x and y.",
        "We obtain the following decision rule to choose the most probable alignment a for given x and y,",
        "Note that a set of possible alignments C(x,y) always includes a negative alignment whose elements are filled with null-mappings (refer to Section 2.3 for further detail).",
        "This allows the formula to withdraw the abbreviation candidate y when any expression in x is unlikely to be a definition.",
        "o Other positions □ Null outside ■ Abbreviation O Null inside # Associate inside",
        "x: Reinvestigate the effect of thyroid transcription factor l^TTF-1) ...",
        "The main advantage of the discriminative alignment model is its ability to incorporate a wide range of non-independent features.",
        "Inspired by feature engineering for Conditional Random Fields (CRFs) (Lafferty et al., 2001), we design two kinds of features: unigram (state) features defined on each letter mapping, and bigram (transition) features defined on each pair of adjacent letter mappings.",
        "Given a triplet, a, x, and y, a global feature function fk(a,x,y) e F sums up the boolean values (0 or 1) of the corresponding local feature #fc(a, x, y, t) at t e {1,T},",
        "In other words, fk(a,x,y) counts the number of times the local feature is fired in the alignment a.",
        "A unigram feature corresponds to the observation at Xi and ijj associated by a mapping at = A unigram feature encodes the condition where the letter in the full form Xi is chosen or unchosen for producing the abbreviation letter yj.",
        "For example, we may infer from the letter mapping at as = (30,1) in Figure 1, that xso is mapped to yi because: X30 is at the head of the word, y\\ is a capital letter, and both £30 and y\\ are at the head of the word and abbreviation.",
        "Bigram features, combining two observations at as and at (1 < s < t < T), are useful in capturing the common characteristics shared by an abbreviation definition.",
        "For instance, we may presume in",
        "Figure 1 that the head letters in the full form might be selectively used for producing the abbreviation, based on the observations at as = (30,1) and ag = (38, 2).",
        "In order to focus on the conditions for consecutive non-null mappings, we choose the previous position s for the given t.",
        "Formula 4 prefers the non-null mapping that is the most adjacent to t over the previous mapping (t – 1).",
        "In Figure 1, transitions ag – an and an – ai3 exist for this reason.",
        "In this study, we express unigram and bigram features with atomic functions (Table 1) that encode observation events of xat{x), yat{yy at, xas(x) – xat(x), and yas(y) – yat(y)- Atomic functions x_ctype, y.ctype, x_position, and y.position present common heuristics used by previous studies.",
        "The function x_word examines the existence of stop words (e.g., the, of, in) to prevent them from producing abbreviation letters.",
        "We also include x_pos (part-of-speech of the word) since a number of full forms are noun phrases.",
        "Functions x_diff, x_diff_wd, and y_diff are designed specifically for bigram features, receiving two positions s and t in their arguments.",
        "The function x_diff mainly deals with abbreviation definitions that include consecutive letters of their full forms, e.g., amplifier (AMP).",
        "The function",
        "<vl, oXoi ojq",
        "5",
        "Table 2 displays the complete list of generation rules for unigram and bigram features, unigram(t) and bigram(s, t).",
        "For each generation rule in unigram(t) and bigram(s, t), we define boolean functions that test the possible values yielded by the corresponding atomic function(s).",
        "Formula 1 requires a sum over the possible alignments, which amounts to 2LM for a sentence (L letters) with an abbreviation (M letters).",
        "It is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (McCallum et al., 2005; Blunsom and Cohn, 2006; Shimbo and Hara, 2007) or approximated by the n-best list of highly probable alignments (Och and Ney, 2002; Liu et al., 2005).",
        "Fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions:",
        "x_diff_wd measures the distance of two words.",
        "The function y_diff models the ordering of abbreviation letters; this function always returns non-negative values if the abbreviation contains letters in the same order as in its full form.",
        "We express unigram and bigram features with the atomic functions.",
        "For example, Formula 5 defines a unigram feature for the event where the capital letter in a full-form word xat{x) produces the identical abbreviation letter yat{y) ■"
      ]
    },
    {
      "heading": "0. (otherwise)",
      "text": [
        "For notation simplicity, we rewrite this boolean function as (arguments a, x, and y are omitted),",
        "In this formula, l{v=v} is an indicator function that equals 1 when v = v and 0 otherwise.",
        "The term v presents a generation rule for a feature, i.e., a combination rule of atomic functions.",
        "Function",
        "Return value",
        "x_ctypeÄ(a, x,i) x_positionÄ(a, x,t) x_char^(a, x, i) x_wordä(a, x, i) x_posÄ(a, x, i)",
        "Xat(x)+S is (u (uppercase), L (lowercase), D (digit), W (whitespace), S (symbol) } letter Xat(x)+S is at me (H (nead), T (tail), S (syllable head), I (inner), W (whitespace) } of the word The lower-cased letter of xat, ,+s",
        "The lower-cased word (offset position S) containing the letter xat(x)",
        "The part-of-speech code of the word (offset position S) containing the letter xat(x)",
        "y-ctype(a,y,t) y_position(a, y, i) y_char(a, y, i)",
        "Vat(v) is (N (NIL) U (uppercase), L (lowercase), D (digit), S (symbol) } letter yat{y) is at the {N (NIL) H (head), T (tail), I (inner)} of the word The lower-cased letter of ya+, *",
        "a_state(a, y, i)",
        "{SKIP (at(y) = 0),MATCH (1 < at(y) < |y|),ABBR (at(y) = \\y\\ + 1)}",
        "x_diff (a, x, s, i) x_diff_wd(a, x,s,i)",
        "(cit(x) – as(x)) if letters xat(x) and xas(x) are in the same word, NONE otherwise The number of words between xa., .",
        "and xa , .",
        "yA\\tt(a,y,s,i)",
        "(at(y) - as(y))",
        "Combination",
        "Rules",
        "unigram(t)",
        "xy_unigram(t) <%> {a_state(t)}",
        "xy_unigram(t) x_unigram(t)",
        "y_unigram(t) x_state^ (t)",
        "x_unigram(t) © y_unigram(t) © (x_unigram(i) <g> y_unigram(t)) x_stateo(t) © x_state_i(t) © x_statei(t) ©(x_state_i(t) <g) x_stateo(t)) © (x_stateo(t) <g) x_statei(t)) |y_ctype(t), y_position(t), y_ctype(t)y_position(t)}",
        "{x_ctypeÄ(t), x_positionÄ(t), x_chara(t), x_worda(t), x_posÄ(t), x_ctypeÄ(t)x_positionÄ(t), x_positionÄ (i)x_posÄ (t), x_posÄ (t)x_ctypeÄ (i), x_ctypeÄ (t)x_positionÄ (t)x_posÄ (i)}",
        "bigram(s, t)",
        "xy_bigram(s, i) <%> {a_state(s)a_state(i)}",
        "xy_bigram(s, t) trans(s, t)",
        "(x_stateo(s) ® x_stateo(t) ® trans(s, t)) © (y_unigram(s) <g> y_unigram(i) <g> trans(s, t)) ©(x_stateo(s) ® y_unigram(s) <g) x_stateo(t) ® y_unigram(i) <g) trans(s, t)) {x_diff (s, t), x_diff_wd(s, t), y_diff (s, t)}",
        "investigate the effect of thyroid transcription factor 1",
        "1.",
        "A full form may appear min(m + 5,2m) words before its abbreviation in the same sentence, where m is the number of alphanumeric letters in the abbreviation (Park and Byrd, 2001).",
        "2.",
        "Every alphanumeric letter in an abbreviation must be associated with the identical (case-insensitive) letter in its full form.",
        "3.",
        "An abbreviation letter must not originate from multiple letters in its full form; a full-form letter must not produce multiple letters.",
        "4.",
        "Words in a full form may be shuffled at most d times, so that all alphanumeric letters in the corresponding abbreviation appear in the rearranged full form in the same order.",
        "We define a shuffle operation as removing a series of word(s) from a full form, and inserting the removed word(s) to another position.",
        "5.",
        "A full form does not necessarily exist in the text span defined by assumption 1.",
        "Due to the space limitation, we do not describe the algorithm for obtaining possible alignments that are compatible with these assumptions.",
        "Alternatively, Figure 2 illustrates a part of possible alignments C(x,y) for the example sentence.",
        "The alignment #2 represents the correct definition for the abbreviation TTF-1.",
        "We always include the negative alignment (e.g., #0) where no abbreviation letters are associated with any letters in x.",
        "The alignments #4-8 interpret the generation process of the abbreviation by shuffling the words in x.",
        "For example, the alignment #6 moves the word 'of to the position between 'factor' and ' 1'.",
        "Shuffled alignments cover abbreviation definitions such as receptor of estrogen (ER) and water activity (AW).",
        "We call the parameter d, distortion parameter, which controls the acceptable level of reordering (distortion) for the abbreviation letters.",
        "Parameter estimation for the abbreviation alignment model is essentially the same as for general maximum entropy models.",
        "Given a training set that consists of 7Y instances, ((aW,xW,yW), ...,(a^N\\x^,y^y)), we maximize the log-likelihood of the conditional probability distribution by using the maximum a posterior (MAP) estimation.",
        "In order to avoid overfitting, we regularize the log-likelihood with either the L\\ or L>2 norm of the weight vector A,",
        "In these formulas, o\\ and 02 are regularization parameters for the L\\ and L2 norms.",
        "Formulas 7 and 8 are maximized by the Orthant-Wise Limited-memory Quasi-Newton (OW-LQN) method (Andrew and Gao, 2007) and the Limited-memory BFGS (L-BFGS) method (Nocedal, 1980)."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "The Medstract Gold Standard Corpus (Pustejovsky et al., 2002) was widely used for evaluating abbreviation recognition methods (Schwartz and Hearst, 2003; Adar, 2004).",
        "However, we cannot use this corpus for training the abbreviation alignment model, since it lacks annotations on the origins of abbreviation letters.",
        "In addition, the size of the corpus is insufficient for a supervised machine-learning method.",
        "Therefore, we built our training corpus with 1,000 scientific abstracts that were randomly chosen from the MEDLINE database.",
        "Although the alignment model is independent of linguistic patterns for abbreviation definitions, in the corpus we found only three abbreviation definitions that were described without parentheses.",
        "Hence, we employed parenthetical expressions, full-form '(' abbreviation ')', to locate possible abbreviation definitions (Wren and Garner, 2002).",
        "In order to exclude parentheses inserting clauses into passages, we consider the inner expression of parentheses as an abbreviation candidate, only if the expression consists of two words at most, the length of the expression is between two to ten characters, the expression contains at least an alphabetic letter, and the first character is alphanumeric.",
        "We asked a human annotator to assign reference abbreviation alignments for 1,420 parenthetical expressions (instances) in the corpus.",
        "If a parenthetical expression did not introduce an abbreviation, e.g., \"... received treatment at 24 months (RRMS),\" the corresponding instance would have a negative alignment (as #0 in Figure 2).",
        "Eventually, our aligned corpus consisted of 864 (60.8%) abbreviation definitions (with positive alignments) and 556 (39.2%) other usages of parentheses (with negative alignments).",
        "Note that the log-likelihood in Formula 7 or 8 increases only if the probabilistic model predicts the reference alignments, regardless of whether they are positive or negative.",
        "We prepared five state-of-the-art systems of abbreviation recognition as baselines: Schwartz and Hearst's method (SH) (Schwartz and Hearst, 2003), SaRAD (Adar, 2004), ALICE (Ao and Takagi, 2005), Chang and Schiitze's method (CS) (Chang and Schütze, 2006), and Nadeau and Turney's method (NT) (Nadeau andTurney, 2005).",
        "We utilized the implementations available on the Web for SH, CS, and ALICE, and we reproduced SaRAD and NT, based on their papers.",
        "Our implementation of NT consists of a classifier that discriminates between positive (true) and negative (false) full forms, using all of the feature functions presented in the original paper.",
        "Although the original paper presented heuristics for generating full-form candidates, we replaced the candidate generator with the function C(x,y), so that the classifier and our alignment model can receive the same set of full-form candidates.",
        "The classifier of the NT system was modeled by the LIB-SVM implementation with Radial Basis Func-",
        "'Biomedical Abbreviation Server: http://abbreviation.stanford.edu/",
        "'Abbreviation Lifter using Corpus-based Extraction: http://uvdb3.hgc.jp/ALICE/ALICE_index.html",
        "tion (RBF) kernel .",
        "If multiple full-form candidates for an abbreviation are classified as positives, we choose the candidate that yields the highest probability estimate.",
        "We trained and evaluated the proposed method on our corpus by performing 10-fold cross validation.",
        "Our corpus includes 13 out of 864 (1.5%) abbreviation definitions in which the abbreviation letters are shuffled.",
        "Thus, we have examined two different distortion parameters, d = 0,1 in this experiment.",
        "The average numbers of candidates produced by the candidate generator C(x,y) per instance were 8.46 (d = 0) and 69.1 (d = 1), respectively.",
        "The alignment model was trained in a reasonable execution time, ca.",
        "5 minutes (d = 0) and 1.5 hours (d = 1).",
        "Table 3 reports the precision (P), recall (R), and Fl score (Fl) on the basis of the number of correct abbreviation definitions recognized by each system.",
        "The proposed method achieved the best Fl score (0.971) of all systems.",
        "The inclusion of distorted abbreviations (d = 1) gained the highest recall (0.981 with L\\ regularization).",
        "Baseline systems with refined heuristics (SaRAD and ALICE) could not outperform the simplest system (SH).",
        "The previous approaches with machine learning (CS and NT) were roughly comparable to rule-based methods.",
        "We also evaluated the alignment model on the Medstract Gold Standard development corpus to examine the adaptability of the alignment model trained with our corpus (Table 4).",
        "Since the original version of the Medstract corpus includes annotation errors, we used the version revised by Ao and Takagi (2005).",
        "For this reason, the performance of ALICE might be overestimated in this evaluation; ALICE delivered much better results than Schwartz & Hearst's method on this corpus.",
        "System",
        "P",
        "R",
        "Fl",
        "Schwartz & Hearst (SH)",
        ".978",
        ".940",
        ".959",
        "SaRAD",
        ".891",
        ".919",
        ".905",
        "ALICE",
        ".961",
        ".920",
        ".940",
        "Chang & Schütze (CS)",
        ".942",
        ".900",
        ".921",
        "Nadeau & Turney (NT)",
        ".954",
        ".871",
        ".910",
        "Proposed (d = 0; L\\)",
        ".973",
        ".969",
        ".971",
        "Proposed (d = 0; L2)",
        ".964",
        ".968",
        ".966",
        "Proposed (d = 1; L\\)",
        ".960",
        ".981",
        ".971",
        "Proposed (d = 1; L2)",
        ".957",
        ".976",
        ".967",
        "The abbreviation alignment model trained with our corpus (d = 1; L{) outperformed the baseline systems for all evaluation metrics.",
        "It is notable that the model could recognize abbreviation definitions with shuffled letters, e.g., transfer of single embryo (SET) and inorganic phosphate (PI), without any manual tuning for this corpus.",
        "In some false cases, the alignment model yielded incorrect probability estimates.",
        "For example, the probabilities of the alignments prepubertal bipolarity, bipolarity, and non-definition (negative) for the abbreviation BP were computed as 3.4%, 89.6%, and 6.7%, respectively; but the first expression prepubertal bipolarity is the correct definition for the abbreviation.",
        "Table 5 shows Fl scores of the proposed method trained with different sets of atomic functions.",
        "The baseline setting (1), which built features only with x_position and x.ctype functions, gained a 0.905 Fl score; further, adding more atomic functions generally improves the score.",
        "However, the x_char and y_char functions decreased the performance since the alignment model was prone to overfit to the training data, relying on the existence of specific letters in the training instances.",
        "Interestingly, the model was flexible enough to achieve a high performance with four atomic functions (5).",
        "Table 6 demonstrates the ability for our approach to obtain effective features; the table shows the top 10 (out of 850,009) features with high weights assigned by the MAP estimation with L\\ regularization.",
        "A unigram and bigram features have prefixes \"U:\" and \"B:\" respectively; a feature expresses conditions at s (bigram features only), conditions at t, and mapping status (match or skip) separated by V symbols.",
        "For example, the #1 feature associates a letter at the head of a full-form word with the uppercase letter at the head of its abbreviation.",
        "The #4 feature is difficult to obtain from manual observations, i.e., the bigram feature suggests the production of two abbreviation letters from two lowercase letters in the same word."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "We have presented a novel approach for recognizing abbreviation definitions.",
        "The task of abbreviation recognition was successfully formalized as a sequential alignment problem.",
        "We developed an aligned abbreviation corpus, and obtained finegrained features that express the events wherein a full forum produces an abbreviation letter.",
        "The experimental results showed remarkable improvements and usefulness of the alignment approach for abbreviation recognition.",
        "We expect the use-fullness of the discriminative model for building an comprehensible abbreviation dictionary.",
        "Future work would be to model cases in which a full form yields non-identical letters (e.g., 'one' – > T and 'deficient' – > '-'), and to demonstrate this approach with more generic linguistic patterns (e.g., aka, abbreviated as, etc.).",
        "We also plan to explore a method for training a model with an unaligned abbreviation corpus, estimating the alignments simultaneously from the corpus."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was partially supported by Grant-in-Aid for Scientific Research on Priority Areas (MEXT, lapan), and Solution-Oriented Research for Science and Technology (1ST, lapan).",
        "System",
        "P",
        "R",
        "Fl",
        "Schwartz & Hearst (SH)",
        ".942",
        ".891",
        ".916",
        "SaRAD",
        ".909",
        ".859",
        ".884",
        "ALICE",
        ".960",
        ".945",
        ".953",
        "Chang & Schütze (CS)",
        ".858",
        ".852",
        ".855",
        "Nadeau & Turney (NT)",
        ".889",
        ".875",
        ".882",
        "Proposed (d = 1; L\\)",
        ".976",
        ".945",
        ".960",
        "#",
        "Feature",
        "A",
        "1",
        "U: x_positiono=H;y_ctypeo=U;y_positiono=H/M",
        "1.7370",
        "2",
        "B: y_positiono=I/y-positiono=I/x_diff=l/M-M",
        "1.3470",
        "3",
        "U: x_ctype_i=L;x_ctypeo=L/S",
        "0.96342",
        "4",
        "B: x_ctypeo=L/x_ctypeo=L/x_diff_wd=0/M-M",
        "0.94009",
        "5",
        "U: x_positiono=I;x_chari='t'/S",
        "0.91645",
        "6",
        "U: x_positiono=H;x_poso=NN;y_ctypeo=U/M",
        "0.86786",
        "7",
        "U: x_ctype_i=S;xctypeo=L;M",
        "0.86474",
        "8",
        "B: x_charo='o7x_ctypeo=L/y_diff=0/M-S",
        "0.71262",
        "9",
        "U: x_char_i='o';x_ctypeo=L/M",
        "0.69764",
        "10",
        "B: x_positiono=H/x_ctype0=U/y_diff=l/M-M",
        "0.66418",
        "#",
        "Atomic function(s)",
        "Fl",
        "(1)",
        "x_position + x_ctype",
        ".905",
        "(2)",
        "(1) + x_char + y_char",
        ".885",
        "(3)",
        "(1) + x_word + x_pos",
        ".941",
        "(4)",
        "(1) + x_diff + x_diff_wd + y_diff",
        ".959",
        "(5)",
        "(1) + y_position + y_ctype",
        ".964",
        "(6)",
        "All atomic functions",
        ".966"
      ]
    }
  ]
}
