{
  "info": {
    "authors": [
      "Ryohei Sasano",
      "Daisuke Kawahara",
      "Sadao Kurohashi"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C08-1097",
    "title": "A Fully-Lexicalized Probabilistic Model for Japanese Zero Anaphora Resolution",
    "url": "https://aclweb.org/anthology/C08-1097",
    "year": 2008
  },
  "references": [
    "acl-C02-1078",
    "acl-C02-1122",
    "acl-J01-4004",
    "acl-J94-4002",
    "acl-N06-1023",
    "acl-N07-1010",
    "acl-P05-1020",
    "acl-P06-1079",
    "acl-W03-1024"
  ],
  "sections": [
    {
      "text": [
        "This paper presents a probabilistic model for Japanese zero anaphora resolution.",
        "First, this model recognizes discourse entities and links all mentions to them.",
        "Zero pronouns are then detected by case structure analysis based on automatically constructed case frames.",
        "Their appropriate antecedents are selected from the entities with high salience scores, based on the case frames and several preferences on the relation between a zero pronoun and an antecedent.",
        "Case structure and zero anaphora relation are simultaneously determined based on probabilistic evaluation metrics."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Anaphora resolution is one of the most important techniques in discourse analysis.",
        "In English, definite noun phrases such as the company and overt pronouns such as he are anaphors that refer to preceding entities (antecedents).",
        "On the other hand, in Japanese, anaphors are often omitted and these omissions are called zero pronouns.",
        "We focus on zero anaphora resolution of Japanese web corpus, in which anaphors are often omitted and zero anaphora resolution plays an important role in discourse analysis.",
        "Zero anaphora resolution can be divided into two phases.",
        "The first phase is zero pronoun detection and the second phase is zero pronoun resolution.",
        "Zero pronoun resolution is similar to coreference resolution and pronoun resolution, which have been studied for many years (e.g. Soon et al. (2001); Mitkov (2002); Ng (2005)).",
        "Isozaki and Hirao (2003) and Iida et al. (2006) focused on zero pronoun resolution assuming perfect pre-detection of zero pronouns.",
        "However, we consider that zero pronoun detection and resolution have a tight relation and should not be handled independently.",
        "Our proposed model aims not only to resolve zero pronouns but to detect zero pronouns.",
        "Zero pronouns are not expressed in a text and have to be detected prior to identifying their antecedents.",
        "Seki et al. (2002) proposed a probabilistic model for zero pronoun detection and resolution that uses hand-crafted case frames.",
        "In order to alleviate the sparseness of hand-crafted case frames, Kawahara and Kurohashi (2004) introduced wide-coverage case frames to zero pronoun detection that are automatically constructed from a large corpus.",
        "They use the case frames as selectional restriction for zero pronoun resolution, but do not utilize the frequency of each example of case slots.",
        "However, since the frequency is shown to be a good clue for syntactic and case structure analysis (Kawahara and Kurohashi, 2006), we consider the frequency also can benefit zero pronoun detection.",
        "Therefore we propose a probabilistic model for zero anaphora resolution that fully utilizes case frames.",
        "This model directly considers the frequency and estimates case assignments for overt case components and antecedents of zero pronoun simultaneously.",
        "In addition, our model directly links each zero pronoun to an entity, while most existing models link it to a certain mention of an entity.",
        "In our model, mentions and zero pronouns are treated similarly and all of them are linked to corresponding entities.",
        "In this point, our model is similar to the coreference model proposed by Luo (2007) and that proposed by Yang et al.",
        "(2008).",
        "Due to this characteristic, our model can utilize information beyond a mention and easily consider salience (the importance of an entity)."
      ]
    },
    {
      "heading": "2. Construction of Case Frames",
      "text": [
        "Case frames describe what kinds of cases each predicate has and what kinds of nouns can fill these case slots.",
        "We construct case frames from a large raw corpus by using the method proposed by Kawahara and Kurohashi (2002), and use them for case structure analysis and zero anaphora resolution.",
        "This section shows how to construct the case frames.",
        "After a large corpus is parsed by a Japanese parser, case frames are constructed from modifier-head examples in the resulting parses.",
        "The problems of case frame construction are syntactic and semantic ambiguities.",
        "That is to say, the parsing results inevitably contain errors and predicate senses are intrinsically ambiguous.",
        "To cope with these problems, case frames are gradually constructed from reliable modifier-head examples.",
        "First, modifier-head examples that have no syntactic ambiguity are extracted, and they are disambiguated by coupling a predicate and its closest case component.",
        "Such couples are explicitly expressed on the surface of text, and can be considered to play an important role in sentence meanings.",
        "For instance, examples are distinguished not by predicates (e.g., \"tsumu (load/accumulate))\", but by couples (e.g., \"nimotsu-wo tsumu (load baggage)\" and \"keiken-wo tsumu (accumulate experience))\".",
        "Modifier-head examples are aggregated in this way, and yield basic case frames.",
        "Thereafter, the basic case frames are clustered to merge similar case frames.",
        "For example, since \"nimotsu-wo tsumu (load baggage)\" and \"busshi-wo tsumu (load supplies)\" are similar, they are clustered.",
        "The similarity is measured using a thesaurus (The National Language Institute for Japanese Language, 2004).",
        "Using this gradual procedure, we constructed case frames from approximately 1.6 billion sentences extracted from the web.",
        "In Table 1, some examples of the resulting case frames are shown.",
        "By using case frames that are automatically constructed from a large corpus, sparseness problem is alleviated to some extent, but still remains.",
        "For instance, there are thousands of named entities (NEs), which cannot be covered intrinsically.",
        "To deal with this sparseness problem, we generalize the examples of case slots.",
        "Kawahara and Kurohashi also give generalized examples such as \"agent\" but only a few types.",
        "We generalize case slot examples based on categories of common nouns and NE classes.",
        "First, we use the categories that Japanese morphological analyzer JUMAN adds to common nouns.",
        "In JUMAN, about twenty categories are defined and tagged to common nouns.",
        "For example, \"ringo (apple),\" \"inu (dog)\" and \"byoin (hospital)\" are tagged as \"FOOD,\" \"ANIMAL\" and \"FACILITY,\" respectively.",
        "For each category, we calculate the rate of categorized example among all case slot examples, and add it to the case slot as \"[CT:FOOD]:0.07.\"",
        "We also generalize NEs.",
        "We use a common standard NE definition for Japanese provided by IREX workshop (1999).",
        "IREX defined eight NE classes as shown in Table 2.",
        "We first recognize NEs in the source corpus by using an NE recognizer (Sasano and Kurohashi, 2008), and then construct case frames from the NE-recognized corpus.",
        "case slot",
        "examples generalized examples with rate",
        "tsumu (1) (load)",
        "gtt (subjective) WO (objective) ni (dative)",
        "he, driver, friend, • • • [CT:PERSON]:0.45, [NE:PERSON]:0.08, • • • baggage, luggage, hay, • • • [CT:ARTIFACT]:0.31, • • car, truck, vessel, seat, • • • [CT:VEHICLE]:0.32, • • •",
        "tsumu (2) (accumulate)",
        "gtt (subjective) WO (objective)",
        "player, children, party, • • • [CT:PERSON]:0.40, [NE:PERSON]:0.i2, • • • experience, knowledge, • • • [CT:ABSTRACT]:0.47, • • •",
        "hanbai (1) (sell)",
        "gtt (subjective) WO (objective) ni (dative) dc (locative)",
        "company, Microsoft, firm, • • • [NE:ORGANIZätion]:0.16, [CT:ORGANIZATION]:0.13, • • •",
        "goods, product, ticket, • • • [CT:ARTIFACT]:0.40, [CT:FOOD]:0.07, • • •",
        "customer, company, user, • • • [CT:PERSON]:0.28, • •",
        "shop, bookstore, Site • • • [CT:FACILITY]:0.40, [CT:LOCATION]:0.39, • • •",
        "As well as categories, for each NE class, we calculate the NE rate among all case slot examples, and add it to the case slot as \"[NE:PERSON]:0.12.\"",
        "The generalized examples are also included in Table 1.",
        "This information is utilized to estimate the case assignment probability, which will be mentioned in Section 3.2.3."
      ]
    },
    {
      "heading": "3. Zero Anaphora Resolution Model",
      "text": [
        "In this section, we propose a probabilistic model for Japanese zero anaphora resolution.",
        "The outline of our model is as follows:",
        "1.",
        "Parse an input text using the Japanese parser KNP and recognize NEs.",
        "2.",
        "Conduct coreference resolution and link each mention to an entity or create new entity.",
        "3.",
        "For each sentence, from the end of the sentence, analyze each predicate by the following steps:",
        "(a) Select a case frame temporarily.",
        "(b) Consider all possible correspondence between each input case component and an case slot of the selected case frame.",
        "(c) Regard case slots that have no correspondence as zero pronoun candidates.",
        "(d) Consider all possible correspondence between each zero pronoun candidate and an existing entity.",
        "(e) For each possible case frame, estimate each correspondence probabilistically, and select the most likely case frame and correspondence.",
        "In this paper, we concentrate on three case slots for zero anaphora resolution: \"ga (subjective),\" \"wo (objective)\" and \"ni (dative),\" which cover about 90% of zero anaphora.",
        "Morphological analysis, NE recognition, syntactic analysis and coreference resolution are conducted as pre-processes for zero anaphora resolution.",
        "Therefore, the model has already recognized existing entities before zero anaphora resolution.",
        "For example, let us consider the following text:",
        "(i) Toyota-wa 1997-nm hybrid car Prius-wo hatsubai(\\axmch).",
        "2000-nen-karaha kaigai (overseas)-demo hanbai(se\\\\)-shiteim.",
        "(Toyota launched the hybrid car Prius in 1997.",
        "<j>i started selling <j>2 overseas in 2000.)",
        "Figure 1 shows the analysis process for this text.",
        "There are three mentions in the first sentence, and the two mentions, hybrid car and Prius, appear in apposition.",
        "Thus, after the pre-processes, two entities, {Toyota} and {hybrid-car, Prius}, are created.",
        "Then, case structure analysis for the predicate hatsubai (launch) is conducted.",
        "First, one of the case frames of hatsubai (launch) is temporarily selected and each input case component is assigned to an appropriate case slot.",
        "For instance, case component Toyota is assigned toga case slot and Prius is assigned to wo case slot.",
        "In this case, though there is a mention hybrid-car that is not a case component of hatsubai (launch) by itself, it refers to the same entity as Prius refers.",
        "Thus, there is no entity that is not linked to hatsubai (launch), and no further analysis is conducted.",
        "Now, let us consider the second sentence.",
        "A mention kaigai (overseas) appears and a new entity {kaigai} is created.",
        "Then, case structure analysis for the predicate hanbai (sell) is conducted.",
        "There is only one overt case component kaigai (overseas), and it is assigned to a case slot of the selected case frame of hanbai (sell).",
        "For instance, the case frame hanbai(l) in Table 1 is selected and kaigai (overseas) is assigned to de (locative) case slot.",
        "In this case, the remaining case slots ga, wo and ni are considered as zero pronouns, and all possible correspondences between zero pronouns and remaining entities are considered.",
        "As a result of probabilistic estimation, the entity {Toyota} is assigned toga case, the entity {hybrid-car, Prius} is assigned to wo case and no entity is assigned to ni case.",
        "Now, we show how to estimate the correspondence probabilistically in the next subsection.",
        "NE class",
        "Examples",
        "ORGANIZATION",
        "NHK Symphony Orchestra",
        "PERSON",
        "Kawasaki Kenjiro",
        "LOCATION",
        "Rome, Sinuiju",
        "ARTIFACT",
        "Nobel Prize",
        "DATE",
        "July 17, April this year",
        "TIME",
        "twelve o'clock noon",
        "MONEY",
        "sixty thousand dollars",
        "PERCENT",
        "20%, thirty percents",
        "Input sentences",
        "Toyota-wa-",
        "Case frames hatsubai (launch)",
        "Prius-wo",
        "hatsubai.",
        "(launch)",
        "kaigai-demo – -(overseas)",
        "hanbai-shlteiru",
        ".",
        "direct case assignment• indirect case assignment (zero anaphora) '",
        "Toyota launched the hybrid car Prius in 1997.",
        "0-, started selling <P2 overseas in 2000.",
        "The proposed model gives a probability to each possible case frame CF and case assignment CA when target predicate v, input case components ICC and existing entities ENT are given.",
        "It also outputs the case frame and case assignment that have the highest probability.",
        "That is to say, our model selects the case frame CFbest and the case assignment CAbest that maximize the probability P{CF,CA\\v,ICC, ENT):",
        "(CFfesU CAbest)",
        "Though case assignment CA usually represents correspondences between input case components and case slots, in our model it also represents correspondences between antecedents of zero pronouns and case slots.",
        "Hereafter, we call the former direct case assignment (DCA) and the latter indirect case assignment (ICA).",
        "Then, we transform P{CFh CAk\\v, ICC, ENT) as follows:",
        "x P(ICAk\\v, ICC, ENT, CFh DCAk) ttP(CFi\\v,ICC) x P(DCAk\\ICC,CFi) P(CFhICC\\v)",
        "('.'",
        "CFi contains the information about v.) P(DCAk\\ICC,CFl)",
        "= P(DCAk,ICC\\CFj) P{ICC\\CFi)",
        "Equation (2) is derived because we assume that the case frame CFi and direct case assignment DCAk are independent of existing entities ENT, and indirect case assignment ICAk is independent of input case components ICC.",
        "Because P(ICC\\v) is constant, we can say that our model selects the case frame CFbest and the direct case assignment DCA^st and indirect case assignment IC Ay,est that maximize the probability P(CF, DC A, ICA\\v, ICC, ENT):",
        "(CFbest,DCAbest,ICAbest) = argmax (p(CF\\v) x P(DCA, ICC\\CF)",
        "The probability P(CFi\\v), called generative probability of a case frame, is estimated from case structure analysis of a large raw corpus.",
        "The following subsections illustrate how to calculate P{DCAk,ICC\\CFi) and P{ICAk\\ENT,CFhDCAk).",
        "_ ga",
        "subjective",
        "company, SONY, firm, ... [NE:ORGANIZATION] 0.15, ...",
        "WO",
        "~ objective",
        "product, CD, model, car, ... [CT:ARTIFACT] 0.40, ...",
        "de",
        "locative",
        "area, shop, worltf, Japan, ...",
        "[CTFACILITY] 0 13....",
        "hanbai (sell)",
        "... 9a",
        "subjective",
        "company, Microsoft, ... [NE:ORGANIZATION] 0.16, ...",
        "WO",
        "objective",
        "goods, product, ticket, ... [CTARTIFACT] 0.40, ...",
        "ni",
        "cystomer,.compa.ny,.user.",
        ".,.",
        "fpT PFR^riMi n* o«",
        "de",
        "locative",
        "shop, bookstore, site, ... [CT:FACILITY] 0.40, ...",
        "For estimation of generative probability of direct case assignment P(DCAk, ICC\\CFi), we follow Kawahara and Kurohashi's (2006) method.",
        "They decompose P{DCAk, ICC\\CF) into the following product depending on whether a case slot Sj is filled with an input case component or vacant:",
        "where the function A(sj) returns 1 if a case slot Sj is filled with an input case component; otherwise 0, rij denotes the content part of the case component, and Cj denotes the surface case of the case component.",
        "The probabilities P(A(sj) = l\\CFi,Sj) and P(A(sj) = 0\\CFi, Sj) are called generative probability of a case slot, and estimated from case structure analysis of a large raw corpus as well as generative probability of a case frame.",
        "The probability P(rij,c3\\CFi, Sj, A(sj) = 1) is called generative probability of a case component and estimated as follows:",
        "P(rij\\CFi, Sj, A(sj) = 1) means the generative probability of a content part rij from a case slot Sj in a case frame CFi, and estimated by using the frequency of a case slot example in the automatically constructed case frames.",
        "P(cj\\sj, A(sj) = 1) is approximated by P(cj\\case-type-of(sj),A(sj) = 1) and estimated from the web corpus in which the relationship between a surface case marker and a case slot is annotated by hand.",
        "To estimate probability of indirect case assignment P{ICAk\\ENT,CFhDCAk) we also decompose it into the following product depending",
        "intra-sentence: case components of",
        "on whether a case slot Sj is filled with an entity entj or vacant:",
        "P{ICAk\\ENT,CFhDCAk) = where the function A'(sj) returns 1 if a case slot Sj is filled with an entity entj; otherwise 0.",
        "Note that we only consider case slots ga, wo and ni that is not filled with an input case component.",
        "We approximate P{A'{sj) = l,entj\\ENT,CFh Sj) and P{A'{sj) = 0\\ENT, CFh Sj) as follows:",
        "Equation (8) is derived because we assume P(A'(sj) = l\\CFi,Sj) is independent of existing entities that are not assigned to Sj.",
        "Equation (9) is derived because we assume P(A'(sj) = 0) is independent of ENT and CFi, and only depends on the case type of Sj, such as ga, wo and ni.",
        "P(A'(sj) = 0\\caseJ>ype_of (sj)) is the probability that a case slot has no correspondence after zero anaphora resolution and estimated from anaphoric relation tagged corpus.",
        "Let us consider the probability P(A'(sj) = 11entj, CFi,Sj).",
        "We decompose entj into content part rijm, surface case cJn and location class ljn.",
        "Here, location classes denote the locational relations between zero pronouns and their antecedents.",
        "We defined twelve location classes as described in Table 3.",
        "In Table 3, Vz means a predicate that has a zero pronoun.",
        "Note that we also consider the locations of zero pronouns that are linked to the target entity as location class candidates.",
        "Now we roughly approximate P(A'(sj) = l\\entj, CFi, Sj) as follows:",
        "u",
        "parent predicate of Vz",
        "L2",
        "parent predicate of Vz\" (parallel)",
        "L3",
        "child predicate of Vz",
        "U",
        "child predicate of Vz (parallel)",
        "L5",
        "parent predicate of parent noun phrase of Vz",
        "Le",
        "parent predicate of parent predicate of Vz (parallel)",
        "L7",
        "other noun phrases following Vz",
        "Ls",
        "other noun phrases preceding Vz",
        "inter-sentence: noun phrases in",
        "Lg",
        "1 sentence before",
        "Lio",
        "2 sentences before",
        "Lu",
        "3 sentences before",
        "Ll2",
        "more than 3 sentences before",
        "n",
        "P{A{Sj)",
        "= l,rij,Cj\\CFi,Sj)",
        "Sj:A(sj)=",
        "=1",
        "n",
        "P(A(s3)",
        "= 0\\CFhSj)",
        "Sj:A(sj)=",
        "=0",
        "n",
        "{p(A(s3",
        ") = l\\CFl,sj)",
        "Sj:A(sj)=",
        "x P{rij,",
        "c3\\CFhs3,A(s3) = l))",
        "n",
        "P(A(s3)",
        "= 0\\CF,Sj) (5)",
        "Sj:A(sj)=",
        "=0",
        "Note that because ent3 is often mentioned more than one time, there are several combinations of content part n3m, surface case c3n and location class ljn candidates.",
        "We select the pair of m and n with the highest probability.",
        "Equation (10) is derived because we assume n3m, Cjn and l3n are independent of each other.",
        "Equation (11) is derived because we approximate P(A' = l\\CFhljn,Sj) as P(A' = l\\ljn, caseJype-of(sj)), and assume P(n3m) and P(c3n) are independent of CFi and s3.",
        "Since these approximation is too rough, specifically, P(n3m) and P(c3n) tend to be somewhat smaller than P(,njm\\CFi, Sj) and P(cjn\\CFh Sj) and equation (11) often becomes too large, we introduce a parameter o>(< 1) and use the a-times value as P{A' = l\\entj,CFi,Sj).",
        "The first term of equation (11) represents how likely an entity that contains rijm as a content part is considered to be an antecedent, the second term represents how likely an entity that contains Cjn as a surface case is considered to be an antecedent, and the third term gives the probability that an entity that appears in location class ljn is an antecedent.",
        "The probabilities P(rijm) and P(cjn) are estimated from a large raw corpus.",
        "The probabilities P(cjn\\caseJ,ype-of(sj)) and P(A' = l\\ljn, caseJype-of(sj)) are estimated from the web corpus in which the relationship between an antecedent of a zero pronoun and a case slot, and the relationship between its surface case marker and a case slot are annotated by hand.",
        "Then, let us consider the probability P(rijm\\CFi, Sj, A'(sj) = 1) in the next subsection.",
        "P(njm\\CF, Sj, A'=l) is similar to P{nj\\CFhSj, A=l) and can be estimated approximately from case frames using the frequencies of case slot examples.",
        "However, while A'(sj) = 1 means Sj is not filled with input case component but filled with an entity as the result of zero anaphora resolution, case frames are constructed by extracting only the input case component.",
        "Therefore, the content part of a zero anaphora antecedent rijm is often not included in the case slot examples.",
        "To cope with this problem, we utilize generalized examples.",
        "When one mention of an entity is tagged any category or recognized as an NE, we also use the category or the NE class as the content part of the entity.",
        "For examples, if an entity {Prius} is recognized as an artifact name and assigned to wo case of the case frame hanbai(l) in Table 1, the system also calculates:",
        "besides:",
        "P(Prius\\hanbai(l),wo, A'(wo) = 1) P(Prius) and uses the higher value.",
        "Previous works reported the usefulness of salience for anaphora resolution (Lappin and Leass, 1994; Mitkov et al., 2002).",
        "In order to consider salience of an entity, we introduce salience score, which is calculated by the following set of simple rules:",
        "• +2 : mentioned with topical marker \"wa\".",
        "• +1 : mentioned without topical marker \"wa\".",
        "+0.5 : assigned to a zero pronoun.",
        "• x0.7 : beginning of each sentence.",
        "For examples, we consider the salience score of the entity {Toyota} in (i) in Section 3.1.",
        "In the first sentence, since {Toyota} is mentioned with topical marker \"wa\", the salience score is 2.",
        "At the beginning of the second sentence it becomes 1.4, and after assigned to the zero pronoun of \"hanbai\" it becomes 1.9.",
        "Note that we use the salience score not as a probabilistic clue but as a filter to consider the target entity as a possible antecedent.",
        "When we use the salience score, we only consider the entities that have the salience score no less than 1."
      ]
    },
    {
      "heading": "4. Experiments 4.1 Setting",
      "text": [
        "We created an anaphoric relation-tagged corpus consisting of 186 web documents (979 sentences).",
        "We selected 20 documents for test and used the other 166 documents for calculating several probabilities.",
        "Since the anaphoric relations in some web documents were not so clear and too difficult to recognize, we did not select such documents for test.",
        "In the 20 test documents, 122 zero anaphora relations were tagged between one of the mentions of the antecedent and the target predicate that had the zero pronoun.",
        "Each parameter for proposed model was estimated using maximum likelihood from the data described in Table 4.",
        "The case frames were automatically constructed from web corpus comprising 1.6 billion sentences.",
        "The case structure analysis was conducted on 80 million sentences in the web corpus, and P(rij) and P(cj) were calculated from the same 80 million sentences.",
        "In order to concentrate on zero anaphora resolution, we used the correct morphemes, named entities, syntactic structures and coreferential relations that were annotated by hand.",
        "Since correct coreferential relations were given, the number of created entities was same between the gold standard and the system output because zero anaphora resolution did not create new entities.",
        "We conducted experiments of zero anaphora resolution.",
        "As the parameter a introduced in Section 3.2.2., we tested 3 values 1, 1/2, and 1/4.",
        "For comparison, we also tested Kawahara and Kurohashi's (2004) model.",
        "The experimental results are shown in Table 5, in which recall R, precision P and F-measure F were calculated by:",
        "# of correctly recognized zero anaphora # of zero anaphora tagged in corpus ' # of system outputted zero anaphora '",
        "Kawahara and Kurohashi's model achieved almost 50% as F-measure against newspaper articles.",
        "However, as a result of our experiment against web documents, it achieved only about 20% as F-measure.",
        "This may be because anaphoric relations in web documents were not so clear as those in newspaper articles and more difficult to recognize.",
        "As to the parameter a, the larger a tended to output more zero anaphora, and the highest F-measure was achieved against a = 1/2.",
        "When using a = 1/2, there were 72 (=122-50) zero pronouns that were tagged in the corpus and not resolved correctly.",
        "Only 12 of them were correctly detected and assigned to a wrong entity, that is, 60 of them were not even detected.",
        "Therefore, we can say our recall errors were mainly caused by the low recall of zero pronoun detection.",
        "In order to confirm the effectiveness of generalized examples of case slots and salience score, we also conducted experiments under several conditions.",
        "We set a = 1/2 in these experiments.",
        "The results are shown in Table 6, in which CT means generalized categories, NE means generalized NEs and SS means salience score.",
        "Without using any generalized examples, the F-measure is less than Kawahara and Kurohashi's method, which use similarity to deal with sparse -ness of case slot examples, and we can confirm the effectiveness of the generalized examples.",
        "While generalized categories much improved the F-measure, generalized NEs contribute little.",
        "This may be because the NE rate is smaller than common noun rate, and so the effect is limited.",
        "We also confirmed that the salience score filter improved F-measure.",
        "Moreover, by using salience score filter, the zero anaphora resolution becomes about ten times faster.",
        "This is because the system can avoid checking entities with low salience as antecedent candidates.",
        "probability",
        "data",
        "raw corpus",
        "raw corpus",
        "P(cj\\case-type-of (sj), A(sj) = 1) P(cj\\caseJbype-of(sj), A\\sj) = 1)",
        "tagged corpus",
        "tagged corpus",
        "P(nj\\CFi,Sj,A(sj) = l) P(nJ|CFi)Sj,A'(Sj) = l)",
        "case frames",
        "case frames",
        "P(CF\\Vi)",
        "case structure analysis",
        "P(A(s]) = {0,l}\\CFl,s]) P(A'(sj) = 0\\case-type-of(sj)) P(A'(sj)=l\\lj, casedype-of (sj))",
        "case structure analysis",
        "tagged corpus",
        "tagged corpus",
        "R",
        "P",
        "F",
        "Kawahara & Kurohashi",
        ".230 (28/122)",
        ".173 (28/162)",
        ".197",
        "Proposed (a",
        "(a = (a =",
        "= 1) 1/2) 1/4)",
        ".426 (52/122) .410 (50/122) .295 (36/122)",
        ".271 (52/192) .373 (50/134) .419 (36/86)",
        ".331 .391",
        ".346",
        "We compare our accuracies with (Seki et al., 2002).",
        "They achieved 48.9% in precision, 88.2% in recall, and 62.9% in F-measure for zero pronoun detection, and 54.0% accuracy for antecedent estimation on 30 newspaper articles, that is, they achieved about 34% in F-measure for whole zero pronoun resolution.",
        "It is difficult to directly compare their results with ours due to the difference of the corpus, but our method achieved 39% in F-measure and we can confirm that our model achieves reasonable performance considering the task difficulty."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, we proposed a probabilistic model for Japanese zero anaphora resolution.",
        "By using automatically constructed wide-coverage case frames that include generalized examples and introducing salience score filter, our model achieves reasonable performance against web corpus.",
        "As future work, we plan to conduct large-scale experiments and integrate this model to a fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis (Kawahara and Kurohashi, 2006).",
        "CT",
        "NE",
        "SS",
        "R",
        "P",
        "F",
        "V",
        ".131 (16/122)",
        ".205 (16/78)",
        ".160",
        "V",
        "V",
        ".164 (20/122)",
        ".247 (20/81)",
        ".197",
        "V",
        "V",
        ".402 (49/122)",
        ".368 (49/133)",
        ".384",
        "V",
        "V",
        ".385 (47/122)",
        ".196 (47/240)",
        ".260",
        "V",
        "V",
        "V",
        ".410 (50/122)",
        ".373 (50/134)",
        ".391"
      ]
    }
  ]
}
