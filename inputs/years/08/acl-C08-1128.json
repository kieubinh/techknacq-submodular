{
  "info": {
    "authors": [
      "Jia Xu",
      "Jianfeng Gao",
      "Kristina Toutanova",
      "Hermann Ney"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C08-1128",
    "title": "Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/C08-1128",
    "year": 2008
  },
  "references": [
    "acl-C96-2141",
    "acl-J05-4005",
    "acl-J93-2003",
    "acl-N04-1033",
    "acl-P02-1017",
    "acl-P02-1038",
    "acl-P02-1040",
    "acl-P06-1085",
    "acl-W04-1118",
    "acl-W06-1655"
  ],
  "sections": [
    {
      "text": [
        "Words in Chinese text are not naturally separated by delimiters, which poses a challenge to standard machine translation (MT) systems.",
        "In MT, the widely used approach is to apply a Chinese word seg-menter trained from manually annotated data, using a fixed lexicon.",
        "Such word segmentation is not necessarily optimal for translation.",
        "We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT.",
        "Experiments show that our method improves a state-of-the-art MT system in a small and a large data environment."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Chinese sentences are written in the form of a sequence of Chinese characters, and words are not separated by white spaces.",
        "This is different from most European languages and poses difficulty in many natural language processing tasks, such as machine translation.",
        "It is difficult to define \"correct\" Chinese word segmentation (CWS) and various definitions have been proposed.",
        "In this work, we explore the idea that the best segmentation depends on the task, and concentrate on developing a CWS method for MT, which leads to better translation performance.",
        "The common solution in Chinese-to-English translation has been to segment the Chinese text using an off-the-shelf CWS method, and to apply a standard translation model given the fixed segmentation.",
        "The most widely applied method for MT is unigram segmentation, such as segmentation using the LDC (LDC, 2003) tool, which requires a manual lexicon containing a list of Chinese words and their frequencies.",
        "The lexicon and frequencies are obtained using manually annotated data.",
        "This method is suboptimal for MT.",
        "For example, ^K(paper) and W(card) can be two words or composed into one word ^KW(cards).",
        "Since Â£K Wdoes not exist in the manual lexicon, it cannot be generated by this method.",
        "In addition to unigram segmentation, other methods have been proposed.",
        "For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation.",
        "However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown.",
        "In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation.",
        "We refine the method in training using a Bayesian semi-supervised CWS approach motivated by (Goldwa-ter et al., 2006).",
        "We describe a generative model which consists of a word model and two alignment models, representing the monolingual and bilingual information, respectively.",
        "In our methods, we first segment Chinese text using a unigram segmented and then learn new word types and word distributions, which are suitable for MT.",
        "Our experiments on both large (NIST) and small (IWSLT) data tracks of Chinese-to-English translation show that our method improves the performance of a state-of-the-art machine translation system."
      ]
    },
    {
      "heading": "2. Review of the Baseline System",
      "text": [
        "In statistical machine translation, we are given a Chinese sentence in characters cf = c\\.. .ck which is to be translated into an English sentence e{ = ei... ej.",
        "In order to obtain a more adequate mapping between Chinese and English words, cf-is usually segmented into words f( = f\\... fj in preprocessing.",
        "In our baseline system, we apply the commonly used unigram model to generate the segmentation.",
        "Given a manually compiled lexicon containing words and their relative frequencies Ps(f'j), the best segmentation f( is the one that maximizes the joint probability of all words in the sentence, with the assumption that words are independent of each other:",
        "argmax JJP^/'j",
        "where the maximization is taken over Chinese word sequences whose character sequence is cf -.",
        "Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993).",
        "Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004).",
        "Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty.",
        "The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002).",
        "The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing."
      ]
    },
    {
      "heading": "3. Unigram Dirichlet Process Model for CWS",
      "text": [
        "The simplest version of our model is based on a unigram Dirichlet Process (DP) model, using only monolingual information.",
        "Different from a standard unigram model for CWS, our model can introduce new Chinese word types and learn word distributions automatically from unlabeled data.",
        "According to this model, a corpus of Chinese words /i,... fm,..., fM is generated via:",
        "where G is a distribution over words drawn from a Dirichlet Process prior with base measure Po and concentration parameter a.",
        "We never explicitly estimate G but instead integrate over its possible values and perform Bayesian inference.",
        "It is easy to compute the",
        "!The notational convention will be as follows: we use the symbol Pr(-) to denote general probability distributions with (nearly) no specific assumptions.",
        "In contrast, for model-based probability distributions, we use the generic symbol P(-).",
        "probability of a Chinese word given a set of already generated words, while integrating over G. This is done by casting Chinese word generation as a Chinese restaurant process (CRP) (Aldous, 1985), i.e. a restaurant with an infinite number of tables (approximately corresponding to Chinese word types), each table with infinite number of seats (approximately corresponding to Chinese word frequencies).",
        "The Dirichlet Process model can be viewed intuitively as a cache model (Goldwater et al., 2006).",
        "Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f-j:",
        "where N(fj) is the number of Chinese words fj in the previous context.",
        "N is the total number of Chinese words, Po is the base probability over words, and a influences the probability of introducing a new word at each step and controls the size of the lexicon.",
        "The probability of generating a word from the cache increases as more instances of that word are seen.",
        "For the base distribution Po, which governs the generation of new words, we use the following distribution (called the spelling model):",
        "where ^ is the number of characters in the document, i.e. character vocabulary size, and L is the number of Chinese characters of word /.",
        "We note that this is a Poisson distribution on word length and a unigram distribution on characters given the length.",
        "We used A = 2 and a = 0.3 in our experiments."
      ]
    },
    {
      "heading": "4. CWS Model for MT",
      "text": [
        "As a solution to the problems with the conventional approach to CWS mentioned in Section 1, we propose a generative model for CWS in Section 4.1, and then extend the model to a more general but deficient model, similar to a maximum entropy model in which most features are derived from the submodels of the generative model.",
        "The generative model assume that a corpus of parallel sentences (c\\K&\\~) is generated along with a hidden sequence of Chinese words f\\ and a hidden word alignment b\\ for every sentence.",
        "The alignment indicates the aligned Chinese word for each English word e^, where /o indicates a special null word as in the IBM models.",
        "Without assuming any special form for the probability of a sentence pair along with hidden variables, we can factor it into a monolingual Chinese sentence probability and a bilingual translation probability as follows:",
        "where cf ) is 1 if the characters of the sequence of words f\\J are c\\K, and to 0 otherwise.",
        "We can drop the conditioning on c\\K in Pr(e[,b[\\fi), because the characters are deterministic given the words.",
        "The joint probability of the observations (cix,e{) can be obtained by summing over all possible values of the hidden variables // and b[.",
        "In Sections 4.1.1 and 4.1.2, we will describe the modeling assumptions behind the monolingual Chinese sentence model and the translation model, respectively.",
        "We use the Dirichlet Process unigram word model introduced in section 3.",
        "In this model, the parameters of a distribution over words G are first drawn from the Dirichlet prior DP {a, Pq).",
        "Words are then independently generated according to G. The probability of a sequence of Chinese words in a sentence is thus: j",
        "We employ the Dirichlet Process inverse IBM model 1 to generate English words and alignment given the Chinese words.",
        "In this model, for every Chinese word / (including the null word), a distribution over English words Gf is first drawn from a Dirichlet Process prior DP(a, Po(e)), where Po(e) we used the empirical distribution over English words in the parallel data.",
        "Then, given these parameters, the probability of an English sentence and alignment given a Chinese sentence (sequence of words) is given by:",
        "This is the same model form as inverse IBM model 1, except we have placed Dirichlet Process priors on the Chinese-word specific distributions over English words.",
        "ditioned on eaj.",
        "In practice, we observed that using a word-alignment model in one direction is not sufficient.",
        "We then added a factor to our model which includes word alignment in the other direction , i.e. a Dirichlet Process IBM model 1.",
        "We ignore the detailed description here, because the calculation is the same as that of the inverse IBM model 1.",
        "According to this model, for every English word e (including the null word), a distribution over Chinese words Ge is first drawn from a Dirichlet Process prior DP(a, Po(f)).",
        "Here, for the base distribution Po(/) we used the same spelling model as for the monolingual unigram Dirichlet Process prior.",
        "The probability of a sequence of Chinese words f{ and a word alignment af given a sequence of English words e{ is then:",
        "We put the monolingual model and the translation models in both directions together into a single model, where each of the component models is weighted by a scaling factor.",
        "This is similar to a maximum entropy model.",
        "We fit the weights of the sub-models on a development set by maximizing the BLEU score of the final translation.",
        "where Z is the normalization factor.",
        "In practice we do not re-normalize the probabilities and our model is thus deficient because it does not sum to 1 over valid observations.",
        "However, we found the model work very good in our experiments.",
        "Similar deficient models have been used very successfully before, for example, in the IBM models 3-6 and in the unsupervised grammar induction model of (Klein and Manning, 2002)."
      ]
    },
    {
      "heading": "5. Gibbs Sampling Training",
      "text": [
        "It is generally impossible to find the most likely segmentation according to our Bayesian model using exact inference, because the hidden variables do not allow exact computation of the integrals.",
        "Nonetheless, it is possible to define algorithms using Markov chain Monte Carlo (MCMC) that produce a stream of samples from the posterior distribution of the hidden variables given the observations.",
        "We applied the Gibbs sampler (Geman and Geman, 1984) â one of the simplest MCMC methods, in which transitions between states of the",
        "Markov chain result from sampling each component of the state conditioned on the current value of all other variables.",
        "In our problem, the observations are D = (di, ..dn,.., (In), where dn=(c^,e[) indicates a bilingual sentence pair, the hidden variables are the word segmentations ff and the alignments in two directions af and b[.",
        "To perform Gibbs sampling, we start with an initial word segmentation and initial word alignments, and iteratively re-sample the word-segmentation and alignments according to our model of Equation 4.",
        "Note that for efficiency, we only allow limited modifications to the initial word alignments.",
        "Thus we only use models derived from IBM-1 (instead of IBM-4) for comparing different word segmentations.",
        "On the other hand, re-sampling the segmentation causes re-linking alignment points to parts or groups of the original words.",
        "Hence, we organize our sampling process around possible word boundaries.",
        "For each character Cfc in each sentence, we consider two alternative segmentations: Ck+ indicates the segmentation where there is a boundary after Ck and Ck~ indicates the segmentation where there is no boundary after Ck, keeping all other boundaries fixed.",
        "Let / denote the single word spanning character Ck when there is no boundary after it, and /',/\" denote the two adjacent words resulting if there is a boundary: /' includes Ck and /\" starts just to the right, with character Ck+i- The introduction of /' and /\" leads to M new possible alignments in the E-to-C direction b^v â  â  â , b\\M, such as in Figure 1.",
        "Together with the boundary vs no-boundary state at each character position, we re-sample a set of alignment links between English words and any of the Chinese words /,/', and /\", keeping all other word alignments in the sentence pair fixed.",
        "(See Figures 1 and 2.)",
        "Input: D with an initial segmentation and alignments Output: D with sampled segmentation and alignments for n = 1 to N for k = 1 to K that cy e dn",
        "Create M+l candidates, cba~Â£m and cba^, where cbaÂ£m: there is a word boundary after cy.",
        "cba^: there is no word boundary after cy.",
        "Compute probabilities",
        "P(cba^\\dhnk~) Sample boundary and relevant alignments Update counts",
        "Thus at each step in the Gibbs sampler, we consider a set of alternatives for the boundary after Cfc and relevant alignment links, keeping all other hidden variables fixed.",
        "At each step, we need to compute the probability of each of the alternatives, given the fixed values of the other hidden variables.",
        "We introduce some notation to make the presentation easier.",
        "For every position k in sentence pair n, we denote by dhnk~ the observations and hidden variables for all sentences other than sentence n, and the observations and hidden variables inside sentence n, not involving character position Cfc.",
        "The fixed variables inside the sentence are the words not neighboring position k, and the alignments in both directions to these words.",
        "In the process of sampling, we consider a set of alternatives: segmentation Ck+ along with the product space of relevant alignments in both direc-",
        "btM, and at, and segmentation ck",
        "along with relevant alignments bk~ and ak.",
        "For brevity, we denote these alternatives by cbak,m+and cbak~.",
        "We describe how we derive the set of alternatives in section 5.2 and how we compute their probabilities in section 5.1.",
        "of parallel sentences, where N is the number of parallel sentences.",
        "For the Gibbs sampling algorithm in Table 1, we need to compute the probability of each alternative segmentation/alignments, given the fixed values of the rest of the data dhnk~.",
        "The probability of the hidden variables in the alternatives is proportional to the joint probability of the hidden variables and observations, and thus it is sufficient to compute the probability of the latter.",
        "We compute these probabilities using the Chinese restaurant process sampling scheme for the Dirichlet Process, thus integrating over all of the possible values of the distributions G,Gf and Ge.",
        "E-to-C",
        "C-to-E",
        "e'",
        "e\"",
        "e'e\"",
        "\\e't",
        "e\".",
        "e'.e\".",
        "e'",
        "e\".",
        "\\",
        "f",
        "1",
        "f\"",
        "ft",
        "t",
        "|f",
        "t",
        "f'",
        "Let cbak denote an alternative hypothesis including boundary or no boundary at position k, and relevant alignments to English words in both directions of the one or two Chinese words resulting from the segmentation at k. The probability of this configuration given by our model is:",
        "P(cbak\\dhnk ) (x Pm(cbak\\dhnk where Pm(cbak\\dhnk~) is the monolingual word probability, and Pfe(cbak\\dhnk~) and Pef(cbak\\dhnk~) are the translation probabilities in the two directions.â Pef{cbak\\dhnk )A2 â¢ Pfe{cbak\\dhnk )Xs,",
        "We now describe the computations of each of the component probabilities.",
        "The word model probability Pm{cabk\\dhnk~) in Equation 5 is derived from Equations 3 and 1:",
        "There are two cases, depending on whether the hypothesis specifies that there is a boundary after character ck, in which case we need the probabilities of the two resulting words /', and /\", or there is no boundary, in which case we need the probability of the single word /.",
        "(See the initial states in Figures 1 and 2, respectively.)",
        "Let N denote the total number of word tokens in the rest of the corpus dhnk~, and N(f) denote the number of instances of word / in dhnk~.",
        "The probabilities in the two cases are Pm(cl\\dhnk~) oc Pm(ck \\dhnk ) oc",
        "Here Po(f) is computed using Equation 2.",
        "The translation model probabilities depend on whether or not there is a segmentation boundary at ck and which English words are aligned to the relevant Chinese words.",
        "In the first case, assume that there is a word boundary in cabk, and that English words {e'} are aligned to /' and words {e\"} are aligned to /\" in the E-to-C direction according to the alignment bk, and that /' is aligned to e*' and /\" is aligned to e*\" in the C-to-E direction according to the alignment ak (see the initial state in Figure 1).",
        "Here we overloaded notations and use bk and ak to indicate the alignments of the relevant Chinese words at position k to any English words.",
        "Let / denote the total number of English words in the sentence, and J+1 denote the number of Chinese words according to this segmentation.",
        "We also denote the total number of English words aligned to either /' or /\" in the E-to-C direction by P.",
        "The translation model probability in the E-to-C direction is thus:",
        "Here we compute P(e\\f, dhnk ) as:",
        "where the counts are computed over the fixed assignments dhnk~.",
        "The translation probability in the other direction is similarly computed as:",
        "And P(f\\e, dhnk~) is computed as:",
        "In the second case, if the hypothesis in evaluation does not have a word boundary at position k, the total number of Chinese words would be one less, i.e. J instead of J +1 in the equations above, and there would be a single set of English words aligned to the word / in the E-to-C direction, and a single word e* aligned to / in the C-to-E direction (see the initial state in Figure 2.",
        "The probability of this hypothesis is computed analogously.",
        "As mentioned earlier, we consider alternative alignments which deviate minimally from the current alignments, and which satisfy the constraints of the IBM model 1 in both directions.",
        "In order to describe the set of alternatives, we consider two cases, depending on whether there is a boundary at the current character before sampling at position k.",
        "Case 1.",
        "There was no boundary at ck in the previous state (see Figure 1).",
        "If there is no boundary at ck, there is a single word / spanning that position.",
        "We denote by {e} the set of English words aligned to / at that state in the E-to-C direction and by e* the English word aligned to / in the C-to-E direction.",
        "Since every state we consider satisfies the IBM one-to-many constraints, there is exactly one English word aligned to / in the C-to-E direction and the words {e} have no other words aligned to them in the E-to-C direction.",
        "In this case, we consider as hypothesis cbak~ the same segmentation and alignment as in the previous state, (see Table 1 for an overview of the alternative hypotheses.)",
        "We consider M different hypotheses which include a boundary at k in this case, where M depends on the number of words {e} aligned to / in the previous state.",
        "Because we are breaking the word / into two words /' and /\" by placing a boundary at ck, we need to realign the words {e} to either /' or /\".",
        "Additionally we need to align /' and /\" to English words in the C-to-E direction.",
        "The number of different hypotheses is equal to 2P where P = |{e}|.",
        "These alternatives arise by considering that each of the words in {e} needs to align to either /' or /\", and there are 2Pcombinations of these alignments.",
        "For example, if {e} = {ei,e2}, after splitting the word / there are four possible alignments, illustrated in Figure 1: I.",
        "(/',ei) and (f\",e2), II.",
        "(f',e2) and (/Vi), III.",
        "(/', ei) and (/', e2), IV.",
        "(/\", d) and (/\", e2).",
        "For the alignment akin the C-to-E direction, we consider only one option, in which both resulting words /' and /\" align to e*.",
        "These alternatives form cbak}m+ in Table 1.",
        "Case 2.",
        "There was a boundary at ck in the previous state (see Figure 2).",
        "In this case, for the hypotheses ck we consider only one alternative, which is exactly the same as the assignment of segmentation and alignments in the previous state.",
        "Thus we have M = 1 in Table 1.",
        "Let /' and /\" denote the two words at position k in the previous state, {e'} and {e\"} denote the sets of English words aligned to them in the E-to-C direction, respectively, and e*' and e*\" denote the English words aligned to /' and /\" in the C-to-E direction.",
        "We consider only one hypothesis cbak~ where there is no boundary at ck.",
        "In this hypothesis, there is a single word / = /'/\" spanning position k, and all words {e'} U {e\"} align to / in the E-to-C direction.",
        "For the C-to-E direction we consider the \"better\" of the alignments (/, e'J and (/,e\") where the better alignment is defined as the one having higher probability according to the C-to-E word translation probabilities.",
        "Input: D, F0",
        "Output: At, Ft",
        "Run GIZA++ on (D, Ft-\\) to obtain AtRun GS on (D, Ft-i,At) to obtain Ft",
        "So far, we have described how we re-sample word segmentation and alignments according to our model, starting from an initial segmentation and alignments from GIZA++.",
        "Putting these pieces together, the algorithm is summarized in Table 1.",
        "We found that we can further improve performance by repeatedly aligning the corpus using GIZA++, after deriving a new segmentation using our model.",
        "The complete algorithm which includes this step is shown in Table 2, where Ft indicates the word segmentation at iteration t and Atdenotes the GIZA++ corpus alignment in both directions.",
        "The GS re-segmentation step is done according to the algorithm in Table 1.",
        "Using this algorithm, we obtain a new segmentation of the Chinese data and train the translation models using this segmentation as in the baseline MT system.",
        "To segment the test data for translation, we use a unigram model, trained with maximum likelihood estimation off of the final segmentation of the training corpus Ft."
      ]
    },
    {
      "heading": "6. Translation Experiments",
      "text": [
        "We performed experiments using our models for CWS on a large and a small data track.",
        "We evaluated performance by measuring WER (word error rate), PER (position-independent word error rate), BLEU (Papineni et al., 2002) and TER (translation error rate) (Snover et al., 2006) using multiple references.",
        "We first report the experiments using our monolingual unigram Dirichlet Process model for word segmentation on the NIST machine translation task (NIST, 2005).",
        "Because of the computational requirements, we only employed the monolingual word model for this large data track, i.e. the feature weights were Ai = 1, A2 = 0, A3 = 0.",
        "Therefore, no alignment information needs to be maintained in this case.",
        "The bilingual training corpus is a superset of corpora in the news domain collected from different sources",
        "We took'LDC (LDC, 2003) as a baseline CWS method (Base).",
        "As shown in Table 3, the training corpus in each language contains more than two million sentences.",
        "There are 56 million Chinese characters.",
        "The LDC and GS word segmentation methods generated 39.2 and 40.5 million running words, respectively.",
        "The scaling factors of the translation models described in Section 2.2 were optimized on the development corpus, MT-eval 05 with 1082 sentences.",
        "The resulting systems were evaluated on the test corpora MT-eval 02-04.",
        "For convenience, we only list the statistics of the first English reference.",
        "Starting from the baseline LDC output as initial word segmentation, we performed Gibbs sampling (GS) of word segmentations using 30 iterations over the Chinese training corpus.",
        "Since BLEU is the official NIST measure of translation performance, we show the translation results measured in BLEU score only.",
        "As shown in Table 4, on the development data MT-eval 05, the BLEU score was improved by 0.4% absolute or more than 1% relative using GS.",
        "Similarly, the absolute BLEU scores are also improved on all other test sets, in the range of 0.04% to 0.4%.",
        "We can see that even a monolingual semi-supervised word segmentation method can outperform a supervised one in MT, probably because the training/test corpora contain many unknown words and words have different frequencies in our MT data from they do in the manually labeled CWS data.",
        "We evaluate our full model, using both monolingual and bilingual information, on the IWSLT data.",
        "As shown in Table 5, the Chinese training corpus was segmented using the unigram seg-menter (Base) described in Section 2.1 and our GS method.",
        "Since the unigram segmenter performs better in our experiments, we took it as the baseline and the method for initialization in later experiments.",
        "We see that the vocabulary size of the Chinese training corpus was reduced more significantly by GS than by the baseline method, even",
        "Table 5 : Statistics of corpora in task IWSLT.",
        "though they resulted in a similar number of running words.",
        "This shows that the distribution of Chinese words is more concentrated when using GS.",
        "The parameter optimizations were performed on the Dev2 data with 500 sentences, and evaluations were done both on Dev3 and on Eval data, i.e. the evaluation corpus of (IWSLT, 2007).",
        "The model weights A of GS from Section 5.1.2 were optimized using the Powell (Press et al., 2002) algorithm with respect to the BLEU score.",
        "We obtained Ai = 1.4, A2 = 1 and A3 = 0.8 as optimal values and T = 4 as the optimal number of iterations of realignment with GIZA++.",
        "For a fair comparison, we evaluated on various CWS methods including translation on characters , LDC (LDC, 2003), ICT (Zhang et al., 2003), unigram, 9-gram and GS.",
        "Improvements using GS can be seen in Table 6.",
        "Under all test sets and evaluation criteria, GS outperforms the baseline method.",
        "The absolute WER decreases with 1.2% on Dev3 and with 1.1% on Eval data over baseline.",
        "We compared the translation outputs using GS with the baseline method.",
        "On the Eval data, 196 sentences are different out of 489 lines, where 64 sentences from GS are better, 33 sentences are worse, and the rests have similar translation qualities.",
        "Table 7 shows two examples from the Eval corpus.",
        "We list segmentations produced by the baseline and GS methods, as well as the translations corresponding to these segmentations.",
        "The GS method generates better translation results than the baseline method in these cases.",
        "Data",
        "Sents.",
        "Words[K]",
        "Voc.fK]",
        "Cn.",
        "En.",
        "Cn.",
        "En.",
        "Chars",
        "2M",
        "56M",
        "49.5M",
        "65.4",
        "211",
        "Base",
        "39.2M",
        "95.7",
        "GS",
        "40.5M",
        "95.4",
        "02",
        "878",
        "23.1",
        "28.0",
        "2.04",
        "4.34",
        "03",
        "919",
        "24.6",
        "29.2",
        "2.21",
        "4.91",
        "04",
        "1788",
        "49.8",
        "60.7",
        "2.61",
        "6.71",
        "05",
        "1082",
        "30.8",
        "34.2",
        "2.30",
        "5.39",
        "Test",
        "Sents.",
        "Words[K]",
        "Voc.",
        "Cn.",
        "En.",
        "Cn.",
        "En.",
        "Chars",
        "42.9K",
        "520",
        "420",
        "2780",
        "9930",
        "Base",
        "394",
        "8800",
        "GS",
        "398",
        "6230",
        "Dev2",
        "500",
        "3.74",
        "3.82",
        "1004",
        "821",
        "Dev3",
        "506",
        "4.01",
        "3.90",
        "980",
        "820",
        "Eval",
        "489",
        "3.39",
        "3.72",
        "904",
        "810",
        "MT-eval",
        "LDC(Base)",
        "GS",
        "2005",
        "32.85",
        "33.26",
        "2002",
        "34.32",
        "34.36",
        "2003",
        "33.41",
        "33.75",
        "2004",
        "33.74",
        "34.06",
        "Test",
        "Method",
        "WER",
        "PER",
        "BLEU",
        "TER",
        "Dev2",
        "Unigram (Base)",
        "38.2",
        "31.2",
        "55.4",
        "37.0",
        "GS",
        "36.8",
        "30.0",
        "56.6",
        "35.5",
        "Dev3",
        "Unigram (Base)",
        "33.5",
        "27.5",
        "60.4",
        "32.1",
        "GS",
        "32.3",
        "26.6",
        "61.0",
        "31.4",
        "Eval",
        "Characters",
        "49.3",
        "41.8",
        "35.4",
        "47.5",
        "LDC",
        "46.2",
        "40.0",
        "39.2",
        "45.0",
        "ICT",
        "45.9",
        "40.4",
        "40.1",
        "44.9",
        "Unigram (Base)",
        "46.8",
        "40.2",
        "41.6",
        "45.6",
        "9-gram",
        "46.9",
        "40.4",
        "40.1",
        "45.4",
        "GS",
        "45.9",
        "40.0",
        "41.6",
        "44.8",
        "Baseline W iff Ho von",
        "GS REF b) Baseline do you have a shorter way ?",
        "is there a shorter route ?",
        "please show me the in .",
        "please show me the total price .",
        "can you tell me the total amount ?"
      ]
    },
    {
      "heading": "7. Conclusion and future work",
      "text": [
        "We showed that it is possible to learn Chinese word boundaries such that the translation performance of Chinese-to-English MT systems is improved.",
        "We presented a Bayesian generative model for parallel Chinese-English sentences which uses word segmentation and alignment as hidden variables, and incorporates both monolingual and bilingual information to derive a segmentation suitable for MT.",
        "Starting with an initial word segmentation, our method learns both new Chinese words and distributions for these words.",
        "In a large and a small data environment, our method outperformed the standard Chinese word segmentation approach in terms of the Chinese to English translation quality.",
        "In future work, we plan to enrich our monolingual and bilingual models to better represent the true distribution of the data."
      ]
    },
    {
      "heading": "8. Acknowledgments",
      "text": [
        "Jia Xu conducted this research during her internship at Microsoft Research.",
        "This material is also partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.",
        "HR0011-06-C-0023."
      ]
    }
  ]
}
