{
  "info": {
    "authors": [
      "Ming Zhou",
      "Bo Wang",
      "Shujie Liu",
      "Mu Li",
      "Dongdong Zhang",
      "Tiejun Zhao"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C08-1141",
    "title": "Diagnostic Evaluation of Machine Translation Systems Using Automatically Constructed Linguistic Check-Points",
    "url": "https://aclweb.org/anthology/C08-1141",
    "year": 2008
  },
  "references": [
    "acl-A00-2019",
    "acl-E06-1032",
    "acl-J03-1002",
    "acl-P02-1040",
    "acl-P03-1054",
    "acl-P04-1077",
    "acl-P07-1091",
    "acl-W04-3250",
    "acl-W05-0904",
    "acl-W05-0909",
    "acl-W07-0736",
    "acl-W07-0738"
  ],
  "sections": [
    {
      "text": [
        "We present a diagnostic evaluation platform which provides multi-factored evaluation based on automatically constructed check-points.",
        "A checkpoint is a linguistically motivated unit (e.g. an ambiguous word, a noun phrase, a verb~obj collocation, a prepositional phrase etc.",
        "), which are predefined in a linguistic taxonomy.",
        "We present a method that automatically extracts checkpoints from parallel sentences.",
        "By means of checkpoints, our method can monitor a MT system in translating important linguistic phenomena to provide diagnostic evaluation.",
        "The effectiveness of our approach for diagnostic evaluation is verified through experiments on various types of MT systems."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Automatic MT evaluation is a crucial issue for MT system developers.",
        "The state-of-the-art methods for automatic MT evaluation are using an n-gram based metric represented by BLEU (Pa-pineni et al., 2002) and its variants.",
        "Ever since its invention, the BLEU score has been a widely accepted benchmark for MT system evaluation.",
        "Nevertheless, the research community has been aware of the deficiencies of the BLEU metric (Callison-Burch et al., 2006).",
        "For instance, BLEU fails to sufficiently capture the vitality of natural languages: all grams of a sentence are treated equally ignoring their linguistic significance; only consecutive grams are considered ignoring the skipped grams of certain linguistic relations; candidate translation gets acknowledged only if it uses exactly the same lexicon as the reference ignoring the variation in lexical choice.",
        "Furthermore, BLEU is useful for optimizing and improving statistical MT systems but it has shown to be ineffective in comparing systems with different architectures (e.g., rule-based vs. phrase-based) (Callison-Burch et al., 2006).",
        "Another common deficiency of the state-of-the-art evaluation approaches is that they cannot clearly inform MT developers on the detailed strengths and flaws of an MT system, and therefore there is no way for us to understand the capability of certain modules of an MT system, and the capability of translating certain kinds of language phenomena.",
        "For the purpose of system development, MT developers need a diagnostic evaluation approach to provide the feedback on the translation ability of an MT system with regard to various important linguistic phenomena.",
        "We propose a novel diagnostic evaluation approach.",
        "Instead of assigning a general score to an MT system we evaluate the capability of the system in handling various important linguistic test cases called Check-Points.",
        "A checkpoint is a linguistically motivated unit, (e.g. an ambiguous word, a noun phrase, a verb~obj collocation, a prepositional phrase etc.)",
        "which are predefined in a linguistic taxonomy for diagnostic evaluation.",
        "The reference of a checkpoint is its corresponding part in the target sentence.",
        "The evaluation is performed by matching the candidate translation corresponding to the references of the check-points.",
        "The extraction of the checkpoints is an automatic process using word aligner and parsers.",
        "We control the noise of the word aligner and parsers within tolerable scope by selecting reliable subset of the checkpoints and weighting the references with confidence.",
        "The checkpoints of various kinds extracted in this way have shown to be effective in performing diagnostic evaluation of MT systems.",
        "In addition, scores of checkpoints are also approved to be useful to improve the ranking of MT systems as additional features at sentence level and document level.",
        "The rest of the paper is structured in the following way: Section 2 gives the overview of the process of the diagnostic evaluation.",
        "Section 3 introduces the design of checkpoint taxonomy.",
        "Section 4 explains the details of construction of checkpoint database and the methods of reducing the noise of aligner and parsers.",
        "Section 5 explains the matching approach.",
        "In Section 6, we introduce the experiments on different MT systems to demonstrate the capability of the diagnostic evaluation.",
        "In Section 7, we show that the checkpoints can be used to improve the current ranking methods of MT systems.",
        "Section 8 compares our approach with related evaluation approaches.",
        "We conclude this work in Section 9."
      ]
    },
    {
      "heading": "2. Overview of Diagnostic Evaluation",
      "text": [
        "In our implementation, we first build a checkpoint database encoded in XML by associating a test sentence with qualified checkpoints it contains.",
        "This process can be described as following",
        "• Collect a large amount of parallel sentences from the web or book collections.",
        "• Parse the sentences of source language and target language.",
        "• Perform the word alignments between each sentence pair.",
        "• For each category of check-points, extract the checkpoints from the parsed sentence pairs.",
        "• Determine the references of each checkpoint in source language based on the word alignment.",
        "With the extracted checkpoint database, the diagnostic evaluation of an MT system is performed with the following steps:",
        "• The test sentences are selected from the database based on the selected categories of checkpoints to be evaluated.",
        "• For each check-point, we calculate the number of matched n-grams of the refer-",
        "ences against the translated sentence of the MT system.",
        "The credit of the MT system in translating this checkpoint is obtained after necessary normalization.",
        "• The credit of a category can be obtained by summing up the credits of all checkpoints of this category.",
        "Then the credit of an MT system can be obtained by summing up the credits of all categories.",
        "• Finally, scores of system, category groups (e.g.",
        "Words), single category (e.g. Noun), and detail information of n-gram matching of each checkpoint are all provided to the developers to diagnose the MT system."
      ]
    },
    {
      "heading": "3. Linguistic Check-Point Taxonomy",
      "text": [
        "The taxonomy of automatic diagnostic evaluation should be widely accepted so that the diagnostic results can be explained and shared with each other.",
        "We will also need to remove the sophisticated categories that are out of the capability of current NLP tools to recognize.",
        "In light of this consideration, for Chinese-English machine translation, we adopted the manual taxonomy introduced by (Lv, 2000; Liu, 2002) and removed items that are beyond the capability of our parsers.",
        "The taxonomy includes typical check-pints at word, phrase and sentence levels.",
        "Some examples of the representative checkpoints at different levels are provided below:",
        "• Word level check-points:",
        "b.",
        "Ambiguous word e.g., JKplay)",
        "• Phrase level check-points:",
        "b.",
        "Repetitive word combination.",
        "e.g., Iflf (have a look)",
        "c. Subjective-predicate phrase e.g., tt!l*i&, (he*said)",
        "• Sentence level check-points:",
        "a.",
        "\"BA\" sentence: tiffi(BA) ^♦MT .",
        "(He took away the book.)",
        "b.",
        "\"BEI\" sentence: #jfS$(BEI)fri#T.",
        "(The vase was broken.)",
        "Our implementation of Chinese-English checkpoint taxonomy contains 22 categories and English-Chinese checkpoint taxonomy contains 20 categories.",
        "Table 1 and 2 show the two taxonomies.",
        "In practice, any tag in parsers (e.g. NP) can be easily added as new category."
      ]
    },
    {
      "heading": "4. Construction of Check-Point Database",
      "text": [
        "Given a bilingual corpus with word alignment, the construction of checkpoint database consists of following two steps.",
        "First, the information of pos-tag, dependency structure and constituent structure can be identified with parsers.",
        "Then checkpoints of different categories are identified.",
        "Check-points of word-level categories such as Chinese idiom and English ambiguous words are extracted with human-made dictionaries, and the checkpoints of New-Word are extracted with a new word list mined from the web.",
        "A set of human-made rules are employed to extract certain categories involving sentence types such as compound sentence.",
        "Second, for a check-point, with the word alignment information, the corresponding target language portion is identified as the reference of this check-point.",
        "The following example illustrates the process of extracting checkpoints from a parallel sentence pair.",
        "• A Chinese-English sentence pair:",
        "They opposed the building of reserve funds.",
        "• Word segmentation and pos-tagging: M]/R M/V ||al/V ^#^/N .",
        "/P",
        "• Word alignment:",
        "• The checkpoints in table 3 are extracted:",
        "To extract the categories of checkpoints of different schema of syntactic analysis such as constitute structure and dependency structure, three parsers including a Chinese skeleton parser (a kind of dependency parser) (Zhou, 2000), Stanford statistical parser and Berkeley statistical parser (Klein 2003) are used to parse the Chinese and English sentences.",
        "As explained in next section, these multiple parsers are also used to select high confident check-points.",
        "To get word alignment, an existing tool GIZA++ (Och 2003) is used.",
        "The reliability of the checkpoints mainly depends on the accuracy of the parsers.",
        "We can achieve high quality word level checkpoints with the state-of-the-art POS tagger (94% precision) and dictionaries of various purposes.",
        "For sentence level categories, the parser tags and manually compiled rules can also achieve 95% accuracy.",
        "For some kinds of categories at phrase level which parsers cannot produce high accuracy, we only select the checkpoints which can be identified by multiple parsers, that is, adopt the intersection of the parsers results.",
        "Table 4 shows the improvement brought by this approach.",
        "Column 1 and 2 shows the precision of 6 major types of phrases in Stanford and Berkeley parser.",
        "Column 3 shows the precision of intersection and column 4 shows the reduction of the number of checkpoints when adopting the intersection results.",
        "The test corpus is a part of",
        "Word level",
        "Ambiguous word",
        "New word",
        "Idiom",
        "Noun",
        "Verb",
        "Adjective",
        "Pronoun",
        "Adverb",
        "Preposition",
        "Quantifier",
        "Repetitive word",
        "Collocation",
        "Phrase level",
        "Subject-predicate phrase",
        "Predicate-object phrase",
        "Preposition-object phrase",
        "Measure phrase",
        "Location phrase",
        "Sentence level",
        "BA sentence",
        "BEI sentence",
        "SHI sentence",
        "YOU sentence",
        "Compound sentence",
        "Table 1: Chinese checkpoint taxonomy",
        "Word level",
        "Noun",
        "Verb (with Tense)",
        "Modal verb",
        "Adjective",
        "Adverb",
        "Pronoun",
        "Preposition",
        "Ambiguous word",
        "Plurality",
        "Possessive",
        "Comparative & Superlative degree",
        "Phrase level",
        "Noun phrase",
        "Verb phrase",
        "Adjective phrase",
        "Adverb phrase",
        "Preposition phrase",
        "Sentence level",
        "Attribute clause",
        "Adverbial clause",
        "Noun clause",
        "Hyperbaton |",
        "Category",
        "Check-point",
        "Reference",
        "New word",
        "reserve funds",
        "Ambiguous word",
        "Sal",
        "building",
        "Predicate - object phrase",
        "the building of reserve funds",
        "Subject-predicate phrase",
        "They opposed",
        "Penn Chinese Treebank which is not contained in the training corpus of two statistical parsers.",
        "(Klein 2003).",
        "Except for sentence level checkpoints whose references are the whole sentences and New Word, Idiom checkpoints whose references are extracted from dictionary, the quality of the references are impacted by the alignment accuracy.",
        "To alleviate the noise of aligner we use the lexical dictionary to check the reliability of references.",
        "Suppose c is a check-point, for each reference c.r of c we calculate the dictionary matching degree DM(c.r) with the source side c.s of c:",
        "WordCnt (c.r)",
        "Where Dic(x) is a word bag contains all words in the dictionary translations of each source word in x. CoCnt(x, y) is the count of the common words in x and y. WordCnt(x) is the count of words in x.",
        "Specially, if c.r is not obtained based on alignment DM(c.r) will be 1.",
        "Because the limitation of dictionary, a zero DM score not always means the reference is completely wrong, so we force the DM score to be not less than a minimum value (e.g. 0.1).",
        "DM score will further be used in evaluation in section 5.",
        "To better understand the reliability of the references and explore whether increasing the number of checkpoints could also alleviate the impact of noise, we built two checkpoint databases from a human-aligned corpus (with 60,000 sentence pairs) and an automatically aligned corpus (using GIZA++) respectively and tested 10 different SMT systems with them.",
        "The Spearman correlation is calculated between two ranked lists of the 10 evaluation results against the two databases.",
        "A higher correlation score means that the impact of the mistakes in word alignment is weaker.",
        "The experiment is repeated on 6 subsets of the database with the size from 500 sentences to 16K sentences to check the impact of the corpus size.",
        "At system level, high correlations are found at different corpus sizes.",
        "At category level, correlations are found to be low for some categories at small size and become higher at larger corpus size.",
        "The results indicate that the impact of the alignment quality can be ignored if the corpus size is at large scale.",
        "As the checkpoints can be extracted fully automatically, increasing the size of checkpoint database will not bring extra cost and efforts.",
        "Empirically, the proper scale is set to be 2000 or more sentences according to the Table 6.",
        "word alignment at different of test corpus."
      ]
    },
    {
      "heading": "5. Matching Check-Points for Evaluation",
      "text": [
        "Evaluation can be carried out at multiple options: for certain linguistic category, a group of categories or entire taxonomy.",
        "For instance, in Chinese-English translation task, if a MT developer would like to know the ability to translate idiom, then a number of parallel sentences containing idiom checkpoints are selected from the database.",
        "Then the system translation sentences are matched to the references of the checkpoints of idioms.",
        "Stf%",
        "Brk%",
        "Inter%",
        "Tpts redu%",
        "NP",
        "87.37",
        "86.03",
        "95.83",
        "17.06",
        "VP",
        "87.34",
        "82.87",
        "95.23",
        "19.68",
        "PP",
        "90.60",
        "88.56",
        "96.00",
        "11.50",
        "QP",
        "98.12",
        "92.90",
        "99.21",
        "6.31",
        "ADJP",
        "91.95",
        "90.87",
        "96.41",
        "10.20",
        "ADVP",
        "95.21",
        "94.25",
        "92.64",
        "3.92",
        "500",
        "1K",
        "2K",
        "4K",
        "8K",
        "16K",
        "Ambiguous",
        "word",
        "0.98",
        "0.98",
        "0.98",
        "0.98",
        "0.96",
        "0.98",
        "Noun",
        "0.93",
        "0.99",
        "0.99",
        "0.89",
        "0.8",
        "0.86",
        "Verb",
        "0.97",
        "0.97",
        "0.99",
        "0.99",
        "0.95",
        "0.92",
        "Adjective",
        "0.16",
        "0.19",
        "0.57",
        "0.75",
        "0.77",
        "0.97",
        "Pronoun",
        "0.96",
        "1",
        "0.93",
        "0.99",
        "0.97",
        "0.99",
        "Adverb",
        "0.38",
        "0.77",
        "0.8",
        "0.96",
        "0.72",
        "0.84",
        "Preposition",
        "0.56",
        "0.86",
        "0.9",
        "0.9",
        "0.97",
        "0.96",
        "Quantifier",
        "1",
        "0.46",
        "0.46",
        "0.98",
        "0.85",
        "0.96",
        "Repetitive Word",
        "0.99",
        "0.99",
        "0.97",
        "0.89",
        "0.73",
        "0.95",
        "Collocation",
        "0.42",
        "0.77",
        "0.77",
        "0.77",
        "0.73",
        "0.88",
        "Subject-predicate phrase",
        "0.06",
        "0.8",
        "0.95",
        "1",
        "0.96",
        "0.84",
        "Predicate-object phrase",
        "0.84",
        "0.96",
        "0.78",
        "0.7",
        "0.78",
        "0.88",
        "Preposition-object phrase",
        "0.51",
        "0.5",
        "0.93",
        "0.95",
        "0.87",
        "0.99",
        "Measure phrase",
        "0.91",
        "0.67",
        "0.95",
        "0.95",
        "0.87",
        "0.97",
        "Location phrase",
        "0.62",
        "0.54",
        "0.55",
        "0.55",
        "0.85",
        "0.89",
        "SYSTEM",
        "0.95",
        "0.95",
        "0.98",
        "0.99",
        "0.97",
        "0.98",
        "To calculate the credit at different occasions of matching, similar to BLEU, we split each reference of a checkpoint into a set of n-grams and sum up the gains over all grams as the credit of this check-point.",
        "Especially, if the checkpoint is not consecutive we use a special token (e.g. \"*\") to represent a component which can be wildcard matched by any word sequence.",
        "We use the following examples to demonstrate the splitting and matching of grams.",
        "• Consecutive check-point: Check point: ^ktiM.",
        "Reference: playing a drum",
        "Candidate translation: He is playing a drum.",
        "Matched n-grams: playing; a; drum; playing a; a drum; playing a drum",
        "• Not consecutive check-point: Check point: jM]*fr Reference: They*playing",
        "Candidate translation: They are playing cop per drum.",
        "Matched n-grams: They; playing; They * playing",
        "Additionally, to match word inflections, 3 different options of matching granularity are defined as follows.",
        "• Normal: matching with exact form.",
        "• Lower-case: matching with lowercase.",
        "• Stem: matching with the stem of the word.",
        "For a checkpoint c and references set R of c, we select the r in R which matches the translation best based on formula (2).",
        "y Match(n - gram)",
        "rer y Count(n - gram')",
        "When we calculate the recall of a set of checkpoints C (C can be a single check-point, a category or a category group), r of each checkpoint c in C are merged into one reference set R and the recall of C is obtained using formula (3) on R*.",
        "A penalty is also introduced to punish the redundancy of candidate sentences, where length(T) is the average length of all translation sentences and length(R) is the average length of all reference sentences."
      ]
    },
    {
      "heading": "1. Otherwise",
      "text": [
        "Then, the final score of C will be: Score (C) = Re (C) • Penalty (5)"
      ]
    },
    {
      "heading": "6. Experiments on MT System Diagnoses",
      "text": [
        "In this section, to demonstrate the ability of our approach in the diagnoses of MT systems, we apply diagnostic evaluation to 3 statistical MT (SMT) systems and a rule-based MT (RMT) system respectively.",
        "We compare two SMT systems to understand the strength and shortcoming of each of them, and also compare a SMT system with the RMT system.",
        "The test corpus is NIST05 test data with 54852 check-points.",
        "First SMT system (system A) is an implementation of classical phrase based SMT.",
        "The second SMT system (system B) shares the same decoder with system A and introduces a preprocess to reorder the long phrases in source sentences according to the syntax structure before decoding (Chiho Li et al., 2007).",
        "The third SMT system (system C) is a popular internet service and the RMT system (system D) is a popular commercial system.",
        "In the first experiment, we diagnose the system A and B and compare the results as shown in table 7.",
        "When evaluated using BLEU, system B achieved a 0.005 points increase on top of system A which is not a very significant difference.",
        "The diagnostic results in table 7 provide much richer information.",
        "The results indicate that two systems perform similar at the word level categories while at all phrase level categories, system B performs better.",
        "This result reflects the benefit from the reordering of complex phrases in system B. Paired t-statistic score for each pair of category scores is also calculated by repeating the evaluation on a random copy of the test set with replacement (Koehn 2004).",
        "An absolute score beyond 2.17 of paired t-statistic means the difference of the samples is statistically significant (above 95%).",
        "Table 8 and 9 show an instance of the checkpoint and its evaluation in this experiment.",
        "In the second experiment, we diagnose system C and D and compare the results.",
        "The BLEU score of system C is 0.3005 and system D is 0.2606.",
        "Table 10 shows the diagnostic results on categories with significant differences.",
        "Scores calculated with 3 matching options described in section 5 are given (\"Lower\" means Lowercase.",
        "The scores are listed in the form \"SMT score/RMT score\").",
        "The diagnostic results indicate that system C performs better on most categories than system D, but system D performs better on categories like idiom, pronoun and preposition.",
        "This result reveals a key difference between two types of MT systems: the SMT works well on the open categories that can be handled by context, while the RMT works well on closed categories which are easily translated by linguistic rules.",
        "As the results of two experiments demonstrate, the diagnostic evaluation provides rich information of the capability of translating various important linguistic categories beyond a single system score.",
        "It successfully distinguishes the specific difference between the MT systems whose system level performance is similar.",
        "It can also diagnose the MT system with different architectures.",
        "Diagnostic evaluation tells the developers about the direction to improve the system.",
        "Along with the scores of categories, the diagnostic evaluation provides the system translation and references at every checkpoint so that the developers can trace and understand about how the MT system works on every single instance."
      ]
    },
    {
      "heading": "7. Experiments on Ranking MT Systems",
      "text": [
        "Offering a general evaluation at system level is the major goal of state-of-the-art evaluation methods including widely accepted n-gram metrics.",
        "The absence of linguistic knowledge in BLEU motivated many work to integrate linguistic features into evaluation metric.",
        "In (Yang 2007), the evaluation of SMT systems is alternately formulated as a ranking problem.",
        "Different linguistic features are combined with BLEU such as matching rate of dependency relations of translation candidates against the reference sentences.",
        "The experiments demonstrate that the dependency matching rate feature can increase the ranking accuracy in some cases.",
        "Compared to dependency structure, the linguistic categories in our approach showcase more extensive features.",
        "It would be interesting to see whether the linguistic categories can be used to further improve the ranking of SMT systems.",
        "In experiments, we use the scores of linguistic categories, dependency matching rate, scores of BLEU and other popular metrics as ranking features of MT systems and trained by Ranking SVM of SVMlight (Joachims, 1998).",
        "We performed the ranking experiments on ACL 2005 workshop data, ranking 7 MT translations with threefold cross-validation both on sentence level and document level.",
        "The Spearman score is used to calculate the correlation with human assessments.",
        "Table 11 and 12 show the results of the different feature sets on sentence level and document level respectively.",
        "1 System A",
        "System B",
        "1 T score |",
        "WORDs 1",
        "Idiom",
        "0.1933",
        "0.2370",
        "13.38",
        "Adjective",
        "0.5836",
        "0.5577",
        "-17.43",
        "Pronoun",
        "0.7566",
        "0.7344",
        "-13.49",
        "Adverb",
        "0.5365",
        "0.5433",
        "7.11",
        "Preposition",
        "0.6529",
        "0.6456",
        "-6.21",
        "Repetitive word",
        "0.3363",
        "0.3958",
        "9.86",
        "PHRASEs 1",
        "Subject-predicate",
        "0.5117",
        "0.5206",
        "7.36",
        "Predicate-object",
        "0.4041",
        "0.4180",
        "15.52",
        "Predicate-complement",
        "0.4409",
        "0.5125",
        "9.51",
        "Measure phrase",
        "0.5030",
        "0.5092",
        "3.56",
        "Location phrase",
        "0.5245",
        "0.5338",
        "2.83",
        "GROUPs I",
        "WORDs",
        "0.4839",
        "0.4855",
        "2.03",
        "PHRASEs",
        "0.4744",
        "0.4964",
        "13.97",
        "SYSTEM (Linguistic)",
        "0.4263",
        "0.4370",
        "16.50",
        "SYSTEM (BLEU)",
        "0.3564",
        "0.3614",
        "7.91",
        "Source Sentence",
        "Category",
        "Preposition Object Phrase",
        "Check-Point",
        "Reference 1",
        "in this country DM = 0.5",
        "Reference 2",
        "in his country DM = 0.5",
        "System A Translation",
        "but the prime minister of thailand Dex-in vowed to continue in domestic the search.",
        "System B Translation",
        "but the prime minister of thailand Dex-in vowed to continue the search in his country.",
        "Table 8: An instance of the check-point.",
        "System A",
        "System B",
        "Ref 1: Match/Total",
        "1/6",
        "2/6",
        "Ref 2: Match/Total",
        "1/6",
        "6/6",
        "Score",
        "0.17",
        "1 1",
        "Table 9: N-gram matching rate and scores.",
        "Type",
        "Normal",
        "Lower",
        "Stem",
        "Ambiguous word",
        "0.49/0.42",
        "0.50/0.42",
        "0.53/0.46",
        "New word",
        "0.13/0.13",
        "0.37/0.32",
        "0.42/0.35",
        "Idiom",
        "0.43/0.66",
        "0.46/0.67",
        "0.51/0.71",
        "Pronoun",
        "0.60/0.68",
        "0.69/0.75",
        "0.66/0.75",
        "Preposition",
        "0.38/0.42",
        "0.42/0.45",
        "0.43/0.46",
        "Collocation",
        "0.66/0.54",
        "0.66/0.55",
        "0.70/0.56",
        "Subject-predicate phrase",
        "0.46/0.30",
        "0.51/0.36",
        "0.58/0.42",
        "Predicate-object phrase",
        "0.37/0.25",
        "0.37/0.26",
        "0.47/0.29",
        "Compound sentence",
        "0.22/0.16",
        "0.23/0.16",
        "0.23/0.17",
        "As shown in experiment results linguistic categories (LC), when used alone, are better related with human assessments than BLEU and GTM.",
        "When combined with the baseline metrics (BLEU & NIST), LC scores further improve the correlation score, better than dependence matching rate (DP).",
        "LC scores are obtained by matching the exact form of the words as ME-TEOR(exact) does.",
        "NIST+LC combination score is better than METEOR(exact) at sentence and document level, and also better than ME-TEOR(exact&syn) (syn means wnsynonymy module in METEOR) at document level.",
        "This results indicate the ability of linguistic features in improving the performance of ranking task."
      ]
    },
    {
      "heading": "8. Comparison with Related Work",
      "text": [
        "This work is inspired by (Yu, 1993) with many extensions.",
        "(Yu, 1993) proposed MTE evaluation system based on checkpoints for English-Chinese machine translation systems with human craft linguistic taxonomy including 3,200 pairs of sentences containing 6 classes of check-points.",
        "Their checkpoints were manually constructed by human experts, therefore it will be costly to build new test corpus while the checkpoints in our approach are constructed automatically.",
        "Another limitation of their work is that only binary score is used for credits while we use n-gram matching rate which provides a broader coverage of different levels of matching.",
        "There are many recent work motivated by n-gram based approach.",
        "(Callison-Burch et al., 2006) criticized the inadequate accuracy of evaluation at the sentence level.",
        "(Lin and Och, 2004) used longest common subsequence and skip-bigram statistics.",
        "(Banerjee and Lavie, 2005) calculated the scores by matching the unigrams on the surface forms, stemmed forms and senses.",
        "(Liu et al., 2005) used syntactic features and un-labeled head-modifier dependencies to evaluate MT quality, outperforming BLEU on sentence level correlations with human judgment.",
        "(Gime-nez and Marquez, 2007) showed that linguistic features at more abstract levels such as dependency relation may provide more reliable system rankings.",
        "(Yang et al., 2007) formulates MT evaluation as a ranking problems leading to greater correlation with human assessment at the sentence level.",
        "There are many differences between these n-gram based methods and our approach.",
        "In n-gram approach, a sentence is viewed as a collection of n-grams with different length without differentiating the specific linguistic phenomena.",
        "In our approach, a sentence is viewed as a collection of checkpoints with different types and depth, conforming to a clear linguistic taxonomy.",
        "Furthermore, in n-gram approach, only one general score at the system level is provided which make it not suitable for system diagnoses, while in our approach we can give scores of linguistic categories and provide much richer information to help developers to find the concrete strength and flaws of the system, in addition to the general score.",
        "The n-gram based metric is not very effective when comparing the systems with different architectures or systems with similar general score, while our approach is more effective in both cases by digging into the multiple linguistic levels and disclosing the latent differences of the systems."
      ]
    },
    {
      "heading": "9. Conclusion and Future Work",
      "text": [
        "This paper presents an automatically diagnostic evaluation methods on MT based on linguistic checkpoints automatically constructed.",
        "In contrast with the metrics which only give a general score, our evaluation system can give developers feedback about the faults and strength of an MT system regarding specific linguistic category or category group.",
        "Different with the existing work based on check-points, our work presents an approach to automatically generate the checkpoint database.",
        "We show that although there is some noise brought from word alignment and parsing, we can effectively alleviate the problem by refining the parser results, weighting the reference with confidence score and providing large quantity of check-points.",
        "| Mean Correlation |",
        "BLEU 4",
        "0.245",
        "NIST 5",
        "0.307",
        "GTM (e=2)",
        "0.251",
        "METEOR(exact)",
        "0.306",
        "METEOR(exact&syn)",
        "0.327",
        "DP",
        "0.246",
        "LC",
        "0.263",
        "BLEU+DP",
        "0.270",
        "BLEU+ LC",
        "0.288",
        "BLEU+ DP +LC",
        "0.307",
        "NIST+ LC",
        "0.322",
        "NIST+ DP +LC",
        "0.333",
        "Table 11: Sentence level ranking (DP means dependency and LC means linguistic categories)",
        "| | Mean Correlation |",
        "BLEU 4",
        "0.305",
        "NIST 5",
        "0.373",
        "GTM (e=2)",
        "0.327",
        "METEOR(exact)",
        "0.363",
        "METEOR(exact&syn)",
        "0.394",
        "DP",
        "0.323",
        "LC",
        "0.369",
        "BLEU+DP",
        "0.325",
        "BLEU+ LC",
        "0.387",
        "BLEU+ DP +LC",
        "0.332",
        "NIST+ LC",
        "0.409",
        "NIST+ DP +LC",
        "0.359",
        "Table 12: Document level ranking",
        "The experiments demonstrate that this method can uncover the specific difference between MT systems with similar architectures and different architectures.",
        "It is also demonstrated that the linguistic checkpoints can be used as new features to improve the ranking task of MT systems.",
        "Although we present the diagnostic evaluation method with Chinese-English language pair, our approach can be applied to other language pair if syntax parser and word aligner are available.",
        "The taxonomy used in current proposal is based on the human-made linguistic system.",
        "An interesting problem to be explored in the future is whether the taxonomy could be constructed automatically from the parsing results."
      ]
    }
  ]
}
