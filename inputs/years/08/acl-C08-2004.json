{
  "info": {
    "authors": [
      "Mohit Bansal",
      "Claire Cardie",
      "Lillian Lee"
    ],
    "book": "COLING – Posters",
    "id": "acl-C08-2004",
    "title": "The Power of Negative Thinking: Exploiting Label Disagreement in the Min-cut Classification Framework",
    "url": "https://aclweb.org/anthology/C08-2004",
    "year": 2008
  },
  "references": [
    "acl-H05-1042",
    "acl-P04-1035"
  ],
  "sections": [
    {
      "text": [
        "Treating classification as seeking minimum cuts in the appropriate graph has proven effective in a number of applications.",
        "The power of this approach lies in its ability to incorporate label-agreement preferences among pairs of instances in a provably tractable way.",
        "Label disagreement preferences are another potentially rich source of information, but prior NLP work within the minimum-cut paradigm has not explicitly incorporated it.",
        "Here, we report on work in progress that examines several novel heuristics for incorporating such information.",
        "Our results, produced within the context of a politically-oriented sentiment-classification task, demonstrate that these heuristics allow for the addition of label-disagreement information in a way that improves classification accuracy while preserving the efficiency guarantees of the minimum-cut framework."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Classification algorithms based on formulating the classification task as one of finding minimum s-t cuts in edge-weighted graphs – henceforth minimum cuts or min cuts – have been successfully employed in vision, computational biology, and natural language processing.",
        "Within NLP, applications include sentiment-analysis problems (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006) and content selection for text generation (Barzilay and Lapata, 2005).",
        "As a classification framework, the minimum-cut approach is quite attractive.",
        "First, it provides a principled, yet flexible mechanism for allowing problem-specific relational information – including several types of both hard and soft constraints – to influence a collection of classification decisions.",
        "Second, in many important cases, such as when all the edge weights are non-negative, finding a minimum cut can be done in a provably efficient manner.",
        "To date, however, researchers have restricted the semantics of the constraints mentioned above to encode pairwise \"agreement\" information only.",
        "There is a computational reason for this restriction: \"agreement\" and \"disagreement\" information are arguably most naturally expressed via positive and negative edge weights, respectively; but in general, the inclusion of even a relatively small number of negative edge weights makes finding a minimum cutNP-hard (McCormick et al., 2003).",
        "To avoid this computational issue, we propose several heuristics that encode disagreement information with non-negative edge weights.",
        "We instantiate our approach on a sentiment-polarity classification task – determining whether individual conversational turns in U.S. Congressional floor debates support or oppose some given legislation.",
        "Our preliminary results demonstrate promising improvements over the prior work of Thomas et al. (2006), who considered only the use of agreement information in this domain."
      ]
    },
    {
      "heading": "2. Method",
      "text": [
        "Binary classification problems are usually approached by considering each classification decision in isolation.",
        "More formally, let Xtest =",
        "{x\\, X2, ■ ■ •, xn} be the test instances, drawn from some universe X, and let C = {01,02} be the two possible classes.",
        "Then, the usual approach can often be framed as labeling each Xi according to some individual-preference function Ind:XxC^SR, such as the signed distance to the dividing hyperplane according to an SVM or the posterior class probability assigned by a Naive Bayes classifier.",
        "But when it is difficult to accurately classify a particular x% in isolation, there is a key insight that can help: knowing that Xi has the same label as an easily-categorized Xj makes labeling Xi easy.",
        "Thus, suppose we also have an association-preference function Assoc: IxX->S expressing a reward for placing two items in the same class; an example might be the output of an agreement classifier or a similarity function.",
        "Then, we can search for a classification function o(xi\\Xtest) – note that all of Xtest can affect an instance's label – that minimizes the total \"pining\" of the test items for the class they were not assigned to due to either their individual or associational preferences:",
        "where c(xi\\Xtest) is the class \"opposite\" to c(xi\\Xtest), and the free parameter a regulates the emphasis on agreement information.",
        "Solutions to the above minimization problem correspond to minimum s-t cuts in a certain graph, and 7/both Ind and Assoc are non-negative functions, then, surprisingly, minimum cuts can be found in polynomial time; see Kleinberg and Tardos (2006, Section 7.10) for details.",
        "But, as mentioned above, allowing negative values makes finding a solution intractable in the general case.",
        "The starting point for our work is Thomas et al.",
        "(2006) (henceforth TPL).",
        "The reason for this choice is that TPL used minimum-cut-based classification wherein signed distances to dividing SVM hyperplanes were employed to define Ind (a;, c) and Assoc(a;, x').",
        "It was natural to use SVMs, since association was determined by classification rather than similarity – specifically, categorizing references by one congressperson to another as reflecting agreement or not – but as a result, negative association-preferences (e.g., negative distance to a hyperplane) had to be accounted for.",
        "We formalize TPL's approach at a high level as follows.",
        "Let Ind':IxC-»3f and Assoc':IxI^S be initial individual-and association-preference functions, such as the signed distances mentioned above.",
        "TPL create two non-negative conversion functions /: 3?",
        " – > [0,1] and g: 3?",
        " – > [0,1], and then define so that an optimal classification can be found in polynomial time, as discussed above.",
        "We omit the exact definitions of / and g in order to focus on what is important here: roughly speaking, / and g normalize values and handle outliers, with the following crucial exception.",
        "While negative initial individual preferences for one class can be translated into positive individual preferences for the other, there is no such mechanism for negative values of Assoc'; so TPL resort to defining g to be 0 for negative arguments.",
        "They thus discard potentially key information regarding the strength of label disagreement preferences.",
        "Instead of discarding the potentially crucial label-disagreement information represented by negative Assoc' values, we propose heuristics that seek to incorporate this valuable information, but that keep Ind and Assoc non-negative (by piggybacking off of TPL's pre-existing conversion-function strategy) to preserve the min-cut-classification efficiency guarantees.",
        "We illustrate our heuristics with a running example.",
        "Consider a simplified setting with only two instances x\\ and X2, f(z) = z; g(z) = 0 if z < 0, 1 otherwise; and Ind' values (numbers labeling left or right arrows in the diagrams below, e.g., Ind'(a;i,ci) = .7) and Assoc' value (the 2 labeling the up-and-down arrow) as depicted here:",
        "Then, the resulting TPL Ind and Assoc values are",
        "'Thus, strictly speaking, / and g also depend on Ind', Assoc', and Xt£St, but we suppress this dependence for notational compactness.",
        "- [-7]-",
        "Xi",
        ".3]-",
        "Cl",
        "$[-2]",
        "02",
        "- [-6]-",
        "X2 ~",
        ".4]-",
        "Note that since the initial 2 association value is ignored, c(xi\\Xtest) = c(x2\\Xtest) = ci appears to be a good classification according to TPL.",
        "The Scale all up heuristic Rather than discard disagreement information, a simple strategy is to just scale up all initial association preferences by a sufficiently large positive constant N:",
        "This heuristic ensures that the more negative the Assoc' value, the lower the cost of separating the relevant item pair (whereas TPL don't distinguish between negative Assoc' values).",
        "The heuristic below tries to be more proactive, forcing such pairs to receive different labels.",
        "The SetTo heuristic We proceed through Xi,X2,... in order.",
        "Each time we encounter an Xi where Assoc'(xi,Xj) < 0 for some j > i, we try to force x% and Xj into different classes by altering the four relevant individual-preferences affecting this pair of instances, namely, f(lnd'(xi,c{)), f(lnd'(xi,c2)), f(lnd'(xj,c{)), and f(lnd'(xj,c2)).",
        "Assume without loss of generality that the largest of these values is the first one.",
        "If we respect that preference to put Xi in c\\, then according to the association-preference information, it follows that we should put Xj in c2.",
        "We can instantiate this chain of reasoning by setting for some constant f3 G (.5,1], and making no change to TPL's definition of Assoc.",
        "For [3 = .8 in our example, we get",
        "Note that as we proceed through x\\,x2,--- in order, some earlier changes may be undone.",
        "The IncBy heuristic A more conservative version of the above heuristic is to increment and decrement the individual-preference values so that they are somewhat preserved, rather than completely replace them with fixed constants:",
        "For (3 = .1, our example becomes"
      ]
    },
    {
      "heading": "3. Evaluation",
      "text": [
        "For evaluation, we adopt the sentiment-classification problem tackled by TPL: classifying speech segments (individual conversational turns) in a U.S. Congressional floor debate as to whether they support or oppose the legislation under discussion.",
        "TPL describe many reasons why this is an important problem.",
        "For our purposes, this task is also very convenient because all of TPL's computed raw and converted Ind' and Assoc' data are publicly available at www.",
        "cs.",
        "Cornell.",
        "edu/home/llee/data/convote .html.",
        "Thus, we used their calculated values to implement our algorithms as well as to reproduce their original results.",
        "One issue of note is that TPL actually inferred association preferences between speakers, not speech segments.",
        "We do the same when applying SetTo or IncBy to a pair {xi,Xj} by considering the average of /(Ind'(a;fc, ci)) over all Xk uttered by the speaker of Xi, instead of just /(Ind'(o;j, ci)).",
        "The other three relevant individual values are treated similarly.",
        "We also make appropriate modifications (according to SetTo and IncBy) to the individual preferences of all such Xk simultaneously, not just Xi, and similarly for Xj.",
        "A related issue is that TPL assume that all speech segments by the same speaker should have the same label.",
        "To make experimental comparisons meaningful, we follow TPL in considering two different instantiations of this assumption.",
        "In segment-based classification, Assoc(xi,Xj) is set to an arbitrarily high positive constant if the same speaker uttered both x% and Xj.",
        "In speaker-based classification, lnd'(xi,c) is produced by running",
        "«-[•7]-",
        "X\\ – ",
        "[•3]-",
        "Cl",
        "$[0]",
        "C2",
        "-[•6]-",
        "X2 -",
        "lnd(xi, ci)",
        "= min(l,/(Ind'(^,Ci)) + /3)",
        "lnd(xi, c2)",
        "= max(0, f(lnd'(xi, c2)) - (3)",
        "lnd(xj, ci)",
        "= max(0, f(lrid'(xj,c\\)) – [3)",
        "lnd(xj,c2)",
        "= min(l,/(Ind'(av,c2))+/3)",
        "- [-8]-",
        "Xi -",
        ".2]-",
        "Cl",
        "C2",
        "- [-5]-",
        "x2 -",
        ".5]^",
        "«-[•7]-",
        "X\\ – ",
        "[.3]-",
        "Cl",
        "tin",
        "C2",
        "-[•6]-",
        "X2 -",
        "[.4]-",
        "lnd(xi,ci)",
        "= max(/3, f(lnd'(xi, ci)))",
        "lnd(xi,c2)",
        "= min(l - (3, /(Ind'^, c2)))",
        "lnd(xj,c\\)",
        "= min(l - (3, f(Ind'(xj,a)))",
        "lnd(xj,c2)",
        "= max(/3, f{\\iid'(xj, c2)))",
        "<-[.8]-",
        "X\\ – ",
        "[.2]-",
        "Cl",
        "C2",
        "<-[.2]-",
        "x2 -",
        "[•8]-",
        "Figure 1: Experimental results.",
        "\"SVM\": classification using only individual-preference information.",
        "Values of [3 are indicated in parentheses next to the relevant algorithm names.",
        "an SVM on the concatenation of all speeches uttered by Xi's speaker.",
        "Space limits preclude inclusion of further details; please see TPL for more information.",
        "The association-emphasis parameter a was trained on held-out data, with ties broken in favor of the largest a in order to emphasize association information.",
        "We used Andrew Goldberg's HIPR code (http://avglab.com/andrew/soft.html) to compute minimum cuts.",
        "The resultant test-set classification accuracies are presented in Figure 1.",
        "We see that Scale all up performs worse than TPL, but the more proactive heuristics (SetTo, IncBy) almost always outperform TPL on segment-based classification, sometimes substantially so, and outperform TPL on speaker-based classification for half of the variations.",
        "We therefore conclude that label disagreement information is indeed valuable; and that incorporating label disagreement information on top of the (positive) label agreement information that TPL leveraged can be achieved using simple heuristics; and that good performance enhancements result without any concomitant significant loss of efficiency.",
        "These results are preliminary, and the divergence in behaviors between different heuristics in different settings requires investigation.",
        "Additional future work includes investigating more sophisticated (but often therefore less tractable) formalisms for joint classification; and looking at whether approximation algorithms for finding minimum cuts in graphs with negative edge capacities can be effective.",
        "Acknowledgments We thank Jon Kleinberg and the reviewers for helpful comments.",
        "Portions of this work were done while the first author was visiting Cornell University.",
        "This paper is based upon work supported in part by the National Science Foundation under grant nos.",
        "IIS-0329064, BCS-0624277, and IIS-0535099, a Cornell University Provost's Award for Distinguished Scholarship, a Yahoo!",
        "Research Alliance gift, an Alfred P. Sloan Research Fellowship, and by DHS grant N0014-07-1-0152.",
        "Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity."
      ]
    }
  ]
}
