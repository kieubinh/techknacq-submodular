{
  "info": {
    "authors": [
      "Jun Hatori",
      "Yusuke Miyao",
      "Jun'ichi Tsujii"
    ],
    "book": "COLING – Posters",
    "id": "acl-C08-2011",
    "title": "Word Sense Disambiguation for All Words using Tree-Structured Conditional Random Fields",
    "url": "https://aclweb.org/anthology/C08-2011",
    "year": 2008
  },
  "references": [
    "acl-D07-1111",
    "acl-P05-1005",
    "acl-W02-1006",
    "acl-W04-0827",
    "acl-W04-0838",
    "acl-W06-1670",
    "acl-W07-2090"
  ],
  "sections": [
    {
      "text": [
        "We propose a supervised word sense disambiguation (WSD) method using tree-structured conditional random fields (TCRFs).",
        "By applying TCRFs to a sentence described as a dependency tree structure, we conduct WSD as a labeling problem on tree structures.",
        "To incorporate dependencies between word senses, we introduce a set of features on tree edges, in combination with coarse-grained tagsets, and show that these contribute to an improvement in WSD accuracy.",
        "We also show that the tree-structured model outperforms the linear-chain model.",
        "Experiments on the Senseval-3 data set show that our TCRF model performs comparably with state-of-the-art WSD systems."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Word sense disambiguation (WSD) is one of the fundamental underlying problems in computational linguistics.",
        "The task of WSD is to determine the appropriate sense for each polysemous word within a given text.",
        "Traditionally, there are two task settings for WSD: the lexical sample task, in which only one targeted word is disambiguated given its context, and the all-words task, in which all content words within a text are disambiguated.",
        "Whilst most of the WSD research so far has been toward the lexical sample task, the all-words task has received relatively less attention, suffering from a serious knowledge bottleneck problem.",
        "Since it is considered to be a necessary step toward practical applications, there is an urgent need to improve the performance of WSD systems that can handle the all-words task.",
        "In this paper, we propose a novel approach for the all-words task based on tree-structured conditional random fields (TCRFs).",
        "Our TCRF model incorporates the inter-word sense dependencies, in combination with wordnet hierarchical information and a coarse-grained tagset, namely super-senses, by which we can alleviate the data sparse -ness problem."
      ]
    },
    {
      "heading": "2. Background",
      "text": [
        "Since the all-words task requires us to disambiguate all content words, it seems reasonable to assume that we could perform better WSD by considering the sense dependencies among words, and optimizing word senses over the whole sentence.",
        "Specifically, we base our model on the assumption that there are strong sense dependencies between a head word and its dependents in a dependency tree; therefore, we employ the dependency tree structures for modeling the sense dependencies.",
        "There have been a few WSD systems that incorporate the inter-word sense dependencies (e.g. Mihalcea and Faruque (2004)).",
        "However, to the extent of our knowledge, their effectiveness has not explicitly examined thus far for supervised WSD.",
        "Supersense a supersense corresponds to the lexicographers' file ID in wordnet, with which each noun or verb synset is associated.",
        "Since they are originally introduced for ease of lexicographers' work, their classification is fairly general, but not too abstract, and is hence expected to act as good coarse-grained semantic categories.",
        "The numbers of the supersenses are 26 and 15 for nouns and verbs.",
        "The effectiveness of the use of supersenses and other coarse-grained tagsets for WSD has been recently shown by several researchers (e.g. Kohomban and Lee (2005), Cia-ramita and Altun (2006), and Mihalcea et al.",
        "(2007)).",
        "Sense number A sense number is the number of a sense of a word in WordNet.",
        "Since senses of a word are ordered according to frequency, the sense number can act as a powerful feature for WSD, which offers a preference for frequent senses, and especially as a back-off feature, which enables our model to output the first sense when no other feature is available for that word.",
        "Conditional Random Fields (CRFs) are graph-based probabilistic discriminative models proposed by Lafferty et al.",
        "(2001).",
        "Tree-structured CRFs (TCRFs) are different from widely used linear-chain CRFs, in that the probabilistic variables are organized in a tree structure rather than in a linear sequence.",
        "Therefore, we can consider them more appropriate for modeling the semantics of sentences, which cannot be represented by linear structures.",
        "Although TCRFs have not yet been applied to WSD, they have already been applied to some NLP tasks, such as semantic annotation (Tang et al., 2006), proving to be useful in modeling the semantic structure of a text.",
        "Formulation In CRFs, the conditional probability of a label set y for an observation sequence x is calculated by where E and V are the sets of edges and vertices, fj and gu are the feature vectors for an edge and a vertex, Xj and ^ WQ me weight vectors for them, and Z(x) is the normalization function.",
        "For a detailed description of TCRFs, see Tang et al.",
        "(2006).",
        "man confidence"
      ]
    },
    {
      "heading": "3. WSD Model using Tree-structured CRFs",
      "text": [
        "Let us consider the following sentence.",
        "(i) The man destroys confidence in banks.",
        "In the beginning, we parse a given sentence by using a dependency parser.",
        "The left-hand side of Figure 1 shows the dependency tree for Sentence (i) in the CoNLL-X dependency format.",
        "Next, we convert the outputted tree into a tree of content words, as illustrated in the right-hand side of Figure 1, since our WSD task does not focus on the disambiguation of function words.",
        "Finally, we conduct WSD as a labeling task on tree structures, by maximizing the probability of a tree of word senses, given scores for vertex and edge features.",
        "Using the information in wordnet, we define four sense labels for a word: a sense s i (v), a synset S2(v), a topmost synset ss(v), and a supersense s^{v).",
        "A topmost synset Ss(v) is the superordinate synset at the topmost level in the WordNet hierarchy, and note that a supersense S4 (v) is only available for nouns and verbs.",
        "We incorporate all these labels together into the vertex and edge features described in the following sections.",
        "Most of the vertex features we use are those used by Lee and Ng (2002).",
        "All these features are combined with each of the four sense labels sn(v), and incorporated as §k in Equation (1).",
        "• Word form, lemma, and part of speech.",
        "• Word forms, lemmas, and parts of speech of the head and dependents in a dependency tree.",
        "• Bag-of-words within 60-words window.",
        "• Parts-of-speech of neighboring six words.",
        "• Local n-gram within neighboring six words.",
        "Additionally, we include as a vertex feature the sense number, introduced in Section 2.2.",
        "For each edge, all possible sense bigrams (i.e. si(v)-si(v'),si(v)-s2(v'),- ■ • ,s4(v)-s4«)), and the combination of sense bigrams with dependency relation labels (e.g. 'SUB,' 'NMOD') and/or removed function words in between (e.g. 'of,' 'in') are defined as edge features, which correspond to fj in Equation (1)."
      ]
    },
    {
      "heading": "4. Experiment",
      "text": [
        "In the experiment, we use as our main evaluation data set the Brown-1 and Brown-2 sections of SemCor.",
        "The last files in the five largest categories in Brown-1 are used for development, and the rest of Brown-1 and all files in Brown-2 are alternately used for training and testing.",
        "We also use the Senseval-3 English all-words data (Snyder and Palmer, 2004) for testing, in order to compare the performance of our model with other systems.",
        "The statistics of the data sets are shown in Table 1.",
        "All sentences are parsed by the Sagae's dependency parser (Sagae and Tsujii, 2007), and the TCRF model is trained using Amis (Miyao and Tsujii, 2002).",
        "During the development phase, we tune the parameter of L2 regularization for CRFs.",
        "Note that, in all experiments, we try all content words annotated with WordNet synsets; therefore, the recalls are always equal to the precisions.",
        "First, we trained and evaluated our models on SemCor.",
        "Table 2 shows the overall performance of our models.",
        "Baseline model is the first sense baseline.",
        "No-Edge model uses only the vertex features, while each of the Sn-edge models makes use of the edge features associated with",
        "Table 3: The comparison of the performance of WSD systems evaluated on the Senseval-3 English all-words test set.",
        "a sense label sn, where n e {1,2,3,4}.",
        "The All-Edge model incorporates all possible combinations of sense labels.",
        "The only difference in the all-edge' model is that it omits features associated with dependency relation labels, so that we can compare the performance with the all-edge'(Linear) model, which is based on the linear-chain model.",
        "In the experiment, all models with one or more edge features outperformed both the No-Edge and Baseline model.",
        "The All-Edge model achieved 75.78% and 77.49% recalls for the two data sets, with 0.41% and 0.43% improvements over the no-edge model.",
        "By the stratified shuffling test (Cohen, 1995), these differences are shown to be statistically significant, with the exception of S3-edge model.",
        "Also, the tree-structured model all-edge' is shown to outperform the linear-chain model all-edge'(Linear) by 0.13% for both data sets (p = 0.013, 0.006).",
        "Finally, we trained our models on the Brown-1 and Brown-2 sections, and evaluated them on the Senseval-3 English all-words task data.",
        "Table 3 shows the comparison of our model with the state-of-the-art WSD systems.",
        "Considering the difference in the amount of training data, we can conclude that the performance of our TCRF model is comparable to state-of-the-art WSD systems, for all systems in Table 3 other than Simil-Prime (Kohomban and Lee, 2005) utilizes other sense-annotated data, such as the Senseval data sets and example sentences in WordNet.",
        "#sentences",
        "#words",
        "Development",
        "470",
        "5,178",
        "Brown-1",
        "10,712",
        "100,804",
        "Brown-2",
        "8,956",
        "85,481",
        "Senseval-3",
        "300",
        "2,081",
        "System",
        "Recall",
        "PNNL (Tratz et al„ 2007)",
        "67.0%",
        "Simil-Prime (Kohomban and Lee, 2005)",
        "66.1%",
        "All-Edge",
        "65.5%",
        "Gambl (Decadt et al., 2004)",
        "65.2%",
        "SenseLearner (Mihalcea et al.,2004)",
        "64.6%",
        "Baseline",
        "62.2%",
        "Table 2: The performance of our system trained and evaluated on SemCor.",
        "The statistical significance of the improvement over No-Edge model is shown in the 'Offset' fields, where '»,' '>,' and '~' denote p < 0.01, p < 0.05, andp > 0.05, respectively."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, we proposed a novel approach for the all-words WSD based on TCRFs.",
        "Our proposals are twofold: one is to apply tree-structured CRFs to dependency trees, and the other is to use bigrams of fine-and coarse-grained senses as edge features.",
        "In our experiment, the sense dependency features are shown to improve the WSD accuracy.",
        "Since the combination with coarse-grained tagsets are also proved to be effective, they can be used to alleviate the data sparseness problem.",
        "Moreover, we explicitly proved that the tree-structured model outperforms the linear-chain model, indicating that dependency trees are more appropriate for representing semantic dependencies.",
        "Although our model is based on a simple framework, its performance is comparable to state-of-the-art WSD systems.",
        "Since we can use additionally other sense-annotated resources and sophisticated machine learning techniques, our model still has a great potential for improvement.",
        "Training set",
        "Brown-1",
        "Brown-2",
        "Testing set",
        "Brown-2",
        "Brown-1",
        "Model",
        "Recall",
        "Offset",
        "#correct",
        "Recall",
        "Offset",
        "#correct",
        "All-Edge'",
        "75.77%",
        "0.40%",
        ">",
        "64766/85481",
        "77.45%",
        "0.39%",
        ">",
        "78077/100804",
        "All-Edge' (Linear)",
        "75.64%",
        "0.27%",
        ">",
        "64662/85481",
        "77.32%",
        "0.26%",
        ">",
        "77944/100804",
        "All-Edge",
        "75.78%",
        "0.41%",
        ">",
        "64779/85481",
        "77.49%",
        "0.43%",
        ">",
        "78114/100804",
        "S4-edge",
        "75.46%",
        "0.09%",
        ">",
        "64507/85481",
        "77.15%",
        "0.09%",
        ">",
        "77769/100804",
        "S3-edge",
        "75.40%",
        "0.03%",
        "64452/85481",
        "77.13%",
        "0.07%",
        ">",
        "77750/100804",
        "S2-edge",
        "75.45%",
        "0.08%",
        ">",
        "64494/85481",
        "77.12%",
        "0.06%",
        ">",
        "77738/100804",
        "SI-edge",
        "75.44%",
        "0.07%",
        ">",
        "64491/85481",
        "77.10%",
        "0.04%",
        ">",
        "77724/100804",
        "No-Edge",
        "75.37%",
        "0.00%",
        "64427/85481",
        "77.06%",
        "0.00%",
        "77677/100804",
        "Baseline",
        "74.36%",
        "63567/85481",
        "75.91%",
        "76524/100804"
      ]
    }
  ]
}
