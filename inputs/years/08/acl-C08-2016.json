{
  "info": {
    "authors": [
      "Yusuke Miyao",
      "Jun'ichi Tsujii"
    ],
    "book": "COLING â€“ Posters",
    "id": "acl-C08-2016",
    "title": "Exact Inference for Multi-label Classification using Sparse Graphical Models",
    "url": "https://aclweb.org/anthology/C08-2016",
    "year": 2008
  },
  "references": [
    "acl-J96-1002",
    "acl-W02-1001",
    "acl-W07-1013",
    "acl-W07-1017"
  ],
  "sections": [
    {
      "text": [
        "This paper describes a parameter estimation method for multi-label classification that does not rely on approximate inference.",
        "It is known that multi-label classification involving label correlation features is intractable, because the graphical model for this problem is a complete graph.",
        "Our solution is to exploit the sparsity of features, and express a model structure for each object by using a sparse graph.",
        "We can thereby apply the junction tree algorithm, allowing for efficient exact inference on sparse graphs.",
        "Experiments on three data sets for text categorization demonstrated that our method increases the accuracy for text categorization with a reasonable cost."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "This paper describes an exact inference method for multi-label classification (Schapire and Singer, 2000; Ghamrawi and McCallum, 2005), into which label correlation features are incorporated.",
        "In general, directly solving this problem is computationally intractable, because the graphical model for this problem is a complete graph.",
        "Nevertheless, an important characteristic of this problem, in particular for text categorization, is that only a limited number of features are active; i.e., nonzero, for a given object x.",
        "This sparsity of features is a desirable characteristic, because we can remove the edges of the graphical model when no corresponding features are active.",
        "We can therefore expect that a graphical model for each object is a sparse graph.",
        "When a graph is sparse, we can apply the junction tree algorithm (Cowell et al., 1999), allowing for efficient exact inference on sparse graphs.",
        "Our method is evaluated on three data sets for text categorization; one is from clinical texts, and the others are from newswire articles.",
        "We observe the trade-off between accuracy and training cost, while changing the number of label correlation features to be included."
      ]
    },
    {
      "heading": "2. Multi-label Classification",
      "text": [
        "Given a set of labels, L = {li,..., 1\\l\\}, multi-label classification is the task of assigning a subset y C L to a document x.",
        "In the framework of statistical machine learning, this problem can be formulated as a problem of maximizing a scoring function rj:",
        "As is usually the case in statistical machine learning, we represent a probabilistic event, (x,y), with a feature vector, f(x,y) = (fi(x,y),...,f\\f\\(x,y)).",
        "In text categorization, most effective features represent a frequency of a word w in a document; i.e., if / g y, otherwise, where cx(w) is a frequency ofw in x.",
        "The most popular method for multi-label classification is to create \\L\\ binary classifiers, each of which determines whether or not to assign a single label (Yang and Pedersen, 1997).",
        "However, since the decision for each label is independent of the decision for other labels, this method cannot be sensitive to label correlations, or the tendency of label cooccurrences.",
        "A recent research effort has been devoted to the modeling of label correlations.",
        "While a number of approaches have been proposed for dealing with label correlations (see Tsoumakas and",
        "Katakis (2007) for the comprehensive survey), the intuitively-appealing method is to incorporate features on two labels into the model (Ghamrawi and McCallum, 2005).",
        "The following label correlation feature indicates a cooccurrence of two labels and a word:",
        "if I, l' g y, otherwise."
      ]
    },
    {
      "heading": "3. A Method for Exact Inference",
      "text": [
        "A critical difficulty encountered in the model with label correlation features is the computational cost for training and decoding.",
        "When features on every pair of labels are included in the model, its graphical model becomes a complete graph, which indicates that the exact inference for this model is NP-hard.",
        "However, not all edges are necessary in actual inference, because of the sparsity of features.",
        "That is, we can remove edges between I and V when no corresponding features are active; i.e., fi,i',w(x, y) = 0 for all w. In text categorization, when feature selection is performed, many edges can be removed because of this characteristic.",
        "Therefore, our idea is to enjoy this sparsity of features.",
        "We construct a graphical model for each document, and put edges only when one or more features are active on the corresponding label pair.",
        "When a graph is sparse, we can apply a method for exact inference, such as the junction tree algorithm (Cowell et al., 1999).",
        "The junction tree algorithm is a generic algorithm for exact inference on any graphical model, and it allows for efficient inference on sparse graphs.",
        "The method converts a graph into a junction tree, which is a tree of cliques in the original graph.",
        "When we have a junction tree for each document, we can efficiently perform belief propagation in order to compute argmax in Equation (1), or the marginal probabilities of cliques and labels, necessary for the parameter estimation of machine learning classifiers, including perceptrons (Collins, 2002), and maximum entropy models (Berger et al., 1996).",
        "The computational complexity of the inference on junction trees is proportional to the exponential of the tree width, which is the maximum number of labels in a clique, minus one.",
        "An essential idea of this method is that a graphical model is constructed for each document.",
        "Even when features are defined on all pairs of labels, active features for a specific document are limited.",
        "When combined with feature selection, this",
        "Table 1 : Statistics of evaluation data sets method greatly increases the sparsity of the resulting graphs, which is key to efficiency.",
        "A weakness of this method comes from the assumption of feature sparseness.",
        "We are forced to apply feature selection, which is considered effective in text categorization, but not necessarily for other tasks.",
        "The design of features is also restricted in order to ensure the sparsity of features."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We evaluate our method for multi-label classification using three data sets for text categorization.",
        "Table 1 shows the statistics of these data.",
        "In this table, \"card.\"",
        "denotes the average number of labels assigned to a document.",
        "cmc2 007 is a data set used in the Computational Medicine Center (CMC) Challenge 2007 (Pestian et al., 2007).",
        "This challenge aimed at the assignment of ICD-9-CM codes, such as cough and pneumonia, to clinical free texts.",
        "It should be noted that this data is controlled, so that both training and test sets include the exact same label combinations, and the number of combinations is 90.",
        "This indicates that this task can be solved as a classification of 90 classes.",
        "However, since this is an unrealistic situation for actual applications, we do not rely on this characteristic in this work.",
        "reuterslO and reuters90 are taken from the Reuters-21578 collection, which is a popular benchmark for text categorization.",
        "This text collection consists of newswire articles, and each document is assigned topic categories, such as grain and ship.",
        "We split the data into training and test sets, according to the so-called ModApte split.",
        "Available at http : //www .",
        "computationalmedicine .",
        "org \"^Available at http://www.daviddlewis.com/resources/ testcollections/reuters21578/",
        "# train",
        "# test",
        "# labels",
        "card.",
        "cmc2007",
        "978",
        "976",
        "45",
        "1.23",
        "reuterslO",
        "6,490",
        "2,545",
        "10",
        "1.10",
        "reuters90",
        "7,770",
        "3,019",
        "90",
        "1.24",
        "K",
        "V",
        "c",
        "cmc2007",
        "1,000",
        "10",
        "0",
        "reuterslO",
        "5,000",
        "20",
        "5",
        "reuters90",
        "5,000",
        "80",
        "5",
        "From this data, we create two data sets.",
        "The first set, reuterslO, is a subset of theModApte split, to which the 10 largest categories are assigned.",
        "The other, reuters90, consists of documents that are labeled by 90 categories, having at least one document in each of the training and test sets.",
        "In the following experiments, we run two machine learning classifiers: Bayes Point Machines (Bpm) (Herbrich et al., 2001), and the maximum entropy model (Me) (Berger et al., 1996).",
        "For Bpm, we run 100 averaged perceptrons (Collins, 2002) with 10 iterations for each.",
        "For Me, the orthant-wise quasi-Newton method (Andrew and Gao, 2007) is applied, with the hyper parameter for l\\ regularization fixed to 1.0.",
        "We use word unigram features that represent the frequency of a particular word in a target document.",
        "We also use features that indicate the nonexistence of a word, which we found effective in preliminary experiments; feature fi,w(x,y) is 1 if l g y and w is not included in the document x.",
        "Words are stemmed and number expressions are normalized to a unique symbol.",
        "Words are not used if they are included in the stopword list (322 words), or they occur fewer than a threshold, c, in training data.",
        "We set c = 5 for reuterslO and reuters90, following previous works (Gham-rawi and McCallum, 2005), while c = 0 for cmc2 007, because the data is small.",
        "These features are selected according to averaged mutual information (information gain), which is the most popular method in previous works (Yang and Pedersen, 1997; Ghamrawi and McCallum, 2005).",
        "For each label, features are sorted according to this score, and top-ranked features are included in the model.",
        "By preliminary experiments, we fixed parameters, k for word unigram features and v for non-existence features, for each data set, as shown in Table 2.",
        "The same method is applied to the selection of label correlation features.",
        "In the following experiments, we observe the accuracy and training time by changing the threshold parameter 7 for the selection of label correlation features.",
        "Table 3 shows microaveraged F-scores (micro-Fl) and subset accuracies (sub.",
        "acc.)",
        "(Ghamrawi and McCallum, 2005) while varying 7, the number of label correlation features.",
        "In all data sets and with all classifiers, the accuracy is increased by incorporating label correlation features.",
        "The results also demonstrate that the accuracy saturates, or even decreases, with large 7.",
        "This indicates that the feature selection is necessary not only for obtaining efficiency, but also for higher accuracy.",
        "cmc2007",
        "BPM",
        "ME",
        "7",
        "micro-F 1",
        "sub.",
        "acc.",
        "micro-F 1",
        "sub.",
        "acc.",
        "0",
        "82.79",
        "69.88",
        "83.09",
        "69.06",
        "100",
        "83.49",
        "70.70",
        "83.68",
        "70.39",
        "200",
        "82.95",
        "69.67",
        "83.67",
        "70.18",
        "400",
        "83.03",
        "69.98",
        "83.49",
        "70.49",
        "800",
        "83.51",
        "71.41",
        "83.58",
        "70.70",
        "1600",
        "83.10",
        "70.49",
        "83.56",
        "71.00",
        "3200",
        "80.74",
        "66.70",
        "82.02",
        "69.57",
        "reuterslO",
        "BPM",
        "ME",
        "7",
        "micro-F 1",
        "sub.",
        "acc.",
        "micro-F 1",
        "sub.",
        "acc.",
        "0",
        "94.23",
        "89.71",
        "93.71",
        "88.76",
        "500",
        "94.22",
        "89.98",
        "93.80",
        "89.19",
        "1000",
        "94.43",
        "90.37",
        "94.07",
        "89.55",
        "2000",
        "94.46",
        "90.61",
        "94.04",
        "89.94",
        "4000",
        "94.12",
        "90.26",
        "94.12",
        "89.98",
        "8000",
        "94.14",
        "90.61",
        "94.50",
        "90.81",
        "16000",
        "93.92",
        "90.29",
        "94.30",
        "90.88",
        "reuters90",
        "BPM",
        "ME",
        "7",
        "micro-F 1",
        "sub.",
        "acc.",
        "micro-F 1",
        "sub.",
        "acc.",
        "0",
        "84.07",
        "77.91",
        "86.83",
        "79.50",
        "500",
        "84.96",
        "78.27",
        "86.89",
        "79.66",
        "1000",
        "85.38",
        "78.70",
        "86.94",
        "79.99",
        "2000",
        "85.73",
        "79.79",
        "86.55",
        "79.93",
        "4000",
        "85.72",
        "79.73",
        "86.54",
        "80.23",
        "8000",
        "85.90",
        "80.19",
        "86.77",
        "80.39",
        "16000",
        "86.17",
        "80.52",
        " â€“ ",
        " â€“ ",
        "7",
        "max.",
        "width",
        "avg.",
        "width",
        "time (sec.)",
        "0",
        "0",
        "0.00",
        "90",
        "100",
        "2",
        "1.17",
        "132",
        "200",
        "3",
        "1.51",
        "145",
        "400",
        "3",
        "1.71",
        "165",
        "800",
        "4",
        "2.11",
        "200",
        "1600",
        "5",
        "2.93",
        "427",
        "3200",
        "4",
        "3.99",
        "2280",
        "reuterslO",
        "7",
        "max.",
        "width",
        "avg.",
        "width",
        "time (sec.)",
        "0",
        "0",
        "0.00",
        "787",
        "500",
        "2",
        "1.72",
        "1378",
        "1000",
        "3",
        "2.00",
        "1752",
        "2000",
        "4",
        "2.16",
        "2594",
        "4000",
        "6",
        "2.90",
        "7183",
        "8000",
        "6",
        "4.22",
        "21555",
        "16000",
        "6",
        "5.67",
        "116535",
        "reuters90",
        "7",
        "max.",
        "width",
        "avg.",
        "width",
        "time (sec.)",
        "0",
        "0",
        "0.00",
        "26172",
        "500",
        "5",
        "1.74",
        "28067",
        "1000",
        "6",
        "2.24",
        "38510",
        "2000",
        "6",
        "3.22",
        "42479",
        "4000",
        "8",
        "3.68",
        "60029",
        "8000",
        "14",
        "4.56",
        "153268",
        "16000",
        "17",
        "6.39",
        " â€“ ",
        "Table 4 shows tree widths, and the time for the training of the Me models.",
        "As shown, the graphical model is represented effectively with sparse graphs, even when the number of label correlation features is increased.",
        "With these results, we can conclude that our method can model label correlations with a tractable cost.",
        "The accuracy for cmc2 007 is significantly better than the results reported in Patrick et al.",
        "(2007) (micro-F 1=81.1) in a similar setting, in which only word unigram features are used.",
        "Our best result is approaching the results of Crammer et al.",
        "(2007) (micro-F 1=84.6), which exploits various linguistically motivated features.",
        "Numerous results have been reported for reuterslO, and most of them report the microaveraged F-score around 91 to 94, while our best result is comparable to the state-of-the-art accuracy.",
        "For reuters90, Ghamrawi and McCallum (2005) achieved an improvement in the microaveraged F-score from 86.34 to 87.01, which is comparable to our result."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "This paper described a method for the exact inference for multi-label classification with label correlation features.",
        "Experimental results on text categorization with the CMC challenge data and the Reuters-21578 text collection demonstrated that our method improves the accuracy for text categorization with a tractable cost.",
        "The availability of exact inference enables us to apply various machine learning methods not yet investigated in this paper, including support vector machines.",
        "From the perspective of machine learning research, feature selection methods should be reconsidered.",
        "While we used a feature selection method that is widely accepted in text categorization research, it has no direct connection with machine learning models.",
        "Since feature selection methods motivated by the optimization criteria of machine learning models have been proposed (Riezler and Vasserman, 2004), we expect that the integration of our proposal with those methods will open up a new framework for multi-label classification."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was partially supported by Grant-in-Aid for Specially Promoted Research (MEXT, Japan) and Grant-in-Aid for Young Scientists (MEXT, Japan)."
      ]
    }
  ]
}
