{
  "info": {
    "authors": [
      "John DeNero",
      "Alexandre Bouchard-Côté",
      "Dan Klein"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-D08-1033",
    "title": "Sampling Alignment Structure under a Bayesian Translation Model",
    "url": "https://aclweb.org/anthology/D08-1033",
    "year": 2008
  },
  "references": [
    "acl-D07-1038",
    "acl-J03-1002",
    "acl-J93-2003",
    "acl-N07-1018",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P06-1085",
    "acl-P06-1124",
    "acl-P07-1107",
    "acl-P07-2045",
    "acl-P08-2007",
    "acl-W02-1018",
    "acl-W06-3105",
    "acl-W06-3123",
    "acl-W07-0403",
    "acl-W07-0715",
    "acl-W07-0734"
  ],
  "sections": [
    {
      "text": [
        "We describe the first tractable Gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment.",
        "We propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work, where overly large phrases memorize the training data.",
        "Phrase table weights learned under our model yield an increase in BLEU score over the word-alignment based heuristic estimates used regularly in phrase-based translation systems."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In phrase-based translation, statistical knowledge of translation equivalence is primarily captured by counts of how frequently various phrase pairs occur in training bitexts.",
        "Since bitexts do not come segmented and aligned into phrase pairs, these counts are typically gathered by fixing a word alignment and applying phrase extraction heuristics to this word-aligned training corpus.",
        "Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges.",
        "In this paper, we address the two most significant challenges in phrase alignment modeling.",
        "The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008).",
        "Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006).",
        "We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators.",
        "Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice).",
        "The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and small structures, the larger structures win out in maximum likelihood estimation.",
        "Indeed, the maximum likelihood estimate of a joint phrase alignment model analyzes each sentence pair as one large phrase with no internal structure (Marcu and Wong, 2002).",
        "We describe two non-parametric priors that empirically avoid this degenerate solution.",
        "Fixed word alignments are used in virtually every statistical machine translation system, if not to extract phrase pairs or rules directly, then at least to constrain the inference procedure for higher-level models.",
        "We estimate phrase translation features consistently using an inference procedure that is not constrained by word alignments, or any other heuristic.",
        "Despite this substantial change in approach, we report translation improvements over the standard word-alignment-based heuristic estimates of phrase table weights.",
        "We view this result as an important step toward building fully model-based translation systems that rely on fewer procedural heuristics."
      ]
    },
    {
      "heading": "2. Phrase Alignment Model",
      "text": [
        "While state-of-the-art phrase-based translation systems include an increasing number of features, translation behavior is largely driven by the phrase pair count ratios </>(e|/) and </>(/|e).",
        "These features are typically estimated heuristically using the counts c((e, /)) of all phrase pairs in a training corpus that are licensed by word alignments:",
        "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314 – 323, Honolulu, October 2008.",
        "©2008 Association for Computational Linguistics",
        "muy buen grado (a) example word alignment",
        "Thank , you",
        "I shall gladly.",
        "do so (b) example phrase alignment",
        "Figure 1: In this corpus example, the phrase alignment model found the non-literal translation pair {gladly, de muy buen grado) while heuristically-combined word alignment models did not.",
        "(a) is a grow-diag-final-and combined IBM Model 4 word alignment; (b) is a phrase alignment under our model.",
        "In this process, m = n = |a|; all phrases in both sentences are aligned one-to-one.",
        "We parameterize the choice of i using a geometric distribution, denoted Pg, with stop parameter p$:",
        "Each aligned phrase pair {e, /) is drawn from a multinomial distribution 6J which is unknown.",
        "We fix a simple distortion model, setting the probability of a permutation of the foreign phrases proportional to the product of position-based distortion penalties for each phrase:",
        "where pos( ) denotes the word position of the start of a phrase, and s the ratio of the length of the English to the length of the foreign sentence.",
        "This positional distortion model was deemed to work best by Marcu and Wong (2002).",
        "We can now state the joint probability for a phrase-aligned sentence consisting of i phrase pairs:",
        "While this model has several free parameters in addition to Oj, we fix them to reasonable values to focus learning on the phrase pair distribution.",
        "Sentence pairs do not always contain equal information on both sides, and so we revise the generative story to include unaligned phrases in both sentences.",
        "When generating each component of a sentence pair, we first decide whether to generate an aligned phrase pair or, with probability p0, an unaligned phrase.Then, we either generate an aligned phrase pair from Oj or an unaligned phrase from On, where On is a multinomial over phrases.",
        "Now, when generating e1:m, /i:n and alignment a, the number of phrases m + n can be greater than 2 • |a|.",
        "In contrast, a generative model that explicitly aligns pairs of phrases { e, /) gives us well-founded alternatives for estimating phrase pair scores.",
        "For instance, we could use the model's parameters as translation features.",
        "In this paper, we compute the expected counts of phrase pairs in the training data according to our model, and derive features from these expected counts.",
        "This approach endows phrase pair scores with well-defined semantics relative to a probabilistic model.",
        "Practically, phrase models can discover high-quality phrase pairs that often elude heuristics, as in Figure 1.",
        "In addition, the modelbased approach fits neatly into the framework of statistical learning theory for unsupervised problems.",
        "We first describe the symmetric joint model of Marcu and Wong (2002), which we will extend.",
        "A two-step generative process constructs an ordered set of English phrases e1:m, an ordered set of foreign phrases /1:n, and a phrase-to-phrase alignment between them, a = {(j, k)j indicating that {ej, /k) is an aligned pair.",
        "1.",
        "Choose a number of components i and generate each of i phrase pairs independently.",
        "2.",
        "Choose an ordering for the phrases in the foreign language; the ordering for English is fixed by the generation order.",
        "!We choose the foreign to reorder without loss of generality.",
        "To unify notation, we denote unaligned phrases as phrase pairs with one side equal to null: {e, null) or {null, /).",
        "Then, the revised model takes the form:",
        "In this definition, the distribution On gives nonzero weight only to unaligned phrases of the form {e,null) or {null,/), while Oj gives non-zero weight only to aligned phrase pairs."
      ]
    },
    {
      "heading": "3. Model Training and Expectations",
      "text": [
        "Our model involves observed sentence pairs, which in aggregate we can call x, latent phrase segmentations and alignments, which we can call z, and parameters Oj and On, which together we can call 0.",
        "A model such as ours could be used either for the learning of the key phrase pair parameters in 0, or to compute expected counts of phrase pairs in our data.",
        "These two uses are very closely related, but we focus on the computation of phrase pair expectations.",
        "For exposition purposes, we describe a Gibbs sampling algorithm for computing expected counts of phrases under P(z|x, 0) for fixed 0.",
        "Such expectations would be used, for example, to compute maximum likelihood estimates in the E-step of EM.",
        "In Section 4, we instead compute expectations under P(z|x), with 0 marginalized out entirely.",
        "In a Gibbs sampler, we start with a complete phrase segmentation and alignment, state zo, which sets all latent variables to some initial configuration.",
        "We then produce a sequence of sample states z^, each of which differs from the last by some small local change.",
        "The samples zj are guaranteed (in the limit) to consistently approximate the conditional distribution P(z|x, 0) (or P(z|x) later).",
        "Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model.",
        "Normalizing these expected counts yields estimates for the features 0(e|/) and </>(/|e).",
        "Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007).",
        "However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007).",
        "Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008).",
        "That is, we could not run EM exactly, even if we wanted maximum likelihood estimates.",
        "Expected phrase pair counts under P(z|x, 0) have been approximated before in order to run EM.",
        "Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space.",
        "DeNero et al.",
        "(2006) instead proposed an exponential-time dynamic program pruned using word alignments.",
        "Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008).",
        "None of these approximations are consistent, and they offer no method of measuring their biases.",
        "Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4).",
        "Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations \"closely enough.\"",
        "Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours.",
        "However, the basic sampling step they propose - resampling all segmentations and alignments for a sequence at once - requires a #P-hard computation.",
        "While this asymptotic complexity was apparently not prohibitive in the case of morphological alignment, where the sequences are short, it is prohibitive in phrase alignment, where the sentences are often very long.",
        "Our Gibbs sampler repeatedly applies each of five operators to each position in each training sentence pair.",
        "Each operator freezes all of the current state z% except a small local region, determines all the ways that region can be reconfigured, and then chooses a (possibly) slightly different zj+1 from among those outcomes according to the conditional probability of each, given the frozen remainder of the state.",
        "This frozen region of the state is called a Markov blanket (denoted m), and plays a critical role in proving the correctness of the sampler.",
        "The first operator we consider is Swap, which changes alignments but not segmentations.",
        "It freezes the set of phrases, then picks two English phrases e1and e2 (or two foreign phrases, but we focus on the English case).",
        "All alignments are frozen except the phrase pairs {e1, /1) and {e2, /2).",
        "Swap chooses between keeping {e1, /1) and {e2, /2) aligned as they are (outcome o0), or swapping their alignments to create {e1, /2) and {e2, /1) (outcome o1).",
        "Swap chooses stochastically in proportion to each outcome's posterior probability: P(o0 |m, x, 0) and P(o1 |m,x, 0).",
        "Each phrase pair in each outcome contributes to these posteriors the probability of adding a new pair, deciding whether it is null, and generating the phrase pair along with its contribution to the distortion probability.",
        "This is all captured in a succinct potential function ^({e, /)) = (c) Toggle (d) Flip Two",
        "Thus, outcome o0 is chosen with probability",
        "Operators in a Gibbs sampler require certain conditions to guarantee the correctness of the sampler.",
        "First, they must choose among all possible configurations of the unfrozen local state.",
        "Second, immediately reapplying the operator from any outcome must yield the same set of outcome options as be-fore.",
        "If these conditions are not met, the sampler may no longer be guaranteed to yield consistent approximations of the posterior distribution.",
        "A subtle issue arises with Swap as defined: should it also consider an outcome o2 of {e1, null) and {e2,null) that removes alignments?",
        "No part of the frozen state is changed by removing these alignments, so the first Gibbs condition dictates that we must include o2.",
        "However, after choosing o2, when we reapply the operator to positions e1 and",
        "Figure 2: Each local operator manipulates a small portion of a single alignment.",
        "Relevant phrases are exaggerated for clarity.",
        "The outcome sets (depicted by arrows) of each possible configuration are fully connected.",
        "Certain configurations cannot be altered by certain operators, such as the final configuration in Swap.",
        "Unalterable configurations for Toggle have been omitted for space.",
        "e2, we freeze all alignments except {e1,null) and {e2 ,null), which prevents us from returning to o0.",
        "Thus, we fail to satisfy the second condition.",
        "This point is worth emphasizing because some priorwork has treated Gibbs sampling as randomized search and, intentionally or otherwise, proposed inconsistent operators.",
        "Luckily, the problem is not with Swap, but with our justification of it: we can salvage Swap by augmenting its Markov blanket.",
        "Given that we have selected {e1 , /1 ) and {e2, /2), we not only freeze all other alignments and phrase boundaries, but also the number of aligned phrase pairs.",
        "With this count held invariant, o2 is not among the possible outcomes of Swap given m. Moreover, regardless of the outcome chosen, Swap can immediately be reapplied at the same location with the same set of outcomes.",
        "All the possible starting configurations and outcome sets for Swap appear in Figure 2(a).",
        "Current State",
        "Includes segmentations and alignments for all sentence pairs",
        "The boys are eating",
        "Apply the Flip operator to English position 1",
        "Markov Blanket",
        "Freezes most of the segmentations and alignments, along with the alignment count",
        "Compute the conditional probability of each outcome",
        "Outcomes",
        "An exhaustive set of possibilities given the Markov blanket",
        "Finally, select a new state proportional to its conditional probability",
        "Figure 3: The three steps involved in applying the FLIP operator.",
        "The Markov blanket freezes all segmentations except English position 1 and all alignments except those for Ellos and The boys.",
        "The blanket also freezes the number of alignments, which disallows the lower right outcome.",
        "we cannot return to the original state by reapplying FLIP.",
        "Consequently, when a position is already segmented and both adjacent phrases are currently aligned, FLIP cannot unsegment the point because it can't create two aligned phrase pairs with the one larger phrase that results (see bottom of Figure 2(b)).",
        "Both SWAP and FLIP freeze the number of alignments in a sentence.",
        "The Toggle operator, on the other hand, can add or remove individual alignment links.",
        "In TOGGLE, we first choose an e1 and /1.",
        "If (e1, /1) G a or both e1 and /1 are null, we freeze all segmentations and the rest of the alignments, and choose between including (e1; /1) in the alignment or leaving both ei and /i unaligned.",
        "If only one of e1 and /1 are aligned, or they are not aligned to each other, then TOGGLE does nothing.",
        "Together, Flip, Swap and Toggle constitute a complete Gibbs sampler that consistently samples from the posterior P(z\\x, 9).",
        "Not only are these operators valid Gibbs steps, but they also can form a path of positive probability from any source state",
        "SWAP can arbitrarily shuffle alignments, but we need a second operator to change the actual phrase boundaries.",
        "The FLIP operator changes the status of a single segmentation position to be either a phrase boundary or not.",
        "In this sense FLIP is a bilingual analog of the segmentation boundary flipping operator of Goldwater et al.",
        "(2006).",
        "Figure 3 diagrams the operator and its Markov blanket.",
        "First, FLIP chooses any between-word position in either sentence.",
        "The outcome sets for FLIP vary based on the current segmentation and adjacent alignments, and are depicted in Figure 2.",
        "Again, for FLIP to satisfy the Gibbs conditions, we must augment its Markov blanket to freeze not only all other segmentation points and alignments, but also the number of aligned phrase pairs.",
        "Otherwise, we end up allowing outcomes from which to any target state in the space of phrase alignments (formally, the induced Markov chain is irreducible).",
        "Such a path can at worst be constructed by unalign-ing all phrases in the source state with Toggle, composing applications of FLIP to match the target phrase boundaries, then applying TOGGLE to match the target alignments.",
        "We include two more local operators to speed up the rate at which the sampler explores the hypothesis space.",
        "In short, FLIP TWO simultaneously flips an English and a foreign segmentation point (to make a large phrase out of two smaller ones or vice versa), while MOVE shifts an aligned phrase boundary to the left or right.",
        "We omit details for lack of space.",
        "With our sampling procedure in place, we can now estimate the expected number of times a given phrase pair occurs in our data, for fixed 9, using a Monte-Carlo average,",
        "?",
        "?",
        "The left hand side is simple to compute; we count aligned phrase pairs in each sample we generate.",
        "In practice, we only count phrase pairs after applying every operator to every position in every sentence (one iteration).",
        "Appropriate normalizations of these expected counts can be used either in an M-step as maximum likelihood estimates, or to compute values for features 0(/\\e) and 0(e\\/)."
      ]
    },
    {
      "heading": "4. Nonparametric Bayesian Priors",
      "text": [
        "The Gibbs sampler we presented addresses the inference challenges of learning phrase alignment models.",
        "With slight modifications, it also enables us to include prior information into the model.",
        "In this section, we treat 9 as a random variable and shape its prior distribution in order to correct the well-known degenerate behavior of the model.",
        "The structure of our joint model penalizes explanations that use many small phrase pairs.",
        "Each phrase pair token incurs the additional expense of generation and distortion.",
        "In fact, the maximum likelihood estimate of the model puts mass on (e, /) pairs that span entire sentences, explaining the training corpus with one phrase pair per sentence.",
        "Previous phrase alignment work has primarily mitigated this tendency by constraining the inference procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a non-compositional constraint (Cherry and Lin, 2007; Zhang et al., 2008).",
        "However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure.",
        "Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference.",
        "A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al.",
        "(2008), but it did not avoid the degenerate solution.",
        "Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass.",
        "May and Knight (2007) added additional model terms to balance the cost of long and short derivations in a syntactic alignment model.",
        "We control this degenerate behavior by placing a Dirichlet process (DP) prior over 9j, the distribution over aligned phrase pairs (Ferguson, 1973).",
        "If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be an appropriate prior.",
        "A draw from a K-dimensional Dirichlet distribution is a list of K real numbers in [0,1] that sum to one, which can be interpreted as a distribution over K phrase pair types.",
        "However, since the event space of possible phrase pairs is in principle unbounded, we instead use a Dirichlet process.",
        "A draw from a DP is a countably infinite list of real numbers in [0,1] that sum to one, which we interpret as a distribution over a countably infinite list of phrase pair types.",
        "The Dirichlet distribution and the DP distribution have similar parameterizations.",
        "A K-dimensional Dirichlet can be parameterized with a concentration parameter a > 0 and a base distribution M0 = ..., ßK-1), with ßi G (0,1).",
        "This parameterization has an intuitive interpretation: under these parameters, the average of independent samples from the Dirichlet will converge to Mo.",
        "That is, the average of the ith element of the samples will converge to ßi.",
        "Hence, the base distribution M0 characterizes the sample mean.",
        "The concentration parameter a only affects the variance of the draws.",
        "Similarly, we can parameterize the Dirichlet process with a concentration parameter a (that affects only the variance) and a base distribution M0 that determines the mean of the samples.",
        "Just as in the finite Dirichlet case, M0 is simply a probability distribution, but now with countably infinite support: all possible phrase pairs in our case.",
        "In practice, we can use an unnormalized M0 (a base measure) by appropriately rescaling a.",
        "In our model, we select a base measure that strongly prefers shorter phrases, encouraging the model to use large phrases only when it has sufficient evidence for them.",
        "We continue the model:",
        "Pe(e) = Pg(\\e\\;pa) • (' .",
        "PWA is the IBM model 1 likelihood ofone phrase conditioned on the other (Brown et al., 1994).",
        "P/ and Pe are uniform over types for each phrase length: the constants n/ and ne denote the vocabulary size of the foreign and English languages, respectively, and PG is a geometric distribution.",
        "Above, 9j is drawn from a DP centered on the geometric mean of two joint distributions over phrase pairs, each of which is composed of a monolingual unigram model and a lexical translation component.",
        "This prior has two advantages.",
        "First, we pressure the model to use smaller phrases by increasing ps(ps = 0.8 in experiments).",
        "Second, we encourage good phrase pairs by incorporating IBM Model 1 distributions.",
        "This use of word alignment distributions is notably different from lexical weighting or word alignment constraints: we are supplying prior knowledge that phrases will generally follow word alignments, though with enough corpus evidence they need not (and often do not) do so in the posterior samples.",
        "The model proved largely insensitive to changes in the sparsity parameter a, which we set to 100 for experiments.",
        "Introducing unaligned phrases invites further degenerate megaphrase behavior: a sentence pair can be generated cheaply as two unaligned phrases that each span an entire sentence.",
        "We attempted to place a similar DP prior over 9N, but surprisingly, this modeling choice invoked yet another degenerate behavior.",
        "The DP prior imposes a rich-get-richer property over the phrase pair distribution, strongly encouraging the model to reuse existing pairs rather than generate new ones.",
        "As a result, common words consistently aligned to null, even while suitable translations were present, simply because each null alignment reinforced the next.",
        "For instance, the was always unaligned.",
        "Instead, we fix 9N to a simple unigram model that is uniform over word types.",
        "This way, we discourage unaligned phrases while focusing learning on 9j.",
        "For simplicity, we reuse P/ (/) and Pe(e) from the prior over 9j.",
        "• Pf (/) if e = null .",
        "The represents a choice of whether the aligned phrase is in the foreign or English sentence.",
        "Our entire model now has the general form P(x, z, 9j); all other model parameters have been fixed.",
        "Instead of searching for a suitable 9j, we sample from the posterior distribution P(z\\x) with 9j marginalized out.",
        "To this end, we convert our Gibbs sampler into a collapsed Gibbs sampler using the Chinese Restaurant Process (CRP) representation of the DP (Aldous, 1985).",
        "With the CRP, we avoid the problem of explicitely representing samples from the DP.",
        "CRP-based samplers have served the community well in related language tasks, such as word segmentation and coreference resolution (Goldwater et al., 2006; Haghighi and Klein, 2007).",
        "Under this representation, the probability of each sampling outcome is a simple expression in terms of the state of the rest of the training corpus (the Markov blanket), rather than explicitly using 9j.",
        "Let zm be the set of aligned phrase pair tokens observed in the rest of the corpus.",
        "Then, when (e, /) is aligned (that is, neither e nor / are null), the conditional probability for a pair (e, /) takes the form:",
        "where county />(zm) is the number of times that (e, /) appears in zm.",
        "We can write this expression thanks to the exchangeability of the model.",
        "For further exposition of this collapsed sampler posterior,",
        "Minimal extracted phrases",
        "Sampled phrases All extracted phrases",
        "Figure 4: The distribution of phrase pair sizes (denoted English length x foreign length) favors small phrases under the model.",
        "The sampler remains exactly the same as described in Section 3, except that the posterior conditional probability of each outcome uses a revised potential function Vrjp((e, /)) =",
        "We also evaluate a hierarchical Dirichlet process (HDP) prior over 9j, which draws monolingual distributions 9E and 9F from a DP and 9j from their cross-product:",
        "M0((e,/)) = [9f(/)PwA(e\\/) • 9E(e)PwA(/\\e)]9F - DP (Pf ,a') 9e - DP(Pe,a') .",
        "This prior encourages novel phrase pairs to be composed of phrases that have been used before.",
        "In the sampler, we approximate table counts for 9E and 9F with their expectations, which can be computed from phrase pair counts (see the appendix of Gold-water et al.",
        "(2006) for details).",
        "The HDP prior gives a similar distribution over phrase sizes.",
        "-P$) (1-P0) t((e,/)) (5((e,/)) e & / non-null -P$) • P0 • 9N((e, /)) otherwise .",
        "0DP is like 0, but the fixed 9j is replaced with the constantly-updated t function.",
        "Figure 4 shows a histogram of phrase pair sizes in the distribution of expected counts under the model.",
        "As reference, we show the size distribution of both minimal and all phrase pairs extracted from word alignments using the standard heuristic.",
        "Our model tends to select minimal phrases, only using larger phrases when well motivated.",
        "This result alone is important: a model-based solution with no inference constraint has yielded a non-degenerate distribution over phrase lengths.",
        "Note that our sampler does find the degenerate solution quickly under a uniform prior, confirming that the model, and not the inference procedure, is selecting these small phrases."
      ]
    },
    {
      "heading": "5. Translation Results",
      "text": [
        "We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task.",
        "We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the grow-diag-final-and combination heuristic (Koehn et al., performed better than any alternative combination heuristic.",
        "The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment.",
        "We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations.",
        "Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing.",
        "We report results with and without lexical weighting, denoted lex.",
        "We tuned and tested on development corpora for the 2006 translation workshop.",
        "The parameters for each phrase table were tuned separately using minimum error rate training (Och, 2003).",
        "Results are",
        "Table 1: BLEU results for learned distributions improve over a heuristic baseline.",
        "Estimate labels are described fully in section 5.3.",
        "The label lex indicates the addition of a lexical weighting feature.",
        "scored with lowercased, tokenized NIST BLEU, and exact match METEOR (Papineni et al., 2002; Lavie and Agarwal, 2007).",
        "The baseline system gives a BLEU score of 29.8, which increases to 30.5 with lex, as shown in Table 1.",
        "For reference, training on all sentences of length less than 40 (the shared task baseline default) gives 32.4 BLEU with lex.",
        "We initialized the sampler with a configuration derived from the word alignments generated by the baseline.",
        "We greedily constructed a phrase alignment from the word alignment by identifying minimal phrase pairs consistent with the word alignment in each region of the sentence.",
        "We then ran the sampler for 100 iterations through the training data.",
        "Each iteration required 12 minutes under the DP prior, and 30 minutes under the HDP prior.",
        "Total running time for the HDP model neared two days on an eight-processor machine with 16 Gb of RAM.",
        "Estimating phrase counts under the DP prior decreases BLEU to 28.8, or 29.1 under the HDP prior.",
        "This gap is not surprising: heuristic extraction discovers many more phrase pairs than sampling.",
        "Note that sacrificing only 0.7 BLEU while shrinking the phrase table by 92% is an appealing trade-off in resource-constrained settings.",
        "The estimates DP-composed and HDP-composed in Table 1 take expectations of a more liberal count function.",
        "While sampling, we count not only aligned phrase pairs, but also larger ones composed of two or more contiguous aligned pairs.",
        "This count function is similar to the phrase pair extraction heuristic, but never includes unaligned phrases in any way.",
        "Expectations of these composite phrases still have a probabilistic interpretation, but they are not the structures we are directly modeling.",
        "Notably, these estimates outperform the baseline by 0.3 BLEU without ever extracting phrases from word alignments, and performance increases despite a reduction in table size.",
        "We can instead increase coverage by smoothing the learned estimates with the heuristic counts.",
        "The estimates DP-smooth and HDP-smooth add counts extracted from word alignments to the sampler's running totals, which improves performance by 0.4 BLEU over the baseline.",
        "This smoothing balances the lower-bias sampler counts with the lower-variance heuristics ones."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "Our novel Gibbs sampler and nonparametric priors together address two open problems in learning phrase alignment models, approximating inference consistently and efficiently while avoiding degenerate solutions.",
        "While improvements are modest relative to the highly developed word-alignment-centered baseline, we show for the first time competitive results from a system that uses word alignments only for model initialization and smoothing, rather than inference and estimation.",
        "We view this milestone as critical to eventually developing a clean probabilistic approach to machine translation that unifies model structure across both estimation and decoding, and decreases the use of heuristics.",
        "Phrase",
        "Exact",
        "Pair",
        "NIST",
        "Match",
        "Estimate",
        "Count",
        "BLEU",
        "meteor",
        "Heuristic",
        "4.4M",
        "29.8",
        "52.4",
        "DP",
        "0.6M",
        "28.8",
        "51.7",
        "HDP",
        "0.3M",
        "29.1",
        "52.0",
        "DP-composed",
        "3.7M",
        "30.1",
        "52.7",
        "HDP-composed",
        "3.1M",
        "30.1",
        "52.6",
        "DP-smooth",
        "4.8M",
        "30.1",
        "52.5",
        "HDP-smooth",
        "4.6M",
        "30.2",
        "52.7",
        "Heuristic + lex",
        "4.4M",
        "30.5",
        "52.9",
        "DP-smooth + lex",
        "4.8M",
        "30.4",
        "53.0",
        "HDP-smooth + lex",
        "4.6M",
        "30.7",
        "53.2"
      ]
    }
  ]
}
