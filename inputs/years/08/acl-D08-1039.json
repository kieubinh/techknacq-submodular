{
  "info": {
    "authors": [
      "Saša Hasan",
      "Juri Ganitkevitch",
      "Hermann Ney",
      "Jesús Andrés-Ferrer"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-D08-1039",
    "title": "Triplet Lexicon Models for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/D08-1039",
    "year": 2008
  },
  "references": [
    "acl-D07-1007",
    "acl-J01-2004",
    "acl-J93-2003",
    "acl-P07-1005",
    "acl-W03-1003",
    "acl-W97-1014"
  ],
  "sections": [
    {
      "text": [
        "Sasa Hasan, Juri Ganitkevitch, Hermann Ney, Jesus Andrés-Ferrer j*",
        "Human Language Technology and Pattern Recognition, RWTH Aachen University, Germany jüniversidad Politecnica de Valencia, Dept.",
        "Sist.",
        "Informaticos y Computation",
        "{hasan,ganitkevitch,ney}@cs.rwth-aachen.de jandres@dsic.upv.es",
        "/' f source target This paper describes a lexical trigger model for statistical machine translation.",
        "We present various methods using triplets incorporating long-distance dependencies that can go beyond the local context of phrases or n-gram based language models.",
        "We evaluate the presented methods on two translation tasks in a reranking framework and compare it to the related IBM model 1.",
        "We show slightly improved translation quality in terms of BLEU and TER and address various constraints to speed up the training based on Expectation-Maximization and to lower the overall number of triplets without loss in translation performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Data-driven methods have been applied very successfully within the machine translation domain since the early 90s.",
        "Starting from single-word-based translation approaches, significant improvements have been made through advances in modeling, availability of larger corpora and more powerful computers.",
        "Thus, substantial progress made in the past enables today's MT systems to achieve acceptable results in terms of translation quality for specific language pairs such as Arabic-English.",
        "If sufficient amounts of parallel data are available, statistical MT systems can be trained on millions of",
        "*The work was carried out while the author was at the Human Language Technology and Pattern Recognition group at RWTH Aachen University and partly supported by the Valen-cian Conselleria d'Empresa, Universität i Ciència under grants CTBPRA/2005/ and BEFPI/2007/014.",
        "Figure 1: Triplet example: a source word f is triggered by two target words e and el, where one of the words is within and the other outside the considered phrase pair (indicated by the dashed line).",
        "sentence pairs and use an extended level of context based on bilingual groups of words which denote the building blocks of state-of-the-art phrase-based SMT systems.",
        "Due to data sparseness, statistical models are often trained on local context only.",
        "Language models are derived from n-grams with n < 5 and bilingual phrase pairs are extracted with lengths up to 10 words on the target side.",
        "This captures the local dependencies of the data in detail and is responsible for the success of data-driven phrase-based approaches.",
        "In this work, we will introduce a new statistical model based on lexicalized triplets (f, e, e') which we will also refer to as cross-lingual triggers of the form (e, e' – f ).",
        "This can be understood as two words in one language triggering one word in another language.",
        "These triplets, modeled by p(f |e,e'), are closely related to lexical translation probabilities based on the IBM model 1, i.e. p(f |e).",
        "Several constraints and setups will be described later on in more detail, but as an introduction one can",
        "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 372-381, Honolulu, October 2008.",
        "©2008 Association for Computational Linguistics",
        "think of the following interpretation which is depicted in Figure 1: Using a phrase-based MT approach, a source word f is triggered by its translation e which is part of the phrase being considered, whereas another target word e' outside this phrase serves as an additional trigger in order to allow for more fine-grained distinction of a specific word sense.",
        "Thus, this cross-lingual trigger model can be seen as a combination of a lexicon model (i.e. f and e) and a model similar to monolingual longrange (i.e. distant bigram) trigger models (i.e. e and e', although these dependencies are reflected indirectly via e' – f) which uses both local (in-phrase) and global (in-sentence) information for the scoring.",
        "The motivation behind this approach is to get nonlocal information outside the current context (i.e. the currently considered bilingual phrase pair) into the translation process.",
        "The triplets are trained via the EM algorithm, as will be shown later in more detail."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "In the past, a significant number of methods has been presented that try to capture long-distance dependencies, i.e. use dependencies in the data that reach beyond the local context of n-grams or phrase pairs.",
        "In language modeling, monolingual trigger approaches have been presented (Rosenfeld, 1996; Tillmann and Ney, 1997) as well as syntactical methods that parse the input and model long-range dependencies on the syntactic level by conditioning on the predecessing words and their corresponding parent nodes (Chelba and Jelinek, 2000; Roark, 2001).",
        "The latter approach was shown to reduce perplexities and improve the WER in speech recognition systems.",
        "One drawback is that the parsing process might slow down the system significantly and the approach is complicated to be integrated directly in the search process.",
        "Thus, the effect is often shown offline in reranking experiments using n-best lists.",
        "One of the simplest models that can be seen in the context of lexical triggers is the IBM model 1 (Brown et al., 1993) which captures lexical dependencies between source and target words.",
        "It can be seen as a lexicon containing correspondents of translations of source and target words in a very broad sense since the pairs are trained on the full sentence level.",
        "The model presented in this work is very close to the initial IBM model 1 and can be seen as taking another word into the conditioning part, i.e. the triggering items.",
        "Furthermore, since the second trigger can come from any part of the sentence, we also have a link to long-range monolingual triggers as presented above.",
        "A long-range trigram model is presented in (Della Pietra et al., 1994) where it is shown how to derive a probabilistic link grammar in order to capture long-range dependencies in English using the EM algorithm.",
        "Expectation-Maximization is used in the presented triplet model as well which is described in more detail in Section 3.",
        "Instead of deriving a grammar automatically (based on POS tags of the words), we rely on a fully lexicalized approach, i.e. the training is taking place at the word level.",
        "Related work in the context of fine-tuning language models by using cross-lingual lexical triggers is presented in (Kim and Khudanpur, 2003).",
        "The authors show how to use cross-lingual triggers on a document level in order to extract translation lexicons and domain-specific language models using a mutual information criterion.",
        "Recently, word-sense disambiguation (WSD) methods have been shown to improve translation quality (Chan et al., 2007; Carpuat and Wu, 2007).",
        "Chan et al.",
        "(2007) use an SVM based classifier for disambiguating word senses which are directly incorporated in the decoder through additional features that are part of the log-linear combination of models.",
        "They use local collocations based on surrounding words left and right of an ambiguous word including the corresponding parts-of-speech.",
        "Although no long-range dependencies are modeled, the approach yields an improvement of +0.6% BLEU on the NIST Chinese-English task.",
        "In Carpuat and Wu (2007), another state-of-the-art WSD engine (a combination of naive Bayes, maximum entropy, boosting and Kernel PCA models) is used to dynamically determine the score of a phrase pair under consideration and, thus, let the phrase selection adapt to the context of the sentence.",
        "Although the baseline is significantly lower than in the work of Chan et al., this setup reaches an improvement of 0.5% BLEU on the NIST CE task and up to 1.1% BLEU on the",
        "IWSLT'06 test sets.",
        "The work in this paper tries to complement the WSD approaches by using long-range dependencies.",
        "If triggers from a local context determine different lexical choice for the word being triggered, the setting is comparable to the mentioned WSD approaches (although local dependencies might already be reflected sufficiently in the phrase models).",
        "A distant second trigger, however, might have a beneficial effect for specific languages, e.g. by capturing word splits (as it is the case in German for verbs with separable prefixes) or, as already mentioned, allowing for a more fine-grained lexical choice of the word being triggered, namely based on another word which is not part of the current local, i.e. phrasal, context.",
        "The basic idea of triplets of the form (e, f' – f), called multi-word extensions, is also mentioned in (Tillmann, 2001) but neither evaluated nor investigated in further detail.",
        "In the following sections, we will describe the model proposed in this work.",
        "In Section 3, a detailed introduction is given, as well as the EM training and variations of the model.",
        "The different settings will be evaluated in Section 4, where we show experiments on the IWSLT Chinese-English and TC-STAR EPPS English-Spanish/Spanish-English tracks.",
        "A discussion of the results and further examples are given in Section 5.",
        "Final remarks and future work are addressed in Section 6."
      ]
    },
    {
      "heading": "3. Model",
      "text": [
        "As an extension to commonly used lexical word pair probabilities p(f le) as introduced in (Brown et al., 1993), we define our model to operate on word triplets.",
        "A triplet (f, e, e') is assigned a value a(f le, e') > 0 with the constraint such that",
        "Ve,e' : £ a(f |e,e') = l.",
        "Throughout this paper, e and e' will be referred to as the first and the second trigger, respectively.",
        "In view of its triggers f will be termed the effect.",
        "For a given bilingual sentence pair (f J, e{), the probability of a source word f given the whole target sentence e{ for the triplet model is defined as:",
        "where - denotes a normalization factor based on the corresponding target sentence length, i.e.",
        "The introduction of a second trigger (i.e. ek in Eq.",
        "1) enables the model to combine local (i.e. word or phrase level) and global (i.e. sentence level) information.",
        "In the following, we will describe the training procedure of the model via maximum likelihood estimation for the unconstrained case.",
        "The goal of the training procedure is to maximize the log-likelihood Fall of the triplet model for a given bilingual training corpus {(fJ, e{)}N consisting of N sentence pairs:",
        "where Jn and In are the lengths of the nth source and target sentences, respectively.",
        "As there is no closed form solution for the maximum likelihood estimate, we resort to iterative training via the EM algorithm (Dempster et al., 1977).",
        "We define the auxiliary function Q(ß; ß) based on Fall where ß is the new estimate within an iteration which is to be derived from the current estimate ß.",
        "Here, ß stands for the entire set of model parameters to be estimated, i.e. the set of all {a(f |e, e')}.",
        "Thus, we obtain where Zn is defined as in Eq.",
        "2.",
        "Using the method of Lagrangian multipliers for the normalization constraint, we take the derivative with respect to where A(f, e, e') is a relative weight accumulator over the parallel corpus:",
        "Cn(e,e') = ^ ^ ö(e,ei)ö(e',ek).",
        "The function ö(-, •) denotes the Kronecker delta.",
        "The resulting training procedure is analogous to the one presented in (Brown et al., 1993) and (Tillmann and Ney, 1997).",
        "The next section presents variants of the basic unconstrained model by putting restrictions on the valid regions of triggers (in-phrase vs. out-of-phrase) and using alignments obtained from either GIZA++ training or forced alignments in order to reduce the model size and to incorporate knowledge already obtained in previous training steps.",
        "Based on the unconstrained triplet model presented in Section 3, we introduce additional constraints, namely the phrase-bounded and the path-aligned triplet model in the following.",
        "The former reduces the number of possible triplets by posing constraints on the position of where valid triggers may originate from.",
        "In order to obtain phrase boundaries on the training data, we use forced alignments, i.e. translate the whole training data by constraining the translation hypotheses to the target sentences of the training corpus.",
        "Path-aligned triplets use an alignment constraint from the word alignments that are trained with GIZA++.",
        "Here, we restrict the first trigger pair (f, e) to the alignment path as based on the alignment matrix produced by IBM model 4.",
        "These variants require information in addition to the bilingual sentence pair (fJ, e{), namely a corresponding phrase segmentation n = } with",
        "The phrase-bounded triplet model (referred to as pphr in the following), restricts the first trigger e to the same phrase as f, whereas the second trigger e'is set outside the phrase, resulting in",
        "The path-aligned triplet model (denoted by Paiignin the following), restricts the scope of e to words aligned to f by A, yielding:",
        "where the zj are, again, the appropriate normalization terms.",
        "Also, to account for non-aligned words (analogously to the IBM models), the empty word eo is considered in all three model variations.",
        "We show the effect of the empty word in the experiments (Section 4).",
        "Furthermore, we can train the presented models in the inverse direction, i.e. P(e|f, f '), and combine the two directions in the rescoring framework.",
        "The next section presents a set of experiments that evaluate the performance of the presented triplet model and its variations."
      ]
    },
    {
      "heading": "1. 3 a phrase pair that covers e i and f j 0 otherwise",
      "text": [
        "for the phrase-bounded method and, similarly, a word alignment A = {aij} where"
      ]
    },
    {
      "heading": "1. if e i is aligned to f j 0 otherwise",
      "text": []
    },
    {
      "heading": "4. Experiments",
      "text": [
        "In this section, we describe the system setup used in this work, including the translation tasks and the corresponding training corpora.",
        "The experiments are based on an n-best list reranking framework.",
        "The experiments were carried out using a state-of-the-art phrase-based SMT system.",
        "The dynamic programming beam search decoder uses several models during decoding by combining them log-linearly.",
        "We incorporate phrase translation and word lexicon models in both directions, a language model, as well as phrase and word penalties including a distortion model for the reordering.",
        "While generating the hypotheses, a word graph is created which compactly represents the most likely translation hypotheses.",
        "Out of this word graph, we generate n-best lists and use them to test the different setups as described in Section 3.",
        "In the experiments, we use 10,000-best lists containing unique translation hypotheses, i.e. duplicates generated due to different phrase segmentations are reduced to one single entry.",
        "The advantage of this reranking approach is that we can directly test the obtained models since we already have fully generated translations.",
        "Thus, we can apply the triplet lexicon model based on P(f |e, e') and its inverse counterpart P(e|f, f ') directly.",
        "During decoding, since e' could be from anywhere outside the current phrase, i.e. even from a part which lies beyond the current context which has not yet been generated, we would have to apply additional constraints during training (i.e. make further restrictions such as i' < i for a trigger pair (ei, ei')).",
        "Optimization of the model scaling factors is carried out using minimum error rate training (MERT) on the development sets.",
        "The optimization criterion is 100-BLEU since we want to maximize the BLEU score.",
        "For the first part of the experiments, we use the corpora that were released for the IWSLT'07 evaluation campaign.",
        "The training corpus consists of approximately 43K Chinese-English sentence pairs, mainly coming from the BTEC corpus (Basic Travel Expression Corpus).",
        "This is a multilingual speech corpus which contains tourism-related material, such as transcribed conversations about making reservations, asking for directions or conversations as taking place in restaurants.",
        "For the experiments, we use the clean data track, i.e. transcriptions of read speech.",
        "As the development set which is used for tuning the parameters of the baseline system and the reranking framework, we use the IWSLT'04 evaluation set (500 sentence pairs).",
        "The two blind test sets which are used to evaluate the final performance of the models are the official evaluation sets from IWSLT'05 (506 sentences) and IWSLT'07 (489 sentences).",
        "The average sentence length of the training corpus is 10 words.",
        "Thus, the task is somewhat limited and very domain-specific.",
        "One of the advantages of this setting is that preliminary experiments can be carried out quickly in order to analyze the effects of the different models in detail.",
        "This and the small vocabulary size (12K entries) makes the corpus ideal for first \"rapid application development\"-style setups without having to care about possible constraints due to memory requirements or CPU time restrictions.",
        "Furthermore, additional experiments are based on the EPPS corpus (European Parliament Plenary Sessions) as used within the FTE (Final Text Edition) track of the TC-STAR evaluations.",
        "The corpus contains speeches held by politicians at plenary sessions of the European Parliament that have been transcribed, \"corrected\" to make up valid written texts and translated into several target languages.",
        "The language pairs considered in the experiments here are Spanish-English and English-Spanish.",
        "The training corpus consists of roughly 1.3M sentence pairs with 35.5M running words on the English side.",
        "The vocabulary sizes are considerably larger than for the IWSLT task, namely around 170K on the target side.",
        "As development set, we use the development data issued for the 2006 evaluation (1122 sentences), whereas the two blind test sets are the official evaluation data from 2006 (TC-Star'06, 1117 sentences) and 2007 (TC-Star'07, 1130 sentences).",
        "One of the first questions that arises is how many EM iterations should be carried out during training of the triplet model.",
        "Since the IWSLT task is small,",
        "Since EPPS is a considerably harder task (larger vocabulary and longer sentences), the training of a full unconstrained triplet model cannot be done due to memory restrictions.",
        "One possibility to reduce the number of extracted triplets is to apply a maximum distance constraint in the training procedure, i.e. only trigger pairs are considered where the distance between first and second trigger is below or equal to the specified maximum.",
        "Table 3 shows the effect of a maximum distance constraint for the Spanish-English direction.",
        "Due to the large amount of triplets (we extract roughly two billion triplets for the EPPS data), we drop all triplets that occur less than 3 times which results in 640 million triplets.",
        "Also, due to time restrictions, we only train 4 iterations and compare it to 4 iterations of the same setting with the maximum distance set to 10.",
        "The training with the maximum distance constraints ends with a total of 380 million triplets.",
        "As can be seen (Table 3), the performance is comparable while cutting down the computation time from 9.2 to 3.1 hours.",
        "The experiments were carried out on a 2.2GHz Opteron machine with 16 GB of memory.",
        "The overall gain is +0.4-0.6% BLEU and up to -0.4% in TER.",
        "We even observe a slight increase in BLEU for the TC-Star'07 set which might be a random effect due to optimization on the development set where the behavior is the same as for TC-Star'06.",
        "Table 5: Results on EPPS, English-Spanish, maximum approximation, palign combined, occ3, 10 EM iterations.",
        "Results on EPPS English-Spanish for the phrase-bounded triplet model are presented in Table 4.",
        "Since the number of triplets is less than for the unconstrained model, we can lower the cutoff from 3 to 2 (denoted in the table by occ3 and occ2, respectively).",
        "There is a small additional gain on the TC-Star 07 test set by this step, with a total of +0.7%",
        "BLEU for TC-Star 06 and +0.8% BLEU for TC-Star 07.",
        "Table 5 shows results for a variation of the path-aligned triplet model palign that restricts the first trigger to the best aligned word as estimated in the IBM model 1, thus using a maximum-approximation of the given word alignment.",
        "The model was trained on two word alignments, firstly the one contained in the forced alignments on the training data, and secondly on an IBM-4 word alignment generated using GIZA++.",
        "For this second model we also demonstrate the improvement obtained when increasing the triplet lexicon size by using less trimming.",
        "Another experiment was carried out to investigate the effect of immediate neighboring words used as triggers within the palign setting.",
        "This is equivalent to using a \"maximum distance of 1\" constraint.",
        "We obtained worse results, namely a 0.2-0.3% drop in last row), although the training is significantly faster with this setup, namely roughly 30 minutes per it-",
        "TC-Star'06 BLEU TER",
        "TC-Star'07 BLEU TER",
        "baseline",
        "52.3 34.57",
        "50.4 36.46",
        "trip fe+ef pall",
        "52.9 34.32",
        "50.6 36.34",
        "+ max dist 10",
        "52.9 34.20",
        "50.8 36.22",
        "TC-Star'06 BLEU TER",
        "TC-Star'07 BLEU TER",
        "baseline",
        "49.5 37.65",
        "51.0 36.03",
        "trip fe+ef pvhr",
        "50.2 37.01",
        "51.5 35.38",
        "+ occ2",
        "50.2 37.06",
        "51.8 35.32",
        "TC-Star'06",
        "TC-Star'07",
        "BLEU",
        "TER",
        "BLEU TER",
        "baseline",
        "49.5",
        "37.65",
        "51.0 36.03",
        "using FA",
        "50.0",
        "37.18",
        "51.7 35.52",
        "using IBM4",
        "50.0",
        "37.12",
        "51.7 35.43",
        "+ occ2",
        "50.2",
        "36.84",
        "52.0 35.10",
        "+ max dist 1",
        "50.0",
        "37.10",
        "51.7 35.51",
        "Table 6: Final results on EPPS English-Spanish, constrained triplet models, 10 EM iterations, compared to standard IBM model 1.",
        "eration using less than 2 GB of memory.",
        "However, this shows that triggers outside the immediate context help overall translation quality.",
        "Additionally, it supports the claim that the presented methods are a complementary alternative to the WSD approaches mentioned in Section 2 which only consider the immediate context of a single word.",
        "Finally, we compare the constrained models to an unconstrained setting and, again, to a standard IBM model 1.",
        "Table 6 shows that the palign model constrained on using the IBM-4 word alignments yields +0.7% in BLEU on TC-Star'06 which is +0.2% more than with a standard IBM model 1.",
        "TER decreases by -0.3% when compared to model 1.",
        "For the TC-Star 07 set, the observations are similar.",
        "The oracle TER scores of the development n-best list are 25.16% for English-Spanish and 27.0% for Spanish-English, respectively."
      ]
    },
    {
      "heading": "20. 30 EM iterations",
      "text": [
        "Figure 2: Effect of EM iterations on IWSLT'04, left axis shows BLEU (higher numbers better), right axis (dashed graph) shows TER score (lower numbers better).",
        "Table 1: Different setups showing the effect ofsingletons and empty words for IWSLT CE IWSLT'04 (dev) and IWSLT'05 (test) sets, Paü triplets, 20 EM iterations.",
        "we can quickly run the experiments on a full unconstrained triplet model without any cutoff or further constraints.",
        "Figure 2 shows the rescoring performance for different numbers of EM iterations.",
        "The first 10 iterations significantly improve the triplet model performance for the IWSLT task.",
        "After that, there are no big changes.",
        "The performance even degrades a little bit after 30 iterations.",
        "For the IWSLT task, we therefore set a fixed number of 20 EM iterations for the following experiments since it shows a good performance in terms of both BLEU and TER score.",
        "The oracle TER scores of the 10k-best lists are 14.18% for IWSLT'04, 11.36% for IWSLT'05 and 18.85% for IWSLT'07, respectively.",
        "The next chain of experiments on the IWSLT task investigates the impact of changes to the setup of training an unconstrained triplet model, such as the addition of the empty word and the inclusion of singletons (i.e. triplets that were only seen once in the",
        "Table2: Comparisonoftripletvariants onIWSLTCEtest sets, 20 EM iterations, with singletons and empty words.",
        "training data).",
        "This might show the importance of rare events in order to derive strategies when moving to larger tasks where it is not feasible to train all possible triplets, such as e.g. on the EPPS task (as shown later) or the Chinese-English NIST task.",
        "The results for the unconstrained model are shown in Table 1, beginning with a full triplet model in reverse direction, Pall(ef,f'), that contains no singletons and no empty words for the triggering side.",
        "In this setting, singletons seem to help on dev but there is no clear improvement on one of the test sets, whereas empty words do not make a significant difference but can be used since they do not harm either.",
        "The baseline can be improved by +0.6% BLEU and around -0.5% in TER on the IWSLT 04 set.",
        "For the various setups, there are no big differences in the TER score which might be an effect of optimization on BLEU.",
        "Therefore, for further experiments using the constraints from Section 3.2, we use both singletons and empty words as the default.",
        "Adding the other direction P(f |e, e') results in another increase, with a total of +0.8% BLEU and -0.8% TER, which shows that the combination of both directions helps overall translation quality.",
        "The results on the two test sets are shown in Table 2.",
        "As can be seen, we arrive at similar improvements, namely +0.6% BLEU and -0.3% TER on IWSLT'05 and +0.8% BLEU and -0.4% TER on IWSLT'07, respectively.",
        "The constrained models, i.e. the phrase-bounded (Pphr) and path-aligned (Palign) triplets are outperformed by the full unconstrained case, although on IWSLT 07 both unconstrained and path-aligned models are close.",
        "For a fair comparison, we added a classical IBM model 1 in the rescoring framework.",
        "It can be seen that the presented triplet models slightly outperform",
        "IWSLT'05",
        "IWSLT'07",
        "BLEU",
        "TER",
        "BLEU TER",
        "baseline",
        "61.1",
        "30.59",
        "38.9 45.60",
        "IBM model 1",
        "61.5",
        "30.29",
        "39.4 45.31",
        "trip fe+ef Pall",
        "61.7",
        "30.24",
        "39.7 45.24",
        "trip fe+ef Pphr",
        "61.5",
        "30.32",
        "39.1 45.36",
        "trip fe+ef V align",
        "61.2",
        "30.60",
        "39.7 45.02",
        "IWSLT'04",
        "IWSLT'05",
        "BLEU",
        "TER",
        "BLEU TER",
        "baseline",
        "56.7",
        "35.49",
        "61.1 30.59",
        "Pali(e|f, f ')",
        "57.1",
        "35.03",
        "61.3 30.55",
        "w/ singletons",
        "57.3",
        "35.04",
        "61.3 30.61",
        "w/ empties",
        "57.3",
        "35.00",
        "61.2 30.65",
        "+ Pail (f |e,e')",
        "57.5",
        "34.69",
        "61.7 30.24",
        "Table 3: Effect of using maximum distance constraint for Pan on EPPS Spanish-English test sets, occ3, 4 EM iterations due to time constraints.",
        "the simple IBM model 1.",
        "Note that IBM model 1 is a special case of the triplet lexicon model if the second trigger is the empty word."
      ]
    },
    {
      "heading": "5. Discussion",
      "text": [
        "From the results of our reranking experiments, we can conclude that the presented triplet lexicon model outperforms the baseline single-best hypotheses of the decoder.",
        "When comparing to a standard IBM model 1, the improvements are significantly smaller though measurable.",
        "So far, since IBM model 1 is considered one of the stronger rescoring models, these results look promising.",
        "An unconstrained triplet model has the best performance if training is feasible since it also needs the most memory and time to be trained, at least for larger tasks.",
        "In order to cut down computational requirements, we can apply phrase-bounded and path-aligned training constraints that restrict the possibilities of selecting triplet candidates (in addition to simple",
        "Table 7: Example of triplets and related IBM model 1 lexical probabilities.",
        "The triggers \"taxpayer\" and \"bill\" have a new effect (\"pagar\"), previously not seen in the top ranks of the lexicon.",
        "thresholding).",
        "Although no clear effect could be observed for adding empty words on the triggering side, it does not harm and, thus, we get a similar functionality to IBM model 1 being \"integrated\" in the triplet lexicon model.",
        "The phrase-bounded training variant uses forced alignments computed on the whole training data (i.e. search constrained to producing the target sentences of the bilingual corpus) but could not outperform the path-aligned model which reuses the alignment path information obtained in regular GIZA++ training.",
        "Additionally, we observe a positive impact from triggers lying outside the immediate context of one predecessor or successor word.",
        "Table 7 shows an excerpt of the top entries for (e, e') = (taxpayer, bill) and compares it to the top entries of a lexicon based on IBM model 1.",
        "We observe a triggering effect since the Spanish word pagar (to pay) is triggered at top position by the two English words taxpayer and bill.",
        "The average distance of taxpayer and bill is 5.4 words.",
        "The models presented in this work try to capture this property and apply it in the scoring of hypotheses in order to allow for better lexical choice in specific contexts.",
        "In Table 8, we show an example translation where rescoring with the triplet model achieves higher n-gram coverage on the reference translation than the variant based on IBM model 1 rescoring.",
        "The differing phrases are highlighted.",
        "TC-Star'06",
        "TC-Star'07",
        "BLEU",
        "TER",
        "BLEU TER",
        "baseline",
        "49.5",
        "37.65",
        "51.0 36.03",
        "IBM model 1",
        "50.0",
        "37.12",
        "51.8 35.51",
        "Pall, OCC3",
        "50.0",
        "37.17",
        "51.8 35.43",
        "Pphr, OCC2",
        "50.2",
        "37.06",
        "51.8 35.32",
        "V align, OCC2",
        "50.2",
        "36.84",
        "52.0 35.10",
        "f",
        "e",
        "e'",
        "a(f |e,e')",
        "pagar",
        "taxpayer",
        "bill",
        "0.76",
        "factura",
        "taxpayer",
        "bill",
        "0.11",
        "contribuyente",
        "taxpayer",
        "bill",
        "0.10",
        "f",
        "e",
        "-",
        "Pibmi (f |e)",
        "contribuyente",
        "taxpayer",
        "0.40",
        "contribuyentes",
        "taxpayer",
        "0.18",
        "europeo",
        "taxpayer",
        "0.08",
        "factura",
        "bill",
        "0.19",
        "ley",
        "bill",
        "0.18",
        "proyecto",
        "bill",
        "0.11",
        "Table 8: A translation example on TC-Star 07 Spanish-English comparing the effect of the triplet model to a standard IBM-1 model."
      ]
    },
    {
      "heading": "6. Outlook",
      "text": [
        "We have presented a new lexicon model based on triplets extracted on a sentence level and trained iteratively using the EM algorithm.",
        "The motivation of this approach is to add an additional second trigger to a translation lexicon component which can come from a more global context (on a sentence level) and allow for a more fine-grained lexical choice given a specific context.",
        "Thus, the method is related to word sense disambiguation approaches.",
        "We showed improvements by rescoring n-best lists of the IWSLT Chinese-English and EPPS",
        "Spanish-English/English-Spanish task.",
        "In total, we achieve up to +1% BLEU for some of the test sets in comparison to the decoder baseline and up to +0.3% BLEU compared to IBM model 1.",
        "Future work will address an integration into the decoder since the performance of the current rescor-ing framework is limited by the quality of the n-best lists.",
        "For the inverse model, p(e|f, f '), an integration into the search is directly possible.",
        "Further experiments will be conducted, especially on large tasks such as the NIST Chinese-English and Arabic-English task.",
        "Training on these huge databases will only be possible with an appropriate selection of promising triplets."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This material is partly based upon work supported by the Defense Advanced Research Projects Agency and was partly realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation.",
        "The authors would like to thank the anonymous reviewers for their valuable comments.",
        "Source sentence",
        "... respecto de la Position Comun del Consejo con vistas a la adoption del Reglamento del Parlamente Eu-ropeo y del Consejo relativo al...",
        "IBM-1",
        "rescoring",
        "... on the Council common position with a view to the adoption of the Rules of Procedure of the European Parliament and of the Council...",
        "Triplet rescoring",
        "... on the common position of the Council with a view to the adoption of the regulation of the European Parliament and of the Council",
        "Reference translation",
        "... as regards the Common Position of the Council with a view to the adoption of a European Parliament and Council Regulation as regards the..."
      ]
    }
  ]
}
