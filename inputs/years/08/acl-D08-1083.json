{
  "info": {
    "authors": [
      "Yejin Choi",
      "Claire Cardie"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-D08-1083",
    "title": "Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis",
    "url": "https://aclweb.org/anthology/D08-1083",
    "year": 2008
  },
  "references": [
    "acl-C04-1200"
  ],
  "sections": [
    {
      "text": [
        "Yejin Choi and Claire Cardie",
        "Determining the polarity of a sentiment-bearing expression requires more than a simple bag-of-words approach.",
        "In particular, words or constituents within the expression can interact with each other to yield a particular overall polarity.",
        "In this paper, we view such subsentential interactions in light of compositional semantics, and present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.",
        "Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%).",
        "We also find that \"content-word negators\", not widely employed in previous work, play an important role in determining expression-level polarity.",
        "Finally, in contrast to conventional wisdom, we find that expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Determining the polarity of sentiment-bearing expressions at or below the sentence level requires more than a simple bag-of-words approach.",
        "One of the difficulties is that words or constituents within the expression can interact with each other to yield a particular overall polarity.",
        "To facilitate our discussion, consider the following examples:",
        "In the first example, \"doubt\" in isolation carries a negative sentiment, but the overall polarity of the sentence is positive because there is a negator \"not\", which flips the polarity.",
        "In the second example, both \"eliminated\" and \"doubt\" carry negative sentiment in isolation, but the overall polarity of the sentence is positive because \"eliminated\" acts as a negator for its argument \"doubt\".",
        "In the last example, there are effectively two negators - \"not\" and \"eliminated\" which reverse the polarity of \"doubt\" twice, resulting in the negative polarity for the overall sentence.",
        "These examples demonstrate that words or constituents interact with each other to yield the expression-level polarity.",
        "And a system that simply takes the majority vote of the polarity of individual words will not work well on the above examples.",
        "Indeed, much of the previous learning-based research on this topic tries to incorporate salient interactions by encoding them as features.",
        "One approach includes features based on contextual valence shifters (Polanyi and Zaenen, 2004), which are words that affect the polarity or intensity of sentiment over neighboring text spans (e.g., Kennedy al.",
        "(2007)).",
        "Another approach encodes frequent sub-sentential patterns (e.g., McDonald et al.",
        "(2007)) as features; these might indirectly capture some of the subsentential interactions that affect polarity.",
        "How-",
        "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793-801, Honolulu, October 2008.",
        "©2008 Association for Computational Linguistics",
        "ever, both types of approach are based on learning models with a flat bag-of-features: some structural information can be encoded as higher order features, but the final representation of the input is still a flat feature vector that is inherently too limited to adequately reflect the complex structural nature of the underlying subsentential interactions.",
        "(Liang et al.,",
        "Moilanen and Pulman (2007), on the other hand, handle the structural nature of the interactions more directly using the ideas from compositional semantics (e.g., Montague (1974), Dowty et al.",
        "(1981)).",
        "In short, the Principle ofCompositionality states that the meaning of a compound expression is a function of the meaning of its parts and of the syntactic rules by which they are combined (e.g., Montague (1974), Dowty et al.",
        "(1981)).",
        "And Moilanen and Pulman (2007) develop a collection of composition rules to assign a sentiment value to individual expressions, clauses, or sentences.",
        "Their approach can be viewed as a type of structural inference, but their handwritten rules have not been empirically compared to learning-based alternatives, which one might expect to be more effective in handling some aspects of the polarity classification task.",
        "In this paper, we begin to close the gap between learning-based approaches to expression-level polarity classification and those founded on compositional semantics: we present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.",
        "Adopting the view point of compositional semantics, our working assumption is that the polarity of a sentiment-bearing expression can be determined in a two-step process: (1) assess the polarities of the constituents of the expression, and then (2) apply a relatively simple set of inference rules to combine them recursively.",
        "Rather than a rigid application of handwritten compositional inference rules, however, we hypothesize that an ideal solution to the expression-level polarity classification task will be a method that can exploit ideas from compositional semantics while providing the flexibility needed to handle the complexities of real-world natural language – exceptions, unknown words, missing semantic features, and inaccurate or missing rules.",
        "The learning-based approach proposed in this paper takes a first step in this direction.",
        "In addition to the novel learning approach, this paper presents new insights for content-word negators, which we define as content words that can negate the polarity of neighboring words or constituents.",
        "(e.g., words such as \"eliminated\" in the example sentences).",
        "Unlike function-word negators, such as \"not\" or \"never\", content-word negators have been recognized and utilized less actively in previous work.",
        "(Notable exceptions include e.g., Niu et al.",
        "(2005), Wilson et al.",
        "(2005), and Moilanen",
        "In our experiments, we compare learning-and non-learning-based approaches to expression-level polarity classification – with and without compositional semantics – and find that (1) simple heuristics based on compositional semantics outperform (89.7% in accuracy) other reasonable heuristics that do not incorporate compositional semantics (87.7%); they can also perform better than simple learning-based methods that do not incorporate compositional semantics (89.1%), (2) combining learning with the heuristic rules based on compositional semantics further improves the performance (90.7%), (3) content-word negators play an important role in determining the expression-level polarity, and, somewhat surprisingly, we find that (4) expression-level classification accuracy uniformly decreases as additional, potentially disambiguating, context is considered.",
        "In what follows, we first explore heuristic-based approaches in §2, then we present learning-based approaches in §3.",
        "Next we present experimental results in §4, followed by related work in §5."
      ]
    },
    {
      "heading": "2. Heuristic-Based Methods",
      "text": [
        "This section describes a set of heuristic-based methods for determining the polarity of a sentiment-bearing expression.",
        "Each assesses the polarity of the words or constituents using a polarity lexicon that indicates whether a word has positive or negative polarity, and finds negators in the given expression using a negator lexicon.",
        "The methods then infer the expression-level polarity using voting-based heuristics (§ 2.1) or heuristics that incorporate compositional semantics (§2.2).",
        "The lexicons are described",
        "Definition of Compose( arg1, arg2 ) (COMPOsition with Majority Class) else if (Polarity( arg1 ) == Polarity( arg2 )) then Polarity( arg1 )",
        "else the majority polarity of data Compose( arg1, arg2 ) = For COMPOPR: if (arg1 is a negator) then - Polarity( arg2 ) (COMPOsition with PRiority) else Polarity( arg 1 )",
        "We first explore five simple heuristics based on voting.",
        "Vote is defined as the majority polarity vote by words in a given expression.",
        "That is, we count the number of positive polarity words and negative polarity words in a given expression, and assign the majority polarity to the expression.",
        "In the case of a tie, we default to the prevailing polarity of the data.",
        "For Neg(1), we first determine the majority polarity vote as above, and then if the expression contains any function-word negator, flip the polarity of the majority vote once.",
        "Neg(n) is similar to Neg(1), except we flip the polarity of the majority vote n times after the majority vote, where n is the number of function-word negators in a given expression.",
        "NegEx(1) and NegEx(n) are defined similarly as Neg(1) and Neg(n) above, except both function-word negators and content-word negators are considered as negators when flipping the polarity of the majority vote.",
        "See Table 1 for summary.",
        "Note that a word can be both a negator and have a negative prior polarity.",
        "For the purpose of voting, if a word is defined as a negator per the voting scheme, then that word does not participate in the majority vote.",
        "Whereas the heuristics above use voting-based inference, those below employ a set of handwritten rules motivated by compositional semantics.",
        "Table 2 shows the definition of the rules along with motivating examples.",
        "In order to apply a rule, we first detect a syntactic pattern (e.g., [destroyed]VP [the terrorism]NP), then apply the Compose function as defined in Table 2 (e.g., Compose([destroyed], [the terrorism]) by rule #2).",
        "For brevity, we refer to Neg(1) and Neg(n) collectively as Neg, and NegEx(1) and NegEx(n) collectively as NegEx.",
        "Vote",
        "neg(l)",
        "neg(n)",
        "NegEx(I)",
        "NegEx(n)",
        "compo",
        "type of negators",
        "none",
        "function-word",
        "function-word & content-word",
        "maximum # of negations applied",
        "0",
        "1",
        "n",
        "1",
        "n",
        "n",
        "scope of negators",
        "N/A",
        "over the entire expression",
        "compositional",
        "Rules",
        "Examples",
        "1",
        "Polarity(not_[argl] ) =",
        "-i Polarity( argl )",
        "not[bad]orfli.",
        "2",
        "Polarity( [VP]_[NP] ) =",
        "Compose( [VP], [NP] )",
        "[destroyed]vp [the terrorism]NP.",
        "3",
        "Polarity( [VPl]_to_[VP2] ) =",
        "Compose( [VP1], [VP2] )",
        "[refused] y pi to [deceive]yp2 the man.",
        "4",
        "Polarity( [adj]_to_[VP] ) =",
        "Compose( [adj], [VP] )",
        "[unlikely] to [destroy] vp the planet.",
        "5",
        "Polarity( [NP1]_[IN]_[NP2] ) =",
        "Compose( [NP1], [NP2] )",
        "[lack]Arpi [of]in [crime]np2 in rural areas.",
        "6",
        "Polarity( [NP]_[VP] ) =",
        "Compose( [VP], [NP] )",
        "[pollution]Np [has decreased]y p.",
        "7",
        "Polarity( [NP]_be_[adj] ) =",
        "Compose( [adj], [NP] )",
        "[harm] is [minimal] a(§.",
        "Compose first checks whether the first argument is a negator, and if so, flips the polarity of the second argument.",
        "Otherwise, Compose resolves the polarities of its two arguments.",
        "Note that if the second argument is a negator, we do not flip the polarity of the irst argument, because the irst argument in general is not in the semantic scope of the negation.",
        "Instead, we treat the second argument as a constituent with negative polarity.",
        "We experiment with two variations of the Compose function depending on how conflicting polarities are resolved: COMPOMC uses a Compose function that defaults to the Majority Class of the polarity of the data, while CompoPR uses a Compose function that selects the polarity of the argument that has higher semantic PRiority.",
        "For brevity, we refer to CompoPR and CompoMC collectively as Compo.",
        "The polarity lexicon is initialized with the lexicon of Wilson et al.",
        "(2005) and then expanded using the General Inquirer dictionary.",
        "In particular, a word contained in at least two of the following categories is considered as positive: Positiv, Pstv, PosAff, Pleasur, Virtue, Increas, and a word contained in at least one of the following categories is considered as negative: Negativ, Ngtv, NegAff, Pain, Vice, Hostile, fail, EnlLoss, WlbLoss, Tran-Loss.",
        "For the (function- and content-word) negator lexicon, we collect a handful of seed words as well as General Inquirer words that appear in either NotLw or Decreas category.",
        "Then we expand the list of content-negators using the synonym information of WordNet (Miller, 1995) to take a simple vote among senses.",
        "based on parse trees might further improve the performance."
      ]
    },
    {
      "heading": "3. Learning-Based Methods",
      "text": [
        "While we expect that a set of handwritten heuristic rules motivated by compositional semantics can be effective for determining the polarity of a sentiment-bearing expression, we do not expect them to be perfect.",
        "Interpreting natural language is such a complex task that writing a perfect set of rules would be extremely challenging.",
        "Therefore, a more ideal solution would be a learning-based method that can exploit ideas from compositional semantics while providing the flexibility to the rigid application of the heuristic rules.",
        "To this end, we present a novel learning-based approach that incorporates inference rules inspired by compositional semantics into the learning procedure (§3.2).",
        "To assess the effect of compositional semantics in the learning-based methods, we also experiment with a simple classiication approach that does not incorporate compositional semantics (§3.1).",
        "The details of these two approaches are elaborated in the following subsections.",
        "Given an expression x consisting of n words x\\, xn, the task is to determine the polarity y € {positive, negative} of x.",
        "In our simple binary classiication approach, x is represented as a vector of features f (x), and the prediction y is given by argmaxy w • f(x, y ), where w is a vector of parameters learned from training data.",
        "In our experiment, we use an online SVM algorithm called MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003) for training.",
        "For each x, we encode the following features:",
        "• Lexical: We add every word xi in x, and also add the lemma of xi produced by the CASS partial parser toolkit (Abney, 1996).",
        "• Dictionary: In order to mitigate the problem of unseen words in the test data, we add features that describe word categories based on the General Inquirer dictionary.",
        "We add this feature for each xi that is not a stop word.",
        "• vote: We experiment with two variations of voting-related features: for SC-Vote, we add",
        "Simple Classification",
        "Classification with Compositional Inference Find K best z and denote them as Z = {z(1),z(K)}",
        "(if such z6ad not found in Z, skip parameter update for this.)",
        "If loss_compo(y*, z*, x) > 0",
        "zgood j_ mjnfe z(fc) s ^ loss_compo(y*, z^fc\\ x) = 0",
        "(if such zg°°d not found in Z, stick to the original z*.)",
        "/ < – loss_compo(y*, z6ad, x) - loss_compo(y*, z*, x) w < – update(w, /, z*,zbad) Deinitions of score functions and loss functions",
        "Figure 1: Training procedures.",
        "y* € {positive, negative} denotes the true label for a given expression x = x1, ...,xn.",
        "z* denotes the pseudo gold standard for hidden variables z.",
        "a feature that indicates the dominant polarity of words in the given expression, without considering the effect of negators.",
        "For SC-NegEx, we count the number of content-word negators as well as function-word negators to determine whether the inal polarity should be flipped.",
        "Then we add a conjunctive feature that indicates the dominant polarity together with whether the final polarity should be flipped.",
        "For brevity, we refer to SC-Vote and SC-NegEx collectively as SC.",
        "Notice that in this simple binary classiication setting, it is inherently dificult to capture the compositional structure among words in x, because f(x, y) is merely a flat bag of features, and the prediction is governed simply by the dot product of f(x, y) and the parameter vector w.",
        "Next, instead of determining y directly from x, we introduce hidden variables z = (z\\,...,zn) as intermediate decision variables, where zi € {positive, negative, negator, none}, so that zirepresents whether xi is a word with positive/negative polarity, or a negator, or none of the above.",
        "For simplicity, we let each intermediate decision variable zi (a) be determined independently from other intermediate decision variables, and (b)",
        "For each token xi; if x% is a word in the negator lexicon",
        "then z* – negator else if x% is in the polarity lexicon as negative",
        "then z* – negative else if x% is in the polarity lexicon as positive",
        "then z* – positive else",
        "then z* – none Figure 2: Constructing Soft Gold Standard z* depend only on the input x, so that zi = argmaxz.",
        "w • f(x, zi, i), where f(x, zi, i) is the feature vector encoding around the ith word (described on the next page).",
        "once we determine the intermediate decision variables, we apply the heuristic rules motivated by compositional semantics (from Table 2) in order to obtain the inal polarity y of x.",
        "That is, y = C(x, z), where C is the function that applies the compositional inference, either CompoPR or CompoMC.",
        "For training, there are two issues we need to handle: the irst issue is dealing with the hidden variables z.",
        "Because the structure of compositional inference C does not allow dynamic programming, it is intractable to perform exact expectation-maximization style training that requires enumerating all possible values of the hidden variables z.",
        "Instead, we propose a simple and tractable training rule based on the creation of a soft gold standard for z.",
        "In particular, we exploit the fact that in our task, we can automatically construct a reasonably accurate gold standard for z, denoted as z* : as shown in Figure 2, we simply rely on the negator and polarity lexicons.",
        "Because z* is not always correct, we allow the training procedure to replace z* with potentially better assignments as learning proceeds: in the event that the soft gold standard z* leads to an incorrect prediction, we search for an assignment that leads to a correct prediction to replace z* .",
        "The exact procedure is given in Figure 1, and will be discussed again shortly.",
        "Figure 1 shows how we modify the parameter update rule of MIRA (Crammer and Singer, 2003) to reflect the aspect of compositional inference.",
        "In the event that the soft gold standard z* leads to an incorrect prediction, we search for zgood, the assignment with highest score that leads to a correct prediction, and replace z* with zgood.",
        "In the event of no such zgood being found among the K-best assignments of z, we stick with z*.",
        "The second issue is finding the assignment of z with the highest score(z) = J2%w • f(x, z%, i) that leads to an incorrect prediction y = C (x, z).",
        "Because the structure of compositional inference C does not allow dynamic programming, finding such an assignment is again intractable.",
        "We resort to enumerating only over K-best assignments instead.",
        "If none of the K-best assignments of z leads to an incorrect prediction y, then we skip the training instance for parameter update.",
        "Features.",
        "For each x% in x, we encode the following features:",
        "• Lexical: We include the current word x% as well as the lemma of x% produced by CASS partial parser toolkit (Abney, 1996).",
        "We also add a boolean feature to indicate whether the current word is a stop word.",
        "• Dictionary: In order to mitigate the problem with unseen words in the test data, we add features that describe word categories based on the General Inquirer dictionary.",
        "We add this feature for each x% that is not a stop word.",
        "We also add a number of boolean features that provide following properties of x% using the polarity lexicon and the negator lexicon:",
        "- whether x% is a function-word negator - whether x% is a content-word negator - whether x% is a negator of any kind",
        "- the polarity of x% according to Wilson et al.",
        "(2005)'s polarity lexicon",
        "- the polarity of x% according to the lexicon derived from the General Inquirer dictionary - conjunction of the above two features",
        "• Vote: We encode the same vote feature that we use for SC-NegEx described in § 3.1.",
        "As in the heuristic-based compositional semantics approach (§ 2.2), we experiment with two variations of this learning-based approach: CCI-CompoPR and CCI-CompoMC, whose compositional inference rules are CompoPR and CompoMC respectively.",
        "For brevity, we refer to both variations collectively as CCI-Compo."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "The experiments below evaluate our heuristic-and learning-based methods for subsentential sentiment analysis (§ 4.1).",
        "In addition, we explore the role of context by expanding the boundaries of the sentiment-bearing expressions (§ 4.2).",
        "For evaluation, we use the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005), which consists of 535 newswire documents manually annotated with phrase-level subjectivity information.",
        "We evaluate on all strong (i.e., intensity of expression is 'medium' or higher), sentiment-bearing (i.e., polarity is 'positive' or 'negative') expressions.",
        "As a result, we can assume the boundaries of the expressions are given.",
        "performance is reported using 10-fold cross-validation on 400 documents; a separate 135 documents were used as a development set.",
        "Based on pilot experiments on the development data, we set parameters for MIRA as follows: slack variable to 0.5, and the number of incorrect labels (constraints) for each parameter update to 1.",
        "The number of iterations (epochs) for training is set to 1 for simple classification, and to 4 for classification with compositional inference.",
        "We use K = 20 for classification with compositional inference.",
        "Results.",
        "Performance is reported in Table 3.",
        "Interestingly, the heuristic-based methods Neg (~ 82.2%) that only consider function-word negators perform even worse than Vote (86.5%), which does not consider negators.",
        "On the other hand, the NegEx methods (87.7%) that do consider content-word negators as well as function-word negators perform better than Vote.",
        "This confirms the importance of content-word negators for determining the polarities of expressions.",
        "The heuristic-based methods motivated by compositional semantics Compo further improve the performance over NegEx, achieving up to 89.7% accuracy.",
        "In fact, these heuristics perform even better than the SC learning-based methods (~ 89.1%).",
        "This shows that heuristics that take into account the compositional structure of the expression can perform better than learning-based methods that do not exploit such structure.",
        "Finally, the learning-based methods that incorporate compositional inference CCI-Compo (~ 90.7%) perform better than all of the previous methods.",
        "The difference between CCI-CompoPR (90.7%) and SC-NegEx (89.1%) is statistically significant at the .05 level by paired t-test.",
        "The difference between Compo and any other heuristic that is not based on computational semantics is also statistically significant.",
        "In addition, the difference between CCICompoPR (learning-based) and Com-poMC (non-learning-based) is statistically significant, as is the difference between NegEx and Vote.",
        "one might wonder whether employing additional context outside the annotated expression boundaries could further improve the performance.",
        "Indeed, conventional wisdom would say that it is necessary to employ such contextual information (e.g., Wilson et al.",
        "(2005)).",
        "In any case, it is important to determine whether our results will apply to more real-world settings where human-annotated expression boundaries are not available.",
        "To address these questions, we gradually relax our previous assumption that the exact boundaries of expressions are given: for each annotation boundary, we expand the boundary by x words for each direction, up to sentence boundaries, where x € {1, 5, oo}.",
        "We stop expanding the boundary if it will collide with the boundary of an expression with a different polarity, so that we can consistently recover the expression-level gold standard for evaluation.",
        "This expansion is applied to both the training and test data, and the performance is reported in Table 4.",
        "From this experiment, we make the following observations:",
        "• Expanding the boundaries hurts the perfor-",
        "Heuristic-Based",
        "Learning-Based",
        "Vote Neg Neg Neg Neg (1) (n) Ex Ex (1) (n)",
        "compo compo MC PR",
        "sc sc Vote Neg EX",
        "CCI CCI compo compo MC PR",
        "86.5 82.0 82.2 87.7 87.7",
        "89.7 89.4",
        "88.5 89.1",
        "90.6 90.7",
        "Data",
        "Heuristic-Based",
        "Learning-Based",
        "Vote Neg Neg Neg Neg (1) (n) Ex Ex (1) (n)",
        "compo compo MC PR",
        "sc sc Vote Neg EX",
        "CCI CCI compo compo MC PR",
        "[-0.+0]",
        "[-5,+5]",
        "[-oo,+oo]",
        "86.5 82.0 82.2 87.7 87.7 86.4 81.0 81.2 87.2 87.2 85.9 79.0 79.4 85.7 85.6 85.3 75.8 76.9 83.9 83.9",
        "89.7 89.4 89.3 89.0 88.2 88.0 87.0 86.9",
        "88.5 89.1",
        "88.3 88.4",
        "86.4 87.1 85.8 85.8",
        "90.6 90.7 89.5 89.4",
        "88.7 88.7 87.3 87.5",
        "mance for any method.",
        "This shows that most of relevant context for judging the polarity is contained within the expression boundaries, and motivates the task of finding the boundaries of opinion expressions.",
        "• The NegEx methods perform better than Vote only when the expression boundaries are reasonably accurate.",
        "When the expression boundaries are expanded up to sentence boundaries, they perform worse than Vote.",
        "We conjecture this is because the scope of negators tends to be limited to inside of expression boundaries.",
        "• The Compo methods always perform better than any other heuristic-based methods.",
        "And their performance does not decrease as steeply as the NegEx methods as the expression boundaries expand.",
        "We conjecture this is because methods based on compositional semantics can handle the scope ofnegators more adequately.",
        "• Among the learning-based methods, those that involve compositional inference (CCI-Compo) always perform better than those that do not (SC) for any boundaries.",
        "And learning with compositional inference tend to perform better than the rigid application of heuristic rules (Compo), although the relative performance gain decreases once the boundaries are relaxed."
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "The task focused on in this paper is similar to that of Wilson et al.",
        "(2005) in that the general goal of the task is to determine the polarity in context at a subsentence level.",
        "However, Wilson et al.",
        "(2005) formulated the task differently by limiting their evaluation to individual words that appear in their polarity lexicon.",
        "Also, their approach was based on a flat bag of features, and only a few examples of what we call content-word negators were employed.",
        "our use of compositional semantics for the task of polarity classification is preceded by Moilanen and Pulman (2007), but our work differs in that we integrate the key idea of compositional semantics into learning-based methods, and that we perform empirical comparisons among reasonable alternative approaches.",
        "For comparison, we evaluated our approaches on the polarity classification task from SemEval-07 (Strapparava and Mihalcea, 2007).",
        "We achieve 88.6% accuracy with CompoPR, 90.1% with SCNegEx, and 87.6% with CCICom-poMC.",
        "There are a number of possible reasons for our lower performance vs. Moilanen and Pulman (2007) on this data set.",
        "First, SemEval-07 does not include a training data set for this task, so we use 400 documents from the MPQA corpus instead.",
        "In addition, the SemEval-07 data is very different from the MPQA data in that (1) the polarity annotation is given only at the sentence level, (2) the sentences are shorter, with simpler structure, and not as many negators as the MPQA sentences, and (3) there are many more instances with positive polarity than in the MPQA corpus.",
        "Nairn et al.",
        "(2006) also employ a \"polarity\" propagation algorithm in their approach to the semantic interpretation of implicatives.",
        "However, their notion of polarity is quite different from that assumed here and in the literature on sentiment analysis.",
        "In particular, it refers to the degree of \"commitment\" of the author to the truth or falsity of a complement clause for a textual entailment task.",
        "McDonald et al.",
        "(2007) use a structured model to determine the sentence-level polarity and the document-level polarity simultaneously.",
        "But decisions at each sentence level does not consider structural inference within the sentence.",
        "Among the studies that examined content-word negators, Niu et al.",
        "(2005) manually collected a small set of such words (referred as \"words that change phases\"), but their lexicon was designed mainly for the medical domain and the type of negators was rather limited.",
        "Wilson et al.",
        "(2005) also manually collected a handful of content-word negators (referred as \"general polarity shifters\"), but not extensively.",
        "Moilanen and Pulman (2007) collected a more extensive set of negators semi-automatically using WordNet 2.1, but the empirical effect of such words was not explicitly investigated."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "In this paper, we consider the task of determining the polarity of a sentiment-bearing expression, considering the effect of interactions among words or constituents in lightofcompositionalsemantics.",
        "We presented a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.",
        "our approach can be considered as a small step toward bridging the gap between computational semantics and machine learning methods.",
        "our experimental results suggest that this direction of research is promising.",
        "Future research includes an approach that learns the compositional inference rules from data."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported in part by National Science and by Department of Homeland Security Grant N0014-07-1-0152.",
        "We also thank Eric Breck, Lillian Lee, Mats Rooth, the members of the Cornell NLP reading seminar, and the EMNLP reviewers for insightful comments on the submitted version of the paper."
      ]
    }
  ]
}
