{
  "info": {
    "authors": [
      "Amarnag Subramanya",
      "Jeff A. Bilmes"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-D08-1114",
    "title": "Soft-Supervised Learning for Text Classification",
    "url": "https://aclweb.org/anthology/D08-1114",
    "year": 2008
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Amarnag Subramanya & Jeff Bilmes",
        "of Electrical Engineering, University of Washington, Seattle, WA 98195, USA.",
        "We propose a new graph-based semi-supervised learning (SSL) algorithm and demonstrate its application to document categorization.",
        "Each document is represented by a vertex within a weighted undirected graph and our proposed framework minimizes the weighted Kullback-Leibler divergence between distributions that encode the class membership probabilities of each vertex.",
        "The proposed objective is convex with guaranteed convergence using an alternating minimization procedure.",
        "Further, it generalizes in a straightforward manner to multi-class problems.",
        "We present results on two standard tasks, namely Reuters-21578 and WebKB, showing that the proposed algorithm significantly outperforms the state-of-the-art."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Semi-supervised learning (SSL) employs small amounts of labeled data with relatively large amounts of unlabeled data to train classifiers.",
        "In many problems, such as speech recognition, document classification, and sentiment recognition, annotating training data is both time-consuming and tedious, while unlabeled data are easily obtained thus making these problems useful applications of SSL.",
        "Classic examples of SSL algorithms include self-training (Yarowsky, 1995) and co-training (Blum and Mitchell, 1998).",
        "Graph-based SSL algorithms are an important class of SSL techniques that have attracted much of attention of late (Blum and Chawla, 2001; Zhu et al., 2003).",
        "Here one assumes that the data (both labeled and unlabeled) is embedded within a low-dimensional manifold expressed by a graph.",
        "In other words, each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices.",
        "Most graph-based SSL algorithms fall under one of two categories - those that use the graph structure to spread labels from labeled to unlabeled samples (Szummer and Jaakkola, 2001; Zhu and Ghahra-mani, 2002) and those that optimize a loss function based on smoothness constraints derived from the graph (Blum and Chawla, 2001; Zhu et al., 2003; Joachims, 2003; Belkin et al., 2005).",
        "Sometimes the two categories are similar in that they can be shown to optimize the same underlying objective (Zhu and Ghahramani, 2002; Zhu et al., 2003).",
        "In general graph-based SSL algorithms are non-parametric and transductive.",
        "A learning algorithm is said to be transductive if it is expected to work only on a closed data set, where a test set is revealed at the time of training.",
        "In practice, however, transductive learners can be modified to handle unseen data (Zhu, 2005a; Sindhwani et al., 2005).",
        "A common drawback of many graph-based SSL algorithms (e.g. (Blum and Chawla, 2001; Joachims, 2003; Belkin et al., 2005)) is that they assume binary classification tasks and thus require the use of suboptimal (and often computationally expensive) approaches such as one vs. rest to solve multi-class problems, let alone structured domains such as strings and trees.",
        "There are also issues related to degenerate solutions (all un-labeled samples classified as belonging to a single",
        "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090-1099, Honolulu, October 2008.",
        "©2008 Association for Computational Linguistics",
        "class) (Blum and Chawla, 2001; Joachims, 2003; Zhu and Ghahramani, 2002).",
        "For more background on graph-based and general SSL and their applications, see (Zhu, 2005a; Chapelle et al., 2007; Blitzer and Zhu, 2008).",
        "In this paper we propose a new algorithm for graph-based SSL and use the task of text classification to demonstrate its benefits over the current state-of-the-art.",
        "Text classification involves automatically assigning a given document to a fixed number of semantic categories.",
        "Each document may belong to one, many, or none of the categories.",
        "In general, text classification is a multi-class problem (more than 2 categories).",
        "Training fully-supervised text classifiers requires large amounts of labeled data whose annotation can be expensive (Dumais et al., 1998).",
        "As a result there has been interest is using SSL techniques for text classification (Joachims, 1999; Joachims, 2003).",
        "However past work in semi-supervised text classification has relied primarily on one vs. rest approaches to overcome the inherent multi-class nature of this problem.",
        "We believe such an approach may be suboptimal because, disregarding data overlap, the different classifiers have training procedures that are independent of one other.",
        "In order to address the above drawback we propose a new framework based on optimizing a loss function composed of Kullback-Leibler divergence (KL-divergence) (Cover and Thomas, 1991) terms between probability distributions defined for each graph vertex.",
        "The use of probability distributions, rather than fixed integer labels, not only leads to a straightforward multi-class generalization, but also allows us to exploit other well-defined functions of distributions, such as entropy, to improve system performance and to allow for the measure of uncertainty.",
        "For example, with a single integer, at most all we know is its assignment.",
        "With a distribution, we can continuously move from knowing an assignment with certainty (i.e., an entropy of zero) to expressions of doubt or multiple valid possibilities (i.e., an entropy greater than zero).",
        "This is particularly useful for document classification as we will see.",
        "We also show how one can use the alternating minimization (Csiszar and Tusnady, 1984) algorithm to optimize our objective leading to a relatively simple, fast, easy-to-implement, guaranteed to converge, iterative, and closed form update for each iteration."
      ]
    },
    {
      "heading": "2. Proposed Graph-Based Learning Framework",
      "text": [
        "We consider the transductive learning problem, i.e., given a training set V = {V\\, Vu }, where V\\ and Vuare the sets of labeled and unlabeled samples respectively, the task is to infer the labels for the samples in Vu.",
        "In other words, Vu is the \"test-set.\"",
        "Here Vl = {(xt,yi)}ll=l, Vu = {x,}^+1, x, G X (the input space of the classifier, and corresponds to vectors of features) and îji G Y (the space of classifier outputs, and for our case is the space of non-negative integers).",
        "Thus |Y| = 2 yields binary classification while |Y| > 2 yields multi-class.",
        "We define n = I + u, the total number of samples in the training set.",
        "Given V, most graph-based SSL algorithms utilize an undirected weighted graph Ç = (V,E) where V = {1,..., n} are the data points in V and E = V x V are the set of undirected edges between vertices.",
        "We use Wij G W to denote the weight of the edge between vertices i and j. W is referred to as the weight (or affinity) matrix of Q.",
        "As will be seen shortly, the input features effect the final classification results via W, i.e., the graph.",
        "Thus graph construction is crucial to the success of any graph-based SSL algorithm.",
        "Graph construction \"is more of an art, than science\" (Zhu, 2005b) and is an active research area (Alexandrescu and Kirch-hoff, 2007).",
        "In general the weights are formed as Wij = sim(xi, Xj)5(j G AC(i)).",
        "Here K(i) is the set of i's fc-nearest-neighbors (/CNN), sim(xi,Xj) is a given measure of similarity between x^ and Xj, and 6(c) returns a 1 if c is true and 0 otherwise.",
        "Getting the similarity measure right is crucial for the success of any SSL algorithm as that is what determines the graph.",
        "Note that setting K(i) = \\V\\ = n results in a fully-connected graph.",
        "Some popular similarity measures include",
        "II xi Ibll xi lb",
        "where || x^ ||2 is the £2 norm, and (xj,x-,-) is the inner product of x^ and Xj.",
        "The first similarity measure is an RBF kernel applied on the squared Euclidean distance while the second is cosine similarity.",
        "In this paper all graphs are constructed using cosine similarity.",
        "We next introduce our proposed approach.",
        "For every i G V, we define a probability distribution pi over the elements of Y.",
        "In addition let rj, j = 1.. .1 be another set of probability distributions again over the elements of Y (recall, Y is the space of classifier outputs).",
        "Here {fj}j represents the labels of the supervised portion of the training data.",
        "If the label for a given labeled data point consists only of a single integer, then the entropy of the corresponding rj is zero (the probability of that integer will be unity, with the remaining probabilities being zero).",
        "If, on the other hand, the \"label\" for a given labeled data point consists of a set of integers (e.g., if the object is a member of multiple classes), then rj is able to represent this property accordingly (see below).",
        "We emphasize again that both pi and r, are probability distributions, with rj fixed throughout training.",
        "The goal of learning in this paper is to find the best set of distributions pi, Mi that attempt to: 1) agree with the labeled data rj wherever it is available; 2) agree with each other (when they are close according to a graph); and 3) be smooth in some way.",
        "These criteria are captured in the following new multi-class SSL optimization procedure:",
        "+^YlYlwhdkl(pi\\\\Pj) H(pi and where p = (pi,...,pn) denotes the entire set of distributions to be learned, H(pi) = – J2yPi(y)\\°ëPi(y) is the standard Shannon entropy function of Pi, DKL,(Pi\\\\Qj) is the KL-divergence between pi and qj, and ß and v are hy-perparameters whose selection we discuss in section 5.",
        "The distributions Vi are derived from V\\ (as mentioned above) and this can be done in one of the following ways: (a) if fji is the single supervised label for input Xj then fi(y) = 5(y = fji), which means that Vi gives unity probability for y equaling the label yi, (b) if yi = {yf\\..., y^}, k < |Y| is a set of possible outputs for input Xj, meaning an object validly falls into all of the corresponding categories, we set ri(y) = (l/k)ö(y G fji) meaning that ri is uniform over only the possible categories and zero otherwise; (c) if the labels are somehow provided in the form of a set of non-negative scores, or even a probability distribution itself, we just set n to be equal to those scores (possibly) normalized to become a valid probability distribution.",
        "Among these three cases, case (b) is particularly relevant to text classification as a given document many belong to (and in practice may be labeled as) many classes.",
        "The final classification results, i.e., the final labels for Vu, are then given by y = argmaxpi(y).",
        "We next provide further intuition on our objective function.",
        "SSL on a graph consists of finding a labeling Vu that is consistent with both the labels provided in T>i and the geometry of the data induced by the graph.",
        "The first term of C\\ will penalize the solution pi i G {1,..., I}, when it is far away from the labeled training data V\\, but it does not insist that pi = n, as allowing for deviations from r% can help especially with noisy labels (Bengio et al., 2007) or when the graph is extremely dense in certain regions.",
        "As explained above, our framework allows for the case where supervised training is uncertain or ambiguous.",
        "We consider it reasonable to call our approach soft-supervised learning, generalizing the notion of semi-supervised learning, since there is even more of a continuum here between fully supervised and fully unsupervised learning than what typically exists with SSL.",
        "Soft-supervised learning allows uncertainty to be expressed (via a probability distribution) about any of the labels individually.",
        "The second term of C\\ penalizes a lack of consistency with the geometry of the data and can be seen as a graph regularizer.",
        "If Wij is large, we prefer a solution in which pi and pj are close in the KL-divergence sense.",
        "While KL-divergence is asymmetric, given that G is undirected implies W is symmetric (Wij = Wji) and as a result the second term is inherently symmetric.",
        "The last term encourages each pi to be close to the uniform distribution if not preferred to the contrary by the first two terms.",
        "This acts as a guard against degenerate solutions commonly encountered in SSL (Blum and Chawla, 2001; Joachims, 2003).",
        "For example, consider the case where part of the graph is almost completely disconnected from any labeled vertex (which is possible in the fc-nearest neighbor case).",
        "In such situations the third term ensures that the nodes in this disconnected region are encouraged to yield a uniform distribution, validly expressing the fact that we do not know the labels of these nodes based on the nature of the graph.",
        "More generally, we conjecture that by maximizing the entropy of each pi, the classifier has a better chance of producing high entropy results in graph regions of low confidence (e.g. close to the decision boundary and/or low density regions).",
        "This overcomes a common drawback of a large number of state-of-the-art classifiers that tend to be confident even in regions close to the decision boundary.",
        "We conclude this section by summarizing some of the features of our proposed framework.",
        "It should be clear that C\\ uses the \"manifold assumption\" for SSL (see chapter 2 in (Chapelle et al., 2007)) – it assumes that the input data can be embedded within a low-dimensional manifold (the graph).",
        "As the objective is defined in terms of probability distributions over integers rather than just integers (or to real-valued relaxations of integers (Joachims, 2003; Zhu et al., 2003)), the framework generalizes in a straightforward manner to multi-class problems.",
        "Further, all the parameters are estimated jointly (compare to one vs. rest approaches which involve solving |Y| independent problems).",
        "Furthermore, the objective is capable of handling label training data uncertainty (Pearl, 1990).",
        "Of course, this objective would be useless if it wasn't possible to efficiently and easily optimize it on large data sets.",
        "We next describe a method that can do this."
      ]
    },
    {
      "heading": "3. Learning with Alternating Minimization",
      "text": [
        "As long as /x, v > 0, the objective Ci(p) is convex.",
        "This follows since DxLiViWVj) is convex in the pair (pi,Pj) (Cover and Thomas, 1991), negative entropy is convex, and a positive-weighted linear combination of a set of convex functions is convex.",
        "Thus, the problem of minimizing C\\ over the space of collections of probability distributions (a convex set) constitutes a convex programming problem (Bertsekas, 2004).",
        "This property is extremely beneficial since there is a unique global optimum and there are a variety of methods that can be used to yield that global optimum.",
        "One possible method might take the derivative of the objective along with Lagrange multipliers to ensure that we stay within the space of probability distributions.",
        "This method can sometimes yield a closed form single-step analytical expression for the globally optimum solution.",
        "Unfortunately, however, our problem does not admit such a closed form solution because the gradient of Ci(p) with respect to p%(y) is of the form, kipi{y) log pi (y) + k2Pi(y) + k3 (where ki, k2, k3are fixed constants).",
        "Sometimes, optimizing the dual of the objective can also produce a solution, but unfortunately again the dual of our objective also does not yield a closed form solution.",
        "The typical next step, then, is to resort to iterative techniques such as gradient descent along with modifications to ensure that the solution stays within the set of probability distributions (the gradient of C\\ alone will not necessarily point in the direction where p is still a valid distribution) - one such modification is called the method of multipliers (MOM).",
        "Another solution would be to use computationally complex (and complicated) algorithms like interior point methods (IPM).",
        "While all of the above methods (described in detail in (Bertsekas, 2004)) are feasible ways to solve our problem, they each have their own drawbacks.",
        "Using MOM, for example, requires the careful tuning of a number of additional parameters such as learning rates, growth factors, and so on.",
        "IPM involves inverting a matrix of the order of the number of variables and constraints during each iteration.",
        "We instead adopt a different strategy based on alternating minimization (Csiszar and Tusnady, 1984).",
        "This approach has a single additional optimization parameter (contrasted with MOM), admits a closed form solution for each iteration not involving any matrix inversion (contrasted with IPM), and yields guaranteed convergence to the global optimum.",
        "In order to render our approach amenable to AM, however, we relax our objective C\\ by defining a new (third) set of distributions for all training samples Qi, i = 1,..., n denoted collectively like the above using the notation q = (qi,..., qn).",
        "We define a new objective to be optimized as follows:",
        "Before going further, the reader may be wondering at this juncture how might it be desirable for us to have apparently complicated the objective function in an attempt to yield a more computationally and methodologically superior machine learning procedure.",
        "This is indeed the case as will be spelled out below.",
        "First, in C2 we have defined a new weight matrix [W%j = w'^ of the same size as the original where W = W + aln, where In is the n x n identity matrix, and where a > 0 is a non-negative constant (this is the optimization related parameter mentioned above).",
        "This has the effect that w'ü > wu.",
        "In the original objective C\\, Wu is irrelevant since Dkl{v\\\\v) = 0 for all p, but since there are now two distributions for each training point, there should be encouragement for the two to approach each other.",
        "Like C\\, the first term of C2 ensures that the labeled training data is respected and the last term is a smoothness regularizer, but these are done via different sets of distributions, q and p respectively – this choice is what makes possible the relatively simple analytical update equations given below.",
        "Next, we see that the two objective functions in fact have identical solutions when the optimization enforces the constraint that p and q are equal:",
        "Indeed, as a gets large, the solutions considered viable are those only where p = q.",
        "We thus have that:",
        "lim minC2(p,q) = minCi(p).",
        "Therefore, the two objectives should yield the same solution as long as a > Wij for alH, j.",
        "A key advantage of this relaxed objective is that it is amenable to alternating minimization, a method to produce a sequence of sets of distributions (pra, qra) as follows:",
        "It can be shown (we omit the rather lengthy proof due to space constraints) that the sequence generated using the above minimizations converges to the minimum of C2(p, q), i.e., provided we start with a distribution that is initialized properly q(°\\y) > 0 V y G Y.",
        "The update equations for p(ra) and q(ra) are given by and where Zi is a normalizing constant to ensure Pi is a valid probability distribution.",
        "Note that each iteration of the proposed framework has a closed form solution and is relatively simple to implement, even for very large graphs.",
        "Henceforth we refer to the proposed objective optimized using alternating minimization as AM."
      ]
    },
    {
      "heading": "4. Connections to Other Approaches",
      "text": [
        "Label propagation (LP) (Zhu and Ghahramani, 2002) is a graph-based SSL algorithms that performs Markov random walks on the graph and has a straightforward extension to multi-class problems.",
        "The update equations for LP (which also we use for our LP implementations) may be written as",
        "Note the similarity to the update equation for q\\ in our AM case.",
        "It has been shown that the squared-loss based SSL algorithm (Zhu et al., 2003) and LP have similar updates (Bengio et al., 2007).",
        "The proposed objective C\\ is similar in spirit to the squared-loss based objective in (Zhu et al., 2003; Bengio et al., 2007).",
        "Our method, however, differs in that we are optimizing the KL-divergence over probability distributions.",
        "We show in section 5 that KL-divergence based loss significantly outperforms the squared-loss.",
        "We believe that this could be due to the following: 1) squared loss is appropriate under a Gaussian loss model which may not be optimal under many circumstances (e.g. classification); 2) KL-divergence Dkl(p\\\\q) is based on a relative (relative to p) rather than an absolute error; and 3) under certain natural assumptions, KL-divergence is asymptotically consistent with respect to the underlying probability distributions.",
        "AM is also similar to the spectral graph transducer (Joachims, 2003) in that they both attempt to find labellings over the unlabeled data that respect the smoothness constraints of the graph.",
        "While spectral graph transduction is an approximate solution to a discrete optimization problem (which is NP hard), AM is an exact solution obtained by optimizing a convex function over a continuous space.",
        "Further, while spectral graph transduction assumes binary classification problems, AM naturally extends to multi-class situations without loss of convexity.",
        "Entropy Minimization (EnM) (Grandvalet and Bengio, 2004) uses the entropy of the unlabeled data as a regularizer while optimizing a parametric loss function defined over the labeled data.",
        "While the objectives in the case of both AM and EnM make use of the entropy of the unlabeled data, there are several important differences: (a) EnM is not graph-based, (b) EnM is parametric whereas our proposed approach is non-parametric, and most importantly, (c) EnM attempts to minimize entropy while the proposed approach aims to maximize entropy.",
        "While this may seem a triviality, it has catastrophic consequences in terms of both the mathematics and meaning.",
        "The objective in case of EnM is not convex, whereas in our case we have a convex formulation with simple update equations and convergence guarantees.",
        "(Wang et al., 2008) is a graph-based SSL algorithm that also employs alternating minimization style optimization.",
        "However, it is inherently squared-loss based which our proposed approach outperforms (see section 5).",
        "Further, they do not provide or state convergence guarantees and one side of their update approximates an NP-complete optimization procedure.",
        "The information regularization (IR) (Corduneanu and Jaakkola, 2003) algorithm also makes use of a KL-divergence based loss for SSL.",
        "Here the input space is divided into regions {Ri} which might or might not overlap.",
        "For a given point x% G Ri, IR attempts to minimize the KL-divergence between Pi(yi\\xi) and PRi(y), the agglomerative distribution for region Ri.",
        "Given a graph, one can define a region to be a vertex and its neighbor thus making IR amenable to graph-based SSL.",
        "In (Corduneanu and Jaakkola, 2003), the agglomeration is performed by a simple averaging (arithmetic mean).",
        "While IR suggests (without proof of convergence) the use of alternating minimization for optimization, one of the steps of the optimization does not admit a closed-form solution.",
        "This is a serious practical drawback especially in the case of large data sets.",
        "(Tsuda, 2005) (hereafter referred to as PD) is an extension of the IR algorithm to hypergraphs where the agglomeration is performed using the geometric mean.",
        "This leads to closed form solutions in both steps of the alternating minimization.",
        "There are several important differences between IR and PD on one side and our proposed approach: (a) neither IR nor PD use an entropy regularizer, and (b) the update equation for one of the steps of the optimization in the case of PD (equation 13 in (Tsuda, 2005)) is actually a special case of our update equation for pi (y) and may be obtained by setting Wij = 1/2.",
        "Further, our work here may be easily extended to hypergraphs."
      ]
    },
    {
      "heading": "5. Results",
      "text": [
        "We compare our algorithm (AM) with other state-of-the-art SSL-based text categorization algorithms, namely, (a) SVM (Joachims, 1999), (c) Spectral Graph Transduction (SGT) (Joachims, 2003), and (d) Label Propagation (LP) (Zhu and Ghahramani, 2002).",
        "Note that only SGT and LP are graph-based algorithms, while SVM is fully-supervised (i.e., it does not make use of any of the unlabeled data).",
        "We implemented SVM and TSVM using SVM Light (Joachims, b) and SGT using SGT Light (Joachims, a).",
        "In the case of SVM, TSVM and SGT we trained | Y| classifiers (one for each class) in a one vs. rest manner precisely following (Joachims, 2003).",
        "We used the \"ModApte\" split of the Reuters-21578 dataset collected from the Reuters newswire in 1987 (Lewis et al., 1987).",
        "The corpus has 9,603 training (not to be confused with V) and 3,299 test documents (which represents Vv).",
        "Of the 135 potential topic categories only the 10 most frequent categories are used (Joachims, 1999).",
        "Categories outside the 10 most frequent were collapsed into one class and assigned a label \"other\".",
        "For each document i in the training and test sets, we extract features in the following manner: stop-words are removed followed by the removal of case and information about inflection (i.e., stemming) (Porter, 1980).",
        "We then compute TFIDF features for each document (Salton and Buckley, 1987).",
        "All graphs were constructed using cosine similarity with TFIDF features.",
        "For this task Y = { earn, acq, money, grain, crude, trade, interest, ship, wheat, corn, average}.",
        "For LP and AM, we use the output space Y' = YU{ other }.",
        "For documents in V\\ that are labeled with multiple categories, we initialize ri to have equal non-zero probability for each such category.",
        "For example, if document i is annotated as belonging to classes { acq, grain, wheat}, then ri(acq) = Vi(grain) = Ti(wheat) = 1/3.",
        "We created 21 transduction sets by randomly sampling I documents from the training set with the constraint that each of 11 categories (top 10 categories and the class other) are represented at least once in each set.",
        "These samples constitute V\\.",
        "All algorithms used the same transduction sets.",
        "In the case of SGT, LP and AM, the first transduction set was used to tune the hyperparameters which we then held fixed for all the remaining 20 transduction sets.",
        "For all the graph-based approaches, we ran a search over K G {2, 10, 50, 100, 250, 500, 1000, 2000, n} (note K, = n represents a fully connected graph).",
        "In addition, in the case of AM, we set a = 2 for all experiments, and we ran a search over ß G {le-8, le^l, 0.01, 0.1, 1, 10, 100} and v G {le-8, le-6, le-A 0.01, 0.1}, for SGT the search was over c G {3000, 3200, 3400, 3800, 5000, 100000} (see (Joachims, 2003)).",
        "We report precision-recall break even point (PRBEP) results on the 3,299 test documents in Table 1.",
        "PRBEP has been a popular measure in information retrieval (see e.g. (Raghavan et al., 1989)).",
        "It is defined as that value for which precision and recall are equal.",
        "Results for each category in Table 1 were obtained by averaging the PRBEP over",
        "Table 1: P/R Break Even Points (PRBEP) for the top 10 categories in the Reuters data set with / = 20 and u = 3299.",
        "All results are averages over 20 randomly generated transduction sets.",
        "The last row is the macro-average over all the categories.",
        "Note AM is the proposed approach.",
        "the 20 transduction sets.",
        "The final row \"average\" was obtained by macro-averaging (average of averages).",
        "The optimal value of the hyperparameters in case of LP was K, = 100; in case of AM, K, = 2000, ß = \\q-A, v = le-2; and in the case of SGT, K = 100, c = 3400.",
        "The results show that AM outperforms the state-of-the-art on 6 out of 10 categories and is competitive in 3 of the remaining 4 categories.",
        "Further it significantly outperforms all other approaches in case of the macro-averages.",
        "AM is significant over its best competitor SGT at the 0.0001 level according to the difference of proportions significance test.",
        "Figure 1 shows the variation of \"average\" PRBEP against the number of labeled documents (I).",
        "For each value of I, we tuned the hyperparameters over the first transduction set and used these values for all the other 20 sets.",
        "Figure 1 also shows error-bars (± standard deviation) all the experiments.",
        "As expected, the performance of all the approaches improves with increasing number of labeled documents.",
        "Once again in this case, AM, outperforms the other approaches for all values of I.",
        "World Wide Knowledge Base (WebKB) is a collection of 8282 web pages obtained from four academic domains.",
        "The web pages in the WebKB set are labeled using two different polychotomies.",
        "The first is according to topic and the second is according to web domain.",
        "In our experiments we only considered the first polychotomy, which consists of 7 categories: course, department, faculty, project, staff, student, and other.",
        "Following (Nigam et al., 1998) we only use documents from categories course, department, faculty, project which gives 4199 documents for the four categories.",
        "Each of the documents is in HTML format containing text as well as other information such as HTML tags, links, etc.",
        "We used both textual and non-textual information to construct the feature vectors.",
        "In this case we did not use either stop-word removal or stemming as this has been found to hurt performance on this task (Nigam et al., 1998).",
        "As in the the case of the Reuters data set we extracted TFIDF features for each document and constructed the graph using cosine similarity.",
        "Category",
        "SVM",
        "TSVM",
        "SGT",
        "LP",
        "AM",
        "earn",
        "91.3",
        "95.4",
        "90.4",
        "96.3",
        "97.9",
        "acq",
        "67.8",
        "76.6",
        "91.9",
        "90.8",
        "97.2",
        "money",
        "41.3",
        "60.0",
        "65.6",
        "57.1",
        "73.9",
        "grain",
        "56.2",
        "68.5",
        "43.1",
        "33.6",
        "41.3",
        "crude",
        "40.9",
        "83.6",
        "65.9",
        "74.8",
        "55.5",
        "trade",
        "29.5",
        "34.0",
        "36.0",
        "56.0",
        "47.0",
        "interest",
        "35.6",
        "50.8",
        "50.7",
        "47.9",
        "78.0",
        "ship",
        "32.5",
        "46.3",
        "49.0",
        "26.4",
        "39.6",
        "wheat",
        "47.9",
        "44.4",
        "59.1",
        "58.2",
        "64.3",
        "corn",
        "41.3",
        "33.7",
        "51.2",
        "55.9",
        "68.3",
        "average",
        "48.9",
        "59.3",
        "60.3",
        "59.7",
        "66.3",
        "As in (Bekkerman et al., 2003), we created four roughly-equal random partitions of the data set.",
        "In order to obtain V\\, we first randomly choose a split and then sample I documents from that split.",
        "The other three splits constitute Vu.",
        "We believe this is more realistic than sampling the labeled web-pages from a single university and testing web-pages from the other universities (Joachims, 1999).",
        "This method of creating transduction sets allows us to better evaluate the generalization performance of the various algorithms.",
        "Once again we create 21 transduction sets and the first set was used to tune the hyperparameters.",
        "Further, we ran a search over the same grid as used in the case of Reuters.",
        "We report precision-",
        "Table 2: P/R Break Even Points (PRBEP) for the WebKB data set with / = 48 and u = 3148.",
        "All results are averages over 20 randomly generated transduction sets.",
        "The last row is the macro-average over all the classes.",
        "AM is the proposed approach.",
        "recall break even point (PRBEP) results on the 3,148 test documents in Table 2.",
        "For this task, we found that the optimal value of the hyperparameter were: in the case of LP, K, = 1000; in case of AM, K, = 1000, ß = le-2, v = le-A; and in case of SGT, K = 100, c = 3200.",
        "Once again, AM is significant at the 0.0001 level over its closest competitor LP.",
        "Figure 2 shows the variation of PRBEP with number of labeled documents (0 and was generated in a similar fashion as in the case of the Reuters data set."
      ]
    },
    {
      "heading": "6. Discussion",
      "text": [
        "We note that LP may be cast into an AM-like framework by using the following sequence of updates,",
        "i AM ■ - SGT",
        "- s - TSVM « - SVM",
        "Class",
        "SVM",
        "TSVM",
        "SGT",
        "LP",
        "AM",
        "course",
        "46.5",
        "43.9",
        "29.9",
        "45.0",
        "67.6",
        "faculty",
        "14.5",
        "31.2",
        "42.9",
        "40.3",
        "42.5",
        "project",
        "15.8",
        "17.2",
        "17.5",
        "27.8",
        "42.3",
        "student",
        "15.0",
        "24.5",
        "56.6",
        "51.8",
        "55.0",
        "average",
        "23.0",
        "29.2",
        "36.8",
        "41.2",
        "51.9",
        "To compare the behavior of AM and LP, we applied this form of LP along with AM on a simple 5-node binary-classification SSL graph where two nodes are labeled (node 1 and 2) and the remaining nodes are unlabeled (see Figure 3, top).",
        "Since this is binary classification (|F| = 2), each distribution pi or qi can be depicted using only a single real number between 0 and 1 corresponding to the probability that each vertex is class 2 (yes two).",
        "We show how both LP and AM evolve starting from exactly the same random starting point q° (Figure 3, bottom).",
        "For each algorithm, the figure shows that both algorithms clearly converge.",
        "Each alternate iteration of LP is such that the labeled vertices oscillate due to its clamping back to the labeled distribution, but that is not the case for AM.",
        "We see, moreover, qualitative differences in the solutions as well - e.g., AM's solution for the pendant node 5 is less confident than is LP's solution.",
        "More empirical comparative analysis between the two algorithms of this sort will appear in future work.",
        "We have proposed a new algorithm for semi-supervised text categorization.",
        "Empirical results show that the proposed approach significantly outperforms the state-of-the-art.",
        "In addition the proposed approach is relatively simple to implement and has guaranteed convergence properties.",
        "While in this work, we use relatively simple features to construct the graph, use of more sophisticated features and/or similarity measures could lead to further improved results."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported by ONR MURI grant N000140510388, by NSF grant IIS-0093430, by the Companions project (1ST programme under EC grant IST-FP6-034434), and by a Microsoft Research Fellowship.",
        "' 1",
        "■",
        "-",
        "-",
        "-",
        "-",
        "-",
        "?\"",
        ",<°> „«",
        "V",
        "g<>",
        "t\"t",
        "■t",
        "t",
        "qm",
        "qm qm qm qm q(u) q(i2) ^ q(u) q(is"
      ]
    }
  ]
}
