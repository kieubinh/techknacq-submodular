{
  "info": {
    "authors": [
      "Jin-Shea Kuo",
      "Haizhou Li"
    ],
    "book": "Proceedings of the Third International Joint Conference on Natural Language Processing",
    "id": "acl-I08-1049",
    "title": "Multi-View Co-Training of Transliteration Model",
    "url": "https://aclweb.org/anthology/I08-1049",
    "year": 2008
  },
  "references": [
    "acl-J98-4003",
    "acl-P04-1021",
    "acl-P06-1010",
    "acl-P06-1142"
  ],
  "sections": [
    {
      "text": [
        "Multi-View Co-training of Transliteration Model",
        "Chung-Hwa Telecomm.",
        "Laboratories, Taiwan",
        "This paper discusses a new approach to training of transliteration model from unlabeled data for transliteration extraction.",
        "We start with an inquiry into the formulation of transliteration model by considering different transliteration strategies as a multi-view problem, where each view exploits a natural division of transliteration features, such as phoneme-based, grapheme-based or hybrid features.",
        "Then we introduce a multi-view Co-training algorithm, which leverages compatible and partially uncorrelated information across different views to effectively boost the model from unlabeled data.",
        "Applying this algorithm to transliteration extraction, the results show that it not only circumvents the need of data labeling, but also achieves performance close to that of supervised learning, where manual labeling is required for all training samples."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Named entities are important content words in text documents.",
        "In many applications, such as cross-language information retrieval (Meng et al., 2001; Virga and Khudanpur, 2003) and machine translation (Knight and Graehl, 1998; Chen et al., 2006), one of the fundamental tasks is to identify these words.",
        "Imported foreign proper names constitute a good portion of such words, which are newly translated into Chinese by transliteration.",
        "Transliteration is a process of translating a foreign word into the native language by preserving its pronunciation in the original language, otherwise known as translation-by-sound.",
        "Haizhou Li",
        "Institute for Infocomm Research,",
        "As new words emerge everyday, no lexicon is able to cover all transliterations.",
        "It is desirable to find ways to harvest transliterations from real world corpora.",
        "In this paper, we are interested in the learning of English to Chinese (E-C) transliteration model for transliteration extraction from the Web.",
        "A statistical transliteration model is typically trained on a large amount of transliteration pairs, also referred to a bilingual corpus.",
        "The correspondence between a transliteration pair may be described by the mapping of different basic pronunciation units (BPUs) such as phoneme-based, or grapheme-based one, or both.",
        "We can see each type of BPU mapping as a natural division of transliteration features, which represents a view to the phonetic mapping problem.",
        "By using different BPUs, we approach the transliteration modeling and extraction problems from different views.",
        "This paper is organized as follows.",
        "In Section 2, we briefly introduce previous work.",
        "In Section 3, we conduct an inquiry into the formulation of transliteration model or phonetic similarity model (PSM) and consider it as a multi-view problem.",
        "In Section 4, we propose a multi-view Co-training strategy for PSM training and transliteration extraction.",
        "In Section 5, we study the effectiveness of proposed algorithms.",
        "Finally, we conclude in Section 6."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Studies on transliteration have been focused on transliteration modeling and transliteration extraction.",
        "The transliteration modeling approach deduces either phoneme-based or grapheme-based mapping rules using a generative model that is trained from a large bilingual corpus.",
        "Most of the works are devoted to phoneme-based transliteration modeling (Knight and Graehl, 1998; Lee, 1999).",
        "Suppose that EW is an English word and CW is its Chinese transliteration.",
        "EW and CW form an EC transliteration pair.",
        "The phoneme-based approach first converts EW into an intermediate phonemic representation p, and then converts p into its Chinese counterpart CW.",
        "The idea is to transform both source and target words into comparable phonemes so that the phonetic similarity between two words can be measured easily.",
        "Recently the grapheme-based approach has attracted much attention.",
        "It was proposed by Jeong et al., 2006b), which is also known as direct orthography mapping.",
        "It treats the transliteration as a statistical machine translation problem under monotonic constraint.",
        "The idea is to obtain the bilingual orthographical correspondence directly to reduce the possible errors introduced in multiple conversions.",
        "However, the grapheme-based transliteration model has more parameters than phoneme-based one does, thus expects a larger training corpus.",
        "Most of the reported works have been focused on either phoneme-or grapheme-based approaches.",
        "2006b) recently proposed using a mix of phoneme and grapheme features, where both features are fused into a single learning process.",
        "The feature fusion was shown to be effective.",
        "However, their methods hinge on the availability of a labeled bilingual corpus.",
        "In transliteration extraction, mining translations or transliterations from the ever-growing multilingual Web has become an active research topic, for example, by exploring query logs (Brill et al., 2001) and parallel (Nie et al., 1999) or comparable corpora (Sproat et al., 2006).",
        "Transliterations in such a live corpus are typically unlabeled.",
        "For model-based transliteration extraction, recent progress in machine learning offers different options to exploit unlabeled data, that include active learning (Lewis and Catlett, 1994) and Co-training (Nigam and Ghani, 2000;",
        "Tür et al.",
        "2005).",
        "Taking the prior work a step forward, this paper explores a new way of fusing phoneme and grapheme features through a multi-view Co-training algorithm (Blum and Mitchell, 1998), which starts with a small number of labeled data to bootstrap a transliteration model to automatically harvest transliterations from the Web."
      ]
    },
    {
      "heading": "3. Phonetic Similarity Model with Multiple Views",
      "text": [
        "Machine transliteration can be formulated as a generative process, which takes a character string in source language as input and generates a character string in the target language as output.",
        "Conceptually, this process can be regarded as a 3-step decoding: segmentation of both source and target strings into basic pronunciation units (BPUs), relating the source BPUs with target units by resolving different combinations of alignments and unit mappings in finding the most probable BPU pairs.",
        "A BPU can be defined as a phoneme sequence, a grapheme sequence, or a part of them.",
        "A transliteration model establishes the phonetic relationship between BPUs in two languages to measure their similarity, therefore, it is also known as the phonetic similarity model (PSM).",
        "To introduce the multi-view concept, we illustrate the BPU transfers in Figure 1, where each transfer is represented by a direct path with different line style.",
        "There are altogether four different paths: the phoneme-based path V1(T1^T2^T3), the grapheme-based path V4 (T4), and their variants, V2(T1^T5) and V3(T6^T3).",
        "The last two paths make use of the intermediate BPU mappings between phonemes and graphemes.",
        "Each of the paths represents a view to the mapping problem.",
        "Given a labeled bilingual corpus, we are able to train a transliteration model for each view easily.",
        "The EC transliteration has been studied extensively in the paradigm of noisy channel model (Manning and Scheutze, 1999), with EW as the observation and CW as the input to be recovered.",
        "Applying Bayes rule, the transliteration can be described by Eq.",
        "(1), where we need to deal with two probability distributions: P(EW^CW9, the probability of transliterating CW to EW, also known as the unit mapping rules, and P(CW), the probability distribution of CW, known as the target language model.",
        "Representing EW in English BPU sequence EP = {ep1,...epm ,...epM} and CW in Chinese one, CP = {cp1,...cpn,...cpN} , a typical transliteration probability can be expressed as,",
        "The language model, P(CW), can be represented by Chinese characters n-gram statistics (Manning and Scheutze, 1999) and expressed in Eq.",
        "(3).",
        "In the case of bigram, we have,",
        "We next rewrite Eq.",
        "(2) for the four different views depicted in Figure 1 in a systematic manner.",
        "The phoneme-based approach approximates the transliteration probability distribution by introducing an intermediate phonemic representation.",
        "In this way, we convert the words in the source language, say EW = e1,e2...eK , into English syllables ES, then Chinese syllables CS and finally the target language, say Chinese CW = c1,c2...cK in sequence.",
        "Eq.",
        "(2) can be rewritten by replacing EP and CP with ES and CS, respectively, and expressed by Eq.",
        "(4).",
        "The three probabilities correspond to the three-step mapping in V1 path.",
        "The phoneme-based approach suffers from multiple step mappings.",
        "This could compromise overall performance because none of the three steps guarantees a perfect conversion.",
        "The grapheme-based approach is inspired by the transfer model (Vauqois, 1988) in machine translation that estimates P(EW | CW) directly without interlingua representation.",
        "This method aims to alleviate the imprecision introduced by the multiple transfers in phoneme-based approach.",
        "In practice, a grapheme-based approach converts the English graphemes to Chinese graphemes in one single step.",
        "Suppose that we have EW = e1,e2...eK and CW = c1,c2...cK where ek and ck are aligned grapheme units.",
        "Under the noisy channel model, we can estimate P(EW | CW) based on the alignment statistics which is similar to the lexical mapping in statistical machine translation.",
        "P(EW | CW) *]~[^P(eJCfc) (5) Eq.",
        "(5) is a grapheme-based alternative to Eq.",
        "(2).",
        "A tradeoff between the phoneme-and grapheme-based approaches is to take shortcuts to the mapping between phonemes and graphemes of two languages via V2 or V3, where only two steps of mapping are involved.",
        "For V3, we rewrite Eq.",
        "(2) as Eq.",
        "(6):",
        "where P(EW | CS) translates Chinese sounds into English words.",
        "For V2, we rewrite Eq.",
        "(2) as Eq.",
        "(7):",
        "where P(ES | CW) translates Chinese words into English sounds.",
        "Eqs.",
        "(4) - (7) describe the four paths of transliteration.",
        "In a multi-view problem, one partitions the domain's features into subsets, each of which is sufficient for learning the target concept.",
        "Here the target concept is the label of transliteration pair.",
        "Given a collection of EC pair candidates, the transliteration extraction task can be formulated as a hypothesis test, which makes a binary decision as to whether a candidate EC pair is a genuine transliteration pair or not.",
        "Given an EC pair X={EW,CW}, we have H0 , which hypothesizes that EW and CW form a genuine EC pair, and H1, which hypothesizes otherwise.",
        "The likelihood ratio is given as a = P(X |H0)/P(X |Hj), where P(X | H0) and P(X | H0) are derived from P(EW\\CW).",
        "By comparing a with a threshold t , we make the binary decision as that in (Kuo et al., 2007).",
        "As discussed, each view takes a distinct path that has its own advantages and disadvantages in terms of model expressiveness and complexity.",
        "Each view represents a weak learner achieving moderately good performance towards the target concept.",
        "Next, we study a multi-view Co-training process that leverages the data of different views from each other in order to boost the accuracy of a PSM model.",
        "Multi-View Learning Framework",
        "The PSM can be trained in a supervised manner using a manually labeled corpus.",
        "The advantage of supervised learning is that we can establish a model quickly as long as labeled data are available.",
        "However, this method suffers from some practical constraints.",
        "First, the derived model can only be as good as the data it sees.",
        "Second, the labeling of corpus is labor intensive.",
        "To circumvent the need of manual labeling, here we study three adaptive strategies cast in the machine learning framework, namely unsupervised learning, Co-training and Co-EM.",
        "Unsupervised learning minimizes human supervision by probabilistically labeling data through an Expectation and Maximization (EM) (Dempster et al., 1977) process.",
        "The unsupervised learning strategy can be depicted in Figure 2 by taking the dotted path, where the extraction process accumulates all the acquired transliteration pairs in a repository for training a new PSM.",
        "A new PSM is in turn used to extract new transliteration pairs.",
        "The unsupervised learning approach only needs a few labeled samples to bootstrap the initial model for further extraction.",
        "Note that the training samples are noisy and hence the quality of initial PSM therefore has a direct impact on the final performance.",
        "The multi-view setting (Muslea et al., 2002) applies to learning problems that have a natural way to divide their features into different views, each of which is sufficient to learn the target concept.",
        "Blum and Mitchell (1998) proved that for a problem with two views, the target concept can be learned based on a few labeled and many unlabeled examples, provided that the views are compatible and uncorrelated.",
        "Intuitively, the transliteration problem has compatible views.",
        "If an EC pair forms a transliteration, then this is true across all different views.",
        "However, it is arguable that the four views in Figure 1 are uncorrelated.",
        "Studies (Nigam and Ghani, 2000; Muslea et al., 2002) shown that the views do not have to be entirely uncorrelated for Co-training to take effect.",
        "This motivates our attempt to explore multi-view Co-training for learning models in transliteration extraction.",
        "PSM Evaluation & Stop Criterion Co-training",
        "To simplify the discussion, here we take a two-view (V1 and V2) example to show how Co-training can potentially help.",
        "To start with, one can learn a weak hypothesis PSM1 using V1 based on a few labeled examples and then apply PSM1 to all unlabeled examples.",
        "If the views are uncorrelated, or at least partially uncorrelated, these newly labeled examples seen from V1 augment the training set for V2.",
        "These newly labeled examples present new information from the V2 point of view, from which one can in turn update the PSM2.",
        "As the views are compatible, both V1 and V2 label the samples consistently according to the same probabilistic transliteration criteria.",
        "In this way, PSMs are boosted each other through such an iterative process between two different views.",
        "------------L_,",
        "Training i Repository",
        "_PSM Learner i",
        "Unsupervised i",
        "f-^",
        " – ^ PSM Learner 1",
        "Training Repository",
        " – ► PSM Learner n",
        "a) .",
        "A small set of labeled samples and a set of unlabeled samples.",
        "b) .",
        "Two learners A and B are trained on the labeled set.",
        "1) Loop for k iterations:",
        "a) .",
        "Learners A and B predict the labels of the unlabeled data to augment the labeled set; b) .",
        "Learners A and B are trained on the augmented labeled set.",
        "2) Combine models from Learners A and B.",
        "Extending the two-view to multi-view, one can develop multiple learners from several subsets of features, each of which approaches the problem from a unique perspective, called a view when taking the Co-training path in Figure 2.",
        "Finally, we use outputs from multi-view learners to approximate the manual labeling.",
        "The multi-view learning is similar to unsupervised learning in the sense that the learning alleviates the need of labeling and starts with very few labeled data.",
        "However, it is also different from the unsupervised learning because the latter does not leverage the natural split of compatible and uncorrelated features.",
        "Two variants of two-view learning strategy can be summarized in Table 1 and Table 2, where the algorithm in Table 1 is referred to as Co-training and the one in Table 2 as Co-EM (Nigam and Ghani.",
        "2000; Muslea et al., 2002).",
        "In Co-training, Learners A and B are trained on the same training data and updated simultaneously.",
        "In Co-EM, Learners A and B are trained on labeled set predicted by each other's view, with their models being updated in sequence.",
        "In other words, the Co-EM algorithm interchanges the probabilistic labels generated in the view of each other before a new EM iteration.",
        "In both cases, the unsupervised, multi-view algorithms use the hypotheses learned to probabilistically label the examples.",
        "a) .",
        "A small set of labeled samples and a set of unlabeled samples.",
        "b) .",
        "Learner A is trained on a labeled set to predict the labels of the unlabeled data.",
        "1) Loop for k iterations",
        "a) .",
        "Learner B is trained on data labeled by",
        "Learner A to predict the labels of the unlabeled data; b) .",
        "Learner A is trained on data labeled by",
        "Learner B to predict the labels of the unlabeled data;",
        "The extension of algorithms in Table 1 and 2 to the multi-view transliteration problem is straightforward.",
        "After an ensemble of learners are trained, the overall PSM can be expressed as a linear combination of the learners, where wi is the weight of ith learner Pl (EW | CW), which can be learnt by using a development corpus."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "To validate the effectiveness of the learning framework, we conduct a series of experiments in transliteration extraction on a development corpus described later.",
        "First, we repeat the experiment in (Kuo et al., 2006) to train a PSM using PSA and GSA feature fusion in a supervised manner, which serves as the upper bound of Co-training or Co-EM system performance.",
        "We then train the PSMs with single view V1, V2, V3 and V4 alone in an unsupervised manner.",
        "The performance achieved by each view alone can be considered as the baseline for multi-view benchmarking.",
        "Then, we run two-view Co-training for different combinations of views on the same development corpus.",
        "We expect to see positive effects with the multi-view training.",
        "Finally, we run the experiments using two-view Co-training and Co-EM and compare the results.",
        "A 500 MB development corpus is constructed by crawling pages from the Web for the experiments.",
        "We first establish a gold standard for performance evaluation by manually labeling the corpus based on the following criteria: (i) if an EW is partly translated phonetically and partly translated semantically, only the phonetic transliteration constituent is extracted to form a transliteration pair; (ii) multiple EC pairs can appear in one sentence; (iii) an EW can have multiple valid Chinese transliterations and vice versa.",
        "We first derive 80,094 EC pair candidates from the 500 MB corpus by spotting the co-occurrence of English and Chinese words in the same sentences.",
        "This can be done automatically without human intervention.",
        "Then, the manual labeling process results in 8,898 qualified EC pairs, also referred to as Distinct Qualified Transliteration Pairs (DQTPs).",
        "To establish comparison, we first train a PSM using all 8,898 DQTPs in a supervised manner and conduct a closed test as reported in Table 3.",
        "We further implement three PSM learning strategies and conduct a systematic series of experiments by following the recognition followed by validation strategy proposed in (Kuo et al., 2007).",
        "For performance benchmarking, we define the precision as the ratio of extracted number of DQTPs over that of total extracted pairs, recall as the ratio of extracted number of DQTPs over that of total DQTPs, and F-measure as in Eq.",
        "(9).",
        "They are collectively referred to as extraction performance.",
        "As formulated in Section 4.1, first, we derive an initial PSM using randomly selected 100 seed DQTPs for each learner and simulate the Web-based learning process: (i) extract EC pairs using the PSM; (ii) add all of the extracted EC pairs to the DQTP pool; (iii) re-estimate the PSM for each view by using the updated DQTP pool.",
        "This process is also known as semi-supervised EM (Muslea et al., 2002).",
        "As shown in Figure 3, the unsupervised learning algorithm consistently improves the initial PSM using in all four views.",
        "To appreciate the effectiveness of each view, we report the F-measures on each individual view V1, V2, V3 and V4, as 0.680, 0.620, 0.541 and 0.520, respectively at the 6th iteration.",
        "We observe that V1, the phoneme-based path, achieves the best result.",
        "We report three typical combinations of two co-working learners or two-view Co-training.",
        "Like in unsupervised learning, we start with the same 100 seed DQTPs and an initial PSM model by following the algorithm in Table 1 over 6 iterations.",
        "With two-view Co-training, we obtain 0.726, 0.705, 0.590 and 0.716 in terms of F-measures for V1+V2, V2+V3, V3+V4 and V1+V4 at the 6thiteration, as shown in Figure 4.",
        "Comparing Figure 3 and 4, we find that Co-training consistently outperforms unsupervised learning by exploiting compatible information across different views.",
        "The V1+V2 Co-training outperforms other Co-training combinations, and surprisingly achieves close performance to that of supervised learning.",
        "Supervised -V1"
      ]
    },
    {
      "heading": "3. 4 #Iteration",
      "text": []
    },
    {
      "heading": "2. x recall x precision",
      "text": [
        "recall + precision",
        "Precision",
        "Recall",
        "F-measure",
        "Closed test 0.834",
        "0.663",
        "0.739",
        "Next we start with the same 100 seed DQTPs by initializing the training pool and carry out Co-EM on the same corpus.",
        "We build PSM1 for Learner A and PSM2 for Learner B.",
        "To start with, PSM1 is learnt from the initial labeled set.",
        "We then follow the algorithm in Table 2 by looping in the following two steps over 6 iterations: (i) estimate the PSM2 from the samples labeled by Learner A (V1) to extract the high confident EC pairs and augment the DQTP pool with the probabilistically labeled EC pairs; (ii) estimate the PSM1 from the samples labeled by Learner B (V2) to extract the high confident EC pairs and augment the DQTP pool with the probabilistically labeled EC pairs.",
        "We report the results in Figure 5.",
        "■Supervised",
        "To summarize, we compare the performance of six learning methods studied in this paper in Table 4.",
        "The Co-training and Co-EM learning approaches have alleviated the need of manual labeling, yet achieving performance close to supervised learning.",
        "The multi-view learning effectively leverages multiple compatible and partially uncorrelated views.",
        "It reduces the need of labeled samples from 80,094 to just 100.",
        "We also compare the multi-view learning algorithm with active learning on the same development corpus using same features.",
        "We include the results from previously reported work (Kuo et al., 2006) into Table 4 (see Exp.",
        "2) where multiple features are fused in a single active learning process.",
        "In Exp.",
        "2, PSA feature is the equivalent of V1 feature in Exp.",
        "4; GSA feature is the equivalent of V4 feature in Exp.",
        "4.",
        "In Exp.",
        "4, we carry out V1+V4 two-view Co-training.",
        "It is interesting to find that the multi-view learning in this paper achieves better results than active learning in terms of F-measure while reducing the need of manual labeling from 8,191 samples to just 100.",
        "Fusion of phoneme and grapheme features in transliteration modeling was studied in many previous works.",
        "However, it was done through the combination of phoneme and grapheme similarity scores (Bilac and Tanaka, 2004), or by pooling phoneme and grapheme features together into a single-view training process (Oh and Choi, 2006b).",
        "This paper presents a new approach that leverages the information across different views to effectively boost the learning from unlabeled data.",
        "We have shown that both Co-training and Co-EM not only outperform the unsupervised learning of single view, but also alleviate the need of data labeling.",
        "This reaffirms that multi-view is a viable solution to the learning of transliteration model and hence transliteration extraction.",
        "Moving forward, we believe that contextual feature in documents presents another compatible, uncorrelated, and complementary view to the four views.",
        "We validate the effectiveness of the proposed algorithms by conducting experiments on transliteration extraction.",
        "We hope to extend the work further by investigating the possibility of applying the multi-view learning algorithms to machine translation.",
        "F-",
        "measure",
        "# of",
        "Exp.",
        "Learning algorithm",
        "samples to label",
        "1",
        "Supervised",
        "0.739",
        "80,094",
        "2",
        "Active Learning (Kuo et al., 2006)",
        "0.710",
        "8,191",
        "3",
        "Unsupervised (V1)",
        "0.680",
        "100",
        "4",
        "Co-training (V1+V4)",
        "0.716",
        "100",
        "5",
        "Co-training (V1+V2)",
        "0.726",
        "100",
        "6",
        "Co-EM (V1+V2)",
        "0.725",
        "100"
      ]
    }
  ]
}
