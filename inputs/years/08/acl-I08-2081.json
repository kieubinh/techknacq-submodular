{
  "info": {
    "authors": [
      "Yingju Xia",
      "Hao Yu",
      "Gang Zou"
    ],
    "book": "Proceedings of the Third International Joint Conference on Natural Language Processing",
    "id": "acl-I08-2081",
    "title": "Dimensionality Reduction with Multilingual Resource",
    "url": "https://aclweb.org/anthology/I08-2081",
    "year": 2008
  },
  "references": [],
  "sections": [
    {
      "text": [
        "YingJu Xia Hao Yu Gang Zou",
        "Fujitsu Research & Development Center Co.,LTD.",
        "13F Tower A, Ocean International Center, No.56 Dong Si Huan Zhong Rd, Chaoyang District,",
        "Beijing, China, 100025",
        "Query and document representation is a key problem for information retrieval and filtering.",
        "The vector space model (VSM) has been widely used in this domain.",
        "But the VSM suffers from high dimensionality.",
        "The vectors built from documents always have high dimensionality and contain too much noise.",
        "In this paper, we present a novel method that reduces the dimensionality using multilingual resource.",
        "We introduce a new metric called TC to measure the term consistency constraints.",
        "We deduce a TC matrix from the multilingual corpus and then use this matrix together with the term-by-document matrix to do the Latent Semantic Indexing (LSI).",
        "By adopting different TC threshold, we can truncate the TC matrix into small size and thus lower the computational cost of LSI.",
        "The experimental results show that this dimensionality reduction method improves the retrieval performance significantly."
      ]
    },
    {
      "heading": "1. Introduction 1.1 Basic concepts",
      "text": [
        "The vast amount of electronic information that is available today requires effective techniques for accessing relevant information from it.",
        "The methodologies developed in information retrieval aim at devising effective means to extract relevant documents in a collection when a user query is given.",
        "In information retrieval and filtering, Query and document representation is a key problem and many techniques have been developed.",
        "Among these techniques, the vector space model (VSM) proposed by Salton (1971; 1983) has been widely used.",
        "In the VSM, a document is represented by a vector of terms.",
        "The cosine of the angle between two document vectors indicates the similarity between the corresponding documents.",
        "A smaller angle corresponds to a larger cosine value and indicates higher document similarity.",
        "A query, which describes the information need, is encoded as a vector as well.",
        "Retrieval of documents that satisfy the information need is achieved by finding the documents most similar to the query, or equivalently, the document vectors closest to the query vector.",
        "There are several advantages to this approach beyond its mathematical simplicity.",
        "Above all, it is efficient to compute and store the word counts.",
        "This is one reason that why VSM is widely used for query and document representation.",
        "But this method has problem that the vectors built from documents always have high dimensionality and contain too much noise.",
        "The high dimensionality causes high computational and memory requirements while noise in the vectors degrades the system performance.",
        "To address these problems, many dimensionality reduction techniques have been applied to query and document representation.",
        "Among these techniques, Latent Semantic Indexing (LSI) (Deer-wester et al., 1990; Hofmann, 1999; Ding, 2000; Jiang and Liftman, 2000; Ando, 2001; Kokiopou-lou and Saad, 2004; Lee et al., 2006) is a well-known approach.",
        "LSI constructs a smaller document matrix that retains only the most important information from the original by using the Singular Value Decomposition (SVD).",
        "Many modifications have been made to this approach (Hofmann, 1999; Ding, 2000; Jiang and Littman, 2000; Kokiopoulou and Saad, 2004; Sun et al., 2004; Husbands et al., 2005).",
        "Among them, IRR (Ando and Lee, 2001) is a subspace-projection method that counteracts tendency to ignore minority-class documents.",
        "This is done by repeatedly rescaling vectors to amplify the presence of documents poorly represented in previous iterations.",
        "In concept indexing (CI) (Karypis and Han, 2000) method, the original set of documents is first clustered into k similar groups, and then for each group, the centroid vector (i.e., the vector obtained by averaging the documents in the group) is used as one of the k axes of the lower dimensional space.",
        "The key motivation behind this dimensionality reduction approach is the view that each centroid vector represents a concept present in the collection, and the lower dimensional representation expresses each document as a function of these concepts.",
        "George and Han (2000) extend concept indexing in the context of supervised dimensionality reduction.",
        "To capture the concept, phrase also has been used as indexing entries (Mao and Chu, 2002).",
        "The LPI method (Isbell and Viola, 1999) tries to discover the local structure and obtains a compact document representation subspace that best detects the essential semantic structure.",
        "The LPI uses Locality Preserving Projections (LPP) (Xiaofei He and Partha, 2003) to learn a semantic space for document representation.",
        "Xiaofei He et al., (2004) try to get sets of highly-related words, queries and documents are represented by their distance to these sets.",
        "These algorithms have successfully reduced the dimensionality and improve the retrieval performance but at the mean time they led to a high computational complexity.",
        "In this study, we propose a novel method that reduces the dimensionality using multilingual resource.",
        "We first introduce a new metric called TC to measure the term consistency constraints.",
        "We use this metric to deduce a TC matrix from the multilingual corpus.",
        "Then we combine this matrix to the term-by-document matrix and do the Latent Semantic Indexing.",
        "By adopting different TC threshold, we can truncate the TC matrix into small size and thus lower the computational cost of LSI.",
        "The remainder of this paper is organized as follows.",
        "Section 2 describes the dimensionality reduction method using multilingual resource.",
        "Section 3 shows the experimental results to evaluate the dimensionality reduction method.",
        "Finally, we provide conclusions and remarks of future work in Section 4."
      ]
    },
    {
      "heading": "2. Dimensionality reduction using multilingual resource",
      "text": [
        "As mentioned above, the queries and documents are represented by vectors of terms.",
        "The weight of each term indicates its contribution to the vectors.",
        "Many weighting schemes have been proposed.",
        "The simplest form is to use the term-frequency (TF) as the term weight.",
        "In this condition, a document can be represented as a vector d = (tf1, f2,..., tfn ), where tfi is the frequency of the ith term in the document.",
        "A widely used refinement to this model is to weight each term based on its inverse document frequency (IDF) in the documents collection.",
        "This is commonly done by multiplying the frequency of each term i by log(Af / dfi ), where N is the total number of documents in the collection, and dfi is the number of documents that contain the ith term.",
        "This leads to the TF-IDF representation of the documents.",
        "Although the TF-IDF weighting scheme has many variants (Buckley, 1985; Berry et al., 1999; Robertson et al., 1999), the idea is the same one that uses the statistical information such as TF and IDF to calculate the term weight of vectors.",
        "This kind of statistical information is independence with languages.",
        "For example, in one language, say La, we have a vocabulary V = [w1a, w2a, wna} and a documents collection Da = [d1a, d2a,..., dma }.",
        "If this documents collection has a parallel corpus in language Lb, say, Db = [d1b, d2b,..., dmb } and a vocabulary V = [wb, w2b, wnb}.",
        "When we put a query Qka = [qkla, qaa } (qkla G V) into an information retrieval system.",
        "The information retrieval system will converts the query Qkaand the documents in the collection Da into vectors.",
        "By calculating the similarity between query Qkaand each document dia, the system selects the documents whose similarity is higher than a threshold as the results Rka.",
        "If we translate the query Qka into language Lb and get query Qkb, when putting the Qkb into the same information retrieval system, we get the retrieval results Rkb.",
        "Since the Qka and Qkb contain the same content and only expressed in different languages.",
        "We expect that Rka and Rkb will contain the same content.",
        "If this assumption holds, the vocabulary which is used to build queries and documents vectors should have high representative ability.",
        "Since the weight of each term in the vector is calculated by the statistical information such as TF and IDF.",
        "If the vocabulary V and V have high representative ability, their statistical information will be consistent as well.",
        "This is the main motivation of our dimensionality reduction method.",
        "The most straightforward way to measure the word's representability in multilingual resource is to calculate the TF and IDF of each word in different languages.",
        "But this method has one problem that the TF-IDF scheme is dedicated for each single document, the same word will have different weight in different documents.",
        "It is impractical to impose the consistency constraint to every document.",
        "Even we can do that, this method still has the drawback that it is very difficult to port to another documents collection.",
        "To address this problem, we consider the whole documents collection as one single document.",
        "In this condition, the IDF will be a fixed number.",
        "We introduce a new metric to measure the term consistency called TC.",
        "Figure 1 and Figure 2 illustrate the basic idea.",
        "In these figures, the curve Lashows the word logarithmic frequency in the documents collection of language La, the curve Lbshows the corresponding translation's logarithmic frequency in the documents collection of language Lb.",
        "TC, and TCj are the term consistency of w, and Wj respectively.",
        "Figure 1 shows the TC in normal condition that the average word frequency in language a is proximate to that of language b.",
        "In this case, the TC is defined as below:",
        "Here ft is the frequency of w\" in language a. ft is the frequency of the w a's translation in language b.",
        "In multilingual case, the TC(w ) will be defined as below:",
        "TC(w) = min(TC(wb),... TC(wn)) (2) In the case that the average word frequency in language a is different with that of language b, we will first calculate the moving average as shown in the Figure 2.",
        "After that, we use the moving average to calculate the TC of wt as below:",
        "Here H is distance between the moving average and the original one.",
        "w i words wj",
        "Figure 1.",
        "TC in normal condition",
        "w i words w",
        "Figure 2.",
        "TC in shift condition",
        "Once we get the TC of every word in language a, we present it in a diagonal matrix Tt xt =diag(TC1,",
        "When applying the TC matrix Tt xt in information retrieval, we combine Tt xt into the term-by-document matrix Atxd .",
        "Where Atxd = [aiJ] and the ai is the weight of term i in document .",
        "We get a new matrix Btxd =TtxtAtxd .",
        "Then following the classical LSI, we replace Bt xd by a low-rank approximation derived from its truncated Singular Value Decomposition (SVD):",
        "nt xd ~ u t xir^n xrvdxn",
        "The main problem of LSI is that it usually led to a high computational complexity since the matrix Btxd usually in 10-10 dimensional space.",
        "To lower the computational cost, we truncate the TC matrix Ttxt according to different TC threshold we get B r x d _Tr x rAr xd .",
        "Since r is small than t, the computational cost on the matrix Brxd will lower than Btxd .",
        "Note that the matrix Brxd is deduced from the TC matrix Tt xt which is sorted by word representative ability.",
        "It will contain less noise and outperform the original matrix Atxd .",
        "The experimental results have shown the effective of this method.",
        "Language a Language b",
        "M",
        "lTCi",
        "l",
        "ff",
        "H",
        "M",
        "11",
        "M",
        "w",
        "Ii",
        "r",
        "In",
        "TCi",
        "Language a Language b",
        "\\|",
        "1",
        "Pi",
        "âAal",
        "TCj",
        "\" \"Moving average",
        "1",
        "r ir",
        "For one word w?",
        "in language La, there are always several translations in language Lb, say (wi1b, wi2b,..., wikb).",
        "To handle this one-to-many phenomenon, we calculate the co-occurrence of w,aand each translation and select the highest one as the translation of w,a."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "We adopt a VSM based IR system to evaluate the dimensionality reduction method presented in Section 2.",
        "The term weight in the term-by-document matrix is calculated by the TF-IDF weighting scheme.",
        "The training corpus comes from Chinese Linguistic Data Consortium (http://www.chineseldc.org/, abbreviate as CLDC).",
        "Its code number is \"2004-863009\".",
        "This parallel corpus contains parallel texts in Chinese, English and Japanese.",
        "It is aligned to sentence level.",
        "The sentence alignment is manually verified and the sampling examination shows the accuracy reaches 99.9%.",
        "The experiments are conducted on two test corpora.",
        "The first one is the information retrieval test corpus gotten from CLDC (\"2003-863-006\").",
        "It is a Chinese IR corpus and contains 20 topics for test.",
        "Each topic has key words and description and narrative.",
        "The second one is the Reuters 2001 data (http://about.reuters.com/researchandstandards/cor pus/ ).",
        "This corpus is a collection of about 810,000 Reuters English news stories from August 20, 1996",
        "Filtering Tracks (Robertson and Soboroff, 2002).",
        "In TREC-10, 84 Reuters categories were used to simulate user profiles.",
        "The evaluate measure is a version of van Rijsbergen(1979)'s F measure with /?=1(we denote it as F1).",
        "The table1 and table2 show the experimental results conducted on Chinese and English test Corpus respectively.",
        "In these tables, we compare our method with basic LSI and LPI (Xiaofei et.al, 2004).",
        "In the table1, the 'C-E' means the TC matrix gotten from Chinese-English training collection (deduced from the trilingual training corpus).",
        "The 'C-J' means that the TC matrix gotten from Chinese-Japanese training collection, and so force the 'C-E-J'.",
        "All the TC matrices have been normalized to range from 0 to 1.",
        "The threshold 6 is used to truncate the TC matrix into small size.",
        "Bigger 6 corresponds to smaller truncated TC matrix.",
        "Note that here 6 is discrete since for some 6, the size of truncated matrix is very similar.",
        "For example, when 6 = 0.85 and 6 = 0.9, the size of truncated TC matrices are the same one.",
        "From the experimental results, we can see that our method make great enhancement to the basic LSI method.",
        "And our method also outperforms the LPI method in both test corpora.",
        "Comparing the performance on different training collection, we can find that the difference is subtle.",
        "In Chinese test corpus, the TC matrix gotten from C-E-J training collection get the best performance (F1=0.4226) at #=0.45 while the C-E test collection get 0.4204",
        "LSI: 0.3785, LPI: 0.405",
        "e",
        "C-E",
        "C-J",
        "C-E-J",
        "0.3",
        "0.404",
        "0.4014",
        "0.4124",
        "0.4",
        "0.4098",
        "0.406",
        "0.4185",
        "0.45",
        "0.4159",
        "0.4185",
        "0.4226",
        "0.5",
        "0.4204",
        "0.4124",
        "0.4105",
        "0.55",
        "0.4061",
        "0.4027",
        "0.3997",
        "0.6",
        "0.3913",
        "0.3992",
        "0.396",
        "0.8",
        "0.3856",
        "0.3867",
        "0.3842",
        "0.85",
        "0.3744",
        "0.3754",
        "0.3768",
        "Table1.F1 measure of Chinese test corpus",
        "LSI: 0.3416, LPI: 0.3556",
        "6",
        "E-C",
        "E-J",
        "E-C-J",
        "0.3",
        "0.356",
        "0.3478",
        "0.3578",
        "0.4",
        "0.3578",
        "0.3596",
        "0.3702",
        "0.45",
        "0.3698",
        "0.3651",
        "0.3734",
        "0.5",
        "0.3636",
        "0.3575",
        "0.363",
        "0.55",
        "0.3523",
        "0.3564",
        "0.3477",
        "0.6",
        "0.3422",
        "0.3448",
        "0.3458",
        "0.8",
        "0.3406",
        "0.3397",
        "0.3378",
        "0.85",
        "0.3304",
        "0.3261",
        "0.3278",
        "Table2.",
        "F1 measure of English test corpus",
        "at #=0.5 and the C-J test collection get 0.4185 at #=0.45.",
        "For the English test corpus, the trilingual training collection also gets the best performance.",
        "But the difference between bilingual and trilingual training collection is also subtle (E-C-J: Fl=0.3734, E-C: Fl=0.3698, E-J: Fl=0.3651).",
        "In the English test corpus, all the training collection get the best performance at 6=0.45.",
        "As mentioned before, the bigger 6 means the smaller size of the truncated TC matrix.",
        "While small size of the truncated TC matrix means low computational cost and high system speed.",
        "This is one of the advantages of our method over the traditional LSI method.",
        "We conducted some experiments to test the system speed on different threshold 6.",
        "We use the number of documents per second (docs/s) to denote this kind of system speed.",
        "The experiment is conducted on the personal computer with a Pentium (R) 4 processor @2.8GHz, 256 KB cache and 512 MB memory.",
        "Table 3 shows the experimental results that the # vs. system speed and Figure 3 illustrates the Fl measure vs. the system speed.",
        "Table 3.",
        "9 vs. system speed",
        "Figure 3.",
        "Fl measure vs. system speed"
      ]
    },
    {
      "heading": "4. Conclusions",
      "text": [
        "In this paper, we present a novel method that reduces the dimensionality using multilingual resource.",
        "We deduce a TC matrix from the multilingual corpus and then truncate it to small size according to different TC threshold.",
        "Then we use the truncated matrix together with the term-by-document matrix to do the LSI analysis.",
        "Since the truncated TC matrix is sorted by word representative ability.",
        "It will contain less noise than the original term-by-document matrix.",
        "The experimental results have shown the effectiveness of this method.",
        "In the future, we will try to find the optimal truncate threshold # automatically.",
        "And since it is more difficult to get the parallel corpora than comparable corpora, we will explore using comparable corpora to do the dimensionality reduction.",
        "Acknowledgement",
        "This research was carried out through financial support provided under the NEDO International Joint Research Grant Program (NEDO Grant).",
        "9",
        "C-E",
        "C-J",
        "C-E-J",
        "0.3",
        "1039.3",
        "1034.4",
        "1355.0",
        "0.4",
        "1148.4",
        "1188.9",
        "1372.5",
        "0.45",
        "1290.5",
        "1246.9",
        "1391.3",
        "0.5",
        "1323.9",
        "1323.3",
        "1469.6",
        "0.55",
        "1393.3",
        "1392.6",
        "1563.8",
        "0.6",
        "1413.3",
        "1508.8",
        "1590.1",
        "0.8",
        "1513.1",
        "1555.6",
        "1660.5",
        "0.85",
        "1641.1",
        "1778.2",
        "1773.5"
      ]
    }
  ]
}
