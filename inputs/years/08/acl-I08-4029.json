{
  "info": {
    "authors": [
      "Yu-Chieh Wu",
      "Jie-Chi Yang",
      "Yue-Shi Lee"
    ],
    "book": "Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-I08-4029",
    "title": "Description of the NCU Chinese Word Segmentation and Part-of-Speech Tagging for SIGHAN Bakeoff 2007",
    "url": "https://aclweb.org/anthology/I08-4029",
    "year": 2008
  },
  "references": [
    "acl-C04-1081",
    "acl-D07-1131",
    "acl-I05-3034",
    "acl-P04-1059",
    "acl-P07-2017",
    "acl-W03-0407",
    "acl-W04-3230",
    "acl-W04-3236",
    "acl-W06-0127",
    "acl-W06-2937",
    "acl-W95-0107"
  ],
  "sections": [
    {
      "text": [
        "Description of the NCU Chinese Word Segmentation and Part-of-Speech",
        "Tagging for SIGHAN Bakeoff 2007",
        "of Computer Science and Information Engineering National Central University",
        "Taoyuan, Taiwan",
        "Graduate Institute of Network Learning Technology National Central University",
        "of Computer Science and Information Engineering Ming Chuan University",
        "In Chinese, most of the language processing starts from word segmentation and part-of-speech (POS) tagging.",
        "These two steps tokenize the word from a sequence of characters and predict the syntactic labels for each segmented word.",
        "In this paper, we present two distinct sequential tagging models for the above two tasks.",
        "The first word segmentation model was basically similar to previous work which made use of conditional random fields (CRF) and set of predefined dictionaries to recognize word boundaries.",
        "Second, we revise and modify support vector machine-based chunking model to label the POS tag in the tagging task.",
        "Our method in the WS task achieves moderately rank among all participants, while in the POS tagging task, it reaches very competitive results."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "With the rapid expansion of online text articles such as blog, web news, and research/technical reports, there is an increasing demand for text mining and management.",
        "Different from western-like languages, handling oriented languages is far more difficult since there is no explicit boundary symbol to indicate what a word is in the text.",
        "However the most important preliminary step for natural language processing is to tokenize words and separate them from the word sequence.",
        "In Chinese, the word tokenization is also known as word segmentation or Chinese word tokenization.",
        "The problem of the Chinese word segmentation is very critical for most Chinese linguistics because the error segmented words deeply affects the downstream purpose, like POS tagging and parsing.",
        "In addition tokenizing the unknown words is also an unavoidable problem.",
        "To support the above targets, it is necessary to detect the boundaries between words in a given sentence.",
        "In tradition, the Chinese word segmentation technologies can be categorized into three types, (heuristic) rule-based, machine learning, and hybrid.",
        "Among them, the machine learning-based techniques showed excellent performance in many recent research studies (Peng et al., 2004; Zhou et al., 2005; Gao et al., 2004).",
        "This method treats the word segmentation problem as a sequence of word classification.",
        "The classifier online assigns either \"boundary\" or \"non-boundary\" label to each word by learning from the large annotated corpora.",
        "Machine learning-based word segmentation method is quite similar to the word sequence inference techniques, such as part-of-speech (POS) tagging (Clark et al., 2003; Gimenez and Marquez, 2003), phrase chunking (Lee and Wu, 2007) and word dependency parsing (Wu et al., 2006, 2007).",
        "In this paper, we present two prototype systems for Chinese word segmentation and POS tagging tasks.",
        "The former was basically an extension of previous literatures (Ng and Low, 2004; Zhou et al., 2006), while the latter incorporates the unknown word and known word tagging into one step.",
        "The two frameworks were designed based on two variant machine learning algorithms, namely CRF and SVM.",
        "In our pilot study, the SVM showed better performance than CRF in the POS tagging task.",
        "To identify unknown words, we also encode the suffix and prefix features to represent the training example.",
        "The strategy was showed very effective for improving both known and unknown word chunking on both Chinese and English phrase chunking (Lee and Wu, 2007).",
        "In this year, the presented word segmentation method achieved moderate rank among all participants.",
        "Meanwhile, the proposed SVM-based POS tagging model reached very competitive accuracy in most POS tasks.",
        "For example, our method yields second best result on the CTB POS tagging track.",
        "The rest of this paper is organized as follows.",
        "Section 2 describes employed machine learning algorithms, CRF and SVM.",
        "In section 3, we present the proposed word segmentation and POS tagging framework which used for the SIGHAN-bake-off this year.",
        "Experimental result and evaluations are reported in section 4.",
        "Finally, in section 5, we draw conclusion and future remarks."
      ]
    },
    {
      "heading": "2. Classification Algorithms",
      "text": [
        "Conditional random field (CRF) was an extension of both Maximum Entropy Model (MEMs) and Hidden Markov Models (HMMs) that was firstly introduced by (Lafferty et al., 2001).",
        "CRF defined conditional probability distribution P(YX) of given sequence given input sentence where Y is the \"class label\" sequence and X denotes as the observation word sequence.",
        "A CRF on (X,Y) is specified by a feature vector F of local context and the corresponding feature weight A .",
        "The F can be treated as the combination of state transition and observation value in conventional HMM.",
        "To determine the optimal label sequence, the CRF uses the following equation to estimate the most probability.",
        "The most probable label sequence y can be efficiently extracted via the Viterbi algorithm.",
        "However, training a CRF is equivalent to estimate the parameter set A for the feature set.",
        "In this paper, we directly use CRF++ (Kudo and Matsumoto, 2003) which included the quasi-Newton L-BFGS method (Nocedal and Wright, 1999) to iterative update the parameters.",
        "Assume we have a set of training examples,",
        "(xi, yi), (x2, y2),...,(xn, yn), xi e 91D, y, e {+1, -1} where xi is a feature vector in D-dimension space of the i-th example, and yi is the label of xi either positive or negative.",
        "The training of SVMs involves minimizing the following object function (primal form, soft-margin (Vapnik, 1995)):",
        "The loss function indicates the loss of misclassification risk.",
        "Usually, the hinge-loss is used (Vapnik, 1995; Keerthi and DeCoste, 2005).",
        "The factor C in (1) is a parameter that allows one to trade off training error and margin size.",
        "To classify a given testing example X, the decision rule takes the following form:",
        "at represents the weight of training example xtwhich lies on the hyperplane, and b denotes as a bias threshold.",
        "SVs means the support vectors and obviously has the non-zero weights of ai.",
        "K(X, xt) = <fi(X) <25(xt) is a predefined kernel function that might transform the original feature space from 91D to 91D (usually D<<D').",
        "In the linear kernel form, the K( X, x ) simply compute the dot products of the two variables.",
        "By introducing of the polynomial kernel, we rewrite the decision function of (1) as:",
        "and d is the polynomial kernel degree.",
        "In many NLP problems, the training and testing examples are represented as bits of binary vectors.",
        "In this section, we focus on this case.",
        "Later, we present a general form without considering this constraint."
      ]
    },
    {
      "heading": "3. System Description",
      "text": [
        "In this section, we first describe the problem settings for the word segmentation problems.",
        "In section 3.2, the proposed POS tagging framework is then presented.",
        "Similar to English text chunking (Ramshaw and",
        "Marcus, 1995; Lee and Wu, 2007), the word sequence classification model aims to classify each word via encoding its context features.",
        "By encoding with BIES (LMR tagging scheme) or IOB2 style, both WS and NER problems can be viewed as a sequence of word classification.",
        "During testing, we seek to find the optimal word type for each Chinese character.",
        "These types strongly reflect the actual word boundaries for Chinese words or named entity phrases.",
        "As reported by (Zhou et al., 2006), the use of richer tag set can effectively enhance the performance.",
        "They extend the tag of \"Begin of word\" into \"second-begin\" and \"third-begin\" to capture more character types.",
        "However, there are some ambiguous problem to the 3-character Chinese words and 4-character Chinese words.",
        "For example, to encode \"^lltjt\" with his extended tag set, the first character can be encoded as \"B\" tag.",
        "But for the second character, we can use \"second-begin\" or \"I\" tag to represent the middle of word.",
        "In order to make the extension clearer, in this paper, we explicitly extend the B tag and E tag with \"after begin\" (BI), and \"before end\" (IE) tags.",
        "Table 1 lists the difference between the traditional",
        "BIES and the proposed E-BIES encodings methods.",
        "Table 2 illustrates an example of how the BIES and E-BIES encode with different number of characters.",
        "To effect classify each character, in this paper, we adopted most feature types to train the CRF (Kudo and Matsumoto, 2004).",
        "Table 3 lists the adopted feature templates.",
        "The dictionary flag is very similar to previous literature (Ng and Low, 2004) while we adding up English full-character into our dictionary.",
        "BIES",
        "E-BIES",
        "Begin of a word",
        "B",
        "B",
        "After begin of a word",
        "-",
        "BI",
        "Middle of a word",
        "I",
        "I",
        "Before end of a word",
        "-",
        "IE",
        "End of a word",
        "E",
        "E",
        "Single word",
        "S",
        "S",
        "N-character word",
        "BIES",
        "E-BIES",
        "S",
        "S",
        "B,E",
        "B,E",
        "B,I,E",
        "B,BI,E",
        "B,I,I,E",
        "B,BI,IE,E",
        "B,I,I,I,E",
        "B,BI,I,IE,E",
        "Feature Type",
        "Context Position",
        "Description",
        "Unigram",
        "C-2,C-1,C0,C1,C2",
        "Chinese character fe ature",
        "Nearing Bigram",
        "(C-2,C-0(C-iA) (CiAXCiA)",
        "Bi-character feature",
        "Jump Bigram",
        "(C-i,Ci)",
        "Non-continuous character feature",
        "Dictionary Flag",
        "C0",
        "Date, Digital, English letter or punctuatio n",
        "Dictionary Flag iV-gram",
        "(C-i,C0,Ci)",
        "iV-gram of the dictionary flags",
        "96.8% in English WSJ.",
        "The learner used in his literature is maximum entropy model.",
        "However the main limitation of his POS tagging strategy is that the unknown word classification problem was not resolved.",
        "To circumvent this vita, we simply extend the idea of SVM-based chunker (Lee and Wu, 2007) and develop our own SVM-based POS tagger.",
        "Although CRF showed excellent performance in word segmentation task, in English POS tagging, the SVM is more effective than CRF.",
        "Also in our closed experiment, we had tried transformation-based error-driven learner (TBL), CRF, and SVM classifiers.",
        "The pilot experiment showed that the SVM outperformed the other two learners and achieved almost 94% accuracy in the CTB data.",
        "Meanwhile TBL reached the worst result than the other two classifiers (~88%).",
        "Handling unknown word is very important to POS tagging problem.",
        "As pointed out by (Lee and Wu, 2007; Gimenez, and Marquez, 2003), the introduction of suffix features can effectively help to guess the unknown words for tagging and chunking.",
        "Different from (Gimenez and Marquez, 2003), we did not derive data for unknown word guessing.",
        "Instead, we directly encode all suffix-and prefix-features for each training instance.",
        "In training phase, the rich feature types are able to disambiguate not only the unknown word guessing, but also improve the known word classification.",
        "As reported by (Lee and Wu, 2007), the strategy did improve the English and Chinese chunking performance for both known and unknown words.",
        "ture selection experiment for each tagging corpus, instead a unified feature set was used due to the time line.",
        "We trust our POS tagger could be further improved by removing or adding new feature set.",
        "The learner used in this paper (SVM) is mainly developed by our own (Wu et al., 2007).",
        "The cost factor C is simply set as 0.15 for all languages.",
        "Furthermore, to remove rare words, we eliminate the words which appear no more than twice in the training data."
      ]
    },
    {
      "heading": "4. Evaluations and Experimental Result",
      "text": [
        "In this year, we mainly focus on the close track for WS and POS tagging tracks.",
        "The CTB, SXU, and NCC corpora were used for evaluated the presented word segmentation method, while all the released POS tagging data were tested by our",
        "SVM-based tagger, included CityU, CKIP, CTB,",
        "NCC, and PKU.",
        "Both settings of the two models were set as previously noted.",
        "The evaluation of the two tasks was mainly measured by the three metrics, namely, recall, precision, and f-measure.",
        "However, the evaluation process for the POS tagging track is somewhat different from WS.",
        "In WS, participant should reform the testing data into sentence level whereas in the POS tagging track the word had been correctly segmented.",
        "Thus the measurement of the POS tagging track is mainly accuracy-based (correct or incorrect).",
        "tagging task",
        "The used feature set of our POS tagger is listed in Table 4.",
        "In this paper, we did not conduct the fea-",
        "In this year, we only select the following three data to perform our method for the word segmentation task.",
        "They are CTB, NCC, and SXU where the NCC and SXU are fresh in this year.",
        "Table5 shows the experimental results of our model in the close WS track with except for CKIP and CityU corpora.",
        "Feature Type",
        "Context Position",
        "Description",
        "Unigram",
        "W-2,W-1,W0,W1,W2",
        "Chinese word feat ure",
        "Nearing Bigram",
        "(W-2,W-1)(W-1,W0) (W1,W0)(W1,W2)",
        "Bi-word feature",
        "Jump Bigram",
        "(W-2,W0)(W-1,W1)",
        "(W2,Wo)(Wi,W3)",
        "Non-continuous character feature",
        "Possible tags",
        "Possible POS tag i n the training data",
        "Prefix 3/2/1 characters",
        "W-1,W0,W1",
        "Pre-characters of word",
        "Suffix 3/2/1 characters",
        "W-1,W0,W1",
        "Post-characters of word",
        "Recall",
        "Precision",
        "F-measure",
        "CTB",
        "0.9471",
        "0.9500",
        "0.9486",
        "NCC",
        "0.9236",
        "0.9269",
        "0.9252",
        "SXU",
        "0.9505",
        "0.9515",
        "0.9510",
        "As shown above, our method in the CTB data showed 10th best out of 26 submissions.",
        "In the NCC and SXU datasets, our method achieved 19/26 and 18/30 rank.",
        "In overall, the presented ex-tend-BIES scheme seems to work well on the CTB data and results in middle rank in comparison to the other participants.",
        "In the second experiment, we focus on the designed POS tagging model.",
        "To measure the effectiveness, we apply our method to all the released dataset, i.e., CityU, CKIP, CTB, NCC, and PKU.",
        "Similar to WS task, our method is still very effective to CTB dataset.",
        "It turns out our method achieved second best in the CTB, while for the other corpora, it achieved 4th best among all the participants.",
        "We also found that our method was very close to the top 1 score about 1.3% (CKIP) to worse than the best system in 0.8% in overall accuracy.",
        "We conclude that by selecting suitable features and cost factor C to SVM, our method can be further improved.",
        "We left the work as future direction."
      ]
    },
    {
      "heading": "5. Conclusions and Future Work",
      "text": [
        "Chinese word segmentation is the most important infrastructure for many Chinese linguistic technologies such as text categorization and information retrieval.",
        "In this paper, we present simple Chinese word segmentation and part-of-speech tagging models based on the conventional sequence classification technique.",
        "We treat the two tasks as two different learning framework and applying CRF and SVM as separated learners.",
        "Without any prior knowledge and rules, such a simple technique shows satisfactory results on both word segmentation and part-of-speech tagging tasks.",
        "In POS tagging task, our model shows very competitive results which merely spend few hours to train.",
        "To reach state-of-the-art, our method still needs to further select features and parameter tunings.",
        "In the future, one of the main directions is to extend this model toward full unsupervised learning from large un-annotated text.",
        "Mining from large unla-beled data have been showed benefits to improve the original accuracy.",
        "Thus, not only the stochastic feature analysis, but also adjust the learner from unlabeled data are important future remarks.",
        "Riv",
        "Roov",
        "Rmt",
        "Accuracy",
        "CityU",
        "0.9326",
        "0.4322",
        "0.8707",
        "0.8865",
        "CKIP",
        "0.9504",
        "0.5631",
        "0.9065",
        "0.9160",
        "CTB",
        "0.9554",
        "0.7135",
        "0.9183",
        "0.9401",
        "NCC",
        "0.9658",
        "0.5822",
        "0.9116",
        "0.9456",
        "PKU",
        "0.9591",
        "0.5832",
        "0.9173",
        "0.9368"
      ]
    }
  ]
}
