{
  "info": {
    "authors": [
      "Praneeth M. Shishtla",
      "Karthik Gali",
      "Prasad Pingali",
      "Vasudeva Varma"
    ],
    "book": "Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages",
    "id": "acl-I08-5015",
    "title": "Experiments in Telugu NER: A Conditional Random Field Approach",
    "url": "https://aclweb.org/anthology/I08-5015",
    "year": 2008
  },
  "references": [
    "acl-A97-1029",
    "acl-W03-0428"
  ],
  "sections": [
    {
      "text": [
        "Praneeth M Shishtla, Karthik Gali, Prasad Pingali and Vasudeva Varma",
        "{praneethms,karthikg}@students.iiit.ac.in,{pvvpr,vv}@iiit.ac.in",
        "Language Technologies Research Centre International Institute of Information Technology Hyderabad, India",
        "Named Entity Recognition(NER) is the task of identifying and classifying tokens in a text document into predefined set of classes.",
        "In this paper we show our experiments with various feature combinations for Telugu NER.",
        "We also observed that the prefix and suffix information helps a lot in finding the class of the token.",
        "We also show the effect of the training data on the performance of the system.",
        "The best performing model gave an Fß=1 measure of 44.91.",
        "The language independent features gave an Fß=1measure of 44.89 which is close to Fß=1measure obtained even by including the language dependent features."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The objective of NER is to identify and classify all tokens in a text document into predefined classes such as person, organization, location, miscellaneous.",
        "The Named Entity information in a document is used in many of the language processing tasks.",
        "NER was created as a subtask in Message Understanding Conference (MUC) (Chinchor, 1997).",
        "This reflects the importance of NER in the area of Information Extraction (IE).",
        "NER has many applications in the areas of Natural Language Processing, Information Extraction, Information Retrieval and speech processing.",
        "NER is also used in question answering systems (Toral et al., 2005; Molla et al., 2006), and machine translation systems (Babych and Hartley, 2003).",
        "It is also a subtask in organizing and retrieving biomedical information (Tsai, 2006).",
        "The process of NER consists of two steps• identification of boundaries of proper nouns.",
        "• classification of these identified proper nouns.",
        "The Named Entities(NEs) should be correctly identified for their boundaries and later correctly classified into their class.",
        "Recognizing NEs in an English document can be done easily with a good amount of accuracy(using the capitalization feature).",
        "Indian Languages are very much different from the English like languages.",
        "Some challenges in named entity recognition that are found across various languages are: Many named entities(NEs) occur rarely in the corpus i.e they belong to the open class of nouns.",
        "Ambiguity of NEs.",
        "Ex Washington can be a person's name or a place name.",
        "There are many ways of mentioning the same Named Entity(NE).",
        "In case of person names, Ex: Abdul Kalam, A.P.J.Kalam, Kalam refer to the same person.",
        "And, in case of place names Waran-gal, WGL both refer to the same location.",
        "Named Entities mostly have initial capital letters.",
        "This discriminating feature of NEs can be used to solve the problem to some extent in English.",
        "Indian Languages have some additional challenges: We discuss the challenges that are specific to Telugu.",
        "Absence of capitalization.",
        "Ex: The condensed form of the person name S.R.Shastry is written as S.R.S in English and is represented as srs in Telugu.",
        "Agglutinative property of the Indian Languages makes the identification more difficult.",
        "Agglutinative languages such as Turkish or Finnish, Telugu etc.",
        "differ from languages like English in the way lexical forms are generated.",
        "Words are formed by productive affixations of derivational and inflectional suffixes to roots or stems.",
        "For example: warangal, warangal ki, warangalki, warangallo, warangal ni etc .. all refer to the place Waran-gal.",
        "where lo, ki, ni are all postpostion markers in Telugu.",
        "All the postpositions get added to the stem hyderabad.",
        "There are many ways of representing acronyms.",
        "The letters in acronyms could be the English alphabet or the native alphabet.",
        "Ex: B.J.P and BaJaPa both are acronyms of Bharatiya Janata Party.",
        "Telugu has a relatively free word order when compared with English.",
        "The morpohology of Telugu is very complex.",
        "The Named Entity Recognition algorithm must be able handle most of these above variations which otherwise are not found in languages like English.",
        "There are not rich and robust tools for the Indian Languages.",
        "For Telugu, though a Part Of Speech(POS) Tagger for Telugu, is available, the accuracy is less when compared to English and Hindi."
      ]
    },
    {
      "heading": "2. Problem Statement",
      "text": [
        "NER as sequence labelling task",
        "Named entity recognition (NER) can be modelled as a sequence labelling task (Lafferty et al., 2001).",
        "Given an input sequence of words Wn = w1w2w3...wn, the NER task is to construct a label sequence L\" = l1l2l3 ...ln , where label li either belongs to the set of predefined classes for named entities or is none(representing words which are not proper nouns).",
        "The general label sequence l1 has the highest probability of occuring for the word sequence W1 among all possible label sequences, that is"
      ]
    },
    {
      "heading": "3. Conditional Random Fields",
      "text": [
        "Conditional Random Fields (CRFs) (Wallach, 2004) are undirected graphical models used to calculate the conditional probability of values on designated output nodes given values assigned to other designated input nodes.",
        "In the special case in which the output nodes of the graphical model are linked by edges in a linear chain, CRFs make a first-order Markov independence assumption, and thus can be understood as conditionally-trained finite state machines(FSMs).",
        "Let o = ( O1,O2,...OT) be some observed input data sequence, such as a sequence of words in text in a document,(the values on n input nodes of the graphical model).",
        "Let S be a set of FSM states, each of which is associated with a label, l e L. Let s = (s1 ,s2,... sT,) be some sequence of states,(the values on T output nodes).",
        "By the Hammersley-Clifford theorem CRFs define the conditional probability of a state sequence given an input sequence to be",
        "where Zo is a normalization factor over all state sequences, is an arbitrary feature function over its arguments, and Ak is a learned weight for each feature function.",
        "A feature function may, for example, be defined to have value 0 or 1.",
        "Higher A weights make their corresponding FSM transitions more likely.",
        "CRFs define the conditional probability of a label sequence based on total probability over the state sequences, P(l|o) = Es:l(S)=lP(s|o) where l(s) is the sequence of labels corresponding to the labels of the states in sequence s. Note that the normalization factor, Zo, (also known in statistical physics as the partition function) is the sum of the scores of all possible state sequences,",
        "seST t=1 k",
        "and that the number of state sequences is exponential in the input sequence length,T.",
        "In arbitrarily-structure CRFs, calculating the partition function in closed form is intractable, and approximation methods such as Gibbs sampling, or loopy belief propagation must be used."
      ]
    },
    {
      "heading": "4. Features",
      "text": [
        "There are many types of features used in general NER systems.",
        "Many systems use binary features i.e. the word-internal features, which indicate the presence or absence of particular property in the word.",
        "(Mikheev, 1997; Wacholder et al., 1997; Bikel et al., 1997).",
        "Following are examples of binary features commonly used.",
        "All-Caps (IBM), Internal capitalization (eBay), initial capital (Abdul Kalam), uncapitalized word (can), 2-digit number (83, 28), 4-digit number (1273, 1984), all digits (8, 31, 1228) etc.",
        "The features that correspond to the capitalization are not applicable to Telugu.",
        "We have not used any binary features in our experiments.",
        "Gazetteers are used to check if a part of the named entity is present in the gazetteers.",
        "We don't have proper gazetteers for Telugu.",
        "Lexical features like a sliding window [w-2,w-1,wo,w1,w2] are used to create a lexical history view.",
        "Prefix and suffix tries were also used previously(Cucerzan and Yarowsky,1999).",
        "Linguistics features like Part Of Speech, Chunk, etc are also used.",
        "We donot have a highly accurate Part Of Speech(POS) tagger.",
        "In order to obtain some POS and chunk information, we ran a POS Tagger and chunker for telugu (PVS and G, 2007) on the data.",
        "And from that, we used the following features in our experiments.",
        "Language Independent Features",
        "Language Dependent Features",
        "POS of current word: POS0Chunk of current word: Chunk0",
        "Each feature is capable of providing some information about the NE.",
        "The word window helps in using the context information while guessing the tag of the token.",
        "The prefix and suffix feature to some extent help in capturing the variations that may occur due to agglutination.",
        "The POS tag feature gives a hint whether the word is a proper noun.",
        "When this is a proper noun it has a chance of being a NE.",
        "The chunk feature helps in finding the boundary of the NE.",
        "In Indian Languages suffixes and other inflections get attached to the words increasing the length ofthe word and reducing the number of occurences of that word in the entire corpus.",
        "The character n-grams can capture these variations."
      ]
    },
    {
      "heading": "5. Experimental Setup",
      "text": [
        "We conducted the experiments on the developement data released as a part of NER for South and SouthEast Asian Languages (NERSSEAL) Competetion.",
        "The corpus in total consisted of 64026 tokens out of which 10894 were Named Entities(NEs).",
        "We divided the corpus into training and testing sets.",
        "The training set consisted of 46068 tokens out of which 8485 were NEs.",
        "The testing set consisted of 17951 tokens out of which 2407 were NEs.",
        "The tagset as mentioned in the release, was based on AUKBC's ENAMEX,TIMEX and NAMEX, has the following tags: NEP (Person), NED (Designation), NEO (Organization), NEA (Abbreviation), NEB (Brand), NETP (Title-Person), NETO (Title-Object), NEL (Measure) & NETE (Terms).",
        "The corpus is tagged using the IOB tagging scheme (Ramshaw and Marcus, 1995).",
        "In this scheme each line contains a word at the beginning followed by its tag.",
        "The tag encodes the type of named entity and whether the word is in the beginning or inside the NE.",
        "Empty lines represent sentence(document) boundaries.",
        "An example is given in table 1.",
        "Words tagged with O are outside of named entities and the I-XXX tag is used for words inside a named entity of type XXX.",
        "Whenever two entities of type XXX are immediately next to each other, the first word of the second entity will be tagged B-XXX in order to show that it starts another entity.",
        "This tagging scheme is the IOB scheme originally put forward by Ramshaw and Marcus (1995).",
        "To evaluate the performance of our Named Entity Recognizer, we used three standard metrics namely precision, recall and f-measure.",
        "Precision measures the number of correct Named Entities(NEs) in the machine tagged file over the total number of NEs in the machine tagged file and the recall measures the number of correct NEs in the machine tagged file over the total number of NEs in the golden standard file while F-measure is the weighted harmonic mean of precision and recall:",
        "w1, .. , wn-1, wn.POSn: POS nth token.",
        "Chn: Chunk of nth token.",
        "pren: Prefix information of nth token.",
        "(prefix sufn: Suffix information of nth token.",
        "(suffix",
        "The more the features, the better is the performance.",
        "The inclusion of the word window, prefix and suffix features have increased the Fp=1 measure significantly.",
        "Whenever the suffix feature is included, the performance of the system increased.",
        "This shows that the system is able to caputure those agglutinative language variations.",
        "We also have experimented changing the training data size.",
        "While varying the training data size, we have tested the performance on the same amount of testing data of 17951 tokens."
      ]
    },
    {
      "heading": "6. Conclusion & Future Work",
      "text": [
        "The inclusion of prefix and suffix feature helps in improving the Fp=1 measure (also recall) of the system.",
        "As the size of the training data is increased, the Fp=1 measure is increased.",
        "Even without the language specific information the system is able to perform well.",
        "The suffix feature helped improve the recall.",
        "This is due to the fact that the POS tagger also uses the same features in predicting the POS tags.",
        "Prefix, suffix and word are three non-linguistic features that resulted in good performance.",
        "We plan to experiment with the character n-gram approach (Klein et al., 2003) and include gazetteer information.",
        "Token",
        "Named Entity Tag",
        "Swami",
        "B-NEP",
        "Vivekananda",
        "I-NEP",
        "was",
        "O",
        "born",
        "O",
        "on",
        "O",
        "January",
        "B-NETI",
        ",",
        "I-NETI",
        "12",
        "I-NETI",
        "in",
        "O",
        "Calcutta",
        "B-NEL",
        "O",
        "Features",
        "Precision",
        "Recall",
        "Fß=1",
        "Cho",
        "51.41%",
        "9.19%",
        "15.59",
        "POSo",
        "46.32%",
        "9.52%",
        "15.80",
        "POSo.Cho",
        "46.63%",
        "9.69%",
        "16.05",
        "W-3+3.Cho",
        "59.08%",
        "19.50%",
        "29.32",
        "W-3+3.POSo",
        "58.43%",
        "19.61%",
        "29.36",
        "Cho.",
        "pren",
        "53.97%",
        "24.76%",
        "33.95",
        "POSo.",
        "pren",
        "53.94%",
        "24.93%",
        "34.10",
        "POSo.Cho.",
        "pren",
        "53.94%",
        "25.32%",
        "34.46",
        "POSo.sufn",
        "47.51%",
        "29.36%",
        "36.29",
        "POS0.Ch0.sufn",
        "48.02%",
        "29.24%",
        "36.35",
        "Cho.sufn",
        "48.55%",
        "29.13%",
        "36.41",
        "W-3+3.POS0.",
        "pren",
        "62.98%",
        "27.45%",
        "38.24",
        "W-3+3 .POSo.Cho.",
        "pren",
        "62.95%",
        "27.51%",
        "38.28",
        "W-3+3.Cho.",
        "pren",
        "62.88%",
        "27.62%",
        "38.38",
        "W-3+3.POSo.sufn",
        "60.09%",
        "30.53%",
        "40.49",
        "W-3+3 .POSo.Cho.sufn",
        "59.93%",
        "30.59%",
        "40.50",
        "W-3+3.Cho.sufn",
        "61.18%",
        "30.81%",
        "40.98",
        "POSo.Cho.",
        "pren.su fn",
        "57.83%",
        "34.57%",
        "43.27",
        "POSo.",
        "pren.sufn",
        "57.41%",
        "34.73%",
        "43.28",
        "Cho.",
        "pren.sufn",
        "57.80%",
        "34.68%",
        "43.35",
        "W-3+3.Cho.",
        "pren.sufn",
        "64.12%",
        "34.34%",
        "44.73",
        "W-3+3.POS0.",
        "pren.sufn",
        "64.56%",
        "34.29%",
        "44.79",
        "W-3+3.",
        "POSo.Cho.",
        "pren.sufn",
        "64.07%",
        "34.57%",
        "44.91",
        "Features",
        "Precision",
        "Recall",
        "Fß=1",
        "w",
        "57.05%",
        "20.62%",
        "30.29",
        "pre",
        "53.65%",
        "23.87%",
        "33.04",
        "suf",
        "47.75%",
        "29.19%",
        "36.23",
        "w.pre",
        "63.08%",
        "27.56%",
        "38.36",
        "w.suf",
        "60.93%",
        "30.76%",
        "40.88",
        "pre.suf",
        "57.94%",
        "34.96%",
        "43.61",
        "w.pre.suf",
        "64.80%",
        "34.34%",
        "44.89",
        "Number of Words",
        "Precision",
        "Recall",
        "Fß=1",
        "2500",
        "51.37%",
        "9.47%",
        "15.99",
        "5000",
        "64.74%",
        "11.93%",
        "20.15",
        "7500",
        "61.32%",
        "13.50%",
        "22.13",
        "10000",
        "66.88%",
        "23.31%",
        "34.57",
        "12500",
        "63.42%",
        "27.39%",
        "38.26",
        "15000",
        "63.55%",
        "31.26%",
        "41.91",
        "17500",
        "60.58%",
        "30.64%",
        "40.70",
        "20000",
        "58.32%",
        "30.03%",
        "39.64",
        "22500",
        "57.72%",
        "29.75%",
        "39.26",
        "25000",
        "59.33%",
        "29.92%",
        "39.78",
        "27500",
        "60.91%",
        "30.03%",
        "40.23",
        "30000",
        "62.77%",
        "30.42%",
        "40.98",
        "32500",
        "62.66%",
        "30.64%",
        "41.16",
        "35000",
        "62.08%",
        "30.81%",
        "41.18",
        "37500",
        "61.02%",
        "30.87%",
        "41.00",
        "40000",
        "61.60%",
        "31.09%",
        "41.33",
        "42500",
        "62.12%",
        "32.44%",
        "42.62",
        "45000",
        "62.70%",
        "32.77%",
        "43.05",
        "47500",
        "63.20%",
        "32.72%",
        "43.12",
        "50000",
        "64.29%",
        "34.29%",
        "44.72"
      ]
    }
  ]
}
