{
  "info": {
    "authors": [
      "Roser Morante"
    ],
    "book": "LREC",
    "id": "acl-L08-1602",
    "title": "Semantic Role Labeling Tools Trained on the Cast3LB-CoNNL-SemRol Corpus",
    "url": "https://aclweb.org/anthology/L08-1602",
    "year": 2008
  },
  "references": [
    "acl-C04-1186",
    "acl-D07-1096",
    "acl-D07-1121",
    "acl-J02-3001",
    "acl-J05-1004",
    "acl-N06-2033",
    "acl-P05-1073",
    "acl-W04-2412",
    "acl-W05-0620",
    "acl-W06-2920",
    "acl-W06-2933",
    "acl-W07-2008",
    "acl-W99-0623"
  ],
  "sections": [
    {
      "text": [
        "Semantic role labeling tools trained on the Cast3LB-CoNNL-SemRol corpus",
        "ILK-Tilburg University Postbus 90153, 5000 LE Tilburg R.Morante@uvt.nl",
        "In this paper we present the Cast3LB-CoNLL-SemRol corpus, currently the only corpus of Spanish annotated with dependency syntax and semantic roles, and the tools that have been trained on the corpus: an ensemble of parsers and two dependency-based semantic role labelers that are the only semantic role labelers based on dependency syntax available for Spanish at this moment.",
        "One of the systems uses information from gold standard syntax, whereas the other one uses information from predicted syntax.",
        "The results of the first system (86 Fi) are comparable to current state of the art results for constituent-based semantic role labeling of Spanish.",
        "The results of the second are 11 points lower.",
        "This work has been carried out as part of the project Tecnicas semiautomaticas para el etiquetado de roles semanticos en corpus del espanol."
      ]
    },
    {
      "heading": "1.. Introduction",
      "text": [
        "In this paper we present the Cast3LB-CoNLL-SemRol corpus, currently the only corpus of Spanish annotated with dependency syntax and semantic roles, and the tools that have been trained on the corpus: an ensemble of parsers and two dependency-based semantic role labelers that are the only semantic role labelers based on dependency syntax available for Spanish at this moment.",
        "One of the systems uses information from gold standard syntax, whereas the other one uses information from predicted syntax.",
        "The results of the first system (86 Fi) are comparable to current state of the art results for constituent-based semantic role labeling of Spanish.",
        "The results of the second are 11 points lower.",
        "This work has been carried out as part of the project Tecnicas semiautomaticas para el etiquetado de roles semanticos en corpus del espanol.",
        "Automatic semantic role labeling (ASRL) is a natural language processing task that consists in identifying the arguments of the predicates in a sentence and assigning a semantic role to them.",
        "The notion of semantic role is originally due to (Fillmore, 1968).",
        "Semantic roles are symbolic entities that describe the function of the participants in an event from the point of view of the situation in the real world.",
        "They allow to know who did what to whom when where how, etc.",
        "A predicate with a certain meaning assigns certain semantic roles to the participants in the event that the predicate denotes, regardless of the syntactic form of the sentence.",
        "For example, the predicate annotate would be assigned the same semantic roles in the two sentences in Example 1 despite the syntactic differences (active versus passive construction).",
        "So, a semantic role can be expressed with different syntactic structures and can occupy different positions in the sentence, especially in Spanish, where order constraints are less strict than in English.",
        "In Example 1, ARG0 is a postverbal prepositional phrase in (a), whereas it is a preverbal noun phrase in (b).",
        "(1) a.",
        "[The corpusARG1] has been annotated [by three annotatorsARGo].",
        "b.",
        "[Three annotatorsARG0] annotated [the corpusARGi].",
        "Most of the semantic role labelers developed for English use information from constituent syntax.",
        "An exception is (Hacioglu, 2004).",
        "The basic referents are the systems participating in the CoNLL Shared Tasks 2004 (Marquez and Carreras, 2004) and 2005 (Carreras and Marquez, 2005).",
        "Some additional relevant systems are those in (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Toutanova et al., 2005; Surdeanu et al., 2007).",
        "Most systems for English are trained on the PropBank (Palmer et al., 2005) corpus.",
        "In Section 2. we will point out the differences between the annotation of semantic roles in PropBank and in the Cast3LB-CoNLL-SemRol corpus.",
        "For Catalan and Spanish two systems were developed for Task 9 Multilevel Semantic Annotation of Catalan and Spanish (Marquez et al., 2007) in the framework of Se-mEval 2007, and a combined system that implements joint learning strategies is the one in (Surdeanu et al., 2008).",
        "The systems that we present in this paper are trained on the Cast3LB-CoNLL-SemRol corpus (Morante, 2006).",
        "The main difference between the systems that we present in this paper and the systems mentioned above is that they use information from dependency syntax, instead of constituent syntax.",
        "Apart from semantic role labelers, the corpus has also been used to train three dependency parsers: Nivre's MaltParser (Nivre, 2006; Nivre et al., 2006), Canisius' memory-based constraint-satisfaction inference parser (Canisius and Tjong Kim Sang, 2007), and a new memory-based parser that operates with a single word-pair relation classifier.",
        "Since parser combination has proved to improve the performance of individual parsers (Henderson and Brill, 1999; Zeman and Zabokrtsky, 2005; Sagae and Lavie, 2006), we also experimented with an ensemble of parsers that integrates the three individual parsers.",
        "As far as we know, this is the first ensemble of dependency parsers for Spanish.",
        "The contents of the paper are organised as follows.",
        "Section 2. presents the Cast3LB-CoNLL-SemRol corpus.",
        "Section 3. describes the ensemble of parsers and Section 4. the semantic role labelers.",
        "Finally, in Section 5. we put forward some conclusions.",
        "Table 1: Example sentence of the Cast3LB-CoNLL corpus of Spanish"
      ]
    },
    {
      "heading": "2.. The Cast3LB-CoNLL corpus of Spanish",
      "text": [
        "The Cast3LB-CoNLL corpus of Spanish is a manually revised version of the Cast3LB treebank (Civit et al., 2006) used in the CoNLL Shared Task 2006 that on dependency parsing (Buchholz and Marsi, 2006).",
        "The revision of the corpus was necessary due to the existence of errors, mostly caused by the conversion of the Cast3LB treebank, in constituent format, to the CoNLL shared task dependency format.",
        "The revision consisted of correcting errors in the dependency assignment and labeling.",
        "Examples of systematic errors were tagging the head of a noun phrase as a child of a prenominal adjective in that noun phrase, or tagging the verb head of a subordinate sentence as a child of a complement of that verb.",
        "In a few cases, wrong assignment of syntactic functions was also corrected, like cases of punctuation elements being assigned a syntactic function.",
        "The Cast3LB-CoNLL-SemRol (Morante, 2006) is the",
        "Cast3LB-CoNLL corpus with an additional manually annotated layer of semantic roles.",
        "Only the semantic roles of predicates expressed by verbs have been annotated.",
        "The set of semantic roles has been defined with the purpose of generalising as much as possible the semantic relation that holds between a predicate and its complements.",
        "The annotation is not meant to code information about the syntactic structure.",
        "We defined 26 semantic roles that were sufficient to annotate the corpus (details and examples can be found",
        "• ARG0 is assigned to prototypical agents, usually the subject of accusative verbs, ergative verbs in ergative constructions, and unergative verbs.",
        "We do not distinguish between different types of agents, like for example experiencers.",
        "• ARG1 is a prototypical theme, usually the object of accusative verbs, the subject of passive constructions, or the subject of ergative verbs in unaccusative constructions.",
        "We do not distinguish between different types of ARG1 like themes or patients.",
        "• ARGM is the role assigned per default, when no other roles",
        "can be assigned.",
        "• ARG_ATR is assigned to attributes, the complements required by the verbs ser (\"to be\") and estar (\"to be\") in their attributive meaning.",
        "• ARG_BEN is assigned to arguments of the verb that refer to the entity that benefits from an action or to whom the action is directed.",
        "Most of the times it is the indirect object of ditransitive verbs like dar (\"give\") or decir (\"say\").",
        "• ARG_CAU is assigned to the complements that refer to the entity or to the fact that causes an event to happen.",
        "It can be the subject of causative constructions, a prepositional phrase introduced by prepositions por or al, or a subordinate clause introduced by subordinate conjunctions por lo que, pues, porque.",
        "• ARGCOMP is assigned to complements that refer to entities that have a relation of company, coordination, or collaboration with the entity that performs, experiences or suffers the event.",
        "ARG_COMP constituents are usually introduced by prepositions con or junto con.",
        "• ARGCONC is assigned to the complement of a verb that expresses a circumstance, despite which the event expressed by the verb holds or has taken place.",
        "ARG_CONC constituents are usually introduced by preposition a pesar de or subordinate conjunction aunque.",
        "• ARGCOND is assigned to complements of the verb that express a condition.",
        "ARG_COND constituents are usually",
        "1",
        "Asimismo",
        "asimismo",
        "r",
        "rg",
        "_",
        "2",
        "MOD",
        "_",
        "2",
        "defiende",
        "defender",
        "v",
        "vm",
        "num=s,per=3,mod=i,tmp=p",
        "0",
        "ROOT",
        "_",
        "3",
        "la",
        "el",
        "d",
        "da",
        "num=s,gen=f",
        "4",
        "ESP",
        "-",
        "4",
        "financiaciOn",
        "financiacion",
        "n",
        "nc",
        "num=s,gen=f",
        "2",
        "CD",
        "ARG1",
        "5",
        "publica",
        "publica",
        "a",
        "aq",
        "num=s,gen=f",
        "4",
        "CN",
        "-",
        "6",
        "de",
        "de",
        "s",
        "sp",
        "for s",
        "4",
        "CN",
        "-",
        "7",
        "la",
        "el",
        "d",
        "da",
        "num=s,gen=f",
        "8",
        "ESP",
        "_",
        "8",
        "investigaciOn investigaciOn",
        "n",
        "nc",
        "num=s,gen=f",
        "6",
        "_",
        "_",
        "9",
        "basica",
        "basico",
        "a",
        "aq",
        "num=s,gen=f",
        "8",
        "CN",
        "-",
        "10",
        "y",
        "y",
        "c",
        "cc",
        "_",
        "2",
        "CTE",
        "_",
        "11",
        "pone",
        "poner",
        "v",
        "vm",
        "num=s,per=3,mod=i,tmp=p",
        "10",
        "CDO",
        "_",
        "12",
        "de",
        "de",
        "s",
        "sp",
        "for=s",
        "11",
        "CC",
        "ARGST",
        "13",
        "manifiesto",
        "manifiesto",
        "n",
        "nc",
        "gen=m,num=s",
        "12",
        "_",
        "-",
        "14",
        "que",
        "que",
        "c",
        "cs",
        "_",
        "18",
        "_",
        "_",
        "15",
        "las",
        "el",
        "d",
        "da",
        "gen=f,num=p",
        "16",
        "ESP",
        "-",
        "16",
        "empresas",
        "empresa",
        "n",
        "nc",
        "gen=f,num=p",
        "18",
        "SUJ",
        "ARG1",
        "17",
        "se",
        "el",
        "p",
        "p0",
        "per=3",
        "18",
        "_",
        "_",
        "18",
        "centran",
        "centrar",
        "v",
        "vm",
        "num=p,per=3,mod=i,tmp=p",
        "11",
        "CD",
        "ARG1",
        "19",
        "mas",
        "mas",
        "r",
        "rg",
        "_",
        "20",
        "CC",
        "ARG_EXT",
        "20",
        "en",
        "en",
        "s",
        "sp",
        "for=s",
        "18",
        "CREG",
        "ARG_LOC",
        "21",
        "la",
        "el",
        "d",
        "da",
        "num=s,gen=f",
        "22",
        "ESP",
        "-",
        "22",
        "I+D",
        "I+D",
        "n",
        "np",
        "_",
        "20",
        "_",
        "_",
        "23",
        "con",
        "con",
        "s",
        "sp",
        "for=s",
        "18",
        "CC",
        "ARG_PRP",
        "24",
        "objetivos",
        "objetivo",
        "n",
        "nc",
        "gen=m,num=p",
        "23",
        "_",
        "-",
        "25",
        "de",
        "de",
        "s",
        "sp",
        "for=s",
        "24",
        "CN",
        "_",
        "26",
        "mercado",
        "mercado",
        "n",
        "nc",
        "gen=m,num=s",
        "25",
        "_",
        "_",
        "27",
        "F",
        "Fp",
        "-",
        "2",
        "PUNC",
        "-",
        "introduced by the subordinate conjunction si.",
        "• ARGCONS is assigned to complements of the verb that express a consequence.",
        "• ARGDEST is used with events in which an entity moves along a path, physically or metaphorically.",
        "ARGJDEST is assigned to complements that express final end of the path, or destination.",
        "It can also express direction, or the entity into which something is converted with predicates that express change of state.",
        "• ARGEXT is assigned to complements that express the frequency or intensity with which something happens.",
        "It is usually assigned to adverbial constituents.",
        "• ARGINSTR is assigned to complements that refer to an entity used as instrument to perform the action, literally or metaphorically.",
        "It is usually assigned to constituents introduced by prepositions con or gracias a.",
        "• ARGLOC is assigned to complements of the predicate that refer to the location where the event happens.",
        "• ARGMNR is assigned to constituents that express the manner in which an event happens.",
        "• ARGMEANS is assigned to constituents that express the means by which an event happens.",
        "• ARGOP is assigned to constituents that express a term of comparison, an entity, event, or property that is compared or opposed to the main entity, event, or property.",
        "• Like ARGJDEST, ARGOR is used with events in which an entity moves along a path, physically or metaphorically.",
        "It is assigned to complements that express the origin of the path.",
        "It can also refer to the location from which the event is performed.",
        "• ARGPRED is assigned to predicative complements.",
        "• ARGPRP is assigned to constituents that express the purpose or goal of an event.",
        "It is usually introduced by prepositions para, a, con elfin de, a fin de, etc.",
        "• ARGQUANT is assigned to constituents that express quantity.",
        "• ARGRES is assigned to constituents that express the result of an event.",
        "• ARGSRC is assigned to the constituents that express the source of one of the entities involved in the event.",
        "It is not related to events that express a path.",
        "• ARGST is assigned to constituents that express the state in which an entity affected by the event is.",
        "• ARGTMP is assigned to constituents that express temporal aspects of an event.",
        "• ARGTOP is assigned to constituents that express what the event is about, on what does the event focus, or what is the topic of the event.",
        "and 2 verbs appear more than 800 times: ser, 1348 times and haber, 812 times.",
        "Table 1 shows an example sentence of the corpus.",
        "Like in the CoNLL Shared Task 2006, sentences are separated by a blank line and fields are separated by a single tab character.",
        "A sentence consists of tokens, each one starting on a new line.",
        "A token consists of the following 8 fields that contain information about morphosyntactic features and non-projective dependencies:",
        "1.",
        "ID: token counter, starting at 1 for each new sentence.",
        "2.",
        "FORM: word form or punctuation symbol.",
        "3.",
        "LEMMA: lemma of word form.",
        "4.",
        "CPOSTAG: coarse-grained part-of-speech tag.",
        "5.",
        "POSTAG: fine-grained part-of-speech tag.",
        "6.",
        "FEATS: unordered set of syntactic and/or morphological features, separated by a vertical bar.",
        "If features are not available, the value of the feature is an underscore.",
        "The complete description of the CPOSTAG, POSTAG, and FEATS tags can be found in (Civit, 2002).",
        "7.",
        "HEAD: head of the current token, which is either a value of ID or zero ('0') for the sentence root.",
        "8.",
        "DEPREL: dependency relation to the HEAD.",
        "The set of tags is described in (Morante, 2006).",
        "Semantic role labeling systems for English are usually trained on the PropBank corpus (Palmer et al., 2005).",
        "In the PropBank project, a layer of predicate-argument information was added to the Penn Treebank.",
        "The verb semantic arguments are numbered beginning by 0.",
        "Arg0 is the prototypical Agent and Arg1 is the prototypical Patient.",
        "For the higher numbers, as (Palmer et al., 2005, p.4) indicate, it is not possible to generalize across verbs.",
        "Sentence (a) in Example 2 would be annotated as in 2b.",
        "(2) a.",
        "He wouldn't accept anything of value from those he was writing about.",
        "The Cast3LB-CoNLL-SemRol corpus is annotated differently because the goal of the annotation is to capture generalisations of the semantic relation that holds between a predicate and its complements across verbs in order to check if the systems can learn these generalisations.",
        "A difference with PropBank is that we do not make a distinction between arguments subcategorised by the verb and adjuncts.",
        "All complements are annotated with the same set of roles.",
        "Another difference with PropBank is that what in PropBank are arguments with a number higher than 1 (Arg2, Arg3, etc) are notional semantic roles without a number in the Cast3LB-CoNLL-SemRol corpus.",
        "The distinction between Arg0 and Arg1 is kept.",
        "For example, A2 would be replaced by ARGOR in Example 2b, whereas",
        "ARG2-at would be replaced by ARGiOC in Example 3, capturing the fact that ARGiOC and ARGOR are different semantic notions.",
        "This distinction cannot be made if the A2 label is used.",
        "(3) He said that if [ARG1 little cocoa] [ARGM-MNR actually] has [rel arrived] at [ARG2-at the ports] , shipping delays could result.",
        "The corpus Cast3LB-CoNLL-SemRol is similar to the corpora of Spanish that were used in Task 9 of SemEval 2007 (Marquez et al., 2007).",
        "The main difference is that Cast3LB-CoNLL-SemRol is annotated with dependency syntax whereas the Task 9 corpora are annotated with constituent syntax."
      ]
    },
    {
      "heading": "3.. Ensemble of dependency parsers",
      "text": [
        "The corpus Cast3LB-CoNLL has been used to train an ensemble of dependency parsers that integrates three parsers: the MaltParser (Nivre, 2006), a parser based on constraint satisfaction (Canisius and Tjong Kim Sang, 2007), and a memory-based parser (Morante, 2008).",
        "Like in (Sagae and Lavie, 2006), the ensemble that we present works in two stages.",
        "In the first stage, each of the three parsers analyzes an input sentence and produces a syntactic structure.",
        "The unlabeled attachment scores in this stage range from 82 to 86 %, according to the evaluation metrics used in the CoNLL Shared Task 2006.",
        "In the second stage, a voting algorithm is applied that takes into account the results of the parsers in the first stage in order to provide a final solution.",
        "The MaltParser 0.4 (Nivre, 2006; Nivre et al., 2006) is an inductive dependency parser that uses four essential components: a deterministic algorithm for building labeled projective dependency graphs; history-based feature models for predicting the next parser action; support vector machines for mapping histories to parser actions; and graph transformations for recovering non-projective structures.",
        "For our experiments we trained the parser using the support vector machines algorithm (LIBSVM (Chang and Lin, 2005)), with the same parameter options used by (Nivre et al., 2006) in the CoNLL Shared Task 2006.",
        "The parser algorithm used was Nivre, with the options arc order eager, shift before reduce and allow reduction of unattached tokens.",
        "The memory-based constraint satisfaction parser (Canisius and Tjong Kim Sang, 2007) uses three memory-based classifiers that predict weighted soft-constraints on the structure of the parse tree.",
        "Each predicted constraint covers a small part of the complete dependency tree, and overlap between them ensures that global output structure is taken into account.",
        "A dynamic programming algorithm for dependency parsing is used to find the optimal solution to the constraint satisfaction problem thus obtained.",
        "The memory-based single classifier parser (Morante, 2008) consists of a single classifier that predicts the relation between two words in a sentence, and a decision heuristics that chooses among the dependency relations that the classifier has predicted for one word, based on information from the classifier output.",
        "The global results of the three parsers are shown in Table 2 in terms of Labeled Attachment Score (LAS), Unlabeled Attachment Score (UAS), and Label Accuracy (LAc) according to the evaluation metrics used in the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006).",
        "The MP performs significantly better than MB1 and MB2, whereas MB1 and MB2 perform similarly in spite of the fact that their approach to memory-based learning is different: MB1 applies constraint satisfaction, and MB2 is based on only one classifier and heuristics that rely on the distance of the predicted class to the nearest neighbor and on the class distribution.",
        "The ensemble system operates in two stages.",
        "In the first stage, each of the three parsers analyzes an input sentence and produces a dependency graph.",
        "The results of the individual parsers were presented in Table 2 in the previous section.",
        "In the second stage, a voting system distills a final dependency graph out of the three first-stage dependency graphs.",
        "Voting techniques have been previously applied to dependency parsing (Sagae and Lavie, 2006; Zeman and •Zabokrtskyy, 2005).",
        "We provide results of three different voting systems, that take into account agreement among classifiers and/or the normalized F1 value of each classifier for each dependency relation:",
        "• VS1: the system votes for the solution of the single classifier that has the higher F1 for the dependency relation that the single classifier predicts.",
        "• VS2: the system votes for the solution of the MP, unless MB1 and MB2 agree, in which case the MB1 and MB2 solution is chosen.",
        "• VS3: the system votes for the solution of the MP, unless MB1 and MB2 agree or the three parsers disagree.",
        "In the first case, the MB1 and MB2 solution is chosen, and in the second, the system votes for the solution of the single classifier that has the higher F1 for the syntactic function that the single classifier predicts.",
        "MP MB1 MB2",
        "LAS UAS LAc",
        "80.45 % 75.74 % 75.44 % 87.42 % 82.44 % 82.75 % 85.12% 81.95% 81.35%",
        "Table 3: LAS, UAS, and LAc of the different versions of the ensemble of parsers compared to the MaltParser.",
        "• VS4: the system votes for system VS1 unless two single systems agree.",
        "In this case, the system votes for the solution agreed by them.",
        "The results of the different versions of the ensemble system are presented in Table 3 as well as the improvement over the MP.",
        "Results show that combined systems VS1, VS2 and VS3 perform better than the best parser, although the difference is insignificant, since it reduces the error of MP in less than 5% (4.44%).",
        "Combined system VS4 improves only in accuracy over the results of the best system.",
        "VS1 is the system that improves the least because the MP has the better F1 scores for 19 of the 25 dependency relations.",
        "That VS2 and VS3 do no improve significantly might be due to the fact that some agreement cases between MB1 and MB2 can be errors.",
        "VS3 is the voting system that performs better: by voting for the agreement between MB1 and MB2, or for the system with higher F1 in case of complete disagreement, more errors are eliminated than errors are introduced.",
        "For further research it would be interesting to analyze if it is possible to eliminate more errors by introducing specific voting strategies per dependency relation.",
        "The related work we are aware of deals with languages other than Spanish.",
        "(Zeman and Zabokrtsky, 2005) tested several approaches for combining dependency parsers for Czech.",
        "They found that the best method was accuracy-aware voting, which reduced the error of the best parser in 13%.",
        "Differences between their approach an ours are that they experiment with seven parsers, they perform stacking, and they check that the resulting structure is a well-formed tree.",
        "(Sagae and Lavie, 2006) experiment with six parsers on the Wall Street Journal corpus.",
        "They apply a two stage procedure of reparsing focusing on unlabeled dependencies.",
        "In the first stage, m different parsers analyze an input sentence.",
        "In the second stage, a parsing algorithm is applied taking into account the analysis produced by each parser in the first stage.",
        "They reparse the sentence based on the output of m parsers in order to maximize the number of votes for a well-formed dependency structure.",
        "Their experiments increase the accuracy of the best parser in 1.7%.",
        "(Nivre et al., 2007) combined the outputs of the parsers participating in the CoNLL Shared Task 2007 on dependency parsing using the method of (Sagae and Lavie, 2006).",
        "They show that accuracy never falls below the performance of the top three systems, although it degrades after ten different parsers have been added."
      ]
    },
    {
      "heading": "4.. Semantic role labelers",
      "text": [
        "Two semantic role labelers have been trained on the Cast3LB-CoNLL-SemRol corpus.",
        "The only difference between the systems is that one uses information from gold standard syntax, whereas the other one uses information from predicted syntax.",
        "The engine of the two semantic role labelers is a memory-based classifier.",
        "Memory-based language processing (Daelemans and van den Bosch, 2005) is based on the idea that NLP problems can be solved by storing annotated examples of the problem in their literal form in memory, and applying similarity-based reasoning on these examples in order to solve new ones.",
        "The system based on gold standard syntax solves the task in three phases:",
        "1.",
        "A preprocessing phase that consists of identifying the potential candidates to be assigned a semantic role or a semantic verb class.",
        "The system starts by detecting a target verb and the clause boundaries in order to look for the siblings of the verb that exist within the same clause.",
        "These tokens will be the focal elements of the examples in each training set.",
        "For a sentence like the one presented in Table 1 the focal elements would be the ones in Table 4.",
        "Each of the rows in the table would be an instance, with its corresponding features.",
        "2.",
        "A classification phase, i.e. the actual assignment of roles and verb classes.",
        "We use the IB1 classifier as implemented in TiMBL (version 6.0) (Daelemans et al., 2007), a supervised inductive algorithm for learning classification tasks based on the k-nearest neighbor classification rule (Cover and Hart, 1967).",
        "In IB1, similarity is defined by a feature-level distance metric between a test instance and a memorized example.",
        "The metric combines a per-feature value distance metric with global feature weights that account for relative differences in discriminative power of the features.",
        "The IB1 algorithm is parameterized by using Jeffrey Divergence as the similarity metric, gain ratio for feature weighting, using 11 k-nearest neighbors, and weighting the class vote of neighbors as a function of their inverse linear distance.",
        "VS1",
        "dif.MP",
        "VS2",
        "dif.MP",
        "VS3",
        "dif.MP",
        "VS4",
        "dif.MP",
        "LAS",
        "S0.53%",
        "+0.0S",
        "Sl.04%",
        "+0.59",
        "Sl.09%",
        "+0.64",
        "79.71%",
        "-0.74",
        "UAS",
        "S7.43%",
        "+0.01",
        "S7.6S%",
        "+0.26",
        "S7.6S%",
        "+0.26",
        "S6.07%",
        "-1.35",
        "LAc",
        "S5.22%",
        "+0.10",
        "S5.71%",
        "+0.59",
        "S5.7S%",
        "+0.66",
        "S5.92%",
        "+0.S0",
        "Focal element",
        "Verb",
        "asimismo",
        "defiende",
        "financiacin",
        "defiende",
        "manifiesto",
        "pone",
        "centran",
        "pone",
        "empresas",
        "centran",
        "se",
        "centran",
        "ms",
        "centran",
        "I+D",
        "centran",
        "objetivos",
        "centran",
        "3.",
        "A postprocessing phase, in which the predictions of some semantic roles are corrected by taking into consideration all the predictions in the clause.",
        "For example, if the system has predicted two ARG0 for the same predicate, one of the predictions is modified.",
        "There are three groups of features: about the sibling in focus, about the verb, and about the clause.",
        "• Sibling in focus (26): content word, content word lemma, gender and number; is content word a named entity?",
        "a temporal adverb?",
        "a locative adverb?",
        "; POS of the two previous words to the content word and of the three next words; POS and lemma of the three first words of the sibling; preposition; POS and POS type of the head; syntactic function; relative position to the verb; relative position to the verb of the next sibling; string with the POS of all words in the sibling; string with all nouns, adjectives, adverbs and verbs in the sibling.",
        "• Verb (12): distance to the sibling in focus; word, lemma, POS type, two previous and next words; concordance in gender with the content word of the sibling; is the verb causative?",
        "pronominal?",
        "passive?",
        "• Clause (12): number of constituents with function CC (adverbial complement); number of constituents; relative position to the verb of siblings with syntactic function SUJ, CD, CAG, CI, ATR, CPRED.CD,",
        "CPRED.SUJ, CREG; string with the POS of the head of all siblings; string with the syntactic function of all siblings.",
        "We divided the corpus in train, development and test sets.",
        "Feature selection was performed by starting with a set of basic features (essentially the identity and the parts-of-speech tags of the head words involved, in their local context) and gradually adding new features.",
        "Table 5 shows the results of the system for the test set.",
        "It achieves 0.86 F1, 0.88 precision and 0.84 recall.",
        "The system performs very well with very frequent roles (ARG0, ARG1) and with roles that have morphosyntactic markers (ARG_ATR, ARGJPRED, ARGJBEN).",
        "With unfrequent roles performance is variable (ARGCOMP 0.77 versus ARGOP 0.33).",
        "The system performs less well with some roles that are relatively frequent, that do not have syntactic markers and that might encode a diversity of semantic concepts (ARGJMNR).",
        "We computed the effect of removing groups of features.",
        "Removing the features with information about the sibling",
        "Table 5: Results of the SRL-GS system per role.",
        "in focus causes a decrease of 12.12 in F1; removing the features about the verb, a decrease of 1.31; the features about the clause, a decrease of 0.97, and the features about the content word, a decrease of 0.64.",
        "So the features about the sibling in focus seem to be the most informative ones.",
        "The system based on predicted syntax is exactly the same as the previous system, except for the fact that it uses information from syntax predicted with the MaltParser as presented in Subsection 3.1.1..",
        "The results of this system are presented in Table 6: 0.75 F1, 0.82 precision, and 0.68 recall.",
        "Compared to the SRL-GS system, performance drops 0.11 points, mainly due to a decrease in recall (-0.16).",
        "Precision decreases in 0.6 points.",
        "Performance decreases around 0.10 with the most frequent roles (ARG0, ARG1).",
        "It does not decrease with two roles that have very clear morphosyntactic markers (ARG_ATR, ARGJPRED), and it decreases considerably (-0.24) with one ARGJBEN).",
        "The decrease with unfrequent roles is irregular (from nothing to 0.51).",
        "Removing the features with information about the sibling in focus causes a decrease of 6.17 in F1; removing the features about the verb, a decrease of 2.23; the features about the clause, a decrease of 0.95, and about the content word, a decrease of 0.76.",
        "So, compared to the SRL-GS system, the decrease caused by removing the features about the sibling",
        "Semantic Role",
        "Total",
        "Precision",
        "Recall",
        "Fi",
        "ARGCONC",
        "9",
        "0.83",
        "0.55",
        "0.66",
        "ARGTMP",
        "192",
        "0.80",
        "0.86",
        "0.83",
        "ARGJMEANS",
        "2",
        "0.00",
        "0.00",
        "0.00",
        "ARGJ3EN",
        "78",
        "0.86",
        "0.88",
        "0.87",
        "ARGOP",
        "4",
        "0.50",
        "0.25",
        "0.33",
        "ARGTOP",
        "23",
        "0.63",
        "0.22",
        "0.32",
        "ARGJDEST",
        "36",
        "0.60",
        "0.66",
        "0.63",
        "ARG0",
        "285",
        "0.91",
        "0.93",
        "0.92",
        "ARGCONS",
        "6",
        "1.00",
        "0.17",
        "0.28",
        "ARGM",
        "53",
        "0.50",
        "0.23",
        "0.31",
        "ARGOR",
        "21",
        "0.92",
        "0.52",
        "0.67",
        "ARGCOND",
        "9",
        "1.00",
        "0.55",
        "0.71",
        "ARGST",
        "5",
        "1.00",
        "0.20",
        "0.33",
        "ARGXOC",
        "124",
        "0.91",
        "0.78",
        "0.84",
        "ARGCAU",
        "34",
        "0.67",
        "0.85",
        "0.75",
        "ARGCOMP",
        "14",
        "0.83",
        "0.71",
        "0.77",
        "ARG^ATR",
        "140",
        "0.98",
        "1.00",
        "0.99",
        "ARGJRES",
        "1",
        "0.00",
        "0.00",
        "0.00",
        "ARGJ>RP",
        "33",
        "0.79",
        "0.69",
        "0.74",
        "ARGJPRED",
        "45",
        "0.91",
        "0.91",
        "0.91",
        "ARGSRC",
        "2",
        "0.50",
        "0.50",
        "0.50",
        "ARGJEXT",
        "22",
        "0.92",
        "0.54",
        "0.68",
        "ARG1",
        "735",
        "0.94",
        "0.93",
        "0.94",
        "ARGJNSTR",
        "14",
        "0.80",
        "0.28",
        "0.42",
        "ARGJMNR",
        "88",
        "0.65",
        "0.62",
        "0.63",
        "Overall",
        "1975",
        "0.88",
        "0.84",
        "0.86",
        "Table 6: Results ofthe SRL-PS system per role compared to the results ofthe SRL-GS system.",
        "in focus is lower.",
        "This can be explained by the errors in the syntactic tree that the SRL-PS system has as input.",
        "The results of the SRL-GS system are comparable to the results of the existing semantic role labeling systems for Spanish that use information from gold standard constituent syntax: the highest score of the systems participating in Task 9 of SemEval 2007 (Marquez et al., 2007) was 0.84; (Morante and van den Bosch, 2007) report a maximum Fi of 0.85 with a memory-based system very similar to SRL-GS.",
        "(Surdeanu et al., 2008) report 0.86 with a combined system that implements joint learning strategies.",
        "Because the corpus used and the annotation is not exactly the same as in the mentioned systems, the results are not completely comparable.",
        "However, they give an indication that the performance of systems that use information from dependency syntax is similar to that of the systems that use information from constituent syntax.",
        "(Hacioglu, 2004) describes a system for English that achieves 0.84 Fi using information from dependency syntax converted from constituent syntax.",
        "We cannot compare the results of the SRL-PS system because we are not aware ofsemantic role labelers ofSpanish that use predicted syntax."
      ]
    },
    {
      "heading": "5.. Conclusions",
      "text": [
        "In this paper we presented the Cast3LB-CoNLL-SemRol corpus, currently the only corpus of Spanish annotated with dependency syntax and semantic roles, and the tools that have been trained on the corpus: an ensemble of parsers and two dependency-based semantic role labelers that are the only semantic role labelers based on dependency syntax available for Spanish at this moment.",
        "The results of the ensemble of parsers are only slightly better than the results of the best parser; the error reduction of the label accuracy score reaches 4.44%.",
        "This is due to the fact that there are only three parsers, one of which performs clearly better than the other two, which perform very similarly.",
        "The best results were obtained by the voting system that gives priority to the decisions of the best parser, unless the other two parsers agree, in which case their solution is chosen, or the three parsers disagree, in which case the system votes for the solution of the single classifier that has the higher F1 for the dependency relation that the single classifier predicts.",
        "We consider the results to be promising enough to continue our research.",
        "The results of the semantic role labelers shows that the performance of the system that uses information from predicted syntax decreases in 0.11 compared to the performance of the system that uses gold standard syntax.",
        "In both systems the features from the sibling in focus are very informative, though the effect of removing these features is lower in the system based on predicted syntax.",
        "The results of the system based on gold standard dependency syntax are similar to the results obtained by existing systems based on gold standard constituent syntax.",
        "We conclude that syntactic information increases performance ofa semantic role labeler regardless of the type of syntax used (constituent or dependency).",
        "However, if a system uses information from predicted syntax, other features have to be found in order to compensate for errors in the syntactic tree.",
        "Using the same features provokes a considerable decrease in performance.",
        "Further research will focus on improving the system based on predicted syntax by incorporating an ensemble of parsers, instead of a single parser, and on engineering different features that are more robust to syntactic errors."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This research has been funded by the grant EX2005-1145 awarded by the Spanish Ministerio de Education y Ciencia to the project Tecnicas semiautomaticas para el etiquetado de roles semanticos en corpus del espanol.",
        "Semantic Role",
        "Total",
        "Precision",
        "Recall",
        "Fi",
        "dif",
        "ARGCONC",
        "9",
        "1.00",
        "0.11",
        "0.20",
        "-0.46",
        "ARGTMP",
        "192",
        "0.S2",
        "0.67",
        "0.74",
        "-0.09",
        "ARGJMEANS",
        "2",
        "0.00",
        "0.00",
        "0.00",
        "0.00",
        "ARGJ3EN",
        "7S",
        "0.77",
        "0.53",
        "0.63",
        "-0.24",
        "ARGOP",
        "4",
        "1",
        "0.25",
        "0.40",
        "+0.09",
        "ARGTOP",
        "23",
        "0.71",
        "0.21",
        "0.33",
        "+0.01",
        "ARGJDEST",
        "36",
        "0.44",
        "0.30",
        "0.36",
        "-0.27",
        "ARG0",
        "2S5",
        "0.S1",
        "0.S3",
        "0.S2",
        "-0.10",
        "ARGCONS",
        "6",
        "0.25",
        "0.16",
        "0.20",
        "-0.0S",
        "ARGM",
        "53",
        "0.25",
        "0.07",
        "0.11",
        "-0.20",
        "ARGOR",
        "21",
        "1.00",
        "0.23",
        "0.3S",
        "-0.29",
        "ARGCOND",
        "9",
        "1.00",
        "0.11",
        "0.20",
        "-0.51",
        "ARGST",
        "5",
        "0.00",
        "0.00",
        "0.00",
        "-0.33",
        "ARGXOC",
        "124",
        "0.70",
        "0.70",
        "0.70",
        "-0.14",
        "ARGCAU",
        "34",
        "0.60",
        "0.50",
        "0.54",
        "-0.21",
        "ARGCOMP",
        "14",
        "0.33",
        "0.21",
        "0.26",
        "-0.51",
        "ARG^ATR",
        "140",
        "0.9S",
        "1.00",
        "0.99",
        "0.00",
        "ARGJRES",
        "1",
        "0.00",
        "0.00",
        "0.00",
        "0.00",
        "ARGJPRP",
        "33",
        "0.71",
        "0.30",
        "0.42",
        "-0.32",
        "ARGJ>RED",
        "45",
        "0.S6",
        "0.SS",
        "0.S7",
        "0.04",
        "ARGSRC",
        "2",
        "0.50",
        "0.50",
        "0.50",
        "0.00",
        "ARGJEXT",
        "22",
        "0.S1",
        "0.40",
        "0.54",
        "-0.14",
        "ARG1",
        "735",
        "0.S7",
        "0.7S",
        "0.S2",
        "-0.12",
        "ARGJNSTR",
        "14",
        "0.S5",
        "0.42",
        "0.57",
        "+0.15",
        "ARGJMNR",
        "SS",
        "0.76",
        "0.36",
        "0.49",
        "-0.14",
        "Overall",
        "1975",
        "0.S2",
        "0.6S",
        "0.75",
        "-0.11"
      ]
    }
  ]
}
