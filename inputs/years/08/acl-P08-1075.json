{
  "info": {
    "authors": [
      "Xiaojin Zhu",
      "Andrew B. Goldberg",
      "Michael Rabbat",
      "Robert Nowak"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P08-1075",
    "title": "Learning Bigrams from Unigrams",
    "url": "https://aclweb.org/anthology/P08-1075",
    "year": 2008
  },
  "references": [
    "acl-D07-1090"
  ],
  "sections": [
    {
      "text": [
        "Xiaojin Zhut and Andrew B. Goldberg* and Michael Rabbat* and Robert Nowak§",
        "^Department of Computer Sciences, University of Wisconsin-Madison ^Department of Electrical and Computer Engineering, McGill University § Department of Electrical and Computer Engineering, University of Wisconsin-Madison",
        "Traditional wisdom holds that once documents are turned into bag-of-words (unigram count) vectors, word orders are completely lost.",
        "We introduce an approach that, perhaps surprisingly, is able to learn a bigram language model from a set of bag-of-words documents.",
        "At its heart, our approach is an EM algorithm that seeks a model which maximizes the regularized marginal likelihood of the bag-of-words documents.",
        "In experiments on seven corpora, we observed that our learned bigram language models: i) achieve better test set perplexity than unigram models trained on the same bag-of-words documents, and are not far behind \"oracle bigram models\" trained on the corresponding ordered documents; ii) assign higher probabilities to sensible bigram word pairs; iii) improve the accuracy of ordered-document recovery from a bag-of-words.",
        "Our approach opens the door to novel phenomena, for example, privacy leakage from index files."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "A bag-of-words (BOW) is a basic document representation in natural language processing.",
        "In this paper, we consider a BOW in its simplest form, i.e., a unigram count vector or word histogram over the vocabulary.",
        "When performing the counting, word order is ignored.",
        "For example, the phrases \"really neat\" and \"neat really\" contribute equally to a BOW.",
        "Obviously, once a set of documents is turned into a set of BOWs, the word order information within them is completely lost – or is it?",
        "In this paper, we show that one can in fact partly recover the order information.",
        "Specifically, given a set of documents in unigram-count BOW representation, one can recover a non-trivial bigram language model (LM), which has part of the power of a bigram LM trained on ordered documents.",
        "At first glance this seems impossible: How can one learn bigram information from unigram counts?",
        "However, we will demonstrate that multiple BOW documents enable us to recover some higher-order information.",
        "Our results have implications in a wide range of natural language problems, in particular document privacy.",
        "With the wide adoption of natural language applications like desktop search engines, software programs are increasingly indexing computer users' personal files for fast processing.",
        "Most index files include some variant of the BOW.",
        "As we demonstrate in this paper, if a malicious party gains access to BOW index files, it can recover more than just unigram frequencies: (i) the malicious party can recover a higher-order LM; (ii) with the LM it may attempt to recover the original ordered document from a BOW by finding the most-likely word permutation.",
        "Future research will quantify the extent to which such a privacy breach is possible in theory, and will find solutions to prevent it.",
        "'A trivial bigram LM is a unigram LM which ignores history: P (v\\u) = P (v).",
        "ever, to the best of our knowledge, none addresses this reverse direction of learning higher-order LMs from lower-order data.",
        "This work is inspired by recent advances in inferring network structure from co-occurrence data, for example, for computer networks and biological pathways (Rabbat et al., 2007)."
      ]
    },
    {
      "heading": "2. Problem Formulation and Identifiability",
      "text": [
        "We assume that a vocabulary of size W is given.",
        "For notational convenience, we include in the vocabulary a special \"begin-of-document\" symbol (d) which appears only at the beginning of each document.",
        "The training corpus consists of a collection of n BOW documents {xi,..., xnj.",
        "Each BOW xj is a vector ..., xiW) where xiu is the number of times word u occurs in document i.",
        "Our goal is to learn a bigram LM 9, represented as a W x W transition matrix with 9uv = P(v|u), from the BOW corpus.",
        "Note P(v|(d)) corresponds to the initial state probability for word v, and P((d) |u) = 0, Vu.",
        "It is worth noting that traditionally one needs ordered documents to learn a bigram LM.",
        "A natural question that arises in our problem is whether or not a bigram LM can be recovered from the BOW corpus with any guarantee.",
        "Let X denote the space of all possible BOWs.",
        "As a toy example, consider W = 3 with the vocabulary {(d), A, B}.",
        "Assuming all documents have equal length |x| = 4 (including (d)),then X = {((d):1, A:3, B:0), ((d):1, A:2, B:1), ((d):1, A:1, B:2), ((d):1, A:0, B:3)}.",
        "Our training BOW corpus, when sufficiently large, provides the marginal distribution p(x) for x G X.",
        "Can we recover a bigram LM from p(x)?",
        "To answer this question, we first need to introduce a generative model for the BOWs.",
        "We assume that the BOW corpus is generated from a bigram LM 9 in two steps: (i) An ordered document is generated from the bigram LM 9; (ii) The document's unigram counts are collected to produce the BOW x.",
        "Therefore, the probability of a BOW x being generated by 9 can be computed by marginalizing over unique orderings z of x:",
        "where <r(x) is the set of unique orderings, and |x| is the document length.",
        "For example, if x =((d):1,",
        "+(1 – r)(l – q)p = 0.3725 (1 – r)q = 0.1875.",
        "The above system has only one valid solution, which is the correct set of bigram LM parameters (r, p, q) = (0.25, 0.9, 0.5).",
        "However, if the true parameters were (r, p, q) = (0.1, 0.2, 0.3) with proportions of BOWs being 0.4%, 19.8%, 8.1%, respectively, it is easy to verify that the system would have multiple valid solutions: (0.1, 0.2, 0.3), (0.8819, 0.0673, 0.8283), and (0.1180, 0.1841, 0.3030).",
        "In general, if p(x) is known from the training BOW corpus, when can we guarantee to uniquely recover the bigram LM 9?",
        "This is the question of identifiability, which means the transition matrix 9 satisfying (1) exists and is unique.",
        "Identifiability is related to finding unique solutions of a system of polynomial equations since (1) is such a system in the elements of 9.",
        "The details are beyond the scope of this paper, but applying the technique in (Basu and Boston, 2000), it is possible to show that for W = 3 (including (d)) we need longer documents (|x| > 5) to ensure identifiability.",
        "The identifiability of more general cases is still an open research question."
      ]
    },
    {
      "heading": "3. Bigram Recovery Algorithm",
      "text": [
        "In practice, the documents are not truly generated from a bigram LM, and the BOW corpus may be small.",
        "We therefore seek a maximum likelihood estimate of 9 or a regularized version of it.",
        "Equivalently, we no longer require equality in (1), but instead find 9 that makes the distribution P(x|9) as close to p(x) as possible.",
        "We formalize this notion below.",
        "Given a BOW corpus {x1,...,xn}, its normalized log likelihood under 9 is 1(9) = u£?=1 logP(xj|9), where C = £n=1(N – 1) is the corpus length excluding ( d) 's.",
        "The idea is to find 9 that maximizes 1(9).",
        "This also brings P(x|9) closest to p(x) in the KL-divergence sense.",
        "However, to prevent overfitting, we regularize the problem so that 9 prefers to be close to a \"prior\" bigram LM 0.",
        "The prior 0 is also estimated from the BOW corpus, and is discussed in Section 3.4.",
        "We define the regularizer to be an asymmetric dissimilarity D(0,9) between the prior 0 and the learned model 9.",
        "The dissimilarity is 0 if 9 = 0, and increases as they diverge.",
        "Specifically, the KL-divergence between two word distributions conditioned on the same history u is KL(0u.||9u.)",
        "= Y^W=l </>uv log .",
        "We define D(0,9) to be the average KL-divergence over all histories: D(0, 9) = W £KL(0u.||9u.",
        "), which is convex in 9 (Cover and Thomas, 1991).",
        "We will use the following derivative later: dD(0,9)/d9uv =",
        " – </>uv/(W9uv ).",
        "We are now ready to define the regularized optimization problem for recovering a bigram LM 9 from the BOW corpus:",
        "The weight A controls the strength of the prior.",
        "The constraints ensure that 9 is a valid bigram matrix, where 1 is an all-one vector, and the non-negativity constraint is element-wise.",
        "Equivalently, (2) can be viewed as the maximum a posteriori (MAP) estimate of 9, with independent Dirichlet priors for each row of 9: p(9u) = Dir(9u|au) and hyperparameters",
        "The summation over hidden ordered documents z in P(x|9) couples the variables and makes (2) a non-concave problem.",
        "We optimize 9 using an EM algorithm.",
        "We derive the EM algorithm for the optimization problem (2).",
        "Let O(9) = 1(9) – AD(0,9) be the objective function.",
        "Let 9(t-1) be the bigram LM at iteration t – 1.",
        "We can lower-bound O as follows:",
        "= L(9,9(t-1)).",
        "We used Jensen's inequality above since log() is concave.",
        "The lower bound L involves P(z|9(t-1), x), the probability of hidden orderings of the BOW under the previous iteration's model.",
        "In the E-step of EM we compute P(z|9(t-1), x), which will be discussed in Section 3.3.",
        "One can verify that L(9, 9(t-1)) is concave in 9, unlike the original objective O(9).",
        "In addition, the lower bound \"touches\" the objective at 9(t-1), i.e., L(9(t-1), 9(t-1)) = O(9(t-1)).",
        "The EM algorithm iteratively maximizes the lower bound, which is now a concave optimization problem: max9 L(9,9(t-1)), subject to 91 = 1.",
        "The non-negativity constraints turn out to be automatically satisfied.",
        "Introducing Lagrange multipliers //u for each history u = 1... W, we form the Lagrangian A:",
        "w / w \\ A = L(9, 9(t-1)) – £ A, PT 9uv – 1 .",
        "Taking the partial derivative with respect to 9uv and setting it to zero: dA/d9uv = 0, we arrive at the following update:",
        "9uv Yl P(z|9(*-1), x)Cuv(z) + \"W^uv.",
        "Input: BOW documents {xi,..., x„}, a prior bigram LM 0, weight A.",
        "1. t =1.",
        "Initialize 0(o) = 0."
      ]
    },
    {
      "heading": "2.. Repeat until the objective 0(0) converges:",
      "text": [
        "(a) (E-step) Compute P(z|0(t-i), x) for z e a(xj),i = 1,.",
        ".. ,n.",
        "(b) (M-step) Compute 0(t) using (3).",
        "Let t = t + 1.",
        "Output: The recovered bigram LM 0.",
        "The normalization is over v = 1... W. We use cuv (z) to denote the number of times the bigram \"uv\" appears in the ordered document z.",
        "This is the M-step of EM.",
        "Intuitively, the first term counts how often the bigram \"uv\" occurs, weighing each ordering by its probability under the previous model; the second term pulls the parameter towards the prior.",
        "If the weight of the prior A – to, we would have 0uv = </>uv.",
        "The update is related to the MAP estimate for a multinomial distribution with a Dirichlet prior, where we use the expected counts.",
        "We initialize the EM algorithm with 9(0) = 0.",
        "The EM algorithm is summarized in Table 1.",
        "The E-step needs to compute the expected bigram counts of the form",
        "However, this poses a computational problem.",
        "The summation is over unique ordered documents.",
        "The number of unique ordered documents can be on the order of |x|!, i.e., all permutations of the BOW.",
        "For a short document of length 15, this number is already 10.",
        "Clearly, brute-force enumeration is only feasible for very short documents.",
        "Approximation is necessary to handle longer ones.",
        "A simple Monte Carlo approximation to (4) would involve sampling ordered documents z1,z2,...,zL according to zi ~ P(z|9,x), and replacing (4) with ^L=1 cuv(z^/L.",
        "This estimate is unbiased, and the variance decreases linearly with the number of samples, L. However, sampling directly from P is difficult.",
        "Instead, we sample ordered documents zi ~ R(zi|0, x) from a distribution R which is easy to generate, and construct an approximation using importance sampling (see, e.g., (Liu, 2001)).",
        "With each sample, z , we associate a weight wi oc P(zi\\9, x)/R(zi|^, x).",
        "The importance sampling approximation to (4) is then given by (£L=1 WiCuv(zi))/(£f=1 Wi).",
        "Re-weighting the samples in this fashion accounts for the fact that we are using a sampling distribution R which is different the target distribution P, and guarantees that our approximation is asymptotically unbiased.",
        "The quality of an importance sampling approximation is closely related to how closely R resembles P; the more similar they are, the better the approximation, in general.",
        "Given a BOW x and our current bigram model estimate, 9, we generate one sample (an ordered document zi) by sequentially drawing words from the bag, with probabilities proportional to 9, but properly normalized to form a distribution based on which words remain in the bag.",
        "For example, suppose x = ((d):1, A:2, B:1, C:1).",
        "Then we set zi1 = (d), and sample zi2 = A with probability ^(d)A/(2^(d)A + 0<d)B + ^(d)0).",
        "Similarly,if zi(j-1) = u and if v is in the original BOW that hasn't been sampled yet, then we set the next word in the ordered document zij equal to v with probability proportional to cv 9uv, where cv is the count of v in the remaining BOW.",
        "For this scheme, one can verify (Rabbat et al., 2007) that the importance weight corresponding to a sampled ordered document zi = (zi1,..., ziixi) is given by wi ^ n l^^!^ i=|t ezt-izi.",
        "In our implementation, the number of importance samples used for a document x is 10|x| if the length of the document |x| > 8; otherwise we enumerate <r(x) without importance sampling.",
        "The quality of the EM solution 9 can depend on the prior bigram LM 0.",
        "To assess bigram recoverability from a BOW corpus alone, we consider only priors estimated from the corpus itself.",
        "Like 9, 0 is a W x W transition matrix with (/>uv = P (v|u).",
        "When appropriate, we set the initial probability (/>(d)v proportional to the number of times word v appears in the BOW corpus.",
        "We consider three prior models: Prior 1: Unigram 0unigram.",
        "The most naive 0 is a unigram LM which ignores word history.",
        "The probability for word v is estimated from the BOW corpus frequency of v, with add-1 smoothing: ^wmgram ^ 1 + ^£iv.",
        "We should point out that the unigram prior is an asymmetric bigram, i.e.,"
      ]
    },
    {
      "heading": "1. unigram = / unigram",
      "text": [
        "Prior 2: Frequency of Document Cooccurrence (FDC) 0fdc.",
        "Let £(u, v|x) = 1 if words u = v co-occur (regardless of their counts) in BOW x, and 0 otherwise.",
        "In the case u = v, £(u, u|x) = 1 only if u appears at least twice in x.",
        "Let cJud<c = £n=1 #(u, v|xi) be the number of BOWs in which u, v co-occur.",
        "The FDC prior is 0u^c oc cJud<c + 1.",
        "The co-occurrence counts cfdcare symmetric, but 0fdc is asymmetric because of normalization.",
        "FDC captures some notion of potential transitions from u to v. FDC is in spirit similar to Kneser-Ney smoothing (Kneser and Ney, 1995) and other methods that accumulate indicators of document membership.",
        "Prior 3: Permutation-Based (Perm) 0perm.",
        "Recall that cuv (z) is the number of times the bigram \"uv\" appears in an ordered document z.",
        "We define cuVrm = £n=1 Ez€CT(xi) [cuv(z)], where the expectation is with respect to all unique orderings of each BOW.",
        "We make the zero-knowledge assumption of uniform probability over these orderings, rather than P(z|9) as in the EM algorithm described above.",
        "EM will refine these estimates, though, so this is a natural starting point.",
        "Space precludes a full discussion, but it can be proven that cuVrm = £n=1 xiuxiv /|xi| if u = v, and cC\"\" = £n=1 £iu(£iu - 1)/|xi|.",
        "Finally, (pW; oc cuv +1.",
        "Given a BOW x and a bigram LM 9, we formulate document recovery as the problem z* = argmaxz€(J(x)P(z|9).",
        "In fact, we can generate the top N candidate ordered documents in terms of P(z|9).",
        "We use A* search to construct such an N-best list (Russell and Norvig, 2003).",
        "Each state is an ordered, partial document.",
        "Its successor states append one more unused word in x to the partial document.",
        "The actual cost g from the start (empty document) to a state is the log probability of the partial document under bigram 9.",
        "We design a heuristic cost h from the state to the goal (complete document) that is admissible: the idea is to overuse the best bigram history for the remaining words in x.",
        "Let the partial document end with word we.",
        "Let the count vector for the remaining BOW be (c1,..., cW).",
        "One admissible heuristic is h = logl!W=1 P(u|bh(u); 9)cu, where the \"best history\" for word type u is bh(u) = argmaxv#vu, and v ranges over the word types with non-zero counts in (c1,...,cw), plus we.",
        "It is easy to see that h is an upper bound on the bigram log probability that the remaining words in x can achieve.",
        "We use a memory-bounded A* search similar to (Russell, 1992), because long BOWs would otherwise quickly exhaust memory.",
        "When the priority queue grows larger than the bound, the worst states (in terms of g + h) in the queue are purged.",
        "This necessitates a double-ended priority queue that can pop either the maximum or minimum item.",
        "We use an efficient implementation with Splay trees (Chong and Sahni, 2000).",
        "We continue running A* after popping the goal state from its priority queue.",
        "Repeating this N times gives the N-best list."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We show experimentally that the proposed algorithm is indeed able to recover reasonable bigram LMs from BOW corpora.",
        "We observe:",
        "1.",
        "Good test set perplexity: Using test (held-out) set perplexity (PP) as an objective measure of LM quality, we demonstrate that our recovered bigram LMs are much better than naive unigram LMs trained on the same BOW corpus.",
        "Furthermore, they are not far behind the \"oracle\" bigram LMs trained on ordered documents that correspond to the BOWs.",
        "2.",
        "Sensible bigram pairs: We inspect the recovered bigram LMs and find that they assign higher probabilities to sensible bigram pairs (e.g., \"i mean\", \"oh boy\", \"that's funny\"), and lower probabilities to nonsense pairs (e.g., \"i yep\", \"you let's\", \"right lot\").",
        "3.",
        "Document recovery from BOW: With the bigram LMs, we show improved accuracy in recovering ordered documents from BOWs.",
        "We describe these experiments in detail below.",
        "Table 2: Corpora statistics: vocabulary size, document count, total token count, and mean document length.",
        "We note that although in principle our algorithm works on large corpora, the current implementation does not scale well (Table 3 last column).",
        "We therefore experimented on seven corpora with relatively small vocabulary sizes, and with short documents (mostly one sentence per document).",
        "Table 2 lists statistics describing the corpora.",
        "The first six contain text transcripts of conversational telephone speech from the small vocabulary \"SVitch-board 1\" data set.",
        "King et al.",
        "constructed each corpus from the full Switchboard corpus, with the restriction that the sentences use only words in the corresponding vocabulary (King et al., 2005).",
        "We refer to these corpora as SV10, SV25, SV50, SV100, SV250, and SV500.",
        "The seventh corpus comes from the SumTime-Meteo data set (Sripada et al., 2003), which contains real weather forecasts for offshore oil rigs in the North Sea.",
        "For the SumTime corpus, we performed sentence segmentation to produce documents, removed punctuation, and replaced numeric digits with a special token.",
        "For each of the seven corpora, we perform 5-fold cross validation.",
        "We use four folds other than the k-th fold as the training set to train (recover) bigram LMs, and the k-th fold as the test set for evaluation.",
        "This is repeated for k = 1... 5, and we report the average cross validation results.",
        "We distinguish the original ordered documents (training set zi,... zn, test set zn+1,..., zm) and the corresponding BOWs (training set x1... xn, test set xn+1 ... xm).",
        "In all experiments, we simply set the weight A = 1 in (2).",
        "Given a training set and a test set, we perform the following steps:",
        "1.",
        "Build prior LMs 0X from the training BOW corpus x1,... for X = unigram, fdc, perm."
      ]
    },
    {
      "heading": "2.. Recover the bigram LMs 9 X with the EM al-",
      "text": [
        "gorithm in Table 1, from the training BOW corpus x1,... xn and using the prior from step 1.",
        "3.",
        "Compute the MAP bigram LM from the ordered training documents z1,... zn.",
        "We call this the \"oracle\" bigram LM because it uses order information (not available to our algorithm), and we use it as a lower-bound on perplexity.",
        "4.",
        "Test all LMs on zn+1,..., zm by perplexity.",
        "Table 3 reports the 5-fold cross validation mean-test-set-PP values for all corpora, and the run time per EM iteration.",
        "Because of the long running time, we adopt the rule-of-thumb stopping criterion of \"two EM iterations\".",
        "First, we observe that all bigram LMs perform better than unigram LMs 0umgrameven though they are trained on the same BOW corpus.",
        "Second, all recovered bigram LMs 6X improved upon their corresponding baselines 4>X.",
        "The difference across every row is statistically significant according to a two-tailed paired t-test with p < 0.05.",
        "The differences among PP(9X) for the same corpus are also significant (except between Qumgram and eperm for SV500).",
        "Finally, we observe that 6permtends to be best for the smaller vocabulary corpora, whereas 9fdc dominates as the vocabulary grows.",
        "To see how much better we could do if we had ordered training documents z1 , .",
        ".",
        ".",
        ", zn, we present the mean-test-set-PP of \"oracle\" bigram LMs in Table 4.",
        "We used three smoothing methods to obtain oracle LMs: absolute discounting using a constant of 0.5 (we experimented with other values, but 0.5 worked best), Good-Turing, and interpolated Witten-Bell as implemented in the SRILM toolkit (Stolcke, 2002).",
        "We see that our recovered LMs (trained on unordered BOW documents), especially for small vocabulary corpora, are close to the oracles (trained on ordered documents).",
        "For the larger datasets, the recovery task is more difficult, and the gap between the oracle LMs and the 9 LMs widens.",
        "Note that the oracle LMs do much better than the recovered LMs on the SumTime corpus; we suspect the difference is due to the larger vocabulary and significantly higher average sentence length (see Table 2).",
        "The next set of experiments compares the recovered bigram LMs to their corresponding prior LMs in terms of how they assign probabilities to word pairs.",
        "One naturally expects probabilities for frequently occurring bigrams to increase, while rare or nonsensical bigrams' probabilities should decrease.",
        "For a prior-bigram pair (0, 9), we evaluate the change in probabilities by computing the ratio phw = IShl) = .",
        "For a given history h we sort words w by this ratio rather than by actual bigram probability because the bigrams with the highest and lowest probabilities tend to stay the same, while the changes accounting for differences in PP scores are more noticeable by considering the ratio.",
        "Corpus",
        "\\v |",
        "#Docs",
        "# Tokens",
        "\\x\\",
        "SV10",
        "10",
        "6775",
        "7792",
        "1.2",
        "SV25",
        "25",
        "9778",
        "13324",
        "1.4",
        "SV50",
        "50",
        "12442",
        "20914",
        "1.7",
        "SV100",
        "100",
        "14602",
        "28611",
        "2.0",
        "SV250",
        "250",
        "18933",
        "51950",
        "2.7",
        "SV500",
        "500",
        "23669",
        "89413",
        "3.8",
        "SumTime",
        "882",
        "3341",
        "68815",
        "20.6",
        "Due to space limitation, we present one specific result (FDC prior, fold 1) for the SV500 corpus in Table 5.",
        "Other results are similar.",
        "The table lists a few most frequent unigrams as history words h (left), and the words w with the smallest (center) and largest (right) phw ratio.",
        "Overall we see that our EM algorithm is forcing meaningless bigrams (e.g., \"i goodness\", \"oh thing\") to have lower probabilities, while assigning higher probabilities to sensible bigram pairs (e.g., \"really good\", \"that's funny\").",
        "Note that the reverse of some common expressions (e.g., \"right that's\") also rise in probability, suggesting the algorithm detects that the two words are of-",
        "Table 4: Mean test set perplexities for oracle bigram LMs trained on z1;..., zn and tested on zn+1,..., zm.",
        "For reference, the rightmost column lists the best result using a recovered bigram LM (QPerm for the first three corpora, 0fdc for the latter four).",
        "ten adjacent, but lacks sufficient information to nail down the exact order.",
        "We now play the role of the malicious party mentioned in the introduction.",
        "We show that, compared to their corresponding prior LMs, our recovered bigram LMs are better able to reconstruct ordered documents out of test BOWs xn+1,..., xm.",
        "We perform document recovery using 1-best A* decoding.",
        "We use \"document accuracy\" and \"n-gram accuracy\" (for n = 2, 3) as our evaluation criteria.",
        "We define document accuracy (Accdoc) as the fraction of documents for which the decoded document matches the true ordered document exactly.",
        "Similarly, n-gram accuracy (Accn) measures the fraction of all n-grams in test documents (with n or more words) that are recovered correctly.",
        "For this evaluation, we compare models built for the SV500 corpus.",
        "Table 6 presents 5-fold cross validation average test-set accuracies.",
        "For each accuracy measure, we compare the prior LM with the recovered bigram LM.",
        "It is interesting to note that the FDC and Perm priors reconstruct documents surprisingly well, but we can always improve them by running our EM algorithm.",
        "The accuracies obtained by 9 are statistically significantly better (via two-tailed paired t-tests with p < 0.05) than their corresponding priors 0 in all cases except Accdoc for 9Perm versus 0perm.",
        "Furthermore, 9fdc and 9permare significantly better than all other models in terms of all three reconstruction accuracy measures.",
        "Corpus",
        "X",
        "PP((X )",
        "PP(OX )",
        "Time/",
        "Iter",
        "SVlO",
        "unigram",
        "fdc",
        "perm",
        "7.48 6.52 6.50",
        "6.95 6.47 6.45",
        "< 1s",
        "< 1s",
        "< 1s",
        "SV25",
        "unigram",
        "fdc",
        "perm",
        "l6.4 l2.3 l2.2",
        "l2.8 ll.8 ll.7",
        "O.ls O.ls O.ls",
        "SV50",
        "unigram",
        "fdc",
        "perm",
        "29.l l9.6 l9.5",
        "l9.7 l7.8 l7.7",
        "2s 4s 5s",
        "SVlOO",
        "unigram",
        "fdc",
        "perm",
        "45.4 29.5 30.0",
        "27.8 25.3 25.6",
        "7s lls lls",
        "SV250",
        "unigram",
        "fdc",
        "perm",
        "9l.8 60.0 65.4",
        "5l.2 47.3 49.7",
        "5m",
        "8m 8m",
        "SV5OO",
        "unigram",
        "fdc",
        "perm",
        "l49.l l04.8 l23.9",
        "87.2 8O.l 87.4",
        "3h 3h 3h",
        "SumTime",
        "unigram",
        "fdc",
        "perm",
        "l29.7 l03.2 l87.9",
        "8l.8 77.7 85.4",
        "4h 4h 3h",
        "Corpus",
        "Absolute Discount",
        "Good-Turing",
        "WittenBell",
        "O*",
        "SVlO",
        "6.27",
        "6.28",
        "6.27",
        "6.45",
        "SV25",
        "l0.5",
        "l0.6",
        "l0.5",
        "ll.7",
        "SV50",
        "l4.8",
        "l4.9",
        "l4.8",
        "l7.7",
        "SVlOO",
        "20.0",
        "2O.l",
        "20.0",
        "25.3",
        "SV250",
        "33.7",
        "33.7",
        "33.8",
        "47.3",
        "SV500",
        "50.9",
        "50.9",
        "5l.3",
        "8O.l",
        "SumTime",
        "l0.8",
        "l0.5",
        "l0.6",
        "77.7",
        "Table 5: The recovered bigram LM 6fdc decreases nonsense bigram probabilities (center column) and increases sensible ones (right column) compared to the prior 4>fdc on the SV500 corpus.",
        "Table 7: Subset of SV500 documents that only 0perm or 8perm (but not both) reconstructs correctly.",
        "The correct reconstructions are in bold.",
        "Table 6: Percentage of correctly reconstructed documents, 2-grams and 3-grams from test BOWs in SV500, 5-fold cross validation.",
        "The same trends continue for 4-grams and 5-grams (not shown).",
        "We conclude our experiments with a closer look at some BOWs for which << and 6 reconstruct differently.",
        "As a representative example, we compare 6Perm to ^perm on one test set of the SV500 corpus.",
        "There are 92 documents that are correctly reconstructed by 6perm but not by <0Perm.",
        "In contrast, only 65 documents are accurately reordered by <perm but not by 6perm.",
        "Table 7 presents a subset of these documents with six or more words.",
        "Overall, we conclude that the recovered bigram LMs do a better job at reconstructing BOW documents."
      ]
    },
    {
      "heading": "5. Conclusions and Future Work",
      "text": [
        "We presented an algorithm that learns bigram language models from BOWs.",
        "We plan to: i) investigate ways to speed up our algorithm; ii) extend it to trigram and higher-order models; iii) handle the mixture of BOW documents and some ordered documents (or phrases) when available; iv) adapt a general English LM to a special domain using only BOWs from that domain; and v) explore novel applications of our algorithm."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank Ben Liblit for tips on doubled-ended priority queues, and the anonymous reviewers for valuable comments.",
        "This work is supported in part by the Wisconsin Alumni Research Foundation, NSF CCF-0353079 and CCF-0728767, and the Natural Sciences and Engineering Research Council (NSERC) of Canada.",
        "h",
        "w (smallest phw)",
        "w (largest phw)",
        "i",
        "yep, bye-bye, ah, goodness, ahead",
        "mean, guess, think, bet, agree",
        "you",
        "let's, us, fact, such, deal",
        "thank, bet, know, can, do",
        "right",
        "as, lot, going, years, were",
        "that's, all, right, now, you're",
        "oh",
        "thing, here, could, were, doing",
        "boy, really, absolutely, gosh, great",
        "that's",
        "talking, home, haven't, than, care",
        "funny, wonderful, true, interesting, amazing",
        "really",
        "now, more, yep, work, you're",
        "sad, neat, not, good, it's",
        "</>Perm reconstructions of test BOWs",
        "gperm reconstructions of test BOWs",
        "just it's it's it's just going",
        "it's probably out there else something",
        "the the have but it doesn't",
        "you to talking nice was it yes",
        "that's well that's what i'm saying",
        "a little more here home take",
        "and they can very be nice too",
        "i think well that's great i'm",
        "but was he because only always",
        "that's think i don't i no",
        "that in and it it's interesting",
        "that's right that's right that's difficult",
        "so just not quite a year",
        "well it is a big dog",
        "so do you have a car",
        "it's just it's just it's going",
        "it's probably something else out there",
        "but it doesn't have the the",
        "yes it was nice talking to you",
        "well that's that's what i'm saying",
        "a little more take home here",
        "and they can be very nice too",
        "well i think that's great i'm",
        "but only because he was always",
        "no i don't i think that's",
        "and it it's interesting that in",
        "right that's that's right that's difficult",
        "so just not a quite year",
        "well it is big a dog",
        "so you do have a car",
        "1 Accdoc",
        "1 Acc",
        "1 Acc",
        "X",
        "OX",
        "OX",
        "4>X",
        "OX",
        "unigram",
        "11.1",
        "26.8",
        "17.7",
        "32.",
        "8",
        "2.7",
        "11.8",
        "fdc",
        "30.2",
        "31.0",
        "33.0",
        "3S.1",
        "11.4",
        "13.3",
        "perm",
        "30.9",
        "31.S",
        "32.7",
        "34.8",
        "ll.S",
        "13.1"
      ]
    }
  ]
}
