{
  "info": {
    "authors": [
      "Tim Paek",
      "Sudeep Gandhe",
      "Max Chickering"
    ],
    "book": "Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue",
    "id": "acl-W08-0108",
    "title": "Rapidly Deploying Grammar-Based Speech Applications with Active Learning and Back-off Grammars",
    "url": "https://aclweb.org/anthology/W08-0108",
    "year": 2008
  },
  "references": [
    "acl-W07-1805"
  ],
  "sections": [
    {
      "text": [
        "Tim Paek, Sudeep Gandhe, David Maxwel Chickering",
        "Grammar-based approaches to spoken language understanding are utilized to a great extent in industry, particularly when developers are confronted with data sparsity.",
        "In order to ensure wide grammar coverage, developers typically modify their grammars in an iterative process of deploying the application, collecting and transcribing user utterances, and adjusting the grammar.",
        "In this paper, we explore enhancing this iterative process by leveraging active learning with back-off grammars.",
        "Because the back-off grammars expand coverage of user utterances, developers have a safety net for deploying applications earlier.",
        "Furthermore, the statistics related to the back-off can be used for active learning, thus reducing the effort and cost of data transcription.",
        "In experiments conducted on a commercially deployed application, the approach achieved levels of semantic accuracy comparable to transcribing all failed utterances with 87% less transcriptions."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Although research in spoken language understanding is typically pursued from a statistical perspective, grammar-based approaches are utilized to a great extent in industry (Knight et al., 2001).",
        "Speech recognition grammars are often manually authored and iteratively modified as follows: Typically, context-free grammars (CFG) are written in a format such as Speech Recognition Grammar",
        "Specification (SRGS) (W3C, 2004) and deployed.",
        "Once user utterances are collected and transcribed, the grammars are then adjusted to improve their coverage.",
        "This process continues until minimal",
        "OOG utterances are observed.",
        "In this paper, we explore enhancing this iterative process of grammar modification by combining back-off grammars, which expand coverage of user utterances, with active learning, which reduces \"the number of training examples to be labeled by automatically processing unlabeled examples, and then selecting the most informative ones with respect to a specified cost function for a human to label\" (Hakkani-Tur et al., 2002).",
        "This paper comprises three sections.",
        "In Section 2, we describe our overall approach to rapid application development (RAD).",
        "In Section 3, we explain how data transcription can be reduced by leveraging active learning based on statistics related to the usage of back-off grammars.",
        "Finally, in Section 4, we evaluate the active learning approach with simulation experiments conducted on data collected from a commercial grammar-based speech application."
      ]
    },
    {
      "heading": "2. RAD Approach & Related Work",
      "text": [
        "Working under the assumption that developers in industry will continue to use CFGs for rapid application development, our approach to grammar modification is as follows:",
        "1.",
        "Create a CFG (either manually or automatically).",
        "1.1 Generate a back-off grammar from the CFG.",
        "2.",
        "Deploy the application.",
        "2.1 Use the back-off grammar for OOG utterances.",
        "3.",
        "Gather data from users.",
        "4.",
        "Selectively transcribe data by using statistics related to the back-off for active learning; i.e., transcribe only those utterances that satisfy the active learning criterion.",
        "5.",
        "Modify CFG either manually or automatically and go to step 1.1.",
        "To begin with, developers start with a CFG in Step 1.",
        "If they had access to a grammatical platform such as Regulus (Rayner et al., 2006), they could in principle construct a CFG automatically for any new domain, though most developers will probably manually author the grammar.",
        "Two steps are added to the typical iterative process.",
        "In Step 1.1, we generate a back-off grammar from the CFG.",
        "One way to accomplish this is by constructing a backoff CFG using filler models (Paek et al., 2007), which when applied to the same command-and-control task in Section 4 can result in a 35% relative reduction in semantic error rate for OOG utterances.",
        "However, the back-off grammar could also be a SLM trained on artificial data created from the CFG (Galescu et al., 1998).",
        "Whatever back-off mechanism is employed, its coverage should be wider than the original CFG so that utterances that fail to be recognized by the CFG, or fall below an acceptable confidence threshold, can be handled by the back-off in a second or simultaneous pass.",
        "That is the gist of Step 2.1, the second additional step.",
        "It is not only important to generate a back-off grammar, but it must be utilized for handling possible OOG utterances.",
        "Our approach attempts to reduce the usual cost associated with grammar modification after the application has been deployed and data collected in Step 4.",
        "The idea is simple: Exploit the fast and accurate CFG recognition of in-grammar (ING) utterances by making OOG utterances handled by the back-off grammar ING.",
        "In other words, expand CFG coverage to include whatever gets handled by the back-off grammar.",
        "This idea is very complementary with a two-pass recognition approach where the goal is to get utterances correctly recognized by a CFG on the first pass so as to minimize computational expenses (Paek et al., 2007).",
        "All of this can be accomplished with reduced transcription effort by keeping track of and leveraging back-off statistics for active learning.",
        "If the back-off is a CFG, we keep track of statistics related to which CFG rules were utilized the most, whether they allowed the task to be successfully completed, etc.",
        "If the back-off is a SLM, we keep track of similar statistics related to the semantic alignment and mapping in spoken language understanding.",
        "Given an active learning criterion, these statistics can be used to selectively transcribe utterances which can then be used to modify the CFG in Step 5 so that OOG utterances become ING.",
        "Section 3 covers this in more detail.",
        "Finally, in Step 5, the CFG grammar is modified using the selectively transcribed utterances.",
        "Although developers will probably want to do this manually, it is possible to automate much of this step by making grammar changes with minimal edit distance or Levenshtein distance.",
        "Leveraging a wider coverage back-off grammar is of course not new.",
        "For grammar-based applications, several researchers have investigated using a CFG along with a back-off grammar either simultaneously via a domain-trained SLM (Gorrell et al., 2002), or in two-pass recognition using either an SLM trained on CFG data (Gorrell, 2003) or a dictation n-gram (Dusan & Flanagan, 2002).",
        "To our knowledge however, no prior research has considered leveraging statistics related to the back-off grammar for active learning, especially as part of a RAD approach."
      ]
    },
    {
      "heading": "3. Active Learning",
      "text": [
        "Our overall approach utilizes back-off grammars to provide developers with a safety net for deploying applications earlier, and active learning to reduce transcription effort and cost.",
        "We now elaborate on active learning, demonstrate the concept with respect to a CFG back-off.",
        "Active learning aims at reducing transcription of training examples by selecting utterances that are most likely to be informative according to a specified cost function (Hakkani-Tur et al., 2002).",
        "In the speech community, active learning has been successfully applied to reducing the transcription effort for ASR (Hakkani-Tur et al., 2002), SLU (Tur et al., 2003b), as well as finding labeling errors (Tur et al., 2003).",
        "In our case, the examples are user utterances that need to be transcribed, and the learning involves modifying a CFG to achieve wider coverage of user expressions.",
        "Instead of passively transcribing everything and modifying the CFG as such, the grammar can \"actively\" participate in which utterances are transcribed.",
        "The usual procedure for selecting utterances for grammar modification is to transcribe at least all failed utterances, such as those that fall below a rejection threshold.",
        "By leveraging a back-off grammar, developers have more information with which to select utterances for transcription.",
        "For a CFG back-off, how frequently a back-off rule fired can serve as an active learning criterion because that is where OOG utterances are handled.",
        "Given this active learning criterion, the algorithm would proceed as follows (where i denotes iteration, St denotes the set of transcribed utterances, and Su denotes the set of all utterances):",
        "[1] Modify CFGi using St and generate corresponding back-offi from the CFGi.",
        "[2] Recognize utterances in set Su using CFGi + backoffi.",
        "[3] Compute statistics on what back-off rules fired when and how frequently.",
        "[4] Select the k utterances that were handled by the most frequently occurring back-off rule and transcribe them.",
        "Call the new transcribed set as Si.",
        "[6] Stop when CFGi achieves a desired level of semantic accuracy, or alternatively when back-off rules only handle a desired percentage of Su, otherwise go to Step 1.",
        "Note that the set Su grows with each iteration and follows as a result of deploying an application with a CFGi + back-offi.",
        "Step [1] corresponds to Step 5, 1.1, and 2.1 of our approach.",
        "Steps [2-4] above constitute the active learning criterion and can be adjusted depending on what developers want to optimize.",
        "This algorithm currently assumes that runtime efficiency is the main objective (e.g., on a mobile device); hence, it is critical to move utterances recognized in the second pass to the first pass.",
        "If developers are more interested in learning new semantics, in Step [4] above they could transcribe utterances that failed in the back-off.",
        "With an active learning criterion in place, Step [6] provides a stopping criterion.",
        "This too can be adjusted, and may even target budgetary objectives."
      ]
    },
    {
      "heading": "4. Evaluation",
      "text": [
        "For evaluation, we used utterances collected from 204 users of Microsoft Voice Command, a grammar-based command-and-control (C&C) application for high-end mobile devices (see Paek et al., 2007 for details).",
        "We partitioned 5061 transcribed utterances into five sets, one of which was used exclusively for testing.",
        "The remaining four were used for iterative CFG modification.",
        "For the first iteration, we started with a CFG which was a degraded version of the grammar currently shipped with the Voice Command product.",
        "It was obtained by using the mode, or the most frequent user utterance, for each CFG rule.",
        "We compared two approaches: CFGFull, where each iterative CFG was modified using the full set of transcribed utterances that resulted in a failure state (i.e., when a false recognition event occurred or the phrase confidence score fell below 45%, which was set by a proprietary tuning procedure for optimizing word-error rate), and CFGActive, where each iterative CFG was modified using only those transcribed utterances corresponding to the most frequently occurring CFG back-off rules.",
        "For both CFGFull and CFG Active, CFGi was modified using the same set of heuristics akin to minimal edit distance.",
        "In order to assess the value of using the back-off grammar as a safety net, we also compared CFG_Full+Back-off, where a derived CFG back-off was utilized whenever a failure state occurred with CFG Full, and CFG_Active+Back-off, where again a CFG back-off was utilized, this time with the back-off derived from the CFG trained on selective utterances.",
        "As our metric, we evaluated semantic accuracy since that is what matters most in C&C settings.",
        "Furthermore, because recognition of part of an utterance can increase the odds of ultimately achieving task completion (Paek et al., 2007), we carried out separate evaluations for the functional constituents of a C&C utterance (i.e., keyword and slot) as well as the complete phrase (keyword + slot).",
        "We computed accuracy as follows: For any single utterance, the recognizer can either accept or reject it.",
        "If it is accepted, then the semantics of the utterance can either be correct (i.e., it matches what the user intended) or incorrect, hence:",
        "where CA denotes accepted commands that are correct, IA denotes accepted commands that are incorrect, and R denotes the number of rejections.",
        "Table 2 displays semantic accuracies for both CFG Full and CFG Active.",
        "Standard errors about the mean were computed using the jacknife procedure with 10 re-samples.",
        "Notice that both CFG Full and CFG Active initially have the same accuracy levels because they start off with the same degraded CFG.",
        "The highest accuracies obtained almost always occurred in the second iteration after modifying the CFG with the first batch of transcriptions.",
        "Thereafter, all accuracies seem to decrease.",
        "In order to understand why this would be case, we computed the coverage of the ith CFG on the holdout set.",
        "This is reported in the 'OOG%' column.",
        "Comparing CFG Full to CFG Active on",
        "Table 2.",
        "Semantic accuracies for partial (keyword or slot) and full phrase recognitions (keyword + slot) using a CFG trained on either \"Full\" or \"Active\" transcriptions (i.e., selective transcriptions based on active learning).",
        "Parentheses indicate standard error about the mean.",
        "The 'i' column represents iteration.",
        "The 'Utterances Transcribed' column is cumulative.",
        "The 'OOG%' column represents coverage of the ith CFG on the holdout set.",
        "Rows containing \"Back-off' evaluate 2-pass recognition using both the CFG and a derived CFG back-off.",
        "keyword + slot accuracy, CFG Full decreases in accuracy after the second iteration as does CFG Active.",
        "However, the OOG% of CFG Full is much lower than CFG Active.",
        "In fact, it seems to level off after the second iteration, suggesting that perhaps the decrease in accuracies reflects the increase in grammar perplexity; that is, as the grammar covers more of the utterances, it has more hypotheses to consider, and as a result, performs slightly worse.",
        "Interestingly, after the last iteration, CFG Active for keyword + slot and slot accuracies was slightly higher (69.G6%) than CFG Full (66.88%) (p = .GS).",
        "Furthermore, this was done with 193 utterances as opposed to 1393, or 87% less transcriptions.",
        "For keyword accuracy, CFG Active (64.G9%) was slightly worse than CFG Full (66.1G%) (p < .GS).",
        "With respect to the value of having a back-off grammar as a safety net, we found that both CFG Full and CFG Active achieved much higher accuracies with the back-off for keyword, slot, and keyword + slot accuracies.",
        "Notice also that the differences between CFG Full and CFG Active after the last iteration were much closer to each other than without the back-off, suggesting applications should always be deployed with a back-off.",
        "Conclusion",
        "In this paper, we explored enhancing the usual iterative process of grammar modification by leveraging active learning with back-off grammars.",
        "Because the back-off grammars expand coverage of user utterances to handle OOG occurrences, developers have a safety net for deploying applications earlier.",
        "Furthermore, because statistics related to the back-off can be used for active learning, developers can reduce the effort and cost of data transcription.",
        "In our simulation experiments, leveraging active learning achieved levels of semantic accuracy comparable to transcribing all failed utterances with 87% less transcriptions.",
        "Approach",
        "i",
        "Utterances Transcribed",
        "Keyword Accuracy",
        "Slot Accuracy",
        "Keyword + Slot Accuracy",
        "Processing Time (ms)",
        "OOG%",
        "1",
        "G",
        "SG.2S% (G.13%)",
        "46.84% (G.22%)",
        "46.84% (G.22%)",
        "387 (3.9GGS)",
        "61.1G%",
        "CFG_Full",
        "2",
        "S9G",
        "66.20% (0.12%)",
        "71.02% (0.23%)",
        "70.59% (0.23%)",
        "4G1 (4.GS86)",
        "31.92%",
        "3",
        "1GGG",
        "6S.8G% (G.^/o)",
        "69.72% (G.19%)",
        "69.G6% (G.19%)",
        "422 (4^8G4)",
        "31.3G%",
        "4",
        "1393",
        "66.1G% (G.13%)",
        "67.S4% (G.22%)",
        "66.88% (G.21%)",
        "433 (4.7G61)",
        "3G.9S%",
        "1",
        "G",
        "66.7G% (G.1G%)",
        "66.23% (G.22%)",
        "66.G1% (G.22%)",
        "631 (11.132G)",
        "61.1G%",
        "CFG Full +",
        "2",
        "S9G",
        "73.32% (0.11%)",
        "72.11% (0.22%)",
        "71.68% (0.23%)",
        "S62 (1G.4696)",
        "31.92%",
        "Back-off",
        "3",
        "1GGG",
        "72.S2% (G.12%)",
        "72.11% (G.21%)",
        "71.46% (G.22%)",
        "S84 (1G.498S)",
        "31.3G%",
        "4",
        "1393",
        "73.G2% (G.1G%)",
        "71.G2% (G.23%)",
        "7G.37% (G.23%)",
        "S92 (1G.68GS)",
        "3G.9S%",
        "1",
        "G",
        "5G.25% (G.13%)",
        "46.84% (G.22%)",
        "46.84% (G.22%)",
        "387 (3.9GGS)",
        "61.1G%",
        "CFG_Active",
        "2",
        "87",
        "64.G9% (G.13%)",
        "74.29% (0.21%)",
        "74.07% (0.22%)",
        "39S (4.1469)",
        "42.G9%",
        "3",
        "138",
        "64.29% (0.15%)",
        "7G.1S% (G.22%)",
        "69.SG% (G.24%)",
        "4G9 (4.337S)",
        "38.G2%",
        "4",
        "193",
        "64.G9% (G.1S%)",
        "69.72% (G.23%)",
        "69.G6% (G.24%)",
        "413 (4.4G1S)",
        "37.93%",
        "1",
        "G",
        "66.7G% (G.1G%)",
        "66.23% (G.22%)",
        "66.G1% (G.22%)",
        "631 (11.132G)",
        "61.1G%",
        "CFG Active",
        "2",
        "87",
        "72.52% (0.10%)",
        "76.91% (0.19%)",
        "76.47% (0.21%)",
        "S68 (1G.3494)",
        "42.G9%",
        "+ Back-off",
        "3",
        "138",
        "71.72% (G.14%)",
        "71.9G% (G.24%)",
        "71.24% (G.27%)",
        "S81 (1G.633G)",
        "38.G2%",
        "4",
        "193",
        "71.21% (G.1S%)",
        "71.9G% (G.2S%)",
        "71.24% (G.26%)",
        "S8G (1GJ266)",
        "37.93%"
      ]
    }
  ]
}
