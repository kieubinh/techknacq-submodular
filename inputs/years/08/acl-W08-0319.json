{
  "info": {
    "authors": [
      "Ondřej Bojar",
      "Jan Hajič"
    ],
    "book": "Proceedings of the Third Workshop on Statistical Machine Translation",
    "id": "acl-W08-0319",
    "title": "Phrase-Based and Deep Syntactic English-to-Czech Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/W08-0319",
    "year": 2008
  },
  "references": [
    "acl-C00-2163",
    "acl-D07-1091",
    "acl-H05-1066",
    "acl-J92-4003",
    "acl-N06-1032",
    "acl-P03-1021",
    "acl-P05-1034",
    "acl-P05-1067",
    "acl-P07-2045",
    "acl-W07-0735"
  ],
  "sections": [
    {
      "text": [
        "Phrase-Based and Deep Syntactic English-to-Czech Statistical Machine Translation *",
        "Ondfej Bojar and Jan Hajic",
        "Institute of Formal and Applied Linguistics UFAL MFF UK, Malostranske namesti 25",
        "CZ-11800 Praha, Czech Republic {boj ar,haj ic}@ufal.mff.cuni.cz",
        "This paper describes our two contributions to WMT08 shared task: factored phrase-based model using Moses and a probabilistic tree-transfer model at a deep syntactic layer."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Czech is a Slavic language with very rich morphology and relatively free word order.",
        "The Czech morphological system (Hajic, 2004) defines 4,000 tags in theory and 2,000 were actually seen in a big tagged corpus while the English Penn Treebank tagset contains just about 50 tags.",
        "In our parallel corpus (see below), the English vocabulary size is 148k distinct word forms but more than twice as big in Czech, 343k distinct word forms.",
        "When translating to Czech from an analytic language such as English, target word forms have to be chosen correctly to produce a grammatical sentence and preserve the expressed relations between elements in the sentence, e.g. verbs and their modifiers.",
        "This year, we have taken two radically different approaches to English-to-Czech MT.",
        "Section 2 describes our setup of the phrase-based system Moses (Koehn et al., 2007) and Section 3 focuses on a system with probabilistic tree transfer employed at a deep syntactic layer and the new challenges this approach brings.",
        "*The work on this project was supported by the grants FP6-IST-5-034291-STP (EuroMatrix), MSM0021620838, MSMT CR LC536, and GA405/06/0589."
      ]
    },
    {
      "heading": "2. Factored Phrase-Based MT to Czech",
      "text": [
        "Bojar (2007) describes various experiments with factored translation to Czech aimed at improving target-side morphology.",
        "We use essentially the same setup with some cleanup and significantly larger target-side training data:",
        "Parallel data from CzEng 0.7 (Bojar et al., 2008), with original sentence-level alignment and tokeniza-tion.",
        "The parallel corpus was taken as a monolithic text source disregarding differences between CzEng data sources.",
        "We use only 1-1 aligned sentences.",
        "Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses.",
        "We based the word alignment on Czech and English lemmas (base forms of words) as provided by the combination of taggers and lemmatizers by Hajic (2004) for Czech and",
        "English.",
        "We symmetrized the two GIZA++ runs using grow-diag-final heuristic.",
        "Truecasing.",
        "We attempted to preserve meaning-bearing case distinctions.",
        "The Czech lemmatizer produces case-sensitive lemmas and thus makes it easy to cast the capitalization of the lemma back on the word form.",
        "For English we approximate the same effect by a two-step procedure.",
        "Decoding steps.",
        "We use a simple two-step scenario similar to class-based models (Brown and others, 1992): (1) the source English word forms are translated to Czech word forms and (2) full Czech morphological tags are generated from the Czech forms.",
        "Language models.",
        "We use the following 6 independently weighted language models for the target (Czech) side:",
        "• 3-grams of word forms based on all CzEng 0.7",
        "• 3-grams of word forms in Project Syndicate section of CzEng (in-domain for WMT07 and WMT08 NC-test set), 1.8M tokens,",
        "• three models of 7-grams of morphological tags from the same sources.",
        "Lexicalized reordering using the monotone/swap/discontinuous bidirectional model based on both source and target word forms.",
        "MERT.",
        "We use the minimum-error rate training procedure by Och (2003) as implemented in the Moses toolkit to set the weights ofthe various translation and language models, optimizing for BLEU.",
        "Final detokenization is a simple rule-based procedure based on Czech typographical conventions.",
        "Finally, we capitalize the beginnings of sentences.",
        "See BLEU scores in Table 2 below."
      ]
    },
    {
      "heading": "3. MT with a Deep Syntactic Transfer",
      "text": [
        "Czech has a well-established theory of linguistic analysis called Functional Generative Description (Sgall et al., 1986) supported by a big treebanking enterprise (Hajic and others, 2006) and ongoing adaptations for other languages including English (Cinkova and others, 2004).",
        "There are two layers",
        "typical shape.",
        "In other sentences we change the case only if a typically lowercase word is capitalized (e.g. at the beginning of the sentence) or if a typically capitalized word is all-caps.",
        "Unknown words in title-like sentences are lowercased and left intact in other sentences.",
        "of syntactic analysis, both formally captured as labelled ordered dependency trees: the analytical (a-, surface syntax) representation bears a 1-1 correspondence between tokens in the sentence and nodes in the tree; the tectogrammatical (t-, deep syntax) representation contains nodes only for autose-mantic words and adds nodes for elements not expressed on the surface but required by the grammar (e.g. dropped pronouns).",
        "We use the following tools to automatically annotate plaintext up to the t-layer: (1) TextSeg (Ceska, 2006) for tokenization, (2) tagging and lemmatization see above, (3) parsing to a-layer: Collins (1996) followed by head-selection rules for English, McDonald and others (2005) for Czech, (4) parsing to t-layer: Zabokrtsky (2008) for English, Klimes (2006) for Czech.",
        "The transfer step is based on Synchronous Tree Substitution Grammars (STSG), see Bojar and Cmejrek (2007) for a detailed explanation.",
        "The essence is a log-linear model to search for the most likely synchronous derivation ö of the source T and target T2dependency trees:",
        "& s.t.",
        "source is Ti",
        "The key feature function hm in STSG represents the probability of attaching pairs of dependency treelets t\\.2 such as in Figure 1 into aligned pairs of frontiers C\"^) in another treelet pair j given frontier state labels (e.g. _Pred-_VP in Figure 1):",
        "Other features include e.g. number of internal nodes (drawn as • in Figure 1) produced, number of treelets produced, and more importantly the traditional n-gram language model if the target (a-)tree is linearized right away or a binode model promoting likely combinations of the governor g(e) and the child c(e) of an edge e e T2:",
        "The probabilistic dictionary of aligned treelet pairs is extracted from node-aligned (GIZA++ on linearized trees) parallel automatic treebank as in Moses' training: all treelet pairs compatible with the node alignment.",
        "Labels of nodes at the t-layer are not atomic but consist of more than 20 attributes representing various linguistic features.",
        "We can consider the attributes as individual factors (Koehn and Hoang, 2007) .",
        "This allows us to condition the translation choice on a subset of source factors only.",
        "In order to generate a value for each target-side factor, we use a sequence of mapping steps similar to Koehn and Hoang (2007).",
        "For technical reasons, our current implementation allows to generate factored targetside only when translating a single node to a single node, i.e. preserving the tree structure.",
        "In our experiments we used 8 source (English) t-node attributes and 14 target (Czech) attributes.",
        "Table 1 shows BLEU scores for various configurations of our decoder.",
        "The abbreviations indicate between which layers the tree transfer was employed (e.g. \"eact\" means English a-layer to Czech t-layer).",
        "The \"p\" layer is an approximation of phrase-based MT: the surface \"syntactic\" analysis is just a left-to-right linear tree.",
        "For setups ending in t-layer, we use a deterministic generation the of Czech sentence by Ptacek and Zabokrtsky (2006).",
        "For WMT08 shared task, Table 2, we used a variant of the \"etct factored\" setup with the annotation pipeline as incorporated in TectoMT (Zabokrtsky, 2008) environment and using TectoMT internal",
        "MT on WMT07 DevTest.",
        "DevTest NC Test News Test",
        "rules for t-layer parsing and generation instead of Klimes (2006) and (Ptacek and Zabokrtsky, 2006).",
        "Our syntax-based approach does not reach scores of phrase-based MT due to the following reasons:",
        "Cumulation of errors at every step ofanalysis.",
        "Data loss due to incompatible parses and node alignment.",
        "Unlike e.g. Quirk et al.",
        "(2005) or Huang et al.",
        "(2006) who parse only one side and project the structure, we parse both languages independently.",
        "Natural divergence and random errors in either of the parses and/or the alignment prevent us from extracting many treelet pairs.",
        "Combinatorial explosion in target node attributes.",
        "Currently, treelet options are fully built in advance.",
        "Uncertainty in the many t-node attributes leads to too many insignificant variations while e.g. different lexical choices are pushed off the stack.",
        "While vital for final sentence generation (see Table 1), fine-grained t-node attributes should be produced only once all key structural, lexical and form decisions have been made.",
        "The same sort of explosion makes complicated factored setups not yet feasible in Moses, either.",
        "Tree-based Transfer",
        "LM Type",
        "BLEU",
        "epcp",
        "n-gram",
        "10.9±0.6",
        "eaca",
        "n-gram",
        "S.S±0.6",
        "epcp",
        "none",
        "S.7±0.6",
        "eaca",
        "none",
        "6.6±0.5",
        "etca",
        "n-gram",
        "6.3±0.6",
        "etct factored, preserving structure",
        "binode",
        "5.6±0.5",
        "etct factored, preserving structure",
        "none",
        "5.3±0.5",
        "eact, target side atomic",
        "binode",
        "3.0±0.3",
        "etct, atomic, all attributes",
        "binode",
        "2.6±0.3",
        "etct, atomic, all attributes",
        "none",
        "1.6±0.3",
        "etct, atomic, just t-lemmas",
        "none",
        "0.7±0.2",
        "Phrase-based (Moses) as reported by Bojar (2007) Vanilla n-gram Factored to improve target morphology n-gram",
        "12.9±0.6 14.2±0.7",
        "Lack of n-gram LM in the (deterministic) generation procedures from a t-tree.",
        "While we support final LM-based rescoring, there is too little variance in n-best lists due to the explosion mentioned above.",
        "Too many model parameters given our stack limit.",
        "We use identical MERT implementation to optimize ATOs but in the large space of hypotheses, MERT does not converge.",
        "Our approach should not be confused with the TectoMT submission by Zdenek Zabokrtsky with a deterministic transfer: heuristics fully exploiting the similarity of English and Czech t-layers.",
        "Ding and Palmer (2005) improve over word-based MT baseline with a formalism very similar to STSG.",
        "Though not explicitly stated, they seem not to encode frontiers in the treelets and allow for adjunction (adding siblings), like Quirk et al.",
        "(2005), which significantly reduces data sparseness.",
        "Riezler and III (2006) report an improvement in MT grammaticality on a very restricted test set: short sentences parsable by an LFG grammar without back-off rules."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "We have presented our best-performing factored phrase-based English-to-Czech translation and a highly experimental complex system with tree-based transfer at a deep syntactic layer.",
        "We have discussed some of the reasons why the phrase-based MT currently performs much better."
      ]
    }
  ]
}
