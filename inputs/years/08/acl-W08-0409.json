{
  "info": {
    "authors": [
      "Yanjun Ma",
      "Sylwia Ozdowska",
      "Yanli Sun",
      "Andy Way"
    ],
    "book": "Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2)",
    "id": "acl-W08-0409",
    "title": "Improving Word Alignment Using Syntactic Dependencies",
    "url": "https://aclweb.org/anthology/W08-0409",
    "year": 2008
  },
  "references": [
    "acl-C96-2141",
    "acl-H05-1011",
    "acl-H05-1012",
    "acl-J00-2004",
    "acl-J03-1002",
    "acl-J93-1003",
    "acl-J93-2003",
    "acl-J97-3002",
    "acl-N03-1017",
    "acl-N06-1014",
    "acl-P02-1040",
    "acl-P02-1050",
    "acl-P03-1021",
    "acl-P05-1057",
    "acl-P06-1009",
    "acl-P06-2014",
    "acl-P07-1001",
    "acl-P07-2045",
    "acl-W00-1314",
    "acl-W04-2207",
    "acl-W04-3226",
    "acl-W05-0812",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "text": [
        "Yanjun Ma Sylwia Ozdowska Yanli Sun Andy Way School of Computing, Dublin City University, Dublin, Ireland {yma,sozdowska,away}@computing.dcu.ie School of Applied Language and Intercultural Studies, Dublin City University, Dublin, Ireland yanli.sun2@mail.dcu.ie",
        "We introduce a word alignment framework that facilitates the incorporation of syntax encoded in bilingual dependency tree pairs.",
        "Our model consists of two sub-models: an anchor word alignment model which aims to find a set of high-precision anchor links and a syntax-enhanced word alignment model which focuses on aligning the remaining words relying on dependency information invoked by the acquired anchor links.",
        "We show that our syntax-enhanced word alignment approach leads to a 10.32% and 5.57% relative decrease in alignment error rate compared to a generative word alignment model and a syntax-proof discriminative word alignment model respectively.",
        "Furthermore, our approach is evaluated extrinsically using a phrase-based statistical machine translation system.",
        "The results show that SMT systems based on our word alignment approach tend to generate shorter outputs.",
        "Without length penalty, using our word alignments yields statistically significant improvement in Chinese-English machine translation in comparison with the baseline word alignment."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Automatic word alignment can be defined as the problem of determining translational correspondences at word level given a parallel corpus of aligned sentences.",
        "Bilingual word alignment is a fundamental component of most approaches to statistical machine translation (SMT).",
        "Dominant approaches to word alignment can be classified into two main schools: generative and discriminative word alignment models.",
        "Generative word alignment models, initially developed at IBM (Brown et al., 1993), and then augmented by an HMM-based model (Vogel et al., 1996), have provided powerful modeling capability for word alignment.",
        "However, it is very difficult to incorporate new features into these models.",
        "Discriminative word alignment models, based on discriminative training of a set of features (Liu et al., 2005; Moore, 2005), on the other hand, are more flexible to incorporate new features, and feature selection is essential to the performance ofthe system.",
        "Syntactic annotation of bilingual corpora, which can be obtained more efficiently and accurately with the advances in monolingual language processing, is a potential information source for word alignment tasks.",
        "For example, Part-of-Speech (POS) tags of source and target words can be used to tackle the data sparseness problem in discriminative word alignment (Liu et al., 2005; Blunsom and Cohn, 2006).",
        "Shallow parsing has also been used to provide relevant information for alignment (Ren et al., 2007; Sun et al., 2000).",
        "Deeper syntax, e.g. phrase or dependency structures, has been shown useful in generative models (Wang and Zhou, 2004; Lopez and Resnik, 2005), heuristic-based models (Ayan et al., 2004; Ozdowska, 2004) and even for syntactically motivated models such as ITG (Wu, 1997; Cherry and Lin, 2006).",
        "In this paper, we introduce an approach to improve word alignment by incorporating syntactic dependencies.",
        "Our approach is motivated by the fact that words tend to be dependent on each other.",
        "If we can first obtain a set of reliable anchor links, we could take advantage of the syntactic dependencies relating unaligned words to aligned anchor words to expand the alignment.",
        "Figure 1 gives an illustrating example.",
        "Note that the link (2,4) can be easily identified, but the link involving the fourth Chinese word (a function word denoting 'time') (4,4) is hard.",
        "In such cases, we can make use of the dependency relationship ('tclause') between c2 and c4 to help the alignment process.",
        "Given such an observation, our model is composed of two related alignment models.",
        "The first one is an anchor alignment model which is used to find a set of anchor links; the other one is a syntax-enhanced alignment model aiming to process the words left unaligned after anchoring.",
        "The remainder of this paper is organized as follows.",
        "In Section 2, we introduce our syntax-enhanced discriminative word alignment approach.",
        "The feature functions used are described in Section 3.",
        "Experimental setting and results are presented in Section 4 and 5 respectively.",
        "In Section 6, we compare our approach with other related word alignment approaches.",
        "Section 7 concludes the paper and gives avenues for future work."
      ]
    },
    {
      "heading": "2. Word Alignment Model 2.1 Notation",
      "text": [
        "While in this paper we focus on Chinese-English, the method proposed is applicable to any language pair.",
        "The notation will assume Chinese-English word alignment and Chinese-English MT.",
        "Here we adopt a notation similar to (Brown et al., 1993).",
        "Given a Chinese sentence cJ consisting of J words {c1,cj} and an English sentence e{ consisting of / words e1 , ...,e/, we define the alignment A between cJ and e{ as a subset of the Cartesian product of the word positions:",
        "Our alignment representation is restricted so that each source word can only be aligned to one target word.",
        "The alignment A consists of associations j – i = a from a source position j to a target position i = ûj.",
        "The 'null' alignment aj = 0 with the 'empty' word e0 is used to account for source words that are not aligned to any target word.",
        "We use Aa to denote a subset of A.",
        "The indices of the K source words involved in Aa are represented as AK and the corresponding target indices for Ak are represented as aAk.",
        "The unaligned source words are represented as A.",
        "Given a source sentence cJ and target sentence e{, we seek to find the optimum alignment A such that:",
        "We use a model (2) that directly models the linkage between source and target words similarly to (It-tycheriah and Roukos, 2005).",
        "We decompose this model into an anchor alignment model (3) and a syntax-enhanced model (4) by distinguishing the anchor alignment from the non-anchor alignment.",
        "The anchor alignment model p£(Aa ) aims to find a set of high precision links.",
        "Various approaches can be used for this purpose.",
        "In this paper we adopted the following two approaches.",
        "The problem of word alignment is regarded as a process of word linkage disambiguation, i.e. choosing the correct links between words from all competing hypothesis (Melamed, 2000; Deng and Gao, 2007).",
        "We constrain the link probabilities in such a way",
        "Condition (5) implies that for the source word Cj, the link with the target word e» is more probable (with reliability threshold e1) than the link with any other target word.",
        "Condition (6) guarantees that for the target word e», Cj is the only most probable (with threshold e2) source word to be linked to.",
        "We can use the asymmetric IBM models for bidirectional word alignment and get the intersection.",
        "The syntax-enhanced model is used to model the alignment of the words left unaligned after anchoring.",
        "We directly model the linkage between source and target words using a discriminative word alignment framework where various features can be incorporated.",
        "Given a source word cj and the target sentence e1 , we search for the alignment aj such",
        "IBM model 1 is a position-independent word alignment model which is often used to bootstrap parameters for more complex models.",
        "Model 1 models the conditional distribution and uses a uniform distribution for the dependencies between source word positions and target word positions.",
        "The log-likelihood ratio statistic has been found to be accurate for modeling the associations between rare events (Dunning, 1993).",
        "It has also been successfully used to measure the associations between word pairs (Melamed, 2000; Moore, 2005).",
        "Given the following contingency table:",
        "the log-likelihood ratio can be defined as:",
        "where B(k|n,p) = )pk(1 – p)ra_fc are binomial probabilities.",
        "The probability parameters can be obtained using maximum likelihood estimates:",
        "In this decision rule, we assume that a set of highly reliable anchor alignments Aa has been obtained, and Tc (resp.",
        "Te) is used to denote the dependency structure for source (resp.",
        "target) language.",
        "In such a framework, various machine learning techniques can be used for parameter estimation.",
        "The POS tags can provide effective information for addressing the data sparseness problem using the lexical features (Liu et al., 2005; Blunsom and Cohn, 2006).",
        "The POS translation probability can be easily obtained using maximum likelihood estimation from an annotated corpus:",
        "Pr(TcITe) where Tc is a Chinese word's POS tag and Te is an English word's POS tag.",
        "COL(Tc, Te) is the count of Tc and Te being linked to each other in the corpus, and COF(Te) is the frequency of Te in the corpus.",
        "a",
        "b",
        "c",
        "d"
      ]
    },
    {
      "heading": "3. Feature Function for Syntax-Enhanced",
      "text": [
        "The various features used in our syntax-enhanced model can be classified into three groups: statistics-based features, syntactic features and relative distortion features.",
        "The dependency relation Re (resp.",
        "Rc) between two English (resp.",
        "Chinese) words ej and (resp.",
        "cj and cj') in the dependency tree of the English sentence e1 (resp.",
        "Chinese sentence c1J ) can be represented as atriple <ej; Re, >(resp.",
        "<cj, Rc, ej'>).",
        "Given c1J , e1 and their syntactic dependency trees Tcj , Tej, if ej is aligned to cj and aligned to cj', according to the dependency correspondence assumption (Hwa et al., 2002), there exists a triple",
        "While we are not aiming to justify the feasibility of the dependency correspondence assumption by proving to what extent Re = Rc under the condition described above, we do believe that cj and cj' are likely to be dependent on each other.",
        "Given the anchor alignment Aa, a candidate link (j, i) and the dependency trees, we can design four classes of feature functions.",
        "The agreement features can be further classified into dependency agreement features and dependency label agreement features.",
        "Given a candidate link (j, i) and the anchor alignment Aa, the dependency agreement (DA) feature function is defined as follows:",
        "We can define the dependency label agreement feature as follows:",
        "1 if 3 <cj, Rc, cj' >, <ej,i?e,ej' > hDLA-1 = <!",
        "and (j', i') e AA,RC = fie, (14) 0 otherwise.",
        "Similarly we can obtain hDLA-2 by changing the dependency direction.",
        "Given a candidate link(j, i) and anchoralignment Aa, source language dependency features are used to capture the dependency label between a source word cj and a source anchor word ck e A.",
        "For example, a feature function relating to dependency type 'PRD' can be defined as:",
        "1 if 3 <cj, Rc, cj' > hsrc-1-PRD =4 and Rc ='PRD', (15) 0 otherwise.",
        "By changing the direction we can obtain",
        "hsrc-2-PRD.",
        "Target word dependency features can be defined in a similar way as source word dependency features.",
        "The target anchor feature defines whether the target word ej is an anchor word.",
        "0 otherwise.",
        "By changing the dependency direction between the words cj and cj', we can derive another dependency agreement feature:",
        "hsrc-1-PRD",
        "1 if i G ûa, 0 otherwise.",
        "We can design features encoding the relative distortion information which can be used to evaluate a candidate link by computing its relative position change with respect to the anchor alignment.",
        "The relative position change of a candidate link l = (j, i) is formally defined as follows:",
        "where (iL,jL) is the leftmost anchor link of l, (iR, jR) is the rightmost anchor link of l. The less the relative position changes, the more likely the candidate link is.",
        "With a set of anchor alignments, we can obtain the distribution of the relative position changes from an annotated corpus using maximum likelihood estimation.",
        "In our experiments, we used the following four probabilities: p(D = 0), p(D = 1, 2), p(D = 3, 4) and p(D > 4).",
        "We manually annotated word alignments on de-vset3.",
        "Since manual word alignment is an ambiguous task, we also explicitly allow for ambiguous alignments, i.e. the links are marked as sure (S) or possible (P) (Och and Ney, 2003).",
        "IWSLT devset3 consists of 502 sentence pairs after cleaning.",
        "We used the first 300 sentence pairs for training, the following 50 sentence pairs as validation set and the last 152 sentence pairs for testing.",
        "Training was performed using the default training set (39,952 sentence pairs), to which we added the set devset1 (506 sentence pairs).",
        "We used devset2 (506 sentence pairs, 16 references) to tune various parameters in the MT system and IWSLT 2007 test set (489 sentence pairs, 6 references) for testing."
      ]
    },
    {
      "heading": "4. Experimental Setting",
      "text": [
        "The experiments were carried out using the Chinese-English datasets provided within the IWSLT 2007 evaluation campaign (Fordyce, 2007), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002).",
        "This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad.",
        "We tagged all the sentences in the training and de-vset3 using a maximum entropy-based POS tagger-MXPOST (Ratnaparkhi, 1996), trained on the Penn English and Chinese Treebanks.",
        "Both Chinese and English sentences are parsed using the Malt dependency parser (Nivre et al., 2007), which achieved 84% and 88% labelled attachment scores for Chinese and English respectively.",
        "In our experiments, we treated anchor alignment and syntax-enhanced alignment as separate processes in a pipeline.",
        "The anchor alignments are kept fixed so that the parameters in the syntax-enhanced model can be optimized.",
        "We used the support vector machine (SVM) toolkit-SVMJight to optimize the parameters in (7).",
        "Our model is constrained in such a way that each source word can only be aligned to one target word.",
        "Therefore, in training, we transform each possible link involving the words left unaligned after anchoring into an event.",
        "In testing, the source words are consumed in sequence and the target words serve as states.",
        "The SVM dual variable was used to measure the reliability of each candidate link and the alignment link for each word is made independently, which makes the alignment search much easier.",
        "A threshold t was set as the minimal reliability score for each link.",
        "t is optimized according to alignment error rate (21) on the validation set.",
        "We used the GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003) for word alignment, and the heuristics described in (Och and Ney, 2003) to derive the intersection and refined alignment.",
        "We use a standard log-linear phrase-based SMT (PB-SMT) model as a baseline: GIZA++ implementation of IBM word alignment model 4, the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a trigram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007) to decode.",
        "We evaluate the intrinsic quality of predicted alignment A with precision, recall and alignment error rate (AER).",
        "Slightly differently from (Och and Ney, 2003), we use possible alignments in computing recall.",
        "We also extrinsically measure the word alignment quality via a Chinese-English translation task.",
        "The translation output is measured using BLEU (Pap-ineni et al., 2002).",
        "As we can see in Table 2, our precision-oriented approach to acquire anchor alignments was accomplished quite well.",
        "All four different anchor alignment models achieved high precision.",
        "However, the recall differs dramatically, with model 4 achieving the highest recall and the heuristics-based approach receiving the lowest.",
        "To investigate the influence of the anchor alignment model, we first obtained the intersection of the words left unaligned after anchoring using each of the anchor alignment models.",
        "We evaluate the alignment of these words against the gold-standard alignments involving these words.",
        "The influence of anchor alignment on the performance of the syntax-enhanced model can be seen in Table 3.",
        "The performance of the syntax-enhanced model is closely related to that of the anchor alignment method.",
        "As can be seen from Table 2 and 3, HMM anchoring achieves the best precision and so does the syntax-enhanced alignment; IBM model 4 achieves the best recall and so does the syntax-enhanced alignment.",
        "Finally, the best alignment performances are obtained with IBM model 4 anchoring, with the difference in recall between HMM and IBM model 4 anchoring being more significant than the difference in precision.",
        "The influence of incorporating syntactic dependencies into the word alignment process is shown in Table 4.",
        "Syntax plays a positive role in all different anchor alignment configurations.",
        "The influence grows proportionally to the strength of the anchor alignment model.",
        "With the Model 4 intersection used as the set of anchor alignments, adding syntactic dependency features into the syntax-enhanced alignment model yields a 5.57% relative decrease in",
        "anchor model",
        "precision",
        "recall",
        "f-measure",
        "AER",
        "Heuristics",
        "0.9774",
        "0.4047",
        "0.5724",
        "0.3947",
        "Model 1",
        "0.9509",
        "0.5011",
        "0.6563",
        "0.3157",
        "HMM",
        "0.9802",
        "0.5327",
        "0.6903",
        "0.2809",
        "Model 4",
        "0.9777",
        "0.5677",
        "0.7179",
        "0.2533",
        "anchor model",
        "precision",
        "recall",
        "f-score",
        "AER",
        "Heuristics",
        "0.4505",
        "0.3270",
        "0.3790",
        "0.6210",
        "Model 1",
        "0.5538",
        "0.3894",
        "0.4573",
        "0.5427",
        "HMM",
        "0.5932",
        "0.3611",
        "0.4489",
        "0.5511",
        "Model 4",
        "0.5660",
        "0.4216",
        "0.4832",
        "0.5168",
        "model",
        "precision",
        "recall",
        "f-score",
        "AER",
        "HMM refined Syntax-HMM",
        "0.8043 0.8744",
        "0.7592 0.7304",
        "0.7811 0.7959",
        "0.2059 0.1845",
        "Model 4 refined Syntax-Model 4",
        "0.7941 0.8566",
        "0.7987 0.7685",
        "0.7964 0.8102",
        "0.1929 0.1730",
        "AER.",
        "We interpret the contribution of each feature in terms of feature weights in SVM model training.",
        "The weights for the most discriminative features in each feature class in Chinese-English word alignment (using HMM intersection as anchor alignment) are shown in Table 5.",
        "As we can see, all statistics-based features are informative.",
        "Two target dependency features are informative: 'PRD' denoting 'predicative' dependency, and 'AMOD' denoting 'adjective/adverb modifier' dependency."
      ]
    },
    {
      "heading": "5. Experimental Results 5.1 Word Alignment",
      "text": [
        "We performed word alignment bidirectionally using our approach to obtain the union and compared our results with two strong baselines based on generative word alignment models.",
        "The results are shown in Table 1.",
        "We can see that both the syntax-enhanced model based on HMM intersection anchors (Syntax-HMM) and on IBM model 4 anchors (Syntax-Model 4) are better than the pure generative word alignment models.",
        "Our approach is superior in precision with a disadvantage in recall.",
        "The best result achieved 10.32% relative decrease in AER compared to the baseline when we use IBM model 4 intersection to obtain the set of anchor alignments.",
        "Research has shown that an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al., 2006).",
        "Hereafter, we used a Chinese-English MT task to extrinsically evaluate the quality of our word alignment.",
        "Table 6 shows the influence of our word alignment approach on MT quality.",
        "On development set, we achieved statistically significant improvement using both our syntax-enhanced models – Syntax-HMM (p<0.002) and Syntax-Model 4 (p<0.008).",
        "On the test set, we observed that the MT output based on our alignment model tends to be shorter than the reference translations and the BLEU score is considerably penalized.",
        "If we ignore the length penalty ('BP' in Table 6) in significance testing, the improvement on test set is also statistically significant: p<0.04 for both Syntax-HMM and SyntaxModel 4.",
        "However, an indepth manual analysis needs to be carried out in order to determine the exact nature of the shorter sentences derived."
      ]
    },
    {
      "heading": "6. Comparison with Previous Work",
      "text": [
        "Our syntax-enhanced model is a discriminative word alignment model.",
        "Certain generative word alignment models (e.g. HMM or IBM 4) also take the first-order dependencies into account.",
        "However, long distance dependencies between words are hard to incorporate into these models because of the explosive number of parameters.",
        "On the other hand, like existing discriminative models, our approach uses a set of informative features based on co-occurrence statistics, e.g. log-likelihood ratio and DICE score.",
        "The advantage of our approach is the mechanism by which syntactic features may be incorporated.",
        "model",
        "precision",
        "recall",
        "f-score",
        "AER",
        "Heuristics no syntax w. syntax",
        "0.8362 0.8376",
        "0.6751 0.6894",
        "0.7470 0.7563",
        "0.2302 0.2240",
        "Model 1 no syntax w. syntax",
        "0.8759 0.8542",
        "0.6902 0.7160",
        "0.7720 0.7790",
        "0.2045 0.2011",
        "HMM no syntax w. syntax",
        "0.8655 0.8744",
        "0.7168 0.7304",
        "0.7841 0.7959",
        "0.1952 0.1845",
        "Model 4 no syntax w. syntax",
        "0.8697 0.8566",
        "0.7340 0.7685",
        "0.7961 0.8102",
        "0.1832 0.1730",
        "dev.",
        "set",
        "test set",
        "Baseline",
        "0.5412",
        "0.3510 (BP=0.96)",
        "Syntax-HMM",
        "0.6015",
        "0.3409 (BP=0.86)",
        "Syntax-Model 4",
        "0.5834",
        "0.3585 (BP=0.91)",
        "weight",
        "Model 1 Score",
        "0.1416",
        "POS",
        "0.0540",
        "Log-likelihood Ratio",
        "0.0856",
        "relative distortion",
        "0.0606",
        "DA-1",
        "0.0227",
        "DLA-2",
        "0.0927",
        "tgt-l-PRD",
        "0.0961",
        "tgt-2-AMOD",
        "0.0621",
        "Some previous research also tried to make use of syntax in word alignment.",
        "(Wang and Zhou, 2004) investigated the benefit of monolingual parsing for alignment.",
        "They learned a generalized word association measure (crosslingual word similarities) based on monolingual dependency structures and improved alignment performances over IBM model 2 and certain heuristic-based models.",
        "(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework.",
        "Compared to these models, our approach directly takes advantage of dependency relations as they are transformed into feature functions incorporated into a discriminative word alignment framework."
      ]
    },
    {
      "heading": "7. Conclusion and Future Work",
      "text": [
        "In this paper, we proposed a model that can facilitate the incorporation of syntax into word alignment and measured the combination of a set of syntactic features.",
        "Experimental results have shown that syntax is useful in word alignment and especially effective in improving the recall.",
        "We have also observed that in our word alignment framework, the two submodels are closely related and the quality of the anchor alignment model plays an important role in the system performance.",
        "The promising results will lead us to improve our model in the following aspects.",
        "First, the two submodels in our approach are two separate processes performed in pipeline.",
        "We plan to jointly optimize the two models in one go.",
        "Second, some of our experiments used complex IBM models, e.g. IBM Model 4, to obtain anchor alignment.",
        "We plan to boostrap the alignment using simple heuristics without relying on complex IBM models.",
        "Third, the alignment searching process assumed the alignment link for each word is made independently.",
        "A feasible markovian assumption will be tested for searching.",
        "Fourth, a comparison with traditional discriminative word alignment models is also necessary to justify the merits of our approach.",
        "Finally, we also plan to adapt our approach to larger data sets and more language pairs."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is supported by Science Foundation Ireland (grant number OS/IN/1732).",
        "Prof. Rebecca Hwa from University of Pittsburgh and Dr. Yang Liu from the Institute of Computing Technology, Chinese Academy of Sciences, are kindly acknowledged for providing us with their word alignment guidelines.",
        "We would also like to thank the anonymous reviewers for their insightful comments."
      ]
    }
  ]
}
