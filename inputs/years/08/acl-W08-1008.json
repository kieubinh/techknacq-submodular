{
  "info": {
    "authors": [
      "Sandra Kübler"
    ],
    "book": "Proceedings of the Workshop on Parsing German",
    "id": "acl-W08-1008",
    "title": "The PaGe 2008 Shared Task on Parsing German",
    "url": "https://aclweb.org/anthology/W08-1008",
    "year": 2008
  },
  "references": [
    "acl-D07-1096",
    "acl-J93-2004",
    "acl-W06-1614"
  ],
  "sections": [
    {
      "text": [
        "The PaGe 2008 Shared Task on Parsing German*",
        "Sandra Kubler",
        "The ACL 2008 Workshop on Parsing German features a shared task on parsing German.",
        "The goal of the shared task was to find reasons for the radically different behavior of parsers on the different treebanks and between constituent and dependency representations.",
        "In this paper, we describe the task and the data sets.",
        "In addition, we provide an overview of the test results and a first analysis."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "German is one of the very few languages for which more than one syntactically annotated resource exists.",
        "Other languages for which this is the case include English (with the Penn treebank (Marcus et al., 1993), the Susanne Corpus (Sampson, 1993), and the British section of the ICE Corpus (Wallis and Nelson, 2006)) and Italian (with ISST (Mon-tegmagni et al., 2000) and TUT (Bosco et al., 2000)).",
        "The three German treebanks are Negra (Skut et al., 1998), Tiger (Brants et al., 2002), and TuBa-D/Z (Hinrichs et al., 2004).",
        "We will concentrate on Tiger and TuBa-D/Z here; Negra is annotated with an annotation scheme very similar to Tiger but is smaller.",
        "In contrast to other languages, these two treebanks are similar on many levels: Both treebanks are based on newspaper text, both use the STTS part of speech (POS) tagset (Thielen and Schiller, 1994), and both use an annotation",
        "*I am very grateful to Gerald Penn, who suggested this workshop and the shared task, took over the biggest part of the workshop organization and helped with the shared task.",
        "scheme based on constituent structure augmented with grammatical functions.",
        "However, they differ in the choices made in the annotation schemes, which makes them ideally suited for an investigation of how these decisions influence parsing accuracy in different parsers.",
        "On a different level, German is an interesting language for parsing because of the syntactic phenomena in which the language differs from English, the undoubtedly most studied language in parsing: German is often listed as a non-configurational language.",
        "However, while the word order is freer than in English, the language exhibits a less flexible word order than more typical non-configurational languages.",
        "A short overview of German word order phenomena is given in section 2.",
        "The structure of this paper is as follows: Section 2 discusses three characteristics of German word order, section 3 provides a definition of the shared task, and section 4 gives a short overview of the treebanks and their annotation schemes that were used in the shared task.",
        "In section 5, we give an overview of the participating systems and their results."
      ]
    },
    {
      "heading": "2. German Word Order",
      "text": [
        "In German, the order of non-verbal phrases is relatively free, but the placement of the verbal elements is determined by the clause type.",
        "Thus, we will first describe the placement of the finite verb, then we will explain phrasal ordering, and finally we will look at discontinuous constituents.",
        "In German, the clause type determines the placement of finite verbs: In non-embedded declarative clauses, as in (1a), the finite verb is in second position (V2).",
        "In yes/no questions, as in (1b), the finite verb is the clause-initial constituent (V1), and in embedded clauses, as in (1c), it appears clause finally (Vn).",
        "(1) a.",
        "Der Mann hat das Auto gekauft.",
        "The man has the car bought 'The man has bought the car.'",
        "b. Hat der Mann das Auto gekauft?",
        "Has the man the car bought 'Has the man bought the car?'",
        "a. dass der Mann das Auto gekauft hat.",
        "that the man the car bought has",
        "All non-finite verbs appear at the right periphery of the clause (cf. 2), independently of the clause type.",
        "(2) Der Mann sollte das Auto gekauft haben.",
        "The man should the car bought have 'The man should have bought the car.'",
        "Apart from the fixed placement of the verbs, the order of the non-verbal elements is flexible.",
        "In (3), any of the four complements and adjuncts of the main verb (ge)geben can be in sentence-initial position, depending on the information structure of the sentence.",
        "(3) a. Das Kind hat dem Mann gestern den",
        "The child has the man yesterday the Ball gegeben.",
        "ball given 'The child has given the ball to the man yesterday.'",
        "b. Dem Mann hat das Kind gestern den Ball gegeben.",
        "c. Gestern hat das Kind dem Mann den Ball gegeben.",
        "d. Den Ball hat das Kind gestern dem Mann gegeben.",
        "In addition, the ordering of the elements that occur between the finite and the non-finite verb forms is also free so that there are six possible linearizations for each of the examples in (3a-d).",
        "One exception to the free ordering of non-verbal elements is the ordering of pronouns.",
        "If the pronouns appear to the right of the finite verb in V1 and V2 clauses, they are adjacent to the finite verb in fixed order.",
        "(4) Gestern hat sie sie ihm gegeben.",
        "Yesterday has she her/them him given.",
        "'Yesterday, she gave her/them to him.'",
        "In (4), three pronouns are present.",
        "Although the pronoun sie is ambiguous between nominative/accusative singular and nominative/accusative plural, the given example is unambiguous with respect to case since the nominative precedes the accusative, which in turn precedes the dative.",
        "Due to the flexible phrase ordering, the grammatical functions of constituents in German, unlike in English, cannot be deduced from the constituents' location in the constituent tree.",
        "As a consequence, parsing approaches to German need to be based on treebank data which contain a combination of constituent structure and grammatical functions - for parsing and evaluation.",
        "For English, in contrast, grammatical functions are often used internally in parsers but suppressed in the final parser output.",
        "Another characteristic of German word order is the frequency of discontinuous constituents.",
        "The sentence in (5) shows an extraposed relative clause that is separated from its head noun das Buch by the non-finite verb gelesen.",
        "(5) Der Mann hat das Buch gelesen, das ich The man has the book read, which I ihm empfohlen habe.",
        "him recommended have 'The man read the book that I recommended to him.'",
        "In German, it is also possible to partially front VPs, such as in sentence (6).",
        "This sentence is taken from the TuBa-D/Z treebank.",
        "(6) Fur den Berliner Job qualifiziert hat sich For the Berlin job qualified has himself Zimmermann auch durch seinen Blick furs Zimmermann also by his view for the finanziell Machbare.",
        "financially doable 'Zimmermann qualified for the job in Berlin partially because of his view for what is financially feasible.'",
        "Here, the canonical word order would be Zimmermann hat sich auch durch seinen Blick furs finanziell Machbare fur den Berliner Job qualifiziert.",
        "Such discontinuous structures occur frequently in the Tiger and TuBa-D/Z treebanks and are handled differently in the two annotation schemes, as will be discussed in more detail in section 4."
      ]
    },
    {
      "heading": "3. Task Definition",
      "text": [
        "In this section, we give the definition of the shared task.",
        "We provided two subtasks: parsing constituent structure and parsing the dependency representations.",
        "Both subtasks involved training and testing on data from the two treebanks, Tiger and TuBa-D/Z.",
        "The dependency format was derived from the constituent format so that the sentences were identical in the two versions.",
        "The participants were given training sets, development sets, and test sets of the two treebanks.",
        "The training sets contained 20894 sentences per treebank, the development and test set consisted of 2611 sentences each.",
        "The test sets contained gold standard POS labels.",
        "In these sets, sentence length was restricted to a maximum of 40 words.",
        "Since for some sentences in both treebanks, the annotation consists of more than one tree, all trees were joined under a virtual root node, VROOT.",
        "Since some parsers cannot assign grammatical functions to part of speech tags, these grammatical functions were provided for the test data as attached to the POS tags.",
        "Participants were asked to perform a test without these functions if their parser was equipped to provide them.",
        "Two participants did submit these results, and in both cases, these results were considerably lower.",
        "Evaluation for the constituent version consisted of the PARSEVAL measures precision, recall, and Fi measure.",
        "All these measures were calculated on combinations of constituent labels and grammatical functions.",
        "Part of speech labels were not considered in the evaluation.",
        "Evaluation for the dependency version consisted of labeled and unlabeled attachment scores.",
        "For this evaluation, we used the scripts provided by the CoNLL shared task 2007 on dependency parsing (Nivre et al., 2007)."
      ]
    },
    {
      "heading": "4. The Treebanks",
      "text": [
        "The two treebanks used for the shared task were the Tiger Corpus, (Brants et al., 2002) version 2, and the TuBa-D/Z treebank (Hinrichs et al., 2004; Telljohann et al., 2006), version 3.",
        "Both treebanks use German newspapers as their data source: the Frankfurter Rundschau newspaper for Tiger and the 'die tageszeitung' (taz) newspaper for TuBa-D/Z.",
        "The average sentence length is very similar: In Tiger, sentences have an average length of 17.0, and in TuBa-D/Z, 17.3.",
        "This can be regarded as an indication that the complexity of the two texts is comparable.",
        "Both treebanks use the same POS tagset, STTS (Thielen and Schiller, 1994), and annotations based on phrase structure grammar, enhanced by a level of predicate-argument structure.",
        "Despite all the similarities presented above, the constituent annotations differ in four important aspects: 1) Tiger does not allow for unary branching whereas TuBa-D/Z does; 2) in Tiger, phrase internal annotation is flat whereas TuBa-D/Z uses phrase internal structure; 3) Tiger uses crossing branches to represent long-distance relationships whereas TuBa-D/Z uses a pure tree structure combined with functional labels to encode this information.",
        "The two treebanks also use different notions of grammatical functions: TuBa-D/Z defines 36 grammatical functions covering head and non-head information, as well as subcategorization for complements and modifiers.",
        "Tiger utilizes 51 grammatical functions.",
        "Apart from commonly accepted grammatical functions, such as SB (subject) or OA (accusative object), Tiger grammatical functions in-",
        "Figure 1: Tiger annotation with crossing branches.",
        "Figure 2: Tiger annotation with resolved crossing branches.",
        "clude others, e.g. RE (repeated element) or RC (relative clause).",
        "(7) Beim Munchner Gipfel ist die At the Munich Summit is the sprichwörtliche bayerische Gemutlichkeit proverbial Bavarian 'Gemutlichkeit' von einem Bild verdrangt worden, das by a picture supplanted been, which im Wortsinne an einen Polizeistaat in the literal sense of a police state erinnert.",
        "reminds 'At the Munich Summit, the proverbial Bavarian 'Gemutlichkeit' was supplanted by an image that is evocative of a police state.'",
        "Figure 1 shows a typical tree from the Tiger treebank for sentence (7).",
        "The syntactic categories are shown in circular nodes, the grammatical functions as edge labels in square boxes.",
        "A major phrasal category that serves to structure the sentence as a whole is the verb phrase (VP).",
        "It contains non-finite verbs (here: verdrangt worden) as well as their complements and adjuncts.",
        "The subject NP (die sprichwörtliche bayerische Gemütlichkeit) is outside the VP and, depending on its linear position, leads to crossing branches with the VP.",
        "This happens in all cases where the subject follows the finite verb as in Figure 1.",
        "Notice also that the PPs are completely flat.",
        "An additional crossing branch results from the direct attachment of the extraposed relative clause (the lower S node with function RC) to the noun that it modifies.",
        "As mentioned in the previous section, Tiger trees must be transformed into trees without crossing branches prior to training PCFG parsers.",
        "The standard approach for this transformation is to reattach crossing non-head constituents as sisters of the lowest mother node that dominates all the crossing constituent and its sister nodes in the original Tiger tree.",
        "Figure 2 shows the result of this transformation",
        "Figure 3: TüBa-D/Z annotation without crossing branches.",
        "of the tree in Figure 1.",
        "Crossing branches not only arise with respect to the subject at the sentence level but also in cases of extraposition and fronting of partial constituents.",
        "As a result, approximately 30% of all Tiger trees contain at least one crossing branch.",
        "Thus, tree transformations have a major impact on the type of constituent structures that are used for training probabilistic parsing models.",
        "Figure 3 shows the TuBa-D/Z annotation for sentence (8), a sentence with a very similar structure to the Tiger sentence shown in Figure 1.",
        "Crossing branches are avoided by the introduction of topological structures (here: VF, LK, MF, VC, NF, and C) into the tree.",
        "Notice also that compared to the Tiger annotation, TuBa-D/Z introduces more internal structure into NPs and PPs.",
        "In TuBa-D/Z, longdistance relationships are represented by a pure tree structure and specific functional labels.",
        "Thus, the extraposed relative clause is attached to the matrix clause directly, but its functional label ON-MOD explicates that it modifies the subject ON.",
        "(8) In Bremen sind bisher nur Fakten geschaffen in Bremen are so far only facts produced worden, die jeder modernen Stadtplanung been, which any modern city planning entgegenstehen.",
        "contradict 'In Bremen, so far only such attempts have been made that are opposed to any modern city planning.'",
        "The constituent representations from both treebanks were converted into dependencies.",
        "The conversion aimed at finding dependency representations for both treebanks that are as similar to each other as possible.",
        "Complete identity is impossible because the treebanks contain different levels of distinction for different phenomena.",
        "The conversion is based on the original formats of the treebanks including crossing branches.",
        "The target dependency format was defined based on the dependency grammar by Foth (2003).",
        "For the conversion, we used pre-existing dependency converters for Tiger trees (Daum et al., 2004) and for TUBa-D/Z trees (Vers-ley, 2005).",
        "The dependency representations of the trees in Figures 1 and 3 are shown in Figures 4 and 5.",
        "Note that the long-distance relationships are converted into non-projective dependencies."
      ]
    },
    {
      "heading": "5. Submissions and Results",
      "text": [
        "The shared task drew submissions from 3 groups: the Berkeley group, the Stanford group, and the Vaxjo group.",
        "Four more groups or individuals had registered but did not submit any data.",
        "The submitted systems and results are described in detail in papers in this volume (Petrov and Klein, 2008; Raf-ferty and Manning, 2008; Hall and Nivre, 2008).",
        "All three systems submitted results for the constituent task.",
        "For the dependency task, the Vaxjo group had the only submission.",
        "For this reason, we will concentrate on the analysis of the constituent results and will mention the dependency results only shortly.",
        "Beim M. Gipfel ist die sprichw.",
        "bayer.",
        "Gem.",
        "von einem Bild verdrängt worden, das im Worts, an einen P.staat erinnert.",
        "Figure 4: Tiger dependency annotation.",
        "SUBJ ADV",
        "In Bremen sind bisher nur Fakten geschaffen worden, die jeder modernen Stadtplanung entgegenstehen.",
        "The results of the constituent analysis are shown in Table 1.",
        "The evaluation was performed with regard to labels consisting of a combination of syntactic labels and grammatical functions.",
        "A subject noun phrase, for example, is only counted as correct if it has the correct yield, the correct label (i.e. NP for Tiger and NX for TuBa-D/Z), and the correct grammatical function (i.e. SB for Tiger and ON for TuBa-D/Z).",
        "The results show that the Berkeley parser reaches the best results for both treebanks.",
        "The other two parsers compete for second place.",
        "For Tiger, the Vaxjo parser outperforms the Stanford parser, but for TuBa-D/Z, the situation is reversed.",
        "This gives an indication that the Vaxjo parser seems better suited for the flat annotations in Tiger while the Stanford parser is better suited for the more hierarchical structure in TuBa-D/Z.",
        "Note that all parsers reach much higher F-scores for TuBa-D/Z.",
        "A comparison of how well suited two different annotation schemes are for parsing is a surprisingly difficult task.",
        "A first approach would be to compare the parser performance for specific categories, such as for noun phrases, etc.",
        "However, this is not possible for Tiger and TuBa-D/Z.",
        "On the one hand, the range of phenomena described as noun phrases, for example, is different in the two treebanks.",
        "The most obvious difference in annotation schemes is that TuBa-D/Z annotates unary branching structures while Tiger does not.",
        "As a consequence, in TuBa-D/Z, all pronouns and substituting demonstratives are annotated as noun phrases; in Tiger, they are attached directly to the next higher node (cf. the relative pronouns, POS tag PRELS, in Figures 1 and 3).",
        "Kubler (2005) and Maier (2006) suggest a method for comparing such different annotation schemes by approximating them stepwise so that the decisions which result in major changes can be isolated.",
        "They come to the conclusion that the differences between the two annotation schemes is a least partially due to inconsistencies introduced into Tiger style annotations during the resolution of crossing branches.",
        "However, even this method cannot give any indication which annotation scheme provides more useful information for systems that use such parses as input.",
        "To answer this question, an in vivo evaluation would be necessary.",
        "It is, however, rather difficult to find systems into which a parser can be plugged in without too many modifications of the system.",
        "On the other hand, it is a well-known fact that the PARSEVAL measures favor annotation schemes with hierarchical structures, such as in TuBa-D/Z, in comparison to annotation schemes with flat structures (Rehbein and van Genabith, 2007).",
        "Here, Tiger and TuBa-D/Z differ significantly: in Tiger, phrases receive a flat annotation.",
        "Prepositional phrases, for example, do not contain an explicitly annotated noun phrase.",
        "TuBa-D/Z phrases, in contrast, are more hierarchical; preposition phrases do contain a noun phrase, and non phrases distinguish between pre-and post-modification.",
        "For this reason, the evaluation presented in Table 1 must be taken with more than a grain of salt as a comparison of annotation schemes.",
        "However, it seems safe to follow Kubler et al.",
        "(Kubler et al., 2006) in the assumption that the major grammatical functions, subject (SB/ON), accusative object (OA), and dative object (DA/OD) are comparable.",
        "Again, this is not completely true because in the case of one-word NPs, these functions are attached to the POS tags and thus are given in the input.",
        "Another solution, which was pursued by Rehbein and van Genabith (2007), is the introduction of new unary branching nodes in the tree in cases where such grammatical functions are originally attached to the POS tag.",
        "We refrained from using this solution because it introduces further inconsistencies (only a subset of unary branching nodes are explicitly annotated), which make it difficult for a parser to decide whether to group such phrases or not.",
        "The evaluation shown in Table 2 is based on all nodes which were annotated with the grammatical function in question.",
        "PN 1",
        "ATTR",
        "n",
        "The results presented in Table 2 show that the differences between the two treebanks are inconclusive.",
        "While the Stanford parser performs consistently better on TuBa-D/Z, the Berkeley parser handles accusative objects better in Tiger, and the Vaxjo parser subjects and dative objects.",
        "The results indicate that the Berkeley parser profits from the Tiger annotation of accusative objects, which are grouped in the verb phrase while TuBa-D/Z groups all objects in their fields directly without resorting to a verb phrase.",
        "However, this does not explain why the Berkeley parser cannot profit from the subject attachment on the clause level in Tiger to the same degree.",
        "The results of the dependency evaluation for the Vaxjo system are shown in Table 3.",
        "The results are important for the comparison of constituent and dependency parsing since in the conversion to dependencies, most of the differences between the annotation schemes, and as a consequence, the preference of the PARSEVAL measures have been neutralized.",
        "Therefore, it is interesting to see that the results for Tiger are slightly better than the results for TuBa-D/Z, both for unlabeled (UAS) and labeled attachment scores.",
        "The reasons for these differences are unclear: either the Tiger texts are easier to parse, or the (original annotation and) conversion from Tiger is more consistent.",
        "Another surprising fact is that the dependency results are clearly better than the constituent ones.",
        "This is partly due to the fact that the dependency representation is often less informative than then constituent representation.",
        "One example for this can be found in coordinations: in dependency representations, the scope ambiguity in phrases like young men and women is not resolved.",
        "This gives parsers fewer opportunities to go wrong.",
        "However, this cannot explain all the differences.",
        "Especially the better performance on the major grammatical functions cannot be explained in this way.",
        "Tiger",
        "TüBa-D/Z",
        "system",
        "precision",
        "recall",
        "F-score",
        "precision",
        "recall",
        "F-score",
        "Berkeley",
        "69.23",
        "70.41",
        "69.81",
        "83.91",
        "84.04",
        "83.97",
        "Stanford",
        "58.52",
        "57.63",
        "58.07",
        "79.26",
        "79.22",
        "79.24",
        "Växjö",
        "67.06",
        "63.40",
        "65.18",
        "76.44",
        "74.79",
        "75.60",
        "Tiger",
        "TüBa-D/Z",
        "system",
        "GF",
        "precision",
        "recall",
        "F-score",
        "precision",
        "recall",
        "F-score",
        "Berkeley",
        "SB/ON",
        "74.46",
        "78.31",
        "76.34",
        "78.33",
        "77.08",
        "77.70",
        "OA",
        "60.08",
        "66.61",
        "63.18",
        "58.11",
        "65.81",
        "61.72",
        "DA/OD",
        "49.28",
        "41.72",
        "43.19",
        "59.46",
        "44.72",
        "51.05",
        "Stanford",
        "SB/ON",
        "64.40",
        "63.11",
        "63.75",
        "71.16",
        "77.76",
        "74.31",
        "OA",
        "45.52",
        "45.91",
        "45.71",
        "47.23",
        "51.28",
        "49.17",
        "DA/OD",
        "12.40",
        "9.82",
        "10.96",
        "24.42",
        "8.54",
        "12.65",
        "Växjö",
        "SB/ON",
        "75.33",
        "73.00",
        "74.15",
        "72.37",
        "69.53",
        "70.92",
        "OA",
        "57.01",
        "57.65",
        "57.33",
        "58.07",
        "57.55",
        "57.81",
        "DA/OD",
        "55.45",
        "37.42",
        "44.68",
        "63.75",
        "20.73",
        "31.29",
        "A closer look at the grammatical functions shows that here, precision and recall are higher than for constituent parses.",
        "This is a first indication that dependency representation may be more appropriate for languages with freer word order.",
        "A comparison between the two treebanks is inconclusive: for the accusative object, the results are similar between the treebanks.",
        "For subjects, the results for Tiger are better while for dative objects, the results for TuBa-D/Z are better.",
        "This issue requires closer investigation."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "This is the first shared task on parsing German, which provides training and test sets from both major treebanks for German, Tiger and TuBa-D/Z.",
        "For both treebanks, we provided a constituent and a dependency representation.",
        "it is our hope that these data sets will spark more interest in the comparison of different annotation schemes and their influence on parsing results.",
        "The evaluation of the three participating systems has shown that for both treebanks, the use of a latent variable grammar in the Berkeley system is beneficial.",
        "However, many questions remain unanswered and require further investigation: To what extent do the evaluation metrics distort the results?",
        "Does a measure exist that is neutral towards the differences in annotation?",
        "Is the dependency format better suited for parsing German?",
        "Are the differences between the dependency results of the two treebanks indicators that Tiger provides more important information for dependency parsing?",
        "Or can the differences be traced back to the conversion algorithms?"
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "First and foremost, we want to thank all the people and organizations that generously provided us with treebank data and without whom the shared task would have been literally impossible: Erhard Hin-richs, University of Tubingen (TuBa-D/Z), and Hans Uszkoreit, Saarland University and DFKI (Tiger).",
        "Secondly, we would like to thank Wolfgang Maier and Yannick Versley who performed the data conversions necessary for the shared task.",
        "Additionally, Wolfgang provided the scripts for the constituent evaluation.",
        "Tiger",
        "TuBa-D/Z",
        "UAS",
        "92.63",
        "91.45",
        "LAS",
        "90.80",
        "88.64",
        "precision recall",
        "precision recall",
        "SUBJ",
        "90.20 89.82",
        "88.99 88.55",
        "OBJA",
        "77.93 82.19",
        "77.18 82.71",
        "OBJD",
        "57.00 44.02",
        "67.88 45.90"
      ]
    }
  ]
}
