{
  "info": {
    "authors": [
      "Kenichi Ichioka",
      "Fumiyo Fukumoto"
    ],
    "book": "Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications",
    "id": "acl-W08-2005",
    "title": "Graph-Based Clustering for Semantic Classification of Onomatopoetic Words",
    "url": "https://aclweb.org/anthology/W08-2005",
    "year": 2008
  },
  "references": [
    "acl-C00-1026",
    "acl-C02-1114",
    "acl-C04-1036",
    "acl-C90-1025",
    "acl-H05-1052",
    "acl-J05-4002",
    "acl-J90-1003",
    "acl-P98-2127",
    "acl-P99-1004",
    "acl-W06-1664",
    "acl-W06-3811",
    "acl-W07-2086"
  ],
  "sections": [
    {
      "text": [
        "Graph-based Clustering for Semantic Classification of Onomatopoetic",
        "Words",
        "This paper presents a method for semantic classification of onomatopoetic words like llO<<9>~ 0<<9>~ (hum)\" and \"fr> £> h C ~h A (clip clop)\" which exist in every language, especially Japanese being rich in onomatopoetic words.",
        "We used a graph-based clustering algorithm called Newman clustering.",
        "The algorithm calculates a simple quality function to test whether a particular division is meaningful.",
        "The quality function is calculated based on the weights of edges between nodes.",
        "We combined two different similarity measures, distributional similarity, and orthographic similarity to calculate weights.",
        "The results obtained by using the Web data showed a 9.0% improvement over the baseline single distributional similarity measure."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Onomatopoeia which we call onomatopoetic word (ono word) is the formation of words whose sound is imitative of the sound of the noise or action designated, such as 'hiss' (McLeod, 1991).",
        "It is one of the linguistic features of Japanese.",
        "Consider two sentences from Japanese.",
        "\"I m too sleepy because I awoke to the slippers in the hall.\"",
        "©2008.",
        "Some rights reserved.",
        "(2) mzBT* IXtztftzMZX V y^og-eiB \"I'm too sleepy because I awoke to the pit-a-pat of slippers in the hall.\"",
        "Sentences (1) and (2) are almost the same sense.",
        "However, sentence (2) which includes ono word, \"HtzUtz (pit-a-pat)\" is much better to make the scene alive, or represents an image clearly.",
        "Therefore large-scale semantic resource of ono words is indispensable for not only NLP, but also many semantic-oriented applications such as Question Answering, Paraphrasing, and MT systems.",
        "Although several machine-readable dictionaries which are fine-grained and large-scale semantic knowledge like WordNet, COMLEX, and EDR dictionary exist, there are none or few onomatopoetic thesaurus.",
        "Because (i) it is easy to understand its sense of ono word for Japanese, and (ii) it is a fast-changing linguistic expressions, as it is a vogue word.",
        "Therefore, considering this resource scarcity problem, semantic classiication of ono words which do not appear in the resource but appear in corpora is very important.",
        "In this paper, we focus on Japanese onomatopo-etic words, and propose a method for classifying them into a set with similar meaning.",
        "We used the Web as a corpus to collect ono words, as they appear in different genres of dialogues including broadcast news, novels and comics, rather than a well-edited, balanced corpus like newspaper articles.",
        "The problem using a large, heterogeneous collection of Web data is that the Web counts are far more noisy than counts obtained from textual corpus.",
        "We thus used a graph-based clustering algorithm, called Newman clustering for classifying ono words.",
        "The algorithm does not simply calculate the number of shortest paths between pairs of nodes, but instead calculates a quality function of how good a cluster structure found by an algorithm is, and thus makes the computation far more efficient.",
        "The efficacy of the algorithm depends on a quality function which is calculated by using the weights of edges between nodes.",
        "We combined two different similarity measures, and used them to calculate weights.",
        "One is co-occurrence based distributional similarity measure.",
        "We tested mutual information (MI) and a % statistic as a similarity measure.",
        "Another is orthographic similarity which is based on a feature of ono words called \"sound symbolism\".",
        "Sound symbolism indicates that phonemes or phonetic sequences express their senses.",
        "As ono words imitate the sounds associated with the objects or actions they refer to, their phonetic sequences provide semantic clues for classiication.",
        "The empirical results are encouraging, and showed a 9.0% improvement over the baseline single distributional similarity measure."
      ]
    },
    {
      "heading": "2. Previous Work",
      "text": [
        "There are quite a lot of work on semantic classii-cation of words with corpus-based approach.",
        "The earliest work in this direction are those of (Hindle, 1990), (Lin, 1998), (Dagan et al., 1999), (Chen and Chen, 2000), (Geffet and Dagan, 2004) and (Weeds and Weir, 2005).",
        "They used distributional similarity.",
        "Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words.",
        "Features typically correspond to other words that co-occur with the characterized word in the same context.",
        "Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus.",
        "He compared the result of automatically created thesaurus with WordNet and Roget, and reported that the result was signiicantly closer to WordNet than Roget Thesaurus was.",
        "Graph representations for word similarity have also been proposed by several researchers (Jan-nink and Wiederhold, 1999; Galley and McKeown, 2003; Muller et al., 2006).",
        "Sinha and Mihalcea (2007) proposed a graph-based algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik's metric (Resnik, 1995), and algorithms for graph centrality.",
        "They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 – 8% as compared to the previous work (Mihalcea, 2005).",
        "In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition.",
        "The graph structure is built by linking pairs of words which participate in particular syntactic relationships.",
        "An incremental cluster-building algorithm using the graph structure achieved 82% accuracy at a lexical acquisition task, evaluated against WordNet 10 classes, and each class consists of 20 words.",
        "Matsuo et al.",
        "(2006) proposed a method of word clustering based on a word similarity measure by Web counts.",
        "They used Newman clustering for clustering algorithm.",
        "They evaluated their method using two sets of word classes.",
        "One is derived from the Web data, and another is from WordNet.",
        "Each set consists of 90 noun words.",
        "They reported that the results obtained by Newman clustering were better than those obtained by average-link agglomerative clustering.",
        "Our work is similar to their method in the use of Newman clustering.",
        "However, they classiied Japanese noun words, while our work is the irst to aim at detecting semantic classiication of onomatopoetic words.",
        "Moreover, they used only a single similarity metric, cooccurrence based similarity, while Japanese, especially \"kanji\" characters of noun words provide semantic clues for classifying words."
      ]
    },
    {
      "heading": "3. System Description",
      "text": [
        "The method consists of three steps: retrieving cooccurrences using the Web, calculating similarity between ono words, and classifying ono words by using Newman clustering.",
        "One criterion for calculating semantic similarity between ono words is co-occurrence based similarity.",
        "We retrieved frequency of two ono words occurring together by using the Web search engine, Google.",
        "The similarity between them is calculated based on their co-occurrence frequency.",
        "Like much previous work on semantic classiication of the lexicons, our assumption is that semantically similar words appear in similar contexts.",
        "A lot of strategies for searching words are provided in Google.",
        "Of these we focused on two methods: Boolean search AND and phrase-based search.",
        "When we use AND boolean search, i.e., (Oi Oj) where Oi and Oj are ono words, we can retrieve the number of documents which include both Oiand Oj.",
        "In contrast, phrase-based search, i.e., (\"Oi Oj\") retrieves documents which include two adjacent words Oi and Oj.",
        "The second step is to calculate semantic similarity between ono words.",
        "We combined two different similarity measures: the co-occurrence frequency based similarity and orthographic similarity measures.",
        "We focused on two popular measures: the mutual information (MI) and % statistics.",
        "Orthographic similarity has been widely used in spell checking and speech recognition systems (Damerau, 1964).",
        "Our orthographic similarity measure is based on a unit of phonetic sequence.",
        "The key steps of the similarity between two ono words is deined as:",
        "1.",
        "Convert each ono word into phonetic sequences.",
        "The \"hiragana\" characters of ono word are converted into phonetic sequences by a unique rule.",
        "Basically, there are 19 consonants and 5 vowels, as listed in Table 1.",
        "Consider phonetic sequences \"hyu-hyu-\" of ono word \"O-ty – 'O-ty – \" (hum).",
        "It is segmented into 4 consonants \"hy\", \"-\", \"hy\" and \"-\", and two vowels, \"u\" and \"u\".",
        "2.",
        "Form a vector in n-dimensional space.",
        "Each ono word is represented as a vector of consonants(vowels), where each dimension of the vector corresponds to each consonant and vowel, and each value of the dimension is frequencies of its corresponding consonant(vowel).",
        "3.",
        "Calculate orthographic similarity.",
        "The orthographic similarity between ono words, Oi and Oj is calculated based on the consonant and vowel distributions.",
        "We used two popular measures, i.e., the cosine similarity, and askew divergence.",
        "The cosine measures the similarity of the two vectors by calculating the cosine of the angle between vectors.",
        "a-skew divergence is defined as:",
        "where D(x||y) refers to Kullback-Leibler and deined as:",
        "Consonant",
        "-, N, Q, h, hy, k, ky, m, my, n,",
        "ny, r, ry, s, sy, t, ty, w, y",
        "Vowel",
        "a, i, u, e, o",
        "Lee (1999) reported the best results with a = 0.9.",
        "We used the same value.",
        "We defined a similarity metric by combining co-occurrence based and orthographic similarity measures:"
      ]
    },
    {
      "heading": "1.. Mutual Information",
      "text": [
        "Church and Hanks (1990) discussed the use of the mutual information statistics as a way to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence preferences between verbs and prepositions (content word/function word).",
        "Let Oi and Oj be ono words retrieved from the Web.",
        "The mutual information MI (Oi, Oj) is defined as:",
        "In Eq.",
        "(1), f (Oi, Oj) refers to the frequency of Oi and Oj occurring together, and Oa« is a set of all ono words retrieved from the Web."
      ]
    },
    {
      "heading": "2.. x 2 statistic",
      "text": [
        "The x(Oi, Oj) is defined as:",
        "Soi and Sau in Eq.",
        "(5) refer to Eq.",
        "(2) and (3), respectively.",
        "A major difference between x and MI is that the former is a normalized value.",
        "We classified ono words collected from the WWW.",
        "Therefore, the clustering algorithm should be efficient and effective even in the very high dimensional spaces.",
        "For this purpose, we chose a graph-based clustering algorithm, called Newman clustering.",
        "The Newman clustering is a hierarchical clustering algorithm which is based on Network structure (Newman, 2004).",
        "The network structure consists of nodes within which the node-node connections are edges.",
        "It produces some division of the nodes into communities, regardless of whether the network structure has any natural such division.",
        "Here, \"community\" or \"cluster\" have in common that they are groups of densely interconnected nodes that are only sparsely connected with the rest of the network.",
        "To test whether a particular division is meaningful a quality function Q is defined:",
        "where eij is the sum of the weight of edges between two communities i and j divided by the sum of the weight of all edges, and ai = J2j eij, i.e., the expected fraction of edges within the cluster.",
        "Here are the key steps of that algorithm:",
        "1.",
        "Given a set of n ono words S = {Oi, • • •, On}.",
        "Create a network structure which consists of nodes Oi, • • •, On, and edges.",
        "Here, the weight of an edge between Oi and Ojis a similarity value obtained by Eq.",
        "(7).",
        "If the \"network density\" of ono words is smaller than the parameter 9, we cut the edge.",
        "Here, \"network density\" refers to a ratio selected from the topmost edges.",
        "For example, if it was 0.9, we used the topmost 90% of all edges and cut the remains, where edges are sorted in the descending order of their similarity values.",
        "2.",
        "Starting with a state in which each ono word is the sole member of one of n communities, we repeatedly joined communities together in pairs, choosing at each step the join that results in the greatest increase.",
        "3.",
        "Suppose that two communities are merged into one by a join operation.",
        "The change in Q upon joining two communities i and j is given by:",
        "4.",
        "Apply step 3. to every pair of communities.",
        "5.",
        "Join two communities such that AQ is maximum and create one community.",
        "If AQ < 0, go to step 7.",
        "6.",
        "Re-calculate eij and ai of the joined community, and go to step 3.",
        "7.",
        "Words within the same community are regarded as semantically similar.",
        "The computational cost of the algorithm is known as O((m + n)n) or O(n), where m and n are the number of edges and nodes, respectively."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "The data for the classiication of ono words have been taken from the Japanese ono dictionary (Ono, 2007) that consisted of 4,500 words.",
        "Of these, we selected 273 words, which occurred at least 5,000 in the document URLs from the WWW.",
        "The minimum frequency of a word was found to be 5,220, while the maximum was about 26 million.",
        "These words are classiied into 10 classes.",
        "Word classes and examples of ono words from the dictionary are listed in Table 2.",
        "\"Id\" denotes id number of each class.",
        "\"Sense\" refers to each sense of ono word within the same class, and \"Num\" is the number of words which should be assigned to each class.",
        "Each word marked with bracket denotes phonetic sequences consisting of consonants and vowels.",
        "We retrieved co-occurrences of ono words shown in Table 2 using the search engine, Google.",
        "We applied Newman clustering to the input words.",
        "For comparison, we implemented standard k-means which is often used as a baseline, as it is one of the simplest unsupervised clustering algorithms, and compared the results to those obtained by our method.",
        "We used Euclidean distance (L norm) as a distance metric used in the fc-means.",
        "For evaluation of classification, we used Precision(Prec), Recall(Rec), and F-measure which is a measure that balances precision and recall (Bilenko et al., 2004).",
        "The precise definitions of these measures are given below:",
        "p jfPairsG'orrectlyPredictedlnS amecluster #TotalPairsPredictedInSameCluster P jfPairsG'orrectlyPredictedlnS ameC'luster #T otalPairsInSameCluster",
        "The results are shown in Table 3.",
        "\"Co-occ.",
        "& Sounds\" in Data refers to the results obtained by our method.",
        "\"Co-occ.\"",
        "denotes the results obtained by a single measure, co-occurrence based distributional similarity measure, and \"Sounds\" shows the results obtained by orthographic similarity.",
        "\"9\" in Table 3 shows a parameter 9 used in the Newman clustering.",
        "Table 3 shows best performance of each method against 9 values.",
        "The best result was obtained when we used phrase-based search and a combined measure of co-occurrence(MI) and sounds (cos), and F-score was 0.451.",
        "Table 3 shows that overall the results using phrase-based search were better than those of AND search, and the maximum difference of F-score between them was 20.6% when we used a combined measure.",
        "We note that AND boolean search did not consider the position of a word in a document, while our assumption was that semantically similar words appeared in similar contexts.",
        "As a result, two ono words which were not semantically similar were often retrieved by AND boolean search.",
        "For example, consider two antonymous words, \"a,h,a,h,a\" (grinning broadly) and \"w,a,-,N\" (Wah, Wah).",
        "The co-occurrence frequency obtained by AND was 5,640, while that of phrase-based search was only one.",
        "The observation shows that we ind phrase-based search to be a good choice.",
        "Id",
        "Sense",
        "Num",
        "Onomatopoetic words",
        "1",
        "laugh",
        "63",
        "foi liili (a,Q,h,a,Q,h,a), folili (a,h,a,h,a), t>lili (w,a,h,a,h,a) hlihli (a,h,a,a,h,a), V>HO (i,h,i,h,i), \"5 3 L 3 L (u,Q,s,i,Q,s,i), • • •",
        "2",
        "cry",
        "34",
        "$> – h (a,-N), \"5 fc> – ki (u,w,a,-N), ^6^^6^ (a,N,a,N), (e,N,e,N) \"5 2> 9 2> (u,r,u,u,r,u), \"5 2> 2> ^ (u,r,u,r,u,N), 9 2> -3(u,r,u,Q), x.",
        " – ^ (e,-,N), • • •",
        "3",
        "pain",
        "34",
        "V^îV^î (i,k,a,i,k,a), O 0 O <0 (h,i,r,i,h,i,r,i), (k,a,s,i,k,a,s,i) Wh&k, (k,a,N,k,a,N), • • •",
        "4",
        "anger",
        "33",
        "Ä>--3(k,a,-Q), ip%hi (k,a,t,i,N), (k,a,t,u,N), Ä>-3(k,a,Q), Ä>-3^ (k,a,Q,k,a), tßfytßfy (k,a,m,i,k,a,m,i), frty frï) (k,a,r,i,k,a,r,i), frhfrk, (k,a,N,k,a,N), • • •",
        "5",
        "spook",
        "31",
        "$>t)t> (a,w,a,w,a), \"5 tf-Q – (u,ky,a,-), ^ (k,a,-N), < (k,i,k,u) < -3(k,i,k,u,Q), < t) (k,i,k,u,r,i), < ^ (k,i,k,u,N), • • •",
        "6",
        "panic",
        "25",
        "(a,k,u,s,e,k,u), hfc^fc (a,t,a,h,u,t,a), J>-3J>-3(a,Q,h,u,a,Q,h,u), $>fo$>fo (a,w,a,a,w,a)- • •",
        "7",
        "bloodless",
        "27",
        "Ä>< o(k,a,k,u,Q), o(k,a,k,u,Q), £*o£>t> (k,a,Q,k,a,r,i), < 0 (k,a,Q,k,u,r,i) < ^ (k,a,k,u,N), (ky,a,h,u,N), – (ky,u,-), • • •",
        "8",
        "deem",
        "13",
        "\"5 3 t 0 (u,Q,t,o,r,i), # v9> – A/ (ky,u,-,N), # v9> A/ (ky,u,N) 5 < < (t,u,k,u,t,u,k,u), • • •",
        "9",
        "feel delight",
        "6",
        "HH (u,s,i,u,s,i), § ^O\"^ ^rJ (ky,a,h,i,ky,a,h,i)",
        "î li î li (u-,h,a-,u,-h,a), fSWSl^ (h,o,i,h,o,i), 5^2>^ (r,u,N,r,u,N), • • •",
        "10",
        "balk",
        "7",
        "V^UV^U (i,s,i,i,s,i), (u,s,i,u,s,i), &TfcT (o,s,u,o,s,u) C/c C/c (k,u,t,a,k,u,t,a), % U % U (m,o,s,i,m,o,s,i), • • •",
        "Total",
        "273",
        "To examine the effectiveness of the combined similarity measure, we used a single measure as a quality function of the Newman clustering, and compared these results with those obtained by our method.",
        "As shown in Table 3, the results with combining similarity measures improved overall performance.",
        "In the phrase-based search, for example, the F-score using a combined measure \"Co-occ(MI) & Sounds(cos)\" was 23.8% better than the baseline single measure \"Sounds(cos)\", and 9.0% better a single measure \"Co-occ(MI)\".",
        "Figure 1 shows F-score by \"Co-occ(MI) & Sounds(cos)\" and \"Co-occ(MI)\" against changes in 9.",
        "These curves were obtained by phrase-based search.",
        "We can see from Figure 1 that the F-score by a combined measure \"Co-occ(MI) & Sounds(cos)\" was better than \"Co-occ(MI)\" with 9 value ranged from .001 to .25.",
        "One possible reason for the difference of F-score between them is the edges selected by varying 9.",
        "Figure 2 shows the results obtained by each single measure, and a combined measure to examine how the edges selected by varying 9 affect overall performance, F-measure.",
        "\"Precision\" in Figure 2 refers to the ratio of correct ono word pairs (edges) divided by the total number of edges.",
        "Here, correct ono word pairs were created by using the Japanese ono dictionary, i.e., we extracted word pairs within the same sense of the dictionary.",
        "Surprisingly, there were no significant difference between a combined measure \"Co-occ(MI) & Sounds(cos)\" and a single measure \"Co-occ(MI)\" curves, while the precision of a single measure \"Sounds\" was constantly worse than that obtained by a combined measure.",
        "Another possible reason for the difference of F-score is due to product of MI and Cos in Eq.",
        "(7).",
        "Further work is needed to analyze these results in detail.",
        "We examined the results obtained by standard fc-means and Newman clustering algorithms.",
        "As can be seen clearly from Table 3, the results with Newman clustering were better than those of the standard fc-means at all search and similarity measures, especially the result obtained by Newman clustering showed a 16.2 % improvement over the fc-means when we used Co-occ.",
        "(MI) & Sounds(cos) & phrase-based search.",
        "We recall that we used 273 ono words for clustering.",
        "However, Newman clustering is applicable for a large number of nodes and edges without decreasing accuracy too much, as it does not simply calculate the number of shortest paths between pairs of nodes, but instead calculates a simple quality function.",
        "Quantitative evaluation by applying the method to larger data from the Web is worth trying for future work.",
        "Data",
        "Algo.",
        "Sim (Co-occ.)",
        "Sim (Sounds)",
        "Search method",
        "e",
        "Prec",
        "Ree",
        "F",
        "# of clusters",
        "Co-occ.",
        "& Sounds",
        "fc-means",
        "x",
        "cos",
        "AND",
        ".050",
        ".134",
        ".799",
        ".229",
        "10",
        "Phrase",
        ".820",
        ".137",
        ".880",
        ".236",
        "10",
        "MI",
        "AND",
        ".050",
        ".134",
        ".562",
        ".216",
        "10",
        "Phrase",
        ".150",
        ".190",
        ".618",
        ".289",
        "10",
        "x",
        "adiv",
        "AND",
        ".680",
        ".134",
        ".801",
        ".229",
        "10",
        "Phrase",
        ".280",
        ".138",
        ".882",
        ".238",
        "10",
        "MI",
        "AND",
        ".040",
        ".134",
        ".602",
        ".219",
        "10",
        "Phrase",
        ".140",
        ".181",
        ".677",
        ".285",
        "10",
        "Newman",
        "x",
        "cos",
        "AND",
        ".170",
        ".182",
        ".380",
        ".246",
        "9",
        "Phrase",
        ".100",
        ".322",
        ".288",
        ".304",
        "14",
        "MI",
        "AND",
        ".050",
        ".217",
        ".282",
        ".245",
        "13",
        "Phrase",
        ".080",
        ".397",
        ".520",
        ".451",
        "7",
        "x",
        "adiv",
        "AND",
        ".130",
        ".212",
        ".328",
        ".258",
        "9",
        "Phrase",
        ".090",
        ".414",
        ".298",
        ".347",
        "17",
        "MI",
        "AND",
        ".090",
        ".207",
        ".325",
        ".253",
        "6",
        "Phrase",
        ".160",
        ".372",
        ".473",
        ".417",
        "8",
        "Co-occ.",
        "fc-means",
        "x",
        "-",
        "AND",
        ".460",
        ".138",
        ".644",
        ".227",
        "10",
        "Phrase",
        ".110",
        ".136",
        ".870",
        ".236",
        "10",
        "MI",
        "AND",
        ".040",
        ".134",
        ".599",
        ".219",
        "10",
        "Phrase",
        ".150",
        ".191",
        ".588",
        ".286",
        "10",
        "Newman",
        "x",
        "-",
        "AND",
        ".700",
        ".169",
        ".415",
        ".240",
        "8",
        "Phrase",
        ".190",
        ".301",
        ".273",
        ".286",
        "14",
        "MI",
        "AND",
        ".590",
        ".159",
        ".537",
        ".245",
        "3",
        "Phrase",
        ".140",
        ".275",
        ".527",
        ".361",
        "5",
        "Sounds",
        "fc-means",
        "-",
        "cos",
        "-",
        ".050",
        ".145",
        ".321",
        ".199",
        "10",
        "adiv",
        "-",
        ".020",
        ".126",
        ".545",
        ".204",
        "10",
        "Newman",
        "-",
        "cos",
        "-",
        ".270",
        ".151",
        ".365",
        ".213",
        "4",
        "adiv",
        "-",
        ".350",
        ".138",
        ".408",
        ".206",
        "3",
        "Finally, to provide feedback for further development of our classification approach, we performed a qualitative analysis of errors.",
        "Consider the following clusters (the Newman output for Co-occ.",
        "(MI), Sounds(cos) and phrase-based search), where each parenthetic sequences denotes ono word:",
        "Three main error types were identified:",
        "1.",
        "Morphological idiosyncrasy: This was the most frequent error type, exemplified in A1, where \"(t,o,Q,k,i,N,t,o,Q,k,i,N)\"",
        "(pain sense) was incorrectly clustered with other two words (laugh sense) merely because orthographic similarity between them was large, as the phonetics sequences of \"(t,o,Q,k,i,N,t,o,Q,k,i,N)\" included \"t\" and \"o\".",
        "2.",
        "Sparse data: Many of the low frequency ono words performed poorly.",
        "In A2, \"(o,-,o,-)\" (cry sense) was classified with other three words (laugh sense) because it occurred few in our data.",
        "3.",
        "Problems of polysemy: In A3, \"(m,o,s,o,m,o,s,o)\" (pain sense) was clustered with other two words (balfc sense) of its gold standard class.",
        "However, the ono word has another sense, balfc sense when it co-occurred with action verbs."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We have focused on onomatopoetic words, and proposed a method for classifying them into a set of semantically similar words.",
        "We used a graph-based clustering algorithm, called Newman clustering with a combined different similarity measures.",
        "The results obtained by using the Web data showed a 9.0% improvement over the baseline single distributional similarity measure.",
        "There are number of interesting directions for future research.",
        "The distributional similarity measure we used is the basis of the ono words, while other content words such as verbs and adverbs are also effective for classifying ono words.",
        "In the future, we plan to investigate the use of these words and work on improving the accuracy of classification.",
        "As shown in Table 2, many of the ono words consist of duplicative character sequences such as \"h\" and \"a\" of \"a,h,a,h,a\", and \"h\" and \"i\" of \"i,h,i,h,i\".",
        "Moreover, characters which consist of ono words within the same class match.",
        "For example, the hiragana character \"fi\" (h,a) frequently appears in laugh sense class.",
        "These observations indicate that integrating edit-distance and our current similarity measure will improve overall performance.",
        "Another interesting direction is a problem of polysemy.",
        "It clearly supports the classiication of (Ono, 2007) to insist that some ono words belong to more than one cluster.",
        "For example, \"(i,s,o,i,s,o)\" has at least two senses, panic and feel delight sense.",
        "In order to accommodate this, we should apply an appropriate soft clustering technique (Tishby et al., 1999; Reichardt and Born-holdt, 2006; Zhang et al., 2007)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the anonymous reviewers for their helpful suggestions.",
        "This material is supported in part by the Grant-in-aid for the Japan Society for the Promotion of Science(JSPS)."
      ]
    }
  ]
}
