{
  "info": {
    "authors": [
      "Thai Phuong Nguyen",
      "Akira Shimazu",
      "Tu Bao Ho",
      "Minh Le Nguyen",
      "Vinh Van Nguyen"
    ],
    "book": "CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning",
    "id": "acl-W08-2119",
    "title": "A Tree-to-String Phrase-based Model for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/W08-2119",
    "year": 2008
  },
  "references": [
    "acl-D07-1091",
    "acl-N03-1017",
    "acl-P00-1056",
    "acl-P06-1077",
    "acl-P07-1089"
  ],
  "sections": [
    {
      "text": [
        "College of Technology Vietnam National University, Hanoi thainpOvnu.edu.vn Akira Shimazu, Tu-Bao Ho, Minh Le Nguyen, and Vinh Van Nguyen",
        "Though phrase-based SMT has achieved high translation quality, it still lacks of generalization ability to capture word order differences between languages.",
        "In this paper we describe a general method for tree-to-string phrase-based SMT.",
        "We study how syntactic transformation is incorporated into phrase-based SMT and its effectiveness.",
        "We design syntactic transformation models using unlexicalized form of synchronous context-free grammars.",
        "These models can be learned from source-parsed bitext.",
        "Our system can naturally make use of both constituent and non-constituent phrasal translations in the decoding phase.",
        "We considered various levels of syntactic analysis ranging from chunking to full parsing.",
        "Our experimental results of English-Japanese and English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Based on the kind of linguistic information which is made use of, syntactic SMT can be divided into four types: tree-to-string, string-to-tree, tree-to-tree, and hierarchical phrase-based.",
        "The tree-to-string approach (Collins et al., 2005; Nguyen and Shimazu, 2006; Liu et al., 2006 and 2007) supposes that syntax of the source language is known.",
        "This approach can be applied when a source language parser is available.",
        "The string-to-tree approach (Yamada and Knight, 2001; Galley et al., 2006) focuses on syntactic modelling of the target language in cases it has syntactic resources such as treebanks and parsers.",
        "The tree-to-tree approach models the syntax of both languages, therefore extra cost is required.",
        "The fourth approach (Chiang, 2005) constraints phrases under context-free grammar structure without any requirement of linguistic annotation.",
        "In this paper, we present a tree-to-string phrase-based method which is based on synchronous CFGs.",
        "This method has two important properties: syntactic transformation is used in the decoding phase including a word-to-phrase tree transformation model and a phrase reordering model; phrases are the basic unit of translation.",
        "Since we design syntactic transformation models using un-lexicalized synchronous CFGs, the number of rules is small.",
        "Previous studies on tree-to-string SMT are different from ours.",
        "Collins et al.",
        "Collins et al.",
        "(2005) used hand crafted rules to carry out word reordering in the preprocessing phase but not decoding phase.",
        "Nguyen and Shimazu (2006) presented a more general method in which lexicalized syntactic reordering models based on PCFGs can be learned from source-parsed bitext and then applied in the preprocessing phase.",
        "Liuetal.",
        "(2006) changed the translation unit from phrases to tree-to-string alignment templates (TATs) while we do not.",
        "TATs was represented as xRs rules while we use synchronous CFG rules.",
        "In order to overcome the limitation that TATs can not capture non-constituent phrasal translations, Liu et al.",
        "(2007) proposed forest-to-string rules while our system can naturally make use of such kind of phrasal translation by word-to-phrase tree transformation.",
        "We carried out experiments with two language pairs English-Japanese and English-Vietnamese.",
        "Our system achieved significant improvements over Pharaoh, a state-of-the-art phrase-based SMT system.",
        "We also analyzed the dependence of translation quality on the level of syntactic analysis (shallow or deep).",
        "Figure 1 shows the architecture of our system.",
        "The input of this system is a source-language tree and the output is a target-language string.",
        "This system uses all features of conventional phrase-based SMT as in (Koehn et al., 2003).",
        "There are two new features including a word-to-phrase tree transformation model and a phrase reordering model.",
        "The decoding algo-",
        "'See Section 6.2.",
        "rithm is a tree-based search algorithm.",
        "Tree-based search",
        "- argmax",
        "Word-to-pri rase tree transforma lion mod et .h^faf) Phrase reorderinq model:",
        "Language model: f)",
        "^ Phrase Translation model;"
      ]
    },
    {
      "heading": "2. Translation Model",
      "text": [
        "We use an example of English-Vietnamese translation to demonstrate the translation process as in Figure 2.",
        "Now we describe a tree-to-string SMT model based on synchronous CFGs.",
        "The translation process is:",
        "there are CD jj NNS two big apples",
        "Figure 2: The translation process.",
        "where Ti is a source tree, Ti is a source phrase tree, T3 is a reordered source phrase tree, and T4 is a target phrase tree.",
        "Using the first order chain rule, the join probability over variables (trees) in graphical representation 1 is approximately calculated by:",
        "P(Ti) can be omitted since only one syntactic tree is used.",
        "P{T2\\T\\) is a word-to-phrase tree transformation model we describe later.",
        "P{Tz\\T2) is a reordering model.",
        "P{Ti\\T^) can be calculated using a phrase translation model and a language model.",
        "This is the fundamental equation of our study represented in this paper.",
        "In the next section, we will describe how to transform a word-based CFG tree into a phrase-based CFG tree."
      ]
    },
    {
      "heading": "3. Word-to-Phrase Tree Transformation",
      "text": [
        "According to this formalism, a tree is represented by phrase structure.",
        "If we extract a CFG from a tree or set of trees, there will be two possible rule forms:",
        "• A – > a where a is a sequence of nonterminals (syntactic categories).",
        "• B – > 7 where 7 is a terminal symbol (or a word in this case).",
        "We consider an example of a syntactic tree and a simple CFG extracted from that tree.",
        "However, we are considering phrase-based translation.",
        "Therefore the right hand side of the second rule form must be a sequence of terminal symbols (or a phrase) but not a single symbol (a word).",
        "Suppose that the phrase table contains a phrase \"am a student\" which leads to the following possible tree structure:",
        "We have to find out some way to transform a CFG tree into a tree with phrases at leaves.",
        "In the next subsection we propose such an algorithm.",
        "Table 1 represents our algorithm to transform a CFG tree to a phrase CFG tree.",
        "When designing this algorithm, our criterion is to preserve the original structure as much as possible.",
        "This algorithm includes two steps.",
        "There are a number of notions concerning this algorithm:",
        "• A CFG rule has a head symbol on the right hand side.",
        "Using this information, head child of a node on a syntactic tree can be determined.",
        "• If a node is a pre-terminal node (containing POS tag), its head word is itself.",
        "If a node is an inner node (containing syntactic constituent tag), its head word is retrieved through the head child.",
        "• Word span of a node is a string of its leaves.",
        "For instance, word span of subtree (NP (PRP$ your) (NN class)) is \"your class\".",
        "Now we consider an example depicted in Figure 3 and 4.",
        "Head children are tagged with functional label H. There are two phrases: \"is a\" and \"in your class\".",
        "After the Step 1, the phrase \"is a\" is attached to (VBZ is).",
        "The phrase \"in your class\" is attached to (IN in).",
        "In Step 2, the node (V is) is replaced by (V \"is a\") and (DT a) is removed from its father NP.",
        "Similarly, (IN in) is replaced by (IN \"in your class\") and the subtree NP on the right is removed.",
        "NNP VBZ+",
        "Fred is s NP sluderil in your class",
        "NNP-H VBZ-H",
        "Figure 3: Tree transformation - step 1.",
        "Solid arrows show the allocation process of \"is a\".",
        "Dotted arrows demonstrate the allocation process of \"in your class\"",
        "The proposed algorithm has some properties.",
        "We state these properties without presenting proof.",
        "• Uniqueness: Given a CFG tree and a phrase segmentation, by applying Algorithm 1, one and only one phrase tree is generated.",
        "• Constituent subgraph: A phrase CFG tree is a connected subgraph of input tree if leaves are ignored.",
        "• Flatness: A phrase CFG tree is flatter than input tree.",
        "• Outside head: The head of a phrase is always a word whose head outside the phrase.",
        "If there is more than one word satisfying this condition, the word at the highest level is chosen.",
        "• Dependency subgraph: Dependency graph of a phrase CFG tree is a connected subgraph of input tree's dependency graph if there exist no detached nodes.",
        "The meaning of uniqueness property is that our algorithm is a deterministic procedure.",
        "The constituent-subgraph property will be employed in the next section for an efficient decoding algorithm.",
        "When a syntactic tree is transformed, a number of subtrees are replaced by phrases.",
        "The head word of a phrase is the contact point of that phrase with the remaining part of a sentence.",
        "From the dependency point of view, a head word should depend on an outer word rather than an inner word.",
        "About dependency-subgraph property, when there is a detached node, an indirect dependency will become a direct one.",
        "In any cases, there is no change in dependency direction.",
        "We can observe dependency trees in Figure 5.",
        "The first two trees are source dependency tree and phrase dependency tree of the previous example.",
        "The last one corresponds to the case in which a detached node exists.",
        "+",
        "Input:",
        "A CFG tree, a phrase segmentation",
        "+",
        "Output:",
        "A phrase CFG tree",
        "+",
        "Step 1:",
        "Allocate phrases to leaf nodes in a top-down manner: A phrase is allocated to head word of phrase contains the head word.",
        "This head word is then considered as the phrase head.",
        "a node if the",
        "+",
        "Step 2:",
        "Transform the syntactic tree by replacing leaf nodes by their allocated phrase and removing span is a substring of phrases.",
        "all nodes whose",
        "Fred is a student /nyour class",
        "Fred is a student /nyour class",
        "Figure 5: Dependency trees.",
        "The third tree corresponds with phrase segmentation: \"Fred | is a student | in your class\"",
        "We have proposed an algorithm to create a phrase CFG tree from a pair of CFG tree and phrase segmentation.",
        "Two questions naturally arise: \"is there a way to evaluate how good a phrase tree is?\"",
        "and \"is such an evaluation valuable?\"",
        "Note that phrase trees are the means to reorder the source sentence represented as phrase segmentations.",
        "Therefore a phrase tree is surely not good if no right order can be generated.",
        "Now the answer to the second question is clear.",
        "We need an evaluation method to prevent our program from generating bad phrase trees.",
        "In other words, good phrase trees should be given a higher priority.",
        "We define the phrase tree probability as the product of its rule probability given the original CFG rules:",
        "where T' is a phrase tree whose CFG rules are LHSi RHS[.",
        "LHSi RHS{ are original CFG rules.",
        "RHS{ are subsequences of RHSi.",
        "Since phrase tree rules should capture changes made by the transformation from word to phrase, we use '+' to represent an expansion and '-' to show an overlap.",
        "These symbol will be added to a nonterminal on the side having a change.",
        "In the previous example, since a head noun in the word tree has been expanded on the right, the corresponding symbol in phrase tree is NN-H+.",
        "A nonterminal X can become one of the following symbols X, -X, +X, X-,X+, -X-,-X+, +X-, +X+.",
        "Conditional probabilities are computed in a separate training phase using a source-parsed and word-aligned bitext.",
        "First, all phrase pairs consistent with the word alignment are collected.",
        "Then using this phrase segmentation and syntactic trees we can generate phrase trees by word-to-phrase tree transformation and extract rules."
      ]
    },
    {
      "heading": "4. Phrase Reordering Model",
      "text": [
        "Reordering rules are represented as SCFG rules which can be un-lexicalized or source-side lexicalized (Nguyen and Shimazu, 2006).",
        "In this paper, we used un-lexicalized rules.",
        "We used a learning algorithm as in (Nguyen and Shimazu, 2006) to learn weighted SCFGs.",
        "The training requirements include a bilingual corpus, a word alignment tool, and a broad coverage parser of the source language.",
        "The parser is a constituency analyzer which can produce parse tree in Penn Treebank's style.",
        "The model is applicable to language pairs in which the target language is poor in resources.",
        "We used phrase reorder rules whose '+' and '-' symbols are removed."
      ]
    },
    {
      "heading": "5. Decoding",
      "text": [
        "A source sentence can have many possible phrase segmentations.",
        "Each segmentation in combination with a source tree corresponds to a phrase tree.",
        "A phrase-tree forest is a set of those trees.",
        "A naive decoding algorithm is that for each segmentation, a phrase tree is generated and then the sentence is translated.",
        "This algorithm is very slow or even intractable.",
        "Based on the constituent-subgraph property of the tree transformation algorithm, the forest of phrase trees will be packed into a tree-structure container whose backbone is the original CFG tree.",
        "A translation option encodes a possibility to translate a source phrase (at a leaf node of a phrase tree) to another phrase in target language.",
        "Since our decoder uses a log-linear translation model, it can exploit various features of translation options.",
        "We use the same features as (Koehn et al., 2003).",
        "Basic information of a translation option includes:",
        "• source phrase",
        "• target phrase",
        "• phrase translation score (2)",
        "• lexical translation score (2)",
        "• word penalty",
        "Translation options of an input sentence are collected before any decoding takes place.",
        "This allows a faster lookup than consulting the whole phrase translation table during decoding.",
        "Note that the entire phrase translation table may be too big to fit into memory.",
        "A translation hypothesis represents a partial or full translation of an input sentence.",
        "Initial hypotheses correspond to translation options.",
        "Each translation hypothesis is associated with a phrase-tree node.",
        "In other words, a phrase-tree node has a collection of translation hypotheses.",
        "Now we consider basic information contained in a translation hypothesis:",
        "• the cost so far",
        "• list of child hypotheses",
        "• left language model state and right language model state",
        "First we consider structure of a syntactic tree.",
        "A tree node contains fields such as syntactic category, child list, and head child index.",
        "A leaf node has an additional field of word string.",
        "In order to extend this structure to store translation hypotheses, a new field of hypothesis collection is appended.",
        "A hypothesis collection contains translation hypotheses whose word spans are the same.",
        "Actually, it corresponds to a phrase-tree node.",
        "A hypothesis collection whose word span is [11,12] at a node whose tag is X expresses that:",
        "• There is a phrase-tree node (X, 11,12).",
        "• There exist a phrase [ii, 12] or",
        "• There exist a subsequence of X's child list:",
        "where jo = ii and jn = 12",
        "• Suppose that [i,j] is X's span, then [11,12] is a valid phrase node's span if and only if: ii <= i ori<ii <= 3 and there exist a phrase [io, ii – 1] overlapping X's span at [i, ii – 1].",
        "A similar condition is required of 3.",
        "Table 2 shows our decoding algorithm.",
        "Step 1 distributes translation options to leaf nodes using a procedure similar to Step 1 of algorithm in Table 1.",
        "Step"
      ]
    },
    {
      "heading": "2. helps check valid subsequences in Step 3 fast. Step",
      "text": [
        "3 is a bottom-up procedure, a node is translated if all of its child nodes have been translated.",
        "Step 3.1 calls syntactic transformation models.",
        "After reordered in Step 3.2, a subsequence will be translated in Step 3.3 using a simple monotonic decoding procedure resulting in new translation hypotheses.",
        "We used a beam pruning technique to reduce the memory cost and to accelerate the computation."
      ]
    },
    {
      "heading": "6. Experimental Results",
      "text": [
        "We used Reuters, an English-Japanese bilingual corpus, and Conversation, an English-Vietnamese corpus (Table 4).",
        "These corpora were split into data sets as shown in Table 3.",
        "Japanese sentences were analyzed by ChaSen, a word-segmentation tool.",
        "A number of tools were used in our experiments.",
        "Vietnamese sentences were segmented using a word-segmentation program (Nguyen et al., 2003).",
        "For learning phrase translations and decoding, we used Pharaoh (Koehn, 2004), a state-of-the-art phrase-based SMT system which is available for research purpose.",
        "For word alignment, we used the GIZA++ tool (Och and Ney, 2000).",
        "For learning language models, we used SRILM toolkit (Stolcke, 2002).",
        "For MT evaluation, we used BLEU measure (Papineni et al., 2001) calculated by the NIST script version lib.",
        "For the parsing task, we used Charniak's parser (Char-niak, 2000).",
        "For experiments with chunking (or shallow parsing), we used a CRFs-based chunking toolto split a source sentence into syntactic chunks.",
        "Then a pseudo CFG rule over chunks is built to generate a two-level syntactic tree.",
        "This tree can be used in the our syntax-directed (SD) SMT system with chunking and full parsing respectively.",
        "On both Conversation corpus and Reuters corpus: The BLEU score of our phrase-based SMT system is comparable to that of Pharaoh; The BLEU score of our SD system with full",
        "Corpus Size",
        "Training",
        "Development Testing",
        "Conversation 16,809",
        "15,734",
        "403",
        "672",
        "Reuters 57,778",
        "55,757",
        "1,000",
        "1,021",
        "Table 3 : Corpora and data sets.",
        "English",
        "Vietnamese",
        "Sentences",
        "16,809",
        "Average sent.",
        "len.",
        "8.5",
        "8.0",
        "Words",
        "143,373",
        "130,043",
        "Vocabulary",
        "9,314",
        "9,557",
        "English",
        "Japanese",
        "Sentences",
        "57,778",
        "Average sent.",
        "len.",
        "26.7",
        "33.5",
        "Words",
        "1,548,572",
        "1,927,952",
        "Vocabulary",
        "31,702",
        "29,406",
        "Pharaoh PB system",
        "SD system (chunking)",
        "Conversation Reuters",
        "Table 6: BLEU score comparison between phrase-based SMT and syntax-directed SMT.",
        "PB=phrase-based; SD=syntax-directed same way as trees produced by Charniak's parse",
        "In Table 5, we report statistics of CFG rules, phrase CFG rules, word-to-phrase tree transformation (W2PTT) rules, and reordering rules.",
        "All counted rules were in un-lexicalized form.",
        "Those numbers are very small in comparison with the number of phrasal translations (up to hundreds of thousands on our corpora).",
        "There were a number of \"un-seen\" CFG rules which did not have a corresponding reordering rule.",
        "A reason is that those rules appeared once or several times in the training corpus; however, their hierarchical alignments did not satisfy the conditions for inducing a reordering rule since word alignment is not perfect (Nguyen and Shimazu, 2006).",
        "Another reason is that there were CFG rules which required nonlocal reordering.",
        "This may be an issue for future research: a Markovization technique for SCFGs.",
        "Without this feature, BLEU scores decreased around 0.5 on both corpora.",
        "We now consider a linguistically motivated example of English-Vietnamese translation to show that phrase segmentation can be evaluated through phrase tree scoring.",
        "This example was extracted from Conversation test set.",
        "English sentence: for my wife's mother Vietnamese word order: for mother's wife my Phrase segmentation 1: for my wife \\ 's \\ mother P1=P(PP^IN+ NP | PP^IN NP)xP(-NP^-NP NN | NP^NP",
        "The first phrase segmentation is bad (or even unacceptable) since the right word order can not be achieved from this segmentation by phrase reordering and word reordering within phrases.",
        "The second phrase segmentation is much better.",
        "Source syntax tree and phrase trees are shown in Figure 6.",
        "The first phrase tree has a much smaller probability (Pl=-7.17) than the second (P2=-2.06).",
        "SD systempaj-gjjjg js higher than that of our phrase-based sysarS\"1^m- On Conversation corpus, our SD system with chunking has a higher performance in terms of BLEU",
        "score than our phrase-based system.",
        "Using sign test (Lehmann, 1986), we verified the improvements are statistically significant.",
        "However, on Reuters corpus, performance of the SD system with chunking is much lower than the phrase-based system's.",
        "The reason is that in English-Japanese translation, chunk is a too shallow syntactic structure to capture word order information.",
        "For example, a prepositional chunk often includes only preposition and adverb, therefore such information does not help reordering prepositional phrases.",
        "We built a SMT system for phrase-based log-translation models.",
        "This system has two decobeam search and syntax-based.",
        "We implementealgorithm in Section 5 for the syntax-based dec",
        "We also implemented a rule induction module and a module for minimum error rate training.",
        "We used the system for our experiments reported later.",
        "+",
        "Input:",
        "A source CFG tree, a translation-option collection",
        "+",
        "Output:",
        "The best target sentence",
        "+",
        "Step 1:",
        "Allocate translation options to hypothesis collections at leaf nodes.",
        "+",
        "Step 2:",
        "Compute overlap vector for all nodes.",
        "+",
        "Step 3:",
        "For each node, if all of its children have been translated, then for each valid",
        "sub-sequence of child list, carry out the following steps:",
        "+",
        "Step 3.1:",
        "Retrieve transformation rules",
        "+",
        "Step 3.2:",
        "Reorder the sub-sequence",
        "+",
        "Step 3.3:",
        "Translate the reordered subsequence and update corresponding",
        "hypothesis collections",
        "Corpus",
        "CFG",
        "PhraseCFG",
        "W2PTT",
        "Reorder",
        "Conversation",
        "2,784",
        "2,684",
        "8,862",
        "2,999",
        "Reuters",
        "7,668",
        "5,479",
        "13,458",
        "7,855",
        "Conversation 36.S5",
        "Table 7: BLEU score with different syntactic levels.",
        "Level-i means syntactic transformation was applied to tree nodes whose level smaller than or equal to i.",
        "The level of a pre-terminal node (POS tag) is 0.",
        "The level of an inner node is the maximum of its children's levels.",
        "Since in practice, chunking and full parsing are often used, in Table 6, we showed translation quality of the two cases.",
        "It is interesting if we can find how syntactic analysis can affect BLEU score at more intermediate levels (Table 7).",
        "On the Conversation corpus, using syntax trees of level-1 is effective in comparison with baseline.",
        "The increase of syntactic level makes a steady improvement in translation quality.",
        "Note that when we carried out experiments with chunking (considered as level-1 syntax) the translation speed (including chunking) of our tree-to-string system was much faster than baseline systems'.",
        "This is an option for developing applications which require high speed such as web translation."
      ]
    },
    {
      "heading": "7. Related Works",
      "text": [
        "To advance the state of the art, SMT system designers have experimented with tree-structured translation models.",
        "The underlying computational models were synchronous context-free grammars and finite-state tree transducers which conceptually have a better expressive power than finite-state transducers.",
        "We create Tables 8 and 9 in order to compare syntactic SMT methods including ours.",
        "The first row is a baseline phrasal SMT approach.",
        "The second column in Table 8 only describes input types because the output is often string.",
        "Syntactic SMT methods are different in many aspects.",
        "Methods which make use of phrases (in either explicit or implicit way) can beat the baseline approach (Table 8) in terms of BLEU metric.",
        "Two main problems these models aim to deal with are word order and word choice.",
        "In order to accomplish this purpose, the underlying formal grammars (including synchronous context-free grammars and tree transducers) can be fully lexicalized or un-lexicalized (Table 9).",
        "Liu et al.",
        "(2007) proposed forest-to-string rules to capture non-constituent phrasal translation while our \"system can naturally make use of such kind of phrasal translation by using word-to-phrase tree transformation.",
        "Liu et al.",
        "(2007) also discussed about how the phenomenon of non-syntactic bilingual phrases is dealt with in other SMT methods.",
        "Galley et al.",
        "(2006) handled non-constituent phrasal translation by traversing the tree upwards until reaches a node that subsumes the phrase.",
        "Marcu et al.",
        "(2006) reported that approximately 28% of bilingual phrases are non-syntactic on their English-Chinese corpus.",
        "They proposed using a pseudo nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure.",
        "One new xRs rule is required to explain how the new nonterminal symbol can be combined with others.",
        "This technique brought a significant improvement in performance to their string-to-tree noisy channel SMT system."
      ]
    },
    {
      "heading": "8. Conclusions",
      "text": [
        "We have presented a general tree-to-string phrase-based method.",
        "This method employs a syntax-based reordering model in the decoding phase.",
        "By word-to-phrase tree transformation, all possible phrases are considered in translation.",
        "Our method does not suppose a uniform distribution over all possible phrase segmentations as (Koehn et al., 2003) since each phrase tree has a probability.",
        "We believe that other kinds of translation unit such as n-gram (Jos et al., 2006), factored phrasal translation (Koehn and Hoang, 2007), or treelet (Quirk et al., 2005) can be used in this method.",
        "We would like to consider this problem as a future study.",
        "Moreover we would like to use n-best trees as the input of our system.",
        "A number",
        "Table 8: A comparison of syntactic SMT methods (part 1).",
        "FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Transducer.",
        "Table 9: A comparison of syntactic SMT methods (part 2).",
        "xRs is a kind of rule which maps a syntactic pattern to a string, for example VP(AUX(does), RB(not),x0:VB) – > ne, x0, pas.",
        "In the column Rule lexicalization level: full=lexicalization using vocabularies of both source language and target language.",
        "of non-local reordering phenomena such as adjunct attachment should be handled in the future.",
        "Method",
        "Input",
        "Theoretical",
        "Decoding style",
        "Linguistic",
        "Phrase",
        "Performance",
        "model",
        "information",
        "usage",
        "Koehnetal.",
        "(2003)",
        "string",
        "FSTs",
        "beam search",
        "no",
        "yes",
        "baseline",
        "Yamada and Knight (2001)",
        "string",
        "SCFGs",
        "parsing",
        "target",
        "no",
        "not better",
        "Melamed (2003)",
        "string",
        "SCFGs",
        "parsing",
        "both sides",
        "no",
        "not better",
        "Chiang (2005)",
        "string",
        "SCFGs",
        "parsing",
        "no",
        "yes",
        "better",
        "Quirk et al.",
        "(2005)",
        "dep.",
        "tree",
        "TT s",
        "parsing",
        "source",
        "yes",
        "better",
        "Galley et al.",
        "(2006)",
        "string",
        "TT s",
        "parsing",
        "target",
        "yes",
        "better",
        "Liu et al.",
        "(2006)",
        "tree",
        "TT s",
        "tree transf.",
        "source",
        "yes",
        "better",
        "Our work",
        "tree",
        "SCFGs",
        "tree transf.",
        "source",
        "yes",
        "better",
        "Method",
        "Rule form",
        "Rule function",
        "Rule lexicalization level",
        "Koehnetal.",
        "(2003)",
        "no",
        "no",
        "no",
        "Yamada and Knight (2001)",
        "SCFG rule",
        "reorder and function-word ins./del.",
        "unlexicalized",
        "Melamed (2003)",
        "SCFG rule",
        "reorder and word choice",
        "full",
        "Chiang (2005)",
        "SCFG rule",
        "reorder and word choice",
        "full",
        "Quirk etal.",
        "(2005)",
        "Treelet pair",
        "word choice",
        "full",
        "Galley et al.",
        "(2006)",
        "xRs rule",
        "reorder and word choice",
        "full",
        "Liu etal.",
        "(2006)",
        "xRs rule",
        "reorder and word choice",
        "full",
        "Our work",
        "SCFG rule",
        "reorder",
        "unlexicalized"
      ]
    }
  ]
}
