{
  "info": {
    "authors": [
      "Zhanyi Liu",
      "Haifeng Wang",
      "Hua Wu",
      "Sheng Li"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1051",
    "title": "Collocation Extraction Using Monolingual Word Alignment Method",
    "url": "https://aclweb.org/anthology/D09-1051",
    "year": 2009
  },
  "references": [
    "acl-C04-1141",
    "acl-J90-1003",
    "acl-J93-1003",
    "acl-J93-2003",
    "acl-P06-1120"
  ],
  "sections": [
    {
      "text": [
        "Zhanyi Liu1,2, Haifeng Wang, Hua Wu, Sheng Li",
        "Harbin Institute of Technology, Harbin, China Toshiba (China) Research and Development Center, Beijing, China",
        "{liuzhanyi,wanghaifeng,wuhua}@rdc.toshiba.com.cn",
        "lisheng@hit.edu.cn",
        "Statistical bilingual word alignment has been well studied in the context of machine translation.",
        "This paper adapts the bilingual word alignment algorithm to monolingual scenario to extract collocations from monolingual corpus.",
        "The monolingual corpus is first replicated to generate a parallel corpus, where each sentence pair consists of two identical sentences in the same language.",
        "Then the monolingual word alignment algorithm is employed to align the potentially collocated words in the monolingual sentences.",
        "Finally the aligned word pairs are ranked according to refined alignment probabilities and those with higher scores are extracted as collocations.",
        "We conducted experiments using Chinese and English corpora individually.",
        "Compared with previous approaches, which use association measures to extract collocations from the co-occurring word pairs within a given window, our method achieves higher precision and recall.",
        "According to human evaluation in terms of precision, our method achieves absolute improvements of 27.9% on the Chinese corpus and 23.6% on the English corpus, respectively.",
        "Especially, we can extract collocations with longer spans, achieving a high precision of 69% on the long-span (>6) Chinese collocations."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000).",
        "In this paper, a collocation is composed of two words occurring as either a consecutive word sequence or an interrupted word sequence in sentences, such as \"by accident\" or \"take ... advice\".",
        "The collocations in this paper include phrasal verbs (e.g. \"put on\"), proper nouns (e.g. \"New York\"), idioms (e.g. \"dry run\"), compound nouns (e.g. \"ice cream\"), correlative conjunctions (e.g. \"either .",
        "or\"), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjec-tive+preposition (e.g. \"break rules\", \"strong tea\", \"softly whisper\", \"fully aware\", and \"fond of\").",
        "Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004).",
        "These approaches use association measures to discover collocations from the word pairs in a given window.",
        "To avoid explosion, these approaches generally limit the window size to a small number.",
        "As a result, long-span collocations can not be extracted.",
        "In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist.",
        "Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high.",
        "The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and We-hrli, 2006) and WordNet (Pearce, 2001).",
        "This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources.",
        "The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment.",
        "The",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487-495, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "monolingual corpus is replicated to generate a parallel corpus, where each sentence pair consists of two identical sentences in the same language, instead of a sentence in one language and its translation in another language.",
        "We adapt the bilingual word alignment algorithm to the monolingual scenario to align the potentially collocated word pairs in the monolingual sentences, with the constraint that a word is not allowed to be aligned with itself in a sentence.",
        "In addition, we propose a ranking method to finally extract the collocations from the aligned word pairs.",
        "This method assigns scores to the aligned word pairs by using alignment probabilities multiplied by a factor derived from the exponential function on the frequencies of the aligned word pairs.",
        "The pairs with higher scores are selected as collocations.",
        "The main contribution of this paper is that the well studied bilingual statistical word alignment method is successfully adapted to monolingual scenario for collocation extraction.",
        "Compared with the previous approaches, which use association measures to extract collocations, our method achieves much higher precision and slightly higher recall.",
        "The MWA method has the following three advantages.",
        "First, it explicitly models the co-occurring frequencies and position information of word pairs, which are integrated into a model to search for the potentially collocated word pairs in a sentence.",
        "Second, a new feature, fertility, is employed to model the number of words that a word can collocate with in a sentence.",
        "Finally, our method can obtain the longspan collocations.",
        "Human evaluations on the extracted Chinese collocations show that 69% of the long-span (>6) collocations are correct.",
        "Although the previous methods could also extract long-span collocations by setting the larger window size, the precision is very low.",
        "In the remainder of this paper, Section 2 describes the MWA model for collocation extraction.",
        "Section 3 describes the initial experimental results.",
        "In Section 4, we propose a method to improve the MWA models.",
        "Further experiments are shown in Sections 5 and 6, followed by a discussion in Section 7.",
        "Finally, the conclusions are presented in Section 8.",
        "2 Collocation Extraction With Monolingual Word Alignment Method",
        "Given a bilingual sentence pair, a source language word can be aligned with its correspond-",
        "HPA fxMÀ Ä MB Wf * M *® \\m •",
        "tuan-dui fu-ze-ren zai xiang-mu jinxing zhong qi guan-jian zuo-yong .",
        "The team leader plays a key role in the project undertaking .",
        "Figure 1.",
        "Bilingual word alignment",
        "ing target language word.",
        "Figure 1 shows an example of Chinese-to-English word alignment.",
        "In Figure 1, a word in one language is aligned with its counterpart in the other language.",
        "For examples, the Chinese word \" HßA/tuan-dui\" is aligned with its English translation \"team\", while the Chinese word \"j^îlfÀ/fu-ze-ren\" is aligned with its English translation \"leader\".",
        "In the Chinese sentence in Figure 1, there are some Chinese collocations, such as (HßA/tuan-dui, j^ilfÀ/fu-ze-ren).",
        "There are also some English collocations in the English sentence, such as (team, leader).",
        "We separately illustrate the collocations in the Chinese sentence and the English sentence in Figure 2, where the collocated words are aligned with each other.",
        "HPA fiMÀ Ä MB Wif * M *® o",
        "(a) Collocations in the Chinese sentence",
        "The team leader plays a key role in the project undertaking.",
        "(b) Collocations in the English sentence",
        "Comparing the alignments in Figures 1 and 2, we can see that the task of monolingual collocations construction is similar to that of bilingual word alignment.",
        "In a bilingual sentence pair, a source word is aligned with its corresponding target word, while in a monolingual sentence, a word is aligned with its collocates.",
        "Therefore, it is reasonable to regard collocation construction as a task of aligning the collocated words in monolingual sentences.",
        "Statistical bilingual word alignment method, which has been well studied in the context of machine translation, can extract the aligned bilingual word pairs from a bilingual corpus.",
        "This paper adapts the bilingual word alignment algorithm to monolingual scenario to align the collocated words in a monolingual corpus.",
        "the word alignments A = {(i,ai) | i e [1,l]} can be obtained by maximizing the word alignment probability of the sentence, according to Eq.",
        "(1).",
        "The team leader plays a key role in the project undertaking .",
        "Where (i, ai ) e A means that the word wi is aligned with the word wa .",
        "In a monolingual sentence, a word never collocates with itself.",
        "Thus the alignment set is denoted as A = {(i,ai ) | i e [1,l] & ai ^ i} .",
        "We adapt the bilingual word alignment model, IBM Model 3 (Brown et al., 1993), to monolingual word alignment.",
        "The probability of the alignment sequence is calculated using Eq.",
        "(2).",
        "Where <pi denotes the number of words that are aligned with wi.",
        "Three kinds of probabilities are involved:",
        "- Word collocation probability t(w f | wa ) , which describes the possibility of w collocating with wa ;",
        "- Position collocation probability d(j, a;, l), which describes the probability of a word in position aj collocating with another word in position j;",
        "- Fertility probability n($ | wi ), which describes the probability of the number of words that a word wi can collocate with (refer to subsection 7.1 for further discussion).",
        "Figure 3 shows an example of word alignment on the English sentence in Figure 2 (b) with the MWA method.",
        "In the sentence, the 7th word \"role\" collocates with both the 4th word \"play\" and the 6th word \"key\".",
        "Thus, t(w4 | w7) and t(w6 | w7 ) describe the probabilities that the word \"role\" collocates with \"play\" and \"key\", respectively.",
        "d(4 | 7,12) and d(6 | 7,12) describe the probabilities that the word in position 7 collocates with the words in position 4 and 6 in a sentence with 12 words.",
        "For the word \"role\", 07is 2, which indicates that the word \"role\" collocates with two words in the sentence.",
        "To train the MWA model, we implement a MWA tool for collocation extraction, which uses similar training methods for bilingual word alignment, except that a word can not be aligned to itself.",
        "Given a monolingual corpus, we use the trained MWA model to align the collocated words in each sentence.",
        "As a result, we can generate a set of aligned word pairs on the corpus.",
        "According to the alignment results, we calculate the frequency for two words aligned in the corpus, denoted as freq(wi, Wj ).",
        "In our method, we filtered those aligned word pairs whose frequencies are lower than 5.",
        "Based on the alignment frequency, we estimate the alignment probabilities for each aligned word pair as shown in Eq.",
        "(3) and (4).",
        "freq(Wi, Wj )",
        "With alignment probabilities, we assign scores to the aligned word pairs and those with higher scores are selected as collocations, which are estimated as shown in Eq.",
        "(5)."
      ]
    },
    {
      "heading": "3. Initial Experiments",
      "text": [
        "In this experiment, we used the method as described in Section 2 for collocation extraction.",
        "Since our method does not use any linguistic information, we compared our method with the",
        "'o ■ ^ _ \" \"* .",
        ".",
        "baseline methods without using linguistic knowledge.",
        "These baseline methods take all co-occurring word pairs within a given window as collocation candidates, and then use association measures to rank the candidates.",
        "Those candidates with higher association scores are extracted as collocations.",
        "In this paper, the window size is set to [-6, +6].",
        "The experiments were carried out on a Chinese corpus, which consists of one year (2004) of the Xinhua news corpus from LDC, containing about 28 millions of Chinese words.",
        "Since punctuations are rarely used to construct collocations, they were removed from the corpora.",
        "To automatically estimate the precision of extracted collocations on the Chinese corpus, we built a gold set by collecting Chinese collocations from handcrafted collocation dictionaries, containing 56,888 collocations.",
        "The precision is automatically calculated against the gold set according to Eq.",
        "(6).",
        "Where CTop-N and Cgold denote the top collocations in the N-best list and the collocations in the gold set, respectively.",
        "We compared our method with several baseline methods using different association meas-ures: co-occurring frequency, log-likelihood",
        "log(frequency)",
        "ratio, chi-square test, mutual information, and t-test.",
        "Among them, the log-likelihood ratio measure achieves the best performance.",
        "Thus, in this paper, we only show the performance of the log-likelihood ratio measure.",
        "Figure 4 shows the precisions of the top N collocations as N steadily increases with an increment of 1K, which are extracted by our method and the baseline method using log-likelihood ratio as the association measure.",
        "The absolute precision of collocations is not high in the figure.",
        "For example, among the top 200K collocations, about 4% of the collocations are correct.",
        "This is because our gold set contains only about 57K collocations.",
        "Even if all collocations in the gold set are included in the 200K-best list, the precision is only 28%.",
        "Thus, it is more useful to compare precision curves for collocations in the N-best lists extracted by different methods.",
        "In addition, since this gold set only includes a small number of collocations, the precision curves of our method and the baseline method are getting closer, as N increases.",
        "For example, when N is set to 200K, our method and the baseline method achieved precisions of 4.09% and 3.12%, respectively.",
        "And when N is set to 400K, they achieved 2.78% and 2.26%, respectively.",
        "For convenience of comparison, we set N up to 200K in the experiments.",
        "From the results, it can also be seen that, among the N-best lists with N less than 20K, the precision of the collocations extracted by our method is lower than that of the collocations extracted by the baseline, and became higher when N is larger than 20K.",
        "In order to analyze the possible reasons, we investigated the relationships among the frequencies of the aligned word pairs, the alignment probabilities, and precisions of collocations, which are shown in Figure 5.",
        "From the figure, we can see (1) that the lower the frequencies of the aligned word pairs are, the higher the alignment probabilities are; and (2) that the precisions of the aligned word pairs with lower frequencies is lower.",
        "According to the above observations, we conclude that it is the word pairs with lower frequencies but higher probabilities that caused the lower precision of the top 20K collocations extracted by our method."
      ]
    },
    {
      "heading": "4. Improved MWA Method",
      "text": [
        "According to the analysis in subsection 3.2, we need to penalize the aligned word pairs with lower frequencies.",
        "In order to achieve the above goal, we need to refine the alignment probabilities by using a penalization factor derived from a function on the frequencies of the aligned word pairs.",
        "This function y = f ( x) should satisfy the following two conditions, where x represents the log function of frequencies.",
        "(1) The function is monotonic.",
        "When x is set to a smaller number, y is also small.",
        "This results in the penalization on the aligned word pairs with lower frequencies.",
        "we don't penalize the aligned word pairs with higher frequencies.",
        "According to the above descriptions, we propose to use the exponential function in Eq.",
        "(7).",
        "Figure 6 describes this function.",
        "The constant b in the function is used to adjust the shape of the line.",
        "The line is sharp with b set to a small number, while the line is flat with b set to a larger number.",
        "In our case, if b is set to a larger number,",
        "-Refined probability Baseline (Log-likelihood ratio)",
        "we assign a larger penalization weight to those aligned word pairs with lower frequencies.",
        "According to the above discussion, we can use the following measure to assign scores to the aligned words pairs generated by the MWA method.",
        "log( freq( wi ,wy ))",
        "Where wi and wj are two aligned words.",
        "p(wi|wj) and p(wj|wi) are alignment probabilities as shown in Eq.",
        "(3) and (4).",
        "log( freq ( wl, wj )) is the log function of the frequencies of the aligned word pairs (wi, wj)."
      ]
    },
    {
      "heading": "5. Evaluation on Chinese corpus",
      "text": [
        "We used the same Chinese corpus described in Section 3 to evaluate the improved method as shown in Section 4.",
        "In the experiments, b was tuned by using a development set and set to 25.",
        "In this section, we evaluated the extracted collocations in terms of precision using both automatic evaluation and human evaluation.",
        "Automatic Evaluation",
        "Figure 7 shows the precisions of the collocations in the N-best lists extracted by our method and the baseline method against the gold set in Section 3.",
        "For our methods, we used two different measures to rank the aligned word pairs: alignment probabilities in Eq.",
        "(5) and refined",
        "b =2^",
        "fx",
        "Table 1.",
        "Manual evaluation of the top 1K Chinese collocations.",
        "The precisions of our method and the baseline method are 56.9% and 29.0%, respectively.",
        "alignment probabilities in Eq.",
        "(8).",
        "From the results, it can be seen that with the refined alignment probabilities, our method achieved the highest precision on the N-best lists, which greatly outperforms the best baseline method.",
        "For example, in the top 1K list, our method achieves a precision of 20.6%, which is much higher than the precision of the baseline method (11.7%).",
        "This indicates that the exponential function used to penalize the alignment probabilities plays a key role in demoting most of the aligned word pairs with low frequencies.",
        "Human Evaluation",
        "In automatic evaluation, the gold set only contains collocations in the existing dictionaries.",
        "Some collocations related to specific corpora are not included in the set.",
        "Therefore, we selected the top 1K collocations extracted by our improved method to manually estimate the precision.",
        "During human evaluation, the true collocations are denoted as \"True\" in our experiments.",
        "The false collocations were further classified into the following classes.",
        "A: The candidate consists of two words that are semantically related, such as (E^fc doctor, tfj^dr nurse).",
        "B: The candidate is a part of the multi-word (>3) collocation.",
        "For example, self, $IM mechanism) is a part of the three-word collocation ([=|$$ self, regulating, $IM mechanism).",
        "C: The candidates consist of the adjacent words that frequently occur together, such as (jtil he, i& say) and very, good).",
        "D: Two words in the candidates have no relationship with each other, but occur together frequently, such as Beijing, B month) and and, % for).",
        "are much more than those extracted by the baseline method.",
        "Further analysis shows that, in addition to extracting short-span collocations, our method extracted collocations with longer spans as compared with the baseline method.",
        "For example, (&bip in, s\\Xï<B state) and (FfJ^P because, Hitfc so) are two long-span collocations.",
        "Among the 1K collocations, there are 48 collocation candidates whose spans are larger than 6, which are not covered by the baseline method since the window size is set to 6.",
        "And 33 of them are true collocations, with a higher precision of 69%.",
        "Classes C and D account for the most part of the false collocations.",
        "Although the words in these two classes co-occur frequently, they can not be regarded as collocations.",
        "And we also found out that the errors in class D produced by the baseline method are much more than that of those produced by our method.",
        "This indicates that our MWA method can remove much more noise from the frequently occurring word pairs.",
        "In Class A, the two words are semantically related and occur together in the corpus.",
        "These kinds of collocations can not be distinguished from the true collocations by our method without additional resources.",
        "Since only bigram collocations were extracted by our method, the multi-word ( > 3) collocations were split into bigram collocations, which caused the error collocations in Class B.",
        "Corpus size vs. precision",
        "Here, we investigated the effect of the corpus size on the precision of the extracted collocations.",
        "We evaluated the precision against the gold set as shown in the automatic evaluation.",
        "First, the whole corpus (one year of newspaper) was split into 12 parts according to the published months.",
        "Then we calculated the precisions as the training",
        "Our method",
        "Baseline",
        "569",
        "290",
        "25",
        "16",
        "5",
        "4",
        "240",
        "251",
        "161",
        "439",
        "Table 2.",
        "Manual evaluation of the top 1K English collocations.",
        "The precisions of our method and the baseline method are 59.1% and 35.5%, respectively.",
        "Our method Baseline",
        "corpus increases part by part.",
        "The top 20K collocations were selected for evaluation.",
        "Figure 8 shows the experimental results.",
        "The precision of collocations extracted by our method is obviously higher than that of collocations extracted by the baseline method.",
        "When the size of the training corpus became larger, the difference between our method and the baseline method also became bigger.",
        "When the training corpus contains more than 9 months of corpora, the precision of collocations extracted by the baseline method did not increase anymore.",
        "However, the precision of collocations extracted by our method kept on increasing.",
        "This indicates the MWA method can extract more true collocations of higher quality when it is trained with larger size of training data.",
        "Recall was evaluated on a manually labeled subset of the training corpus.",
        "The subset contains 100 sentences that were randomly selected from the whole corpus.",
        "The sentence average length is 24.",
        "All true collocations (660) were labeled manually.",
        "The recall was calculated according to",
        "Eq.",
        "(9).",
        "# (Csubset )",
        "Here, CTop-N denotes the top collocations in the N-best list and Csubset denotes the true collocations in the subset.",
        "Figure 9 shows the recalls of collocations extracted by our method and the baseline method on the labeled subset.",
        "The results show that our method can extract more true collocations than the baseline method.",
        "In our experiments, the baseline method extracts about 20 millions of collocation candidates, while our method only extracts about 3 millions of collocation candidates.",
        "Although the collocations of our method are much less than that of the baseline, the experiments show that the recall of our method is higher.",
        "This again proved that our method has the stronger ability to distinguish true collocations from false collocations."
      ]
    },
    {
      "heading": "6. Evaluation on English corpus",
      "text": [
        "We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus.",
        "The English corpus contains about 20 millions of words.",
        "We estimated the precision of the top 1K collocations.",
        "Table 2 shows the results.",
        "The classification of the false collocations is the same as that in Table 1.",
        "The results show that our methods outperformed the baseline method using loglikelihood ratio.",
        "And the distribution of the false collocations is similar to that on the Chinese corpus.",
        "Our method",
        "Baseline",
        "True",
        "591",
        "355",
        "A",
        "11",
        "4",
        "t* 1 B",
        "19",
        "20",
        "False c",
        "200",
        "136",
        "D",
        "179",
        "485",
        "We used the method described in subsection 5.2 to calculate the recall.",
        "100 English sentences were labeled manually, obtaining 205 true collocations.",
        "Figure 10 shows the recall of the collocations in the N-best lists.",
        "From the figure, it can be seen that the trend on the English corpus is similar to that on the Chinese corpus, which indicates that our method is language-independent."
      ]
    },
    {
      "heading": "7. Discussion",
      "text": [
        "In the MWA model as described in subsection 2.1, (f>i denotes the number of words that can align with wi .",
        "Since a word only collocates with a few other words in a sentence, we should set a maximum number for ( , denote as ( max .",
        "In order to set (max, we examined the true collocations in the manually labeled set described in subsection 5.2.",
        "We found that 78% of words collocate with only one word, and 17% of words collocate with two words.",
        "In sum, 95% of words in the corpus can only collocate with at most two words.",
        "According to the above observation, we set ( max to 2.",
        "In order to further examine the effect of ( maxon collocation extraction, we used several different (max in our experiments.",
        "The comparison results are shown in Figure 11.",
        "The highest precision is achieved when ( maxis set to 2.",
        "This result verifies our observation on the corpus.",
        "One of the advantages of our method is that long-span collocations can be reliably extracted.",
        "In this subsection, we investigate the distribution of the span of the aligned word pairs.",
        "For the aligned word pairs occurring more than once, we calculated the average span as shown in Eq.",
        "(10).",
        "secorpus",
        "freq(wt, wj )",
        "Where, Span(wi, w j ; s) is the span of the words wi and wj in the sentence s; AveSpan(wi,wj ) is the average span.",
        "The distribution is shown in Figure 12.",
        "It can be seen that the number of the aligned word pairs decreased exponentially as the average span increased.",
        "About 17% of the aligned word pairs have spans longer than 6.",
        "According to the human evaluation result for precision in subsection 5.1, the precision of the long-span collocations is even higher than that of the short-span collocations.",
        "This indicates that our method can extract reliable collocations with long spans."
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "We have presented a monolingual word alignment method to extract collocations from monolingual corpus.",
        "We first replicated the monolingual corpus to generate a parallel corpus, in which each sentence pair consists of the two identical sentences in the same language.",
        "Then we adapted the bilingual word alignment algorithm to the monolingual scenario to align the potentially collocated word pairs in the monolingual sentences.",
        "In addition, a ranking method was proposed to finally extract the collocations from the aligned word pairs.",
        "It scores collocation candidates by using alignment probabilities multiplied by a factor derived from the exponential function on the frequencies.",
        "Those with higher scores are selected as collocations.",
        "Both Chinese and English collocation extraction experiments indicate that our method outperforms previous approaches in terms of both precision and recall.",
        "For example, according to the human evaluations on the Chinese corpus, our method achieved a precision of 56.9%, which is much higher than that of the baseline method (29.0%).",
        "Moreover, we can extract collocations with longer span.",
        "Human evaluation on the extracted Chinese collocations shows that 69% of the long-span (>6) collocations are correct."
      ]
    }
  ]
}
