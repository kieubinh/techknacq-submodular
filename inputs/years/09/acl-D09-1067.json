{
  "info": {
    "authors": [
      "Lin Sun",
      "Anna Korhonen"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1067",
    "title": "Improving Verb Clustering with Automatically Acquired Selectional Preferences",
    "url": "https://aclweb.org/anthology/D09-1067",
    "year": 2009
  },
  "references": [
    "acl-C08-1057",
    "acl-C08-1082",
    "acl-D08-1007",
    "acl-E03-1034",
    "acl-J06-2001",
    "acl-P06-2012",
    "acl-P06-4020",
    "acl-P07-1028",
    "acl-P07-1115",
    "acl-P08-1050",
    "acl-P08-1057",
    "acl-P08-1063",
    "acl-W02-1016",
    "acl-W04-3213"
  ],
  "sections": [
    {
      "text": [
        "Improving Verb Clustering with Automatically Acquired Selectional",
        "Preferences",
        "Lin Sun and Anna Korhonen",
        "In previous research in automatic verb classification, syntactic features have proved the most useful features, although manual classifications rely heavily on semantic features.",
        "We show, in contrast with previous work, that considerable additional improvement can be obtained by using semantic features in automatic classification: verb selectional preferences acquired from corpus data using a fully unsupervised method.",
        "We report these promising results using a new framework for verb clustering which incorporates a recent subcategorization acquisition system, rich syntactic-semantic feature sets, and a variation of spectral clustering which performs particularly well in high dimensional feature space."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Verb classifications have attracted a great deal of interest in natural language processing (NLP).",
        "They have proved useful for various important NLP tasks and applications, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Za-pirain etal., 2008).",
        "Verb classes are useful because they offer a powerful tool for generalization and abstraction which can be beneficial when faced e.g. with the problem of data sparsity.",
        "Particularly useful can be classes which capture generalizations over a range of (cross-)linguistic properties, such as the ones proposed by Levin (1993).",
        "Being defined in terms of similar meaning and (morpho-)syntactic behaviour of words, Levin style classes generally incorporate a wider range of properties than e.g. classes defined solely on semantic grounds (Miller, 1995).",
        "In recent years, a variety of approaches have been proposed for automatic induction of verb classes from corpus data (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; Ö Séaghdha and Copestake, 2008; Vlachos et al., 2009).",
        "This work opens up the opportunity of learning and tuning classifications tailored to the application and domain in question.",
        "Although manual classification may always yields higher accuracy, automatic verb classification is cost-effective and gathers statistical information as a side-effect of the acquisition process which is difficult for humans to gather but can be highly useful for NLP applications.",
        "To date, both supervised and unsupervised machine learning (ml) methods have been proposed for verb classification and used to classify a variety of features extracted from raw, tagged and/or parsed corpus data.",
        "The best performing features on cross-domain verb classification have been syntactic in nature (e.g. syntactic slots, subcategorization frames (SCFs)).",
        "Disappointingly, semantic features have not yielded significant additional improvement, although they play a key role in manual and theoretical work on verb classification and could thus be expected to offer a considerable contribution to classification performance.",
        "Since the accuracy of automatic verb classification shows room for improvement, we further investigate the potential of semantic features verb selectional preferences (SPs) - for the task.",
        "We introduce a novel approach to verb clustering which involves the use of (i) a recent subcategorization frame (SCF) acquisition system (Preiss et al., 2007) which produces rich lexical, SCF and syntactic data, (ii) novel syntactic-semantic feature sets extracted from this data which incorporate a variety of linguistic information, including SPs, and (iii) a new variation of spectral cluster-",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 638-647, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "ing based on the MNCut algorithm (Meila and Shi, 2001) which is well-suited for dealing with the resulting, high dimensional feature space.",
        "Using this approach, we show on two well-established test sets that automatically acquired SPs can be highly useful for verb clustering.",
        "They yield high performance when used in combination with syntactic features.",
        "We obtain our promising results using a fully unsupervised approach to SP acquisition which differs from previous approaches in that it does not exploit WordNet (Miller, 1995) or other lexical resources.",
        "It is based on clustering argument head data in the grammatical relations associated with verbs.",
        "We describe our features in section 2 and the clustering methods in section 3.",
        "Experimental evaluation and results are reported in sections 4 and 5, respectively.",
        "Section 6 provides discussion and describes related work, and section 7 concludes."
      ]
    },
    {
      "heading": "2. Features",
      "text": [
        "Our target classification is the taxonomy of Levin (1993) where verbs taking similar diathesis alternations are assumed to share meaning components and are organized into semantically coherent classes.",
        "The main feature of this classification is a diathesis alternation which manifests at the level of syntax in alternating sets of SCF (e.g. in the causative/inchoative alternation an NP frame alternates with an intransitive frame: Tony broke the window <-> The window broke).",
        "Since automatic detection of diathesis alternations is very challenging (McCarthy, 2001), most work on automatic classification has exploited the fact that similar alternations tend to result in similar scfs.",
        "The research reported so far has used mainly syntactic features for classification, ranging from shallow syntactic slots (e.g. NPs preceding or following the verb) to scfs.",
        "Some researchers have discovered that supplementing basic syntactic features with information about adjuncts, co-occurrences, tense, and/or voice of the verb have resulted in better performance.",
        "However, additional information about semantic SPs of verbs has not yielded considerable improvement on verb classification although SPs can be strong indicators of diathesis alternations (McCarthy, 2001) and although fairly precise semantic descriptions, including information about verb se-",
        "!See section 6 for discussion on previous work.",
        "lectional restrictions, can be assigned to the majority of Levin classes, as demonstrated by VerbNet (Kipper-Schuler, 2005).",
        "SP acquisition from undisambiguated corpus data is arguably challenging (Brockmann and La-pata, 2003; Erk, 2007; Bergsma et al., 2008).",
        "It is especially challenging in the context of verb classification where SP models are needed for specific syntactic slots for which the data may be sparse, and the resulting feature vectors integrating both syntactic and semantic features may be high dimensional.",
        "However, we wanted to investigate whether better results could be obtained if the features were optimised for richness, the feature extraction for accuracy, and a clustering method capable of dealing with the resulting high dimensional feature space was employed.",
        "We adopted a recent scf acquisition system which has proved more accurate than previous comparable systems but which has not been employed for verb clustering before: the system of Preiss et al.",
        "(2007).",
        "This system tags, lemmatizes and parses corpus data using the current version of the rasp (Robust Accurate Statistical Parsing) toolkit (Briscoe et al., 2006), and on the basis of resulting grammatical relations (GRs) assigns each occurrence of a verb to one of 168 verbal scfs classes.",
        "The system provides a filter which can be used to remove adjuncts from the resulting lexicon.",
        "We do not employ this filter since adjuncts have proved informative for verb classification (Sun et al., 2008; Joanis et al., 2008).",
        "However, we do frequency-based thresholding to minimise the noise (e.g. erroneous scfs) and sparse data in verb classification and to ensure that only features supported by several verbs are used in classification: we only consider SCFs and GRs which have frequency larger than 40 with 5 or more verbs.",
        "The system produces a rich lexicon which includes raw and processed input sentences and provides a variety of material for verb clustering, including e.g. (statistical) information related to the part-of-speech (pos) tags, GRs, SCFs, argument heads, and adjuncts of verbs.",
        "Using this material, we constructed a wide range of feature sets for experimentation, both shallow and deep syntactic and semantic features.",
        "As described below, some of the feature types have been employed in previous works and some are novel.",
        "The first feature set Fl includes information about the lexical context (co-occurrences) of verbs which has proved useful for supervised verb classification (Li and Brew, 2008):",
        "Fl: Co-occurrence (CO): We adopt the best method of Li and Brew (2008) where collocations are extracted from the four words immediately preceding and following a lemma-tized verb.",
        "Stop words are removed prior to extraction, and the 600 most frequent resulting cos are kept.",
        "F2-F3 provide information about lexical preferences of verbs in argument head positions of specific GRs associated with the verb:",
        "F2: Prepositional preference (pp): the type and frequency of prepositions in the indirect object relation.",
        "F3: Lexical preference (LP): the type and frequency of nouns and prepositions in the subject, object, and indirect object relation.",
        "All the other feature sets include information about SCFs which have been widely employed in verb classification, e.g. (Schulte im Walde, 2006; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008).",
        "F4-F7 include basic SCF information and/or refine it with additional information which has proved useful in previous works:",
        "F4: SCFs and relative frequencies with verbs.",
        "SCFs abstract over particles and prepositions.",
        "F5: F4 with cos (Fl).",
        "The SCF and CO feature vectors are concatenated.",
        "F6: F4 with the tense of the verb.",
        "The frequency of verbal POS tags is calculated specific to each SCF.",
        "F7: F4 with PPs (F2).",
        "This feature parameterizes SCFs for prepositions.",
        "F8: Basic SCF feature corresponding to F4 but extracted from the VALEX lexicon (Korhonen etal.,2006).",
        "The following 9 feature sets are novel.",
        "They build on F7, refining it further.",
        "F9-F11 refine F7 with information about LPs:",
        "F9: F7 with F3 (subject only) F10: F7 with F3 (object only) Fil: F7 with F3 (subject, object, indirect object)",
        "F12-17 refine F7 with SPs.",
        "We adopt a fully unsupervised approach to SP acquisition.",
        "We acquire the SPs by",
        "1. taking the GR relations (subject, object, indirect object) associated with verbs,",
        "2. extracting all the argument heads in these relations which occur with frequency > 20 with more than 3 verbs, and",
        "3. clustering the resulting N most frequent argument heads into M classes using the spectral clustering method described in the following section.",
        "We tried the N settings {200, 500} and the M settings {10,20,30,80}.",
        "The best settings N = 200, M = 20 and N = 500, M = 30 are reported in this paper.",
        "We enforce the features to be shared by all the potential members of a verb class.",
        "The expected class size is approximately N/K, and we allow for 10% outliers (the features occurring less than (N/K) x 0.9 verbs are thus removed).",
        "The resulting SPs are combined with SCFs in a similar fashion as LPs are combined with SCFs in F9-F11:"
      ]
    },
    {
      "heading": "3. Clustering methods",
      "text": [
        "We use two clustering methods: (i) pairwise clustering (pc) which obtained the best performance in comparison with several other methods in recent work on biomedical verb clustering (Korho-nen et al., 2008), and (ii) a method which is new to the task (and to the best of our knowledge, to NLP): a variation of spectral clustering which exploits the MNCut algorithm (Meila and Shi, 2001) (spec).",
        "Spectral clustering has been shown to be effective for high dimensional and non-convex data in NLP (Chen et al., 2006) and it has been applied to German verb clustering by Brew and Schulte im Walde (2002).",
        "However, previous work has used Ng et al.",
        "(2002)'s algorithm, while we adopt the MNCut algorithm.",
        "The latter has shown a wider applicability (von Luxburg, 2007; Verma and Meila, 2003) and it can be justified from the random walk view, which has a clear probabilistic interpretation.",
        "Clustering groups a given set of items (verbs in our experiment) V = {vn}n=i mto a disjoint partition of K classes / = {Ik}k=v Both our algorithms take a similarity matrix as input.",
        "We construct this from the skew divergence (Lee, 2001).",
        "The skew divergence between two feature vectors v and?/ is dskew(v, v') = D(v'\\\\a-v + (1 – a) V) where D is the KL-divergence.",
        "v is smoothed with v'.",
        "The level of smoothing is controlled by a whose value is set to a value close to 1 (e.g. 0.9999).",
        "We symmetrize the skew divergence as follows: d(v,v')sskew = \\{dskew(v,v') +",
        "dskew(v',v)).",
        "spec is typically used with the Radial Basis Function (RBF) kernel.",
        "We adopt a new kernel similar to the symmetrized KL divergence kernel (Moreno et al., 2004) which avoids the need for scale parameter estimation.",
        "w(v,v') = exp(-dsskew(v,v')) The similarity matrix W is constructed where Wij = w(vi,Vj).",
        "Pairwise clustering",
        "pc (Puzicha et al., 2000) is a method where a cost criterion guides the search for a suitable partition.",
        "This criterion is realized through a cost function of the similarity matrix W and partition I:",
        "where rij is the size of the jth cluster and Avgsim^ is the average similarity between cluster members.",
        "Spectral clustering",
        "In spec, the similarities Wij are viewed as the weight on the edges ij of a graph G over V. The similarity matrix W is thus the adjacency matrix for G. The degree of a vertex % is di = J2f=i wv-A cut between two partitions A and A' is defined to be Cut(A A') = ZmeA,neA' W™-",
        "In MNCut algorithm, the similarity matrix W is transformed to a stochastic matrix P.",
        "The degree matrix D is a diagonal matrix where Da = di.",
        "It was shown by Meila and Shi (2001) that if P has the K leading eigenvectors that are piecewise constant with respect to a partition /* and their eigenvalues are not zero, then /* minimizes the multiway normalized cut(MNCut):",
        "Pmn can be interpreted as the transition probability between vertices m, n. The criterion can thus be expressed as MNCut(J) = Ef=i(!",
        "- P(h -»■ Ik\\Ik)) (Meila, 2001), which is the sum of transition probabilities across different clusters.",
        "The criterion finds the partition where the random walks are most likely to happen within the same cluster.",
        "In practice, the K leading eigenvectors of P is not piecewise constant.",
        "But we can extract the partition by finding the approximately equal elements in the eigenvectors using a clustering algorithm like K-means.",
        "The numerator of MNCut is similar to the cost function of pc.",
        "The main differences between the two algorithms are: 1) MNCut takes into account of the cross cluster similarity, while pc does not.",
        "2) pc optimizes the cost function using deterministic annealing, whereas spec uses eigensystem decomposition.",
        "The spectral clustering algorithm is based on the Multicut algorithm (Meila and Shi, 2001).",
        "Input: Dataset S, Number of clusters K"
      ]
    },
    {
      "heading": "1.. Compute similarity matrix W and Degree matrix D",
      "text": []
    },
    {
      "heading": "2.. Construct stochastic matrix P using equation 1",
      "text": [
        "3.",
        "Compute the eigenvalues and eigenvectors {^n,xn}n=i of ^» where A„ > Xn+i, form a matrix X = [x2, ■ ■ ■, xk] by stacking the eigenvectors in columns.",
        "4.",
        "Form a matrix Y from X by normalizing the row sums to have norm 1: Yij = Xij/ ^ij) 5.",
        "Consider the row of Y to be the transformed feature vectors for each verb and cluster them into clusters C\\... Ck using K-means clustering algorithm.",
        "Output: Clusters C\\... Ck"
      ]
    },
    {
      "heading": "4. Experimental evaluation",
      "text": [
        "We employed two test sets which have been used to evaluate previous work on English verb classification:",
        "Tl The test set of Joanis et al.",
        "(2008) provides a classification of 835 verbs into 15 (some coarse, some fine-grained) Levin classes.",
        "11 tests are provided for 2-14 way classifications.",
        "We employ the 14 way classification because this corresponds the closest to our target (Levin's fine-grained) classification.",
        "We select 586 verbs according to Joanis et al.",
        "'s selection criteria, resulting in 10-120 verbs per class.",
        "We restrict the class imbalance to 1:1.5..",
        "This yields 205 verbs (10-15 verbs per class) which is similar to the subset of Tl employed by Stevenson and Joanis (2003).",
        "T2 The test set of Sun et al.",
        "(2008) classifies 204 verbs to 17 fine-grained Levin classes, so that each class has 12 member verbs.",
        "For each verb in Tl and T2, we extracted all the occurrences (up to 10,000) from the raw corpus data gathered originally for constructing the",
        "Table 2: (i) The total number of features and (ii) the average per verb for all the feature sets",
        "valex lexicon (Korhonen et al., 2006).",
        "The data was gathered from five corpora, including e.g. the British National Corpus (Leech, 1992) and the North American News Text Corpus (Graff, 1995).",
        "The average frequency of verbs in Tl was 1448 and T2 2166, showing that Tl is a more sparse data set.",
        "The data was first processed using the feature extraction module.",
        "Table 2 shows (i) the total number of features in each feature set and (ii) the average per verb in the resulting lexicons for Tl and T2.",
        "We normalized the feature vectors by the sum of the feature values before applying the clustering techniques.",
        "Since both clustering algorithms have an element of randomness, we run them multiple times.",
        "The step 5 of spec (K-means) was run for 50 times.",
        "The result that minimizes the distortion (the distances to cluster centroid) is reported.",
        "pc was run 20 times, and the results are averaged.",
        "Tl",
        "T2",
        "Object Drop",
        "26.",
        "{1,3,7}",
        "Remove",
        "10.1",
        "Recipient",
        "13.",
        "{1,3}",
        "Send",
        "11.1",
        "Admire",
        "31.2",
        "Get",
        "13.5.1",
        "Amuse",
        "31.1",
        "Hit",
        "18.1",
        "Run",
        "51.3.2",
        "Amalgamate",
        "22.2",
        "Sound",
        "43.2",
        "Characterize",
        "29.2",
        "Light &",
        "43.",
        "{1,4}",
        "Peer",
        "30.3",
        "Substance",
        "Amuse",
        "31.1",
        "Cheat",
        "10.6",
        "Correspond",
        "36.1",
        "Steal &",
        "10.",
        "{5,1}",
        "Manner",
        "37.3",
        "Remove",
        "of speaking",
        "Wipe",
        "10.4.",
        "{1,2}",
        "Say",
        "37.7",
        "Spray / Load",
        "9.7",
        "Nonverbal",
        "40.2",
        "Fill",
        "9.8",
        "expression",
        "Putting",
        "9.1-6",
        "Light",
        "43.1",
        "Change of State",
        "45.1-4",
        "Other change",
        "45.4",
        "of state",
        "Mode with",
        "47.3",
        "Motion",
        "Run",
        "51.3.2",
        "Put",
        "9.1",
        "Tl",
        "T2",
        "total",
        "avg",
        "total",
        "avg",
        "CO",
        "Fl",
        "1328",
        "764",
        "743",
        "382",
        "LP(p)",
        "F2",
        "61",
        "37",
        "55",
        "25",
        "LP (all)",
        "F3",
        "2521",
        "526",
        "1481",
        "295",
        "SCF",
        "F4",
        "88",
        "46",
        "86",
        "38",
        "SCF+CO",
        "F5",
        "1466",
        "833",
        "856",
        "422",
        "SCF+POS",
        "F6",
        "319",
        "114",
        "299",
        "87",
        "SCF+P",
        "F7",
        "282",
        "96",
        "273",
        "76",
        "SCF (V)",
        "F8",
        "-",
        "-",
        "92",
        "45",
        "SCF+LP (s)",
        "F9",
        "1747",
        "324",
        "1474",
        "225",
        "SCF+LP (o)",
        "F10",
        "2817",
        "424",
        "2319",
        "279",
        "SCF+LP (all)",
        "Fll",
        "4250",
        "649",
        "3515",
        "426",
        "SCF+SP20 (s)",
        "F12",
        "821",
        "235",
        "690",
        "145",
        "SCF+SP20 (o)",
        "F13",
        "792",
        "218",
        "706",
        "135",
        "SCF+SP20 (all)",
        "F14",
        "1333",
        "357",
        "1200",
        "231",
        "SCF+SP30 (s)",
        "F15",
        "977",
        "274",
        "903",
        "202",
        "SCF+SP30 (o)",
        "F16",
        "1026",
        "273",
        "1012",
        "205",
        "SCF+SP30 (all)",
        "F17",
        "1720",
        "451",
        "1640",
        "330",
        "To facilitate meaningful comparisons, we employ the same measures for evaluation as previously employed e.g. by Korhonen et al.",
        "(2008); O Séaghdha and Copestake (2008).",
        "The first measure is modified purity (mPUR) a global measure which evaluates the mean precision of clusters.",
        "Each cluster is associated with its prevalent class.",
        "The number of verbs in a cluster K that take this class is denoted by nprevaient(K.).",
        "Verbs that do not take it are considered as errors.",
        "Clusters where nprevaient(K.) = 1 are disregarded as not to introduce a bias towards singletons:",
        "^nprevalent(k.)>2 nprevalent(ki)",
        "number of verbs The second measure is weighted class accuracy (ACC): the proportion of members of dominant clusters DOM-CLUST^ within all classes c^.",
        "Y]?--i verbs in dom-clust^",
        "number of verbs",
        "mPUR and ACC can be seen as a measure of precision^) and recall(R) respectively.",
        "We calculate F measure as the harmonic mean of P and R:",
        "mPUR + ACC",
        "The random baseline(BL) is calculated as follows:",
        "bl =1/number of classes"
      ]
    },
    {
      "heading": "5. Results",
      "text": [
        "Table 3 includes the F-measure results for all the feature sets when the two methods (PC and spec) are used to cluster verbs in the test sets Tl and T2, respectively.",
        "A number of tendencies can be observed in the results.",
        "Firstly, the results for T2 are clearly better than those for Tl.",
        "Including a higher number of verbs lower in frequency from classes of variable granularity, Tl is probably a more challenging test set than T2.",
        "T2 is controlled for the number and frequency of verbs to facilitate cross-class comparisons.",
        "While this may contribute to better results, T2 is a more accurate test set for us in the sense that it offers a better correspondence with our target (fine-grained Levin) classes.",
        "Secondly, the difference between the two clustering methods is clear: the new spec outperforms pc on both test sets and across all the feature sets.",
        "The performance of the two methods is still fairly similar with the more basic, less sparse feature sets (F1-F2, F4, F6-7) but when the more sophisticated feature sets are used (F3, F5, F9-F17) spec performs considerably better.",
        "This demonstrates that it is clearly a better suited method for high dimensional feature sets.",
        "Comparing the feature sets, the simple cooccurrence based Fl performs clearly better than the random baseline.",
        "F2 and F3 which exploit lexical data in the argument head positions of GRs prove significantly better than Fl.",
        "F3 yields surprisingly good results on T2: it is the second best feature set on this test set.",
        "Also on Tl, F3 performs better than the scF-based feature sets F4-F7.",
        "This demonstrates the usefulness of lexical data when obtained from argument positions in relevant GRs.",
        "Our basic scf feature set F4 performs considerably better than the comparable feature set F8 obtained from the valex lexicon.",
        "The difference is 19.50 in F-measure.",
        "As both lexicons were extracted from the same corpus data, the improvement can be attributed to improved parser and scf acquisition performance (Preiss et al., 2007).",
        "F5-F7 refine the basic scf feature set F4 further.",
        "F5 which combines a scf with co information proved the best feature set in the supervised verb classification experiment of Li and Brew (2008).",
        "In our experiment, F5 produces substantially lower result than co and scf alone (i.e.",
        "Tl",
        "T2",
        "PC",
        "SPEC",
        "PC",
        "SPEC",
        "bl",
        "7.14",
        "7.14",
        "5.88",
        "5.88",
        "CO",
        "Fl",
        "15.62",
        "33.85",
        "17.86",
        "40.94",
        "LP(p)",
        "F2",
        "40.40",
        "38.97",
        "50.98",
        "49.02",
        "LP (all)",
        "F3",
        "42.94",
        "47.50",
        "41.08",
        "74.55",
        "SCF",
        "F4",
        "34.22",
        "36.16",
        "52.33",
        "57.78",
        "SCF+CO",
        "F5",
        "26.43",
        "28.70",
        "19.52",
        "29.10",
        "SCF+POS",
        "F6",
        "36.14",
        "34.75",
        "44.44",
        "46.70",
        "SCF+P",
        "F7",
        "43.57",
        "43.85",
        "63.40",
        "63.28",
        "SCF (V)",
        "F8",
        "-",
        "-",
        "34.08",
        "38.30",
        "SCF+LP (s)",
        "F9",
        "47.72",
        "56.09",
        "65.94",
        "71.65",
        "SCF+LP (o)",
        "F10",
        "43.09",
        "48.43",
        "57.11",
        "73.97",
        "SCF+LP (all)",
        "Fll",
        "45.87",
        "54.63",
        "56.30",
        "72.97",
        "SCF+SP20 (s)",
        "F12",
        "46.67",
        "57.75",
        "39.52",
        "71.67",
        "SCF+SP20 (o)",
        "F13",
        "44.95",
        "51.70",
        "40.76",
        "70.78",
        "scF+SP20(all)",
        "F14",
        "48.19",
        "55.12",
        "39.68",
        "73.09",
        "SCF+SP30 (s)",
        "F15",
        "45.89",
        "56.10",
        "64.44",
        "80.35",
        "SCF+SP30 (o)",
        "F16",
        "42.01",
        "48.74",
        "52.75",
        "70.52",
        "scF+SP30(all)",
        "F17",
        "46.66",
        "52.68",
        "51.07",
        "68.67",
        "Fl and F4).",
        "However, our corpus is smaller (Li and Brew used the large Gigaword corpus), our SCFs are different, and our approach is unsupervised, making meaningful comparisons difficult.",
        "F6 combines F4 with information about verb tense.",
        "This was not helpful: F6 produces worse results than F4.",
        "F7, on the other hand, yields better results than F4 on both test sets.",
        "This demonstrates what the previous research has shown: SCF perform better when parameterized for prepositions.",
        "Looking at our novel feature sets F9-F17, F9-Fll combine the most accurate SCF feature set F4 with the LP-based features F2-F3.",
        "Although the feature space gets more sparse, all the feature sets outperform F2-F3 on Tl.",
        "On T2, F3 performs exceptionally well, and thus yields a better result than F9-F11, but F9-F11 nevertheless perform clearly better than the best scF-based feature set F4 alone.",
        "The differences among F9, F10 and Fl 1 are small on T2, but on Tl F9 yields the best performance.",
        "It could be that F9 works the best for the more sparse Tl because it suffers the least from data sparsity (it uses LPs only for the subject relation).",
        "F12-F17 replace the LPs in F9-F11 by semantic SPs.",
        "When only 20 clusters are used as SP models and acquired from the smaller sample of (200) argument heads (F12-F14), SPs do not perform better than LPs on T2.",
        "A small improvement can be observed on Tl, especially with Fl2 which uses only the subject data (yielding the best F measure on Tl: 57.75%).",
        "However, when 30 more finegrained clusters are acquired from a bigger sample of (500) argument heads (F15-F17), lower results can be seen on Tl.",
        "On T2, on the other hand, F15 yields dramatic improvement and we get the best performance for this test set: 80.35% F-measure.",
        "The fact that no improvement is observed when using F16 and F17 on T2 could be explained by the fact that SPs are stronger for the subject position which also suffers less from the sparse data problem than e.g. i. object position.",
        "The fact that no improvement is observed on Tl is likely to be due to the fact that verbs have strong SPs only at the finer-grained level of Levin classification.",
        "Recall that in Tl, as many as half of the classes are coarser-grained.",
        "The best performing feature sets on both Tl and T2 were thus our new SP-based feature sets.",
        "We conducted qualitative analysis of the best 30 SP clusters in the T2 data created using spec to find out whether these clusters were really semantic in nature, i.e. captured semantically meaningful preferences.",
        "As no gold standard specific to our verb classification task was available, we did manual cluster analysis using VerbNet (vn) as aid.",
        "In vn, Levin classes are assigned with semantic descriptions: the arguments of SCFs involved in diathesis alternations are labeled with thematic roles some of which are labeled with selectional restrictions.",
        "From the 30 thematic role types in vn, as many as 20 are associated with the 17 Levin classes in T2.",
        "The most frequent role in T2 is agent, followed by theme, location, patient, recipient, and source.",
        "From the 36 possible selectional restriction types, 7 appear in T2; the most frequent ones being +animate and +organization, followed by +concrete, +location, and +communication.",
        "As SP clusters capture selectional preferences rather than restrictions, we examined manually whether the 30 clusters (i) capture semantically meaningful classes, and whether they (ii) are plausible given the vn semantic descriptions/restrictions for the classes in T2.",
        "The analysis revealed that all the 30 clusters had a predominant, semantically motivated SP supported by the majority of the member nouns.",
        "Although many clusters could be further divided into more specific SPs (and despite the fact that some nouns were clearly misclassified), we were able to assign each cluster a descriptive label characterizing the predominant SP.",
        "Table 4 shows 15 sampie clusters, the SP labels assigned to them, and a number of example nouns in these clusters.",
        "Human",
        "mother, wife, parent, girl, child",
        "Role",
        "patient, student, user, worker, teacher",
        "Body-part",
        "neck, shoulder, back, knee, corner",
        "Authority",
        "committee, police, court, council, board",
        "Organization",
        "society, firm, union, bank, institution",
        "Money",
        "cash, currency, pound, dollar, fund",
        "Amount",
        "proportion, value, size, speed, degree",
        "Time",
        "minute, moment, night, hour, year",
        "Path",
        "street, track, road, stair, route",
        "Building",
        "office, shop, hotel, hospital, house",
        "Region",
        "site, field, area, land, island",
        "Technology",
        "system, model, facility, engine, machine",
        "Task",
        "operation, test, study, analysis, duty",
        "Arrangement",
        "agreement, policy, term, rule, procedure",
        "Matter",
        "aspect, subject, issue, question, case",
        "Problem",
        "difficulty, challenge, loss, pressure, fear",
        "Idea",
        "argument, concept, idea, theory, belief",
        "Power",
        "control, lead, influence, confidence, ability",
        "Form",
        "colour, style, pattern, shape, design",
        "Item",
        "letter, book, goods, flower, card",
        "When comparing each SP cluster against the VN semantic descriptions/restrictions for T2, we found that each predominant SP was plausible.",
        "Also, the SPs frequent in our data were also frequent among the 17 classes according to VN.",
        "For example, the many SP clusters labeled as arrangements, issues, ideas and other abstract concepts were also frequent in T2, e.g. among COMMUNICATION (37), CHARACTERISE (29.2), AMALGAMATE (22.2) and other classes.",
        "This analysis showed that the SP models which performed well in verb clustering were semantically meaningful for our task.",
        "An independent evaluation using one of the standard datasets available for SP acquisition research (Brockmann and Lapata, 2003) is of course needed to determine how well the acquisition method performs in comparison with other existing methods.",
        "Finally, we evaluated the quality of the verb clusters created using the SP-based features.",
        "We found that some of the errors were similar to those seen on T2 when using syntactic features: errors due to polysemy and syntactic idiosyncracy.",
        "However, a new error type clearly due to the SP-based feature was detected.",
        "A small number of classes got confused because of strong similar SPs in the subject (agent) position.",
        "For example, some peer (30.3) verbs (e.g. look, peer) were found in the same cluster with say (37.7) verbs (e.g. shout, yell) - an error which purely syntactic features do not produce.",
        "Such errors were not numerous and could be addressed by developing more balanced SP models across different GRs."
      ]
    },
    {
      "heading": "6. Discussion and related work",
      "text": [
        "Although features incorporating semantic information about verb SPs make theoretical sense they have not proved equally promising in previous experiments which have compared them against syntactic features in verb classification.",
        "Joanis et al.",
        "(2008) incorporated an 'animacy' feature (a kind of a 'SP') which was determined by classifying e.g. pronouns and proper names in data to this single SP class.",
        "A small improvement was obtained when this feature was used in conjunction with syntactic features in supervised classification.",
        "Joanis (2002) and Schulte im Walde (2006) experimented with more conventional SPs with syntactic features in English and German verb classification, respectively.",
        "They employing top level",
        "Table 5 : Previous verb classification results",
        "WordNet (Miller, 1995) and Germanet (Kunze and Lemnitzer, 2002) classes as SP models.",
        "Joanis (2002) obtained no improvement over syntactic features, whereas Schulte im Walde (2006) obtained insignificant improvement.",
        "Korhonen et al.",
        "(2008) combined SPs with SCFs when clustering biomedical verbs.",
        "The SPs were acquired automatically from syntactic slots of SCFs (not from GRs as in our experiment) using PC clustering.",
        "A small improvement was obtained using LPs extracted from the same syntactic slots, but the SP clusters offered no improvement.",
        "Recently, Schulte im Walde et al.",
        "(2008) proposed an interesting SP acquisition method which involves combining EM training and the MDL principle for an verb classification incorporating SPs.",
        "However, no comparison against purely syntactic features is provided.",
        "In our experiment, we obtained a considerable improvement over syntactic features, despite using a fully unsupervised approach to both verb clustering and SP acquisition.",
        "In addition to the rich, syntactic-semantic feature sets, our good results can be attributed to the clustering technique capable of dealing with them.",
        "The potential of spectral clustering for the task was recognised earlier by Brew and Schulte im Walde (2002).",
        "Although a different version of the algorithm was employed and applied to German (rather than to English), and although no SP features were used, these earlier experiments did demonstrate the ability of the method to perform well in high dimensional feature space.",
        "To get an idea of how our performance compares with that of related approaches, we examined recent works on verb classification (supervised and unsupervised) which were evaluated on same test sets using comparable evaluation measures.",
        "These works are summarized in table 5.",
        "ACC and F-measure are shown for Tl and T2, respectively.",
        "Method",
        "Result",
        "Tl",
        "Li et al.",
        "2008",
        "supervised",
        "66.3",
        "Joanis et al.",
        "2008",
        "supervised",
        "58.4",
        "Stevenson et al.",
        "2003",
        "semi-supervised unsupervised",
        "29 31",
        "SPEC",
        "unsupervised",
        "57.55",
        "T2",
        "Sun et al.",
        "2008",
        "supervised unsupervised",
        "62.50 51.6",
        "O Séaghdha et al.",
        "2008",
        "supervised",
        "67.3",
        "SPEC",
        "unsupervised",
        "80.35",
        "On Tl, the best performing supervised method reported so far is that of Li and Brew (2008).",
        "Li and Brew used Bayesian Multinomial Regression for classification.",
        "A range of feature sets integrating cos, SCFs and/or LPs were evaluated.",
        "The combination of cos and SCFs gave the best result, shown in the table.",
        "Joanis et al.",
        "(2008) report the second best supervised result on Tl, using Support Vector Machines for classification and features derived from linguistic analysis: syntactic slots, slot overlaps, tense, voice, aspect, and animacy of NPs.",
        "Stevenson and Joanis (2003) report a semi-and unsupervised experiment on Tl.",
        "A feature set similar to that of Joanis et al.",
        "(2008) was employed (features were selected in a semi-supervised fashion) and hierarchical clustering was used.",
        "Our unsupervised method SPEC performs substantially better than the unsupervised method of Stevenson et al.",
        "and nearly as well as the supervised approach of Joanis et al.",
        "(2008) (note, however, that the different experiments involved different subsets of Tl so are not entirely comparable).",
        "On T2, the best performing supervised method so far is that of O Séaghdha and Copestake (2008) which employs a distributional kernel method to classify SCF features parameterized for prepositions in the automatically acquired VALEX lexicon.",
        "Using exactly the same data and feature set, Sun et al.",
        "(2008) obtain a slightly lower result when using a supervised method (Gaussian) and a notably lower result when using an unsupervised method (PC clustering).",
        "Our method performs considerably better and also outperforms the supervised method of O Séaghdha and Copestake (2008)."
      ]
    },
    {
      "heading": "7. Conclusion and Future Work",
      "text": [
        "We introduced a new approach to verb clustering which involves the use of (i) rich lexical, SCF and GR data produced by a recent SCF system, (ii) novel syntactic-semantic feature sets which combine a variety of linguistic information, and (iii) a new variation of spectral clustering which is particularly suited for dealing with the resulting, high dimensional feature space.",
        "Using this approach, we showed on two well-established test sets that automatically acquired SPs can be highly useful for verb clustering.",
        "This result contrasts with most previous works but is in line with theoretical work on verb classification which relies not only on syntactic but also on semantic features (Levin, 1993).",
        "In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification.",
        "Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes.",
        "Brockmann and Lapata (2003) have showed that WordNet-based approaches do not always outperform simple frequency-based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach (Erk, 2007; Bergsma et al., 2008).",
        "The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.",
        "In addition, we plan to investigate other potentially useful features for verb classification (e.g. named entities and preposition classes) and explore semi-automatic ML technology and active learning for guiding the classification.",
        "Finally, we plan to conduct a bigger experiment with a larger number of verbs, and conduct evaluation in the context of practical application tasks."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Our work was funded by the Dorothy Hodgkin Postgraduate Award, the Royal Society University Research Fellowship, and the EPSRC grant EP/F030061/1, UK.",
        "We would like to thank Paula Buttery for letting us use her implementation of the SCF classifier and Yuval Krymolowski for the support he provided for feature extraction."
      ]
    }
  ]
}
