{
  "info": {
    "authors": [
      "Nan Duan",
      "Mu Li",
      "Tong Xiao",
      "Ming Zhou"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1114",
    "title": "The Feature Subspace Method for SMT System Combination",
    "url": "https://aclweb.org/anthology/D09-1114",
    "year": 2009
  },
  "references": [
    "acl-C08-1005",
    "acl-D07-1105",
    "acl-D08-1011",
    "acl-E06-1005",
    "acl-J03-1002",
    "acl-J04-4002",
    "acl-N07-1029",
    "acl-N07-2015",
    "acl-P02-1038",
    "acl-P03-1021",
    "acl-P05-1033",
    "acl-P05-3026",
    "acl-P06-1066",
    "acl-P06-1077",
    "acl-P07-1040",
    "acl-W04-3250",
    "acl-W06-1606",
    "acl-W07-0736",
    "acl-W08-0329"
  ],
  "sections": [
    {
      "text": [
        "Nan Duan, Mu Li, Tong Xiao, Ming Zhou",
        "Tianjin University Microsoft Research Asia Northeastern University Tianjin, China Beijing, China Shenyang, China",
        "Recently system combination has been shown to be an effective way to improve translation quality over single machine translation systems.",
        "In this paper, we present a simple and effective method to systematically derive an ensemble of SMT systems from one baseline linear SMT model for use in system combination.",
        "Each system in the resulting ensemble is based on a feature set derived from the features of the baseline model (typically a subset of it).",
        "We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work.",
        "Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks.",
        "Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years.",
        "Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance.",
        "As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research.",
        "There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality.",
        "A system combination model usually takes n-best translations of single systems as input, and depending on the combination strategy, different methods can be used.",
        "Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word-level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and La-vie, 2005; Matusov et al., 2006; Sim et al., 2007) .",
        "Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise.",
        "Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008) , most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way.",
        "To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs.",
        "In this paper, we address the problem of building an ensemble of diversified machine translation systems from a single translation engine for system combination.",
        "In particular, we propose a novel Feature Subspace method for the ensemble construction based on any baseline SMT model which can be formulated as a standard linear function.",
        "Each system within the ensemble is based on a group of features directly derived from the baseline model with minimal efforts (which is typically a subset of the features used in the baseline model), and the resulting system is optimized in the derived feature space accordingly.",
        "We evaluated our method on the test sets for",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1096-1104, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "machine translation tasks using two baseline SMT systems with state-of-the-art performance.",
        "Experimental results show that the feature subspace method can bring significant improvements to both baseline systems.",
        "The rest of the paper is organized as follows.",
        "The motivation of our work is described on Section 2.",
        "In Section 3, we first give a detailed description about feature subspace method, including the principle to select subspaces from all possible options, and then an «-gram consensus based sentence-level system combination method is presented.",
        "Experimental results are given in Section 4.",
        "Section 5 discusses some related issues and concludes the paper."
      ]
    },
    {
      "heading": "2. Motivation",
      "text": [
        "Our motivations for this work can be described in the following two aspects.",
        "The first aspect is related to the cost of building single systems for system combination.",
        "In previous work, the SMT systems used in combination differ mostly in two ways.",
        "One is the underlying models adopted by individual systems.",
        "For example, using an ensemble of systems respectively based on phrase-based models, hierarchical models or even syntax-based models is a common practice.",
        "The other is the methods used for feature function estimation such as using different word alignment models, language models or distortion models.",
        "For the first solution, building a new SMT system with different methodology is by no means an easy task even for an experienced SMT researcher, because it requires not only considerable effects to develop but also plenty of time to accumulate enough experiences to fine tune the system.",
        "For the second alternative, usually it requires time-consuming retraining for word alignment or language models.",
        "Also some of the feature tweaking in this solution is system or language specific, thus for any new systems or language pairs, human engineering has to be involved.",
        "For example, using different word segmentation methods for Chinese can generate different word alignment results, and based on which a new SMT system can be built.",
        "Although this may be useful to combination of Chinese-to-English translation, it is not applicable to most of other language pairs.",
        "Therefore it will be very helpful if there is a lightweight method that enables the SMT system ensemble to be systematically constructed based on an existing SMT system.",
        "Table 1: An example of translations generated from the same decoder but with different feature settings.",
        "The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the ensemble.",
        "Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment.",
        "To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1.",
        "In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005).",
        "In the default setting all the features are used as usual in the decoder, and we find that the translation of the Chinese word MjK (sea water) is missing in the output.",
        "This can be explained with the data shown in Table 2.",
        "Because of noises and word alignment errors in the parallel training data, the inaccurate translation phrase MjK Mit = desalination is assigned with a high value of the phrase translation probability feature p(e\\f).",
        "Although the correct translation can also be composed by two phrases V§7jC == sea water and Mit = desalination, its overall translation score cannot beat the incorrect one because the combined phrase translation probability of these two phrases are much smaller than p(desalination\\M7 Mit) .",
        "However, if we intentionally remove the p(e\\f) feature from the model, the preferred translation can be generated as shown in the result of FS-PEF because in this way the bad estimation of p(e\\f) for this phrase is avoided.",
        "Source sentence",
        "+h mm w M7k mit xm &p ftih",
        "Ref translation",
        "China's largest sea water desalini-zation project settles in Zhoushan",
        "Default translation",
        "China 's largest desalination project in Zhoushan",
        "translation",
        "China 's largest sea water desalination project in Zhoushan",
        "Chinese",
        "English",
        "1",
        "n& mi",
        "desalination",
        "0.4000",
        "2",
        "sea water",
        "0.1748",
        "3",
        "mit",
        "desalination",
        "0.0923",
        "This example gives us the hint that building decoders based on subspaces of a standard model could help with working around some negative impacts of inaccurate estimations of feature values for some input sentences.",
        "The subspace-based systems are expected to work similarly to statistical classifiers trained on subspaces of a full feature space - though the overall accuracy of baseline system might be better than any individual systems, for a specific sentence some individual systems could generate better translations.",
        "It is expected that employing an ensemble of subspace-based systems and making use of consensus between them will outperform the baseline system.",
        "3 Feature Subspace Method for SMT System Ensemble Construction",
        "In this section, we will present in detail the method for systematically deriving SMT systems from a standard linear SMT model based on feature subspaces for system combination.",
        "Nowadays most of the state-of-the-art SMT systems are based on linear models as proposed in Och and Ney (2002).",
        "Let hm (f, e) be a feature function, and Am be its weight, an SMT model D can be formally written as:",
        "Noticing that Equation (1) is a general formulation independent of any specific features, technically for any subset of features used in D, a new SMT system can be constructed based on it, which we call a sub-system.",
        "Next we will use 0 to denote the full feature space defined by the entire set of features used in D, and s Q 0 is a feature subset that belongs to p(0), the power set of 0.",
        "The derived subsystem based on subset s Q 0 is denoted by ds.",
        "Although in theory we can use all the subsystems derived from every feature subset in p(0), it is still desirable to use only some of them in practice.",
        "The reasons for this are twofold.",
        "First, the number of possible subsystems (2'°') is exponential to the size of 0.",
        "Even when the number of features in 0 is relatively small, i.e. 10, there will be up to 1024 subsystems in total, which is a large number for combination task.",
        "Larger feature sets will make the system combination practically infeasible.",
        "Second, not every subsystem could contribute to the system combination.",
        "For example, feature subsets only containing very small number of features will lead to subsystems with very poor performance; and the language model feature is too important to be ignored for a subsystem to achieve reasonably good performance.",
        "In our work, we only consider feature subspaces with only one difference from the features in 0.",
        "For each non-language model feature ht, a subsystem dt is built by removing ht from 0.",
        "Allowing for the importance of the language model (LM) feature to an SMT model, we do not remove any LM feature from any sub-system.",
        "Instead, we try to weaken the strength of a LM feature by lowering its «-gram order.",
        "For example, if a 4-gram language model is used in the baseline system D, then a trigram model can be used in one sub-system, and a bigram model can be used in another.",
        "In this way more than one subsystem can be derived based on one LM feature.",
        "When varying a language model feature, the one-feature difference principle is still kept: if we lower the order of a language model feature, no other features are removed or changed.",
        "The remaining issue of using weakened LM features is that the resulting ensemble is no longer strictly based on subspace of 0.",
        "However, this theoretical imperfection can be remedied by introducing 0 , a super-space of 0 to include all lower-order LM features.",
        "In this way, an augmented baseline system D can be built based on 0 , and the baseline system D itself can also be viewed as a subsystem of D .",
        "We will show in the experimental section that D actually performs even slightly better than the original baseline system D, but results of subsystem combination are significantly better that both D and D .",
        "After the subsystem ensemble is constructed, each subsystem tunes its feature weights independently to optimize the evaluation metrics on the development set.",
        "Let T> = [d1,..., dn} be the set of subsystems obtained by either removing one non-LM feature or changing the order of a LM feature, and \"H^ be the n-best list produced by dt.",
        "Then H(T>), the translation candidate pool to the system combination model can be written as:",
        "The advantage of this method is that it allows us to systematically build an ensemble of SMT systems at a very low cost.",
        "From the decoding perspective, all the subsystems share a common decoder, with minimal extensions to the baseline systems to support the use of specified subset of feature functions to compute the overall score for translation hypotheses.",
        "From the model training perspective, all the non-LM feature functions can be estimated once for all sub-systems.",
        "The only exception is the language model feature, which may be of different values across multiple subsystems.",
        "However, since lower-order models have already been contained in higher-order model for the purpose of smoothing in almost all statistical language model implementations, there is also no extra training cost.",
        "In our work, we use a sentence-level system combination model to select best translation hypothesis from the candidate pool H( ) .",
        "This method can also be viewed to be a hypotheses re-ranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method (Rosti et al., 2007).",
        "The score function in our combination model is formulated as follows:",
        "where hLM(e) is the language model score for e, is the length of , and ( , H( )) is a translation consensus based scoring function.",
        "The computation of ( , H( )) is further decomposed into weighted linear combination of a set of n-gram consensus based features, which are defined in terms of the order of n-gram to be matched between current candidate and other translation in H( ).",
        "Given a translation candidate e, the n-gram agreement feature between and other translations in the candidate pool is defined as:",
        "where the function Gn (e, e ) counts the occurrences of n-grams of e in e :",
        "Here S(-,-) is the indicator function -8(e\\+n-1, e ) is 1 when the n-gram e\\+n~x appears in e , otherwise it is 0.",
        "In order to give the combination model an opportunity to penalize long but inaccurate translations, we also introduce a set of n-gram disagreement features in the combination model:",
        "Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2 m + 2 if m orders of n-gram are to be matched in computing ip(e,H(T>)).",
        "Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003).",
        "Our method is similar to the work proposed by Hildebrand and Vogel (2008).",
        "However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al.",
        "(2009) did in their co-decoding method."
      ]
    },
    {
      "heading": "4. Experiments 4.1 Data",
      "text": [
        "Chinese-to-English translation tasks.",
        "Both corpora provide 4 reference translations per source sentence.",
        "Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model.",
        "Translation performance was measured in terms of case-insensitive NIST version of BLEU score which computes the brevity penalty using the shortest reference translation for each segment, and all the results will be reported in percentage numbers.",
        "Statistical significance is computed using the bootstrap re-sampling method proposed by Koehn (2004).",
        "Statistics of the data sets are summarized in Table 3.",
        "Data set",
        "#Sentences",
        "#Words",
        "MT03 (dev)",
        "919",
        "23,782",
        "MT04 (test)",
        "1,788",
        "47,762",
        "MT05 (test)",
        "1,082",
        "29,258",
        "We use the parallel data available for the NIST 2008 constrained track of Chinese-to-English machine translation task as bilingual training data, which contains 5.1M sentence pairs, 128M Chinese words and 147M English words after pre-processing.",
        "GIZA++ toolkit (Och and Ney, 2003) is used to perform word alignment in both directions with default settings, and the intersect-diag-grow method is used to generate symmetric word alignment refinement.",
        "The language model used for all systems is a 5-gram model trained with the English part of bilingual data and Xinhua portion of LDC English Giga-word corpus version 3.",
        "In experiments, multiple language model features with the order ranging from 2 to 5 can be easily obtained from the 5-gram one without retraining.",
        "Theoretically our method is applicable to all linear model based SMT systems.",
        "In our experiments, two in-house developed systems are used to validate our method.",
        "The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005).",
        "Phrasal rules are extracted from all bilingual sentence pairs, while hierarchical rules with variables are extracted from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words.",
        "The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al.",
        "(2006).",
        "All bilingual data are used to extract phrases up to length 3 on the source side.",
        "In following experiments, we only consider removing common features shared by both baseline systems for feature subspace generation.",
        "Rule penalty feature and lexicalized reordering feature, which are particular to SYS1 and SYS2, are not used.",
        "We list the features in consideration as follows:",
        "• PEF and PFE: phrase translation probabilities p(e\\f) and p(f\\e)",
        "• PEFLEX and PFELEX: lexical weights Plex (e\\f) and plex(f\\e)",
        "• PP: phrase penalty",
        "• WP: word penalty",
        "• BLP: bi-lexicon pair counting how many entries of a conventional lexicon co-occurring in a given translation pair",
        "• LM-n: language model with order n",
        "Based on the principle described in Section 3.1, we generate a number of feature subspaces for each baseline system as follows:",
        "• For non-LM features (PEF, PFE, PEFLEX, PFELEX, PP, WP and BLP), we remove one of them from the full feature space each time.",
        "Thus 7 feature subspaces are generated, which are denoted as FS-PEF , FS-PFE ,",
        "FS-PEFLEX , FS-PFELEX , FS-pp , FS-WP Mid FS-BLP respectively.",
        "The 5-gram LM feature is used in each of them.",
        "• For LM features (LM-n), we change the order from 2 to 5 with all the other non-LM features present.",
        "Thus 4 LM-related feature subspaces are generated, which are denoted",
        "as FSlm-2 , FSlm-3 , FSlm-a and FSlm-s respectively.",
        "FSLM-s is essentially the full feature space of baseline system.",
        "For each baseline system, we construct a total of 11 subsystems by using above feature subspaces.",
        "The baseline system is also contained within them because of using FSlm-5 .",
        "We call all subsystems are non-baseline subsystems except the one derived by using FSlm-s .",
        "By default, the beam size of 60 is used for all systems in our experiments.",
        "The size of n-best list is set to 20 for each sub-system, and for baseline systems, this size is set to 220, which equals to the size of the combined n-best list generated by total 11 sub-systems.",
        "The order of n-gram agreement and disagreement features used in sentence-level combination model ranges from unigram to 4-gram.",
        "We first evaluate the oracle performance on the n-best lists of baseline systems and on the combined n-best lists of subsystems generated from each baseline system.",
        "The oracle translations are obtained by using the metric of sentence-level BLEU score (Ye et al., 2007).",
        "Table 4 shows the evaluation results, in which Baseline stands for baseline system with a 5-gram LM feature, and FS stands for 11 subsystems derived from the baseline system.",
        "SYS1",
        "SYS2",
        "BLEU/TER",
        "BLEU/TER",
        "MT04",
        "Baseline",
        "49.68/0.6411",
        "49.50/0.6349",
        "FS",
        "51.05/0.6089",
        "50.53/0.6056",
        "MT05",
        "Baseline",
        "48.89/0.5946",
        "48.37/0.5944",
        "FS",
        "50.69/0.5695",
        "49.81/0.5684",
        "Baseline",
        "For both SYS1 and SYS2, feature subspace method achieves higher oracle BLEU and lower",
        "TER scores on both MT04 and MT05 test sets, which gives the feature subspace method more potential to achieve higher performance than the baseline systems.",
        "We then investigate the ratio of translation candidates in the combined n-best lists of non-baseline subsystems that are not included in the baseline's n-best list.",
        "Table 5 shows the statistics.",
        "From Table 5 we can see that only less than half of the translation candidates of subsystems overlap with those the of baseline systems.",
        "This result, together with the oracle BLEU and TER score estimation, helps eliminate the concern that no diversities or better translation candidates can be obtained by using sub-systems.",
        "SMT System",
        "Next we validate the effect of feature subspace method on single SMT systems.",
        "Figure 1 shows the evaluation results of different systems on the MT05 test set.",
        "From the figure we can see that the overall accuracy of baseline systems is better than any of their derived sub-systems, and except the subsystem derived by using FSlm-2, the performance of all the systems are fairly similar.",
        "FS-PEFLEX FS-PFELEX",
        "SYS1 SYS2 MFS-LM-4",
        "We then evaluate the system combination method proposed in Section 3.2 with all the subsystems for each baseline system.",
        "Table 6 shows the results on both MT04 and MT05 data sets, in which FS-Comb denotes the system combination using 11 sub-systems.",
        "From Table 6 we can see that by using FS-Comb we obtain about 1.1~1.3 points of BLEU gains over baseline systems.",
        "We also include in Table 6 the results for Baseline+mLM, which stands for the augmented baseline system as described in Section 3.1 using a bunch of LM features from bigram to 5-gram.",
        "It can be seen that both augmented baseline systems outperform their corresponding baseline systems slightly but consistently on both data sets.",
        "Table 6: Translation results of Baseline, Base-line+mLM and FS-Comb (+: significant better than baseline system with p < 0.05; ++: significant better than baseline system with p < 0.01; *: no significant improvement).",
        "We also investigate the results when we incrementally add the n-best list of each subsystem into a candidate pool to see the effects when different numbers of subsystems are used in combination.",
        "In order to decide the sequence of subsystems to add, we first evaluate the performance of pairwise combinations between each subsystem and its baseline system on the development set.",
        "That is, for each sub-system, we combine its n-best list with the n-best list of its baseline system and perform system combination for MT03 data set.",
        "Then we rank the subsystems by the pairwise combination performance from high to low, and use this ranking as the sequence to add n-best lists of sub-systems.",
        "Each time when a new n-best list is added, the combination performance based on the enlarged candidate pool is evaluated.",
        "Figure 2 shows the results on both MT04 and MT05 test sets, in which SYSl-fs and SYS2-fs denote the subsystems derived from SYS1 and SYS2 respectively, and X-axis is the number of subsystems used for combination each time and Y-axis is the BLEU score.",
        "From the figure we can see that although in some cases the performance slightly drops when a new subsystem is added, generally using more subsystems always leads to better results.",
        "MT04",
        "MT05",
        "SYS1",
        "69.71%",
        "69.69%",
        "SYS2",
        "59.07%",
        "58.54%",
        "MT04",
        "MT05",
        "Baseline",
        "39.07",
        "38.72",
        "SYS1",
        "Baseline+mLM",
        "39.34+",
        "39.14+",
        "FS-Comb",
        "40.43++",
        "39.79++",
        "Baseline",
        "38.84",
        "38.30",
        "SYS2",
        "Baseline+mLM",
        "38.95*",
        "38.63+",
        "FS-Comb",
        "39.92++",
        "39.49++",
        "Next we examine the performance of baseline systems when different beam sizes are used in decoding.",
        "The results on MT05 test set are shown in Figure3, where X-axis is the beam size.",
        "In Figure 3, SYS1+mLM and SYS2+mLM denote augmented baseline systems of SYS1 and SYS2 with multiple LM features.",
        "From Figure 3 we can see that augmented baseline systems (with multiple LM features) outperform the baseline systems (with only one LM feature) for all beam sizes ranging from 20 to 220.",
        "In this experiment we did not observe any significant performance improvements when using larger beam sizes than the default setting, but using more subsystems in combination almost always bring improvements.",
        "indicate that almost half of the final translations are contributed by the non-baseline sub-systems.",
        "In order to find the optimal size of n-best list for combination, we compare the combination results of using list sizes from 10-best up to 500-best for each sub-system.",
        "In this experiment, system combination was performed on the combined n-best list from total 11 subsystems with different list size each time.",
        "Figure 4 shows the results on the MT03 dev set and the MT04 and MT05 test sets for both SYS1 and SYS2.",
        "X-axis is the n-best list size of each sub-system.",
        "ooooooooooo",
        "Finally, we investigate the ratio of final translations coming from the n-best lists of non-baseline subsystems only.",
        "Table 7 shows the results on both MT04 and MT05 test sets, which",
        "We can see from the figure that for all data sets the optimal n-best list size is around 50, but the improvements are not significant over the results when 20-best translations are used.",
        "The reason for the small optimal n-best list size could be that the low-rank hypotheses might introduce more noises into the combined translation candidate pool for sentence-level combination (Hasan et al., 2007; Hildebrand and Vogel, 2008).",
        "In the last experiment, we investigate the effect of feature subspace method when multiple SMT systems are used in system combination.",
        "Evaluation results are reported in Table 8.",
        "The system combination method described in Section 3.2 is used to combine outputs from two baseline systems (with only one 5-gram LM feature) and subsystems generated from both baseline systems (22 in total), with their results denoted as Baseline Comb (both) and FS Comb (both) respectively.",
        "We also include the combination results of subsystems based on one baseline system for reference in the table.",
        "MT04",
        "MT05",
        "SYS1-fs",
        "44.63%",
        "46.12%",
        "SYS2-fs",
        "47.54%",
        "44.73%",
        "On both MT04 and MT05 test sets, the results of system combination based on subsystems are significantly better than those of baseline systems, which show that our method can also help with system combination when more than one system are used.",
        "We can also see that using multiple systems based on different SMT models and using our subspace based method can help each other: the best performance can only be achieved when both are employed."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, we have presented a novel and effective Feature Subspace method for the construction of an ensemble of machine translation systems based on a baseline SMT model which can be formulated as a standard linear function.",
        "Each system within the ensemble is based on a subset of features derived from the baseline model, and the resulting ensemble can be used in system combination to improve translation quality.",
        "Experimental results on NIST Chinese-to-English translation tasks show that our method can bring significant improvements to two baseline systems with state-of-the-art performance, and it is expected that our method can be employed to improve any linear model based SMT systems.",
        "There is still much room for improvements in the current work.",
        "For example, we still use a simple one-feature difference principle for feature subspace generation.",
        "In the future, we will explore more possibilities for feature subspaces selection and experiment with our method in a word-level system combination model.",
        "MT04",
        "MT05",
        "Baseline Comb (both)",
        "39.98",
        "39.43",
        "FS-Comb (SYS1)",
        "40.43",
        "39.79",
        "FS-Comb (SYS2)",
        "39.92",
        "39.49",
        "FS Comb (both)",
        "40.96",
        "40.38"
      ]
    }
  ]
}
