{
  "info": {
    "authors": [
      "Aminul Islam",
      "Diana Zaiu Inkpen"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1129",
    "title": "Real-Word Spelling Correction using Google Web 1T 3-grams",
    "url": "https://aclweb.org/anthology/D09-1129",
    "year": 2009
  },
  "references": [
    "acl-J99-1003"
  ],
  "sections": [
    {
      "text": [
        "Real-Word Spelling Correction using Google Web IT 3-grams",
        "mdislam@site.uottawa.ca",
        "Diana Inkpen",
        "diana@site.uottawa.ca",
        "We present a method for detecting and correcting multiple real-word spelling errors using the Google Web IT 3-gram data set and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm.",
        "Our method is focused mainly on how to improve the detection recall (the fraction of errors correctly detected) and the correction recall (the fraction of errors correctly amended), while keeping the respective precisions (the fraction of detections or amendments that are correct) as high as possible.",
        "Evaluation results on a standard data set show that our method outperforms two other methods on the same task."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Real-word spelling errors are words in a text that occur when a user mistakenly types a correctly spelled word when another was intended.",
        "Errors of this type may be caused by the writer's ignorance of the correct spelling of the intended word or by typing mistakes.",
        "Such errors generally go unnoticed by most spellcheckers as they deal with words in isolation, accepting them as correct if they are found in the dictionary, and flagging them as errors if they are not.",
        "This approach would be sufficient to detect the non-word error myss in \"It doesn't know what the myss is all about.\"",
        "but not the real-word error muss in \"It doesn't know what the muss is all about.\"",
        "To detect the latter, the spell-checker needs to make use of the surrounding context such as, in this case, to recognise that fuss is more likely to occur than muss in the context of all about.",
        "Ironically, errors of this type may even be caused by spelling checkers in the correction of non-word spelling errors when the auto-correct feature in some word-processing software sometimes silently change a non-word to the wrong real word (Hirst and Budanitsky, 2005), and sometimes when correcting a flagged error, the user accidentally make a wrong selection from the choices offered (Wilcox-O'Hearn et al., 2008).",
        "An extensive review of real-word spelling correction is given in (Pedler, 2007; Hirst and Budanitsky, 2005) and the problem of spelling correction more generally is reviewed in (Kukich, 1992).",
        "The Google Web IT data set (Brants and Franz, 2006), contributed by Google Inc., contains English word n-grams (from unigrams to 5-grams) and their observed frequency counts calculated over 1 trillion words from web page text collected by Google in January 2006.",
        "The text was tokenised following the Penn Treebank tokenisa-tion, except that hyphenated words, dates, email addresses and URLs are kept as single tokens.",
        "The sentence boundaries are marked with two special tokens <S> and </S>.",
        "Words that occurred fewer than 200 times were replaced with the special token <UNK>.",
        "Table 1 shows the data sizes of the Web IT corpus.",
        "The n-grams themselves must appear at least 40 times to be included in the Web IT corpus.",
        "It is expected that this data will be useful for statistical language modeling, e.g.,",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1241-1249, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "Table 1",
        ": Google Web IT Data Sizes",
        "Number of",
        "Number",
        "Size on disk",
        "(in KB)",
        "Tokens",
        "1,024,908,267,229",
        "N/A",
        "Sentences",
        "95,119,665,584",
        "N/A",
        "Unigrams",
        "13,588,391",
        "185,569",
        "Bigrams",
        "314,843,401",
        "5,213,440",
        "Trigrams",
        "977,069,902",
        "19,978,540",
        "4-grams",
        "1,313,818,354",
        "32,040,884",
        "5-grams",
        "1,176,470,663",
        "33,678,504",
        "for machine translation or speech recognition, as well as for other uses.",
        "In this paper, we present a method for detecting and correcting multiple real-word spelling errors using the Google Web IT 3-gram data set, and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm (details are in section 3.1).",
        "By multiple errors, we mean that if we have n words in the input sentence, then we try to detect and correct at most n-\\ errors.",
        "We do not try to detect and correct an error, if any, in the first word as it is not computationally feasible to search in the Google Web IT 3-grams while keeping the first word in the 3-gram as a variable.",
        "Our intention is to focus on how to improve the detection recall (the fraction of errors correctly detected) or correction recall (the fraction of errors correctly amended) while maintaining the respective precisions (the fraction of detections or amendments that are correct) as high as possible.",
        "The reason behind this intention is that if the recall for any method is around 0.5, this means that the method fails to detect or correct around 50 percent of the errors.",
        "As a result, we can not completely rely on these type of methods, for that we need some type of human interventions or suggestions to detect or correct the rest of the undetected or uncorrected errors.",
        "Thus, if we have a method that can detect or correct almost 80 percent of the errors, even generating some extra candidates that are incorrect is more helpful to the human.",
        "This paper is organized as follow: Section 2 presents a brief overview of the related work.",
        "Our proposed method is described in Section 3.",
        "Evaluation and experimental results are discussed in Section 4.",
        "We conclude in Section 5."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Work on real-word spelling correction can roughly be classified into two basic categories: methods based on semantic information or human-made lexical resources, and methods based on machine learning or probability information.",
        "Our proposed method falls into the latter category.",
        "The 'semantic information' approach first proposed by Hirst and St-Onge (1998) and later developed by Hirst and Budanitsky (2005) detected semantic anomalies, but was not restricted to checking words from predefined confusion sets.",
        "This approach was based on the observation that the words that a writer intends are generally semantically related to their surrounding words, whereas some types of real-word spelling errors are not, such as (using Hirst and Budanitsky's example), \"It is my sincere hole (hope) that you will recover swiftly.\"",
        "Such \"malapropisms\" cause \"a perturbation of the cohesion (and coherence) of a text.\"",
        "Hirst and Budanitsky (2005) use semantic distance measures in WordNet (Miller et al., 1993) to detect words that are potentially anomalous in context - that is, semantically distant from nearby words; if a variation in spelling results in a word that was semantically closer to the context, it is hypothesized that the original word is an error (a \"malapropism\") and the closer word is its correction.",
        "Machine learning methods are regarded as lexical disambiguation tasks and confusion sets are used to model the ambiguity between words.",
        "Normally, the machine learning and statistical approaches rely on predefined confusion sets, which are sets (usually pairs) of commonly confounded words, such as {their, there, they're} and {principle, principal}.",
        "The methods learn the characteristics of typical context for each member of the set and detect situations in which one member occurs in context that is more typical of another.",
        "Such methods, therefore, are inherently limited to a set of common, predefined errors, but such errors can include both content and function words.",
        "Given an occurrence of one of its confusion set members, the spellchecker's job is to predict which member of that confusion set is the most appropriate in the context.",
        "Golding and Roth (1999), an example of a machine-learning method, combined the Winnow algorithm with weighted-majority voting, using nearby and adjacent words as features.",
        "Another example of a machine-learning method is that of Carlson etal.",
        "(2001).",
        "Mays et al.",
        "(1991) proposed a statistical method using word-trigram probabilities for detecting and correcting real-word errors without requiring predefined confusion sets.",
        "In this method, if the trigram-derived probability of an observed sentence is lower than that of a sentence obtained by replacing one of the words with a spelling variation, then we hypothesize that the original is an error and the variation is what the user intended.",
        "Wilcox-O'Hearn et al.",
        "(2008) analyze the advantages and limitations of Mays et al.",
        "(1991)'s method, and present a new evaluation of the algorithm, designed so that the results can be compared with those of other methods, and then construct and evaluate some variations of the algorithm that use fixed-length windows.",
        "They consider a variation of the method that optimizes over relatively short, fixed-length windows instead of over a whole sentence (except in the special case when the sentence is smaller than the window), while respecting sentence boundaries as natural breakpoints.",
        "To check the spelling of a span of d words requires a window of length d+4 to accommodate all the trigrams that overlap with the words in the span.",
        "The smallest possible window is therefore 5 words long, which uses 3 trigrams to optimize only its middle word.",
        "They assume that the sentence is bracketed by two BoS and two EoS markers (to accommodate trigrams involving the first two and last two words of the sentence).",
        "The window starts with its left-hand edge at the first BoS marker, and the Mays et al.",
        "(1991)'s method is run on the words covered by the trigrams that it contains; the window then moves d words to the right and the process repeats until all the words in the sentence have been checked.",
        "As Mays et al.",
        "(1991)'s algorithm is run separately in each window, potentially changing a word in each, Wilcox-O'Hearn et al.",
        "(2008)'s method as a side-effect also permits multiple corrections in a single sentence.",
        "Wilcox-O'Hearn et al.",
        "(2008) show that the trigram-based real-word spelling-correction method of Mays et al.",
        "(1991) is superior in performance to the WordNet-based method of Hirst and Budanitsky (2005), even on content words (\"malapropisms\"), especially when supplied with a realistically large trigram model.",
        "Wilcox-O'Hearn et al.",
        "(2008) state that their attempts to improve the method with smaller windows and with multiple corrections per sentence were not successful, because of excessive false positives.",
        "Verberne (2002) proposed a trigram-based method for real-word errors without explicitly using probabilities or even localizing the possible error to a specific word.",
        "This method simply assumes that any word trigram in the text that is attested in the British National Corpus (Burnard, 2000) is correct, and any unattested trigram is a likely error.",
        "When an unattested trigram is observed, the method then tries the spelling variations of all words in the trigram to find attested trigrams to present to the user as possible corrections.",
        "The evaluation of this method was carried out on only 7100 words of the Wall Street Journal corpus, with 31 errors introduced (i.e., one error in every approximately 200 words) obtaining a recall of 0.33 for correction, a precision of 0.05 and a F-measure of 0.086."
      ]
    },
    {
      "heading": "3. Proposed Method",
      "text": [
        "The proposed method first tries to determine some probable candidates and then finds the best one among the candidates or sorts them based on some weights.",
        "We consider a string similarity function and a normalized frequency value function in our method.",
        "The following sections present a detailed description of each of these functions followed by the procedure to determine some probable candidates along with the procedure to sort the candidates.",
        "We use the longest common subsequence (LCS) (Allison and Dix, 1986) measure with some normalization and small modifications for our string similarity measure.",
        "We use the same three different modified versions of LCS that we (Islam and Inkpen, 2008) used, along with another modified version of LCS, and then take a weighted sum of these.",
        "Kondrak (2005) showed that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively.",
        "Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR).",
        "But LCSR does not take into account the length of the shorter string which sometimes has a significant impact on the similarity score.",
        "Islam and Inkpen (2008) normalized the longest common subsequence so that it takes into account the length of both the shorter and the longer string and called it normalized longest common subse-",
        "quence (NLCS) which is:",
        "len(LCS(si,Sj)Y len(si) x len(sj)",
        "While in classical LCS, the common subsequence needs not be consecutive, in spelling correction, a consecutive common subsequence is important for a high degree of matching.",
        "We (Islam and Inkpen, 2008) used maximal consecutive longest common subsequence starting at character 1, MCLCS\\ and maximal consecutive longest common subsequence starting at any character n, MCLCSn.",
        "MCLCSi takes two strings as input and returns the shorter string or maximal consecutive portions of the shorter string that consecutively match with the longer string, where matching must be from first character (character 1) for both strings.",
        "MCLCSn takes two strings as input and returns the shorter string or maximal consecutive portions of the shorter string that consecutively match with the longer string, where matching may start from any character (character n) for both of the strings.",
        "We normalized MCLCSi and MCLCSn and called it normalized MC LCS i (NMCLCSi) and normalized MCLCSn (NMCLCSn), respectively.",
        "v2 =NMCLCSi(si,Sj)",
        "len(MCLCSi(si,Sj)Y",
        "Islam and Inkpen (2008) did not consider consecutive common subsequences ending at the last character, though MCLCSn sometimes covers this, but not always.",
        "We argue that the consecutive common subsequence ending at the last character is as significant as the consecutive common subsequence starting at the first character.",
        "So, we introduce the maximal consecutive longest common subsequence ending at the last character, MCLCSZ (Algorithm 1).",
        "Algorithm 1, takes two strings as input and returns the shorter string or the maximal consecutive portions of the shorter string that consecutively matches with the longer string, where matching must end at the last character for both strings.",
        "We normalize MCLCSZ and call it normalized MCLCSZ (NMCLCSZ).",
        "len(MCLCSz(si,Sj)Y len(si) x len(sj) (4)",
        "We take the weighted sum of these individual values Vi, V2, Vs, and V4 to determine string similarity score, where a 1, 0:2, 0:3, 0:4 are weights and oti + CK2 + as + CK4 = 1.",
        "Therefore, the similarity of the two strings, S G [0,1] is:",
        "We determine the normalized frequency value of each candidate word for a single position with respect to all other candidates for the same position.",
        "If we find n replacements of a word Wi which are {wn,Wi2,--- > wij> ' ' ' > win}, and their frequencies {/a, M, ••• ,fij,-- - , fin}, where fa is the frequency of a 3-gram (where any candidate word Wij is a member of the 3-gram), then we determine the normalized frequency value of any candidate word Wij, represented as F(wij) G (0,1], as the frequency of the 3-gram having Wij over the maximum frequency among all the candidate words for that position:",
        "Our task is to correct real-word spelling error from an input text using Google Web IT 3-gram data set.",
        "Let us consider an input text W which",
        "len{MCLCSn{si, Sj)f",
        "Algorithm 1: MCLCSZ ( Maximal Consecutive LCS ending at the last character)",
        "input : Si, Sj /* Si and Sj are input strings where \\si\\ < \\sj\\ */",
        "output: str /* str is the Maximal Consecutive LCS ending at the last character */",
        "We use the following steps:",
        "1.",
        "We define the term cut off frequency for word Wi or word Wi+\\ as the frequency of the 3-gram Wi-\\ w% i«i+i in the Google Web IT 3-grams, if the said 3-gram exists.",
        "Otherwise, we set the cut off frequency of Wi as 0.",
        "The intuition behind using the cut off frequency is the fact that, if the word is misspelled, then the correct one should have a higher frequency than the misspelled one.",
        "Thus, using the cut off frequency, we isolate a large number of candidates that we do not need to process.",
        "2.",
        "We find all the 3-grams (where only Wi is changed while Wi-\\ and Wi+\\ are unchanged) having frequency greater than the cut off frequency of Wi (determined in step 1).",
        "Let us consider that we find n replacements of Wi which are R\\ = {wn, Wi2, ■ ■ ■ , Win} and their frequencies ^1 = {hi, fa, ■■■ , fin} where hj is the frequency of the 3-gram Wi-\\ w^ Wi+\\.",
        "3.",
        "We determine the cut off frequency for word Wi-\\ or word Wi as the frequency of the 3-gram Wi-2 Wi-\\ Wi in the Google Web IT 3-grams, if the said 3-gram exists.",
        "Otherwise, we set the cut off frequency of Wi as 0.",
        "4.",
        "We find all the 3-grams (where only Wi is changed while Wi-2 and Wi-\\ are unchanged) having frequency greater than the cut off frequency of w% (determined in step 3).",
        "Let us consider that we find n replacements of Wi which are R2 = {wn, Wi2, ■ ■ ■ , Win} and their frequencies",
        "We use the following steps:",
        "2.",
        "We find all the 3-grams (where only wmis changed while wm-2 and wm-i are unchanged) having frequency greater than the cut off frequency of wm (determined in step 1).",
        "Let us consider that we find n replacements of wm which are R2 = {wmi, wm2, • • • , wmn} and their frequencies",
        "F2 = {fa, fi2, • • • , fin} where is the frequency of the 3-gram Wi-2 Wi-\\ w^.",
        "3.",
        "For each wmj G R2, we calculate the string similarity between wmj and wm using equation (5) and then assign a weight using the following equation only to the words that return the string similarity value greater than 0.5.",
        "Equation (7) is used to ensure a balanced weight between the string similarity function and the normalized frequency value function where ß refers to how much importance we give to the string similarity function with respect to the normalized frequency value function.",
        "7.",
        "We sort the words found in step 5 and in step 6 that were given weights, if any, in descending order by the assigned weights and keep only one word as candidate word.",
        "1.",
        "We determine the cut off frequency for word W2 as the frequency of the 3-gram w\\W2W^ in the Google Web IT 3-grams, if the said 3-gram exists.",
        "Otherwise, we set the cut off frequency of 102 as 0.",
        "where f2j is the frequency of the 3-gram w\\",
        "W2j W3.",
        "4.",
        "We sort the words found in step 3 that were given weights, if any, in descending order by the assigned weights and keep only one word as candidate word.",
        "1.",
        "We determine the cut off frequency for word wm as the frequency of the 3-gram wm-2 wm-i wm in the Google Web IT 3-grams, if the said 3-gram exists.",
        "Otherwise, we set the cut off frequency of wm as 0.",
        "the frequency of the 3-gram wm-2 Wm-i",
        "Wmj.",
        "4.",
        "We sort the words found in step 3 that were given weights, if any, in descending order by the assigned weights and keep only one word as the candidate word."
      ]
    },
    {
      "heading": "1. str ^NULL",
      "text": [
        "3 while I Si I > c do",
        "4 x < – SubStr(Si, – c, 1) /* returns cth character of Si from the end */",
        "s y < – SubStr(sj, – c, 1) /* returns cth character of Sj from the end */",
        "6 if x = y then"
      ]
    },
    {
      "heading": "9. return str",
      "text": [
        "h increment c i2 end",
        "after tokenization has m words, i.e., = {wi,W2, • • •, Our method aims to correct",
        "m-1 spelling errors, for all m-1 word positions, except for the first word position, as we do not try to correct the first word.",
        "We use a slight different way to correct the first word (i.e., 102) and the last word (i.e., wm) among those m-1 words, than for the rest of the words.",
        "First, we discuss how we find the candidates for a word (say Wi, where 2<i<m) which is not either W2 or wm.",
        "Then, we discuss the procedure to find the candidates for either W2 or wm.",
        "Our method could have worked for the first word too.",
        "We did not do it here due",
        "- Hyphenated word are usually separated, and hyphenated numbers usually form one token.",
        "- Sequences of numbers separated by slashes (e.g., in dates) form one token.",
        "- Sequences that look like urls or email addresses form one token.",
        "to efficiency reasons.",
        "Google 3-grams are sorted based on the first word, then the second word, and so on.",
        "Based on this sorting, all Google 3-grams are stored in 97 different files.",
        "All the 97 Google 3-gram files could have been needed to access a single word, instead of accessing just one 3-gram file as we do for any other words.",
        "This is because when the first word needs to be corrected, it might be in any file among those 97 Google 3-gram files.",
        "No error appears in the first position among 1402 inserted malapropisms.",
        "The errors start appearing from the second position till the last position."
      ]
    },
    {
      "heading": "4. Evaluation and Experimental Results",
      "text": [
        "We used as test data the same data that Wilcox-O'Hearn et al.",
        "(2008) used in their evaluation of Mays et al.",
        "(1991) method, which in turn was a replication of the data used by Hirst and St-Onge (1998) and Hirst and Budanitsky (2005) to evaluate their methods.",
        "The data consisted of 500 articles (approximately 300,000 words) from the 1987-89 Wall Street Journal corpus, with all headings, identifiers, and so on removed; that is, just a long stream of text.",
        "It is assumed that this data contains no errors; that is, the Wall Street Journal contains no malapropisms or other typos.",
        "In fact, a few typos (both non-word and real-word) were noticed during the evaluation, but they were small in number compared to the size of the text.",
        "Malapropisms were randomly induced into this text at a frequency of approximately one word in 200.",
        "Specifically, any word whose base form was listed as a noun in WordNet (but regardless of whether it was used as a noun in the text; there was no syntactic analysis) was potentially replaced by any spelling variation found in the lexicon of the ispell spelling checker.",
        "A spelling variation was defined as any word with an edit distance of 1 from the original word; that is, any single-character insertion, deletion, or substitution, or the transposition of two characters, that results in another real word.",
        "Thus, none of the induced malapropisms were derived from closed-class words, and none were formed by the insertion or deletion of an apostrophe or by splitting a word.",
        "The data contained 1402 inserted malapropisms.",
        "Because it had earlier been used for evaluating Mays et al.",
        "(1991)'s trigram method, which operates at the sentence level, the data set had been divided into three parts, without regard for article boundaries or text coherence: sentences into which no malapropism had been induced; the original versions of the sentences that received malapropisms; and the malapropized sentences.",
        "In addition, all instances of numbers of various kinds had been replaced by tags such as <INTEGER>, <DOLLAR VALUE>, and PERCENTAGE VALUE>.",
        "Actual (random) numbers or values were restored for these tags.",
        "Some spacing anomalies around punctuation marks were corrected.",
        "A detailed description of this data can be found in (Hirst, 2008; Wilcox-O'Hearn et al., 2008).",
        "SUCCESSFUL CORRECTION:",
        "The Iran revelations were particularly disturbing to the Europeans because they came on the heels of the Reykjavik summit between President Reagan and Soviet reader – > leader [leader] Mikhail Gorbachev.",
        "Even the now sainted Abraham Lincoln was often reviled while in officer – > office [office], sometimes painted by cartoonists and editorial writers as that baboon in the White House.",
        "FALSE POSITIVE:",
        "• by such public displays of interest in Latinos – > Latin [Latinos], many undocumented • • •",
        "The southeast Asian nation was one reported contributor – > contribution [contributor] to the Nicaraguans.",
        "FALSE NEGATIVE:",
        "Kevin Mack, Geldermann president and chief executive officer, didn't return calls for comment on the Clayton purchaser [purchase].",
        "U.S. manufactures [manufacturers], in short, again are confronting a ball game in which they will be able to play.",
        "TRUE POSITIVE DETECTION, FALSE POSITIVE CORRECTION:",
        "Hawkeye also is known to rear – > reader [fear] that a bankruptcy-law filing by the parent company, which theoretically shouldn't affect the operations of its member banks, would spark runs on the banks that could drag down the whole entity.",
        "The London Daily News has quoted sources saying as many as 23 British mercenaries were enlisted by KMS to lid – > slide [aid] the Contras.",
        "Table 2: Examples of successful and unsuccessful corrections.",
        "Italics indicate observed word, arrow indicates correction, square brackets indicate intended word.",
        "Some examples of successful and unsuccessful corrections using our proposed method are shown in Table 2.",
        "Table 3 shows our method's results on the described data set compared with the results for the trigram method of Wilcox-O'Hearn et al.",
        "(2008)",
        "Detection correction",
        "Table 3: A comparison of recall, precision, and F\\ score for three methods of malapropism detection and correction on the same data set.",
        "and the lexical cohesion method of Hirst and Budanitsky (2005).",
        "The data shown here for trigram method are not from (Wilcox-O'Hearn et al., 2008), but rather are later results following some corrections reported in (Hirst, 2008).",
        "We have not tried optimizing our adjustable parameters: ß and as, because the whole data set was used as testing set by the other methods we compare with.",
        "To keep the comparison consistent, we did not use any portion of the data set for training purpose.",
        "Having optimized parameters could lead to a better result.",
        "The performance is measured using Recall (R), Precision (P) and F\\\\",
        "true positives true positives + false negatives",
        "true positives true positives + false positives",
        "The fraction of errors correctly detected is the detection recall and the fraction of detections that are correct is the detection precision.",
        "Again, the fraction of errors correctly amended is the correction recall and the fraction of amendments that are correct is the correction precision.",
        "To give an example, consider a sentence from the data set: \"The Philippine president, in her commencement address at the academy, complained that the U.S. was living – > giving [giving] advice instead of the aid – > said [aid] it pledged.",
        "\", where italics indicate the observed word, arrow indicates the correction and the square brackets indicate the intended word.",
        "The detection recall of this sentence is 1.0 and the precision is 0.5.",
        "The correction recall of this sentence is 1.0 and the precision is 0.5.",
        "For both cases, the F\\ score is 0.667.",
        "We loose some precision because our method tries to detect and correct errors for all the words (except the first word) in the input sentence, and, as a result, it generates more false positives than the other methods.",
        "Even so, we get better F\\ scores than the other competing methods.",
        "Accepting 8.3 percents extra incorrect detections, we get 34.6 percents extra correct detections of errors, and similarly, accepting 12.2 percents extra incorrect amendments, we get 27.2 percents extra correct amendments of errors compared with the trigrams method (Wilcox-O'Hearn et al., 2008)."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "The Google 3-grams proved to be very useful in detecting real-word errors, and finding the corrections.",
        "We did not use the 4-grams and 5-grams because of data sparsity.",
        "When we tried with 5-grams the results were lower than the ones presented in Section 4.",
        "Having sacrificed a bit the precision score, our proposed method achieves a very good detection recall (0.89) and correction recall (0.76).",
        "Our attempts to improve the detection recall or correction recall, while maintaining the respective precisions as high as possible are helpful to the human correctors who post-edit the output of the real-word spell checker.",
        "If there is no postediting, at least more errors get corrected automatically.",
        "Our method could also detect and correct misspelled words, not only malapropisms, without any modification.",
        "In future work, we plan to extend our method to allow for deleted or inserted words, and to find the corrected strings in the Google Web IT n-grams.",
        "In this way we will be able to correct grammar errors too.",
        "We also plan more experiments using the 5-grams, but backing off to 4-grams and 3-grams when needed."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is funded by the Natural Sciences and Engineering Research Council of Canada.",
        "We want to thank Professor Graeme Hirst from the Department of Computer Science, University of Toronto, for providing the evaluation data set.",
        "R",
        "P Fx R P",
        "Fi",
        "Lexical cohesion",
        "(Hirst and Budanitsky, 2005)",
        "0.306",
        "0.225 0.260 0.281 0.207",
        "0.238",
        "Trigrams",
        "(Wilcox-O'Hearn et al., 2008)",
        "0.544",
        "0.528 0.536 0.491 0.503",
        "0.497",
        "Multiple 3-grams",
        "0.890",
        "0.445 0.593 0.763 0.381",
        "0.508"
      ]
    }
  ]
}
