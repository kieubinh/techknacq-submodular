{
  "info": {
    "authors": [
      "Preslav Nakov",
      "Hwee Tou Ng"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1141",
    "title": "Improved Statistical Machine Translation for Resource-Poor Languages Using Related Resource-Rich Languages",
    "url": "https://aclweb.org/anthology/D09-1141",
    "year": 2009
  },
  "references": [
    "acl-A00-1002",
    "acl-I08-8003",
    "acl-J00-2004",
    "acl-J03-1002",
    "acl-J04-4002",
    "acl-J93-2003",
    "acl-J99-1003",
    "acl-N01-1020",
    "acl-N03-2016",
    "acl-N06-1003",
    "acl-N07-1061",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P05-1066",
    "acl-P05-1074",
    "acl-P07-1083",
    "acl-P07-1108",
    "acl-P07-2045",
    "acl-W07-0702",
    "acl-W09-0431",
    "acl-W95-0115",
    "acl-W99-0626"
  ],
  "sections": [
    {
      "text": [
        "We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones.",
        "More precisely, we improve the translation from a resource-poor source language X\\ into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X\\-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X\\.",
        "The evaluation for Indonesian^English (using Malay) and Spanish^English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recent developments in statistical machine translation (SMT), e.g., the availability of efficient implementations of integrated open-source toolkits like Moses (Koehn et al., 2007), have made it possible to build a prototype system with decent translation quality for any language pair in a few days or even hours.",
        "In theory.",
        "In practice, doing so requires having a large set of parallel sentence-aligned bilingual texts (a bi-text) for that language pair, which is often unavailable.",
        "Large high-quality bi-texts are rare; except for Arabic, Chinese, and some official languages of the European Union (EU), most of the 6,500+ world languages remain resource-poor from an SMT viewpoint.",
        "While manually creating a small bi-text could be relatively easy, building a large one is hard, e.g., because of copyright.",
        "Most bi-texts for SMT come from parliament debates and legislation of Hwee Tou Ng",
        "multi-lingual countries (e.g., French-English from Canada, and Chinese-English from Hong Kong), or from international organizations like the United Nations and the European Union.",
        "For example, the Europarl corpus of parliament proceedings consists of about 1.3M parallel sentences (up to 44M words) per language for 11 languages (Koehn, 2005), and the JRC-Acquis corpus provides a comparable amount of European legislation in 22 languages (Steinberger et al., 2006).",
        "The official languages of the EU are especially lucky in that respect; while this includes such \"classic SMT languages\" like English and French, and some important international ones like Spanish and Portuguese, most of the rest have a limited number of speakers and were resource-poor until recently; this is changing quickly because of the increasing volume of EU parliament debates and the ever-growing European legislation.",
        "Thus, becoming an official language of the EU has turned out to be an easy recipe for getting resource-rich in bi-texts quickly.",
        "Of course, not all languages are that 'lucky', but many can still benefit.",
        "In this paper, we propose using bi-texts for resource-rich language pairs to build better SMT systems for resource-poor ones by exploiting the similarity between a resource-poor language and a resource-rich one.",
        "The proposed method allows non-EU languages to benefit from being closely related to one or more official languages of the EU, the most obvious candidates being Norwegian (related to Swedish), Moldavian (related to Romanian), and Macedonian (related to Bulgarian).",
        "After Croatia joins the EU, Serbian, Bosnian and Montenegrin will be able to benefit from Croatian gradually turning resource-rich (all four split from Serbo-Croatian after the breakup of Yugoslavia).",
        "The newly-made EU-official (and thus not as resource -",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1358-1367, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "rich) Czech and Slovak are another possible pair of candidates.",
        "As we will see below, even such resource-rich languages like Spanish and Portuguese can benefit from the proposed method.",
        "Of course, many pairs of closely related languages can be also found outside of Europe, Malay and Indonesian being just one such example we will experiment with.",
        "The remainder of the present paper is organized as follows: Section 2 presents our method, Section 3 describes the experiments, and Section 4 discusses the results and the general applicability of the approach.",
        "Section 5 provides an overview of the related work.",
        "Finally, Section 6 concludes and suggests possible directions for future work."
      ]
    },
    {
      "heading": "2. Method",
      "text": [
        "We propose a novel language-independent approach for improving statistical machine translation for resource-poor languages by exploiting their similarity to resource-rich ones.",
        "More precisely, we improve the translation from a resource-poor source language X\\ into a resource-rich target language Y given a bi-text containing a limited number of parallel sentences for X\\-Y and a much larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X\\.",
        "Our method exploits the similarity between related languages with respect to word order, syntax, and, most importantly, vocabulary overlap - related languages share a large number of cognates.",
        "Before we present the method, we will describe two simple strategies for integrating the bi-text for X2-Y into a phrase-based SMT system for X\\-Y.",
        "We can simply concatenate the bi-texts for X\\-Y and X2-Y into one large bi-text and use it to train an SMT system.",
        "This offers several advantages.",
        "First, it can yield improved word alignments for the sentences that came from the X\\-Y bi-text, e.g., since the additional sentences can provide new contexts for the rare words in that bi-text; rare words are hard to align, which could have a disastrous effect on the subsequent phrase extraction stage.",
        "Second, it can provide new source-language side translation options, thus increasing the lexical coverage and reducing the number of unknown words at translation time; it can also provide new useful non-compositional phrases on the sourcelanguage side, thus yielding more fluent translation output.",
        "Third, it can offer new target-language side phrases for known source phrases, which could improve fluency by providing more translation options for the language model (LM) to choose from.",
        "Fourth, bad phrases including words from X2 that do not exist in X\\ will be effectively ignored at translation time since they could never possibly match the input, while bad new target-language translations still have the chance to be filtered out by the language model.",
        "However, simple concatenation can be problematic.",
        "First, when concatenating the small bi-text for X\\-Y with the much larger one for X2-Y, the latter will dominate during word alignment and phrase extraction, thus hugely influencing both lexical and phrase translation probabilities, which can yield poor performance.",
        "This can be counteracted by repeating the small bi-text several times so that the large one does not dominate.",
        "Second, since the bi-texts are merged mechanically, there is no way to distinguish between phrases extracted from the bi-text for X\\-Y (which should be good), from those coming from the bi-text for X2-Y (whose quality might be questionable).",
        "An alternative way of making use of the additional bi-text for X2-Y to train an improved SMT system for X\\ – > Y is to build separate phrase tables from X\\-Y and X2-Y, which can then be (a) used together, e.g., as alternative decoding paths, (b) merged, e.g., using one or more extra features to indicate the bi-text each phrase came from, or (c) interpolated, e.g., using simple linear interpolation.",
        "Building two separate phrase tables offers several advantages.",
        "First, the good phrases from the bi-text for X\\-Y are clearly distinguished from (or given a higher weight in the linear interpolation compared to) the potentially bad ones from the X2-Y bi-text.",
        "Second, the lexical and the phrase translation probabilities are combined in a principled manner.",
        "Third, using an X2-Y bi-text that is much larger than that for X\\-Y is not problematic any more.",
        "Fourth, as with bi-text merging, there are many additional source-and target-language phrases, which offer new translation options.",
        "On the negative side, the opportunity is lost to obtain improved word alignments for the sentences in the X\\-Y bi-text.",
        "Taking into account the potential advantages and disadvantages of the above strategies, we propose a method that tries to get the best of both: (i) increased lexical coverage by using additional phrase pairs independently extracted from X2-Y, and (ii) improved word alignments for X\\-Y by biasing the word alignment process with additional sentence pairs from X2-Y (possibly also repeating X\\-Y several times).",
        "A detailed description of the method follows:",
        "1.",
        "Build a bi-text Bcat that is a concatenation of the bi-texts for X\\-Y and X2-Y.",
        "Generate word alignments for Bcat, extract phrases, and build a phrase table Tcat.",
        "2.",
        "Build a bi-text Brep from the X\\-Y bi-text repeated k times followed by one copy of the X2-Y bi-text.",
        "Generate word alignments for Brep, then truncate them, only keeping word alignments for one copy of the X\\-Y bi-text.",
        "Use these word alignments to extract phrases, and build a phrase table Trep_trunc.",
        "3.",
        "Produce a phrase table Tmerged by combining Teat and Trep_trunc, giving priority to the latter, and use it in an X\\ – > Y SMT system.",
        "As we mentioned above, our method relies on the existence of a large number of cognates between related languages.",
        "While linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kon-drak, 2007; Mann and Yarowsky, 2001; Melamed, 1999).",
        "In this paper, we adopt the latter definition.",
        "Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. senor in Portuguese and Spanish), but often stem from real phonological differences.",
        "For example, the Portuguese suffix -çâo corresponds to the Spanish suffix -cion, e.g., evoluçâo vs. evolution.",
        "Such correspondences can be quite frequent and thus easy to learn automatically.",
        "Even more frequent can be the inflectional variations.",
        "For example, in Portuguese and Spanish respectively, verb endings like -ou vs. 6 (for 3rd person singular, simple past tense), e.g., visitou vs. visito, or -ei vs. -é (for 1st person singular, simple past tense), e.g., visitei vs. visité.",
        "If such systematic differences exist between the languages X\\ and X2, it might be useful to learn and to use them as a preprocessing step in order to transliterate the X2 side of the X2-Y bi-text and thus increase its vocabulary overlap with the source language X\\.",
        "We will describe our approach to automatic transliteration in more detail in Section 3.4 below."
      ]
    },
    {
      "heading": "3. Experiments 3.1 Language Pairs",
      "text": [
        "We experimented with two language pairs: the closely related Malay and Indonesian and the more dissimilar Spanish and Portuguese.",
        "Malay and Indonesian are mutually intelligible, but differ in pronunciation and vocabulary.",
        "An example follows :",
        "• Malay: Semua manusia dilahirkan bebas dan samarata dari segi kemuliaan dan hak-hak.",
        "• Indonesian: Semua orang dilahirkan merdeka dan mempunyai martabat dan hak-hak yang soma.",
        "Spanish and Portuguese also exhibit a noticeable degree of mutual intelligibility, but differ in pronunciation, spelling, and vocabulary.",
        "Unlike Malay and Indonesian, however, they also differ syntactically and have a high degree of spelling differences as demonstrated by the following examples:",
        "• Spanish: Senora Presidenta, estimados cole-gas, lo que estâ sucediendo en Oriente Medio es una tragedia.",
        "• Portuguese: Senhora Présidente, caws cole-gas, o que estâ a acontecer no Medio Oriente é uma tragédia.",
        "In our experiments, we used the following number of training sentence pairs (number of words shown in parentheses) for English (en), Indonesian (in), Malay (ml), Portuguese(pt), and Spanish (es):",
        "• Indonesian-English (in-en):",
        "- monolingual English eriin: 5.1M words.",
        "• Malay-English (ml-en):",
        "- monoling.",
        "English enmi: 27.9M words.",
        "• Spanish-English (es-en):",
        "- monolingual English enes-.pt'- 45.3M words (the same as for pt-en).",
        "• Portuguese-English (pt-en):",
        "- 1,230,038 pairs (35.9M, 34.6M words).",
        "- monolingual English enes-vt. 45.3M words (the same as for es-en).",
        "All of the above datasets contain sentences with up to 100 tokens.",
        "In addition, for each of the four language pairs, we have a development and a testing bi-text, each with 2,000 parallel sentence pairs.",
        "We made sure the development and the testing bi-texts shared no sentences with the training bi-texts; we further excluded from the monolingual English data all sentences from the English sides of the training and the development bi-texts.",
        "The training bi-text datasets for es-en and pt-en were built from release v.3 of the Europarl corpus, excluding the Q4/2000 portion out of which we created our testing and development datasets.",
        "We built the in-en bi-texts from texts that we downloaded from the Web.",
        "We translated the Indonesian texts to English using Google Translate, and we matched them against the English texts using a cosine similarity measure and heuristic constraints based on document length in words and in sentences, overlap of numbers, words in uppercase, and words in the title.",
        "Next, we extracted pairs of sentences from the matched document pairs using competitive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold.",
        "The ml-en was built in a similar manner.",
        "In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text.",
        "We then built separate directed word alignments for English^X and X^English (X G {Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect + grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004).",
        "We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty.",
        "We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-baseddistortion cost, and the parameters from the phrase table.",
        "We set all weights by optimizing Bleu (Pap-ineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish), and we used them in a beam search decoder (Koehn et al., 2007) to translate 2,000 test sentences (Indonesian or Spanish) into English.",
        "Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu.",
        "As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences.",
        "Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of automatically extracted likely cognates.",
        "The system was applied on the Portuguese side of the pt-en training bi-text.",
        "Classic approaches to automatic cognate extraction look for non-stopwords with similar spelling that appear in parallel sentences in a bi-text (Kon-drak et al., 2003).",
        "In our case, however, we need to extract cognates between Spanish and Portuguese given pt-en and es-en bi-texts only, i.e., without having a pt-es bi-text.",
        "Although it is easy to construct a pt-es bi-text from the Europarl corpus, we chose not to do so since, in general, synthesizing a bi-text for X1-X2 would be impossible: e.g., it cannot be done for ml-in given our training datasets for in-en and ml-en since the English sides of these bi-texts have no sentences in common.",
        "Thus, we extracted the list of likely cognates between Portuguese and Spanish from the training pt-en and es-en bi-texts using English as a pivot as follows: We started with IBM model 4 word alignments, from which we extracted four conditional lexical translation probabilities: Pi{pj\\ei) and Vi(ei\\pj) for Portuguese-English, and Pr{sk\\e-i) and Pr(ei|sfc) for Spanish-English, where pj, and Sk stand for a Portuguese, an English and a Spanish word respectively.",
        "Following Wu and Wang (2007), we then induced conditional lexical translation probabilities Pr(pj\\sk) and Pr(sk\\Pj) for Portuguese-Spanish as follows:",
        "Prfelsfc) = EiPr(Pilei>sfc)Pr(ei|sfc)",
        "Assuming pj is conditionally independent of Sk given ei, we can simplify the above expression:",
        "Pr(pj\\sk) = EiPr(Pj|ei)Pr(ei|sfc)",
        "Similarly, for Pr(sk\\Pj), we obtain",
        "We excluded all stopwords, words of length less than three, and those containing digits.",
        "We further calculated Prod (pj, Sk) = Pr(pj\\sk)Pr(sk\\pj), and we excluded all Portuguese-Spanish word pairs (pj,Sk) for which Prod(pj, Sk) < 0.01.",
        "From the remaining pairs, we extracted likely cognates based on Prod (p.,-, Sk) and on the orthographic similarity between pj and Sk-",
        "Following Melamed (1995), we measured the orthographic similarity using the longest common subsequence ratio (lcsr), defined as follows:",
        "where lcs(si,S2) is the longest common subsequence of si and S2, and \\s\\ is the length of s.",
        "We retained as likely cognates all pairs for which lcsr was 0.58 or higher; that value was found by Kondrak et al.",
        "(2003) to be optimal for a number of language pairs in the Europarl corpus.",
        "Finally, we performed competitive linking (Melamed, 2000), assuming that each Portuguese wordform had at most one Spanish best cognate match.",
        "Thus, using the values of Prod (p.,-, Sk), we induced a fully-connected weighted bipartite graph.",
        "Then, we performed a greedy approximation to the maximum weighted bipartite matching in that graph {i.e., competitive linking) as follows: First, we accepted as cognates the cross-lingual pair (pj, Sk) with the highest Prod (p.,-, Sk) in the graph, and we discarded pj and Sk from further consideration.",
        "Then, we accepted the next highest-scored pair, and we discarded the involved wordforms and so forth.",
        "The process was repeated until there were no matchable pairs left.",
        "As a result of the above procedure, we ended up with 28,725 Portuguese-Spanish cognate pairs, 9,201 (or 32%) of which had spelling differences.",
        "For each pair in the list of cognate pairs, we added spaces between any two adjacent letters for both wordforms, and we further appended the start and the end characters ~ and $.",
        "For example, the cognate pair evoluçâo - evolution became",
        "A evoluçâo $ – \"evolucion $",
        "We randomly split the resulting list into a training (26,725 pairs) and a development dataset (2,000 pairs), and trained and tuned a character-level phrase-based monotone SMT system similar to (Finch and Sumita, 2008) to transliterate a Portuguese wordform into a Spanish wordform.",
        "We used a Spanish language model trained on 14M word tokens (obtained from the above-mentioned 45.3M-token monolingual English corpus after excluding punctuation, stopwords, words of length less than three, and those containing digits): one per line and character-separated with added start and end characters as in the above example.",
        "We set both the maximum phrase length and the language model order to ten; this value was found by tuning on the development dataset.",
        "The system was tuned using MERT, and the feature weights were saved.",
        "The tuning Bleu was 95.22%, while the baseline Bleu, for leaving the Portuguese words intact, was 87.63%.",
        "Finally, the training and the tuning datasets were merged, and a new training round was performed.",
        "The resulting system was used with the saved feature weights to transliterate the Portuguese side of the training pt-en bi-text, which yielded a new ptes-en training bi-text.",
        "We did the same for Malay into Indonesian.",
        "We extracted 5,847 cognate pairs, 844 (or 14.4%) of which had spelling differences, and we trained a transliteration system.",
        "The highest tuning Bleu was 95.18% (for maximum phrase size and LM order of 10), but the baseline was 93.15%.",
        "We then retrained the system on the combination of the training and the development datasets, and we transliterated the Malay side of the training ml-en bi-text, obtaining a new mkn-en training bi-text.",
        "Table 1: Cross-lingual SMT experiments (shown in bold).",
        "Columns 2-5 present the bi-texts used for training, development and testing, and the monolingual data used to train the English language model.",
        "The following columns show the resulting Bleu (in %s) for different numbers of training sentence pairs.",
        "In this subsection, we study the similarity between the original and the additional source languages.",
        "First, we measured the vocabulary overlap between Spanish and Portuguese, which was feasible since our training pt-en and es-en bi-texts are from the same time span in the Europarl corpus and their English sides largely overlap.",
        "We found 110,053 Portuguese and 121,444 Spanish word types, and 44,461 (or 36.6%) of them were identical.",
        "Unfortunately, we could not do the same for Malay and Indonesian since the English sides of the in-en and ml-en bi-texts do not overlap.",
        "Second, following the setup of the baseline system, we performed cross-lingual experiments.",
        "The results are shown in Table 1.",
        "As we can see, this yielded a huge decrease in Bleu compared to the baseline - three to five times - even for very large training datasets, and even when a proper English LM and development dataset were used: compare line 1 to lines 3-6, and line 7 to lines 9-11.",
        "Third, we tried transliteration.",
        "Bleu doubled for Spanish (see lines 10-11), but improved far less for Indonesian (lines 5-6).",
        "Training on the transliterated data and testing on Malay and Portuguese yielded about 10-12% relative decrease for Malay (lines 1-2) but 50% for Portuguese (lines 7-8).",
        "Thus, unlike Spanish and Portuguese, there were far less systematic spelling variations between Malay and Indonesian.",
        "A closer inspection confirmed this: many extracted likely Malay-Indonesian cognate pairs with spelling differences were in fact forms of a word existing in both languages, e.g., kata and berkata {'to say').",
        "We performed various experiments combining the original and an additional training bi-text:",
        "Two-tables: We built two separate phrase tables for the two bi-texts, and we used them in the alternative decoding path model of Birch et al.",
        "(2007).",
        "Interpolation: We built two separate phrase tables for the original and for the additional bi-text, and we used linear interpolation to combine the corresponding conditional probabilities: Pr(e|s) = aProria(e|s) + (1 - a)Prextra(e\\s).",
        "We optimized the value of a on the development dataset, trying .5, .6, .7, .8 and .9; we used the same a for all four conditional probabilities.",
        "Merge: We built separate phrase tables, Tcyrigand Textra, for the original and for the additional training bi-text.",
        "We then concatenated them giving priority to Torig : We kept all phrase pairs from Tarig, adding to them those ones from Textra that were not present in Tcyrig.",
        "For each phrase pair added, we retained its associated conditional probabilities and the phrase penalty.",
        "We further added three additional features to each entry in the new table: F\\, F2 and F3.",
        "The value of F\\ was 1 if the phrase pair came from Tcyrig, and 0.5 otherwise.",
        "Similarly, F2=l if the phrase pair came from Textra, and F2=0.5 otherwise.",
        "The value of F3 was 1 if the pair came from both Tcyrig and Textra, and 0.5 otherwise.",
        "We experimented using (1) Fi only, (2) F1 and F2, (3) Fu F2, and F3.",
        "We set all feature weights using MERT, and we optimized the number of features on the development set.",
        "#",
        "Train",
        "LM",
        "Dev",
        "Test",
        "10K",
        "20K",
        "40K",
        "80K",
        "160K",
        "320K",
        "640K",
        "1230K",
        "1",
        "ml-en",
        "enm;",
        "ml-en",
        "ml-en",
        "44.93",
        "46.98",
        "47.15",
        "48.04",
        "49.01",
        "-",
        "-",
        "-",
        "2",
        "mlin-en",
        "enmi",
        "ml-en",
        "ml-en",
        "38.99",
        "40.96",
        "41.02",
        "41.88",
        "42.81",
        "-",
        "-",
        "-",
        "3",
        "ml-en",
        "enmi",
        "ml-en",
        "in-en",
        "13.69",
        "14.58",
        "14.76",
        "15.12",
        "15.84",
        "-",
        "-",
        "-",
        "4",
        "ml-en",
        "enm;",
        "in-en",
        "in-en",
        "13.98",
        "14.75",
        "14.91",
        "15.51",
        "16.27",
        "-",
        "-",
        "-",
        "5",
        "ml-en",
        "GHin",
        "in-en",
        "in-en",
        "15.56",
        "16.38",
        "16.52",
        "17.04",
        "17.90",
        "-",
        "-",
        "-",
        "6",
        "mlin-en",
        "GIl^n",
        "in-en",
        "in-en",
        "16.44",
        "17.36",
        "17.62",
        "18.14",
        "19.15",
        "-",
        "-",
        "-",
        "7",
        "pt-en",
        "6fles:p£",
        "pt-en",
        "pt-en",
        "21.28",
        "23.11",
        "24.43",
        "25.72",
        "26.43",
        "27.10",
        "27.78",
        "27.96",
        "8",
        "pteS-en",
        "^-Hes:p£",
        "pt-en",
        "pt-en",
        "10.91",
        "11.56",
        "12.16",
        "12.50",
        "12.83",
        "13.27",
        "13.48",
        "13.71",
        "9",
        "pt-en",
        "pt-en",
        "es-en",
        "4.40",
        "4.77",
        "4.57",
        "5.02",
        "4.99",
        "5.32",
        "5.08",
        "5.34",
        "10",
        "pt-en",
        "GHes :pt",
        "es-en",
        "es-en",
        "4.91",
        "5.12",
        "5.64",
        "5.82",
        "6.35",
        "6.87",
        "6.44",
        "7.10",
        "11",
        "ptes-en",
        "GIles :pt",
        "es-en",
        "es-en",
        "8.18",
        "9.03",
        "9.97",
        "10.66",
        "11.35",
        "12.26",
        "12.69",
        "13.79",
        "Concatxfc: We concatenated k copies of the original and one copy of the additional training bi-text; we then trained and tuned an SMT system as for the baseline.",
        "The value for k was optimized on the development dataset.",
        "Concatxfc:align: We concatenated k copies of the original and one copy of the additional training bi-text.",
        "We then generated IBM model 4 word alignments, and we truncated them, only keeping them for one copy of the original training bi-text.",
        "Next, we extracted phrase pairs, thus buildng a phrase table, and we tuned an SMT system as for the baseline.",
        "Our Method: Our method was described in Section 2.",
        "We used merge to combine the phrase tables for concatxfc:align and concatxl, considering the former as Tcyrig and the latter as Textra.",
        "We had two parameters to tune: k and the number of extra features in the merged phrase table.",
        "Figure 1: Impact of k on Bleu for concatxk for different number of extra ml-en sentence pairs in Indonesian^English SMT."
      ]
    },
    {
      "heading": "4. Results and Discussion",
      "text": [
        "First, we studied the impact of k on concatxfc for Indonesian^English SMT using Malay as an additional language.",
        "We tried all values of k such that l<fc<16 with lOOOOn extra ml-en sentence pairs, ne{l,2,4,8,16}.",
        "As we can see in Figure 1, the highest Bleu scores are achieved for (n;fc)e{(l;2),(2;2),(4;4),(8;7),(16;16)}, i.e., when k w n. In order to limit the search space, we used this relationship between k and n in our experiments (also for Portuguese and Spanish).",
        "Table 2 shows the results for experiments on improving Indonesian ^English SMT using 10K, 20K, ..., 160K additional ml-en pairs of parallel sentences.",
        "Several observations can be made.",
        "First, using more additional sentences yields better results.",
        "Second, with one exception, all experiments yield improvements over the baseline.",
        "Third, the improvements are always statistically significant for our method, according to (Collins et al., 2005)'s sign test.",
        "Overall, among the different bi-text combination strategies, our method performs best, followed by concatx k, merge, and interpolate, which are very close in performance; these three strategies are the only ones to consistently yield higher Bleu as the number of additional ml-en sentence pairs grows.",
        "Methods like concatxl, concatxfc:align and two-tables are somewhat inconsistent in that respect.",
        "The latter method performs worst and is the only one to go below the baseline (for 10K ml-en pairs).",
        "Table 3 shows the results when using pt-en data to improve Spanish^English SMT.",
        "Overall, the results and the conclusions that can be made are consistent with those for Table 2.",
        "We can further observe that, as the size of the original bi-text increases, the gain in Bleu decreases, which is to be expected.",
        "Note also that here transliteration is very important: it doubles the absolute gain in Bleu.",
        "Finally, Table 4 shows a comparison to the pivoting technique of Callison-Burch et al.",
        "(2006).",
        "for English^Spanish SMT.",
        "Despite using just Portuguese, we achieve an improvement that is, in five out of six cases, much better than what they achieve with eight pivot languages (which include not only Portuguese, but also two other Romance languages, French and Italian, which are closely related to Spanish).",
        "Moreover, our method yields improvements for very large original datasets -1.2M pairs, while theirs stops improving at 160K.",
        "However, our improvements are only statistically significant for 160K original pairs or less.",
        "Finally, note that our translation direction is reversed.",
        "Based on the experimental results, we can make several conclusions.",
        "First, we have shown that using bi-text data from related languages improves SMT: we achieved up to 1.35 and 3.37 improvement in Bleu for in-en (+ml-en) and es-en (+pt-eri) respectively.",
        "Second, while simple concatenation can help, it is problematic when the additional sentences outnumber the ones from the original bi-text.",
        "Third, concatenation can work very well if the original bi-text is repeated enough times so that the additional bi-text does not dominate.",
        "Fourth, merging phrase tables giving priority to the original bi-text and using additional fea-",
        "J2>",
        "f\\ ^ /\"\"W ^ ^\\ /",
        "-",
        ".",
        "10k ^>-20k",
        "^40k ^80k -*- 160k",
        "-",
        "Table 2: Improving Indonesian^English SMT using ml-en data.",
        "Shown are the Bleu scores (in %s) for different methods.",
        "A subscript shows the best parameter value(s) found on the development set and used on the test set to produce the given result.",
        "Bleu scores that are statistically significantly better than the baseline/our method are marked on the left/right side by < (for p < 0.01) or - (for p < 0.05).",
        "es-en pt-en Transi.",
        "Baseline Two tables Interpol.",
        "Merge concatxl concatxfc concatxfc:align Our method",
        "Table 3: Improving Spanish^English SMT using 160K additional pt-en sentence pairs.",
        "Column three shows whether transliteration was used; the following columns list the Bleu scores (in %s) for different methods.",
        "A small subscript shows the best parameter value(s) found on the development set and used on the test set to produce the given result.",
        "Bleu scores that are statistically significantly better than the baseline/our method are marked on the left/right side by < (for p < 0.01) or - (for p < 0.05).",
        "tures is a good strategy.",
        "Fifth, part of the improvement when combining bi-texts is due to increased vocabulary coverage because of cognates, but another part comes from improved word alignments.",
        "Sixth, the best results are achieved when the latter two sources are first isolated and then combined (our method).",
        "Finally, transliteration can help a lot in case of systematic spelling variations between the original and the additional source languages."
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "In this section, we describe two general lines of related previous research: using cognates between the source and the target language, and source-language side paraphrasing with a pivot language.",
        "Many researchers have used likely cognates to obtain improved word alignments and thus build better SMT systems.",
        "Al-Onaizan et al.",
        "(1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed, 1995) described in (Tiedemann, 1999) as a similarity measure.",
        "They used these cognates to improve word alignments with IBM models 1-4 in three different ways: (1) by seeding the parameters of IBM model 1, (2) by constraining the word cooccurrences when training IBM models 1-4, and (3) by adding the cognate pairs to the bi-text as additional \"sentence pairs\".",
        "The last approach performed best and was later used by Kondrak et al.",
        "(2003) who demonstrated improved SMT for nine European languages.",
        "Unlike these approaches, which extract cognates between the source and the target language, we use cognates between the source and some other related language that is different from the target.",
        "Moreover, we only implicitly rely on the existence of such cognates; we do not try to extract them at all, and we leave them in their original sentence contexts.",
        "in-en",
        "ml-en",
        "Baseline",
        "Two tables",
        "Interpol.",
        "Merge",
        "concatxl",
        "concatxfc",
        "concatxfc:align",
        "Our method",
        "28.4K 28.4K 28.4K 28.4K 28.4K",
        "10K 20K 40K 80K 160K",
        "23.80< 23.80< 23.80< 23.80< 23.80<",
        "^23.79< 24.24< 24.27< 24.11<",
        "<24.58<",
        "23.89<9)24.22<8)24-27<8)-24-46<8)<24.58<8)",
        "23.97<3)-24.46<3)",
        "24.43f3)<24.67(3)<24 79-",
        "24.29< 24.37< 24.38^ 24.17< ^24.43<",
        "24.29<x)-24.48(2)^24.54(4)-24.65<8)<25.00(16)",
        "24.01<)<24.35^2)<24.39<4)",
        "24.18<8)-24.27<6)",
        "<24.51(2;1) (+0.72) <24.70(2;2) (+0.90) <24.73(4;2) (+0.93) <24.97(8;3) (+1.17) <25.15(16;3) (+1.35)",
        "Table 4: Comparison to the pivoting technique of Callison-Burch et al.",
        "(2006) for English^Spanish.",
        "Shown are Bleu scores (in %s) and absolute improvement over the baseline for training bi-texts with different numbers of parallel sentences (10K, 20K, ..., 1230K) and fixed amount of additional data: (1) about 1.3M sentence pairs for each of eight additional languages in Callison-Burch et al.",
        "(2006)'s pivoting, and (2) 160K and 1,230K pairs for one language (Portuguese) for our method.",
        "Statistically significant improvements over the baseline are marked with a * (for p < 0.01) and with a ° (for p < 0.05).",
        "Another relevant line of research is on using multilingual parallel corpora to improve SMT using additional languages as pivots.",
        "Callison-Burch et al.",
        "(2006) improved English^Spanish and English^French SMT using source-language paraphrases extracted with the pivoting technique of Bannard and Callison-Burch (2005) and eight additional languages from the Europarl corpus (Koehn, 2005).",
        "For example, using German as a pivot, they extracted English paraphrases from a parallel English-German corpus by looking for English phrases that were aligned to the same German phrase: e.g., if under control and in check were aligned to unter con-trolle, they were hypothesized to be paraphrases with some probability.",
        "Such (English) paraphrases were added as additional entries in the phrase table of an English^Spanish/English^French phrase-based SMT system and paired with the foreign (Spanish/French) translation of the original (English) phrase.",
        "The system was then tuned with MERT using an extra feature penalizing low-probability paraphrases; this yielded up to 1.8% absolute improvement in Bleu.",
        "Other important publications about pivoting approaches for machine translation include (Wu and Wang, 2007), (Utiyama and Isahara, 2007), (Hajic et al., 2000) and (Habash and Hu, 2009).",
        "Unlike pivoting, which can only improve source-language lexical coverage, we augment both the source-and the target-language sides.",
        "Second, while pivoting ignores context when extracting paraphrases, we take it into account.",
        "Third, by using as an additional language one that is related to the source, we are able to get increase in Bleu that is comparable and even better than what pivoting achieves with eight pivot languages.",
        "On the negative side, our approach is limited in that it requires that X2 be related to X\\, while the pivoting language Z does not need to be related to X\\ nor to Y.",
        "However, we only need one additional parallel corpus (for X2-Y), while pivoting needs two: one for X\\-Z and one for Z-Y.",
        "Finally, note that our approach is orthogonal to pivoting, and thus the two can be combined."
      ]
    },
    {
      "heading": "6. Conclusion and Future Work",
      "text": [
        "We have proposed a novel method for improving SMT for resource-poor languages by exploiting their similarity to resource-rich ones.",
        "In future work, we would like to extend that approach in several interesting directions.",
        "First, we want to make better use of multilingual parallel corpora, e.g., while we had access to a Spanish-Portuguese-English corpus, we used it as two separate bi-texts Spanish-English and Portuguese-English.",
        "Second, we would like to exploit multiple auxiliary resource-rich languages the resource-poor source language is related to.",
        "Third, we could also experiment with using auxiliary languages that are related to the target language."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research was supported by research grant POD0713875.",
        "Direction",
        "System",
        "10K",
        "20K",
        "40K",
        "80K",
        "160K",
        "320K",
        "1,230K",
        "en^es",
        "baseline",
        "22.6",
        "25.0",
        "26.5",
        "26.5",
        "28.7",
        "30.0",
        "-",
        "pivoting (+8 languages x ~1.3M pairs)",
        "23.3",
        "26.0",
        "27.2",
        "28.0",
        "29.0",
        "30.0",
        "-",
        "improvement",
        "+0.7",
        "+1.0",
        "+0.7",
        "+1.5",
        "+0.3",
        "+0.0",
        "-",
        "es^en",
        "baseline",
        "22.87",
        "24.71",
        "25.80",
        "27.08",
        "27.90",
        "28.46",
        "29.90",
        "our method (+1 language x 160K pairs)",
        "23.98*",
        "25.65*",
        "26.49*",
        "27.30°",
        "28.05",
        "28.52",
        "29.87",
        "improvement",
        "+1.11*",
        "+0.94*",
        "+0.69*",
        "+0.22°",
        "+0.15",
        "+0.06",
        "-0.03",
        "our method (translit., +1 lang, x 160K)",
        "25.73*",
        "26.36*",
        "26.95*",
        "27.49*",
        "28.16",
        "28.43",
        "29.94",
        "improvement",
        "+2.86*",
        "+1.65*",
        "+1.15*",
        "+0.41*",
        "+0.26",
        "-0.03",
        "+0.04",
        "our method (+1 language x 1.23M pairs)",
        "24.23*",
        "25.70*",
        "26.78*",
        "27.49",
        "28.22°",
        "28.58",
        "29.84",
        "improvement",
        "+1.36*",
        "+0.99*",
        "+0.98*",
        "+0.41",
        "+0.32°",
        "+0.12",
        "-0.06",
        "our method (translit., +1 lang, x 1.23M)",
        "26.24*",
        "26.82*",
        "27.47*",
        "27.85*",
        "28.50*",
        "28.70",
        "29.99",
        "improvement",
        "+3.37*",
        "+2.11*",
        "+1.67*",
        "+0.77*",
        "+0.60*",
        "+0.24",
        "+0.09"
      ]
    }
  ]
}
