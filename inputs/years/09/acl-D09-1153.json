{
  "info": {
    "authors": [
      "Weiwei Sun",
      "Zhifang Sui",
      "Meng Wang",
      "Xin Wang"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1153",
    "title": "Chinese Semantic Role Labeling with Shallow Parsing",
    "url": "https://aclweb.org/anthology/D09-1153",
    "year": 2009
  },
  "references": [
    "acl-D08-1034",
    "acl-I08-2132",
    "acl-J08-2004",
    "acl-N01-1025",
    "acl-N03-2009",
    "acl-N04-1032",
    "acl-N07-1070",
    "acl-P06-2013",
    "acl-W00-0730",
    "acl-W04-2412",
    "acl-W05-0634",
    "acl-W95-0107"
  ],
  "sections": [
    {
      "text": [
        "Weiwei Sun and Zhifang Sui and Meng Wang and Xin Wang",
        "Institute of Computational Linguistics Peking University Key Laboratory of Computational Linguistics Ministry of Education, China",
        "weiwsun@gmail.com; {szf,wm}@pku.edu.cn; xinwang.cpku@gmail.com;",
        "Most existing systems for Chinese Semantic Role Labeling (SRL) make use of full syntactic parses.",
        "In this paper, we evaluate SRL methods that take partial parses as inputs.",
        "We first extend the study on Chinese shallow parsing presented in (Chen et al., 2006) by raising a set of additional features.",
        "On the basis of our shallow parser, we implement SRL systems which cast SRL as the classification of syntactic chunks with IOB2 representation for semantic roles (i.e. semantic chunks).",
        "Lwo labeling strategies are presented: 1) directly tagging semantic chunks in one-stage, and 2) identifying argument boundaries as a chunking task and labeling their semantic types as a classification task.",
        "Lor both methods, we present encouraging results, achieving significant improvements over the best reported SRL performance in the literature.",
        "Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In the last few years, there has been an increasing interest in Semantic Role Labeling (SRL) on several languages, which consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types.",
        "Nearly all previous Chinese SRL research took full syntactic parsing as a necessary preprocessing step, such as (Sun and lurafsky, 2004; Xue, 2008; Ding and Chang, 2008).",
        "Many features are extracted to encode the complex syntactic information.",
        "In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and Marquez, 2004).",
        "However, it is still unknown how these methods perform on other languages, such as Chinese.",
        "Lo date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a P-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Lreebank (CLB).",
        "Lhis performance drops to 71.9 when a real parser is used instead (Xue, 2008).",
        "Comparatively, the best English SRL results reported drops from 91.2 (Pradhan et al., 2008) to 80.56 (Surdeanu et al., 2007).",
        "Lhese results suggest that as still in its infancy stage, Chinese full parsing acts as a central bottleneck that severely limits our ability to solve Chinese SRL.",
        "On the contrary, Chinese shallow parsing has gained a promising result (Chen et al., 2006); hence it is an alternative choice for Chinese SRL.",
        "Lhis paper addresses the Chinese SRL problem on the basis of shallow syntactic information at the level of phrase chunks.",
        "We first extend the study on Chinese chunking presented in (Chen et al., 2006) by raising a set of additional features.",
        "Lhe new set of features yield improvement over the strong chunking system described in (Chen et al., 2006).",
        "On the basis of our shallow parser, we implement lightweight systems which solve SRL as a sequence labeling problem.",
        "Lhis is accomplished by casting SRL as the classification of syntactic chunks (e.g. NP-chunk) into one of semantic labels with IOB2 representation (?).",
        "With respect to the labeling strategy, we distinguish two different approaches.",
        "Lhe first one directly recognizes semantic roles by an IOB-type sequence tagging.",
        "Lhe second approach divides the problem into two independent sub tasks: 1) Argument Identification (AI) and 2) Semantic Role Classification (SRC).",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1475-1483, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "A Chinese word consists of one or more characters, and each character, in most cases, is a morpheme.",
        "The problem of how the words are constructed from morphemes, known as word formation, is very important for a majority of Chinese language processing tasks.",
        "To capture Chinese verb formation information, we introduce a rule-based algorithm with a number of heuristics.",
        "Experimental results indicate that word formation features can help both shallow parsing and SRL.",
        "We present encouraging SRL results on CPB.",
        "The best F-measure performance (74.12) with gold segmentation and POS tagging can be achieved by the first method.",
        "This result yield significant improvement over the best reported SRL performance (71.9) in the literature (Xue, 2008).",
        "The best recall performance (71.50) can be achieved by the second method.",
        "This result is also much higher than the best reported recall (65.6) in (Xue, 2008)."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Previous work on Chinese SRL mainly focused on how to implement SRL methods which are successful on English, such as (Sun and Jurafsky, 2004; Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2008).",
        "Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese.",
        "Their experiments were evaluated only on ten specified verbs with a small collection of Chinese sentences.",
        "This work made the first attempt on Chinese SRL and produced promising results.",
        "After the CPB was built, (Xue and Palmer, 2005) and (Xue, 2008) have produced more complete and systematic research on Chinese SRL.",
        "Ding and Chang (2008) divided SRC into two subtasks in sequence.",
        "Under the hierarchical architecture, each argument should first be determined whether it is a core argument or an adjunct, and then be classified into fine-grained categories.",
        "Chen et al.",
        "(2008) introduced an application of transduc-tive SVM in Chinese SRL.",
        "Because their experiments took hand-crafted syntactic trees as input, how transductive SVMs perform in Chinese SRL in realistic situations is still unknown.",
        "Most existing systems for automatic Chinese SRL make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels.",
        "On the contrary, in English SRL research, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees.",
        "For example, Hacioglu and Ward (2003) considered SRL as a chunking task; Pradhan et al.",
        "(2005) introduced a new procedure to incorporate SRL results predicted respectively on full and shallow syntactic parses.",
        "Previous work on English suggests that even good labeling performance has been achieved by full parse based SRL systems, partial parse based SRL systems can still enhance their performance.",
        "Though better understanding of SRL with shallow parsing on English is achieved by CoNLL-2004 shared task (Carreras and Marquez, 2004), little is known about how these SRL methods perform on Chinese."
      ]
    },
    {
      "heading": "3. Chinese Shallow Parsing",
      "text": [
        "There have been some research on Chinese shallow parsing, and a variety of chunk definitions have been proposed.",
        "However, most of these studies did not provide sufficient detail.",
        "In our system, we use chunk definition presented in (Chen et al., 2006), which provided a chunk extraction tool.",
        "The tool to extract chunks from CTB was developed by modifying the English tool used in CoNLL-2000 shared task, Chunklink, and is publicly available at http://www.nlplab.cn/chenwl/chunking.html.",
        "The definition of syntactic chunks is illustrated in Line CH in Figure 1.",
        "For example, \"fëHï^-Bj/the insurance company\", consisting of two nouns, is a noun phrase.",
        "With IOB2 representation (Ramshaw and Marcus, 1995), the problem of Chinese chunking can be regarded as a sequence labeling task.",
        "In this paper, we first implement the chunking method described in (Chen et al., 2006) as a strong baseline.",
        "To conveniently illustrate, we denote a word in focus with a fixed window W-2W-\\ww^\\w^2, where w is current token.",
        "The baseline features includes:",
        "Until now, the insurance company has provided insurance services for the Sanxia Project.",
        "Figure 1: An example from Chinese PropBank.",
        "That means 18 features are used to represent a given token.",
        "For instance, the bi-gram Word features at 5th word position (\"^ /company\") in Figure 1 are \", _W, \"UVt-&W, \"^wJ-B\", \"B-Ä\".",
        "To improve shallow parsing, we raised an additional set of features.",
        "We will discuss these features in section 5."
      ]
    },
    {
      "heading": "4. SRL with Shallow Parsing",
      "text": [
        "The CPB is a project to add predicate-argument relations to the syntactic trees of the CTB.",
        "Similar to English PropBank, the semantic arguments of a predicate are labeled with a contiguous sequence of integers, in the form of AN (i.e. Arg/V); the adjuncts are annotated as such with the label AM (i.e. ArgM) followed by a secondary tag that represents the semantic classification of the adjunct.",
        "The assignment of argument labels is illustrated in Figure 1, where the predicate is the verb \"$q|f^/provide\".",
        "For example, the noun phrase \"fëHï^-Bj/the insurance company\" is labeled as AO, meaning that it is the proto-Agent of ÜÄ; the preposition phrase \"ISit Et Ill/until now\" is labeled as AM-TMP, indicating a temporal component.",
        "SRL is a complex task which has to be decomposed into a number of simpler decisions and tagging schemes in order to be addressed by learning techniques.",
        "Regarding the labeling strategy, we can distinguish at least two different strategies.",
        "The first one consists of performing role identification directly as IOB-type sequence tagging.",
        "The second approach consists of dividing the problem into two independent sub tasks.",
        "In the one-stage strategy, on the basis of syntactic chunks, we define semantic chunks which do not overlap nor embed using IOB2 representation.",
        "Syntactic chunks outside a chunk receive the tag O.",
        "For syntactic chunks forming a chunk of type A*, the first chunk receives the B-A* tag (Begin), and the remaining ones receive the tag I-A* (Inside).",
        "Then a SRL system can work directly by using sequence tagging techinique.",
        "Since the semantic annotation in the PropBank corpus does not have any embedded structure, there is no loss of information in this representation.",
        "The line Ml in Figure 1 illustrates this semantic chunk definition.",
        "In the two-stage architecture, we divide Chinese SRL into two sub tasks: 1) semantic chunking for AI, in which the argument boundaries are predicted, and 2) classification for SRC, in which the already recognized arguments are assigned role labels.",
        "In the first stage, we define semantic chunks BA which means begin of an argument and IA which means inside of an argument.",
        "In the second stage, we solve SRC problem as a multi-class classification.",
        "The lines M2-AI and M2-SRC in Figure 1 illustrate this two-stage architecture.",
        "For example, the noun phrase \"fëHï^-Bj/the insurance company\" is proto-Agent, and thus should be labeled as BA in the AI chunking phase, and then be tagged as AO.",
        "The phrase \"^jHiRlfM/for the Sanxia Project\" consists of three chunks, which should be labeled as B-A, I-A, and IA respectively in the AI chunking phase, then these three chunks as a whole argument should be recognized as A2.",
        "There is also another semantic chunk definition, where the basic components of a semantic chunk are words rather than syntactic chunks.",
        "A good election for this problem is chunk-by-chunk processing instead of word-by-word.",
        "The motivation is twofold: 1) phrase boundaries are almost always consistent with argument boundaries; 2) chunk-by-chunk processing is computationally less expensive and allows systems to explore a relatively larger context.",
        "This paper performs a chunk-by-chunk processing, but admitting a processing by words within the target verb chunks.",
        "xm",
        "ÜÄ",
        "im m&",
        "[P]",
        "[NR]",
        "[NN]",
        "[VP]",
        "[NN NN]",
        "[PP",
        "NP",
        "NP]",
        "[VP]",
        "[NP]",
        "B-A2",
        "I-A2",
        "I-A2",
        "B-V",
        "B-AI",
        "B-A",
        "I-A",
        "I-A",
        "B-V",
        "B-A",
        "A2",
        "Rel",
        "AI",
        "Most of the feature templates are \"standard\", which have been used in previous SRL research.",
        "We give a brief description of \"standard\" features, but explain our new features in detail.",
        "In the semantic chunking tasks, i.e. the one-stage method and the first step in the two-stage method, we use the same set of features.",
        "The features are extracted from three types of elements: syntactic chunks, target verbs, links between chunks and target verbs.",
        "They are formed making use of words, POS tags and chunks of the sentence.",
        "Xue (2008) put forward a rough verb classification where verb classes are automatically derived from the frame files, which are verb lexicon for the CPB annotation.",
        "This kind of verb class information has been shown very useful for Chinese SRL.",
        "Our system also includes this feature.",
        "In our experiments, we represent a verb in two dimensions: 1) number of arguments, and 2) number of framesets.",
        "For example, a verb may belong to the class \"C1C2,\" which means that this verb has two framesets, with the first frameset having one argument and the second having two arguments.",
        "To conveniently illustrate, we denote a token chunk with a fixed context Wi-i[CkWi...Wh – Wj]wj+i, where Wh is the head word of this chunk Cfc.",
        "The complete list of features is listed here.",
        "Extraction on Syntactic Chunks Chunk type: c^.",
        "Length: the number of words in a chunk.",
        "Head word/POS tag.",
        "The rules described in (Sun and Jurafsky, 2004) are used to extract head word.",
        "I OB chunk tag of head word: chunk tag of head word with IOB2 representation (e.g. B-NP, I-NP).",
        "Chunk words/POS tags context.",
        "Chunk context includes one word before and one word after: Wi-i and Wj+\\.",
        "POS tag chain: sequential containers of each word's POS tag: Wi_____Wj.",
        "For example, this feature for is \"NN_NN\".",
        "Position: the position of the phrase with respect to the predicate.",
        "It has three values as before, after and here.",
        "Extraction on Target Verbs Given a target verb wv and its context, we extract the following features.",
        "Predicate, its POS tag, and its verb class.",
        "Predicate IOB chunk tag context: the chain of IOB2 chunk tags centered at the predicate within a window of size -2/+2.",
        "Predicate POS tag context: the POS tags of the words that immediately precede and follow the predicate.",
        "Number of predicates: the number of predicates in the sentence.",
        "Extraction on Links To capture syntactic properties of links between the chunks and the verbs, we use the following features.",
        "Path: a flat path is defined as a chain of base phrases between the token and the predicate.",
        "At both ends, the chain is terminated with the POS tags of the predicate and the headword of the token.",
        "Distance: we have two notions of distance.",
        "The first is the distance of the token from the predicate as a number of base phrases, and the second is the same distance as the number of VP chunks.",
        "Combining Features We also combine above features as some new features.",
        "Conjunctions of position and head word, target verb, and verb class, including: position_Wh, positiori-Wv, position-Wh-Wv, position-dass, and position jWh-dass.",
        "Conjunctions of position and POS tag of head word, target verb, and verb class, including: position-Wh-Wv, positionjWh, and position-Wh-dass.",
        "In the SRC stage of the two-stage method, different from previous work, our system only uses word-based features, i.e. features extracted from words and POS tags, to represent a given argument.",
        "Experiments show that a good semantic role classifier can be trained by using only word-based features.",
        "To gather all argument position information predicted in AI stage, we design a coarse frame feature, which is a sequential collection of arguments.",
        "So far, we do not know the detailed semantic type of each argument, and we use XP as each item in the frame.",
        "To distinguish the argument in focus, we use a special symbol to indicate the corresponding frame item.",
        "For instance, the Frame feature for argument fëHïûlx %■ is XP+XP+XP+XP+V+!XP, where !XP means that it is the argument in focus.",
        "Denote 1) a given argument wi-2wi-i[wiwi+i...wj-iwj}wj+iwj+2, and 2) a given predicate wv.",
        "The features for SRC are listed as follows.",
        "Words/POS tags context of arguments: the contents and POS tags of the following words: Wi,",
        "Wj+\\, Wj+2\\ the POS tags of the following words:",
        "Wi+l, Wi+2, Wj+i, Wj+2.",
        "Token Position.",
        "Predicate, its POS, and its verb class.",
        "Coarse Frame.",
        "Combining features: conjunctions of boundary words, including Wi-i-Wj+i and Wi-2-Wj+2', conjunction of POS tags of boundary words, including Wi-i-Wj+i and Wi-2-Wj+2', conjunction of token position, boundary words, and predicate word, including positiori-Wi-Wj, Wi-Wjjwv; positiori-Wi-Wj-Wv; conjunction of token position, boundary words' POS tags, and predicate word, also including positiori-Wi-Wj, Wi-Wj-Wv; positionjWiJWjJwv; conjunction of predicate and frame; conjunction of target verb class and frame; conjunction of boundary words' POS tags, and predicate word."
      ]
    },
    {
      "heading": "5. Automatic Chinese Verb Formation Analyzing",
      "text": [
        "Chinese words consist of one or more characters, and each character, in most cases, is a morpheme which is the smallest meaningful unit of the language.",
        "According to the number of morphemes, the words can be grouped into two sets, simple words (consisting of one morpheme) and compound words (consisting of two morphemes or more).",
        "There are 9 kinds of word formation in Chinese compound words, and table 1 shows the detail with examples.",
        "Note that, attributive-head and complementarity are not for Chinese verbs.",
        "Table 1 : Example Words with Formation",
        "The internal structure of a word constraints its external grammatical behavior, and the formation of a verb can provide very important information for Chinese SRL.",
        "Take \"0 hB/exceed\" as an example, the two characters are both verbal morphemes, and the character \"0\" means \"pass\" and the character \"hB\" with the meaning of \"over\" shows the complement of the action of \"0\".",
        "In this word, \"hB\" is usually collocated with an object, and hence a Patient role should comes after the verb \"0tB\".",
        "Note that, the verb \"0\", however, is unlikely to have an object.",
        "Take \"ÏË ^/haircut\" as another example, the first character \"ïfi\" is a verbal morpheme with the meaning of \"cut\" and the second character is a nominal morpheme with the meaning of \"hair\".",
        "In this word, \"S\" acts as the object of \"ÏË\", and the word \"US\" is unlikely to have an Patient any more in the sentential context.",
        "To automatically analyze verb formation, we introduce a rule-based algorithm.",
        "Pseudo code in Algorithm 1 illustrates our algorithm.",
        "This algorithm takes three string (one or more Chinese characters) sets as lexicon knowledge:",
        "• adverbial suffix set A: strings in A are usually realized as the modifier in a adverbial-head type word, e.g. ^fi/not, ^Ij/not, Ê/always, #/both, ^/all.",
        "• object head set Ö: strings in Ö are usually realized as the head in a verb-object type word, e.g. ^/change, t£/get, Wtalk, ^;/send.",
        "Types",
        "Examples",
        "reduplication affixation subject-verb verb-object",
        "verb-complement verb-result adverbial-head coordinate attributive-head* complementarity*",
        "##(look) M(tliink) ïtftCntensify) ^^(feel) ^ H (hear) PM(dictate) $c;J@(quit smoking) MM.",
        "(haircut) MJn (inform) implant) StB (exceed) =tî$(boil) Hi iff (retreat) iH^I(misuse) Sfe (cherish) iâM(chase) iljW(rumor) Ü ^(hospital) ift.|c(paper) ^ EE (horse)",
        "Algorithm 1: Verb Formation Analyzing.",
        "Data: adverbial suffix set A, object head set",
        "Ö, complement suffix set C input : word W = c\\...c.n and its POS P output: head character h, adverbial character a, complement character c, object character o else if cn G C and Cn-ic«, G C and P='W\" then",
        "I h – Ci, c – Cn,/Cn, – icra, else if ci e A then",
        "• complement suffix set C: strings in C are usually realized as complement in a verb-complement type word: e.g. hB/out, A/in, ^/finish, 5^/come, ^3?lJ/not.",
        "Note that, to date there is no word formation annotation corpus, so direct evaluation of our rule-based algorithm is impossible.",
        "This paper makes task-oriented evaluation which measures improvements in SRL.",
        "The majority of Chinese nouns are of type attributive-head.",
        "This means that for most nouns the last character provides very important information indicating the head of the noun.",
        "For example, the word formations of \"$_W/peach\", \"$îp W/willow\" and \"Ht^W/boxtree\" (three different kinds of trees), are attributive-head and they have the same head word \"W/tree\".",
        "While for verbs, the majority are of three types: verb-object, coordinate and adverbial-head.",
        "For example, words \"M A/enlarge\", \"JuM/make more drastic\" and \"M ^/accelerate\" have the same head \"in/add\".",
        "The head morpheme is very useful in alleviating the data sparseness in word level.",
        "However, for any given word, it is very hard to accurately find the head.",
        "In the shallow paring experiments, we use a very simple rule to get a pseudo head character: 1) extracting the last word for a noun, and 2) extracting the first word for a verb.",
        "The new features include:",
        "Pattern 1: conjunction of pseudo head of Wi-i and POS tags of Wi-i and Wi.",
        "Pattern 2: conjunction of pseudo head of Wi and POS tags of Wi-i and Wi.",
        "Pattern 3: conjunction of length/POS tags of",
        "Wi-i, Wi, Wi+i.",
        "We use some new verb formation features to improve our SRL system.",
        "The new features are listed as follows.",
        "The first four are used in semantic chunking task, and all are used in SRC task.",
        "FirstAast characters.",
        "Word length.",
        "Conjunction of word length and first/last character.",
        "Conjunction of token position and first/last character.",
        "m.",
        "The complement string of a verb (e.g. \"_B\" in The object string of a verb (e.g. \"M\" in \"M",
        "R\")."
      ]
    },
    {
      "heading": "6. Results and Discussion",
      "text": [
        "Experiments in previous work are mainly based on CPB and CTB, but the experimental data preparing procedure does not seem consistent.",
        "For example, the sum of each semantic role reported in (Ding and Chang, 2008) is extremely smaller than the corresponding occurrence statistics in original data files in CPB.",
        "In this paper, we modify CoNLL-2005 shared task software to process CPB and CTB.",
        "In our experiments, we use the CPB 1.0 and CTB 5.0.",
        "The data is divided into three parts: files from chtb_081 to chtb_899 are used as training set; files from chtb_041 to chtb_080 as development set; files from chtb_001 to chtb_040, and chtb_900 to chtb_931 as test set.",
        "The data setting is the same as (Xue, 2008).",
        "The results were evaluated for precision, recall and F-measure numbers using the srl-eval.pl script provided by CoNLL-2005 shared task.",
        "For both syntactic and semantic chunking, we used TinySVM along with YamCha (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001).",
        "In the chunking experiments, all SVM classifiers were realized with a polynomial kernel of degree 2.",
        "Pair-wise strategy is used to solve multi-class classification problem.",
        "For the SRC experiments, we use a linear SVM classifier, along with One-Vs-All approach for multi-class classification.",
        "SVMiin, a fast linear SVM solvers, is used for supervised learning.",
        "Z2-SVM-MFN (modified finite newton) method is used to solve the optimization problem (Keerthi and DeCoste, 2005).",
        "Table 2 summarizes the overall shallow parsing performance on test set.",
        "The first line shows the performance of baseline.",
        "Comparing the best system performance 94.13 F-measure of CoNLL 2000 shared task (Syntactic Chunking on English), we can see Chinese shallow parsing has reached a comparable result, tough the comparison of numeric performance is not very fair, because of different languages, different chunk definition, different training data sizes, etc..",
        "The second line Ours shows the performance when new features are added, from which we can see the word formation based features can help shallow parsing.",
        "Table 3 shows the detailed performance of noun phrase (NP) and verb phrase (VP), which make up most of phrase chunks in Chinese.",
        "Our new features help NP more, whereas the effect of new features for VP is not significant.",
        "That is in part because most VP chunk recognition error is caused by long dependency, where word formation features do not work.",
        "Take the sentences below for example:"
      ]
    },
    {
      "heading": "1.. Ivp @«#ftfiJL achieve victory.)",
      "text": [
        "(Therefore (we)",
        "2.",
        "Iadvp SÄ] Ivp AshBïf] KiMUW^ H iS 3\\ Ë) » (Therefore the major changes have not been met before.)",
        "The contexts of the word \" 13 jib/therefore\" in the two sentences are similar, where \"@jifc\" is followed by verbal components.",
        "In the second sentence, the word \"@ jib/therefore\" will be correctly recognized as an adverbial phrase unless classifier knows the following component is a clause.",
        "Unfortunately, word formation features cannot supply this kind of information.",
        "Table 4 lists the overall SRL performance numbers on test set using different methods mentioned earlier; these results are based on features computed from gold standard segmentation and POS tagging, but automatic recognized chunks, which is parsed by our improved shallow parsing system.",
        "For the AI and the whole SRL tasks, we report the precision (P), recall (R) and the F/j=1-measure scores, and for the SRC task we report the classification accuracy (A).",
        "The first line (Xue, 2008) shows the SRL performance reported in (Xue, 2008).",
        "To the authors' knowledge, this result is best SRL performance in the literature.",
        "Line 2 and 3 shows the performance of the one-stage systems: 1) Line Ml – is the performance without word formation features; 2) Line M1+ is the performance when verb formation features are added.",
        "Line 4 to 8 shows the performance of the two-stage systems: 1) Line M2-/AI and M2+/AI shows the performance of AI phase without and within word formation features respectively; 2) Line M2 – /SRC shows the SRC performance with trivial word-based features (i.e. frame features and verb formation features are not used); 3) Line M2+wf/SRC is the improved SRC performance when coarse verb formation features are added; 4) Line M2+/SRC is the SRC performance with all features; 5) Line M2 – AI+SRC shows the performance of SRL system, which uses baseline features to identify arguments, and use all features to classify arguments.",
        "P(%)",
        "R(%)",
        "F/3=l",
        "NP(Baseline) NP(Ours)",
        "90.84 91.42",
        "90.05 90.78",
        "90.44 91.10",
        "VP(Baseline) VP(Ours)",
        "94.44 94.65",
        "94.55 94.74",
        "94.50 94.69",
        "P(%)",
        "R(%)",
        "F/3=l",
        "Baseline Ours",
        "93.54 93.83",
        "93.00 93.39",
        "93.27 93.61",
        "P(%)",
        "R(%)",
        "A(%)",
        "P/3=l",
        "(Xue, 2008)",
        "79.5",
        "65.6",
        "-",
        "71.9",
        "Ml-",
        "79.02",
        "69.12",
        "-",
        "73.74",
        "M1 +",
        "79.25",
        "69.61",
        "-",
        "74.12",
        "M2-/AI",
        "80.34",
        "75.11",
        "-",
        "77.63",
        "M2+/AI",
        "80.01",
        "75.15",
        "-",
        "77.51",
        "M2-/SRC",
        "-",
        "-",
        "92.57",
        "-",
        "M2+wf/SRC",
        "-",
        "-",
        "93.25",
        "-",
        "M2+/SRC",
        "-",
        "-",
        "93.42",
        "-",
        "M2-AI+SRC",
        "76.48",
        "71.50",
        "-",
        "73.90",
        "The results summarized in Table 4 indicate that according to the-state-of-the-art in Chinese parsing, SRL systems based on shallow parsing outperforms the ones based on full parsing.",
        "Comparison between one-stage strategy and two-stage strategy indicates 1) that there is no significant difference in the F-measure; and 2) that two-stage strategy method can achieve higher recall while one-stage strategy method can achieve higher precision.",
        "Both the one-stage strategy and two-stage strategy methods yield significant improvements over the best reported SRL performance in the literature, especially in terms of recall performance.",
        "Comparison SRL performance with full parses and partial parses indicates that both models have strong and weak points.",
        "The full parse based method can implement high precision SRL systems, while the partial parse based methods can implement high recall SRL systems.",
        "This is further justification for combination strategies that combine these independent SRL models.",
        "Generally, Table 4 shows that verb formation features can enhance Chinese SRL, especially for fine-grained role classification.",
        "The effect of word formation in formation in both shallow parsing and SRL suggests that automatic word formation analyzing is very important for Chinese language processing.",
        "The rule-based algorithm is just a preliminary study on this new topic, which requires",
        "Table 5 : SRL performance with arguments of different length more research effort.",
        "Though our SRC module does not use any parsing information, our system can achieve 93.42% accuracy, comparing the best gold parse based result 94.68% in the literature.",
        "This result suggests that Chinese SRC system, even without parsing, can reach a considerable good performance.",
        "The main reason is that in Chinese, arguments with different semantic types have discriminative boundary words, which can be extracted without parsing.",
        "It is very clear that the main bottleneck for Chinese SRL is to accurately identify arguments rather than to disambiguate their detailed semantic types.",
        "Table 5 summarizes the labeling performance for argument of different length.",
        "It is not surprising that arguments are more and more difficult to rightly recognize as the increase of their length.",
        "But the performance decline slows up when the length of arguments is larger than 10.",
        "In other words, some of the arguments that are composed of many words can still be rightly identified.",
        "The main reason for this point is that these arguments usually have clear collocation words locating at argument boundaries.",
        "Take the sentences below for example, the object of the verb \"ill©/include\" has a definite collocation word \"^r/etc\", and therefore this object is easy to be recognized as a Ai."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "In this paper, we discuss Chinese SRL on the basis of partial syntactic structure.",
        "Our systems advance the state-of-the-art in Chinese SRL.",
        "We first extend the study on Chinese shallow parsing and implement a good shallow parser.",
        "On the basis of partial parses, SRL are formulated as a sequence labeling problem, performing IOB2 decisions on the syntactic chunks of the sentence.",
        "We exploit a wide variety of features based on words, POS tags, and partial syntax.",
        "Additionally, we discuss a language special problem, i.e. Chinese word formation.",
        "Experimental results show that coarse word formation information can help shallow parsing, especially for NP-chunk recognition.",
        "A rule-based algorithm is put forward to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL.",
        "Num of words",
        "P(%)",
        "R(%)",
        "F/3=l",
        "Length =",
        "1",
        "84.69%",
        "75.48%",
        "79.82",
        "Length =",
        "2",
        "82.14%",
        "74.21%",
        "77.97",
        "Length =",
        "3",
        "75.43%",
        "63.98%",
        "69.24",
        "Length =",
        "4",
        "75.71%",
        "65.63%",
        "70.32",
        "Length =",
        "5",
        "72.46%",
        "64.38%",
        "68.18",
        "Length =",
        "6",
        "72.97%",
        "66.21%",
        "69.43",
        "Length =",
        "7",
        "77.03%",
        "67.65%",
        "72.04",
        "Length =",
        "8",
        "74.39%",
        "57.28%",
        "64.72",
        "Length =",
        "9",
        "66.67%",
        "51.16%",
        "57.89",
        "Length =",
        "10",
        "68.08%",
        "58.28%",
        "62.80",
        "Length",
        "11+",
        "67.40%",
        "57.71%",
        "62.18"
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is supported by NSFC Project 60873156, 863 High Technology Project of China 2006AA01Z144 and the Project of Toshiba (China) R&D Center.",
        "We would like to thank Weiwei Ding for his good advice on this research.",
        "We would also like to thank the anonymous reviewers for their helpful comments."
      ]
    }
  ]
}
