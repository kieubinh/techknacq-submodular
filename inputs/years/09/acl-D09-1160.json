{
  "info": {
    "authors": [
      "Naoki Yoshinaga",
      "Masaru Kitsuregawa"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1160",
    "title": "Polynomial to Linear: Efficient Classification with Conjunctive Features",
    "url": "https://aclweb.org/anthology/D09-1160",
    "year": 2009
  },
  "references": [
    "acl-C02-1054",
    "acl-C04-1002",
    "acl-C08-1046",
    "acl-C08-1079",
    "acl-D07-1062",
    "acl-J96-1002",
    "acl-N04-1039",
    "acl-N06-1020",
    "acl-P03-1004",
    "acl-P07-1104",
    "acl-P07-2017",
    "acl-P08-1068",
    "acl-P08-2060",
    "acl-W02-2016",
    "acl-W03-1018",
    "acl-W08-0804"
  ],
  "sections": [
    {
      "text": [
        "Institute of Industrial Science University of Tokyo 4-6-1 Komaba, Meguro-ku, Tokyo kitsure@tkl.iis.u-tokyo.ac .",
        "jp",
        "This paper proposes a method that speeds up a classifier trained with many conjunctive features: combinations of (primitive) features.",
        "The key idea is to pre-compute as partial results the weights of primitive feature vectors that appear frequently in the target NLP task.",
        "A trie compactly stores the primitive feature vectors with their weights, and it enables the classifier to find for a given feature vector its longest prefix feature vector whose weight has already been computed.",
        "Experimental results for a Japanese dependency parsing task show that our method speeded up the svm and llm classifiers of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications, and it is inadequate to process Web-scale corpora for knowledge acquisition (Pantel, 2007; Saeger et al., 2009) or semi-supervised learning (McClosky et al., 2006; Spoustovâ et al., 2009).",
        "One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams).",
        "Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features.",
        "The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007).",
        "However, 'explicit' feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier.",
        "Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995).",
        "The kernel-based classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008).",
        "l\\-regularized log-linear models (£i-LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshi-rani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007).",
        "However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training ^i-LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification.",
        "In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy.",
        "In this study, we provide a simple, but effective solution to the inefficiency of classifiers trained with higher-order conjunctive features (or polynomial kernel), by exploiting the Zipfian nature of language data.",
        "The key idea is to precompute the weights of primitive feature vectors and use them as partial results to compute the weight of a given feature vector.",
        "We use a trie called the feature sequence trie to efficiently find for a given feature vector its longest prefix feature vector whose weight has been computed.",
        "The trie is built from feature vectors generated by applying the classifier to actual data in the classification task.",
        "The time complexity of the classifier approaches time that",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1542-1551, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "is linear with respect to the number of primitive features when the retrieved feature vector covers most of the features in the input feature vector.",
        "We implemented our algorithm for svm and llm classifiers and evaluated the performance of the resulting classifiers in a Japanese dependency parsing task.",
        "Experimental results show that it successfully speeded up classifiers trained with higher-order conjunctive features by a factor of 10.",
        "The rest of this paper is organized as follows.",
        "Section 2 introduces LLMs and svms.",
        "Section 3 proposes our classification algorithm.",
        "Section 4 presents experimental results.",
        "Section 5 concludes with a summary and addresses future directions."
      ]
    },
    {
      "heading": "2. Preliminaries",
      "text": [
        "In this paper, we focus on linear classifiers that calculate the probability (or score) by summing up weights of individual features.",
        "Examples include not only log-linear models but also support vector machines with kernel expansion (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003).",
        "Below, we introduce these two classifiers and their ways to consider feature combinations.",
        "In classification-based NLP, the target task is modeled as one or more classification steps.",
        "For example in part-of-speech (pos) tagging, each classification decides whether to assign a particular label (pos tag) to a given sample (each word in a given sentence).",
        "Each sample is then represented by & feature vector x, whose element Xi is a value of a feature function fi G T.",
        "Here, we assume a binary feature function fi(x) G {0,1}, in which a non-zero value means that particular context data appears in the sample.",
        "We say that a feature fi is active in sample x when Xi = fi(x) = 1 and |a;| represents the number of active features in a; (|a;| = \\{fi\\fi(x) = 1}|).",
        "The log-linear model (LLM), or also known as maximum-entropy model (Berger et al., 1996), is a linear classifier widely used in the NLP literature.",
        "Let the training data of LLMs be {(xi,yi)}f=1, where Xi G {0, l}n is a feature vector and iji is a class label associated with a^.",
        "We assume a binary label iji G {±1} here to simplify the argument.",
        "The classifier provides conditional probability p(y\\x) for a given feature vector x and a label y:",
        "where fi,y(x, y) is a feature function that returns a non-zero value when fi(x) = 1 and the label is y, Wity G M is a weight associated with fi>y, and zix) = Y,y exP Ei wityfity{x, y) is the partition function.",
        "We can consider feature combinations in LLMs by explicitly introducing a new conjunctive feature fjr/>y(x, y) that is activated when a particular set of features T' ç T to be combined is activated (namely, fr>,y(x,y) = A/iy€^/ fi,y{x,y)).",
        "We then introduce an l\\-regularized LLM LLM), in which the weight vector w is tuned so as to maximize the logarithm of the a posteriori probability of the training data:",
        "Hyper-parameter C thereby controls the degree of over-fitting (solution sparseness).",
        "Interested readers may refer to the cited literature (Andrew and Gao, 2007) for the optimization procedures.",
        "A support vector machine (SVM) is a binary classifier (Cortes and Vapnik, 1995).",
        "Training with samples {(xi,yi)}f=1 where Xi G {0, l}n and yi G {±1} yields the following decision function:",
        "where b G M, <fr : M.n i – > HH and support vectors xj G SV (subset of training samples), each of which is associated with weight ctj G M. We hereafter call g(x) the weight function.",
        "Nonlinear mapping function <fi is chosen to make the training samples linearly separable in RH space.",
        "Kernel function k(xj, x) = (f)(xj)T(f)(x) is then introduced to compute the dot product in RH space without mapping xto<p(x).",
        "To consider combinations of primitive features fj G T, we use a polynomial kernel kd(xj, x) = (xjx + l)d. From Eq.",
        "3, we obtain the weight function for the polynomial kernel as:",
        "Since we assumed that x% is a binary value representing whether a (primitive) feature fi is active in the sample, the polynomial kernel of degree d implies a mapping (f>d from x to <fid(x) that has H = ^2k=0 (^) dimensions.",
        "Each dimension represents a (weighted) conjunction of d features in the original sample x. Kernel Expansion (svm-ke) The time complexity of Eq.",
        "4 is 0(|x \\ ■ \\SV\\).",
        "This cost is usually high for classifiers used in NLP tasks because they often have many support vectors (|<SV| > 10, 000).",
        "Kernel expansion (KE) was proposed by Isozaki and Kazawa (2002) to convert Eq.",
        "4 into the linear sum of the weights in the mapped feature space as in LLM (p(y\\x) in Eq.",
        "1):",
        "where xd is a binary feature vector whose element xf has a non-zero value when (<fid(x))i > 0, w is the weight vector for xd in the expanded feature space Td and is precalculated from the support vectors xj and their weights ctj.",
        "Interested readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w.",
        "The time complexity of Eq.",
        "5 (and Eq.",
        "1) is 0(\\xd\\), which is linear with respect to the number of active features in xd within the expanded feature space Td.",
        "Heuristic Kernel Expansion (svm-hke) To make the weight vector sparse, Kudo and Matsumoto (2003) proposed a heuristic method that filters out less useful features whose absolute weight values are less than a predefined threshold a.",
        "They reported that increased threshold value a resulted in a dramatically sparse feature space Td, which had the side-effects of accuracy degradation and classifier speed-up."
      ]
    },
    {
      "heading": "3. Proposed Method",
      "text": [
        "In this section, we propose a method that speeds up a classifier trained with many conjunctive features.",
        "Below, we focus on a kernel-based classifier trained with a polynomial kernel of degree d (here,",
        "!For example, given an input vector x = (xi,X2)Tand a support vector x' = (x'1,x2)T, the 2nd-order polynomial kernel returns k^ix',x) = (x^xi + x'2X2 + l) = 3x'iXi + 3x'2X2 + 2x1X1X2^2 + 1 ('.'",
        "x'i,Xi € {0,1}).",
        "This function thus implies a mapping <j>2(x) = (1, y/3xi, \\/3x2, \\/2xiX2)T. In the following argument, we ignore the dimension of the constant in the mapped space and assume constant b is set to include it.",
        "feature weight matrix height computation (Eq.",
        "5) VtülyV^2yOA)3yOw4y Weight computation (Eq.",
        "6)",
        "svms), but an analogous argument is possible for linear classifiers (e.g., LLMs).",
        "We hereafter represent a binary feature vector x as a set of active features {fi\\fi(x) = 1}.",
        "x can thereby be represented as an element of the power set 2T of the set of features T.",
        "Let us remember that weight function g(x) in Eq.",
        "5 maps x G 2T to W G M. If we could calculate Wx = g(x) for all possible x in advance, we could obtain g(x) by simply checking |a;| elements, namely, in 0(|a;|) time.",
        "However, because |{a;|a; G 2\"F}| = 2^ and |JF| is likely to be very large (often \\F\\ > 10,000 in NLP tasks), this calculation is impractical.",
        "We then compute and store weight Wxi = g(x') for x G Vc(c 2T), a certain subset of the possible value space, and compute g(x) for x fi Vc by using precalculated weight WXc for xc Ç x in the following way:",
        "Intuitively speaking, starting from partial weight WXc, we add up remaining weights of primitive features / G T that are not active in xc but active in x and conjunctive features that combine / and the other active features in x.",
        "An example of this computation (d = 2) is depicted in Figure 1.",
        "We can efficiently compute g(x) for a vector x that has four active features /i> Î2, fi, and /4 (and x has their six conjunctive features) using precalculated weight ^{1,2,3}; we should first check the three features /1, /2, and fs to retrieve ^{1,2,3} and next check the remaining four features related to f±, namely f±, /i;4, /2,4, and /3;4, in order to add up the remaining weights, while the normal computation in Eq.",
        "5 should check the four primitive and six conjunctive features to get the individual weights.",
        "Expected time complexity Counting the number of features to be checked in the computation, we obtain the time complexity f(x, d) of Eq.",
        "6 as:",
        "(e.g., |ar| = 11211 and \\x \\ = 6 ).",
        "Note that when \\xc\\ becomes close to |a;|, this time complexity actually approaches 0(|a;|).",
        "Thus, to minimize this computational cost, xcis to be chosen from Vc as follows:",
        "There are two issues with speeding up the classifier by the computation shown in Eq.",
        "6.",
        "First, since we can store weights for only a small fraction of possible feature vectors (namely, | Vc| <C 2^), we should choose Vc so as to maximize its impact on the speed-up.",
        "Second, we should quickly find an optimal xc from Vc for a given feature vector x.",
        "The solution to the first problem is to enumerate partial feature vectors that frequently appear in the target task.",
        "Note that typical linguistic features used in nlp tasks usually consist of disjunctive sets of features (e.g., word surface and pos), in which each set is likely to follow Zipf's law (Zipf, 1949) and correlate with each other.",
        "We can expect the distribution of feature vectors, the mixture of Zipf distributions, to be Zipfian.",
        "This has been confirmed for word n-grams (Egghe, 2000) and itemset support distribution (Chuang et al., 2008).",
        "We can thereby expect that a small set of partial feature vectors commonly appear in the task.",
        "To solve the second problem, we introduce a feature sequence trie (fstrie), which represents a hierarchy of feature vectors, to enable the classifier to efficiently retrieve (sub-)optimal xc (in Eq.",
        "9) for a given feature vector x.",
        "We build an fstrie in the following steps:",
        "Step 1: Apply the target classifier to actual (raw) data in the task to enumerate possible feature vectors (hereafter, source feature vectors).",
        "source feature vectors feature sequence trie root",
        "This is the maximum number of conjunctive features.",
        "Step 2: Sort the features in each source feature vector according to their frequency in the training data (in descending order).",
        "Step 3: Build a trie from the source feature vectors by regarding feature indices as characters and store weights of all prefix feature vectors.",
        "An fstrie built from six source feature vectors is shown in Figure 2.",
        "In fstries, a path from the root to another node represents a feature vector.",
        "An important point here is that the fstrie stores the weights of all prefix feature vectors of the source feature vectors, and the trie structure enables us to retrieve for a given feature vector x the weight of its longest prefix vector xc Ç x in 0(\\xc\\) time.",
        "To handle feature functions in LLMs (Eq.",
        "1), we store partial weight WXcty = Y.iwi,yfi,y(xc,y) for each label y on the node that expresses xc.",
        "Since we sort the features in the source feature vectors according to their frequency, the prefix feature vectors exclude less frequent features in the source feature vectors.",
        "Lexical features or finer-grained features (e.g., pos-subcategory) are usually less frequent than coarse-grained features (e.g., pos), so they lie in the latter part of the feature vectors.",
        "This sorting helps us to retrieve longer feature vector xc for input feature vector x that will have diverse infrequent features.",
        "It also minimizes the size of fstrie by sharing the common frequent prefix (e.g., {/i, /ia} in Figure 2).",
        "Pruning nodes from fstrie We have so far described the way to construct an fstrie from the source feature vectors.",
        "However, a naive enumeration of source feature vectors will result in the explosion of the fstrie size, and we want to have a principled way to control the fstrie size rather than reducing the processed data size.",
        "Below, we present a method that prunes useless prefix feature vectors (nodes) from the constructed fstrie to maximize its impact on the classifier efficiency.",
        "Algorithm 1 Prune Nodes from Fstrie Input: fstrie T, nodejimit JVeN Output: fstrie T l: while # of nodes in T > N do",
        "x'eleaf{T)",
        "3: remove xc, T 4: end while 5: return T",
        "We adopt a greedy strategy that iteratively prunes a leaf node (one prefix feature vector and its weight) from the fstrie built from all the source feature vectors, according to a certain utility score calculated for each node.",
        "In this study, we consider two metrics for each prefix feature vector xcto calculate its utility score.",
        "Probability p(xc), which denotes how often the stored weight WXc will be used in the target task.",
        "The maximum-likelihood estimation provides probability:",
        "Algorithm 2 Compute Weight with Fstrie where nx G N is the frequency count of a source feature vector x in the processed data.",
        "Computation reduction Ad(xc), which denotes how much computation is reduced by WXc to calculate a weight of x D xc.",
        "This can be estimated by counting the number of conjunctive features we additionally have to check when we remove xc.",
        "Since the fstrie stores the weight of a prefix feature vector xc- c xcsuch that \\xc-\\ = \\xc\\ – 1 (e.g., in Figure 2, Xc- = {fi,h} for xc = {/i,/2,/4}), we can define the computation reduction as:",
        "(■•■ Eq.",
        "8).",
        "We calculate utility score of each node xc in the fstrie as u(xc) = p(xc) ■ Ad(xc), which means the expected computation reduction by xc in the target task, and prune the lowest-utility-score leaf nodes from the fstrie one by one (Algorithm 1).",
        "If several prefix vectors have the same utility score, we eliminate them in numerical descending order.",
        "Input: fstrie T, weight vector w G",
        "feature vector x G 2rOutput: weight W = g(x) G R for all feature fj G xd – xd do",
        "W <- W + wj end for return W",
        "Our classification algorithm is shown in detail in Algorithm 2.",
        "The classifier first sorts the active features in input feature vector x according to their frequency in the training data.",
        "Then, for x, it retrieves the longest common prefix vector xc from the fstrie (line 2 in Algorithm 2).",
        "It then adds the weights of the remaining features to partial weight WXc (line 5 in Algorithm 2).",
        "Note that the remaining features whose weights we sum up (line 4 in Algorithm 2) are primitive and conjunctive features that relate to / G x – xc, which appear less frequently than /' G xc in the training data.",
        "Thus, when we apply our algorithm to classifiers with the sparse solution (e.g., svm-HKEs or ^i-LLMs), \\xd\\ – \\xd\\ can be much smaller than the theoretical expectation (Eq.",
        "8).",
        "We confirmed this in the following experiments."
      ]
    },
    {
      "heading": "4. Evaluation",
      "text": [
        "We applied our algorithm to svm-ke, svm-hke, and £\\-LLM classifiers and evaluated the resulting classifiers in a lapanese dependency parsing task.",
        "To the best of our knowledge, there are no previous reports of an exact weight calculation faster than linear summation (Eqs.",
        "1 and 5).",
        "We also compared our svm classifier with a classifier called polynomial kernel inverted (pki: Kudo and Matsumoto (2003)), which uses the polynomial kernel (Eq.",
        "4) and inverted indexing to support vectors.",
        "A lapanese dependency parser inputs bunsetsu-segmented sentences and outputs the correct head (bunsetsu) for each bunsetsu; here, a bunsetsu is a grammatical unit in lapanese consisting of one or more content words followed by zero or more function words.",
        "A parser generates a feature vec-",
        "Modifier, head word (surface-form, pos, pos-subcategory, modifiée inflection form), functional word (surface-form, bunsetsu pos, pos-subcategory, inflection form), brackets, quotation marks, punctuation marks, position in",
        "sentence (beginning, end) Between distance (1, 2-5, 6-), case-particles, brackets, bunsetsus quotation marks, punctuation marks",
        "Table 1 : Feature set used for experiments.",
        "tor for a particular pair of bunsetsus (modifier and modifiée candidates) by exploiting the head-final and projective (Nivre, 2003) nature of dependency relations in Japanese.",
        "The classifier then outputs label y = '+1' (dependent) or ' – 1' (independent).",
        "Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost state-of-the-art accuracy (Iwatate et al., 2008).",
        "This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself.",
        "Due to space limitations, we omit the details of the parsing algorithm.",
        "We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1).",
        "Note that features listed in the 'Between bunsetsus' row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector.",
        "This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition.",
        "For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.",
        "The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively.",
        "The training samples generated from the training set included 150,064 positive and 146,712 negative samples.",
        "The following experiments were performed on a server with an Intel® Xeon™ 3.20-GHz CPU.",
        "We used TinySVM and a simple C++ library for maximum entropy classification to train SVMs and ^i-LLMs, respectively.",
        "We used Darts-Clone,",
        "Table 2: Specifications of LLMs and SVMs.",
        "The accuracy marked with or '>' was significantly better than the d = 2 counterpart (p < 0.01 or 0.01 < p < 0.05 by McNemar's test).",
        "a double-array trie (Aoe, 1989; Yata et al., 2008), as a compact trie implementation.",
        "All these libraries and algorithms are implemented in C++.",
        "The code for building f stries occupies 100 lines, while the code for the classifier occupies 20 lines (except those for kernel expansion).",
        "Specifications of SVMs and LLMs used here are shown in Table 2; \\Jrd\\ is the number of active features, while I xd I is the average number of active features in each classification for the test corpus.",
        "Dependency accuracy is the ratio of dependency relations correctly identified by the parser, while sentence accuracy is the exact match accuracy of complete dependency relations in a sentence.",
        "For LLM training, we designed explicit conjunctive features for all the d or lower-order feature combinations to make the results comparable to those of SVMs.",
        "We could not train d = 4 LLMs due to parameter explosion.",
        "We varied SVM soft margin parameter c from 0.1 to 0.000001 and LLM width factor parameter uj, which controls the impact of the prior, from 1.0 to 5.0, and adjusted the values to maximize dependency accuracy for the development set: [d, c) = (1,0.1), (2,0.005), (3,0.0001), (4, 0.000005) for SVMs and (d, uj) = (1,1.0), (2, 2.0), (3, 4.0) for ^i-LLMs.",
        "The accuracy of around 90.9% (svm-ke, d = 3,4) is close to the performance of state-of-theart parsers (Iwatate et al., 2008), and the model statistics are considered to be complex (or realistic) enough to evaluate our classifier's utility.",
        "The number of support vectors of SVMs was 71, 766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task.",
        "Model type Model d uj 1 a",
        "Model statistics",
        "\\Td\\ \\xd\\",
        "Dep.",
        "Sent, acc.",
        "acc.",
        "svm-ke 1 0",
        "39712 27.3",
        "88.29 46.49",
        "svm-ke 2 0",
        "1478109 380.6",
        "90.76 53.83",
        "svm-ke 3 0 svm-hke 3 0.001 svm-hke 3 0.002 svm-hke 3 0.003",
        "26194354 3286.7 13247675 2725.9 2514385 2238.1 793195 1855.4",
        "90.93^54.43^ 90.92» 54.39» 90.91» 54.32> 90.83 54.21",
        "svm-ke 4 0 svm-hke 4 0.0002 svm-hke 4 0.0004 svm-hke 4 0.0006",
        "293416102 20395.4 96522236 15282.1 19245076 11565.0 7277592 8958.2",
        "90.9P54.69»",
        "90.93» 54.53> 90.96» 54.64» 90.84 54.48>",
        "4-llm 1 1.0",
        "9268 26.5",
        "88.22 46.06",
        "4-llm 2 2.0",
        "32575 309.8",
        "90.62 53.46",
        "4-llm 3 3.0 4-llm 3 4.0 4-llm 3 5.0",
        "129503 2088.3 85419 1803.0 63046 1699.5",
        "90.71 54.09>90.61 53.79 90.59 53.55",
        "We could clearly observe that the number of active features \\xd\\ increased dramatically according to the order d of feature combinations.",
        "The density of \\xd\\ for SVMs was very high (e.g., \\x\\ = 3286.7, close to the maximum shown in Eq.",
        "8: (27.3 + 5 x 27.3)/6 ~ 3414.",
        "For d > 3 models, we attempted to control the size of the feature space \\Jrd\\ by changing the model's hyper-parameters: threshold a for the SVM-HKE and width factor u for the £\\-LLM.",
        "Although we successfully reduced the size of the feature space \\Jrd\\, we could not dramatically reduce the average number of active features |ccd| in each classification while keeping the accuracy advantage.",
        "This confirms that the solution sparseness does not suffice to obtain an efficient classifier.",
        "We obtained source feature vectors to build fstries by applying parsers with the target classifiers to a raw corpus in the target domain, 3,258,313 sentences of 1991-94 Mainichi news articles that were morphologically analyzed by JUMAN and segmented into bunsetsus by KNPWe first built fstrieL using all the source feature vectors.",
        "We then attempted to reduce the number of prefix feature vectors in fstrieL to l/2ra the size by Algorithm 1.",
        "We refer to fstries built from 1/32 and 1/1024 of the prefix feature vectors in fstrieLas fstrieM and fstries in the following experiments.",
        "Because we exploited Algorithm 2 to calculate the weights of the prefix feature vectors, it took less than one hour (59 min.",
        "29 sec.)",
        "on the 3.20-GHz server to build fstrieL (and calculate the utility score for all the nodes in it) for the slowest SVM-KE (d = 4) from the 40,409,190 source feature vectors (62,654,549 prefix feature vectors) generated by parsing the 3,258,313 sentences.",
        "S 0.5 ......-......................................■.............................................................................",
        "Results for svm-ke with dense feature space",
        "The performances of parsers having svm-ke classifiers with and without the fstrie are given in Table 3.",
        "The 'speed-up' column shows the speed-up factor of the most efficient classifier (bold) versus the baseline classifier without fstries.",
        "Since each classifier solved a slightly different number of classification steps (112,853 ± 0.15%), we show the (average) cumulative classification time for a sentence.",
        "The Mem.",
        "columns show the size of weight vectors for svm-ke classifiers and the size of fstriess, fstriesM, and fstriesL, respectively.",
        "The fstries successfully speeded up svm-ke classifiers with the dense feature space.",
        "The svm-ke classifiers without fstries were still faster than pki, but as expected from a large \\xd\\ value, the classifiers with higher conjunctive features were much slower than the classifier with only primitive features by factors of 13 (d = 2), 109 (d = 3) and 738 (d = 4) and the classification time accounted for most of the parsing time.",
        "The average classification time of our classifiers plotted against fstrie size is shown in Figure 3.",
        "Surprisingly, we obtained a significant speed-up even with tiny fstrie sizes of < 1 MB.",
        "Furthermore, we naively controlled the fstrie size by sim-",
        "Model type d",
        "PKI",
        "classify",
        "[ms/sent.]",
        "Baseline Mem.",
        "Time [ms/sent.]",
        "(MB) classify (total)",
        "Proposed w/ fstriesMem.",
        "Time [ms/sent.]",
        "(MB) classify (total)",
        "Proposed w/ fstrieMMem.",
        "Time [ms/sent.]",
        "(MB) classify (total)",
        "Proposed w/ fstrieLMem.",
        "Time [ms/sent.]",
        "(MB) classify (total)",
        "Speed up",
        "SVM-KE 1",
        "13.480",
        "0.2 0.003 (0.015)",
        "+0.6 0.006 (0.018)",
        "+20.2 0.007 (0.018)",
        "+662.9 0.016 (0.029)",
        "NA",
        "SVM-KE 2",
        "10.313",
        "13.5 0.041 (0.054)",
        "+0.5 0.020 (0.032)",
        "+18.0 0.021 (0.034)",
        "+662.4 0.023 (0.036)",
        "2.1",
        "SVM-KE 3",
        "10.945",
        "142.2 0.345 (0.361)",
        "+0.5 0.163 (0.178)",
        "+18.2 0.108 (0.123)",
        "+667.0 0.079(0.093)",
        "4.4",
        "SVM-KE 4",
        "12.603",
        "648.0 2.338 (2.363)",
        "+0.5 1.156 (1.178)",
        "+18.6 0.671 (0.690)",
        "+675.9 0.415(0.432)",
        "5.6",
        "naive – utility score – ",
        "ply reducing the number of sentences processed to 1/2\",.",
        "jYie impact on the speed-up of the resulting fstries (naive) and the fstries constructed by our utility score (utility-score) on svm-ke (d = 4) is shown in Figure 4.",
        "The Zipfian nature of language data let us obtain a substantial speed-up even when we naively reduced the fstrie size, and the utility score further decreased the fstrie size required to obtain the same speed-up.",
        "We needed less than 1/3 size fstries to achieve the same speedup: 0.671 ms./sent.",
        "(18.6 MB) (utility-score) vs. 0.680 ms./sent.",
        "(67.1 MB) (naive).",
        "Results for svm-hke and 4-llm classifiers with sparse feature space The performances of parsers having svm-hke and ^i-llm classifiers with and without the fstrie are given in Table 4.",
        "The fstries successfully speeded up the svm-hke and ^i-llm classifiers by factors of 10.7 (svm-hke, d = 4, a = 0.0006) and 11.6 (4-llm, d = 3, uj = 3.0).",
        "We obtained more speedup when we used fstries for classifiers with more sparse feature space Td (Figures 5 and 6).",
        "The parsing speed with d = 3 models are now comparable to the parsing speed with d = 2 models.",
        "Without fstries, little speed-up of svm-hke classifiers versus the svm-ke classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features \\xd\\ in the classification.",
        "This result conforms to the results reported in (Kudo and Matsumoto, 2003).",
        "Model type d a 1 uj",
        "Baseline Mem.",
        "Time [ms/sent.]",
        "(mb) classify (total)",
        "Proposed w/ fstriesMem.",
        "Time [ms/sent.]",
        "(mb) classify (total)",
        "Proposed w/ fstrieMMem.",
        "Time [ms/sent.]",
        "(mb) classify (total)",
        "Proposed w/ fstrieLMem.",
        "Time [ms/sent.]",
        "(mb) classify (total)",
        "Speed up",
        "svm-hke 3 0.001 svm-hke 3 0.002 svm-hke 3 0.003",
        "64.6 0.348 (0.363) 13.9 0.332(0.346) 4.2 0.314(0.328)",
        "+0.5 0.151 (0.166) +0.5 0.123 (0.137) +0.4 0.102(0.115)",
        "+17.6 0.097 (0.111) +17.0 0.074(0.088) +14.7 0.057 (0.070)",
        "+638.0 0.070(0.084) +612.2 0.053 (0.067) +526.2 0.041 (0.054)",
        "5.0 6.2 7.8",
        "svm-hke 4 0.0002 svm-hke 4 0.0004 svm-hke 4 0.0006",
        "235.0 2.258 (2.280) 82.8 2.038 (2.058) 32.2 1.802(1.820)",
        "+0.5 1.022(1.042) +0.5 0.816 (0.835) +0.4 0.646 (0.662)",
        "+17.7 0.558 (0.575) +16.8 0.414(0.430) +15.7 0.311 (0.326)",
        "+637.1 0.330(0.346) +601.7 0.234 (0.249) +558.9 0.168(0.183)",
        "6.8 8.7 10.7",
        "4-llm 1 1.0",
        "0.1 0.004 (0.016)",
        "+0.8 0.006 (0.018)",
        "+25.0 0.007 (0.019)",
        "+787.7 0.016 (0.029)",
        "NA",
        "4-llm 2 2.0",
        "0.4 0.043 (0.055)",
        "+0.6 0.016 (0.028)",
        "+20.5 0.015(0.027)",
        "+698.0 0.018 (0.030)",
        "2.9",
        "4-llm 3 3.0 4-llm 3 4.0 4-llm 3 5.0",
        "1.0 0.314(0.326) 0.7 0.300 (0.313) 0.5 0.290 (0.302)",
        "+0.5 0.091 (0.103) +0.5 0.082 (0.094) +0.5 0.076 (0.088)",
        "+17.8 0.041 (0.054) +16.3 0.036 (0.049) +15.1 0.032(0.045)",
        "+601.0 0.027(0.040) +550.1 0.024 (0.037) +510.7 0.022(0.035)",
        "11.6 12.4 13.3",
        " – ",
        "2.5",
        "U",
        "/sin]",
        "2",
        "U",
        "J",
        "1.5",
        "e",
        "c",
        "ta",
        "1",
        "clas:",
        "0.5",
        "u",
        "0",
        "updated in November 2008, to see how much the impact of fstries lessens when the test data and the data processed to build fstries mismatch.",
        "The parsing time was 156.4 sec.",
        "without fstrieL, while it was just 35.9 sec.",
        "with fstrieL.",
        "The speed-up factor of 4.4 on weblog feeds was slightly worse than that on news articles (0.346/0.067 = 5.2) but still evident.",
        "This implies that sorting features in building fstries yielded prefix features vectors that commonly appear in this task, by excluding domain-specific features such as lexical features.",
        "In summary, our algorithm successfully minimized the efficiency gap among classifiers with different degrees of feature combinations and made accurate classifiers trained with higher-order feature combinations practical."
      ]
    },
    {
      "heading": "5. Conclusion and Future Work",
      "text": [
        "Our simple method speeds up a classifier trained with many conjunctive features by using precalculated weights of (partial) feature vectors stored in a feature sequence trie (fstrie).",
        "We experimentally demonstrated that it speeded up SVM and LLM classifiers for a Japanese dependency parsing task by a factor of 10.",
        "We also confirmed that the sparse feature space provided by i4-LLMs and SVM-HKEs contributed much to size reduction of the fstrie required to achieve the same speed-up.",
        "The implementations of the proposed algorithm for LLMs and SVMs (with a polynomial kernel) and the Japanese dependency parser will be available at http://www.tkl.iis.u-tokyo.ac.jp/~ynaga/.",
        "We plan to apply our method to wider range of classifiers used in various NLP tasks.",
        "To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs.",
        "When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage.",
        "We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al.",
        "(2009)).",
        "It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers.",
        "We will in the future consider an issue of speeding up decoding with structured models (Lafferty et al., 2001; Miyao and Tsujii, 2002; Sutton et al., 2004).",
        "Acknowledgment The authors wish to thank Susumu Yata and Yoshimasa Tsuruoka for letting the authors to use their pre-release libraries.",
        "The authors also thank Nobuhiro Kaji and the anonymous reviewers for their valuable comments."
      ]
    }
  ]
}
