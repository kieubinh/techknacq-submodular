{
  "info": {
    "authors": [
      "Claudio Giuliano",
      "Alfio Massimiliano Gliozzo",
      "Carlo Strapparava"
    ],
    "book": "Computational Linguistics",
    "id": "acl-J09-4007",
    "title": "Kernel Methods for Minimally Supervised WSD",
    "url": "https://aclweb.org/anthology/J09-4007",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Claudio Giuliano*",
        "Fondazione Bruno Kessler - IRST",
        "Alfio Massimiliano Gliozzo*",
        "We present a semi-supervised technique for word sense disambiguation that exploits external knowledge acquired in an unsupervised manner.",
        "In particular, we use a combination of basic kernel functions to independently estimate syntagmatic and domain similarity, building a set of word-expert classifiers that share a common domain model acquired from a large corpus of unlabeled data.",
        "The results show that the proposed approach achieves state-of-the-art performance on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although it uses a considerably smaller number of training examples than other methods."
      ]
    },
    {
      "heading": "1.. Introduction",
      "text": [
        "A significant challenge in manynatural language processing tasks is to reduce the need for labeled training data while maintaining an acceptable performance.",
        "This is espe-ciallytrue for word sense disambiguation (WSD) because when moving from the somewhat artificial lexical-sample task to the more realistic all-words task it is practically impossible to collect a large number of training examples for each word sense.",
        "Thus, manysupervised approaches, explicitlydesigned for the lexical-sample task, cannot be applied to the all-words task, even though theyexhibit excellent performance.",
        "This has led to the somewhat paradoxical situation in which completelydifferent methods have been developed for the two tasks, although theyrepresent two sides of the same coin.",
        "To address this problem, in recent work we presented a semi-supervised approach based on kernel methods for WSD (Strapparava, Gliozzo, and Giuliano 2004; Gliozzo, Giuliano, and Strapparava 2005; Giuliano, Gliozzo, and Strapparava 2006).",
        "In particular, we explored the following research directions: (1) independentlymodeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (2) exploiting external knowledge acquired from unlabeled data, with the purpose of drasticallyreducing the amount of labeled training data.",
        "The first direction is based on the linguistic assumption that syntagmatic and domain (associative) relations are crucial for representing sense distinctions, but theyare originated bydifferent phenomena.",
        "Regarding the second direction, one can hope to obtain a more accurate prediction",
        "* FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy.",
        "E-mail: giuliano@fbk.eu.",
        "** FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy.",
        "E-mail: gliozzo@fbk.eu.",
        "1 FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy.",
        "E-mail: strappa@fbk.eu.",
        "Submission received: 23 December 2006; revised submission received: 28 February2008; accepted for publication: 17 April 2008.",
        "bytaking into account unlabeled data relevant to the learning problem (Chapelle, Scholkopf, and Zien 2006).",
        "As a matter of fact, to test this hypothesis, most of the lexical sample tasks of Senseval-3 (Mihalcea and Edmonds 2004) were provided with a large amount of unlabeled training data, as well as the usual labeled training data.",
        "However, at that time, we were the onlyteam to use the unlabeled data (Strapparava, Gliozzo, and Giuliano 2004).",
        "In this article, we review our technique that combines domain and syntagmatic information in order to define a complete kernel for WSD.",
        "The rest of the article is organized as follows.",
        "In Section 2, we provide a general introduction to the kernel methods, in which we give the basis for understanding our approach.",
        "Exploiting kernel methods, we can define and combine individual kernels representing information from different sources in a principled way.",
        "After this introductory section, in Section 3 we present the kernels that we developed for WSD.",
        "This includes a detailed description of the individual kernels and the waywe define the composite ones.",
        "We present our experiments in Section 4.",
        "The results obtained on a range of lexical-sample tasks and on the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our approach achieves state-of-the-art performance.",
        "Finally, in Section 5, we offer conclusions and some directions for future research."
      ]
    },
    {
      "heading": "2.. Kernel Methods",
      "text": [
        "Kernel methods are a popular machine learning approach within the natural language processing community.",
        "They are theoretically well founded in statistical learning theoryand have shown good empirical results in manyapplications (Vapnik 1999; Cristianini and Shawe-Taylor 2000; Scholkopf and Smola 2002; Shawe-Taylor and Cristianini 2004).",
        "The strategyadopted bykernel methods consists of splitting the learning problem into two parts.",
        "Theyfirst embed the input data in a suitable feature space, and then use a linear algorithm to discover nonlinear patterns in the input space.",
        "Typically, the mapping is performed implicitlybya so-called kernel function.",
        "The kernel function is a similaritymeasure between the input data that depends exclusivelyon the specific data type and domain.",
        "A typical similarity function is the inner product between feature vectors.",
        "Characterizing the similarityof the inputs plays a crucial role in determining the success or failure of the learning algorithm, and it is one of the central questions in the field of machine learning.",
        "Formally, the kernel is a function K : X x X – R that takes as input two data objects (e.g., vectors, texts, or parse trees) and outputs a real number characterizing their similarity, with the property that the function is symmetric and positive semi-definite.",
        "That is, for all xi, xj £ X satisfies where cf> is an (implicit) mapping from X to an (inner product) feature space F.",
        "Kernels are used inside learning algorithms such as support vector machines (SVM) or kernel perceptrons as the interface between the algorithm and the data.",
        "The kernel function is then the onlydomain specific element of the system, while the learning algorithm is a general purpose component.",
        "The idea behind the SVM (one of the best known kernel-based learning algorithms) is to map the set of training data into a high-dimensional feature space F via a mapping function cf> : X – F, and construct a separating hyperplane with maximum margin (i.e., the minimum distance between the hyperplane and data points) in that space.",
        "The use of an appropriate non-linear transformation | of the input yields a nonlinear decision boundaryin the input space.",
        "Kernel functions make possible the use of feature spaces with an exponential or even infinite number of dimensions.",
        "Instead of performing the explicit feature mapping cf>, one can use a kernel function, which permits the (efficient) computation of inner products in high-dimensional feature spaces without explicitly carrying out the mapping | .",
        "This is called the kernel trick in the machine learning literature (Boser, Guyon, and Vapnik 1992).",
        "Finally, we point out the theoretical tools required to create new kernels, and combine individual kernels to form composite ones.",
        "Of course, not everysimilarityfunction is a valid kernel because, bydefinition, kernels should be equivalent to some inner product in a feature space.",
        "The function K : X x X – R is a valid kernel provided that bymeans of kernels maybe more intuitive than performing the explicit mapping in the feature space.",
        "Furthermore, this formulation does not require the set X to be a vector space: for example, we shall define kernels that take strings as input.",
        "This result is not onlyuseful because it opens new perspectives to define kernel functions that onlyimplicitlycorrespond to a feature mapping | .",
        "Another consequence is that it can be used to prove a set of rules for combining basic kernels to obtain composite ones.",
        "This will allow us to integrate heterogeneous sources of information in a simple and effective way.",
        "We shall use the following properties of kernels to define our composite kernels.",
        "Let k and k2 be kernels over X x X; then the following functions are kernels:",
        "In summary, we can define a kernel function by following different strategies: (1) providing an explicit feature mapping cf> : X – Rn; (2) defining a similarityfunction that is symmetric and positive semi-definite; and (3) composing different valid kernels, using the closure properties of kernels.",
        "This forms the basis for the approach described in the following section."
      ]
    },
    {
      "heading": "3.. Kernel Methods for WSD",
      "text": [
        "Our approach to WSD consists of representing linguistic phenomena independentlyand then defining a combination method to integrate them.",
        "As described in the previous section, the kernel function is the onlytask-specific component of the learning algorithm.",
        "Thus, to develop a WSD system, we only need to define appropriate kernel functions to represent the domain and syntagmatic aspects of sense distinction and, second, exploit the properties of kernel functions to define a composite kernel to combine and extend the individual kernels.",
        "The resulting WSD system consists of two families of kernels: the domain and the syntagmatic kernels.",
        "The former family, described in Section 3.1, models the domain",
        "1 Given a set of vectors S = {x1,xi}, the kernel matrix K is defined as the l x l matrix K whose entries are Kj = k(xi,xj) = (4>(x;), 4>(xj)},where k is a kernel function that evaluates the inner products in a feature space with feature map cf>.",
        "2 A symmetric matrix is positive semi-definite if its eigenvalues are all non-negative.",
        "Actually, as we will see in Section 3.2 using Proposition 1, it is quite easyto verifythis property.",
        "(normalization)",
        "An example of a domain matrix.",
        "Medicine Computer Science",
        "aspects of sense distinction; it is composed of the domain kernel (KD) and the bag-of-words kernel (KBoW).",
        "The latter, described in Section 3.2, represents the syntagmatic aspects of sense distinction; it is composed of the collocation kernel (Kq,h) and the part-of-speech kernel (KPoS).",
        "Finally, Section 3.3 describes the composite kernel for WSD.",
        "It has been shown that domain information is fundamental for WSD (Magnini et al.",
        "2002).",
        "For instance, the (domain) polysemy between the computer science and the medicine senses of the word virus can be solved byconsidering the domain of the context in which it appears.",
        "Gliozzo, Strapparava, and Dagan (2004) proposed a WSD method that exploits onlydomain information.",
        "In the context of kernel methods, domain information can be exploited bydefining a kernel function that estimates the domain similaritybetween the contexts of the words to be disambiguated.",
        "The simplest method to estimate the domain similarity between two texts is to compute the cosine similarityof their vector representations in the vector space model (VSM).",
        "The VSM is a k-dimensional space Rk, in which the text tj is represented bya vector tj, where the ith component is the term frequencyof the term w; in tj.",
        "However, such an approach does not deal well with lexical variability and ambiguity.",
        "For instance, despite the fact that the sentences He is affected by AIDS and HIV is a virus express closely-related concepts, their similarity is zero in the VSM because theyhave no words in common (theyare represented byorthogonal vectors).",
        "On the other hand, due to the ambiguityof the word virus, the similaritybetween the sentences The laptop has been infected by a virus and HIV is a virus is greater than zero, even though theyconveyverydifferent messages.",
        "To overcome this problem, we introduce the domain model (DM) and show how to use it to define a domain VSM in which texts and terms are represented in a uniform way.",
        "A DM is composed of soft clusters of terms.",
        "Each cluster represents a semantic domain, that is, a set of terms that often co-occur in texts having similar topics.",
        "A DM is represented bya k x k!",
        "rectangular matrix D, containing the degree of association among terms and domains, as illustrated in Table 1.",
        "The matrix D is used to define a function V : Rk – Rk , that maps the vector ttj represented in the standard VSM into the vector i'- in the domain VSM.",
        "V is defined as follows:",
        "3 In Wong, Ziarko, and Wong (1985), Equation (2) is used to define a generalized vector space model, of which the domain VSM is a particular instance.",
        "where tj is represented as a row vector, IIDF is a k x k diagonal matrix such that iI/DF = IDF(wi ), and IDF(wi ) is the inverse document frequencyof wi.",
        "In the domain space, the similarityis estimated bytaking into account second order relations among terms.",
        "For example, the similarityof the two sentences He is affected by AIDS and HIV is a virus is veryhigh, because the terms AIDS, HIV,and virus are stronglyassociated with the medicine domain.",
        "A DM can be estimated from manuallyconstructed lexical resources, such as Word-Net Domains (Magnini and Cavaglia 2000), or byperforming a term-clustering process on a (large) corpus.",
        "However, the second approach is more attractive because it allows us to automaticallyacquire DMs for different languages and domains.",
        "In Gliozzo, Giuliano, and Strapparava (2005), we use singular valued decomposition (SVD) to acquire DMs from a corpus represented byits term-by-document matrix T, in a unsupervised way.",
        "SVD decomposes the term-by-document matrix T into three matrixes T ~ VSk/ UT, where V and U are orthogonal matrices (i.e., VTV = I and UTU = I) whose columns are the eigenvectors of TTT and TTT, respectively, and Sk/ is the diagonal k x k matrix containing the highest k!",
        "<< k eigenvalues of T, and all the remaining elements set to 0.",
        "The parameter k!",
        "is the dimensionalityof the domain VSM and can be fixed in advance.",
        "Under this setting, we define the domain matrix D as follows:",
        "D = INVv/ËiJ (3) where IN is a diagonal matrix such that iN = ,1 , vj\\ is the ith row of the matrix",
        "Note that in this case, with respect to Table 1, the domains are represented bythe columns of the matrix D and theydo not have an explicit name.",
        "Byusing a small number of domains, we can define a verycompact representation of the DM and, consequently, reduce the memory requirements while preserving most of the information.",
        "There exist veryefficient algorithms to perform the SVD process on sparse matrices, allowing us to perform this operation on large corpora in a verylimited time and with reduced memoryrequirements.",
        "Therefore, we can define the domain kernel to estimate the domain similarity between the contexts of the words to be disambiguated.",
        "It is a variant of the latent semantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited to define an explicit mapping V : Rk – Rk from the classical VSM into the domain VSM.",
        "The domain kernel is explicitlydefined as follows:",
        "Kd (ti, tj ) = (V(ti ), V(tj )> (4) where V is the domain mapping defined in Equation (2).",
        "4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semantic",
        "indexing of documents in large corpora (Deerwester et al.",
        "1990).",
        "5When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space (Deerwester et al.",
        "1990).",
        "The onlydifference in our formulation is that the vectors representing the terms in the domain VSM are normalized bythe matrix IN, and then rescaled, according to their IDF value, by matrix IIDF.",
        "Note the analogywith the tf-idf term weighting schema (Salton and McGill 1983), widely",
        "used in information retrieval.",
        "6 To perform the SVD, we used LIBSVDC, an optimized package for sparse matrices that allows us to perform this step in a few minutes even for large corpora.",
        "It can be downloaded from",
        "http://tedlab.mit.edu/~dr/SVDLIBC/.",
        "A standard approach for detecting topic (domain) similarityis to extract bag-of-words features from a wide window of text around the words to be disambiguated.",
        "Based on this representation, we define a linear kernel called the bag-of-words kernel (KBoW).",
        "KBoW is a particular case of the domain kernel in which D = I in Equation (2), where I is the identitymatrix.",
        "The BoW kernel does not require a DM; therefore, it can be applied to the strictlysupervised settings, in which external knowledge is not available.",
        "To summarize, the domain kernel allows us to plug external knowledge into the supervised learning process; it will be compared and combined with the standard bag-of-words approach in Section 4.",
        "In the following section, we shall see that domain models are also useful for defining soft-matching collocation kernels.",
        "Collocations (such as bigrams and trigrams) extracted from the local context of the word to be disambiguated are typically used to capture syntagmatic relations (Yarowsky 1994).",
        "However, traditional approaches to WSD fail to represent non-contiguous or shifted collocations, and fail to consider lexical variability.",
        "For example, suppose we have to disambiguate the verb to score in the sentence Ronaldo scored the first goal,given the labeled example The football player scored two goals in the second half as training.",
        "A traditional approach has no clues to return the right answer because the two sentences have no features in common.",
        "The use of kernels on strings allows us to overcome the aforementioned problems byrepresenting (non-contiguous) collocations and exploiting external lexical knowledge sources to define non-zero measures of similaritybetween words (soft-matching criteria).",
        "In this formulation, words taken in their context are compared bykernels that sum the number of common (non-contiguous) collocations of words, considering lexical variability, and part-of-speech tags, avoiding an explicit feature mapping that would lead to an exponential number of features.",
        "String kernels (or sequence kernels) are a familyof kernel functions developed to compute the inner product among images of strings in high-dimensional feature space using dynamic programming techniques.",
        "The gap-weighted subsequences kernel is one of the most general types of kernel based on sequences.",
        "Roughly speaking, it compares two strings bymeans of the number of contiguous and non-contiguous substrings of a given length theyhave in common.",
        "Non-contiguous occurrences are penalized according to the number of gaps theycontain.",
        "Formally, let S be an alphabet of \\S\\ symbols, and s = s1s2 ...s\\s\\ be a finite sequence over S (i.e., si £ S,1 < i < \\s\\).",
        "Let i = [ii,i2,in], with 1 < i1 < i2 < ... < in < \\s\\, be a subset of the indices in s; we will denote as s[i] £ Sn the subsequence si1 si2 ...sin.Notethat s[i] does not necessarilyform a contiguous subsequence of s; for example, if s is the sequence \"Ronaldo scored the first goal\" and i = [2,5], then s[i] is \"scored goal\".",
        "The length spanned by s[i]in s is = in – i1 + 1.",
        "The feature space associated with the gap-weighted subsequences kernel of length n is indexed by I =Sn, with the embedding given by where 0<À< 1 is the decayfactor used to penalize non-contiguous subsequences.",
        "The associate kernel is defined as",
        "An explicit computation of Equation (6) is unfeasible even for small values of n. To evaluate Kn more efficiently, we use the recursive formulation based on a dynamic programming implementation (Lodhi et al.",
        "2002; Saunders, Tschach, and Shawe-Taylor 2002; Cancedda et al.",
        "2003).",
        "It is defined in the following equations:",
        "where Kn and are auxiliaryfunctions with a similar definition to Kn used to facilitate the computation.",
        "Based on these definitions, Kn can be computed in 0(n\\s\\\\t\\).",
        "Using this recursive definition, it turns out that computing all kernel values for subsequences of lengths up to n is not significantlymore costlythan computing the kernel for n only.",
        "The syntagmatic kernel is defined as a sum of gap-weighted subsequences kernels that operate at word and part-of-speech tag level.",
        "In particular, following the approach proposed byCancedda et al.",
        "(2003), it is possible to adapt sequence kernels to operate at word level byinstancing the alphabet S with the vocabulary V = {w1, w2,..., w^}.",
        "Moreover, we restrict the generic definition of the gap-weighted subsequences kernel to recognize collocations in the local context of a specified word.",
        "The resulting kernel, called the n-gram collocation kernel (KC^), operates on sequences of lemmata around a specified word l0 (i.e., l_3, l_2, l0, l+2, l+3).",
        "This formulation allows us to estimate the number of common (sparse) subsequences of lemmata (i.e., collocations) between two examples, in order to capture syntagmatic similarity.",
        "Analogously, we define the part-of-speech kernel (K^g) to operate on sequences of part-of-speech tags p_3, p_2, p_1, p0, p+1, p+2, p+3, where p0 is the part-of-speech tag of l0.",
        "The collocation kernel and the part-of-speech kernel are defined byEquations (14) and (15), respectively.",
        "7 Notice that bychoosing À = 1, sparse subsequences are not penalized.",
        "On the other hand, the kernel does not take into account sparse subsequences with À – > 0.",
        "Both kernels depend on the parameter n, the length of the non-contiguous subsequences, and À, the decayfactor.",
        "For example, KLC(iU allows us to represent all (sparse) bigrams in the local context of a word.",
        "Finally, the syntagmatic kernel is defined as",
        "In the preceding definition, onlyexact word-matches contribute to the similarity.",
        "To solve this problem, external lexical knowledge is fed into the supervised learning process, allowing us to define the soft-matching collocation kernel.",
        "In particular, we define two alternative soft-matching criteria by exploiting synonymy relations in WordNet and DMs acquired from corpora.",
        "Both criteria are based on the assumption that every word in a sentence can be substituted byanother preserving the original meaning, if these words are paradigmatically related (e.g., synonyms, hyponyms, or domain related words).",
        "For example, if we consider as equivalent the terms Ronaldo and football player, then the sentence The football player scores the first goal is equivalent to Ronaldo scores the first goal, providing a strong evidence to disambiguate the verb to score in the second sentence.",
        "Following the approach proposed byShawe-Taylor and Cristianini (2004), the soft-matching gap-weighted subsequences kernel is now calculated recursivelyusing Equations (7)-(9), (11), and (12), replacing Equation (10) bythe equation:",
        "and modifying Equation (13) to:",
        "where axy are entries in a similaritymatrix A between terms.",
        "In order to ensure that the resulting kernel is still valid, A must be positive semi-definite.",
        "In the following sections, we describe the two alternative soft-matching criteria based on WordNet Synonymy and Domain Proximity, respectively.",
        "To show that the similaritymatrices are positive semi-definite, we use the following result.",
        "Proposition 1",
        "Amatrix A is positive semi-definite if and onlyif A = BTB for some real matrix B.",
        "The proof is given in Shawe-Taylor and Cristianini (2004).",
        "WordNet Synonymy.",
        "The first soft-matching criterion is based on WordNet to define a similaritymatrix between words.",
        "In particular, we substitute two words if theyare synonyms.",
        "To this end, a word is represented as vector whose dimensions are associated",
        "8 We used WordNet 1.7.1 and MultiWordNet for English and Italian experiments, respectively.",
        "with the synsets.",
        "Formally, we define the term-by-synset matrix S as the matrix whose rows are indexed bythe terms and whose columns are indexed bythe synsets.",
        "The (i, j)th entryof S is 1 if the synset Sj contains the term wf, 0 otherwise.",
        "The matrix S gives rise to the similaritymatrix A = SST between terms.",
        "Because A can be rewritten as A = (ST)TST = BTB, it follows directlyfrom Proposition 1 that it is positive semi-definite.",
        "Domain Proximity.",
        "The second soft-matching criterion exploits the domain models introduced in Section 3.1 to define a similaritymatrix between words.",
        "Once a DM has been defined bythe matrix D, the domain space is a k' dimensional space, in which both texts and terms are represented bymeans of domain vectors, that is, vectors representing the domain relevances among the linguistic object and each domain.",
        "The domain vector w' for the term wi eV is the ith row of D, where V = {w1, w2,..., Wk} is the vocabularyof the corpus.",
        "The term-by-domain matrix D gives rise to the similaritymatrix A = DDTbetween terms.",
        "It follows byProposition 1 that A is positive semi-definite.",
        "We shall show that the syntagmatic kernel is more effective than standard bigrams and trigrams of lemmata and part-of-speech tags typically used as features in WSD.",
        "Having defined all the individual kernels representing syntagmatic and domain aspects of sense distinction, we can define the composite kernel to combine and extend the individual kernels.",
        "The closure properties of the kernel functions allows us to define the composite kernel as where Ki is a valid individual kernel.",
        "The individual kernels are normalized – this plays an important role in allowing us to integrate information from heterogeneous feature spaces.",
        "Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empiricallyshown the effectiveness of combining kernels in this way: The composite kernel consistently improves the performance of the individual ones.",
        "In addition, this formulation allows us to evaluate the individual contribution of each information source.",
        "In order to show the effectiveness of the proposed domain model in supervised learning, we defined two WSD kernels, Kwsd and K'wsd.",
        "Theyare completelyspecified by the n individual kernels that compose them in Equation (19).",
        "Kwsd is composed by Kq,h, Kpos,and KBoW; K'wsd is composed by Kq,h, Kpos, KBoW,and Kd.",
        "The onlydifference between the two is that K'wsd uses the domain kernel KD to exploit external knowledge while Kwsd onlyuses the labeled training data."
      ]
    },
    {
      "heading": "4.. Evaluation",
      "text": [
        "Experiments were carried out on various tasks of Senseval-3 (Mihalcea and Edmonds 2004).",
        "First of all, we conducted a preliminaryset of experiments on the Catalan, English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1.",
        "Second, in order to show the general applicabilityof the proposed method, we evaluated the system on the English all-words task; the results are presented in Section 4.2.",
        "All the experiments were performed using the SVM package (Chang and Lin 2001) customized to embed our own kernels.",
        "The parameters were optimized byfive-fold cross-validation on the training set.",
        "In this section, we report the evaluation of our method on the Catalan, English, Italian, and Spanish lexical-sample tasks of Senseval-3 (Mihalcea and Edmonds 2004).",
        "Table 2 describes the tasks we have considered.",
        "For each task, it summarizes the number of words to be disambiguated, the mean polysemy, the size of the labeled training set, the size of the test set, and the size of the unlabeled training set, respectively.",
        "For the Catalan, Italian, and Spanish tasks, we acquired the DMs from the unlabeled corpora made available bythe task organizers.",
        "For the English task, we used a DM acquired from the British National Corpus (BNC) as the task organizers have not provided anyunlabeled training data.",
        "The objectives of these experiments are to (a) estimate the impact of different knowledge sources in WSD; (b) studythe effectiveness of the kernel combination; (c) understand the benefits of plugging external information in a supervised framework; and (d) verifythe portabilityof our methodologyto different languages.",
        "4.1.1 Results.",
        "Table 3 reports the results of the individual kernels KBoW, KD, KCon,and KPoS and their combinations Kwsd and K'wsd (the baselines for the tasks are reported in Table 5).",
        "In our experiments, the parameters n and À (see Equation (5)) are optimized byfive-fold cross-validation.",
        "For KCZoU, we obtained the best results with n = 2and À = 0.5.",
        "For Kp,oS, n = 3and À – 0.",
        "The domain cardinality k' was set to 50.",
        "Table 4 shows the performance of the syntagmatic kernel in different configurations: hard and soft matching.",
        "As a baseline, we report the result of a standard approach consisting of explicit bigrams and trigrams of words and part-of-speech tags around the words to be disambiguated (Yarowsky 1994).",
        "We evaluated the impact of the domain kernel on the overall performance bycomparing the learning curves of K'wsd and Kwsd on the four lexical-sample tasks.",
        "Figure 1 shows the results of our experiments.",
        "The points of the learning curves are obtained bysampling the same percentage of training examples for",
        "Description of the lexical-sample tasks of Senseval-3.",
        "Task",
        "#w",
        "mean polysemy",
        "#train",
        "#test",
        "#unlab",
        "Catalan",
        "27",
        "3.11",
        "4,469",
        "2,253",
        "23,935",
        "English",
        "57",
        "6.47",
        "7,860",
        "3,944",
        "-",
        "Italian",
        "45",
        "6.30",
        "5,145",
        "2,439",
        "74,788",
        "Spanish",
        "46",
        "3.30",
        "8,430",
        "4,195",
        "61,252",
        "The performance (F1) of the basic and composite kernels on the Catalan, English, Italian, and Spanish lexical-sample tasks of Semeval-3.",
        "Performance (F1) of the syntagmatic kernel for the Catalan, English, Italian, and Spanish lexical-sample tasks of Semeval-3.",
        "each word.",
        "Finally, Table 5 summarizes the results we obtained, providing a comparison with the state of the art.",
        "4.1.2 Discussion.",
        "Table 3 shows that domain information and syntagmatic information are crucial for WSD, and their combination significantlyoutperforms the individual kernels, showing the effectiveness of the kernel combination method.",
        "In addition, the domain kernel KD outperforms the bag-of-words kernel KBoW, and the composite kernel K'wsd that makes use of domain information outperforms the one Kwsd based onlyon the labeled training data, demonstrating our assumption (see Section 3).",
        "Table 4 shows that the syntagmatic kernel outperforms the baseline (bigrams and trigrams) in anyconfiguration (hard-/soft-matching).",
        "The soft-matching criteria further improve the classification performance.",
        "It is interesting to note that the domain proximity obtained better results than WordNet synonymy (note that we do not have a Catalan or a Spanish WordNet).",
        "The different results observed for Italian and English using the domain proximitysoft-matching criterion are probablydue to the small size of the unlabeled English corpus.",
        "Figure 1 shows that K'wsd outperforms Kwsd on all lexical sample tasks, even with a small number of examples.",
        "It is worth noting, as reported in Table 5, that K'wsd achieves the same performance as Kwsd using about half of the labeled training data.",
        "This result shows that the proposed semi-supervised learning approach consisting of acquiring domain models from unlabeled corpora is effective, as it allows us to drasticallyreduce the amount of labeled training data and provide a viable solution for the knowledge acquisition bottleneck problem in WSD.",
        "To the best of our knowledge, K'usd turns out to be the best system for all the tested tasks of Senseval-3, further improving the state of the art by0.4% to 8.2% for English and Italian, respectively.",
        "Finally, we have demonstrated the language independency",
        "Kernel",
        "Catalan",
        "English",
        "Italian",
        "Spanish",
        "KBoW",
        "81.3",
        "63.7",
        "43.3",
        "78.2",
        "Kd",
        "85.2",
        "65.5",
        "44.5",
        "84.4",
        "KColl",
        "84.2",
        "68.5",
        "54.0",
        "83.6",
        "KPoS",
        "79.6",
        "64.0",
        "44.4",
        "79.5",
        "Kwsd",
        "85.2",
        "69.7",
        "53.1",
        "84.2",
        "K'",
        "wsd",
        "89.0",
        "73.3",
        "61.3",
        "88.2",
        "Method",
        "Catalan",
        "English",
        "Italian",
        "Spanish",
        "Bigrams and trigrams",
        "82.6",
        "67.3",
        "51.0",
        "81.9",
        "Hard matching",
        "83.8",
        "67.7",
        "51.9",
        "82.9",
        "Soft matching (WordNet)",
        "-",
        "67.3",
        "51.3",
        "-",
        "Soft matching (Domain proximity)",
        "84.2",
        "68.5",
        "54.0",
        "83.6",
        "Percentage of training set",
        "Figure 1",
        "From left to right, top to bottom, learning curves for the Catalan, English, Italian, and Spanish lexical-sample tasks of Semeval-3.",
        "of our approach.",
        "The DMs have been acquired for different languages from different unlabeled corpora byadopting exactlythe same methodology, without requiring any external lexical resource or ad hoc rule.",
        "Encouraged bythe excellent results obtained on the lexical-sample tasks, we evaluated our approach on the all-words task, in which a verysmall amount of labeled training",
        "Comparative evaluation on the lexical sample tasks.",
        "and the percentage of sense-tagged examples required by K'wsd to achieve the same performance as Kwsd with full training.",
        "Columns report: the Most Frequent baseline, the inter-annotator agreement,the F1 of the best system at Senseval-3, the F1 of Kwsd,the F1 of K'wsd, DM+ (the improvement due to DM, i.e., K",
        "(/",
        "-",
        "J",
        "Kwsd_i_",
        "5 0.2",
        "0.4 0.6",
        "0.8 1",
        "Percentage of training set",
        "-",
        "-",
        "-",
        "//",
        "ll",
        "-",
        "11",
        "II",
        "I",
        "/(wsd_i_",
        "Kwsd' – – ",
        "0.2",
        "0.4 0.6 Percentage of training set",
        "0.8 1",
        "-",
        "Kwsd l",
        "Kwsd – -k- – ",
        "/",
        "Kwsd – i – ",
        "Kwsd – **■ – ",
        "Task",
        "MF",
        "Agreement",
        "BEST",
        "Kwsd",
        "K'",
        "wsd",
        "DM+",
        "% oftra",
        "Catalan",
        "66.3",
        "93.1",
        "85.2",
        "85.2",
        "89.0",
        "3.8",
        "46",
        "English",
        "55.2",
        "67.3",
        "72.9",
        "69.7",
        "73.3",
        "3.6",
        "54",
        "Italian",
        "18.0",
        "89.0",
        "53.1",
        "53.1",
        "61.3",
        "8.2",
        "51",
        "Spanish",
        "67.7",
        "85.3",
        "84.2",
        "84.2",
        "88.2",
        "4.0",
        "50",
        "The performance (F1) of the basic kernels and composite kernels on the English all-words task of Senseval-3.",
        "basic kernels composite kernels",
        "K bnc îzsem v v v v pbnc i/sem",
        "D KD KBoW KPoS KColl Kwsd Kwsd Kwsd",
        "data is typically available.",
        "We performed the evaluation on the English all-words task of Senseval-3 (Snyder and Palmer 2004).",
        "The test set was extracted from two Wall Street Journal articles and one text from the Brown Corpus.",
        "The test set consists of 945 words (2,041 word occurrences) to be disambiguated with WordNet 1.7.1 senses.",
        "The inter-annotator agreement rate in the preparation of the corpus was approximately72.5%.",
        "The most frequent (MF) baseline using the first WordNet sense heuristic obtained 60.9%.",
        "We have trained and tested the system exploiting the following resources: (1) Word-Net 1.7.1 as sense repository; (2) SemCor, considering onlythose words appearing in the Senseval-3 all-words data set – we extracted about 61,700 tagged examples that constitute the onlylabeled training set exploited bythe system; and (3) the BNC, from which we extracted the unlabeled training data.",
        "4.2.1 Results.",
        "We trained 734 word-expert classifiers on the SemCor corpus.",
        "The labeled examples for each classifier range from a minimum of one example to a maximum of 2,275 examples.",
        "We return a random sense for those words that have no training examples in SemCor.",
        "We have acquired two DMs, one from the BNC (i.e., KD;the same we used in the lexical-sample task) and one from SemCor (i.e., KDm), obtaining a slightlybetter performance with the latter.",
        "Table 6 shows the performance of the individual kernels KBoW, KD, KCon,and KPoS, and their composite kernels Kwsd, KD and KsDm.",
        "Since for 210 words in the test set we have no training examples, to better understand the results obtained, we performed an evaluation on the subset of the test set for which at least one training example is available in SemCor.",
        "Evaluating onlyon these words the performance increases from 65.2% to 70.0%, and the most frequent baseline becomes 65.7%.",
        "Tables 7 and 8 present a more detailed analysis that considers results grouped according to the amount of training available and the mean polysemy of the words in the test set, excluding from the data set the monosemous words.",
        "Table 7 shows the results (F1) of Kswm at different ranges of polysemy.",
        "Table 8 presents the results (F1) of Kwsid on those words that have a given number of training examples.",
        "This evaluation is limited to the best composite kernel 4.2.2 Discussion.",
        "We compared our approach with the three best systems that participated in the English all-words task of Senseval-3.",
        "The best system (Decadt et al.",
        "2004) has comparable performance (65.2) to ours; however, it uses a larger training set composed of 563,129 sense-tagged words.",
        "The training corpus was built bymerging",
        "9 Texts semanticallyannotated with WordNet 1.6 senses (created at Princeton University), and automaticallymapped to WordNet 1.7, WordNet 1.7.1, and WordNet 2.0.",
        "Downloadable from http://uuu.cs.unt.edu/~rada/dounloads.html.",
        "10 Note that for these words the WordNet first sense is not necessarilythe most frequent sense.",
        "The performance (F1) of at different ranges of polysemy.",
        "Most Frequent baseline (MF) is also reported.",
        "Range of polysemy",
        "The performance (F1) of on words with a given number of training examples.",
        "Most Frequent baseline (MF) and mean polysemy for each partition are also reported.",
        "Range of training examples",
        "SemCor, and English lexical-sample and all-words data sets taken from all the previous editions of Senseval.",
        "The system proposed by Mihalcea and Faruque (2004) scored second (64.6).",
        "The dimension of their training set is comparable to ours; however, theyalso use additional information drawn from WordNet to derive semantic generalizations using syntactic dependencies.",
        "Finally, the third system (Yuret 2004) obtained 64.1 using a larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMind Word Expert, Senseval-2, and Senseval-3 lexical-sample tasks).",
        "The small difference between the two domain models seems to indicate that a limited amount of unlabeled data is sufficient to improve the overall performance, and the use of unlabeled data taken from the training set helps to slightlyimprove the overall performance.",
        "However, the domain model can be acquired from a different corpus (e.g., the BNC) without significantlyaffecting the overall performance.",
        "Finally, the results reported in Tables 7 and 8 show that our approach is able to disambiguate with good accuracy(F1 = 76%) words with a number of training examples that ranges from 1 to 10, outperforming the most frequent baseline by3%.",
        "This is an interesting result given the extremelysmall number of training examples available.",
        "On the other hand, the more training is available for a given word, the more polysemous that word is.",
        "Nevertheless, the algorithm always outperforms the baseline and has a more significant difference for increasing values of the mean polysemy (from 3% to 16%).",
        "These results, together with the ones obtained in the lexical sample tasks, show that the domain kernel is able to boost the overall performance when little training data are available, as well as with enough training data.",
        "The benefit is even more pronounced for the latter case, even though the disambiguation task is more complex due to the high polysemy of highly frequent words."
      ]
    },
    {
      "heading": "5.. Conclusions",
      "text": [
        "This article summarizes the results of a word expert semi-supervised algorithm for WSD based on a combination of kernel functions.",
        "First, we evaluated our methodology on four lexical-sample tasks of Senseval-3, significantlyimproving the state of the art for all of them.",
        "In particular, we demonstrated that using external knowledge inside a supervised framework is a viable methodologyto reduce the amount of training data required for learning.",
        "In our approach, the external knowledge is represented bymeans of domain models automaticallyacquired from corpora in a totallyunsupervised way.",
        "Then, we applied the method so defined to the English all-words task of Senseval-3, achieving state-of-the-art performance while requiring less labeled training data compared to the other systems we have found in the literature.",
        "Some slight improvement maybe possible byexploiting syntactic information produced bya parser.",
        "In the framework of kernel methods, this expansion can be done by adding a tree kernel (i.e., a kernel function that evaluates the similarityamong parse trees) to our composite kernel.",
        "However, the performance achieved is close to the upper bound, if we consider the inter-annotator agreement as an indication of the upper-bound performance.",
        "Finally, we think that our semi-supervised approach is at the moment an effective solution for developing a sense-tagging system.",
        "Indeed, we tested the system on the English lexical-sample task of SemEval 2007, still obtaining state-of-the-art performance (Pradhan et al.",
        "2007).",
        "Therefore, we plan to make available an optimized version of our system, and to exploit it for ontology learning, textual entailment, and information retrieval."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Claudio Giuliano was supported bythe X-Media project (uuu.x-media-project.org), sponsored bythe European Commission as part of the Information SocietyTechnologies (IST) programme under EC grant IST-FP6-026978.",
        "Alfio Massimiliano Gliozzo and Carlo Strapparava were supported bythe ONTOTEXT project, sponsored bythe Autonomous Province of Trento under the FUP-2004 research program."
      ]
    }
  ]
}
