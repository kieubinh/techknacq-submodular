{
  "info": {
    "authors": [
      "Percy Liang",
      "Dan Klein"
    ],
    "book": "HLT-NAACL",
    "id": "acl-N09-1069",
    "title": "Online EM for Unsupervised Models",
    "url": "https://aclweb.org/anthology/N09-1069",
    "year": 2009
  },
  "references": [
    "acl-D07-1031",
    "acl-I08-4003",
    "acl-J01-3002",
    "acl-J03-1002",
    "acl-P05-1012",
    "acl-P06-1085",
    "acl-P07-1049",
    "acl-P07-1094",
    "acl-P08-1046",
    "acl-P08-1100",
    "acl-P08-1109",
    "acl-W02-1001"
  ],
  "sections": [
    {
      "text": [
        "Percy Liang Dan Klein",
        "The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence.",
        "In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM.",
        "We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In unsupervised NLP tasks such as tagging, parsing, and alignment, one wishes to induce latent linguistic structures from raw text.",
        "Probabilistic modeling has emerged as a dominant paradigm for these problems, and the EM algorithm has been a driving force for learning models in a simple and intuitive manner.",
        "However, on some tasks, EM can converge slowly.",
        "For instance, on unsupervised part-of-speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007).",
        "The slowness of EM is mainly due to its batch nature: Parameters are updated only once after each pass through the data.",
        "When parameter estimates are still rough or if there is high redundancy in the data, computing statistics on the entire dataset just to make one update can be wasteful.",
        "In this paper, we investigate two flavors of online EM – incremental EM (Neal and Hinton, 1998) and stepwise EM (Sato and Ishii, 2000; Cappe and Moulines, 2009), both of which involve updating parameters after each example or after a mini-batch (subset) of examples.",
        "Online algorithms have the potential to speed up learning by making updates more frequently.",
        "However, these updates can be seen as noisy approximations to the full batch update, and this noise can in fact impede learning.",
        "This tradeoff between speed and stability is familiar to online algorithms for convex supervised learning problems – e.g., Perceptron, MIRA, stochastic gradient, etc.",
        "Unsupervised learning raises two additional issues: (1) Since the EM objective is non-convex, we often get convergence to different local optima of varying quality; and (2) we evaluate on accuracy metrics which are at best loosely correlated with the EM likelihood objective (Liang and Klein, 2008).",
        "We will see that these issues can lead to surprising results.",
        "In Section 4, we present a thorough investigation of online EM, mostly focusing on stepwise EM since it dominates incremental EM.",
        "For stepwise EM, we find that choosing a good stepsize and mini-batch size is important but can fortunately be done adequately without supervision.",
        "With a proper choice, stepwise EM reaches the same performance as batch EM, but much more quickly.",
        "Moreover, it can even surpass the performance of batch EM.",
        "Our results are particularly striking on part-of-speech tagging: Batch EM crawls to an accuracy of 57.3% after 100 iterations, whereas stepwise EM shoots up to 65.4% after just two iterations."
      ]
    },
    {
      "heading": "2. Tasks, models, and datasets",
      "text": [
        "In this paper, we focus on unsupervised induction via probabilistic modeling.",
        "In particular, we define a probabilistic model p(x, z; 6) of the input (e.g., a sentence) and hidden output z (e.g., a parse tree) with parameters 6 (e.g., rule probabilities).",
        "Given a set of unlabeled examples x(1),..., x(n), the standard training objective is to maximize the marginal log-likelihood of these examples:",
        "A trained model 6 is then evaluated on the accuracy of its predictions: argmaxz p(z | x(i); 6) against the true output z(i); the exact evaluation metric depends on the task.",
        "What makes unsupervised induction hard at best and ill-defined at worst is that the training objective (1) does not depend on the true outputs at all.",
        "We ran experiments on four tasks described below.",
        "Two of these tasks – part-of-speech tagging and document classification – are \"clustering\" tasks.",
        "For these, the output z consists of labels; for evaluation, we map each predicted label to the true label that maximizes accuracy.",
        "The other two tasks – segmentation and alignment – only involve unla-beled combinatorial structures, which can be evaluated directly.",
        "Part-of-speech tagging For each sentence x = (x1,..., xi), represented as a sequence of words, we wish to predict the corresponding sequence of part-of-speech (POS) tags z = ..., We used a simple bigram HMM trained on the Wall Street Journal (WSJ) portion of the Penn Treebank (49208 sentences, 45 tags).",
        "No tagging dictionary was used.",
        "We evaluated using per-position accuracy.",
        "Document classification For each document x = (x1;..., xi) consisting of I words, we wish to predict the document class z e {1,..., 20}.",
        "Each document x is modeled as a bag of words drawn independently given the class z.",
        "We used the 20 Newsgroups dataset (18828 documents, 20 classes).",
        "We evaluated on class accuracy.",
        "Word segmentation For each sentence x = (x1;..., xi), represented as a sequence of English phonemes or Chinese characters without spaces separating the words, we would like to predict a segmentation of the sequence into words z = (z1,..., z|z|), where each segment (word) z is a contiguous subsequence of 1,..., £ Since the naive unigram model has a degenerate maximum likelihood solution that makes each sentence a separate word, we incorporate a penalty for longer segments:",
        "p(x, z; 6) oc ELI!",
        "1 P(xzk; 6)e-|zfc|', where 0 > 1 determines the strength of the penalty.",
        "For English, we used 0 = 1.6; Chinese, 0 = 2.5.",
        "To speed up inference, we restricted the maximum segment length to 10 for English and 5 for Chinese.",
        "We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al.",
        "(2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences).",
        "We evaluated using F1 on word tokens.",
        "To the best of our knowledge, our penalized unigram model is new and actually beats the more complicated model of Johnson (2008) 83.5% to 78%, which had been the best published result on this task.",
        "Word alignment For each pair of translated sentences x = (e1,..., ene,_/!,..., fnf), we wish to predict the word alignments z e {0,1}nenf.",
        "We trained two IBM model 1s using agreement-based learning (Liang et al., 2008).",
        "We used the first 30K sentence pairs of the English-French Hansards data from the NAACL 2003 Shared Task, 447+37 of which were hand-aligned (Och and Ney, 2003).",
        "We evaluated using the standard alignment error rate (AER)."
      ]
    },
    {
      "heading": "3. EM algorithms",
      "text": [
        "Given a probabilistic model p(x, z; 6) and unlabeled examples x(1),..., x(n), recall we would like to maximize the marginal likelihood of the data (1).",
        "Let 0(x, z) denote a mapping from a fully-labeled example (x, z) to a vector of sufficient statistics (counts in the case of multinomials) for the model.",
        "For example, one component of this vector for HMMs would be the number of times state 7 emits the word \"house\" in sentence x with state sequence z.",
        "Given a vector of sufficient statistics /t, let 6(/t) denote the maximum likelihood estimate.",
        "In our case, 6(/t) are simply probabilities obtained by normalizing each block of counts.",
        "This closed-form",
        "Batch EM",
        "M – M [replace old with new] solution is one of the features that makes EM (both batch and online) attractive.",
        "In the (batch) EM algorithm, we alternate between the E-step and the M-step.",
        "In the E-step, we compute the expected sufficient statistics // across all the examples based on the posterior over z under the current parameters 0(m).",
        "In all our models, this step can be done via a dynamic program (for example, forward-backward for POS tagging).",
        "In the M-step, we use these sufficient statistics // to re-estimate the parameters.",
        "Since the M-step is trivial, we represent it implicitly by #(•) in order to concentrate on the computation of the sufficient statistics.",
        "This focus will be important for online EM, so writing batch EM in this way accentuates the parallel between batch and online.",
        "To obtain an online EM algorithm, we store a single set of sufficient statistics / and update it after processing each example.",
        "For the i-th example, we compute sufficient statistics si.",
        "There are two main variants of online EM algorithms which differ in exactly how the new si is incorporated into",
        "The first is incremental EM (iEM) (Neal and Hinton, 1998), in which we not only keep track of / but also the sufficient statistics si,..., sn for each example (/ = YIL1 si).",
        "When we process example i, we subtract out the old si and add the new si.",
        "Sato and Ishii (2000) developed another variant, later generalized by Cappe and Moulines (2009), which we call stepwise EM (sEM).",
        "In sEM, we interpolate between / and si based on a stepsize nk (k is the number of updates made to / so far).",
        "The two algorithms are motivated in different ways.",
        "Recall that the log-likelihood can be lower",
        "Incremental EM (iEM) si – initialization for i = 1,..., n",
        "Stepwise EM (sEM)",
        "M – initialization; k = 0 where H(qi) is the entropy of the distribution qi(z | x(i)).",
        "Batch EM alternates between optimizing L with respect to q1,..., qn in the E-step (represented implicitly via sufficient statistics //) and with respect to 0 in the M-step.",
        "Incremental EM alternates between optimizing with respect to a single qi and 0.",
        "Stepwise EM is motivated from the stochastic approximation literature, where we think of approximating the update // in batch EM with a single sample si.",
        "Since one sample is a bad approximation, we interpolate between si and the current Thus, sEM can be seen as stochastic gradient in the space of sufficient statistics.",
        "Stepsize reduction power a Stepwise EM leaves open the choice of the stepsize r?fc.",
        "Standard results from the stochastic approximation literature state that Y fc=o nfc = and Y fc=o nl < are sufficient to guarantee convergence to a local optimum.",
        "In particular, if we take r?fc = (k + 2)-a, then any 0.5 < a < 1 is valid.",
        "The smaller the a, the larger the updates, and the more quickly we forget (decay) our old sufficient statistics.",
        "This can lead to swift progress but also generates instability.",
        "Mini-batch size m We can add some stability to sEM by updating on multiple examples at once instead of just one.",
        "In particular, partition the n examples into mini-batches of size m and run sEM, treating each mini-batch as a single example.",
        "Formally, for each i = 0, m, 2m, 3m,..., first compute the sufficient statistics si+1,..., si+m on x(i+1),..., x(i+m) and then update / using si+1 + • • • + si+m.",
        "The larger the m, the less frequent the updates, but the more stable they are.",
        "In this way, mini-batches interpolate between a pure online (m = 1) and a pure batch (m = n) algorithm.",
        "Fast implementation Due to sparsity in NLP, the sufficient statistics of an example si are nonzero for a small fraction of its components.",
        "For iEM, the time required to update / with si depends only on the number of nonzero components of si.",
        "However, the sEM update is / – (1 – nk)m+nksi, and anai've implementation would take time proportional to the total number of components.",
        "The key to a more efficient solution is to note that 0(m) is invariant to scaling of m. Therefore, we can store S = t-j – 7^ – r instead of / and make the following sparse update:",
        "that 0(m) = 0(S).",
        "For both iEM and sEM, we also need to efficiently compute 0(m).",
        "We can do this by maintaining the normalizer for each multinomial block (sum of the components in the block).",
        "This extra maintenance only doubles the number of updates we have to make but allows us to fetch any component of 0(m) in constant time by dividing out the normalizer.",
        "Incremental EM increases L monotonically after each update by virtue of doing coordinate-wise ascent and thus is guaranteed to converge to a local optimum of both L and I (Neal and Hinton, 1998).",
        "However, I is not guaranteed to increase after each update.",
        "Stepwise EM might not increase either L or I after each update, but it is guaranteed to converge to a local optimum of I given suitable conditions on the stepsize discussed earlier.",
        "Incremental and stepwise EM actually coincide under the following setting (Cappe and Moulines, 2009): If we set (a, m) = (1,1) for sEM and initialize all si = 0 for iEM, then both algorithms make the same updates on the first pass through the data.",
        "They diverge thereafter as iEM subtracts out old sis, while sEM does not even remember them.",
        "One weakness of iEM is that its memory requirements grow linearly with the number of examples due to storing s1 , .",
        ".",
        ".",
        ", sn.",
        "For large datasets, these sis might not even fit in memory, and resorting to physical disk would be very slow.",
        "In contrast, the memory usage of sEM does not depend on n.",
        "The relationship between iEM and sEM (with m = 1) is analogous to the one between exponentiated gradient (Collins et al., 2008) and stochastic gradient for supervised learning of log-linear models.",
        "The former maintains the sufficient statistics of each example and subtracts out old ones whereas the latter does not.",
        "In the supervised case, the added stability of exponentiated gradient tends to yield better performance.",
        "For the unsupervised case, we will see empirically that remembering the old sufficient statistics offers no benefit, and much better performance can be obtained by properly setting (a, m) for sEM (Section 4)."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We now present our empirical results for batch EM and online EM (iEM and sEM) on the four tasks described in Section 2: part-of-speech tagging, document classification, word segmentation (English and Chinese), and word alignment.",
        "We used the following protocol for all experiments: We initialized the parameters to a neutral setting plus noise to break symmetries.",
        "Training was performed for 20 iterations.",
        "No parameter smoothing was used.",
        "All runs used a fixed random seed for initializing the parameters and permuting the examples at the beginning of each iteration.",
        "We report two performance metrics: log-likelihood normalized by the number of examples and the task-specific accuracy metric (see Section 2).",
        "All numbers are taken from the final iteration.",
        "Stepwise EM (sEM) requires setting two optimization parameters: the stepsize reduction power a and the mini-batch size m (see Section 3.2).",
        "As Section 4.3 will show, these two parameters can have a large impact on performance.",
        "As a default rule of thumb, we chose (a,m) e {0.5,0.6,0.7,0.8, 0.9,1.0} x {1, 3,10, 30,100, 300,1K, 3K, 10K} to maximize log-likelihood; let sEM^ denote stepwise EM with this setting.",
        "Note that this setting requires no labeled data.",
        "We will also consider fixing (a, m) = (1,1) (sEMj) and choosing (a, m) to maximize accuracy (sEMa).",
        "In the results to follow, we first demonstrate that online EM is faster (Section 4.1) and sometimes leads to higher accuracies (Section 4.2).",
        "Next, we explore the effect of the optimization parameters (a, m) (Section 4.3), briefly revisiting the connection between incremental and stepwise EM.",
        "Finally, we show the stability of our results under different random seeds (Section 4.4).",
        "One of the principal motivations for online EM is speed, and indeed we found this motivation to be empirically well-justified.",
        "Figure 1 shows that, across all five datasets, sEM^ converges to a solution with at least comparable log-likelihood and accuracy with respect to batch EM, but sEM^ does it anywhere from about 2 (word alignment) to 10 (POS tagging) times faster.",
        "This supports our intuition that more frequent updates lead to faster convergence.",
        "At the same time, note that the other two online EM variants in Figure 1 (iEM and sEMj) are prone to catastrophic failure.",
        "See Section 4.3 for further discussion on this issue.",
        "It is fortunate but perhaps not surprising that stepwise EM is faster than batch EM.",
        "But Figure 1 also shows that, somewhat surprisingly, sEM^ can actually converge to a solution with higher accuracy, in particular on POS tagging and document classification.",
        "To further explore the accuracy-increasing potential of sEM, consider choosing (a, m) to maximize accuracy (sEM„).",
        "Unlike sEM^, sEMa does require labeled data.",
        "In practice, ( a, m) can be tuned",
        "Table 1: Accuracy of batch EM and stepwise EM, where the optimization parameters (a, m) are tuned to either maximize log-likelihood (sEM^) or accuracy (sEMa).",
        "With an appropriate setting of (a, m), stepwise EM outperforms batch EM significantly on POS tagging and document classification.",
        "on a small labeled set along with any model hyperparameters.",
        "Table 1 shows that sEMa improves the accuracy compared to batch EM even more than sEM^.",
        "The result for POS is most vivid: After one iteration of batch EM, the accuracy is only at 24.0% whereas sEMa is already at 54.5%, and after two iterations, at 65.4%.",
        "Not only is this orders of magnitude faster than batch EM, batch EM only reaches 57.3% after 100 iterations.",
        "We get a similarly striking result for document classification, but the results for word segmentation and word alignment are more modest.",
        "A full understanding of this phenomenon is left as an open problem, but we will comment on one difference between the tasks where sEM improves accuracy and the tasks where it doesn't.",
        "The former are \"clustering\" tasks (POS tagging and document classification), while the latter are \"structural\" tasks (word segmentation and word alignment).",
        "Learning of clustering models centers around probabilities over words given a latent cluster label, whereas in structural models, there are no cluster labels, and it is the combinatorial structure (the segmentations and alignments) that drives the learning.",
        "Likelihood versus accuracy From Figure 1, we see that stepwise EM (sEM^) can outperform batch EM in both likelihood and accuracy.",
        "This suggests that stepwise EM is better at avoiding local minima, perhaps leveraging its stochasticity to its advantage.",
        "However, on POS tagging, tuning sEM to maximize accuracy (sEM„) results in a slower increase in likelihood: compare sEM„ in Figure 2 with sEM^ in Figure 1(a).",
        "This shouldn't surprise us too much given that likelihood and accuracy are only loosely",
        "EM",
        "sEM£",
        "sEMa",
        "ae",
        "me",
        "aa",
        "ma",
        "pos",
        "57.3",
        "59.6",
        "66.7",
        "0.7",
        "3",
        "0.5",
        "3",
        "doc",
        "39.1",
        "47.8",
        "49.9",
        "0.8",
        "1K",
        "0.5",
        "3K",
        "seg(en)",
        "80.5",
        "80.7",
        "83.5",
        "0.7",
        "1K",
        "1.0",
        "100",
        "seg(ch)",
        "78.2",
        "77.2",
        "78.1",
        "0.6",
        "10K",
        "1.0",
        "10K",
        "align",
        "78.8",
        "78.9",
        "78.9",
        "0.7",
        "10K",
        "0.7",
        "10K",
        "sEM,sEM^",
        "iterations",
        "(a) POS tagging (c) Word segmentation (English) (b) Document classification (d) Word segmentation (Chinese) (e) Word alignment (f) Results after convergence",
        "Figure 1: Accuracy and log-likelihood plots for batch EM, incremental EM, and stepwise EM across all five datasets.",
        "sEM^ outperforms batch EM in terms of convergence speed and even accuracy and likelihood; iEM and sEMj fail in some cases.",
        "We did not run iEM on POS tagging due to memory limitations; we expect the performance would be similar to sEMj, which is not very encouraging (Section 4.3).",
        "correlated (Liang and Klein, 2008).",
        "But it does suggest that stepwise EM is injecting a bias that favors accuracy over likelihood – a bias not at all reflected in the training objective.",
        "We can create a hybrid (sEMa+EM) that combines the strengths of both sEMa and EM: First run sEMa for 5 iterations, which quickly takes us to a part of the parameter space yielding good accuracies; then run EM, which quickly improves the likelihood.",
        "Fortunately, accuracy does not degrade as likelihood increases (Figure 2).",
        "Recall that stepwise EM requires setting two optimization parameters: the stepsize reduction power a and the mini-batch size m. We now explore the effect of (a, m) on likelihood and accuracy.",
        "As mentioned in Section 3.2, larger mini-batches (increasing m) stabilize parameter updates, while larger stepsizes (decreasing a) provide swifter",
        "a – e – e",
        "3 – Q – E",
        "z",
        "77",
        "_",
        "1 * ?",
        "1 * *",
        "4",
        "S – I*",
        "7",
        "1",
        "EM",
        "7",
        "sEM,",
        "1",
        "h- sEM^",
        "î=é=ê",
        "/",
        "T",
        "A",
        "/",
        "7",
        " – ¥",
        "EM",
        "7",
        "sEM,",
        "7",
        " – sEM^",
        "_",
        "4i",
        "- EM",
        "sEM,",
        "- sEM^",
        "accuracy",
        "log-likelihood",
        "EM",
        "sEM£",
        "EM",
        "sEM£",
        "pos",
        "57.3",
        "59.6",
        "-6.03",
        "-6.08",
        "doc",
        "39.1",
        "47.8",
        "-7.96",
        "-7.88",
        "seg(en)",
        "80.5",
        "80.7",
        "-4.11",
        "-4.11",
        "seg(ch)",
        "78.2",
        "77.2",
        "-7.27",
        "-7.28",
        "align",
        "78.8",
        "78.9",
        "-5.05",
        "-5.12",
        "DQO accuracy",
        "DOC log-likelihood",
        "Figure 3: Effect of optimization parameters (stepsize reduction power a and mini-batch size m) on accuracy and likelihood.",
        "Numerical results are shown for document classification.",
        "In the interest of space, the results for each task are compressed into two gray scale images, one for accuracy (top) and one for log-likelihood (bottom), where darker shades represent larger values.",
        "Bold (red) numbers denote the best a for a given m.",
        "iterations iterations",
        "Figure 2: sEMa quickly obtains higher accuracy than batch EM but suffers from a slower increase in likelihood.",
        "The hybrid sEMa+EM (5 iterations of EMa followed by batch EM) increases both accuracy and likelihood sharply.",
        "progress.",
        "Remember that since we are dealing with a nonconvex objective, the choice of stepsize not only influences how fast we converge, but also the quality of the solution that we converge to.",
        "Figure 3 shows the interaction between a and m in terms of likelihood and accuracy.",
        "In general, the best (a, m) depends on the task and dataset.",
        "For example, for document classification, larger m is critical for good performance; for POS tagging, it is better to use smaller values of a and m.",
        "Fortunately, there is a range of permissible settings (corresponding to the dark regions in Figure 3) that lead to reasonable performance.",
        "Furthermore, the settings that perform well on likelihood generally correspond to ones that perform well on accuracy, which justifies using sEM^.",
        "A final observation is that as we use larger mini-batches (larger m), decreasing the stepsize more gradually (smaller a) leads to better performance.",
        "Intuitively, updates become more reliable with larger m, so we can afford to trust them more and incorporate them more aggressively.",
        "Stepwise versus incremental EM In Section 3.2, we mentioned that incremental EM can be made equivalent to stepwise EM with a = 1 and m = 1 (sEMj).",
        "Figure 1 provides the empirical support: iEM and sEMj have very similar training curves.",
        "Therefore, keeping around the old sufficient statistics does not provide any advantage and still requires a substantial storage cost.",
        "As mentioned before, setting (a, m) properly is crucial.",
        "While we could simulate mini-batches with iEM by updating multiple coordinates simultaneously, iEM is not capable of exploiting the behavior of a < 1.",
        "All our results thus far represent single runs with a fixed random seed.",
        "We now investigate the impact of randomness on our results.",
        "Recall that we use randomness for two purposes: (1) initializing the parameters (affects both batch EM and online EM),",
        "1",
        "3",
        "10",
        "30",
        "100",
        "300",
        "1K",
        "3K",
        "10K",
        "0.5",
        "5.4",
        "5.4",
        "5.5",
        "5.6",
        "6.0",
        "25.7",
        "48.8",
        "49.9",
        "44.6",
        "0.6",
        "5.4",
        "5.4",
        "5.6",
        "5.6",
        "22.3",
        "36.1",
        "48.7",
        "49.3",
        "44.2",
        "0.7",
        "5.5",
        "5.5",
        "5.6",
        "11.1",
        "39.9",
        "43.3",
        "48.1",
        "49.0",
        "43.5",
        "0.8",
        "5.6",
        "5.6",
        "6.0",
        "21.7",
        "47.3",
        "45.0",
        "47.8",
        "49.5",
        "42.8",
        "0.9",
        "5.8",
        "6.0",
        "13.4",
        "32.4",
        "48.7",
        "48.4",
        "46.4",
        "49.4",
        "42.4",
        "1.0",
        "6.2",
        "11.8",
        "19.6",
        "35.2",
        "47.6",
        "49.5",
        "47.5",
        "49.3",
        "41.7",
        "■■",
        "■1",
        "IB ■■■■",
        "■ BBB",
        "■■",
        "■■■■■■I",
        "■■■1",
        "■ ■■",
        "■",
        "■■",
        "■■■1",
        "m mm",
        "IIBI",
        "■■■■■■■",
        "■■■■",
        "IHHIHHI",
        "■■",
        "□",
        "m",
        "■BBB",
        "■",
        "■■■■■■■",
        "■■■■■1",
        "IB ■■■■■",
        "■ BBB",
        "■■■■",
        "IB llll",
        "■■",
        "■■■■■■■",
        "■■■■■■I",
        "IB BBBBB",
        "IBBB",
        "■■■■■■■",
        "■■■■■■I",
        "IB BBBBB",
        "■■",
        "■■■■■■■",
        "■■■■■1",
        "IB ■■",
        "IIIB",
        "■■■■■■■",
        "■■■■■■1",
        "IB BBBBB",
        "BBBB",
        "1",
        "3",
        "10",
        "30",
        "100",
        "300",
        "1K",
        "3K",
        "10K",
        "0.5",
        "-8.875",
        "-8.71",
        "-8.61",
        "-8.555",
        "-8.505",
        "-8.172",
        "-7.92",
        "-7.906",
        "-7.916",
        "0.6",
        "-8.604",
        "-8.575",
        "-8.54",
        "-8.524",
        "-8.235",
        "-8.041",
        "-7.898",
        "-7.901",
        "-7.916",
        "0.7",
        "-8.541",
        "-8.533",
        "-8.531",
        "-8.354",
        "-8.023",
        "-7.943",
        "-7.886",
        "-7.896",
        "-7.918",
        "0.8",
        "-8.519",
        "-8.506",
        "-8.493",
        "-8.228",
        "-7.933",
        "-7.896",
        "-7.883",
        "-7.89",
        "-7.922",
        "0.9",
        "-8.505",
        "-8.486",
        "-8.283",
        "-8.106",
        "-7.91",
        "-7.889",
        "-7.889",
        "-7.891",
        "-7.927",
        "1.0",
        "-8.471",
        "-8.319",
        "-8.204",
        "-8.052",
        "-7.919",
        "-7.889",
        "-7.892",
        "-7.896",
        "-7.937",
        "Table 2: Mean and standard deviation over different random seeds.",
        "For EM and sEM, the first number after ± is the standard deviation due to different initializations of the parameters.",
        "For sEM, the second number is the standard deviation due to different permutations of the examples.",
        "Standard deviation for log-likelihoods are all < 0.01 and therefore left out due to lack of space.",
        "and (2) permuting the examples at the beginning of each iteration (affects only online EM).",
        "To separate these two purposes, we used two different seeds, Si G {1, 2,3,4, 5} and Sp e {1, 2, 3, 4, 5} for initializing and permuting, respectively.",
        "Let X be a random variable denoting either log-likelihood or accuracy.",
        "We define the variance due to initialization as var(E(X | Si)) (E averages over Sp for each fixed Si) and the variance due to permutation as E(var(X | Si)) (E averages over Si).",
        "These two variances provide an additive decomposition of the total variance: var(X) = var(E(X | Si))+ E(var(X | Si)).",
        "Table 2 summarizes the results across the 5 trials for EM and 25 for sEM^.",
        "Since we used a very small amount of noise to initialize the parameters, the variance due to initialization is systematically smaller than the variance due to permutation.",
        "sEM^ is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples.",
        "Overall, the accuracy of sEM^ is more variable than that of EM, but not by a large amount."
      ]
    },
    {
      "heading": "5. Discussion and related work",
      "text": [
        "As datasets increase in size, the demand for online algorithms has grown in recent years.",
        "One sees this clear trend in the supervised NLP literature – examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few.",
        "Empirically, online methods are often faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008).",
        "However, in the unsupervised NLP literature, online methods are rarely seen, and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008).",
        "Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima.",
        "Stepwise EM provides finer control via its optimization parameters and has proven quite successful.",
        "One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models – for example, Goldwater et al.",
        "(2006) and Goldwater and Griffiths (2007).",
        "These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM.",
        "In contrast, stepwise EM does not require this storage and operates more in the spirit of a truly online algorithm.",
        "Besides speed, online algorithms are of interest for two additional reasons.",
        "First, in some applications, we receive examples sequentially and would like to estimate a model in real-time, e.g., in the clustering of news articles.",
        "Second, since humans learn sequentially, studying online EM might suggest new connections to cognitive mechanisms."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "We have explored online EM on four tasks and demonstrated how to use stepwise EM to overcome the dangers of stochasticity and reap the benefits of frequent updates and fast learning.",
        "We also discovered that stepwise EM can actually improve accuracy, a phenomenon worthy of further investigation.",
        "This paper makes some progress on elucidating the properties of online EM.",
        "With this increased understanding, online EM, like its batch cousin, could become a mainstay for unsupervised learning.",
        "accuracy",
        "log-likelihood",
        "EM",
        "sEMe",
        "EM sEM£",
        "pos doc",
        "seg(en)",
        "56.2 ±1.36",
        "41.2 ±1.97",
        "80.5 ±0.0",
        "58.8 ±0.73,1.41 51.4 ±0.97, 2.82 81.0 ±0.0, 0.42",
        "-6.01 -6.09 -7.93 -7.88 -4.1 -4.1 -7.26 -7.27 -5.04 -5.11",
        "seg(ch)",
        "align",
        "78.2 ±0.0 79.0 ±0.14",
        "77.2 ±0.0,0.04",
        "78.8 ±0.14, 0.25"
      ]
    }
  ]
}
