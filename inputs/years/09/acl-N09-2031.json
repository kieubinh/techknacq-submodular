{
  "info": {
    "authors": [
      "Kun Yu",
      "Jun'ichi Tsujii"
    ],
    "book": "HLT-NAACL, Companion Volume: Short Papers",
    "id": "acl-N09-2031",
    "title": "Extracting Bilingual Dictionary from Comparable Corpora with Dependency Heterogeneity",
    "url": "https://aclweb.org/anthology/N09-2031",
    "year": 2009
  },
  "references": [
    "acl-C02-2020",
    "acl-I08-1013",
    "acl-J03-1002",
    "acl-P07-1084",
    "acl-P07-2055",
    "acl-W95-0114"
  ],
  "sections": [
    {
      "text": [
        "Kun Yu Junichi Tsujii",
        "This paper proposes an approach for bilingual dictionary extraction from comparable corpora.",
        "The proposed approach is based on the observation that a word and its translation share similar dependency relations.",
        "Experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional context-based approach that uses bag-of-words around translation candidates."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Bilingual dictionary plays an important role in many natural language processing tasks.",
        "For example, machine translation uses bilingual dictionary to reinforce word and phrase alignment (Och and Ney, 2003), cross-language information retrieval uses bilingual dictionary for query translation (Grefenstette, 1998).",
        "The direct way of bilingual dictionary acquisition is aligning translation candidates using parallel corpora (Wu, 1994).",
        "But for some languages, collecting parallel corpora is not easy.",
        "Therefore, many researchers paid attention to bilingual dictionary extraction from comparable corpora (Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and Morin, 2008; Robitaille et al., 2006; Morin et al., 2007; Otero, 2008), in which texts are not exact translation of each other but share common features.",
        "Context-based approach, which is based on the observation that a term and its translation appear in similar lexical contexts (Daille and Morin, 2008), is the most popular approach for extracting bilingual dictionary from comparable corpora and has shown its effectiveness in terminology extraction (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007).",
        "But it only concerns about the lexical context around translation candidates in a restricted window.",
        "Besides, in comparable corpora, some words may appear in similar context even if they are not translation of each other.",
        "For example, using a Chinese-English comparable corpus from Wikipedia and following the definition in (Fung, 1995), we get context heterogeneity vector of three words (see Table 1).",
        "The Euclidean distance between the vector of 'tri^^economics)' and 'economics' is 0.084.",
        "But the Euclidean distance between the vector of 'tSifl-^' and 'medicine' is 0.075.",
        "In such case, the incorrect dictionary entry ' tS^^/medicine' will be extracted by context-based approach.",
        "To solve this problem, we investigate a comparable corpora from Wikipedia and find the following phenomenon: if we preprocessed the corpora with a dependency syntactic analyzer, a word in source language shares similar head and modifiers with its translation in target language, no matter whether they occur in similar context or not.",
        "We call this phenomenon as dependency heterogeneity.",
        "Based on this observation, we propose an approach to extract bilingual dictionary from comparable corpora.",
        "Not like only using bag-of-words around translation candidates in context-based approach, the proposed approach utilizes the syntactic analysis of comparable corpora to recognize the meaning of translation candidates.",
        "Besides, the lexical information used in the proposed approach does not restrict in a small window, but comes from the entire sentence.",
        "We did experiments with 250 randomly selected translation pairs.",
        "Results show that compared with the approach based on context heterogeneity, the proposed approach improves the accuracy of dictionary extraction significantly."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "In previous work about dictionary extraction from comparable corpora, using context similarity is the most popular one.",
        "At first, Fung (1995) utilized context heterogeneity for bilingual dictionary extraction.",
        "Our proposed approach borrows Fung's idea but extends context heterogeneity to dependency heterogeneity, in order to utilize rich syntactic information other than bag-of-words.",
        "After that, researchers extended context heterogeneity vector to context vector with the aid of an existing bilingual dictionary (Fung, 2000; Chiao and Zweigenbaum, and Morin, 2008).",
        "In these works, dictionary extraction is fulfilled by comparing the similarity between the context vectors of words in target language and the context vectors of words in source language using an external dictionary.",
        "The main difference between these works and our approach is still our usage of syntactic dependency other than bag-of-words.",
        "In addition, except for a morphological analyzer and a dependency parser, our approach does not need other external resources, such as the external dictionary.",
        "Because of the well-developed morphological and syntactic analysis research in recent years, the requirement of analyzers will not bring too much burden to the proposed approach.",
        "Word",
        "Context Heterogeneity Vector",
        "15^^ (economics)",
        "(0.185, 0.006)",
        "economics",
        "(0.101, 0.013)",
        "medicine",
        "(0.113,0.028)",
        "Besides of using window-based contexts, there were also some works utilizing syntactic information for bilingual dictionary extraction.",
        "Otero (2007) extracted lexico-syntactic templates from parallel corpora first, and then used them as seeds to calculate similarity between translation candidates.",
        "Otero (2008) defined syntactic rules to get lexico-syntactic contexts of words, and then used an external bilingual dictionary to fulfill similarity calculation between the lexico-syntactic context vectors of translation candidates.",
        "Our approach differs from these works in two ways: (1) both the above works defined syntactic rules or templates by hand to get syntactic information.",
        "Our approach uses data-driven syntactic analyzers for acquiring dependency relations automatically.",
        "Therefore, it is easier to adapt our approach to other language pairs.",
        "(2) the types of dependencies used for similarity calculation in our approach are different from Otero's work.",
        "Otero (2007; 2008) only considered about the modification dependency among nouns, prepositions and verbs, such as the adjective modifier of nouns and the object of verbs.",
        "But our approach not only uses modifiers of translation candidates, but also considers about their heads."
      ]
    },
    {
      "heading": "3. Dependency Heterogeneity of Words in Comparable Corpora",
      "text": [
        "Dependency heterogeneity means a word and its translation share similar modifiers and head in comparable corpora.",
        "Namely, the modifiers and head of unrelated words are different even if they occur in similar context.",
        "For example, Table 2 collects the most frequently used 10 modifiers of the words listed in Table 1.",
        "It shows there are 3 similar modifiers (italic words) between 'S^^(economics)' and 'economics'.",
        "But there is no similar word between the modifiers of 'S5£?F^' and that of 'medicine'.",
        "Table 3 lists the most frequently used 10 heads (when a candidate word acts as subject) of the three words.",
        "If excluding copula, 'S5£?F^' and 'economics' share one similar head (italic words).",
        "But 'SSifl-^' and 'medicine' shares no similar head.",
        "4 Bilingual Dictionary Extraction with Dependency Heterogeneity",
        "Based on the observation of dependency heterogeneity in comparable corpora, we propose an approach to extract bilingual dictionary using dependency heterogeneity similarity.",
        "Before calculating dependency heterogeneity similarity, we need to preprocess the comparable corpora.",
        "In this work, we focus on Chinese-English bilingual dictionary extraction for single-nouns.",
        "Therefore, we first use a Chinese morphological analyzer (Nakagawa and Uchi-moto, 2007) and an English pos-tagger (Tsuruoka et al., 2005) to analyze the raw corpora.",
        "Then we use Malt-Parser (Nivre et al., 2007) to get syntactic dependency of both the Chinese corpus and the English corpus.",
        "The dependency labels produced by MaltParser (e.g. SUB) are used to decide the type of heads and modifiers.",
        "After that, the analyzed corpora are refined through following steps: (1) we use a stemmer to do stemming for the English corpus.",
        "Considering that only nouns are treated as translation candidates, we use stems for translation candidate but keep the original form of their heads and modifiers in order to avoid excessive stemming.",
        "(2) stop words are removed.",
        "For English, we use the stop word list from (Fung, 1995).",
        "For Chinese, we remove 'Eft (of)' as stop word.",
        "(3) we remove the dependencies including punctuations and remove the sentences with more than k (set as 30 empirically) words from both English corpus and Chinese corpus, in order to reduce the effect of parsing error on dictionary extraction.",
        "iSSF^^economics)",
        "economics",
        "medicine",
        "is",
        "is",
        "±<J#J/average",
        "has",
        "tends",
        "4^/graduate",
        "was",
        "include",
        "^-R/admit",
        "emphasizes",
        "moved",
        "t&/can",
        "non-rivaled",
        "means",
        "iM^/split",
        "became",
        "requires",
        "/leave",
        "assume",
        "includes",
        "ttVcompare",
        "relies",
        "were",
        "/become",
        "can",
        "has",
        "i$M/e mphas ize",
        "replaces",
        "may",
        "is£$F^(economics)",
        "economics",
        "medicine",
        "ttH/micro",
        "keynesian",
        "physiology",
        "^^/macro",
        "new",
        "Chinese",
        "Ï~\\~Sl /computation",
        "institutional",
        "traditional",
        "0f!new",
        "positive",
        "biology",
        "î&fê/politics",
        "classical",
        "internal",
        "^^/university",
        "labor",
        "science",
        "WMM/classicists",
        "development",
        "clinical",
        "MM/ development",
        "engineering",
        "veterinary",
        "Sii/theory",
        "finance",
        "western",
        "^^/demonstration",
        "international",
        "agriculture",
        "Equation 1 shows the definition of dependency heterogeneity vector of a word W. It includes four elements.",
        "Each element represents the heterogeneity of a dependency relation.",
        "'NMOD' (noun modifier), 'SUB' (subject) and 'OBJ' (object) are the dependency labels produced by MaltParser.",
        "H ) number of different heads of W with NMOD label HNMODHead (W )",
        "English sentences and 665,789 Chinese sentences for dependency heterogeneity vector learning.",
        "To evaluate the proposed approach, we randomly select 250 Chinese/English single-noun pairs from the aligned titles of the collected pages as testing data, and divide them into 5 folders.",
        "Accuracy (see equation 3) and MMR (Voor-hees, 1999) (see equation 4) are used as evaluation metrics.",
        "The average scores of both accuracy and MMR among 5 folders are also calculated.",
        "Accuracy = 2_, tl N",
        "HSUBHead (W ) \" HOBJHead W ) =",
        "total number of heads of W with NMOD label number of different heads of W with SUB label",
        "total number of heads of W with SUB label number of different heads of W with OBJ label total number of heads of W with OBJ label number of different modifiers of W with NMOD label total number of modifiers of W with NMOD label",
        "After calculating dependency heterogeneity vector of translation candidates, bilingual dictionary entries are extracted according to the distance between the vector of Ws in source language and the vector of Wt in target language.",
        "We use Euclidean distance (see equation 2) for distance computation.",
        "The smaller distance between the dependency heterogeneity vectors of Ws and Wt, the more likely they are translations of each other.",
        "DH (Ws,Wt) – \"\\DIMODHead + DSUUBHead + DOBJHead + DlMODMod (2)DNMODHead = HNMODHead (Ws ) – HNMODHead (Wt ) DSUBHead = HSUBHead (Ws) – HSUBHead (Wt ) DOBJHead = HOBJHead (Ws) – HOBJHead (Wt) DNMODMod = HNMODMod (Ws) – HNMODMod (Wt)",
        "For example, following above definitions, we get dependency heterogeneity vector of the words analyzed before (see Table 4).",
        "The distances between these vectors are D^täiJr^, economics) = 0.222, DH(ti'ffr¥, medicine) = 0.496.",
        "It is clear that the distance between the vector of 'trJJr \"^(economics)' and 'economics' is much smaller than that between 'täifl-^' and 'medicine'.",
        "Thus, the pair 'tt'ffr^/economics' is extracted successfully."
      ]
    },
    {
      "heading": "5. Results and Discussion 5.1 Experimental Setting",
      "text": [
        "We collect Chinese and English pages from Wikipediawith inter-language link and use them as comparable corpora.",
        "After corpora preprocessing, we get 1,132,492",
        "1, if there exists correct translation in top n ranking 0, otherwise",
        "rl means the rank of the correct translation in top n ranking N means the total number of words for evaluation",
        "Two approaches were evaluated in this experiment.",
        "One is the context heterogeneity approach proposed in (Fung, 1995) (context for short).",
        "The other is our proposed approach (dependency for short).",
        "The average results of dictionary extraction are listed in Table 5.",
        "It shows both the average accuracy and average MMR of extracted dictionary entries were improved significantly (McNemar's test, p<0.05) by the proposed approach.",
        "Besides, the increase of top5 evaluation was much higher than that of top10 evaluation, which means the proposed approach has more potential to extract precise bilingual dictionary entries.",
        "_Table 5.",
        "Average results of dictionary extraction._",
        "I context _dependency_",
        "ave.accu \\ ave.MMR ave.accu ave.MMR",
        "In the proposed approach, a dependency heterogeneity vector is defined as the combination of head and modifier heterogeneities.",
        "To see the effects of different dependency heterogeneity on dictionary extraction, we evaluated the proposed approach with different vector definitions, which are",
        "only-head'.",
        "(HNMODHead , HSUBHead , HOBJHead ) only-mod: (HNMODMod ) only-NMOD .",
        "(HNMODHead, HNMODMod )",
        "N i=1 rankl",
        "O, otherwise",
        "n means top n evaluation,",
        "Word",
        "Dependency Heterogeneity Vector",
        "^^^(economics)",
        "(O.398, O.677, O.733, O.471)",
        "economics",
        "(O.466, O.5OO, O.625, O.432)",
        "medicine",
        "(O.748, O.524, O.542, O.22O)",
        "ave.accu",
        "ave.MMR",
        "ave.accu",
        "ave.MMR",
        "context",
        "O.132",
        "O.O64",
        "O.296",
        "O.O86",
        "dependency",
        "0.208",
        "0.104",
        "0.380",
        "0.128",
        "only-mod",
        "O.156",
        "O.O8O",
        "O.336",
        "O.1O3",
        "only-head",
        "O.176",
        "O.O77",
        "O.336",
        "O.O98",
        "only-NMODs",
        "O.2OO",
        "O.O94",
        "O.364",
        "O.115",
        "The results are listed in Table 6.",
        "It shows with any types of vector definitions, the proposed approach outperformed the context approach.",
        "Besides, if comparing the results of dependency, only-mod, and only-head, a conclusion can be drawn that head dependency heterogeneities and modifier dependency heterogeneities gave similar contribution to the proposed approach.",
        "At last, the difference between the results of dependency and only-NMOD shows the head and modifier with NMOD label contributed more to the proposed approach.",
        "To do detailed analysis, we collect the dictionary entries that are not extracted by context approach but extracted by the proposed approach (good for short), and the entries that are extracted by context approach but not extracted by the proposed approach (bad for short) from top10 evaluation results with their occurrence time (see Table 7).",
        "If neglecting the entries '^flfVpassports' and ' JljfiiVshanghai', we found that the proposed approach tended to extract correct bilingual dictionary entries if both the two words occurred frequently in the comparable corpora, but failed if one of them seldom appeared.",
        "_Table 7.",
        "Good and bad dictionary entries._",
        "But there are two exceptions: (1) although ' iljij: (shanghai)' and 'shanghai' appeared frequently, the proposed approach did not extract them correctly; (2) both 'i^fl^(passport)' and 'passports' occurred less than 100 times, but they were recognized successfully by the proposed approach.",
        "Analysis shows the cleanliness of the comparable corpora is the most possible reason.",
        "In the English corpus we used for evaluation, many words are incorrectly combined with 'shanghai' by ' br' (i.e. line break), such as 'airportbrshanghai'.",
        "These errors affected the correctness of dependency heterogeneity vector of 'shanghai' greatly.",
        "Compared with the dirty resource of 'shanghai', only base form and plural form of 'passport' occur in the English corpus.",
        "Therefore, the dependency heterogeneity vectors of 'JAM' and 'passports' were precise and result in the successful extraction of this dictionary entry.",
        "We will clean the corpora to solve this problem in our future work."
      ]
    },
    {
      "heading": "6. Conclusion and Future Work",
      "text": [
        "This paper proposes an approach, which not uses the similarity of bag-of-words around translation candidates but considers about the similarity of syntactic dependencies, to extract bilingual dictionary from comparable corpora.",
        "Experimental results show that the proposed approach outperformed the context-based approach significantly.",
        "It not only validates the feasibility of the proposed approach, but also shows the effectiveness of applying syntactic analysis in real application.",
        "There are several future works under consideration including corpora cleaning, extending the proposed approach from single-noun dictionary extraction to multi-words, and adapting the proposed approach to other language pairs.",
        "Besides, because the proposed approach is based on the syntactic analysis of sentences with no more than k words (see Section 4.1), the parsing accuracy and the setting of threshold k will affect the correctness of dependency heterogeneity vector learning.",
        "We will try other thresholds and syntactic parsers to see their effects on dictionary extraction in the future."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research is sponsored by Microsoft Research Asia Web-scale Natural Language Processing Theme.",
        "Good",
        "Bad",
        "Chinese",
        "English",
        "Chinese",
        "English",
        "jew/122",
        "crucifixion/19",
        "5l,g/568",
        "velocity/175",
        "7KM/6",
        "aquarium/31",
        "history/2376",
        "mixture/179",
        "*l£R/1775",
        "organizations/2194",
        "brick/66",
        "movement/1541",
        "quantification/31",
        "passports/80",
        "±i#/843",
        "shanghai/1247"
      ]
    }
  ]
}
