{
  "info": {
    "authors": [
      "Dipanjan Das",
      "Noah A. Smith"
    ],
    "book": "ACL-IJCNLP",
    "id": "acl-P09-1053",
    "title": "Paraphrase Identification as Probabilistic Quasi-Synchronous Recognition",
    "url": "https://aclweb.org/anthology/P09-1053",
    "year": 2009
  },
  "references": [
    "acl-C04-1051",
    "acl-D07-1003",
    "acl-I05-5002",
    "acl-I05-5003",
    "acl-J97-3002",
    "acl-N03-1003",
    "acl-N06-1003",
    "acl-P04-1083",
    "acl-P05-1012",
    "acl-P79-1016",
    "acl-W04-3219",
    "acl-W05-1203",
    "acl-W05-1205",
    "acl-W05-1612",
    "acl-W06-1603",
    "acl-W06-3104",
    "acl-W07-1401",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "text": [
        "Dipanjan Das and Noah A. Smith",
        "Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA",
        "We present a novel approach to deciding whether two sentences hold a paraphrase relationship.",
        "We employ a generative model that generates a paraphrase of a given sentence, and we use probabilistic inference to reason about whether two sentences share the paraphrase relationship.",
        "The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006).",
        "Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features.",
        "We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The problem of modeling paraphrase relationships between natural language utterances (McK-eown, 1979) has recently attracted interest.",
        "For computational linguists, solving this problem may shed light on how best to model the semantics of sentences.",
        "For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzi-lay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006).",
        "The paraphrase identification problem asks whether two sentences have essentially the same meaning.",
        "Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic \"overlap\" features.",
        "Such overlap features give the best-published classification accuracy for the paraphrase identification task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence structure (or \"alignment\") between the parts of two sentences.",
        "In this paper, we adopt a model that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others.",
        "Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be \"inspired\" by the structure of s.",
        "Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase.",
        "This combination of loose syntax and lexical semantics is similar to the \"Jeopardy\" model of Wang et al.",
        "(2007).",
        "This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model.",
        "We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model.",
        "This combined model leverages complementary strengths of the two approaches, outperforming a strong state-of-the-art baseline (Wan et al., 2006).",
        "This paper is organized as follows.",
        "We introduce our probabilistic model in §2.",
        "The model makes use of three quasi-synchronous grammar models (Smith and Eisner, 2006, QG, hereafter) as components (one modeling paraphrase, one modeling not-paraphrase, and one a base grammar); these are detailed, along with latent-variable inference and discriminative training algorithms, in §3.",
        "We discuss the Microsoft Research Paraphrase Corpus, upon which we conduct experiments, in §4.",
        "In §5, we present experiments on paraphrase identification with our model and make comparisons with the existing state-of-the-art.",
        "We describe the product of experts and our lexical overlap model, and discuss the results achieved in §6.",
        "We relate our approach to prior work (§7) and conclude (§8)."
      ]
    },
    {
      "heading": "2. Probabilistic Model",
      "text": [
        "Since our task is a classification problem, we require our model to provide an estimate of the posterior probability of the relationship (i.e., \"paraphrase,\" denoted p, or \"not paraphrase,\" denoted n), given the pair of sentences.",
        "Here, pq denotes model probabilities, c is a relationship class (p or n), and si and s2 are the two sentences.",
        "We choose the class according to:",
        "We define the class-conditional probabilities of the two sentences using the following generative story.",
        "First, grammar Go generates a sentence s. Then a class c is chosen, corresponding to a class-specific probabilistic quasi-synchronous grammar Gc.",
        "(We will discuss QG in detail in §3.",
        "For the present, consider it a specially-defined probabilistic model that generates sentences with a specific property, like \"paraphrases s,\" when c = p.) Given s, Gc generates the other sentence in the pair, s'.",
        "When we observe a pair of sentences si and s2 we do not presume to know which came first (i.e., which was s and which was s').",
        "Both orderings are assumed to be equally probable.",
        "For class c, where c can be p or n; Gp(s) is the QG that generates paraphrases for sentence s, while Gn(s) is the QG that generates sentences that are not paraphrases of sentence s. This latter model may seem counter-intuitive: since the vast majority of possible sentences are not paraphrases of s, why is a special grammar required?",
        "Our use of a Gn follows from the properties of the corpus currently used for learning, in which the negative examples were selected to have high lexical overlap.",
        "We return to this point in §4."
      ]
    },
    {
      "heading": "3. QG for Paraphrase Modeling",
      "text": [
        "Here, we turn to the models Gp and Gn in detail.",
        "3.1 Background",
        "Smith and Eisner (2006) introduced the quasi-synchronous grammar formalism.",
        "Here, we describe some of its salient aspects.",
        "The model arose out of the empirical observation that translated sentences have some isomorphic syntactic structure, but divergences are possible.",
        "Therefore, rather than an isomorphic structure over a pair of source and target sentences, the syntactic tree over a target sentence is modeled by a source sentence-specific grammar \"inspired\" by the source sentence's tree.",
        "This is implemented by associating with each node in the target tree a subset of the nodes in the source tree.",
        "Since it loosely links the two sentences' syntactic structures, QG is well suited for problems like word alignment for MT (Smith and Eisner, 2006) and question answering (Wang et al., 2007).",
        "Consider a very simple quasi-synchronous context-free dependency grammar that generates one dependent per production rule.",
        "Let s = (si,sm) be the source sentence.",
        "The grammar rules will take one of the two forms:",
        "where t and t' range over the vocabulary of the target language, and l and k e {0,m} are indices in the source sentence, with 0 denoting null.Hard or soft constraints can be applied between l and k in a rule.",
        "These constraints imply permissible \"configurations.\"",
        "For example, requiring l = 0 and, if k = 0 then sk must be a child of si in the source tree, we can implement a synchronous dependency grammar similar to (Melamed, 2004).",
        "Smith and Eisner (2006) used a quasi-synchronous grammar to discover the correspondence between words implied by the correspondence between the trees.",
        "We follow Wang et al.",
        "(2007) in treating the correspondences as latent variables, and in using a WordNet-based lexical semantics model to generate the target words.",
        "We describe how we model PQ(t | Gp(s)) and PQ(t | Gn(s)) for source and target sentences s and t (appearing in Eq.",
        "2 alternately as si and s2).",
        "A dependency tree on a sequence w = wk) is a mapping of indices of words to indices of syntactic parents, rp : {1,...,k} – { 0, ... , k } , and a mapping of indices of words to dependency relation types in l, : {1,k} – l. The set of indices children of to its left, {j : rw(j) = i, j < i}, is denoted Aw(i), and pw (i) is used for right children.",
        "Wj has a single parent, denoted by wTp(i).",
        "Cycles are not allowed, and w0 is taken to be the dummy \"wall\" symbol, $, whose only child is the root word of the sentence (normally the main verb).",
        "The label for Wj is denoted by r^i).",
        "We denote the whole tree of a sentence w by rw, the subtree rooted at the ith word by rw'j.",
        "Consider two sentences: let the source sentence s contain m words and the target sentence t contain n words.",
        "Let the correspondence x : {1,...,n} – {0, ...,m} be a mapping from indices of words in t to indices of words in s. (We require each target word to map to at most one source word, though multiple target words can map to the same source word, i.e., x(i) = x(j) while i = j.)",
        "When x(i) = 0, the ith target word maps to the wall symbol, equivalently a \"null\" word.",
        "Each of our QGs Gp and Gn generates the alignments x, the target tree r*, and the sentence t. Both Gp and Gn are structured in the same way, differing only in their parameters; henceforth we discuss Gp; Gn is similar.",
        "We assume that the parse trees of s and t are known.",
        "Therefore our model defines:",
        "Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index where pvai and pkid are valence and child-production probabilities parameterized as discussed in §3.4.",
        "Note the recursion in the second-to-last line.",
        "We next describe a dynamic programming solution for calculating p(rt | Gp(rs)).",
        "In §3.4 we discuss the parameterization of the model.",
        "Let C(i, l) refer to the probability of rt,j, assuming that the parent of tj, trt(j), is aligned to si.",
        "For leaves of rt, the base case is:",
        "where k ranges over possible values of x(i), the source-tree node to which tj is aligned.",
        "The recursive case is:",
        "We assume that the wall symbols to and so are aligned, so p(rt | Gp(rs)) = C(r, 0), where r is the index of the root word of the target tree rt.",
        "It is straightforward to show that this algorithm requires O(mn) runtime and O(mn) space.",
        "The valency distribution pvai in Eq.",
        "4 is estimated in our model using the transformed treebank (see footnote 4).",
        "For unobserved cases, the conditional probability is estimated by backing off to the parent POS tag and child direction.",
        "We discuss next how to parameterize the probability pkid that appears in Equations 4, 5, and 6.",
        "This conditional distribution forms the core of our QGs, and we deviate from earlier research using QGs in defining pkid in a fully generative way.",
        "In addition to assuming that dependency parse trees for s and t are observable, we also assume each word wj comes with POS and named entity tags.",
        "In our experiments these were obtained automatically using MXPOST (Ratnaparkhi, 1996) and BBN's Identifinder (Bikel et al., 1999).",
        "Xpisrel(Isrel(ti) | Sx(i)) XPword(ti | Isrel(ti),Sx(i))",
        "We consider each of the factors above in turn.",
        "Configuration In QG, \"configurations\" refer to the tree relationship among source-tree nodes (above, s and sx(i)) aligned to a pair of parent-child target-tree nodes (above, tj and ti).",
        "In deriving tt,j, the model first chooses the configuration that will hold among ti, tj, sx(i) (which has yet to be chosen), and s (line 7).",
        "This is defined for configuration c log-linearly by:",
        "Permissible configurations in our model are shown in Table 1.",
        "These are identical to prior work (Smith and Eisner, 2006; Wang et al., 2007), except that we add a \"root\" configuration that aligns the target parent-child pair to null and the head word of the source sentence, respectively.",
        "Using many permissible configurations helps remove negative effects from noisy parses, which our learner treats as evidence.",
        "Fig.",
        "1 shows some examples of major configurations that Gp discovers in the data.",
        "Source tree alignment After choosing the configuration, the specific node in ts that ti will align to, sx(i) is drawn uniformly (line 8) from among those in the configuration selected.",
        "Dependency label, POS, and named entity class The newly generated target word's dependency label, POS, and named entity class drawn from multinomial distributions p1ab, ppos, and pne that condition, respectively, on the configuration and the POS and named entity class of the aligned source-tree word sx(i) (lines 9-11).",
        "Table 1: Permissible configurations.",
        "i is an index in t whose configuration is to be chosen; j = Tp(i) is i's parent.",
        "WordNet relation(s) The model next chooses a lexical semantics relation between sx(j) and the yet-to-be-chosen word tj (line 12).",
        "Following Wang et al.",
        "(2007), we employ a 14-feature loglinear model over all logically possible combinations of the 14 WordNet relations (Miller, 1995).",
        "Similarly to Eq.",
        "14, we normalize this log-linear model based on the set of relations that are nonempty in WordNet for the word sx(j).",
        "Word Finally, the target word is randomly chosen from among the set of words that bear the lexical semantic relationship just chosen (line 13).",
        "This distribution is, again, defined log-linearly:",
        "Here aw is the Good-Turing unigram probability estimate of a word w from the Gigaword corpus (Graff, 2003).",
        "In addition to the QG that generates a second sentence bearing the desired relationship (paraphrase or not) to the first sentence s, our model in §2 also requires a base grammar Go over s.",
        "We view this grammar as a trivial special case of the same QG model already described.",
        "Go assumes the empty source sentence consists only of",
        "Configuration Description",
        "parent-child",
        "Tp(x(i)) = appended with r|(x(i))",
        "child-parent",
        "x (i) = Tp (x ( j )), appended with t| (x ( j ))",
        "grandparent-",
        "Tp (Tp(x(i))) = x(j), appended with",
        "grandchild",
        "Tf (x(i))",
        "siblings",
        "Tp (x(i)) = Tp(x(j)),x(i) = x(j)",
        "same-node",
        "x(i) = x(j)",
        "c-command",
        "the parent of one source-side word is an ancestor of the other source-side word",
        "root",
        "x(j) = 0, x(i) is the root of s",
        "child-null",
        "x(i) = 0",
        "parent-null",
        "x(j) = 0, x(i) is something other than root of s",
        "other",
        "catch-all for all other types of configurations, which are permitted",
        "(a) parent-child (b) child-parent",
        "fill----complete dozens injured questionnaire questionnaire wounded",
        "(e) same-node (f) siblings",
        "quarter---first-quarter ^aaiiy",
        "(c) grandparent-grandchild (d) c-command",
        "_ wi|| collected signatures",
        "Liscouski necessary",
        "massive U.S",
        "Secretary treasury",
        "Figure 1: Some example configurations from Table 1 that Gp discovers in the dev.",
        "data.",
        "Directed arrows show head-modifier relationships, while dotted arrows show alignments.",
        "a single wall node.",
        "Thus every word generated under Go aligns to null, and we can simplify the dynamic programming algorithm that scores a tree ts under Go:",
        "where the final product is 1 when tj has no children.",
        "It should be clear that p(s | Go) = C'(0).",
        "We estimate the distributions over dependency labels, POS tags, and named entity classes using the transformed treebank (footnote 4).",
        "The distribution over words is taken from the Gigaword corpus (as in §3.4).",
        "It is important to note that Go is designed to give a smoothed estimate of the probability of a particular parsed, named entity-tagged sentence.",
        "It is never used for parsing or for generation; it is only used as a component in the generative probability model presented in §2 (Eq.",
        "2).",
        "we train the model discriminatively by maximizing regularized conditional likelihood:",
        "The parameters 6 to be learned include the class priors, the conditional distributions of the dependency labels given the various configurations, the POS tags given POS tags, the NE tags given NE tags appearing in expressions 9-11, the configuration weights appearing in Eq.",
        "14, and the weights of the various features in the log-linear model for the lexical-semantics model.",
        "As noted, the distributions pvai, the word unigram weights in Eq.",
        "15, and the parameters of the base grammar are fixed using the treebank (see footnote 4) and the Giga-word corpus.",
        "Since there is a hidden variable (x), the objective function is non-convex.",
        "We locally optimize using the L-BFGS quasi-Newton method (Liu and Nocedal, 1989).",
        "Because many of our parameters are multinomial probabilities that are constrained to sum to one and L-BFGS is not designed to handle constraints, we treat these parameters as un-normalized weights that get renormalized (using a softmax function) before calculating the objective."
      ]
    },
    {
      "heading": "4. Data and Task",
      "text": [
        "In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004).",
        "The corpus contains 5,801 pairs of sentences that have been marked as \"equivalent\" or \"not equivalent.\"",
        "It was constructed from thousands of news sources on the web.",
        "Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases.",
        "5,801 out of these 49,375 pairs were randomly selected and presented to human judges for refinement into true and false paraphrases.",
        "3,900 of the pairs were marked as having",
        "About 120 potential jurors were being asked to complete a lengthy questionnaire .",
        "The jurors weretaken into the courtroom in groups of 40 and a^sed^tofilTout a questionnaire^",
        "Figure 2: Discovered alignment of Ex.",
        "19 produced by Gp.",
        "Observe that the model aligns identical words and also \"complete\" and \"fill\" in this specific case.",
        "This kind of alignment provides an edge over a simple lexical overlap model.",
        "\"mostly bidirectional entailment,\" a standard definition of the paraphrase relation.",
        "Each sentence was labeled first by two judges, who averaged 83% agreement, and a third judge resolved conflicts.",
        "We use the standard data split into 4,076 (2,753 paraphrase, 1,323 not) training and 1,725 (1147 paraphrase, 578 not) test pairs.",
        "We reserved a randomly selected 1,075 training pairs for tuning.We cite some examples from the training set here:",
        "(18) Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.",
        "With the scandal hanging over Stewart's company, revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.",
        "(19) About 120 potential jurors were being asked to complete a lengthy questionnaire.",
        "The jurors were taken into the courtroom in groups of 40 and asked to fill out a questionnaire.",
        "Ex.",
        "18 is a true paraphrase pair.",
        "Notice the high lexical overlap between the two sentences (unigram overlap of 100% in one direction and 72% in the other).",
        "Ex.",
        "19 is another true paraphrase pair with much lower lexical overlap (unigram overlap of 50% in one direction and 30% in the other).",
        "Notice the use of similar-meaning phrases and irrelevant modifiers that retain the same meaning in both sentences, which a lexical overlap model cannot capture easily, but a model like a QG might.",
        "Also, in both pairs, the relationship cannot be called total bidirectional equivalence because there is some extra information in one sentence which cannot be inferred from the other.",
        "Ex.",
        "20 was labeled \"not paraphrase\":",
        "(20) \"There were a number of bureaucratic and administrative missed signals - there's not one person who's responsible here,\" Gehman said.",
        "In turning down the NIMA offer, Gehman said, \"there were a number of bureaucratic and administrative missed signals here.",
        "There is significant content overlap, making a decision difficult for a naive lexical overlap classifier.",
        "(In fact, pq labels this example n while the lexical overlap models label it p.)",
        "The fact that negative examples in this corpus were selected because of their high lexical overlap is important.",
        "It means that any discriminative model is expected to learn to distinguish mere overlap from paraphrase.",
        "This seems appropriate, but it does mean that the \"not paraphrase\" relation ought to be denoted \"not paraphrase but deceptively similar on the surface.\"",
        "It is for this reason that we use a special QG for the n relation."
      ]
    },
    {
      "heading": "5. Experimental Evaluation",
      "text": [
        "Here we present our experimental evaluation using pq.",
        "We trained on the training set (3,001 pairs) and tuned model metaparameters (c in Eq.",
        "17) and the effect of different feature sets on the development set (1,075 pairs).",
        "We report accuracy on the official MSRPC test dataset.",
        "If the posterior probability pq(p | s1, s2) is greater than 0.5, the pair is labeled \"paraphrase\" (as in Eq.",
        "1).",
        "We replicated a state-of-the-art baseline model for comparison.",
        "Wan et al.",
        "(2006) report the best published accuracy, to our knowledge, on this task, using a support vector machine.",
        "Our baseline is a reimplementation of Wan et al.",
        "(2006), using features calculated directly from s1 and s2 without recourse to any hidden structure: proportion of word unigram matches, proportion of lemma-tized unigram matches, BLEU score (Papineni et al., 2001), BLEU score on lemmatized tokens, F measure (Turian et al., 2003), difference of sentence length, and proportion of dependency relation overlap.",
        "The SVM was trained to classify positive and negative examples of paraphrase using SVM1ight (Joachims, 1999).",
        "Metaparameters, tuned on the development data, were the regularization constant and the degree of the polynomial kernel (chosen in [10-5,10] and 1-5 respec-tively.",
        ").",
        "It is unsurprising that the SVM performs very well on the MSRPC because of the corpus creation process (see Sec. 4) where an SVM was applied as well, with very similar features and a skewed decision process (Dolan and Brockett, 2005).",
        "http://svmlight.joachims.org Our replication of the Wan et al.",
        "model is approximate, because we used different preprocessing tools: MX-",
        "POST for POS tagging (Ratnaparkhi, 1996), MSTParser for parsing (McDonald et al., 2005), and Dan Bikel's interface (http://www.cis.upenn.edu/~dbikel/ software.html#wn) to WordNet (Miller, 1995) for lemmatization information.",
        "Tuning led to C = 17 and polynomial degree 4.",
        "baselines Wan et al.",
        "SVM (reported) Wan et al.",
        "SVM (replication) lexical semantics features removed",
        "Pq all features",
        "c-command disallowed (best; see text) § product of experts",
        "Wan et al.",
        "SVM and pl oracles Wan et al.",
        "SVM and pq _Pq and pl_",
        "Tab.",
        "2 shows performance achieved by the baseline SVM and variations on pq on the test set.",
        "We performed a few feature ablation studies, evaluating on the development data.",
        "We removed the lexical semantics component of the QG, and disallowed the syntactic configurations one by one, to investigate which components of pq contributes to system performance.",
        "The lexical semantics component is critical, as seen by the drop in accuracy from the table (without this component, pq behaves almost like the \"all p\" baseline).",
        "We found that the most important configurations are \"parent-child,\" and \"child-parent\" while damage from ablating other configurations is relatively small.",
        "Most interestingly, disallowing the \"c-command\" configuration resulted in the best absolute accuracy, giving us the best version of pq.",
        "The c-command configuration allows more distant nodes in a source sentence to align to parent-child pairs in a target (see Fig. 1d).",
        "Allowing this configuration guides the model in the wrong direction, thus reducing test accuracy.",
        "We tried disallowing more than one configuration at a time, without getting improvements on development data.",
        "We also tried ablating the WordNet relations, and observed that the \"identical-word\" feature hurt the model the most.",
        "Ablating the rest of the features did not produce considerable changes in accuracy.",
        "The development data-selected pq achieves higher recall by 1 point than Wan et al.",
        "'s SVM, but has precision 2 points worse.",
        "It is quite promising that a linguistically-motivated probabilistic model comes so close to a string-similarity baseline, without incorporating string-local phrases.",
        "We see several reasons to prefer the more intricate QG to the straightforward SVM.",
        "First, the QG discovers hidden alignments between words.",
        "Alignments have been leveraged in related tasks such as textual entailment (Giampic-colo et al., 2007); they make the model more interpretable in analyzing system output (e.g., Fig. 2).",
        "Second, the paraphrases of a sentence can be considered to be monolingual translations.",
        "We model the paraphrase problem using a direct machine translation model, thus providing a translation interpretation of the problem.",
        "This framework could be extended to permit paraphrase generation, or to exploit other linguistic annotations, such as representations of semantics (see, e.g., Qiu et al., 2006).",
        "Nonetheless, the usefulness of surface overlap features is difficult to ignore.",
        "We next provide an efficient way to combine a surface model with pq."
      ]
    },
    {
      "heading": "6. Product of Experts",
      "text": [
        "Incorporating structural alignment and surface overlap features inside a single model can make exact inference infeasible.",
        "As an example, consider features like n-gram overlap percentages that provide cues of content overlap between two sentences.",
        "One intuitive way of including these features in a QG could be including these only at the root of the target tree, i.e. while calculating C(r, 0).",
        "These features have to be included in estimating pkjd, which has log-linear component models (Eq.",
        "7- 13).",
        "For these bigram or trigram overlap features, a similar log-linear model has to be normalized with a partition function, which considers the (unnormalized) scores ofall possible target sentences, given the source sentence.",
        "We therefore combine pq with a lexical overlap model that gives another posterior probability estimate pL(c | si, s2) through a product of experts (PoE; Hinton, 2002), pj(c | si, s2)",
        "Accuracy Precision Recall",
        "66.49",
        "66.49",
        "100.00",
        "p-class precision, and",
        "75.63",
        "77.00",
        "90.00",
        "p-class recall on the test",
        "75.42",
        "76.88",
        "90.14",
        "set(N = 1,725).",
        "See",
        "68.64",
        "68.84",
        "96.51",
        "text for differences in",
        "73.33",
        "74.48",
        "91.10",
        "implementation",
        "73.86",
        "74.89",
        "91.28",
        "between Wan et al.",
        "and",
        "75.36",
        "78.12",
        "87.44",
        "our replication; their",
        "76.06",
        "79.57",
        "86.05",
        "reported score does not",
        "80.17",
        "100.00",
        "92.07",
        "include the full test set.",
        "83.42",
        "100.00",
        "96.60",
        "83.19",
        "100.00",
        "95.29",
        "Eq.",
        "21 takes the product of the two models' posterior probabilities, then normalizes it to sum to one.",
        "PoE models are used to efficiently combine several expert models that individually constrain different dimensions in high-dimensional data, the product therefore constraining all of the dimensions.",
        "Combining models in this way grants to each expert component model the ability to \"veto\" a class by giving it low probability; the most probable class is the one that is least objectionable to all experts.",
        "Probabilistic Lexical Overlap Model We devised a logistic regression (LR) model incorporating 18 simple features, computed directly from si and s2, without modeling any hidden correspondence.",
        "LR (like the QG) provides a probability distribution, but uses surface features (like the SVM).",
        "The features are of the form precisionn(number of n-gram matches divided by the number of n-grams in si), recalln (number of n-gram matches divided by the number of n-grams in s2) and Fn (harmonic mean of the previous two features), where 1 < n < 3.",
        "We also used lemma-tized versions of these features.",
        "This model gives the posterior probability pL(c | si, s2), where c g {p, n}.",
        "We estimated the model parameters analogously to Eq.",
        "17.",
        "Performance is reported in Tab.",
        "2; this model is on par with the SVM, though trading recall in favor of precision.",
        "We view it as a probabilistic simulation of the SVM more suitable for combination with the QG.",
        "Training the PoE Various ways of training a PoE exist.",
        "We first trained pq and pL separately as described, then initialized the PoE with those parameters.",
        "We then continued training, maximizing (unregularized) conditional likelihood.",
        "Experiment We used pq with the \"c-command\" configuration excluded, and the LR model in the product of experts.",
        "Tab.",
        "2 includes the final results achieved by the PoE.",
        "The PoE model outperforms all the other models, achieving an accuracy of 76.06%.",
        "The PoE is conservative, labeling a pair as p only if the LR and the QG give it strong p probabilities.",
        "This leads to high precision, at the expense of recall.",
        "Oracle Ensembles Tab.",
        "2 shows the results of three different oracle ensemble systems that correctly classify a pair if either of the two individual systems in the combination is correct.",
        "Note that the combinations involving pq achieve 83%, the human agreement level for the MSRPC.",
        "The LR and SVM are highly similar, and their oracle combination does not perform as well."
      ]
    },
    {
      "heading": "7. Related Work",
      "text": [
        "There is a growing body of research that uses the to build models of paraphrase.",
        "As noted, the most successful work has used edit distance (Zhang and Patrick, 2005) or bag-of-words features to measure sentence similarity, along with shallow syntactic features (Finch et al., 2005; Wan et al., 2006; used predicate-argument annotations.",
        "Most related to our approach, Wu (2005) used inversion transduction grammars – a synchronous context-free formalism (Wu, 1997) – for this task.",
        "Wu reported only positive-class (p) precision (not accuracy) on the test set.",
        "He obtained 76.1%, while our PoE model achieves 79.6% on that measure.",
        "Wu's model can be understood as a strict hierarchical maximum-alignment method.",
        "In contrast, our alignments are soft (we sum over them), and we do not require strictly isomorphic syntactic structures.",
        "Most importantly, our approach is founded on a stochastic generating process and estimated discriminatively for this task, while Wu did not estimate any parameters from data at all."
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "In this paper, we have presented a probabilistic model of paraphrase incorporating syntax, lexical semantics, and hidden loose alignments between two sentences' trees.",
        "Though it fully defines a generative process for both sentences and their relationship, the model is discriminatively trained to maximize conditional likelihood.",
        "We have shown that this model is competitive for determining whether there exists a semantic relationship between them, and can be improved by principled combination with more standard lexical overlap approaches."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors thank the three anonymous reviewers for helpful comments and Alan Black, Frederick Crabbe, Jason Eisner, Kevin Gimpel, Rebecca Hwa, David Smith, and Mengqiu Wang for helpful discussions.",
        "This work was supported by DARPA grant NBCH-1080004."
      ]
    }
  ]
}
