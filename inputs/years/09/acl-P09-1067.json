{
  "info": {
    "authors": [
      "Zhifei Li",
      "Jason M. Eisner",
      "Sanjeev P. Khudanpur"
    ],
    "book": "ACL-IJCNLP",
    "id": "acl-P09-1067",
    "title": "Variational Decoding for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/P09-1067",
    "year": 2009
  },
  "references": [
    "acl-C96-2215",
    "acl-D07-1104",
    "acl-D08-1065",
    "acl-J05-2002",
    "acl-J07-2003",
    "acl-N03-1017",
    "acl-N06-1045",
    "acl-P00-1056",
    "acl-P02-1001",
    "acl-P03-1006",
    "acl-P03-1021",
    "acl-P05-1010",
    "acl-P06-2101",
    "acl-P07-1019",
    "acl-P08-1024",
    "acl-P08-1025",
    "acl-W06-3110",
    "acl-W08-0402",
    "acl-W96-0214"
  ],
  "sections": [
    {
      "text": [
        "Zhifei Li and Jason Eisner and Sanjeev Khudanpur",
        "Statistical models in machine translation exhibit spurious ambiguity.",
        "That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations).",
        "In principle, the goodness of a string is measured by the total probability of its many derivations.",
        "However, finding the best string (e.g., during decoding) is then computationally intractable.",
        "Therefore, most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation.",
        "Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding.",
        "Our particular variational distributions are parameterized as n-gram models.",
        "We also analytically show that interpolating these n-gram models for different n is similar to minimum-risk decoding for bleu (Tromble et al., 2008).",
        "Experiments show that our approach improves the state of the art."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Ambiguity is a central issue in natural language processing.",
        "Many systems try to resolve ambiguities in the input, for example by tagging words with their senses or choosing a particular syntax tree for a sentence.",
        "These systems are designed to recover the values of interesting latent variables, such as word senses, syntax trees, or translations, given the observed input.",
        "However, some systems resolve too many ambiguities.",
        "They recover additional latent variables – so-called nuisance variables – that are not of interest to the user.",
        "For example, though machine translation (MT) seeks to output a string, typical MT systems (Koehn et al., 2003; Chiang, 2007) will also recover a particular derivation of that output string, which specifies a tree or segmentation and its alignment to the input string.",
        "The competing derivations of a string are interchangeable for a user who is only interested in the string itself, so a system that unnecessarily tries to choose among them is said to be resolving spurious ambiguity.",
        "Of course, the nuisance variables are important components of the system's model.",
        "For example, the translation process from one language to another language may follow some hidden tree transformation process, in a recursive fashion.",
        "Many features of the model will crucially make reference to such hidden structures or alignments.",
        "However, collapsing the resulting spurious ambiguity – i.e., marginalizing out the nuisance variables – causes significant computational difficulties.",
        "The goodness of a possible MT output string should be measured by summing up the probabilities of all its derivations.",
        "Unfortunately, finding the best string is then computationally intractable (Sima'an, 1996; Casacuberta and Higuera, 2000).",
        "Therefore, most systems merely identify the single most probable derivation and report the corresponding string.",
        "This corresponds to a Viterbi approximation that measures the goodness of an output string using only its most probable derivation, ignoring all the others.",
        "In this paper, we propose a variational method that considers all the derivations but still allows tractable decoding.",
        "Given an input string, the original system produces a probability distribution p over possible output strings and their derivations (nuisance variables).",
        "Our method constructs a second distribution q e Q that approximates p as well as possible, and then finds the best string according to q.",
        "The last step is tractable because each q e Q is defined (unlike p) without reference to nuisance variables.",
        "Notice that q here does not approximate the entire translation process, but only the distribution over output strings for a particular input.",
        "This is why it can be a fairly good approximation even without using the nuisance variables.",
        "In practice, we approximate with several different variational families Q, corresponding to n-gram (Markov) models of different orders.",
        "We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al.",
        "(2008).",
        "Experiments show that our approach improves the state of the art.",
        "The methods presented in this paper should be applicable to collapsing spurious ambiguity for other tasks as well.",
        "Such tasks include data-oriented parsing (DOP), applications of Hidden Markov Models (HMMs) and mixture models, and other models with latent variables.",
        "Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al., 2005)."
      ]
    },
    {
      "heading": "2. Background",
      "text": [
        "In MT, spurious ambiguity occurs both in regular phrase-based systems (e.g., Koehn et al.",
        "(2003)), where different segmentations lead to the same translation string (Figure 1), and in syntax-based systems (e.g., Chiang (2007)), where different derivation trees yield the same string (Figure 2).",
        "In the Hiero system (Chiang, 2007) we are using, each string corresponds to about 115 distinct derivations on average.",
        "We use x to denote the input string, and D(x) to consider the set of derivations then considered by the system.",
        "Each derivation d e D(x) yields some translation string y = Y(d) in the target language.",
        "We write D(x,y) = {d e D(x) : Y(d) = y} to denote the set of all derivations that yield y.",
        "Thus, the set of translations permitted by the model is T(y) = {y : D(x,y) = 0} (or equivalently, T(y) = {Y(d) : d e D(x)}).",
        "We write y* for the translation string that is actually output.",
        "For a given input sentence x, a decoding method identifies a particular \"best\" output string y*.",
        "The maximum a posteriori (MAP) decision rule is",
        "machine translation/|software| machinqtranslation software",
        "Figure 1: Segmentation ambiguity in phrase-based MT: two different segmentations lead to the same translation string.",
        "Figure 2: Tree ambiguity in syntax-based MT: two derivation trees yield the same translation string.",
        "(An alternative decision rule, minimum Bayes risk (MBR), will be discussed in Section 4.)",
        "To obtain p(y | x) above, we need to marginalize over a nuisance variable, the derivation of y.",
        "Therefore, the MAP decision rule becomes y* = argmax where p(y, d | x) is typically derived from a loglinear model as follows, where 7 is a scaling factor to adjust the sharpness of the distribution, the score s(x, y, d) is a learned linear combination of features of the triple (x,y,d), and Z(x) is a normalization constant.",
        "Note that p(y, d | x) = 0 if y = Y(d).",
        "Our derivation set D(x) is encoded in polynomial space, using a hypergraph or lattice.",
        "However, both | D (x) | and |T(x)| may be exponential in |x|.",
        "Since the marginalization needs to be carried out for each member of T(x), the decoding problem of (2) turns out to be NP-hard, as shown by Sima'an (1996) for a similar problem.",
        "To approximate the intractable decoding problem of (2), most MT systems (Koehn et al., 2003; Chiang, 2007) use a simple Viterbi approximation,",
        "Clearly, (5) replaces the sum in (2) with a max.",
        "In other words, it approximates the probability of a translation string by the probability of its mostprobable derivation.",
        "(5) is found quickly via (6).",
        "The Viterbi approximation is simple and tractable, but it ignores most derivations.",
        "Another popular approximation enumerates the N best derivations in D(x), a set that we call ND(x).",
        "Modifying (2) to sum over only these derivations is called crunching by May and Knight (2006):"
      ]
    },
    {
      "heading": "3. Variational Approximate Decoding",
      "text": [
        "The Viterbi and crunching methods above approximate the intractable decoding of (2) by ignoring most of the derivations.",
        "In this section, we will present a novel variational approximation, which considers all the derivations but still allows tractable decoding.",
        "There are several popular approaches to approximate inference when exact inference is intractable (Bishop, 2006).",
        "Stochastic techniques such as Markov Chain Monte Carlo are exact in the limit of infinite runtime, but tend to be too slow for large problems.",
        "By contrast, deterministic variational methods (Jordan et al., 1999), including message-passing (Minka, 2005), are inexact but scale up well.",
        "They approximate the original intractable distribution with one that factorizes better or has a specific parametric form (e.g., Gaussian).",
        "In our work, we use a fast variational method.",
        "Variational methods generally work as follows.",
        "When exact inference under a complex model p is intractable, one can approximate the posterior p(y | x) by a tractable model q(y), where q e Q is chosen to minimize some information loss such as the KL divergence KL(p || q).",
        "The simpler model q can then act as a surrogate for p during inference.",
        "For each input sentence x, we assume that a baseline MT system generates a hypergraph HG(x) that compactly encodes the derivation set D(x) along with a score for each d e D(x), which we interpret as p(y, d | x) (or proportional to it).",
        "For any single y e T(x), it would be tractable using HG(x) to compute p(y | x) = dp(y, d | x).",
        "However, as mentioned, it is intractable to find argmaxy p(y | x) as required by the MAP decoding (2), so we seek an approximate distribution",
        "For a fixed x, we seek a distribution q e Q that minimizes the KL divergence from p to q (both regarded as distributions over y): argmax gee",
        "So far, in order to approximate the intractable optimization problem (2), we have defined another optimization problem (10).",
        "If computing p( y | x) during decoding is computationally intractable, one might wonder if the optimization problem (10) is any simpler.",
        "We will show this is the case.",
        "The trick is to parameterize q as a factorized distribution such that the estimation of q*and decoding using q* are both tractable through efficient dynamic programs.",
        "In the next three subsections, we will discuss the parameterization, estimation, and decoding, respectively.",
        "In (10), Q is a family of distributions.",
        "If we select a large family Q, we can allow more complex distributions, so that q* will better approximate p. If we select a smaller family Q, we can guarantee that q* will have a simple form with many conditional independencies, so that q*(y) and y* = argmaxy q*(y) are easier to compute.",
        "Since each q(y) is a distribution over output strings, a natural choice for Q is the family of n-gram models.",
        "To obtain a small KL divergence (8), we should make n as large as possible.",
        "In fact, q* – p as n – oo.",
        "Of course, this last point also means that our computation becomes intractable as n – oo.",
        "However, if p(y | x) is defined by a hypergraph HG(x) whose structure explicitly incorporates an m-gram language model, both training and decoding will be efficient when m > n. We will give algorithms for this case that are linear in the size of HG(x).",
        "Formally, each q e Q takes the form where W is a set of n-gram types.",
        "Each w e W is an n-gram, which occurs cw (y) times in the string y, and w may be divided into an (n – 1)-gram prefix h(w) (the history) and a 1-gram suffix r(w) (the rightmost or current word).",
        "We sketch a method that works for any language model given by a weighted FSA, L. The variational family Q can be specified by any deterministic weighted FSA, Q, with weights parameterized by 0.",
        "One seeks 0 to minimize (8).",
        "Intersect HG(x) with an \"unweighted\" version of Q in which all arcs have weight 1, so that Q does not prefer any string to another.",
        "By lifting weights into an expectation semiring (Eisner, 2002), it is then possible to obtain expected transition counts in Q (where the expectation is taken under p), or other sufficient statistics needed to estimate 0.",
        "This takes only time O(|HG(x)|) when L is a left-to-right refinement of Q (meaning that any two prefix strings that reach the same state in L also reach the same state in Q), for then intersecting L or HG(x) with Q does not split any states.",
        "That is the case when L and Q are respectively pure m-gram and n-gram models with m > n, as assumed in (12) and Figure 4.",
        "It is also the case when Q is a pure n-gram model and L is constructed not to back off beyond n-grams; or when the variational family Q is defined by deliberately taking the FSA Q to have the same topology as L.",
        "The parameters that specify a particular q e Q are the (normalized) conditional probability distributions q(r(w) | We will now see how to estimate these parameters to approximate p(- | x) for a given x at test time.",
        "Note that the objective function (8)-(10) asks us to approximate p as closely as possible, without any further smoothing.",
        "(It is assumed that p is already smoothed appropriately, having been constructed from channel and language models that were estimated with smoothing from finite training data.)",
        "Our actual job is exactly the same, except that p is specified not by a corpus but by the hypergraph HG(x).",
        "The only change is that the n-gram counts c(w) are no longer integers from a corpus, but are expected counts under p:",
        "Ey Cw (y)p(y j x) Ey,d Cw (y)p(y d j x)",
        "Now, the question is how to efficiently compute (12) from the hypergraph HG(x).",
        "To develop the intuition, we first present a brute-force algorithm in Figure 3.",
        "The algorithm is brute-force since it first needs to unpack the hypergraph and enumerate each possible derivation in the hypergraph (see line 1), which is computationally intractable.",
        "The algorithm then enumerates each n-gram and (n – 1)-gram in y and accumulates its soft count into the expected count, and finally obtains the parameters of q* by taking count ratios via (12).",
        "Figure 4 shows an efficient version that exploits the packed-forest structure of HG(x) in computing the expected counts.",
        "Specifically, it first runs the inside-outside procedure, which annotates each node (say v) with both an inside weight /?",
        "(v) and an outside weight a(v).",
        "The inside-outside also finds Z(x), the total weight of all derivations.",
        "With these weights, the algorithm then explores the hypergraph once more to collect the expected",
        "In fact, if p were the empirical distribution over strings in a training corpus, then q* of (10) is just the maximum-likelihood n-gram model – whose parameters, trivially, are just unsmoothed ratios of the n-gram and (n – 1)-gram counts in the training Brute-Force-MLE(HG(x)) 1 for y, d in HG(x) > each derivation 2 for w in y > each n-gram type",
        "When translating x at runtime, the q* constructed from HG(x) will be used as a surrogate for p during decoding.",
        "We want its most probable string:",
        "Since q* is an n-gram model, finding y* is equivalent to a shortest-path problem in a certain graph whose edges correspond to n-grams (weighted with negative log-probabilities) and whose vertices correspond to (n – 1)-grams.",
        "However, because q* only approximates p, y* of (13) may be locally appropriate but globally inadequate as a translation of x.",
        "Observe, e.g., that an n-gram model q*(y) will tend to favor short strings y, regardless of the length of x.",
        "Suppose x = le chat chasse la souris (\"the cat chases the mouse\") and q* is a bigram approximation to p(y | x).",
        "Presumably q*(the | START), q*(mouse | the), and q*(END | mouse) are all large in HG(x).",
        "So the most probable string y* under q* may be simply \"the mouse,\" which is short and has a high probability but fails to cover x.",
        "Therefore, a better way of using q* is to restrict the search space to the original hypergraph, i.e.:",
        "This ensures that y* is a valid string in the original hypergraph HG(x), which will tend to rule out inadequate translations like \"the mouse.\"",
        "If our sole objective is to get a good approximation to p(y | x), we should just use a single n-gram model q* whose order n is as large as possible, given computational constraints.",
        "This may be regarded as favoring n-grams that are likely to appear in the reference translation (because they are likely in the derivation forest).",
        "However, in order to score well on the bleu metric for MT evaluation (Papineni et al., 2001), which gives partial credit, we would also like to favor lower-order n-grams that are likely to appear in the reference, even if this means picking some less-likely highorder n-grams.",
        "For this reason, it is useful to interpolate different orders of variational models, where n may include the value of zero, in which case log q0(y) = |y|, corresponding to a conventional word penalty feature.",
        "In the geometric interpolation above, the weight 0n controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006).",
        "Lastly, note that Viterbi and variational approximation are different ways to approximate the exact probability p(y | x), and each of them has pros and cons.",
        "Specifically, Viterbi approximation uses the correct probability of one complete derivation, but ignores most of the derivations in the hypergraph.",
        "In comparison, the variational approximation considers all the derivations in the hypergraph, but uses only aggregate statistics of fragments of derivations.",
        "Therefore, it is desirable to interpolate further with the Viterbi approximation when choosing the final translation output: where the first term corresponds to the interpolated variational decoding of (15) and the second term corresponds to the Viterbi decoding of (4).",
        "Assuming #v > 0, the second term penalizes translations with no good derivation in the hypergraph.For n < m, any of these decoders (14)-(16) may be implemented efficiently by using the n-gram variational approximations q* to rescore HG(x) – preserving its hypergraph topology, but modifying the hyperedge weig hts.",
        "While the original weights gave derivation d a score of logp(d | x), the weights as modified for (16) will give d a score of En #n • log q.",
        "*,(Y(d)) + 6>v • log p(d | x).",
        "We then find the best-scoring derivation and output its target yield; that is, we find argmaxy€T(x) via Y(argmaxd€D(x))."
      ]
    },
    {
      "heading": "3. > accumulate soft count",
      "text": []
    },
    {
      "heading": "7. return q*",
      "text": [
        "Dynamic-Programming-MLE (H G (x))"
      ]
    },
    {
      "heading": "1. run inside-outside on the hypergraph HG (x)",
      "text": [
        "2 for v in HG(x) > each node"
      ]
    },
    {
      "heading": "3. for e G B(v) > each incoming hyperedge",
      "text": []
    },
    {
      "heading": "5. for u G T(e) > each antecedent node",
      "text": []
    },
    {
      "heading": "7. > accumulate soft count",
      "text": [
        "8 for w in e > each n-gram type"
      ]
    },
    {
      "heading": "12. return q *",
      "text": [
        "Figure 4: Dynamic programming estimation of q*.",
        "B(v) represents the set of incoming hyperedges of node v; pe represents the weight of the hyperedge e itself; T(e) represents the set of antecedent nodes of hyperedge e. Please refer to the text for the meanings of other notations.",
        "counts.",
        "For each hyperedge (say e), it first gets the posterior weight Ce (see lines 4-6).",
        "Then, for each n-gram type (say w), it increments the expected count by Cw (e) • Ce, where Cw(e) is the number of copies of n-gram w that are added by hyperedge e, i.e., that appear in the yield of e but not in the yields of any of its antecedents u G T (e).",
        "While there may be exponentially many derivations, the hypergraph data structure represents them in polynomial space by allowing multiple derivations to share subderivations.",
        "The algorithm of Figure 4 may be run over this packed forest in time O(|HG(x)|) where |HG(x)| is the hyper-graph's size (number of hyperedges)."
      ]
    },
    {
      "heading": "4. Variational vs. Min-Risk Decoding",
      "text": [
        "In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR):",
        "where l(y,y') represents the loss of y if the true answer is y', and the risk of y is its expected loss.",
        "Statistical decision theory shows MBR is optimal if p(y' | x) is the true distribution, while in practice p(y' | x) is given by a model at hand.",
        "We now observe that our variational decoding resembles the MBR decoding of Tromble et al.",
        "(2008).",
        "They use the following loss function, of which a linear approximation to BLEU (Papineni et al., 2001) is a special case, where w is an n-gram type, N is a set of n-gram types with n G [1, 4], Cw (y) is the number of occurrence of the n-gram w in y, and £w (y') is an indicator function to check if y' contains at least one occurrence of w. With the above loss function, Tromble et al.",
        "(2008) derive the MBR rule where g(w | x) is a specialized \"posterior\" probability of the n-gram w, and is defined as",
        "Now, let us divide N, which contains n-gram types of different n, into several subsets Wn, each of which contains only the n-grams with a given length n. We can now rewrite (19) as follows,",
        "Clearly, their rule (21) has a quite similar form to our rule (15), and we can relate (20) to (12) and (22) to (11).",
        "This justifies the use of interpolation in Section 3.2.3.",
        "However, there are several important differences.",
        "First, the n-gram \"posterior\" of (20) is very expensive to compute.",
        "In fact, it requires an intersection between each n-gram in the lattice and the lattice itself, as is done by Tromble",
        "l5The MBR becomes the MAP decision rule of (l) if a so-called zero-one loss function is used: l(y, y') = 0 if y = y'; otherwise l(y, y') = l.",
        "l6Note that Tromble et al.",
        "(2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hypergraph with spurious ambiguity.",
        "by assuming Ow = O|w| and, et al.",
        "(2008).",
        "In comparison, the optimal n-gram probabilities of (12) can be computed using the inside-outside algorithm, once and for all.",
        "Also, g(w | x) of (20) is not normalized over the history of w, while q*(r(w) | h(w)) of (12) is.",
        "Lastly, the definition of the n-gram model is different.",
        "While the model (11) is a proper probabilistic model, the function of (22) is simply an approximation of the average n-gram precisions of y.",
        "A connection between variational decoding and minimum-risk decoding has been noted before (e.g., Matsuzaki et al.",
        "(2005)), but the derivation above makes the connection formal.",
        "DeNero et al.",
        "(2009) concurrently developed an alternate to MBR, called consensus decoding, which is similar to ours in practice although motivated quite differently."
      ]
    },
    {
      "heading": "5. Experimental Results",
      "text": [
        "We report results using an open source MT toolkit, called Joshua (Li et al., 2009), which implements",
        "Hiero (Chiang, 2007).",
        "We work on a Chinese to English translation task.",
        "Our translation model was trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method based on the n-gram matches between training and test sets in the foreign side.",
        "We also used a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on a data set consisting of a 130M words in English Gigaparallel corpora.",
        "We use GIZA++ (Och and Ney, 2000), a suffix-array (Lopez, 2007), SRILM (Stol-cke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively.",
        "We use standard beam-pruning and cube-pruning parameter settings, following Chiang (2007), when generating the hypergraphs.",
        "The NIST MT'03 set is used to tune model weights (e.g. those of (16)) and the scaling factor",
        "Table 1: bleu scores for Viterbi, Crunching, MBR, and variational decoding.",
        "All the systems improve significantly over the Viterbi baseline (paired permutation test, p < 0.05).",
        "In each column, we boldface the best result as well as all results that are statistically indistinguishable from it.",
        "In MBR, K is the number of unique strings.",
        "For Crunching and Crunch-ing+MBR, N represents the number of derivations.",
        "On average, each string has about 115 distinct derivations.",
        "The variational method \"1to4gram+wp+vt\" is our full interpolation (16) of four variational n-gram models (\"1to4gram\"), the Viterbi baseline (\"vt\"), and a word penalty feature (\"wp\").",
        "Y of (3), and MT'04 and MT'05 are blind test-sets.",
        "We will report results for lowercase bleu-4, using the shortest reference translation in computing brevity penalty.",
        "Table 1 presents the bleu scores under Viterbi, crunching, MBR, and variational decoding.",
        "Both crunching and MBR show slight significant improvements over the Viterbi baseline; variational decoding gives a substantial improvement.",
        "The difference between MBR and Crunch-ing+MBR lies in how we approximate the distribution p(y' | x) in (17).",
        "For MBR, we take p(y' | x) to be proportional to pviterbi(y' | x) if y' is among the K best distinct strings on that measure, and 0 otherwise.",
        "For Crunching+MBR, we take p(y' | x) to be proportional to pcrunch(y' | x), which is based on the N best derivations.",
        "Table 2 presents the bleu results under different ways in using the variational models, as discussed in Section 3.2.3.",
        "As shown in Table 2a, decoding with a single variational n-gram model (VM) as per (14) improves the Viterbi baseline (except the case with a unigram VM), though often not statistically significant.",
        "Moreover, a bigram (i.e., \"2gram\") achieves the best bleu scores among the four different orders of VMs.",
        "The interpolation between a VM and a word penalty feature (\"wp\") improves over the unigram",
        "Viterbi",
        "35.4",
        "32.6",
        "MBR (K =1000)",
        "35.8",
        "32.7",
        "Crunching (N=10000)",
        "35.7",
        "32.8",
        "Crunching+MBR (N=10000)",
        "35.8",
        "32.7",
        "Variational (lto4gram+wp+vt)",
        "36.6",
        "33.5",
        "(d) interpolation of several n-gram VMs, the Viterbi model, and a word penalty feature",
        "Table 2: bleu scores under different variational decoders discussed in Section 3.2.3.",
        "A star * indicates a result that is significantly better than Viterbi decoding (paired permutation test, p < 0.05).",
        "We boldface the best system and all systems that are not significantly worse than it.",
        "The brevity penalty bp in bleu is always 1, meaning that on average y* is no shorter than the reference translation, except for the \"1gram\" systems in (a), which suffer from brevity penalties of 0.826 and 0.831.",
        "VM dramatically, but does not improve higherorder VMs (Table 2b).",
        "Adding the Viterbi feature (\"vt\") into the interpolation further improves the lower-order models (Table 2c), and all the improvements over the Viterbi baseline become statistically significant.",
        "At last, interpolation of several variational models does not yield much further improvement over the best previous model, but makes the results more stable (Table 2d).",
        "While the BLEu scores reported show the practical utility of the variational models, it is also interesting to measure how well each individual variational model q(y) approximates the distribution p(y | x).",
        "Ideally, the quality of approximation should be measured by the KL divergence KL(p || q) =f H(p, q) – H(p), where the cross-entropy H(p, q) = – £y p(y | x)log q(y), and",
        "Table 3: Cross-entropies H(p, q) achieved by various approximations q.",
        "The notation H denotes the sum of cross-entropies of all test sentences, divided by the total number of test words.",
        "A perfect approximation would achieve H(p), which we estimate using the true Hd (p) and a 10000-best list.",
        "the entropy H(p) =f – ^y p(y | x) logp(y | x).",
        "Unfortunately H(p) (and hence KL = H(p, q) – H(p)) is intractable to compute.",
        "But, since H(p) is the same for all q, we can simply use H(p, q) to compare different models q.",
        "Table 3 reports the cross-entropies H(p, q) for various models q.",
        "We also report the derivational entropy Hd(p) = – dP(d | x) logp(d | x).",
        "From this, we obtain an estimate of H(p) by observing that the \"gap\" Hd(p) – H(p) equals Ep(y)[H(d | y)], which we estimate from our 10000-best list.",
        "Table 3 confirms that higher-order variational models (drawn from a larger family Q) approximate p better.",
        "This is necessarily true, but it is interesting to see that most of the improvement is obtained just by moving from a unigram to a bigram model.",
        "Indeed, although Table 3 shows that better approximations can be obtained by using higher-order models, the best bleu score in Tables 2a and 2c was obtained by the bigram model.",
        "After all, p cannot perfectly predict the reference translation anyway, hence may not be worth approximating closely; but p may do a good job of predicting bigrams of the reference translation, and the bleu score rewards us for those."
      ]
    },
    {
      "heading": "6. Conclusions and Future Work",
      "text": [
        "We have successfully applied the general variational inference framework to a large-scale MT task, to approximate the intractable problem of MAp decoding in the presence of spurious ambiguity.",
        "We also showed that interpolating variational models with the Viterbi approximation can compensate for poor approximations, and that interpolating them with one another can reduce the Bayes risk and improve BLEU.",
        "Our empirical results improve the state of the art.",
        "as - Ed6D(x) p(d I x) log q(Y(d)).",
        "(a) decoding with a single variational model",
        "Decoding scheme",
        "MT'04",
        "MT'05",
        "Viterbi",
        "35.4",
        "32.6",
        "lgram",
        "25.9",
        "24.5",
        "2gram",
        "36.1",
        "33.4",
        "3gram",
        "36.0*",
        "33.l",
        "4gram",
        "35.8*",
        "32.9",
        "(b) interpolation between",
        "a single variational",
        "model and a word penalty feature",
        "lgram+wp",
        "29.7",
        "27.7",
        "2gram+wp",
        "35.5",
        "32.6",
        "3gram+wp",
        "36.l",
        "33.l",
        "4gram+wp",
        "35.7",
        "32.8",
        "(c) interpolation of a single variational model, the",
        "Viterbi model, and a word penalty feature",
        "lgram+wp+vt",
        "35.6",
        "32.8",
        "2gram+wp+vt",
        "36.5*",
        "33.5",
        "3gram+wp+vt",
        "35.8",
        "32.9",
        "4gram+wp+vt",
        "35.6",
        "32.8",
        "Measure",
        "H(p, •)",
        "Hd(p)",
        "H(p)",
        "bits/word",
        "qi",
        "MT'04",
        "2.33",
        "1.68 1.57",
        "1.53",
        "1.36",
        "1.03",
        "MT'05",
        "2.31",
        "1.69 1.58",
        "1.54",
        "1.37",
        "1.04",
        "Many interesting research directions remain open.",
        "To approximate the intractable MAp decoding problem of (2), we can use different variational distributions other than the n-gram model of (11).",
        "Interpolation with other models is also interesting, e.g., the constituent model in Zhang and Gildea (2008).",
        "We might also attempt to minimize KL(q || p) rather than KL(p || q), in order to approximate the mode (which may be preferable since we care most about the 1-best translation under p) rather than the mean of p (Minka, 2005).",
        "One could also augment our n-gram models with non-local string features (Rosenfeld et al., 2001) provided that the expectations of these features could be extracted from the hypergraph.",
        "Variational inference can also be exploited to solve many other intractable problems in MT (e.g., word/phrase alignment and system combination).",
        "Finally, our method can be used for tasks beyond MT.",
        "For example, it can be used to approximate the intractable MAp decoding inherent in systems using HMMs (e.g. speech recognition).",
        "It can also be used to approximate a context-free grammar with a finite state automaton (Nederhof, 2005)."
      ]
    }
  ]
}
