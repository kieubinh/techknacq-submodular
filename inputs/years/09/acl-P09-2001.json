{
  "info": {
    "authors": [
      "Shay B. Cohen",
      "Noah A. Smith"
    ],
    "book": "ACL-IJCNLP: Short Papers",
    "id": "acl-P09-2001",
    "title": "Variational Inference for Grammar Induction with Prior Knowledge",
    "url": "https://aclweb.org/anthology/P09-2001",
    "year": 2009
  },
  "references": [
    "acl-P04-1061",
    "acl-P06-1072",
    "acl-P92-1017"
  ],
  "sections": [
    {
      "text": [
        "Shay B. Cohen and Noah A. Smith",
        "Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA",
        "Variational EM has become a popular technique in probabilistic NLP with hidden variables.",
        "Commonly, for computational tractability, we make strong independence assumptions, such as the mean-field assumption, in approximating posterior distributions over hidden variables.",
        "We show how a looser restriction on the approximate posterior, requiring it to be a mixture, can help inject prior knowledge to exploit soft constraints during the variational E-step."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Learning natural language in an unsupervised way commonly involves the expectation-maximization (EM) algorithm to optimize the parameters of a generative model, often a probabilistic grammar (Pereira and Schabes, 1992).",
        "Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008).",
        "Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions.",
        "Bishop et al.",
        "(1998) showed how to use a less strict assumption: a mixture of factorized distributions.",
        "In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance.",
        "For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words.",
        "Graca et al.",
        "(2007) added linear constraints on expected values of features of the hidden variables in an alignment task.",
        "In this paper, we use posterior mixtures to inject bias or prior knowledge into a Bayesian model.",
        "We show that empirically, injecting prior knowledge improves performance on an unsupervised Chinese grammar induction task."
      ]
    },
    {
      "heading": "2. Variational Mixtures with Constraints",
      "text": [
        "Our EM variant encodes prior knowledge in an approximate posterior by constraining it to be from a mixture family of distributions.",
        "We will use x to denote observable random variables, y to denote hidden structure, and 9 to denote the to-be-learned parameters of the model (coming from a subset of for some £).",
        "a will denote the parameters of a prior over 6.",
        "The mean-field assumption in the Bayesian setting assumes that the posterior has a factored form:",
        "Traditionally, variational inference with the mean-field assumption alternates between an E-step which optimizes q(y) and then an M-step which optimizes q{9)} The mean-field assumption makes inference feasible, at the expense of optimizing a looser lower bound on the likelihood (Bishop, 2006).",
        "The lower bound that the algorithm optimizes is the following:",
        "where H(q) denotes the entropy of distribution q.",
        "We focus on changing the E-step and as a result, changing the underlying bound, F(q(6, y), a).",
        "Similarly to Bishop et al.",
        "(1998), instead of making the strict mean-field assumption, we assume that the variational model is a mixture.",
        "One component of the mixture might take the traditional form, but others will be used to encourage certain",
        "!This optimization can be nested inside another EM algorithm that optimizes a; this is our approach.",
        "q(0) is traditionally conjugate to the likelihood for computational reasons, but our method is not limited to that kind of prior, as seen in the experiments.",
        "tendencies considered a priori to be appropriate.",
        "Denoting the probability simplex of dimension r Ar = {<Ai,...,Ar> S Kr : Aj > 0, £[=1 h = 1}, we require that:",
        "for A e Ar.",
        "Qj will denote the family of distributions for the ith mixture component, and Q( Ar) will denote the family implied by the mixture of Qi,..., Qr where the mixture coefficients A e Ar.",
        "A comprise r additional variational parameters, in addition to parameters for each qi(y) and <fe(0).",
        "When one of the mixture components Qi is sufficiently expressive, A will tend toward a degenerate solution.",
        "In order to force all mixture components to play a role – even at the expense of the tightness of the variational bound – we will impose hard constraints on A: A e Arc Ar.",
        "In our experiments (§3), Ar will be mostly aline segment corresponding to two mixture coefficients.",
        "The role of the variational EM algorithm is to optimize the variational bound in Eq.",
        "2 with respect to q(y), q(0), and A.",
        "Keeping this intention in mind, we can replace the E-step and M-step in the original variational EM algorithm with 2r + 1 coordinate ascent steps, for 1 < i < r:",
        "M-step: For each i e {l,...,r}, optimize the bound given A and <?i'(^)li/e{i,...,r}\\{i} and Qi'(y)\\i'e{i r} by selecting a new distribution <l:i<>).",
        "C-step: Optimize the bound by selecting a new set of coefficients A e Ar in order to optimize the bound with respect to the mixture coefficients.",
        "We call the revised algorithm constrained mixture variational EM.",
        "For a distribution r(h), we denote by KL(Qj||r) the following:",
        "where KL( || ) denotes the Kullback-Leibler divergence.",
        "The next proposition, which is based on a result in Graca et al.",
        "(2007), gives an intuition of how modifying the variational EM algorithm with Q = Q( Ar) affects the solution:",
        "Proposition 1.",
        "Constrained mixture variational EM finds local maxima for a function G(q,ex) such that where L(A, at) = ^ AiKL(Q»\\\\p(6, y | x, ex.",
        ")).",
        "We can understand mixture variational EM as penalizing the likelihood with a term bounded by a linear function of the A, minimized over Ar.",
        "We will exploit that bound in §2.2 for computational tractability.",
        "The variational EM algorithm still identifies only local maxima.",
        "Different proposals have been for pushing EM toward a global maximum.",
        "In many cases, these methods are based on choosing different initializations for the EM algorithm (e.g., repeated random initializations or a single carefully designed initializer) such that it eventually gets closer to a global maximum.",
        "We follow the idea of annealing proposed in Rose et al.",
        "(1990) and Smith and Eisner (2006) for the A by gradually loosening hard constraints on A as the variational EM algorithm proceeds.",
        "We define a sequence of Ar(i) for t = 0,1,... such that Ar(i) C Ar(t + 1).",
        "First, we have the inequality:",
        "We say that the annealing schedule is r-separated if we have for any a :",
        "T-separation requires consecutive families Q(Ar(t)) and Q(Ar(t + 1)) to be similar.",
        "Proposition 1 stated the bound we optimize, which penalizes the likelihood by subtracting a positive KL divergence from it.",
        "With the r-separation condition we can show that even though we penalize likelihood, the variational EM algorithm will still increase likelihood by a certain amount.",
        "Full details are omitted for space and can be found in ?",
        ").",
        "Input: initial parameters a( ', observed data x,",
        "annealing schedule Ar : N – > 2ArOutput: learned parameters a and approximate posterior q(0,y) E-step: repeat E-step: forall i e [r] do: qf+i\\y) < – argmax",
        "t<-t + l; until convergence ;",
        "We now turn to further alterations of the bound in Eq.",
        "2 to make it more tractable.",
        "The main problem is the entropy term which is not easy to compute, because it includes a log term over a mixture of distributions from Qj.",
        "We require the distributions in Qi to factorize over the hidden structure y, but this only helps with the first term in Eq.",
        "2.",
        "We note that because the entropy function is convex, we can get a lower bound on H(q):",
        "Substituting the modified entropy term into Eq.",
        "2 still yields a lower bound on the likelihood.",
        "This change makes the E-step tractable, because each distribution <?j(y) can be computed separately by optimizing a bound which depends only on the variational parameters in that distribution.",
        "In fact, the bound on the left hand side in Proposition 1 becomes the function that we optimize instead of G(q, a).",
        "Without proper constraints, the A update can be intractable as well.",
        "It requires maximizing a linear objective (in A) while constraining the A to be from a particular subspace of the probability simplex, Ar(i).",
        "To solve this issue, we require that Ar(i) is polyhedral, making it possible to apply linear programming (Boyd and Vandenberghe, 2004).",
        "with A G Ar(tflnal) and (&(0,y)) G Qj.",
        "The algorithm for optimizing this bound is in Fig. 1, which includes an extra M-step to optimize ol (see extended report)."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "We tested our method on the unsupervised learning problem of dependency grammar induction.",
        "For the generative model, we used the dependency model with valence as it appears in Klein and Manning (2004).",
        "We used the data from the Chinese treebank (Xue et al., 2004).",
        "Following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure, and sentences of length more than 10 were removed from the set.",
        "We experimented with a Dirichlet prior over the parameters and logistic normal priors over the parameters, and found the latter to still be favorable with our method, as in Cohen et al.",
        "(2008).",
        "We therefore report results with our method only for the logistic normal prior.",
        "We do inference on sections 1-270 and 301-1151 of CTB10 (4,909 sentences) by running the EM algorithm for 20 iterations, for which all algorithms have their variational bound converge.",
        "To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard (attachment accuracy).",
        "For parsing, we use the minimum Bayes risk parse.",
        "Our mixture components Qj are based on simple linguistic tendencies of Chinese syntax.",
        "These observations include the tendency of dependencies to (a) emanate from the right of the current position and (b) connect words which are nearby (in string distance).",
        "We experiment with six mixture components: (1) RightAttach: Each word's parent is to the word's right.",
        "The root, therefore, is always the rightmost word; (2) ALLRlGHT: The rightmost word is the parent of all positions in the sentence (there is only one such tree); (3) Left-Chain: The tree forms a chain, such that each",
        "Table 1: Results (attachment accuracy).",
        "The baselines are LeftChain as a parsing model (attaches each word to the word on its right), non-Bayesian em, and mean-field variational em without any constraints.",
        "These are compared against the six mixture components mentioned in the text.",
        "(I) corresponds to simplex annealing experiments (A^' = 0.85); (II – III) correspond to fixed values, 0.85 and 0.95, for the mixture coefficients.",
        "With the last row, a2 to A4 are always (f – Ai)/3.",
        "Boldface denotes the best result in each row.",
        "word is governed by the word to its right; (4) ver-bAsRoot: Only verbs can attach to the wall node $; (5) NounSequence: Every sequence of n NN (nouns) is assumed to be a noun phrase, hence the first n – 1 NNs are attached to the last NN; and (6) shortdep: Allow only dependencies of length four or less.",
        "This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006).",
        "These components are added to a variational DMV model without the sum-to-1 constraint on 6.",
        "This complements variational techniques which state that the optimal solution during the E-step for the mean-field variational EM algorithm is a weighted grammar of the same form of p(x, y | 0) (DMV in our case).",
        "Using the mixture components this way has the effect of smoothing the estimated grammar event counts during the E-step, in the direction of some prior expectations.",
        "Let Ai correspond to the component of the original DMV model, and let A2 correspond to one of the components from the above list.",
        "Variational techniques show that if we let Ai obtain the value 1, then the optimal solution will be Ai = 1 and A2 = 0.",
        "We therefore restrict Ai to be smaller than 1.",
        "More specifically, we use an annealing process which starts by limiting Aitobe<s = 0.85 (and hence limits A2 to be > 0.15) and increases s at each step by 1% until s reaches 0.95.",
        "In addition, we also ran the algorithm with Ai fixed at 0.85 and Ai fixed at 0.95 to check the effectiveness of annealing on the simplex.",
        "ponents has a clear advantage over the mean-field assumption.",
        "The best result with a single mixture is achieved with annealing, and the VerbAsRoot component.",
        "A combination of the mixtures (RightAttach) together with VerbAsRoot and ShortDep led to an additional improvement, implying that proper selection of several mixture components together can achieve a performance gain."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "We described a variational EM algorithm that uses a mixture model for the variational model.",
        "We refined the algorithm with an annealing mechanism to avoid local maxima.",
        "We demonstrated the effectiveness of the algorithm on a dependency grammar induction task.",
        "Our results show that with a good choice of mixture components and annealing schedule, we achieve improvements for this task over mean-field variational inference.",
        "LeftChain",
        "34.9",
        "vanilla em",
        "38.3",
        "LN, mean-field",
        "48.9",
        "This paper:",
        "I",
        "II",
        "III",
        "RightAttach",
        "49.1",
        "47.1",
        "49.8",
        "AllRight",
        "49.4",
        "49.4",
        "48.4",
        "'c",
        "LeftChain",
        "47.9",
        "46.5",
        "49.9",
        "VerbAsRoot",
        "50.5",
        "50.2",
        "49.4",
        "NounSequence",
        "48.9",
        "48.9",
        "49.9",
        "ShortDep",
        "49.5",
        "48.4",
        "48.4",
        "RA+VAR+SD",
        "50.5",
        "50.6",
        "50.1"
      ]
    }
  ]
}
