{
  "info": {
    "authors": [
      "Furu Wei",
      "Wenjie Li",
      "Yanxiang He"
    ],
    "book": "ACL-IJCNLP: Short Papers",
    "id": "acl-P09-2030",
    "title": "Co-Feedback Ranking for Query-Focused Summarization",
    "url": "https://aclweb.org/anthology/P09-2030",
    "year": 2009
  },
  "references": [
    "acl-C00-1072",
    "acl-N03-1020"
  ],
  "sections": [
    {
      "text": [
        "Furu Wei'' Wenjie Li and Yanxiang He",
        "The Hong Kong Polytechnic University, Hong Kong Wuhan University, China",
        "{csfwei,cswjli}@comp.polyu.edu.hk {frwei,yxhe}@whu.edu.cn",
        "IBM China Research Laboratory, Beijing, China",
        "In this paper, we propose a novel ranking framework - Co-Feedback Ranking (Co-FRank), which allows two base rankers to supervise each other during the ranking process by providing their own ranking results as feedback to the other parties so as to boost the ranking performance.",
        "The mutual ranking refinement process continues until the two base rankers cannot learn from each other any more.",
        "The overall performance is improved by the enhancement of the base rankers through the mutual learning mechanism.",
        "We apply this framework to the sentence ranking problem in query-focused summarization and evaluate its effectiveness on the DUC 2005 data set.",
        "The results are promising."
      ]
    },
    {
      "heading": "1. Introduction and Background",
      "text": [
        "Sentence ranking is the issue of most concern in extractive summarization.",
        "Feature-based approaches rank the sentences based on the features elaborately designed to characterize the different aspects of the sentences.",
        "They have been extensively investigated in the past due to their easy implementation and the ability to achieve promising results.",
        "The use of feature-based ranking has led to many successful (e.g. top five) systems in DUC 2005-2007 query-focused summarization (Over et al., 2007).",
        "A variety of statistical and linguistic features, such as term distribution, sentence length, sentence position, and named entity, etc., can be found in literature.",
        "Among them, query relevance, centroid (Radev et al., 2004) and signature term (Lin and Hovy, 2000) are most remarkable.",
        "There are two alternative approaches to integrate the features.",
        "One is to combine features into a unified representation first, and then use it to rank the sentences.",
        "The other is to utilize rank fusion or rank aggregation techniques to combine the ranking results (orders, ranks or scores) produced by the multiple ranking functions into a unified rank.",
        "The most popular implementation of the latter approaches is to linearly combine the features to obtain an overall score which is then used as the ranking criterion.",
        "The weights of the features are either experimentally tuned or automatically derived by applying learning-based mechanisms.",
        "However, both of the above-mentioned \"combine-then-rank\" and \"rank-then-combine\" approaches have a common drawback.",
        "They do not make full use of the information provided by the different ranking functions and neglect the interaction among them before combination.",
        "We believe that each individual ranking function (we call it base ranker) is able to provide valuable information to the other base rankers such that they learn from each other by means of mutual ranking refinement, which in turn results in overall improvement in ranking.",
        "To the best of our knowledge, this is a research area that has not been well addressed in the past.",
        "The inspiration for the work presented in this paper comes from the idea of Co-Training (Blum and Mitchell, 1998), which is a very successful paradigm in the semi-supervised learning framework for classification.",
        "In essence, co-training employs two weak classifiers that help augment each other to boost the performance of the learning algorithms.",
        "Two classifiers mutually cooperate with each other by providing their own labeling results to enrich the training data for the other parties during the supervised learning process.",
        "Analogously, in the context of ranking, although each base ranker cannot decide the overall ranking well on itself, its ranking results indeed reflect its opinion towards the ranking from its point of view.",
        "The two base rankers can then share their own opinions by providing the ranking results to each other as feedback.",
        "For each ranker, the feedback from the other ranker contains additional information to guide the refinement of its ranking results if the feedback is defined and used appropriately.",
        "This process continues until the two base rankers can not learn from each other any more.",
        "We call this ranking paradigm Co-Feedback Ranking (Co-FRank).",
        "The way how to use the feedback information varies depending on the nature of a ranking task.",
        "In this paper, we particularly consider the task of query-focused summarization.",
        "We design a new sentence ranking algorithm which allows a query-dependent ranker and a query-independent ranker mutually learn from each other under the Co-FRank framework."
      ]
    },
    {
      "heading": "2. Co-Feedback Ranking for Query-Focused Summarization",
      "text": [
        "Given a set of objects O, one can define two base ranker f and f2.",
        "f1 (o) -> 3?, f2 (o) -> 3?, Vo e O .",
        "The ranking results produced by f and f individually are by no means perfect but the two rankers can provide relatively reasonable ranking information to supervise each other so as to jointly improve themselves.",
        "One way to do Co-Feedback ranking is to take the most confident ranking results (e.g. highly ranked instances based on orders, ranks or scores) from one base ranker as feedback to update the other's ranking results, and vice versa.",
        "This process continues iteratively until the termination condition is reached, as depicted in Procedure 1.",
        "While the standard Co-Training algorithm requires two sufficient and redundant views, we suggest f1 and f2 be two independent rankers which emphasize two different aspects of the objects in O.",
        "1: Rank O withf and obtain the ranking results r1; 2: Rank O withf and obtain the ranking results r2; 3: Repeat",
        "4: Select the top N ranked objects t1 from r1 as feedback to supervise f2, and re-rank O using f2 and t1 ; Update r2;",
        "5: Select the top N ranked objects t2 from r2 as feedback to supervise f1, and re-rank O usingf and t2 ; Update r1;",
        "The termination condition I(O) can be defined according to different application scenarios.",
        "For example, I(O) may require the top K ranked objects in r1 and r2 to be identical if one is particularly interested in the top ranked objects.",
        "It is also very likely that r1 and r2 do not change any more after several iterations (or the top K objects do not change).",
        "In this case, the two base rankers can not learn from each other any more, and the Co-Feedback ranking process should terminate either.",
        "The final ranking results can be easily determined by combining the two base rankers without any parameter, because they have already learnt from each other and can be equally treated.",
        "on Co-FRank",
        "The task of query-focused summarization is to produce a short summary (250 words in length) for a set of related documents D with respect to the query q that reflects a user's information need.",
        "We follow the traditional extractive summarization framework in this study, where the two critical processes involved are sentence ranking and sentence selection, yet we focus more on the sentence ranking algorithm based on Co-FRank.",
        "As for sentence selection, we incrementally add into the summary the highest ranked sentence if it doesn't significantly repeatthe information already included in the summary until the word limitation is reached.",
        "In the context of query-focused summarization, two kinds of features, i.e. query-dependent and query-independent features are necessary and they are supposed to complement each other.",
        "We then use these two kinds of features to develop the two base rankers.",
        "The query-dependent feature (i.e. the relevance of the sentence s to the query q) is defined as the cosine similarity between s and q.",
        "The words in s and q vectors are weighted by tf*isf Meanwhile, the query-independent feature (i.e. the sentence significance based on word centroid) is defined as where c(w) is the centroid weight of the word w in s and c(w) = zseD (f â–  isfw )/ND .",
        "nd is the total number of the sentences in D, tf ws is the frequency of w in s, and isfw = log(NDs /sfw) is the inverse sentence frequency (ISF) of w, where sfw is the sentence frequency of w in D. The sentence ranking algorithm based on Co-FRank is detailed in the following Algorithm 1.",
        "2: Rank S with f1 and obtain the ranking results r1; 3: Rank S with f2 and obtain the ranking results r2; 7: Select the top N ranked sentences at round n t\" from r1 as feedback for f2, and re-rank S using f2 and T\",",
        "from r2 as feedback for f1, and re-rank S using f1 and t\" ;",
        "nk)<-V f1(si) + (1-v)^1(si) (4)9: Until the top K sentences in r1 and r2 are the same, both r1 and r2 do not change any more, or maximum iteration round is achieved.",
        "10: Calculate the final ranking results,",
        "The update strategies used in Algorithm 1, as formulated in Formulas (3) and (4), are designed based on the intuition that the new ranking of the sentence s from one base ranker (say f1) consists of two parts.",
        "The first part is the initial ranking produced by f1.",
        "The second part is the similarity between s and the top N feedback provided by the other ranker (say f2), and vice versa.",
        "The top K ranked sentences by f are supposed to be highly supported by f2.",
        "As a result, a sentence that is similar to those top ranked sentences should deserve a high rank as well.",
        "sim(s.Tn) captures the effect of such feedback at round n and the definition of it may vary with regard to the application background.",
        "For example, it can be defined as the maximum, the minimum or the average similarity value between si and a set of feedback sentences in t2 .",
        "Through this mutual interaction, the two base rankers supervise each other and are expected as a whole to produce more reliable ranking results.",
        "We assume each base ranker is most confident with its first ranked sentence and set N to 1.",
        "Accordingly, sim(s. ,Tn )is defined as the similarity between si and the one sentence in Tn.",
        "n is a balance factor which can be viewed as the proportion of the dependence of the new ranking results on its initial ranking results.",
        "K is set to 10 as 10 sentences are basically sufficient for the summarization task we work on.",
        "We carry out at most 5 iterations in the current implementation.",
        "Experimental Study",
        "We take the DUC 2005 data set as the evaluation corpus in this preliminary study.",
        "ROUGE (Lin and Hovy, 2003), which has been officially adopted in the DUC for years is used as the evaluation criterion.",
        "For the purpose of comparison, we implement the following two basic ranking functions and the linear combination of them for reference, i.e. the query relevance based ranker (denoted by QRR, same as f1) and the word centroid based ranker (denoted by WCR, same as f2), and the linear combined ranker, LCR= X QRR+(1- X )WCR, where X is a combination parameter.",
        "QRR and WCR are normalized by (x - min)/(max- min) , where x, max and min denote the original ranking score, the maximum ranking score and minimum ranking score produced by a ranker, respectively.",
        "Table 1 shows the results of the average recall scores of ROUGE-1, ROUGE-2 and ROUGE-SU4 along with their 95% confidence intervals included within square brackets.",
        "Among them, ROUGE-2 is the primary DUC evaluation criterion.",
        "* The worst results produced by LCR when X = 0.1 + The worst results produced by Co-FRank when n = 0.6 ** The best results produced by LCR when X = 0.4 ++ The best results produced by Co-FRank when n = 0.8",
        "Note that the improvement of LCR over QRR and WCR is rather significant if the combination parameter X is selected appropriately.",
        "Besides, Co-FRank is always superior to LCR regardless of the best or the worst ouput, and the improvement is visible.",
        "The reason is that both QRR and WCR are enhanced step by step in Co-FRank, which in turn results in the increased overall performance.",
        "The trend of the improvement has been clearly observed in the experiments.",
        "This observation validates our motivation and the rationality of the algorithm proposed in this paper and motivates our further investigation on this topic.",
        "We continue to examine the parameter settings in LCR and Co-FRank.",
        "Table 2 shows the results of LCR when the value of X changes from 0.1 to 1.0, and Table 3 shows the results of Co-FRank with rj ranging from 0.5 to 0.9.",
        "Notice that rj is not a combination parameter.",
        "We believe that a base ranker should have at least half belief in its initial ranking results and thus the value of the rj should be greater than 0.5.",
        "We find that LCR heavily depends on X. LCR produces relatively good and stable results with X varying from 0.4 to 0.6.",
        "However, the ROUGE scores drop apparently when X heading towards its two end values, i.e. 0.1 and 1.0.",
        "As shown in Table 3, the Co-FRank can always produce stable and promising results regardless of the change of rj.",
        "More important, even the worst result produced by Co-FRank still outperforms the best result produced by LCR.",
        "We then compare our results to the DUC participating systems.",
        "We present the following representative ROUGE results of (1) the top three DUC participating systems according to ROUGE-2 scores (S15, S17 and S10); and (2) the NIST baseline which simply selects the first sentences from the documents.",
        "It is clearly shown in Table 4 that Co-FRank can produce a very competitive result, which significantly outperforms the NIST baseline and meanwhile it is superior to the best participating system in the DUC 2005."
      ]
    },
    {
      "heading": "4. Conclusion and Future Work",
      "text": [
        "In this paper, we propose a novel ranking framework, namely Co-Feedback Ranking (Co-FRank), and examine its effectiveness in query-focused summarization.",
        "There is still a lot of work to be done on this topic.",
        "Although we show the promising achievements of Co-Frank from the perspective of experimental studies, we expect a more theoretical analysis on Co-FRank.",
        "Meanwhile, we would like to investigate more appropriate techniques to use feedback, and we are interested in applying Co-FRank to the other applications, such as opinion summarization where the integration of opinion-biased and document-biased ranking is necessary."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The work described in this paper was supported by the Hong Kong Polytechnic University internal the grants (G-YG80 and G-YH53) and the China NSF grant (60703008).",
        "X",
        "ROUGE-l",
        "ROUGE-2",
        "ROUGE-SU4",
        "O.l",
        "O.35l3",
        "O.O645",
        "O.ll77",
        "[O.3449, O.3572]",
        "[O.O6l3, O.O676]",
        "[O.ll45, O.l2O9]",
        "O.2",
        "O.3623",
        "O.O699",
        "O.l235",
        "[O.3559, O.3685]",
        "[O.O662, O.O736]",
        "[O.ll97, O.l27l]",
        "O.3",
        "O.372l",
        "O.O74l",
        "O.l28l",
        "[O.366O, O.3778]",
        "[O.O7O6, O.O778]",
        "[O.l246, O.l3l8]",
        "O.4",
        "O.3753",
        "O.O757",
        "O.l3O2",
        "[O.3692, O.38l3]",
        "[O.O7l9, O.O796]",
        "[O.l265, O.l34O]",
        "O.5",
        "O.3756",
        "O.O755",
        "O.l3O7",
        "[O.3698, O.38l4]",
        "[O.O7l7, O.O793]",
        "[O.l272, O.l342]",
        "O.6",
        "O.377O",
        "O.O754",
        "O.l323",
        "[O.37lO, O.3826]",
        "[O.O7l6, O.O79l]",
        "[O.l286, O.l357]",
        "O.7",
        "O.3698",
        "O.O7l8",
        "O.l284",
        "[O.3636, O.3759]",
        "[O.O68O, O.O756]",
        "[O.l246, O.l3l8]",
        "O.8",
        "O.3672",
        "O.O7O6",
        "O.l27l",
        "[O.36l3, O.373O]",
        "[O.O669, O.O743]",
        "[O.l234, O.l3O5]",
        "O.9",
        "O.365l",
        "O.O689",
        "O.l258",
        "[O.359l, O.37O8]",
        "[O.O652, O.O726]",
        "[O.l22O, O.l293]",
        "r",
        "ROUGE-l",
        "ROUGE-2",
        "ROUGE-SU4",
        "O.5",
        "O.375O",
        "O.O766",
        "O.l3O8",
        "[O.3687, O.38lO]",
        "[O.O727, O.O8O4]",
        "[O.l27O, O.l344]",
        "O.6",
        "O.3769",
        "O.O762",
        "O.l3l7",
        "[O.37l2, O.3829]",
        "[O.O724, O.O799]",
        "[O.l282, O.l35l]",
        "O.7",
        "O.3775",
        "O.O763",
        "O.l3l9",
        "[O.37l3, O.3835]",
        "[O.O724, O.O8Ol]",
        "[O.l282, O.l354]",
        "0.8",
        "0.3783",
        "0.0775",
        "0.1323",
        "[0.3719, 0.3852]",
        "[0.0733, 0.0810]",
        "[0.1293, 0.1360]",
        "O.9",
        "O.3779",
        "O.O765",
        "O.l3l9",
        "[O.3722, O.3835]",
        "[O.O728, O.O8O3]",
        "[O.l285, O.l354"
      ]
    }
  ]
}
