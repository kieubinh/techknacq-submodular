{
  "info": {
    "authors": [
      "Minwoo Jeong",
      "Chin-Yew Lin",
      "Gary Geunbae Lee"
    ],
    "book": "ACL-IJCNLP: Short Papers",
    "id": "acl-P09-2071",
    "title": "Efficient Inference of CRFs for Large-Scale Natural Language Data",
    "url": "https://aclweb.org/anthology/P09-2071",
    "year": 2009
  },
  "references": [
    "acl-N03-1028",
    "acl-P06-2054"
  ],
  "sections": [
    {
      "text": [
        "Minwoo Jeong** Chin-Yew Lin* Gary Geunbae Lee*",
        "tPohang University of Science & Technology, Pohang, Korea ^Microsoft Research Asia, Beijing, China",
        "This paper presents an efficient inference algorithm of conditional random fields (CRFs) for large-scale data.",
        "Our key idea is to decompose the output label state into an active set and an inactive set in which most unsupported transitions become a constant.",
        "Our method unifies two previous methods for efficient inference of CRFs, and also derives a simple but robust special case that performs faster than exact inference when the active sets are sufficiently small.",
        "We demonstrate that our method achieves dramatic speedup on six standard natural language processing problems."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Conditional random fields (CRFs) are widely used in natural language processing, but extending them to large-scale problems remains a significant challenge.",
        "For simple graphical structures (e.g. linear-chain), an exact inference can be obtained efficiently if the number of output labels is not large.",
        "However, for large number of output labels, the inference is often prohibitively expensive.",
        "To alleviate this problem, researchers have begun to study the methods of increasing inference speeds of CRFs.",
        "Pal et al.",
        "(2006) proposed a Sparse Forward-Backward (SFB) algorithm, in which marginal distribution is compressed by approximating the true marginals using Kullback-Leibler (KL) divergence.",
        "Cohn (2006) proposed a Tied Potential (TP) algorithm which constrains the labeling considered in each feature function, such that the functions can detect only a relatively small set of labels.",
        "Both of these techniques efficiently compute the marginals with a significantly reduced runtime, resulting in faster training and decoding of CRFs.",
        "This paper presents an efficient inference algorithm of CRFs which unifies the SFB and TP approaches.",
        "We first decompose output labels states into active and inactive sets.",
        "Then, the active set is selected by feasible heuristics and the parameters of the inactive set are held a constant.",
        "The idea behind our method is that not all of the states contribute to the marginals, that is, only a",
        "*Parts of this work were conducted during the author's internship at Microsoft Research Asia.",
        "small group of the labeling states has sufficient statistics.",
        "We show that the SFB and the TP are special cases of our method because they derive from our unified algorithm with a different setting of parameters.",
        "We also present a simple but robust variant algorithm in which CRFs efficiently learn and predict large-scale natural language data."
      ]
    },
    {
      "heading": "2. Linear-chain CRFs",
      "text": [
        "Many versions of CRFs have been developed for use in natural language processing, computer vision, and machine learning.",
        "For simplicity, we concentrate on linear-chain CRFs (Lafferty et al., 2001; Sutton and McCallum, 2006), but the generic idea described here can be extended to CRFs of any structure.",
        "Linear-chain CRFs are conditional probability distributions over label sequences which are conditioned on input sequences (Lafferty et al., 2001).",
        "Formally, x = {xt}J=1 and y = {yt}J=i are sequences of input and output variables.",
        "Respectively, where T is the length of sequence, xt G X and yt ey where X is the finite set of the input observations and y is that of the output label state space.",
        "Then, a first-order linear-chain CRF is defined as:",
        "where *t is the local potential that denotes the factor at time t, and A is the parameter vector.",
        "Z(x) is a partition function which ensures the probabilities of all state sequences sum to one.",
        "We assume that the potentials factorize according to a set of observation features {4>l} and transition features {4>k}, as follows:",
        "where {X\\} and {A|} are weight parameters which we wish to learn from data.",
        "Inference is significantly challenging both in learning and decoding CRFs.",
        "Time complexity is 0(T\\y\\) for exact inference (i.e., forward-backward and Viterbi algorithm) of linear-chain CRFs (Lafferty et al., 2001).",
        "The inference process is often prohibitively expensive when \\y\\ is large, as is common in large-scale tasks.",
        "This problem can be alleviated by introducing approximate inference methods based on reduction of the search spaces to be explored."
      ]
    },
    {
      "heading": "3. Efficient Inference Algorithm 3 .1 Method",
      "text": [
        "The key idea of our proposed efficient inference method is that the output label state y can be decomposed to an active set A and an inactive set Ac.",
        "Intuitively, many of the possible transitions {yt-i – > yt) do not occur, or are unsupported, that is, only a small part of the possible labeling set is informative.",
        "The inference algorithm need not precisely calculate marginals or maximums (more generally, messages) for unsupported transitions.",
        "Our efficient inference algorithm approximates the unsupported transitions by assigning them a constant value.",
        "When \\A\\ < \\y\\, both training and decoding times are remarkably reduced by this approach.",
        "We first define the notation for our algorithm.",
        "Let Ai be the active set and A\\ be the inactive set of output label i where ^ = Ai U AI.",
        "We define Ai as:",
        "where S is a criterion function of transitions {yt-i – > yt) and e is a hyperparameter.",
        "For clarity, we define the local factors as:",
        "Note that we can ignore the subscript t at {yt-i = j, yt = i) by defining an HMM-like model, that is, transition matrix 4 is independent of t.",
        "As exact inference, we use the forward-backward procedure to calculate marginals (Sutton and McCal-lum, 2006).",
        "We formally describe here an efficient calculation of a and f3 recursions for the forward-backward procedure.",
        "The forward value at{i) is the sum of the unnormalized scores for all partial paths that start at t = 0 and converge at yt = i at time t. The backward value f3t(i) similarly defines the sum of unnormalized scores for all partial paths that start at time t + 1 with state yt+i = j and continue until the end of the sequences, t = T + 1.",
        "Then, we decompose the equations of exact a and f3 recursions as follows:",
        "ieAj iey",
        "where uj is a shared transition parameter value for set A^, that is, = ivif j e A\\.",
        "Note that 2^ at(i) = 1 (Sutton and McCallum, 2006).",
        "Because all unsupported transitions in AI are calculated simultaneously, the complexities of Eq.",
        "(8) and (9) are approximately 0{T\\Aavg\\\\y\\) where \\ AavgI is the average number of states in the active set, i.e., ^ J2t=i IA I-The worst case complexity of our a and f3 equations is 0(T\\y\\).",
        "Similarly, we decompose a 7 recursion for the Viterbi algorithm as follows:",
        "where jt(i) is the sum of unnormalized scores for the best-scored partial path that starts at time t = 0 and converges at yt = i at time t. Because uj is constant, maxjey 7t-i(j) can be pre-calculated at time t – 1.",
        "By analogy with Eq.",
        "(8) and (9), the complexity is approximately 0(T\\ Aavg \\\\y\\).",
        "To implement our inference algorithm, we need a method of choosing appropriate values for the setting function 5 of the active set and for the constant value uj of the inactive set.",
        "These two problems are closely related.",
        "The size of the active set affects both the complexity of inference algorithm and the quality of the model.",
        "Therefore, our goal for selecting 5 and uj is to make a plausible assumption that does not sacrifice much accuracy but speeds up when applying large state tasks.",
        "We describe four variant special case algorithms.",
        "Method 1: We set 6(i,j) = Z(L) and uj = 0 where L is a beam set, L = {h, Z2,..., lm} and the subpartition function Z{L) is approximated by Z{L) « at-iU)- m tms method, all sub-marginals in the inactive set are totally excluded from calculation of the current marginal, a and f3 in the inactive sets are set to 0 by default.",
        "Therefore, at each time step t the algorithm prunes all states i in which at{i) < e. It also generates a subset L of output labels that will be exploited in next time step t + l. This method has been derived theoretically from the process of selecting a compressed marginal distribution within a fixed KL divergence of the true marginal (Pal et al., 2006).",
        "This method most closely resembles SFB algorithm; hence we refer an alternative of SFB.",
        "Method 2: We define S(iJ) = 1*^ -11 and w = 1.",
        "In practice, unsupported transition features are not parameterized; this means that Afc = 0 and = 1 if j G AI.",
        "Thus, this method estimates nearly-exact",
        "practice, dynamically selecting L increases the number of computations, and this is the main disadvantage of Method 1.",
        "However, in inactive sets at-i(j) = 0 by default; hence, we need not calculate /3t-i(j).",
        "Therefore, it counterbalances the extra computations in /?",
        "recursion.",
        "CRFs if the hyperparameter is e = 0; hence this criterion does not change the parameter.",
        "Although this method is simple, it is sufficiently efficient for training and decoding CRFs in real data.",
        "Method 3: We define 6(i,j) = Ep(4>\\{i,j)) where Ep(z) is an empirical count of event z in training data.",
        "We also assign a real value for the inactive set, i.e., uj = c e R, c^O, 1.",
        "The value c is estimated in the training phase; hence, c is a shared parameter for the inactive set.",
        "This method is equivalent to TP (Cohn, 2006).",
        "By setting e larger, we can achieve faster inference, a tradeoff exists between efficiency and accuracy.",
        "Method 4: We define the shared parameter as a function of output label y in the inactive set, i.e., c{y).",
        "As in Method 3, c{y) is estimated during the training phase.",
        "When the problem expects different aspects of unsupported transitions, this method would be better than using only one parameter c for all labels in inactive set."
      ]
    },
    {
      "heading": "4. Experiment",
      "text": [
        "We evaluated our method on six large-scale natural language data sets (Table 1): Penn Treebankfor part-of-speech tagging (PTB), phrase chunking data (CoNLLOO), named entity recognition data (CoNLL03), grapheme-to-phoneme conversion data (NetTalk), spoken language understanding data (Communicator) (Jeong and Lee, 2006), and finegrained named entity recognition data (Encyclopedia) (Lee et al., 2007).",
        "The active set is sufficiently small in Communicator and Encyclopedia despite their large numbers of output labels.",
        "In all data sets, we selected the current word, ±2 context words, bigrams, trigrams, and prefix and suffix features as basic feature templates.",
        "A template of part-of-speech tag features was added for CoNLLOO, CoNLL03, and Encyclopedia.",
        "In particular, all tasks except PTB and NetTalk require assigning a label to a phrase rather than to a word; hence, we used standard \"BIO\" encoding.",
        "We used un-normalized log-likelihood, accuracy and training/decoding times as our evaluation measures.",
        "We did not use cross validation and development set for tuning the parameter because our goal is to evaluate the efficiency of inference algorithms.",
        "Moreover, using the previous state-of-the-art features we expect the achievement of better accuracy.",
        "All our models were trained until parameter estimation converged with a Gaussian prior variance of 4.",
        "During training, a pseudo-likelihood parameter estimation (Sutton and McCallum, 2006) was used as an initial weight (estimated in 30 iterations).",
        "We used complete and dense input/output joint features for dense model (Dense), and only supported features that are used at least once in the training examples for sparse",
        "form better, the sparse model performs well in practice without significant loss of accuracy (Sha and Pereira, 2003).",
        "Table 1: Data sets: number of sentences in the training (#Train) and the test data sets (#Test), and number of output labels (#Label).",
        "I^^!",
        "denotes the average number of active set when uj = 1, i.e., the supported transitions that are used at least once in the training set.",
        "model (Sparse).",
        "All of our model variants were based on Sparse model.",
        "For the hyper parameter e, we empirically selected 0.001 for Method 1 (this preserves 99% of probability density), 0 for Method 2, and 4 for Methods 3 and 4.",
        "Note that e for Methods 2, 3, and 4 indicates an empirical count of features in training set.",
        "All experiments were implemented in C++ and executed in Windows 2003 with XEON 2.33 GHz Quad-Core processor and 8.0 Gbyte of main memory.",
        "We first show that our method is efficient for learning CRFs (Figure 1).",
        "In all learning curves, Dense generally has a higher training log-likelihood than Sparse.",
        "For PTB and Encyclopedia, results for Dense are not available because training in a single machine failed due to out-of-memory errors.",
        "For both Dense and Sparse, we executed the exact inference method.",
        "Our proposed method (Method 1~4) performs faster than Sparse.",
        "In most results, Method 1 was the fastest, because it was terminated after fewer iterations.",
        "However, Method 1 sometimes failed to converge, for example, in Encyclopedia.",
        "Similarly, Method 3 and 4 could not find the optimal solution in the NetTalk data set.",
        "Method 2 showed stable results.",
        "Second, we evaluated the accuracy and decoding time of our methods (Table 2).",
        "Most results obtained using our method were as accurate as those of Dense and Sparse.",
        "However, some results of Method 1, 3, and 4 were significantly inferior to those of Dense and Sparse for one of two reasons: 1) parameter estimation failed (NetTalk and Encyclopedia), or 2) approximate inference caused search errors (CoNLLOO and Communicator).",
        "The improvements of decoding time on Communicator and Encyclopedia were remarkable.",
        "Finally, we compared our method with two open-source implementations of CRFs: mallet and crf++.",
        "Mallet can support the Sparse model, and the crf++ toolkit implements only the Dense model.",
        "We compared them with Method 2 on the Communicator data set.",
        "In the accuracy measure, the results were 91.56 (mallet), 91.87 (crf++), and 91.92 (ours).",
        "Our method performs 5^50 times faster for training (1,774 s for mallet, 18,134 s for crf++,",
        "Set",
        "#Train",
        "#Test",
        "#Label",
        "\\AUJ=L\\",
        "\\-^*-avg \\",
        "PTB",
        "38,219",
        "5462",
        "45",
        "30.01",
        "CoNLLOO",
        "8,936",
        "2,012",
        "22",
        "6.59",
        "CoNLL03",
        "14,987",
        "3,684",
        "8",
        "4.13",
        "NetTalk",
        "18,008",
        "2,000",
        "51",
        "22.18",
        "Communicator",
        "13,111",
        "1,193",
        "120",
        "3.67",
        "Encyclopedia",
        "25,348",
        "6,336",
        "279",
        "3.27",
        "(b) CoNLLOO (d) NetTalk (e) Communicator (f) Encyclopedia",
        "Training time (sec)",
        "Figure 1: Result of training linear-chain CRFs: Un-normalized training log-likelihood and training times are compared.",
        "Dashed lines denote the termination of training step.",
        "Table 2: Decoding result; columns are percent accuracy (Acc), and decoding time in milliseconds (Time) measured per testing example.",
        "'*' indicates that the result is significantly different from the Sparse model.",
        "N/A indicates failure due to out-of-memory error.",
        "and 368 s for ours) and 7~ 12 times faster for decoding (2.881 ms for MALLET, 5.028 ms for CRF++, and 0.418 ms for ours).",
        "This result demonstrates that learning and decoding CRFs for large-scale natural language problems can be efficiently solved using our method."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We have demonstrated empirically that our efficient inference method can function successfully, allowing for a significant speedup of computation.",
        "Our method links two previous algorithms, the SFB and the TP.",
        "We have also showed that a simple and robust variant method (Method 2) is effective in large-scale problems.",
        "The empirical results show a significant improvement in the training and decoding speeds especially when the problem has a large state space of output labels.",
        "Future work will consider applications to other large-scale problems, and more-general graph topologies.",
        "Method",
        "PTB",
        "Acc Time",
        "CoNLLOO",
        "Acc Time",
        "CoNLL03",
        "Acc Time",
        "NetTalk",
        "Acc Time",
        "Communicator",
        "Acc Time",
        "Encyclopedia",
        "Acc Time",
        "Dense Sparse Method 1 Method 2 Method 3 Method 4",
        "N/A N/A 96.6 1.12 96.8 0.74 96.6 0.92",
        "96.5 0.84",
        "96.6 0.85",
        "96.1 0.89 95.9 0.62 95.9 0.55 *95.7 0.52 *94.2 0.51 *92.1 0.51",
        "95.8 0.26",
        "95.9 0.21 *94.0 0.24",
        "95.9 0.21 95.9 0.24 95.9 0.24",
        "88.4 0.49 88.4 0.44 *88.3 0.34 *87.4 0.32 *78.2 0.29 *77.9 0.30",
        "91.6 0.94 91.9 0.83",
        "91.7 0.73 91.9 0.30",
        "*86.7 0.30 91.9 0.29",
        "N/A N/A 93.6 34.75 *69.2 15.77",
        "93.6 4.99",
        "93.7 6.14 93.3 4.88"
      ]
    }
  ]
}
