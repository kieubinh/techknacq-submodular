{
  "info": {
    "authors": [
      "Lidan Zhang",
      "Kwok-Ping Chan"
    ],
    "book": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics",
    "id": "acl-W09-0204",
    "title": "A Study of Convolution Tree Kernel with Local Alignment",
    "url": "https://aclweb.org/anthology/W09-0204",
    "year": 2009
  },
  "references": [
    "acl-C02-1150",
    "acl-C08-1082",
    "acl-C08-1088",
    "acl-J06-1003",
    "acl-J08-2003",
    "acl-P02-1034",
    "acl-P04-1043",
    "acl-P04-1054",
    "acl-P07-1026",
    "acl-P99-1004",
    "acl-W01-0514",
    "acl-W04-3212",
    "acl-W05-0620"
  ],
  "sections": [
    {
      "text": [
        "HKU, Hong Kong",
        "lzhang@cs.hku.hk",
        "kpchan@cs.hku.hk",
        "This paper discusses a new convolution tree kernel by introducing local alignments.",
        "The main idea of the new kernel is to allow some syntactic alternations during each match between subtrees.",
        "In this paper, we give an algorithm to calculate the composite kernel.",
        "The experiment results show promising improvements on two tasks: semantic role labeling and question classification."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recently kernel-based methods have become a state-of-art technique and been widely used in natural language processing applications.",
        "In this method, a key problem is how to design a proper kernel function in terms of different data representations.",
        "So far, there are two kinds of data representations.",
        "One is to encode an object with a flat vector whose element correspond to an extracted feature from the object.",
        "However the feature vector is sensitive to the structural variations.",
        "The extraction schema is heavily dependent on different problems.",
        "On the other hand, kernel function can be directly calculated on the object.",
        "The advantages are that the original topological information is to a large extent preserved and the introduction of additional noise may be avoided.",
        "Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction(Zelenko et al., 2003), named entity recognition(Culotta and Sorensen, 2004), semantic role labeling(Moschitti et al., 2008) and so on.",
        "To compute the structural kernel function, Haussler (1999) introduced a general type of kernel function, called\" Convolution kernel\".",
        "Based on this work, Collins and Duffy (2002) proposed a tree kernel calculation by counting the common subtrees.",
        "In other words, two trees are considered if and only if these two trees are exactly same.",
        "In real sentences, some structural alternations within a given phrase are permitted without changing its usage.",
        "Therefore, Moschitti (2004) proposed partial trees to partially match between subtrees.",
        "Kashima and Koyanagi (2002) generalize the tree kernel to labeled order tree kernel with more flexible match.",
        "And from the idea of introducing linguistical knowledge, Zhang et al.",
        "(2007) proposed a grammar-driven tree kernel, in which two subtrees are same if and only if the corresponding two productions are in the same manually defined set.",
        "In addition, the problem of hard matching can be alleviated by processing or mapping the trees.",
        "For example, Tai mapping (Kuboyama et al., 2006) generalized the kernel from counting subtrees to counting the function of mapping.",
        "Moreover multi-source knowledge can benefit kernel calculation, such as using dependency information to dynamically determine the tree span (Qian et al., 2008).",
        "In this paper, we propose a tree kernel calculation algorithm by allowing variations in productions.",
        "The variation is measured with local alignment score between two derivative POS sequences.",
        "To reduce the computation complexity, we use the dynamic programming algorithm to compute the score of any alignment.",
        "And the top n alignments are considered in the kernel.",
        "Another problem in Collins and Duffy's tree kernel is context-free.",
        "It does not consider any semantic information located at the leaf nodes of the parsing trees.",
        "To lexicalized tree kernel, Bloehdorn et al.",
        "(2007) considered the associated term similarity by virtue of WordNet.",
        "Shen et al.",
        "(2003) constructed a separate lexical feature containing words on a given path and merged into the kernel in linear combination.",
        "The paper is organized as follows.",
        "In section 2, we describe the commonly used tree kernel.",
        "In section 3, we propose our method to make use of the local alignment information in kernel calculation.",
        "Section 4 presents the results of our experiments for two different applications ( Semantic Role Labeling and Question Classification).",
        "Finally section 5 provides our conclusions."
      ]
    },
    {
      "heading": "2. Convolution Tree Kernel",
      "text": [
        "The main idea of tree kernel is to count the number of common subtrees between two trees Ti and T2.",
        "In convolutional tree kernel (Collins and Duffy, 2002), a tree(T) is represented as a vector h(T) = (hi(T),...,ht(T),...,hn(T)), where hl{T) is the number of occurrences of the ith tree fragment in the tree T. Since the number of subtrees is exponential with the parse tree size, it is infeasible to directly count the common subtrees.",
        "To reduce the computation complexity, a recursive kernel calculation algorithm was presented.",
        "Given two trees T1 and T2, where, NTl and NT2 are the sets of all nodes in trees Ti and T2, respectively.",
        "Il(n) is the indicator function to be 1 if i-th subtree is rooted at node n and 0 otherwise.",
        "And A(ni,n2) is the number of common subtrees rooted at ni and n2.",
        "It can be computed efficiently according to the following rules:",
        "(2) If the productions at n1 and n2 are same, and n1 and n2 are pre-terminals, then where nc(ni) is the number of children of n1 in the tree.",
        "Note that n1 = n2 because the productions at ni and n2 are same.",
        "ch(n1}j) represents the jth child of node n1.",
        "And 0 < A < 1 is the parameter to downweight the contribution of larger tree fragments to the kernel.",
        "It corresponds to K(T1T2) = Ei Asizeihl(T1)hl(T2), where sizei is the number of rules in the i'th fragment.",
        "The time complexity of computing this kernel is O(\\NTl | • \\Nt2 |)."
      ]
    },
    {
      "heading": "3. Tree Kernel with Local Alignment",
      "text": [
        "As we referred, one of problems in the basic tree kernel is its hard match between two rules.",
        "In other words, at each tree level, the two subtrees are required to be perfectly equal.",
        "However, in real sentences, some modiiers can be added into a phrase without changing the phrase's function.",
        "For example, two sentences are given in Figure 1.",
        "Considering \"A1\" role, the similarities between two subtrees(in circle) are 0 in (Collins and Duffy, 2002), because the productions \"NP^DT ADJP NN\" and \"NP^DT NN\" are not identical.",
        "From linguistical point of view, the adjective phrase is optional in real sentences, which does not change the corresponding semantic role.",
        "Thus the modiier components(like \"ADJP\" in the above example) should be neglected in similarity comparisons.",
        "To make the hard match flexible, we can align two string sequences derived from the same node.",
        "Considering the above example, an alignment might be \"DT ADJP NN\" vs \"DT - NN\", by inserting a symbol(-).",
        "The symbol(-) corresponds to a \"NULL\" subtree in the parser tree.",
        "And the \"NULL\" subtree can be regarded as a null character in the sentence, see Figure 1(c).",
        "Convolution kernels, studied in (Haussler, 1999) gave the framework to construct a complex kernel from its simple elements.",
        "Suppose x e X can be decomposed into X.",
        "Let R be a relation over X1 x ... x Xm x X such that R(X) is true iff x\\,...,xm are parts of x. R-1(x) = {X\\R(X,x)}, which returns all components.",
        "For example, x is any string, then x can be its characters.",
        "The convolution kernel K is defined as:",
        "Considering our problem, for example, a derived string sequence x by the rule \"n1 – x\".",
        "R(xi' x) is true iff xi appears in the right hand of x.",
        "Given two POS sequences x and y derived from two nodes n1 and n2, respectively, A(x,y) denotes all the possible alignments of the sequence.",
        "The general form of the kernel with local alignment is defined as:",
        "where, (ij) denotes the ith and jth variation for x and y, ASis the score for alignment i and j.",
        "And ch(n1' i, d) selects the dth subtree for the ith aligned schema of node n1.",
        "It is easily to prove the above kernel is positive semi-definite, since the kernel K (nl' n2) is positive semi-definite.",
        "The native computation is impractical because the number of all possible alignments(\\A(x'y)\\) is exponential with respect to \\ x\\ and \\ y\\ .",
        "In the next section, we will discuss how to calculate AS for each alignment.",
        "The local alignment(LA) kernel was usually used in bioinformatics, to compare the similarity between two protein sequences(x and y) by exploring their alignments(Saigo et al., 2004).",
        "where (3 > 0 is a parameter, A(x'y) denotes all possible local alignments between x and y, and s(x' n) is the local alignment score for a given alignment schema n, which is equal",
        "In equation( 5), S is a substitution matrix, and g is a gap penalty function.",
        "The alignment score is the sum of the substitution score between the correspondence at the aligned position, minus the sum of the gap penalty for the case that'-' symbol is inserted.",
        "In natural language processing, the substitution matrix can be selected as identity matrix and no penalty is accounted.",
        "Obviously, the direct computation of the original KLA is not practical.",
        "Saigo (2004) presented a dynamic programming algorithm with time complexity O (\\ x \\-\\ y \\).",
        "In this paper, this dynamic algorithm is used to compute the kernel matrix, whose element(i,j) is used as AS(i'j measurement in equation(3).",
        "Now we embed the above local alignment score into the general tree kernel computation.",
        "Equation(3) can be rewritten into following:",
        "To further reduce the computation complexity, a threshold (f) is used to filter out alignments with low scores.",
        "This can help to avoid over-generated subtrees and only select the significant alignments.",
        "In other words, by using the threshold (f), we can select the salient subtree variations for kernels.",
        "The final kernel calculation is shown below:",
        "After filtering, the kernel is still positive semi-definite.",
        "This can be easily proved using the theorem in (Shin and Kuboyama, 2008), since this subset selection is transitive.",
        "More specifically, if s(x,y,n) > £/\\s(y,z,n') > £, then s(x, z,n + n') > £.",
        "The algorithm to compute the local alignment tree kernel is given in algorithm 1.",
        "For any two nodes pairfe and yj ), the local alignment score M(xi}yj) is assigned.",
        "In the kernel matrix calculation, the worst case occurs when the tree is balanced and most of the alignments are selected.",
        "Algorithm 1 algorithm for local alignment tree kernel_",
        "Require: 2 nodes u\\,n2 in parse trees;The productions are n\\ – ► x\\, ...,xm and n2 – ► return A'(ni,n2) if n and n2 are not same then if both n and n2 are pre-terminals then calculate kernel matrix by equation( 4) for each possible alignment do",
        "calculate A'{ni,n2) by equation(7) end for end if end if"
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We use the CoNLL-2005 SRL shared task data(Carreras and Marquez, 2005) as our experimental data.",
        "It is from the Wall Street Journal part of the Penn Treebank, together with predicate-arguments information from the PropBank.",
        "According to the shared task, sections 02-21 are used for training, section 24 for development and section 23 as well as some data from Brown corpus are left for test.",
        "The data sets are described in Table 1.",
        "Sentences",
        "Arguments",
        "Training",
        "39,832",
        "239,858",
        "Dev",
        "1,346",
        "8,346",
        "Test",
        "WSJ",
        "1,346",
        "8,346",
        "Brown",
        "450",
        "2,350",
        "Considering the two steps in semantic role labeling, i.e. semantic role identification and recognition.",
        "We assume identification has been done correctly, and only consider the semantic role classification.",
        "In our experiment, we focus on the semantic classes include 6 core (A0-A5), 12 adjunct(AM-) and 8 reference(R-) arguments.",
        "In our implementation, SVM-Light-TK(Moschitti, 2004) is modified.",
        "For SVM multi-classifier, the ONE-vs-ALL (OVA) strategy is selected.",
        "In all, we prepare the data for each semantic role (r) as following:",
        "(1) Given a sentence and its correct full syntactic parse tree;",
        "(2) Let P be the predicate.",
        "Its potential arguments A are extracted according to (Xue",
        "(3) For each pair < p,a >e P x A: if a covers exactly the words of semantic role of p, put minimal subtree < p,a > into positive example set (T+); else put it in the negative examples (T-)",
        "In our experiments, we set ( = 0.5.",
        "The classification performance is evaluated with respect to accuracy, precision(p), recall(r) and F\\ = 2pr/(p + r).",
        "Table 2 compares the performance of our method and other three famous kernels on WSJ test data.",
        "We implemented these three methods with the same settings described in the papers.",
        "It shows that our kernel achieves the best performance with 88.48% accuracy.",
        "The advantages of our approach are: 1).",
        "the alignments allow soft syntactic structure match; 2).",
        "threshold can avoid overgeneration and selected salient alignments.",
        "P(%)",
        "R(%)",
        "Fß=l",
        "Development",
        "81.03",
        "68.91",
        "74.48",
        "WSJ Test",
        "84.97",
        "79.45",
        "82.11",
        "Brown Test",
        "76.95",
        "70.94",
        "73.51",
        "WSJ+Brown",
        "82.98",
        "75.40",
        "79.01",
        "WSJ",
        "P(%)",
        "F",
        "A0",
        "81.28",
        "83.90",
        "82.56",
        "A1",
        "84.22",
        "66.39",
        "74.25",
        "A2",
        "77.27",
        "62.36",
        "69.02",
        "A3",
        "93.33",
        "21.21",
        "34.57",
        "A4",
        "82.61",
        "51.35",
        "63.33",
        "A5",
        "100.00",
        "40.00",
        "57.41",
        "AM-ADV",
        "74.21",
        "56.21",
        "63.92",
        "AM-CAU",
        "75.00",
        "46.09",
        "57.09",
        "AM-DIR",
        "57.14",
        "16.00",
        "25.00",
        "AM-DIS",
        "77.78",
        "70.00",
        "73.68",
        "AM-EXT",
        "75.00",
        "53.10",
        "62.18",
        "AM-LOC",
        "89.66",
        "74.83",
        "81.57",
        "AM-MNR",
        "84.62",
        "48.20",
        "61.41",
        "AM-MOD",
        "96.64",
        "92.00",
        "94.26",
        "AM-NEG",
        "99.30",
        "95.30",
        "97.26",
        "AM-PNC",
        "48.20",
        "28.31",
        "35.67",
        "AM-PRD",
        "50.00",
        "30.00",
        "37.50",
        "AM-TMP",
        "87.87",
        "73.43",
        "80.00",
        "R-A0",
        "81.08",
        "67.80",
        "73.85",
        "R-A1",
        "77.50",
        "49.60",
        "60.49",
        "R-A2",
        "58.00",
        "42.67",
        "49.17",
        "R-AM-CAU",
        "100.00",
        "25.00",
        "40.00",
        "R-AM-EXT",
        "100.00",
        "100.00",
        "100.00",
        "R-AM-LOC",
        "100.00",
        "55.00",
        "70.97",
        "R-AM-MNR",
        "50.00",
        "25.00",
        "33.33",
        "R-AM-TMP",
        "85.71",
        "52.94",
        "65.46",
        "Accuracy(%)",
        "(Collins and Duffy, 2002)",
        "84.35",
        "(Moschitti, 2004)",
        "86.72",
        "(Zhang et al., 2007)",
        "87.96",
        "Our Kernel",
        "88.48",
        "Another problem in the tree kernel (Collins and Duffy, 2002) is the lack of semantic information, since the match stops at the preterminals.",
        "All the lexical information is encoded at the leaf nodes of parsing trees.",
        "However, the semantic knowledge is important in some text applications, like Question Classification.",
        "To introduce semantic similarities between words into our kernel, we use the framework in Bloehdorn et al.",
        "(2007) and rewrite the rule (2) in the iterative tree kernel calculation(in section 2).",
        "(2) If the productions at n1 and n2 are same, and n1 and n2 are pre-terminals, then A(n1}n2) = Xakw (w1,w2) where w1 and w2 are two words derived from pre-terminals ni and n2, respectively, and the parameter a is to control the contribution of the leaves.",
        "Note that each preterminal has one child or equally covers one word.",
        "So kw(w1,w2) actually calculate the similarity between two words wi and w2.",
        "In general, there are two ways to measure the semantic similarities.",
        "One is to derive from semantic networks such as WordNet (Mavroeidis et al., 2005; Bloehdorn et al., 2006).",
        "The other way is to use statistical methods of distributional or co-occurrence (O Seaghdha and Copestake, 2008) behavior of the words.",
        "WordNet can be regarded as direct graphs semantically linking concepts by means of relations.",
        "Table 4 gives some similarity measures between two arbitrary concepts c1 and c2.",
        "For our application, the word-to-word similarity can be obtained by maximizing the corresponding concept-based similarity scores.",
        "In our implementation, we use WordNet::Similarity package(Patwardhan et al., 2003) and the noun hierarchy of WordNet.",
        "In Table 4, dep is the length of path from a node to its global root, lso(c1,c2) represents the lowest superordinate of c1 and c2.",
        "The detail definitions can be found in (Budanitsky and Hirst, 2006) .",
        "As an alternative, Latent Semantic Anal-ysis(LSA) is a technique.",
        "It calculates the words similarities by means of occurrence of terms in documents.",
        "Given a term-by-document matrix X, its singular value decomposition is: X = U£VT, where £ is a diagonal matrix with singular values in decreasing arrangement.",
        "The column of U are singular vectors corresponding to the individual singular value.",
        "Then the latent semantic similarity kernel of terms U and tj is:",
        "where Uk = IkU is to project U onto its first k dimensions.",
        "Ik is the identity matrix whose first k diagonal elements are 1 and all the other elements are 0.",
        "And Ulk is the i-throw of the matrix Uk.",
        "From equation (8), the LSA-based similarity between two terms is the inner product of the two projected vectors.",
        "The details of LSA can be found in (Cristianini et al., 2002; Choi etal., 2001).",
        "In this set of experiment, we evaluate different types of kernels for Question Classifica-tion(QC) task.",
        "The duty of QC is to categorize questions into different classes.",
        "In this paper we use the same dataset as introduced in(Li and Roth, 2002).",
        "The dataset is divided into 5500 questions for training and 500 questions from TREC 20 for testing.",
        "The total training samples are randomly divided into 5 subsets with sizes 1,000, 2,000, 3,000, 4,000 and 5,500 respectively.",
        "All the questions are labeled into 6 coarse grained categories and 50 fine grained categories: Abbreviations (abbreviation and expansion), Entity (animal, body, color, creation, currency, medical, event, food, instrument, language, letter, plant, product, religion, sport, substance, symbol, technique, term, vehicle, word), Description (definition, description, manner, reason), Human (description, group, individual, title), Location (city, country, mountain, state) and Numeric (code, count, date, distance, money, order, percent, period, speed, temperature, size, weight).",
        "Similarity",
        "Definition",
        "Wu and Palmer",
        "Resnik",
        "Lin",
        "simwUP (Ci,C2) – d(ci,lso(ci,C2))+d(c2,lso(c1,c2))+2dep(lso(ci,C2))",
        "simREs(ci,c2) – - log P(lso(ci,c2))",
        "simrr-,(C C ) – 2logp(lso(cl,c2)) simLIN (C1,C2) – log p(ci)+log P(c2)",
        "In this paper, we compare the linear kernel based on bag-of-word (BOW), the original tree kernel (TK), the local alignment tree kernel (section 3, LATK) and its correspondences with LSA similarity and a set of semantic-enriched LATK with different similarity metrics.",
        "To obtain the parse tree, we use Charniak parser for every question.",
        "Like the previous experiment, SVM-Light-TK software and the OVA strategy are implemented.",
        "In all experiments, we use the default parameter in SVM(e.g. margin parameter) and set a = 1.",
        "In LSA model, we set k = 50.",
        "Finally, we use multi-classification accuracy to evaluate the performance.",
        "Table 5 gives the results of the experiments.",
        "We can see that the local alignment tree kernel increase the multi-classification accuracy of the basic tree kernel by about 0.4%.",
        "The introduction of semantic information further improves accuracy.",
        "Among WordNet-based metrics, \"Wu and Palmer\" metric achieves the best result, i.e. 92.5%.",
        "As a whole, the WordNet-based similarities perform better than LSA-based measurement."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, we propose a tree kernel calculation by allowing local alignments.",
        "More flexible productions are considered in line with modifiers in real sentences.",
        "Considering text related applications, words similarities have been merged into the presented tree kernel.",
        "These similarities can be derived from different WordNet-based metrics or document statistics.",
        "Finally experiments are carried on two different applications (Semantic Role Labeling and Question Classification).",
        "For further work, we plan to study exploiting semantic knowledge in the kernel.",
        "A promising direction is to study the different effects of these semantic similarities.",
        "We are interested in some distributional similarities (Lee, 1999) given certain context.",
        "Also the effectivenss of the semantic-enriched tree kernel in SRL is another problem.",
        "Accuracy(%)",
        "1000",
        "2000",
        "3000",
        "4000",
        "5500",
        "BOW",
        "77.1",
        "83.3",
        "87.2",
        "87.3",
        "89.2",
        "TK",
        "80.2",
        "86.2",
        "87.4",
        "88.6",
        "91.2",
        "LATK",
        "80.4",
        "86.5",
        "87.5",
        "88.8",
        "91.6",
        "WUP",
        "81.3",
        "87.3",
        "88.0",
        "89.8",
        "92.5",
        "a = 1",
        "RES",
        "81.0",
        "87.1",
        "87.9",
        "89.5",
        "92.2",
        "LIN",
        "81.1",
        "87.0",
        "88.0",
        "89.3",
        "92.4",
        "LSA(k = 50)",
        "80.8",
        "86.9",
        "87.8",
        "89.3",
        "91.7"
      ]
    }
  ]
}
