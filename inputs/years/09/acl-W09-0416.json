{
  "info": {
    "authors": [
      "Jinhua Du",
      "Yifan He",
      "Sergio Penkale",
      "Andy Way"
    ],
    "book": "Proceedings of the Fourth Workshop on Statistical Machine Translation",
    "id": "acl-W09-0416",
    "title": "MATREX: The DCU MT System for WMT 2009",
    "url": "https://aclweb.org/anthology/W09-0416",
    "year": 2009
  },
  "references": [
    "acl-N04-1022",
    "acl-N07-1029",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P05-1033",
    "acl-P96-1041",
    "acl-W06-3110",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "text": [
        "MaTrEx: The DCU MT System for WMT 2009",
        "Jinhua Du, Yifan He, Sergio Penkale, Andy Way",
        "Centre for Next Generation Localisation Dublin City University Dublin 9, Ireland",
        "{jdu,yhe,spenkale,away}@computing.dcu.ie",
        "In this paper, we describe the machine translation system in the evaluation campaign of the Fourth Workshop on Statistical Machine Translation at EACL 2009.",
        "We describe the modular design of our multi-engine MT system with particular focus on the components used in this participation.",
        "We participated in the translation task for the following translation directions: French-English and English-French, in which we employed our multi-engine architecture to translate.",
        "We also participated in the system combination task which was carried out by the MBR decoder and Confusion Network decoder.",
        "We report results on the provided development and test sets."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In this paper, we present a multi-engine MT system developed at DCU, MaTrEx (Machine Translation using Examples).",
        "This system exploits EBMT, SMT and system combination techniques to build a cascaded translation framework.",
        "We participated in both the French-English and English-French News tasks.",
        "In these two tasks, we employ three individual MT system which are 1) Baseline: phrase-based system (PB); 2) EBMT: Monolingually chunking both source and target sides of the dataset using a marker-based chun-ker (Gough and Way, 2004).",
        "3) HPB: a typical hierarchical phrase-based system (Chiang, 2005).",
        "Meanwhile, we also use a word-level combination framework (Rosti et al., 2007) to combine the multiple translation hypotheses and employ a new rescoring model to generate the final result.",
        "For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and",
        "Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al., 2000).",
        "We then build the CN using the TER metric (Snover et al., 2006), and finally search and generate the translation.",
        "The remainder of this paper is organised as follows: Section 2 details the various components of our system, in particular the multi-engine strategies used for the shared task.",
        "In Section 3, we outline the complete system setup for the shared task and provide results on the development and test sets.",
        "Section 4 is our conclusion."
      ]
    },
    {
      "heading": "2. The MaTrEx System",
      "text": [
        "The MaTrEx system is a combination-based multi-engine architecture, which exploits aspects of both the EBMT and SMT paradigms.",
        "This architecture includes three individual systems which are phrase-based, example-based and hierarchical phrase-based.",
        "The combination structure is the MBR decoder and CN decoder, which is based on the word-level combination strategy.",
        "In the final stage, we use a new rescoring module to process the N-best list generated by the combination module.",
        "See Figure 1 as a detailed illustration.",
        "EBMT obtains resources using the Marker Hypothesis (Green, 1979), a psycholinguistic constraint which posits that all languages are marked for surface syntax by a specific closed set of lexemes or morphemes which signify context.",
        "Given a set of closed-class words we segment each sentence into chunks, creating a chunk at each new occurrence of a marker word, with the restriction that each segment must contain at least one non-marker word (Gough and Way, 2004).",
        "Baseline",
        "Dev/MERT",
        "Rescore/MERT Decoding",
        "Mutiple 1-best",
        "MBR Decoder",
        "System Combination",
        "CN Decoder",
        "We then align these segments using an edit-distance-style algorithm, in which the insertion and deletion probabilities depend on word-to-word translation probabilities and word-to-word cognates (Stroppa and Way, 2006).",
        "We extracted phrases of at most 7 words on each side.",
        "We then merged these phrases with the phrases extracted by the baseline system adding word alignment information, and used this system seeded with this additional information.",
        "HPB translation system is a reimplementation of the hierarchical phrase translation model which is based on PSCFG (Chiang, 2005).",
        "We generate recursively PSCFG rules from the initial rules as where N is a rule which is initial or includes nonterminals.",
        "where 1 < i < j < m and 1 < u < v < n, at which point a new rule can be obtained, named, where k is an index for the nonterminal X.",
        "The number of nonterminals permitted in a rule is no more than two.",
        "When extracting hierarchical rules,we set some limitations that initial rules are of no more than 7 words in length and other rules should have no more than 5 terminals and nonterminals, and we disallow rules with adjacent source-side and target-side nonterminals.",
        "The decoder is an enhanced CYK-style chart parser that maximizes the derivation probability and spans up to 12 source words.",
        "A 4-gram language model generated by SRI Language Modeling toolkit (SRILM) (Stolcke, 2002) is used in the cube-pruning process.",
        "The search space is pruned with a chart cell size limit of 50.",
        "For multiple system combination, we implement an MBR-CN framework as shown in Figure 1.",
        "Instead of using a single system output as the skeleton, we employ a minimum Bayes-risk decoder to select the best single system output from the merged N-best list by minimizing the BLEU (Pa-pineni et al., 2002) loss.",
        "The confusion network is built by the output of MBR as the backbone which determines the word order of the combination.",
        "The other hypotheses are aligned against the backbone based on the TER metric.",
        "NULL words are allowed in the alignment.",
        "Each arc in the CN represents an alternative word at that position in the sentence and the number of votes for each word is counted when constructing the network.",
        "The features we used are as follows:",
        "• word posterior probability (Fiscus, 1997);",
        "• 3,4-gram target language model;",
        "• word length penalty;",
        "• Null word length penalty;",
        "weights of confusion network.",
        "Rescore is a very important part in post-processing which can select a better hypothesis from the N-best list.",
        "We add some new global features in rescore model.",
        "The features we used are as follows:",
        "• Direct and inverse IBM model;",
        "• 3, 4-gram target language model;",
        "EBMT",
        "TestSet",
        "Rescore",
        "J",
        "• Sentence length posterior probability (Zens and Ney, 2006);",
        "• N-gram posterior probabilities within the N-Best list (Zens and Ney, 2006);",
        "• Minimum Bayes Risk probability;",
        "• Length ratio between source and target sentence;",
        "The weights are optimized via MERT algorithm."
      ]
    },
    {
      "heading": "3. Experimental Setup",
      "text": [
        "The following section describes the system and experimental setup for the French-English and English-French translation tasks.",
        "We used Europarl and Giga data for this evaluation.",
        "The statistics of parallel data are shown in",
        "Table 1.",
        "In this table, Sen indicates the number of sentence pairs; Len denotes the maximum sentence length of each corpus.",
        "This year the translation task is only evaluated on News Domain.",
        "Experimental results showed that giga data is more correlated than Europarl and the BLEU score is significantly im-proved(See Table 4).",
        "Monolingual Corpus",
        "In this evaluation, we trained a small 4-gram language model using data in Table 1 and a large 4-gram language model using data in Table 2.",
        "We configured these two LMs for Baseline and EBMT systems while HPB only used the large one.",
        "In the above table, E/N/NC refers to Eu-roparl/News/New_Commentary corpus.",
        "We preprocessed both Europarl and Giga Release 1 corpus.",
        "For the Europarl corpus, we removed the reserved characters in GIZA++ and tokenized and lowercased the corpus with tools provided by WMT09.",
        "The Giga corpus was too large for our resource, so we performed sentence selection before cleaning, in the following steps.",
        "• We split the Giga corpus into even segments, each segment consisting of 20 lines.",
        "• We trained an SVM classifier on English side with positive examples from the monolingual news data and negative examples from noisy sentences (numbers, meaningless word combinations, and random segments) from the Giga corpus.",
        "We used \"-ly\" and \"-ing\" to approximate adverbs and present participles and did not use other POS-induced features, as in (Ferizis and Bailey, 2006).",
        "We added these features to remove noise: average length of sentences, frequency of capitalized characters, frequency of numerical characters and short word penalty (equals to 1 when average length of words < 4, and 0 otherwise).",
        "We used the classifier to remove 20% segments of lowest scores.",
        "• We selected 1, 600 words having the highest mutual information scores with monolingual training data against the Giga corpus.",
        "• We selected 100,000 segments where these words occurred most frequently.",
        "However the sentence was dropped if the length ratio between English and French was larger than 1.5 or less than 0.67.",
        "The two language models were done using the SRILM employing linear interpolation and modified KN discounting (Chen and Goodman, 1996).",
        "The configuration for the three systems is listed in Table 3.",
        "In this table, E indicates the Europarl corpus which is used for all three systems, and G stands for the Giga corpus which is only used for the Baseline system.",
        "We can see from Table 3 that the size of the HPB phrase-table is more than 2 times as large as the other phrase tables.",
        "How to filter and process such a huge hierarchical table is a challenging problem.",
        "Corpra",
        "Sen",
        "Token-En",
        "Token-Fr",
        "Len",
        "Europarl",
        "1.46M",
        "39,240,672",
        "42,252,067",
        "80",
        "Giga",
        "2M",
        "48,648,104",
        "57,869,002",
        "65",
        "Language",
        "Sen",
        "Token",
        "Source",
        "English",
        "9,966,838",
        "240,849,221",
        "E/N/NC",
        "French",
        "9,966,838",
        "260,520,313",
        "E/N/NC",
        "System",
        "P-Table",
        "Length",
        "LM",
        "Features",
        "Baseline-E",
        "55.9M",
        "7",
        "2",
        "15",
        "Baseline-G",
        "58.4M",
        "7",
        "2",
        "15",
        "EBMT",
        "59.4M",
        "7",
        "2",
        "15",
        "HPB",
        "122M",
        "5",
        "1",
        "8",
        "We tuned our systems on the development set devset2009-a and devset2009-b, and performed the crossover experiment by these two devsets.",
        "The system output is evaluated with respect to BLEU score.",
        "In Table 4, we used devset2009-b to tune the various parameters in our three single systems and devset2009-a for testing.",
        "In terms of the Europarl data, we can see that the three systems we used achieved similar performance on the test set for both translation directions, with the Baseline-E system yielding slightly better results than the other two.",
        "We then used the translations of the devset2009-a produced by each system to tune the parameters of our system combination module.",
        "From Table 4, we can see that using MBR and confusion network decoding leads to a slight improvement over the strongest single system, i.e. the baseline Phrase-Based SMT system.",
        "Rescoring the N-best lists yielded an increase of 0.5 (2.0 relative) absolute BLEU points over the baseline for French-English Translation and 0.29 (1.28 relative) absolute BLEU points for English-French Translation.",
        "Table 5 is the results on 2009 Test Data.",
        "The scores with a slash in the last two rows are lowercased and cased respectively.",
        "From the table we can see that combination yielded 0.45 and 0.79 absolute BLEU points over the best single system for Fr-En and En-Fr direction respectively.",
        "However, 1.93 (7.2 relative) and 1.64 (6.58 relative) BLEU points are dropped between cased and lowercased results of both directions.",
        "Accordingly, training an effective recasing model is very important for our future work."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "This paper presents our machine translation system in WMT2009 shared task campaign.",
        "We developed a multi-engine framework which combined the output results of the three MT systems and generated a new N-best list after CN decoding.",
        "Then by using some global features the rescoring model generated the final translation output.",
        "The experimental result proved that the combination module and rescoring module are effective in our framework.",
        "We also applied simple yet effective methods of genre and topical classification to remove noise and out-of-domain sentences in the Giga corpus, from which we built better translation models than from Europarl.",
        "In future work, we will refine our system framework to investigate its effect on the tasks presented here, and we will develop more powerful post-processing tools such as recaser to reduce the",
        "BLEU loss.",
        "Acknowledgments",
        "This work is supported by Science Foundation Ireland (Grant No.",
        "07/CE/I1142).",
        "Thanks also to the reviewers for their insightful comments and suggestions.",
        "System",
        "Fr-En",
        "En-Fr",
        "Baseline-E",
        "25.64",
        "24.47",
        "Baseline-G",
        "26.75",
        " – ",
        "EBMT",
        "25.67",
        "24.43",
        "HPB",
        "25.20",
        "24.19",
        "Combination",
        "27.20/25.14",
        "25.26/22.28",
        "Official-Auto",
        "26.86/24.93",
        "23.78/22.14",
        "System",
        "Fr-En",
        "En-Fr",
        "Baseline-E",
        "22.24",
        "22.68",
        "Baseline-G",
        "24.90",
        "1",
        "EBMT",
        "22.04",
        "22.12",
        "HPB",
        "21.69",
        "21.12",
        "MBR",
        "25.11",
        "22.68",
        "CN",
        "25.24",
        "22.76",
        "Rescore",
        "25.40",
        "22.97"
      ]
    }
  ]
}
