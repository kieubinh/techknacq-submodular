{
  "info": {
    "authors": [
      "Chris Dyer",
      "Hendra Setiawan",
      "Yuval Marton",
      "Philip Resnik"
    ],
    "book": "Proceedings of the Fourth Workshop on Statistical Machine Translation",
    "id": "acl-W09-0426",
    "title": "The University of Maryland Statistical Machine Translation System for the Fourth Workshop on Machine Translation",
    "url": "https://aclweb.org/anthology/W09-0426",
    "year": 2009
  },
  "references": [
    "acl-C04-1051",
    "acl-C08-1064",
    "acl-D07-1104",
    "acl-D08-1076",
    "acl-E03-1076",
    "acl-I08-1066",
    "acl-J03-1002",
    "acl-J93-1003",
    "acl-N04-1022",
    "acl-N06-1003",
    "acl-P01-1008",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P07-1090",
    "acl-P08-1115",
    "acl-P99-1067",
    "acl-W08-0318"
  ],
  "sections": [
    {
      "text": [
        "Chris Dyer , Hendra Setiawant, Yuval Marton*^ and Philip Resnik*\"",
        "^UMIACS Laboratory for Computational Linguistics and Information Processing",
        "*Department of Linguistics University of Maryland, College Park, MD 20742, USA",
        "{redpony,hendra,ymarton,resnik} AT umd.edu",
        "This paper describes the techniques we explored to improve the translation of news text in the German-English and Hungarian-English tracks of the WMT09 shared translation task.",
        "Beginning with a convention hierarchical phrase-based system, we found benefits for using word segmentation lattices as input, explicit generation of beginning and end of sentence markers, minimum Bayes risk decoding, and incorporation of a feature scoring the alignment of function words in the hypothesized translation.",
        "We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "For the shared translation task of the Fourth Workshop on Machine Translation (WMT09), we focused on two tasks: German to English and Hungarian to English translation.",
        "Despite belonging to different language families, German and Hungarian have three features in common that complicate translation into English:"
      ]
    },
    {
      "heading": "1.. productive compounding (especially of nouns),",
      "text": []
    },
    {
      "heading": "2.. rich inflectional morphology,",
      "text": [
        "3. widespread mid-to long-range word order differences with respect to English.",
        "Since these phenomena are poorly addressed with conventional approaches to statistical machine translation, we chose to work primarily toward mitigating their negative effects when constructing our systems.",
        "This paper is structured as follows.",
        "In Section 2 we describe the baseline model, Section 3 describes the various strategies we employed to address the challenges just listed, and Section 4 summarizes the final translation system."
      ]
    },
    {
      "heading": "2. Baseline system",
      "text": [
        "Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs.",
        "First, such a system makes use of lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to-English translation (Koehn et al., 2008).",
        "Additionally, since the decoder is based on a CKY parser, it can consider all licensed reorderings of the input in polynomial time, and German and Hungarian may require quite substantial reordering.",
        "Although such decoders and models have been common for several years, there have been no published results for these language pairs.",
        "The baseline system translates lowercased and tokenized source sentences into lowercased target sentences.",
        "The features used were the rule translation relative frequency P(e|/), the \"lexical\" translation probabilities P1ex(e|/) and P1ex(/|e), arule count, a target language word count, the target (English) language model P(e{), and a \"pass-through\" penalty for passing a source language word to the target side.",
        "The rule feature values were computed online during decoding using the suffix array method described by Lopez (2007).",
        "To construct the translation suffix arrays used to compute the translation grammar, we used the parallel training data provided.",
        "The preprocessed training data was filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) in both directions and symmetrized using the grow-diag-final-and heuristic.",
        "We trained a 5-gram language model from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002).",
        "We divided the 2008 workshop \"news test\" sets into two halves of approximately 1000 sentences each and designated one the dev set and the other the dev-test set.",
        "Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, bleu and ter (Papineni et al., 2002; Snover et al., 2006), during system development and tuning:",
        "Although we are not aware of any work demonstrating that this combination ofmetrics correlates better than either individually in sentence ranking, Yaser Al-Onaizan (personal communication) reports that it correlates well with the human evaluation metric hter.",
        "In this paper, we report uncased ter and bleu individually.",
        "To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al., 2008).",
        "The loss function we used was the linear combination of ter and bleu described in the previous section."
      ]
    },
    {
      "heading": "3. Experimental variations",
      "text": [
        "This section describes the experimental variants explored.",
        "Both German and Hungarian have a large number of compound words that are created by concatenating several morphemes to form a single orthographic token.",
        "To deal with productive compounding, we employ word segmentation lattices, which are word lattices that encode alternative possible segmentations of compound words.",
        "Doing so enables us to use possibly inaccurate approaches to guess the segmentation of compound words, allowing the decoder to decide which to use during translation.",
        "This is a further development of our general source-lattice approach to decoding (Dyer et al., 2008).",
        "To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment.",
        "To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize wer (Macherey et al., 2008).",
        "We reused the same features and weights to create the Hungarian lattices.",
        "For the test data, we created a lattice of every possible segmentation of any word 6 characters or longer and used forward-backward pruning to prune out low-probability segmentation paths (Sixtus and Ortmanns, 1999).",
        "We then concatenated the lattices in each sentence.",
        "To build the translation model for lattice system, we segmented the training data using the one-best split predicted by the segmentation model, and word aligned this with the English side.",
        "This variant version of the training data was then concatenated with the baseline system's training data.",
        "Source",
        "Condition",
        "bleu",
        "ter",
        "German",
        "baseline",
        "20.8",
        "60.7",
        "lattice",
        "21.3",
        "59.9",
        "Hungarian",
        "baseline lattice",
        "11.0 12.3",
        "71.1 70.4",
        "To avoid the necessity of manually creating segmentation examples to train the segmentation model, we attempted to generate sets of training examples by selecting the compound splits that were found along the path chosen by the decoder's one-best translation.",
        "Unfortunately, the segmentation system generated in this way performed slightly worse than the one-best baseline and so we continued to use the parameter settings derived from the manual segmentation.",
        "Incorporating an n-gram language model probability into a CKY-based decoder is challenging.",
        "When a partial hypothesis (also called an \"item\") has been completed, it has not yet been determined what strings will eventually occur to the left of its first word, meaning that the exact computation must deferred, which makes pruning a challenge.",
        "In typical CKY decoders, the beginning and ends of the sentence (which often have special characteristics) are not conclusively determined until the whole sentence has been translated and the probabilities for the beginning and end sentence probabilities can be added.",
        "However, by this point it is often the case that a possibly better sentence beginning has been pruned away.",
        "To address this, we explicitly generate beginning and end sentence markers as part of the translation process, as suggested by Xiong et al.",
        "(2008).",
        "The results of doing this are shown in Table 2.",
        "In order to deal with the sparsity associated with a rich source language morphology and limited-size parallel corpora (bitexts), we experimented with a novel approach to paraphrasing out-of-vocabulary (OOV) source language phrases in our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004).",
        "Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than city-block distance to measure profile similarity.",
        "The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6.",
        "The two most likely translations for each paraphrase were added to the grammar in order to provide mappings to English for OOV Hungarian phrases.",
        "This attempt at monolingually-derived source-side paraphrasing did not yield improvements over baseline.",
        "Preliminary analysis suggests that the approach does well at identifying many content words in translating extracted paraphrases of OOV phrases (e.g., a kommunista part vezetaje => , leader of the communist party or a ra tervezett => until the planned to), but at the cost of more frequently omitting target words in the output.",
        "Although our baseline hierarchical system permits long-range reordering, it lacks a mechanism to identify the most appropriate reordering for a specific sentence translation.",
        "For example, when the most appropriate reordering is a long-range one, our baseline system often also has to consider shorter-range reorderings as well.",
        "In the worst case, a shorter-range reordering has a high probability, causing the wrong reordering to be chosen.",
        "Our baseline system lacks the capacity to address such cases because all the features it employs are independent of the phrases being moved; these are modeled only as an unlexicalized generic nonterminal symbol.",
        "To address this challenge, we included what we call a dominance feature in the scoring of hypothesis translations.",
        "Briefly, the premise of this feature is that the function words in the sentence hold the key reordering information, and therefore function words are used to model the phrases being moved.",
        "The feature assesses the quality of a reordering by looking at the phrase alignment between pairs of function words.",
        "In our experiments, we treated the 128 most frequent words in the corpus as function words, similar to Setiawan et al.",
        "(2007).",
        "Due to space constraints, we will discuss the details in another publication.",
        "As Table 3 reports, the use of this feature yields positive results.",
        "Source",
        "Condition",
        "bleu",
        "ter",
        "German",
        "baseline",
        "21.3",
        "59.9",
        "+boundary",
        "21.6",
        "60.1",
        "Hungarian",
        "baseline +boundary",
        "12.3 12.8",
        "70.4 70.4",
        "Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004).",
        "This seeks the translation E of the input lattice F that has the least expected loss, measured by some loss function L:",
        "We approximate the posterior distribution P (E|F) and the set of possible candidate translations using the unique 500-best translations of a source lattice F. If H (E, F) is the decoder's path weight, this is:",
        "The optimal value for the free parameter a must be experimentally determined and depends on the ranges of the feature functions and weights used in the model, as well as the amount and kind of pruning using during decoding.",
        "For our submission, we used a = 1.",
        "Since our goal is to minimize ter-bleu we used this as the loss function in (2).",
        "Table 4 shows the results on the dev-test set for MBR decoding."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "Table 5 summarizes the impact on the dev-test set of all features included in the University of Maryland system submission."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No.",
        "HR0011-06-2-001, and the Army Research Laboratory.",
        "Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of the sponsors.",
        "Discussions with Chris Callison-Burch were helpful in carrying out the monolingual paraphrase work.",
        "Source",
        "Decoder",
        "bleu",
        "ter",
        "German",
        "Max-D",
        "22.2",
        "59.8",
        "MBR",
        "22.6",
        "59.4",
        "Hungarian",
        "Max-D MBR",
        "12.6 12.8",
        "70.0 69.8",
        "Source",
        "Condition",
        "bleu",
        "ter",
        "German",
        "baseline",
        "21.6",
        "60.1",
        "+dom",
        "22.2",
        "59.8",
        "Hungarian",
        "baseline +dom",
        "12.8",
        "12.6",
        "70.4 70.0",
        "Condition",
        "German",
        "Hungarian",
        "bleu",
        "ter",
        "bleu",
        "ter",
        "baseline",
        "20.8",
        "60.7",
        "11.0",
        "71.1",
        "+lattices",
        "21.3",
        "59.9",
        "12.3",
        "70.4",
        "+boundary",
        "21.6",
        "60.1",
        "12.8",
        "70.4",
        "+dom",
        "22.2",
        "59.8",
        "12.6",
        "70.0",
        "+MBR",
        "22.6",
        "59.4",
        "12.8",
        "69.8"
      ]
    }
  ]
}
