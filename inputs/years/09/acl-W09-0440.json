{
  "info": {
    "authors": [
      "Jesús Giménez",
      "Lluís Màrquez"
    ],
    "book": "Proceedings of the Fourth Workshop on Statistical Machine Translation",
    "id": "acl-W09-0440",
    "title": "On the Robustness of Syntactic and Semantic Features for Automatic MT Evaluation",
    "url": "https://aclweb.org/anthology/W09-0440",
    "year": 2009
  },
  "references": [
    "acl-C04-1072",
    "acl-C04-1180",
    "acl-N03-2021",
    "acl-P04-1014",
    "acl-P04-1077",
    "acl-P06-2003",
    "acl-W05-0904",
    "acl-W05-0907",
    "acl-W05-0909",
    "acl-W07-0411",
    "acl-W07-0707",
    "acl-W07-0738"
  ],
  "sections": [
    {
      "text": [
        "Jesus Gimenez and Lluis Marquez",
        "Linguistic metrics based on syntactic and semantic information have proven very effective for Automatic MT Evaluation.",
        "However, no results have been presented so far on their performance when applied to heavily ill-formed low quality translations.",
        "In order to glean some light into this issue, in this work we present an empirical study on the behavior of a heterogeneous set of metrics based on linguistic analysis in the paradigmatic case of speech translation between non-related languages.",
        "Corroborating previous findings, we have verified that metrics based on deep linguistic analysis exhibit a very robust and stable behavior at the system level.",
        "However, these metrics suffer a significant decrease at the sentence level.",
        "This is in many cases attributable to a loss of recall, due to parsing errors or to a lack of parsing at all, which may be partially ameliorated by backing off to lexical similarity."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recently, there is a growing interest in the development of automatic evaluation metrics which exploit linguistic knowledge at the syntactic and semantic levels.",
        "For instance, we may find metrics which compute similarities over shallow syntactic structures/sequences (Gimenez and Marquez, 2007; Popovic and Ney, 2007), constituency trees (Liu and Gildea, 2005) and dependency trees (Liu and Gildea, 2005; Amigo et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007).",
        "We may also find metrics operating over shallow semantic structures, such as named entities and semantic roles (Gimenez and Marquez, 2007).",
        "Linguistic metrics have been proven to produce more reliable system rankings than metrics limiting their scope to the lexical dimension, in particular when applied to test beds with a rich system typology, i.e., test beds in which there are automatic outputs produced by systems based on different paradigms, e.g., statistical, rule-based and human-aided (Gimenez and Marquez, 2007).",
        "The reason is that they are able to capture deep MT quality distinctions which occur beyond the shallow level of lexical similarities.",
        "However, these metrics have the limitation of relying on automatic linguistic processors, tools which are not equally available for all languages and whose performance may vary depending on the type of analysis conducted and the application domain.",
        "Thus, it could be argued that linguistic metrics should suffer a significant quality drop when applied to a different translation domain, or to ill-formed sentences.",
        "Clearly, metric scores computed on partial or wrong syntactic/semantic structures will be less informed.",
        "But, should this necessarily lead to less reliable evaluations?",
        "In this work, we have analyzed this issue by conducting a contrastive empirical study on the behavior of a heterogeneous set of metrics over several evaluation scenarios of decreasing translation quality.",
        "In particular, we have studied the case of Chinese-to-English speech translation, which is a paradigmatic example of low quality and heavily ill-formed output.",
        "The rest of the paper is organized as follows.",
        "In Section 2, prior to presenting experimental work, we describe the set of metrics employed in our experiments.",
        "We also introduce a novel family of metrics which operate at the properly semantic level by analyzing similarities over discourse representations.",
        "Experimental work is then presented in Section 3.",
        "Metrics are evaluated both in terms of human likeness and human acceptability (Amigo et al., 2006).",
        "Finally, in Section 4, main conclusions are summarized and future work is outlined."
      ]
    },
    {
      "heading": "2. A Heterogeneous Metric Set",
      "text": [
        "We have used a heterogeneous set of metrics selected out from the metric repository provided with the IQmt evaluation package (Gimenez and Marquez, 2007).",
        "We have considered several metric representatives from different linguistic levels (lexical, syntactic and semantic).",
        "A brief description of the metric set is available in Appendix A.",
        "In addition, taking advantage of newly available semantic processors, we have designed a novel family of metrics based on the Discourse Representation Theory, a theoretical framework offering a representation language for the examination of contextually dependent meaning in discourse (Kamp, 1981).",
        "A discourse is represented in a discourse representation structure (DRS), which is essentially a variation of first-order predicate calculus – its forms are pairs of first-order formulae and the free variables that occur in them.",
        "'DR' metrics analyze similarities between automatic and reference translations by comparing their respective DRSs.",
        "These are automatically obtained using the C&C Tools (Clark and Cur-ran, 2004).",
        "Sentences are first parsed on the basis of a combinatory categorial grammar (Bos et al., 2004).",
        "Then, the BOXER component (Bos, 2005) extracts DRSs.",
        "As an illustration, Figure 1 shows the DRS representation for the sentence \"Every man loves Mary.\".",
        "The reader may find the output of the BOXER component (top) together with the equivalent first-order formula (bottom).",
        "DRS may be viewed as semantic trees, which are built through the application of two types of DRS conditions:",
        "basic conditions: one-place properties (predicates), two-place properties (relations), named entities, time-expressions, cardinal expressions and equalities.",
        "complex conditions: disjunction, implication, negation, question, and propositional attitude operations.",
        "Three kinds of metrics have been defined:",
        "DR-STM-Z (Semantic Tree Matching) These metrics are similar to the Syntactic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSs instead of constituency trees.",
        "All semantic subpaths in the candidate and the reference trees are retrieved.",
        "The fraction of matching subpaths of a given length, l e [1..9], is computed.",
        "Then, average accumulated scores up to a given length are retrieved.",
        "For instance, 'DR-STM-4' corresponds to the average accumulated proportion of matching subpaths up to length-4.",
        "DR-Or-t These metrics compute lexical overlapping between discourse representation structures (i.e., discourse referents and discourse conditions) according to their type 't'.",
        "For instance, 'DR-Or-pred' roughly reflects lexical overlapping between the referents associated to predicates (i.e., one-place properties), whereas 'DR-Or-imp' reflects lexical overlapping between referents associated to implication conditions.",
        "We also introduce the 'DR-Or -★' metric, which computes average lexical overlapping over all DRS types.",
        "DR-Orp-t These metrics compute morphosyn-tactic overlapping (i.e., between parts of speech associated to lexical items) between discourse representation structures of the same type t. We also define the 'DR-Orp -★' metric, which computes average morphosyn-tactic overlapping over all DRS types.",
        "Note that in the case of some complex conditions, such as implication or question, the respective order of the associated referents in the tree is important.",
        "We take this aspect into account by making order information explicit in the construction of the semantic tree.",
        "We also make explicit the type, symbol, value and date of conditions when these are applicable (e.g., predicates, relations, named entities, time expressions, cardinal expressions, or anaphoric conditions).",
        "Finally, the extension to the evaluation setting based on multiple references is computed by assigning the maximum score attained against each individual reference.",
        "named(y, mary, per)",
        "Formally:"
      ]
    },
    {
      "heading": "3. Experimental Work",
      "text": [
        "In this section, we present an empirical study on the behavior of a heterogeneous set of metrics based on linguistic analysis in the case of speech translation between non-related languages.",
        "We have used the test bed from the Chinese-to-English translation task at the \"2006 Evaluation Campaign on Spoken Language Translation \" (Paul, 2006).",
        "The test set comprises 500 translation test cases corresponding to simple conversations (question/answer scenario) in the travel domain.",
        "In addition, there are 3 different evaluation subscenarios of increasing translation difficulty, according to the translation source:",
        "CRR: Translation of correct recognition results (as produced by human transcribers).",
        "ASR read: Translation of automatic read speech recognition results.",
        "ASR spont: Translation of automatic spontaneous speech recognition results.",
        "For the purpose of automatic evaluation, 7 human reference translations and automatic outputs by 14 different MT systems for each evaluation subscenario are available.",
        "In addition, we count on the results of a process of manual evaluation.",
        "For each subscenario, 400 test cases from 6 different system outputs were evaluated, by three human assessors each, in terms of adequacy and fluency on a 1-5 scale (LDC, 2005).",
        "A brief numerical description of these test beds is available in Table 1.",
        "It includes the number of human references and system outputs available, as well as the number of sentences per output, and the number of system outputs and sentences per system assessed.",
        "For the sake of completeness, we report the performance of the Automatic Speech Recognition (ASR) system, in terms of accuracy, over the source Chinese utterances, both at the word and sentence levels.",
        "Also, in order to give an idea of the translation quality exhibited by automatic systems, average adequacy and fluency scores are also provided.",
        "Our experiment requires a mechanism for evaluating the quality of evaluation metrics, i.e., a metaevaluation criterion.",
        "The two most prominent are:",
        "• Human Acceptability: Metrics are evaluated in terms of their ability to capture the degree of acceptability to humans of automatic translations, i.e., their ability to emulate human assessors.",
        "The underlying assumption is that good translations should be acceptable to human evaluators.",
        "Human acceptability is usually measured on the basis of correlation between automatic metric scores and human assessments of translation quality.",
        "• Human Likeness: Metrics are evaluated in terms of their ability to capture the features which distinguish human from automatic translations.",
        "The underlying assumption is that good translations should resemble human translations.",
        "Human likeness is usually measured on the basis of discriminative power (Lin and Och, 2004b; Amigo et al.,",
        "2005).",
        "In this work, metrics are evaluated both in terms of human acceptability and human likeness.",
        "In the case of human acceptability, metric quality is measured on the basis of correlation with human assessments both at the sentence and document (i.e., system) levels.",
        "We compute Pearson correlation coefficients.",
        "The sum of adequacy and fluency is used as a global measure of quality.",
        "Assessments from different judges have been averaged.",
        "In the case of human likeness, we use the probabilistic KING measure defined inside the qarla Framework (Amigo et al., 2005).",
        "KING represents the probability, estimated over the set of test cases, that the score attained by a human reference is equal or greater than the score attained by any automatic translation.",
        "Although KING computations do not require human assessments, for the sake of comparison, we have limited to the set of test cases counting on human assessments.",
        "Table 2 presents meta-evaluation results for a set of metric representatives from different linguistic levels over the three subscenarios defined ('CRR', 'ASR read' and 'ASR spont').",
        "Highest scores in each column have been highlighted.",
        "Lowest scores appear in italics.",
        "System-level Behavior",
        "At the system level (Rsys, columns 7-9), the highest quality is in general attained by metrics based on deep linguistic analysis, either syntactic or semantic.",
        "Among lexical metrics, the highest correlation is attained by BLEU and the variant of GTM rewarding longer matchings (e = 2).",
        "As to the impact of sentence ill-formedness, while most metrics at the lexical level suffer a significant variation across the three subscenarios, the performance of metrics at deeper linguistic levels is in general quite stable.",
        "However, in the case of the translation of automatically recognized spontaneous speech (ASR spont) we have found that the 'sr-cv-*' and 'sr-mr-*' metrics, respectively based on lexical overlapping and matching over semantic roles, suffer a very significant decrease far below the performance of most lexical metrics.",
        "Although 'sr-or-*' has performed well on other test beds (Gimenez and Marquez, 2007), its low performance over the BTEC data suggests that it is not fully portable across all kind of evaluation scenarios.",
        "Finally, it is highly remarkable the degree of robustness exhibited by semantic metrics introduced in Section 2.1.",
        "In particular, the metric variants based on lexical and morphosyntactic overlapping over discourse representations ('dr-or-*' and 'dr-Orp-*', respectively), obtain a high system-level correlation with human assessments across the three subscenarios.",
        "Sentence-level Behavior",
        "At the sentence level (KING and Rsnt, columns 1-6), highest quality is attained in most cases by metrics based on lexical matching.",
        "This result was expected since all MT systems are statistical and the test set is in-domain, that is it belongs to the",
        "CRR",
        "ASR read",
        "ASR spont",
        "#human-references",
        "7",
        "7",
        "7",
        "#system-outputs",
        "14",
        "14",
        "13",
        "#sentences",
        "500",
        "500",
        "500",
        "#OUtputSassessed",
        "6",
        "6",
        "6",
        "#sentencesassessed",
        "400",
        "400",
        "400",
        "Word Recognition Accuracy",
        " – ",
        "0.74",
        "0.68",
        "Sentence Recognition Accuracy",
        " – ",
        "0.23",
        "0.17",
        "Average Adequacy",
        "1.40",
        "1.02",
        "0.93",
        "Average Fluency",
        "1.16",
        "0.98",
        "0.98",
        "Table 2: Meta-evaluation results for a set of metric representatives from different linguistic levels same domain in which systems have been trained.",
        "Therefore, translation outputs have a strong tendency to share the sublanguage (i.e., word selection and word ordering) represented by the predefined set of human reference translations.",
        "Metrics based on lexical overlapping and matching over shallow syntactic categories and syntactic structures ('SP-Op-*', 'SP-Oc-*', 'CP-Op-*', 'CP-Oc-*', 'DP-Oi-*', 'DP-Oc-*', and 'DP-Or-*') perform similarly to lexical metrics.",
        "However, computing NIST scores over base phrase chunk sequences ('SP-NISTiob', 'SP-NISTc') is not as effective.",
        "Metrics based on headword chain matching ('DP-HWCw', 'DP-HWCc', 'DP-HWCr') suffer also a significant decrease.",
        "Interestingly, the metric based on syntactic tree matching ('CP-STM ) performed well in all scenarios.",
        "Metrics at the shallow semantic level suffer also a severe drop in performance.",
        "Particularly significant is the case case of the 'SR-Or metric, which does not consider any lexical information.",
        "Interestingly, the 'SR-O™' variant, which only differs in that it distinguishes between SRs associated to different verbs, performs slightly better.",
        "At the semantic level, metrics based on lexical and morphosyntactic overlapping over discourse representations ('DR-Or-* and 'DR-Orp-* ) suffer only a minor decrease, whereas semantic tree matching ('DR-STM ) reports as a specially bad predictor of human acceptability (Rsnt).",
        "However, the most remarkable result, in relation to the goal of this work, is that the behavior of syntactic and semantic metrics across the three evaluation subscenarios is, in general, quite stable – the three values in each subrow are in a very similar range.",
        "Therefore, answering the question posed in the introduction, sentence ill-formedness is not a limiting factor in the performance of linguistic metrics.",
        "Human Likeness",
        "Human Acceptability",
        "KING",
        "Rsnt",
        "Rsys",
        "ASR",
        "ASR",
        "ASR",
        "ASR",
        "ASR",
        "ASR",
        "Level",
        "Metric",
        "CRR",
        "read",
        "spont",
        "CRR",
        "read",
        "spont",
        "CRR",
        "read",
        "spont",
        "1-WER",
        "0.63",
        "0.69",
        "0.71",
        "0.47",
        "0.50",
        "0.48",
        "0.50",
        "0.32",
        "0.52",
        "1-PER",
        "0.71",
        "0.79",
        "0.79",
        "0.44",
        "0.48",
        "0.45",
        "0.67",
        "0.39",
        "0.60",
        "1-TER",
        "0.69",
        "0.75",
        "0.77",
        "0.49",
        "0.52",
        "0.50",
        "0.66",
        "0.36",
        "0.62",
        "BLEU",
        "0.69",
        "0.72",
        "0.73",
        "0.54",
        "0.53",
        "0.52",
        "0.79",
        "0.74",
        "0.62",
        "Lexical",
        "NIST",
        "0.79",
        "0.84",
        "0.85",
        "0.53",
        "0.54",
        "0.53",
        "0.12",
        "0.26",
        "-0.02",
        "GTM (e = 1)",
        "0.75",
        "0.81",
        "0.83",
        "0.50",
        "0.52",
        "0.52",
        "0.35",
        "0.10",
        "-0.09",
        "GTM (e = 2)",
        "0.72",
        "0.78",
        "0.79",
        "0.62",
        "0.64",
        "0.61",
        "0.78",
        "0.65",
        "0.62",
        "METEOR™ nsyn",
        "0.81",
        "0.86",
        "0.86",
        "0.44",
        "0.50",
        "0.48",
        "0.55",
        "0.39",
        "0.08",
        "ROUGEw_i.2",
        "0.74",
        "0.79",
        "0.81",
        "0.58",
        "0.60",
        "0.58",
        "0.53",
        "0.69",
        "0.43",
        "Oi",
        "0.74",
        "0.81",
        "0.82",
        "0.57",
        "0.62",
        "0.58",
        "0.77",
        "0.51",
        "0.34",
        "SP-Op-*",
        "0.75",
        "0.80",
        "0.82",
        "0.54",
        "0.59",
        "0.56",
        "0.77",
        "0.54",
        "0.48",
        "SP-Oc-*",
        "0.74",
        "0.81",
        "0.82",
        "0.54",
        "0.59",
        "0.55",
        "0.82",
        "0.52",
        "0.49",
        "Shallow",
        "SP-NIST;",
        "0.79",
        "0.84",
        "0.85",
        "0.52",
        "0.53",
        "0.52",
        "0.10",
        "0.25",
        "-0.03",
        "Syntactic",
        "SP-NISTp",
        "0.74",
        "0.78",
        "0.80",
        "0.44",
        "0.42",
        "0.43",
        "-0.02",
        "0.24",
        "0.04",
        "SP-NISTi0b",
        "0.65",
        "0.69",
        "0.70",
        "0.33",
        "0.32",
        "0.35",
        "-0.09",
        "0.17",
        "-0.09",
        "SP-NISTC",
        "0.55",
        "0.59",
        "0.59",
        "0.24",
        "0.22",
        "0.25",
        "-0.07",
        "0.19",
        "0.08",
        "CV-Op-k",
        "0.75",
        "0.81",
        "0.82",
        "0.57",
        "0.63",
        "0.59",
        "0.84",
        "0.67",
        "0.52",
        "CY-Oc-k",
        "0.74",
        "0.80",
        "0.82",
        "0.60",
        "0.64",
        "0.61",
        "0.71",
        "0.53",
        "0.43",
        "DP-O;-*",
        "0.68",
        "0.75",
        "0.76",
        "0.48",
        "0.50",
        "0.50",
        "0.84",
        "0.77",
        "0.67",
        "DP-Oc-*",
        "0.71",
        "0.76",
        "0.77",
        "0.41",
        "0.46",
        "0.43",
        "0.76",
        "0.65",
        "0.71",
        "Syntactic",
        "DP-Or-*",
        "0.75",
        "0.80",
        "0.81",
        "0.51",
        "0.53",
        "0.51",
        "0.81",
        "0.75",
        "0.62",
        "DP-HWC™",
        "0.54",
        "0.57",
        "0.57",
        "0.29",
        "0.32",
        "0.28",
        "0.73",
        "0.74",
        "0.37",
        "DP-HWCC",
        "0.48",
        "0.51",
        "0.52",
        "0.17",
        "0.18",
        "0.22",
        "0.73",
        "0.64",
        "0.67",
        "DP-HWCr",
        "0.44",
        "0.49",
        "0.48",
        "0.20",
        "0.21",
        "0.25",
        "0.71",
        "0.58",
        "0.56",
        "CP-STM",
        "0.71",
        "0.77",
        "0.80",
        "0.53",
        "0.56",
        "0.54",
        "0.65",
        "0.58",
        "0.47",
        "SR-Mr-*",
        "0.40",
        "0.43",
        "0.45",
        "0.29",
        "0.28",
        "0.29",
        "0.52",
        "0.60",
        "0.20",
        "SR-Or-*",
        "0.45",
        "0.49",
        "0.51",
        "0.35",
        "0.35",
        "0.36",
        "0.56",
        "0.58",
        "0.14",
        "Shallow",
        "SR-Or",
        "0.31",
        "0.33",
        "0.35",
        "0.16",
        "0.15",
        "0.18",
        "0.68",
        "0.73",
        "0.53",
        "Semantic",
        "SR-M™-*",
        "0.38",
        "0.41",
        "0.42",
        "0.33",
        "0.34",
        "0.34",
        "0.79",
        "0.81",
        "0.42",
        "SR-O™-*",
        "0.40",
        "0.44",
        "0.45",
        "0.36",
        "0.38",
        "0.38",
        "0.64",
        "0.72",
        "0.72",
        "SR-Or„",
        "0.36",
        "0.40",
        "0.40",
        "0.27",
        "0.31",
        "0.29",
        "0.34",
        "0.78",
        "0.38",
        "DR-Or-*",
        "0.67",
        "0.73",
        "0.75",
        "0.48",
        "0.53",
        "0.50",
        "0.86",
        "0.74",
        "0.77",
        "Semantic",
        "DR-Orp-*",
        "0.59",
        "0.64",
        "0.65",
        "0.34",
        "0.35",
        "0.33",
        "0.84",
        "0.78",
        "0.95",
        "DR-STM",
        "0.58",
        "0.63",
        "0.65",
        "0.23",
        "0.26",
        "0.26",
        "0.75",
        "0.62",
        "0.67",
        "Improved Sentence-level Behavior",
        "By inspecting particular instances, we have found that linguistic metrics are, in many cases, unable to produce any evaluation result.",
        "The number of un-scored sentences is particularly significant in the case of SR metrics.",
        "For instance, the 'SR-cv-*' metric is unable to confer an evaluation score in 57% of the cases.",
        "Several reasons explain this fact.",
        "The first and most important is that linguistic metrics rely on automatic processors trained on out-of-domain data, which are, thus, prone to error.",
        "Second, we argue that the test bed itself does not allow for fully exploiting the capabilities of these metrics.",
        "Apart from being based on a reduced vocabulary (2,346 distinct words), test cases consist mostly of very short segments (14.64 words on average), which in their turn consist of even shorter sentences (8.55 words on average).",
        "A possible solution could be to back off to a measure of lexical similarity in those cases in which linguistic processors are unable to produce any linguistic analysis.",
        "This should significantly increase their recall.",
        "With that purpose, we have designed two new variants for each of these metrics.",
        "Given a linguistic metric x, we define:",
        "• xb – by backing off to lexical overlapping, Oi, only when the linguistic processor was not able to produce a parsing.",
        "Lexical scores are conveniently scaled so that they are in a similar range to x scores.",
        "Specifically, we multiply them by the average x score attained over all other test cases for which the parser succeeded.",
        "Formally, given a test case t belonging to a set of test cases T:",
        "Human Likeness",
        "Human Acceptability",
        "KING",
        "Rsnt",
        "Rsys",
        "ASR",
        "ASR",
        "ASR",
        "ASR",
        "ASR",
        "ASR",
        "Level",
        "Metric",
        "CRR",
        "read",
        "spont",
        "CRR",
        "read",
        "spont",
        "CRR",
        "read",
        "spont",
        "Lexical",
        "NIST",
        "0.79",
        "0.84",
        "0.85",
        "0.53",
        "0.54",
        "0.53",
        "0.12",
        "0.26",
        "-0.02",
        "GTM (e = 2)",
        "0.72",
        "0.78",
        "0.79",
        "0.62",
        "0.64",
        "0.61",
        "0.78",
        "0.65",
        "0.62",
        "METEORiunsyn.",
        "0.81",
        "0.86",
        "0.86",
        "0.44",
        "0.50",
        "0.48",
        "0.55",
        "0.39",
        "0.08",
        "Oi",
        "0.74",
        "0.81",
        "0.82",
        "0.57",
        "0.62",
        "0.58",
        "0.77",
        "0.51",
        "0.34",
        "CP-Op-*",
        "0.75",
        "0.81",
        "0.82",
        "0.57",
        "0.63",
        "0.59",
        "0.84",
        "0.67",
        "0.52",
        "Syntactic",
        "CP-Oc-*",
        "0.74",
        "0.80",
        "0.82",
        "0.60",
        "0.64",
        "0.61",
        "0.71",
        "0.53",
        "0.43",
        "DP-O;-*",
        "0.68",
        "0.75",
        "0.76",
        "0.48",
        "0.50",
        "0.50",
        "0.84",
        "0.77",
        "0.67",
        "SR-Mr-*",
        "0.40",
        "0.43",
        "0.45",
        "0.29",
        "0.28",
        "0.29",
        "0.52",
        "0.60",
        "0.20",
        "SR- Mr-*6",
        "0.68",
        "0.72",
        "0.73",
        "0.31",
        "0.30",
        "0.31",
        "0.52",
        "0.60",
        "0.20",
        "SR-Mr-*i",
        "0.84",
        "0.86",
        "0.88",
        "0.34",
        "0.34",
        "0.34",
        "0.56",
        "0.63",
        "0.25",
        "SR-Or-*",
        "0.45",
        "0.49",
        "0.51",
        "0.35",
        "0.35",
        "0.36",
        "0.56",
        "0.58",
        "0.14",
        "SR-Or-*b",
        "0.71",
        "0.75",
        "0.78",
        "0.38",
        "0.38",
        "0.38",
        "0.56",
        "0.58",
        "0.14",
        "SR-Or-*i",
        "0.84",
        "0.88",
        "0.89",
        "0.41",
        "0.41",
        "0.41",
        "0.62",
        "0.60",
        "0.22",
        "SR-Or",
        "0.31",
        "0.33",
        "0.35",
        "0.16",
        "0.15",
        "0.18",
        "0.68",
        "0.73",
        "0.53",
        "SR-Orb",
        "0.54",
        "0.58",
        "0.60",
        "0.19",
        "0.18",
        "0.20",
        "0.68",
        "0.73",
        "0.53",
        "Shallow",
        "SR-Ori",
        "0.72",
        "0.77",
        "0.79",
        "0.26",
        "0.26",
        "0.27",
        "0.80",
        "0.73",
        "0.67",
        "Semantic",
        "SR-M™-*",
        "0.38",
        "0.41",
        "0.42",
        "0.33",
        "0.34",
        "0.34",
        "0.79",
        "0.81",
        "0.42",
        "SR-Mr,,-*b",
        "0.70",
        "0.73",
        "0.74",
        "0.34",
        "0.35",
        "0.34",
        "0.79",
        "0.81",
        "0.42",
        "SR-Mrt,-*i",
        "0.88",
        "0.90",
        "0.92",
        "0.36",
        "0.38",
        "0.37",
        "0.81",
        "0.82",
        "0.45",
        "SR-O™-*",
        "0.40",
        "0.44",
        "0.45",
        "0.36",
        "0.38",
        "0.38",
        "0.64",
        "0.72",
        "0.72",
        "SR-Or,,-*b",
        "0.72",
        "0.76",
        "0.77",
        "0.38",
        "0.40",
        "0.39",
        "0.64",
        "0.72",
        "0.72",
        "SR-Or„-*i",
        "0.88",
        "0.90",
        "0.91",
        "0.40",
        "0.42",
        "0.41",
        "0.69",
        "0.74",
        "0.74",
        "SR-Or„",
        "0.36",
        "0.40",
        "0.40",
        "0.27",
        "0.31",
        "0.29",
        "0.34",
        "0.78",
        "0.38",
        "SR-Or^b",
        "0.66",
        "0.70",
        "0.71",
        "0.29",
        "0.32",
        "0.30",
        "0.34",
        "0.78",
        "0.38",
        "SR-Orvi",
        "0.83",
        "0.86",
        "0.88",
        "0.33",
        "0.36",
        "0.33",
        "0.49",
        "0.82",
        "0.56",
        "DR-Or-*",
        "0.67",
        "0.73",
        "0.75",
        "0.48",
        "0.53",
        "0.50",
        "0.86",
        "0.74",
        "0.77",
        "DR-Or-*b",
        "0.69",
        "0.75",
        "0.77",
        "0.50",
        "0.53",
        "0.50",
        "0.90",
        "0.69",
        "0.56",
        "Semantic",
        "DR-Or-*;",
        "0.83",
        "0.87",
        "0.89",
        "0.53",
        "0.57",
        "0.53",
        "0.88",
        "0.70",
        "0.61",
        "DR-Orp-*",
        "0.59",
        "0.64",
        "0.65",
        "0.34",
        "0.35",
        "0.33",
        "0.84",
        "0.78",
        "0.95",
        "DR-Orp-*b",
        "0.61",
        "0.65",
        "0.67",
        "0.35",
        "0.36",
        "0.34",
        "0.86",
        "0.71",
        "0.57",
        "DR-Orp-*i",
        "0.80",
        "0.84",
        "0.85",
        "0.43",
        "0.46",
        "0.43",
        "0.90",
        "0.75",
        "0.70",
        "DR-STM",
        "0.58",
        "0.63",
        "0.65",
        "0.23",
        "0.26",
        "0.26",
        "0.75",
        "0.62",
        "0.67",
        "DR-STM-b",
        "0.64",
        "0.68",
        "0.71",
        "0.23",
        "0.26",
        "0.27",
        "0.75",
        "0.62",
        "0.67",
        "DR-STM-i",
        "0.83",
        "0.87",
        "0.87",
        "0.33",
        "0.36",
        "0.36",
        "0.84",
        "0.63",
        "0.66",
        "where ok(T) is the subset of test cases in T which were successfully parsed.",
        "• Xi – by linearly interpolating x and Oiscores for all test cases, via arithmetic mean:",
        "In both cases, system-level scores are calculated by averaging over all sentence-level scores.",
        "Table 3 shows meta-evaluation results on the performance of these variants for several representatives from the SR and DR families.",
        "For the sake of comparison, we also show the scores attained by the base versions, and by some of the top-scoring metrics from other linguistic levels.",
        "The first observation is that in all cases the new variants outperform their respective base metric, being linear interpolation the best alternative.",
        "The increase is particularly significant in terms of human likeness.",
        "New variants even outperform lexical metrics, including the Oi metric, which suggests that, in spite of its simplicity, this is a valid combination scheme.",
        "However, in terms of human acceptability, the gain is only moderate, and still their performance is far from top-scoring metrics.",
        "Sentence-level improvements are also reflected at the system level, although to a lesser extent.",
        "Interestingly, in the case of the translation of automatically recognized spontaneous speech (ASR spont, column 9), mixing with lexical overlapping improves the low-performance 'SR-cv and 'SR-orv' metrics, at the same time that it causes a significant drop in the high-performance 'DR-cv' and 'DR-orp' metrics.",
        "Still, the performance of linguistic metrics at the sentence level is under the performance of lexical metrics.",
        "This is not surprising.",
        "After all, apart from relying on automatic processors, linguistic metrics focus on very partial aspects of quality.",
        "However, since they operate at complementary quality dimensions, their scores are suitable for being combined."
      ]
    },
    {
      "heading": "4. Conclusions and Future Work",
      "text": [
        "We have presented an empirical study on the robustness of a heterogeneous set of metrics operating at different linguistic levels for the particular case of Chinese-to-English speech translation of basic travel expressions.",
        "As an additional contribution, we have presented a novel family of metrics which operate at the semantic level by analyzing discourse representations.",
        "Corroborating previous findings by Gimenez and Marquez (2007), results at the system level, show that metrics guided by deeper linguistic knowledge, either syntactic or semantic, are, in general, more effective and stable than metrics which limit their scope to the lexical dimension.",
        "However, at the sentence level, results indicate that metrics based on deep linguistic analysis are not as reliable overall quality estimators as lexical metrics, at least when applied to low quality translations, as it is the case.",
        "This behavior is mainly attributable a drop in recall due to parsing errors.",
        "By inspecting particular sentences we have observed that in many cases these metrics are unable to produce any result.",
        "In that respect, we have showed that backing off to lexical similarity is a valid and effective strategy so as to improve the performance of these metrics.",
        "But the most remarkable result, in relation to the goal of this work, is that syntactic and semantic metrics exhibit a very robust behavior across the three evaluation subscenarios of decreasing translation quality analyzed.",
        "Therefore, sentence ill-formedness is not a limiting factor in the performance of linguistic metrics.",
        "The quality drop, when moving from the system to the sentence level, seems, thus, more related to a shift in the application domain.",
        "For future work, we are currently studying the possibility of further improving the sentence-level behavior of present evaluation methods by combining the outcomes of metrics at different linguistic levels into a single measure of quality (citation omitted for the sake of anonymity)."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This research has been funded by the Spanish Ministry of Education and Science, project OpenMT (TIN2006-15307-C03-02).",
        "Our NLP group has been recognized as a Quality Research Group (2005 SGR-00130) by DURSI, the Research Department of the Catalan Government.",
        "We are grateful to the SLT Evaluation Campaign organizers and participants for providing such valuable test beds."
      ]
    }
  ]
}
