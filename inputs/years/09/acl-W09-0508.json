{
  "info": {
    "authors": [
      "Pierre Lison",
      "Geert-Jan M. Kruijff"
    ],
    "book": "Proceedings of SRSL 2009, the 2nd Workshop on Semantic Representation of Spoken Language",
    "id": "acl-W09-0508",
    "title": "An Integrated Approach to Robust Processing of Situated Spoken Dialogue",
    "url": "https://aclweb.org/anthology/W09-0508",
    "year": 2009
  },
  "references": [
    "acl-D07-1071",
    "acl-I05-1015",
    "acl-P02-1041",
    "acl-P04-1015",
    "acl-W03-1013"
  ],
  "sections": [
    {
      "text": [
        "Language Technology Lab, DFKI GmbH, Saarbrücken, Germany pierre.lison@dfki.de",
        "Language Technology Lab,",
        "DFKI GmbH,",
        "Saarbrücken, Germany gj@dfki.de",
        "Spoken dialogue is notoriously hard to process with standard NLP technologies.",
        "Natural spoken dialogue is replete with disfluent, partial, elided or ungrammatical utterances, all of which are very hard to accommodate in a dialogue system.",
        "Furthermore, speech recognition is known to be a highly error-prone task, especially for complex, open-ended discourse domains.",
        "The combination of these two problems - ill-formed and/or misrecognised speech inputs - raises a major challenge to the development of robust dialogue systems.",
        "We present an integrated approach for addressing these two issues, based on a incremental parser for Combinatory Cate-gorial Grammar.",
        "The parser takes word lattices as input and is able to handle ill-formed and misrecognised utterances by selectively relaxing its set of grammatical rules.",
        "The choice of the most relevant interpretation is then realised via a discriminative model augmented with contextual information.",
        "The approach is fully implemented in a dialogue system for autonomous robots.",
        "Evaluation results on a Wizard of Oz test suite demonstrate very significant improvements in accuracy and robustness compared to the baseline."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Spoken dialogue is often considered to be one of the most natural means of interaction between a human and a robot.",
        "It is, however, notoriously hard to process with standard language processing technologies.",
        "Dialogue utterances are often incomplete or ungrammatical, and may contain numerous disfluencies like fillers (err, uh, mm), repetitions, self-corrections, etc.",
        "Rather than getting crisp-and-clear commands such as \"Put the red ball inside the box!",
        "\", it is more likely the robot will hear such kind of utterance: \"right, now, could you uh put the red ball yeah inside the ba/ box!\".",
        "This is natural behaviour in human-human interaction (Fernandez and Ginzburg, 2002) and can also be observed in several domain-specific corpora for human-robot interaction (Topp et al., 2006).",
        "Moreover, even in the (rare) case where the utterance is perfectly well-formed and does not contain any kind of disfluencies, the dialogue system still needs to accomodate the various speech recognition errors thay may arise.",
        "This problem is particularly acute for robots operating in real-world noisy environments and deal with utterances pertaining to complex, open-ended domains.",
        "The paper presents a new approach to address these two difficult issues.",
        "Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07).",
        "In order to account for natural spoken language phenomena (more flexible word order, missing words, etc.",
        "), they augment their grammar framework with a small set of non-standard combinatory rules, leading to a relaxation of the grammatical constraints.",
        "A discriminative model over the parses is coupled with the parser, and is responsible for selecting the most likely interpretation(s) among the possible ones.",
        "In this paper, we extend their approach in two important ways.",
        "First, ZC07 focused on the treatment of ill-formed input, and ignored the speech recognition issues.",
        "Our system, to the contrary, is able to deal with both ill-formed and misrec-ognized input, in an integrated fashion.",
        "This is done by augmenting the set of non-standard combinators with new rules specifically tailored to deal with speech recognition errors.",
        "Second, the only features used by ZC07 are syntactic features (see 3.4 for details).",
        "We significantly extend the range of features included in the discriminative model, by incorporating not only syntactic, but also acoustic, semantic and contextual information into the model.",
        "An overview of the paper is as follows.",
        "We first describe in Section 2 the cognitive architecture in which our system has been integrated.",
        "We then discuss the approach in detail in Section 3.",
        "Finally, we present in Section 4 the quantitative evaluations on a WOZ test suite, and conclude."
      ]
    },
    {
      "heading": "2. Architecture",
      "text": [
        "The approach we present in this paper is fully implemented and integrated into a cognitive architecture for autonomous robots.",
        "A recent version of this system is described in (Hawes et al., 2007).",
        "It is capable of building up visuo-spatial models of a dynamic local scene, continuously plan and execute manipulation actions on objects within that scene.",
        "The robot can discuss objects and their material-and spatial properties for the purpose of visual learning and manipulation tasks.",
        "Figure 2 illustrates the architecture schema for the communication subsystem incorporated in the cognitive architecture (only the comprehension part is shown).",
        "Starting with ASR, we process the audio signal to establish a word lattice containing statistically ranked hypotheses about word sequences.",
        "Subsequently, parsing constructs grammatical analyses for the given word lattice.",
        "A grammatical analysis constructs both a syntactic analysis of the utterance, and a representation of its meaning.",
        "The analysis is based on an incremental chart parserfor Combinatory Categorial Grammar (Steedman and Baldridge, 2009).",
        "These meaning representations are ontologically richly sorted, relational structures, formulated in a (propositional) description logic, more precisely in the HLDS formalism (Baldridge and Kruijff, 2002).",
        "The parser compacts all meaning representations into a single packed logical form (Carroll and Oepen, 2005; Kruijff et al., 2007).",
        "A packed LF represents content similar across the different analyses as a single graph, using over-and underspecification of how different nodes can be connected to capture lexical and syntactic forms of ambiguity.",
        "At the level of dialogue interpretation, a packed logical form is resolved against a SDRS-like dialogue model (Asher and Lascarides, 2003) to establish contextual co-reference and dialogue moves.",
        "Linguistic interpretations must finally be associated with extralinguistic knowledge about the environment - dialogue comprehension hence needs to connect with other subarchitectures like vision, spatial reasoning or planning.",
        "We realise this information binding between different modalities via a specific module, called the \"binder\", which is responsible for the ontology-based mediation ac-cross modalities (Jacobsson et al., 2008).",
        "The combinatorial nature of language provides virtually unlimited ways in which we can communicate meaning.",
        "This, of course, raises the question of how precisely an utterance should then be understood as it is being heard.",
        "Empirical studies have investigated what information humans use when comprehending spoken utterances.",
        "An important observation is that interpretation in context plays a crucial role in the comprehension of utterance as it unfolds (Knoeferle and Crocker, 2006).",
        "During utterance comprehension, humans combine linguistic information with scene understanding and \"world knowledge\".",
        "Several approaches in situated dialogue for human-robot interaction have made similar obserrobot's understanding can be improved by relating utterances to the situated context.",
        "As we will see in the next section, by incorporating contextual information into our model, our approach to robust processing of spoken dialogue seeks to exploit this important insight."
      ]
    },
    {
      "heading": "3. Approach",
      "text": [
        "Our approach to robust processing of spoken dialogue rests on the idea of grammar relaxation: the grammatical constraints specified in the grammar are \"relaxed\" to handle slightly ill-formed or misrecognised utterances.",
        "Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007).",
        "In Combinatory Categorial Grammar, the rules are used to assemble categories to form larger pieces of syntactic and semantic structure.",
        "The standard rules are application (<, >), composition (B), and type raising (T) (Steedman and Baldridge, 2009).",
        "Several types of non-standard rules have been introduced.",
        "We describe here the two most important ones: the discourse-level composition rules, and the ASR correction rules.",
        "We invite the reader to consult (Lison, 2008) for more details on the complete set of grammar relaxation rules.",
        "In natural spoken dialogue, we may encounter utterances containing several independent \"chunks\" without any explicit separation (or only a short pause or a slight change in intonation), such as",
        "(1) \"yes take the ball no the other one on your left right and now put it in the box.\"",
        "Even if retrieving a fully structured parse for this utterance is difficult to achieve, it would be useful to have access to a list of smaller \"discourse units\".",
        "Syntactically speaking, a discourse unit can be any type of saturated atomic categories from a simple discourse marker to a full sentence.",
        "The type raising rule Tdu allows the conversion of atomic categories into discourse units:",
        "where A represents an arbitrary saturated atomic category (s, np, pp, etc.",
        ").",
        "The rule >C is responsible for the integration of two discourse units into a single structure:",
        "Speech recognition is a highly error-prone task.",
        "It is however possible to partially alleviate this problem by inserting new error-correction rules (more precisely, new lexical entries) for the most frequently misrecognised words.",
        "Ifwe notice e.g. thatthe ASRsystem frequently substitutes the word\"wrong\" forthe word \"round\" during the recognition (because oftheirphonolog-ical proximity), we can introduce a new lexical entry in the lexicon in order to correct this error:",
        "round h adj : ©attitude(wrong) (2)",
        "A set of thirteen new lexical entries of this type have been added to our lexicon to account for the most frequent recognition errors.",
        "Using more powerful grammar rules to relax the grammatical analysis tends to increase the number of parses.",
        "We hence need a a mechanism to discriminate among the possible parses.",
        "The task of selecting the most likely interpretation among a set of possible ones is called parse selection.",
        "Once all the possible parses for a given utterance are computed, they are subsequently filtered or selected in order to retain only the most likely interpreta-tion(s).",
        "This is done via a (discriminative) statistical model covering a large number of features.",
        "Formally, the task is defined as a function F : X – Y where the domain X is the set of possible inputs (in our case, X is the set of possible word lattices), and Y the set of parses.",
        "We assume:",
        "1.",
        "A function GEN(x) which enumerates all possible parses for an input x.",
        "In our case, this function simply represents the set of parses of x which are admissible according to the CCG grammar.",
        "2.",
        "A d-dimensional feature vector f(x,y) e Kd, representing specific features of the pair (x,y).",
        "It can include various acoustic, syntactic, semantic or contextual features which can be relevant in discriminating the parses.",
        "3.",
        "A parameter vector w eSd.",
        "The function F, mapping a word lattice to its most likely parse, is then defined as:",
        "yGGEN(x)",
        "where wT • f(x,y) is the inner product J2<d=i ws fs(x, y), and can be seen as a measure of the \"quality\" of the parse.",
        "Given the parameters w, the optimal parse of a given utterance x can be therefore easily determined by enumerating all the parses generated by the grammar, extracting their features, computing the inner product wT • f (x,y), and selecting the parse with the highest score.",
        "The task of parse selection is an example of structured classification problem, which is the problem of predicting an output y from an input x, where the output y has a rich internal structure.",
        "In the specific case of parse selection, x is a word lattice, and y a logical form.",
        "In order to estimate the parameters w, we need a set of training examples.",
        "Unfortunately, no corpus of situated dialogue adapted to our task domain is available to this day, let alone semantically annotated.",
        "The collection of in-domain data via Wizard of Oz experiments being a very costly and time-consuming process, we followed the approach advocated in (Weilhammer et al., 2006) and generated a corpus from a handwritten task grammar.",
        "To this end, we first collected a small set of WoZ data, totalling about a thousand utterances.",
        "This set is too small to be directly used as a corpus for statistical training, but sufficient to capture the most frequent linguistic constructions in this particular context.",
        "Based on it, we designed a domain-specific CFG grammar covering most of the utterances.",
        "Each rule is associated to a semantic HLDS representation.",
        "Weights are automatically assigned to each grammar rule by parsing our corpus, hence leading to a small stochastic CFG grammar augmented with semantic information.",
        "Once the grammar is specified, it is randomly traversed a large number of times, resulting in a larger set (about 25.000) of utterances along with their semantic representations.",
        "Since we are interested in handling errors arising from speech recognition, we also need to \"simulate\" the most frequent recognition errors.",
        "To this end, we synthe- sise each string generated by the domain-specific CFG grammar, using a text-to-speech engine, feed the audio stream to the speech recogniser, and retrieve the recognition result.",
        "Via this technique, we are able to easily collect a large amount of training data.",
        "The algorithm we use to estimate the parameters w using the training data is a perceptron.",
        "The algorithm is fully online - it visits each example in turn and updates w if necessary.",
        "Albeit simple, the algorithm has proven to be very efficient and accurate for the task of parse selection (Collins and Roark, 2004; Collins, 2004; Zettlemoyer and",
        "Collins, 2005; Zettlemoyer and Collins, 2007).",
        "The pseudo-code for the online learning algorithm is detailed in [Algorithm 1].",
        "It works as follows: the parameters w are first initialised to some arbitrary values.",
        "Then, for each pair (xi, zi) in the training set, the algorithm searchs for the parse y' with the highest score according to the current model.",
        "If this parse happens to match the best parse which generates zi (which we shall denote y*), we move to the next example.",
        "Else, we perform a simple perceptron update on the parameters:",
        "The iteration on the training set is repeated T times, or until convergence.",
        "The most expensive step in this algorithm is the calculation of y' = argmaxy€GEN(x.)",
        "wT • f(xi, y) - this is the decoding problem.",
        "It is possible to prove that, provided the training set (xi, zi) is separable with margin 5 > 0, the algorithm is assured to converge after a finite number of iterations to a model with zero training errors (Collins and Roark, 2004).",
        "See also (Collins, 2004) for convergence theorems and proofs.",
        "As we have seen, the parse selection operates by enumerating the possible parses and selecting the Algorithm 1 Online perceptron learning",
        "- T: number of iterations over the training set",
        "- GEN(x): function enumerating possible parses for an input x, according to the CCG grammar.",
        "- GEN(x, z): function enumerating possible parses for an input x and which have semantics z, according to the CCG grammar.",
        "- L(y) maps a parse tree y to its logical form.",
        "- Initial parameter vector w0 % Initialise % Loop T times on the training examples for t = 1...T do for i = 1...n do % Compute best parse according to current model Let y' = argmaxy6GEN(x.)",
        "w • f (xi,y) % If the decoded parse = expected parse, update the parameters % Search the best parse for utterance xi with semantics zi",
        "Let y* = argmaxseGEN(I,1z.)",
        "wT ^ f (xi,y) % Update parameter vector w Setw = w + f(xi,y*) - f(xi,y')",
        "end for end for return parameter vector w",
        "one with the highest score according to the linear model parametrised by w.",
        "The accuracy of our method crucially relies on the selection of \"good\" features f(x, y) for our model - that is, features which help discriminating the parses.",
        "They must also be relatively cheap to compute.",
        "In our model, the features are of four types: semantic features, syntactic features, contextual features, and speech recognition features.",
        "What are the substructures of a logical form which may be relevant to discriminate the parses?",
        "We define features on the following information sources:",
        "1.",
        "Nominals: for each possible pair (prop, sort), we include a feature fi in f(x, y) counting the number of nominals with ontological sort sort and proposition prop in the logical form.",
        "2.",
        "Ontological sorts: occurrences of specific ontological sorts in the logical form.",
        "3.",
        "Dependency relations: following (Clark and Curran, 2003), we also model the dependency structure of the logical form.",
        "Each dependency relation is defined as a triple (sorta, sortb, label), where sorta denotes the sort of the incoming nominal, sortb the sort of the outgoing nominal, and label is the relation label.",
        "4.",
        "Sequences of dependency relations: number of occurrences of particular sequences (ie.",
        "bigram counts) of dependency relations.",
        "The features on nominals and ontological sorts aim at modeling (aspects of) lexical semantics -e.g. which meanings are the most frequent for a given word -, whereas the features on relations and sequence of relations focus on sentential semantics - which dependencies are the most frequent.",
        "These features therefore help us handle lexical and syntactic ambiguities.",
        "By \"syntactic features\", we mean features associated to the derivational history of a specific parse.",
        "The main use of these features is to penalise to a correct extent the application of the non-standard rules introduced into the grammar.",
        "To this end, we include in the feature vector f (x, y) a new feature for each non-standard rule, which counts the number of times the rule was applied in the parse.",
        "In the derivation shown in the figure 4, the rule corr (correction of a speech recognition error) is applied once, so the corresponding feature value is set to 1.",
        "The feature values for the remaining rules are set to 0, since they are absent from the parse.",
        "These syntactic features can be seen as a penalty given to the parses using these non-standard rules, thereby giving a preference to the \"normal\" parses over them.",
        "This mechanism ensures that the grammar relaxation is only applied \"as a last resort\" when the usual grammatical analysis fails to provide a full parse.",
        "Of course, depending on the relative frequency of occurrence of these rules in the training corpus, some of them will be more strongly penalised than others.",
        "As we have already outlined in the background section, one striking characteristic of spoken dialogue is the importance of context.",
        "Understanding the visual and discourse contexts is crucial to resolve potential ambiguities and compute the most likely interpretation(s) of a given utterance.",
        "The feature vector f(x, y) therefore includes various features related to the context:",
        "1.",
        "Activated words: our dialogue system maintains in its working memory a list of contextually activated words (cfr.",
        "(Lison and Krui-jff, 2008)).",
        "This list is continuously updated as the dialogue and the environment evolves.",
        "For each context-dependent word, we include one feature counting the number of times it appears in the utterance string.",
        "2.",
        "Expected dialogue moves: for each possible dialogue move, we include one feature indicating if the dialogue move is consistent with the current discourse model.",
        "These features ensure for instance that the dialogue move following a QuestionYN is a Accept, Reject or another question (e.g. for clarification requests), but almost never an Opening.",
        "3.",
        "Expected syntactic categories: for each atomic syntactic category in the CCG grammar, we include one feature indicating if the category is consistent with the current discourse model.",
        "These features can be used to handle sentence fragments.",
        "Finally, the feature vector f(x,y) also includes features related to the speech recognition.",
        "The ASR module outputs a set of (partial) recognition hypotheses, packed in a word lattice.",
        "One example of such a structure is given in Figure 5.",
        "Each recognition hypothesis is provided with an associated confidence score, and we want to favour the hypotheses with high confidence scores, which are, according to the statistical models incorporated in the ASR, more likely to reflect what was uttered.",
        "To this end, we introduce three features: the acoustic confidence score (confidence score provided by the statistical models included in the ASR), the semantic confidence score (based on a \"concept model\" also provided by the ASR), and the ASR ranking (hypothesis rank in the word lattice, from best to worst)."
      ]
    },
    {
      "heading": "4. Experimental evaluation",
      "text": [
        "We performed a quantitative evaluation of our approach, using its implementation in a fully integrated system (cf.",
        "Section 2).",
        "To set up the experiments for the evaluation, we have gathered a corpus of human-robot spoken dialogue for our task-domain, which we segmented and annotated manually with their expected semantic interpretation.",
        "The data set contains 195 individual utterances along with their complete logical forms.",
        "Three types of quantitative results are extracted from the evaluation results: exact-match, partial-match, and word error rate.",
        "Tables 1, 2 and 3 illustrate the results, broken down by use of grammar relaxation, use of parse selection, and number of recognition hypotheses considered.",
        "Each line in the tables corresponds to a possible configuration.",
        "Tables 1 and 2 give the precision, recall and F\\ value for each configuration (respectively for the exact-and partial-match), and Table 3 gives the Word Error Rate [WER].",
        "The first line corresponds to the baseline: no grammar relaxation, no parse selection, and use of the first NBest recognition hypothesis.",
        "The last line corresponds to the results with the full approach: grammar relaxation, parse selection, and use of 10 recognition hypotheses.",
        "Here are the comparative results we obtained:",
        "• Regarding the exact-match results between the baseline and our approach (grammar relaxation and parse selection with all features activated for NBest 10), the Fi-measure climbs from 43.0 % to 67.2 %, which means a relative difference of 56.3 %.",
        "• For the partial-match, the F1 measure goes from 68.0 % for the baseline to 87.3 % for our approach - a relative increase of 28.4 %.",
        "• We obverse a significant decrease in WER: we go from 20.5 % for the baseline to 15.7 % with our approach.",
        "The difference is statistically significant (p-value for t-tests is 0.036), and the relative decrease of 23.4 %."
      ]
    },
    {
      "heading": "5. Conclusions",
      "text": [
        "We presented an integrated approach to the processing of (situated) spoken dialogue, suited to the specific needs and challenges encountered in human-robot interaction.",
        "In order to handle disfluent, partial, ill-formed or misrecognized utterances, the grammar used by the parser is \"relaxed\" via the introduction of a set of non-standard combinators which allow for the insertion/deletion of specific words, the combination of discourse fragments or the correction of speech recognition errors.",
        "The relaxed parser yields a (potentially large) set of parses, which are then packed and retrieved by the parse selection module.",
        "The parse selection is based on a discriminative model exploring a set of relevant semantic, syntactic, contextual and acoustic features extracted for each parse.",
        "The parameters of this model are estimated against an automatically generated corpus of ( utterance, logical form) pairs.",
        "The learning algorithm is an percep-tron, a simple albeit efficient technique for parameter estimation.",
        "As forthcoming work, we shall examine the potential extension of our approach in new directions, such as the exploitation of parse selection for incremental scoring/pruning of the parse chart, the introduction of more refined contextual features, or the use of more sophisticated learning algorithms, such as Support Vector Machines.",
        "Size of word lattice (number of NBests)",
        "Grammar relaxation",
        "Parse selection",
        "Precision",
        "Recall",
        "F1 -value",
        "(Baseline)",
        "1",
        "No",
        "No",
        "40.9",
        "45.2",
        "43.0",
        "1",
        "No",
        "Yes",
        "59.0",
        "54.3",
        "56.6",
        "1",
        "Yes",
        "Yes",
        "52.7",
        "70.8",
        "60.4",
        "3",
        "Yes",
        "Yes",
        "55.3",
        "82.9",
        "66.3",
        "5",
        "Yes",
        "Yes",
        "55.6",
        "84.0",
        "66.9",
        "(Full approach)",
        "10",
        "Yes",
        "Yes",
        "55.6",
        "84.9",
        "67.2",
        "Size of word lattice (number of NBests)",
        "Grammar relaxation",
        "Parse selection",
        "Precision",
        "Recall",
        "F1 -value",
        "(Baseline)",
        "1",
        "No",
        "No",
        "86.2",
        "56.2",
        "68.0",
        "1",
        "No",
        "Yes",
        "87.4",
        "56.6",
        "68.7",
        "1",
        "Yes",
        "Yes",
        "88.1",
        "76.2",
        "81.7",
        "3",
        "Yes",
        "Yes",
        "87.6",
        "85.2",
        "86.4",
        "5",
        "Yes",
        "Yes",
        "87.6",
        "86.0",
        "86.8",
        "(Full approach)",
        "10",
        "Yes",
        "Yes",
        "87.7",
        "87.0",
        "87.3",
        "Size of word lattice (NBests)",
        "Grammar relaxation",
        "Parse selection",
        "WER",
        "1",
        "No",
        "No",
        "20.5",
        "1",
        "Yes",
        "Yes",
        "l9.4",
        "3",
        "Yes",
        "Yes",
        "16.5",
        "5",
        "Yes",
        "Yes",
        "l5.7",
        "10",
        "Yes",
        "Yes",
        "15.7"
      ]
    }
  ]
}
