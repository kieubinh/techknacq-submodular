{
  "info": {
    "authors": [
      "Sandra Candito",
      "Benoît Crabbé",
      "Djamé Seddah"
    ],
    "book": "Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference",
    "id": "acl-W09-1008",
    "title": "On Statistical Parsing of French with Supervised and Semi-Supervised Strategies",
    "url": "https://aclweb.org/anthology/W09-1008",
    "year": 2009
  },
  "references": [
    "acl-A00-1031",
    "acl-A00-2018",
    "acl-J04-4004",
    "acl-J98-4004",
    "acl-L08-1246",
    "acl-N06-1020",
    "acl-P03-1013",
    "acl-P03-1054",
    "acl-P05-1010",
    "acl-P05-1038",
    "acl-P06-1055",
    "acl-W01-0521",
    "acl-W04-3224",
    "acl-W06-1614"
  ],
  "sections": [
    {
      "text": [
        "On statistical parsing of French with supervised and semi-supervised",
        "strategies",
        "Marie Candito*, Benoît Crabbé* and Djamé Seddaho",
        "* Université Paris 7 o Université Paris 4",
        "UFRL et INRIA (Alpage) LALIC et INRIA (Alpage)",
        "30 rue du Château des Rentiers 28 rue Serpente",
        "F-75013 Paris – France F-75006 Paris – France",
        "This paper reports results on grammatical induction for French.",
        "We investigate how to best train a parser on the French Treebank (Abeille et al., 2003), viewing the task as a trade-off between generalizability and interpretability.",
        "We compare, for French, a supervised lexicalized parsing algorithm with a semi-supervised un-lexicalized algorithm (Petrov et al., 2006) along the lines of (Crabbe and Candito, 2008).",
        "We report the best results known to us on French statistical parsing, that we obtained with the semi-supervised learning algorithm.",
        "The reported experiments can give insights for the task of grammatical learning for a morphologically-rich language, with a relatively limited amount of training data, annotated with a rather flat structure."
      ]
    },
    {
      "heading": "1. Natural language parsing",
      "text": [
        "Despite the availability of annotated data, there have been relatively few works on French statistical parsing.",
        "Together with a treebank, the availability of several supervised or semi-supervised grammatical learning algorithms, primarily set up on English data, allows us to figure out how they behave on French.",
        "Before that, it is important to describe the characteristics of the parsing task.",
        "In the case of statistical parsing, two different aspects of syntactic structures are to be considered : their capacity to capture regularities and their interpretability for further processing.",
        "Generalizability Learning for statistical parsing requires structures that capture best the underlying regularities of the language, in order to apply these patterns to unseen data.",
        "Since capturing underlying linguistic rules is also an objective for linguists, it makes sense to use supervised learning from linguistically-defined generalizations.",
        "One generalization is typically the use of phrases, and phrase-structure rules that govern the way words are grouped together.",
        "It has to be stressed that these syntactic rules exist at least in part independently of semantic interpretation.",
        "Interpretability But the main reason to use supervised learning for parsing, is that we want structures that are as interprétable as possible, in order to extract some knowledge from the analysis (such as deriving a semantic analysis from a parse).",
        "Typically, we need a syntactic analysis to reflect how words relate to each other.",
        "This is our main motivation to use supervised learning : the learnt parser will output structures as defined by linguists-annotators, and thus interpretable within the linguistic theory underlying the annotation scheme of the treebank.",
        "It is important to stress that this is more than capturing syntactic regularities : it has to do with the meaning of the words.",
        "It is not certain though that both requirements (generalizability / interpretability) are best met in the same structures.",
        "In the case of supervised learning, this leads to investigate different instantiations of the training trees, to help the learning, while keeping the maximum interpretability of the trees.",
        "As we will see with some of our experiments, it may be necessary to find a trade-off between generalizability and interpretability.",
        "Further, it is not guaranteed that syntactic rules infered from a manually annotated treebank produce the best language model.",
        "This leads to methods that use semi-supervised techniques on a treebank-infered grammar backbone, such as (Matsuzaki et al., 2005; Petrov et al., 2006).",
        "The plan of the paper is as follows : in the next section, we describe the available treebank for French, and how its structures can be interpreted.",
        "In section 3, we describe the typical problems encountered when parsing using a plain probabilistic context-free grammar, and existing algorithmic solutions that try to circumvent these problems.",
        "Next we describe experiments and results when training parsers on the French data.",
        "Finally, we discuss related work and conclude."
      ]
    },
    {
      "heading": "2. Interpreting the French trees",
      "text": [
        "The French Treebank (Abeille et al., 2003) is a publicly available sample from the newspaper Le Monde, syntactically annotated and manually corrected for French.",
        "To encode syntactic information, it uses a combination of labeled constituents, morphological annotations and functional annotation for verbal dependents as illustrated in Figure 1.",
        "This constituent and functional annotation was performed in two successive steps : though the original release (Abeille et al., 2000) consists of 20,648 sentences (hereafter Ftb-v0), the functional annotation was performed later on a subset of 12351 sentences (hereafter FTB).",
        "This subset has also been revised, and is known to be more consistently annotated.",
        "This is the release we use in our experiments.",
        "Its key properties, compared with the Penn Treebank, (hereafter PTB) are the following : Size : The FTB is made of 385 458 tokens and 12351 sentences, that is the third of the Ptb.",
        "The average length of a sentence is 31 tokens in the",
        "Ftb, versus 24 tokens in the Ptb.",
        "Inflection : French morphology is richer than English and leads to increased data sparseness for statistical parsing.",
        "There are 24098 types in the Ftb, entailing an average of 16 tokens occurring for each type (versus 12 for the Ptb).",
        "Flat structure : The annotation scheme is flatter in the Ftb than in the Ptb.",
        "For instance, there are no VPs for finite verbs, and only one sentential level for sentences whether introduced by complementizer or not.",
        "We can measure the corpus flatness using the ratio between tokens and non terminal symbols, excluding preterminals.",
        "We obtain 0.69 NT symbol per token for Ftb and 1.01 for the Ptb.",
        "Compounds : Compounds are explicitly annotated (see the compound peut-être in Figure 1 ) and very frequent : 14,52% of tokens are part of a compound.",
        "They include digital numbers (written with spaces in French 10 000), very frozen compounds pomme de terre (potato) but also named entities or sequences whose meaning is compositional but where insertion is rare or difficult (garde d'enfant (child care)).",
        "Now let us focus on what is expressed in the French annotation scheme, and why syntactic information is split between constituency and functional annotation.",
        "Syntactic categories and constituents capture distributional generalizations.",
        "A syntactic category groups forms that share distributional properties.",
        "Nonterminal symbols that label the constituents are a further generalizations over sequences ofcat-egories or constituents.",
        "For instance about anywhere it is grammatical to have a given NP, it is implicitly assumed that it will also be grammatical - though maybe nonsensical - to have instead any other NPs.",
        "Of course this is known to be false in many cases : for instance NPs with or without determiners have very different distributions in French (that may justify a different label) but they also share a lot.",
        "Moreover, if words are taken into account, and not just sequences of categories, then constituent labels are a very coarse generalization.",
        "Constituents also encode dependencies : for instance the different PP-attachment for the sentences I ate a cake with cream / with a fork reflects that with cream depends on cake, whereas with a fork depends on ate.",
        "More precisely, a syntagmatic tree can be interpreted as a dependency structure using the following conventions :",
        "for each constituent, given the dominating symbol and the internal sequence of symbols, (i) a head symbol can be isolated and (ii) the siblings of that head can be interpreted as containing dependents of that head.",
        "Given these constraints, the syntag-matic structure may exhibit various degree of flatness for internal structures.",
        "Functional annotation Dependencies are encoded in constituents.",
        "While X-bar inspired constituents are supposed to contain all the syntactic information, in the Ftb the shape of the constituents does not necessarily express unambiguously the type of dependency existing between a head and a dependent appearing in the same constituent.",
        "Yet this is crucial for example to extract the underlying predicate-argument structures.",
        "This has led to a \"flat\" annotation scheme, completed with functional annotations that inform on the type of dependency existing between a verb and its dependents.",
        "This was chosen for French to reflect, for instance, the possibility to mix post-verbal modifiers and complements (Figure 2), or to mix post-verbal subject and post-verbal indirect complements : a post verbal NP in the Ftb can correspond to a temporal modifier, (most often) a direct object, or an inverted subject, and in the three cases other subcategorized complements may appear.",
        "une lettre avait été envoyée la semaine dernière aux N",
        "i , salariés",
        "np-Suj vN==~npP-OrBr pp-àobj Le Conseil a notifié sa décision à D N",
        "i i la banque"
      ]
    },
    {
      "heading": "3. Algorithms for probabilistic grammar learning",
      "text": [
        "We propose here to investigate how to apply statistical parsing techniques mainly tested on English, to another language - French -.",
        "In this section we briefly introduce the algorithms investigated.",
        "Though Probabilistic Context Free Grammars (PCFG) is a baseline formalism for probabilistic parsing, it suffers a fundamental problem for the purpose of natural language parsing : the independence assumptions made by the model are too strong.",
        "In other words all decisions are local to a grammar rule.",
        "However as clearly pointed out by (Johnson, 1998) decisions have to take into account non local grammatical properties: for instance a noun phrase realized in subject position is more likely to be realized by a pronoun than a noun phrase realized in object position.",
        "Solving this first methodological issue, has led to solutions dubbed hereafter as unlexicalized statistical parsing (Johnson, 1998; Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrovetal., 2006).",
        "A second class of non local decisions to be taken into account while parsing natural languages are related to handling lexical constraints.",
        "As shown above the subcategorization properties of a predicative word may have an impact on the decisions concerning the tree structures to be associated to a given sentence.",
        "Solving this second methodological issue has led to solutions dubbed hereafter as lexicalized parsing (Charniak, 2000; Collins, 1999).",
        "In a supervised setting, a third and practical problem turns out to be critical: that of data sparseness since available treebanks are generally too small to get reasonable probability estimates.",
        "Three class of solutions are possible to reduce data sparseness: (1) enlarging the data manually or automatically (e.g. (McClosky et al., 2006) uses self-training to perform this step) (2) smoothing, usually this is performed using a markovization procedure (Collins, 1999; Klein and Manning, 2003a) and (3) make the data more coarse (i.e. clustering).",
        "The first algorithm we use is the lexicalized parser of (Collins, 1999).",
        "It is called lexicalized, as it annotates non terminal nodes with an additional latent symbol: the head word of the subtree.",
        "This additional information attached to the categories aims at capturing bilexical dependencies in order to perform informed attachment choices.",
        "The addition of these numerous latent symbols to non terminals naturally entails an over-specialization of the resulting models.",
        "To ensure generalization, it therefore requires to add additional simplifying assumptions formulated as a variant of usual naive Bayesian-style simplifying assumptions: the probability of emitting a non head node is assumed to depend on the head and the mother node only, and not on other sibling nodes.",
        "Since Collins demonstrated his models to significantly improve parsing accuracy over bare PCFG, lexicalization has been thought as a major feature for probabilistic parsing.",
        "However two problems are worth stressing here: (1) the reason why these models improve over bare PCFGs is not guaranteed to be tied to the fact that they capture bilexical dependencies and (2) there is no guarantee that capturing non local lexical constraints yields an optimal language model.",
        "Concerning (1) (Gildea, 2001) showed that full lexicalization has indeed small impact on results : he reimplemented an emulation of Collins' Model 1 and found that removing all references to bilex-ical dependencies in the statistical model , resulted in a very small parsing performance decrease (ParsEval recall on Wsj decreased from 86.1 to 85.6).",
        "Further studies conducted by (Bikel, 2004a) proved indeed that bilexical information were used by the most probable parses.",
        "The idea is that most bilexical parameters are very similar to their back-off distribution and have therefore a minor impact.",
        "In the case of French, this fact can only be more true, with one third of training data compared to English, and with a much richer inflection that worsens lexical data sparseness.",
        "Concerning (2) the addition of head word annotations is tied to the use of manually defined heuristics highly dependent on the annotation scheme of the PTB.",
        "For instance, Collins' models integrate a treatment of coordination that is not adequate for the FTB-like coordination annotation.",
        "Another class of algorithms arising from (Johnson, 1998; Klein and Manning, 2003a) attempts to attach additional latent symbols to treebank categories without focusing exclusively on lexical head words.",
        "For instance the additional annotations will try to capture non local preferences like the fact that an NP in subject position is more likely realized as a pronoun.",
        "The first unlexicalized algorithms set up in this trend (Johnson, 1998; Klein and Manning, 2003a) also use language dependent and manually defined heuristics to add the latent annotations.",
        "The specialization induced by this additional annotation is counterbalanced by simplifying assumptions, dubbed markovization (Klein and Manning, 2003a).",
        "Using hand-defined heuristics remains problematic since we have no guarantee that the latent annotations added in this way will allow to extract an optimal language model.",
        "A further development has been first introduced by (Matsuzaki et al., 2005) who recasts the problem of adding latent annotations as an unsupervised learning problem: given an observed PCFG induced from the treebank, the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set H of latent symbols.",
        "The parameters of the latent grammar are estimated from the observed trees using a specific instantiation of em.",
        "This first procedure however entails a combinatorial explosion in the size of the latent grammar as |H| increases.",
        "(Petrov et al., 2006) (hereafter Bky) overcomes this problem by using the following algorithm: given a Pcfg Go induced from the treebank, iteratively create n grammars G1... Gn (with n = 5 in practice), where each iterative step is as follows :",
        "• Split Create a new grammar Gi from Gi-1by splitting every non terminal of Gi in two new symbols.",
        "Estimate Gi's parameters on the observed treebank using a variant of inside-outside.",
        "This step adds the latent annotation to the grammar.",
        "• Merge For each pair of symbols obtained by a previous split, try to merge them back.",
        "If the likelihood of the treebank does not get significantly lower (fixed threshold) then keep the symbol merged, otherwise keep the split.",
        "• Smooth This step consists in smoothing the probabilities of the grammar rules sharing the same left hand side.",
        "This algorithm yields state-of-the-art results on",
        "English.",
        "Its key interest is that it directly aims at finding an optimal language model without (1) making additional assumptions on the annotation scheme and (2) without relying on hand-defined heuristics.",
        "This may be viewed as a case of semi-supervised learning algorithm since the initial supervised learning step is augmented with a second step of unsupervised learning dedicated to assign the latent symbols."
      ]
    },
    {
      "heading": "4. Experiments and Results",
      "text": [
        "We investigate how some treebank features impact learning.",
        "We describe first the experimental protocol, next we compare results of lexicalized and unlexicalized parsers trained on various \"instantiations\" of the xml source files of the Ftb, and the impact of training set size for both algorithms.",
        "Then we focus on studying how words impact the results of the Bkyalgorithm.",
        "Treebank setting For all experiments, the treebank is divided into 3 sections : training (80%), development (10%) and test (10%), made of respectively 9881, 1235 and 1235 sentences.",
        "We systematically report the results with the compounds merged.",
        "Namely, we preprocess the treebank in order to turn each compound into a single token both for training and test.",
        "Software and adaptation to French For the Collins algorithm, we use Bikel's implementation (Bikel, 2004b) (hereafter Bikel), and we report results using Collins model 1 and model 2, with internal tagging.",
        "Adapting model 1 to French requires to design French specific head propagation rules.",
        "To this end, we adapted those described by (Dybro-Johansen, 2004) for extracting a Stochastic Tree Adjoining Grammar parser on French.",
        "And to adapt model 2, we have further designed French specific argument/adjunct identification rules.",
        "For the Bky approach, we use the Berkeley implementation, with an horizontal markovization h=0, and 5 split/merge cycles.",
        "All the required knowledge is contained in the treebank used for training, except for the treatment of unknown or rare words.",
        "It clusters unknown words using typographical and morphological information.",
        "We adapted these clues to French, following (Arun and Keller, 2005).",
        "Finally we use as a baseline a standard PCFG algorithm, coupled with a trigram tagger (we refer to this setup as TnT/LnCky algorithm).",
        "Metrics For evaluation, we use the standard Par-sEval metric of labeled precision/recall, along with unlabeled dependency evaluation, which is known as a more annotation-neutral metric.",
        "Unla-beled dependencies are computed using the (Lin, 1995) algorithm, and the Dybro-Johansen's head propagation rules cited above.",
        "The unlabeled dependency F-score gives the percentage of input words (excluding punctuation) that receive the correct head.",
        "As usual for probabilistic parsing results, the results are given for sentences of the test set of less than 40 words (which is true for 992 sentences of the test set), and punctuation is ignored for F-score computation with both metrics.",
        "We first derive from the Ftb a minimally-informed treebank, TreebankMin, instantiated from the xml source by using only the major syntactic categories and no other feature.",
        "In each experiment (Table 1) we observe that the Bky algorithm significantly outperforms Collins models, for both metrics.",
        "parser",
        "metric",
        "bky",
        "Bikel Ml",
        "Bikel M2",
        "tnt/",
        "LnCky",
        "ParsEval LP",
        "85.25",
        "78.86",
        "80.68",
        "68.74",
        "ParsEval LR",
        "84.46",
        "78.84",
        "80.58",
        "67.93",
        "ParsEvalFi",
        "84.85",
        "78.85",
        "80.63",
        "68.33",
        "Unlab.",
        "dep.",
        "Prec.",
        "90.23",
        "85.74",
        "87.60",
        "79.50",
        "Unlab.",
        "dep.",
        "Rec.",
        "89.95",
        "85.72",
        "86.90",
        "79.37",
        "Unlab.",
        "dep.",
        "Fi",
        "90.09",
        "85.73",
        "87.25",
        "79.44",
        "4.S Impact of training data size",
        "How do the unlexicalized and lexicalized approaches perform with respect to size?",
        "We compare in figure 3 the parsing performance Bky and CollinsM 1, on increasingly large subsets of the Ftb, in perfect tagging mode and using a more detailed tagset (CC tagset, described in the next experiment).",
        "The same 1235-sentences test set is used for all subsets, and the development set's size varies along with the training set's size.",
        "Bky outperforms the lexicalized model even with small amount of data (around 3000 training sentences).",
        "Further, the parsing improvement that would result from more training data seems higher for Bky than for Bikel.",
        "This potential increase for Bky results if we had more French annotated data is somehow confirmed by the higher results reported for Bky training on the Penn Treebank (Petrov et al., 2006) : Fi =90.2.",
        "We can show though that the 4 points increase when training on English data is not only due to size : we extracted from the Penn Treebank a subset comparable to the Ftb, with respect to number of tokens and average length of sentences.",
        "We obtain Fi=88.61 with Bky training.",
        "It is well-known that certain treebank transformations involving symbol refinements improve",
        "PCFGs (see for instance parent-transformation of (Johnson, 1998), or various symbol refinements in (Klein and Manning., 2003b)).",
        "Lexicalization itself can be seen as symbol refinements (with backoff though).",
        "For Bky, though the key point is to automatize symbol splits, it is interesting to study whether manual splits still help.",
        "We have thus experimented Bky training with various tagsets.",
        "The Ftb contains rich morphological information, that can be used to split preterminal symbols : main coarse category (there are 13), subcategory (subcat feature refining the main cat), and inflectional information (mph feature).",
        "We report in Table 2 results for the four tagsets, where terminals are made of : min: main cat, subcat: main cat + subcat feature, max: cat + subcat + all inflectional information, cc: cat + verbal mood + wh feature.",
        "tagging)",
        "The corpus instantiation with cc tagset is our best trade-off between tagset informativeness and obtained parsing performance.",
        "It is also the best result obtained for French probabilistic parsing.",
        "This demonstrates though that the Bky learning is not optimal since manual a priori symbol refinements significantly impact the results.",
        "We also tried to learn structures with functional annotation attached to the labels : we obtain ParsEval F1=78.73 with tags from the CC tagset + grammatical function.",
        "This degradation, due to data sparseness and/or non local constraints badly captured by the model, currently constrains us to use a language model without functional informations.",
        "As stressed in the introduction, this limits the interpretability of the parses and it is a tradeoff between generalization and interpretability.",
        "French has a rich morphology that allows some degree of word order variation, with respect to",
        "Tagset",
        "Nb of tags",
        "Parseval",
        "Unlab.",
        "dep",
        "Tagging",
        "Fi",
        "Fi",
        "Acc",
        "min",
        "13",
        "84.85",
        "90.09",
        "97.35",
        "subcat",
        "34",
        "85.74",
        "-",
        "96.63",
        "max",
        "250",
        "84.13",
        "-",
        "92.20",
        "cc",
        "28",
        "86.41",
        "90.99",
        "96.83",
        "English.",
        "For probabilistic parsing, this can have contradictory effects : (i) on the one hand, this induces more data sparseness : the occurrences of a French regular verb are potentially split into more than 60 forms, versus 5 for an English verb; (ii) on the other hand, inflection encodes agreements, that can serve as clues for syntactic attachments.",
        "Experiment In order to measure the impact of inflection, we have tested to cluster word forms on a morphological basis, namely to partly cancel inflection.",
        "Using lemmas as word form classes seems too coarse : it would not allow to distinguish for instance between a finite verb and a participle, though they exhibit different distributional properties.",
        "Instead we use as word form classes, the couple lemma + syntactic category.",
        "For example for verbs, given the CC tagset, this amounts to keeping 6 different forms (for the 6 moods).",
        "To test this grouping, we derive a treebank where words are replaced by the concatenation of lemma + category for training and testing the parser.",
        "Since it entails a perfect tagging, it has to be compared to results in perfect tagging mode : more precisely, we simulate perfect tagging by replacing word forms by the concatenation form+tag.",
        "Moreover, it is tempting to study the impact of a more drastic clustering of word forms : that of using the sole syntactic category to group word forms (we replace each word by its tag).",
        "This amounts to test a pure unlexicalized learning.",
        "Discussion Results are shown in Figure 4.",
        "We make three observations : First, comparing the terminal=tag curves with the other two, it appears that the parser does take advantage of lexical information to rank parses, even for this \"unlexicalized\" algorithm.",
        "Yet the relatively small increase clearly shows that lexical information remains underused, probably because of lexical data sparseness.",
        "Further, comparing terminal=lemma+tag and ter-minal=form+tag curves, we observe that grouping words into lemmas helps reducing this sparseness.",
        "And third, the lexicon impact evolution (i.e. the increment between terminal=tag and termi-nal=form+tag curves) is stable, once the training size is superior to approx.",
        "3000 sentences.",
        "This suggests that only very frequent words matter, otherwise words' impact should be more and more important as training material augments."
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "Previous works on French probabilistic parsing are those of (Arun and Keller, 2005), (Schluter and van Genabith, 2007), (Schluter and van Genabith, 2008).",
        "One major difficulty for comparison is that all three works use a different version of the training corpus.",
        "Arun reports results on probabilistic parsing, using an older version of the FTB and using lexicalized models (Collins M1 and M2 models, and the bigram model).",
        "It is difficult to compare our results with Arun's work, since the treebank he has used is obsolete (Ftb-v0).",
        "He obtains for Model 1 : LR=80.35 / LP=79.99, and for the bigram model : LR=81.15 / LP=80.84, with minimal tagset and internal tagging.",
        "The results with Ftb (revised subset of Ftb-v0) with minimal tagset (Table 1) are comparable for CoLLINSM1, and nearly 5 points higher for B KY.",
        "It is also interesting to review (Àrun and Keller, 2005) conclusion, built on a comparison with the German situation : at that time lexicalization was thought (Dubey and Keller, 2003) to have no sizable improvement on German parsing, trained on the Negra treebank, that uses a flat structures.",
        "So (Àrun and Keller, 2005) conclude that since lex-icalization helps much more for parsing French, with a flat annotation, then word-order flexibility is the key-factor that makes lexicalization useful (if word order is fixed, cf. French and English) and useless (if word order is flexible, cf. German).",
        "This conclusion does not hold today.",
        "First, it can be noted that as far as word order flexibility is concerned, French stands in between English and German.",
        "Second, it has been proven that lexicalization helps German probabilistic parsing (Kiibler et al., 2006) .",
        "Finally, these authors show that markovization of the unlexicalized Stanford parser gives almost the same increase in performance than lex-icalization, both for the Negra treebank and the Tiiba-D/Z treebank.",
        "This conclusion is reinforced by the results we have obtained : the unlexicalized, markovized, PCFG-LA algorithm outperforms the Collins' lexicalized model.",
        "(Schluter and van Genabith, 2007) aim at learning LFG structures for French.",
        "To do so, and in order to learn first a Collins parser, N. Schluter created a modified treebank, the Mft, in order (i) to fit her underlying theoretical requirements, (ii) to increase the treebank coherence by error mining and (iii) to improve the performance of the learnt parser.",
        "The Mft contains 4739 sentences taken from the Ftb, with semi-automatic transformations.",
        "These include increased rule stratification, symbol refinements (for information propagation), coordination raising with some manual re-annotation, and the addition of functional tags.",
        "Mft has also undergone a phase of error mining, using the (Dickinson and Meurers, 2005) software, and following manual correction.",
        "She reports a 79.95% F-score on a 400 sentence test set, which compares almost equally with Arun's results on the original 20000 sentence treebank.",
        "So she attributes her results to the increased coherence of her smaller treebank.",
        "Indeed, we ran the Bky training on the Mft, and we get F-score=84.31.",
        "While this is less in absolute than the Bky results obtained with Ftb (cf. results in table 2), it is indeed very high if training data size is taken into account (cf. the Bky learning curve in figure 3).",
        "This good result raises the open question of identifying which modifications in the Mft (error mining and correction, tree transformation, symbol refinements) have the major impact."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "This paper reports results in statistical parsing for French with both unlexicalized (Petrov et al., 2006) and lexicalized parsers.",
        "To our knowledge, both results are state of the art on French for each paradigm.",
        "Both algorithms try to overcome PCFG's simplifying assumptions by some specialization ofthe grammatical labels.",
        "For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head.",
        "We observe that the second approach (Bky) constantly outperforms the lexicalist strategy a la (Collins, 1999).",
        "We observe however that (Petrov et al., 2006)'s semi-supervised learning procedure is not fully optimal since a manual refinement of the treebank labelling turns out to improve the parsing results.",
        "Finally we observe that the semi-supervised Bky algorithm does take advantage of lexical information : removing words degrades results.",
        "The preterminal symbol splits percolates lexical distinctions.",
        "Further, grouping words into lemmas helps for a morphologically rich language such as French.",
        "So, an intermediate clustering standing between syntactic category and lemma is thought to yield better results in the future."
      ]
    },
    {
      "heading": "7. Acknowledgments",
      "text": [
        "We thank N. Schluter and J. van Genabith for kindly letting us run Bky on the Mft, and A. Arun for answering our questions.",
        "We also thank the reviewers for valuable comments and references.",
        "The work of the second author was partly funded by the \"Prix Diderot Innovation 2007\", from University Paris 7."
      ]
    }
  ]
}
