{
  "info": {
    "authors": [
      "Kathrin Spreyer",
      "Jonas Kuhn"
    ],
    "book": "CoNLL",
    "id": "acl-W09-1104",
    "title": "Data-Driven Dependency Parsing of New Languages Using Incomplete and Noisy Training Data",
    "url": "https://aclweb.org/anthology/W09-1104",
    "year": 2009
  },
  "references": [
    "acl-D07-1013",
    "acl-H01-1035",
    "acl-H05-1066",
    "acl-H94-1020",
    "acl-I08-1064",
    "acl-J03-1002",
    "acl-N03-1017",
    "acl-N06-1019",
    "acl-N06-1020",
    "acl-P04-1061",
    "acl-P04-1062",
    "acl-P05-1013",
    "acl-P05-1044",
    "acl-P06-1146",
    "acl-P08-1061",
    "acl-P08-1068",
    "acl-P08-1108",
    "acl-P92-1017",
    "acl-W06-2920",
    "acl-W06-2932",
    "acl-W06-2933"
  ],
  "sections": [
    {
      "text": [
        "Kathrin Spreyer and Jonas Kuhn",
        "We present a simple but very effective approach to identifying high-quality data in noisy data sets for structured problems like parsing, by greedily exploiting partial structures.",
        "We analyze our approach in an annotation projection framework for dependency trees, and show how dependency parsers from two different paradigms (graph-based and transition-based) can be trained on the resulting tree fragments.",
        "We train parsers for Dutch to evaluate our method and to investigate to which degree graph-based and transition-based parsers can benefit from incomplete training data.",
        "We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006)."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Many weakly supervised approaches to NLP rely on heuristics or filtering techniques to deal with noise in unlabeled or automatically labeled training data, e.g., in the exploitation of parallel corpora for cross-lingual projection of morphological, syntactic or semantic information.",
        "While heuristic approaches can implement (linguistic) knowledge that helps to detect noisy data (e.g., Hwa et al.",
        "(2005)), they are typically task-and language-specific and thus introduce a component of indirect supervision.",
        "Non-heuristic filtering techniques, on the other hand, employ reliability measures (often unrelated to the task) to predict high-precision data points (e.g., Yarowsky et al.",
        "(2001)).",
        "In order to reach a sufficient level of precision, filtering typically has to be aggressive, especially for highly structured tasks like parsing.",
        "Such aggressive filtering techniques incur massive data loss and enforce trade-offs between the quality and the amount of usable data.",
        "Ideally, a general filtering strategy for weakly supervised training of structured analysis tools should eliminate noisy subparts in the automatic annotation without discarding its high-precision aspects; thereby data loss would be kept to a minimum.",
        "In this paper, we propose an extremely simple approach to noise reduction which greedily exploits partial correspondences in a parallel corpus, i.e., correspondences potentially covering only substructures of translated sentences.",
        "We implemented this method in an annotation projection framework to create training data for two dependency parsers representing different parsing paradigms: The MST-Parser (McDonald et al., 2005) as an instance of graph-based dependency parsing, and the Malt-Parser (Nivre et al., 2006) to represent transition-based dependency parsing.",
        "In an empirical evaluation, we investigate how they react differently to incomplete and noisy training data.",
        "Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006).",
        "After a summary of related work in Sec. 2, we discuss dependency tree projection (Sec.",
        "3) and partial correspondence (Sec.",
        "4).",
        "In Sec. 5, we give an overview of graph-and transition-based dependency parsing and describe how each can be adapted for training on partial training data in Sec. 6.",
        "Experimental results are presented in Sec. 7, followed by an analysis in Sec. 8.",
        "Sec.",
        "9 concludes.",
        "You are absolutely right U heeft volkomen gelijk",
        "You are absolutely right",
        "U heeft volkomen gelijk",
        "Figure 1: Dependency tree projection from English to Dutch.",
        "(a) Ideal scenario with bidirectional alignments.",
        "(b) Projection fails due to weak alignments.",
        "(c) Constrained fallback projection.",
        "b.",
        "a.",
        "c."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Annotation projection has been applied to many different NLP tasks.",
        "On the word or phrase level, these include morphological analysis, part-of-speech tagging and NP-bracketing (Yarowsky et al., 2001), temporal analysis (Spreyer and Frank, 2008), or semantic role labeling (Padó and Lapata, 2006).",
        "In these tasks, word labels can technically be introduced in isolation, without reference to the rest of the annotation.",
        "This means that an aggressive filter can be used to discard unreliable data points (words in a sentence) without necessarily affecting high-precision data points in the same sentence.",
        "By using only the bidirectional word alignment links, one can implement a very robust such filter, as the bidirectional links are generally reliable, even though they have low recall for overall translational correspondences (Koehn et al., 2003).",
        "The bidirectional alignment filter is common practice (Padó and Lapata, 2006); a similar strategy is to discard entire sentences with low aggregated alignment scores (Yarowsky et al., 2001).",
        "On the sentence level, Hwa et al.",
        "(2005) were the first to project dependency trees from English to Spanish and Chinese.",
        "They identify unreliable target parses (as a whole) on the basis of the number of unaligned or over-aligned words.",
        "In addition, they manipulate the trees to accommodate for non-isomorphic sentences.",
        "Systematic non-parallelisms between source and target language are then addressed by hand-crafted rules in a post-projection step.",
        "These rules account for an enormous increase in the unlabeled f-score of the direct projections, from 33.9 to 65.7 for Spanish and from 26.3 to 52.4 for Chinese.",
        "But they need to be designed anew for every target language, which is time-consuming and requires knowledge of that language.",
        "Research in the field of unsupervised and weakly supervised parsing ranges from various forms of EM training (Pereira and Schabes, 1992; Klein and Manning, 2004; Smith and Eisner, 2004; Smith and Eisner, 2005) over bootstrapping approaches like self-training (McClosky et al., 2006) to feature-based enhancements of discriminative reranking models (Koo et al., 2008) and the application of semi-supervised SVMs (Wang et al., 2008).",
        "The partial correspondence method we present in this paper is compatible with such approaches and can be combined with other weakly supervised machine learning schemes.",
        "Our approach is similar to that of Clark and Curran (2006) who use partial training data (ccg lexical categories) for domain adaptation; however, they assume an existing CCG resource for the language in question to provide this data."
      ]
    },
    {
      "heading": "3. Projection of Dependency Trees",
      "text": [
        "Most state-of-the-art parsers for natural languages are data-driven and depend on the availability ofsuf-ficient amounts of labeled training data.",
        "However, manual creation of treebanks is time-consuming and labour-intensive.",
        "One way to avoid the expensive annotation process is to automatically label the training data using annotation projection (Yarowsky et al., 2001): Given a suitable resource (such as a parser) in language L\\, and a word-aligned parallel corpus with languages L\\ and L2, label the L\\-portion of the parallel text (with the parser) and copy the annotations to the corresponding (i.e., aligned) elements in language L2.",
        "This is illustrated in Fig. 1a.",
        "The arrows between English and Dutch words indicate the word alignment.",
        "Assuming we have a parser to produce the dependency tree for the English sentence, we build the tree for the Dutch sentence by establishing arcs between words wD (e.g., Ik) and hD (heb) if there are aligned pairs (wD ,wE) #sents w/ avg.",
        "sent vocab projected parse length (lemma) (Ik and I) and (hD, hE) (heb and have) such that hEis the head of wE in the English tree.",
        "Annotation projection assumes direct correspondence (Hwa et al., 2005) between languages (or annotations), which – although it is valid in many cases – does not hold in general: non-parallelism between corresponding expressions in Li and L2causes errors in the target annotations.",
        "The word alignment constitutes a further source for errors if it is established automatically – which is typically the case in large parallel corpora.",
        "We have implemented a language-independent framework for dependency projection and use the Europarl corpus (Koehn, 2005) as the parallel text.",
        "Europarl consists of the proceedings of the European Parliament, professionally translated in 11 languages (approx.",
        "30mln words per language).",
        "The data was aligned on the word level with Giza++ (Och and Ney, 2003).",
        "In the experiments reported here, we use the language pair English-Dutch, with English as the source for projection (Li ) and Dutch as L2.",
        "The English portion of the Europarl corpus was lemmatized and POS tagged with the Tree-Tagger (Schmid, 1994) and then parsed with Malt-Parser (which is described in Sec. 6), trained on a dependency-converted version of the WSJ part from the Penn Treebank (Marcus et al., 1994), but with the automatic POS tags.",
        "The Dutch sentences were only POS tagged (with TreeTagger).",
        "We quantitatively assess the impact of various filtering techniques on a random sample of 100,000 English-Dutch sentence pairs from Europarl (avg.",
        "24.9 words/sentence).",
        "The English dependency trees are projected to their Dutch counterparts as explained above for Fig. 1a.",
        "The first filter we examine is the one that considers exclusively bidirectional alignments.",
        "It admits dependency arcs to be projected only if the head hE and the dependent wE are each aligned bidirectionally with some word in the Dutch sentence.",
        "This is indicated in Fig. 1b, where the English verb are is aligned with the Dutch translation heeft only in one direction.",
        "This means that none of the dependencies involving are are projected, and the projected structure is not connected.",
        "We will discuss in subsequent sections how less restricted projection methods can still incorporate such data.",
        "Table 1 shows the quantitative effect of the bidirectional filter in the row labeled 'bidirectional'.",
        "The proportion of usable sentences is reduced to 2.11%.",
        "Consequently, the vocabulary size diminishes by a factor of 10, and the average sentence length drops considerably from almost 25 to less than 7 words, suggesting that most non-trivial examples are lost.",
        "As an instance of a more relaxed projection of complete structures, we also implemented a fallback to unidirectional links which projects further dependencies after a partial structure has been built based on the more reliable bidirectional links.",
        "That is, the dependencies established via unidirectional alignments are constrained by the existing subtrees, and are subject to the wellformedness conditions for dependency trees.",
        "Fig.",
        "1c shows how the fallback mechanism, initialized with the unconnected structure built with the bidirectional filter, recovers a parse tree for the weakly aligned sentence pair in Fig. 1b.",
        "Starting with the leftmost word in the Dutch sentence and its English translation (U and You), there is a unidirectional alignment for the head of You: are is aligned to heeft, so U is established as a dependent of heeft via fallback.",
        "Likewise, heeft can now be identified as the root node.",
        "Note that the (incorrect) alignment between heeft and You will not be pursued because it would lead to heeft being a dependent of itself and thus violating the wellformed-",
        "Table 2: Fragmented parses projected with the alignment filter.",
        "The sentences included in the data set 'bi+frags<3' are in boldface.",
        "ness conditions.",
        "Finally, the subtree rooted in gelijk is incorporated as the second dependent of heeft.",
        "As expected, the proportion of examples that pass this filter rises, to 6.42% (Table 1, 'fallback').",
        "However, we will see in Sec. 7 that parsers trained on this data do not improve over parsers trained on the bidirectionally aligned sentences alone.",
        "This is presumably due to the noise that inevitably enters the training data through fallback."
      ]
    },
    {
      "heading": "4. Partial Correspondence Projection",
      "text": [
        "So far, we have only considered complete trees, i.e., projected structures with exactly one root node.",
        "This is a rather strict requirement, given that even state-of-the-art parsers sometimes fail to produce plausible complete analyses for long sentences, and that non-sentential phrases such as complex noun phrases still contain valuable, non-trivial information.",
        "We therefore propose partial correspondence projection which, in addition to the complete annotations produced by tree-oriented projection, yields partial structures: It admits fragmented analyses in case the tree-oriented projection cannot construct a complete tree.",
        "Of course, the nature of those fragments needs to be restricted so as to exclude data with no (interesting) dependencies.",
        "E.g., a sentence of five words with a parse consisting of five fragments provides virtually no information about dependency structure.",
        "Hence, we impose a limit (fixed at 3 after quick preliminary tests on automatically labeled development data) on the number of fragments that can make up an analysis.",
        "Alternatively, one could require a minimum fragment size.",
        "As an example, consider again Fig. 1b.",
        "This example would be discarded in strict tree projection, but under partial correspondence it is included as a partial analysis consisting of three fragments:",
        "U heeft volkomen gelijk Although the amount of information provided in this analysis is limited, the arc between gelijk and volkomen, which is strongly supported by the alignment, can be established without including potentially noisy data points that are only weakly aligned.",
        "We use partial correspondence in combination with bidirectional projection.",
        "As can be seen in Table 1 ('bi+frags<3'), this combination boosts the amount of usable data to a range similar to that of the fallback technique for trees; but unlike the latter, partial correspondence continues to impose a high-precision filter (bidirectionality) while improving recall through relaxed structural requirements (partial correspondence).",
        "Table 2 shows how fragment size varies with sentence length."
      ]
    },
    {
      "heading": "5. Data-driven Dependency Parsing",
      "text": [
        "Models for data-driven dependency parsing can be roughly divided into two paradigms: Graph-based and transition-based models (McDonald and Nivre, 2007).",
        "In the graph-based approach, global optimization considers all possible arcs to find the tree T s.t.",
        "where D is the set of all well-formed dependency trees for the sentence, AT is the set of arcs in T, and s(i, j, l) is the score of an arc between words Wj and Wj with label l. The specific graph-based parser we use in this paper is the MSTParser of McDonald et al.",
        "(2005).",
        "The MSTParser learns the scoring function s using an online learning algorithm (Crammer and Singer, 2003) which maximizes the margin between T and D \\ {T}, based on a loss function that counts the number of words with incorrect parents relative to the correct tree.",
        "In contrast to the global optimization employed in graph-based models, transition-based models construct a parse tree in a stepwise way: At each point, the locally optimal parser action (transition) t* is determined greedily on the basis of the current configuration c (previous actions plus local features):",
        "#frags",
        "1",
        "2",
        "3",
        "4-15",
        ">15",
        "#words",
        "<4",
        "425",
        "80",
        "12",
        "-",
        "-",
        "4-9",
        "1,331",
        "1,375",
        "1,567",
        "4,793",
        "-",
        "10-19",
        "339",
        "859",
        "1,503",
        "27,910",
        "522",
        "20-30",
        "17",
        "45",
        "143",
        "20,756",
        "10,087",
        ">30",
        "0",
        "5",
        "5",
        "4,813",
        "23,362",
        "where T is the set of possible transitions.",
        "As a representative of the transition-based paradigm, we use the MaltParser (Nivre et al., 2006).",
        "It implements incremental, deterministic parsing algorithms and employs SVMs to learn the transition scores s."
      ]
    },
    {
      "heading": "6. Parsing with Fragmented Trees",
      "text": [
        "To make effective use of the fragmented trees produced by partial correspondence projection, both parsing approaches need to be adapted for training on sentences with unconnected substructures.",
        "Here we briefly discuss how we represent these structures, and then describe how we modified the parsers.",
        "We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures.",
        "Specifically, every fragment root specifies as its head an artificial root token wo (distinguished from a true root dependency by a special relation Frag).",
        "Thus, sentences with a fragmented parse are still represented as a single sentence, including all words; the difference from a fully parsed sentence is that unconnected substructures are attached directly under wo.",
        "For instance, the partial parse in Fig. 1b would be represented as follows (details omitted):",
        "In the training phase, the MSTParser tries to maximize the scoring margin between the correct parse and all other valid dependency trees for the sentence.",
        "However, in the case of fragmented trees, the training example is not strictly speaking correct, in the sense that it does not coincide with the desired parse tree.",
        "In fact, this desired tree is among the other possible trees that MST assumes to be incorrect, or at least suboptimal.",
        "In order to relax this assumption, we have to ensure that the loss of the desired tree is zero.",
        "While it is impossible to single out this one tree (since we do not know which one it is), we can steer the margin in the right direction with a loss function that assigns zero loss to all trees that are consistent with the training example, i.e., trees that differ from the training example at most on those words that are fragment roots (e.g., gelijk in Fig. 1).",
        "To reflect this notion of loss during optimization, we also adjust the definition of the score of a tree:",
        "We refer to this modified model as f(iltering)MST.",
        "In the transition-based paradigm, it is particularly important to preserve the original context (including unattached words) of a partial analysis, because the parser partly bases its decisions on neighboring words in the sentence.",
        "Emphasis of the role of isolated Frag dependents as context rather than proper nodes in the tree can be achieved, as with the MSTParser, by eliminating their effect on the margin learned by the SVMs.",
        "Since MaltParser scores local decisions, this simply amounts to suppressing the creation of SVM training instances for such nodes (U and gelijk in (1)).",
        "That is, where the feature model refers to context information, unattached words provide this information (e.g., the feature vector for volkomen in (1) contains the form and POS of gelijk), but there are no instances indicating how they should be attached themselves.",
        "This technique of excluding fragment roots during training will be referred to as fMalt."
      ]
    },
    {
      "heading": "7. Experiments 7.1 Setup",
      "text": [
        "We train instances of the graph-and the transition-based parser on projected dependencies, and occasionally refer to these as \"projected parsers\".",
        "All results were obtained on the held-out CoNLL-X test set of 386 sentences (avg.",
        "12.9 baseline 2 (next) words/sentence) from the Alpino treebank (van der Beek et al., 2002).",
        "The Alpino treebank consists mostly of newspaper text, which means that we are evaluating the projected parsers, which are trained on Europarl, in an out-of-domain setting, in the absence of manually annotated Europarl test data.",
        "Parsing performance is measured in terms of un-labeled attachment score (UAS), i.e., the proportion of tokens that are assigned the correct head, irrespective of the label.",
        "To establish upper and lower bounds for our task of weakly supervised dependency parsing, we proceed as follows.",
        "We train MaltParsers and MST-Parsers on (i) the CoNLL-X training portion of the Alpino treebank (195,000 words), (ii) 100,000 Eu-roparl sentences parsed with the parser obtained from (i), and (iii) the concatenation of the data sets (i) and (ii).",
        "The first is a supervised upper bound (80.05/82.43% UAS) trained on manually labeled in-domain data, while the second constitutes a weaker bound (75.33/73.09%) subject to the same out-of-domain evaluation as the projected parsers, and the third (77.47%) is a self-trained version of(i).",
        "We note in passing that the supervised model does not benefit from self-training.",
        "Two simple baselines provide approximations to a lower bound: Baseline 1 attaches every word to the preceding word, achieving 23.65%.",
        "Analogously, baseline 2 attaches every word to the following word (27.63%).",
        "These systems are summarized in Table 3.",
        "Table 4: UAS ofparsers trainedonprojecteddependency structures for (a) a sample of 100,000 sentences, subject to filtering, (b) 10 random samples, each with 100,000 words after filtering (average scores given), and (c) the entire Europarl corpus, subject to filtering.",
        "Table 4a summarizes the results of training parsers on the 100,000-sentence sample analyzed above.",
        "Both the graph-based (MST) and the transition-based (Malt) parsers react similarly to the more or less aggressive filtering methods, but to different degrees.",
        "The first two rows of the table show the parsers trained on complete trees ('trees (bidirectional)' and 'trees (fallback)').",
        "In spite of the additional training data gained by the fallback method, the resulting parsers do not achieve higher accuracy; on the contrary, there is a drop in UAS, especially in the transition-based model ( – 6.66%).",
        "The increased level of noise in the fallback data has less (but significant) impact on the graph-based counterpart ( – 2.68%).",
        "Turning to the parsers trained on partial correspondence data ('bi+frags<3'), we observe even greater deterioration in both parsing paradigms ifthe data is used as is.",
        "However, in combination with the fMalt/fMST systems ('bi+frags<3 (fMalt/fMST)'), both parsers significantly outperform the tree-significance testing (p<.01) was performed by means of the t-test on the results of 10 training cycles (Table 4c 'trees (fb.)'",
        "only 2 cycles due to time constraints).",
        "For the experiments in Table 4a and 4c, the cycles differed in terms of the order in which sentences where passed to the parser.",
        "In Table 4b we base significance on 10 true random samples for training.",
        "Malt MST",
        "words",
        "Malt",
        "MST",
        "Alpino",
        "80.05 82.43",
        "a. trees (bidirectional)",
        "13,500",
        "65.94",
        "67.76",
        "EP",
        "75.33 73.09",
        "trees (fallback)",
        "62,500",
        "59.28",
        "65.08",
        "Alpino + EP",
        "77.47 81.63",
        "bi+frags<3",
        "68,000",
        "55.09",
        "57.14",
        "baseline 1 (previous)",
        "23.65",
        "bi+frags<3 (fMalt/fMST)",
        "68,000",
        "69.15",
        "70.02",
        "Recall Precision",
        "Table 5: Performance relative to dependency length.",
        "(a) Projected MaltParsers and (b) projected MSTParsers.",
        "oriented models ('trees (bidirectional)') by 3.21% (Malt) and 2.26% (MST).",
        "It would be natural to presume that the superiority of the partial correspondence filter is merely due to the amount of training data, which is larger by a factor of 5.04.",
        "We address this issue by isolating the effect on the quality of the data, and hence the success at noise reduction: In Table 4b, we control for the amount of data that is effectively used in training, so that each filtered training set consists of 100,000 words.",
        "Considering the Malt models, we find that the trends suggested in Table 4a are confirmed: The pattern of relative performance emerges even though any quantitative (dis-)advantages have been eliminated.",
        "Interestingly, the MSTParser does not appear to gain from the increased variety (cf. Table 1) in the partial data: it does not differ significantly from the 'trees (bi.)'",
        "model.",
        "Finally, Table 4c provides the results of training on the entire Europarl, or what remains ofthe corpus after the respective filters have applied.",
        "The results corroborate those obtained for the smaller samples.",
        "In summary, the results support our initial hypothesis that partial correspondence for sentences containing a highly reliable part is preferable to relaxing the reliability citerion, and – in the case of the transition-based MaltParser – also to aggressively filtering out all but the reliable complete trees.",
        "With UASs around 70%, both systems are only 5% behind the average 75.07% UAS achieved for Dutch in the CoNLL-X Shared Task."
      ]
    },
    {
      "heading": "8. Analysis",
      "text": [
        "We have seen that the graph-and the transition-based parser react similarly to the various filtering methods.",
        "However, there are interesting differences in the magnitude of the performance changes.",
        "If we compare the two tree-oriented filters 'trees (bi.)'",
        "and 'trees (fb.",
        ")', we observe that, although both Malt and MST suffer from the additional noise that is introduced via the unidirectional alignments, the drop in accuracy is much less pronounced in the latter, graph-based model.",
        "Recall that in this paradigm, optimization is performed over the entire tree by scoring edges independenly; this might explain why noisy arcs in the training data have only a negligible impact.",
        "Conversely, the transition-based Malt-Parser, which constructs parse trees in steps of locally optimal decisions, has an advantage when confronted with partial structures: The individual fragments provide exactly the local context, plus lexical information about the (unconnected) wider context.",
        "To give a more detailed picture of the differences between predicted and actual annotations, we show the performance (of the parsers from Table 4b) separately for binned arc length (Table 5) and sentence length (Table 6).",
        "As expected, the performance of both the supervised upper bounds (Alpino-",
        "Malt/MST) and the projected parsers degrades as dependencies get longer, and the difference between the two grows.",
        "Performance across sentence length remains relatively stable.",
        "But note that both tables again reflect the pattern we saw in Table 4.",
        "Importantly, the relative ranking (in terms of f-score, not shown, resp.",
        "UAS) is still in place even in long distance dependencies and long sentences.",
        "This indicates that the effects we have described are not artifacts of a bias towards short dependencies.",
        "In addition, Table 5 sheds some light on the impact of fMalt/fMST in terms of the trade-off between precision and recall.",
        "Without the specific adjustments to handle fragments, partial structures in the training data lead to an immense drop in recall.",
        "By contrast, when the adapted parsers fMalt/fMST are applied, they boosts recall back to a level comparable to or even above that of the tree-oriented projection parsers, while maintaining precision.",
        "Again, this effect can be observed across all arc lengths, except arcs to root, which naturally the 'bi+frags' models are overly eager to predict.",
        "Finally, the learning curves in Fig. 2 illustrate how much labeled data would be required to achieve comparable performance in a supervised setting.",
        "The graph-based upper bound (Alpino-MST) reaches the performance of fMST (trained on the entire Europarl) with approx.",
        "25,000 words of manually labeled treebank data; Alpino-Malt achieves the performance of fMalt with approx.",
        "35,000 words.",
        "The manual annotation of even these moderate amounts of data involves considerable efforts, including the creation ofannotation guidelines",
        " – Alpino-Malt",
        "Figure 2: Learning curves for the supervised upper bounds.",
        "They reach the performance of the projected parsers with -25,000 (MST) resp.",
        "35,000 (Malt) words.",
        "and tools, the training ofannotators etc.",
        "9 Conclusion",
        "In the context of dependency parsing, we have proposed partial correspondence projection as a greedy method for noise reduction, and illustrated how it can be integrated with data-driven parsing.",
        "Our experimental results show that partial tree structures are well suited to train transition-based dependency parsers.",
        "Graph-based models do not benefit as much from additional partial structures, but instead are more robust to noisy training data, even when the training set is very small.",
        "In future work, we will explore how well the techniques presented here for English and Dutch work for languages that are typologically further apart, e.g., English-Greek or English-Finnish.",
        "Moreover, we are going to investigate how our approach, which essentially ignores unknown parts of the annotation, compares to approaches that marginalize over hidden variables.",
        "We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The research reported in this paper has been supported by the German Research Foundation DFG as part of SFB 632 \"Information structure\" (project D4; PI: Kuhn)."
      ]
    }
  ]
}
