{
  "info": {
    "authors": [
      "Andre Martins",
      "Noah A. Smith"
    ],
    "book": "Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing",
    "id": "acl-W09-1801",
    "title": "Summarization with a Joint Model for Sentence Extraction and Compression",
    "url": "https://aclweb.org/anthology/W09-1801",
    "year": 2009
  },
  "references": [
    "acl-C08-1018",
    "acl-E06-1038",
    "acl-H05-1066",
    "acl-J98-3005",
    "acl-P02-1057",
    "acl-W00-0403",
    "acl-W02-0401",
    "acl-W02-0406",
    "acl-W03-0501",
    "acl-W03-1101"
  ],
  "sections": [
    {
      "text": [
        "Andre F. T. Martins*t and Noah A. Smith*",
        "Text summarization is one of the oldest problems in natural language processing.",
        "Popular approaches rely on extracting relevant sentences from the original documents.",
        "As a side effect, sentences that are too long but partly relevant are doomed to either not appear in the final summary, or prevent inclusion of other relevant sentences.",
        "Sentence compression is a recent framework that aims to select the shortest subsequence of words that yields an informative and grammatical sentence.",
        "This work proposes a one-step approach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program.",
        "We report favorable experimental results on newswire data."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Ed-mundson, 1969).",
        "Today, the proliferation of digital information makes research on summarization technologies more important than ever before.",
        "In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000).",
        "Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions.",
        "Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information?",
        "While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been done to include it as a module in document summarization systems.",
        "Most existing approaches (with some exceptions, like the vine-growth model of Daume, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or by first compressing all sentences and extracting later.",
        "However, regardless of which operation is performed first – compression or extraction – two-step \"pipeline\" approaches may fail to find overall-optimal solutions; often the summaries are not better that the ones produced by extractive summarization.",
        "On the other hand, a pilot study carried out by Lin (2003) suggests that summarization systems that perform sentence compression have the potential to beat pure extractive systems if they model cross-sentence effects.",
        "In this work, we address this issue by merging the tasks of sentence extraction and sentence compression into a global optimization problem.",
        "A careful design of the objective function encourages \"sparse solutions,\" i.e., solutions that involve only a small number of sentences whose compressions are to be included in the summary.",
        "Our contributions are:",
        "• We cast joint sentence extraction and compression as an integer linear program (ILP);",
        "• We provide a new formulation of sentence compression using dependency parsing information that only requires a linear number of variables, and combine it with a bigram model;",
        "• We show how the full model can be trained in a max-margin framework.",
        "Since a dataset of summaries comprised of extracted, compressed sentences is unavailable, we present a procedure that trains the compression and extraction models separately and tunes a parameter to interpolate the",
        "two models.",
        "The compression model and the full system are compared with state-of-the-art baselines in standard newswire datasets.",
        "This paper is organized as follows: §2-3 provide an overview of our two building blocks, sentence extraction and sentence compression.",
        "§4 describes our method to perform one-step sentence compression and extraction.",
        "§5 shows experiments in newswire data.",
        "Finally, §6 concludes the paper and suggests future work."
      ]
    },
    {
      "heading": "2. Extractive summarization",
      "text": [
        "Extractive summarization builds a summary by extracting a few informative sentences from the documents.",
        "Let D = {t\\,..., tM } be a set of sentences, contained in a single or in multiple related documents.",
        "The goal is to extract the best sequence of sentences (ti1,... , tiK) that summarizes D whose total length does not exceed a fixed budget of J words.",
        "We describe some well-known approaches that wil serve as our experimental baselines.",
        "Extract the leading sentences (Lead).",
        "For single-document summarization, the simplest method consists of greedily extracting the leading sentences while they fit into the summary.",
        "A sentence is skipped if its inclusion exceeds the budget, and the next is examined.",
        "This performs extremely well in newswire articles, due to the journalistic convention of summarizing the article first.",
        "Rank by relevance (Rel).",
        "This method ranks sentences by a relevance score, and then extracts the top ones that can fit into the summary.",
        "The score is typically a linear function of feature values:",
        "Here, each is a feature extracted from sentence ti, and 0d is the corresponding weight.",
        "In our experiments, relevance features include (i) the reciprocal position in the document, (ii) a binary feature indicating whether the sentence is the first one, and (iii) the 1-gram and 2-gram cosine similarity with the headline and with the full document.",
        "Maximal Marginal Relevance (MMR).",
        "For long documents or large collections, it becomes important to penalize the redundancy among the extracted sentences.",
        "Carbonell and Goldstein (1998) proposed greedily adding sentences to the summary S to maximize, at each step, a score of the form where scorerel(ti) is as in Eq.",
        "1 and scorered(ti; S) accounts for the redundancy between ti and the current summary S. In our experiments, redundancy is the 1-gram cosine similarity between the sentence ti and the current summary S. The trade-off between relevance and redundancy is controlled by A G [0,1], which is tuned on development data.",
        "McDonald (2007) proposed a non-greedy variant of MMR that takes into account the redundancy between each pair of candidate sentences.",
        "This is cast as a global optimization problem:",
        "arg max A • £t.eS scorerel(ti) where scorerel(ti) = 0Tlfrel(ti), scorered(ti, tj) = 0Tdfred(ti, tj), and frel(ti) and fred(ti, tj) are feature vectors with corresponding learned weight vectors 6rel and #red.",
        "He has shown how the relevance-based method and the MMR framework (in the non-greedy form of Eq.",
        "3) can be cast as an ILP.",
        "By introducing indicator variables (/Xi)i=i,...,M and )i,j=i)...,M with the meanings"
      ]
    },
    {
      "heading": "1. if t i is to be extracted",
      "text": []
    },
    {
      "heading": "0. otherwise",
      "text": [
        "one can reformulate Eq.",
        "3 as an ILP with O(M) variables and constraints:",
        "subject to binary constraints G {0,1}, the length constraint £i=1 < J (where Ni is the number of words of the ith sentence), and the following \"agreement constraints\" for i, j = 1,..., M"
      ]
    },
    {
      "heading": "1. if t i and t j are both to be extracted",
      "text": [
        "(that impose the logical relation /ij = /i A /j):",
        "Let us provide a compact representation of the program in Eq.",
        "5 that will be used later.",
        "Define our vector of parameters as 9 = [A9rel, -(1 - A)9red].",
        "Packing all the feature vectors (one for each sentence, and one for each pair of sentences) into a matrix F, \" Frel 0 0 Fred .",
        "and Fred 4",
        "[ fred( ti, tj )h<i<j<M, and packing all the variables /i and /ij into a vector /it, the program in Eq.",
        "5 can be compactly written as",
        "max ÖTF/Lt, subject to binary and linear constraints on / .",
        "This formulation requires O(M) variables and constraints.",
        "If we do not penalize sentence redundancy, the redundancy term may be dropped; in this simpler case, F = Frel, the vector / only contains the variables and the program in Eq.",
        "8 only requires O(M) variables and constraints.",
        "Our method (to be presented in §4) will build on this latter formulation."
      ]
    },
    {
      "heading": "3. Sentence Compression",
      "text": [
        "Despite its simplicity, extractive summarization has a few shortcomings: for example, ifthe original sentences are too long or embed several clauses, there is no way of preventing lengthy sentences from appearing in the final summary.",
        "The sentence compression framework (Knight and Marcu, 2000) aims to select the best subsequence of words that still yields a short, informative and grammatical sentence.",
        "Such a sentence compressor is given a sentence t = ..., wN) as input and outputs a subsequence of length L, c = , ...,wjL), with 1 < j1 < ... < jL < N. We may represent this output as a binary vector s of length N, where Sj = 1 iff word Wj is included in the compression.",
        "Note that there are O(2N) possible subsequences.",
        "Past approaches to sentence compression include a noisy channel formulation (Knight and Marcu, 2000; Daumé and Marcu, 2002), heuristic methods that parse the sentence and then trim constituents according to linguistic criteria (Dorr et al., 2003; Zajic et al., 2006), a pure discriminative model (McDonald, 2006), and an ILP formulation (Clarke and La-pata, 2008).",
        "We next give an overview of the two latter approaches.",
        "McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.",
        "Formally, given a sentence t = (w1,..., wN), the score of a compression c = ,..., wjL) decomposes as:",
        "where f(t, j) are feature vectors that depend on the original sentence t and consecutive positions and jj, and 9 is a learned weight vector.",
        "The factorization in Eq.",
        "9 allows exact decoding with dynamic programming.",
        "Clarke and Lapata (2008) cast the problem as an ILP.",
        "In their formulation, Eq.",
        "9 may be expressed as:",
        "where ai, $, and Yij are additional binary variables with the following meanings:",
        "• ai = 1 iff word wi starts the compression;",
        "• = 1 iff word wi ends the compression;",
        "• Yij = 1 iff words wi and Wj appear consecutively in the compression;",
        "and subject to the following agreement constraints:",
        "This framework also allows the inclusion of constraints to enforce grammaticality.",
        "To compress a sentence, one needs to maximize the score in Eq.",
        "10 subject to the constraints in Eq.",
        "11.",
        "Representing the variables through and packing the feature vectors into a matrix F, we obtain the ILP subject to linear and integer constraints on the variables s and V. This particular formulation requires O(N) variables and constraints.",
        "We propose an alternative model for sentence compression that may be formulated as an ILP, as in Eq.",
        "13, but with only O(N) variables and constraints.",
        "This formulation is based on the output ofa dependency parser.",
        "Directed arcs in a dependency tree link pairs of words, namely a head to its modifier.",
        "A dependency parse tree is characterized by a set of labeled arcs of the form (head, modifier, label); see Fig.1 for an example.",
        "Given a sentence t = ..., wN), we write i = n(j) to denote that the ith word is the head (the \"parent\") of the jth word; if j is the root, we write = 0.",
        "Let s be the binary vector describing a possible compression c for the sentence t. For each word j, we consider four possible cases, accounting for the inclusion or not of j and n(j) in the compression.",
        "We introduce (mutually exclusive) binary variables vj10, vj01, and vj00 to indicate each of these cases, s.e., for as, b G {0, S},",
        "Consider feature vectors fn(t, j), fio(t, j), fo1(t, j), and foo(t, j), that look at the surface sentence and at the status of the word j and its head these features have corresponding weight vectors 911, 91o, 9o1, and 9oo.",
        "The score of c is written as:",
        "Diag(F11, F1o, Fo1, Foo) (a block-diagonal matrix).",
        "We have reached in Eq.",
        "15 an ILP isomorphic to the one in Eq.",
        "13, but only with O(N) variables.",
        "There are some agreement constraints between the variables v and s that reflect the logical relations in Eq.",
        "14; these may be written as linear inequalities (cf. Eq.",
        "6), yielding O(N) constraints.",
        "Given this proposal and §3.1, it is also straightforward to extend this model to include bigram features as in Eq.",
        "10; the combination of dependency relation features and bigram features yields a model that is more powerful than both models in Eq.",
        "15 and Eq.",
        "10.",
        "Such a model is expressible as an ILP with O(N) variables and constraints, making use of the variables s, v, a, ff and 7.",
        "In §5, we compare the performance of this model (called \"Bigram\") and the model in Eq.",
        "15 (called \"NoBigram\")."
      ]
    },
    {
      "heading": "4. Joint Compression and Extraction",
      "text": [
        "We next describe our joint model for sentence compression and extraction.",
        "Let D = {t1;..., tM} be a set of sentences as in §2, each expressed as a sequence of words, ti = t«;i1,..., wiNi).",
        "Following §3, we represent a compression of ti an a binary vector si = (sa,..., SiNi), \\fh^i-e s^ij = 1 iffword Wij = b.",
        "It should be noted that more efficient decoders are possible that do not require solving; anILP.",
        "In particf lar, inference in the NoBigram variant can performed in polynomial time with dynamic programming algorithms that propagate messages along tf e dependency parse tree; for theBigram variant, dynamic programming can still be employed with some additional storage.",
        "Our ILP formulaSion, however, is more Suited to the final goal of performing document iummarization (of which our sentence compression model will be a component); furthermore, it also allows the straightforward inclusion of global linguistic constraints, which, as shown by (Clarke Mid Lapata (22008), can greatly improve the grammaticality of the compressions.",
        "is included in the compression.",
        "Now, define a summary of D as a set of sentences obtained by extracting and compressing sentences from D. More precisely, let ..., /xM be binary variables, one for each sentence ti in D; define /i = 1 iff a compression of sentence ti is used in the summary.",
        "A summary of D is then represented by the binary variables ..., , s1,..., sM).",
        "Notice that these variables are redundant:",
        "i.e., an empty compression means that the sentence is not to be extracted.",
        "In the sequel, it will become clear why this redundancy is convenient.",
        "Most approaches up to now are concerned with either extraction or compression, not both at the same time.",
        "We will combine the extraction scores in Eq.",
        "8 and the compression scores in Eq.",
        "15 to obtain a single, global optimization problem; we rename the extraction features and parameters to Fe and 9e and the compression features and parameters to Fc and 9C:",
        "subject to agreement constraints on the variables Viand si (see Eqs.",
        "11 and 14), and new agreement constraints on the variables /it and s1,..., sM to enforce the relation in Eq.",
        "16:",
        "The constraintthatthe lengthofthe summarycannot exceed J words is encoded as:",
        "All variables are further restricted to be binary.",
        "We also want to avoid picking just a few words from many sentences, which typically leads to ungrammatical summaries.",
        "Hence it is desirable to obtain \"sparse\" solutions with only a few sentences extracted and compressed (and most components of / are zero) To do so, we add the constraint which states, for each sentence ti, that ti should be ignored or have at least pNi words extracted.",
        "We fix p = 0.8, enforcing compression rates below 80%.To learn the model parameters 9 = (9e, 9c), we can use a max-margin discriminative learning algorithm like MIRA (Crammer and Singer, 2003), which is quite effective and scalable.",
        "However, there is not (to our knowledge) a single dataset of extracted and compressed sentences.",
        "Instead, as will be described in Sec. 5.1, there are separate datasets of extracted sentences, and datasets of compressed sentences.",
        "Therefore, instead of globally learning the model parameters, 9 = (9e, 9c), we propose the following strategy to learn them separately:",
        "• Learn 9e using a corpus of extracted sentences,",
        "• Learn 9C using a corpus of compressed sentences,",
        "• Tune n so that 9 = (9'e,ri9'c) has good performance on development data.",
        "(This is necessary since each set of weights is learned up to scaling.)"
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "For our experiments, two datasets were used:",
        "The DUC 2002 dataset.",
        "This is a collection of newswire articles, comprised of 59 document clusters.",
        "Each document within the collections (out of a total of 567 documents) has one or two manually created abstracts with approximately 100 words.",
        "Clarke's dataset for sentence compression.",
        "This is the dataset used by Clarke and Lapata (2008).",
        "It contains manually created compressions of82 newspaper articles (1,433 sentences) from the British National Corpus and the American News Text corpus.",
        "To evaluate the sentence compressor alone, we measured the compression rate and the precision, recall, and F1-measure (both macro and micro-averaged) with respect to the \"gold\" compressed",
        "Table 1: Results for sentence compression in the Clarke's test dataset (441 sentences) for our implementation of the baseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model, NoBigram and Bigram.",
        "The compression ratio associated with the reference compressed sentences in this dataset is 69.06%.",
        "In the rightmost column, the statistically indistinguishable best results are emboldened, based on a paired t-test applied to the sequence of F measures (p < 0.01).",
        "sentences, calculated on unigrams.",
        "To evaluate the full system, we used Rouge-N (Lin and Hovy, 2002), a popular n-gram recall-based automatic evaluation measure.",
        "This score compares the summary produced by a system with one or more valid reference summaries.",
        "All our experiments were conducted on a PC with a Intel dual-core processor with 2.66 GHz and 2 Gb RAM memory.",
        "We used ILOG CPLEX, a commercial integer programming solver.",
        "The interface with CPLEX was coded in Java.",
        "We split Clarke's dataset into two partitions, one used for training (1,188 sentences) and the other for testing (441 sentences).",
        "This dataset includes one manual compression for each sentence, that we use as reference for evaluation purposes.",
        "Compression ratio, i.e., the fraction of words included in the compressed sentences, is 69.32% (micro-averaged over the training partition).",
        "For comparison, two baselines were implemented: a simple compressor based on Hedge Trimmer, the headline generation system of Dorr et al.",
        "(2003) and Zajic et al.",
        "(2006), and the discriminative model described by McDonald (2006), which captures \"soft syntactic evidence\" (we reproduced the same set of features).",
        "Both systems require a phrase-structure parser; we used Collins' parser (Collins, 1999); the latter system also derives features from a dependency parser; we used the MST-Parser (McDonald et al., 2005).",
        "We implemented the two variants of our compressor described in §3.2.",
        "NoBigram.",
        "This variant factors the compression score as a sum over individual scores, each depending on the inclusion or not of each word and its head in the compression (see Eq.",
        "15).",
        "An upper bound of 70% was placed on the compression ratio.",
        "As stated in §3.2, inference amounts to solving an ILP with O(N) variables and constraints, N being the sentence length.",
        "We also used MSTParser to obtain the dependency parse trees.",
        "Bigram.",
        "This variant includes an extra term standing for a bigram score, which factors as a sum over pairs of consecutive words.",
        "As in McDonald (2006), we include features that depend on the \"in-between\" words in the original sentence that are to be omitted in the compression.",
        "As stated in §3.2, inference through this model can be done by solving an ILP with O(N) variables and constraints.",
        "step of the algorithm) already provides significant compression, as illustrated in Table 1.",
        "Compression Ratio",
        "P",
        "Micro-Av.",
        "R",
        "Fi",
        "P",
        "Macro-Av.",
        "R",
        "F1",
        "HedgeTrimmer McDonald (2006)",
        "57.64% 71.40%",
        "0.7099 0.7444",
        "0.5925 0.7697",
        "0.6459 0.7568",
        "0.7195 0.7711",
        "0.6547 0.7852",
        "0.6367",
        "0.7696",
        "NoBigram Bigram",
        "71.20% 71.35%",
        "0.7399 0.7472",
        "0.7626 0.7720",
        "0.7510 0.7594",
        "0.7645 0.7737",
        "0.7730 0.7848",
        "0.7604 0.7710",
        "For both variants, we used MSTParser to obtain the dependency parse trees.",
        "The model parameters are learned in a pure discriminative way through a max-margin approach.",
        "We used the 1-best MIRA algorithm (Crammer and Singer, 2003; McDonald et al., 2005) for training; this is a fast online algorithm that requires solving the inference problem at each step.",
        "Although inference amounts to solving an ILP, which in the worst case scales exponentially with the size of the sentence, training the model is in practice very fast for the NoBigram model (a few minutes in the environment described in §5.1) and fast enough for the Bigram model (a couple ofhours using the same equipment).",
        "This is explained by the fact that sentences don't usually exceed a few tens of words, and because of the structure of the ILPs, whose constraint matrices are very sparse.",
        "Table 1 depicts the micro-and macro-averaged precision, recall and F\\-measure.",
        "We can see that both variants outperform the Hedge Trimmer baseline by a great margin, and are in line with the system of McDonald (2006); however, none of our variants employ a phrase-structure parser.",
        "We also observe that our simpler NoBigram variant, which uses a linear-sized ILP, achieves results similar to these two systems.",
        "For the summarization task, we split the DUC 2002 dataset into a training partition (427 documents) and a testing partition (140 documents).",
        "The training partition was further split into a training and a development set.",
        "We evaluated the performance of Lead, Rel, and MMR as baselines (all are described in §2).",
        "Weights for Rel were learned via the SVM-Rank algorithm; to create a gold-standard ranking, we sorted the sentences by Rouge-2 score (with respect to the human created summaries).",
        "We include a Pipeline baseline as well, which ranks all sentences by relevance, then includes their compressions (using the Bigram variant) while they fit into the summary.",
        "We tested two variants of our joint model, combining the Rel extraction model with (i) the NoBi-",
        "Table 2: Results for sentence extraction in the DUC2002 dataset (140 documents).",
        "Bold indicates the best results with statistical significance, according to a paired t-test (p < 0.01); Rouge-2 scores of all systems except Pipeline are indistinguishable according to the same test, with p > 0.05.",
        "gram compression model (§3.2) and (ii) the Bigram variant.",
        "Each variant was trained with the procedure described in §4.",
        "To keep tractability, the inference ILP problem was relaxed (the binary constraints were relaxed to unit interval constraints) and non-integer solution values were rounded to produce a valid summary, both for training and testing.Whenever this procedure yielded a summary longer than 100 words, we truncated it to fit the word limit.",
        "Table 2 depicts the results of each of the above systems in terms of Rouge-1 and Rouge-2 scores.",
        "We can see that both variants of our system are able to achieve the best results in terms of Rouge-1 and Rouge-2 scores.",
        "The suboptimality of extracting and compressing in separate stages is clear from the table, as Pipeline performs worse than the pure extractive systems.",
        "We also note that the configuration Rel + Bigram is not able to outperform Rel + No-Bigram, despite being computationally more expensive (about 25 minutes to process the whole test set, against the 7 minutes taken by the Rel + NoBigram variant).",
        "Fig.",
        "2 exemplifies the summaries produced by our system.",
        "We see that both variants were able to include new pieces of information in the summary without sacrificing grammaticality.",
        "These results suggest that our system, being capable of performing joint sentence extraction and compression to summarize a document, offers a powerful alternative to pure extractive systems.",
        "Finally, we note that no labeled datasets currently exist on which our full model could have been trained with supervision; therefore, although inference is performed",
        "Rouge-1 Rouge-2",
        "Lead Rel",
        "MMR A = 0.25 Pipeline",
        "0.384 ± 0.080 0.177 ± 0.083 0.389 ± 0.074 0.178 ± 0.080 0.392 ± 0.071 0.178 ± 0.077",
        "0.380 ± 0.073 0.173 ± 0.073",
        "Rel + NoBigr r = 1.5 Rel + Bigr r = 4.0",
        "0.403 ± 0.080 0.180 ± 0.082 0.403 ± 0.076 0.180 ± 0.076",
        "MMR baseline:",
        "Australian novelist Peter Carey was awarded the coveted Booker Prize for fiction Tuesday night for his love story, \"Oscar and Lu-cinda\".",
        "A panel of five judges unanimously announced the award of the $26,250 prize after an 80-minute deliberation during a banquet at London's ancient Guildhall.",
        "Carey, who lives in Sydney with his wife and son, said in a brief speech that like the other five finalists he had been asked to attend with a short speech in his pocket in case he won.",
        "Rel + NoBigram:",
        "Australian novelist Peter Carey was awarded the coveted Booker Prize for fiction Tuesday night for his love story, \"Oscar and Lu-cinda\".",
        "A panel of five judges unanimously announced the award of the $26,250 prize after an 80-minute deliberation during a banquet at London's ancient Guildhall.",
        "The judges made their selection from 102 books published in Britain in the past 12 months and which they read in their homes.",
        "Carey, who lives in Sydney with his wife and son, said in a brief speech that like the other five finalists he had been asked to attend with a short speech in his pocket in case he won.",
        "Rel + Bigram:",
        "Australian novelist Peter Carey was awarded the coveted Booker Prize for fiction Tuesday night for his love story, \"Oscar and Lu-cinda\".",
        "A panel of five judges unanimously announced the award of the $26,250 prize after an 80-minute deliberation during a banquet at London's ancient Guildhall.",
        "He was unsuccessful in the prize competition in 1985 when his novel, \"Illywhacker,\" was among the final six.",
        "Carey called the award a \"great honor\" and he thanked the prize sponsors for \"provoking so much passionate discussion about literature perhaps there will be more tomorrow\".",
        "Carey was the only non-Briton in the final six",
        "Figure 2: Summaries produced by the strongest baseline (MMR) and the two variants of our system.",
        "Deleted words are marked as such.",
        "jointly, our training procedure had to learn separately the extraction and the compression models, and to tune a scalar parameter to trade off the two models.",
        "We conjecture that a better model could have been learned if a labeled dataset with extracted compressed sentences existed."
      ]
    },
    {
      "heading": "6. Conclusion and Future Work",
      "text": [
        "We have presented a summarization system that performs sentence extraction and compression in a single step, by casting the problem as an ILP.",
        "The summary optimizes an objective function that includes both extraction and compression scores.",
        "Our model encourages \"sparse\" summaries that involve only a few sentences.",
        "Experiments in newswire data suggest that our system is a valid alternative to existing extraction-based systems.",
        "However, it is worth noting that further evaluation (e.g., human judgments) needs to be carried out to assert the quality of our summaries, e.g., their grammaticality, something that the Rouge scores cannot fully capture.",
        "Future work will address the possibility of including linguistic features and constraints to further improve the grammaticality of the produced summaries.",
        "Another straightforward extension is the inclusion of a redundancy term and a query relevance term in the objective function.",
        "For redundancy, a similar idea of that of McDonald (2007) can be applied, yielding a ILP with O(M + N) variables and constraints (M being the number of sentences and N the total number of words).",
        "However, such model will take into account the redundancy among the original sentences and not their compressions; to model the redundancy accross compressions, a possibility is to consider a linear redundancy score (similar to cosine similarity, but without the normalization), which would result in an ILP with O(N + £i P) variables and constraints, where Pi < M is the number of sentences in which word wi occurs; this is no worse than O(MN).",
        "We also intend to model discourse, which, as shown by Daumé and Marcu (2002), plays an important role in document summarization.",
        "Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors thank the anonymous reviewers for helpful comments, Yiming Yang for interesting discussions, and Dipanjan Das and Sourish Chaudhuri for providing their code.",
        "This research was supported by a grant from FCT through the CMU-Portugal Program and the Information and Communications Technologies Institute (ICTI) at CMU, and also by Priberam Informatica."
      ]
    }
  ]
}
