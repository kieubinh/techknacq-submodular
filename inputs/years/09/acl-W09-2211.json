{
  "info": {
    "authors": [
      "Sajib Dasgupta",
      "Vincent Ng"
    ],
    "book": "Proceedings of the NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing",
    "id": "acl-W09-2211",
    "title": "Discriminative Models for Semi-Supervised Natural Language Learning",
    "url": "https://aclweb.org/anthology/W09-2211",
    "year": 2009
  },
  "references": [
    "acl-C08-1071",
    "acl-N03-1023",
    "acl-P04-1007",
    "acl-P05-1073",
    "acl-P06-1096",
    "acl-P07-1004",
    "acl-P07-1056",
    "acl-P08-1103",
    "acl-W01-0501",
    "acl-W04-3201",
    "acl-W06-1647",
    "acl-W06-1668"
  ],
  "sections": [
    {
      "text": [
        "Sajib Dasgupta and Vincent Ng",
        "1 Discriminative vs. Generative Models",
        "An interesting question surrounding semi-supervised learning for NLP is: should we use discriminative models or generative models?",
        "Despite the fact that generative models have been frequently employed in a semi-supervised setting since the early days of the statistical revolution in NLP, we advocate the use of discriminative models.",
        "The ability of discriminative models to handle complex, high-dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts.",
        "Perhaps more importantly, discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter-to-phoneme conversion (Jiampojamarn et al., 2008), semantic role labeling (Toutanova et al., 2005), syntactic parsing (Taskar et al., 2004), language modeling (Roark et al., 2004), and machine translation (Liang et al., 2006).",
        "While generative models allow the seamless integration of prior knowledge, discriminative models seem to outperform generative models in a \"no prior\", agnostic learning setting.",
        "See Ng and Jordan (2002) and Toutanova (2006) for insightful comparisons of generative and discriminative models."
      ]
    },
    {
      "heading": "2. Discriminative EM?",
      "text": [
        "A number of semi-supervised learning systems can bootstrap from small amounts of labeled data using discriminative learners, including self-training, cotraining (Blum and Mitchell, 1998), and transduc-tive SVM (Joachims, 1999).",
        "However, none of them seems to outperform the others across different domains, and each has its pros and cons.",
        "Self-training can be used in combination with any discriminative learning model, but it does not take into account the confidence associated with the label of each data point, for instance, by placing more weight on the (perfectly labeled) seeds than on the (presumably noisily labeled) bootstrapped data during the learning process.",
        "Co-training is a natural choice if the data possesses two independent, redundant feature splits.",
        "However, this conditional independence assumption is a fairly strict assumption and can rarely be satisfied in practice; worse still, it is typically not easy to determine the extent to which a dataset satisfies this assumption.",
        "Transductive SVM tends to learn better max-margin hyperplanes with the use of unlabeled data, but its optimization procedure is non-trivial and its performance tends to deteriorate if a sufficiently large amount of unlabeled data is used.",
        "Recently, Brefeld and Scheffer (2004) have proposed a new semi-supervised learning technique, EM-SVM, which is interesting in that it incorporates a discriminative model in an EM setting.",
        "Unlike self-training, EM-SVM takes into account the confidence of the new labels, ensuring that the instances that are labeled with less confidence by the SVM have less impact on the training process than the confidently-labeled instances.",
        "So far, EM-SVM has been tested on text classification problems, outperforming transductive SVM.",
        "It would be interesting to see whether EM-SVM can beat existing semi-supervised learners for other NLP tasks."
      ]
    },
    {
      "heading": "3. Effectiveness of Bootstrapping",
      "text": [
        "How effective are the aforementioned semi-supervised learning systems in bootstrapping from small amounts oflabeled data?",
        "While there are quite a few success stories reporting considerable performance gains over an inductive baseline (e.g., parsing (McClosky et al., 2008), coreference resolution (Ng and Cardie, 2003), and machine translation (Ueff-ing et al., 2007)), there are negative results too (see Pierce and Cardie (2001), He and Gildea (2006), Duh and Kirchhoff (2006)).",
        "Bootstrapping performance can be sensitive to the setting of the parameters of these semi-supervised learners (e.g., when to stop, how many instances to be added to the labeled data in each iteration).",
        "To date, however, researchers have relied on various heuristics for parameter selection, but what we need is a principled method for addressing this problem.",
        "Recently, Mc-Closky et al.",
        "(2008) have characterized the conditions under which self-training would be effective for semi-supervised syntactic parsing.",
        "We believe that the NLP community needs to perform more research ofthis kind, which focuses on identifying the algorithm(s) that achieve good performance under a given setting (e.g., few initial seeds, large amounts of unlabeled data, complex feature space, skewed class distributions)."
      ]
    },
    {
      "heading": "4. Domain Adaptation",
      "text": [
        "Domain adaptation has recently become a popular research topic in the NLP community.",
        "Labeled data for one domain might be used to train a initial classifier for another (possibly related) domain, and then bootstrapping can be employed to learn new knowledge from the new domain (Blitzer et al., 2007).",
        "It would be interesting to see if we can come up with a similar semi-supervised learning model for projecting resources from a resource-rich language to a resource-scarce language."
      ]
    }
  ]
}
