{
  "info": {
    "authors": [
      "Pi-Chuan Chang",
      "Huihsin Tseng",
      "Daniel Jurafsky",
      "Christopher D. Manning"
    ],
    "book": "Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation (SSST-3) at NAACL HLT 2009",
    "id": "acl-W09-2307",
    "title": "Discriminative Reordering with Chinese Grammatical Relations Features",
    "url": "https://aclweb.org/anthology/W09-2307",
    "year": 2009
  },
  "references": [
    "acl-D07-1077",
    "acl-N03-1017",
    "acl-N04-1021",
    "acl-N04-4026",
    "acl-N06-1014",
    "acl-P03-1021",
    "acl-P07-2045",
    "acl-W05-0908",
    "acl-W06-3108",
    "acl-W06-3601",
    "acl-W08-0336",
    "acl-W08-1301"
  ],
  "sections": [
    {
      "text": [
        "Pi-Chuan Changa, Huihsin Tsengb, Dan Jurafskya, and Christopher D. Manning^",
        "aComputer Science Department, Stanford University, Stanford, CA 94305 bYahoo!",
        "Inc., Santa Clara, CA 95054",
        "The prevalence in Chinese of grammatical structures that translate into English in different word orders is an important cause of translation difficulty.",
        "While previous work has used phrase-structure parses to deal with such ordering problems, we introduce a richer set of Chinese grammatical relations that describes more semantically abstract relations between words.",
        "Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies.",
        "We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77).",
        "Our Chinese grammatical relations are also likely to be useful for other NLP tasks."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Structural differences between Chinese and English are a major factor in the difficulty of machine translation from Chinese to English.",
        "The wide variety of such Chinese-English differences include the ordering of head nouns and relative clauses, and the ordering of prepositional phrases and the heads they modify.",
        "Previous studies have shown that using syntactic structures from the source side can help MT performance on these constructions.",
        "Most of the previous syntactic MT work has used phrase structure parses in various ways, either by doing syntax-directed translation to directly translate parse trees into strings in the target language (Huang et al., 2006), or by using source-side parses to preprocess the source sentences (Wang et al., 2007).",
        "One intuition for using syntax is to capture different Chinese structures that might have the same",
        "(complete)",
        "H (three)",
        "ffi (city) JSif",
        "(collectively) detj",
        "S<S (these)",
        "(invest)",
        "HE (fixed)",
        "Figure 1: Sentences (a) and (b) have the same meaning, but different phrase structure parses.",
        "Both sentences, however, have the same typed dependencies shown at the bottom of the figure.",
        "meaning and hence the same translation in English.",
        "But it turns out that phrase structure (and linear order) are not sufficient to capture this meaning relation.",
        "Two sentences with the same meaning can have different phrase structures and linear orders.",
        "In the example in Figure 1, sentences (a) and (b) have the same meaning, but different linear orders and different phrase structure parses.",
        "The translation of sentence (a) is: \"In the past three years these municipalities have collectively put together investment in fixed assets in the amount of 12 billion yuan.\"",
        "In sentence (b), \"in the past three years\" has moved its position.",
        "The temporal adverbial ^ M\" (in the past three years) has different linear positions in the sentences.",
        "The phrase structures are different too: in (a) the LCP is immediately under IP while in (b) it is under VP.",
        "(a)",
        "(b)",
        "(root",
        "(root",
        "(ip",
        "(ip",
        "(LCP",
        "(np",
        "(QP (CD S)",
        "(dp (dt SU) )",
        "(CLP (M :££)))",
        "(np (nn ifl) ) )",
        "(LC 3Q )",
        "(vp",
        "(pu • )",
        "(LCP",
        "(np",
        "(QP (CD H)",
        "(dp (dt SU) )",
        "(CLP (M if)))",
        "(np (nn ifl) ) )",
        "(LC Jfl))",
        "(vp",
        "(advp (ad %\\\\) )",
        "(advp (ad Sit) )",
        "(vp (vv",
        "(vp (vv §5$)",
        "(np",
        "(np",
        "(np",
        "(np",
        "(adjp (jj )",
        "(adjp (jj )",
        "(np (nn gr) ) )",
        "(np (nn gr) ) )",
        "(np (nn jgg) ) )",
        "(np (nn jgg) ) )",
        "(qp (cd – ^ – +1Z)",
        "(qp (cd – ^ – +1Z)",
        "(clp (m 7G)))))",
        "(clp (m 7G)))))",
        "(pu • )))",
        "(pu • )))",
        "We propose to use typed dependency parses instead of phrase structure parses.",
        "Typed dependency parses give information about grammatical relations between words, instead of constituency information.",
        "They capture syntactic relations, such as nsubj (nominal subject) and dobj (direct object) , but also encode semantic information such as in the loc (localizer) relation.",
        "For the example in Figure 1, if we look at the sentence structure from the typed dependency parse (bottom of Figure 1), ^ M\" is connected to the main verb yt (finish) by a loc (lo-calizer) relation, and the structure is the same for sentences (a) and (b).",
        "This suggests that this kind of semantic and syntactic representation could have more benefit than phrase structure parses.",
        "Our Chinese typed dependencies are automatically extracted from phrase structure parses.",
        "In English, this kind of typed dependencies has been introduced by de Marneffe and Manning (2008) and de Marneffe et al.",
        "(2006).",
        "Using typed dependencies, it is easier to read out relations between words, and thus the typed dependencies have been used in meaning extraction tasks.",
        "We design features over the Chinese typed dependencies and use them in a phrase-based MT system when deciding whether one chunk of Chinese words (MT system statistical phrase) should appear before or after another.",
        "To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier.",
        "We then apply the phrase orientation classifier as a feature in a phrase-based MT system to help reordering."
      ]
    },
    {
      "heading": "2. Discriminative Reordering Model",
      "text": [
        "Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003).",
        "The disadvantage of these models is their insensitivity to the content of the words or phrases.",
        "More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has introduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases.",
        "Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable.",
        "Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when plugged into a phrase-based MT system.",
        "Their framework allows us to easily add in extra features.",
        "Therefore we use it as a testbed to see if we can effectively use features from Chinese typed dependency structures to help reordering in MT.",
        "We build up the target language (English) translation from left to right.",
        "The phrase orientation classifier predicts the start position of the next phrase in the source sentence.",
        "In our work, we use the simplest class definition where we group the start positions into two classes: one class for a position to the left of the previous phrase (reversed) and one for a position to the right (ordered).",
        "Let cjt j be the class denoting the movement from source position j to source position j of the next phrase.",
        "The definition is:",
        "The phrase orientation classifier model is in the loglinear form:",
        "i is the target position of the current phrase, and f1land eI denote the source and target sentences respectively.",
        "c' represents possible categories of cj, j'.",
        "We can train this log-linear model on lots of labeled examples extracted from all of the aligned MT training data.",
        "Figure 2 is an example of an aligned sentence pair and the labeled examples that can be extracted from it.",
        "Also, different from conventional MERT training, we can have a large number of binary features for the discriminative phrase orientation classifier.",
        "The experimental setting will be described in Section 4. .",
        "Figure 2: An illustration of an alignment grid between a Chinese sentence and its English translation along with the labeled examples for the phrase orientation classifier.",
        "Note that the alignment grid in this example is automatically generated.",
        "The basic feature functions are similar to what Zens and Ney (2006) used in their MT experiments.",
        "The basic binary features are source words within a window of size 3 (d G – 1,0,1) around the current source position j, and target words within a window of size 3 around the current target position i.",
        "In the classifier experiments in Zens and Ney (2006) they also use word classes to introduce generalization capabilities.",
        "In the MT setting it's harder to incorporate the part-of-speech information on the target language.",
        "Zens and Ney (2006) also exclude word class information in the MT experiments.",
        "In our work we will simply use the word features as basic features for the classification experiments as well.",
        "As a concrete example, we look at the labeled example (i = 4, j = 3, j' = 11) in Figure 2.",
        "We include the word features in a window of size 3 around j and i Src2 – Src20:^ Src2rM Tgt – 1:already Tgt0:become Tgt1:a",
        "Assuming we have parsed the Chinese sentence that we want to translate and have extracted the grammatical relations in the sentence, we design features using the grammatical relations.",
        "We use the path between the two words annotated by the grammatical relations.",
        "Using this feature helps the model learn about what the relation is between the two chunks of Chinese words.",
        "The feature is defined as follows: for two words at positions p and q in the Chinese",
        "i",
        "(0)",
        "<s>",
        "(1)",
        "(2)",
        "B",
        "(3)",
        "mi",
        "(4)",
        "(5)",
        "(6)",
        "(7)",
        "(8)",
        "(9)",
        "(10)",
        "(11)",
        "(12)",
        "(13)",
        "mm.",
        "(14)",
        "(15)",
        "</s>",
        "(0) <s>",
        "■",
        "(1) Beihai",
        "(2) has",
        "■",
        "(3) already",
        "■",
        "(4) become",
        "(5) a",
        "(6) bright",
        "(7) star",
        "(8) arising",
        "(9) from",
        "■",
        "(10) China",
        "■",
        "(11) 's",
        "(12) policy",
        "(13) of",
        "(14) opening",
        "m",
        "(15) up",
        "m",
        "(16) to",
        "■",
        "(17) the",
        "(18) outside",
        "■",
        "(19) world",
        "(20) .",
        "(21) </s>",
        "i",
        "J",
        "j\"",
        "class",
        "0",
        "0",
        "1",
        "ordered",
        "1",
        "1",
        "2",
        "ordered",
        "3",
        "2",
        "3",
        "ordered",
        "4",
        "3",
        "11",
        "ordered",
        "5",
        "11",
        "12",
        "ordered",
        "6",
        "12",
        "13",
        "ordered",
        "7",
        "13",
        "9",
        "reversed",
        "8",
        "9",
        "10",
        "ordered",
        "9",
        "10",
        "8",
        "reversed",
        "10",
        "8",
        "7",
        "reversed",
        "15",
        "7",
        "5",
        "reversed",
        "16",
        "5",
        "6",
        "ordered",
        "18",
        "6",
        "14",
        "ordered",
        "20",
        "14",
        "15",
        "ordered",
        "Table 1: The percentage of typed dependencies in files 1-325 in Chinese (CTB6) and English (English-Chinese Translation Treebank) sentence (p < q), we find the shortest path in the typed dependency parse from p to q, concatenate all the relations on the path and use that as a feature.",
        "A concrete example is the sentences in Figure 3, where the alignment grid and labeled examples are shown in Figure 2.",
        "The glosses of the Chinese words in the sentence are in Figure 3, and the English translation is \"Beihai has already become a bright star arising from China's policy of opening up to the outside world.\"",
        "which is also listed in Figure 2.",
        "For the labeled example (i = 4, j = 3, j = 11), we look at the typed dependency parse to find the path feature between ^ and – .",
        "The relevant dependencies are: dobj(SL^k, EJJJJt), cf (EJJJJi, M) and nummod(M, – ).",
        "Therefore the path feature is PATH:dobjR-clfR-nummodR.",
        "We also use the directionality: we add an R to the dependency name if it's going against the direction of the arrow."
      ]
    },
    {
      "heading": "3. Chinese Grammatical Relations",
      "text": [
        "Our Chinese grammatical relations are designed to be very similar to the Stanford English typed dependencies (de Marneffe and Manning, 2008; de Marneffe et al., 2006).",
        "There are 45 named grammatical relations, and a default 46th relation dep (dependent).",
        "If a dependency matches no patterns, it will have the most generic relation dep.",
        "The descriptions of the 45 grammatical relations are listed in Table 2 ordered by their frequencies in files 1-325 of CTB6 (LDC2007T36).",
        "The total number of dependencies is 85748, and other than the ones that fall into the 45 grammatical relations, there are also 7470 dependencies (8.71% of all dependencies) that do not match any patterns, and therefore keep the generic name dep.",
        "Although we designed the typed dependencies to show structures that exist both in Chinese and English, there are many other syntactic structures that only exist in Chinese.",
        "The typed dependencies we designed also cover those Chinese specific structures.",
        "For example, the usage of \"Eitl\" (DE) is one thing that could lead to different English translations.",
        "In the Chinese typed dependencies, there are relations such as cpm (DE as complementizer) or assm (DE as associative marker) that are used to mark these different structures.",
        "The Chinese-specific \"IB\" (BA) construction also has a relation ba dedicated to it.",
        "The typed dependencies annotate these Chinese specific relations, but do not directly provide a mapping onto how they are translated into English.",
        "It becomes more obvious how those structures affect the ordering when Chinese sentences are translated into English when we apply the typed dependencies as features in the phrase orientation classifier.",
        "This will be further discussed in Section 4.4.",
        "To compare the distribution of Chinese typed dependencies with English, we extracted the English typed dependencies from the translation of files 1325 in the English Chinese Translation Treebank 1.0 (LDC2007T02), which correspond to files 1-325 in CTB6.",
        "The English typed dependencies are extracted using the Stanford Parser.",
        "There are 116, 799 total English dependencies, and 85, 748 Chinese ones.",
        "On the corpus we use, there are 45 distinct dependency types (not including dep) in Chinese, and 50 in English.",
        "The coverage of named relations is 91.29% in Chinese and 90.",
        "48% in English; the remainder are the unnamed relation dep.",
        "We looked at the 18 shared relations",
        "Shared relations",
        "Chinese",
        "English",
        "nn",
        "15.48%",
        "6.81%",
        "punct",
        "12.71%",
        "9.64%",
        "nsubj",
        "6.87%",
        "4.46%",
        "rcmod",
        "2.74%",
        "0.44%",
        "dobj",
        "6.09%",
        "3.89%",
        "advmod",
        "4.93%",
        "2.73%",
        "conj",
        "6.34%",
        "4.50%",
        "num/nummod",
        "3.36%",
        "1.65%",
        "attr",
        "0.62%",
        "0.01%",
        "tmod",
        "0.79%",
        "0.25%",
        "ccomp",
        "1.30%",
        "0.84%",
        "xsubj",
        "0.22%",
        "0.34%",
        "cop",
        "0.07%",
        "0.85%",
        "cc",
        "2.06%",
        "3.73%",
        "amod",
        "3.14%",
        "7.83%",
        "prep",
        "3.66%",
        "10.73%",
        "det",
        "1.30%",
        "8.57%",
        "pobj",
        "2.82%",
        "10.49%",
        "China to outside",
        "lccomp open during",
        "measure bright word star nuiranod ^^i^ – ",
        "between Chinese and English in Table 1.",
        "Chinese has more nn, punct, nsubj, rcmod, dobj, advmod, conj, nummod, attr, tmod, and ccomp while English uses more pobj, det, prep, amod, cc, cop, and xsubj, due mainly to grammatical differences between Chinese and English.",
        "For example, some determiners in English (e.g., \"the\" in (1b)) are not mandatory in Chinese:",
        "(1a) jPrtij P/import and export ^^/total value (1b) The total value of imports and exports",
        "In another difference, English uses adjectives (amod) to modify a noun (\"financial\" in (2b)) where Chinese can use noun compounds (\"^ ^/finance\" in (2a)).",
        "(2a) ffiH/Tibet Mfi/finance #fffiJ/system g^/reform (2b) the reform in Tibet's financial system",
        "We also noticed some larger differences between the English and Chinese typed dependency distributions.",
        "We looked at specific examples and provide the following explanations.",
        "prep and pobj English has much more uses of prep and pobj.",
        "We examined the data and found three major reasons:",
        "1.",
        "Chinese uses both prepositions and postpositions while English only has prepositions.",
        "\"After\" is used as a postposition in Chinese example (3a), but a preposition in English (3b):",
        "2.",
        "Chinese uses noun phrases in some cases where English uses prepositions.",
        "For example, \"Z M\" (period, or during) is used as a noun phrase in (4a), but it's a preposition in English.",
        "3.",
        "Chinese can use noun phrase modification in situations where English uses prepositions.",
        "In example (5a), Chinese does not use any prepositions between \"apple company\" and \"new product\", but English requires use of either \"of\" or \"from\".",
        "(5a) ^Jp^isl/apple company Iff* cm/new product (5b) the new product of (or from) Apple The Chinese DE constructions are also often translated into prepositions in English.",
        "cc and punct The Chinese sentences contain more punctuation (punct) while the English translation has more conjunctions (cc), because English uses conjunctions to link clauses (\"and\" in (6b)) while Chinese tends to use only punctuation (\",\" in (6a)).",
        "(6a) SS/these fttTf/city :f±#/social SM/economic ^M/development fflS/rapid , t475\"/local SM/economic 35 ^/strength ^M/clearly iffS/enhance",
        "(6b) In these municipalities the social and economic development has been rapid, and the local economic strength has clearly been enhanced",
        "rcmod and ccomp There are more rcmod and ccomp in the Chinese sentences and less in the English translation, because of the following reasons:",
        "1.",
        "Some English adjectives act as verbs in Chinese.",
        "For example, if (new) is an adjectival predicate in Chinese and the relation between ft (new) and fff S (system) is rcmod.",
        "But \"new\" is an adjective in English and the English relation between \"new\" and \"system\" is amod.",
        "This difference contributes to more rc-mod in Chinese.",
        "(7a) ff/new W/(DE) ^Iff/verify and write off (7b) a new sales verification system",
        "2.",
        "Chinese has two special verbs (VC): kk (SHI) and ^ (WEI) which English doesn't use.",
        "For example, there is an additional relation, ccomp, between the verb kk/(SHI) and I^HBi/reduce in (8a).",
        "The relation is not necessary in English, since k /SHI is not translated.",
        "(8b) Second, China reduced tax substantially in 1996.",
        "conj There are more conj in Chinese than in English for three major reasons.",
        "First, sometimes one complete Chinese sentence is translated into several English sentences.",
        "Our conj is defined for two grammatical roles occurring in the same sentence, and therefore, when a sentence breaks into multiple ones, the original relation does not apply.",
        "Second, we define the two grammatical roles linked by the conjrelation to be in the same word class.",
        "However, words which are in the same word class in Chinese may not be in the same word class in English.",
        "For example, adjective predicates act as verbs in Chinese, but as adjectives in English.",
        "Third, certain constructions with two verbs are described differently between the two languages: verb pairs are described as coordinations in a serial verb construction in Chinese, but as the second verb being the complement of the first verb in English.",
        "abbreviation",
        "short description",
        "Chinese example",
        "typed dependency",
        "counts",
        "percentage",
        "nn",
        "noun compound modifier",
        "nn(+t\\ M*)",
        "13278",
        "15.48%",
        "punct",
        "punctuation",
        "mx m mm,",
        "punct(mm,, )",
        "10896",
        "12.71%",
        "nsubj",
        "nominal subject",
        "nsubj(^ff, f§«)",
        "5893",
        "6.87%",
        "conj",
        "conjunct (links two conjuncts)",
        "m mu»",
        "conj(mU», iä#0",
        "5438",
        "6.34%",
        "dobj",
        "direct object",
        "mm mm t tt – w xw",
        "dobj(MM, xw)",
        "5221",
        "6.09%",
        "advmod",
        "adverbial modifier",
        "mn % mi xw",
        "advmod(mI, %)",
        "4231",
        "4.93%",
        "prep",
        "prepositional modifier",
        "£ %m + %m",
        "prep(^#, £)",
        "3138",
        "3.66%",
        "nummod",
        "number modifier",
        "tt – w xw",
        "nummod(w, t+ – )",
        "2885",
        "3.36%",
        "amod",
        "adjectival modifier",
        "mw& ig",
        "amod(ig, fiTffiffi)",
        "2691",
        "3.14%",
        "pobj",
        "prepositional object",
        "mm mx ms",
        "pobj(mm, ms)",
        "2417",
        "2.82%",
        "rcmod",
        "relative clause modifier",
        "f t äs a m wh",
        "rcmod(WH, ÄS)",
        "2348",
        "2.74%",
        "cpm",
        "complementizer",
        "ff & mm m mm w®",
        "cpm(ff&, m)",
        "2013",
        "2.35%",
        "assm",
        "associative marker",
        "bt m ms",
        "assm(bt, m)",
        "1969",
        "2.30%",
        "assmod",
        "associative modifier",
        "bt m ms",
        "assmod(MS, bt)",
        "1941",
        "2.26%",
        "cc",
        "coordinating conjunction",
        "m su»",
        "cc(SU», m)",
        "1763",
        "2.06%",
        "clf",
        "classifier modifier",
        "tt- w xw",
        "clf(xw, w)",
        "1558",
        "1.82%",
        "ccomp",
        "clausal complement",
        "«fî &s % mm mm im",
        "«compas, mm)",
        "1113",
        "1.30%",
        "det",
        "determiner",
        "s® mm w®",
        "det(w®, s®)",
        "1113",
        "1.30%",
        "lobj",
        "localizer object",
        "lobj(M, ffi^)",
        "1010",
        "1.18%",
        "range",
        "dative object that is a quantifier phrase",
        "rangeait, 7ê)",
        "891",
        "1.04%",
        "asp",
        "aspect marker",
        "&m t fpm",
        "asp(&Jf, T)",
        "857",
        "1.00%",
        "tmod",
        "temporal modifier",
        "Km f t ÄS a",
        "tmod(ÄS, Km)",
        "679",
        "0.79%",
        "plmod",
        "localizer modifier of a preposition",
        "£ s n m± i",
        "plmod(£, I)",
        "630",
        "0.73%",
        "attr",
        "attributive",
        "attr(Ä, Û7ê)",
        "534",
        "0.62%",
        "mmod",
        "modal verb modifier",
        "mm m ms um",
        "mmod(ms, m)",
        "497",
        "0.58%",
        "loc",
        "localizer",
        "loc(à, KI)",
        "428",
        "0.50%",
        "top",
        "topic",
        "üm ë +^ w®",
        "top(ë, iÉIS)",
        "380",
        "0.44%",
        "pccomp",
        "clausal complement of a preposition",
        "m mx mn mm",
        "pccomp(«,",
        "374",
        "0.44%",
        "etc",
        "etc modifier",
        "»ft ^ xm m mm",
        "etc(xm, m)",
        "295",
        "0.34%",
        "lccomp",
        "clausal complement of a localizer",
        "m ft ff ft + ff m m ms",
        "lccomp(^, ffft)",
        "207",
        "0.24%",
        "ordmod",
        "ordinal number modifier",
        "ordmod(t\\ ®-fc)",
        "199",
        "0.23%",
        "xsubj",
        "controlling subject",
        "«fî <&s % mm mm im",
        "xsubj(mm, «fî)",
        "192",
        "0.22%",
        "neg",
        "negative modifier",
        "Km f t ÄS a",
        "neg(ÄS, f )",
        "186",
        "0.22%",
        "rcomp",
        "resultative complement",
        "¥;% Ä?ö",
        "rcomptïjfSÏ,",
        "176",
        "0.21%",
        "comod",
        "coordinated verb compound modifier",
        "MM %fî",
        "comod(MM, %fî)",
        "150",
        "0.17%",
        "vmod",
        "verb modifier",
        "n £ ftM bt ffm m \\m",
        "vmodCTj W, ïî¥)",
        "133",
        "0.16%",
        "prtmod",
        "particles such as Bt, K, +, M",
        "£ rtft bt mm m mm",
        "prtmod(mm,",
        "124",
        "0.14%",
        "ba",
        "\"ba\" construction",
        "m ft ^ «",
        "95",
        "0.11%",
        "dvpm",
        "manner DE(Jtil) modifier",
        "m* ±fe Ki m*",
        "dvpm(W*, ffi)",
        "73",
        "0.09%",
        "dvpmod",
        "a \"XP+DEV(±fe)\" phrase that modifies VP",
        "m* ä Ki m*",
        "dvpmod(KI, W*)",
        "69",
        "0.08%",
        "prnmod",
        "parenthetical modifier",
        "AE KM ( 1990- 1995 )",
        "prnmod(KM, 1995)",
        "67",
        "0.08%",
        "cop",
        "copular",
        "m ë s&S£ m mm",
        "cop(gi&g£, ë)",
        "59",
        "0.07%",
        "pass",
        "passive marker",
        "ft *às ä a ft* rt",
        "pass(^s, ft)",
        "53",
        "0.06%",
        "nsubjpass",
        "nominal passive subject",
        "s ft $m mtt it m m**",
        "nsubjpass($î\\fK S)",
        "14",
        "0.02%"
      ]
    },
    {
      "heading": "4. Experimental Results",
      "text": [
        "We use various Chinese-English parallel corporafor both training the phrase orientation classifier, and for extracting statistical phrases for the phrase-based MT system.",
        "The parallel data contains 1,560,071 sentence pairs from various parallel corpora.",
        "There are 12,259,997 words on the English side.",
        "Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008).",
        "After segmentation, there are 11,061,792 words on the Chinese side.",
        "The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic.",
        "For the phrase orientation classifier experiments, we extracted labeled examples using the parallel data and the alignment as in Figure 2.",
        "We extracted 9,194,193 total valid examples: 86.09% of them are ordered and the other 13.91% are reversed.",
        "To evaluate the classifier performance, we split these examples into training, dev and test set (8 : 1 : 1).",
        "The phrase orientation classifier used in MT experiments is trained with all of the available labeled examples.",
        "Our MT experiments use a reimplementation of Moses (Koehn et al., 2003) called Phrasal, which provides an easier API for adding features.",
        "We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules.",
        "Documents of Gigaword released during the removed.",
        "For features in MT experiments, we incorporate Moses' standard eight features as well as the lexicalized reordering features.",
        "To have a more comparable setting with (Zens and Ney, 2006), we also have a baseline experiment with only the standard eight features.",
        "Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003).",
        "The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences.",
        "We evaluate the",
        "LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85, LDC2002L27 and LDC2005T34.",
        "tences), and MT05 (1082 sentences).",
        "4.2 Phrase Orientation Classifier",
        "Table 3: Feature engineering of the phrase orientation classifier.",
        "Accuracy is defined as (#correctly labeled examples) divided by (#all examples).",
        "The macro-F is an average of the accuracies of the two classes.",
        "The basic source word features described in Section 2 are referred to as Src, and the target word features as Tgt.",
        "The feature set that Zens and Ney (2006) used in their MT experiments is Src+Tgt.",
        "In addition to that, we also experimented with source word features Src2 which are similar to Src, but take a window of 3 around / instead of j.",
        "In Table 3 we can see that adding the Src2 features increased the total number of features by almost 50%, but also improved the performance.",
        "The PATH features add fewer total number of features than the lexical features, but still provide a 10% error reduction and 1.63 on the macro-F1 on the dev set.",
        "We use the best feature sets from the feature engineering in Table 3 and test it on the test set.",
        "We get 94.28% accuracy and 87.17 macro-FL The overall improvement of accuracy over the baseline is 8.19 absolute points.",
        "In the MT setting, we use the log probability from the phrase orientation classifier as an extra feature.",
        "The weight of this discriminative reordering feature is also tuned by MERT, along with other Moses features.",
        "In order to understand how much the PATH features add value to the MT experiments, we trained two phrase orientation classifiers with different features: one with the Src+Src2+Tgt feature set, and the other one with Src+Src2+Tgt+PATH.",
        "The results are listed in Table 4.",
        "We compared to two different baselines: one is Moses8Features which has a distance-based reordering model, the other is Baseline which also includes lexicalized reordering features.",
        "From the table we can see that using the discriminative reordering model with PATH features gives significant improvement over both base-",
        "Feature Sets",
        "#features",
        "Train.",
        "Acc.",
        "Acc.",
        "(%)",
        "Train.",
        "Macro-F",
        "Dev Acc.",
        "(%)",
        "Dev",
        "Macro-F",
        "Majority class",
        "-",
        "86.09",
        "-",
        "86.09",
        "-",
        "Src",
        "1483696",
        "89.02",
        "71.33",
        "88.14",
        "69.03",
        "Src+Tgt",
        "2976108",
        "92.47",
        "82.52",
        "91.29",
        "79.80",
        "Src+Src2+Tgt",
        "4440492",
        "95.03",
        "88.76",
        "93.64",
        "85.58",
        "Src+Src2+Tgt+PATH",
        "4691887",
        "96.01",
        "91.15",
        "94.27",
        "87.22",
        "Table 4: MT experiments of different settings on various NIST MT evaluation datasets.",
        "All differences marked in bold are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005).",
        "products] of ail level",
        "If Tfj| +¥ \\ I* S FS. '",
        "whole cityj this year [ industry total output valuey",
        "gross industrial output value] of the [whole city] this year",
        "a relative clause, such as PATH:advmod-rcmod and PATH:rcmod.",
        "They also indicate the phrases are more likely to be chosen in reversed order.",
        "Another frequent pattern that has not been emphasized in the previous literature is PATH:det-nn, meaning that a [DT NPiNP2] in Chinese is translated into English as [NP2 DT NP1].",
        "Examples with this feature are in Figure 4.",
        "We can see that the important features decided by the phrase orientation model are also important from a linguistic perspective."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "lines.",
        "If we use the discriminative reordering model without PATH features and only with word features, we still get improvement over the Moses8Features baseline, but the MT performance is not significantly different from Baseline which uses lexical-ized reordering features.",
        "From Table 4 we see that using the Src+Src2+Tgt+PATH features significantly outperforms both baselines.",
        "Also, if we compare between Src+Src2+Tgt and Src+Src2+Tgt+PATH, the differences are also statistically significant, which shows the effectiveness of the path features.",
        "4.4 Analysis: Highly-weighted Features in the Phrase Orientation Model",
        "There are a lot of features in the log-linear phrase orientation model.",
        "We looked at some highly-weighted PATH features to understand what kind of grammatical constructions were informative for phrase orientation.",
        "We found that many path features corresponded to our intuitions.",
        "For example, the feature PATH:prep-dobjR has a high weight for being reversed.",
        "This feature informs the model that in Chinese a PP usually appears before VP, but in English they should be reversed.",
        "Other features with high weights include features related to the DE construction that is more likely to translate to",
        "We introduced a set of Chinese typed dependencies that gives information about grammatical relations between words, and which may be useful in other NLP applications as well as MT.",
        "We used the typed dependencies to build path features and used them to improve a phrase orientation classifier.",
        "The path features gave a 10% error reduction on the accuracy of the classifier and 1.63 points on the macro-F1 score.",
        "We applied the log probability as an additional feature in a phrase-based MT system, which improved the BLEU score of the three test sets significantly (0.59 on MT02, 1.00 on MT03 and 0.77 on MT05).",
        "This shows that typed dependencies on the source side are informative for the reordering component in a phrase-based system.",
        "Whether typed dependencies can lead to improvements in other syntax-based MT systems remains a question for future research."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors would like to thank Marie-Catherine de Marneffe for her help on the typed dependencies, and Daniel Cer for building the decoder.",
        "This work is funded by a Stanford Graduate Fellowship to the first author and gift funding from Google for the project \"Translating Chinese Correctly\".",
        "Setting",
        "#MERT features",
        "MT06(tune)",
        "MT02",
        "MT03",
        "MT05",
        "Moses8Features",
        "8",
        "31.49",
        "31.63",
        "31.26",
        "30.26",
        "Moses8Features+DiscrimRereorderNoPATH",
        "9",
        "31.76(+0.27)",
        "31.86(+0.23)",
        "32.09(+0.83)",
        "31.14(+0.88)",
        "Moses8Features+DiscrimRereorderWithPATH",
        "9",
        "32.34(+0.85)",
        "32.59(+0.96)",
        "32.70(+1.44)",
        "31.84(+1.58)",
        "Baseline (Moses with lexicalized reordering)",
        "16",
        "32.55",
        "32.56",
        "32.65",
        "31.89",
        "Baseline+DiscrimRereorderNoPATH",
        "17",
        "32.73(+0.18)",
        "32.58(+0.02)",
        "32.99(+0.34)",
        "31.80(-0.09)",
        "Baseline+DiscrimRereorderWithPATH",
        "17",
        "32.97(+0.42)",
        "33.15(+0.59)",
        "33.65(+1.00)",
        "32.66(+0.77)",
        "every",
        "product j"
      ]
    }
  ]
}
