{
  "info": {
    "authors": [
      "Jana Z. Sukkarieh",
      "Svetlana Stoyanchev"
    ],
    "book": "Proceedings of the 2009 Workshop on Applied Textual Inference (TextInfer)",
    "id": "acl-W09-2509",
    "title": "Automating Model Building in c-rater",
    "url": "https://aclweb.org/anthology/W09-2509",
    "year": 2009
  },
  "references": [
    "acl-W03-0210"
  ],
  "sections": [
    {
      "text": [
        "Automating Model Building in crater",
        "Jana Z. Sukkarieh Svetlana Stoyanchev",
        "Educational Testing Service Stony Brook University",
        "Rosedale Road, Princeton, NJ 08541 Stony Brook, NY, 11794",
        "crater is Educational Testing Service's technology for the content scoring of short student responses.",
        "A major step in the scoring process is Model Building where variants of model answers are generated that correspond to the rubric for each item or test question.",
        "Until recently, Model Building was knowledge-engineered (KE) and hence labor and time intensive.",
        "In this paper, we describe our approach to automating Model Building in crater.",
        "We show that crater achieves comparable accuracy on automatically built and KE models."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "crater (Leacock and Chodorow, 2003) is Educational Testing Service's (ETS) technology for the automatic content scoring of short freetext student answers, ranging in length from a few words to approximately 100 words.",
        "While other content scoring systems [e.g., Intelligent.",
        "Essay Assessor (Foltz, Laham and Landauer, 2003), SEAR (Christie, 1999), IntelliMetric (Vantage Learning Tech, 2000)] take a holistic approach, crater takes an analytical approach to scoring content.",
        "The item rubrics specify content in terms of main points or concepts required to appear in a student's correct answer.",
        "An example of a test question or item follows:",
        "We view crater's task as a textual entailment (TE) problem.",
        "We use TE here to mean either a paraphrase or an inference (up to the context of the item or test question).",
        "crater's task is reduced to a TE problem in the following way:",
        "Given a concept, C, (e.g., \"body increases its temperature\") and a student answer, A, (e.g., either \"the body raises temperature,\" \"the body responded.",
        "His temperature was 37° and now it is 38°,\" or \"Max has a fever\") and the context of the item, the goal is to check whether C is an inference or paraphrase of A (in other words, A implies C and A is true).",
        "There are four main steps in crater.",
        "The first one is Model Building (MB), where a set of model answers are generated (either manually or automatically).",
        "Second, crater automatically processes model answers and students' answers using a set of natural language processing (NLP) tools and extracts the linguistic features.",
        "Third, the matching algorithm Goldmap uses the linguistic features culminated from both MB and NLP to automatically determine whether a student's response entails the expected concepts.",
        "Finally, crater applies the scoring rules to produce a score and feedback that justifies the score to the student.",
        "Item 1 (Full credit: 2 points)",
        "Stimulus: A Reading passage",
        "Prompt:",
        "In the space below, write the question that Alice was most likely trying to answer when she performed Step B.",
        "Concepts or main/key points:",
        "Ci: How does rain formation occur in winter?",
        "C2: How is rain formed?",
        "C3: How do temperature and altitude contribute to the formation of rain?",
        "Scoring rules: 2 points for C1 1 for C2 (only if C1 is not present) 1 for C3 (only if C1 and C2 are not present) Otherwise 0",
        "Until recently, MB was knowledge-engineered (KE).",
        "The KE approach for one item required, on average, 12 hours of time and labor.",
        "This paper describes our approach to automatic MB.",
        "We show that crater achieves comparable accuracy on automatically-and manually-built models.",
        "Section 2 outlines others' work in this domain and emphasizes the contribution of this paper.",
        "Section 3 outlines crater.",
        "In Section 4, we describe how MB works.",
        "Section 5 explains how we automate the process.",
        "Prior to the conclusion, we report the evaluation of this work."
      ]
    },
    {
      "heading": "2. Automatic Content Scoring: Others' Work",
      "text": [
        "A few systems that deal with both short answers and analytic-based content exist.",
        "The task, in general, is reduced to comparing a student's answer to a model answer.",
        "Recent work by Mohler and Mihalcea (2009) at the University of North Texas uses unsupervised methods in text-to-text semantic similarity comparing unseen students' answers to one correct answer.",
        "Previous work, including crater, used supervised techniques to compare unseen students' answers to the space of potentially \"all possible correct answers\" specified in the rubric of the item at hand.",
        "The techniques varied from information extraction with knowledge-engineered patterns representing the model answers [Automark at Intelligent Assessment",
        "Technologies (Mitchell, 2002), the Oxford-",
        "UCLES system (Sukkarieh, et.",
        "al., 2003) at the University of Oxford] to data mining techniques using very shallow linguistic features [e.g., Sukkarieh and Pulman (2005) and Car-melTC at Carnegie Mellon University (Rose, et al.",
        "2003)].",
        "Data mining techniques proved not to be very transparent when digging up justifications for scores.",
        "crater's model building process is similar to generating patterns but the patterns in crater are written in English instead of a formal language.",
        "The aim of the process is to produce a non-trivial space of possible correct answers guided by a subset of the students' answers.",
        "The motivation is that the best place to look for variations and refinements for the rubric is the students' answers.",
        "This is what test developers do before piloting a large-scale exam.",
        "From an NLP point of view, the idea is that generating this space will make scoring an unseen answer easier than just having one correct answer.",
        "However, similar to what other systems reported, generating manually-engineered patterns is very costly.",
        "In Sukkarieh et al.",
        "(2004) there was an attempt to generate patterns automatically but the results reported were not comparable to those using manually-generated patterns.",
        "This paper presents improvements on previous supervised approaches by automating the process of model-answer building using well-known NLP methods and resources while yielding comparable results to knowledge-engineered methods."
      ]
    },
    {
      "heading": "3. c-rater, in Brief",
      "text": [
        "In crater, manual MB has its own graphical interface, Alchemist.",
        "MB uses the NLP tools and Goldmap (which reside in the crater Engine).",
        "On the other hand, Goldmap depends on the model generated.",
        "The crater Engine performs NLP on input text and concept recognition or TE between the input text and each concept (see Figure 1).",
        "First, a student answer is processed for spelling corrections in an attempt to decrease the noise for subsequent NLP tools.",
        "In the next stage, parts-of-speech tagging and parsing are performed (the OpenNLP parser is used http://opennlp.sourceforge.net).",
        "In the third stage, a parse tree is passed through a feature extractor.",
        "Manually-generated rules extract features from the parse tree.",
        "The result is a flat structure representing phrases, predicates, and relationships between predicates and entities.",
        "Each phrase is annotated with a label indicating whether it is independent or dependent.",
        "Each entity is annotated with a syntactic and semantic role.",
        "In the pronoun resolution stage, pronouns are resolved to either an entity in the student's answer or the question.",
        "Finally, a morphology analyzer reduces words to their lemmas.",
        "The culmination of the above tools results in a set of linguistic features used by the matching algorithm, Goldmap.",
        "In addition to the item-independent linguistic features collected by the NLP tools, Goldmap uses item-dependent features specified in MB to decide whether a student's answer, A, and a model answer match, i.e. that concept C represented in the model answer, is entailed by A.",
        "each required token if needed (e.g., {descend, go~down, ...} are similar to words like \"fall\").",
        "Figure 1. crater Engine"
      ]
    },
    {
      "heading": "4. KE Model Building",
      "text": [
        "A dataset of student answers for an item is split into development (DEV), cross-validation (XVAL), and blind (BLIND) datasets.",
        "DEV is used to build the model, XVAL is used to validate it and BLIND is used to evaluate it.",
        "All datasets are double-scored holistically by human raters and the scoring process takes an average 3 hours per item for a dataset of roughly 200 answers.",
        "For each concept Ci in item X, a model builder uses DEV to create a set of Model Sentences (MSij) that s/he believes entails concept Ci in the context of the item.",
        "S/he is required to write MSij in complete sentences.",
        "For each model sentence MSiju the model builder selects the Required Lexicon (RLijk), a set of the most essential lexical entities required to appear in a student's answer.",
        "Then, for each RLijk, the model builder selects a set of Similar Lexicon (SLijkt), guided by the list of words automatically extracted from a dependency-based thesaurus (cs.ualberta.ca/~lindek/downloads.htm).",
        "The process is exemplified in Figure 2.",
        "Presented with the concept, \"What causes rain to form in winter time?,\" a model builder writes model sentences like \"Why does rain fall in the winter?,\" highlights or selects lexical items that s/he believes are the required tokens (e.g., \"why,\" \"rain,\" \"fall,\" \"in,\" \"winter\") and writes a list of similar lexical entities for Figure 2.",
        "KE Model Building",
        "The model for each item X is comprised of the scoring rules, the collections of model sentences MSiJ, associated lexical entities RLijk, and corresponding similar lexicon SLiJkt.",
        "Each model answer is written in terms of MSiJwhere:",
        "MSij entails Ci for i=l,..., N, and N is the number of concepts specified for item X.",
        "For each concept Ci, Goldmap checks whether answer A entails Ci, by checking whether A entails one of the model sentences MSj given the additional features RLijk and corresponding SLiJkt.",
        "In practice, model building works as follows.",
        "The model builder, guided by the DEV dataset and holistic scores, starts with writing a few model sentences and selects corresponding required (RLiJk) and similar (SLiJkt) lexicon.",
        "S/he then uses the crater engine to automatically evaluate the model using the DEV dataset, i.e., using the model produced up to that point.",
        "Goldmap is used to detect if any answers in the DEV dataset contain any of the model sentences and scores are assigned for each answer.",
        "If the scoring agreement between crater and each of the two human raters (in terms of a kappa statistic) is much lower than that between the two human raters, then the model is judged unsuitable and the process continues iteratively until kappa statistics on the DEV dataset are satisfactory, i.e., crater's agreement with human raters is as high as the kappa between human raters.",
        "Once kappa statistics on DEV are satisfactory, the model builder uses crater to evaluate the model on the XVAL dataset automatically.",
        "Again, until the scoring agreement between crater and human raters on XVAL dataset is satisfactory, the model builder iteratively changes the model.",
        "Unlike the DEV dataset, the XVAL dataset is never seen by a model builder.",
        "The logic here is that over-fitting DEV is a concern, making it hard or impossible to generalize beyond this set.",
        "Hence, the results on XVAL can help prevent over-fitting and ideally would predict results over unseen data.",
        "Note that a model builder can introduce what we call a negative concept C-for a concept Ci and adjust the scoring rules accordingly.",
        "When this happens, a model builder writes model sentences MSi~J entailing Ci~ , and selects required words RLi~Jk and corresponding similar words SLi~Jkt in the same way for any other (positive) concept.",
        "On average, MB takes 12 hours of manual work per item (plus 2 hours, on average, for an optional model review by someone other than the model builder).",
        "This process is time consuming and error-prone despite utilizing a user-friendly interface like Alchemist.",
        "In addition, the satisfaction criterion while building a model is subjective to the model builder."
      ]
    },
    {
      "heading": "5. Automated Model Building",
      "text": [
        "The process of writing model sentences described above involves: 1) finding the parts of students' answers containing the concept for each expected concept, 2) abstracting over \"similar\" parts, and 3) representing the abstraction in one (or more) model sentence(s).",
        "The process, as mentioned earlier, is similar to writing rules for information extraction, but here one writes them in English sentences and not in a formal language.",
        "In practice, there is no mechanism in Alchemist to cluster \"similar\" parts and MB, in this aspect, is not performed in any systematic manner.",
        "Hence, we introduce what we call concept-based scoring - used instead of the holistic human scoring.",
        "In concept-based scoring, human raters annotate students' responses for each concept C, and highlight the part of the answer that entails C. In Sukkarieh and Blackmore (2009), we describe concept-based scoring in detail and how this helps in the KE-MB approach.",
        "In this paper, we extend the approach by showing how concept-based scores used in the automated approach reduce the time needed for MB substantially while yielding comparable results.",
        "Concept-based scoring is done manually.",
        "On average, it takes around 3.5 hours per item for a dataset of roughly 200 answers.",
        "The MB process is reduced to:"
      ]
    },
    {
      "heading": "1.. Concept-based scoring",
      "text": []
    },
    {
      "heading": "2.. Automatically selecting required lexicon",
      "text": []
    },
    {
      "heading": "3.. Automatically selecting similar lexicon",
      "text": [
        "While holistic scoring takes on average 3 hours for a dataset of 200 answers, concept-based scoring takes 3.5 hours for the same set.",
        "However, automated MB takes 0 hours of human intervention-a substantial reduction over the 12 hours required for manual MB.",
        "We have developed a concept-based scoring interface (CBS) that can be customized for each item [due to lack of space we do not include an illustration].",
        "The CBS interface displays a student's answer to an item and all of the concepts corresponding to that item.",
        "The terms {Absent, Present, Negated} are what we call analytic or concept-based scores.",
        "Using CBS, the human scorer clicks Present when a concept is present and Negated when a concept is negated or refuted (the default is Absent).",
        "This is done for each concept.",
        "The human scorer also highlights the part of a student's answer that entails the concept in the context of the item.",
        "We call a quote corresponding to concept C 'Positive Evidence' or 'Negative Evidence' for Present and Negated, respectively.",
        "For example, assume a student answer for Item 1 is \"Her research tells us a lot about rain and hail; in particular, the impact that temperature variations have on altitude contribute to the formation of rain.\"",
        "For Concept C3, the human rater highlights the Positive Evidence, \"the impact that temperature variations have on altitude contribute to the formation of rain. \"",
        "Parts of answers corresponding to one piece of Evidence (positive or negative) do not need to be in the same sentence and could be scattered over a few lines.",
        "Similar to the KE approach, we split the double-concept-based scored dataset into DEV and XVAL sets.",
        "However, the splitting is done according to the presence (or absence) of a concept.",
        "We use stratified sampling (Tucker, 1998) trying to uniformly split data such that each concept is represented in the DEV as well as the XVAL datasets.",
        "As mentioned earlier, the KE approach can include negative concepts; currently we do not use Negative Evidence automatically.",
        "In the remainder of this paper, Evidence is taken to mean the collection of Positive Evidence.",
        "Motivation",
        "During manual MB with Alchemist, a model builder is guided by the complete set of students' answers in the DEV dataset, including holistic scores.",
        "Concept-based scoring allows a model builder, if we were to continue the manual MB, to be guided by concept-based scores and students' answers highlighted with the Evidence that corresponds to each concept when writing model sentences as shown, where MSiJ entails Ci and Eir entails Ci.",
        "Further, students may misspell, write ungrammatically, or use incomplete sentences.",
        "Hence, Evidence may contain spelling and grammatical errors.",
        "Evidence may also be in the form of incomplete sentences.",
        "Although human model builders generating sentences with Alchemist are asked to write complete MSi}], there is no reason why MSi]] needs to be in the form of complete sentences.",
        "The NLP tools in the crater engine can cope with a reasonable amount of misspelled words as well as ungrammatical and/or incomplete sentences.",
        "We observe the following:",
        "1.",
        "Concepts are seen as a set of model sentences that are subsumed by the list of model sentences built by humans 2.",
        "Evidence is seen as a list of model \"sentences\" that nearly subsume the set generated by humans (i.e., the intersection is not empty) Approach",
        "In the automatic approach, we select the Evidence highlighted in the DEV dataset as MSiJs.",
        "We either choose the intersection of Evidence (i.e., where both human raters agree) or the union (i.e., highlighted by either human) as entailing a concept.",
        "Required lexicon for an item includes the most essential lexicon for this item.",
        "In the KE approach, the required lexicon is selected by the model builder, who makes a judgment about it.",
        "In Alchemist, a model builder is presented with a tokenized model sentence and s/he clicks on a token to select it as a required lexical entity.",
        "We have observed that selecting required lexicon RLijk involves ignoring or removing noise, such as stop-words (e.g., \"a,\" \"the,\" \"to,\" etc.",
        "), from the presented model sentence.",
        "For example, a model builder may select the words, \"how,\" \"rain,\" \"formation,\" and \"winter\" in the model sentence \"How does rain formation occur in the winter?\"",
        "and ignore the rest.",
        "In addition, there might be words other than stop-words that can be ignored.",
        "For example, if a model builder writes, \"It may help Alice and scientists to know how rain formation occurs in the winter\" - the tokens \"scientists\" and \"Alice\" are not stop-words and can be ignored.",
        "Approach",
        "We evaluate five methods of automatically selecting the required lexicon:",
        "In addition to the list of stop-words provided in Van Rijsbergen's book (Rijsbergen, 2004) and the ones we extracted from WordNet 2.0 http://wordnet.princeton.edu/ (except for \"zero,\" \"minus,\" \"plus,\" and \"opposite\"), we have developed a list of approximately 2,000 stop-words based on students' data.",
        "This includes various interjections and common short message service (SMS) abbreviations that are found in students' data (see Table 1 for examples).",
        "The feature extractor in crater, mentioned in Section 2, labels the various noun and verb phrases with a corresponding syntactic or semantic role using in-house developed rules.",
        "We extract the heads of these by simply considering the rightmost lexical entity with an expected POS tag, i.e., for noun phrases we look for the rightmost nominal lexical entity, for verb phrases we look for the rightmost verbs.",
        "We consider all phrases or syntactic roles, i.e., not only noun and verb phrases but also adjective and adverb phrases.",
        "The mutual information (MI) method measures the mutual dependence of two variables.",
        "MI in natural language tasks has been used for information retrieval (Manning et.",
        "al., 2008) and for feature selection in classification tasks (Stoyanchev and Stent, 2009).",
        "Here, MI selects words that are indicative of the correct answer while filtering out the words that are also frequent in incorrect answers.",
        "Our algorithm selects a lexical term if it has high mutual dependence with a correct concept or Evidence in students' answers.",
        "For each term mentioned in a students' answer we compute mutual information measure (I) :",
        "where Nn is the number of student answers with the term co-occurring with a correct concept or Evidence, N0i is the number of student answers with a correct concept but without the term, Ni0 is the number of student answers with the term but without a correct concept, N00 is the number of student answers with neither the term nor a correct concept, Ni is the total number of student answers with the term, N is the total number of utterances with a correct concept, and N is the total number of utterances.",
        "The MI method selects the terms or words predictive of both presence and absence of a concept.",
        "In this task we are interested in finding the terms that indicate presence of a correct concept.",
        "We ignore the words that are more likely to occur without the concept (the words for which N11< N10).",
        "In this study, after looking at the list of words produced, we simply selected the top 40 words with the highest mutual information measure."
      ]
    },
    {
      "heading": "1.. Consider all tokens in MS iJ",
      "text": []
    },
    {
      "heading": "2.. Consider all tokens in MS iJ without stop-words",
      "text": []
    },
    {
      "heading": "3.. Consider all heads of NPs and VPs (nouns and verbs)",
      "text": [
        "4.",
        "Consider all heads of all various syntactic roles including adjectives and adverbs",
        "5.",
        "Consider the lexicon with the highest mutual information measures, with all lexical tokens in model sentences corresponding to the same concept Motivation Approach",
        "Concept Ci",
        "Evidence Eir",
        "MSij",
        "Ci",
        "Eii",
        "MS11",
        "Eisi",
        "MSiti",
        "C2",
        "E21",
        "MS21",
        "E2s2",
        "MS2,2",
        "The first method does not need any elaboration.",
        "In the following, we briefly elaborate on each of the other methods.",
        "In the KE approach, once a model builder selects a required word, a screen on Alchemist lists similar words extracted automatically from Dekang Lin's dependency-based thesaurus.",
        "The model builder can also use other resources like Roget's thesaurus (http://gutenberg.org/etext/22) and WordNet 3.0 (http://wordnet.princeton.edu/).",
        "The model builder can also write her/his own words that s/he believes are similar to the required word.",
        "Other than choosing no similar lexicon to a required word W, automatically selecting similar lexicon consists of the following experiments:",
        "1.",
        "Umm",
        "2.",
        "Aka",
        "3.",
        "Coz",
        "4.",
        "Viz.",
        "5. e.g.",
        "6.",
        "Hmm",
        "7.",
        "Phew",
        "8.",
        "Aha",
        "9.",
        "Wow",
        "10.",
        "Ta",
        "ll.Yippee",
        "12.",
        "NTHING",
        "13.",
        "Dont know",
        "14.",
        "Nada",
        "15.",
        "Guess",
        "16.",
        "Yoink",
        "17.",
        "RUOK",
        "18.",
        "SPK"
      ]
    },
    {
      "heading": "1.. All words similar to W in Dekang Lin's generated list",
      "text": [
        "2.",
        "Direct synonyms for W or its lemma from WordNet 3.0 (excluding compounds).",
        "Compounds are excluded because we noticed many irrelevant compounds that could not replace uni-grams in our data.",
        "3.",
        "All similar words for W or its lemma from WordNet 3.0, i.e., direct synonyms, related words and hypernyms (excluding compounds).",
        "Hypernyms of W are restricted to a maximum of 2 levels up from W",
        "To summarize, for each concept in the KE approach, a model builder writes a set of Model Sentences, manually selects Required Lexicon and Similar Lexicon for each required word.",
        "In the automated approach, all of the above is selected automatically.",
        "Table 2 summarizes the methods or experiments.",
        "We refer to a method or experiment in the order of selection of RLijkand SLijkt; e.g., we denote the method where all words were required and similar lexicon chosen from WordNet Direct synonyms by AWD.",
        "HSVocWA denotes the method where heads of NPs and VPs with similar words from Word-Net All, i.e., direct, related, and hypernyms are selected.",
        "A method name preceded by I or U refers to Evidence Intersection or Union, respectively.",
        "For each item, there are 40 experiments/methods performed with Evidence as model sentences.",
        "Table 2.",
        "Parameters and \"Values\" of Model Building",
        "Before presenting the evaluation results, we make a note about spelling correction.",
        "crater has its own automatic spelling corrector.",
        "Here, we only outline how spelling correction relates to a model.",
        "In the KE approach, model sentences are assumed to not having spelling errors.",
        "We use the model sentences, the stimulus (if it exists), and the prompt of the item for additional guidance to select the correctly-spelled word from a list of potential correctly-spelled words designated by the spelling corrector.",
        "On the other hand, the Evidence can be misspelled.",
        "Consequently, when the Evidence is considered for model sentences, the spelling corrector first performs spelling correction on the Evidence, using stimulus, concepts, and prompts as guides.",
        "The students' answers are then corrected, as in the KE approach."
      ]
    },
    {
      "heading": "6. Evaluation",
      "text": [
        "The study involves 12 test items developed at ETS for grades 7 and 8.",
        "There are seven Reading Comprehension items, denoted R1-R7 and five Mathematics items, denoted M1-M5.",
        "Score points for the items range from 0 to 3 and the number of concepts ranges from 2 to 7.",
        "The answers for these items were collected in schools in Maine, USA.",
        "The number of answers collected for each item ranges from 190264.",
        "Answers were concept-based scored by two human raters (H1, H2).",
        "We split the double-scored students' answers available into DEV (90-100 answers), XVAL (40-50) and BLIND (60-114).",
        "Training data refer to DEV together with XVAL datasets.",
        "Results are reported in terms of un-weighted kappa, representing scoring agreement with humans on the BLIND dataset.",
        "H1/2 refers to the agreement between the two humans, c-H1/2 denotes the average of kappa values between crater and each human (c-H1 and c-H2).",
        "Table 3 reports the best kappa over the 40 experiments on",
        "BLIND (Auto I or U).",
        "The baseline (Auto C) uses concepts as model sentences.",
        "Table 3.",
        "Best on BLIND over all experiments",
        "Model Sentences",
        "Required Lexicon",
        "Similar Lexicon",
        "Concepts",
        "All words (A)",
        "None chosen (N)",
        "(C)",
        "Evidence",
        "All words with no stop-",
        "Lin all (L)",
        "Intersection",
        "words (S)",
        "(I)",
        "Evidence",
        "Heads of NPs and VPs",
        "WordNet direct",
        "Union (U)",
        "(HSvoc)",
        "synonyms (WD)",
        "Heads of all phrases (HA)",
        "WordNet all",
        "similar words",
        "(WA)",
        "Highest Mutual informa-",
        "tion measure (M)",
        "#Training",
        "Auto",
        "Auto",
        "Item",
        "(Blind)",
        "H1/2",
        "Manual",
        "C",
        "I or U",
        "c-H1/2",
        "c-H1/2",
        "c-H1/2",
        "R1",
        "l50 (ll4)",
        "l.O",
        "0.94",
        "0.5l",
        "0.97",
        "R2",
        "l50 (ll3)",
        "0.76",
        "0.69",
        "0.2S",
        "0.76",
        "R3",
        "l50 (l07)",
        "0.96",
        "0.87",
        "0.lS",
        "0.88",
        "R4",
        "l50 (66)",
        "0.77",
        "0.71",
        "0.46",
        "0.75",
        "R5",
        "l30 (60)",
        "0.7l",
        "0.58",
        "0.22",
        "0.61",
        "R6",
        "l30 (6l)",
        "0.7l",
        "0.73",
        "0.23",
        "0.77",
        "R7",
        "l30 (6l)",
        "0.S7",
        "0.55",
        "0.42",
        "0.42",
        "M1",
        "l30 (67)",
        "0.7l",
        "0.6",
        "0.0",
        "0.66",
        "M2",
        "l30 (67)",
        "0.S",
        "0.71",
        "0.54",
        "0.67",
        "M3",
        "l30 (67)",
        "0.S6",
        "0.76",
        "0.0",
        "0.79",
        "M4",
        "l30 (67)",
        "0.S7",
        "0.82",
        "0.l3",
        "0.82",
        "M5",
        "l30 (67)",
        "0.77",
        "0.63",
        "0.29",
        "0.65",
        "The accuracy using the automated approach with Evidence as model sentences is comparable to that of the KE approach (noted in the column labeled, \"Manual\") with a 0.1 maximum difference in un-weighted kappa statistics.",
        "The first methods (in terms of running order) yielding the best results for the items (in order of appearance in Table 3) are ISWD, ISW, ISN, IMN, IHSVocN, UHALA, ISN, UHSVocN, SLA, ISN, IHAN and IHS-VocWA.",
        "The methods yielding the best results (regardless of running order) for all items using the Evidence were:",
        "IHAN U/IHAWD IHAWA",
        "U/IHALA U/IHSvocN IHSvocWA UHSvocLA UHSvocWA UHSvocWD",
        "This approach was only evaluated on a small number of items.",
        "We expect that some methods will outperform others through additional evaluation.",
        "In an operational setting (i.e., not a research environment), we must choose a model before we score the BLIND data.",
        "Hence, a voting strategy over all the experiments has to be devised based on the results on DEV and XVAL.",
        "Following our original logic, i.e., using XVAL to avoid over-fitting and predicting the results of BLIND, we implemented a simple voting strategy.",
        "We considered c-H1/2 on XVAL for each experiment.",
        "We found the maximum over all the c-H1/2 for all experiments.",
        "The model corresponding to the maximum was considered the model for the item and used to score the BLIND data.",
        "When there was a tie, the first method to yield the maximum W chosen.",
        "Table 4 shows the results on BLIND using the voting strategy.",
        "The results are comparable to those of the manual approach except for R7 which has 7 concepts, the highest number of concepts among all items.",
        "The results also show that the voting strategy did not select the \"best\" model or experiment.",
        "We notice that some methods were better in detecting whether an answer entailed a concept C than detecting whether it entailed another concept D, specified for the same item.",
        "This implies that the voting strategy will have to be a function that not only considers the overall kappa agreement (i.e., holistic scores), but concept-based agreement (i.e., using concept-based scores).",
        "Next, we noticed that for R7, XVAL did not predict the results on BLIND.",
        "This was mainly due to the inability to apply stratified sampling with such a small sample size when there are 7 concepts involved.",
        "Further, we may need to take advantage of the training data differently, e.g. an «-fold cross-validation approach.",
        "Finally, when there is a tie, factors other than running order should be considered.",
        "Table 4.",
        "Voting Strategy results on BLIND",
        "In all of the above experiments, the Evidence was corrected using the crater's automatic spelling corrector using the stimulus (in case of Reading), the concepts, and the prompts to guide the selection of the correctly-spelled words."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "Analytic-based content scoring is an application of textual entailment.",
        "The complexity of the problem increases due to the noise in student data, the context of an item, and different subject areas.",
        "In this paper, we have shown that building a crater scoring model for an item can be reduced from 12 to 0 hours of human intervention with comparable scoring performance.",
        "This is a significant improvement on research to date using supervised techniques.",
        "In addition, as far as we know, no one other than Calvo et al.",
        "(2005) made any comparisons between a manually-built \"thesaurus\" (e.g. WordNet) and an automatically-generated \"thesaurus\" (e.g. Dekang Lin's database) in an NLP task or application prior to our work.",
        "Our next step is to evaluate (and refine) the approach on a larger set of items.",
        "Further improvements will include using Negative Evidence, automating concept-based scoring, investigating a context-sensitive selection of similar words using the students' answers and experimenting with various voting strategies.",
        "Finally, we need to compare the results reported using unsupervised techniques on the same items and datasets if possible.",
        "c-H1/2",
        "c-H1/2",
        "c-H1/2",
        "R1",
        "150 (114)",
        "1.0",
        "0.94",
        "0.51",
        "0.88",
        "R2",
        "150 (113)",
        "0.76",
        "0.69",
        "0.18",
        "0.61",
        "R3",
        "150 (107)",
        "0.96",
        "0.87",
        "0.18",
        "0.86",
        "R4",
        "150 (66)",
        "0.77",
        "0.71",
        "0.38",
        "0.67",
        "R5",
        "130 (60)",
        "0.71",
        "0.58",
        "0.17",
        "0.51",
        "R6",
        "130 (61)",
        "0.71",
        "0.73",
        "0.13",
        "0.73",
        "R7",
        "130 (61)",
        "0.87",
        "0.55",
        "0.39",
        "0.16",
        "M1",
        "130 67)",
        "0.71",
        "0.6",
        "0.0",
        "0.65",
        "M2",
        "130 67)",
        "0.8",
        "0.71",
        "0.54",
        "0.58",
        "M3",
        "130 67)",
        "0.86",
        "0.76",
        "0.0",
        "0.79",
        "M4",
        "130 67)",
        "0.87",
        "0.82",
        "0.13",
        "0.68",
        "M5",
        "130 67)",
        "0.77",
        "0.63",
        "0.26",
        "0.49"
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Special thanks to Michael Flor, Rene Lawless, Sarah Ohls and Waverely VanWinkle."
      ]
    }
  ]
}
