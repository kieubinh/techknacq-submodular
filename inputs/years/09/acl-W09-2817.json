{
  "info": {
    "authors": [
      "Anja Belz",
      "Eric Kow",
      "Jette Viethen"
    ],
    "book": "Proceedings of the 2009 Workshop on Language Generation and Summarisation (UCNLG+Sum 2009)",
    "id": "acl-W09-2817",
    "title": "The GREC Named Entity Generation Challenge 2009: Overview and Evaluation Results",
    "url": "https://aclweb.org/anthology/W09-2817",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "The grec-neg Task at Generation Challenges 2009 required participating systems to select coreference chains for all people entities mentioned in short encyclopaedic texts about people collected from Wikipedia.",
        "Three teams submitted six systems in total, and we additionally created four baseline systems.",
        "Systems were tested automatically using a range of existing intrinsic metrics.",
        "We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools.",
        "In addition, systems were tested in an intrinsic evaluation involving human judges.",
        "This report describes the grec-neg Task and the evaluation methods applied, gives brief descriptions of the participating systems, and presents the evaluation results."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The grec-neg task is about how to generate appropriate references to people entities in the context of a piece of discourse longer than a sentence.",
        "Rather than requiring participants to generate referring expressions (res) from scratch, the grec-neg data provides sets of possible res for selection.",
        "This was the first time we ran a shared task using this data.",
        "grec-neg is a step further from the related grec-msr Task in that it requires systems to generate plural as well as singular references, for all people entities mentioned in a text (grec-msr in contrast only had singular references to a single entity).",
        "Moreover in grec-neg, possible res for each entity are provided as one set for each entity (rather than one set for each context), so the task of selecting an appropriate re for a given context is harder than in grec-msr.",
        "The main aim for participating systems in grec-neg'09 was to select an appropriate type of re (name, common noun, pronoun, or empty reference).",
        "The immediate motivating application context for the grec Tasks is the improvement of referential clarity and coherence in extractive summaries and multiply edited texts (such as Wikipedia articles) by regenerating REs contained in them.",
        "The motivating theoretical interest for the GREC Tasks is to discover what kind of information is useful in the input when making decisions about different properties of referring expressions when such expressions are being generated in context (this is in contrast to most traditional referring expression generation work in NLG which views the reg task as context-independent).",
        "The grec-neg data is derived from the newly created GREC-People corpus which consists of1,000 annotated introduction sections from Wikipedia articles in the category People.",
        "Nine teams from seven countries registered for the grec-neg'09 Task, of which three teams ultimately submitted six systems in total (see Table 1).",
        "We also used the corpus texts themselves as 'system' outputs, and created four baseline systems.",
        "We evaluated the resulting 11 systems using a range of intrinsic and extrinsic evaluation methods.",
        "This report presents the results of all evaluations (Section 6), along with descriptions of the grec-neg data (Sections 2) and task (Section 3), the test sets and evaluation methods (Section 4), and the participating systems (Section 5).",
        "Table 1: grec-neg'09 teams and systems."
      ]
    },
    {
      "heading": "2. GREC-NEG Data",
      "text": [
        "The grec-neg data is derived from the newly created GREC-People corpus which consists of 1,000 annotated introduction sections from Wikipedia articles in the category People.",
        "An introduction section was defined as the textual content of a Wikipedia article from the title up to (and excluding) the first section heading, the table of contents or the end of the text, whichever comes sooner.",
        "Each text belongs to one of three subcategories: inventors, chefs and early music composers.",
        "For the purposes of the grec-neg'09 competition, the grec-People corpus was divided into training, development and test data.",
        "The number of texts in the 3 data sets and 3 subdomains are as follows:",
        "Team",
        "System name(s)",
        "Univ.",
        "Delaware",
        "ICSI, Berkeley",
        "Univ.",
        "Wolverhampton",
        "UDel-NEG-1, UDel-NEG-2, UDel-NEG-3 ICSI-CRF",
        "WLV-STAND, WLV-BIAS",
        "In these texts we have annotated mentions of people by marking up the word strings that function as referential expressions (res) and annotating them with coreference information as well as syntactic and semantic features.",
        "The subject of each text is a person, so there is at least one coreference chain in each text.",
        "The numbers of coreference chains (entities) in the 900 texts in the training/development sets are as shown in Table 2.",
        "The texts vary greatly in length, from 13 words to 935, with an average of 128.98 words.",
        "This section describes the different types of referring expression (re) that we annotated in the grec-People corpus.",
        "These manual annotations were then automatically checked and converted to the xml format described in Section 2.2 (which encodes slightly less information, as explained below).",
        "In terminology and the treatment of syntax used in the annotation scheme and discussion of it in this report we rely heavily on The Cambridge Grammar of the English Language by Huddleston and Pullum which we will refer to as CGEL for short below (Huddleston and Pullum, 2002).",
        "In the example sentences below, (unbroken) underlines are used for referential expressions (res) that are an example of the specific type of re they are intended to illustrate, whereas dashed underlines are used for other annotated res.",
        "Coreference between res is indicated by subscripts i, j,... immediately to the right of an underline (their scope is one example sentence, i.e. an i in one example sentence does not represent the same entity as an i in another example sentence).",
        "Square brackets indicate supplements.",
        "The syntactic component relativised by a relative pronoun is indicated by vertical bars.",
        "Supplements and their anchors (in the case of appositive supplements), and relative clauses and the component they relativise (in the case of relative-clause supplements) are co-indexed by superscript x,y,....",
        "Dependents integrated in an re are indicated by curly brackets.",
        "Supplements and dependents are highlighted in bold where they specifically are being discussed.",
        "In the xml format of the annotations, the beginning and end of a reference is indicated by other properties discussed in the following sections (e.g. syntactic category) are encoded as attributes on these tags (for details see Section 2.2).",
        "For grec-neg'09 we decided not to transfer the annotations of integrated dependents and relative clauses to the xml format.",
        "Such dependents are included within <refex> .. .</refex> annotations where appropriate, but without being marked up as separate constituents.",
        "This section describes the types of res we annoted in the grec-People Corpus.",
        "I Subject nps: referring subject nps, including pronouns and special cases of vp coordination:",
        "1.",
        "He, was born in Ramsay township, near Almonte, Ontario, Canada, the eldest son of \\Scottish immigrants, {John Naismith and Margaret Young} \\x fc {who, k had arrived in the area in 1851 and .",
        ", worked in the mining industry]x.",
        "2.",
        "The Bariu Musa brothers, .",
        "k were three 9th century Persian scholars, of Baghdad, active in the House of Wisdom.",
        "Ia Subjects of gerund-participials:",
        "1.",
        "Hisi research on hearing and speech eventually culminated in Bell, being awarded the first U.S. patent for the invention of the telephone in 1876.",
        "2.",
        "Fessendeni used the alternator-transmitter to send out a short program from Brant Rock, which included his, playing the song O Holy Night on the violin and_i reading a passage from the Bible.",
        "II Object nps: referring nps including pronouns that function as direct or indirect objects of vps and prepositional phrases; e.g.:",
        "We have annotated two kinds of supplements in the GREC-People corpus, supplementary relative clauses (CGEL, p. 1058), and appositive supplements.",
        "The former is not transferred to the XML annotation, for more information see (Belz, 2009).",
        "The following examples illustrate annotation of appositive supplements (which are in bold):",
        "As can be seen from some of the examples above, we annotated all embedded references.",
        "The maximum depth of embedding that occurs in the GREC-People corpus is 3.",
        "We annotated all plural REs that refer to groups of people where the number of group members is known.",
        "For an explanation of our treatment of res that are coordinations of NPs, see the grec-neg'09 documentation (Belz, 2009).",
        "We have annotated all mentions of individual person entities even if they are not actually named anywhere in the text, and including cases of both definite and indefinite references, e.g.:",
        "1.",
        "The resolution 's sponsori described it as ...",
        "2.",
        "... with the help of Robert CailliaUj and a {young} student staff {at CERN}k."
      ]
    },
    {
      "heading": "1.. Many of the alpinists arrested with Vitaly Abalakov i",
      "text": [
        "were executed.",
        "2.",
        "Het entrusted them, h l to Ishaq bin Ibrahim al-Mus 'abix_, [a former governor of Baghdad] xm.",
        "All",
        "Inventors",
        "Chefs",
        "Composers",
        "Total",
        "1,000",
        "307",
        "306",
        "387",
        "Training",
        "809",
        "249",
        "248",
        "312",
        "Development",
        "91",
        "28",
        "28",
        "35",
        "Test",
        "100",
        "31",
        "30",
        "39",
        "Table 2: Numbers of person entities (hence coreference chains) in texts in the training/development data, e.g. there are 38 texts which mention exactly 5 person entities.",
        "IIa Reflexive pronouns:",
        "1.",
        "Smithi called himselfi the \"Komikal Konjurer\".",
        "III Subject-determiner genitives: genitive NPs (including genitive forms of pronouns) that function as subject-determiners, i.e. syntactic components that \"combine the function of determiner, marking the NP as definite, with that of complement (more specifically subject).\"",
        "(CGEL, p. 56):",
        "1.",
        "Theyi .",
        "k shared the 1956 Nobel Prize in Physics for their .",
        ", invention.",
        "2.",
        "On the eve of his, death in 1605, the Mughal empire spanned almost 500 million acres (doubling during AkbaLAi reign).",
        "Note that this category excludes lexicalised cases, e.g. the so-called \"Newton's method\".",
        "IIIa REs in composite nominals: this is the only type of RE we have annotated that is not an NP, but a nominal.",
        "This type functions as integrated attributive complement, e.g.:",
        "1.",
        "The Eichengriini version was ignored by historians ...",
        "2.",
        "The new act was a great success, largely despite the various things Blacktoni and Smithy were doing between the Edison h films.",
        "Note that this category too excludes lexicalised cases, e.g. the Nobel Prizes; the Gatling gun."
      ]
    },
    {
      "heading": "1.. John W. Campbell, Jr. x",
      "text": [
        "[the editor of Astounding magazine .",
        "2. was the eldest of the six children of Thomas Aspdinx, [a bricklayer living in the Hunslet district of Leeds ^",
        "In the XML version, anchor and supplement are simply annotated as two (or occasionally three) independent, usually adjacent res (refexs); the syntactic function of the second (and third) re is marked as appositive supplement",
        "(SYNFUNC= \" app - supp \" ).",
        "Figure 1 shows one of the XML-annotated texts from the grec-neg data.",
        "Each such text consists of two initial lines of xml declarations followed by a grec- item.",
        "A grec- item consists of a TEXT element followed by an ALT-REFEX element.",
        "A text has one attribute (an id unique within the corpus), and is composed of one title followed by any number of paragraphs.",
        "A title is just a string of characters.",
        "A paragraph is any combination of character strings and REF elements.",
        "The REF element indicates a reference, in the sense of 'an instance of referring' (which could, in principle, be realised by gesture or graphically, as well as by a string of words, or a combination of these).",
        "A ref is composed of one refex element (the 'selected' referential expression for the given reference; in the corpus texts it is the referential expression found in the corpus).",
        "The attributes of the REF element are ENTiTY (entity identifier), mention (mention identifier), semcat (semantic category), syncat (syntactic category), and synfunc (syntactic function).",
        "For full details and ranges of values see (Belz, 2009).",
        "entity and mention together constitute a unique identifier for a reference within a text; together",
        "Entities",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "8",
        "9",
        "10",
        "11",
        "12",
        "13",
        "14",
        "15",
        "16",
        "17",
        "18",
        "19",
        "20",
        "21",
        "22",
        "23",
        "Texts",
        "437",
        "192",
        "80",
        "63",
        "38",
        "31",
        "16",
        "18",
        "4",
        "7",
        "9",
        "1",
        "1",
        "0",
        "0",
        "0",
        "0",
        "0",
        "0",
        "1",
        "1",
        "0",
        "1",
        "<TITLE>Alexander Fleming</TITLE>",
        "<REFEX ENTITY=\"0\" REG08-TYPE=\"name\" CASE=\"plain\">Fleming</REFEX> </REF> published many articles on bacteriology, immunology, and chemotherapy.",
        "<REF ENTITY=\"0\" MENTION=\"3\" SEMCAT=\"person\" SYNCAT=\"np\" SYNFUNC=\"subj-det\">",
        "<REFEX ENTITY=\"0\" REG08-TYPE=\"pronoun\" CASE=\"genitive\">his</REFEX> </REF> best-known achievements are the discovery of the enzyme lysozyme in 1922 and the discovery of the antibiotic substance penicillin from the fungus Penicillium notatum in 1928, for which <REF ENTITY=\"0\" MENTION=\"4\" SEMCAT=\"person\" SYNCAT=\"np\" SYNFUNC=\"subj\">",
        "<REFEX ENTITY=\"0\" REG08-TYPE=\"pronoun\" CASE=\"nominative\">he</REFEX> </REF> shared the Nobel Prize in Physiology or Medicine in 1945 with <REF ENTITY=\"1\" MENTION=\"1\" SEMCAT=\"person\" SYNCAT=\"np\" SYNFUNC=\"obj\"> <ALT-REFEX> <REFEX ENTITY=\"0\" REG08-TYPE=\"name\" CASE=\"genitive\">Fleming's</REFEX> <REFEX ENTITY=\"0\" REG08-TYPE=\"name\" CASE=\"genitive\">Sir Alexander Fleming's</REFEX> <REFEX ENTITY=\"0\" REG08-TYPE=\"name\" CASE=\"plain\">Fleming</REFEX> <REFEX ENTITY=\"0\" REG08-TYPE=\"name\" CASE=\"plain\">Sir Alexander Fleming</REFEX> <REFEX ENTITY=\"0\" REG08-TYPE=\"pronoun\" CASE=\"accusative\">him</REFEX> <REFEX ENTITY=\"0\" REG08-TYPE=\"pronoun\" CASE=\"genitive\">his</REFEX> <REFEX ENTITY=\"0\" REG08-TYPE=\"pronoun\" CASE=\"nominative\">he</REFEX> <REFEX ENTITY=\"0\" REG08-TYPE=\"pronoun\" CASE=\"nominative\">who</REFEX> <REFEX ENTITY=\"1\" REG08-TYPE=\"name\" CASE=\"genitive\">Florey's</REFEX> <REFEX ENTITY=\"1\" REG08-TYPE=\"pronoun\" CASE=\"accusative\">him</REFEX> <REFEX ENTITY=\"1\" REG08-TYPE=\"pronoun\" CASE=\"genitive\">his</REFEX> <REFEX ENTITY=\"1\" REG08-TYPE=\"pronoun\" CASE=\"nominative\">he</REFEX> <REFEX ENTITY=\"1\" REG08-TYPE=\"pronoun\" CASE=\"nominative\">who</REFEX> <REFEX ENTITY=\"2\" REG08-TYPE=\"name\" CASE=\"genitive\">Chain's</REFEX> <REFEX ENTITY=\"2\" REG08-TYPE=\"pronoun\" CASE=\"accusative\">him</REFEX> <REFEX ENTITY=\"2\" REG08-TYPE=\"pronoun\" CASE=\"genitive\">his</REFEX> <REFEX ENTITY=\"2\" REG08-TYPE=\"pronoun\" CASE=\"nominative\">he</REFEX>",
        "<REFEX ENTITY=\"2\" REG08-TYPE=\"pronoun\" CASE=\"nominative\">who</REFEX> </ALT-REFEX> </GREC-ITEM>",
        "with the text id, they constitute a unique identifier for a reference within the entire corpus.",
        "A refex element indicates a referential expression (a word string that can be used to refer to an entity).",
        "The attributes of the refex element are",
        "REG0 8-TYPE(name, common, pronoun, empty), and CASE (nominative, accusative, etc.",
        ").",
        "We allow arbitrary-depth embedding of references.",
        "This means that a refex element may have ref element(s) embedded in it.",
        "See also next but one paragraph for embedding in REFEX elements that are contained in alt-refex lists.",
        "The second (and last) component of a grec-item is an alt-refex element which is a list of refex elements.",
        "For the grec-neg'09 Task, these were obtained by collecting the set of all REFEXs that are in the text, and adding several defaults including pronouns and other cases (e.g. genitive) of res already in the list.",
        "ref elements that are embedded in refex elements contained in an alt-refex list have an unspecified mention id (the '?'",
        "value).",
        "Furthermore, such REF elements have had their enclosed REFEX removed.",
        "For example: <ALT-REFEX>"
      ]
    },
    {
      "heading": "3. The GREC-NEG Task",
      "text": [
        "The test data inputs were identical to the training/development data (Figure 1), except that ref elements in the test data do not contain a REFEX element, i.e. they are 'empty'.",
        "The task for participating systems is to select one REFEX from the ALT-REFEX list for each REF in each TEXT in the test sets.",
        "If the selected refex contains an embedded ref then participating systems also need to select a refex for this embedded ref and to set the value of its mention attribute.",
        "The same applies to all further embedded refexs, at any depth of embedding."
      ]
    },
    {
      "heading": "4. Evaluation Procedures",
      "text": [
        "The grec-neg data set was divided into training, development and test data.",
        "We performed evaluations on the test data, using a range of different evaluation methods, including intrinsic and extrinsic, automatically assessed and human-evaluated, as described in the following sections.",
        "Participants computed evaluation scores on the development set, using the geval -2.0.pl code provided by us which computes Word String Accuracy, reg'08-Type Recall and Precision, string-edit distance and bleu.",
        "We created two versions of the test data for the grec-neg Task:",
        "1. grec-neg Test Set 1a: randomly selected 10% subset (100 texts) of the grec-People corpus (with the same proportion of texts in the 3 subdomains as in the training/development data).",
        "2. grec-neg Test Set 1b: the same subset of texts as in (1a); for this set we did not use the resin the corpus, but replaced each of them with human-selected alternatives obtained in an online experiment as described in (Belz and Varges, 2007); this test set therefore contains three versions of each text where all the refexs in a given version were selected by one 'author'.",
        "Test Set 1a has a single version of each text, and the scoring metrics below that are based on counting matches (Word String Accuracy counts matching word strings, reg08-Type Recall/Precision count matching reg08-Type attribute values) simply count the number of matches a system achieves against that single text.",
        "Test Set 1b, however, has three versions of each text, so the match-based metrics first calculate the number of matches for each of the three versions and then use (just) the highest number of matches.",
        "The chief humanlikeness measures we computed were reg08-Type Recall and Precision.",
        "reg08-Type Precision is defined as the proportion of refexs selected by a participating system which match the reference refexs (where match counts are obtained as explained in the preceding section).",
        "reg08-Type Recall is defined as the proportion of reference refexs for which a participating system has produced a match.",
        "The reason why we use reg08-Type Recall and Precision for grec-neg rather than reg08-Type Accuracy as in grec-msr is that in grec-neg (unlike in grec-msr) there may be a different number of refexs in system outputs and the reference texts in the test set (because there are embedded references in grec-People, and systems may select refexs with or without embedded references for any given ref).",
        "We also computed String Accuracy, defined as the proportion of word strings selected by a participating system that match those in the reference texts.",
        "This was computed on complete, 'flattened' word strings contained in the outermost refex i.e. embedded refex word strings were not considered separately.",
        "We also computed bleu-3, nist, string-edit distance and length-normalised string-edit distance, all on word strings defined as for String Accuracy.",
        "bleu and nist are designed for multiple output versions, and for the string-edit metrics we computed the mean of means over the three text-level scores (computed against the three versions of a text).",
        "For details, see grec-msr report in this volume.",
        "Given that the motivating application context for the grec-neg Task is improving referential clarity and coherence in multiply edited texts, we designed the human-assessed intrinsic evaluation as a preference-judgment test where subjects expressed their preference, in terms of two criteria, for either the original Wikipedia text or the version of it with system-generated referring expressions in it.",
        "The intrinsic human evaluation involved outputs for 30 randomly selected items from the test set from 5 of the 6 participating systems, the four baselines and the original corpus texts (10 systems in total).",
        "We used a Repeated Latin Squares design which ensures that each subject sees the same number of outputs from each system and for each test set item.",
        "There were three 10x10 squares, and a total of 600 individual judgments in this evaluation (60 per system: 2 criteria x 3 articles x 10 evaluators).",
        "We recruited 10 native speakers of English from among students currently completing a linguistics-related degree at Kings College London and University College London.",
        "Following detailed instructions, subjects did two practice examples, followed by the 30 texts to be evaluated, in random order.",
        "Subjects carried out the evaluation over the internet, at a time and place of their choosing.",
        "They were allowed to interrupt and resume the experiment (though discouraged from doing so).",
        "Figure 2 shows what subjects saw during the evaluation of an individual text pair.",
        "The place (left/right) of the original Wikipedia article was randomly determined for each individual evaluation of a text pair.",
        "People references are highlighted in yellow/orange, those that are identical in both texts are yellow, those that are different are orange.",
        "The evaluator's task is to express their preference in terms of each quality criterion by moving the slider pointers.",
        "Moving the slider to the left means expressing a preference for the text on the left, moving it to the right means preferring the text on the right; the further to the left/right the slider is moved, the stronger the preference.",
        "The two criteria were explained in the introduction as follows (the wording of the first is from duc):"
      ]
    },
    {
      "heading": "1.. Referential Clarity: It should be easy to identify who",
      "text": [
        "the referring expressions are referring to.",
        "If a person is mentioned, it should be clear what their role in the story is.",
        "So, a reference would be unclear if a person is referenced, but their identity or relation to the story remains unclear.",
        "2.",
        "Fluency: A referring expression should 'read well', i.e. it should be written in good, clear English, and the use of titles and names should seem natural.",
        "Note that the Fluency criterion is independent of the Referential Clarity criterion: a reference can be perfectly clear, yet not be fluent.",
        "It was not evident to the evaluators that sliders were associated with numerical values.",
        "Slider pointers started out in the middle of the scale (no preference).",
        "The values associated with the points on the slider ranged from -10.0 to +10.0.",
        "An evaluation we piloted in reg'08 was an automatic approach to extrinsic evaluation (for a more detailed description, see the grec-msr results report elsewhere in this volume).",
        "The basic premise is that poorly chosen reference chains seem likely to affect the reader's ability to resolve res.",
        "In our automatic extrinsic method, the role of the reader is played by an automatic coreference resolution tool and the expectation is that the tool performs worse (is less able to identify coreference chains) with more poorly chosen referential expressions.",
        "To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers – those included in LingPipe and Opennlp (Morton, 2005) – and averaged results.",
        "For the same reason we used three different performance measures: muc-6 (Vilain et al., 1995), ceaf (Luo, 2005), and b-cubed (Bagga and Baldwin, 1998)."
      ]
    },
    {
      "heading": "5. Systems",
      "text": [
        "Base-rand, Base-freq, Base-1st, Base-name:",
        "We created four baseline systems each with a different way of selecting a REFEX from those refexs in the alt-refex list that have matching entity ids.",
        "Base-rand selects a refex at random.",
        "Base-lst selects the first refex.",
        "Base-freq selects the first refex with a reg08-type that is the overall most frequent (as determined from the training/development data) given the syncat, synfunc and semcat of the reference.",
        "Basename selects the shortest REFEX with attribute",
        "REG0 8 -TYPE=name.",
        "UDel: The udel-neg-1 system is identical to the udel system that was submitted to the grec-msr Task (for a description of that system see grec-msr'09 results report in this volume), except that it was adapted to the different data format of grec-neg.",
        "udel-neg-2 is identical to udel-neg-1 except that it was retrained on grec-neg data and the feature set was extended by entity and mention ids.",
        "udel-neg-3 additionally utilised improved identification of other entities.",
        "ICSI-CRF: The icsi-crf system construes the grec-msr task as a sequence labelling task and determines the most likely current class label given preceding labels using a Conditional Random Field model trained using the follow features for the current reference, the most recent preceding reference, and the most recent reference to the same entity: preceding and following word unigram and bigram; suffix of preceding and following word; preceding and following punctuation; reference id; and whether this is the beginning of a paragraph.",
        "If more than one class label remains, the last in the list of possible resin the grec-msr data is selected.",
        "WLV: The wlv systems start with sentence splitting and pos tagging.",
        "wlv-stand then employs a J48 decision tree classifier to obtain a probability for each ref/refex pair that it is a good pair in the current context.",
        "The context is represented by the following set of features.",
        "Features of the REFEX word string: is it the longest of the possible refexs; number of words; all refex features supplied in grec-neg data.",
        "Features of the ref: is it part of the first chain in the text; is it the first mention of the entity; is it at the beginning of the sentence; all ref features supplied in grec-neg data.",
        "Other features: do the preceding words match \", but\", \"and then\" and similar phrases; distance in sentences to last mention; reg08-Type selected for the two preceding REFs; pos tags of 4 words before and 3 words after; correlation between synfunc and case values; size of the chain.",
        "wlv-bias is the same except that it is retrained on reweighted training instances.",
        "The reweighting scheme assigns a cost of 3 to false negatives and 1 to false positives."
      ]
    },
    {
      "heading": "6. Results",
      "text": [
        "This section presents the results of all the evaluation methods described in Section 4.",
        "We start with reg08-Type Precision and Recall, the intrinsic automatic metrics which participating teams were told was going to be the chief evaluation method, followed by Word String Accuracy and other intrinsic automatic metrics (Section 6.2), the intrinsic human evaluation (Section 6.3) and the extrinsic automatic evaluation (Section 6.4).",
        "Participants computed scores for the development set (91 texts) themselves, using the geval evaluation tool provided by us.",
        "These scores are shown in Table 5, and are also included in the participants' reports elsewhere in this volume.",
        "reg08-Type Recall and Precision results for Test Set 1a are shown in column 2 of Table 3.",
        "As would be expected, results on the test data are",
        "System",
        "REG08-Type",
        "WS Acc.",
        "Norm.",
        "SE",
        "Recall",
        "Precision",
        "ICSI-CRF",
        "83.05",
        "83.05",
        "0.786",
        "0.197",
        "WLV-BIAS",
        "77.61",
        "80.26",
        "0.735",
        "0.239",
        "UDelNEG-3",
        "75.27",
        "75.27",
        "0.333",
        "0.636",
        "UDelNEG-2",
        "74.95",
        "74.95",
        "0.323",
        "0.646",
        "UDelNEG-1",
        "68.87",
        "68.87",
        "0.315",
        "0.658",
        "WLV-STAND",
        "66.20",
        "68.46",
        "0.626",
        "0.351",
        "Table 3: REG08-Type Precision and Recall scores against corpus version of Test Set for complete set and for subdomains; homogeneous subsets (Tukey hsd, alpha = .05) for complete set only.",
        "Table 4: reg08-Type Recall and Precision scores against human topline version of Test Set for complete set and for subdomains; homogeneous subsets (Tukey hsd, alpha = .05) for complete set only.",
        "somewhat worse (than on the development data).",
        "Also included in this table are results for the 4 baseline systems, and it is clear that selecting the most frequent re type given semcat, synfunc and syncat (as done by the Base-freq system) provides a strong baseline for re type selection.",
        "The last 6 columns in Table 3 contain Recall (R) and Precision (P) results for the three subdomains.",
        "For most of the systems results are slightly better for Inventors than for Composers, and better for Composers than for Chefs.",
        "A contributing factor to this may be the fact that texts in Chefs tend to be much more colloquial.",
        "Base-1st has by far the worst results; this is because it selects the empty reference in almost all cases (because alt-refex lists are sorted and if a list contains an empty reference it will end up at the beginning).",
        "We carried out univariate anovas with System as the fixed factor, and 'Number of refexs in a text' as a random factor, and reg08-Type Recall as the dependent variable in one anova, and reg08-Type Precision in the other.",
        "The result for Recall was F(10)704) = 81.547,p < 0.001.",
        "The result for Precision was F(10 722) = 79.3 59, p < 0.",
        "001.",
        "The columns containing capital letters in Table 3 show the homogeneous subsets ofsystems as determined by a post-hoc Tukey hsd analysis.",
        "Systems whose scores are not significantly different (at the .05 level) share a letter.",
        "Table 4 shows analogous results computed against Test Set 1b (which has three versions of each text).",
        "These should be considered as the chief results of the grec-neg'09 Task evaluations, as stated in the participants' guidelines.",
        "Table 4 includes results for the corpus texts, computed (as are results for the system outputs in Table 4) against the three versions of each text in Test Set 1b.",
        "We performed univariate anovas with System as the fixed factor, Number of refexs as a random factor, and Recall as the dependent variable in one, and Precision in the other.",
        "The result for Recall was F(10)724) = 72.528, p < .001), and for Precision F(10 722) = 75.476, p < .001.",
        "For both cases, we compared the mean scores with Tukey's hsd.",
        "As can be seen from the resulting homogeneous subsets (letter columns in Table 4), system ranks are the same for Precision and for Recall.",
        "In terms of Precision, the difference between the corpus texts and the icsi-crf system was not significant.",
        "In addition to the chief evaluation measure reported on in the preceding section, we computed",
        "System",
        "REG08-Type Precision and Recall Scores against Corpus (Test Set la)",
        "Ail",
        "Chefs",
        "Composers",
        "Inventors",
        "Precision",
        "Recall",
        "R",
        "P",
        "R",
        "P",
        "R",
        "P",
        "ICSI-CRF",
        "79.12",
        "A",
        "76.92",
        "A",
        "70.01",
        "73.54",
        "78.11",
        "80.18",
        "80.05",
        "81.86",
        "WLV-BIAS",
        "73.77",
        "B",
        "72.70",
        "A",
        "69.82",
        "71.52",
        "73.53",
        "74.38",
        "73.65",
        "74.56",
        "WLV-STAND",
        "64.49",
        "C",
        "63.55",
        "B",
        "58.28",
        "59.70",
        "65.38",
        "66.14",
        "64.78",
        "65.59",
        "Base-freq",
        "61.52",
        "C",
        "59.6",
        "B",
        "49.41",
        "51.86",
        "63.95",
        "65.74",
        "60.59",
        "62.12",
        "UDel-NEG-2",
        "53.21",
        "D",
        "51.14",
        "C",
        "44.38",
        "47.17",
        "50.50",
        "52.22",
        "57.88",
        "59.80",
        "UDel-NEG-3",
        "52.49",
        "D",
        "50.45",
        "C",
        "43.49",
        "46.23",
        "49.79",
        "51.48",
        "57.39",
        "59.29",
        "UDel-NEG-1",
        "50.47",
        "D",
        "48.51",
        "C",
        "42.90",
        "45.60",
        "47.78",
        "49.41",
        "54.43",
        "56.23",
        "Base-rand",
        "43.32",
        "E",
        "42.00",
        "D",
        "38.76",
        "40.43",
        "41.77",
        "43.00",
        "45.07",
        "46.21",
        "Base-name",
        "40.60",
        "E",
        "39.09",
        "D",
        "44 97",
        "47.80",
        "39.06",
        "40.32",
        "34.24",
        "35.28",
        "Base-lst",
        "10.99",
        "F",
        "10.81",
        "E",
        "12.43",
        "12.73",
        "9.30",
        "9.43",
        "12.07",
        "12.22",
        "System",
        "REG08-Type Precision and Recall Scores againsthuman topline (Test Set lb)",
        "Ail",
        "Chefs",
        "Composers",
        "Inventors",
        "Precision",
        "Recall",
        "R",
        "P",
        "R",
        "P",
        "R",
        "P",
        "Corpus",
        "82.67",
        "A",
        "84.01",
        "A",
        "84.24",
        "82.25",
        "84.47",
        "83.26",
        "83.04",
        "82.02",
        "ICSI-CRF",
        "79.33",
        "A",
        "B",
        "78.38",
        "B",
        "76.36",
        "77.54",
        "78.81",
        "79.74",
        "79.30",
        "80.10",
        "WLV-BIAS",
        "77.78",
        "B",
        "77.78",
        "B",
        "77.58",
        "77.58",
        "77.86",
        "77.86",
        "77.81",
        "77.81",
        "WLV-STAND",
        "67.51",
        "C",
        "67.51",
        "C",
        "65.76",
        "65.76",
        "68.60",
        "68.60",
        "67.08",
        "67.08",
        "Base-freq",
        "65.38",
        "C",
        "64.37",
        "C",
        "58.48",
        "59.94",
        "68.07",
        "68.97",
        "62.84",
        "63.64",
        "UDel-NEG-2",
        "57.39",
        "D",
        "56.06",
        "D",
        "55.15",
        "57.23",
        "54.86",
        "55.92",
        "58.85",
        "60.05",
        "UDel-NEG-3",
        "57.25",
        "D",
        "55.92",
        "D",
        "55.76",
        "57.86",
        "54.57",
        "55.62",
        "58.35",
        "59.54",
        "Base-name",
        "55.22",
        "D",
        "54.01",
        "D",
        "54.24",
        "56.29",
        "57.04",
        "58.05",
        "48.63",
        "49.49",
        "UDel-NEG-1",
        "53.57",
        "D",
        "52.32",
        "D",
        "E",
        "51.21",
        "53.14",
        "50.80",
        "51.78",
        "55.86",
        "57.00",
        "Base-rand",
        "48.46",
        "E",
        "47.75",
        "E",
        "47.88",
        "48.77",
        "46.44",
        "47.13",
        "49.88",
        "50.51",
        "Base-lst",
        "12.54",
        "F",
        "12.54",
        "F",
        "13.94",
        "13.94",
        "10.45",
        "10.45",
        "14.96",
        "14.96",
        "Table 6: Word String Accuracy, bleu, nist, and string-edit scores, computed on Test Set 1a (systems in order of Word String Accuracy); homogeneous subsets (Tukey hsd, alpha = .05) for String Accuracy only.",
        "Table 7: Word String Accuracy, bleu, nist, and string-edit scores, computed on Test Set 1b (systems in order of Word String Accuracy); homogeneous subsets (Tukey hsd, alpha = .05) for String Accuracy.",
        "Word String Accuracy and the other string similarity metrics described in Section 4.2.",
        "The resulting scores for Test Set 1a (the corpus texts) are shown in Table 6.",
        "Ranks for peer systems relative to each other are very similar to the results reported in the last section.",
        "However, the ranks of the baseline systems have changed substantially, both in relation to each other and to the peer systems.",
        "In particular, Base-freq has moved all the way down to the bottom of the table.",
        "The reason is that this method is geared towards selecting the correct type of re, but pays no attention to whether it selects a syntactically appropriate re for the given context, instead simply selecting the first re from the alt-refex list that has the selected type; in the grec-neg'09 Task (unlike the gre-msr task) this just happens to be an re in the genitive case most of the time which is overall rarer than nominative/plain.",
        "It is likely that the Word String scores for the udel-neg systems are low for a similar reason.",
        "We performed a univariate anova with System as the fixed factor and Number of refexs as a random factor and Word String Accuracy as the dependent variable.",
        "The result for System was F(10)726) = 103.339; the homogeneous subsets resulting from the Tukey hsd post-hoc analysis are shown in columns 3-9 of Table 6.",
        "Table 7 shows analogous results for human topline Test Set 1b (which has three versions of each text).",
        "We carried out the same kind of anova as for Test Set 1a; the result for System on Word String Accuracy was F(10 726) = 106.755, p < 0.001.",
        "System rankings and homogeneous subsets are the same as for Test Set 1a; scores across the board are somewhat higher, because ofthe way scores are computed for Test Set 1b: it is the highest score a system achieves (at text-level) against any of the three versions of a test set text that is taken into account.",
        "Results for bleu-3, nist and the two string-edit distance metrics are shown in the rightmost 4 columns of Tables 6 and 7.",
        "Systems whose Word String Accuracy scores differ significantly are assigned the same ranks by nist and the two string-edit distance metrics as by Word String Accuracy (except for Base-1st and Base-freq which swap ranks in some.",
        "bleu-3 does the same and also flips icsi-crf and wlv-bias.",
        "In the human intrinsic evaluation, evaluators rated system outputs in terms of whether they preferred them over the original Wikipedia texts.",
        "As a re-",
        "System",
        "String similarity against Corpus (Test Set la)",
        "Word String Accuracy",
        "BLEU-3",
        "NIST",
        "SE",
        "norm.",
        "SE",
        "All",
        "Chefs",
        "Composers",
        "Inventors",
        "ICSI-CRF",
        "74.84",
        "A",
        "68.24",
        "76.63",
        "77.10",
        "0.75",
        "5.78",
        "0.70",
        "0.23",
        "WLV-BIAS",
        "68.57",
        "B",
        "66.35",
        "69.08",
        "69.47",
        "0.76",
        "5.62",
        "0.82",
        "0.29",
        "WLV-STAND",
        "59.55",
        "C",
        "54.72",
        "61.24",
        "60.56",
        "0.73",
        "5.34",
        "1.01",
        "0.39",
        "Base-name",
        "28.48",
        "D",
        "35.53",
        "27.51",
        "24.43",
        "0.5",
        "4.09",
        "1.80",
        "0.67",
        "UDel-NEG-1",
        "16.58",
        "E",
        "20.13",
        "15.09",
        "16.28",
        "0.43",
        "2.47",
        "2.1",
        "0.82",
        "UDel-NEG-2",
        "16.44",
        "E",
        "19.81",
        "14.79",
        "16.54",
        "0.45",
        "2.37",
        "2.08",
        "0.83",
        "UDel-NEG-3",
        "16.37",
        "E",
        "19.18",
        "15.09",
        "16.28",
        "0.45",
        "2.41",
        "2.08",
        "0.83",
        "Base-rand",
        "8.22",
        "F",
        "8.49",
        "7.10",
        "9.92",
        "0.17",
        "0.9",
        "2.43",
        "0.89",
        "Base-lst",
        "7.28",
        "F",
        "7.23",
        "6.36",
        "8.91",
        "0.16",
        "0.98",
        "2.54",
        "0.90",
        "Base-freq",
        "2.52",
        "G",
        "4.40",
        "2.37",
        "1.27",
        "0.31",
        "1.91",
        "2.34",
        "0.90",
        "String similarity against human topline (Test Set lb)",
        "System",
        "Word String Accuracy",
        "BLEU 3",
        "NIST",
        "SE",
        "norm.",
        "SE",
        "All",
        "Chefs",
        "Composers",
        "Inventors",
        "Corpus",
        "81.90",
        "A",
        "83.33",
        "82.25",
        "80.15",
        "0.95",
        "7.15",
        "0.71",
        "0.25",
        "ICSI-CRF",
        "74.55",
        "B",
        "71.70",
        "75.15",
        "75.83",
        "0.86",
        "6.35",
        "0.92",
        "0.31",
        "WLV-BIAS",
        "69.07",
        "C",
        "69.50",
        "68.49",
        "69.72",
        "0.88",
        "6.17",
        "1.03",
        "0.36",
        "WLV-STAND",
        "59.70",
        "D",
        "58.18",
        "60.36",
        "59.80",
        "0.84",
        "5.81",
        "1.21",
        "0.45",
        "Base-name",
        "37.27",
        "E",
        "42.14",
        "36.83",
        "34.10",
        "0.65",
        "5.57",
        "1.73",
        "0.63",
        "UDel-NEG-1",
        "19.25",
        "F",
        "22.96",
        "17.60",
        "19.08",
        "0.51",
        "2.62",
        "2.17",
        "0.82",
        "UDel-NEG-2",
        "18.96",
        "F",
        "22.96",
        "17.31",
        "18.58",
        "0.53",
        "2.42",
        "2.15",
        "0.83",
        "UDel-NEG-3",
        "18.89",
        "F",
        "22.64",
        "17.75",
        "17.81",
        "0.53",
        "2.49",
        "2.15",
        "0.82",
        "Base-rand",
        "10.45",
        "G",
        "10.06",
        "9.91",
        "11.70",
        "0.25",
        "1.11",
        "2.49",
        "0.89",
        "Base-lst",
        "8.65",
        "G",
        "8.49",
        "7.54",
        "10.69",
        "0.24",
        "1.29",
        "2.64",
        "0.92",
        "Base-freq",
        "3.24",
        "H",
        "4.40",
        "3.55",
        "1.78",
        "0.39",
        "2.1",
        "2.40",
        "0.90",
        "Table 8: Results for Clarity and Fluency preference judgement experiment.",
        "Mean = mean of individual scores (where scores ranged from -10.0 to + 10.0); + = number of times system was preferred; – = number of times corpus text (Wikipedia) was preferred; 0 = number of times neither was preferred.",
        "sult of the experiment we had for each system and each evaluation criterion a set of scores ranging from -10.0 to +10.0, where 0 meant no preference, negative scores meant a preference for the Wikipedia text, and positive scores a preference for the system-produced text.",
        "The second column of the left half of Table 8 summarises the Clarity scores for each system in terms of their mean; if the mean is negative the evaluators overall preferred the Wikipedia texts, if it is positive evaluators overall preferred the system.",
        "The more negative the score, the more strongly evaluators preferred the Wikipedia texts.",
        "Columns 9-11 show corresponding counts of how many times each system was preferred (+), dis-preferred ( – ), and neither (0), when compared to Wikipedia.",
        "The other half of Table 8 shows corresponding results for Fluency.",
        "We ran a factorial multivariate anova with Fluency and Clarity as the dependent variables.",
        "In the first version of the anova, the fixed factors were System, Evaluator and Wikipedia_Side (indicating whether the Wikipedia text was shown on the left or right during evaluation).",
        "This showed no significant effect of Wikipedia_Side on either Fluency or Clarity, and no significant interaction between any of the factors.",
        "There was however a mild effect of Evaluator on both Fluency and Clarity.",
        "We ran the anova again, this time with just System and Evaluator as fixed factors.",
        "The result for System on Fluency was F(9 200) = 37.925, p < .001, and for System on Clarity it was F(9 200) = 35.439, p < .001.",
        "Post-hoc Tukey's hsd tests revealed the significant pairwise differences indicated by the letter columns in Table 8.",
        "Correlation between individual Clarity and Fluency ratings as estimated with Pearson's coefficient was r = 696, p < .01, indicating that the two criteria covary to some extent.",
        "Apart from Base-name and wlv- stand switching places, system ranks are the same for Fluency and Clarity.",
        "Moreover, system ranks are very similar to those produced by the string-similarity scores above.",
        "Perhaps the most striking result is that the icsi-crf system does succeed in improving Fluency compared to the original Wikipedia texts: it is preferred 9 times whereas the Wikipedia texts are preferred only 7 times.",
        "Table 9: muc, ceaf and b-cubed F-Scores for all systems; homogeneous subsets (Tukey hsd), alpha = .05, for mean of F-Scores.",
        "We fed the outputs of all 11 systems through the two coreference resolvers, and computed mean muc, ceaf and b-cubed F-Scores as described in Section 4.4.",
        "The second column in Table 9 shows the mean of means of these three F-Scores, to give a single overall result for each of for this evaluation method.",
        "A univariate anova with (text-level) mean F-Score as the dependent variable and System as the single fixed factor revealed a significant main effect of System on mean F-Score (F(10)1089) = 91.634, p < .001).",
        "A post-hoc comparison of the means (Tukey hsd, alpha = .05) found the significant differences indicated by the homogeneous subsets in columns 3-8 (Table 9).",
        "The numbers in the last three columns are the separate muc, ceaf and b-cubed F-Scores for each system, averaged over the two resolver tools (and rounded for reasons of space.",
        "Clarity",
        "Fluency",
        "System",
        "Mean",
        "+",
        "0",
        "-",
        "System",
        "Mean",
        "+",
        "0",
        "-",
        "Corpus",
        "0",
        "A",
        "0",
        "30",
        "0",
        "Corpus",
        "0",
        "A",
        "0",
        "30",
        "0",
        "ICSI-CRF",
        "-1.447",
        "A",
        "B",
        "3",
        "17",
        "10",
        "ICSI-CRF",
        "-0.353",
        "A",
        "9",
        "14",
        "7",
        "WLV-BIAS",
        "-2.437",
        "A",
        "B",
        "C",
        "3",
        "14",
        "13",
        "WLV-BIAS",
        "-2.257",
        "A",
        "B",
        "2",
        "14",
        "14",
        "Base-name",
        "-2.583",
        "B",
        "C",
        "7",
        "7",
        "16",
        "WLV-STAND",
        "-5.823",
        "B",
        "C",
        "1",
        "3",
        "26",
        "WLV-STAND",
        "-4.477",
        "C",
        "D",
        "1",
        "9",
        "20",
        "Base-name",
        "-4.257",
        "C",
        "D",
        "2",
        "5",
        "23",
        "UDelNEG-3",
        "-6.427",
        "D",
        "E",
        "1",
        "4",
        "26",
        "UDelNEG-3",
        "-6.263",
        "C",
        "D",
        "E",
        "1",
        "3",
        "26",
        "UDelNEG-2",
        "-6.667",
        "D",
        "E",
        "1",
        "3",
        "26",
        "UDelNEG-2",
        "-7.13",
        "D",
        "E",
        "0",
        "3",
        "27",
        "Base-rand",
        "-8.183",
        "E",
        "F",
        "0",
        "1",
        "29",
        "Base-rand",
        "-7.513",
        "D",
        "E",
        "0",
        "0",
        "30",
        "Base-freq",
        "-8.26",
        "E",
        "F",
        "0",
        "0",
        "30",
        "Base-freq",
        "-7.57",
        "D",
        "E",
        "0",
        "0",
        "30",
        "Base-lst",
        "-9.357",
        "F",
        "0",
        "0",
        "30",
        "Base-lst",
        "-8.477",
        "E",
        "0",
        "0",
        "30",
        "System",
        "(MUC+CEAF+B3)/3",
        "M",
        "C",
        "B3",
        "WLV-BIAS",
        "62.64",
        "A",
        "57",
        "62",
        "69",
        "ICSI-CRF",
        "61.28",
        "A",
        "B",
        "53",
        "61",
        "69",
        "Base-name",
        "61.11",
        "A",
        "B",
        "55",
        "61",
        "68",
        "Corpus",
        "59.56",
        "A",
        "B",
        "C",
        "53",
        "59",
        "67",
        "UDel-NEG-3",
        "56.13",
        "B",
        "C",
        "D",
        "48",
        "56",
        "65",
        "UDel-NEG-2",
        "55.9",
        "B",
        "C",
        "D",
        "47",
        "55",
        "65",
        "Base-freq",
        "55.85",
        "B",
        "C",
        "D",
        "47",
        "56",
        "65",
        "UDel-NEG-1",
        "54.79",
        "C",
        "D",
        "46",
        "54",
        "64",
        "WLV-STAND",
        "51.69",
        "D",
        "41",
        "53",
        "61",
        "Base-rand",
        "34.86",
        "E",
        "15",
        "38",
        "51",
        "Base-lst",
        "26.36",
        "F",
        "2",
        "31",
        "46"
      ]
    },
    {
      "heading": "7. Concluding Remarks",
      "text": [
        "This was the first time the grec-neg Task was run.",
        "It is a new task not only for an nlg shared-task challenge, but also as a research task in general (post-processing extractive summaries in order to improve their quality seems to be just taking off as a research subfield).",
        "There was substantial interest in the grec-neg Task (as indicated by the nine teams that originally registered).",
        "However, only 3 teams were ultimately able to submit a system.",
        "In particular because of the inclusion of plural references, multiple entities per text and embedded references, the grec-neg Task has a higher entrance level than the grec-msr Task.",
        "We are planning to run it again at Generation Challenges 2010 next year, and are considering the possibility of providing participants with a baseline system which would help e.g. with processing embedded references.",
        "We are also planning to add a named entity recognition preprocessing task, so that this new task in combination with grec-neg can be used to perform end-to-end post-processing of extractive summaries (and other types of multiply edited texts) to improve the clarity and fluency of the referring expressions in them."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Many thanks to the members of the Corpora and siggen mailing lists, and Brighton University colleagues who helped with the online msre selection experiments for grec-neg test set 1b.",
        "Thanks are also due to the Kings College London and University College London students who helped with the intrinsic evaluation experiment."
      ]
    }
  ]
}
