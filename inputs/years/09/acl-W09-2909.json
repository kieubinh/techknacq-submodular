{
  "info": {
    "authors": [
      "Hiromi Wakaki",
      "Hiroko Fujii",
      "Masaru Suzuki",
      "Mika Fukui",
      "Kazuo Sumita"
    ],
    "book": "Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications (MWE 2009)",
    "id": "acl-W09-2909",
    "title": "Abbreviation Generation for Japanese Multi-Word Expressions",
    "url": "https://aclweb.org/anthology/W09-2909",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Hiromi Wakaki^ Hiroko Fujii^ Masaru Suzuki Mika Fukui^ Kazuo Sumita^",
        "'Toshiba Corporation 1 Komukai-Toshiba, Saiwai-ku, Kawasaki, 212-8582, Japan",
        "This paper proposes a novel method for generating Japanese abbreviations from their full forms with the Log-Linear Model (LLM) in order to take advantage of characteristic patterns of Japanese abbreviation.",
        "Our experimental results show that the method is effective for TV program titles that contain colloquial expressions.",
        "The proposed method achieved 78.8% recall for the top 30 candidates, whereas a baseline method using Conditional Random Fields (CRFs) achieved 68.3% recall.",
        "Moreover, from the results of experiments using six data sets classified according to types of character and semantic categories, we show that each performance of the above two methods depends on the types of the full forms."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Much research has been done on abbreviation extraction to detect terms having the same meaning.",
        "However, most previous studies (Hisamitsu and Niwa, 2001; Park and Byrd, 2001; Schwartz and Hearst, 2003; Adar, 2004; Sakai and Ma-suyama, 2005; Nadeau and Turney, 2005; Okazaki and Ananiadou, 2006; Okazaki et al., 2008(1); Okazaki et al., 2008(2)) aimed at extracting abbreviations of organization names and technical terms from well-written documents such as news articles and techincal papers.",
        "Many Japanese terms indicating individual TV programs, songs, comics, novels, and so on, are multi-word expressions and have the characteristics distinct from terms treated in most previous studies on abbreviation extraction.",
        "These terms can take several grammatical forms: a noun phrase, a sentence fragment, and even a sentence.",
        "Also, many of these expressions contain a variety of types of characters: kanji, hiragana, katakana, alphabet, digit, and symbol, and some of them contain colloquial expressions.",
        "Abbreviations of these expressions are often used in colloquial text such as chat or blog, and spoken sentences.",
        "To treat an abbreviation as a term having the same meaning as the original expression for NLP applications such as keyboard-based and speech-based information retrieval, an abbreviation generation method effective for this type of multi-word expressions is needed.",
        "However, it is not easy to ascertain abbreviations associated with their full forms.",
        "This is because although these terms become widely used in speech, they do not appear in well-written documents, such as newspaper articles or research papers, in which the abbreviations are clearly defined for use in the subsequent texts with certain lexical patterns, such as parenthesis.",
        "Therefore this paper describes an approach to generate abbreviation candidates from an original term and to rank them according to their probabilities of abbreviation.",
        "We assume that top-ranked abbreviations will be narrowed down by using Web search results in the future."
      ]
    },
    {
      "heading": "2. Japanese Abbreviation 2.1 Data Sets",
      "text": [
        "Transformations into abbreviations are strongly dependent on languages.",
        "For instance, the term \" 775U~UXf7y (family restaurant)\" is abbreviated as \"7 7 (famires)\" in Japanese, whereas English speakers do not abbreviate it in the same way as Japanese do.",
        "To investigate Japanese abbreviations, we collected them from different perspectives, that is, types of character and semantic categories.",
        "Table 1 shows abbreviation data types, their word counts, and so on.",
        "Ex-",
        "'TV program titles contain colloquial expressions such as slang, pun, coined words, and dialect.",
        "For example, in well-written documents, we do not see such a expression as \"I'm Not An Errand Boy!\"",
        "showed in Figure 2.",
        "amples are given in Figure 2 at the end of this paper.",
        "We extracted abbreviations listed and described on the Japanese Wikipedia site , which is a multilingual project to create a complete and accurate open content encyclopedia.",
        "First, we collected lists of abbreviations classified according to types of Japanese character.",
        "Japanese has three original types of character: kanji, katakana, and hira-gana.",
        "Other types of character are used, such as alphabets, numbers, and symbols.",
        "However, hira-gana is mainly used with kanji, and numbers and symbols are used with other characters.",
        "Therefore, we used three abbreviation lists classified according to alphabetical words , katakana words , and kanji words with hiragana(Figure 2) on Wikipedia.",
        "We extracted pairs of abbreviations and their full forms from each list and obtained 928, 245, and 399 abbreviations, respectively.",
        "Also, we extracted pairs of university names and their abbreviations from a list of university abbreviations on Wikipedia .",
        "In Japanese, many names of organizations have a noun phrase structure combining several nouns, such as names of places (\"H^ (Japan)\", \"'MM.",
        "(Tokyo)\"), names of fields (\"I£f4 (medical)\", (science)\"), for whom (female)\"), and the type of organization(\"^^ (university)\", \"fjJf^LPJT\"(research laboratory)).",
        "Therefore, we used names of universities and extracted 523 abbreviations.",
        "Almost all of the nouns are kanji.",
        "Additionally, we extracted abbreviations of TV program titles from descriptions on each page of Wikipedia.",
        "This is because many TV program titles contain various types of characters or colloquial expressions different from the others we extracted.",
        "However, there are no lists of TV program titles in Wikipedia.",
        "Therefore, we gathered TV program titles satisfying the following criterion: the first sentence of the description of the Wikipedia page of the TV program title indicates that the page is about the TV program.",
        "And, in the same paragraph, if abbreviations are introduced by using key phrases such as \"BSjfpHi A\"(it means \"it is abbreviated as A\"), we extracted bold or parenthetical words in the key phrases.",
        "There were 326 abbreviations.",
        "Finally, we gathered abbreviations of TV program titles in TV schedules written in short form because of space limitations.",
        "In this process, we used program titles in TV schedules in newspapers as short forms and EPG data as long forms.",
        "When a title in the schedule is written with short form of the title with the same date, time, and channel as EPG data, we recognized that it is an abbreviation and the other is its full form.",
        "We extracted 603 abbreviations.",
        "In this paper, we focus on abbreviations that lack some characters compared with the full forms.",
        "The fallowings are well-known characteristics of Japanese abbreviations (Sakai and Masuyama, 2005; Enoki et al., 2007; Murayama and Oku-mura, 2008).",
        "Abbreviations are created according to rules: (1) retain the beginning of a word and omit the rest (truncation); (2) divide an original term into base words, retaining several substrings from some of them, and combine them (contraction).",
        "In particular, four-mora katakana abbreviations are often created by combining two-mora as in the case of the katakana words in Figure 2.",
        "Also, the length of an abbreviation in kanji tends to be two or three letters as in the case of the kanji words in Figure 2.",
        "Moreover, if an original term consists of katakana with the specific characters such as sokuon and chon in the middle, these characters tend to be dropped in abbreviations.",
        "The second and third of katakana terms in Figure 2 are an example of this."
      ]
    },
    {
      "heading": "3. Proposed Method",
      "text": [
        "In this section, we propose a new method to generate Japanese abbreviations by using the Log-Linear Model to rank abbreviation candidates.",
        "As mentioned in Section 2.2, Japanese abbreviation characteristics are evident in the composition of abbreviations, not in generation rules from their full forms.",
        "Therefore, we first generate possible abbreviations from an original term and rank them in descending order of probability of abbreviations.",
        "Our method uses a three-step process as",
        "Table 1: Abbreviation data sets, their types and number of terms(NT), average number of characters with standard deviation(SD), average number of characters per term in each type of character( (a)kanji, (b)katakana, (c)hiragana, (d)alphabet, (e)number, (fjspace), and proportion of terms with a single type of character (SC).",
        "follows: l)base word division, 2)candidate generation, and 3)ranking abbreviations.",
        "In this step, we divide terms into base words for abbreviations because Japanese is an agglutinative language.",
        "In order to deal with neologisms and colloquial expressions, we divide terms by using web search results instead of morphological analyzer.",
        "When a term t is divided into two substrings after the ith charcter t, we denote the anterior half by Si,ant and the posterior half by SiyP0St.",
        "A link strength D(tj) between Si,ant and SiyP0St is defined as follows:",
        "Note that hit(t) is calculated as the number of search results by using the term t in double quotes as one query on the Web.",
        "The formulation of D(ti) is mostly the same as Simpson's Coefficient except that the numerator is modified.",
        "We divide the term t after the fcth character where D(tk) is the smallest and repeat this process by using substrings divided in the previous operation as new t^s recursively.",
        "We heuristically set the stopping conditions as two kanji characters or four characters of other types.",
        "This dividing process works well because a set of words containing a term is stylized expression that is different from a sentence.",
        "For example, suppose that a term t is \"VivaVi-vaV6\", which is one of the TV program titles.",
        "All divisions into two of the term are \"V/ivaVivaV6\", \"Vi/vaVivaV6\", • • •, and \"VivaVivaV/6\".",
        "Here, the symbol \"/\"\" indicates a division point.",
        "Then,",
        "\".",
        "We used Yahoo web search API.",
        "http://developer.yahoo.co.jp/search/webA' 1 /webSearch.html",
        "D(ti)s are calculated as follows:",
        "When D(tg) is the smallest of all D(t{), \"VivaVi-vaV6\" is divided into \"VivaViva\" and \"V6\".",
        "The length of \"V6\" is two and is satisfied with the stopping conditions.",
        "Then, we continue to calculate D(ti) of \"VivaViva\" because the length of the substring is not satisfied with the stopping conditions.",
        "Finally, the divisions are fixed based on the following modifications.",
        "If a division is just before the sokuon or the chon, we eliminate the division because these cannot appear at the beginning of a word.",
        "Also, if the division is just before \"©\"(no), which is a hiragana character and one of the particles used to indicate possession and so on, we insert a division after the \"©\"(no) to make it one word because of the stopping conditions.",
        "Additionally, we combine some segments to form one word when there is a word in a transliteration dictionary of katakana corresponding to an English word.",
        "In this step, we generate abbreviation candidates by applying the following simple rules to all words containing a certain term.",
        "These rules are based on the Japanese abbreviation characteristics described in Section 2.2.",
        "1) Do not use this word",
        "2) Use this word in full",
        "3) Use the first character of this word",
        "Class",
        "Type",
        "NT",
        "(#)",
        "Average number of characters",
        "SC(%)",
        "Av SD",
        "(a) (b) (c) (d) (e) (f)",
        "Character type",
        "Alpha.",
        "928",
        "23.5 9.0 8.8 3.1 6.3 3.8",
        "0.0 0.0 0.0 21.4 0.0 2.1 0.4 8.0 0.3 0.0 0.0 0.2 5.9 0.3 0.2 0.0 0.0 0.0",
        "100.0 79.2 91.0",
        "Kata.",
        "245",
        "Kanji",
        "399",
        "Semantic category",
        "Univ.",
        "523",
        "6.0 1.4 10.5 5.6 10.9 4.2",
        "6.0 0.0 0.0 0.0 0.0 0.0",
        "3.1 3.6 2.1 1.2 0.3 0.3 1.6 4.5 2.6 1.7 0.1 0.4",
        "98.1 28.8 19.1",
        "TV1",
        "326",
        "TV2",
        "603",
        "4) Use the first two characters of this word",
        "5) Use the first three characters of this word",
        "All rules are applied to all words divided by the process in step 1.",
        "For example, in the case of \"Viva/Viva/V6\", all rules are used for \"Viva\", \"Viva\", and \"V6\".",
        "Then, if 3), 3), 2) are used for each base word, we get a candidate \"VVV6\".",
        "With the rules, we can get all candidates combining substrings at the beginning of each word because we used the stop conditions of character length of less than four in step 1.",
        "However, note that we use mora instead of character in the case of phonographic characters.",
        "Also, we eliminate duplicative candidates.",
        "LLM is a probabilistic model widely used as a maximum entropy model for many NLP tasks (Manning and Schütze, 1999).",
        "We use standard LLM to rank the abbreviations.",
        "Consider a set of observations x for each sample of an object or event with y. Log-Linear Model gives a probability p(y\\x; A) of an event by representing an event y as features fj(x^ y^).",
        "Here, Xj(j = 1, ...,M) or ay is a model parameter, and it represents the weight of a feature fj(xhVk)- Also, regularization term Z(x,X) is calculated as follows:",
        "Note that Y(x) represents a set of output y corresponding to x.",
        "The numerator of the Formula (1) is the same as the following by replacing exi as ay.",
        "We formalize the abbreviation generation task as a ranking problem in which the probability p(y\\x; A) of abbreviation y in a given set Y(x) of abbreviation candidates is modeled when its full form x is observed.",
        "For example, assume that you assign a full form \"VivavivaV6\" to x.",
        "The set Y(x) contains abbreviation candidates generated from the full form in Step2 such as \"VVV6\", \"VivaV\", \"ViVi\", and so on.",
        "We used Amis implementation for Log-Linear Model.",
        "1) Features",
        "We use the features below for the Japanese abbreviation characteristics with letter length and so on as mentioned in the Section 2.2.",
        "We denote a substring of a zth base word containing an abbreviation candidate by subi (i = 1, • • •, m), where m is the total number of base words.",
        "Then, let ch(subi) denote letter type of character of subi, and let len(subi) denote length of subi.",
        "Additionally, let sum(len(subi),l,m) denote a summation of len(subi)(i = 1, • • •, m), and let com((fi(i), f2(i)), 1; tn) denote a combination of a feature fi(i) and a feature f2(i) from i = 1 to i = m. Here, we show all categories of features we used as follows:.",
        "A substring of a zth base word is generated by applying one of the rules from 2) to 7) in Step 2.",
        "However, when an abbreviation candidate corresponds to one substring of its full form, we set its base word to the candidate itself even if the candidate was generated by combining some substrings.",
        "Table 2 shows features for \"VVV6\" whose original term is \"VivaVivaV6\".",
        "Its base words subi are \"V\", \"V\", and \"V6\" because of the division as \"V/V/V6\".",
        "When i = 1, ch(subi) is equal to ALPHA, that is, an alphabetical character, and len(subi) is equal to 1.",
        "Therefore, for \"VVV6\", a feature in a category tp is generated by combination of the ch(subi) and len(subi) from i = 1 to i = m, that is, I ALPHA I ALPHA 2ALPIIA.",
        "Other features are also generated by calculating in the same way as tp.",
        "We cannot list all possible features because they depend on compositions of abbreviation candidates.",
        "Therefore, we prepare a zero feature for each category.",
        "If features do not appear in positive examples in a training data set, we assign them to zero features.",
        "For example, because a feature \"1KANJL5KANJI\" in category \"tp\" does not appear in positive examples of a training set, we use",
        "\"tpO\" as an alternative feature.",
        "However, wO is assigned when any features in category \"w\" do not appear in them.",
        "We assign l\\ to a set of all features that appear in positive examples in a training data set, such as 1ALPHAJ ALPHA_2ALPHA, 1_1_2, V, V, V6, ab4.",
        "We also assign Iq to a set of zero features, i.e. tpO, tlO, eO, wO, abO, enumO.",
        "Then, let L denote a set merged l\\ and Iq.",
        "2) Training and Test",
        "First, we obtain the above-mentioned feature set L with a training data set.",
        "Next, these features are assigned to all abbreviation candidates generated from the training data set in step 2.",
        "Then, a parameter a,j (j = 1, • • •, \\L\\) of the Log-Linear Model is calculated by using Amis.",
        "Finally, the probabilities of all abbreviation candidates generated from a test data in step 2 are calculated by the Formula (2)."
      ]
    },
    {
      "heading": "4. Evaluation",
      "text": [
        "CRFs (Lafferty et al., 2001) are Log-Linear Models, which are often used for the labeling or parsing of sequential data and are widely applied for many NLP tasks.",
        "Some researchers already used CRFs for abbreviation extraction (Okazaki et al., 2008(1)) or generation (Saikou et al., 2008).",
        "Therefore, we evaluate a method using CRFs as a baseline.",
        "We formalize the abbreviation generation task as a sequence labeling problem in which each letter contained in an original term is to be used in its abbreviation (Fig.",
        "1).",
        "We also designed features attached to each character: morpheme word containing the letter, reading of the morpheme word,",
        "Figure 1: Feature examples of CRFs and values for the abbreviation \"^k:^ (asabita)\" whose formal form is \"^^k^^y (asa wa bitamin)\".",
        "type of character, the first character or not in the morpheme word, the first character or not in the segment, and so on.",
        "We used MeCab as a morphological analysis and CRF++ implementation for CRFs.",
        "We evaluate recall in the top 1,5, 10, 30, and 50 abbreviation candidates generated with both the proposed method and the baseline method on the six data sets.",
        "The performance is measured under a tenfold cross-validation where the parameters are fine-tuned in the top 30 in the training procedure.",
        "Table 3 shows recall with the baseline method.",
        "Table 4 shows recall, and the bottom row in the table shows differences between recall with CRFs and that with proposed method in the top 30.",
        "In the top 30, recall in Table 3 of alphabetical words, names of universities, and kanji words are 99.1%, 97.9%, and 92.5% respectively.",
        "From the point of view of types of character, most of these are composed of a single type of character as shown in column SC of Table 1.",
        "In contrast, recall in Table 3 of TV program titles 1 and 2 are 68.3% and 80.9% respectively.",
        "These results are much lower than the others.",
        "As a result of applying our method, Table 4 showed that recall of TV program titles improved 10.5% compared with the baseline method.",
        "This is because the method using CRFs cannot use the features of generated abbreviations since it is an approach to decide whether each character of an original form is to be used in its abbreviation.",
        "It seems that this leads to the disadvantages of generating abbreviations of TV program titles containing various types of character and colloquial expressions.",
        "However, there",
        "Category",
        "Feature",
        "tp",
        ".1.",
        "AXjPHLAi.__1.",
        "AXjPHLAi._2 AXjPHLAi.",
        "tl",
        "e",
        "1_1_2",
        "w",
        "V, V, V6",
        "ab",
        "ab4",
        "enum",
        "enum3",
        "Label",
        "Features",
        "word",
        "reading",
        "POS",
        "Head of word",
        "SB",
        "0",
        "SB",
        "Noun",
        "1",
        "1*",
        "X",
        "1*",
        "Particle",
        "1",
        "tf",
        "0",
        "tfizfi-k,",
        "Noun",
        "1",
        "0",
        "tfizfi-k,",
        "Noun",
        "0",
        "s",
        "X",
        "tfizfi-k,",
        "Noun",
        "0",
        ">",
        "X",
        "Noun",
        "0",
        "Table 4: Recall in the top 1, 5, 10, 30, and 50 abbreviation candidates generated with the proposed method, and differences between the recall with CRFs and that with the proposed method in the top 30.",
        "is little difference of recall between the baseline and the proposed method for the TV program titles 2.",
        "This is because most of the TV program titles 2 were systematically created by simple rules such as getting the initial several letters that satisfy space limitations.",
        "On the other hand, recall of the proposed method for alphabetical words, kanji words, and names of universities was – 5.1%, – 7.3%, ^5.4% lower, respectively, than in the case of using the baseline method.",
        "This is because some abbreviations could not be generated by the given generation rules and, as can be seen in Table 4, recall of these data sets peaks.",
        "From these results, we conclude that the baseline method is suited to a term containing a single type of character such as alphabetical words and kanji words, whereas the proposed method is suited to a term containing multiple types of character.",
        "When we used the division in step 2 as an alternative to MeCab, recall with CRFs differed approximately less than ±1% from recall in Table 3.",
        "On the other hand, when we used MeCab as an alternative to the division in step 2, recall with the proposed method was significantly lower than in Table 4.",
        "We cannot compare our performance directly with the previous work because of the differences in data sets.",
        "For reference, Murayama et al.",
        "(2006) reported 68.4% recall in the top 30 with the Noisy-Channel Model.",
        "They used 851 abbreviations corresponding to 748 full forms extracted from Wikipedia.",
        "Saikou et al.",
        "(2008) reported 72.5% recall in the top 30 with CRFs.",
        "They used 51 abbreviations collected by WoZ as test data and 781 abbreviations that appeared in Wikipedia as training data.",
        "Table 4 shows that the baseline method is better for the alphabetical words, names of universities, and kanji words, whereas the proposed method is better for others.",
        "However the classification on Table 1 is made by hand.",
        "Here, we automatically classified them into the following case A and B based on the conditions according to types of character after merging the six data sets in Table 1.",
        "Then, we applied the method with CRFs to the case A and the proposed method to the case B.",
        "Case A is when an original term is (1) an alphabetical term with more than two words, (2) a kanji term in which other characters do not constitute, or (3) a term of (1) or (2) with numerals or symbols.",
        "Case B is when an original term does not fulfill the conditions of the case A.",
        "Recall @n",
        "Alphabet",
        "Katakana",
        "Kanji",
        "Univ.",
        "TV1",
        "TV2",
        "1",
        "89.1%",
        "29.4%",
        "47.9%",
        "19.9%",
        "11.1%",
        "9.3%",
        "5",
        "97.0%",
        "67.3%",
        "71.7%",
        "80.9%",
        "37.5%",
        "45.8%",
        "10",
        "98.4%",
        "77.1%",
        "81.5%",
        "92.9%",
        "48.6%",
        "62.5%",
        "30",
        "99.1%",
        "89.0%",
        "92.5%",
        "97.9%",
        "68.3%",
        "80.9%",
        "50",
        "99.4%",
        "93.9%",
        "94.7%",
        "98.9%",
        "73.8%",
        "86.9%",
        "Recall @n",
        "Alphabet",
        "Katakana",
        "Kanji",
        "Univ.",
        "TV1",
        "TV2",
        "1",
        "87.4%",
        "36.3%",
        "39.1%",
        "33.5%",
        "19.9%",
        "20.2%",
        "5",
        "92.2%",
        "66.5%",
        "65.2%",
        "71.5%",
        "48.2%",
        "42.8%",
        "10",
        "93.0%",
        "81.6%",
        "73.9%",
        "84.9%",
        "61.3%",
        "59.2%",
        "30",
        "94.1%",
        "91.0%",
        "85.2%",
        "92.5%",
        "78.8%",
        "81.1%",
        "50",
        "94.4%",
        "92.7%",
        "86.7%",
        "93.3%",
        "85.3%",
        "85.4%",
        "all",
        "95.6%",
        "94.7%",
        "90.4%",
        "94.8%",
        "93.9%",
        "90.3%",
        "Differences(Recall@30)",
        "-5.1%",
        "+2.0%",
        "^7.3%",
        "^5.4%",
        "+10.5%",
        "+0.2%",
        "ble 5 shows the number of abbreviations in each case for each data set.",
        "The total performance was measured by calculating weighted average for two recall scores, that is, in the case of A and B measured under a tenfold cross-validation in the top 30.",
        "As a result, recall was 97.1% and 76.9% in the case A and B respectively, and the total recall was 89.4%.",
        "Additionally, we conducted an experiment in which the method with CRFs was applied to all the abbreviations as a baseline.",
        "The recall was 87.0% measured under a tenfold cross-validation in the top 30.",
        "The results show that it is better to apply different methods according to types of character than to apply one method to the entire data set."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, we proposed a method for generating Japanese abbreviations from their full forms with LLM.",
        "As a result of experiments, the proposed method was confirmed to be effective for TV program titles.",
        "It achieved 78.8% recall in the top 30, and improved 10.5% from a baseline method using CRFs that achieved 68.3% recall.",
        "We also described difficulties in generating Japanese abbreviations by examining six data sets classified according to types of character and semantic categories.",
        "Consequently, we showed that the baseline method is suited to a term containing a single type of character such as alphabetical words and kanji words, whereas the proposed method is suited to a term containing multiple types of character.",
        "In the future, we will apply the proposed method to Japanese abbreviations generated with transliteration between English and Japanese.",
        "We also plan to narrow down the top ranked abbreviation candidates by using the search results on the Web.",
        "Case A",
        "%",
        "CaseB",
        "%",
        "Total",
        "Alpha",
        "917",
        "(98.8)",
        "11",
        "(1.2)",
        "928",
        "Kata.",
        "0",
        "(0)",
        "245",
        "(100)",
        "245",
        "Kanji",
        "363",
        "(91.0)",
        "36",
        "(9.0)",
        "399",
        "Univ.",
        "513",
        "(98.1)",
        "10",
        "(1.9)",
        "523",
        "TV1",
        "27",
        "(8.3)",
        "299",
        "(91.7)",
        "326",
        "TV2",
        "49",
        "(8.1)",
        "554",
        "(91.9)",
        "603",
        "Total",
        "1869",
        "(61.8)",
        "1155",
        "(38.2)",
        "3024",
        "(Ree @ 30)",
        "CRF",
        "-",
        "-",
        "87.0%",
        "CRF/LLM",
        "97.1%",
        "76.9%",
        "89.4%",
        "Japan Electronics and Information Technology Industries",
        "ssJEITA",
        "Nippon Telephone and Telegraph corporation",
        "NTT",
        "reedom f obile multimedia Access",
        "FOMA",
        "< Katakana Words >",
        "Abbr.",
        "^-h7f7?h7>Xï7y3> (Ootomachikku toransumissh;f – h-?",
        "(0otoma) (automatic transmission)",
        "X – /\"v – =i>L°zl – -$! – (Suupaa conpyuutaa) (super computer)",
        "X/\\=i>(Supa con)",
        "ï 'Jtiyyy h'H – Jl' (American futto booru) (American football)",
        "T*7h (Ame futo)",
        "< Kan jiW ords >",
        "Abbr.",
        "&Mi&&<Dm±RUtelEm\\<Dmm~mt&}m &i£;£(Dokkin hou)",
        "(Shiteki dokusen no kinshi oyobi kousei torihiki no kakuho ni kansuru houritsu) (Act on Prohibition of Private Monopolization and Maintenance of Fair Trade)",
        "^B^KiEflfc^lljSD'S (Zen nihon minshu iryou kikan (Japan Federation of Democratic Medical Institutions)",
        "rengKEîKMiniren)",
        "^^A^MI&^Je!",
        "( Daigaku nyuugaku sikaku kentei) (the University Entrance Qualification Examination)",
        ";W£(Dai ken)",
        "< N am e ofU n ivers ity >",
        "Abbr.",
        "B*mnX¥ (Nihon ika daigaku) (Nippon Medical School)",
        "BE*(Nichiidai), BE(Nichii)",
        "T?TJS f4 ^ (Nagoya shouka daigaku) (Nagoya University of Commerce & Business)",
        "^^^(Meishoudai), £jgi(Meishou)",
        "^(Z^K^c-F-;*;^ (Ochanomizu jyoshi daigaku) (Ochanomizu University)",
        "fc^Ä(Ochajyo), fc^^(Ochadai)",
        "< N am e ofTV prog.",
        "1 >",
        "Abbr.",
        "$fllil£-$!5> (Asa wa bitamin)",
        "Ifltf^tAsa bita)",
        "(Downtown no gaki no tsukai ya arahen de) (Downtown's \"I'm Not An Errand Boy!\")",
        "#+fê(Gaki tsuka), tf+fê^Waki tsu #+(Gaki), tf+ôJfê^Waki no tsuka",
        "jjfiMii ) (Suiyou d: iou) (How do you like Wednesday?)",
        "if5\"T?Lj;5(Doudeshou), 7Kif5(Sui d",
        "< N am e ofTV prog.",
        "2 >",
        "fcL^hlg (Oshare koubou) (A nifty craft center)",
        "fcL^^(Oshare)",
        "M^jftroft (Te io manzai torano ^rUiiStZ/O^ – tTere asobi pafoo)",
        "ZfrR h-b° ;£^f5 (San-kagetsu topikku eikaiwa) h-tfjiitTopi ei) (An English conversation program focusing on one theme per every three months)"
      ]
    }
  ]
}
