{
  "info": {
    "authors": [
      "Christian Chiarcos",
      "Julia Ritz",
      "Manfred Stede"
    ],
    "book": "Proceedings of the Third Linguistic Annotation Workshop (LAW III)",
    "id": "acl-W09-3005",
    "title": "By all these lovely tokens... Merging Conflicting Tokenizations",
    "url": "https://aclweb.org/anthology/W09-3005",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "By all these lovely tokens... *Merging Conflicting Tokenizations",
        "Christian Chiarcos, Julia Ritz and Manfred Stede",
        "Sonderforschungsbereich 632 \"Information Structure\"",
        "24-25, 14476 Golm, Germany {chiarcos j juliaj stedej@ling.uni-potsdam.de",
        "Given the contemporary trend to modular NLP architectures and multiple annotation frameworks, the existence of concurrent tokenizations of the same text represents a pervasive problem in everyday's NLP practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general.",
        "This paper describes a solution for integrating different tokenizations using a standoff XML format, and discusses the consequences for the handling of queries on annotated corpora."
      ]
    },
    {
      "heading": "1. Motivation",
      "text": [
        "For most NLP tasks and linguistic annotations, especially those concerned with syntax (part-of-speech tagging, chunking, parsing) and the interpretation of syntactic structures (esp., the extraction of semantic information), tokens represent the minimal unit of analysis: words (lexemes, semantic units, partly morphemes) on the one hand and certain punctuation symbols on the other hand.",
        "From a corpus-linguistic perspective, tokens also represent the minimal unit of investigation, the minimal character sequence that can be addressed in a corpus query (e.g. using search tools like TIGERSearch (Konig and Lezius, 2000) or CWB (Christ, 1994)).",
        "Tokens also constitute the basis for 'word' distance measurements.",
        "In many annotation tools and their corresponding formats, the order of tokens provides a timeline for the sequential order of structural elements (MMAX (Muller and Strube, 2006), GENAU (Rehm et al., 2009), GrAF (Ide and Suderman, 2007), TIGER XML (Konig and Lezius, 2000)).",
        "In several multi-Taken from the poem September by Helen Hunt Jackson.",
        "layer formats, tokens also define the absolute position of annotation elements, and only by reference to a common token layer, annotations from different layers can be related with each other (NITE (Carletta et al., 2003), GENAU).",
        "Thus, by their function, tokens have the following characteristics: (i) tokens are totally ordered, (ii) tokens cover the full (annotated portion of the) primary data, (iii) tokens are the smallest unit of annotation, and (iv) there is only one single privileged token layer.",
        "The last aspect is especially relevant for the study of richly annotated data, as an integration and serialization of annotations produced by different tools can be established only by reference to the token layer.",
        "From a corpus-linguistic perspective, i.e., when focusing on querying of annotated corpora, tokens need to be well-defined and all information annotated to a particular text is to be preserved without any corruption.",
        "We argue that for this purpose, characteristic (iii) is to be abandoned, and we will describe the data format and an algorithm for merging different tokenizations and their respective annotations.",
        "Our goal is a fully automated merging of annotations that refer to different tokenizations (henceforth T1 and T2) of the same text.",
        "We regard the following criteria as crucial for this task: Information preservation.",
        "All annotations applied to the original tokenizations should be preserved.",
        "Theoretically well-defined notion of token.",
        "It should be possible to give a plausible list of positive criteria that define character sequences as tokens.",
        "Knowledge about the token definition is essential for formulating queries for words, e.g. in a corpus search interface.",
        "Integrative representation.",
        "All annotations that are consistent with the merged tokenization should refer to the merged tokenization.",
        "This is necessary in order to query across multiple annotations originating from different annotation layers or tools.",
        "Unsupervised merging.",
        "The integration of conflicting tokenizations should not require manual interference.",
        "Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997).",
        "However, different research questions or applications induce different conceptions of the term 'word'.",
        "For a shallow morphosyntactic analysis (part of speech tagging), a 'simple' tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1).",
        "A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each.",
        "(1) a. department store b. Herzog-von der Heide c. Red Cross/Red Crescent movement",
        "Similarly, examples (2a) and (2b) can be argued to be treated as one token for (mor-pho)syntactic analyses, respectively.",
        "Despite intervening whitespaces and punctuation symbols, they are complex instances of the 'classical' part-of-speech adjective.",
        "For certain semantic analyses such as in information extraction, however, it may be useful to split these compounds in order to access the inherent complements (E 605, No.",
        "22).",
        "Finally, (3) illustrates a morphology-based tok-enization strategy: the principle of splitting at morpheme boundaries (Marcus et al., 1993, PTB) (token boundaries represented by square brackets).",
        "Morphological tokenization may help distributional (co-occurrence-based) semantics and/or parsing; however, the resulting tokens might be argued as being less intuitive to users of a corpus search tool.",
        "These examples show that different applications (tagging, parsing, information extraction) and the focus on different levels of description (morphology, syntax, semantics) require specialized tok-enization strategies.",
        "When working with multiple tools for standard NLP tasks, thus, it is the norm rather than the exception that they disagree in their tokenization, as shown in ex.",
        "(4).",
        "When creating a corpus that is annotated at multiple levels and/or using several tools, different tok-enizations are not always avoidable, as some tools (automatic NLP tools, but also tools for manual annotation) have integrated tokenizers.",
        "Another challenge is the representation of token boundaries.",
        "Commonly, token boundaries are represented by a line break ('\\n') or the whitespace 'character' (' ') - in which case token-internal whitespaces are replaced, usually by an underscore ('_') -, thereby corrupting the original data.",
        "This practice makes reconciling/merging the data a difficult enterprise.",
        "Given this background, we suggest an XML-based annotation of token boundaries, such that token boundaries are marked without affecting the original primary data.",
        "In a straightforward XML model, tokens are represented by XML elements enclosing primary text slices (c.f.",
        "the BNC encoding scheme (Burnard, 2007)).",
        "However, treating tokens as spans of text by means of the XML hierarchy is impossible for tokenization conflicts as in (4.a) and (4.b)."
      ]
    },
    {
      "heading": "2. Conflicting tokenizations: Straightforward strategies",
      "text": [
        "By 'straightforward strategies', we mean approaches that aim to preserve the definition of tokens as atomic, minimal, unambiguous units of annotation when unifying different tokenizations (henceforth T1 and T2) of the same text.",
        "By 'un-supervised straightforward strategies', we mean tokenization strategies that operate on the primary data only, without consulting external resources such as dictionaries or human expertise.",
        "Unsupervised straightforward strategies to the task include:",
        "1. no merging In a conservative approach, we could create independent annotation projects for every tokenization produced, and thus represent all tokenizations independently.",
        "This, however, rules out any integration or combined evaluation of annotations to T1 and annotations to T2.",
        "2. normalization Adopt one of the source tok-enizations, say T1, as the 'standard' tokenization.",
        "Preserve only the information annotated to T2 that is consistent with T1.",
        "Where tokenization T2 deviates from T!, all annotations to T2 are lost.",
        "3. maximal tokens For every token boundary in T1 that is also found in T2, establish a token boundary in the merged tokenization (cf. Guo's 1997 'critical tokenization').",
        "However, with tokens assumed to be the minimal elements ofanno-tation, we lose linguistic analyses of fine-grained tokens.",
        "With respect to (4.a) and (4.b), the maximal token would be the whole phrase doesn t. Again, this results in a loss of information, as all annotations applied to does, doesn, n t, and t refer to units that are smaller than the resulting token.",
        "4. maximal common substrings For every token boundary in T1 or T2, establish a token boundary, thereby producing minimal tokens: one token for every maximal substring shared between T1 and T2 (cf. Guo's 1997 'shortest tokenization').",
        "By defining the original tokens ('supertokens') as annotations spanning over tokens, all annotations are preserved.",
        "However, the concept of 'token' loses its theoretical motivation; there is no guarantee that maximal common substrings are meaningful elements in any sense: The maximum common substring tokenization of 4.a and 4.b is [does][n]['][t], but [n] is not a well-defined token.",
        "It is neither defined with respect to morphology (like PTB tokens) nor is it motivated from orthography (like TnT tokens), but it is just the remainder of their intersection.",
        "As shown in Table 1, none of the strategies sketched above fulfills all criteria identified in Section 1.1: Avoiding a merging process counteracts data integration; token normalization and maximal tokens violate information preservation, and maximal common substrings violate the requirement to specify a theoretically well-defined notion of token.",
        "As an alternative, we propose a formalism for the lossless integration and representation of con-",
        "information preservation well-defined tokens integrative unsupervised",
        "flicting tokenizations by abandoning the assumption that tokens are an atomic, primitive concept that represents the minimal unit of annotation.",
        "Rather, we introduce annotation elements smaller than the actual token - so-called terminals or terms for short - that are defined according to the maximum common substrings strategy described above.",
        "Then, tokens are deined as nodes that span over a certain range of terms similar to phrase nodes that dominate other nodes in syntax annotations.",
        "The representation of conflicting tokenizations, then, requires a format that is capable to express conflicting hierarchies.",
        "For this purpose, we describe an extension of the PAULA format, a generic format for text-oriented linguistic annotations based on standoff XML."
      ]
    },
    {
      "heading": "3. Conflicting tokenizations in the PAULA format",
      "text": [
        "The PAULA format (Dipper, 2005; Dipper and Gotze, 2005) is a generic XML format, used as a pivot format in NLP pipelines (Stede et al., 2006) and in the web-based corpus interface ANNIS (Chiarcos et al., 2008).",
        "It uses standoff XML representations, and is conceptually closely related to the formats NITE XML (Carletta et al., 2003) and GraF (Ide and Suderman, 2007).",
        "PAULA was speciically designed to support the lossless representation of different types of text-oriented annotations (layer-based/timeline annotations, hierarchical annotations, pointing relations), optimized for the annotation of multiple layers, including conflicting hierarchies and simple addition/deletion routines for annotation layers.",
        "Therefore, primary data is stored in a separate",
        "nodes (structural units of annotation)",
        "token character spans in the primary data that form the basis for higher-level annotation",
        "markable (spans of) token(s) that can be annotated with linguistic information.",
        "Markables represent flat, layer-based annotations defined with respect to the sequence of tokens as a general timeline.",
        "struct hierarchical structures (DAGs or trees) are formed by establishing a dominance relation between a struct (e.g., a phrase) node as parent, and tokens, markables, or other struct nodes as children.",
        "edges (relational units of annotation, connecting tokens, markables, structs)",
        "dominance relation directed edge between a struct and its children",
        "pointing relations directed edge between nodes in general (tokens, markables, structs) labels (annotations: node or edge labels)",
        "features represent annotations attached to a particular (structural or relational) unit of annotation",
        "file.",
        "Multiple annotations are also stored in separate files to avoid interference between concurrent annotations.",
        "Annotations refer to the primary data or to other annotations by means of XLinks and XPointers.",
        "As types oflinguistic annotation, we distinguish nodes (token, markable, struct), edges (dominance and pointing relations) and labels (annotations), as summarized in Table 2.",
        "Each type of annotation is stored in a separate file, so that competing or ambiguous annotations can be represented in an encapsulated way.",
        "PAULA 1.0 is already sufficiently expressive for capturing the data-heterogeneity sketched above, including the representation of overlapping segments, intersecting hierarchies, and alternative annotations (e.g., for ambiguous annotations), but only for annotations above the token level.",
        "Further, PAULA 1.0 relies on the existence of a unique layer of non-overlapping, atomic tokens as minimal units of annotation: For all nodes, their position and sequential order is defined with respect to the absolute position of tokens that they cover; and for the special case of markables, these are defined solely in terms of their token range.",
        "Finally, PAULA 1.0 tokens are totally ordered, they cover the (annotated) primary data completely, and they are non-overlapping.",
        "Only on this basis, the extension and (token-)distance of annotated elements can be addressed; and only by means of unambiguous reference, information from different layers of annotation can be combined and evaluated.",
        "In our extension of the PAULA format, we introduce the new concept of term nodes: atomic terminals that directly point to spans of primary data.",
        "Terms are subject to the same constraints as tokens in PAULA 1.0 (total order, full coverage, non-overlapping).",
        "So, terms can be used in place of PAULA 1.0 tokens to define the extension and position of super-token level and sub-token level annotation elements.",
        "Markables are then defined with respect to (spans of) terminal nodes rather than tokens, such that alternative tokenizations can be expressed as markables in different layers that differ in their extensions.",
        "Although terms adopt several functions formerly associated with tokens, a privileged token layer is still required: In many query languages, including ANNIS-QL (Chiarcos et al., 2008), tokens define the application domain of regular expressions on the primary data.",
        "More importantly, tokens constitute the basis for conventional (\"word\") distance measurements and (\"word\") coverage queries.",
        "Consequently, the constraints on tokens (total order, full coverage and absence of overlap) remain.",
        "The resulting specifications for structural units of annotation are summarized in Table 3.",
        "Distinguishing terminal elements and redefining the token layer as a privileged layer of markables allows us to disentangle the technical concept of 'atomic element' and 'token' as the conventionally assumed minimal unit of linguistic analysis.",
        "In order to integrate annotations on tokens, it is not enough to represent two tokenizations side by side with reference to the same layer of terminal nodes.",
        "Instead, a privileged token layer is to be established and it has to be ensured that annotations can be queried with reference to the token layer.",
        "terms specify character spans in the primary data that form the basis for higher-level annotation markable defined as above, with terms taking the",
        "place oftokens structs defined as above, with terms taking the",
        "place oftokens tokens subclass of structs that are nonoverlapping, arranged in a total order, and cover the full primary data",
        "Then, all annotations whose segmentation is consistent with the privileged token layer are directly linked with tokens.",
        "Alg.",
        "3.1 describes our merging algorithm, and its application to the four main cases of conflicting tokenization is illustrated in Figure 1.",
        "The following section describes its main characteristics and the consequences for querying."
      ]
    },
    {
      "heading": "4. Discussion",
      "text": [
        "Alg.",
        "3.1 produces a PAULA project with one single tokenization.",
        "So, it is possible to define queries spanning across annotations with originally different tokenization:",
        "Extension and precedence queries are tokenization-independent: Markables refer to the term layer, not the tok layer, structs also (indirectly) dominate term nodes.",
        "Dominance queries for struct nodes and tokens yield results whenever the struct node dominates only nodes with tok-compatible source tokenization: Structs dominate tok nodes wherever the original tokenization was consistent with the privileged tokenization tok (case A and C in Fig. 1).",
        "Distance queries are deined with respect to the tok layer, and are applicable to all elements that are are deined with reference to the tok layer (in figure 1: tok1(l, tok2(l, toklb, tok2b in case A; tokabin case B; toka, tokb, tokab in case C; tokab, tokcin case D).",
        "They are not applicable to elements that do not refer to the tok layer (B: toka, tokb; D: toka, tokbc).",
        "The algorithm is unsupervised, and the token concept of the output tokenization is well-defined and consistent (if one of the input tokenizations is adopted as target tokenization).",
        "Also, as shown below, it is integrative (enabling queries across different tokenizations) and information-preserving (reversible).",
        "After a PAULA project has been created, the time complexity of the algorithm is quadratic with respect to the number of characters in the primary data n. This is due to the total order of tokens: Step 2 and 3.a are applied once to all original tokens from left to right.",
        "Step 5 can be reformulated such that for every terminal node, the relationship between the directly dominating to/^ and tok2 is checked.",
        "Then, Step 5 is also in O(n).",
        "In terms of the number of markables m, the time complexity in Step 3.b is in O(n m): for every markable, the corresponding term element is to be found, taking at most n repositioning operations on the term layer.",
        "Assuming that markables within one layer are non-overlapping and that the number of layers is bound by some constant c, then m < n c, so that 3.b is in O(n c).",
        "For realistic scenarios, the algorithm is thus quadratic.",
        "The merging algorithm is reversible - and, thus, lossless - as shown by the splitting algorithm in Alg.",
        "3.2.",
        "For reasons of space, the correctness of this algorithm cannot be demonstrated here, but broadly speaking, it just removes every node that corresponds to an original token of the 'other' tok-enization, plus every node that points to it, so that only annotations remain that are directly applied to the target tokenization.",
        "We focus in this paper on the merging of analyses with different tokenizations for the purpose of users querying a corpus across multiple annota- Alg.",
        "3.1 Merging different tokenizations",
        "0. assume that we have two annotations analysis1 and analysis?",
        "for the same primary data, but with different tokenizations",
        "1. create PAULA 1.0 annotation projects for analysis± and analysis?",
        "with primary data files prim1 and prim?",
        "and token layers tok1 and tok?",
        "respectively."
      ]
    },
    {
      "heading": "2.. harmonize primary data",
      "text": [
        "if prim± equals prim?, then (i) rename prim± to prim (ii) set all references in analysis?",
        "from prim?",
        "to prim (iii) create a new annotation project analysis by copying prim and all annotation layers from analysis± and analysis?",
        "otherwise terminate with error msg"
      ]
    },
    {
      "heading": "3.. harmonize terminal nodes",
      "text": [
        "create a new annotation layer term, then (a) for all overlapping tokens t± e tok± and t?",
        "e tok?",
        ": identify the maximal common substrings of t± and t?",
        "for every substring s, create a new element terms pointing to the corresponding character span in the primary data for every substring s, redefine t± and t?",
        "as markables referring to terms (b) redefine markable spans as spans of terminal nodes for every token t = [termSl ..terms^] e tok± U tok?",
        "and every markable m = [w..xty..z]: set m = [w..xtermSl ..terms^y..z]"
      ]
    },
    {
      "heading": "4.. select token layer",
      "text": [
        "rename tok± to tok, or rename tok?",
        "to tok, (cf. the normalization strategy in Sect.",
        "2) or rename term to tok (cf. the minimal tokens strategy in Sect.",
        "2)"
      ]
    },
    {
      "heading": "5.. token integration",
      "text": [
        "for every original token ot = [a..b] e {tok± U tok?)",
        "\\ tok:",
        "if there is a token t e tok such that t = [a..b], then define ot as a struct with ot = [t], else if there are tokens t1,..,t„ e tok such that t±..tn form a continuous sequence of tokens and t± = [a..x] and tn = [y..b], then define ot as a struct such that ot = [t±,.., tn], otherwise: change nothing",
        "Alg.",
        "3.2 Splitting a PAULA annotation project with two different tokenizations_",
        "0. given a PAULA annotation project analysis with token layer tok, terminal layer term, and two layers l± and l?",
        "(that may be identical to term or tok) that convey the information of the original token layers tok!",
        "and tok?"
      ]
    },
    {
      "heading": "1.. create analysis ± and analysis? as copies of analysis",
      "text": [
        "2. if l± represents a totally ordered, non-overlapping list of nodes that cover the primary data completely, then modify analysis±:",
        "a. for every node in l±: substitute references to tok!",
        "by references to term!",
        "b. remove l?",
        "from analysis± c. if l± = tok±, remove tok!",
        "from analysis±",
        "d. for every annotation element (node/relation) e in analysis± that directly or indirectly points to another node in analysis± that is no longer present, remove e from analysis±",
        "e. remove every annotation layer from analysis± that does not contain an annotation element",
        "f. for every markable in l±: remove references to term!, define the extension of l± nodes directly in terms of spans of text in prim!"
      ]
    },
    {
      "heading": "3.. perform step 2. for l? and analysis?",
      "text": [
        "tion layers.",
        "Although the merging algorithm produces annotation projects that allow for queries integrating annotations from analyses with different tokenization, the structure of the annotations is altered, such that the behaviour of merged and un-merged PAULA projects may be different.",
        "Obviously, token-level queries must refer to the privileged tokenization T!.",
        "Operators querying for the relative precedence or extension ofmarkables are not affected: in the merged annotation project, markables are defined with reference to the layer term: originally co-extensional elements E!",
        "and E2 (i.e. elements covering the same tokens in the source tokenization) will also cover the same terminals in the merged project.",
        "Distance operators (e.g. querying for two tokens with distance 2, i.e. with two tokens in between), however, will operate on the new privileged tokenization, such that results from queries on analysis may differ from those on analysis2.",
        "Dominance operators are also affected, as nodes that directly dominated a token in analysis!",
        "or analysis2 now indirectly dominate it in analysis, with a supertoken as an intermediate node.",
        "Alg.",
        "3.3 Iterative merging: modifications of Alg.",
        "3.1, step.3",
        "if analysis± has a layer of terminal nodes term!, then let T± = term,!, otherwise T± = tok!",
        "if analysis?",
        "has a layer of terminal nodes term?, then let T?",
        "= term?, otherwise T?",
        "= tok?",
        "create a new annotation layer term, then",
        "1. for all overlapping terminals/tokens t!",
        "e T± and t?",
        "e T?",
        ": identify the maximal common substrings of t!",
        "and for every substring s, create a new element termspointing to the corresponding character span in the primary data for every substring s, redefine t!",
        "and t?",
        "as markables referring to terms",
        "2. redefine markable spans as spans of terminal nodes for every node t = [termSl ..terms^] e T± U T?",
        "and every markable m = [w..xty..z]: set m = [w..xtermSl ..terms^y..z]",
        "3. for all original terminals t e T±UT?",
        ": if t is not directly pointed at, remove t from analysis",
        "Accordingly, queries applicable to PAULA projects before the merging are not directly applicable to merged PAULA projects.",
        "Users are to be instructed to keep this in mind and to be aware of the specifications for the merged tokenization and its derivation."
      ]
    },
    {
      "heading": "5. Extensions",
      "text": [
        "In the current formulation, Alg.",
        "3.1 is applied to two PAULA 1.0 projects and generates extended PAULA annotation projects with a term layer.",
        "The algorithm, however, may be applied iteratively, if step 3 is slightly revised, such that extended PAULA annotation projects can also be merged, see Alg.",
        "3.3.",
        "The merging algorithm creates a struct node for every original token.",
        "Although this guarantees reversibility, one may consider to remove such redundant structs.",
        "Alg.",
        "3.4 proposes an optional postprocessing step for the merging algorithm.",
        "This step is optional because these operations are Alg.",
        "3.4 Annotation integration: Optional postprocessing for merging algorithm",
        "6.a.",
        "remove single-token supertoken for every original token ot = [t] e tok1 U tok?",
        "with t e tok: replace all references in analysis toot by references to t, remove ot",
        "6.b.",
        "merging original token layers tok1 and tok?",
        "(if tok1 = tok and tok?",
        "= tok) define new 'super token' layer stok.",
        "for every ot e tok1 U tok?",
        ":",
        "if ot = [t] for some t e tok, then see 6.a if ot = [t1,..,t„] for some t1,..,t„ e tok, and there is ot?",
        "= [t1, ..,tn] e tok1 U tok?",
        "U stok, then replace all references in analysis to ot?",
        "by references to ot, move ot to layer stok, remove ot?",
        "from analysis move all remaining ot e tok1 U tok?",
        "to stok, remove layers tok1 and tok?",
        "6.c.",
        "unify higher-level annotations for every markable mark1 = [term1..term„] and if there is a markable mark?",
        "in analysis such that mark?",
        "= [term1..term„], then replace all references in analysis to mark?",
        "by references to mark1, remove mark?",
        "for every struct struct^ = [c1, ..,cn] that covers exactly the same children as another struct struct?",
        "= [c1,..,cn], replace all references to struct?",
        "by references to struct±, remove struct?",
        "destructive: We lose the information about the origin (analysis!",
        "vs. analysis2) of stok elements and their annotations."
      ]
    },
    {
      "heading": "6. Summary and Related Reasearch",
      "text": [
        "In this paper, we describe a novel approach for the integration of conflicting tokenizations, based on the differentiation between a privileged layer of tokens and a layer of atomic terminals in a standoff XML format: Tokens are defined as structured units that dominate one or more terminal nodes.",
        "Terminals are atomic units only within the respective annotation project (there is no unit addressed that is smaller than a terminal).",
        "By iterative applications of the merging algorithm, however, complex terms may be split up in smaller units, so that they are not atomic in an absolute sense.",
        "Alternatively, terms could be identified a priori with the minimal addressable unit available, i.e., characters (as in the formalization of tokens as charspans and charseqs in the ACE information extraction annotations, Henderson 2000).",
        "It is not clear, however, how a character-based term definition would deal with sub-character and zero extension terms: A character-based definition of terms that represent traces is possible only by corrupting the primary data.",
        "Consequently, a character-based term definition is insufficient unless we restrict ourselves to a particular class of languages, texts and phenomena.",
        "The role of terminals can thus be compared to timestamps: With reference to a numerical timeline, it is always possible to define a new event between two existing timestamps.",
        "Formats specifically designed for time-aligned annotations, e.g.,",
        "EXMARaLDA (Schmidt, 2004), however, typically lack a privileged token layer and a formal concept of tokens.",
        "Instead, tokens, as well as longer or shorter sequences, are represented as markables, defined by their extension on the timeline.",
        "Similarly, GrAF (Ide and Suderman, 2007), although being historically related to PAULA, does not have a formal concept of a privileged token layer in the sense of PAULA.",
        "We do, however, assume that terminal nodes in GrAF can be compared to PAULA 1.0 tokens.",
        "For conflicting tokenizations, Ide and Suderman (2007) suggest that 'dummy' elements are defined covering all necessary tokenizations for controversially tokenized stretches of primary data.",
        "Such dummy elements combine the possible tokeniza-tions for strategies 1 (no merging) and 3 (maximal tokens), so that the information preservation deficit of strategy 3 is compensated by strategy 1, and the integrativity deficit of strategy 1 is compensated by strategy 3 (cf. Table 1).",
        "However, tokens, if defined in this way, are overlapping and thus only partially ordered, so that distance operators are no longer applicable.",
        "Another problem that arises from the introduction of dummy nodes is their theoretical status, as it is not clear how dummy nodes can be distinguished from annotation structured on a conceptual level.",
        "In the PAULA formalization, dummy nodes are not necessary, so that this ambiguity is already resolved in the representation."
      ]
    }
  ]
}
