{
  "info": {
    "authors": [
      "Sittichai Jiampojamarn",
      "Aditya Bhargava",
      "Qing Dou",
      "Kenneth Dwyer",
      "Grzegorz Kondrak"
    ],
    "book": "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009)",
    "id": "acl-W09-3504",
    "title": "DirecTL: a Language Independent Approach to Transliteration",
    "url": "https://aclweb.org/anthology/W09-3504",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "DirecTL: a Language-Independent Approach to Transliteration",
        "Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Kenneth Dwyer, Grzegorz Kondrak",
        "We present DirecTL: an online discriminative sequence prediction model that employs a many-to-many alignment between target and source.",
        "Our system incorporates input segmentation, target character prediction, and sequence modeling in a unified dynamic programming framework.",
        "Experimental results suggest that DirecTL is able to independently discover many of the language-specific regularities in the training data."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "in the transliteration task, it seems intuitively important to take into consideration the specifics of the languages in question.",
        "Of particular importance is the relative character length of the source and target names, which vary widely depending on whether languages employ alphabetic, syllabic, or ideographic scripts.",
        "On the other hand, faced with the reality of thousands of potential language pairs that involve transliteration, the idea of a language-independent approach is highly attractive.",
        "In this paper, we present DirecTL: a transliteration system that, in principle, can be applied to any language pair.",
        "DirecTL treats the transliteration task as a sequence prediction problem: given an input sequence of characters in the source language, it produces the most likely sequence of characters in the target language.",
        "In Section 2, we discuss the alignment of character substrings in the source and target languages.",
        "Our transcription model, described in Section 3, is based on an online discriminative training algorithm that makes it possible to efficiently learn the weights of a large number of features.",
        "In Section 4, we provide details of alternative approaches that incorporate language-specific information.",
        "Finally, in Section 5 and 6, we compare the experimental results of DirecTL with its variants that incorporate language-specific pre-processing, phonetic alignment, and manual data correction."
      ]
    },
    {
      "heading": "2. Transliteration alignment",
      "text": [
        "in the transliteration task, training data consist of word pairs that map source language words to words in the target language.",
        "The matching between character substrings in the source word and target word is not explicitly provided.",
        "These hidden relationships are generally known as alignments.",
        "In this section, we describe an EM-based many-to-many alignment algorithm employed by DirecTL.",
        "In Section 4, we discuss an alternative phonetic alignment method.",
        "We apply an unsupervised many-to-many alignment algorithm (Jiampojamarn et al., 2007) to the transliteration task.",
        "The algorithm follows the expectation maximization (EM) paradigm.",
        "In the expectation step shown in Algorithm 1, partial counts y of the possible substring alignments are collected from each word pair (x , yv) in the training data; T and V represent the lengths of words x and y, respectively.",
        "The forward probability a is estimated by summing the probabilities of all possible sequences of substring pairings from left to right.",
        "The Forward-m2m procedure is similar to lines 5 through 12 of Algorithm 1, except that it uses Equation 1 on line 8, Equation 2 on line 12, and initializes a0;0 := 1.",
        "Likewise, the backward probability f3 is estimated by summing the probabilities from right to left.",
        "The maxX and maxY variables specify the maximum length of substrings that are permitted when creating alignments.",
        "Also, for flexibility, we allow a substring in the source word to be aligned with a \"null\" letter (e) in the target word.",
        "Algorithm 1: Expectation-M2M alignment",
        "Input: xT ,yV, maxX, maxY, 7 Output: 7"
      ]
    },
    {
      "heading": "4. return",
      "text": [
        "In the maximization step, we normalize the partial counts y to the alignment probability 5 using the conditional probability distribution.",
        "The EM steps are repeated until the alignment probability 5 converges.",
        "Finally, the most likely alignment for each word pair in the training data is computed with the standard Viterbi algorithm."
      ]
    },
    {
      "heading": "3. Discriminative training",
      "text": [
        "We adapt the online discriminative training framework described in (Jiampojamarn et al., 2008) to the transliteration task.",
        "Once the training data has been aligned, we can hypothesize that the ith letter substring xi £ x in a source language word is transliterated into the ith substring yi £ y in the target language word.",
        "Each word pair is represented as a feature vector $(x, y).",
        "Our feature vector consists of (1) n-gram context features, (2) HMM-like transition features, and (3) linear-chain features.",
        "The n-gram context features relate the letter evidence that surrounds each letter xi to its output yi.",
        "We include all n-grams that fit within a context window of size c. The c value is determined using a development set.",
        "The HMM-like transition features express the cohesion of the output y in the target language.",
        "We make a first order Markov assumption, so that these features are bi-grams of the form The linear-chain features are identical to the context features, except that yi is replaced with a bi-gram (yi_1,yi).",
        "Algorithm 2 trains a linear model in this feature space.",
        "The procedure makes k passes over the aligned training data.",
        "During each iteration, the model produces the n most likely output words Yj in the target language for each input word xj in the source language, based on the current paAlgorithm 2: Online discriminative training",
        "Input: Data {(xi, yi), (x2, Y2),... , (xm, ym)}, number of iterations k, size of n-best list n Output: Learned weights ip"
      ]
    },
    {
      "heading": "2. for k iterations do",
      "text": []
    },
    {
      "heading": "5. update p according to Yj and y j",
      "text": []
    },
    {
      "heading": "6. return p",
      "text": [
        "rameters tp.",
        "The values of k and n are determined using a development set.",
        "The model parameters are updated according to the correct output yj and the predicted n-best outputs Yj, to make the model prefer the correct output over the incorrect ones.",
        "Specifically, the feature weight vector p is updated by using MIRA, the Margin Infused Relaxed Algorithm (Crammer and Singer, 2003).",
        "MIRA modifies the current weight vector tpo by finding the smallest changes such that the new weight vector tpn separates the correct and incorrect outputs by a margin of at least l(y, y), the loss for a wrong prediction.",
        "We define this loss to be 0 if y = y; otherwise it is 1 + d, where d is the Levenshtein distance between y and y.",
        "The update operation is stated as a quadratic programming problem in Equation 3.",
        "We utilize a function from the SVM1ight package (Joachims, 1999) to solve this optimization problem.",
        "miivn || pn - po II",
        "The arg max operation is performed by an exact search algorithm based on a phrasal decoder (Zens and Ney, 2004).",
        "This decoder simultaneously finds the l most likely substrings of letters x that generate the most probable output y, given the feature weight vector p and the input word xT.",
        "The search algorithm is based on the following dynamic programming recurrence:",
        "To find the n-best predicted outputs, the table Q records the top n scores for each output substring that has the suffix p substring and is generated by the input letter substring xi; here, p is a sub-output generated during the previous step.",
        "The notation 0(x^+1,p',p) is a convenient way to describe the components of our feature vector $(x, y).",
        "The n-best predicted outputs Y can be discovered by backtracking from the end of the table, which is denoted by Q(T + 1, $)."
      ]
    },
    {
      "heading": "4. Beyond D IREC TL",
      "text": [
        "We experimented with converting the original Chinese characters to Pinyin as an intermediate representation.",
        "Pinyin is the most commonly known Romanization system for Standard Mandarin.",
        "Its alphabet contains the same 26 letters as English.",
        "Each Chinese character can be transcribed phonetically into Pinyin.",
        "Many resources for Pinyin conversion are available online.",
        "A small percentage of Chinese characters have multiple pronunciations represented by different Pinyin representations.",
        "For those characters (about 30 characters in the transliteration data), we manually selected the pronunciations that are normally used for names.",
        "This preprocessing step significantly reduces the size of target symbols from 370 distinct Chinese characters to 26 Pinyin symbols which enables our system to produce better alignments.",
        "In order to verify whether the addition of language-specific knowledge can improve the overall accuracy, we also designed intermediate representations for Russian and Japanese.",
        "We focused on symbols that modify the neighboring characters without producing phonetic output themselves: the two yer characters in Russian, and the long vowel and sokuon signs in Japanese.",
        "Those were combined with the neighboring characters, creating new \"super-characters.\"",
        "ALINE (Kondrak, 2000) is an algorithm that performs phonetically-informed alignment of two strings of phonemes.",
        "Since our task requires the alignment of characters representing different writing scripts, we need to first replace every character with a phoneme that is the most likely to be produced by that character.",
        "We applied slightly different methods to the test languages.",
        "In converting the Cyrillic script into phonemes, we take advantage of the fact that the Russian orthography is largely phonemic, which makes it a relatively straightforward task.",
        "In Japanese, we replace each Katakana character with one or two phonemes using standard transcription tables.",
        "For the Latin script, we simply treat every letter as an IPA symbol (International Phonetic Association, 1999).",
        "The IPA contains a subset of 26 letter symbols that tend to correspond to the usual phonetic value that the letter represents in the Latin script.",
        "The Chinese characters are first converted to Pinyin, which is then handled in the same way as the Latin script.",
        "Similar solutions could be engineered for other scripts.",
        "We observed that the transcriptions do not need to be very precise in order for ALINE to produce high quality alignments.",
        "The combination of predictions produced by systems based on different principles may lead to improved prediction accuracy.",
        "We adopt the following combination algorithm.",
        "First, we rank the individual systems according to their top-1 accuracy on the development set.",
        "To obtain the top-1 prediction for each input word, we use simple voting, with ties broken according to the ranking of the systems.",
        "We generalize this approach to handle n-best lists by first ordering the candidate transliterations according to the highest rank assigned by any of the systems, and then similarly breaking ties by voting and system ranking."
      ]
    },
    {
      "heading": "5. Evaluation",
      "text": [
        "In the context of the NEWS 2009 Machine",
        "Transliteration Shared Task (Li et al., 2009), we tested our system on six data sets: from English to",
        "Russian (EnRu) (Kumaran and Kellner, 2007), Japanese Katakana (EnJa), and Korean Hangul (EnKo); and from Japanese Name to Japanese Kanji (JnJk).",
        "We optimized the models' parameters by training on the training portion of the provided data and measuring performance on the development portion.",
        "For the final testing, we trained the models on all the available labeled data (training plus development data).",
        "For each data set, we converted any uppercase letters to lowercase.",
        "Our system outputs the top 10 candidate answers for each input word.",
        "Table 1 reports the performance of our system on the development and final test sets, measured in terms of top-1 word accuracy (ACC).",
        "For certain language pairs, we tested variants of the base",
        "Table 1: Top-1 word accuracy on the development and test sets.",
        "The asterisk denotes the results obtained after the test reference sets were released.",
        "system described in Section 4.",
        "DirecTL refers to our language-independent model, which uses many-to-many alignments.",
        "The Int abbreviation denotes the models operating on the language-specific intermediate representations described in Section 4.1.",
        "The alignment algorithm (Aline or m2m) is given in brackets.",
        "In the EnHi set, many names consisted of multiple words: we assumed a one-to-one correspondence between consecutive English words and consecutive Hindi words.",
        "In Table 1, the results in the first row (DirecTL) were obtained with an automatic cleanup script that replaced hyphens with spaces, deleted the remaining punctuation and numerical symbols, and removed 43 transliteration pairs with a disagreement between the number of source and target words.",
        "The results in the second row (DirecTL+mc) were obtained when the cases with a disagreement were individually examined and corrected by a Hindi speaker.",
        "We did not incorporate any external resources into the models presented in Table 1.",
        "In order to emphasize the performance of our language-independent approach, we consistently used the DirecTL model for generating our \"standard\" runs on all six language pairs, regardless ofits relative performance on the development sets."
      ]
    },
    {
      "heading": "6. Discussion",
      "text": [
        "DirecTL, our language-independent approach to transliteration achieves excellent results, especially on the EnCh, EnRu, and EnHi data sets, which represent a wide range of language pairs and writing scripts.",
        "Both the many-to-many and phonetic alignment algorithms produce highquality alignments.",
        "The former can be applied directly to the training data without the need for an intermediate representation, while the latter does not require any training.",
        "Surprisingly, incorporation of language-specific intermediate representations does not consistently improve the performance of our system, which indicates that DirecTL may be able to discover the structures implicit in the training data without additional guidance.",
        "The EnHi results suggest that manual cleaning of noisy data can yield noticeable gains in accuracy.",
        "On the other hand, a simple method of combining predictions from different systems produced clear improvement on the EnCh set, but mixed results on two other sets.",
        "More research on this issue is warranted."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research was supported by the Alberta Ingenuity, Informatics Circle of Research Excellence (iCORE), and Natural Sciences of Engineering Research Council of Canada (NSERC).",
        "Task",
        "Model",
        "Dev",
        "Test",
        "EnCh",
        "DlRECTL",
        "72.4",
        "71.7",
        "Int(m2m)",
        "73.9",
        "73.4",
        "Int(Aline)",
        "73.8",
        "73.2",
        "Combined",
        "74.8",
        "74.6",
        "EnHi",
        "DirecTL",
        "41.4",
        "49.8",
        "DlRECTL+mc",
        "42.3",
        "50.9",
        "EnJa",
        "DirecTL",
        "49.9",
        "50.0",
        "Int(m2m)*",
        "49.6",
        "49.2",
        "Int(Aline)",
        "48.3",
        "51.0",
        "Combined*",
        "50.6",
        "50.5",
        "EnKo",
        "DirecTL",
        "36.7",
        "38.7",
        "EnRu",
        "DirecTL",
        "80.2",
        "61.3",
        "Int(m2m)",
        "80.3",
        "60.8",
        "Int(Aline)",
        "80.0",
        "60.7",
        "Combined*",
        "80.3",
        "60.8",
        "JnJk",
        "DirecTL",
        "53.5",
        "56.0"
      ]
    }
  ]
}
