{
  "info": {
    "authors": [
      "Eiji Aramaki",
      "Takeshi Abekawa"
    ],
    "book": "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009)",
    "id": "acl-W09-3513",
    "title": "Fast Decoding and Easy Implementation: Transliteration as Sequential Labeling",
    "url": "https://aclweb.org/anthology/W09-3513",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Fast decoding and Easy Implementation: Transliteration as Sequential Labeling",
        "Eiji ARAMAKI Takeshi ABEKAWWA",
        "The University of Tokyo National Institute of Informatics",
        "Although most of previous transliteration methods are based on a generative model, this paper presents a discriminative transliteration model using conditional random fields.",
        "We regard characters) as a kind of label, which enables us to consider a transliteration process as a sequential labeling process.",
        "This approach has two advantages: (1) fast decoding and (2) easy implementation.",
        "Experimental results yielded competitive performance, demonstrating the feasibility of the proposed approach."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "To date, most transliteration methods have relied on a generative model which resembles a statistical machine translation (SMT) model.",
        "Although the generative approach has appealing feasibility, it usually suffers from parameter settings, length biases and decoding time.",
        "We assume a transliteration process as a kind of sequential labeling that is widely employed for various tasks, such as Named Entity Recognition (NER), part-of-speech (POS) labeling, and so on.",
        "Figure 1 shows a lattice of both the transliteration and POS labeling.",
        "As shown in that figure, both tasks share a similar work frame: (1) an input sequence is decomposed into several segments; then (2) each segments produces a label.",
        "Although the label represents a POS in POS labeling, it represents a character (or a character sequence) in the transliteration task.",
        "The proposed approach entails three risks.",
        "1.",
        "Numerous Label Variation: Although POS requires only 10-20 labels at most, a transliteration process requires numerous labels.",
        "In fact, Japanese katakana requires more than 260 labels in the following experiment (we",
        "Figure 1: (i) Part-of-Speech Lattice and (ii) Transliteration Lattice.",
        "consider combinations of characters as a label).",
        "Such a huge label set might require extremely heavy calculation."
      ]
    },
    {
      "heading": "2.. No Gold Standard Data: We build the gold",
      "text": [
        "standard label from character alignment using GIZA++ .",
        "Of course, such gold standard data contain alignment errors, which might decrease labeling performance.",
        "3.",
        "No Language Model: The proposed approach cannot incorporate the target language model.",
        "In spite of the disadvantages listed above, the proposed method offers two strong advantages.",
        "1.",
        "Fast Decoding: Decoding (more precisely labeling) is extremely fast (0.12-0.58 s/input).",
        "Such rapid decoding is useful for various applications, for example, a query expansion for a search engine and so on.",
        "2.",
        "Easy Implementation: Because sequential labeling is a traditional research topic, various algorithms and tools are available.",
        "Using them, we can easily realize various transliteration systems in any language pairs.",
        "The experimental results empirically demonstrate that the proposed method is competitive in several language directions (e.g. English-Chinese)."
      ]
    },
    {
      "heading": "2. Method",
      "text": [
        "We developed a two-stage labeling system.",
        "First, an input term is decomposed into several segments (STEP1).",
        "Next, each segmentation produces sym-bol(s) (STEP2).",
        "For a given noun phrase, consisting n characters, the system gave a label (L\\...Ln) that represents segmentations.",
        "The segmentation is expressed as two types of labels (label B and I), where B signifies a beginning of the segmentation, and I signifies the end of segmentation.",
        "This representation is similar to the IOB representation, which is used in Named Entity Recognition (NER) or chunking.",
        "For label prediction, we used Conditional Random Fields (CRFs), which is a state-of-the-art labeling algorithm.",
        "We regard a source character itself as a CRF feature.",
        "The window size is three (the current character and previous/next character).",
        "Next, the system estimates labels (T...Tm) for each segmentation, where m is the number of seg-",
        "* EN-CH is provided by (Li et al., 2004); EN-TA, EN-KA, EN-HI and EN-RU are from (Kumaran and Kellner, 2007); EN-JA and EN-KO are from http://www.cjk.org/.",
        "mentations (the number of B labels in STEP1).",
        "The label of this step directly represents a target language character(s).",
        "The method of building a gold standard label is described in the next subsection.",
        "Like STEP1, we use CRFs, and regard source characters as a feature (window size=3).",
        "First, character alignment is estimated using GIZA++ as shown at the top of Fig. 2.",
        "The alignment direction is a target language-to-English, assuming that n English characters correspond to a target language character.",
        "The STEP1 label is generated for each English character.",
        "If the alignment is 1:1, we give the character a B label.",
        "If the alignment is n :1, we assign the first character a B label, and give the others I.",
        "Note that we regard null alignment as a continuance of the last segmentation (I).",
        "The STEP2 label is generated for each English segmentation (B or BI*).",
        "If a segmentation corresponds to two or more characters in the target side, we regard the entire sequence as a label (see T5 in Fig. 2)."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "To evaluate the performance of our system, we used a training-set and test-set provided by NEWS(Table 1).",
        "We used the following six metrics (Table 2) using 10 output candidates.",
        "A white paper presents the detailed definitions.",
        "For learning, we used CRF++ with standard parameters (f=20, c=.5).",
        "Notation",
        "Language",
        "Train",
        "Test",
        "EN-CH",
        "English-Chinese",
        "31,961",
        "2,896",
        "EN-JA",
        "English-Japanese",
        "27,993",
        "1,489",
        "EN-KO",
        "English-Korean",
        "4,840",
        "989",
        "EN-HI",
        "English-Hindi",
        "10,014",
        "1,000",
        "EN-TA",
        "English-Tamil",
        "8,037",
        "1,000",
        "EN-KA",
        "English-Kannada",
        "8,065",
        "1,000",
        "EN-RU",
        "English-Russian",
        "5,977",
        "1,000",
        "Table 3 presents the performance.",
        "As shown in the table, a significant difference was found between languages (from low (0.17) to high (0.58)).",
        "The high accuracy results(EN-CH or EN-RU) are competitive with other systems (the middle rank among the NEWS participating systems).",
        "However, several language results (such as EN-KO) were found to have poor performance.",
        "We investigated the difference between highperformance languages and the others.",
        "Table 4 shows the training/test times and the number of labels.",
        "As shown in the table, wide divergence is apparent in the number of labels.",
        "For example, although EN-KO requires numerous labels (536 labels), EN-RU needs only 131 labels.",
        "This divergence roughly corresponds to both training-time and accuracy as follows: (1) EN-KO requires long training time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN-RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC).",
        "This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task.",
        "The test time seemed to have no relation to",
        "* Test time is the average labeling time for an input.",
        "Training time is the average training time for 1000 labels.",
        "both training time and performance.",
        "To investigate what gave effects on test time is a subject for our future work."
      ]
    },
    {
      "heading": "4. Related Works",
      "text": [
        "Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993).",
        "This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007).",
        "As described previously, the proposed discriminative approach differs from them.",
        "Another perspective is that of how to represent transliteration phenomena.",
        "Methods can be classified into three main types: (1) grapheme-based (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, and Isahara, 2007)).",
        "Our proposed method employs a grapheme-based approach.",
        "Employing phonemes is a challenge reserved for future studies.",
        "Aramaki et al.",
        "(2008) proposed a discriminative transliteration approach using Support Vector Machines (SVMs).",
        "However, their goal, which is to judge whether two terms come from the same English words or not, differs from this paper goal.",
        "ACC",
        "Meaiii?",
        "MRR",
        "MAPre/",
        "MAPio",
        "MAPsys",
        "EN-CH",
        "0.580",
        "0.826",
        "0.653",
        "0.580",
        "0.199",
        "0.199",
        "EN-RU",
        "0.531",
        "0.912",
        "G.635",
        "G.531",
        "G.219",
        "0.219",
        "EN-JA",
        "0.457",
        "0.828",
        "G.576",
        "G.445",
        "G.194",
        "0.194",
        "EN-TA",
        "0.365",
        "0.884",
        "G.5G4",
        "G.36G",
        "G.172",
        "0.172",
        "EN-HI",
        "0.363",
        "0.864",
        "G.5G3",
        "G.36G",
        "G.17G",
        "0.170",
        "EN-KA",
        "0.324",
        "0.856",
        "G.438",
        "G.315",
        "G.148",
        "0.148",
        "EN-KO",
        "0.170",
        "0.512",
        "0.218",
        "0.170",
        "0.069",
        "0.069",
        "ACC",
        "Word Accuracy in Top 1.",
        "Meani?",
        "The mean_p measures the fuzzy accuracy that is defined by the edit distance and Longest Common Subsequence (LCS).",
        "MRR",
        "Mean Reciprocal Rank.",
        "1/MRR tells approximately the average rank of the correct transliteration.",
        "MAPre/",
        "Measures the precision in the n – best candidates tightly for each reference.",
        "MAPio",
        "Measures the precision in the 10-best candidates.",
        "IVÏAPsys",
        "Measures the precision in the top Ki-best candidates produced by the system.",
        "Language",
        "Test",
        "Train",
        "# of labels",
        "EN-KO",
        "0.436s",
        "llm09.5s",
        "536",
        "EN-CH",
        "0.201s",
        "6ml8.9s",
        "283",
        "EN-JA",
        "0.247s",
        "4m44.3s",
        "269",
        "EN-KA",
        "0.190s",
        "2m26.6s",
        "231",
        "EN-HI",
        "0.302s",
        "lm55.6s",
        "268",
        "EN-TA",
        "0.124s",
        "lm32.9s",
        "207",
        "EN-RU",
        "0.580s",
        "0m26.3s",
        "131"
      ]
    },
    {
      "heading": "5. Conclusions",
      "text": [
        "This paper presents a discriminative transliteration model using a sequential labeling technique.",
        "Experimental results yielded competitive performance, demonstrating the feasibility of the proposed approach.",
        "In the future, how to incorporate more rich information, such as language model and phoneme, is remaining problem.",
        "We believe this task conversion, from generation to sequential labeling, can be useful for several practical applications.",
        "ACKNOWLEDGMENT",
        "Part of this research is supported by Japanese Grant-in-Aid for Scientific Research (A) Number:20680006."
      ]
    }
  ]
}
