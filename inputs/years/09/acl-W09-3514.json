{
  "info": {
    "authors": [
      "Colin Cherry",
      "Hisami Suzuki"
    ],
    "book": "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009)",
    "id": "acl-W09-3514",
    "title": "NEWS 2009 Machine Transliteration Shared Task System Description: Transliteration with Letter-to-Phoneme Technology",
    "url": "https://aclweb.org/anthology/W09-3514",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Colin Cherry and Hisami Suzuki",
        "We interpret the problem of transliterating English named entities into Hindi or Japanese Katakana as a variant of the letter-to-phoneme (L2P) subtask of text-to-speech processing.",
        "Therefore, we apply a reimplementation of a state-of-the-art, discriminative L2P system (Jiampojamarn et al., 2008) to the problem, without further modification.",
        "In doing so, we hope to provide a baseline for the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), indicating how much can be achieved without transliteration-specific technology.",
        "This paper briefly summarizes the original work and our reimplementation.",
        "We also describe a bug in our submitted implementation, and provide updated results on the development and test sets."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Transliteration occurs when a word is borrowed into a language with a different character set from its language of origin.",
        "The word is transcribed into the new character set in a manner that maintains phonetic correspondence.",
        "When attempting to automate machine transliteration, modeling the channel that transforms source language characters into transliterated target language characters is a key component to good performance.",
        "Since the primary signal followed by human transliterators is phonetic correspondence, it makes sense that a letter-to-phoneme (L2P) transcription engine would perform well at this task.",
        "Of course, transliteration is often framed within the larger problems of translation and bilingual named entity co-reference, making available a number of other interesting features, such as target lexicons (Knight and Graehl, 1998), distributional similarity (Bilac and Tanaka, 2005), or the dates of an entity's mentions in the news (Kle-mentiev and Roth, 2006).",
        "However, this task's focus on generation has isolated the character-level component, which makes L2P technology a near-ideal match.",
        "For our submission, we reimplement the L2P approach described by Jiampojamarn et al.",
        "(2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana tests."
      ]
    },
    {
      "heading": "2. Approach",
      "text": [
        "The core of the L2P transduction engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004).",
        "The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation.",
        "This algorithm is used to conduct a search for a max-weight derivation according to a linear model with indicator features.",
        "A sample derivation is shown in Figure 1.",
        "There are two main categories of features: context and transition features, which follow the first two feature templates described by Jiampojamarn et al.",
        "(2008).",
        "Context features are centered around a transduction operation.",
        "These features include an indicator for the operation itself, which is then conjoined with indicators for all n-grams of source context within a fixed window of the operation.",
        "Transition features are Markov or n-gram features.",
        "They ensure that the produced target string makes sense as a character sequence, and are represented as indicators on the presence of target n-grams.",
        "The feature templates have two main parameters, the size S of the character window from which source context features are drawn, and the maximum length T of target n-gram indicators.",
        "We fit these parameters using grid search over 1-best accuracy on the provided development sets.",
        "The engine's features are trained using the structured perceptron (Collins, 2002).",
        "Jiampo-jamarn et al.",
        "(2008) show strong improvements in the L2P domain using MIRA in place of the perceptron update; unfortunately, we did not implement a best MIRA update due to time constraints.",
        "In our implementation, no special consideration was given to the availability of multiple correct answers in the training data; we always pick the first reference transliteration and treat it as the only correct answer.",
        "Investigating the use of all correct answers would be an obvious next step to improve the system.",
        "Our system made two alternate design decisions (we do not claim improvements) over those made by (Jiampojamarn et al., 2008), mostly based on the availability of software.",
        "First, we employed a beam of 40 candidates in our decoder, to enable efficient use of large language model contexts.",
        "This is put to good use in the Hindi task, where we found n-gram indicators of length up to n = 6 provided optimal development performance.",
        "Second, we employed an alternate character aligner to create our training derivations.",
        "This aligner is similar to recent non-compositional phrasal word-alignment models (Zhang et al., 2008), limited so it can only produce monotone character alignments.",
        "The aligner creates substring alignments, without insertion or deletion operators.",
        "As such, an aligned transliteration pair also serves as a transliteration derivation.",
        "We employed a maximum substring length of 3.",
        "The training data was heuristically cleaned after alignment.",
        "Any derivation found by the aligner that uses an operation occurring fewer than 3 times throughout the entire training set was eliminated.",
        "This reduced training set sizes to 8,511 pairs for English-Hindi and 20,306 pairs for English-Katakana."
      ]
    },
    {
      "heading": "3. The Bug",
      "text": [
        "The submitted version of our system had a bug in its transition features: instead of generating an indicator for every possible n-gram in the generated target sequence, it generated n-grams over target substrings, defined by the operations used during transduction.",
        "Consider, for example, the derivation shown in Figure 1, which generates \"T^'J^7>\".",
        "With buggy trigram transition features, the final operation would produce the single indicator [ >'J 1^7^], instead of the two character-level trigrams [>'J |^7] and [ v)i]\\y].",
        "This leads to problems with data sparsity, which we had not noticed on unrelated experiments with larger training data.",
        "We report results both with the bug and with fixed transition features.",
        "We do so to emphasize the importance of a fine-grained language discriminative language model, as opposed to one which operates on a substring level."
      ]
    },
    {
      "heading": "4. Development",
      "text": [
        "Development consisted of performing a parameter grid search over S and T for each language pair's development set.",
        "All combinations of S = 0... 4 and T = 0... 7 were tested for each language pair.",
        "Based on these experiments, we selected (for the fixed version), values of S = 2, T = 6 for English-Hindi, and S = 4, T = 3 for English-Katakana."
      ]
    },
    {
      "heading": "5. Results",
      "text": [
        "The results of our internal experiments with the official evaluation tool are shown in Table 1.",
        "We report 1-best accuracy on both development and test sets, with both the buggy and fixed versions of our system.",
        "As one can see, the bug makes less of an impact in the English-Katakana setting, where more training data is available.",
        "System / Test set",
        "With Bug",
        "Fixed",
        "Hindi Dev",
        "36.7",
        "39.6",
        "Hindi Test",
        "41.8",
        "46.6",
        "Katakana Dev",
        "46.0",
        "47.1",
        "Katakana Test",
        "46.6",
        "46.9"
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "We have demonstrated that an automatic letter-to-phoneme transducer performs fairly well on this transliteration shared task, with no language-specific or transliteration-specific modifications.",
        "Instead, we simply considered Hindi or Katakana to be an alternate encoding for English phonemes.",
        "In the future, we would like to investigate proper use of multiple reference answers during percep-tron training."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the NEWS 2009 Machine",
        "Transliteration Shared Task organizers forcreating this venue for comparing transliteration methods.",
        "We would also like to thank Chris Quirk for providing us with his alignment software."
      ]
    }
  ]
}
