{
  "info": {
    "authors": [
      "Dmitry Zelenko"
    ],
    "book": "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009)",
    "id": "acl-W09-3526",
    "title": "Combining MDL Transliteration Training with Discriminative Modeling",
    "url": "https://aclweb.org/anthology/W09-3526",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "4300 Fair Lakes Ct. Fairfax, VA 22033, USA dmitry_zelenko@sra.",
        "com"
      ]
    },
    {
      "heading": "2. MDL Training for Transliteration",
      "text": [
        "We present a transliteration system that introduces minimum description length training for transliteration and combines it with discriminative modeling.",
        "We apply the proposed approach to transliteration from English to 8 non-Latin scripts, with promising results."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recent research in transliteration and translation showed utility of increasing the n-gram size in transliteration models and phrase tables (Koehn et al., 2003).",
        "Yet most learning algorithms for training n-gram transliteration models place restrictions on the size of n-gram due to tractability and overfitting issues, and, in the case of machine translation, construct the phrase table after training the model, in an ad-hoc manner.",
        "In this paper, we present a minimum description length (MDL) approach (Grunwald, 2007) for learning transliteration models comprising n-grams of unrestricted size.",
        "Given a bilingual dictionary of transliterated data we seek to derive a transliteration model so that the combined size of the data and the model is minimized.",
        "Use of discriminative modeling for transliteration and translation is another promising direction allowing incorporation of arbitrary features in the transliteration process (Zelenko and Aone, 2006; Goldwasser and Roth, 2008).",
        "Here we propose to use the transliteration model derived via MDL training as a starting point and learn the model weights in the discriminative manner.",
        "The discriminative approach also provides a natural way to integrate the language modeling component into the transliteration decoding process.",
        "We experimentally evaluate the proposed approach on the standard datasets for the task of transliterating from English to 8 non-Latin scripts",
        "In our transliteration setting, we are given a string e written in an alphabet V (e.g., Latin), which is to be transliterated into a string f written in an alphabet V2 (e.g., Chinese).",
        "We consider a transliteration process that is conducted by a transliteration model T, which represents a function mapping a pair of strings (ej, f) into a score T(ej; f) £ R. For an alignment A = {(ej, f j)} of e and f, we define the alignment score T(A) = J2j T(ej; fj).",
        "For a string e and a model T, the decoding process seeks the optimal transliteration T(e) with respect to the model T:",
        "Different assumptions for transliteration models lead to different estimation algorithms.",
        "A popular approach is to assume a joint generative model for pairs (e, f), so that given an alignment A = {(ej ,fj)}, a probability P(e, f) is defined to be nj p(ej , f).",
        "The probabilities p(ej, fj) are estimated using the EM algorithm, and the corresponding transliteration model is T(ej,fj) = log(p(ej, fj)).",
        "We can alternatively model the conditional probability directly: P(f |e) = njp(fj|ej), where we again estimate the conditional probabilities p(fj|ej) via the EM algorithm, and define the transliteration model accordingly: T(ej, fj) = log(p(fj|ej)).",
        "We can also combine joint estimation with conditional decoding, observing that p(fi\\ei) = ^<yei'/i\\.",
        "and using the conditional transliteration model after estimating a joint generative model.",
        "Increasing the maximum n-gram size in probabilistic modeling approaches, at some point, degrades model accuracy due to overfitting.",
        "Therefore, probabilistic approaches typically use a small n-gram size, and perform additional modeling post",
        "factum: examples include joint n-gram modeling and phrase table construction in machine translation.",
        "We propose to apply the MDL principle to transliteration modeling by seeking the model that compresses the transliteration data so that the combined size of the compressed data and the model is minimized.",
        "If T corresponds to a joint probabilistic model P = {p(ei; f)}, then we can use the model to encode the data D = {(e, f )} in bits, where A = {(ei, fi)} is an alignment of e and f.",
        "We can encode each symbol of an alphabet V using log |V | bits so encoding a string s of length |s| from alphabet V takes CV(s) = log | V|(|s| + 1) bits (we add an extra string termination symbol for separability).",
        "Therefore, we encode each transliteration model in bits, where Ct(ej, fj) = Cyx (ej) + Cy2 (fj) log p(ej; fj) is the number of bits used to encode both the pair (ej, fj) and its code according to P. Thus, we seek a probability distribution P that minimizes C(P) = Cd(P) + Ct(P).",
        "Let P be an initial joint probability distribution for a transliteration model T such that a string pair (ej, fj) appeared n(ej, fj) times, and p(ej, fj) = n(ej,fj)/N, where N = E(ei)/i) n(ej,fj).",
        "Then, encoding a pair (ej, fj) takes on average Ciaji) = - logpiaji) bits here we distribute the model size component to all occurrences of (ej, fj) in the data.",
        "Notice that the combined data and model size C (P) = E(ei,/i) n(ej,fj)C(ej,fj).",
        "It is this quantity C(ej; fj) that we propose to use when conducting the MDL training algorithm below.",
        "Gn(ei'h) ~ loSP(e*> fi)- Set combined size C(P)*= E(ei)/i) n(ej,fj)C(ej,fj).",
        "2.",
        "Iterate: during each iteration, for each (e, f) £ D, find the minimum codesize alignment A = org min^Ej C(ej,fj) of ( e, f ) .",
        "Use the alignments to re-estimate P and recompute C. Exit when there is no improvement in the combined model and data size.",
        "Experimentally, we observed fast convergence of the above algorithm just after a few iterations, though we cannot present a convergence proof as yet.",
        "We picked the initial model by computing co-occurrence counts of n-gram pairs in D, that is, n(ej, fj) = E(e,/) min(ne(ej),n/(fj)), where ne(ej) (n/(fj)) is the number of times the n-gram e (f ) appeared in the string e (f).",
        "Note that a Bayesian interpretation of the proposed approach is not straightforward due to the use of empirical component – log p(ej; fj) in model encoding.",
        "Changing the model encoding to use, for example, a code for n(ej; fj) would allow for a direct Bayesian interpretation ofthe proposed code, and we plan to pursue this direction in the future.",
        "The output of the MDL training algorithm is the joint probability model P that we use to define the transliteration model weights as the logarithm of corresponding conditional probabilities: T(et, U) = log Y^ffj-y During the decoding process of inferring f from e via an alignment A, we integrate the language model probability p(f) via a linear combination: TGEN(e) = org max/ {T(A) + p log p(f)/|f |}, where p, is a combination parameter estimated via cross-validation."
      ]
    },
    {
      "heading": "3. Discriminative Training",
      "text": [
        "We use the MDL-trained transliteration model T as a starting point for discriminative training: we consider all n-gram pairs (e , f ) with nonzero probabilities p(ej; fj) as features of a linear discriminative model TDISCR.",
        "We also integrate the normalized language modeling probability po(f) = p(f)]f] in the discriminative model as one of the features: TDISCR(e) = orgmax/{T(A) + T0p0(f)}.",
        "We learn the weights T(ej; fj) and T0 of the discriminative model using the average perceptron algorithm of (Collins, 2002).",
        "Since both the transliteration model and the language model are required to be learned from the same data, and the language modeling probability is integrated into our decoding process, we remove the string e from the language model before processing the example (f, e) during training; we reincorporate the string e in the language model after the example (f, e) is processed by the averaged perceptron algorithm.",
        "We use the discriminatively trained model as the \"standard\" system in our experiments."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We use the standard data for transliterating from English into 8 non-Latin scripts: Chinese (Haizhou et al., 2004); Korean, Japanese (Kanji), and Japanese (Katakana) (CJK Institute, 2009); Hindi, Tamil, Kannada, and Russian (Kumaran and Kellner, 2007).",
        "The data is provided as part of the Named Entities Workshop 2009 Machine Transliteration Shared Task (Li et al., 2009).",
        "For all 8 datasets, we report scores on the standard tests sets provided as part of the evaluation.",
        "Details of the evaluation methodology are presented in (Li et al., 2009).",
        "We perform the same uniform processing of data: names are considered sequences of Unicode characters in their standard decomposed form (NFD).",
        "In particular, Korean Hangul characters are decomposed into Jamo syllabary.",
        "Since the evaluation data are provided in the recomposed form, we recompose output of the transliteration system.",
        "We split multi-word names (in Hindi, Tamil, and Kannada datasets) in single words and conducted training and evaluation on the single word level.",
        "We assume no word order change for multiword names and ignore name pairs with different numbers of words.",
        "We apply pre-set system parameters with very little tuning.",
        "In particular, we utilize a 5-gram language model with Good-Turing discounting.",
        "The MDL training algorithm requires only the cardinalities of the corresponding alphabets as parameters, and we use the following approximate vocabulary sizes typically rounded to the closest power of 2 (except for Chinese and Japanese): for English, Russian, Tamil, and Kannada, we set | V| = 32; for Katakana and Hindi, | V | =64; for Korean Jamo, | V| = 128; for Chinese and Japanese Kanji, |V | = 1024.",
        "We perform 10 iterations of the average per-ceptron algorithm for discriminative training.",
        "For",
        "Table 1: MDL Data and Model Compression showing initial data size, final combined data and model size, the compression ratio, and the number of n-gram pairs in the final model.",
        "Table 2: Experimental results for transliteration from English to 8 non-Latin scripts comparing performance of generative (T1) and corresponding discriminative (T2) models.",
        "both alignment and decoding, we use a beam search decoder, with the beam size set to 100.",
        "Our first set of experiments illustrates compression achieved by MDL training.",
        "Table 1 shows for each for the training datasets, the original size of the data, compressed size ofthe data including the model size, the compression ratio, and the number of n-gram pairs in the final model.",
        "We see very similar compression for all languages.",
        "The number of n-gram pairs for the final model is also relatively small.",
        "In general, MDL training with discriminative modeling allows us to discover a flexible small set of features (n-gram pairs) without placing any restriction on n-gram size.",
        "We can interpret MDL training as searching implicitly for the best bound on the n-gram size together with searching for appropriate features.",
        "Our preliminary experiments also indicate that performance of models produced by the MDL approach roughly corresponds to performance of models trained with the optimal bound on the size of n-gram features.",
        "Init",
        "Comp",
        "Ratio",
        "Diet",
        "Chinese",
        "333 Kb",
        "158 Kb",
        "0.48",
        "5780",
        "Hindi",
        "159 Kb",
        "72 Kb",
        "0.45",
        "1956",
        "Japanese (Kanji)",
        "170 Kb",
        "82 Kb",
        "0.48",
        "4394",
        "Kannada",
        "131 Kb",
        "62 Kb",
        "0.48",
        "2010",
        "Japanese (Katakana)",
        "289 Kb",
        "136 Kb",
        "0.47",
        "3383",
        "Korean",
        "69 Kb",
        "31 Kb",
        "0.45",
        "1181",
        "Russian",
        "78 Kb",
        "37 Kb",
        "0.48",
        "865",
        "Tamil",
        "134 Kb",
        "62 Kb",
        "0.46",
        "1827",
        "Tl(Acc)",
        "T2(Acc)",
        "T2(F)",
        "T2(MRR)",
        "Chinese",
        "0.522",
        "0.619",
        "0.847",
        "0.711",
        "Hindi",
        "0.312",
        "0.409",
        "0.864",
        "0.527",
        "Japanese (Kanji)",
        "0.484",
        "0.509",
        "0.675",
        "0.6",
        "Kannada",
        "0.227",
        "0.345",
        "0.854",
        "0.462",
        "Japanese (Katakana)",
        "0.318",
        "0.420",
        "0.807",
        "0.541",
        "Korean",
        "0.339",
        "0.413",
        "0.702",
        "0.524",
        "Russian",
        "0.488",
        "0.566",
        "0.919",
        "0.662",
        "Tamil",
        "0.267",
        "0.374",
        "0.880",
        "0.512",
        "Table 2 demonstrates that discriminative modeling significantly improves performance of the corresponding generative models.",
        "In this setting, the MDL training step is effectively used for feature construction: its goal is to automatically hone in on a small set of features whose weights are later learned by discriminative methods.",
        "From a broader perspective, it is an open question whether seeking a compact representation of sequential data leads to robust and best-performing models, especially in noisy environments.",
        "For example, state-of-the-art phrase translation models eschew succinct representations, and instead employ broad redundant sets of features (Koehn et al., 2003).",
        "On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009).",
        "Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research."
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Ze-lenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008).",
        "Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (Goldsmith, 2001; Argamon et al., 2004).",
        "(Bo-drumlu et al., 2009) present a related approach on optimizing the alignment dictionary size in machine translation."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "We introduced a minimum description length approach for training transliteration models that allows to avoid overfitting without putting apriori constraints of the size of n-grams in transliteration models.",
        "We plan to apply the same paradigm to other sequence modeling tasks such as sequence classification and segmentation, in both supervised and unsupervised settings."
      ]
    }
  ]
}
