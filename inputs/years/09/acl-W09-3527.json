{
  "info": {
    "authors": [
      "Balakrishnan Varadarajan",
      "Delip Rao"
    ],
    "book": "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009)",
    "id": "acl-W09-3527",
    "title": "$$-extension Hidden Markov Models and Weighted Transducers for Machine Transliteration",
    "url": "https://aclweb.org/anthology/W09-3527",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "e-extension Hidden Markov Models and Weighted Transducers for",
        "Machine Transliteration",
        "Balakrishnan Vardarajan Delip Rao",
        "of Electrical and Computer Engineering Dept.",
        "of Computer Science",
        "We describe in detail a method for transliterating an English string to a foreign language string evaluated on five different languages, including Tamil, Hindi, Russian, Chinese, and Kannada.",
        "Our method involves deriving substring alignments from the training data and learning a weighted finite state transducer from these alignments.",
        "We define an e-extension Hidden Markov Model to derive alignments between training pairs and a heuristic to extract the substring alignments.",
        "Our method involves only two tunable parameters that can be optimized on held-out data."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Transliteration is a letter by letter mapping of one writing system to another.",
        "Apart from the obvious use in writing systems, transliteration is also useful in conjunction with translation.",
        "For example, machine translation BLEU scores are known to improve when named entities are transliterated.",
        "This engendered several investigations into automatic transliteration of strings, named entities in particular, from one language to another.",
        "See Knight and Graehl(1997) and later papers on this topic for an overview.",
        "Hidden Markov Model (HMM) (Rabiner, 1989) is a standard sequence modeling tool used in various problems in natural language processing like machine translation, speech recognition, part of speech tagging and information extraction.",
        "There have been earlier attempts in using HMMs for automatic transliteration.",
        "See (Abdul Jaleel and Larkey, 2003; Zhou et al., 2008) for example.",
        "In this paper, we define an e-extension Hidden Markov Model that allows us to align source and target language strings such that the characters in the source string may be optionally aligned to the e symbol.",
        "We also introduce a heuristic that allows us to extract high quality sub-alignments from the e-aligned word pairs.",
        "This allows us to define a weighted finite state transducer that produces transliterations for an English string by minimal segmentation.",
        "The overview of this paper is as follows: Section 2 introduces e-extension Hidden Markov Model and describes our alignment procedure.",
        "Section 3 describes the substring alignment heuristic and our weighted finite state transducer to derive the final n-best transliterations.",
        "We conclude with a result section describing results from the NEWS 2009 shared task on five different languages."
      ]
    },
    {
      "heading": "2. Learning Alignments",
      "text": [
        "The training data D is given as pairs of strings (e, f) where e is the English string with the corresponding foreign transliteration f. The English string e consists of a sequence of English letters (ei, e2,..., e;v) while f = (fi, f2,..., /m) .",
        "We represent E as the set of all English symbols and F as the set of all foreign symbols.",
        "We also assume both languages have a special null symbol e, that is e G E and e G F.",
        "Our alignment model is a Hidden Markov Model H(X, Y, S, T, Ps), where• X is the start state and Y is the end state.",
        "• S is the set of emitting states with S = |S|.",
        "The emitting states are indexed from 1 to S. The start state X is indexed as state 0 and the end state Y is indexed as state S + 1.",
        "• T is an (S + 1) x (S + 1) stochastic matrix with T = [tij] for i G {0,1,... , S} and j G {1, 2,...,S + 1}.",
        "• Ps = [pef} is an \\£\\ x \\F\\ matrix of joint emission probabilities with pef = P(e, f \\ s) Vs G S.",
        "We define s to be an e-extension of a string of characters s = (c1; c2,..., ck) as the string obtained by pumping an arbitrary number of e symbols between any two adjacent characters cl and Q+i.",
        "That is, s = (d^,..., d»2,..., difc ) where dij = Cj and di = e for im < l < iTO+1 where 1 < l < k. Observe that there are countably infinite e-extensions for a given string s since an arbitrary number of e symbols can be inserted between characters cm and cTO+1.",
        "Let T(s) denote the set of all possible e-extensions for a given string s.",
        "For a given pair of strings (u, v), we define a joint e-extension of (u, v) as the pair (u, v) s.t.",
        "u G T(u) and v G T(v) with \\ u \\ = \\ v \\ and $i s.t.",
        "ùi = vi = e. Due to this restriction, there are finite e-extensions for a pair (u, v) with the length of u and v bounded above by \\ u \\ + \\ v \\.",
        "Let J(u, v) denote the set of all joint e-extensions of (u, v).",
        "Given a pair of strings (e, f) with e = (ei,e2,...,eN) and f = (fi,f2,...,fM), we compute the probability a(e, f, s') that they are transliteration pairs ending in state s' as denote a subsequence of a string u as u^ = ura+1,..., um) .",
        "Using these definitions, we can define a(ei, fj, s) as",
        "Finally for i > l and j > l ,",
        "Similarly the recurrence for //(eN, f;",
        "In order to compute the probability Q(e, f) of a given transliteration pair, the final state has to be the end state S + 1 .",
        "Hence",
        "We also write the probability //(e, f, s') that they are transliteration pairs starting in state s' as",
        "In order to proceed with the E.M. estimation of the parameters T and Ps , we collect the soft counts c(e, f | s) for emission probabilities by looping over the training data D as shown in Fig",
        "Finally the probabilities P(e, f |s) and tij are re-estimated as",
        "Again noting that the start state of the HMM",
        "Q(e, f) = £ ß(e, f,s)to,s.",
        "which contradicts the definition of joint e-extension.",
        "We can also compute the most probable alignment (e, f) between the two strings e and f as ure 1.",
        "Similarly the soft counts ct(s',s) for the transition probabilities are estimated as shown in Figure 2.",
        "The pair(e, f) is considered as analignmentbe-tween the training pair (e, f)."
      ]
    },
    {
      "heading": "3. Transduction of the Transliterated Output",
      "text": [
        "Given an alignment (e, f), we consider all possible sub-alignments (ej, fj) as pairs of substrings obtained from (e, f) such that ei = e, f = e, ej+1 = e and fj+1 = e .",
        "We extract all possible sub-alignments of all the alignments from the training data.",
        "Let A be the bag of all sub-alignments obtained from the training data.",
        "We build a weighted finite state transducer that transduces any string in E+ to F+ using these sub-alignments.",
        "Let (u, v) be an element of A.",
        "From the training data D, observe that A can have multiple realizations of (u, v).",
        "Let N(u, v) be the number of times (u, v) is observed in A.",
        "The empirical probability of transducing string u to v is simply",
        "Forevery pair(u, v) G A , we also compute the probability of transliteration from the HMM H as Q(u, v) from Equation 1.",
        "We construct a finite state transducer Fu v that accepts only u and emits v with a weight wu>vdefined as",
        "Finally we construct a global weighted finite state transducer F by taking the union of all the Fu v and taking its closure.",
        "The weight 6 is typically sufficiently high so that a new english string is favored to be broken into fewest possible sub-strings whose transliterations are available in the training data.",
        "We tune the weights A and 6 by evaluating the accuracy on the held-out data.",
        "The n-best paths in the weighted finite state transducer F represent our n-best transliterations."
      ]
    },
    {
      "heading": "4. Results",
      "text": [
        "We evaluated our system on the standard track data )provided by the NEWS 2009 shared task organizers on five different languages - Tamil, Hindi, Russian, and Kannada was derived from (Ku-maran and Kellner, 2007) and Chinese from (Li et al., 2004).",
        "The results of this evaluation on the test data is shown in Table 1.",
        "For a detailed description of the evaluation measures used we refer the readers to NEWS 2009 shared task whitepaper (Li et al., 2009)."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We described a system for automatic transliteration of pairs of strings from one language to another using e-extension hidden markov models and weighted finite state transducers.",
        "We evaluated our system on all the languages for the NEWS 2009 standard track.",
        "The system presented is language agnostic and can be trained for any language pair within a few minutes on a single core desktop computer.",
        "Language",
        "Top-1",
        "mean",
        "MRR",
        "Accuracy",
        "Fi score",
        "Tamil",
        "0.327",
        "0.870",
        "0.458",
        "Hindi",
        "0.398",
        "0.855",
        "0.515",
        "Russian",
        "0.506",
        "0.901",
        "0.609",
        "Chinese",
        "0.450",
        "0.755",
        "0.514",
        "Kannada",
        "0.235",
        "0.817",
        "0.353"
      ]
    }
  ]
}
