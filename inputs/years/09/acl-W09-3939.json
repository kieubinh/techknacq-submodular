{
  "info": {
    "authors": [
      "Giang Linh Nguy",
      "Václav Novák",
      "Zdeněk Žabokrtský"
    ],
    "book": "Proceedings of the SIGDIAL 2009 Conference",
    "id": "acl-W09-3939",
    "title": "Comparison of Classification and Ranking Approaches to Pronominal Anaphora Resolution in Czech",
    "url": "https://aclweb.org/anthology/W09-3939",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Comparison of Classiication and Ranking Approaches to Pronominal Anaphora Resolution in Czech*",
        "Nguy Giang Linh, Vaclav Novak, Zdenek Zabokrtsky",
        "Charles University in Prague Institute of Formal and Applied Linguistics Malostranske namesti 25, CZ-11800",
        "{linh,novak,zabokrtsky}.ufal.mff.cuni.cz",
        "In this paper we compare two Machine Learning approaches to the task of pronominal anaphora resolution: a conventional classification system based on C5.0 decision trees, and a novel perceptron-based ranker.",
        "We use coref-erence links annotated in the Prague Dependency Treebank 2.0 for training and evaluation purposes.",
        "The perceptron system achieves f-score 79.43% on recognizing coreference of personal and possessive pronouns, which clearly outperforms the classifier and which is the best result reported on this data set so far."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Anaphora Resolution (AR) is a well established task in Natural Language Processing (Mitkov, 2002).",
        "Classification techniques (e.g., single candidate model aimed at answering: \"Is there a coreference link between the anaphor and this antecedent candidate, or not?\")",
        "are very often used for the task, e.g. in Mccarthy and Lehnert (1995) and Soon et al.",
        "(2001).",
        "However, as argued already in Yang et al.",
        "(2003), better results are achieved when the candidates can compete in a pairwise fashion.",
        "It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making.",
        "If we proceed further along this direction, we come to the ranking approach described in Denis and Baldridge (2007), in which the entire candidate set is considered at once and which leads to further significant shift in performance, more recently documented in Denis and",
        "Baldridge (2008).",
        "In this paper we deal with supervised approaches to pronominal anaphora in Czech.",
        "For training and evaluation purposes, we use corefer-ences links annotated in the Prague Dependency Treebank, (Jan Hajic, et al., 2006).",
        "We limit ourselves only to textual coreference (see Section 2) and to personal and possessive pronouns.",
        "We make use of a rich set of features available thanks to the complex annotation scenario of the treebank.",
        "We experiment with two of the above mentioned techniques for AR: a classifier and a ranker.",
        "The former is based on a top-down induction of decision trees (Quinlan, 1993).",
        "The latter uses a simple scoring function whose optimal weight vector is estimated using perceptron learning inspired by Collins (2002).",
        "We try to provide both implementations with as similar input information as possible in order to be able to compare their performance for the given task.",
        "Performance of the presented systems can be compared with several already published works, namely with a rule-based system described in Kucova and Zabokrtsky (2005), some of the \"classical\" algorithms implemented in Nemcik (2006), a system based on decision trees (Ngu.",
        "y, 2006), and a rule-based system evaluated in Ngu.",
        "y and Zabokrtsky (2007).",
        "To illustrate the real complexity of the task, we also provide performance evaluation of a baseline solution.",
        "The most important result claimed in this paper is that, to the best of our knowledge, the presented ranker system outperforms all the previously published systems evaluated on the PDT data.",
        "Moreover, the performance of our ranker (f-score 79.43%) for Czech data is not far from the performance of the state-of-the-art system for English described in Denis and Baldridge (2008) (f-score for 3rd person pronouns 82.2 %).",
        "A side product of this work lies in bringing empirical evidence - for a different language and different data set - for the claim of Denis and Baldridge (2007) that the ranking approach is more appropriate for the task of AR than the classification approach.",
        "The paper is structured as follows.",
        "The data with manually annotated links we use are described in Section 2.",
        "Section 3 outlines preprocessing the data for training and evaluation purposes.",
        "The classifier-based and ranker-based systems are described in Section 4 and Section 5 respectively.",
        "Section 6 summarizes the achieved results by evaluating both approaches using the test data.",
        "Conclusions and final remarks follow in Section 7."
      ]
    },
    {
      "heading": "2. Coreference links in the Prague Dependency Treebank 2.0",
      "text": [
        "The Prague Dependency Treebank 2.0 (PDT 2.0, Jan Hajic, et al.",
        "(2006)) is a large collection of linguistically annotated data and documentation, based on the theoretical framework of Functional Generative Description (FGD; introduced by Sgall (1967) and later elaborated, e.g. in by Sgall et al.",
        "(1986)).",
        "The PDT 2.0 data are Czech newspaper texts selected from the Czech National Corpus (CNC).",
        "The PDT 2.0 has a three-level structure.",
        "On the lowest morphological level, a lemma and a positional morphological tag are added to each token.",
        "The middle analytical level represents each sentence as a surface-syntactic dependency tree.",
        "On the highest tectogrammatical level, each sentence is represented as a complex deep-syntactic dependency tree, see Mikulova and others (2005) for details.",
        "This level includes also annotation of coref-erential links.",
        "The PDT 2.0 contains 3,168 newspaper texts (49,431 sentences) annotated on the tectogram-matical level.",
        "Coreference has been annotated manually in all this data.",
        "Following the FGD, there are two types of coreference distinguished: grammatical coreference and textual coreference (Panevova, 1991).",
        "The main difference between the two coreference types is that the antecedent in grammatical coreference can be identified using grammatical rules and sentence syntactic structure, whereas the antecedent in textual coreference can not.",
        "The further division of grammatical and textual coreference is based on types of anaphors:",
        "Grammatical anaphors: relative pronouns, reflexive pronouns, reciprocity pronouns, restored (surface-unexpressed) \"subjects\" of infinitive verbs below verbs of control,",
        "Textual anaphors: personal and possessive pronouns, demonstrative pronouns.",
        "The data in the PDT 2.0 are divided into three groups: training set (80%), development test set (10%), and evaluation test set (10%).",
        "The training and development test set can be freely exploited, while the evaluation test data should serve only for the very final evaluation of developed tools.",
        "Table 1 shows the distribution of each anaphor type.",
        "The total number of coreference links in the PDT 2.0 data is 45,174.",
        "Personal pronouns including those zero ones and possessive pronouns form 37.4% of all anaphors in the entire corpus (16,888 links).",
        "An example tectogrammatical tree with depicted coreference links (arrows) is presented in Figure 1.",
        "For the sake of simplicity, only three node attributes are displayed below the nodes: tec-togrammatical lemma, functor, and semantic part of speech (tectogrammatical nodes themselves are complex data structures and around twenty attributes might be stored with them).",
        "Tectogrammatical lemma is a canonical word form or an artificial value of a newly created node",
        "Table 1: Distribution of the different anaphor types in the PDT 2.0.",
        "on the tectogrammatical level.",
        "E.g. the (artificial) tectogrammatical lemma #PersPron stands for personal (and possessive) pronouns, be they expressed on the surface (i.e., present in the original sentence) or restored during the annotation of the tectogrammatical tree structure (zero pronouns).",
        "Functor captures the deep-syntactic dependency relation between a node and its governor in the tectogrammatical tree.",
        "According to FGD, functors are divided into actants (ACT - actor, PAT -patient, ADDR - addressee, etc.)",
        "and free modifiers (LOC - location, BEN - benefactor, RHEM - rhematizer, TWHEN - temporal modifier, APP - appurtenance, etc.",
        ").",
        "Semantic parts of speech correspond to basic onomasiological categories (substance, feature, factor, event).",
        "The main semantic POS distinguished in PDT 2.0 are: semantic nouns, semantic adjectives, semantic adverbs and semantic verbs (for example, personal and possessive pronouns belong to semantic nouns)."
      ]
    },
    {
      "heading": "3. Training data preparation",
      "text": [
        "The training phase of both presented AR systems can be outlined as follows:"
      ]
    },
    {
      "heading": "1.. detect nodes which are anaphors (Section 3.1),",
      "text": [
        "2. for each anaphor a%, collect the set of antecedent candidates Cand(ai) (Section 3.2),",
        "3. for each anaphor e^, divide the set of candidates into positive instances (true antecedents) and negative instances (Section 3.3),",
        "4. for each pair of an anaphor a% and an antecedent candidate Cj G Cand(a^), compute",
        "5. given the anaphors, their sets of antecedent candidates (with related feature vectors), and the division into positive and negative candidates, train the system for identifying the true antecedents among the candidates.",
        "Steps 1-4 can be seen as training data preprocessing, and are very similar for both systems.",
        "System-specific details are described in Section 4 and Section 5 respectively.",
        "In the presented work, only third person personal (and possessive) pronouns are considered,be they expressed on the surface or reconstructed.",
        "We treat as anaphors all tectogrammatical nodes with lemma #PersPron and third person stored in the gram/person grammateme.",
        "More than 98 % of such nodes have their antecedents (in the sense of textual coreference) marked in the training data.",
        "Therefore we decided to rely only on this highly precise rule when detecting anaphors.",
        "In our example tree, the node #PersPron representing his on the surface and the node #Per-sPron representing the zero personal pronoun he will be recognized as anaphors.",
        "In both systems, the predicted antecedent of a given anaphor aj is selected from an easy-to-compute set of antecedent candidates denoted as Cand(ai).",
        "We limit the set of candidates to semantic nouns which are located either in the same sentence before the anaphor, or in the preceding sentence.",
        "Table 2 shows that if we disregard cataphoric and longer anaphoric links, we loose a chance for correct answer with only 6 % of anaphors.",
        "Type/Count",
        "train",
        "dtest",
        "etest",
        "Personal pron.",
        "12,913",
        "1,945",
        "2,030",
        "Relative pron.",
        "6,957",
        "948",
        "1,034",
        "Under-control pron.",
        "6,598",
        "874",
        "907",
        "Reflexive pron.",
        "3,381",
        "452",
        "571",
        "Demonstrative pron.",
        "2,582",
        "332",
        "344",
        "Reciprocity pron.",
        "882",
        "110",
        "122",
        "Other",
        "320",
        "35",
        "42",
        "Total",
        "34,983",
        "4,909",
        "5,282",
        "If the true antecedent of a% is not present in Cand(aj), no training instance is generated.",
        "If it is present, the sets of negative and positive instances are generated based on the anaphor.",
        "This preprocessing step differs for the two systems, because the classifier can be easily provided with more than one positive instance per anaphor, whereas the ranker can not.",
        "In the classification-based system, all candidates belonging to the coreferential chain are marked as positive instances in the training data.",
        "The remaining candidates are marked as negative instances.",
        "In the ranking-based system, the coreferential chain is followed from the anaphor to the nearest antecedent which itself is not an anaphor in grammatical coreference.",
        "The first such node is put on the top of the training rank list, as it should be predicted as the winner (E.g., the nearest antecedent of the zero personal pronoun he in the example tree is the relative pronoun who, however, it is a grammatical anaphor, so its antecedent Brien is chosen as the winner instead).",
        "All remaining (negative) candidates are added to the list, without any special ordering.",
        "Our model makes use of a wide range of features that are obtained not only from all three levels of the PDT 2.0 but also from the Czech National Corpus and the EuroWordNet.",
        "Each training or testing instance is represented by a feature vector.",
        "The features describe the anaphor, its antecedent candidate and their relationship, as well as their contexts.",
        "All features are listed in Table 4 in the Appendix.",
        "When designing the feature set on personal pronouns, we take into account the fact that Czech personal pronouns stand for persons, animals and things, therefore they agree with their antecedents in many attributes and functions.",
        "Further we use the knowledge from the Lappin and Leass's algorithm (Lappin and Leass, 1994), the Mitkov's robust, knowledge-poor approach (Mitkov, 2002), and the theory of topic-focus articulation (Kucova et al., 2005).",
        "We want to take utmost advantage of information from the antecedent's and anaphor's node on all three levels as well.",
        "Distance: Numeric features capturing the distance between the anaphor and the candidate, measured by the number of sentences, clauses, tree nodes and candidates between them.",
        "Morphological agreement: Categorial features created from the values of tectogrammatical gender and number and from selected morphological categories from the positional tag of the anaphor and of the candidate.",
        "In addition, there are features indicating the strict agreement between these pairs and features formed by concatenating the pair of values of the given attribute in the two nodes (e.g., masc_neut).",
        "Agreement in dependency functions: Catego-rial features created from the values of tec-togrammatical functor and analytical functor (with surface-syntactic values such as Sb, Pred, Obj) of the anaphor and of the candidate, their agreement and joint feature.",
        "There are two more features indicating whether the candidate/anaphor is an ac-tant and whether the candidate/anaphor is a subject on the tectogrammatical level.",
        "Context: Categorial features describing the context of the anaphor and of the candidate:",
        "• parent - tectogrammatical functor and the semantic POS of the effective parent of the",
        "1G A positional tag from the morphological level is a string of 15 characters.",
        "Every positions encodes one morphological category using one character.",
        "Antecedent location",
        "Percnt.",
        "Previous sentence",
        "37 %",
        "Same sentence, preceding the anaphor",
        "57 %",
        "Same sentence, following the anaphor",
        "5 %",
        "Other",
        "1 %",
        "anaphor and the candidate, their agreement and joint feature; a feature indicating the agreement of both parents' tectogrammatical lemma and their joint feature; a joint feature of the pair of the tectogrammatical lemma of the candidate and the effective parent's lemma of the anaphor; and a feature indicating whether the candidate and the anaphor are siblings.",
        "• coordination - a feature that indicates whether the candidate is a member of a coordination and a feature indicating whether the anaphor is a possessive pronoun and is in the coordination with the candidate",
        "• collocation - a feature indicating whether the candidate has appeared in the same collocation as the anaphor within the text and a feature that indicates the collocation assumed from the Czech National Corpus.",
        "• boundness - features assigned on the basis of contextual boundness (available in the tectogrammatical trees) {contextually bound, contrastively contextually bound, or contextually non-bound} for the anaphor and the candidate; their agreement and joint feature.",
        "• frequency - 1 if the candidate is a denotative semantic noun and occurs more than once within the text; otherwise 0.",
        "Semantics: Semantically oriented feature that indicates whether the candidate is a person name for the present and a set of 63 binary ontological attributes obtained from the EuroWordNet.These attributes determine the positive or negative relation between the candidate's lemma and the semantic concepts."
      ]
    },
    {
      "heading": "4. Classifier-based system",
      "text": [
        "Our classification approach uses C5.0, a successor of C4.5 (Quinlan, 1993), which is probably the most widely used program for inducing decision trees.",
        "Decision trees are used in many AR systems such as Aone and Bennett (1995), Mccarthy and Lehnert (1995), Soon et al.",
        "(2001), and Ng",
        "Our classifier-based system takes as input a set of feature vectors as described in Section 3.4 and their classifications (1 - true antecedent, 0 - non-antecedent) and produces a decision tree that is further used for classifying new pairs of candidate and anaphor.",
        "The classifier antecedent selection algorithm works as follows.",
        "For each anaphor feature vectors $(c, aj) are computed for all candidates c G Cand(aj) and passed to the trained decision tree.",
        "The candidate classified as positive is returned as the predicted antecedent.",
        "If there are more candidates classified as positive, the nearest one is chosen.",
        "If no candidate is classified as positive, a system of handwritten fallback rules can be used.",
        "The fallback rules are the same rules as those used in the baseline system in Section 6.2."
      ]
    },
    {
      "heading": "5. Ranker-based system",
      "text": [
        "In the ranker-based AR system, every training example is a pair (aj, where aj is the anaphoric expression and yj is the true antecedent.",
        "Using the candidate extraction function Cand, we aim to rank the candidates so that the true antecedent would always be the first candidate on the list.",
        "The ranking is modeled by a linear model of the features described in Section 3.4.",
        "According to the model, the antecedent yj for an anaphoric expression a is found as:",
        "cGCand(ai)",
        "The weights tv of the linear model are trained using a modification of the averaged perceptron algorithm (Collins, 2002).",
        "This is averaged percep-tron learning with a modified loss function adapted to the ranking scenario.",
        "The loss function is tailored to the task of correctly ranking the true antecedent, the ranking of other candidates is irrelevant.",
        "The algorithm (without averaging the parameters) is listed as Algorithm 1.",
        "Note that the training instances where yj £ Cand(aj) were excluded from the training.",
        "input : N training examples (aj; yj),",
        "number of iterations T init : tv – 0 ; for t – 1 to T, i – 1 to N do if yj = yj then output: weights tv Algorithm 1: Modified perceptron algorithm for ranking.",
        "$ is the feature extraction function, aj is the anaphoric expression, yj is the true antecedent.",
        "Antecedent selection algorithm using a ranker: For each third person pronoun create a feature vector from the pronoun and the semantic noun preceding the pronoun and is in the same sentence or in the previous sentence.",
        "Use the trained ranking features weight model to get out the candidate's total weight.",
        "The candidate with the highest features weight is identified as the antecedent."
      ]
    },
    {
      "heading": "6. Experiments and evaluation",
      "text": [
        "For the evaluation we use the standard metrics:",
        "Precision _ number of correctly predicted anaphoric third person pronouns",
        "number of all predicted third person pronouns number of correctly predicted anaphoric third person pronouns number of all anaphoric third person pronouns",
        "F-measure"
      ]
    },
    {
      "heading": "2. X Precision X Recall Precision+Recall",
      "text": [
        "We consider an anaphoric third person pronoun to be correctly predicted when we can success-",
        "in Luo (2GG5).",
        "fully indicate its antecedent, which can be any antecedent from the same coreferential chain as the anaphor.",
        "Both the AR systems were developed and tested on PDT 2.0 training and development test data.",
        "Finally they were tested on evaluation test data for the final scoring, summarized in Section 6.3.",
        "We have made some baseline rules for the task of AR and tested them on the PDT 2.0 evaluation test data.",
        "Their results are reported in Table 3.",
        "Baseline rules are following: For each third person pronoun, consider all semantic nouns which precede the pronoun and are not further than the previous sentence, and:",
        "• select the nearest one as its antecedent",
        "• select the nearest one which is a clause subject (BASE 2),",
        "• select the nearest one which agrees in gender and number (BASE 3),",
        "• select the nearest one which agrees in gender and number; if there is no such noun, choose the nearest clause subject; if no clause subject was found, choose the nearest noun (BASE 3+2+1).",
        "Scores for all three systems (baseline, clasifier with and without fallback, ranker) are given in Table 3.",
        "Our baseline system based on the combination of three rules (BASE 3+2+1) reports results superior to the ones of the rule-based system described in Kucova and Zabokrtsky (2005).",
        "Kucova and Zabokrtsky proposed a set of filters for personal pronominal anaphora resolution.",
        "The list of candidates was built from the preceding and the same sentence as the personal pronoun.",
        "After applying each filter, improbable candidates were cut off.",
        "If there was more than one candidate left at the end, the nearest one to the anaphor was chosen as its antecedent.",
        "The reported final success rate was 60.4 % (counted simply as the number of correctly predicted links divided by the number of pronoun anaphors in the test data section).",
        "An interesting point of the classifier-based system lies in the comparison with the rule-based system of Nguy and Zabokrtsky (2007).",
        "Without the rule-based fallback (CLASS), the classifier falls behind the Nguy and Zabokrtsky's system (74.2%), while including the fallback (CLASS+3+2+1) it gives better results.",
        "Overall, the ranker-based system (RANK) significantly outperforms all other AR systems for Czech with the f-score of 79.43%.",
        "Comparing with the model for third person pronouns of Denis and Baldridge (2008), which reports the f-score of 82.2%, our ranker is not so far behind.",
        "It is important to say that our system relies on manually annotated information and we solve the task of anaphora resolution for third person pronouns on the tectogrammatical level of the PDT 2.0.",
        "That means these pronouns are not only those expressed on the surface, but also artificially added (reconstructed) into the structure according to the principles of FGD."
      ]
    },
    {
      "heading": "7. Conclusions",
      "text": [
        "In this paper we report two systems for AR in Czech: the classifier-based system and the ranker-based system.",
        "The latter system reaches f-score 79.43% on the Prague Dependency Treebank test data and significantly outperforms all previously published results.",
        "Our results support the hypothesis that ranking approaches are more appropriate for the AR task than classification approaches.",
        "Rule",
        "P",
        "R",
        "F",
        "BASE 1",
        "17.82%",
        "18.00%",
        "17.90%",
        "BASE 2",
        "41.69%",
        "42.06%",
        "41.88%",
        "BASE 3",
        "59.00%",
        "59.50%",
        "59.24%",
        "BASE 3+2+1",
        "62.55%",
        "63.03%",
        "62.79%",
        "CLASS",
        "69.9%",
        "70.44%",
        "70.17%",
        "CLASS+3+2+1",
        "76.02%",
        "76.60%",
        "76.30%",
        "RANK",
        "79.13%",
        "79.74%",
        "79.43%",
        "Figure 1: Simplified tectogrammatical tree representing the sentence O'Brien, ktery Louganise trenoval deset let, o jeho onemocneni vedel, ale zavaizal se mlCenlm.",
        "(Lit.",
        ": O'Brien, who Louganis trained for ten years, about his injury knew, but (he) tied himself to secrecy.)",
        "Note two coreferential chains {Brien, who, (he)} and { Louganis, his}.",
        "Distance",
        "senLdist",
        "sentence distance between c and aj",
        "clause.dist",
        "clause distance between c and aj",
        "node.dist",
        "tree node distance between c and aj",
        "cand.ord",
        "mention distance between c and aj",
        "Morphological Agreement",
        "gender",
        "t-gender of c and ai, agreement, joint",
        "number",
        "t-number of c and ai, agreement, joint",
        "apos",
        "m-POS of c and ai, agreement, joint",
        "asubpos",
        "detailed POS of c and ai, agreement, joint",
        "agen",
        "m-gender of c and ai, agreement, joint",
        "anum",
        "m-number of c and ai, agreement, joint",
        "acase",
        "m-case of c and ai, agreement, joint",
        "apossgen",
        "m-possessor's gender of c and ai, agreement, joint",
        "apossnum",
        "m-possessor's number of c and ai, agreement, joint",
        "apers",
        "m-person of c and ai, agreement, joint",
        "Functional Agreement",
        "afun",
        "a-functor of c and ai, agreement, joint",
        "fun",
        "t-functor of c and ai, agreement, joint",
        "act",
        "c/ai is an actant, agreement",
        "subj",
        "c/ai is a subject, agreement",
        "Context",
        "parJun",
        "t-functor of the parent of c and c^, agreement, joint",
        "par_pos",
        "t-POS of the parent of c and c^, agreement, joint",
        "parJemma",
        "agreement between the parent's lemma of c and c^, joint",
        "clem_aparlem",
        "joint between the lemma of c and the parent's lemma of ai",
        "c.coord",
        "c is a member of a coordination",
        "app.coord",
        "c and ai are in coordination & ai is a possessive pronoun",
        "sibl",
        "c and ai are siblings",
        "coll",
        "c and ai have the same collocation",
        "cnk_coll",
        "c and ai have the same CNC collocation",
        "tfa",
        "contextual boundness of c and ai, agreement, joint",
        "cJreq",
        "c is a frequent word",
        "Semantics",
        "cand_pers",
        "c is a person name",
        "cand.ewn",
        "semantic position of c's lemma within the EuroWordNet Top Ontology"
      ]
    }
  ]
}
