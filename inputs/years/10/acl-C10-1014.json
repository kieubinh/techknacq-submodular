{
  "info": {
    "authors": [
      "Fan Bu",
      "Xiaoyan Zhu",
      "Ming Li"
    ],
    "book": "COLING",
    "id": "acl-C10-1014",
    "title": "Measuring the Non-compositionality of Multiword Expressions",
    "url": "https://aclweb.org/anthology/C10-1014",
    "year": 2010
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Fan Bu and Xiaoyan Zhu",
        "State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Sci.",
        "and Tech., Tsinghua University",
        "buf08@mails.tsinghua.edu.cn and zxy-dcs@tsinghua.edu.cn",
        "David R. Cheriton School of Computer Science University of Waterloo",
        "mli@uwaterloo.ca",
        "Multiword Expressions (MWEs) appear frequently and ungrammatically in the natural languages.",
        "Identifying MWEs in free texts is a very challenging problem.",
        "This paper proposes a knowledge-free, training-free, and language-independent Multiword Expression Distance (MED).",
        "The new metric is derived from an accepted physical principle, measures the distance from an n-gram to its semantics, and outperforms otherstate-of-the-art methods on MWEs in two applications: question answering and named entity extraction."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "A Multiword Expression (MWE) is a sequence of neighboring words \"whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components\" (Choueka, 1988).",
        "In the paper, MWEs refer to non-compositional lexical units including idioms, terminologies and name entities.",
        "As Jackendoff (1997) notes, the magnitude of MWEs is far greater than what has traditionally been realized within linguistics.",
        "He estimates that the number of MWEs in a speaker's lexicon is of the same order of magnitude as the number of single words.",
        "In WordNet 1.7 (Fellbaum, 1998), 41 percent of the entries are multi-words.",
        "Some specialized domain vocabulary, such as terminology, overwhelmingly consists of MWEs.",
        "Automatic extraction of MWEs is indispensable to many tasks such as machine translation, name entity extraction, information retrieval and question answering.",
        "Due to their non-compositionality, many MWEs cannot be directly identified using grammatical rules, which poses a major challenge to automatic analysis.",
        "Moreover, existing resources like dictionaries can never have adequate and timely coverage.",
        "Therefore people turn to statistical method to characterize MWEs.",
        "Since Church and Hanks (1990) proposed Pointwise Mutual Information (PMI), a variety of measures, such as Log-likelihood, Symmetrical Conditional Probability (SCP) and Mutual Expectation (Dias et al., 2000), have been introduced to measure word association.",
        "Their basic ideas are very similar: the whole n-gram is separated into two parts and the association is determined by the joint probability and the probability of each part.",
        "Pecina (2006) compared 84 bi-gram association measures and found PMI has the best performance in Czech data.",
        "When applying these measures to the n-grams for n > 2, it is not clear how can the association between the deliberately separated two parts represent the non-compositionality of the whole n-gram.",
        "Different policies have been studied to extend these measures into arbitrary n-grams (Silva and Lopes, 1999; Schone and Juraf-sky, 2001; Dias et al., 2000).",
        "Is there a fundamental, less arbitrary, and general approach to this problem?",
        "That is,",
        "• Can we actually derive a MWE metric for n-grams from the first principles, instead of making a seemingly sensible, but really arbitrary, proposal?",
        "• Will such a theoretically justified new metric actually works better than other heuristic",
        "measures for general MWEs?"
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "This paper will answer above questions positively.",
        "We derive an optimal distance metric Multiword Expression Distance (MED).",
        "MED deines the semantic function for n-grams and the information distance (Bennett et al., 1998) from the n-grams to their semantics.",
        "Unlike previous methods it ensures the cohesion of the n-gram directly hence applicable to MWEs of any length.",
        "The MED is naturally generalized to its conditional version.",
        "The extension is based on the observation that many MWEs are domain dependent.",
        "It is true that some MWEs are only used in certain domains, but they are domain free.",
        "For example, we know that \"polymerase chain reaction\" is some sort of terminology even if many of us do not know what it is exactly.",
        "However that is not always the case.",
        "For those who do not watch movies, the sentence \"catch me if you can\" will probably be taken as a non-MWE, instead of a movie name.",
        "The non-compositionality of this sentence appears only in the movies domain.",
        "The experimental results show that given appropriate phrases as conditions, the conditional MED performs better than MED.",
        "We also investigate the eficacy of MED on post-processing of Question Answering (QA) and complex named entity extraction.",
        "The experimental results show that our method outperforms state of art methods (Zhang et al., 2009; Downey et al., 2007) in these two applications.",
        "Moreover, MED is a pure statistical metric which can be easily combined with other methods.",
        "The remainder of this paper is organized as follows: In the next section we review the related work on Multiword Expression and information distance.",
        "Section 3 gives a preliminary introduction to Kolomogorovcomplexity and information distance.",
        "Section 4 proposes the formal deinition of MED.",
        "In Section 5 we discuss the difference between MED and Pointwise Mutual Information.",
        "We apply MED to QA post-processing and complex named entity extraction in Section 6 and evaluate their performance in Section 7.",
        "In the last section we conclude this work.",
        "Researchers have explored various techniques for identifying MWEs.",
        "These approaches could be broadly classiied into three types: linguistic methods, sequential tagging based methods and statistical methods.",
        "The mostly used linguistic information for MWE extraction is words' Part-Of-Speech tags.",
        "Justeson and Katz (1995) extracted technical terminologies from documents using a regular expression on POS-tags of a word sequence, together with some frequency constraints.",
        "Arga-mon et al.",
        "(1998) separated the POS sequence of a multi-word into small POS tiles, counted tile frequency in the MWE and non-MWE training sets and identify new MWEs by these counts.",
        "Although linguistic methods perform well in term extraction on speciic domains, it cannot be generalized to identify arbitrary MWEs.",
        "Several supervised learning methods have been used previously for extracting Name Entities including Hidden MarkovModels, Maximum Entropy MarkovModels and Conditional Random order to allow tractable computation, these models can only use local features in a small window.",
        "Although the approximate inference methods have been incorporated into sequential tagging model to capture non-local information (Finkel et al., 2005), these models are not capable of recognizing complex named entities, especially those containing conjunctions and prepositions.",
        "Experimental results in (Downey et al., 2007) show that statistical methods substantially outperform sequential tagging based methods on identifying complex named entities.",
        "In statistical methods for MWE extraction, Church and Hanks (1990) irst presented Point-wise Mutual Information (PMI) as an objective measure for estimating word association.",
        "Since then, many methods has been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993) and Symmetrical Conditional Probability (Silva and Lopes, 1999).",
        "Pecina (2006) compared 84 bi-gram association measures and concluded that PMI had the best performance in Czech data.",
        "When it comes to measure the non-compositionality for arbitrary n-grams, policies were taken to separate n-gram into two parts X and Y so that it can be measured by existing bi-gram methods (such as PMI).",
        "Silva and Lopes (1999) and Dias et al.",
        "(2000) calculated the arithmetic average of every possible separation.",
        "Schone and Jurafsky (2001) define X and Y to be the word sequences w\\Wi...Wi and iyj+lWJj+2-\"'U'n,where i is chosen to maximize PxPy.",
        "Recently Zhang et al.",
        "(2009) proposed Enhanced Mutual Information (EMI) which measured the cohesion of n-gram by the frequency of itself and the frequency of each word.",
        "The information distance is a universal distance measure between two information carrying entities (Bennett et al., 1998; Li et al., 2001; Li et al., 2004).",
        "The applications of information distance using compression were irst introduced in (Li et al., 2001) and then in (Bennett et al., 2003; Chen et al., 2004).",
        "The experimental results in (Keogh et al., 2004) showed that information distance/compression based method was superior to 51 parameter-laden methods from seven major data mining conferences on their benchmark data.",
        "The web-based approximation of information distance was introduced by Cilibrasi and Vitânyi (2007) to measure the semantic similarity of two words or concepts."
      ]
    },
    {
      "heading": "3. Preliminaries",
      "text": [
        "Kolmogorovcomplexity deines randomness of an individual string.",
        "Fix a universal Turing machine {7,the Kolmogorov complexity of a binary string a; condition to another binary string y Ku(x\\y) is defined as the length of the shortest (prefix-free) program for U that outputs a: with input y.",
        "It can be shown that for a different universal Turing machine U', for all x, y",
        "Between any two information carrying entities, is there an objective distance that is application-independent and unique, similar to the concept of distance in the physical world?",
        "From a commonly accepted physical principle of von Neumann and Landauer that irreversibly processing one bit of information costs 1KT of energy, Bennett et al.",
        "(1998) derived exactly such a distance: the Information Distance.",
        "Information Distance E(x,y) between two objects x and y is the energy to convert between x and y. Bennett et al.",
        "Theorem 1 Up to an additive logarithmic term, E{x,y) = max{K(x\\y),K(y\\x)}.",
        "Thus, the max distance was deined below",
        "Dmax{x,y) = max{K(x\\y),K(y\\x)}.",
        "Dmax was shown to satisfy distance requirements such as positivity, symmetricity and triangle inequality (Bennett et al., 1998).",
        "It was further shown that Dmax is optimal in the sense that it is universal.",
        "That is, it minorizes (up to constant factors) all other nontrivial and computable distances.",
        "More precisely, a distance D is admissible",
        "where the constant C depends only on U'.",
        "Thus, we can simply write Ku{x\\y) as K{x\\y) and K(x\\e) as K(x), where e is the empty string.",
        "Thus, we exclude trivial distances such as d(x, y) = 0 for all x, y.",
        "It was proved in (Bennett et al., 1998) that for any admissible computable distance D, there is a constant c, for all x,y,",
        "Dmax{x,y) < D(x,y) + c.",
        "In other words, if any such distance D discovers some similarity between x and y,so will Dmax.",
        "In order to deal with the information carrying objects of different sizes, the normalized information distance was proposed in (Li et al., 2001).",
        "In (Li et al., 2004), the normalized max distance was deined as:",
        "dmax satisfies positivity, symmetricity, triangle inequality and some weak form ofuniversality (Li et",
        "al., 2004)."
      ]
    },
    {
      "heading": "4. A New Metric for MWE",
      "text": [
        "When applying the Information Distance to identifying MWEs, how to encode n-grams and their semantics is the irst thing to be considered.",
        "It is inappropriate to encode MWEs literally.",
        "For example, when referring to \"kick the bucket\", the three words \"kick\", \"the\" and \"bucket\" cannot represent all the semantics about this expression.",
        "Inspired by Cilibrasi and Vitanyi (2007), we define context of an n-gram as the set of all the web pages containing it.",
        "Also, semantic of an n-gram is deined as the set of all the web pages containing all the words appeared in that n-gram.",
        "For example, the semantic of \"U.S. president\" including not only the pages containing itself but also those containing \"the president of U.S.\" or \"president Obama says that... U.S. government...\".",
        "Let us denote the vocabulary set by S and the set of web pages by fi.",
        "The cardinality of fi is denoted by M=|fi|.Defme G = S+ as the set of n-grams.",
        "A search term t is defined as an n-gram or the conjunction of search terms.",
        "Denote T as the set of search terms and we have G C T.Let <j> : T – > 2n be the context function mapping each search term t to the web set which includes (and only includes) all the web pages containing all the n-grams in t.Let 9 : G – > T be the function mapping each n-gram g = w\\w2...wn to A^u^fhe conjunction of the words in it.",
        "Finally we define the semantic function fj, : G – ► 2n as the composite function (j)o6.",
        "It is obvious that for any n-gram <7,wehave 0(g) Ç /i(g).",
        "Given an n-gram g,we will encode <j>(g) and fi{g) and calculate the distance between them.",
        "While K(x) is not computable, a simple heuristic, noticed by Cilibrasi and Vitanyi (2007), is to use Shannon-Fano code to encode the probability (approximated by its internet frequency) of a;.As-sume that all web pages are equiprobable, with the probability of being returned by search engine being jL.Letp : <j)(T) – t [0,1] be the context probability function in which <j)(T) = {x\\3y E T,x = <j){y)}.",
        "Since each context is a set of webpages, the probability of context c is defined as p(c) = ^ where N = ^2ce(f,(x) \\c\\ ensures p is a valid probability function.",
        "The Shannon-Fano code (Li and Vitdnyi, 2008) length associated with p can then be regarded as an approximation of K,",
        "According to (3),(4) and Theorem 1, Dmax can be approximated as follows:",
        "Dmaxi^i y)",
        "Similarly, we have",
        "Since <f>(g) Ç /j,(g), the Multiword Expression Distance of an n-gram g can be defined as follows:",
        "K max<loë i0(JfnMg)PlQs i0(J)MnMg)i>",
        "Given a search term c as condition, the Conditional Multiword Expression Distance of an n-gram g is defined as follows:",
        "Based normalized information distance, NMED and its conditional version can be derived as follows:",
        "Where N can be estimated from the size of internet by some combinatorial methods.",
        "To implement MED by a general search engine, we assume f2 to be the set of indexed webpages.",
        "Thus, |</>(<7)| and \\(J,(g)\\ can be approximated by the hit numbers given g and the \"logic and\" of eachwordin g as queries.",
        "Yahoo Search is used in our experiments."
      ]
    },
    {
      "heading": "5. Relation with Pointwise Mutual Information",
      "text": [
        "When n = 2, we denote Y,{w\\w2) the probability of a web page containing bi-gram g = w\\Wi and P(ioi /\\w2) the probability of a web page containing w\\ and w2.",
        "Assuming the occurrence of w\\ and w2 are independent, we have",
        "Thus, PMI is inversely proportional to MED under the independence assumption.",
        "This assumption is unadvisable for obvious reasons.",
        "PMI compares the probability of observing a: and y within a given window id (iu=2 when measuring collocation) with the probabilities of observing x and y independently.",
        "However, most of the word sequences in practice (both MWEs and non-MWEs) are far from being independent.",
        "Therefore the assumption potentially creates additional noises to MED, especially when n > 2.",
        "The internet contains billions of pages and thus we can count the pages containing speciied words directly without making independent assumption to overcome data sparseness."
      ]
    },
    {
      "heading": "6. Applications",
      "text": [
        "Some types of questions require a QA system to return phrases as the answers instead of sentences, such as Factoid and List.",
        "Given a question, we need to generate queries, obtain relevant pages from the internet, extract the candidate n-grams from relevant pages and inally rank all the candidates by their likelihood of being an answer.",
        "Some previous work exploited web redundancy to estimate answer validity(Magnini et al., 2002; Zhang et al., 2008).",
        "No research, to our knowledge, has focused on checking the completeness of candidates.",
        "Most of texts on the internet are informal (e.g. they contain uncapitalized proper nouns and incomplete sentence structures).",
        "Parser and named entity recognizers trained on formal corpus are unpractical on recognize NP chunks or name entities on the web.",
        "Observing that each candidate is n-gram and checking the completeness of a candidate is to measure its non-compositionality, we introduce a simple MWEs-based method to rank all candidates by their completeness and merge similar answers.",
        "Given a question and a list of candidate answers:",
        "1.",
        "Extract proper nouns from the question as conditions."
      ]
    },
    {
      "heading": "2.. Calculate the conditional MED (or MED if",
      "text": [
        "no proper noun is found in question) for each candidate.",
        "Then for each pair of literally similar candidates, the one with larger MED distance is removed."
      ]
    },
    {
      "heading": "3.. Rank the rest candidates by conditional",
      "text": [
        "MED.",
        "This method is case insensitive and do not rely on context information.",
        "All of the statistics are performed on the internet thus no local corpus is needed.",
        "In many previous work (McCallum and Li, 2003; Finkel et al., 2005), named entity extraction is combined with classiication, which is known as Name Entity Recognition (NER).",
        "Most of these NER technique are based on sequential tagging models and unsuitable to the task of locating complex named entities in Web text.",
        "In (Downey et al., 2007), the author treated named entity as a type of MWE and proposed the algorithm LEX++ to locate complex named entities.",
        "Inspired by Downey's work, we propose a conditional MED based algorithm MWE++ to extract named entities.",
        "Given a sentence S = {Si, S2---Sn} and parameters t\\,t2 and ö, MWE++ proceeds as follows:",
        "1.",
        "Initialize a sequence of names N = (ni, ri2, um) equal to the maximal contiguous substrings of S that consist entirely of capitalized words.",
        "If the first word of S appears capitalized in the local corpus and it is at the beginning of a sentence more than S of the times, it is omitted from N."
      ]
    },
    {
      "heading": "2.. Until N does not change during last iteration:",
      "text": [
        "(a) Choose the mergeable pair of names (rij, nj+i) with minimum conditional MED.",
        "(b) Replace nmirH and nmini+1 with the single name nminiwmininmini+\\ where wtis the uncapitalized words between nj and rii+v"
      ]
    },
    {
      "heading": "3.. For every names n, in N",
      "text": [
        "(a) Check common prefix and punctuation at boundary of n» via local corpus.",
        "(b) Check number at boundary of nj via internet.",
        "In MWE++, We define two thresholds Ti and T2 to estimate the name entity conidence of a given n-gram.",
        "If MED(<7|.)",
        "is lower than t\\, between t\\ and T2 or higher than T2, conf (g) will be 2 (Definitely), 1 (Probably) or 0 (Impossible).",
        "The con-idence of all initialized capitalized words will be set to 1.",
        "If an n-gram contain unmatched brackets or quotation marks, its confidence will be set to 0.",
        "Also, The confidence of n-gram containing comma will be reduced by 1.",
        "We say a pair of names (nj,nj+i) is mergeable if and only if conf(nitüini_|_i) > max(conf(nj), conf(nj+i)).",
        "After iteration, we will check common preixes, punctuations and numbers at boundary of each names.",
        "If a name nj is immediately preceded by a single number £ and conf(tn,) > 1, we replace rii by inj.",
        "Similarly, a number i immediately following nj is appended to nj when conf(njt) > 1.",
        "Due to the limitation of search engine, punctuation check and common preix check modules are performed on local corpus just the same as LEX++."
      ]
    },
    {
      "heading": "7. Experiments and Analysis",
      "text": [
        "In this section, we evaluate how well can MED separate non-compositional phrases (idioms) from compositional ones.",
        "First we evaluate MED and other four metrics on English.VPC data published on the MWE 2008 shared task.",
        "The data set contains 3078 verb-noun bi-grams and 14 percent of them are annotated as idiomatic.",
        "The average precision of MED, PMI, SCP, t-score and EMI (Zhang et al., 2009) are 0.234, 0.233, 0.285, 0.274 and 0.205.",
        "The result shows that MED is not distinguished on bi-grams test.",
        "It is partly because most idiomatic verb-noun collocations are often used non-idiomatically.",
        "Their compositionality are not necessarily lower than non-idiomatic ones.",
        "We also evaluate different metrics on n-grams of varied lengths.",
        "Since all published MWE data sets we ind only contain bi-grams, we construct our test set as follows.",
        "We irst collected common idioms from the lists of english idioms on Wikipedia.",
        "To get enough common but not idiomatic phrases, we collect common compositional phrases from UsingEnglish.com, englishs-peak.com, Wikipedia and China Daily BBS.",
        "Since it is dificult for non-native speakers to pick up idioms from non-idiomatic ones, we do not manually check all compositional phrases.",
        "The test set contains 1529 idioms and 1798 compositional phrases.",
        "The n-gram frequencies are not sig-niicantly different between idioms and compositional phrases.",
        "The mean and standard deviation are 2.1 x 10 and 7.8 x 10 on idioms and 7.4 x 10 and 4.8 x 10 on compositional phrases.",
        "We employ different measures to rank all the phrases.",
        "Non-conditional MED and NMED are compared with AVG-SCP (Silva and Lopes, 1999), MAX_PMI (Schone and Jurafsky, 2001), EMI (Zhang et al., 2009) and the baseline n-gram frequency.",
        "T-score is not under evaluation because we do not find sound n-gram extension for it.",
        "The precision-recall curve is shown in Fig. 1.",
        "Since the performance of MED and NMED are very close, NMED is not displayed for clarity.",
        "From the result we can see that MED performs substantially better than all the other measures.",
        "Average precision(avp) of the top 3 measures MED, EMI and AVG-SCP are 0.75, 0.71 and 0.66.",
        "^ ___ ;",
        "cision",
        ">^",
        "£ /",
        "Q.",
        "/",
        " – MED",
        " – Frequency _ – MAX PMI",
        ".....AVG SCP",
        " – EMI",
        "Recall",
        "It is dificult to evaluate the method introduced in Section 6.1 directly since QA benchmarks mainly focus on accuracy of the top one answer instead of the completeness of top-n candidates.",
        "Therefore, the experiment is designed as follows.",
        "We extract name lists on different domains from Wikipedia.",
        "For each name in each list, we put it into a search engine and get the context from a random selected snippet.",
        "For each name, We created two incomplete names by randomly adding (or removing) one or two words according to its context.",
        "It is guaranteed that the original name and its counterpart with noise must have at least two words in common.",
        "We tag the original names and the noise added ones in each list as positive and negative samples.",
        "A list can be regarded as the candidates and the list name (or its synonym) can be seen as the key phrase extracted from question.",
        "The test set can be divided into six common categories: movie, book, music, person, organization and video game.",
        "Each category contains one to four lists.",
        "The test set contains 11080 samples in total.",
        "Still, we employ the measures in previous experiments to rank all the candidates to see if the complete names can be separated from the incomplete names.",
        "The results are listed in Table 1.",
        "The overall avp is the average of the avp of each lists weighted by their size.",
        "It is shown that the performance of conditional MED is the best over all metrics, followed by MED.",
        "The reason why EMI and AVGSCP get best results on soccer player and novelists is that they take more advantage of frequency.",
        "Since the length of people's name are short (2 to 3 words), most of negative samples are created by adding words, which makes frequency important.",
        "In this section we evaluate the named entity extraction performance of Algorithm MWE++.",
        "The experiment is done on the corpus, the training set and the test set provided by Downey et al.",
        "(2007).",
        "Four classes of entities (Actor, Book, Company and Film) were manually annotated on both training and test set.",
        "All sentences in the corpus contain named entities from the above four classes (but not annotated).",
        "The corpus consists of 183,726 sentences while the training and the test set contain 200 and 629 sentences, respectively.",
        "Furthermore, test sentences are separated into 100 dificult cases and 529 easy cases.",
        "All dificult cases contain complex name entities (entities containing uncapitalized words), such as \"Procter and",
        "freq",
        "MAX_PMI",
        "AVGJ3CP",
        "EMI",
        "NMED",
        "MED",
        "MED(.|.)",
        "fairy tale",
        "0.493",
        "0.484",
        "0.570",
        "0.515",
        "0.615",
        "0.617",
        "0.657",
        "science fiction",
        "0.500",
        "0.470",
        "0.558",
        "0.525",
        "0.596",
        "0.599",
        "0.633",
        "action movie",
        "0.695",
        "0.523",
        "0.723",
        "0.703",
        "0.763",
        "0.768",
        "0.823",
        "animation",
        "0.561",
        "0.642",
        "0.693",
        "0.489",
        "0.671",
        "0.673",
        "0.689",
        "horror movie",
        "0.595",
        "0.528",
        "0.647",
        "0.633",
        "0.667",
        "0.670",
        "0.692",
        "documentary",
        "0.525",
        "0.549",
        "0.626",
        "0.512",
        "0.596",
        "0.598",
        "0.654",
        "hip hop",
        "0.598",
        "0.627",
        "0.645",
        "0.635",
        "0.652",
        "0.651",
        "0.712",
        "jazz",
        "0.549",
        "0.501",
        "0.543",
        "0.539",
        "0.627",
        "0.625",
        "0.716",
        "rock&roll",
        "0.742",
        "0.567",
        "0.730",
        "0.741",
        "0.708",
        "0.717",
        "0.836",
        "company",
        "0.614",
        "0.584",
        "0.689",
        "0.663",
        "0.754",
        "0.756",
        "0.735",
        "soccer player",
        "0.945",
        "0.648",
        "0.904",
        "0.973",
        "0.911",
        "0.918",
        "0.941",
        "novelists",
        "0.772",
        "0.701",
        "0.870",
        "0.866",
        "0.821",
        "0.828",
        "0.864",
        "PS3 game",
        "0.603",
        "0.675",
        "0.740",
        "0.535",
        "0.742",
        "0.744",
        "0.727",
        "overall",
        "0.612",
        "0.577",
        "0.688",
        "0.629",
        "0.696",
        "0.700",
        "0.726",
        "Gamble\" and \"Gone with the Wind\".",
        "The conditional MED metric in this experiment is redefined as follows:",
        "where C={\"IMDB\",\"Amazon\",\"corporation\"}.",
        "\"IMDB\" is used as the condition of Actor and Film while \"Amazon\" and \"corporation\" are chosen to be the condition of Book and Company.",
        "We compute the conditional MED for all entities on training set.",
        "t\\ is set to the median and t<i is set to the value larger than 90% entities on training set.",
        "is set to 0.5.",
        "MWE++ is performed on the 100 dificult cases.",
        "The results shown in Table 2 convincingly show that MWE++ significantly outperforms LEX++, supervised models (SVMCMM, CRF) and rule-based model (MAN) on identifying complex named entities.",
        "Compared to LEX++, MWE++ is not only more accurate but also more flexible.",
        "LEX++ relies on local corpus while MWE++ does not.",
        "When recognizing new entities, we just need to find appropriate condition words instead of preparing new corpus.",
        "For the sake of completeness, the F-score of MWE++ on easy cases is 91, which is lower than all the other methods.",
        "However this is irrelevant since this part can be made quite accurate by specialized databases and training by any known methods.",
        "All test data in this paper can be downloaded from http://60.195.250.61:8080/download/."
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "We have derived an MWE metric MED from the irst principles via Information Distance.",
        "The new metric measures the distance from an n-gram to its semantics.",
        "It is provably optimal (universal), overcomes several deiciencies of previous approaches, and convincingly outperforms the other methods.",
        "Also, we have taken advantage of the fact that some MWEs are domain dependent.",
        "This feature is important when recognizing named entities and terminologies.",
        "The conditional MED is better than MED when we know what we are looking for.",
        "Since MED is quite different from previous measures, it can be combined with others by machine learning approaches and enhance the overall performance.",
        "Further experiments are needed.",
        "Acknowledgment",
        "This work was supported mainly by Canada's IDRC Research Chair in Information Technology program, Project Number: 104519-006.",
        "It is also supported by the Chinese Natural Science Foundation grant No.",
        "60973104, NSERC",
        "Grant OGP0046506, 863 Grant 2008AA02Z313 fromChina's MinistryofScienceandTechnology, Canada Research Chair program, MITACS, an NSERC Collaborative Grant, and Ontario's Premier's Discovery Award.",
        "Fx",
        "Recall",
        "Precision",
        "MAN",
        "0.18",
        "0.22",
        "0.16",
        "CRF",
        "0.35",
        "0.42",
        "0.31",
        "SVMCMM",
        "0.42",
        "0.48",
        "0.37",
        "LEX++",
        "0.74",
        "0.76",
        "0.72",
        "MWE++",
        "0.83",
        "0.86",
        "0.80"
      ]
    }
  ]
}
