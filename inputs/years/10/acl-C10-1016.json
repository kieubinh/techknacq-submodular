{
  "info": {
    "authors": [
      "Xiaoyan Cai",
      "Wenjie Li",
      "You Ouyang",
      "Hong Yan"
    ],
    "book": "COLING",
    "id": "acl-C10-1016",
    "title": "Simultaneous Ranking and Clustering of Sentences: A Reinforcement Approach to Multi-Document Summarization",
    "url": "https://aclweb.org/anthology/C10-1016",
    "year": 2010
  },
  "references": [
    "acl-C00-1072",
    "acl-C08-1087",
    "acl-H05-1115",
    "acl-N06-2046",
    "acl-P09-1083"
  ],
  "sections": [
    {
      "text": [
        ":Xiaoyan Cai, :Wenjie Li, :You Ouyang, Hong Yan",
        "department of Computing, The Hong Kong Polytechnic University",
        "{csxcai,cswjli,csyouyang}@comp.polyu.edu.hk",
        "lgthyan@polyu.edu.hk",
        "Multi-document summarization aims to produce a concise summary that contains salient information from a set of source documents.",
        "In this field, sentence ranking has hitherto been the issue of most concern.",
        "Since documents often cover a number of topic themes with each theme represented by a cluster of highly related sentences, sentence clustering was recently explored in the literature in order to provide more informative summaries.",
        "Existing cluster-based ranking approaches applied clustering and ranking in isolation.",
        "As a result, the ranking performance will be inevitably influenced by the clustering result.",
        "In this paper, we propose a reinforcement approach that tightly integrates ranking and clustering by mutually and simultaneously updating each other so that the performance of both can be improved.",
        "Experimental results on the DUC datasets demonstrate its effectiveness and robustness."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Automatic multi-document summarization has drawn increasing attention in the past with the rapid growth of the Internet and information explosion.",
        "It aims to condense the original text into its essential content and to assist in filtering and selection of necessary information.",
        "So far extractive summarization that directly extracts sentences from documents to compose summaries is still the mainstream in this field.",
        "Under this framework, sentence ranking is the issue of most concern.",
        "Though traditional feature-based ranking approaches and graph-based approaches employed quite different techniques to rank sentences, they have at least one point in common, i.e., all of them focused on sentences only, but ignored the information beyond the sentence level (referring to Figure 1(a)).",
        "Actually, in a given document set, there usually exist a number of themes (or topics) with each theme represented by a cluster of highly related sentences (Harabagiu and Lacatusu, 2005; Hardy et al., 2002).",
        "These theme clusters are of different size and especially different importance to assist users in understanding the content in the whole document set.",
        "The cluster level information is supposed to have foreseeable influence on sentence ranking.",
        "In order to enhance the performance of summarization, recently cluster-based ranking approaches were explored in the literature (Wan and Yang, 2006; Sun et al., 2007; Wang et al., 2008a,b; Qazvinian and Radev, 2008).",
        "Normally these approaches applied a clustering algorithm to obtain the theme clusters first and then ranked the sentences within each cluster or by exploring the interaction between sentences and obtained clusters (referring to Figure 1(b)).",
        "In other words, clustering and ranking are regarded as two independent processes in these approaches although the cluster-level information has been incorporated into the sentence ranking process.",
        "As a result, the ranking performance is inevitably influenced by the clustering result.",
        "Ranking",
        "Ranking",
        "Ranking",
        "t",
        "t",
        "Clustering",
        "Clustering",
        "t",
        "t",
        "(a)",
        "(b)",
        "(c)",
        "To help alleviate this problem, we argue in this paper that the quality of ranking and clustering can be both improved when the two processes are mutually enhanced (referring to Figure 1(c)).",
        "Based on it, we propose a reinforcement approach that updates ranking and clustering interactively and iteratively to multi-document summarization.",
        "The main contributions of the paper are three-fold: (1) Three different ranking functions are defined in a bi-type document graph constructed from the given document set, namely global, within-cluster and conditional rankings, respectively.",
        "(2) A reinforcement approach is proposed to tightly integrate ranking and clustering of sentences by exploring term rank distributions over the clusters.",
        "(3) Thorough experimental studies are conducted to verify the effectiveness and robustness of the proposed approach.",
        "The rest of this paper is organized as follows.",
        "Section 2 reviews related work in cluster-based ranking.",
        "Section 3 defines ranking functions and explains reinforced ranking and clustering process and its application in multi-document summarization.",
        "Section 4 presents experiments and evaluations.",
        "Section 5 concludes the paper."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Clustering has become an increasingly important topic with the explosion of information available via the Internet.",
        "It is an important tool in text mining and knowledge discovery.",
        "Its ability to automatically group similar textual objects together enables one to discover hidden similarity and key concepts, as well as to summarize a large amount of text into a small number of groups (Karypis et al., 2000).",
        "To summarize a scientific paper, Qazvinian and Radev (2008) presented two sentence selection strategies based on the clusters which were generated by a hierarchical agglomeration algorithm applied in the citation summary network.",
        "One was called C-RR, which started with the largest cluster and extracted the first sentence from each cluster in the order they appeared until the summary length limit was reached.",
        "The other was called",
        "C-LexRank, which was similar to C-RR but adopted LexRank to rank the sentences within each cluster and chose the most salient one.",
        "Meanwhile, Wan and Yang (2008) proposed two models to incorporate the cluster-level information into the process of sentence ranking for generic summarization.",
        "While the Cluster-based Conditional Markov Random Walk model (ClusterCMRW) incorporated the cluster-level information into the text graph and manipulated clusters and sentences equally, the Cluster-based HITS model (ClusterHITS) treated clusters and sentences as hubs and authorities in the HITS algorithm.",
        "Besides, Wang et al.",
        "(2008) proposed a language model to simultaneously cluster and summarize documents.",
        "Nonnegative factorization was performed on the term-document matrix using the term-sentence matrix as the base so that the document-topic and sentence-topic matrices could be constructed, from which the document clusters and the corresponding summary sentences were generated simultaneously."
      ]
    },
    {
      "heading": "3. A Reinforcement Approach to Multi-document Summarization",
      "text": [
        "First of all, let's introduce the sentence-term bi-type graph model for a set of given documents D, based on which the algorithm of reinforced ranking and clustering is developed.",
        "Let G =< V, E,W > , where V is the set of vertices that consists of the sentence set S = {s1, s2,..., sn} and the term set T = t2,...,tm }, i.e., V = SU T, E is the set of edges that connect the vertices, i.e., E = {< Vj,Vj >| Vj,Vj e V} .",
        "W is the adjacency matrix in which the element Wj represents the weight of the edge connecting Vj and Vj .",
        "Formally, W can be decomposed into four blocks, i.e., WSS , WST , WTS and WTT , each representing a sub-graph of the textual objects indicated by the subscripts.",
        "W can be written as",
        "W JWss Wst \\ [Wts Wtt j ' where WST (j, j) is the number of times the term t,- appears in the sentence Sj.",
        "WSS is",
        "the number of common terms in the sentences and Sj .",
        "WTS is equal to wSTT as the relationships between terms and sentences are symmetric.",
        "For simplification, in this study we assume there is no direct relationships between terms, i.e., WTT = 0 .In the future, we will explore effective ways to integrate term semantic relationships into the model.",
        "Recall that our ultimate goal is sentence ranking.",
        "As an indispensable part of the approach, the basic ranking functions need to be defined first.",
        "m) denote the ranking scores of the sentence si",
        "and the term t j in the whole document set, respectively.",
        "Based on the assumptions that",
        "\"Highly ranked terms appear in highly ranked sentences, while highly ranked sentences contain highly ranked terms.",
        "MoreoVer, a sentence is ranked higher if it contains many terms that appear in many other highly ranked sentences.\"",
        "we define",
        "For calculation purpose, r(si) and r(tj) are normalized by",
        "r(si) and r(si ) and r(t, ) <---± – .",
        "Equations (1) and (2) can be rewritten using the matrix form, i.e.,",
        "We call r(S) and r(T) the \"global ranking functions\", because at this moment sentence clustering is not yet involved and all the sentences/terms in the whole document set are ranked together.",
        "Theorem: The solution to r(S) and r(T) given by Equation (3) is the primary eigenvector of X ■ Wst ■ Wts + (1 - X) ■ Wss and",
        "As the iterative process is a power method, it is guaranteed that r(S) converges to the primary eigenvector of X ■ WsT ■ WTs + converge to the primary eigenvector of",
        "3.2.2 Local Ranking (within Clusters) Assume now K theme clusters have been generated by certain clustering algorithm, denoted as C = {C1,C2,...,CK } where Ck (k=1, 2, ..., K) represents a cluster of highly related sentences SCk (e Ck ) which contain the terms TCk (e Ck ) .",
        "The sentences and terms within the cluster Ck form a cluster bi-type graph with the adjacency matrix WCk .",
        "Let rCk (SCk ) and rCk (TCk ) denote the ranking scores of SCkand TCk within Ck .",
        "They are calculated by an",
        "equation similar to Equation (3) by replacing the document level adjacency matrix W with the cluster level adjacency matrix WCk .",
        "We call rCk (SCk ) and rCk (TCk ) the \"within- cluster ranking functions\" with respect to the cluster Ck .",
        "They are the local ranking functions, in contrast to r(S) and r(T) that rank all the sentences and terms in the whole document set D. We believe that it will benefit sentence overall ranking when knowing more details about the ranking results at the finer granularity of theme clusters, instead of at the coarse granularity of the whole document set.",
        "3.2.3 Conditional Ranking (across Clusters) To facilitate the discovery of rank distributions of terms and sentences over all the theme clusters, we further define two \"conditional ranking functions\" r (S | Ck ) and r(T | Ck ) .",
        "These rank distributions are necessary for the parameter estimation during the reinforcement process introduced later.",
        "The conditional ranking score of the term tj on the cluster Ck ,",
        "i.e., r (T | Ck ) is directly derived from TCk , i.e., r (tj | Ck ) = rCk (tj ) if tj e Ck, and r(tj | Ck ) = 0 otherwise.",
        "It is further normalized as",
        "r(t; | Ck ) , .",
        "Then the conditional ranking score of the sentence si on the cluster Ck is deduced from the terms that are included in si , i.e.,",
        "Equation (5) can be interpreted as that the conditional rank of si on Ck is higher if many terms in si are ranked higher in Ck.",
        "Now we have sentence and term conditional ranks over all the theme clusters and are ready to introduce the reinforcement process.",
        "The conditional ranks of the term t j across the",
        "K theme clusters can be viewed as a rank distribution.",
        "Then the rank distribution of the sentence si can be considered as a mixture model over K conditional rank distributions of the terms contained in the sentence si .",
        "And the sentence si can be represented as a K-dimensional vector in the new measure space, in which the vectors can be used to guide the sentence clustering update.",
        "Next, we will explain the mixture model of sentence and use EM algorithm (Bilmes, 1997) to get the component coefficients of the model.",
        "Then, we will present the similarity measure between sentence and cluster, which is used to adjust the clusters that the sentences belong to and in turn modify within-cluster ranking for the sentences in the updated clusters.",
        "For each sentence si , we assume that it follows the distribution r(T | si ) to generate the relationship between the sentence si and the term set T. This distribution can be considered as a mixture model over K component distributions, i.e. the term conditional rank distributions across K theme clusters.",
        "We use Y\\ik to denote the probability that s{ belongs to Ck , then r(T | si ) can be modeled as:",
        "Yik can be explained as p(Ck | ) and",
        "calculated by the Bayesian equation p(Ck | si ) x p(s | Ck ) ■ p(Ck ), where p(s, | Ck ) is assumed to be r(si | Ck) obtained from the conditional rank of si on Ck as introduced before and p(Ck) is the prior probability.",
        "We use EM algorithm to estimate the component coefficients j\\lk along with {p(Ck)}.",
        "A hidden variable Cz, z e {1,2,K} is used to denote the cluster label that a sentence term pair (si,tj) are from.",
        "In addition, we make the independent assumption that the probability of si belonging to Ck and the probability of t j belonging to Ck are independent, i.e., p(si,tj | Ck) = | Ck)■ p(tj | Ck ), where p(si, tj | Ck ) is the probability of si and t j both belonging to Ck .",
        "Similarly, p(tj| Ck) is assumed to ber(tj| Ck) .",
        "Let © be the parameter matrix, which is a n x K matrix ©nXK = {ïi,k } (i = 1, – , n; k = 1, – , K ).",
        "The best © is estimated from the relationships observed in the document bi-type graph, i.e., WST and WSS .",
        "The likelihood of generating all the relationships under the parameter © can be calculated as:",
        "where p(si,tj| ©) is the probability that si and t j both belong to the same cluster, given the current parameter.",
        "As p(si, sj | ©) does not contain variables from © , we only need to consider maximizing the first part of the likelihood in order to get the best estimation of © .",
        "Let L(© | WST ) be the first part of likelihood.",
        "Taking into account the hidden variable Cz , the complete log-likelihood can be written as",
        "In the E-step, given the initial parameter © , which is set to yk = jK for all i and k, the expectation of log-likelihood under the current distribution of CZ is:",
        "The conditional distribution in the above equation, i.e., p(Cz = Ck | ,tj,©) , can be calculated using the Bayesian rule as follows:",
        "x p ( si|Ck ) p(tj|Ck ) p(Cz = Ck ) In the M-Step, we first get the estimation of p(Cz = Ck) by maximizing the expectation Q(©, ©) .",
        "By introducing a Lagrange multiplier X, we get the equation below.",
        "Thus, the estimation of p(Cz = Ck) given previous © is",
        "Then, the parameters ji,k can be calculated with the Bayesian rule as",
        "By setting © = ©, the whole process can be repeated.",
        "The updating rules provided in Equations (7)-(9) are applied at each iteration.",
        "Finally © will converge to a local maximum.",
        "A similar estimation process has been adopted in (Sun et al., 2009), which was used to estimate the component coefficients for author-conference networks.",
        "After we get the estimations of the component coefficients y^k for si , si will be represented asa K dimensional vector s, = (yi>1,yi>2,.--, yUK ).",
        "The center of each cluster can thus be calculated accordingly, which is the mean of s, for all st in the same cluster, i.e., where | Ck | is the size of Ck .",
        "Then the similarity between each sentence and each cluster can be calculated as the cosine similarity between them, i.e.,",
        "Finally, each sentence is reassigned to a cluster that is the most similar to the sentence.",
        "Based on the updated clusters, within-cluster ranking is updated accordingly, which triggers the next round of clustering refinement.",
        "It is expected that the quality of clusters should be improved during this iterative update process since the similar sentences under new attributes will be grouped together, and meanwhile the quality of ranking will be improved along with the better clusters and thus offers better attributes for further clustering.",
        "The overall sentence ranking function f is defined as the ensemble of all the sentence conditional ranking scores on the K clusters.",
        "where ak is a coefficient evaluating the importance of Ck .",
        "It can be formulated as the normalized cosine similarity between a theme cluster and the whole document set for generic summarization, or between a theme cluster and a given query for query-based summarization.",
        "ak e [0,1] and £ak = 1.",
        "Figure 2 below summarizes the whole process that determines the overall sentence ensemble ranking scores.",
        "Input: The bi-type document graph G =< S U T, E,W > , ranking functions, the cluster number K, e = 1, Tre = 0.001, IterNum = 10 .",
        "Output: sentence final ensemble ranking vector f (S).",
        "2.",
        "Get the initial partition for S, i.e. C'k, k = 1,2, – K , calculate cluster centers CenterCkt accordingly."
      ]
    },
    {
      "heading": "4.. Calculate the within-cluster ranking r Ck (T Ck ) ,",
      "text": [
        "rCk (SCk ) and the conditional ranking r(si | Ck ) ;",
        "5.",
        "Get new attribute s i for each sentence si , and new attribute Centercl for each cluster Ckt ;"
      ]
    },
    {
      "heading": "6.. For each sentence s. in S",
      "text": []
    },
    {
      "heading": "8.. Calculate similarity value s , m ( s i d )",
      "text": []
    },
    {
      "heading": "9.. End For",
      "text": []
    },
    {
      "heading": "11.. End For",
      "text": []
    },
    {
      "heading": "14.. End For",
      "text": []
    },
    {
      "heading": "15.. For each sentence s i in S",
      "text": []
    },
    {
      "heading": "18.. End For",
      "text": [
        "In multi-document summarization, the number of documents to be summarized can be very large.",
        "This makes information redundancy appears to be more serious in multi-document summarization than in single-document summarization.",
        "Redundancy control is necessary.",
        "We apply a simple yet effective way to choose summary sentences.",
        "Each time, we compare the current candidate sentence to the sentences already included in the summary.",
        "Only the sentence that is not too similar to any sentence in the summary (i.e., the cosine similarity between them is lower than a threshold) is selected into the summary.",
        "The iteration is repeated until the length of the sentences in the summary reaches the length limitation.",
        "In this paper, the threshold is set to 0.7 as always in our past work."
      ]
    },
    {
      "heading": "4. Experiments and Evaluations",
      "text": [
        "We conduct the experiments on the DUC 2004 generic multi-document summarization dataset and the DUC 2006 query-based multi-document summarization dataset.",
        "According to task definitions, systems are required to produce a concise summary for each document set (without or with a given query description) and the length of summaries is limited to 665 bytes in DUC 2004 and 250 words in DUC 2006.",
        "A well-recognized automatic evaluation in evaluation.",
        "It measures summary quality by counting overlapping units between systemgenerated summaries and human-written reference summaries.",
        "We report two common ROUGE scores in this paper, namely ROUGE-1 and ROUGE-2, which base on Uni-gram match and Bi-gram match, respectively.",
        "Documents and queries are preprocessed by segmenting sentences and splitting words.",
        "Stop words are removed and the remaining words are stemmed using Porter stemmer.",
        "In order to evaluate the performance of reinforced clustering and ranking approach, we compare it with the other three ranking approaches: (1) Global-Rank, which does not apply clustering and simply relies on the sentence global ranking scores to select summary sentences; (2) Local-Rank, which clusters sentences first and then rank sentences within each cluster.",
        "A summary is generated in the same way as presented in (Qazvinian and Radev, 2008).",
        "The clusters are ordered by decreasing size; (3) Cluster-HITS, which also clusters sentences first, but then regards clusters as hubs and sentences as authorities in the HITS algorithm and uses the obtained authority scores to rank and select sentences.",
        "The classical clustering algorithm K-means is used where necessary.",
        "For query-based summarization, the additional query-relevance (i.e. the cosine similarity between sentences and query) is involved to re-rank the candidate sentences chosen by the ranking approaches for generic summarization.",
        "Note that K-means requires a predefined cluster number K. To avoid exhaustive search for a proper cluster number for each document set, we employ the spectra approach introduced in (Li et al., 2007) to predict the number of the expected clusters.",
        "Based on the sentence similarity matrix using the normalized 1-norm, for its eigenvalues Xi defined.",
        "If ai -ai+1 > 0.05 and ai is still close to 1, then set K=i+1.",
        "Tables 1 and 2 below compare the performance of the four approaches on DUC 2004 and 2006 according to the calculated K.",
        "It is not surprised to find that \"Global-Rank\" shows the poorest performance, when it utilizes the sentence level information only whereas the other three approaches all integrate the additional cluster level information in various ways.",
        "In addition, as results illustrate, the performance of \"Cluster-",
        "HITS\" is better than the performance of \"Local-Rank\".",
        "This can be mainly credited to the ability of \"Cluster-HITS\" to consider not only the cluster-level information, but also the sentence-to-cluster relationships, which are ignored in \"Local-Rank\".",
        "It is happy to see that the proposed reinforcement approach, which simultaneously updates clustering and ranking of sentences, consistently outperforms the other three approaches.",
        "Our original intention to propose the reinforcement approach is to hope to generate more accurate clusters and ranking results by mutually refining within-cluster ranking and clustering.",
        "In order to check and monitor the variation trend of the cluster quality during the iterations, we define the following measure K min sim(si, Ck) quan = £ ( Rs,eCk-), (12)",
        "k= £ min sim(si, sj ) l=1,l *ksieCk ,sjeCl",
        "where min sim(si, Ck ) denotes the distance between the cluster center and the border sentence in a cluster that is the farthest away from the center.",
        "The larger it is, the more",
        "compact the cluster is.",
        "min sim(si, sj) , on sieCk ,sjeCl the other hand, denotes the distance between the most distant pair of sentences, one from each cluster.",
        "The smaller it is, the more separated the two clusters are.",
        "The distance is measured by cosine similarity.",
        "As a whole, the larger quan means the better cluster quality.",
        "Figure 3 below plots the values of quan in each iteration on the DUC 2004 and 2006 datasets.",
        "Note that the algorithm converges in less than 6 rounds and 5 rounds on the DUC 2004 and 2006 datasets, respectively.",
        "The curves clearly show the increasment of quan and thus the improved cluster quality.",
        "DUC 2004",
        "ROUGE-1",
        "ROUGE-2",
        "Reinforced",
        "0.37082",
        "0.08351",
        "Cluster-HITS",
        "0.36463",
        "0.07632",
        "Local-Rank",
        "0.36294",
        "0.07351",
        "Global-Rank",
        "0.35729",
        "0.06893",
        "Table 1.",
        "Results on the DUC 2004 dataset",
        "DUC 2006",
        "ROUGE-1",
        "ROUGE-2",
        "Reinforced",
        "0.39531",
        "0.08957",
        "Cluster-HITS",
        "0.38315",
        "0.08632",
        "Local-Rank",
        "0.38104",
        "0.08841",
        "Global-Rank",
        "0.37478",
        "0.08531",
        "While quan directly evaluate the quality of the generated clusters, we are also quite interested in whether the improved clusters quality can further enhance the quality of sentence ranking and thus consequently raise the performance of summarization.",
        "Therefore, we evaluate the ROUGEs in each iteration as well.",
        "Figure 4 below illustrates the changes of ROUGE-1 and ROUGE-2 result on the DUC 2004 and 2006 datasets, respectively.",
        "Now, we have come to the positive conclusion.",
        "In previous experiments, the cluster number is predicted through the eigenvalues of 1-norm normalized sentence similarity matrix.",
        "This number is just the estimated number.",
        "The actual number is hard to predict accurately.",
        "To further examine how the cluster number influences summarization, we conduct the following additional experiments by varying the cluster number.",
        "Given a document set, we let S denote the sentence set in the document set, and set K in the following way:",
        "where e e (0,1) is a ratio controlling the expected cluster number.",
        "The larger e is, the more clusters will be produced.",
        "e ranges from 0.1 to 0.9 in the experiments.",
        "Due to page limitation, we only provide the ROUGE-1 and ROUGE-2 results of the proposed approach, \"Cluster-HITS\" and \"Local-Rank\" on the DUC 2004 dataset in Figure 5.",
        "The similar curves are also observed on the 2006 dataset.",
        "It is shown that (1) the proposed approach outperforms \"Cluster-HITS\" and \"Local-Rank\" in almost all the cases no matter how the cluster number is set; (2) the performances of \"Cluster-HITS\" and \"Local-Rank\" are more sensitive to the cluster number and a large number of clusters appears to deteriorate the performances of both.",
        "This is reasonable.",
        "Actually when e getting close to 1, \"Local-Rank\" approaches to \"Global-Rank\".",
        "These results demonstrate the robustness of the proposed approach."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, we present a reinforcement approach that tightly integrates ranking and clustering together by mutually and simultaneously updating each other.",
        "Experimental results demonstrate the effectiveness and the robustness of the proposed approach.",
        "In the future, we will explore how to integrate term semantic relationships to further improve the performance of summarization.",
        "Acknowledgement",
        "The work described in this paper was supported by an internal grant from the Hong Kong Polytechnic University (G-YG80)."
      ]
    }
  ]
}
