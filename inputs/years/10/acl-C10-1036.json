{
  "info": {
    "authors": [
      "Nan Duan",
      "Mu Li",
      "Dongdong Zhang",
      "Ming Zhou"
    ],
    "book": "COLING",
    "id": "acl-C10-1036",
    "title": "Mixture Model-based Minimum Bayes Risk Decoding using Multiple Machine Translation Systems",
    "url": "https://aclweb.org/anthology/C10-1036",
    "year": 2010
  },
  "references": [
    "acl-D07-1105",
    "acl-D08-1065",
    "acl-J04-4002",
    "acl-J07-2003",
    "acl-J97-3002",
    "acl-N04-1022",
    "acl-P03-1021",
    "acl-P06-1066",
    "acl-P07-1040",
    "acl-P08-1023",
    "acl-P08-1067",
    "acl-P09-1019",
    "acl-P09-1064",
    "acl-P09-1065",
    "acl-P09-1066",
    "acl-P09-1107",
    "acl-W02-1021",
    "acl-W04-3250",
    "acl-W05-1506"
  ],
  "sections": [
    {
      "text": [
        "Mixture Model-based Minimum Bayes Risk Decoding using Multiple",
        "Machine Translation Systems",
        "We present Mixture Model-based Minimum Bayes Risk (MMMBR) decoding, an approach that makes use of multiple SMT systems to improve translation accuracy.",
        "Unlike existing MBR decoding methods defined on the basis of single SMT systems, an MMMBR decoder reranks translation outputs in the combined search space of multiple systems using the MBR decision rule and a mixture distribution of component SMT models for translation hypotheses.",
        "MMMBR decoding is a general method that is independent of specific SMT models and can be applied to various commonly used search spaces.",
        "Experimental results on the NIST Chinese-to-English MT evaluation tasks show that our approach brings significant improvements to single system-based MBR decoding and outperforms a state-of-the-art system combination method."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Minimum Bayes Risk (MBR) decoding is becoming more and more popular in recent Statistical Machine Translation (SMT) research.",
        "This approach requires a second-pass decoding procedure to re-rank translation hypotheses by risk scores computed based on model's distribution.",
        "Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the TV-best list translations.",
        "Their work has shown that MBR decoding performs better than Maximum a Posteriori (MAP) decoding for different evaluation criteria.",
        "After that, many dediMu Li, Dongdong Zhang, Ming Zhou",
        "Microsoft Research Asia",
        "cated efforts have been made to improve the performances of SMT systems by utilizing MBR-inspired methods.",
        "Tromble et al.",
        "(2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from V best lists to lattices, and Kumar et al.",
        "(2009) presented more efficient algorithms for MBR decoding on both lattices and hypergraphs to alleviate the high computational cost problem in Tromble et al.",
        "'s work.",
        "DeNero et al.",
        "(2009) proposed a fast consensus decoding algorithm for MBR for both linear and non-linear similarity measures.",
        "All work mentioned above share a common setting: an MBR decoder is built based on one and only one MAP decoder.",
        "On the other hand, recent research has shown that substantial improvements can be achieved by utilizing consensus statistics over multiple SMT systems (Rosti et al., 2007; Li et al., 2009a; Li et al., 2009b; Liu et al., 2009).",
        "It could be desirable to adapt MBR decoding to multiple SMT systems as well.",
        "In this paper, we present Mixture Model-based Minimum Bayes Risk (MMMBR) decoding, an approach that makes use of multiple SMT systems to improve translation performance.",
        "In this work, we can take advantage of a larger search space for hypothesis selection, and employ an improved probability distribution over translation hypotheses based on mixture modeling, which linearly combines distributions of multiple component systems for Bayes risk computation.",
        "The key contribution of this paper is the usage of mixture modeling in MBR, which allows multiple SMT models to be involved in and makes the computation of n-gram consensus statistics to be more accurate.",
        "Evaluation results have shown that our approach not only brings significant improvements to single system-based MBR decoding but also outperforms a state-of-the-art word-level system combination method.",
        "The rest of the paper is organized as follows: In Section 2, we first review traditional MBR decoding method and summarize various search spaces that can be utilized by an MBR decoder.",
        "Then, we describe how a mixture model can be used to combine distributions of multiple SMT systems for Bayes risk computation.",
        "Lastly, we present detailed MMMBR decoding model on multiple systems and make comparison with single system-based MBR decoding methods.",
        "Section 3 describes how to optimize different types of parameters.",
        "Experimental results will be shown in Section 4.",
        "Section 5 discusses some related work and Section 6 concludes the paper.",
        "2 Mixture Model-based MBR Decoding 2.1 Minimum Bayes Risk Decoding",
        "Given a source sentence F, MBR decoding aims to find the translation with the least expected loss under a probability distribution.",
        "The objective of an MBR decoder can be written as:",
        "where denotes a search space for hypothesis selection; denotes an evidence space for Bayes risk computation; denotes a function that measures the loss between and ; is the underlying distribution based on .",
        "Some of existing work on MBR decoding focused on exploring larger spaces for both and Kg, e.g. from TV-best lists to lattices or hypergraphs (Tromble et al., 2008; Kumar et al., 2009).",
        "Various loss functions have also been investigated by using different evaluation criteria for similarity computation, e.g. Word Error Rate, Position-independent Word Error Rate,",
        "Tromble et al., 2008).",
        "But less attention has been paid to distribution .",
        "Currently, many SMT systems based on different paradigms can yield similar performances but are good at modeling different inputs in the translation task (Koehn et al., 2004a; Och et al., 2004; Chiang, 2007; Mi et al., 2008; Huang, 2008).",
        "We expect to integrate the advantages of different SMT models into MBR decoding for further improvements.",
        "In particular, we make in-depth investigation into MBR decoding concentrating on the translation distribution by leveraging a mixture model based on multiple SMT systems.",
        "There are three major forms of search spaces that can be obtained from an MAP decoder as a byproduct, depending on the design of the decoder: V best lists, lattices and hypergraphs.",
        "An V best list contains the most probable translation hypotheses produced by a decoder.",
        "It only presents a very small portion of the entire search space of an SMT model.",
        "A hypergraph is a weighted acyclic graph which compactly encodes an exponential number of translation hypotheses.",
        "It allows us to represent both phrase-based and syntax-based systems in a unified framework.",
        "Formally, a hypergraph is a pair , where is a set of hypernodes and is a set of hyperedges.",
        "Each hypernode corresponds to translation hypotheses with identical decoding states, which usually include the span of the words being translated, the grammar symbol for that span and the left and right boundary words of hypotheses for computing language model (LM) scores.",
        "Each hyperedge corresponds to a translation rule and connects a head node and a set of tail nodes .",
        "The number of tail nodes is called the arity of the hyperedge and the arity of a hypergraph is the maximum arity of its hyperedges.",
        "If the arity of a hyperedge is zero, is then called a source node.",
        "Each hypergraph has a unique root node and each path in a hypergraph induces a translation hypothesis.",
        "A lattice (Ueffing et al., 2002) can be viewed as a special hypergraph, in which the maximum arity is one.",
        "We first describe how to construct a general distribution for translation hypotheses over multiple SMT systems using mixture modeling for usage in MBR decoding.",
        "Mixture modeling is a technique that has been applied to many statistical tasks successfully.",
        "For the SMT task in particular, given SMT systems with their corresponding model distributions, a mixture model is defined as a probability distribution over the combined search space of all component systems and computed as a weighted sum of component model distributions:",
        "For the mixture model , we replace it by Equation 2 and rewrite the total gain score for hypothesis in Equation 3:",
        "In Equation 2, are system weights which hold following constraints:",
        "and , is the th distribution estimated on the search space based on the log-linear formulation:",
        "where is the score function of the th system for translation , is a scaling factor that determines the flatness of the distribution sharp ( ) or smooth ( ).",
        "Due to the inherent differences in SMT models, translation hypotheses have different distributions in different systems.",
        "A mixture model can effectively combine multiple distributions with tunable system weights.",
        "The distribution of a single model used in traditional MBR can be seen as a special mixture model, where is one.",
        "Let denote machine translation systems, denotes the search space produced by system in MAP decoding procedure.",
        "An MMMBR decoder aims to seek a translation from the combined search space that maximizes the expected gain score based on a mixture model .",
        "We write the objective function of MMMBR decoding as:",
        "For the gain function , we follow Tromscored by the hypothesis length and a linear function of n-gram matches as:",
        "In this definition, is a reference translation, is the length of hypothesis , is an n-gram presented in , is the number of cator function which equals to 1 when occurs in and 0 otherwise.",
        "are model parameters, where is the maximum order of the n-grams involved.",
        "In Equation 4, the total gain score on the combined search space can be further decomposed into each local search space with a specified distribution .",
        "This is a nice property and it allows us to compute the total gain score as a weighted sum of local gain scores on different search spaces.",
        "We expand the local gain score for computed on search space with using log-BLEU as:",
        "We make two approximations for the situations when : the first is and the second is",
        "In fact, due to the differences in generative capabilities of SMT models, training data selection and various pruning techniques used, search spaces of different systems are always not identical in practice.",
        "For the convenience of formal analysis, we treat all as ideal distributions with assumptions that all systems work in similar settings, and translation candidates are shared by all systems.",
        "The method for computing n-gram posterior probability in Equation 5 depends on different types of search space : • When K is an TV-best list, it can be computed immediately by enumerating all translation candidates in the V best list:",
        "• When H; is a hypergraph (or a lattice) that encodes exponential number of hypotheses, it is often impractical to compute this probability directly.",
        "In this paper, we use the algorithm presented in Kumar et al.",
        "(2009) which is described in Algorithm 1:",
        "Thus, the total gain score for hypothesis can be further expanded as:",
        "f * (e,co,H;) counts the edge e with «-gram that has the highest edge posterior probability relative to predecessors in the entire graph , and is the edge posterior probability that can be efficiently computed with standard inside and outside probabilities and as:",
        "where is the weight of hyperedge in , is the normalization factor that equals to the inside probability of the root node in .",
        "Algorithm 1: Compute «-gram posterior probabilities on hypergraph H; (Kumar et al., 2009)",
        "1: sort hypernodes topologically",
        "for each hypernode 3: compute edge posterior probability p; (e | H;) for",
        "each hyperedge 4: for each hyperedge e G H; do 5: merge «-grams on T(e) and keep the highest",
        "probability when «-grams are duplicated 6: apply the rule of edge e to «-grams on T ( e ) and",
        "propagate gram prefixes/suffixes to 7: for each «-gram a> introduced by e do",
        "where is a mixture «gram posterior probability.",
        "The most important fact derived from Equation 6 is that, the mixture of different distributions can be simplified to the weighted sum of «-gram posterior probabilities on different search spaces.",
        "We now derive the decision rule of MMMBR decoding based on Equation 6 below:",
        "We also notice that MAP decoding and MBR decoding are two different ways of estimating the probability and each of them has advantages and disadvantages.",
        "It is desirable to interpolate them together when choosing the final translation outputs.",
        "So we include each system's MAP decoding cost as an additional feature further and modify Equation 7 to:",
        "where is the model cost assigned by the MAP decoder for hypothesis .",
        "Because the costs of MAP decoding on different SMT models are not directly comparable, we utilize the MERT algorithm to assign an appropriate weight for each component system.",
        "Compared to single system-based MBR decoding, which obeys the decision rule below:",
        "MMMBR decoding has a similar objective function (Equation 8).",
        "The key difference is that, in MMMBR decoding, «-gram posterior probability is computed as based on an ensemble of search spaces; meanwhile, in single system-based MBR decoding, this quantity is computed locally on single search space .",
        "The procedure of MMMBR decoding on multiple SMT systems is described in Algorithm 2.",
        "Algorithm 2: MMMBR decoding on multiple SMT systems_",
        "1: for each component system do 2: run MAP decoding and generate the correspond-",
        "ing search space 3: compute the «-gram posterior probability set for based on Algorithm 1"
      ]
    },
    {
      "heading": "5. compute the mixture «-gram posterior probability",
      "text": [
        "for each :",
        "6: for each unique «-gram appeared in do 7: for each search space do 11: for each hyperedge in do 14: return the best path according to Equation 8_"
      ]
    },
    {
      "heading": "3. A Two-Pass Parameter Optimization",
      "text": [
        "In Equation 8, there are two types of parameters: parameters introduced by the gain function and the model cost , and system weights introduced by the mixture model .",
        "Because Equation 8 is not a linear function when all parameters are taken into account, MERT algorithm (Och, 2003) cannot be directly applied to optimize them at the same time.",
        "Our solution is to employ a two-pass training strategy, in which we optimize parameters for MBR first and then system weights for the mixture model.",
        "The inputs of an MMMBR decoder can be a combination of translation search spaces with arbitrary structures.",
        "For the sake of a general and convenience solution for optimization, we utilize the simplest TV-best lists with proper sizes as approximations to arbitrary search spaces to optimize MBR parameters using MERT in the first-pass training.",
        "System weights can be set empirically based on different performances, or equally without any bias.",
        "Note that although we tune MBR parameters on V-best lists, «-gram posterior probabilities used for Bayes risk computation could still be estimated on hypergraphs for non V -best-based search spaces.",
        "After MBR parameters optimized, we begin to tune system weights for the mixture model in the second-pass training.",
        "We rewrite Equation 8 as:",
        "For each , the aggregated score surrounded with braces can be seen as its feature value.",
        "Equation 9 now turns to be a linear function for all weights and can be optimized by the MERT."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We conduct experiments on the NIST Chinese-to-English machine translation tasks.",
        "We use the newswire portion of the NIST 2006 test set (MT06-«w) as the development set for parameter optimization, and report results on the NIST 2008 test set (MT08).",
        "Translation performances are measured in terms of case-insensitive BLEU scores.",
        "Statistical significance is computed using the bootstrap re-sampling method proposed by Koehn (2004b).",
        "Table 1 gives data statistics.",
        "All bilingual corpora available for the NIST 2008 constrained track of Chinese-to-English machine translation task are used as training data, which contain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing.",
        "Word alignments are performed by GIZA++ with an intersect-diag-grow refinement.",
        "Data Set",
        "#Sentence",
        "#Word",
        "MT06-nw (dev)",
        "616",
        "17,316",
        "MT08 (test)",
        "1,357",
        "31,600",
        "A 5-gram language model is trained on the English side of all bilingual data plus the Xinhua portion of LDC English Gigaword Version 3.0.",
        "We use two baseline systems.",
        "The first one (SYS1) is a hierarchical phrase-based system (Chiang, 2007) based on Synchronous Context Free Grammar (SCFG), and the second one (SYS2) is a phrasal system (Xiong et al., 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lexicalized reordering component based on maximum entropy model.",
        "Phrasal rules shared by both systems are extracted on all bilingual data, while hierarchical rules for SYS1 only are extracted on a selected data set, including LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92, which contain about 498,000 sentence pairs.",
        "Translation hypergraphs are generated by each baseline system during the MAP decoding phase, and 1000-best lists used for MERT algorithm are extracted from hyper-graphs by the £-best parsing algorithm (Huang and Chiang, 2005).",
        "We tune scaling factor to optimize the performance of HyperGraph-based",
        "MBR decoding (HGMBR) on MT06-nw for each system (0.5 for SYS1 and 0.01 for SYS2).",
        "We first present the overall results of MMMBR decoding on two baseline systems.",
        "To compare with single system-based MBR methods, we reimplement N-best MBR, which performs MBR decoding on 1000-best lists with the fast consensus decoding algorithm (DeNero et al., 2009), and HGMBR, which performs MBR decoding on a hypergraph (Kumar et al., 2009).",
        "Both methods use log-BLEU as the loss function.",
        "We also compare our method with IHMM Word-Comb, a state-of-the-art word-level system combination approach based on incremental HMM alignment proposed by Li et al.",
        "(2009b).",
        "We report results of MMMBR decoding on both N-best lists (N-best MMMBR) and hypergraphs (Hypergraph MMMBR) of two baseline systems.",
        "As MBR decoding can be used for any SMT system, we also evaluate MBR-IHMM Word-Comb, which uses N-best lists generated by HGMBR on each baseline systems.",
        "The default beam size is set to 50 for MAP decoding and hypergraph generation.",
        "The setting of N-best candidates used for (MBR-) IHMM Word-Comb is the same as the one used in Li et al.",
        "(2009b).",
        "The maximum order of n-grams involved in MBR model is set to 4.",
        "Table 2 shows the evaluation results.",
        "Table 2.",
        "MMMBR decoding on multiple systems (*: significantly better than HGMBR with p < 0.01 ; +: significantly better than IHMM",
        "Word-Comb with )",
        "From Table 2 we can see that, compared to",
        "MAP decoding, N-best MBR and HGMBR only improve the performance in a relative small range (+0.1~+0.6 BLEU), while MMMBR decoding on multiple systems can yield significant improvements on both dev set (+0.9 BLEU on N-best MMMBR and +1.3 BLEU on Hypergraph MMMBR) and test set (+0.9 BLEU on N-best MMMBR and +1.4 BLEU on Hypergraph MMMBR); compared to IHMM Word-Comb, N-best MMMBR can achieve comparable results on both dev and test sets, while Hypergraphs MMMBR can achieve even better results (+0.3 BLEU on dev and +0.6 BLEU on test); compared to MBR-IHMM Word-Comb, Hypergraph MMMBR can also obtain comparable results with tiny improvements (+0.1 BLEU on dev and",
        "Word-Comb has ability to generate new hypotheses, while Hypergraph MMMBR only chooses translations from original search spaces.",
        "We next evaluate performances of MMMBR decoding on hypergraphs generated by different beam size settings, and compare them to (MBR-)",
        "MT06-nw",
        "MT08",
        "SYS1 SYS2",
        "SYS1 SYS2",
        "MAP",
        "38.1 37.1",
        "28.5 28.0",
        "N-best MBR",
        "38.3 37.4",
        "29.0 28.1",
        "HGMBR",
        "38.3 37.5",
        "29.1 28.3",
        "IHMM Word-Comb",
        "39.1",
        "29.3",
        "MBR-IHMM Word-Comb",
        "39.3",
        "29.7",
        "N-best MMMBR",
        "39.0*",
        "29.4*",
        "Hypergraph MMMBR",
        "39.4*+",
        "299*+",
        "IHMM Word-Comb with the same candidate size and HGMBR with the same beam size.",
        "We list the results of MAP decoding for comparison.",
        "The comparative results on MT08 are shown in Figure 1, where X-axis is the size used for all methods each time, Y-axis is the BLEU score,",
        "MAP- and HGMBR- stand for MAP decoding and HGMBR decoding for the th system.",
        "MAP-1 MAP-2 e – HGMBR-1 HGMBR-2 IHMM MBR-IHMM e – MMMBR",
        "Comb and HGMBR with different sizes",
        "From Figure 1 we can see that, MMMBR decoding performs consistently better than both (MBR-) IHMM Word-Comb and HGMBR on all sizes.",
        "The gains achieved are around +0.5 BLEU compared to IHMM Word-Comb, +0.2",
        "BLEU compared to MBR-IHMM Word-Comb, and +0.8 BLEU compared to HGMBR.",
        "Compared to MAP decoding, the best result (30.1) is obtained when the size is 100, and the largest improvement (+1.4 BLEU) is obtained when the size is 50.",
        "However, we did not observe significant improvement when the size is larger than 50.",
        "We then setup an experiment to verify that the mixture model based on multiple distributions is more effective than any individual distributions for Bayes risk computation in MBR decoding.",
        "We use Mix-HGMBR to denote MBR decoding performed on single hypergraph of each system in the meantime using a mixture model upon distributions of two systems for Bayes risk computation.",
        "We compare it with HGMBR and Hypergraph MMMBR and list results in Table 3.",
        "It can be seen that based on the same search space, the performance of Mix-HGMBR is significantly better than that of HGMBR (+0.3/+0.6 BLEU on dev/test).",
        "Yet the performance is still not as good as Hypergraph, which indicates the fact that the mixture model and the combination of search spaces are both helpful to MBR decoding, and the best choice is to use them together.",
        "We also empirically investigate the impacts of different system weight settings upon the performances of Hypergraph MMMBR on dev set in Figure 2, where X-axis is the weight for SYS1, Y-axis is the BLEU score.",
        "The weight Ä2for SYS2 equals to as only two systems involved.",
        "The best evaluation result on dev set is achieved when the weight pair is set to 0.7/0.3 for SYS1/SYS2, which is also very close to the one trained automatically by the training strategy presented in Section 3.2.",
        "Although this training strategy can be processed repeatedly, the performance is stable after the 1st round finished.",
        "4.4 MMMBR Results on Identical Systems with Different Translation Models",
        "Inspired by Macherey and Och (2007), we arrange a similar experiment to test MMMBR decoding for each baseline system on an ensemble of subsystems built by the following two steps.",
        "Firstly, we iteratively apply the following procedure 3 times: at the th time, we randomly sample 80% sentence pairs from the total bilingual data to train a translation model and use it to build a new system based on the same decoder, which is denoted as subsystem i.",
        "Table 4 shows the evaluation results of all subsystems on MT08, where MAP decoding (the former ones) and corresponding HGMBR (the latter ones) are grouped together by a slash.",
        "We set all beam sizes to 20 for a time-saving purpose.",
        "MT08",
        "SYS1",
        "SYS2",
        "HGMBR",
        "29.1",
        "28.3",
        "Mix-HGMBR",
        "29.4",
        "28.9",
        "Hypergraph MMMBR",
        "29.9",
        "Secondly, starting from each baseline system, we gradually add one more subsystem each time and perform Hypergraph MMMBR on hypergraphs generated by current involved systems.",
        "Table 5 shows the evaluation results.",
        "We can see from Table 5 that, compared to the results of MAP decoding, MMMBR decoding can achieve significant improvements when more than one subsystem are involved; however, compared to the results of HGMBR on baseline systems, there are few changes of performance when the number of subsystems increases.",
        "One potential reason is that the translation hypotheses between multiple subsystems under the same SMT model hold high degree of correlation, which is discussed in Macherey and Och (2007).",
        "We also evaluate MBR-IHMM Word-Comb on TV-best lists generated by each baseline system with its corresponding three sub-systems.",
        "Evaluation results are shown in Table 6, where Hypergraph MMMBR still outperforms MBR-IHMM Word-Comb on both baseline systems.",
        "Word-Comb with multiple sub-systems"
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "Employing consensus between multiple systems to improve machine translation quality has made rapid progress in recent years.",
        "System combination methods based on confusion networks (Ros-ti et al., 2007; Li et al., 2009b) have shown state-of-the-art performances in MT benchmarks.",
        "Different from them, MMMBR decoding method does not generate new translations.",
        "It maintains the essential of MBR methods to seek translations from existing search spaces.",
        "Hypothesis selection method (Hildebrand and Vogel, 2008) resembles more our method in making use of «-gram statistics.",
        "Yet their work does not belong to the MBR framework and treats all systems equally.",
        "Li et al.",
        "(2009a) presents a co-decoding method, in which «-gram agreement and disagreement statistics between translations of multiple decoders are employed to re-rank both full and partial hypotheses during decoding.",
        "Liu et al.",
        "(2009) proposes a joint-decoding method to combine multiple SMT models into one decoder and integrate translation hypergraphs generated by different models.",
        "Both of the last two methods work in a white-box way and need to implement a more complicated decoder to integrate multiple SMT models to work together; meanwhile our method can be conveniently used as a second-pass decoding procedure, without considering any system implementation details."
      ]
    },
    {
      "heading": "6. Conclusions and Future Work",
      "text": [
        "In this paper, we have presented a novel MMMBR decoding approach that makes use of a mixture distribution of multiple SMT systems to improve translation accuracy.",
        "Compared to single system-based MBR decoding methods, our method can achieve significant improvements on both dev and test sets.",
        "What is more, MMMBR decoding approach also outperforms a state-of-the-art system combination method.",
        "We have empirically verified that the success of our method comes from both the mixture modeling of translation hypotheses and the combined search space for translation selection.",
        "In the future, we will include more SMT systems with more complicated models into our MMMBR decoder and employ more general MERT algorithms on hypergraphs and lattices (Kumar et al.",
        ", 2009) for parameter optimization.",
        "MT08",
        "SYS1",
        "SYS2",
        "Baseline",
        "28.4/29.0",
        "27.6/27.8",
        "sub-system-1",
        "28.1/28.5",
        "26.8/27.3",
        "sub-system-2",
        "28.3/28.4",
        "27.0/27.1",
        "sub-system-3",
        "27.7/28.0",
        "27.3/27.6",
        "SYS1",
        "SYS2",
        "MAP",
        "28.4",
        "27.6",
        "HGMBR",
        "29.0",
        "27.8",
        "Hypergraph",
        "MMMBR",
        "+ sub-system-1",
        "29.1",
        "27.9",
        "+ sub-system-2",
        "29.1",
        "28.1",
        "+ sub-system-3",
        "29.3",
        "28.3",
        "MT08",
        "SYS1",
        "SYS2",
        "MBR-IHMM Word-Comb",
        "29.1",
        "28.0",
        "Hypergraph MMMBR",
        "29.3",
        "28.3"
      ]
    }
  ]
}
