{
  "info": {
    "authors": [
      "Peter Nilsson",
      "Pierre Nugues"
    ],
    "book": "COLING",
    "id": "acl-C10-1093",
    "title": "Automatic Discovery of Feature Sets for Dependency Parsing",
    "url": "https://aclweb.org/anthology/C10-1093",
    "year": 2010
  },
  "references": [
    "acl-D07-1119",
    "acl-P05-1013",
    "acl-W06-2920",
    "acl-W06-2933",
    "acl-W08-2121",
    "acl-W09-3825"
  ],
  "sections": [
    {
      "text": [
        "Peter Nilsson Pierre Nugues",
        "peter.nilsson.lund@telia.com Pierre.Nugues@cs.lth.se",
        "This paper describes a search procedure to discover optimal feature sets for dependency parsers.",
        "The search applies to the shift-reduce algorithm and the feature sets are extracted from the parser configuration.",
        "The initial feature is limited to the first word in the input queue.",
        "Then, the procedure uses a set of rules founded on the assumption that topological neighbors of significant features in the dependency graph may also have a significant contribution.",
        "The search can be fully automated and the level of greediness adjusted with the number of features examined at each iteration of the discovery procedure.",
        "Using our automated feature discovery on two corpora, the Swedish corpus in CoNLL-X and the English corpus in CoNLL 2008, and a single parser system, we could reach results comparable or better than the best scores reported in these evaluations.",
        "The CoNLL 2008 test set contains, in addition to a Wall Street Journal (WSJ) section, an out-of-domain sample from the Brown corpus.",
        "With sets of 15 features, we obtained a labeled attachment score of 84.21 for Swedish, 88.11 on the WSJ test set, and 81.33 on the Brown test set."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The selection of relevant feature sets is crucial to the performance of dependency parsers and this process is still in large part manual.",
        "Moreover, feature sets are specific to the languages being analyzed and a set optimal for, say, English can yield poor results in Chinese.",
        "With dependency parsers being applied today to dozens of languages, this makes the parametrization of a parser both a tedious and time-consuming operation.",
        "Incidentally, the advent of machine-learning methods seems to have shifted the tuning steps in parsing from polishing up grammar rules to the optimization of feature sets.",
        "And as with the writing of a grammar, the selection of features is a challenging task that often requires a good deal of effort and inspiration.",
        "Most automatic procedures to build feature sets resort to greedy algorithms.",
        "Forward selection constructs a set by adding incrementally features from a predetermined superset while backward elimination removes them from the superset (At-tardi et al., 2007).",
        "Both methods are sometimes combined (Nivre et al., 2006b).",
        "The selection procedures evaluate the relevance of a candidate feature in a set by its impact on the overall parsing score: Does this candidate improve or decrease the performance of the set?",
        "Greedy search, although it simplifies the design of feature sets, shows a major drawback as it starts from a closed superset of what are believed to be the relevant features.",
        "There is a broad consensus on a common feature set including the words close to the top of the stack or the beginning of the queue, for the shift-reduce algorithm, but no clear idea on the limits of this set.",
        "In this paper, we describe an automatic discovery procedure that is not bounded by any prior knowledge of a set of potentially relevant features.",
        "It applies to the shift-reduce algorithm and the initial feature consists solely of the first word of the queue.",
        "The search explores nodes along axes of the parser's data structures and the partially built graph using proximity rules to uncover sequences of relevant, efficient features.",
        "Using this procedure on the Swedish corpus in CoNLL-X and the English corpus in CoNLL 2008, we built feature sets that enabled us to reach a labeled attachment score of 84.21 for Swedish, 88.11 on the Wall Street Journal section of CoNLL 2008, and 81.33 on the Brown part of it with a set cardinality of 15."
      ]
    },
    {
      "heading": "2. Transition-based Parsing",
      "text": [
        "Transition-based methods (Covington, 2001; Nivre, 2003; Yamada and Matsumoto, 2003; Zhang and Clark, 2009) have become a popular approach in multilingual dependency parsing because of their speed and performance.",
        "Transition-based methods share common properties and build a dependency graph from a sequence of actions, where each action is determined using a feature function.",
        "In a data-driven context, the function is typically implemented as a classifier and the features are extracted from the partially built graph and the parser's data structures, most often a queue and a stack.",
        "In this study, we built a parser using Nivre's algorithm (Nivre, 2003).",
        "The parser complexity is linear and parsing completes in at most 2n + 1 operations, where n is the length of the sentence.",
        "Table 1 shows the transitions and actions to construct a dependency graph.",
        "Given a sentence to parse, we used a classifier-based guide to predict the transition sequence to apply.",
        "At each step, the guide extracts features from the parser configuration and uses them as input to a classifier to predict the next transition.",
        "Before training the classification models, we projec-tivized the corpus sentences (Kunze, 1967; Nivre and Nilsson, 2005).",
        "We did not attempt to recover nonprojective sentences after parsing.",
        "We extracted the features using a gold-standard parsing of the training set.",
        "We organized the classification, and hence the feature extraction, as a",
        "Action Parser configuration",
        "Init.",
        "End",
        "Table 1: Parser transitions (Nivre, 2003).",
        "W is the input, G, the graph, S, the stack, and Q, the queue.",
        "The triple (S, Q, G) represents the parser configuration and n, n and n\" are lexical tokens.",
        "(nf,n) represents an arc from nl to n.",
        "two-step process.",
        "The first step determines the action among LeftArc, RightArc, Reduce, and Shift; the second one, the grammatical function, if the action is either a left arc or a right arc.",
        "Once the features are extracted, we train the corresponding models that we apply to the test corpus to predict the actions and the arc labels."
      ]
    },
    {
      "heading": "3. Feature Discovery",
      "text": [
        "We designed an automatic procedure to discover and select features that is guided by the structure of the graph being constructed.",
        "The search algorithm is based on the assumption that if a feature makes a significant contribution to the parsing performance, then one or more of its topological neighbors in the dependency graph may also be significant.",
        "The initial state, from which we derive the initial feature, consists of the first word in the queue.",
        "There is no other prior knowledge on the features.",
        "In the discovery procedure, we considered the nodes of four data structures: the queue, the stack, the sentence, and the graph being constructed.",
        "We extracted three attributes (or fields) from each node: two static ones, the lexical value of the node and its part of speech, and a dynamic one evaluated at parse time: the dependency label of the arc linking the node to its head, if it exists.",
        "We denoted the attributes of node w, respectively, LEX(w), POS(w), and DEP(w).",
        "These attributes are used as input by most dependency parsers, whatever the language being parsed.",
        "The feature search covers three different axes: the parser's data structures - the queue and the stack -, the graph being constructed, and the sentence.",
        "Given a feature set at step n of the discovery procedure, we defined a successor function that generates the set of topological neighbors of all the members in the feature set along these three axes.",
        "For a particular feature:",
        "The data structure axis consists of the nodes in the stack and the queue.",
        "The immediate neighbors of a node in the stack are the adjacent nodes above and below.",
        "In the queue, these are the adjacent nodes before and after it.",
        "The top node on the stack and the next node in the queue have a special connection, since they are the ones used by the parser when creating an arc.",
        "Therefore, we considered them as immediate neighbors to each other.",
        "For a node that is neither in the stack, nor in the queue, there is no connection along this axis.",
        "The graph axes traverse the partially constructed graph horizontally and vertically.",
        "The horizontal axis corresponds to the sibling nodes connected by a common head (Figure 1).",
        "The immediate neighbors of a node are its nearest siblings to the left and to the right.",
        "The vertical axis corresponds to the head and child nodes.",
        "The immediate neighbors are the head node as well as the leftmost and rightmost child nodes.",
        "There is no connection for nodes not yet part of the graph.",
        "The sentence axis traverses the nodes in the order they occur in the original sentence.",
        "The immediate neighbors of a node are the previous and next words in the sentence."
      ]
    },
    {
      "heading": "4. Representing Features and Their Neighbors",
      "text": [
        "We represented features with a parameter format partly inspired by MaltParser (Nivre et al., 2006a).",
        "Horizontal axis Leftmost child",
        "Each parameter consists of two parts.",
        "The first one represents a node in a data structure (STACK or QUEUE) and an attribute:",
        "The nodes are identified using a zero-based index.",
        "Thus STACK1 designates the second node on the stack.",
        "The attribute of a node is one of part of speech (POS), lexical value (LEX), or dependency label (DEP), as for instance LEX(QUEUE0) that corresponds to the lexical value of the first token in the queue.",
        "The second part of the parameter is an optional navigation path that allows to find other destination nodes in the graph.",
        "It consists of a sequence of instructions to move from the start node to the destination node.",
        "The list of possible instructions are:",
        "• h: head of the current node;",
        "• lc/rc: leftmost/rightmost child of the node;",
        "• pw/fw: previous/following word of the node in the original sentence.",
        "An example ofa feature obtained using the navigation part is POS(STACK1 lc pw), which is interpreted as: start from STACK1.",
        "Then, using the instructions lc and pw, move to the left child ofthe start node and to the previous word ofthis child in the sentence.",
        "The requested feature is the part of speech of the destination node."
      ]
    },
    {
      "heading": "5. Initial State and Successor Function",
      "text": [
        "The feature discovery is an iterative procedure that grows the feature set with one new feature at each iteration.",
        "We called generation such an iteration, where generation 1 consists of a single node.",
        "We denoted Feat Set = {f1, f2,---, f} the feature set obtained at generation i.",
        "Although the features of a classifier can be viewed as a set, we also considered them as a tuple, where Feat = (f1,f2,---,f) is the i-tuple at generation i and f, the individual feature discovered at generation k with 1 ^ k ^ i.",
        "This enables us to keep the order in which the individual features are obtained during the search.",
        "We start the feature search with the empty set, 0/, that, by convention, has one neighbor: the first node in the queue QUEUE0.",
        "We chose this node because this is the only one which is certain to exist all along the parsing process.",
        "Intuitively, this is also obvious that QUEUE0 plays a significant role when deciding a parsing action.",
        "We defined the successor function of the empty set as: SUCC (0 ) = {POS(QUEUE 0), LEX (QUEUE 0)}.",
        "The successors of a node consist of itself and all its topological neighbors along the three axes with their three possible attributes: part of speech, lexical value, and dependency label.",
        "For a particular feature in FeatSet, the generation ofits successors is carried out through the following steps:",
        "1.",
        "Interpret the feature with its possible navigation path and identify the destination node n.",
        "2.",
        "Find all existing immediate neighboring nodes of n along the three search axes.",
        "3.",
        "Assign the set of attributes - POS, LEX, and DEP ton and its neighboring nodes.",
        "If at any step the requested node does not exist, the feature evaluates to NOTHING.",
        "The generation of all the neighbors of the features in FeatSet may create duplicates as a same node can sometimes be reached from multiple paths.",
        "For instance, if we move to the leftmost child ofa node and then to the head of this child, we return to the original node.",
        "To compute the successor function, we built a set of rules shown in Table 2.",
        "It corresponds to a subset of the rules described in the axis search (Sect.",
        "3.2) so that it omits the neighbors of a node that would unavoidably create redundancies.",
        "The third column in Table 2 shows the rules to generate the neighbors of POS(QUEUE0).",
        "They correspond to the rows:",
        "PL.",
        "This stands for the POS and LEX attributes of the node.",
        "We only add LEX(QUEUE0) as we already have POS(QUEUE0).",
        "PLD lc and PLD rc.",
        "POS, LEX, and DEP of the node's leftmost and rightmost children.",
        "PLD pw.",
        "POS, LEX, and DEP of the previous word in the original string.",
        "The following word is the same as the next node in the queue, which is added in the next step.",
        "For that reason, following word is not added.",
        "PL QUEUE 1.",
        "POS and LEX of QUEUE 1.",
        "PLD STACK0.",
        "POS, LEX, and DEP of STACK0.",
        "This rule connects the queue to the top node of the stack.",
        "Table 3 summarizes the results of the rule application and shows the complete list of successors of POS(QUEUE0).",
        "In this way, the search for a node's neighbors along the axes is reduced to one direction, either left or right, or up or down, that will depend on the topological relation that introduced the node in the feature set."
      ]
    },
    {
      "heading": "6. Feature Selection Algorithm",
      "text": [
        "At each generation, we compute the Cartesian product of the current feature tuple Feati and the set defined by its neighbors.",
        "We define the set of candidate tuples CandFeati+1 at generation i + 1 as:",
        "Card(SUCC(Feati)).",
        "The members of CandFeati+1 are ranked according to their parsing score on the development",
        "Table 2: Rules to compute the successors of a node.",
        "For each node category given in row 2, the procedure adds the features in the column headed by the category.",
        "PLD stands for the POS, LEX, and DEP attributes.",
        "In the right-hand side of the table, the category corresponds to the last instruction of the navigation path, if it exists, for instance pw in the feature POS(STACK lc pw).",
        "We read the six successors of this node in the fifth column headed by pw: STACK lc pw h, STACK lc pw lc, STACK lc pw rc, STACK lc pw Is, STACK lc pw rs, and STACK lc pw pw.",
        "We then apply all the attributes to these destination nodes to generate the features.",
        "the three attributes POS, LEX, and DEP of the node; PL for POSand LEX.",
        "set and when applying a greedy best-first search, Feati+1 is assigned with the tuple yielding the highest score:",
        "Feati+i evalJbest(CandFeati+i).",
        "The procedure is repeated with the immediate neighbors of Feati+i until the improvement of the score is below a certain threshold.",
        "We extended this greedy version of the discovery with a beam search that retains the N-best successors from the candidate set.",
        "In our experiments, we used beam widths of 4 and 8."
      ]
    },
    {
      "heading": "7. Experimental Setup",
      "text": [
        "In a first experiment, we used the Swedish corpus of the CoNLL-X shared task (Buchholz and Marsi, 2006).",
        "In a second experiment, we applied the feature discovery procedure to the English corpus from CoNLL 2008 (Surdeanu et al., 2008), a dependency corpus converted from the Penn Treebank and the Brown corpus.",
        "In both experiments, we used the LIBSVM package (Chang and Lin, 2001) with a quadratic kernel, y = 0.2, C = 0.4, and e = 0.1.",
        "These parameters are identical to Nivre et al.",
        "(2006b) to enable a comparison ofthe scores.",
        "We evaluated the feature candidates on a development set using the labeled and unlabeled attachment scores (LAS and UAS) that we computed with the eval.pl script from CoNLL-X.",
        "As there was no development set for the Swedish corpus, we created one by picking out every 10th sentence from the training set.",
        "The training was then carried out on the remaining part of the set."
      ]
    },
    {
      "heading": "8. Feature Discovery on a Swedish Corpus",
      "text": [
        "In a first run, the search was optimized for the UAS.",
        "In a second one, we optimized the LAS.",
        "We also report the results we obtained subsequently on the CoNLL-X test set as an indicator of how well the training generalized.",
        "Data structures",
        "Navigation paths",
        "STACKo",
        "STACKn,n > 0",
        "QUEUEq",
        "QUEUEn,n>0",
        "h",
        "lc, rc",
        "Is",
        "rs",
        "pw",
        "fw",
        "PLD",
        "PLD",
        "PL",
        "PL",
        "h",
        "h",
        "h",
        "PLD h",
        "PLD h",
        "lc",
        "lc",
        "lc",
        "lc",
        "lc",
        "PLD lc",
        "PLD lc",
        "PLD lc",
        "rc",
        "rc",
        "rc",
        "rc",
        "rc",
        "PLD rc",
        "PLD rc",
        "PLD rc",
        "ls",
        "ls",
        "ls",
        "ls",
        "ls",
        "PLD ls",
        "PLD ls",
        "rs",
        "rs",
        "rs",
        "rs",
        "rs",
        "PLD rs",
        "PLD rs",
        "pw",
        "pw",
        "pw",
        "pw",
        "pw",
        "PLD pw",
        "PLD pw",
        "PLD pw",
        "fw",
        "fw",
        "fw",
        "fw",
        "fw",
        "PLD fw",
        "PLD fw",
        "PLD STACK",
        "PLD STACK„+1",
        "PL QUEUEq",
        "PL QUEUE„+l",
        "PL QUEUEq",
        "PLD STACKo",
        "Initial feature",
        "POS",
        "QUEUE",
        "0",
        "Successors",
        "LEX",
        "QUEUE",
        "~Ö~",
        "PLD",
        "QUEUE",
        "0",
        "lc",
        "PLD",
        "QUEUE",
        "0",
        "rc",
        "PLD",
        "QUEUE",
        "0",
        "pw",
        "PL",
        "QUEUE",
        "1",
        "PLD",
        "STACK",
        "0",
        "Table 4 shows the feature performance at the first generation sorted by UAS.",
        "The first row shows the two initial feature candidates, {POS(QUEUE0)} and {LEX(QUEUE0)}.",
        "The third row shows the score produced by the initial features alone.",
        "The next rows show the unlabeled and labeled attachment scores with feature pairs combining one of the initial features and the one listed in the row.",
        "The combination of POS(QUEUE0) and POS(STACK) yielded the best UAS: 74.02.",
        "The second feature improves the performance of POS(QUEUE0) by more than 30 points from 43.49.",
        "For each generation, we applied a beam search.",
        "We kept the eight best pairs as starting states for the second generation and we added their neighboring nodes.",
        "Table 5 shows the eight best results out of 38 for the pair {POS(QUEUE 0), POS(STACK0)}.",
        "Parent state: {POS(QUEUEo),POS(STACK0)} {POS(QUEUE 0), POS(STACK0)} on the Swedish corpus.",
        "Out of the 38 successors, we show the eight that yielded the best results.",
        "P stands for POS, L for LEX, and D for DEP.",
        "We iterated the process over a total of 16 generations.",
        "Table 6, left-hand side, shows the list of the best scores for each generation.",
        "The scores on the development set increased steadily until generation 13, then reached a plateau, and declined around generation 15.",
        "The test set closely followed the development set with values about 1% lower.",
        "On this set, we reached a peak performance at generation 12, after which the results decreased.",
        "Table 6, right-hand side, shows the features producing the final score in their order of inclusion in the feature set.",
        "As we applied a beam search, a feature listed at generation i does not necessary correspond to the highest score for this generation, but belongs to the feature tuple producing the best result at generation 16.",
        "We also applied the feature discovery with a search optimized for the labeled attachment score.",
        "This time, we reduced the beam width used in the search from 8 to 4 as we noticed that the candidates between ranks 5 and 8 never contributed to the best scoring feature set for any generation.",
        "We observed a score curve similar to that of the UAS-optimized search.",
        "The train set followed the development set with increasing values for each generation but 1-2% lower.",
        "The optimal value was obtained at generation 15 with 84.21% for the test set.",
        "Then, the score for the test set decreased."
      ]
    },
    {
      "heading": "9. Feature Discovery on a Corpus of English",
      "text": [
        "The training and development sets of the CoNLL 2008 corpus contain text from the Wall Street Journal exclusively.",
        "The test set contains text from the Brown corpus as well as from the Wall Street Journal.",
        "Table 7 shows the results after 16 generations.",
        "We used a beam width of 4 and the tests were optimized for the unlabeled attachment score.",
        "As for Swedish, we reached the best scores around generation 14-15.",
        "The results on the indomain test set peaked at 90.89 and exceeded the results on the development set.",
        "As expected, the results for the out-of-domain corpus were lower, 87.50, however the drop was limited to 3.4."
      ]
    },
    {
      "heading": "10. Discussion and Conclusion",
      "text": [
        "The results we attained with feature set sizes as small as 15 are competitive or better than figures",
        "Dev set",
        "Test set",
        "UAS",
        "LAS",
        "UAS",
        "LAS",
        "Successors",
        "79.50",
        "65.34",
        "79.07",
        "65.86",
        "P QUEUE 1",
        "78.73",
        "66.98",
        "76.04",
        "64.51",
        "L STACK 0 fw",
        "77.42",
        "63.08",
        "74.63",
        "61.86",
        "L QUEUE 1",
        "77.06",
        "64.54",
        "75.28",
        "62.90",
        "L QUEUE 0 pw",
        "76.83",
        "66.01",
        "73.61",
        "63.77",
        "L QUEUE 0",
        "76.63",
        "63.62",
        "74.75",
        "63.17",
        "P STACK 0 fw",
        "76.44",
        "64.24",
        "74.09",
        "62.02",
        "L STACK 0",
        "76.39",
        "63.12",
        "73.99",
        "61.16",
        "L QUEUE 0 lc",
        "Table",
        "5:",
        "Ranking",
        "the",
        "successors of",
        "Table 4: Results of the beam search on the Swedish corpus at the first generation with the two initial feature candidates, {POS(QUEUE0)} and {LEX(QUEUE0)}, respectively on the left-and right-hand side of the table.",
        "The third row shows the score produced by the initial features alone and the next rows, the figures for the candidate pairs combining the initial feature and the successor listed in the row.",
        "The eight best combinations shown in bold are selected for the next generation.",
        "Table 6: Best results for each generation on the Swedish corpus, optimized for UAS.",
        "Figures in bold designate the best scores.",
        "The right-hand side of the table shows the feature sequence producing the best result at generation 16.",
        "Parent state",
        "(POS(QUEUE0))",
        "{LEX [QUEUED)",
        "UAS",
        "LAS",
        "Successors",
        "UAS",
        "LAS",
        "Successors",
        "43.49",
        "26.45",
        "None",
        "42.76",
        "23.56",
        "None",
        "74.02",
        "59.67",
        "POS STACK 0",
        "65.86",
        "52.18",
        "POS STACK 0",
        "67.77",
        "54.50",
        "LEX STACK 0",
        "58.59",
        "45.51",
        "LEX STACK 0",
        "58.37",
        "41.83",
        "POS QUEUE 0 pw",
        "51.98",
        "37.70",
        "POS QUEUE 0",
        "pw",
        "55.28",
        "38.49",
        "LEX QUEUE 0 pw",
        "50.44",
        "29.71",
        "POS QUEUE 1",
        "51.53",
        "30.43",
        "POS QUEUE 1",
        "50.38",
        "35.24",
        "LEX QUEUE 0",
        "pw",
        "51.05",
        "32.66",
        "LEX QUEUE 0 lc",
        "49.37",
        "32.27",
        "POS QUEUE 0",
        "49.71",
        "31.54",
        "POS QUEUE 0 lc",
        "48.91",
        "27.77",
        "LEX QUEUE 1",
        "49.49",
        "29.18",
        "LEX QUEUE 1",
        "48.66",
        "29.91",
        "LEX QUEUE 0",
        "lc",
        "49.37",
        "32.27",
        "LEX QUEUE 0",
        "47.25",
        "28.92",
        "LEX QUEUE 0",
        "rc",
        "48.68",
        "29.34",
        "DEP STACK 0",
        "47.09",
        "28.65",
        "POS QUEUE 0",
        "lc",
        "48.47",
        "30.84",
        "LEX QUEUE 0 rc",
        "46.68",
        "27.08",
        "DEP QUEUE 0",
        "lc",
        "46.77",
        "26.86",
        "DEP QUEUE 0 lc",
        "45.69",
        "27.83",
        "POS QUEUE 0",
        "rc",
        "46.40",
        "29.95",
        "POS QUEUE 0 rc",
        "44.77",
        "26.17",
        "DEP STACK 0",
        "42.27",
        "25.21",
        "DEP QUEUE 0 pw",
        "44.43",
        "26.47",
        "DEP QUEUE 0",
        "rc",
        "41.04",
        "26.56",
        "DEP QUEUE 0 rc",
        "41.87",
        "23.04",
        "DEP QUEUE 0",
        "pw",
        "Generation",
        "Dev set",
        "Test set",
        "Features",
        "UAS",
        "LAS",
        "UAS",
        "LAS",
        "1",
        "43.49",
        "26.45",
        "45.93",
        "30.19",
        "POS QUEUE 0",
        "2",
        "74.02",
        "59.67",
        "71.60",
        "58.37",
        "POS STACK 0",
        "3",
        "79.50",
        "65.34",
        "79.07",
        "65.86",
        "POS QUEUE 1",
        "4",
        "83.58",
        "71.76",
        "82.75",
        "70.98",
        "LEX STACK 0 fw",
        "5",
        "85.96",
        "76.03",
        "84.82",
        "74.75",
        "LEX STACK 0",
        "6",
        "87.23",
        "77.32",
        "86.34",
        "76.52",
        "LEX QUEUE 0 lc",
        "7",
        "88.42",
        "80.00",
        "87.67",
        "78.99",
        "POS STACK 1",
        "8",
        "89.43",
        "81.56",
        "88.09",
        "80.26",
        "LEX QUEUE 1",
        "9",
        "89.84",
        "83.20",
        "88.69",
        "82.33",
        "LEX QUEUE 0",
        "10",
        "90.23",
        "83.89",
        "89.17",
        "83.31",
        "DEP STACK 0 lc",
        "11",
        "90.49",
        "84.31",
        "89.58",
        "83.85",
        "POS STACK 0 fw",
        "12",
        "90.73",
        "84.47",
        "89.66",
        "83.83",
        "LEX STACK 0 fw ls",
        "13",
        "90.81",
        "84.60",
        "89.52",
        "83.75",
        "LEX STACK 0 fw ls lc",
        "14",
        "90.81",
        "84.70",
        "89.32",
        "83.73",
        "POS STACK 1 h",
        "15",
        "90.85",
        "84.67",
        "89.13",
        "83.21",
        "LEX STACK 1 rs",
        "16",
        "90.84",
        "84.68",
        "88.65",
        "82.75",
        "POS STACK 0 fw ls rc",
        "reported by state-of-the-art transition-based systems.",
        "We reached a UAS of 89.66 on the CoNLL-X Swedish corpus.",
        "On the same corpus, the top scores reported in the shared task were slightly lower: 89.54 and 89.50.",
        "Our best LAS was 84.21, and the two best scores in CoNLL-X were 84.58 and 82.55.",
        "Our results for the English corpus from CoNLL 2008 were optimized for an unlabeled attachment score and we obtained 90.89 for the indomain test set and 87.50 for the out-of-domain one.",
        "Our best LAS were 88.11 and 81.33.",
        "Official results in CoNLL 2008 only reported the labeled attachment scores, respectively 90.13 and 82.81.",
        "We believe these results remarkable.",
        "We used a single-parser system as opposed to ensemble systems and the results on the Brown corpus show an excellent resilience and robustness on out-of-domain data.",
        "The automatic discovery produced results matching or exceeding comparable systems, although no prior knowledge of the language being analyzed was used and no feature set was provided to the parser.",
        "Although, a systematic search requires no intuitive guessing, it still consumes a considerable machine time.",
        "Due to the learning algorithm we use, SVM, training a model takes between 1 and 130 hours depending on the size of the corpus.",
        "The number of models to train at each generation corresponds to the number of feature candidates times the beam width.",
        "The first generation contains about 15 feature candidates per feature set and since features are only added, the number of candidates can grow to 100 at generation 10.",
        "We believe there is a margin for improvement both in the parsing scores and in the time needed to determine the feature sets.",
        "Our scores in Swedish were obtained with models trained on 90% of the training set.",
        "They could probably be slightly improved if they had been trained on a complete set.",
        "In our experiments, we used three attributes: the part of speech, lexical value, and dependency label ofthe node.",
        "These attributes could be extended to lemmas and grammatical features.",
        "SVMs yield a high performance, but they are slow to train.",
        "Logistic regression with, for instance, the LIBLINEARpackage (Fan et al., 2008) would certainly reduce the exploration time.",
        "Generation",
        "Dev set UAS LAS",
        "Test set WSJ UAS LAS",
        "Test set Brown UAS LAS",
        "Features",
        "1",
        "45.25",
        "33.77",
        "45.82",
        "34.49",
        "52.12",
        "40.70",
        "POS QUEUE 0",
        "2",
        "64.42",
        "55.64",
        "64.71",
        "56.44",
        "71.29",
        "62.41",
        "LEX STACK 0",
        "3",
        "78.62",
        "68.77",
        "78.99",
        "70.30",
        "78.67",
        "65.17",
        "POS QUEUE 1",
        "4",
        "81.83",
        "76.67",
        "82.46",
        "77.82",
        "80.57",
        "72.95",
        "LEX STACK 0 fw",
        "5",
        "84.43",
        "79.78",
        "84.89",
        "80.88",
        "84.03",
        "76.99",
        "POS STACK 0",
        "6",
        "85.95",
        "81.60",
        "86.61",
        "82.93",
        "84.55",
        "77.80",
        "DEP QUEUE 0 lc",
        "7",
        "86.95",
        "82.73",
        "87.73",
        "84.09",
        "85.26",
        "78.48",
        "LEX STACK 1",
        "8",
        "88.03",
        "83.62",
        "88.52",
        "84.74",
        "85.66",
        "78.73",
        "LEX QUEUE 1",
        "9",
        "88.61",
        "84.97",
        "89.15",
        "86.20",
        "86.29",
        "79.86",
        "LEX QUEUE 0",
        "10",
        "89.09",
        "85.43",
        "89.47",
        "86.60",
        "86.43",
        "80.02",
        "POS QUEUE 2",
        "11",
        "89.54",
        "85.87",
        "90.25",
        "87.40",
        "87.00",
        "80.75",
        "POS STACK 0 pw",
        "12",
        "89.95",
        "86.21",
        "90.63",
        "87.77",
        "86.87",
        "80.46",
        "POS QUEUE 3",
        "13",
        "90.26",
        "86.56",
        "90.64",
        "87.80",
        "87.35",
        "80.86",
        "POS STACK 1 pw",
        "14",
        "90.54",
        "86.81",
        "90.71",
        "87.88",
        "87.50",
        "81.30",
        "POS QUEUE 0 pw",
        "15",
        "90.61",
        "86.94",
        "90.89",
        "88.11",
        "87.47",
        "81.33",
        "LEX STACK 0 lc",
        "16",
        "90.65",
        "87.00",
        "90.88",
        "88.09",
        "87.42",
        "81.28",
        "POS STACK 0 pw ls"
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The research leading to these results has received funding from the European community's seventh framework program FP7/2007-2013, challenge 2, cognitive systems, interaction, robotics, under grant agreement No 230902 – ROSETTA."
      ]
    }
  ]
}
