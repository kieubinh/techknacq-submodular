{
  "info": {
    "authors": [
      "Vahed Qazvinian",
      "Dragomir R. Radev",
      "Arzucan Özgür"
    ],
    "book": "COLING",
    "id": "acl-C10-1101",
    "title": "Citation Summarization Through Keyphrase Extraction",
    "url": "https://aclweb.org/anthology/C10-1101",
    "year": 2010
  },
  "references": [
    "acl-A00-1043",
    "acl-C08-1087",
    "acl-N04-1019",
    "acl-N07-1040",
    "acl-N09-1066",
    "acl-P08-1093",
    "acl-W03-1805",
    "acl-W09-3607"
  ],
  "sections": [
    {
      "text": [
        "Arzucan OzgUr",
        "This paper presents an approach to summarize single scientific papers, by extracting its contributions from the set of citation sentences written in other papers.",
        "Our methodology is based on extracting significant keyphrases from the set of citation sentences and using these keyphrases to build the summary.",
        "Comparisons show how this methodology excels at the task of single paper summarization, and how it outperforms other multi-document summarization methods."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In recent years statistical physicists and computer scientists have shown great interest in analyzing complex adaptive systems.",
        "The study of such systems can provide valuable insight on the behavioral aspects of the involved agents with potential applications in economics and science.",
        "One such aspect is to understand what motivates people to provide the n + 1st review of an artifact given that they are unlikely to add something significant that has not already been said or emphasized.",
        "Citations are part of such complex systems where articles use citations as a way to mention different contributions of other papers, resulting in a collective system.",
        "The focus of this work is on the corpora created based on citation sentences.",
        "A citation sentence is a sentence in an article containing a citation and can contain zero or more nuggets (i.e., non-overlapping contributions) about the cited article.",
        "For example the following sentences are a few citation sentences that appeared in the NLP literature in past that talk about Resnik's work.",
        "The STRAND system (Resnik, 1999), for example, uses structural markup information from the pages, without looking at their content, to attempt to align them.",
        "Resnik (1999) addressed the issue of language identification for finding Web pages in the languages ofinterest.",
        "Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data.",
        ".",
        "The set of citations is important to analyze because human summarizers have put their effort collectively but independently to read the target article and cite its important contributions.",
        "This has been shown in other work too (Elkiss et al., 2008; Nanba et al., 2004; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009).",
        "In this work, we introduce a technique to summarize the set of citation sentences and cover the major contributions of the target paper.",
        "Our methodology first finds the set of keyphrases that represent important information units (i.e., nuggets), and then finds the best set of k sentences to cover more, and more important nuggets.",
        "Our results confirm the effectiveness of the method and show that it outperforms other state of the art summarization techniques.",
        "Moreover, as shown in the paper, this method does not need to calculate the full cosine similarity matrix for a document cluster, which is the most time consuming part of the mentioned baseline methods.",
        "Previous work has used citations to produce summaries of scientific work (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Elkiss et al., 2008).",
        "Other work (Bradshaw, 2003; Bradshaw, 2002) benefits from citations to determine the content of articles and introduce \"Reference Directed Indexing\" to improve the results of a search engine.",
        "In other work, (Nanba and Okumura, 1999) analyze citation sentences and automatically categorize citations into three groups using 160 predefined phrase-based rules to support a system for writing a survey.",
        "Previous research has shown the importance of the citation summaries in understanding what a paper contributes.",
        "In particular, (Elkiss et al., 2008) performed a large-scale study on citation summaries and their importance.",
        "Results from this experiment confirmed that the \"Self Cohesion\" (Elkiss et al., 2008) of a citation summary of an article is consistently higher than the that of its abstract and that citations contain additional information that does not appear in abstracts.",
        "Kan et al.",
        "(2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents.",
        "Kupiec et al.",
        "(1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization.",
        "Siddharthan and Teufel describe a new task to decide the scientific attribution of an article (Sid-dharthan and Teufel, 2007) and show high human agreement as well as an improvement in the performance of Argumentative Zoning (Teufel, 2005).",
        "Argumentative Zoning is a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, Contrast according to their role in the author's argument.",
        "These all show the importance of citation summaries and the vast area for new work to analyze them to produce a summary for a given topic.",
        "The Maximal Marginal Relevance (MMR) summarization method, which is based on a greedy algorithm, is described in (Carbonell and Goldstein, 1998).",
        "MMR uses the full similarity matrix to choose the sentences that are the least similar to the sentences already selected for the summary.",
        "We selected this method as one of our baseline methods, which we have explained in more details in Section 4.",
        "In order to evaluate our method, we use the ACL Anthology Network (AAN), which is a collection ofpapers from the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 13, 000 papers (Radev et al., 2009).",
        "We use 25 manually annotated papers from (Qazvinian and Radev, 2008), which are highly cited articles in AAN.",
        "Table 1 shows the ACL ID, title, and the number of citation sentences for these papers.",
        "The annotation guidelines asked a number of annotators to read the citation summary of each paper and extract a list of the main contributions of that paper.",
        "Each item on the list is a non-overlapping contribution (nugget) perceived by reading the citation summary.",
        "The annotation strictly instructed the annotators to focus on the citing sentences to do the task and not their own background on the topic.",
        "Then, extracted nuggets are reviewed and those nuggets that have only been mentioned by 1 annotator are removed.",
        "Finally, the union of the rest is used as a set of nuggets representing each paper."
      ]
    },
    {
      "heading": "3. Methodology",
      "text": [
        "Our methodology assumes that each citation sentence covers 0 or more nuggets about the cited papers, and tries to pick sentences that maximize nugget coverage with respect to summary length.",
        "These nuggets are essentially represented using keyphrases.",
        "Therefore, we try to extract significant keyphrases in order to represent nuggets each sentence contains.",
        "Here, the keyphrases are ex-",
        "Fact",
        "Occurrences",
        "fi: \" Supervised Learning\"",
        "5",
        "f2 : \" instance/concept relations\"",
        "3",
        "fi\\ \"Part-of-Speech tagging\"",
        "3",
        "fi.",
        "\"filtering QA results\"",
        "2",
        "/b : \"lexico-semantic information\"",
        "2",
        "fe : \"hyponym relations\"",
        "2",
        "Table 1: List of papers chosen from AAN for evaluation together with the number of sentences citing each.",
        "pressed using N-grams, and thus these building units are the key elements to our summarization.",
        "For each citation sentence dj, our method first extracts a set of important keyphrases, Dj, and then tries to find sentences that have a larger number of important and non-redundant keyphrases.",
        "In order to take the first step, we extract statistically significantly frequent N-grams (up to N = 4) from each citing sentence and use them as the set of representative keyphrases for that citing sentence.",
        "A list of keyphrases for each citation sentence can be generated by extracting N-grams that occur significantly frequently in that sentence compared to a large corpus of such N-grams.",
        "Our method for such an extraction is inspired by the previous work by Tomokiyo and Hurst (Tomokiyo and",
        "Hurst, 2003).",
        "A language model, M, is a statistical model that assigns probabilities to a sequence of N-grams.",
        "Every language model is a probability distribution over all N-grams and thus the probabilities of all N-grams of the same length sum up to 1 .",
        "In order to extract keyphrases from a text using statistical significance we need two language models.",
        "The first model is referred to as the Background Model (BM) and is built using a large text corpus.",
        "Here we build the BM using the text of all the paper abstracts provided in AAN.",
        "The second language model is called the Foreground Model (FM) and is the model built on the text from which keyphrases are being extracted.",
        "In this work, the set of all citation sentences that cite a particular target paper are used to build a foreground language model.",
        "Let gj be an N-gram of size i and Cm(q%) denote the count of gj in the model M. First, we extract the counts of each N-grams in both the background (BM) and the foreground corpora (FM).",
        "ACL-ID",
        "Title",
        "# citations",
        "N03-1017",
        "Statistical Phrase-Based Translation",
        "180",
        "P02-1006",
        "Learning Surface Text Patterns For A Question Answering System",
        "74",
        "P05-1012",
        "On-line Large-Margin Training Of Dependency Parsers",
        "71",
        "C96-1058",
        "Three New Probabilistic Models For Dependency Parsing: An Exploration",
        "66",
        "P05-1033",
        "A Hierarchical Phrase-Based Model For Statistical Machine Translation",
        "65",
        "P97-1003",
        "Three Generative, Lexicalized Models For Statistical Parsing",
        "55",
        "P99-1065",
        "A Statistical Parser For Czech",
        "54",
        "J04-4002",
        "The Alignment Template Approach To Statistical Machine Translation",
        "50",
        "D03-1017",
        "Towards Answering Opinion Questions: Separating Facts From Opinions ...",
        "42",
        "P05-1013",
        "Pseudo-Projective Dependency Parsing",
        "40",
        "WOO-0403",
        "Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ...",
        "31",
        "P03-1001",
        "Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked",
        "27",
        "N04-1033",
        "Improvements In Phrase-Based Statistical Machine Translation",
        "24",
        "A00-2024",
        "Cut And Paste Based Text Summarization",
        "20",
        "W00-0603",
        "A Rule-Based Question Answering System For Reading Comprehension Tests",
        "19",
        "A00-1043",
        "Sentence Reduction For Automatic Text Summarization",
        "19",
        "C00-1072",
        "The Automated Acquisition Of Topic Signatures For Text Summarization",
        "19",
        "W05-1203",
        "Measuring The Semantic Similarity Of Texts",
        "17",
        "W03-0510",
        "The Potential And Limitations Of Automatic Sentence Extraction For Summarization",
        "15",
        "W03-0301",
        "An Evaluation Exercise For Word Alignment",
        "14",
        "A00-1023",
        "A Question Answering System Supported By information Extraction",
        "13",
        "D04-9907",
        "Scaling Web-Based Acquisition Of Entailment Relations",
        "12",
        "P05-1014",
        "The Distributional Inclusion Hypotheses And Lexical Entailment",
        "10",
        "H05-1047",
        "A Semantic Approach To Recognizing Textual Entailment",
        "8",
        "H05-1079",
        "Recognising Textual Entailment With Logical Inference",
        "9",
        "unique",
        "all",
        "max freq",
        "uni grams bigrams",
        "3- grams",
        "4- grams",
        "229,631 2,256,385 5,125,249 6,713,568",
        "7,746,792 7,746,791 7,746,790 7,746,789",
        "437,308 73,957 3,600 2,408",
        "gie{BMuFM}",
        "E Cbm(qz)",
        "gie{BM'JFM}",
        "E cFM(gi)",
        "CFM(gi)/NFM",
        "The last equation is also known as Laplace smoothing (Manning and Schütze, 2002) and handles the N-grams in the foreground corpus that have a 0 occurrence frequency in the background corpus.",
        "Next, we extract N-grams from the foreground corpus that have significant frequencies compared to the frequency of the same N-grams in the background model and its individual terms in the foreground model.",
        "To measure how randomly a set of consecutive terms are forming an N-gram, Tomokiyo and Hurst (Tomokiyo and Hurst, 2003) use pointwise divergence.",
        "In particular, for an N-gram of size i,",
        "This equation shows the extent to which the terms forming gj have occurred together randomly.",
        "In other words, it indicates the extent ofin-formation that we lose by assuming independence of each word by applying the unigram model, instead of the N-gram model.",
        "In addition, to measure how randomly a sequence of words appear in the foreground model with respect to the background model, we use pointwise divergence as well.",
        "Here, pointwise divergence defines how much information we lose by assuming that gj is drawn from the background model instead of the foreground model:",
        "(Corley and Mihalcea, 2005) applied or utilized lexical based word overlap measures.",
        "{overlap measures, word overlap, lexical based, utilized lexical}",
        "We set the criteria of choosing a sequence of words as significant to be whether it has positive pointwise divergence with respect to both the background model, and individual terms of the foreground model.",
        "In other words we extract all gjfrom FM for which the both properties are positive:",
        "The equality condition in the second equation is specifically set to handle unigrams, in which 'PFM(9i ) = nj=1 î>FM(Wj ).",
        "In order to handle the text corpora and building the language models, we have used the CMU-Cambridge Language Model toolkit (Clarkson and Rosenfeld, 1997).",
        "We use the set of citation sentences for each paper to build foreground language models.",
        "Furthermore, we employ this tool and make the background model using nearly 11,000 abstracts from AAN.",
        "Table 3 summarizes some of the statistics about the background data.",
        "Once keyphrases (significant N-grams) of each sentence are extracted, we remove all N-grams in which more than half of the terms are stopwords.",
        "For instance, we remove all stopword unigrams, if any, and all bigrams with at least one stop-word in them.",
        "For 3-grams and 4-grams we use a threshold of 2 and 3 stopwords respectively.",
        "After that, the set of remaining N-grams is used to represent each sentence and to build summaries.",
        "Table 4 shows an example of a citation sentence from D06-1621 citing W05-1203 (Corley and Mi-halcea, 2005), and its extracted bigrams.",
        "After extracting the set of keyphrases for each sentence, dj, the sentence is represented using its set of N-grams, denoted by Di.",
        "Then, the goal is to pick sentences (sets) for each paper that cover more important and non-redundant keyphrases.",
        "Essentially, keyphrases that have been repeated in more sentences are more important and could represent more important nuggets.",
        "Therefore, sentences that contain more frequent keyphrases are more important.",
        "Based on this intuition we define the reward of building a summary comprising a set of keyphrases S as where A is the set of all keyphrases from sentences not in the summary.",
        "The set function f has three main properties.",
        "First, it is non-negative.",
        "Second, it is monotone (i.e., For every set v we have f (S + v) > f (S)).",
        "Third, f is sub-modular.",
        "The submodular-ity means that for a set v and two sets S ç T we have",
        "Intuitively, this property implies that adding a set v to S will increase the reward at least as much as it would to a larger set T. In the summarization setting, this means that adding a sentence to a smaller summary will increase the reward ofthe summary at least as much as adding it to a larger summary that subsumes it.",
        "The following theorem formalizes this and is followed by a proof.",
        "Theorem 1 The rewardfunction f is submodular.",
        "We start by defining a gain function G of adding sentence (set) Di to Ski where Ski is the set of keyphrases in a summary built using k – 1 sentences, and Di is a candidate sentence to be added:",
        "Simple investigation through a Venn diagram proof shows that G can be rewritten as",
        "Let's denote Din (Uj=iDj) by ni.",
        "The following equations prove the theorem.",
        "n nsk-i 3 n nsk",
        "Here, S'k is the set of all N-grams in the vocabulary that are not present in Sk.",
        "The gain of adding a sentence, Di, to an empty summary is a non-negative value.",
        "By induction, we will get",
        "Theorem 1 implies the general case of submodu-larity:",
        "Maximizing this submodular function is an NP-hard problem (Khuller et al., 1999).",
        "A common way to solve this maximization problem is to start with an empty set, and in each iteration pick a set that maximizes the gain.",
        "It has been shown before in (Kulik et al., 2009) that if f is a submodular, nondecreasing set function and f (0) = 0, then such a greedy algorithm finds a set S, whose gain is at least as high as (1 – 1/e) of the best possible solution.",
        "Therefore, we can optimize the keyphrase coverage as described in Algorithm 1."
      ]
    },
    {
      "heading": "4. Experimental Setup",
      "text": [
        "We use the annotated data described in Section 2.",
        "In summary, the annotation consisted oftwo parts: nugget extraction and nugget distribution analysis.",
        "Five annotators were employed to annotate the sentences in each of the 25 citation summaries and write down the nuggets (non-overlapping contributions) of the target paper.",
        "Then using these Algorithm 1 The greedy algorithm for summary generation k the number of sentences in the summary Di keyphrases in di si <r- argmaxD.eD |Di n (Uj=iDj)|",
        "end for end for return S nugget sets, each sentence was annotated with the nuggets it contains.",
        "This results in a sentence-fact matrix that helps with the evaluation of the summary.",
        "The summarization goal and the intuition behind the summarizing system is to select a few (5 in our experiments) sentences and cover as many nuggets as possible.",
        "Each sentence in a citation summary may contain 0 or more nuggets and not all nuggets are mentioned an equal number of times.",
        "Covering some nuggets (contributions) is therefore more important than others and should be weighted highly.",
        "To capture this property, the pyramid score seems the best evaluation metric to use.",
        "We use the pyramid evaluation method (Nenkova and Pas-sonneau, 2004) at the sentence level to evaluate the summary created for each set.",
        "We benefit from the list of annotated nuggets provided by the annotators as the ground truth of the summarization evaluation.",
        "These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004).",
        "The pyramid score for a summary is calculated as follows.",
        "Assume a pyramid that has n tiers, Ti, where tier Ti > Tj if i > j (i.e., Ti is not below Tj, and that if a nugget appears in more sentences, it falls in a higher tier.).",
        "Tier Ti contains nuggets that appeared in i sentences, and thus has weight i.",
        "Suppose | Ti| shows the number of nuggets in tier Ti, and Qi is the size of a subset of Ti whose members appear in the summary.",
        "Further suppose Q shows the sum of the weights of the facts that are covered by the summary.",
        "Q = ^n=1 i x Qi.",
        "In addition, the optimal pyramid score for a summary with X facts, is",
        "where j = maxi£n=i |Tt| > X).",
        "The pyramid score for a summary is then calculated as follows.",
        "This score ranges from 0 to 1, and a high score shows the summary contains more heavily weighted facts.",
        "To evaluate the quality of the summaries generated by the greedy algorithm, we compare its pyramid score in each of the 25 citation summaries with those of a gold standard, a random summary, and four other methods.",
        "The gold standards are summaries created manually using 5 sentences.",
        "The 5 sentences are manually selected in a way to cover as many nuggets as possible with higher priority for the nuggets with higher frequencies.",
        "We also created random summaries using Mead (Radev et al., 2004).",
        "These summaries are basically a random selection of 5 sentences from the pool of sentences in the citation summary.",
        "Generally we expect the summaries created by the greedy method to be significantly better than random ones.",
        "Summary generated using bigram-based keyphrases",
        "ID",
        "Sentence",
        "P06-1048:1 J05-4004:18",
        "A00-2024:9 N03-1026T7",
        "P06-2019:5",
        "Ziff-Davis Corpus Most previous work (Jing 2000; Knight and Marcu 2002; Riezler et al. 2003; Nguyen et al. 2004a; Turner and Charniak 2005; McDonald 2006) has relied on automatically constructed parallel corpora for training and evaluation purposes.",
        "Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandras ekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al. 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression paume III and Marcu 2002; Daumé III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process.",
        "The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts.",
        "To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000).",
        "Jing (2000) was perhaps the first to tackle the sentence compression problem.",
        "In addition to the gold and random summaries, we also used 4 baseline state of the art sum-marizers: LexRank, the clustering C-RR and C-LexRank, and Maximal Marginal Relevance (MMR).",
        "LexRank (Erkan and Radev, 2004) works based on a random walk on the cosine similarity of sentences and prints out the most frequently visited sentences.",
        "Said differently, LexRank first builds a network in which nodes are sentences and edges are cosine similarity values.",
        "It then uses the eigenvalue centralities to find the most central sentences.",
        "For each set, the top 5 sentences on the list are chosen for the summary.",
        "The clustering methods, C-RR and C-LexRank, work by clustering the cosine similarity network of sentences.",
        "In such a network, nodes are sentences and edges are cosine similarity of node pairs.",
        "Clustering would intuitively put nodes with similar nuggets in the same clusters as they are more similar to each other.",
        "The C-RR method as described in (Qazvinian and Radev, 2008) uses a round-robin fashion to pick sentences from each cluster, assuming that the clustering will put the sentences with similar facts into the same clusters.",
        "Unlike C-RR, C-LexRank uses LexRank to find the most salient sentences in each cluster, and prints out the most central nodes of each cluster as summary sentences.",
        "Finally, MMR uses the full cosine similarity matrix and greedily chooses sentences that are the least similar to those already selected for the summary (Carbonell and Goldstein, 1998).",
        "In particular,",
        "max Sim(di, dj)",
        "where A is the set of sentences in the summary, initially set to A = 0.",
        "This method is different from ours in that it chooses the least similar sentence to the summary in each iteration.",
        "As mentioned before, we use the text of the abstracts of all the papers in AAN as the background, and each citation set as a separate foreground corpus.",
        "For each citation set, we use the method described in Section 3.1 to extract significant N-grams of each sentence.",
        "We then use the keyphrase set representation of each sentence to build the summaries using Algorithm 1.",
        "For each of the 25 citation summaries, we build 4 different summaries using unigrams, bigrams, 3-grams, and 4-grams respectively.",
        "Table 5 shows a 5-sentence summary created using algorithm 1 for the paper A00-1043 (Jing, 2000).",
        "The pyramid scores for different methods are reported in Figure 1 together with the scores of gold standards, manually created to cover as many nuggets as possible in 5 sentences, as well as summary evaluations of the 4 baseline methods described above.",
        "This Figure shows how the keyphrase based summarization method when employing N-grams of size 3 or smaller, outperforms other baseline systems significantly.",
        "More importantly, Figure 1 also indicates that this method shows more stable results and low variation in summary quality when keyphrases ofsize 3 or smaller are employed.",
        "In contrast, MMR shows high variation in summary qualities making summaries that obtain pyramid scores as low as 0.15.",
        "Another important advantage of this method is that we do not need to calculate the cosine similarity of the pairs of sentences, which would add a running time of O ( | D |1V | ) in the number of documents, |D|, and the size of the vocabulary | V| to the algorithm."
      ]
    },
    {
      "heading": "5. Conclusion and Future Work",
      "text": [
        "This paper presents a summarization methodology that employs keyphrase extraction to find important contributions of scientific articles.",
        "The summarization is based on citation sentences and picks sentences to cover nuggets (represented by keyphrases) or contributions of the target papers.",
        "In this setting the best summary would have as few sentences and at the same time as many nuggets as possible.",
        "In this work, we use pointwise KL-divergence to extract statistically significant N-grams and use them to represent nuggets.",
        "We then apply a new set function for the task of summarizing scientific articles.",
        "We have proved that this function is submodular and concluded that a",
        "Figure 1: Evaluation Results (summaries with 5 sentences): The median pyramid score over 25 datasets using different methods.",
        "greedy algorithm will result in a near-optimum set of covered nuggets using only 5 sentences.",
        "Our experiments in this paper confirm that the summaries created based on the presented algorithm are better than randomly generated summary, and also outperform other state of the art summarization methods in most cases.",
        "Moreover, we show how this method generates more stable summaries with lower variation in summary quality when N-grams of size 3 or smaller are employed.",
        "A future direction for this work is to perform post-processing on the summaries and regenerate sentences that cover the extracted nuggets.",
        "However, the ultimate goal is to eventually develop systems that can produce summaries of entire research areas, summaries that will enable researchers to easily and quickly switch between fields of research.",
        "One future study that will help us generate better summaries is to understand how nuggets are generated by authors.",
        "In fact, modeling the nugget coverage behavior of paper authors will help us identify more important nuggets and discover some aspects of the paper that would otherwise be too difficult by just reading the paper itself."
      ]
    },
    {
      "heading": "6. Acknowledgements",
      "text": [
        "This work is in part supported by the National Science Foundation grant \"iOPENER: A Flexible Framework to Support Rapid Learning in Unfamiliar Research Domains\", jointly awarded to University of Michigan and University of Maryland as IIS 0705832, and in part by the NIH Grant U54 DA021519 to the National Center for Integrative Biomedical Informatics.",
        "Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the supporters."
      ]
    }
  ]
}
