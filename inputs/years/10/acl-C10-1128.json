{
  "info": {
    "authors": [
      "Xiaojun Wan"
    ],
    "book": "COLING",
    "id": "acl-C10-1128",
    "title": "Towards a Unified Approach to Simultaneous Single-Document and Multi-Document Summarizations",
    "url": "https://aclweb.org/anthology/C10-1128",
    "year": 2010
  },
  "references": [
    "acl-C08-1062",
    "acl-C08-1124",
    "acl-D08-1080",
    "acl-D09-1044",
    "acl-I05-2004",
    "acl-N03-1020",
    "acl-P02-1058",
    "acl-P06-1039",
    "acl-P07-1070",
    "acl-P08-2052",
    "acl-W04-3247",
    "acl-W04-3252",
    "acl-W97-0704"
  ],
  "sections": [
    {
      "text": [
        "Towards a Unified Approach to Simultaneous Single-Document and",
        "Multi-Document Summarizations",
        "Institute of Compute Science and Technology The MOE Key Laboratory of Computational Linguistics Peking University",
        "wanxiaojun@icst.pku.edu.cn",
        "Single-document summarization and multi-document summarization are very closely related tasks and they have been widely investigated independently.",
        "This paper examines the mutual influences between the two tasks and proposes a novel unified approach to simultaneous single-document and multi-document summarizations.",
        "The mutual influences between the two tasks are incorporated into a graph model and the ranking scores of a sentence for the two tasks can be obtained in a unified ranking process.",
        "Experimental results on the benchmark DUC datasets demonstrate the effectiveness of the proposed approach for both single-document and multi-document summarizations."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Single-document summarization aims to produce a concise and fluent summary for a single document, and multi-document summarization aims to produce a concise and fluent summary for a document set consisting of multiple related documents.",
        "The two tasks are very closely related in both task definition and solution method.",
        "Moreover, both of them are very important in many information systems and applications.",
        "For example, given a cluster of news articles, a multi-document summary can be used to help users to understand the whole cluster, and a single summary for each article can be used to help users to know the content of the specified article.",
        "To date, single-document and multi-document summarizations have been investigated extensively and independently in the NLP and IR fields.",
        "A series of special conferences or workshops on automatic text summarization (e.g.",
        "SUMMAC, DUC, NTCIR and TAC) have advanced the technology and produced a couple of experimental online systems.",
        "However, the two summarization tasks have not yet been simultaneously investigated in a unified framework.",
        "Inspired by the fact that the two tasks are very closely related and they can be used simultaneously in many applications, we believe that the two tasks may have mutual influences on each other.",
        "In this study, we propose a unified approach to simultaneous single-document and multi-document summarizations.",
        "The mutual influences between the two tasks are incorporated into a graph-based model.",
        "The ranking scores of sentences for single-document summarization and the ranking scores of sentences for multi-document summarization can boost each other, and they can be obtained simultaneously in a unified graph-based ranking process.",
        "To the best of our knowledge, this study is the first attempt for simultaneously addressing the two summarization tasks in a unified graph-based framework.",
        "Moreover, the proposed approach can be easily adapted for topic-focused summa-rizations.",
        "Experiments have been performed on both the single-document and multi-document summarization tasks of DUC2001 and DUC2002.",
        "The results demonstrate that the proposed approach can outperform baseline independent methods for both the two summarization tasks.",
        "The two tasks are validated to have mutual influences on each other.",
        "The rest of this paper is organized as follows: Section 2 introduces related work.",
        "The details of the proposed approach are described in Section 3.",
        "Section 4 presents and discusses the evaluation results.",
        "Lastly we conclude our paper in Section 5."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Document summarization methods can be either extraction-based or abstraction-based.",
        "In this section, we focus on extraction-based methods.",
        "Extraction-based methods for single-document summarization usually assign a saliency score to each sentence in a document and then rank and select the sentences.",
        "The score is usually computed based on a combination of statistical and linguistic features, such as term frequency, sentence position, cue words and stigma words (Luhn, 1969; Edmundson, 1969; Hovy and Lin, 1997).",
        "Machine learning techniques have also been used for sentence extraction (Kupiec et al., 1995; Conroy and O'Leary, mutual reinforcement principle has been exploited to iteratively extract key phrases and sentences from a document (Zha, 2002; Wan et al., 2007a).",
        "Wan et al.",
        "(2007b) propose the Col-labSum algorithm to use additional knowledge in a cluster of documents to improve single document summarization in the cluster.",
        "In recent years, graph-based ranking methods have been investigated for document summarization, such as TextRank (Mihalcea and Tarau, 2004; Mihalcea and Tarau, 2005) and LexPag-eRank (ErKan and Radev, 2004).",
        "Similar to PageRank (Page et al., 1998), these methods first build a graph based on the similarity relationships between the sentences in a document and then the saliency of a sentence is determined by making use of the global information on the graph recursively.",
        "The basic idea underlying the graph-based ranking algorithm is that of \"voting\" or \"recommendation\" between sentences.",
        "Similar methods have been used for generic multi-document summarization.",
        "A typical method is the centroid-based method (Radev et al., 2004).",
        "For each sentence, the method computes a score based on each single feature (e.g. cluster centroids, position and TFIDF) and then linearly combines all the scores into an overall sentence score.",
        "Topic signature is used as a novel feature for selecting important content in NeATS (Lin and Hovy, 2002).",
        "Various sentence features have been combined by using machine learning techniques (Wong et al., 2008).",
        "A popular way for removing redundancy between summary sentences is the MMR algorithm (Car-bonell and Goldstein, 1998).",
        "Themes (or topics, clusters) in documents have been discovered and used for sentence selection (Harabagiu and La-catusu, 2005).",
        "Hachey (2009) investigates the effect of various source document representations on the accuracy of the sentence extraction phase of a multi-document summarization task.",
        "Graph-based methods have also been used to rank sentences in a document set.",
        "The methods first construct a graph to reflect sentence relationships at different granularities, and then compute sentence scores based on graph-based learning algorithms.",
        "For example, Wan (2008) proposes to use only cross-document relationships for graph building and sentence ranking.",
        "Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008).",
        "For topic-focused multi-document summarization, many methods are extensions of generic summarization methods by incorporating the information of the given topic or query into generic summarizers.",
        "In recent years, a few novel methods have been proposed for topic-focused summarization (Daum√© and Marcu, 2006; Wan",
        "Schilder and Kondadadi, 2008; Wei et al., 2008).",
        "The above previous graph-based summarization methods aim to address either single-document summarization or multi-document summarization, and the two summarization tasks have not yet been addressed in a unified graph-based framework."
      ]
    },
    {
      "heading": "3. The Unified Summarization Approach",
      "text": [
        "Given a document set, in which the whole document set and each single document in the set are required to be summarized, we use local saliency to indicate the importance of a sentence in a particular document, and use global saliency to indicate the importance of a sentence in the whole document set.",
        "In previous work, the following two assumptions are widely made for graph-based summarization models:",
        "Assumption 1: A sentence is locally important in a particular document if it is heavily linked with many locally important sentences in the same document.",
        "Assumption 3: A sentence is locally important in a particular document, if it is heavily linked with many globally important sentences in the document set.",
        "Assumption 2: A sentence is globally important in the document set if it is heavily linked with many globally important sentences in the document set.",
        "The above assumptions are the basis for Pag-eRank-like algorithms for single document summarization and multi-document summarization, respectively.",
        "In addition to the above two assumptions, we make the following two assumptions to consider the mutual influences between the two summarization tasks:",
        "The above assumption is reasonable because the documents in the set are relevant and the globally important information in the document set will be expressed in many single documents.",
        "Therefore, if a sentence is salient in the whole document set, the sentence may be salient in a particular document in the set.",
        "Assumption 4: A sentence is globally important in the document set, if it is heavily linked with many locally important sentences.",
        "The above assumption is reasonable because the documents in the set are relevant and the globally important information in the whole set is the aggregation of the locally important information in each single document.",
        "Therefore, if a sentence is salient in a particular document, the sentence has the potential to be salient in the whole document set.",
        "In brief, the local saliency and global saliency of a sentence can mutually influence and boost each other: high local saliency will lead to high global saliency, and high global saliency will lead to high local saliency.",
        "Based on the above assumptions, our proposed approach first builds affinity graphs (each graph is represented by an affinity matrix) to reflect the different kinds of relationships between sentences, respectively, and then iteratively computes the local saliency scores and the global saliency scores of the sentences based on the graphs.",
        "Finally, the algorithm converges and the local saliency score and global saliency score of each sentence are obtained.",
        "The sentences with high local saliency scores in a particular document are chosen into the summary of the single document, and the sentences with high global saliency scores in the set are chosen into the summary of the document set.",
        "Note that for both summarization tasks, after the saliency scores of sentences have been obtained, the greedy algorithm used in (Wan et al., 2007c) is applied to remove redundancy and finally choose both informative and novel sentences into the summary.",
        "Formally, the given document set is denoted as D={di\\1<i<m}, and the whole sentence set is denoted as S={si\\1<i<n}.",
        "We let Infosingle(si) denote the local saliency score of sentence si in a particular document d(si) G D, and it is used to select summary sentences for the single document d(si).",
        "And we let Infomulti(si) denote the global saliency score of sentence si in the whole document set D, and it is used to select summary sentences for the document set D.",
        "The four assumptions in Section 3.1 can be rendered as follows:",
        "Infomsee(s,) - Sj(WA)√üInfosmgk(Sj) (1) Infomult, (s, ) - Sj (Wb )ji /nfomulti (sj ) (2)Infosingee (s,) - Sj(Wc)jJnfomiltAsj) (3)",
        "where Wa, Wb, Wc, Wd are nxn affinity matrices reflecting the different kinds of relationships between sentences in the document set, where n is the number of all sentences in the document set.",
        "The detailed derivation of the matrices will be presented later.",
        "After fusing the above equations, we can obtain the following unified forms:",
        "Infosin gee (si) = MS j (WA ) jiInfo'sin gle (s j ) InfomuU (si ) = MSj (WB ) ji InfomuU (sj )",
        "+ (1 -M)Sj (Wd )jiInfosin see (sj ) (6)However, the above summarization method ignores the feature of sentence position, which has been validated to be very important for document summarizations.",
        "In order to incorporate this important feature, we add one prior score to each computation as follows:",
        "Infosin Sle (si ) = aSj (WA ) ji Infosin g,e (s j ) Inf¬∞mutti (S< ) = aYjJ (WB ) jiInf¬∞mutti (SJ )",
        "where a, √ü, y G [0,1] specify the relative contributions to the final saliency scores from the different factors, and we have a+√ü+y=1.",
        "pri-¬∞rs,ngle(s,) is the prior score for the local saliency of sentence s;-, and here pri¬∞rsingie(s1) is computed based on sentence position of si in the particular document d(si).",
        "pri¬∞rmuiti(s1) is the prior score for the global saliency of sentence si, and we also compute pri¬∞rmu ti(si) based on sentence position of si.",
        "We use two column vectors √º =[Inf¬∞singie(Si)]n*i and v =[Inf¬∞muiti(Si)]nxi to denote the local and global saliency scores of all the sentences in the set, respectively.",
        "And the matrix forms of the above equations are as follows:",
        "[pri¬∞rsin Sk (Si )]nx1",
        "nx1 are the prior column vectors.",
        "The above matrices and prior vectors are constructed as follows, respectively:",
        "WA: This affinity matrix aims to reflect the local relationships between sentences in each single document, which is defined as follows: fsim cos ne (s,, sj ), if d(s; ) = d(sj ) 0, Otherwise where d(si) refers to the document containing sentence s;-.",
        "simc¬∞sine(si,sj) is the cosine similarity between sentences si and sj.",
        "where s i and s are the corresponding term vectors of s; and sj.",
        "Note that we have (WOj = (WA)ji, and we have (WA)ii =0 to avoid self loops.",
        "We can see that the matrix contains only the within-document relationships between sentences.",
        "WB: This affinity matrix aims to reflect the global relationships between sentences in the document set, which is defined as follows:",
        "We can see that the matrix contains only the cross-document relationships between sentences.",
        "We do not include the within-document sentence relationships in the matrix because it has been shown that the cross-document relationships are more appropriate to reflect the global mutual influences between sentences than the within-document relationships in (Wan, 2008).",
        "Wc: This affinity matrix aims to reflect the cross-document relationships between sentences in the document set.",
        "However, the relationships in this matrix are used for carrying the influences of the sentences in other documents on the local saliency of the sentences in a particular document.",
        "If we directly use Equation (13) to compute the matrix, the mutual influences would be overly used.",
        "Because other documents might not be sampled from the same generative model as the specified document, we probably do not want to trust them so much as the specified document.",
        "Thus a confidence value is used to reflect out belief that the document is sampled from the same underlying model as the specified document.",
        "Heuristically, we use the cosine similarity between documents as the confidence value.",
        "And we use the confidence value as the decay factor in the matrix computation as follows:",
        "(simcosine 0/, sj ) X simcosine (dOi X d(sj √Ñ",
        "[0, Otherwise",
        "WD: This affinity matrix aims to reflect the within-document relationships between sentences.",
        "Thus we have WD=WA, which means that the global saliency score of a sentence is influenced only by the local saliency scores of the sentences in the same document, without considering the sentences in other documents.",
        "Note that the above four matrices are symmetric and we can replace WTA , Wj , and WD by WA, WB, WC and WD in Equations (9) and (10), respectively.",
        "priorsingle(Si): It is computed under the assumption that the first sentences in a document are usually more important than other sentences.",
        "p¬∞siti¬∞n(si ) +1 (15) where p¬∞siti¬∞n(si) returns the position number of sentence s; in its document d(si).",
        "For example, if",
        "si is the first sentence in its document, p¬∞siti¬∞n(si) is 1.",
        "The prior weight is then normalized by:",
        "Si pri¬∞rsin glle (si )",
        "priorm√º[ti(si): We also let the prior weight reflect the influence of sentence position.",
        "pri¬∞rmulti (si ) = pri¬∞rsingle (si ) (17)",
        "And then the prior weight is normalized in the same way.",
        "The above definitions are for generic document summarizations and the above algorithm can be easily adapted for topic-focused summa-rizations.",
        "Given a topic q, the only change for the above computation is pri¬∞rmulti(si).",
        "The topic relevance is incorporated into the prior weight as follows:",
        "V T]T..",
        "'ccWl √üWl",
        "In order to solve the iterative problem defined",
        "To guarantee the solution of the above linear system, W is normalized by columns.",
        "If all the elements of a column are zero, we replace the elements with 1/(2n), where 2n equals to the element number of the column.",
        "We then multiply W by a decay factor 6 (0<0<1) to scale down each element in W, but remain the meaning of W. Here, 6 is empirically set to 0.6.",
        "Finally, Equation (21) is rewritten as follows:",
        "Thus, the matrix (I-6W) is a strictly diagonally dominant matrix and the solution of the linear system exists and we can apply the Gauss-Seidel method used in (Li et al., 2008) to solve the linear system.",
        "The GS method is a well-know method for numeric computation in mathematics and the details of the method is omitted here."
      ]
    },
    {
      "heading": "4. Empirical Evaluation",
      "text": [
        "Generic single-document and multi-document summarizations have been the fundamental tasks in DUC 2001 and DUC 2002 (i.e. tasks 1 and 2 in DUC 2001 and tasks 1 and 2 in DUC 2002), and we used the two datasets for evaluation.",
        "DUC2001 provided 309 articles, which were grouped into 30 document sets.",
        "Generic summary of each article was required to be created for task 1, and generic summary of each document set was required to be created for task 2.",
        "The summary length was 100 words or less.",
        "DUC 2002 provided 59 document sets consisting of 567 articles (D088 is excluded from the original 60 document sets by NIST) and generic summaries for each article and each document set with a length of approximately 100 words were required to be created.",
        "The sentences in each article have been separated and the sentence information has been stored into files.",
        "The summary of the two datasets are shown in Table 1.",
        "We used the ROUGE toolkit (Lin and Hovy, 2003) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation.",
        "It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary.",
        "The ROUGE toolkit reported separate recall-oriented scores for 1, 2, 3 and 4-gram, and also for longest common subsequence cooccurrences.",
        "We showed three of the ROUGE metrics in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W (based on weighted longest common subsequence, weight=1.2).",
        "In order to truncate summaries longer than the length limit,",
        "Yjipri¬∞rm",
        "then the iterative equations correspond to following linear system :",
        "DUC 2001",
        "DUC 2002",
        "Task",
        "Tasks 1, 2",
        "Tasks 1, 2",
        "Number of documents",
        "309",
        "567",
        "Number of clusters",
        "30",
        "59",
        "Data source",
        "TREC-9",
        "TREC-9",
        "summary length",
        "100 words",
        "100 words",
        "we used the \"-l 100\" option in ROUGE toolkit.",
        "We also used the \"-m\" option for word stemming.",
        "In the experiments, the combination weight y for the prior score is fixed at 0.15, as in the PageRank algorithm.",
        "Therefore, we have a+√ü=0.85.",
        "Here, we use a/(a+√ü) to indicate the relative contributions of the first two parts in Equations the experiments.",
        "The proposed unified approach (i.e. UnifiedRank) is compared with a few baseline approaches and the top three participating systems.",
        "The graph-based baselines for multi-document summarization are described as follows:",
        "BasicRank: This baseline approach adopts the basic PageRank algorithm to rank sentences based on all sentence relationships in a single document, similar to previous work (Mihalcea and Tarau, 2004).",
        "PositionRank: This baseline approach improves the basic PageRank algorithm by using the position weight of a sentence as the prior score for the sentence.",
        "The position weight of a sentence is computed by using Equation (15).",
        "CollabRank1: This baseline approach is the \"UniformLink(Gold)\" approach proposed in (Wan et al.",
        "2007b).",
        "It uses a cluster of multiple documents to improve single document summarization by constructing a global affinity graph.",
        "CollabRank2: This baseline approach is the \"UnionLink(Gold)\" approach proposed in (Wan et al.",
        "2007b).",
        "BasicRank: This baseline approach adopts the basic PageRank algorithm to rank sentences based on all sentence relationships in document set.",
        "Both within-document and cross-document sentence relationships are used for constructing the affinity graph.",
        "PositionRank: Similarly, this baseline approach improves the basic PageRank algorithm by using the position weight of a sentence as the prior score for the sentence.",
        "TwoStageRank: This baseline approach leverages the results of single document summarization for multi-document summarization.",
        "It first computes the score of each sentence within each single document by using the PositionRank method, and then computes the final score of each sentence within the document set by considering the document-level sentence score as the prior score in the improved PageRank algorithm.",
        "The top three systems are the systems with highest ROUGE scores, chosen from the participating systems on each task, respectively.",
        "Tables 2 and 3 show the comparison results for single-document summarization on DUC2001 and DUC2002, respectively.",
        "Tables 4 and 5 show the comparison results for multi-document summarization on DUC2001 and DUC2002, respectively.",
        "In the tables, SystemX (e.g. Sys-tem28, SystemN) represents one of the top performing systems.",
        "The systems are sorted by decreasing order of the ROUGE-1 scores.",
        "For single-document summarization, the proposed UnifiedRank approach always outperforms the four graph-based baselines over all three metrics on both two datasets.",
        "The performance differences are all statistically significant by using t-test (p-value<0.05).",
        "The ROUGE-1 score of UnifiedRank is higher than that of the best participating systems and the",
        "ROUGE-2 and ROUGE-W scores of UnifiedRank are comparable to that of the best participating systems.",
        "For multi-document summarization, the proposed UnifiedRank approach outperforms all the three graph-based baselines over all three metrics on the DUC2001 dataset, and it outperforms the three baselines over ROUGE-1 and",
        "ROUGE-W on the DUC2002 dataset.",
        "In particular, UnifiedRank can significantly outperform BasicRank and TwoStageRank over all three metrics on the DUC2001 dataset (t-test, p-",
        "ROUGE-W scores of UnifiedRank are higher than that of the best participating systems and the ROUGE-2 score of UnifiedRank is comparable to that of the best participating systems.",
        "The results demonstrate that the single-document and multi-document summarizations can benefit each other by making use of the mutual influences between the local saliency and global saliency of the sentences.",
        "Overall, the proposed unified graph-based approach is effective for both single document summarization and multi-document summarization.",
        "However, the performance improvement for single-document summarization is more significant than that for multi-document summarization, which shows that the global information in a document set is very beneficial to summarization of each single document in the document set.",
        "summarization on DUC2002",
        "In the above experiments, the relative contributions from the first two parts in Equations (9) and (10) are empirically set as a/(a+√ü)=0.4.",
        "In this section, we investigate how the relative contributions influence the summarization performance by varying a/(a+√ü) from 0 to 1.",
        "A small value of a/(a+√ü) indicates that the contribution from the same kind of saliency scores of the sentences is less important than the contribution from the different kind of saliency scores of the sentences, and vice versa.",
        "Figures 1-8 show the",
        "ROUGE-1 and ROUGE-W curves for singledocument summarization and multi-document summarization on DUC2001 and DUC2002, respectively.",
        "For single document summarization, very small value or very large value for a/(a+√ü) will lower the summarization performance values on the two datasets.",
        "The results demonstrate that both the two kinds of contributions are important to the final performance of single document summarization.",
        "For multi-document summarization, a relatively large value (>0.4) for a/(a+√ü) will lead to relatively high performance values on the DUC2001 dataset, but a very large value for a/(a+√ü) will decrease the performance values.",
        "On the DUC2002 dataset, a relatively small value (<0.4) will lead to relatively high performance values, but a very small value for a/(a+√ü) will decrease the performance values.",
        "Though the trends of the curves on the consistent with each other, the results show that both the two kinds of contributions are beneficial to the final performance of multi-document summarization."
      ]
    },
    {
      "heading": "5. Conclusion and Future Work",
      "text": [
        "In this study, we propose a novel unified approach to simultaneous single-document and multi-document summarization by making using of the mutual influences between the two tasks.",
        "Experimental results on the benchmark DUC datasets show the effectiveness of the proposed approach.",
        "In future work, we will perform comprehensive experiments for topic-focused document summarizations to show the robustness of the proposed approach.",
        "System",
        "ROUGE-1",
        "ROUGE-2",
        "ROUGE-W",
        "UnifiedRank",
        "0.45377",
        "0.17649",
        "0.14328",
        "CollabRank2",
        "0.44038",
        "0.16229",
        "0.13678",
        "CollabRank1",
        "0.43890",
        "0.16213",
        "0.13676",
        "PositionRank",
        "0.43596",
        "0.15936",
        "0.13684",
        "BasicRank",
        "0.43407",
        "0.15696",
        "0.13629",
        "Table 2.",
        "Comparison results for single-document summarization on DUC2001",
        "System",
        "ROUGE-1",
        "ROUGE-2",
        "ROUGE-W",
        "UnifiedRank",
        "0.48478",
        "0.21462",
        "0.16877",
        "System28",
        "0.48049",
        "0.22832",
        "0.17073",
        "System21",
        "0.47754",
        "0.22273",
        "0.16814",
        "CollabRank1",
        "0.47187",
        "0.20102",
        "0.16318",
        "CollabRank2",
        "0.47028",
        "0.20046",
        "0.16260",
        "PositionRank",
        "0.46618",
        "0.19853",
        "0.16180",
        "System31",
        "0.46506",
        "0.20392",
        "0.16162",
        "BasicRank",
        "0.46261",
        "0.19457",
        "0.16018",
        "Table 3.",
        "Comparison results for single-document summarization on DUC2002",
        "System",
        "ROUGE-1",
        "ROUGE-2",
        "ROUGE-W",
        "UnifiedRank",
        "0.36360",
        "0.06496",
        "0.10950",
        "PositionRank",
        "0.35733",
        "0.06092",
        "0.10798",
        "BasicRank",
        "0.35527",
        "0.05608",
        "0.10641",
        "TwoStageRank",
        "0.35221",
        "0.05500",
        "0.10515",
        "SystemN",
        "0.33910",
        "0.06853",
        "0.10240",
        "SystemP",
        "0.33332",
        "0.06651",
        "0.10068",
        "SystemT",
        "0.33029",
        "0.07862",
        "0.10215",
        "Table 4.",
        "Comparison results for multi-document summarization on DUC2001",
        "System",
        "ROUGE-1",
        "ROUGE-2",
        "ROUGE-W",
        "UnifiedRank",
        "0.38343",
        "0.07855",
        "0.12341",
        "PositionRank",
        "0.38056",
        "0.08238",
        "0.12292",
        "TwoStageRank",
        "0.37972",
        "0.08166",
        "0.12261",
        "BasicRank",
        "0.37595",
        "0.08304",
        "0.12173",
        "System26",
        "0.35151",
        "0.07642",
        "0.11448",
        "System19",
        "0.34504",
        "0.07936",
        "0.11332",
        "System28",
        "0.34355",
        "0.07521",
        "0.10956"
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "(NCET-08-0006)."
      ]
    }
  ]
}
