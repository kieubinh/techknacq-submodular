{
  "info": {
    "authors": [
      "Jian Zhang",
      "Pascale Fung"
    ],
    "book": "COLING",
    "id": "acl-C10-1146",
    "title": "A Rhetorical Syntax-Driven Model for Speech Summarization",
    "url": "https://aclweb.org/anthology/C10-1146",
    "year": 2010
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Jian Zhang and Pascale Fung",
        "Human Language Technology Center Department of Electronic & Computer Engineering Hong Kong University of Science & Technology (HKUST)",
        "{zjustin, pascale}@ece.ust.hk",
        "We show a novel approach of parsing and reordering rhetorical syntax tree for extractive summarization of presentation speech.",
        "Our previous work showed (Fung et al., 2008) that rhetorical structures are embedded in this type of speech and that exploring this structure helps improve summarization quality.",
        "We further demonstrate that speakers do not follow the strict order of bullet points in the presentation slides, and that a reordering of these points occurs.",
        "We therefore propose a method of parsing presentation transcriptions into a rhetorical syntax tree and then reorder the leaf nodes to transform the speech transcriptions into an extractive summary, akin to a process of presentation slide generation.",
        "Chunking, parsing, and reordering are carried out by 28-class Hidden Markov Support Vector Machine(HMSVM) classifier trained from reference presentations and presentation slides.",
        "Using ROUGE-L F-measure we showed that our rhetorical syntax-driven model gives a 35.8% relative improvement over a binary summarizer with no rhetorical information, a 14.3% improvement over Rhetorical State Hidden Markov Model(RSHMM) (Fung et al., 2008), and a 4.3% improvement over our proposed model with no reordering."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In this paper, we propose to improve extractive summarization of presentation speech using parsing and reordering of the salient points in the speech.",
        "Presentation speech includes classroom lectures, conference talks, business seminars, as well as political debates and parliamentary speech where the speaker gives a presentation according to some prepared slides containing bullet points.",
        "Some of the speech are transcribed into text, others might even be accompanied by short abstracts.",
        "Nevertheless, for learning and collaboration purposes, transcribed text is too long to read whereas short abstracts do not contain enough information (Teufel and Moens, 2002).",
        "Especially, in our conference presentation corpus, on average only about 40% bullet points of each transcription appears in the corresponding conference paper abstract.",
        "The accompanying presentation slides, on the other hand, are much better in summarizing the gists.",
        "In recent years, more research has been conducted on exploring the hierarchical structure for better summarization performance (Fung et al., 2003; Murray et al., 2006; Sauper and Regina, 2009; Tatar et al., 2008).",
        "Unlike text documents, the structure of a spoken document is not immediately apparent in terms of its layout.",
        "However, researchers have shown that structural characteristics of a speech are clearly rendered by not just its linguistic features but also its acoustic features (Fung et al., 2008; Hirschberg and Nakatani, 1996; Nakatani et al., 1995).",
        "The hierarchical layout structure of Power Point slides enhances the understanding by the audience.",
        "In fact, they even provide a kind of extractive summarization that is superior in terms of informativeness than short abstracts.",
        "Unfortunately, presentation slides are not always made available to the audience or for the archive.",
        "In some cases, presentation slides consist of mostly figures and graphs, even videos, but without sufficient text bullet points to summarize the content.",
        "Meanwhile, there are significant amounts of presentation speech online (e.g. political speech, lectures and seminars) that can be rendered more useful if we can summarize them in a format similar to presentation slides.",
        "Following previous research showing that modeling hierarchical structure indeed helps improve summarization performance, we are interested in going a step further in proposing a rhetorical syntax driven summarization model.",
        "Presentation speech is transcribed automatically by an ASR system, then \"parsed\" into a rhetorical syntax tree.",
        "The leaf nodes of the tree, representing actual utterances with rhetorical unit labels, are then \"reordered\" and organized into a target summary.",
        "In this paper, we also propose using a HMSVM (Al-tun et al., 2003) for the parser and the summarizer.",
        "HMSVM has the advantage of considering the interdependence between neighboring sentences.",
        "Reordering rules are automatically learned and candidate sequences generated, before they are scored by the inal summarizer.",
        "This paper is organized as follows: Section 2 describes our motivation, and the rhetorical structure characteristics in lecture speech.",
        "Section 3 details how to parse rhetorical structure of the lecture speech.",
        "Section 4 describes the reordering process.",
        "Section 5 then describes how to produce extractive summaries.",
        "We then describe the corpus, how to create reference summaries, the acoustic/prosodic, linguistic and discourse characteristics of lecture speech, baselines, and our in-house automatic speech recognition system are presented in Section 6.",
        "Section 7 presents the experiment results.",
        "Section 8 describes related work.",
        "We then conclude at the end of this paper."
      ]
    },
    {
      "heading": "2. Rhetorical Syntax Tree of Presentation Speech",
      "text": [
        "Unlike conversational speech, lectures and presentations are planned.",
        "Lecture speakers follow a relatively rigid rhetorical structure at the document level: s/he starts with an overview of the topic to be presented, followed with the actual content with more detailed descriptions, and then concludes at the end.",
        "The speech is given in several coherent \"chunks\" corresponding to the talk outline.",
        "By looking at presentation slides as shown in Figure 1, we can clearly see the chunk boundaries, which always exist at slide transitions, delineate content changes.",
        "Each of the chunks, in turn, contains many coherent text spans, namely the rhetorical units.",
        "Each rhetorical unit contains one or more slides.",
        "We represent the rhetorical structure of presentations by a hierarchical text plan, or a rhetorical tree.",
        "Since lecture speeches are mostly based on presentation slides with main bullet points, the structural format of the presentation slides is a faithful representation of the document-level rhetorical structure of the speech.",
        "At the top level of the tree are the rhetorical chunks, and at the lower level child nodes are rhetorical units.",
        "Each of the chunks contains several rhetorical units, where each unit may contain a number of utterances corresponding to a list of bullet points in the slides.",
        "We propose using the annotation labels commonly shared by most presentation speech for labeling rhetorical chunks as shown in the left column of Table 1.",
        "The chunk label deinitions are derived from the general structure of presentations in the speciic domain of our corpus, namely conference presentations.",
        "Note that whereas we chose to use 7 labels for presentation speech, label definitions are fairly obvious and easy to derive for other genres of speech.",
        "We use a machine-aided manual annotation method to label the training presentation speech data.",
        "Referring to the slides",
        "RUidtl RUmtN!",
        "/N",
        "RUnitl ' MWtN,",
        "RUrdtl ' RUiiitH„",
        "Table 1 : Rhetorical Chunk and Unit Description of each presentation, each sentence in the speech transcription is assigned a label corresponding to one of the 7 chunks deined in Table 1.",
        "First, all bullet point sentences in the slides are assigned a chunk label according to its section.",
        "Referring to our previous work, a Relaxed Dynamic Time Warping program (Zhang et al., 2008) is used to roughly align transcribed sentences to the corresponding slide bullet points.",
        "Chunk labels are also included in this alignment.",
        "Human inspection quickly corrects any alignment mistakes made by the program.",
        "We then label all the sentence of each type \"cj\" rhetorical chunk as \"cj\".",
        "For example, the sentences of a rhetorical chunk which describes \"Methodology: solution/inventive step\" is labeled as \"05\".",
        "Rhetorical units are described in the right column of Table 1.",
        "The rhetorical units, as we explain below, are clustered automatically without explicit labels.",
        "These rhetorical units correspond more or less to the deinitions in Table 1, without human effort.",
        "To obtain the reference rhetorical unit labels of each type of rhetorical chunks in the entire experiment corpus described in Section 6.1, we cluster all utterances that belong to the same chunk in all presentation speech into several rhetorical units by using modiied k-means (MKM) clustering algorithm (Wilpon and Rabiner, 1984; Fung et al., 2003).",
        "MKM starts from the centroid of all utterances in one rhetorical chunk and splits the clusters top down until the sub-clusters stabilize.",
        "Each inal cluster represents one rhetorical unit.",
        "The clustering algorithm is shown as follows.",
        "Given all utterances within the same type of chunk from all presentation speech:",
        "(1) Compute the centroid; (2) Assign sentence feature vectors closest to each centroid to its cluster; (3) Update each centroid feature vector using all sentence feature vectors assigned to each cluster; (4) Iterate step(2) to step (4) until sentence feature vectors stop moving between clusters; (5) Stop if clusters stabilizes, and get final clusters, else goto step (6); (6) Split the cluster with largest intra-cluster distance into two by inding the pair of vectors as new centroids, and repeat steps (2) to step (5).",
        "Using the above algorithm, we ind out the following rhetorical unit clusterings of different kinds of rhetorical chunks on our experiment corpus: (1) two rhetorical units in \"Title\"(ci) chunk: \"ri:ci,i = 1,2\"; (2) three rhetorical units in \"Outline\"^) chunk and \"Conclusion\"^) chunk:",
        "(3) five rhetorical units in \"Motivation\"(c3) chunk, \"Related work\"(c4) chunk, \"Methodology\"^) chunk, and \"Experiment\"^) chunk: \"n:cj, i = 1,2,3,4,5; j = 3,4,5,6\".",
        "These rhetorical unit clusters correspond roughly to the deinitions in the right hand column of Table 1, though no manual labeling is involved."
      ]
    },
    {
      "heading": "3. Parsing Presentation Speech",
      "text": [
        "By using our rhetorical syntax-driven model, the process of parsing presentation speech can be described as follows.",
        "First we extract acous-tic/prosodic features from presentation speech, and linguistic, discourse features from the ASR transcribed text.",
        "These features are described in Section 6.1.",
        "Next we parse the presentation speech into rhetorical units.",
        "Given the ASR transcription of a presentation speech, our task is to parse the transcription sentences into chunks and then into rhetorical units (leaf nodes) that roughly correspond to the rhetorical chunks and units in Table 1 according to their feature vectors.",
        "We consider the parsing process as a multi-class clas-siication problem.",
        "Each rhetorical unit of each rhetorical chunk is represented by one class.",
        "Rhetorical Chunk",
        "Rhetorical Unit",
        "ci (Title)",
        "title, author of the presentation",
        "C2 (Outline)",
        "texture structure;",
        "C3 (Motivation)",
        "aim; problem/phenomenon",
        "C4 (Related work)",
        "rival/contrast; continuation",
        "C5 (Methodology)",
        "solution/inventive step",
        "C6 (Experiment)",
        "corpus description; detailed experimental setup",
        "C7 (Conclusion)",
        "conclusion; future work",
        "Considering that HMSVM (Altun et al., 2003) combines the advantages of maximum margin classifier and kernels with the elegance and efficiency of HMMs, and can effectively handle the dependency between neighboring sentences, we train a twenty-eight-class HMSVM classiier for parsing speech, with one class representing each rhetorical unit.",
        "As an example, the sentences labeled as ^2^5\" belong to the second rhetorical unit of \"Methodology'Xcs) chunk.",
        "We have found that, by looking at our corpus of conference presentations, speakers indeed follow the \"chunk order\" of the slides they use.",
        "We add some constraints existing between \"rm:cj\"and \"rn:cf where \"i ^ j\" according to the \"chunk order\" of the slides.",
        "Function /1 maps each given speech or transcription y to a rhetorical unit label sequence z.",
        "For example we want to learn a discriminant function F\\ : y x Z – > TZ over input/output pairs from which we produce a prediction by maximizing Fi over the output variable for a given input y.",
        "The general form of our hypotheses /1 is:",
        "where w denotes a weighting parameter vector to learn.",
        "Weassume Fi to be linear in some combined feature representation of inputs, described in Section 6.1, and outputs \\I/(y,z).",
        "We then get Fi(y, z;w) = (w, \\&(y, z)).",
        "Moreover, we apply a kernel function K over the joint input/output space such that:",
        "can be written as a sum over the length of the sequence and decomposed as:",
        "where 7 is the rhetorical unit label set.",
        "Z(O) is the length of the observation sequence O in our case.",
        "is composed by mapping functions that depend only on labels at position i and i + 1, O as well as i (Markov property).",
        "We then rewrite F\\ using w = (w^^^tT<E7 as Equation 4.",
        "In decoding process, using this decomposition (Altun et al., 2003) we can define as the maximal score for all labels with label t> at position i.",
        "Using dynamic programming we compute maxug-y V(l(O),v).",
        "The optimal label sequence is recovered by backtracking."
      ]
    },
    {
      "heading": "4. Reordering Rhetorical Unit Sequence",
      "text": [
        "We found that, by looking at our corpus of conference presentations described in Section 6.1, presentation speakers do not always follow the bullet point order within a chunk.",
        "When demonstrating a current slide they may be already introducing the next slide.",
        "About 11% of the rhetorical units in the transcriptions are out of order vis-avis the corresponding bullet points in the presentation slide.",
        "As an integral part of our rhetorical syntax-driven summarization model, the rhetorical unit sequence and consequently the sentence sequence are reordered within a chunk.",
        "The extraction of reordering rules is based on the alignment between source rhetorical unit sequence in the speech transcription and target rhetorical unit sequence in the Power Point slide sentences.",
        "Each sentence is represented by its rhetorical unit label.",
        "For example, from the training set and development set in one of our held-out experiment settings described in Section 7, we extracted the following reordering rules: (1)(r3,ri) – > (ri,ra);",
        "V:i lLi l'i T:î 1\\_Original KU Sequence",
        "(2)(r3,r2) -» (r2,r3);(3)(r4,r2) ->• (r2,r4); (4)(r4,r3) -> (r3,r4);(5)(r5,r3) ->• (r3,r5); (6)(r5,r4) (r4,r5).",
        "In each reordering rule, the left item represents rhetorical unit sequence of the transcription sentences while the right item represents rhetorical unit sequence of bullet points of the corresponding Power Point slides.",
        "From these reordering rules, we can see that the speakers in our corpus talk about content described by future bullet points (i.e in the subsequent rhetorical units), but never seem to repeat content from bullet points in the previous unit(s).",
        "Given a sentence sequence and its corresponding rhetorical unit sequence within each chunk, from left to right, with a shifting window of two, we search for the matching reordering rule and adjust the order of the sentences one matched rule at a time, yielding a set of at most 2L sentence sequence candidates for each chunk where L equals to the length of the sentence.",
        "From our data, we found that there are at most 2 matched rules per sentence sequence.",
        "So including the original sequence, at most 4 candidate sequences are generated for each chunk.",
        "A reordering example is shown in Figure 2.",
        "We apply the reordering rules on a sentence sequence \"(01,02,05,04,05)\" and the corresponding unit sequence \"(r3, r\\,r±, r3, rs)\".",
        "Four candidate reordered sentence sequence are produced.",
        "Without any reordering, we get \"Candidate Sentence Sequence (1)\".",
        "Using reordering Rule 1, we get \"Candidate Sentence Sequence (2)\"."
      ]
    },
    {
      "heading": "5. Rhetorical Syntax-driven Summarization",
      "text": [
        "Following sentence reordering, the extractive summarizer selects salient sentences from each chunk using a binary-class classiier.",
        "The classi-ier is run over all candidate sequences from Section 4 and the system selects the best sequence and its summary sentences according to the output probability of the classiier.",
        "The best sequence {oi...Oj...Ofc} satisfies",
        "argmax lg P(oj E summary sentence set\\cj) i=l",
        "where Cj represents the rhetorical chunk Cj which has several candidate sequences, including the sequence {o\\...Oi...Ok}.",
        "P(oi E summary sentence set\\cj) is output probability of that the sentence o\\ in the rhetorical chunk Cj is summary sentence.",
        "Again, an HMSVM classiier is used at this stage.",
        "The sentence feature vector o now has its rhetorical unit label as an additional feature, to yield a new sentence feature vector o.",
        "For the sentence vector sequence z of each chunk, we label it by using the optimal function Fi{Z,V).",
        "The training stage is similar to that of training the HMSVM parser.",
        "The difference is that the HMSVMs for summarization are binary classi-iers, while the HMSVM parser is a multi-class classiier."
      ]
    },
    {
      "heading": "6. Experimental Setup 6.1 Corpus",
      "text": [
        "We use a lecture speech corpus containing wave iles of 71 presentations recorded from different mandarin speakers at two technical conferences, together with well-formatted Power Point slides, manual transcriptions, and their associated audio data.",
        "Each presentation lasts about 15 minutes on average.",
        "The 71 presentations are split into 391 chunks, and each sentence is assigned a rhetorical chunk label, using the machine-aided human labeling method as described in Section 2.",
        "Each chunk has on average 4.3 rhetorical units.",
        "The reference rhetorical unit labels are created by using unsupervised MKM algorithm described in Section 2.",
        "Since the labeling process also yields an alignment path between transcription sentences and Power Point slide bullet points, we extract those sentences that have the highest alignment scores with the bullet points to form reference summary sentences, then corrected by ive human subjects according to the rhetorical chunk descriptions in the right column of Table 1.",
        "We use the kappa coefficient (Krippendorff, 1980) for measuring stability of each annotator and reproducibility between each pair of annotators.",
        "The average kappa coefficient is higher than 0.85.",
        "i'h",
        "1'",
        "f",
        "Reordered RÜ Sequence by Rule 1",
        "ii",
        "i>",
        "I'd",
        "f",
        "Reordered RU Sequence by Rule 4",
        "ri",
        "]■",
        "l'a",
        "Ii",
        "r",
        "Reordered KU Sequence hy Hulo 1 and 4",
        "Ol",
        "Oî",
        "o3",
        "o4",
        "05",
        "Original Sentence Sequence",
        "\":",
        "o5",
        "Oi",
        "\"",
        "Candidate Sentence Sequence (J)",
        "Ol",
        "Os",
        "1)",
        "Candidate Sentence Seuuence (2)",
        "Ol",
        "02",
        "o.",
        "o3",
        "<\">",
        "Candidate Sentence Sequence (3)",
        "Oi",
        "03",
        "1)",
        "Candidate Sentence Sequence <A)",
        "We use the discourse feature PossionNoun proposed in our previous work (Zhang et al., 2008) which is based on the following assumptions: irst, if a sentence contains new noun words, it probably contains new information.",
        "The noun word's Poisson score varies according to its position.",
        "We use Poisson distribution to approximate the variation.",
        "Second, if a noun word occurs frequently, it is likely to be more important than other noun words, and the sentence with these high frequency noun words should be included in a summary.",
        "We also use the following acoustic and linguistic features for representing sentences.",
        "The acoustic features are: duration of the sentence, average syllable duration of the sentence, F0 and Energy min/max/mean/slope/range value of the sentence.",
        "The linguistic features are: sentence word count, TF/IDF of each word in the sentence, and the word identity in each sentence.",
        "We use three alternate summarization models for comparison.",
        "One is a binary classifier without any rhetorical information, one class for summary sentence and the other for non-summary sentence.",
        "The second is RSHMM (Fung et al., 2008).",
        "The third is our rhetorical syntax tree without reordering.",
        "The two above are built by using acoustic, linguistic, and discourse features for representing the sentences.",
        "We apply rhetorical unit label as an additional feature for building our proposed mod-els(with/without reordering).",
        "Table 2: Summarization average performance of 6-fold cross validation experiment in ROUGE-L F-measure (F-measure)_",
        "(1) Binary: binary classifier as baseline (2) RSHMM: Rhetorical State HMM proposed in our previous work (Fung et al., 2008) (3) without reordering: rhetorical syntax tree based summarizer without reordering (4) Syntax tree with reordering: rhetorical syntax tree based summarizer with reordering",
        "We apply our rhetorical syntax-driven summarization model for ASR transcriptions.",
        "The database for building our in-house ASR system contains 29 hours of audio data from the technical conferences in our corpus.",
        "We choose approximately 21 hours of speech as the training data.",
        "The test data comprises of 12 presentations with approximately 3 hours of audio data.",
        "Our decoding system runs in multiple passes.",
        "Automatic segmentation is irst performed on the lecture speech audio data.",
        "This is followed by bigram decoding with the gender independent (GI) acoustic model (AM) and lattice generation.",
        "Trigram and four-gram branches are created for AM adaptation through lattice expansion and rescoring.",
        "Re-decoding with both adapted AM and adapted language model (LM) is performed to produce 1-best results.",
        "System combination via recognizer output voting error reduction scheme (ROVER) (Fiscus, 1997) is employed by using character based alignment from the trigram and four-gram branch outputs.",
        "The i-nal system obtains a recognition performance of 79.2% character accuracy."
      ]
    },
    {
      "heading": "7. Experimental Results",
      "text": [
        "For evaluating different summarization systems, we perform 6-fold cross validation experiments, and two held-out experiments.",
        "There are many kinds of metrics for evaluating speech summarization performance (Zhu and Penn, 2005; Penn",
        "Without",
        "Syntax",
        "Bianry",
        "RSHMM",
        "reordering",
        "tree with",
        "reordering",
        ".53(.52)",
        ".63(.56)",
        ".69(.61)",
        ".72(.65)",
        "(1) Binary: binary classifier as baseline (2) RSHMM: Rhetorical State HMM proposed in our previous work (Fung et al., 2008) (3) without reordering: rhetorical syntax tree based summarizer without reordering (4) Syntax tree with reordering: rhetorical syntax tree based summarizer with reordering",
        "(A) on manual transcription measure (Lin, 2004) and F-measure (Van Rijsber-gen, 1979) as evaluation metrics in our experiments.",
        "11 documents from the 71 presentations are excluded as our development set.",
        "In the 6-fold cross validation experiments, we divide the remaining 60 presentations into six subsets of equal size.",
        "For each experiment, we use ive subsets to train all models and the remaining subset for testing.",
        "The average performance of these 6-fold cross validation experiments is shown in Table 2.",
        "Among these 60 presentations, 50 are randomly selected as training data for the two held-out experiments, while the remaining ten are used as test data.",
        "Table 3-(A) shows the result of the one held-out experiment on manual transcriptions of test data.",
        "Table 3-(B) shows the other held-out experiment on ASR transcriptions of test data.",
        "From these results, we can see that the proposed rhetorical syntax-driven summarizer with reordering outperforms all other methods.",
        "Table 2 shows that our rhetorical syntax-driven model gives a 35.8% relative improvement over a binary summarizer with no rhetorical information, a 14.3% improvement over RSHMM (Fung et al., 2008), and a 4.3% improvement over our proposed model with no reordering.",
        "These indings suggest that our rhetorical syntax-driven summarization model built by using binary HMSVM classiier apply the sequence information of the sentences within the same rhetorical chunk for improving summarization performance because of the Markov property of HMSVM.",
        "In the above experiments, we all use the rhetorical unit labels produced by our rhetorical structure parser for improving summarization performance.",
        "The average accuracy of the rhetorical structure parser is about 83.2%.",
        "When we use the reference rhetorical unit labels on our cross-validation experiments, the average summarization performance is 0.75 of ROUGE-L F-measure, a 4.2% improvement over that using the rhetorical unit labels produced by the rhetorical structure parser.",
        "Although the overall performance on ASR transcriptions is worse than that of manual transcriptions, the performance is also satisfying.",
        "Furthermore, we also ind that using only acoustic features our model obtains satisfying result, 0.65 of ROUGE-L F-measure, on 6-fold cross validation experiment."
      ]
    },
    {
      "heading": "8. Related Work",
      "text": [
        "(Furui et al., 2008) has shown that feature-based extractive summarization is an approach that is ef-icient and more effective than MMR-based approach for lecture speech summarization.",
        "(Marcu, 1997) described the irst experiment that shows the concepts of rhetorical analysis and nuclearity can be used effectively for text summarization.",
        "(Fung et al., 2003) presented a stochastic HMM framework with modiied K-means and segmental K-means algorithms for extractive text summarization.",
        "(Fung and Ngai, 2006) further presented a stochastic Hidden Markov Story Model for multilingual and multi-document summarization and proposed that monolingual documents recounting the same story (i.e., in the same topic) share a unique story flow (one story, one flow), andsucha flowcan be modeledby HMMs.",
        "(Barzilay and Lee, 2004) presented an unsupervised method for the induction of content models, which capture constraints on topic selection and organization for texts in a particular domain (Branavan et al., 2007) proposed a structured discriminative model for table-of-contents generation on written text that accounts for a wide range of phrase-based and collocation features.",
        "(Eisenstein and Barzilay, 2008) describes a novel Bayesian approach to unsupervised lexical cohesion driven topic segmentation.",
        "Binary",
        "RSHMM",
        "Without reordering",
        "Syntax tree with reordering",
        ".50(.48)",
        ".61(.52)",
        ".67(.59)",
        ".69(.61)",
        "(B) on ASR transcription",
        "Binary",
        "RSHMM",
        "Without reordering",
        "Syntax tree with reordering",
        ".46(.43)",
        ".59(.50)",
        ".66(.57)",
        ".68(.61)",
        "Many researchers have suggested that rhetorical information also exists in spoken documents and eficient modeling of this information is helpful to the summarization task.",
        "(Tatar et al., 2008) and (AKITA and Kawahara, 2007) used the Hearst method (Hearst, 1997) to segment documents and detect topics for text summarization and topic adaptation of speech recognition systems for long speech archives respectively.",
        "(Hi-rohata et al., 2005) consider that humans tend to summarize presentations by extracting important sentences from introduction and conclusion sections, and further propose a summarization method based on this structural characteristic.",
        "They estimated the introduction and conclusion section boundaries based on the Hearst method (Hearst, 1997), using sentence cohesiveness which is measured by a cosine value between content word-frequency vectors, before performing summarization.",
        "Teufel and Moens (Teufel and Moens, 2002) also proposed \"rhetorical status\" as summary unit, but without hierarchical structure or reordering.",
        "Furthermore, many linguists believe that speech acoustics contribute to rhetorical and discourse structure.",
        "(Nakatani et al., 1995) provide empirical evidence that discourses can be segmented reliably, and that acoustic characteristics are used by speakers to convey linguistic structure at the discourse level in the English domain.",
        "There is a large amount of previous work seeking to demonstrate that acoustic prosodic proile of speech closely models its discourse or rhetorical structure (Halliday, 1967; Ladd, 1996; Hirschberg and Nakatani, 1996).",
        "Our work described in this paper is closely related to (Marcu, 1997; Teufel and Moens, 2002) in that we propose using rhetorical units for summarization.",
        "Our method differs from (Teufel and Moens, 2002) in that we assume the relevance or saliency and function of certain text pieces can be determined by analyzing the full hierarchical structure of the text.",
        "Instead of annotating the training data with rhetorical labels manually, we propose using Power Point slides as references.",
        "(He et al., 2000; He et al., 1999) also investigate the correlation between Power Point slides and extractive summaries.",
        "Our learning method is based on classiiers, while (Marcu, 1997) uses rule-based method for parsing rhetorical structure.",
        "We train our rhetorical parser by using those reference rhetorical units of the transcriptions created by aligning Power Point slides.",
        "We propose a syntax-driven parsing model with reordering for summarization, and we propose a different clas-siier, HMSVM, for handling the dependency between neighboring sentences within each chunk, when we accomplish our summarization task."
      ]
    },
    {
      "heading": "9. Conclusion and Discussion",
      "text": [
        "In this paper, we have shown a rhetorical syntax-driven summarization method for presentation speech.",
        "In view of the fact that rhetorical structure in speech is inherently hierarchical, our method chunks, parses, and reorders the presentation utterance sentences before selecting some of them as summary sentences.",
        "We have proposed to use HMSVM classi-iers for the parser and the summarizer, taking into account the dependency between neighboring chunks, rhetorical units and sentences with Markov property.",
        "Our rhetorical syntax-driven summarizer with reordering outperforms a binary summarizer without rhetorical information with 35.8% relative improvement and outperforms RSHMM (Fung et al., 2008) with 14.3% relative improvement.",
        "It gives a 4.3% relative improvement over the same model without reordering.",
        "For future work, we are interested in investigating how to apply our rhetorical syntax-driven method to other genres of speech, such as meetings and parliamentary speech."
      ]
    },
    {
      "heading": "10. ACKNOWLEDGEMENT",
      "text": [
        "This work is partially supported by ITS/189/09 of the Hong Kong Innovation and Technology Fund(ITF).",
        "The authors would like to thank Chan Ho Yin for his valuable input to this work."
      ]
    }
  ]
}
