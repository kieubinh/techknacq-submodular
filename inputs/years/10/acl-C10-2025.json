{
  "info": {
    "authors": [
      "Lei Cui",
      "Dongdong Zhang",
      "Mu Li",
      "Ming Zhou",
      "Tiejun Zhao"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2025",
    "title": "Hybrid Decoding: Decoding with Partial Hypotheses Combination over Multiple SMT Systems",
    "url": "https://aclweb.org/anthology/C10-2025",
    "year": 2010
  },
  "references": [
    "acl-C08-1005",
    "acl-D07-1056",
    "acl-E06-1005",
    "acl-P06-1066",
    "acl-P07-1089",
    "acl-P09-1065",
    "acl-W04-3250"
  ],
  "sections": [
    {
      "text": [
        "Hybrid Decoding: Decoding with Partial Hypotheses Combination over",
        "Multiple SMT Systems*",
        "Lei Cuit, Dongdong Zhang*, Mu Li*, Ming Zhou*, and Tiejun Zhao*",
        "* School of Computer Science and Technology Harbin Institute of Technology",
        "In this paper, we present hybrid decoding – a novel statistical machine translation (SMT) decoding paradigm using multiple SMT systems.",
        "In our work, in addition to component SMT systems, system combination method is also employed in generating partial translation hypotheses throughout the decoding process, in which smaller hypotheses generated by each component decoder and hypotheses combination are used in the following decoding steps to generate larger hypotheses.",
        "Experimental results on NIST evaluation data sets for Chinese-to-English machine translation (MT) task show that our method can not only achieve significant improvements over individual decoders, but also bring substantial gains compared with a state-of-the-art word-level system combination method."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In recent years, system combination for SMT has been known to be quite effective with translation consensus information built from multiple SMT systems.",
        "The combination approaches can be classified into two types.",
        "One is the combination with each system's outputs, which can be seen as full hypotheses combination.",
        "The other is the partial hypotheses (PHS) combination during the decoding phase.",
        "A lot of impressive work has been done to improve the performance of the SMT systems by uti-",
        "This work has been done while the first author was visiting Microsoft Research Asia.",
        "lizing consensus statistics which come from single system or multiple systems.",
        "For example, Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding over n-best list finds a translation that has lowest expected loss with all the other hypotheses, and it shows that improvement over the Maximum a Posteriori (MAP) decoding.",
        "Several word-based methods (Rosti et al., 2007a; Sim et al., 2007) have also been proposed.",
        "Usually, these methods take n-best list from different SMT systems as inputs, and construct a confusion network for second-pass decoding.",
        "There are also a lot of research work to advance the confusion network construction by finding better alignment between the skeleton and the other hypotheses (He et al., 2008; Ayan et al., 2008).",
        "Typically, all the approaches above only use full hypotheses but have no access to the PHS information.",
        "Moreover, some dedicated efforts have been tried by manipulating PHS between multiple MT systems.",
        "Collaborative decoding (co-decoding) (Li et al., 2009) leverages translation consensus by exchanging partial translation results and re-ranking both full and partial hypotheses explored in decoding.",
        "However, no new PHS are generated compared to the individual decoding but only the ranking is affected.",
        "Liu et al.",
        "(2009) proposes joint decoding, a method that integrates multiple translation models in one decoder.",
        "Although joint decoding is able to generate new translations compared to single decoder, it has to use the PHS existed in one of its component decoder at each step.",
        "Different from their work, we propose a new perspective which leverages outputs from local word-level combination.",
        "This will potentially bring much benefit of performance since wordlevel combination can produce more promising PHS.",
        "The word-level system combination method is employed to generate partial translation hypotheses in our hybrid decoding framework.",
        "In this sense, full hypotheses word-level combination (FH-Comb) method(Rosti et al., 2007a; Sim et al., 2007; He et al., 2008; Ayan et al., 2008) can be considered as a special case of hybrid decoding, where their combinations are only performed on the largest hypotheses.",
        "Similar with FH-Comb, hybrid decoding also uses word alignment information.",
        "However, challenge exists in hybrid decoding as word alignment needs to be carefully conducted through the decoding process.",
        "Obviously, document-level word alignment methods such as GIZA++(Och and Ney, 2000) are quite time consuming and unpractical to be embedded into hybrid decoding.",
        "We propose a heuristic method that can conduct word alignment of partial hypotheses based on word alignment information of phrase pairs learnt automatically from the model training process.",
        "In this way, more PHS are generated and the search space is enlarged substantially, which brings better translation results.",
        "The rest of the paper is organized as follows: Section 2 gives a formal description of hybrid decoding, including framework overview, wordlevel PHS combination and parameter estimation.",
        "We conduct experiments with different settings and make comparison between our method and baseline, as well as a state-of-the-art word-level system combination method in Section 3.",
        "Experimental results discussion is presented in Section 4.",
        "Section 5 concludes the paper.",
        "integrating different decoding algorithms.",
        "Without loss of generality, we assume that bottom-up CKY-based decoding is adopted in each individual decoder, which is the same as co-decoding (Li et al., 2009) and joint decoding (Liu et al., 2009).",
        "Hybrid decoding collects n-best PHS of a source span from multiple decoders, then results from word-level PHS combination of that span are given back to each decoder, mixed with the original PHS.",
        "After that, we re-rank the hybrid list and continue the decoding.",
        "In an example with two decoders, parts of the whole decoding process are illustrated in Figure 1 and can be summarized as follows:",
        "Local decoding layer",
        "Local combination layer"
      ]
    },
    {
      "heading": "2. Hybrid Decoding 2.1 Overview",
      "text": [
        "Different system combination methods (Li et al., 2009; Liu et al., 2009) offer different frameworks to coordinate multiple SMT decoders.",
        "Hybrid decoding provides a new scheme to organize multiple decoders to work synchronously.",
        "As the decoding algorithms may differ in multiple de-coders, hybrid decoding has some difficulty in",
        "!In the SMT area, some decoders use left-right decoding to generate the hypothesis and \"Pharaoh\" (Koehnetal.,",
        "Figure 1: Hybrid decoding with two decoders, where the string \"s-e\" means the source span starts from position s and ends at position e. The blank rectangles represent the n-best partial translations of each decoder, and the shaded rectangles illustrate the n-best local combination outputs.",
        "The ovals denote bottom-up CKY-based decoding results.",
        "2003) is one of them, while others adopt bottom-up decoding which is represented by \"Hiero\" (Chiang, 2007).",
        "Ç Decoderl ^)",
        "Ç Decoder2 ^)",
        "_________Î_______",
        "________(___________",
        "X",
        "1.",
        "Individual decoding.",
        "Each individual decoder should maintain the n-best PHS of each span from the bottom.",
        "After all the individual decoders finish translating the same span, they feed their own partial translations into a public container which can be used for word-level PHS combination, then get back the partial combination outputs for step 3.",
        "2.",
        "Local word-level combination.",
        "After fed with PHS from multiple decoders, a confusion network is built and word-level combination for PHS is conducted.",
        "The obtained new partial translations are given back to each individual decoder to continue the decoding.",
        "3.",
        "Mix new PHS with the original ones.",
        "The span in each individual decoder will receive the corresponding new PHS from the local combination outputs.",
        "The feature space of the new PHS is not exactly the same with that of the original ones.",
        "It has to be mapped in some way then the mixed hypotheses are re-ranked.",
        "In the following sub-sections, we first present the background of word-level combination for PHS, then introduce hybrid decoding algorithm in detail, as well as the feature definition and parameter estimation.",
        "Most word-level system combination methods are based on confusion network decoding.",
        "In confusion network construction, one hypothesis has to be selected as the skeleton which determines the word order of the combination results.",
        "Other hypotheses are aligned against the skeleton.",
        "Either votes or some word confidence scores are assigned to each word in the network.",
        "Most of the research on confusion network construction focuses on seeking better word alignment between the skeleton and the other hypotheses.",
        "So far, several word alignment procedures are used for SMT system combination, which mainly are GIZA++ alignments (Matusov et al., 2006), alignments (He et al., 2008).",
        "Similar with general word-level system combination method, wordlevel PHS combination also uses word alignment information.",
        "However, in hybrid decoding, it is quite time-consuming and impractical to conduct word alignment like GIZA++ for each span.",
        "Fortunately, unit hypotheses word alignment can be obtained from the model training process, which is shown in Figure 2.",
        "We devise a heuristic approach for PHS alignment that leverages the translation derivations from the sub-phrases.",
        "The derivation information ultimately comes from the phrase table in phrase-based systems (Koehn et syntactic-based systems (Chiang, 2007; Liu et al., 2007; Galley et al., 2006).",
        "The derivation is built in a phrase-based system as follows.",
        "For example, we have two phrase translations \"ÄCl W ||| our ||| 0-0 1-0\" and \"Iri^F fljjä ||| economic interests ||| 0-0 1-1\" , where string \"m-n\" means the mth word in the source phrase is aligned to the nth word in the target phrase.",
        "When combining the two phrases for generating \" ïfe if] W l& ^ M Ät \" , we obtain the translation hypothesis as \"our economic interests\" and also integrate the alignment fragment to get \"0-0 1-0 2-1 3-2\" .",
        "The case is similar in syntactic-based system for non-terminal substitution, which we will not discuss further here.",
        "Next, we introduce the skeleton-to-hypothesis word alignment algorithm in detail.",
        "With the translation derivations, the skeleton-to-hypothesis (sk2hy) word alignment can be performed based on the source-to-skeleton (so2sk) and source-to-hypothesis (so2hy) word alignment as they share the same source sentence.",
        "The basic idea is to construct the sk2hy word alignment with the minimum correspondence subsets (MCS).",
        "A MCS is defined as a triple < SK, HY, SO > where the SK is the subset of skeleton words, HY is the subset of the hypothesis words, and SO is the minimum source word set that all target words in both SK and HY are aligned to.",
        "Figure 3 shows the algorithm for skeleton-to-hypothesis alignment.",
        "Most of the pseudo-code is self-explained except for some subroutines, which are listed in",
        "Table 1.",
        "1: procedure SkeHypAlign(so2s/c, so2hy) 2: repeat 3: Fetch out a source word to SO 5: repeat 7: SK=GetAlign(SO, so2sk) 9: SOi=GETALIGN(SK, so2sk)",
        "12: simmax = – infinity",
        "13: for all sk G SK do 14: for all hy G H Y do 16: if sim > simmax then 23: ADDALIGN(skmax, hymax) 24: until all the source words are fetched out",
        "25: end procedure",
        "Due to the variety of the word order in n-best outputs, skeleton selection becomes essential in confusion network construction.",
        "The simplest way is to use the top-1 PHS from any individual decoder with the best performance under some criteria.",
        "However, this cannot always lead to better performance on some evaluation metrics (Rosti et al., 2007a).",
        "An alternative would be MBR method with some loss function such as TER (Snover et al., 2006) or BLEU (Papineni et al., 2002).",
        "We show the experimental results of two skeleton selection methods for PHS combination in Section 3.",
        "For a given source sentence f, any individual decoder in hybrid decoding finds the best translation e* among the possible translation hypotheses $(f ) in terms of a ranking function F :",
        "Suppose we have n individual decoders.",
        "The ranking function Fn of the nth decoder can be written as:",
        "where each hn>i(f, e) is a feature function of the nth decoder, and An>i is the corresponding feature weight.",
        "m is the number of features in each decoder.",
        "The final result of hybrid decoder is the top-1 translation from the confusion network, which is constructed on multiple decoders with the last layer's output of CKY-based decoding.",
        "The hybrid decoder acts as a control unit which controls the synchronization of multiple individual decoders.",
        "The algorithm is fully demonstrated in Figure 4.",
        "The hybrid decoder pushes the same span fj to different decoders and gets back the n-best PHS (lines 2-6).",
        "When the span's length is too small, both word alignment and partial combination results are not accurate.",
        "We predefine a fixed threshold ö which is used for determining the start-up of combination (line 7).",
        "When the length condition holds, the n-best PHS of each individual decoder are stored in container G (lines 8).",
        "Confusion network is constructed and new PHS can be extracted from it and are further mixed and sorted with the original ones (lines 11-15).",
        "Subroutines",
        "Description",
        "UNION(A,B)",
        "the union of set A and set B",
        "GETALIGN(S,align)",
        "get the words aligned to S based on align",
        "SIM(wi,W2)",
        "similarity between w1 and w2, we use edit distance here",
        "ADDALIGN(wi,W2)",
        "align w1 with w2",
        "1: procedure HYBRlDDECODlNGfi, D) 2: for l < – 1...n do 5: for all d G D do 7: if j – i > ö then 12: nbest =GETPARHYP(cn) 13: for all d G D do 14: MlxSORT(nbestd, nbest') 17: end for 18: end procedure",
        "Next we present the PHS word-level combination feature functions for hybrid decoding.",
        "Following (Rosti et al., 2007b), four features are utilized to model the PHS as:",
        "Word Confidence Feature hwc(e)",
        "The word confidence feature is computed as hwc(e) = Y^n=1 ^iciw, where n is the number of the systems, ßi is the system confidence of system i, and ciw is the word confidence of word w in system i.",
        "Word Penalty Feature hwp(e)",
        "Word penalty feature is the number of words in the partial hypothesis (PH).",
        "Null Penalty Feature hnp(e)",
        "For null penalty feature, we mean the number of NULL links along the PH when extracted from the confusion network.",
        "Language Model Feature hlm(e)",
        "Different from the above three combination features, which can be obtained during the confusion network construction or hypotheses extraction, the language model feature cannot be summed up on the fly.",
        "Instead, it must be recomputed when building each new PH.",
        "The features used in hybrid decoding can be classified into two categories: features for individual decoders (FID) and features for PHS word-level combination (FComb), and they are independent.",
        "When mixing the new PHS with the original ones of individual decoders, FComb space has to be mapped to a FID space.",
        "However, several features in FID are not defined in FComb, such as source to target (S2T) phrase probability, target to source (T2S) phrase probability, S2T lexical probability, T2S lexical probability and other model specific features.",
        "A mapping function H needs to be defined as follows:",
        "where F/comb denotes the feature vector from FComb space, while F/id is the feature vector from FID space.",
        "An easy mapping function is implemented with an intuitive motivation: PHS combination results are better than the ones in individual decoder and we prefer not to disorder the original search space.",
        "Thus, the undefined feature values of PHS from FComb space are assigned by corresponding feature values of the top-1 PH in original decoder.",
        "Experiments show that our method is not only practical but also quite effective.",
        "Minimum Error Rate Training (MERT) (Och, 2003) algorithm is adopted to estimate feature weights for hybrid decoding.",
        "As hybrid decoder makes use of PHS from both individual decoders and combination results as a whole, we devise a new feature vector representation.",
        "The feature vectors from FID space and FComb space are simply concatenated to form a longer vector without overlapping.",
        "The weights are tuned simultaneously in order to reach a relatively global optima."
      ]
    },
    {
      "heading": "3. Experiment 3.1 Data and Metric",
      "text": [
        "We conducted our experiments on the test data of NIST 2005 and NIST 2006 Chinese-to-English machine translation tasks.",
        "The NIST 2003 test data is used as the development data to tune the parameters.",
        "Statistics of the data sets are shown in Table 2.",
        "Translation performances are measured with case-insensitive BLEU4 score (Papineni et al., 2002).",
        "Statistical significance test is performed using the bootstrap re-sampling method proposed by Koehn (2004).",
        "The bilingual training corpora we used are listed in Table 3, which contains 498K sentence pairs, 12.1M Chinese words and 13.8M English words after pre-processing.",
        "Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default setting, and the intersect-diag-grow method is used to refine the symmetric word alignment.",
        "The language model used for hybrid decoding and all the baseline systems is a 5-gram model trained with the xinhua portion of LDC English Gigaword Version 3.0 plus the English part of bilingual training data.",
        "We use two baseline systems.",
        "The first one (SYS1) is reimplementation of Hiero, a hierarchical phrase-based system (Chiang, 2007) based on Synchronous Context Free Grammar (SCFG).",
        "Phrasal translation rules and hierarchical translation rules with nonterminals are extracted from all the bilingual sentence pairs.",
        "The second one (SYS2) is a phrase-based system (xiong et al., 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex-icalized reordering model (Zhang et al., 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1.",
        "The lexicalized reordering model is trained using the MaxEnt toolkit (Zhang, 2006) where the training instances are extracted from subset of the training corpora,",
        "LDC2005T06, LDC2005T10.",
        "Both systems use the bottom-up CKY-based decoding with cube-pruning (Chiang, 2007) and the beam size is set to 10 for decoding efficiency.",
        "For hybrid decoder, we set ö to be sentence.length – 3, meaning that the PHS of individual decoders only perform local combination in the last three layers.",
        "The reason why we adopt this setting is because we find that starting local combination on short spans hurts the performance badly on test data.",
        "Experimental results are shown in the next section.",
        "We present the overall results of hybrid decoding over two baseline systems on both test sets.",
        "We also implement an IHMM-based word-level system combination method (He et al., 2008) to make comparison with hybrid decoding system, and the n-best candidates used for IHMM-based word-level system combination is set to 10.",
        "Parameters for all the systems are tuned on NIST 2003 test set.",
        "The results are shown in Table 4.",
        "In Table 4, we find that the hybrid decoding performs significantly better than SYS1 and SY2 on both test sets.",
        "Besides, compared to IHMM wordlevel system combination method, hybrid decoding also brings substantial gains with 0.63% and 0.92% points respectively.",
        "Data Set",
        "# Sentences",
        "NIST 2003(dev)",
        "919",
        "NIST 2005(test)",
        "1,082",
        "NIST 2006(test)",
        "1,664",
        "LDC ID",
        "Description",
        "LDC2003E07 LDC2003E14 LDC2005T06 LDC2005T10 LDC2005E83 LDC2006E26 LDC2006E34",
        "LDC2006E85 LDC2006E92",
        "Ch/En Treebank Par Corpus FBIS Multilanguage Texts Ch News Translation Text Part 1 Ch/En News Magazine Par Text GALE Y1 Q1 Release - Translations GALE Y1 - En/Ch Par Financial News GALE Y1 Q2 Release - Translations V2.0",
        "GALE Y1 Q3 Release - Translations GALE Y1 Q4 Release - Translations",
        "Table 4: Hybrid decoding results on test sets, *:significantly better than SYS1 and SYS2 with /.",
        "<0.01, +:significantly better than IHMM Word-Comb with/?<0.01",
        "We also try different layers for determining the start-up of local word-level PHS combination.",
        "Figure 5 gives the intuitive BLEU results.",
        "Figure 5: Performance of hybrid decoding with different start-up settings on NIST 2005 test set, where the \"lastM\" means to conduct local word-level PHS combination in the last M layers from the perspective of CKY-based decoding.",
        "As shown in Figure 5, the performance drops drastically if we start to conduct word-level PHS combination too early.",
        "After considering about efficiency and performance, we determine to do that in the last three layers.",
        "We then investigate the effects on hybrid decoding with different beam sizes, and compare the trend with two baseline systems and IHMM-based word-level system combination method as well.",
        "The results are illustrated in Figure 6.",
        "From what we see in Figure 6, the performance of each system is monotonically increasing as the beam size becomes larger.",
        "Hybrid decoding performs consistently better than IHMM Word-Comb when the beam size is small, and the largest improvement (+0.63% points) is obtained when the beam size is set to 10.",
        "However, as the beam size increases, the performance gap is getting narrow.",
        "One intuitive observation is that hybrid decoding performs slightly worse than IHMM Word-Comb when the beam size is set to 100.",
        "One possible reason for this phenomenon is that, the alignment noise may be introduced to hybrid decoding since we have to generate monolingual alignments with many poor translation derivations.",
        "The confusion network for PHS of each system can be built independently.",
        "We would like to evaluate the performance of single system hybrid decoding.",
        "Table 5 gives the results on both Hiero and BTG decoders.",
        "Table 5 shows that BTG decoder (SYS2) has more potential for so-called \"self-boosting\" .",
        "The self-combination of BTG decoder improves the performance substantially over the baseline.",
        "However, we did not observe any significant improvement for Hiero decoder (SYS1).",
        "Finally, we examine the impacts of skeleton selection for PHS in hybrid decoding.",
        "The results in Table 6 demonstrate that, compared to the top-1 selection method, translation performance can be improved significantly with MBR-based skeleton selection method.",
        "It strongly suggests that choosing the skeleton with more consistent word order will lead to better translation results.",
        "NIST 2005",
        "NIST 2006",
        "SYS1",
        "0.3745",
        "0.3346",
        "SYS2",
        "0.3699",
        "0.3296",
        "IHMM Word-Comb",
        "0.3821*",
        "0.3421*",
        "Hybrid",
        "0.3884*+",
        "0.3513*+",
        "NIST 2005",
        "NIST 2006",
        "SYS1",
        "SYS2",
        "SYS1",
        "SYS2",
        "baseline",
        "0.3745",
        "0.3699",
        "0.3346",
        "0.3296",
        "self-comb",
        "0.3770",
        "0.3758*",
        "0.3358",
        "0.3355*",
        "Table 6: Skeleton selection in hybrid decoding, *:significantly better than top-1 skeleton selection method with .",
        "<0.01"
      ]
    },
    {
      "heading": "4. Discussion",
      "text": [
        "System combination methods have been widely used in SMT to improve the performance.",
        "For example, in (Rosti et al., 2007a), several combination methods have been proposed to make use of different kinds of consensus information.",
        "In (He et al., 2008), better word alignment method is adopted to advance the word-level system combination.",
        "Our method is different from these methods in the sense that we do not exclusively rely on the n-best full hypotheses from each individual decoder, but emphasize the importance of wordlevel combination for PHS.",
        "Thus, it enlarges the search space and is more prone to find better translations.",
        "Experimental results have shown the effectiveness of our method.",
        "The idea of multiple systems collaborative decoding (Li et al., 2009) works well on re-ranking the outputs of each system using n-gram agreement statistics.",
        "However, no new translation results are generated compared to individual decoding.",
        "Our method takes advantage of confusion network to give PHS which cannot be seen before.",
        "we explore the cooperation of multiple systems from a new perspective.",
        "They use translation derivations from different decoders jointly as a bridge to connect different models.",
        "Different from their work, we devise a heuristic method to obtain word alignment information from the derivation of each decoder, which can be embedded for word-level PHS combination easily and efficiently."
      ]
    },
    {
      "heading": "5. Conclusion and Future Work",
      "text": [
        "In this paper, we propose a new SMT decoding framework named hybrid decoding, in which multiple decoders work synchronously to conduct local decoding and local word-level PHS combination in turn.",
        "We also devise a heuristic method to obtain word alignment information directly from the translation derivations, which is both intuitive and efficient.",
        "Experimental results show that with hybrid decoding the overall performance can be improved significantly over both the individual baseline decoder and the state-of-the-art system combination method.",
        "In the future, we will involve more individual SMT decoders into hybrid decoding.",
        "In addition, we would like to keep on this work in two directions.",
        "On the one hand, start-up threshold of PHS combination will be explored in detail to find its underlying impact on hybrid decoding.",
        "On the other hand, we will try to employ a more theoretically sound approach to conduct the feature space mapping from the feature space of confusion network to that of individual decoders.",
        "NIST 2005",
        "NIST 2006",
        "Top-1",
        "0.3817",
        "0.3415",
        "MBR",
        "0.3884*",
        "0.3513*"
      ]
    }
  ]
}
