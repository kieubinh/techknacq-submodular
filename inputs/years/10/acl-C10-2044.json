{
  "info": {
    "authors": [
      "Zhongjun He",
      "Yao Meng",
      "Hao Yu"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2044",
    "title": "Learning Phrase Boundaries for Hierarchical Phrase-based Translation",
    "url": "https://aclweb.org/anthology/C10-2044",
    "year": 2010
  },
  "references": [
    "acl-D08-1024",
    "acl-J04-4002",
    "acl-J07-2003",
    "acl-N03-1017",
    "acl-N09-1025",
    "acl-P02-1038",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P05-1033",
    "acl-P08-1009",
    "acl-P08-1114",
    "acl-P09-1036",
    "acl-W04-3236",
    "acl-W04-3250",
    "acl-W08-0302"
  ],
  "sections": [
    {
      "text": [
        "Zhongjun HE Yao MENG Hao YU",
        "Hierarchical phrase-based models provide a powerful mechanism to capture non-local phrase reorderings for statistical machine translation (SMT).",
        "However, many phrase reorderings are arbitrary because the models are weak on determining phrase boundaries for pattern-matching.",
        "This paper presents a novel approach to learn phrase boundaries directly from word-aligned corpus without using any syntactical information.",
        "We use phrase boundaries, which indicate the beginning/ending of phrase reordering, as soft constraints for decoding.",
        "Experimental results and analysis show that the approach yields significant improvements over the baseline on large-scale Chinese-to-English translation."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The hierarchial phrase-based (HPB) model (Chiang, 2005) outperformed previous phrase-based models (Koehn et al., 2003; Och and Ney, 2004) by utilizing hierarchical phrases consisting ofboth words and variables.",
        "Thus the HPB model has generalization ability: a translation rule learned from a phrase pair can be used for other phrase pairs with the same pattern, e.g. reordering information of a short span can be applied for a large span during decoding.",
        "Therefore, the model captures both short and long distance phrase reorderings.",
        "However, one shortcoming of the HPB model is that it is difficult to determine phrase boundaries for pattern-matching.",
        "Therefore, during decoding, a rule may be applied for all possible source phrases with the same pattern.",
        "However, incorrect pattern-matching will cause wrong translation.",
        "Consider the following rule that is used to translate the Chinese sentence in Figure 1 into English:",
        "The rule translates the Chinese word \"de\" into English word \"in\", and swaps the left sub-phrase covered by XL and the right sub-phrase covered by XR on the target side.",
        "However, XL may pattern-match 5 spans on the left side of \"de\" and XR may pattern-match 3 spans on the right side.",
        "Therefore, the rule produces 15 different derivations.",
        "However, 14 of them are incorrect.",
        "The correct derivation Sc is shown in Figure 2, while one of the wrong derivations Si is shown in Figure 3.",
        "We observe that the basic difference between Sc and Si is the phrase boundary matched by \"Xr\".",
        "In Sc, XR matches the span [7, 9] and moves it as a whole unit.",
        "While in Si, XR matches the span [7,8] and left the last word [9,9] be translated separately.",
        "Similarly, other incorrect derivations are caused by inadequate pattern-matching of XL and/or XR.",
        "Previous research showed that phrases should be constrained to some extent for improving translation quality.",
        "Most of the existing approaches utilized syntactic information to constrain phrases to respect syntactic boundaries.",
        "Chiang (2005) introduced a constituent feature to reward phrases that match a syntactic tree but did not yield significant improvement.",
        "Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary.",
        "This led to a substantial improvements.",
        "Gimpel and Smith (2008) presented rich contextual features on the source side including constituent syntactical features for phrase-based translation.",
        "Cherry (2008) utilized a dependency tree as a soft constraint to detect syntactic cohesion violations for a phrase-based",
        "W^W*5 We titer de shouwei",
        "She1 will2 become3 the4 first5 femalee president7 zongtong",
        "India'sg history1o",
        "Figure 1: An example of Chinese-English translation.",
        "The rule X – {XL de XR, XR in XL) pattern-matches 5 and 3 spans on the left and right of the Chinese word \"de\", respectively.",
        "system.",
        "Xiong et al.",
        "(2009) presented a syntax-driven bracketing model to predict whether two phrases are translated together or not, using syntactic features learned from training corpus.",
        "Although these approaches differ from each other, the main basic idea is the utilization of syntactic information.",
        "In this paper, we present a novel approach to learn phrase boundaries for hierarchical phrase-based translation.",
        "A phrase boundary indicates the beginning or ending of a phrase reordering.",
        "Motivated by Ng and Low (2004) that built a classifier to predict word boundaries for word segmentation, we build a classifier to predict phrase boundaries.",
        "We classify each source word into one ofthe 4 boundary tags: \"b\" indicates the beginning of a phrase, \"m\" indicates a word appears in the middle of a phrase, \"e\" indicates the end of a phrase, \"s\" indicates a single-word phrase.",
        "We use phrase boundaries as soft constraints for decoding.",
        "To do this, we incorporate our classifier as a feature into the HPB model and propose an efficient decoding algorithm.",
        "Compared to the previous work, out approach has the following advantages:",
        "• Our approach maintains the strength of the phrase-based models since it does not require any syntactical information.",
        "Therefore, phrases do not need to respect syntactic boundaries.",
        "• The training instances are directly learned from a word-aligned bilingual corpus, rather than from manually annotated corpus.",
        "ta jiang chengwei yindu y",
        "• The decoder outputs phrase segmentation information as a byproduct, in addition to translation result.",
        "We evaluate our approach on large-scale Chinese-to-English translation.",
        "Experimental results and analysis show that using phrase boundaries as soft constraints achieves significant improvements over the baseline system."
      ]
    },
    {
      "heading": "2. Previous Work",
      "text": [
        "In some languages, such as Chinese, words are not demarcated.",
        "Therefore, it is a preliminary task to determine word boundaries for a sentence, which is the so-called word segmentation.",
        "Ng and Low (2004) regarded word segmentation as a classification problem.",
        "They labelled each Chinese character with one of 4 possible boundary tags: \"b\", \"m\", \"e\" respectively indicates the begin, the middle and the end of a word, and \"s\" indicates a single-character word.",
        "Their segmenter was built within a maximum entropy framework and trained on manually segmented sentences.",
        "Learning phrase boundaries is analogous to word boundaries.",
        "The basic difference is that the unit for learning word boundaries is character while the unit for learning phrase boundaries is word.",
        "In this paper, we adopt the boundary tags presented by Ng and Low (2004) and build a classifier to predict phrase boundaries within maximum entropy framework.",
        "We train it directly on a word-aligned bilingual corpus, without any manually annotation and syntactical information.",
        "We built a hierarchical phrase-based MT system (Chiang, 2007) based on weighted SCFG.",
        "The translation knowledge is represented by rewriting rules:",
        "where X is a non-terminal, a and 7 are source and target strings, respectively.",
        "Both of them contain words and possibly co-indexed non-terminals.",
        "~ describes a one-to-one correspondence between non-terminals in a and 7.",
        "Chiang (2007) used the standard log-linear framework (Och and Ney, 2002) to combine various features:",
        "where hi(a, 7) is a feature function and Ai is the weight of hi.",
        "Analogous to the previous phrase-based model, Chiang defined the following features: translation probabilities p(y|a) and p(a|7), lexical weights pw(7|a) and pw(a|7), word penalty, rule penalty, and a target n-gram language model.",
        "In this paper, we integrate a phrase boundary classifier as an additional feature into the loglinear model to provide soft constraint for pattern-matching during decoding.",
        "The feature weights are optimized by MERT algorithm (Och, 2003)."
      ]
    },
    {
      "heading": "3. Learning Phrase Boundaries",
      "text": [
        "We build a phrase boundary classifier (PBC) within a maximum entropy framework.",
        "The PBC predicts a boundary tag for each source word, considering contextual features:",
        "where, t G (b, m, e, s}, /j is the jth word in source sentence FJ, hi is a feature function and Ai is the weight of hi.",
        "To build PBC, we first present a method to recognize phrase boundaries and extract training examples from word-aligned bilingual corpus, then we define contextual feature functions.",
        "During decoding, intuitively, words within a phrase should be translated or moved together.",
        "Therefore, a phrase boundary should indicate reordering information.",
        "We assign one of the boundary tags ( b, m, e, s) to each word in source sentences.",
        "Thus the word with tag b, e or s is a phrase boundary.",
        "One question is that how to assign boundary tag to a word?",
        "In this paper, we recognize the largest source span which has the monotone translation.",
        "Then we assign boundary jointly held by a short visit tags to each word in the source span, according to their position.",
        "To do this, we first introduce some notations.",
        "Given a bilingual sentence (FJ, E{) together with word alignment matrix A, we use L(Aj) and H(Aj) to represent the lowest and highest target word position which links to the source word /j, respectively.",
        "Since the word alignment for /j maybe \"one-to-many\", all the corresponding target words will appear in the span [L(Aj), H(Aj)].",
        "The first condition indicates that (Fj, eL((Aj2.",
        ")'> ) is a phrase pair as described previously in phrase-based SMT models.",
        "While the second condition indicates that the lower target bound linked to a source word cannot be lower than any target word position linked to the previous source word.",
        "Therefore, a monotone span does not contain crossed links or internal reorderings.",
        "Considering that word alignments could be very noisy and complex in real-world data, we define pseudo-monotone (PM) span by loosening the second condition:",
        "This condition allows crossed links to some extent by loosening the bound of Akl from upper to lower.",
        "Figure 4 (a) shows an example of monotone span, in which the translation is monotone.",
        "While Figure 4 (b) is not a monotone span because there is a cross link between the upper bound of \"£I1T' and the lower bound of \"ijj U\" on the target side.",
        "However, it is a PM span according to the definition.",
        "Note that in some cases, a source word may not be contained in any phrase pair, therefore we consider a single word span as a PM span, specificly.",
        "An interesting feature of PM span is that if two PM spans are consecutive on both source side and their corresponding target side, the two PM spans can be combined as a larger PM span.",
        "Formally,",
        "where [j i, j] and [j +1, j2] are PM spans, [i i, i] and [i + 1, i2] are the target spans corresponding to [j i, j] and [j +1, j2], respectively.",
        "For example, Figure 4 (a) shows a PM phrase pair that consists of two small PM pairs n% jointly\" and W, held by\".",
        "In this paper, we are interested in phrase reordering boundaries for a source sentence.",
        "We define translation span (TS) the largest possible PM span.",
        "A TS may consist of one or more PM spans.",
        "According to our definition, cross links may appear within PM spans but do not appear between PM spans within a TS.",
        "Therefore, TS is the largest possible span that will be translated as a unit and phrase reorderings may occur between TSs during decoding.",
        "To obtain phrase boundary examples from word-aligned bilingual sentences, we first find all possible TSs and then assign boundary tags to each word.",
        "For a TS [j i; j2] (j i < j2) that contain more than two words, we assign \"b\" to the first word / and \"e\" to the last word / 2 , and \"m\" to the middle words / (j < j < j2).",
        "For a single word span TS [j, j], we assign \"s\" to the word / .",
        "Figure 5 shows an example oflabelling source words with boundary tags.",
        "The source sentence is segmented into 4 TSs.",
        "Using the phrase boundary information to guide decoding, the decoder will produce the correct derivation and translation as shown in Figure 2.",
        "become the first female president in",
        "India's history",
        "Figure 5: Illustration for labelling the source words with boundary tags.",
        "The solid boxes present word alignments.",
        "The bordered boxes are",
        "TSs.",
        "The features we used for the PS model are analogous to (Ng and Low, 2004).",
        "For a word W0, we define the following contextual features with a window of \"n\":",
        "• The word feature Wn, which denotes the left (right) n words of the current word W0;",
        "• The part-of-speech (POS) feature Pn, which denotes the POS tag of the word Wn.",
        "For example, the tag of the word \"f&Ä (become)\" in Figure 5 is \"e\", indicating that it is the end of a phrase.",
        "If we set the context window n = 2, the features of the word \"JrScÄ (become)\" are:",
        "We collect TSs from bilingual sentences together with the contextual features and used a MaxEnt toolkit (Zhang, 2004) to train a PBC."
      ]
    },
    {
      "heading": "4. Phrase Boundary Constrained Decoding",
      "text": [
        "Give a source sentence, we can assign boundary tags to each word by running the PBC.",
        "During decoding, a rule is prohibited to pattern-match across phrase boundaries.",
        "By doing this, the PBC is integrated as a hard constraint.",
        "However, this method will invalidate a large number ofrules and the decoder suffers from a risk that there are not enough rules to cover the source sentence.",
        "Alternatively, inspired by previous approaches, we integrate the phrase boundary classifier as a soft constraint by incorporating it as a feature into the HPB model:",
        "hpfec(FiJ)= log(FJ Ptag(t|fj,FiJ)) j=i",
        "To perform translation, for each word /j in a source sentence Fj, we first compute all tag probabilities Ptag(t|fj), where t G (b, m, e, s), j G [1, J], according to Equation 4.",
        "Therefore, we build a 4 x J tag-word probability matrix (TPM).",
        "TPM[i, j] indicates the probability of the word /j labelled with the tag ti.",
        "Table 1 shows the TPM for a source text \"M # J&Ä\".",
        "Then we select rule options from the rule table that can be used for translating the source text.",
        "Since each rule option (/, ê, a) can be regarded as a bilingual sentence with word alignments, thus we find all TS in f and assign an initial tag (IT) for each source word.",
        "This procedure is analogous to label phrase boundary tags for a word-aligned bilingual sentence.",
        "For example, the following rules are used for translating the Chinese sentence in Table 1:",
        "M",
        "b",
        "0.78",
        "0.10",
        "1.2e-5",
        "m",
        "6.4e-8",
        "0.75",
        "5.4e-5",
        "e",
        "2.1e-8",
        "0.11",
        "0.87",
        "s",
        "0.22",
        "0.04",
        "0.13",
        "5",
        "■L",
        "Since both the source sides of these two rules are PM spans according to the word alignments, the IT sequences for rule (8) and (9) are \"b *\"and \"b e\", respectively.",
        "According to Table 1, the initial hp6c score for these two rules can be computed as follows:",
        "Note that to keep the tag sequence valid, e.g. \"m\" follows \"b\" rather than \"s\", the ITs maybe updated during decoding.",
        "The tag-updating should be consistent with the definition of TS as described in Section 3.1.",
        "Specifically, when the non-terminal symbol X is derived from its covered span (X), the boundary tags should be updated.",
        "When a tag of word fj is updated from tkl to tk2, the PBC score should also be updated according to TPM:",
        "The following is a derivation ofthe source sentence in Table 1:",
        "When X is derived, the tag ofits leftboundary word \"ff\" is updated from \"b\" to \"m\".",
        "The reason is that after derivation, the combined span forms a larger PM span and the left boundary of f (X i ) should be updated.",
        "As a result, the hp6c score is recomputed:",
        "The decoding algorithm is efficient since the computing of the PBC score is a procedure of table-lookup."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "Our experiments were on Chinese-to-English translation.",
        "The training corpus (77M+81M) we used are from LDC .",
        "The evaluation metric is BLEU (Papineni et al., 2002), as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where n = 4.",
        "To obtain word alignments, we first ran directions and then refined it by \"grow-diag-final\" method (Koehn et al., 2003).",
        "For the language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train two 4-gram models on xinhua portion of Giga-Word corpus and the English side of the training corpus.",
        "The NIST MT03 test set is used to tune the feature weights of the log-linear model by MERT (Och, 2003).",
        "We tested our system on the NIST MT06 and MT08 test sets.",
        "The results are shown in Table 2.",
        "We tested various settings of the context window.",
        "It is observed that the small values of n (n = 1, 2) drop the BLEU score, suggesting that perhaps there are not enough contextual information.",
        "With more contextual information is used, the BLEU scores are improved over all test sets.",
        "When n = 3, the most significant improvements are obtained on MT06G and MT08.",
        "The improvements over the baseline are statistically significant at p < 0.01 by using the significant test method described in (Koehn, 2004).",
        "While for MT06N, the optimized context window size is n = 4 but the improvement is not statistically significant.",
        "In most cases, with n larger than 3, we do not obtain further improvements because of the data sparseness for training",
        "Table 2: Results on the test sets with different context window (n) of the phrase boundary classifier.",
        "The largest BLEU score on each test set is in bold.",
        "set.",
        "*: significantly better than the baseline at p < 0.01.",
        "the classifier.",
        "6 Discussion",
        "The experimental results show that the phrase boundary constrained method improves the BLEU score over the baseline system.",
        "Furthermore, we are interested in how the PBC affects the translation results?",
        "We compared the outputs generated by the baseline and \"+PBC (n = 3)\" system and found some interesting translations.",
        "For example, the translations of a source sentence of NIST08 are as follows :",
        "• Src: Hi «m ^Bï ÜUS II",
        "• Ref: US Treasury-Secretary2 Arrives-in3 China4 for-a-Visit-withs Environmente and7Exchange-Rate8 as9 Focus i 0; i i",
        "• HPB: US Treasury2 in-environmental-protection and7 visits China4 is9 key i ito-the-concern-of 0 the-exchange-rateI",
        "• +PBC: US Treasury2 arrived-in3 China4 for-a-visit5 environmental-protection6 and7exchange-rate8 is9 concerned-abouti0 the-",
        "In the example, both \"ï ffi\" and in the source sentence are the concern of the \"visit\".",
        "Therefore, the source span [6,8] indicates a cohesive phrase, which should be translated as a whole unit.",
        "However, the baseline translates the spans [6, 7] and [8,8] separately.",
        "It moves [6, 7] before \"visit China\" and [8, 8] after \"concern\".",
        "This makes an mistake on phrase reordering.",
        "We observe that the \"+PBC\" system produces a better translation.",
        "After incorporating the PBC as a soft constraint, the system assigns a boundary tag to each source word and segments the source sentence into three TSs.",
        "According to our definition, TSs are encouraged as pseudo-monotone translation unit during decoding.",
        "As a result, the \"+PBC\" system discourages some arbitrary reordering rules and produces more fluent translation."
      ]
    },
    {
      "heading": "7. Conclusion and Future Work",
      "text": [
        "This paper presented a phrase boundary constrained method for hierarchical phrase-based translation.",
        "A phrase boundary indicates begin or end of a phrase reordering.",
        "We built a phrase boundary classifier within a maximum entropy framework and learned phrase boundary examples directly from word-aligned bilingual corpus.",
        "We proposed an efficient decoding method to integrate the PBC into the decoder as a soft constraint.",
        "Experiments and analysis show that the phrase boundary constrained method achieves significant improvements over the baseline system.",
        "The most advantage of the PBC is that it handles both syntactic and non-syntactic phrases.",
        "In the future, We would like to try different methods to determine more informative phrase boundaries, e.g. Xiong et al.",
        "(2010) proposed a method to learn translation boundaries from a hierarchical tree that decomposed from word alignments using a shift-reduce algorithm.",
        "In addition, we will try more features as described in (Chiang et al., 2008; Chiang et al., 2009), e.g. the length of the phrases that covered by non-terminals.",
        "System",
        "MT06G",
        "MT06N",
        "MT08",
        "baseline",
        "14.66",
        "34.42",
        "26.29",
        "+PBC(n=1)",
        "13.78",
        "33.20",
        "24.58",
        "+PBC (n=2)",
        "14.34",
        "34.21",
        "25.87",
        "+PBC (n=3)",
        "15.19*",
        "34.63",
        "27.25*",
        "+PBC (n=4)",
        "14.76",
        "34.73",
        "26.70"
      ]
    }
  ]
}
