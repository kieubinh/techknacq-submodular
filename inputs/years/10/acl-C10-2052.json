{
  "info": {
    "authors": [
      "Dirk Hovy",
      "Stephen Tratz",
      "Eduard Hovy"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2052",
    "title": "What’s in a Preposition? Dimensions of Sense Disambiguation for an Interesting Word Class",
    "url": "https://aclweb.org/anthology/C10-2052",
    "year": 2010
  },
  "references": [
    "acl-J09-2002",
    "acl-J93-2004",
    "acl-J96-1002",
    "acl-N09-1025",
    "acl-N09-3017",
    "acl-P07-1005",
    "acl-P07-1096",
    "acl-P98-1013",
    "acl-W03-0411",
    "acl-W07-2005",
    "acl-W07-2051"
  ],
  "sections": [
    {
      "text": [
        "What's in a Preposition?",
        "Dimensions of Sense Disambiguation for an Interesting Word Class",
        "Dirk Hovy, Stephen Tratz, and Eduard Hovy",
        "Choosing the right parameters for a word sense disambiguation task is critical to the success of the experiments.",
        "We explore this idea for prepositions, an often overlooked word class.",
        "We examine the parameters that must be considered in preposition disambiguation, namely context, features, and granularity.",
        "Doing so delivers an increased performance that significantly improves over two state-of-the-art systems, and shows potential for improving other word sense disambiguation tasks.",
        "We report accuracies of 91.8% and 84.8% for coarse and fine-grained preposition sense disambiguation, respectively."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Ambiguity is one of the central topics in NLP.",
        "A substantial amount of work has been devoted to disambiguating prepositional attachment, words, and names.",
        "Prepositions, as with most other word types, are ambiguous.",
        "For example, the word in can assume both temporal (\"in May\") and spatial (\"in the US\") meanings, as well as others, less easily classifiable (\"in that vein\").",
        "Prepositions typically have more senses than nouns or verbs (Litkowski and Hargraves, 2005), making them difficult to disambiguate.",
        "Preposition sense disambiguation (PSD) has many potential uses.",
        "For example, due to the relational nature of prepositions, disambiguating their senses can help with all-word sense disambiguation.",
        "In machine translation, different senses of the same English preposition often correspond to different translations in the foreign language.",
        "Thus, disambiguating prepositions correctly may help improve translation quality.",
        "Coarse-grained PSD can also be valuable for information extraction, where the sense acts as a label.",
        "In a recent study, Hwang et al.",
        "(2010) identified preposition related features, among them the coarse-grained PP labels used here, as the most informative feature in identifying caused-motion constructions.",
        "Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing, one of the most frequent sources of parse errors.",
        "Several papers have successfully addressed PSD with a variety of different approaches (Rudz-icz and Mokhov, 2003; O'Hara and Wiebe, 2003; Ye and Baldwin, 2007; O'Hara and Wiebe, 2009; Tratz and Hovy, 2009).",
        "However, while it is often possible to increase accuracy by using a different classifier and/or more features, adding more features creates two problems: a) it can lead to overfitting, and b) while possibly improving accuracy, it is not always clear where this improvement comes from and which features are actually informative.",
        "While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions.",
        "We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative.",
        "We explore the different options for context and feature selection, the influence of different preprocessing methods, and different levels of sense granularity.",
        "Using the resulting parameters in a Maximum Entropy classifier, we are able to improve significantly over existing results.",
        "The general outline we present can potentially be extended to other word classes and improve WSD in general."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Rudzicz and Mokhov (2003) use syntactic and lexical features from the governor and the preposition itself in coarse-grained PP classification with decision heuristics.",
        "They reach an average F-measure of 89% for four classes.",
        "This shows that using a very small context can be effective.",
        "However, they did not include the object of the preposition and used only lexical features for classification.",
        "Their results vary widely for the different classes.",
        "O'Hara and Wiebe (2003) made use of a window size of five words and features from the Penn Treebank (PTB) (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions.",
        "They show that using high level features, such as semantic roles, significantly aid disambiguation.",
        "They caution that using collocations and neighboring words indiscriminately may yield high accuracy, but has the risk of overfit-ting.",
        "O'Hara and Wiebe (2009) show comparisons of various semantic repositories as labels for PSD approaches.",
        "They also provide some results for PTB-based coarse-grained senses, using a five-word window for lexical and hypernym features in a decision tree classifier.",
        "SemEval 2007 (Litkowski and Hargraves, 2007) included a task for fine-grained PSD (more than 290 senses).",
        "The best participating system, that of Ye and Baldwin (2007), extracted part-of-speech and WordNet (Fellbaum, 1998) features using a word window of seven words in a Maximum Entropy classifier.",
        "Tratz and Hovy (2009) present a higher-performing system using a set of 20 positions that are syntactically related to the preposition instead of a fixed window size.",
        "Though using a variety of different extraction methods, contexts, and feature words, none of these approaches explores the optimal configurations for PSD."
      ]
    },
    {
      "heading": "3. Theoretical Background",
      "text": [
        "The following parameters are applicable to other word classes as well.",
        "We will demonstrate their effectiveness for prepositions.",
        "Analyzing the syntactic elements of prepositional phrases, one discovers three recurring elements that exhibit syntactic dependencies and define a prepositional phrase.",
        "The first one is the governing word (usually a noun, verb, or adjec-tive), the preposition itself, and the object of the preposition.",
        "Prepositional phrases can be fronted (\"In May, prices dropped by 5%\"), so that the governor (in this case the verb \"drop\") occurs later in the sentence.",
        "Similarly, the object can be fronted (consider \"a dessert to die for\").",
        "In the simplest version, we can do classification based only on the preposition and the governor or object alone.",
        "Furthermore, directly neighboring words can influence the preposition, mostly two-word prepositions such as \"out of\" or \"because of\".",
        "To extract the words discussed above, one can either employ a fixed window size, (which has to be large enough to capture the words), or select them based on heuristics or parsing information.",
        "The governor and object can be hard to extract if they are fronted, since they do not occur in their unusual positions relative to the preposition.",
        "While syntactically related words improve over fixed-window-size approaches (Tratz and Hovy, 2009), it is not clear which words contribute most.",
        "There should be an optimal context, i.e., the smallest set of words that achieves the best accuracy.",
        "It has to be large enough to capture all relevant information, but small enough to avoid noise words.We surmise that earlier approaches were not utilizing that optimal context, but rather include a lot of noise.",
        "Depending on the task, different levels of sense granularity may be used.",
        "Fewer senses increase the likelihood of correct classification, but may incorrectly conflate prepositions.",
        "A finer granularity can help distinguish nuances and better fit the different contexts.",
        "However, it might suffer from sparse data."
      ]
    },
    {
      "heading": "4. Experimental Setup",
      "text": [
        "We explore the different context types (ixed window size vs. selective), the inluence of the words in that context, and the preprocessing method (heuristics vs. parsing) on both coarse and ine-grained disambiguation.",
        "We use a most-frequent-sense baseline.",
        "In addition, we compare to the state-of-the-art systems for both types of granularity (O'Hara and Wiebe, 2009; Tratz and Hovy, 2009).",
        "Their results show what has been achieved so far in terms of accuracy, and serve as a second measure for comparison beyond the baseline.",
        "We use the MALLET implementation (McCal-lum, 2002) of a Maximum Entropy classifier (Berger et al., 1996) to construct our models.",
        "This classiier was also used by two state-of-the-art systems (Ye and Baldwin, 2007; Tratz and Hovy, 2009).",
        "For fine-grained PSD, we train a separate model for each preposition due to the high number of possible classes for each individual preposition.",
        "For coarse-grained PSD, we use a single model for all prepositions, because they all share the same classes.",
        "We use two different data sets from existing resources for coarse and fine-grained PSD to make our results as comparable to previous work as possible.",
        "For the coarse-grained disambiguation, we use data from the POS tagged version of the Wall Street Journal (WSJ) section of the Penn Tree-Bank.",
        "A subset of the prepositional phrases in this corpus is labelled with a set of seven classes: beneficial (BNF), direction (DIR), extent (EXT), location (LOC), manner (MNR), purpose (PRP), and temporal (TMP).",
        "We extract only those prepositions that head a PP labelled with such a class (N = 35, 917).",
        "The distribution of classes is highly skewed (cf.",
        "Figure 1).",
        "We compare the results of this task to the indings of O'Hara and",
        "Wiebe (2009).",
        "For the ine-grained task, we use data from the SemEval 2007 workshop (Litkowski and Har-graves, 2007), separate XML files for the 34 most frequent English prepositions, comprising 16, 557 training and 8096 test sentences, each instance containing one example of the respective preposition.",
        "Each preposition has between two and 25 senses (9.76 on average) as deined by The Preposition Project (Litkowski and Hargraves, 2005).",
        "We compare our results directly to the indings from Tratz and Hovy (2009).",
        "As in the original workshop task, we train and test on separate sets."
      ]
    },
    {
      "heading": "5. Results",
      "text": [
        "In this section we show experimental results for the inluence of word extraction method (parsing vs. POS-based heuristics), context, and feature selection on accuracy.",
        "Each section compares the results for both coarse and ine-grained granularity.",
        "Accuracy for the coarse-grained task is in all experiments higher than for the ine-grained one.",
        "In order to analyze the impact of the extracPtaiogen method, we compare parsing versus POS-based heuristics for word extraction.",
        "Both O'Hara and Wiebe (2009) and Tratz and Hovy (2009) use constituency parsers to prepro-cess the data.",
        "However, parsing accuracy varies, and the problem of PP attachment ambiguity increases the likelihood of wrong extractions.",
        "This is especially troublesome in the present case, where we focus on prepositions.",
        "We use the MALT parser (Nivre et al., 2007), a state-of-the-art dependency parser, to extract the governor and object.",
        "The alternative is a POS-based heuristics approach.",
        "The only preprocessing step needed is POS tagging of the data, for which we used the system of Shen et al.",
        "(2007).",
        "We then use simple heuristics to locate the prepositions and their related words.",
        "In order to determine the governor in the absence of constituent phrases, we consider the possible governing noun, verb, and adjective.",
        "The object of the preposition is extracted as irst noun phrase head to the right.",
        "This approach is faster than parsing, but has problems with longrange dependencies and fronting of the PP (e.g., the PP appearing earlier in the sentence than its governor).",
        "Interestingly, the extraction method does not signiicantly affect the inal score for ine-grained PSD (see Table 1).",
        "The high score achieved when using the MALT parse for coarse-grained PSD can be explained by the fact that the parser was originally trained on that data set.",
        "The good results we see when using heuristics-based extraction only, however, means we can achieve high-accuracy PSD even without parsing.",
        "We compare the effects of ixed window size versus syntactically related words as context.",
        "Table 2 shows the results for the different types and sizes of contexts.",
        "The results show that the approach using both governor and object is the most accurate one.",
        "Of the ixed-window-size approaches, three words to either side works best.",
        "This does not necessarily relect a general property of that window size, but can be explained by the fact that most governors and objects occur within this window size.",
        "This distance can vary from corpus to corpus, so window size would have to be determined individually for each task.",
        "The difference between using governor and preposition versus preposition and object between coarse and ine-grained classiication might relect the annotation process: while Litkowski and Hargraves (2007) selected examples based on a search for governors, most annotators in the PTB may have based their decision of the PP label on the object that occurs in it.",
        "We conclude that syntactically related words present a better context for classiication than ixed window sizes.",
        "Having established the context we want to use, we now turn to the details of extracting the feature words from that context.",
        "Using higher-level features instead of lexical ones helps accounting for sparse training data (given an ininite amount of data, we would not need to take any higher-level features into account, since every case would be covered).",
        "Compare O'Hara and Wiebe (2009).",
        "Context",
        "coarse",
        "fine",
        "2-word window",
        "91.6",
        "80.4",
        "3-word window",
        "92.0",
        "81.4",
        "4-word window",
        "91.6",
        "79.8",
        "5-word window",
        "91.0",
        "78.7",
        "Governor, prep",
        "80.7",
        "78.9",
        "Prep, object",
        "94.2",
        "56.9",
        "Governor, prep, object",
        "94.0",
        "84.8",
        "extraction method",
        "fine",
        "coarse",
        "MALT",
        "84.4",
        "94.0",
        "Heuristics",
        "84.8",
        "90.9",
        "MALT + Heuristics",
        "84.8",
        "91.8",
        "Following the prepocessing, we use a set of rules to select the feature words, and then generate feature values from them using a variety of feature-generating functions.",
        "The word-selection rules are listed below.",
        "Word-Selection Rules• Governor from the MALT parse",
        "• Object from the MALT parse",
        "• Heuristically determined object of the preposition",
        "• First verb to the left of the preposition",
        "• First verb/noun/adjective to the left of the preposition",
        "• Union of (First verb to the left, First verb/noun/adjective to the left)",
        "• First word to the left",
        "The feature-generating functions, many of which utilize WordNet (Fellbaum, 1998), are listed below.",
        "To conserve space, curly braces are used to represent multiple functions in a single line.",
        "The name of each feature is the combination of the word-selection rule and the output from the feature-generating function.",
        "WordNet-based Features",
        "• All terms in the definitions ('glosses') of the word",
        "• Lexicographer ile names for the word",
        "• Lists of all link types (e.g., meronym links) associated with the word",
        "• Part-of-speech indicators for the existence of NN/VB/JJ/RB entries for the word",
        "• All sentence frames for the word",
        "• All {part, member, substance}-of holonyms for the word",
        "Other Features",
        "• Indicator that the word-inding rule found a word",
        "• Capitalization indicator",
        "• { Lemma, surface form} of the word",
        "• Part-of-speech tag for the word",
        "• The {first, last} {two, three} letters of each word",
        "• Indicators for suffix types (e.g., de-adjectival, de-nominal [non]agentive, de-verbal [non]agentive)",
        "• Indicators for a wide variety of other afixes including those related to degree, number, order, etc.",
        "(e.g., ultra-, poly-, post-)",
        "• Roget's Thesaurus divisions for the word",
        "To establish the impact of each feature word on the outcome, we use leave-one-out and only-one evaluation.",
        "The results can be found in Table 3.",
        "A word that does not perform well as the only attribute may still be important in conjunction with others.",
        "Conversely, leaving out a word may not hurt performance, despite being a good single attribute.",
        "Table 3: Accuracies (%) for Leave-One-Out (LOO) and Only-One Word-Extraction-Rule Evaluation.",
        "none includes all words and serves for comparison.",
        "Important words reduce accuracy for LOO, but rank high when used as only rule.",
        "Independent of the extraction method (MALT parser or POS-based heuristics), the governor is the most informative word.",
        "Combining several heuristics to locate the governor is the best single feature for ine-grained classiication.",
        "The rule looking only for a governing verb fails to account",
        "coarse",
        "fine",
        "Word",
        "LOO",
        "Only",
        "LOO",
        "Only",
        "MALT governor",
        "92.1",
        "80.1",
        "84.3",
        "78.9",
        "MALT object",
        "93.4",
        "94.2",
        "84.9",
        "56.3",
        "Heuristics VB to left",
        "92.0",
        "77.9",
        "85.0",
        "62.1",
        "Heur.",
        "NN/VB/ADJ to left",
        "92.1",
        "78.7",
        "84.3",
        "78.5",
        "Heur.",
        "Governor Union",
        "92.1",
        "78.4",
        "84.5",
        "81.0",
        "Heuristics word to left",
        "92.0",
        "78.8",
        "84.4",
        "77.2",
        "Heuristics object",
        "91.9",
        "93.0",
        "84.9",
        "56.8",
        "none",
        "91.8",
        "-",
        "84.8",
        "-",
        "Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics.",
        "Sorted by preposition.",
        "for noun governors, which consequently leads to a slight improvement when left out.",
        "Curiously, the word directly to the left is a better single feature than the object (for ine-grained classiication).",
        "Leaving either of them out increases accuracy, which implies that their information can be covered by other words.",
        "fine",
        "coarse",
        "fine",
        "coarse",
        "Prep",
        "Total",
        "Acc",
        "Total",
        "Acc",
        "Prep",
        "Total",
        "Acc",
        "Total",
        "Acc",
        "aboard",
        "-",
        "-",
        "6",
        "100.0",
        "like",
        "125",
        "90.4",
        "53",
        "47.2",
        "about",
        "364",
        "94.0",
        "5",
        "80.0",
        "near",
        "-",
        "-",
        "74",
        "93.2",
        "above",
        "23",
        "69.6",
        "78",
        "65.4",
        "nearest",
        "-",
        "-",
        "1",
        "0.0",
        "across",
        "151",
        "96.7",
        "87",
        "79.3",
        "next",
        "-",
        "-",
        "7",
        "71.4",
        "after",
        "53",
        "79.2",
        "841",
        "92.5",
        "of",
        "1478",
        "87.9",
        "71",
        "64.8",
        "against",
        "92",
        "92.4",
        "16",
        "43.8",
        "off",
        "76",
        "84.2",
        "28",
        "75.0",
        "along",
        "173",
        "96.0",
        "45",
        "71.1",
        "on",
        "441",
        "81.4",
        "2287",
        "90.8",
        "alongside",
        "-",
        "-",
        "5",
        "80.0",
        "onto",
        "58",
        "91.4",
        "15",
        "53.3",
        "amid",
        "-",
        "-",
        "58",
        "70.7",
        "out",
        "-",
        "-",
        "90",
        "68.9",
        "among",
        "50",
        "80.0",
        "358",
        "93.9",
        "outside",
        "-",
        "-",
        "62",
        "90.3",
        "amongst",
        "-",
        "-",
        "1",
        "0.0",
        "over",
        "98",
        "79.6",
        "417",
        "89.4",
        "around",
        "155",
        "69.0",
        "107",
        "86.0",
        "past",
        "-",
        "-",
        "6",
        "83.3",
        "as",
        "84",
        "100.0",
        "232",
        "84.5",
        "per",
        "-",
        "-",
        "3",
        "100.0",
        "astride",
        "-",
        "-",
        "2",
        "50.0",
        "round",
        "82",
        "65.9",
        "-",
        "-",
        "at",
        "367",
        "86.4",
        "3078",
        "92.0",
        "since",
        "-",
        "-",
        "449",
        "94.4",
        "atop",
        "-",
        "-",
        "5",
        "100.0",
        "than",
        "-",
        "-",
        "2",
        "0.0",
        "because",
        "-",
        "-",
        "420",
        "91.7",
        "through",
        "208",
        "48.1",
        "364",
        "69.0",
        "before",
        "20",
        "90.0",
        "384",
        "83.3",
        "throughout",
        "-",
        "-",
        "62",
        "93.5",
        "behind",
        "68",
        "77.9",
        "65",
        "87.7",
        "till",
        "-",
        "-",
        "3",
        "100.0",
        "below",
        "-",
        "-",
        "94",
        "71.3",
        "to",
        "572",
        "89.7",
        "3166",
        "97.5",
        "beneath",
        "28",
        "78.6",
        "11",
        "72.7",
        "toward",
        "-",
        "-",
        "55",
        "65.5",
        "beside",
        "29",
        "100.0",
        "4",
        "100.0",
        "towards",
        "102",
        "97.1",
        "2",
        "100.0",
        "besides",
        "-",
        "-",
        "1",
        "0.0",
        "under",
        "-",
        "-",
        "604",
        "91.4",
        "between",
        "102",
        "94.1",
        "98",
        "84.7",
        "underneath",
        "-",
        "-",
        "2",
        "50.0",
        "beyond",
        "-",
        "-",
        "45",
        "64.4",
        "until",
        "-",
        "-",
        "208",
        "94.2",
        "by",
        "248",
        "88.3",
        "1341",
        "87.5",
        "up",
        "-",
        "-",
        "20",
        "75.0",
        "down",
        "153",
        "81.7",
        "16",
        "56.2",
        "upon",
        "-",
        "-",
        "23",
        "73.9",
        "during",
        "39",
        "87.2",
        "547",
        "92.1",
        "via",
        "-",
        "-",
        "22",
        "40.9",
        "except",
        "-",
        "-",
        "1",
        "0.0",
        "whether",
        "-",
        "-",
        "1",
        "100.0",
        "for",
        "478",
        "82.4",
        "1455",
        "84.5",
        "while",
        "-",
        "-",
        "3",
        "33.3",
        "from",
        "578",
        "85.5",
        "1712",
        "90.5",
        "with",
        "578",
        "84.4",
        "272",
        "69.5",
        "in",
        "688",
        "77.0",
        "15706",
        "95.0",
        "within",
        "-",
        "-",
        "213",
        "96.2",
        "inside",
        "38",
        "73.7",
        "24",
        "91.7",
        "without",
        "-",
        "-",
        "69",
        "63.8",
        "into",
        "297",
        "86.2",
        "415",
        "80.0",
        "Overall",
        "8096",
        "84.8",
        "35917",
        "91.8",
        "Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification.",
        "Comparison to O'Hara and Wiebe (2009).",
        "Classes ordered by frequency",
        "To situate our experimental results within the body of work on PSD, we compare them to both a most-frequent-sense baseline and existing work for both granularities (see Table 6).",
        "The results use a syntactically selective context of preposition, governor, object, and word to the left as determined by combined extraction information (POS tagging and parsing).",
        "Table 6: Accuracies (%) for Different Classifications.",
        "Comparison with O'Hara and Wiebe (2009)*, and Tratz and Hovy (2009)**.",
        "Our system easily exceeds the baseline for both coarse and ine-grained PSD (see Table 6).",
        "Comparison with related work shows that we achieve an improvement of 6.5% over Tratz and Hovy (2009), which is significant at p < .0001, and of 4.5% over O'Hara and Wiebe (2009), which is signiicant at p < .0001.",
        "A detailed overview over all prepositions for frequencies and accuracies of both coarse and ine-grained PSD can be found in Table 4.",
        "In addition to overall accuracy, O'Hara and Wiebe (2009) also measure precision, recall and F-measure for the different classes.",
        "They omitted BNF because it is so infrequent.",
        "Due to different training data and models, the two systems are not strictly comparable, yet they provide a sense of the general task dificulty.",
        "See Table 5.",
        "We note that both systems perform better than the most-frequent-sense baseline.",
        "DIR is reliably classiied using the baseline, while EXT and BNF are never selected for any preposition.",
        "Our method adds considerably to the scores for most classes.",
        "The low score for BNF is mainly due to the low number of instances in the data, which is why it was excluded by O'Hara and Wiebe (2009)."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "To get maximal accuracy in disambiguating prepositions – and also other word classes – one needs to consider context, features, and granularity.",
        "We presented an evaluation of these parameters for preposition sense disambiguation (PSD).",
        "We ind that selective context is better than ixed window size.",
        "Within the context for prepositions, the governor (head of the NP or VP governing the preposition), the object of the preposition (i.e., head of the NP to the right), and the word directly to the left of the preposition have the highest inluence.",
        "This corroborates the linguistic intuition that close mutual constraints hold between the elements of the PP.",
        "Each word syntactically and semantically restricts the choice of the other elements.",
        "Combining different extraction mpthods (POS-based heuristics and dependency parsing) works better than either one in isolation, though high accuracy can be achieved just using heuristics.",
        "The impact of context and features varies somewhat for different granularities.",
        "Most Frequent Sense",
        "O'Hara/Wiebe 2009",
        "10-fold CV",
        "Class",
        "prec",
        "rec",
        "f1",
        "prec",
        "rec",
        "f1",
        "prec",
        "rec",
        "f1",
        "LOC",
        "71.8",
        "97.4",
        "82.6",
        "90.8",
        "93.2",
        "92.0",
        "94.7",
        "96.4",
        "95.6",
        "TMP",
        "77.5",
        "39.4",
        "52.3",
        "84.5",
        "85.2",
        "84.8",
        "94.6",
        "94.6",
        "94.6",
        "DIR",
        "91.6",
        "94.2",
        "92.8",
        "95.6",
        "96.5",
        "96.1",
        "94.6",
        "94.5",
        "94.5",
        "MNR",
        "69.9",
        "43.2",
        "53.4",
        "82.6",
        "55.8",
        "66.1",
        "83.3",
        "75.0",
        "78.9",
        "PRP",
        "78.2",
        "48.8",
        "60.1",
        "79.3",
        "70.1",
        "74.4",
        "90.6",
        "83.8",
        "87.1",
        "EXT",
        "0.0",
        "0.0",
        "0.0",
        "81.7",
        "84.6",
        "82.9",
        "87.5",
        "82.1",
        "84.7",
        "BNF",
        "0.0",
        "0.0",
        "0.0",
        "-",
        "-",
        "-",
        "75.0",
        "34.1",
        "46.9",
        "coarse",
        "fine",
        "Baseline",
        "75.8",
        "39.6",
        "Related Work",
        "89.3*",
        "78.3**",
        "Our system",
        "93.9",
        "84.8",
        "Not surprisingly, we see higher scores for coarser granularity than for the more ine-grained one.",
        "We measured success in accuracy, precision, recall, and F-measure, and compared our results to a most-frequent-sense baseline and existing work.",
        "We were able to improve over state-of-the-art systems in both coarse and ine-grained PSD, achieving accuracies of 91.8% and 84.8% respectively."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The authors would like to thank Steve DeNeefe, Victoria Fossum, and Zornitsa Kozareva for comments and suggestions.",
        "StephenTratz is supported by a National Defense Science and Engineering fellowship."
      ]
    }
  ]
}
