{
  "info": {
    "authors": [
      "Azniah Ismail",
      "Suresh Manandhar"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2055",
    "title": "Bilingual lexicon extraction from comparable corpora using in-domain terms",
    "url": "https://aclweb.org/anthology/C10-2055",
    "year": 2010
  },
  "references": [
    "acl-J93-1003",
    "acl-P08-1088",
    "acl-P95-1050",
    "acl-P99-1067",
    "acl-W01-0504",
    "acl-W02-0902",
    "acl-W09-1702",
    "acl-W95-0114"
  ],
  "sections": [
    {
      "text": [
        "Bilingual lexicon extraction from comparable corpora using",
        "in-domain terms",
        "Many existing methods for bilingual lexicon learning from comparable corpora are based on similarity of context vectors.",
        "These methods suffer from noisy vectors that greatly affect their accuracy.",
        "We introduce a method for filtering this noise allowing highly accurate learning of bilingual lexicons.",
        "Our method is based on the notion of in-domain terms which can be thought of as the most important contextually relevant words.",
        "We provide a method for identifying such terms.",
        "Our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary.",
        "In addition, we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In bilingual lexicon extraction, the context-based approach introduced by Rapp (1995) is widely used (Fung, 1995; Diab and Finch, 2000; among others).",
        "The focus has been on learning from comparable corpora since the late 1990s (Rapp, 1999; Koehn and Knight, 2002; among others).",
        "However, so far, the accuracy of bilingual lexicon extraction using comparable corpora is quite poor especially when orthographic features are not used.",
        "Moreover, when orthographic features are not used, a large initial seed dictionary is essential in order to acquire higher accuracy lexicon (Koehn and Knight, 2002).",
        "This means that current methods are not suitable when the language pairs are not closely related or when a large initial seed dictionary is unavailable.",
        "When learning from comparable corpora, a large initial seed dictionary does not necessarily guarantee higher accuracy since the source and target texts are poorly correlated.",
        "Thus, inducing highly accurate bilingual lexicon from comparable corpora has so far been an open problem.",
        "In this paper, we present a method that is able to improve the accuracy significantly without requiring a large initial bilingual dictionary.",
        "Our approach is based on utilising highly associated terms in the context vector of a source word.",
        "For example, the source word powers is highly associated with the context word delegation.",
        "We note that, firstly, both share context terms such as parliament and affairs.",
        "And, secondly, the translation equivalents of powers and delegation in the target language are not only highly associated but they also share context terms that are the translation equivalents of parliament and affairs (see Figure 1)."
      ]
    },
    {
      "heading": "2. Related work",
      "text": [
        "Most of the early work in bilingual lexicon extraction employ an initial seed dictionary.",
        "A large bilingual lexicon with 10k to 20k entries is necessary (Fung, 1995; Rapp, 1999).",
        "Koehn and Knight (2002) introduce techniques for constructing the initial seed dictionary automatically.",
        "Their method is based on using identical spelling features.",
        "The accuracy of such initial bilingual lexicon is almost 90.0 percent and can be increased by restricting the word length (Koehn and Knight, 2002).",
        "Koehn and Knight found approximately 1000 identical words in their German",
        "Figure 1: An example of in-domain terms that co-occur in English and Spanish.",
        "The source word is powers and the target word is poderes.",
        "The word delegation and delegacion are the highly associated words with the source word and the target word respectively.",
        "Their in-domain terms, as shown in the middle, can be used to map the source word in context of word delegation to its corresponding target word in context of delegacion.",
        "and English monolingual corpora.",
        "They expanded the lexicon with the standard context-based approach and achieved about 25.0 percent accuracy (Koehn and Knight, 2002).",
        "Similar techniques were used in Haghighi et al.",
        "(2008) who employ dimension reduction in the extraction method.",
        "They recorded 58.0 percent as their best F\\ score for the context vector approach on non-parallel comparable corpora containing Wikipedia articles.",
        "However, their method scores less on comparable corpora containing distinct sentences derived from the Eu-roparl English-Spanish corpus."
      ]
    },
    {
      "heading": "3. Learning in-domain terms",
      "text": [
        "In the standard context vector approach, we associate each source word and target word with their context vectors.",
        "The source and target context vectors are then compared using the initial seed dictionary and a similarity measure.",
        "Learning from comparable corpora is particularly problematic due to data sparsity, as important context terms may not occur in the training corpora while some may occur but with low frequency and can be missed.",
        "Some limitations may also be due to the size of the initial seed dictionary being small.",
        "The initial seed dictionary can also contribute irrelevant or less relevant features that can mislead the similarity measure especially when the number of dimensions is large.",
        "The approach we adopt attempts to overcome this problem.",
        "In Figure 1, for the source word powers, delegation is the highly associated word.",
        "Both powers and delegation share common contextual terms such as parliament and affairs.",
        "Now the translation equivalent of delegation is delegacion.",
        "For the potential translation equivalent poderes, we see that the common contextual terms shared by powers and poderes are terms parlamento (parliament) and asuntos (affairs).",
        "Figure 2: An example of English-Spanish lexicon learnt for the source word powers.",
        "On the top, the system suggested competencias and rejected poderes when powers is associated with community, democracy or independence.",
        "The word poderes is suggested when powers is associated with justice or delegation.",
        "We observe that these common contextual terms are simultaneously the first-order and second-order context terms of the target word.",
        "They are the shared context terms of the target word and its highly associated context term.",
        "We define these terms as in-domain terms.",
        "These indomain terms can be used to map words to their corresponding translations.",
        "The highly associated context terms can be thought of as sense discriminators that differentiate the different uses of the target word.",
        "In Figure 2, we show how delegation helps in selecting between the \"control or influence\" sense of powers while rejecting the \"ability or skill\" sense.",
        "In this paper, our focus is not on sense disambiguation and we follow current evaluation methods for bilingual lexicon extraction.",
        "However, it is clear that our method can be adapted for building sense disambiguated bilingual lexicons.",
        "To identify the context terms CT(WS) of a source word WS, as in (Rapp, 1999), we use log-likelihood ratio (LL) Dunning (1993).",
        "We choose all words with LL > t\\ where t\\ is a threshold.",
        "The highly associated words then are the top k highest ranked context terms.",
        "In our experiments, we only choose the top 100 highest ranked context terms as our highly associated terms.",
        "In order to compute the log-likelihood ratio of target word a to co-occur with context word b, we create a contingency table.",
        "The contingency table contains the observed values taken from a given corpus.",
        "An example of the contingency table is shown in Table 1.",
        "Here C [i, j] denotes the count of the number of sentences in which i co-occurs with j.",
        "The LL value of a target word a and context word b is given by:",
        "In our work, to find the translation equivalent of a source word WS, we do not use the context terms CT (WS ).",
        "Instead, we use the in-domain terms IDT(WS, WR).",
        "For each highly associated term Wr, we get different in-domain terms.",
        "Furthermore, IDT(Ws, Wr) is a subset of CT(Ws).",
        "C[i,j]",
        "community",
        " – community",
        "powers",
        "124",
        "1831",
        "1955",
        "C(powers)",
        " – powers",
        "11779",
        "460218",
        "471997",
        "C( – powers)",
        "11903",
        "462049",
        "C(community)",
        "C( – community)",
        "The in-domain terms of WS given the context terms WR is given by:",
        "Programme and public are some of the examples of in-domain terms of powers given community as the highly associated term.",
        "Note that ID(WS, Wr) is an in-domain term vector in the source language.",
        "Let WT be a potential translation equivalent for WS.",
        "Let, tr(WR) be a translation equivalent for Wr.",
        "Let ID(Wt, tr(WR)) be an in-domain term vector in the target language.",
        "We use tr(WS|WR) to denote the translation proposed for WS given the highly associated term Wr.",
        "We compute tr(WS| Wr) using:",
        "Our method learns translation pairs that are conditioned on highly associated words (Wr).",
        "Table 2 provides a sample of English-Spanish lexicon learnt for the word power with different WR.",
        "for powers.",
        "In the next section, we introduce a similarity measure that operates on the context vectors in the source language and the target language without requiring a seed dictionary."
      ]
    },
    {
      "heading": "4. Rank-binning similarity measure",
      "text": [
        "Most existing methods for computing similarity cannot be directly employed for measuring the similarity between in-domain term context vectors since each context vector is in a different language.",
        "A bilingual dictionary can be assumed but that greatly diminishes the practicality of the method.",
        "We address this by making an assumption.",
        "We assume that the relative distributions of in-domain context terms of translation equivalent pairs are roughly comparable in the source language and in the target language.",
        "For example, consider the log-likelihood values of the in-domain terms for the translation pair agreement-acuerdo (conditioned on the highly associated term association-associacion) given in Figure 3.",
        "We note that the distribution of in-domain terms are comparable although not identical.",
        "Thus, the distribution can be used as a clue to derive translation pairs but we need a method to compute similarity of the vector of in-domain terms.",
        "Rank-binning or rank histograms are usually used as a diagnostic tool to evaluate the spread of an ensemble rather than as a verification method.",
        "Wong (2009) use the method of rank-binning to roughly examine performance of a system on learning lightweight ontologies.",
        "We apply the rank-binning procedure for measuring the similarity of word pairs.",
        "Pre-processing step:",
        "1.",
        "Let WS be a source language word and x\\, x2, ...,xn be the set of n context terms ranked in descending log-likelihood values of WS (see Table 3).",
        "2.",
        "We transform the rank values of context terms xk into the range [0,1] using:",
        "English",
        "Spanish",
        "Sim",
        "Ws",
        "Wr",
        "tr(WR)",
        "Wt",
        "powers",
        "community",
        "comunidad",
        "competencias",
        "poderes independiente",
        "0.9876",
        "0.9744 0.9501",
        "democracy",
        "democracia",
        "competencias",
        "poderes independiente",
        "0.9948",
        "0.9915 0.9483",
        "independence",
        "independencia",
        "competencias",
        "poderes independiente",
        "0.9939",
        "0.9745 0.9633",
        "justice",
        "justicia",
        "poderes",
        "competencias independiente",
        "0.9922",
        "0.3450 0.9296",
        "delegation",
        "delegacion",
        "poderes",
        "competencias independiente",
        "0.9568",
        "0.9266 0.8408",
        "□ in-domain terms in the",
        "source language ■ in-domain terms in the target language_",
        "/ / / S .",
        "Figure 3: Similar distribution of in-domain terms for agreement with association and acuerdo with asociacion.",
        "Binning procedure",
        "We divide the interval [0,1] into g bins of equal length.",
        "Let bi,..., bg denote the g bins.",
        "Then we map the in-domain terms vector ID(Ws, Wr) into the binned vector bi,..., bg.",
        "For each xk g ID(Ws, Wr), this mapping is done by using the corresponding zk from the preprocessing step.",
        "For each bin, we count the number of different indomain terms that are mapped into this bin.",
        "Thus, if the range of the first bin bi is [0, 0.009] then european legislative parliament are mapped into bi i.e. bi = 3.",
        "The bins are normalised by dividing with | ID(Ws,Wr) |.",
        "Rank binning similarity",
        "We use Euclidean distance to compute similarity between bins.",
        "Given, bins P = pi,... ,pg and Q = qi,... ,Qg, the Euclidean distance is given",
        "In the next section, we describe the setup including the data, the lexicon and the evaluation used in our experiments."
      ]
    },
    {
      "heading": "5. Experimental setup",
      "text": [
        "For comparable text, we derive English and Spanish distinct sentences from the Europarl parallel corpora.",
        "We split the corpora into three parts according to year.",
        "We used about 500k sentences for each language in the experiments.",
        "This approach is further explained in Ismail and Man-andhar (2009) and is similar to Koehn and Knight (2001) and Haghighi et al.",
        "(2008).",
        "For corpus pre-processing, we use sentence boundary detection and tokenization on the raw text before we clean the tags and filter stop words.",
        "We sort and rank words in the text according to their frequencies.",
        "For each of these words, we compute their context term log-likelihood values.",
        "In the experiment, a bilingual lexicon is required for evaluation.",
        "We extract our evaluation lexicon from the Word Reference free online dictionary.",
        "This extracted bilingual lexicon has low coverage.",
        "CT (powers)",
        "Context term",
        "LL",
        "rank",
        "european",
        "491.33",
        "1",
        "0.00000",
        "legislative",
        "482.19",
        "2",
        "0.00406",
        "parliament",
        "408.26",
        "3",
        "0.00813",
        "public",
        "16.96",
        "245",
        "0.99186",
        "programme",
        "15.40",
        "246",
        "0.99593",
        "representatives",
        "15.32",
        "247",
        "1.00000",
        "n = 247",
        "In the experiments, we considered the task of building a bilingual English-Spanish lexicon between the 2000 high frequency source and target words, where we required each individual word to have at least a hundred highly associated context terms that are not part of the initial seed dictionary.",
        "Different highly associated WR terms for a given WT might derive similar (WS, WT) pairs.",
        "In this case, we only considered one of the (Ws, WT) pairs.",
        "In future work, we would like to keep these for word sense discrimination purposes.",
        "Note that we only considered proposed translation pairs whose similarity values are above a threshold t2.",
        "We used the Fi measure to evaluate the proposed lexicon against the evaluation lexicon.",
        "If either WS or WT in the proposed translation pairs is not in the evaluation lexicon, we considered the translation pairs as unknown, although the proposed translation pairs are correct.",
        "Recall is defined as the proportion of the proposed lexicon divided by the size of the lexicon and precision is given by the number of correct translation pairs at a certain recall value."
      ]
    },
    {
      "heading": "6. Experiments",
      "text": [
        "In this section, we look into how the in-domain context vectors affect system performance.",
        "We also examine the potential of rank-binning similarity measure.",
        "Most research in bilingual lexicon extraction so far has employed the standard context vector approach.",
        "In order to explore the potential of the in-domain context vectors, we compare the systems that use in-domain approach against systems that use the standard approach.",
        "We also employ different sets of seed lexicon in each system to be used in the similarity measure:",
        "• Lex7oo : contains 700 cognate pairs from a few Learning Spanish Cognate websites.",
        "• Lexi00: contains 100 bilingual entries of the most frequent words in the source corpus that have translation equivalents in the extracted evaluation lexicon.",
        "We select the top one hundred words in the source corpus, so that their translation equivalents is within the first 2000 high frequency words in the target corpus.",
        "• Lexi60: contains words with similar spelling that occur in both corpora.",
        "We used 160 word pairs with an edit distance value less than 2, where each word is longer than 4 characters.",
        "Models using the standard approach are denoted according to the size of the particular lexicon used in their context similarity measure, i.e. CV-100 for using Lexi00, CV-160 for using Lexi60 and CV-700 for using Lex700.",
        "We use IDT to denote our model.",
        "We use lexicon sizes to distinguish the different variants, e.g. IDT-CV100 for using Lexi00, IDT-CV160 for using Lex^o and IDT-CV700 for using Lex700.",
        "With CV-700, the system achieved 52.6 percent of the best Fi score.",
        "Using the same seed dictionary, the best Fi score has increased about 20 percent points with IDT-CV700 recorded 73.1 percent.",
        "IDT-CV100 recorded about 15.0 percent higher best Fi score than CV-100 with 80.9 and 66.4 percent respectively.",
        "Using an automatically derived seed dictionary, IDT-CV160 yielded 70.0 percent of best Fi score while CV-160 achieved 62.4 percent.",
        "Results in Table 4 shows various precisions px at recall values x.",
        "Model",
        "Pü.iü",
        "Pü.25",
        "Pü.33",
        "Pü.5ü",
        "BestFiscore",
        "CV-700",
        "58.3",
        "61.2",
        "64.8",
        "55.2",
        "52.6",
        "CV-100",
        "52.0",
        "53.0",
        "47.2",
        "44.8",
        "66.4",
        "CV-160",
        "68.5",
        "56.8",
        "48.8",
        "48.8",
        "62.4",
        "IDT-CV700",
        "83.3",
        "90.2",
        "82.0",
        "66.7",
        "73.1",
        "IDT-CV100",
        "80.0",
        "75.8",
        "66.7",
        "69.4",
        "80.9",
        "IDT-CV160",
        "90.0",
        "80.6",
        "73.9",
        "69.2",
        "70.0",
        "We use RB to denote our model based on the rank-binning approach.",
        "Running RB means that no seed dictionary is involved in the similarity measure.",
        "We also ran the similarity measure in the IDT (IDT-RB160) by employing the derived Lexi60 for the in-domain steps.",
        "We ran several tests using IDT-RB160with different numbers of bins.",
        "The results are illustrated in Figure 4.",
        "The IDT-RB160 yielded 63.7 percent of best Fi score with 4 bins.",
        "However, the Fi score starts to dropfrom61.1to 53.0percent with 6 and 8 bins respectively.",
        "With 3 and 2 bins the",
        "Fi score respectively.",
        "Using 1 bin is not be possible as all values fall under one bin.",
        "Thus, the rank-binning similarity measure for the rest of the experiments where RB is mentioned, refers to a 4 bins setting.",
        "No.",
        "of bins",
        "While systems using the standard context simi-laritymeasureyieldedscores higherthan50.0per-cent of best Fi , the RB achieved only 39.2 percent.",
        "However, RB does not employ an initial dictionary and does not use orthographic features.",
        "As mentioned above, the system scored higher when the similarity measure was used in the IDT (i.e. IDT-RB160).",
        "Note that Lexi60 is derived automatically so the approach can also be considered as unsupervised.",
        "The system performance is slightly lower compared to the conventional CV-160.",
        "However, IDT-CV160 outperforms both of the systems (see Figure 5).",
        "Overall, systems that exploit in-domain terms yielded higher Fi scores compared to the conventional context vector approach.",
        "Previous work in extracting bilingual lexicons from comparable corpora generally employ the conventional context vector approach.",
        "Haghighi et al.",
        "(2008) focused on applying canonical correlation analysis (CCA), a dimension reduction technique, to improve the method.",
        "They were using smaller comparable corpora, taken from the first 50k sentences of English Europarl and the second 50kofSpanishEuroparl, and differentini-tial seed dictionary.",
        "Hence, we tested CCA in our experimental setup.",
        "In CV-700 setting, using CCA yields 57.5 percent of the best Fi score compared to 73.1 percent ofthe best Fi score with IDT that we reported in Section 6.2."
      ]
    },
    {
      "heading": "7. Discussion",
      "text": [
        "Our experiments clearly demonstrate that the use of in-domain terms achieves higher Fi scores compared to conventional methods.",
        "It also shows that our method improves upon earlier reported dimension reduction methods.",
        "From our observation, the number of incorrect translation pairs were further reduced when the context terms were filtered.",
        "Recall that the in-domain terms in the target language were actually the shared context terms of the target word and its highly associated context terms.",
        "Nevertheless, this approach actually depends on the initial bilingual lexicon in order to translate those highly associated context terms into the source language.",
        "Table 5 shows some examples of most confidence translation pairs proposed by the IDT-CV100.",
        "Table 5: Some examples of most confident translation pairs proposed by IDT-CV100 ranked by similarity scores.",
        "The initial seed dictionary plays a major role in extracting bilingual lexicon from comparable corpora.",
        "There are a few different ways for us to derive a seed dictionary.",
        "Recall that Lex700 and Lexi00, that are used in the experiments, are derived using different methods.",
        "The Fi scores of the system using Lexi00 were much higher compared to the system using Lex700.",
        "Thus, extending Lexi00 with additional high frequency words may provide higher accuracy.",
        "One important reason is that all bilingual entries in Lexi00 occur frequently in the corpora.",
        "Although the size of Lex700 is larger, it is not surprising that most of the words never occur in the corpora, such as volleyball and romantic.",
        "However, using Lex^o is more interesting since it is derived automatically from the corpora, though one should realize that the relationship between the language pair used in the respective monolingual corpora, English and Spanish, may have largely affect the results.",
        "Thus, for other systems involving unrelated language pairs, the rank-binning similarity measure might be a good option.",
        "As mentioned in Section 5.4, each source word may have more than one highly associated context term, Wr.",
        "Different WR may suggest different target words for the same source word.",
        "For example, given the source word powers and the highly associated word community, competencias is proposed as the best translation equivalent.",
        "On the other hand, for same source word powers, when the highly associated word is delegation, the target word poderes is suggested."
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "We have developed a method to improve the Fi score in extracting bilingual lexicon from comparable corpora by exploiting in-domain terms.",
        "This method also performs well without using an initial seed dictionary.",
        "More interestingly, our work reveals the potential of building word sense disambiguated lexicons.",
        "English",
        "Spanish",
        "Sim score",
        "Correct?",
        "principle",
        "principio",
        "0.9999",
        "Yes",
        "government",
        "estado",
        "0.9999",
        "No",
        "government",
        "gobierno",
        "0.9999",
        "Yes",
        "resources",
        "recursos",
        "0.9999",
        "Yes",
        "difficult",
        "dificil",
        "0.9999",
        "Yes",
        "sector",
        "competencia",
        "0.9998",
        "No",
        "sector",
        "sector",
        "0.9998",
        "Yes",
        "programme",
        "programa",
        "0.9998",
        "Yes",
        "programme",
        "comunidad",
        "0.9998",
        "No",
        "agreement",
        "acuerdo",
        "0.9998",
        "Yes"
      ]
    }
  ]
}
