{
  "info": {
    "authors": [
      "Joohyun Kim",
      "Raymond J. Mooney"
    ],
    "book": "COLING â€“ POSTERS",
    "id": "acl-C10-2062",
    "title": "Generative Alignment and Semantic Parsing for Learning from Ambiguous Supervision",
    "url": "https://aclweb.org/anthology/C10-2062",
    "year": 2010
  },
  "references": [
    "acl-D07-1071",
    "acl-D08-1082",
    "acl-D09-1042",
    "acl-N07-1022",
    "acl-P02-1040",
    "acl-P06-1115",
    "acl-P07-1121",
    "acl-P09-1011",
    "acl-W05-0602"
  ],
  "sections": [
    {
      "text": [
        "We present a probabilistic generative model for learning semantic parsers from ambiguous supervision.",
        "Our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations.",
        "It disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form.",
        "Compared to a previous generative model for semantic alignment, it also supports full semantic parsing.",
        "Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Most approaches to learning semantic parsers that map sentences into complete logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007b; Lu et al., 2008) require fully-supervised corpora that provide full formal logical representations for each sentence.",
        "Such corpora are expensive and difficult to construct.",
        "Several recent projects on \"grounded\" language learning (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Liang et al., 2009) exploit more easily and naturally available training data consisting of sentences paired with world states consisting of multiple potential semantic representations.",
        "This setting is partially motivated by a desire to model how children naturally learn language in the context of a rich, ambiguous perceptual environment.",
        "In particular, Chen and Mooney (2008) introduced the problem of learning to sportscast by simply observing natural language commentary on simulated Robocup robot soccer games.",
        "The training data consists of natural language (NL) sentences ambiguously paired with logical meaning representations (MRs) describing recent events in the game extracted from the simulator.",
        "Most sentences describe one of the extracted recent events; however, the specific event to which it refers is unknown.",
        "Therefore, the learner has to figure out the correct matching (alignment) between NL and MR before inducing a semantic parser or language generator.",
        "Based on an approach introduced by Kate and Mooney (2007), Chen and Mooney (2008) repeatedly retrain both a supervised semantic parser and language generator using an iterative algorithm analogous to Expectation Maximization (EM).",
        "However, this approach is somewhat ad hoc and does not exploit a well-defined probabilistic generative model or real EM training.",
        "On the other hand, Liang et al.",
        "(2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states.",
        "Compared to Chen and Mooney (2008), they demonstrated improved alignment results on Robocup sportscasting data.",
        "However, their model only produces an NL-MR alignment and does not learn either an effective semantic parser or language generator.",
        "In addition, they use a combination of a simple Markov model and a bag-of-words model when generating natural language for MRs, therefore, they do not model context-free linguistic syntax.",
        "Motivated by the limitations of these previous methods, we propose a new generative alignment model that includes a full semantic parsing model proposed by Lu et al.",
        "(2008).",
        "Our approach is capable of disambiguating the mapping between language and meanings while also learning a complete semantic parser for mapping sentences to logical form.",
        "Experimental results on Robocup sportscasting show that our approach outperforms all previous results on the NL-MR matching (alignment) task and also produces competitive performance on semantic parsing and improved language generation."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "The conventional approach to learning semantic parsers (Zelle and Mooney, 1996; Ge and Mooney, 2005; Kate and Mooney, 2006; Zettle-moyer and Collins, 2007; Zettlemoyer and al., 2008) requires detailed supervision unambiguously pairing each sentence with its logical form.",
        "However, developing training corpora for these methods requires expensive expert human labor.",
        "Chen and Mooney (2008) presented methods for grounded language learning from ambiguous supervision that address three related tasks: NL-MR alignment, semantic parsing, and natural language generation.",
        "They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, Wasp (Wong and Mooney, 2007b) or Krisp (Kate and Mooney, 2006), or an existing supervised natural-language generator, Wasp-1(Wong and Mooney, 2007a).",
        "During each iteration, the currently trained parser (generator) is used to produce an improved NL-MR alignment that is used to retrain the parser (generator) in the next iteration.",
        "However, this approach does not use the power of a probabilistic correspondence between an NL and MRs during training.",
        "On the other hand, Liang et al.",
        "(2009) proposed a probabilistic generative approach to produce a Viterbi alignment between NL and MRs.",
        "They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts.",
        "They report improved matching accuracy in the Robocup sportscasting domain.",
        "However, they only addressed the alignment problem and are unable to parse new sentences into meaning representations or generate natural language from logical forms.",
        "In addition, the model uses a weak bag-of-words assumption when estimating links between NL segments and MR facts.",
        "Although it does use a simple Markov model to order the generation of the different fields of an MR record, it does not utilize the full syntax of the NL or MR or their relationship.",
        "Chen et al.",
        "(2010) recently reported results on utilizing the improved alignment produced by Liang et al.",
        "(2009)'s model to initialize their own iterative retraining method.",
        "By combining the approaches, they produced more accurate NL-MR alignments and improved semantic parsers.",
        "Motivated by this prior research, our approach combines the generative alignment model of Liang et al.",
        "(2009) with the generative semantic parsing model of Lu et al.",
        "(2008) in order to fully exploit the NL syntax and its relationship to the MR semantics.",
        "Therefore, unlike Liang et al.",
        "'s simple Markov + bag-of-words model for generating language, it uses a tree-based model to generate grammatical NL from structured MR facts."
      ]
    },
    {
      "heading": "3. Background",
      "text": [
        "This section describes existing models and algorithms employed in the current research.",
        "Our model is built on top of the generative semantic parsing model developed by Lu et al.",
        "(2008).",
        "After learning a probabilistic alignment and parsing model, we also used the Wasp and Wasp-1 systems to produce additional parsing and generation results.",
        "In particular, since our current system is incapable of effectively generating NL sentences from MR logical forms, in order to demonstrate how our matching results can aid NL generation, we use Wasp-1 to learn a generator.",
        "This follows the experimental scheme of Chen et al.",
        "(2010), which demonstrated that an improved NL-MR",
        "S : pass (Player, Player)",
        "Player passes the ball to Player",
        "Player : pinklO Player : pinkll",
        "pinklO pinkll",
        "Figure 1: Sample hybrid tree from English sportscasting dataset where (w, m) = (pink10 passes the ball to pink11, pass(pink10, pinkll)) matching from Liang et al.",
        "(2009) results in better overall parsing and generation.",
        "Finally, our overall generative model uses the IGSL (Iterative Generation Strategy Learning) method of Chen and Mooney (2008) to initially estimate the prior probability of each event-type generating a natural-language comment.",
        "Lu et al.",
        "(2008) introduced a generative semantic parsing model using a hybrid-tree framework.",
        "A hybrid tree is defined over a pair, (w, m), of a natural-language sentence and its logical meaning representation.",
        "The tree expresses a correspondence between word segments in the NL and the grammatical structure of the MR.",
        "In a hybrid tree, MR production rules constitute the internal nodes, while NL words (or phrases) constitute the leaves.",
        "A sample hybrid tree from the English Robocup data is given in Figure 1.",
        "A generative model based on hybrid trees is defined as follows: starting from a root semantic category, the model generates a production of the MR grammar, and then subsequently generates a mixed hybrid pattern of NL words and child semantic categories.",
        "This process is repeated until all leaves in the hybrid tree are NL words (or phrases).",
        "Each generation step is only dependent on the parent step, thus, generation is assumed to be a Markov process.",
        "Lu et al.",
        "(2008)'s generative parsing model estimates the joint probability P(T, w, m), which represents the probability of generating a hybrid tree T with NL w, and MR m. This probability is computed as the product of the probabilities of the steps in the generative process.",
        "Since there are multiple ways to construct a hybrid tree given a pair of NL and MR, the data likelihood of the pair ( w, m) given by the learned model is calculated by summing P(T, w, m) over all the possible hybrid trees for NL w and MR m.",
        "The model is normally trained in a fully supervised setting using NL-MR pairs.",
        "In order to learn from ambiguous supervision, we extend this model to include an additional generative process for selecting the subset of available MRs used to generate NL sentences.",
        "Wasp (Word-Alignment-based Semantic Parsing) is a semantic parsing system that uses syntax-based statistical machine translation techniques.",
        "It induces a probabilistic synchronous context-free grammar (PSCFG) for generating corresponding NL-MR pairs.",
        "Since a PSCFG is symmetric with respect to the two languages it generates, the same learned model can be used for both semantic parsing (mapping NL to MR) and natural language generation (mapping MR to NL), Since there is no prespecified formal grammar for the NL, the Wasp-1 system learns an n-gram language model for the NL side and uses it to choose the most probable NL translation for a given MR using a noisy-channel model.",
        "Chen and Mooney (2008) introduced the IGSL method for determining which event types a human commentator is more likely to describe in natural language.",
        "This is sometimes called strategic generation or content selection, the process of choosing what to say; as opposed to tactical generation, which determines how to say it.",
        "IGSL uses a method analogous to EM to train on ambiguously supervised data and iteratively improve probability estimates for each event type, specifying how likely each MR predicate is to elicit a comment.",
        "The algorithm alternates between two processes: calculating the expected probability of an NL-MR matching based on the currently learned estimates, and updating the probability of each event type based on the expected match counts.",
        "IGSL was shown to be quite effective at predicting which events in a Robocup game a human would comment upon.",
        "In our proposed model, we use IGSL probability scores as initial priors for our event selection model."
      ]
    },
    {
      "heading": "4. Evaluation Dataset",
      "text": [
        "In our experiments, we use the Robocup sportscasting data produced by Chen et al.",
        "(2010), which includes both English and Korean commentaries.",
        "The data was collected by having both English and Korean speakers commentate the final games from the RoboCup simulation soccer league for each year from 2001 through 2004.",
        "Table 1 presents some statistics on this sportscasting data.",
        "To construct the ambiguous training data, each NL commentary sentence is paired with MRs for all extracted simulation events that occurred in the previous 5 seconds (an average of 2.5 events).",
        "Figure 2 shows a sample trace from the Robocup English data.",
        "Each NL commentary sentence normally has several possible MR matches that occurred within the 5-second window, indicated by edges between the NL and MR.",
        "Bold edges represent gold standard matches constructed solely for evaluation purposes.",
        "Note that not every NL has a gold matching MR.",
        "This occurs because the sentence refers to unrecognized or undetected events or situations or because the matching MR lies outside the 5-second window."
      ]
    },
    {
      "heading": "5. Generative Model",
      "text": [
        "Like Liang et al.",
        "(2009)'s generative alignment model, our model is designed to estimate P(w|s), where w is an NL sentence and s is a world state containing a set of possible MR logical forms that can be matched to w. However, our approach is intended to support both determining the most likely match between an NL and its MR in its world state, and semantic parsing, i.e. finding the",
        "Natural Language",
        "Meaning Representation",
        "most probable mapping from a given NL sentence to an MR logical form.",
        "Our generative model consists of two stages:",
        "â€¢ Event selection: P(e|s), chooses the event e in the world state s to be described.",
        "â€¢ Natural language generation: P(w|e), models the probability of generating natural-language sentence w from the MR specified by event e.",
        "The event selection model specifies the probability distribution for picking an event that is likely to be commented upon amongst the multiple MR logical forms in the world state s. The probability of picking an event is assumed to depend only on its event type as given by the predicate of its MR. For example, the MR pass(pinkl0, pinkll) has event type pass and arguments pinklO and pinkll.",
        "Our model is similar to Liang et al.",
        "(2009)'s record choice model, but we only model their notion of salience, denoting that some event types are more likely to be described than others.",
        "We do not model their notion of coherence, which models the order of event types in the commentary.",
        "We found that for sportscasting the order of described events depends only on the sequence of events in the game and does not exhibit any additional detectable pattern due to linguistic preferences.",
        "The probability of picking an event e of type teis denoted by p(te).",
        "If there are multiple events of type t in a world state s, then an event of type t is selected uniformly from the set s(t) of events of type t in state s. Therefore, the probability of picking an event is given by:",
        "English",
        "Korean",
        "# of NL comments",
        "# of extracted MR events",
        "2036 10452",
        "1999 10668",
        "# of NLs w/ matching MRs",
        "# of MRs w/ matching NLs",
        "1868 4670",
        "1913 4610",
        "Avg.",
        "# of MRs per NL",
        "2.50",
        "2.41",
        "The natural-language generation model defines the probability distribution of NL sentences given an MR specified by the previously selected event.",
        "We use Lu et al.",
        "(2008)'s generative model for this step, in which:",
        "where m is the MR logical form defined by event e and T is a hybrid tree defined over the NL-MR pair (w, m).",
        "The probability P(T, w| m) is calculated using the generative semantic parsing model of Lu et al.",
        "(2008) using the joint probability of the NL-MR pair (w, m), i.e. the inside probability of generating ( w, m).",
        "The likelihood of a sentence w is then the sum over all possible hybrid trees defined by the NL-MR pair (w, m).",
        "The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al.",
        "(2009).",
        "Since our event selection model only chooses an event based on its type, the order of its arguments still needs to be addressed.",
        "However, Lu et al.",
        "'s generative model includes ordering the MR arguments (as specified by MR production rules) as well as the generation of NL words and phrases to express these arguments.",
        "Thus, it is unnecessary to separately model argument ordering in our approach."
      ]
    },
    {
      "heading": "6. Learning and Inference",
      "text": [
        "This composite generative model is trained using conventional EM methods.",
        "The process is similar to Lu et al.",
        "(2008)'s, an inside-outside style algorithm using dynamic programming to generate a hybrid tree from the NL-MR pair (w, m), except our model's estimation process additionally deals with calculating expected counts under the posterior P(e| w, s; 0) in the E-step and normalizing the counts to optimize parameters.",
        "The whole process is quite efficient; training time takes about 30 minutes to run on sportscasts of three games in either English or Korean.",
        "Unfortunately, we found that EM tended to get stuck at local maxima with respect to learning the event-type selection probabilities, p(t).",
        "Therefore, we also tried initializing these parameters with the corresponding strategic generation values learned by the IGSL method of Chen and Mooney (2008).",
        "Since IGSL was shown to be quite effective at predicting which event types were likely to be described, the use of IGSL priors provides a good starting point for our event selection model.",
        "Our model is built on top of Lu et al.",
        "(2008)'s generative semantic parsing model, which is also trained in several steps in its best-performing version.",
        "Thus, the overall model is vulnerable to getting stuck in local optima when running EM across these multiple steps.",
        "We also tried using random restarts with different initialization of parameters, but initializing with IGSL priors performed the best in our experimental evaluation."
      ]
    },
    {
      "heading": "7. Experimental Evaluation",
      "text": [
        "We evaluated our proposed model on the Robocup sportscasting data described in Section 4.",
        "Our experimental results cover 3 tasks: NL-MR matching, semantic parsing, and tactical generation.",
        "Following Chen and Mooney (2008), the experiments were conducted using 4-fold (leave one game out) cross validation.",
        "Since the corpus contains data for four separate games, each fold uses 3 games for training and the remaining game for testing for semantic parsing and tactical generation.",
        "Matching performance is measured in training data, since the goal is to disambiguate this data.",
        "All results are averaged across these 4 folds.",
        "We also use the same performance metrics as Chen and Mooney (2008).",
        "The accuracy of matching and semantic parsing are measured using F-measure, the harmonic mean of precision and recall, where precision is the fraction of the system's annotations that are correct, and recall is the fraction of the annotations from the goldstandard that the system correctly produces.",
        "Generation is evaluated using BLEU score (Papineni et al., 2002) between generated sentences and reference NL sentences in the test set.",
        "We compare our results to previous results from Chen and matching results on Robocup data from Liang et al.",
        "(2009).",
        "The goal of matching is to find the most probable NL-MR alignment for ambiguous examples consisting of an NL sentence and multiple potential MR logical forms.",
        "In Robocup sportscasting, the MRs for a given sentence correspond to all game events that occur within a 5-second window prior to the NL comment.",
        "Not all NL sentences have a matching MR in this window, but most do.",
        "During testing, an NL w is matched to an MR m if and only if the learned semantic parser produces m as the most probable parse of w. Thus, our model does not force every NL to match an MR.",
        "If the most probable semantic parse of a sentence does not match any of the possible recent events, it is simply left unmatched.",
        "Matching is evaluated against the gold-standard matches supplied with the data, which are used for evaluation purposes only.",
        "The gold matching data is never used during training.",
        "Table 2 shows the detailed results for both English and Korean data.",
        "Our best approach outperforms all previous methods for both English and Korean by quite large margins.",
        "Note that initializing our EM training with IGSL's estimates improves performance significantly, and this approach outperforms Chen et al.",
        "(2010)'s best method, which also uses IGSL.",
        "In particular, our proposed model outperforms the generative alignment model of Liang et al.",
        "(2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al.",
        "(2008)'s generative language model make our overall model more effective than a simple Markov + bag-of-words model for language generation.",
        "Semantic parsing is evaluated by determining how accurately NL sentences in the test set are correctly mapped to their meaning representations.",
        "Results are presented in Table 3.",
        "For our model, we report results using the parser learned directly from the ambiguous supervision, as well as results for training a supervised parser (both Wasp and Lu et al.",
        "(2009)'s) on the NL-MR matching produced by our model.",
        "We also present results for training Lu et al.",
        "'s parser and Wasp on Liang et al.",
        "'s NL-MR matchings.",
        "English",
        "Korean",
        "Chen and Mooney (2008)",
        "0.681",
        "0.753",
        "Liang et al.",
        "(2009)",
        "0.757",
        "0.694",
        "Chen et al.",
        "(2010)",
        "0.793",
        "0.841",
        "Our model",
        "0.832",
        "0.800",
        "Our model w/ IGSL init",
        "0.885",
        "0.895",
        "English",
        "Korean",
        "Chen and Mooney (2008)",
        "0.702",
        "0.720",
        "Chen etal.",
        "(2010)",
        "0.803",
        "0.812",
        "Our learned parser",
        "0.742",
        "0.764",
        "Lu et al.",
        "+ our matching",
        "0.810",
        "0.794",
        "Wasp + our matching",
        "0.786",
        "0.808",
        "Lu et al.",
        "+ Liang et al.",
        "0.790",
        "0.690",
        "Wasp + Liang et al.",
        "0.803",
        "0.740",
        "Our initial learned semantic parser does not perform better than the best results reported by Chen et al.",
        "(2010), but it is clearly better than the initial results of Chen and Mooney (2008).",
        "Training Wasp and Lu et al.",
        "'s supervised parser on our method's highly accurate set of disambiguated NL-MR pairs improved the results.",
        "Retraining Lu et al.",
        "'s parser gave the best overall results for English, and retraining Wasp gave the second highest results for Korean, only failing to beat the very best results of Chen et al.",
        "(2010).",
        "It is somewhat surprising that simply retraining on the hardened set of most probable NL-MR matches gives better results than the parser trained using EM, which actually exploits the uncertainty in the underlying matches.",
        "Further investigations of this phenomenon are indicated.",
        "Comparing with the corresponding results for training Wasp and Lu et al.",
        "'s supervised parser on the NL-MR matchings produced by Liang et al.",
        "'s alignment method, it is clear that our matchings produce more accurate semantic parsers except when training Wasp on English.",
        "Tactical generation is evaluated based on how well the learned model generates accurate NL sentences from MR logical forms.",
        "Without integrating a language model for the NL, the existing generative model is not very effective for tactical generation.",
        "Lu et al.",
        "(2009) introduced an effective language generator for the hybrid tree framework using a Tree-CRF model; however, we did not have access to this system.",
        "Therefore, for tactical generation, we used the publicly available Wasp-1 system (Wong and Mooney, 2007a) trained on disambiguated NL-MR matches.",
        "This approach also allows direct comparison with the results of Chen and Mooney (2008) and Chen et al.",
        "(2010), who also used Wasp-1 for tactical generation.",
        "Our objective is to show that the more accurate matchings produced by our generative model can improve tactical generation.",
        "Table 4: Tactical Generation Results (BLEU score).",
        "Results are the highest reported in the cited work.",
        "The results are shown in Table 4.",
        "Overall, Wasp-1 trained on the NL-MR matching from our alignment model performs better than all previous methods.",
        "In particular, using the matchings from our method to train Wasp-1 produces better tactical generators than using matchings from Liang et al.",
        "'s approach.",
        "Overall, our model performs particularly well at matching NL and MRs under ambiguous supervision, and the difference is larger for English than Korean.",
        "However, improved matching results do not necessarily translate into significantly better semantic parsers.",
        "For English, the improvement in matching is almost 10 percentage points in F-measure, but the semantic parsing result trained with this more accurate matching shows only 1 point improvement.",
        "Compared to Liang et al.",
        "(2009), our more accurate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation.",
        "The only exception is English parsing using Wasp, which seems to be due to some misleading noise in our alignments.",
        "Wasp seems to be affected more than Lu et al.",
        "'s system by such extraneous noise.",
        "However, in tactical generation, this extraneous noise does not seem to lead to worse performance, and our approach always gives the best results.",
        "as discussed by Chen and Mooney (2008) and Chen et al.",
        "(2010), tactical generation is somewhat easier than semantic parsing in that semantic parsing needs to learn to map a variety of synonymous natural-language expressions to the same meaning representation, while tactical generation only needs to learn one way to produce a correct natural language description of an event.",
        "This difference in the nature of semantic parsing and tactical generation may be the cause of the different trends in the results.",
        "English",
        "Korean",
        "Chen and Mooney (2008) Chen etal.",
        "(2010) Wasp-1 + Liang etal.",
        "Wasp-1 + our matching",
        "0.4560 0.4599 0.4580 0.4727",
        "0.5575 0.6796 0.5828 0.7148"
      ]
    },
    {
      "heading": "8. Conclusions and Future Work",
      "text": [
        "We have presented a novel generative model capable of probabilistically aligning natural-language sentences to their correct meaning representations given the ambiguous supervision provided by a grounded language acquisition scenario.",
        "Our model is also capable of simultaneously learning to semantically parse NL sentences into their corresponding meaning representations.",
        "Experimental results in Robocup sportscasting show that the NL-MR matchings inferred by our model are significantly more accurate than those produced by all previous methods.",
        "Our approach also learns competitive semantic parsers and improved language generators compared to previous methods.",
        "In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al.",
        "(2009), whose generative model is limited by a simple bag-of-words assumption.",
        "In the future, we plan to test our model on more complicated data with higher degrees of ambiguity as well as more complex meaning representations.",
        "One immediate direction is evaluating our approach on the datasets of weather forecasts and NFL football articles used by Liang et al.",
        "(2009).",
        "However, our current model does not support matching multiple meaning representations to the same natural-language sentence, and needs to be extended to allow multiple MRs to generate a single NL sentence."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We thank Wei Lu and Wee Sun Lee for sharing their software and giving helpful comments for the paper.",
        "We also thank Percy Liang for sharing his code and experimental results with us.",
        "additionally, we thank David Chen in UTCS ML group for his comments and advice.",
        "Finally, we thank the anonymous reviewers for their comments.",
        "This work was funded by the NsF grant IIS.",
        "0712907X.",
        "The experiments were executed and run on the Mastodon Cluster, provided by NSF Grant EIA-0303609."
      ]
    }
  ]
}
