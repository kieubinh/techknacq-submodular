{
  "info": {
    "authors": [
      "Shui Liu",
      "Sheng Li",
      "Tiejun Zhao",
      "Min Zhang",
      "Pengyuan Liu"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2086",
    "title": "Head-modifier Relation based Non-lexical Reordering Model for Phrase-Based Translation",
    "url": "https://aclweb.org/anthology/C10-2086",
    "year": 2010
  },
  "references": [
    "acl-D07-1079",
    "acl-D08-1022",
    "acl-J02-1005",
    "acl-J07-2003",
    "acl-J97-3002",
    "acl-J98-4004",
    "acl-N03-1017",
    "acl-N04-1014",
    "acl-P00-1056",
    "acl-P01-1067",
    "acl-P03-1054",
    "acl-P05-1033",
    "acl-P06-1066",
    "acl-P06-1077",
    "acl-P07-2045",
    "acl-P08-1064",
    "acl-P08-1066",
    "acl-P08-1114",
    "acl-P09-1036",
    "acl-P96-1025",
    "acl-W06-1628"
  ],
  "sections": [
    {
      "text": [
        "Shui Liu, Sheng Li, Tiejun Zhao, Min Zhang, Pengyuan Liu",
        "hit.",
        "edu.cn Institute for Infocomm Research mzhang@ i2r.a-star.edu.sg Institute of Computational Linguistics, Peking University liupengyuan@pku.edu.cn",
        "Phrase-based statistical MT (SMT) is a milestone in MT.",
        "However, the translation model in the phrase based SMT is structure free which greatly limits its reordering capacity.",
        "To address this issue, we propose a non-lexical head-modifier based reordering model on word level by utilizing constituent based parse tree in source side.",
        "Our experimental results on the NIST Chinese-English benchmarking data show that, with a very small size model, our method significantly outperforms the baseline by 1.48% bleu score."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Syntax has been successfully applied to SMT to improve translation performance.",
        "Research in applying syntax information to SMT has been carried out in two aspects.",
        "On the one hand, the syntax knowledge is employed by directly integrating the syntactic structure into the translation rules i.e. syntactic translation rules.",
        "On this perspective, the word order of the target translation is modeled by the syntax structure explicitly.",
        "Chiang (2005), Wu (1997) and Xiong (2006) learn the syntax rules using the formal grammars.",
        "While more research is conducted to learn syntax rules with the help of linguistic analysis (Yamada and Knight, 2001; Graehl and Knight, 2004).",
        "However, there are some challenges to these models.",
        "Firstly, the linguistic analysis is far from perfect.",
        "Most of these methods require an off-the-shelf parser to generate syntactic structure, which makes the translation results sensitive to the parsing errors to some extent.",
        "To tackle this problem, n-best parse trees and parsing forest (Mi and Huang, 2008; Zhang, 2009) are proposed to relieve the error propagation brought by linguistic analysis.",
        "Secondly, some phrases which violate the boundary of linguistic analysis are also useful in these models ( DeNeefe et al., 2007; Cowan et al.",
        "2006).",
        "Thus, a tradeoff needs to be found between linguistic sense and formal sense.",
        "On the other hand, instead of using syntactic translation rules, some previous work attempts to learn the syntax knowledge separately and then integrated those knowledge to the original constraint.",
        "Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way.",
        "By doing so, this approach addresses the challenges brought by linguistic analysis through the log-linear model in a soft way.",
        "Starting from the state-of-the-art phrase based mode l Moses ( Koehn e.t.",
        "al, 2007), we propose a head-modifier relation based reordering model and use the proposed model as a soft syntax constraint in the phrase-based translation framework.",
        "Compared with most of previous soft constraint models, we study the way to utilize the constituent based parse tree structure by mapping the parse tree to sets of head-modifier for phrase reordering.",
        "In this way, we build a word level reordering model instead of phrasal/constituent level model.",
        "In our model, with the help of the alignment and the head-modifier dependency based relationship in the source side, the reordering type of each target word with alignment in source side is identified as one of predefined reordering types.",
        "With these reordering types, the reordering of phrase in translation is estimated on word level.",
        "JU^/W Ä^r/NN rfeMk/NN UftlW H#/NN (ft/DEC M^/NN",
        "Fig 1.",
        "An Constituent based Parse Tree"
      ]
    },
    {
      "heading": "2. Baseline",
      "text": [
        "Moses, a state-of-the-art phrase based SMT system is used as our baseline system.",
        "In Moses, given the source language f and target language e, the decoder is to find:",
        "where p(e|f) can be computed using phrase translation model, distortion model and lexical reordering model.",
        "pLM(e) can be computed using the language model.",
        "©length(e) is word penalty mode l.",
        "Among the above models, there are three reordering-related components: language model, lexical reordering model and distortion model.",
        "The language model can reorder the local target words within a fixed window in an implied way.",
        "The lexical reordering model and distortion reordering model tackle the reordering problem between adjacent phrase on lexical level and alignment level.",
        "Besides these reordering model, the decoder induces distortion pruning constraints to encourage the decoder translate the leftmost uncovered word in the source side firstly and to limit the reordering within a certain range."
      ]
    },
    {
      "heading": "3. Model",
      "text": [
        "In this paper, we utilize the constituent parse tree of source language to enhance the reordering capacity of the translation model.",
        "Instead of directly employing the parse tree fragments (Bod, 1992; Johnson, 1998) in reordering rules (Huang and Knight, 2006; Liu 2006; Zhang and Jiang 2008), we make a mapping from trees to sets of head-modifier dependency relations (Collins 1996 ) which can be obtained from the constituent based parse tree with the help of head rules ( Bikel, 2004 ).",
        "According to Klein and Manning (2003) and Collins (1999), there are two shortcomings in nary Treebank grammar.",
        "Firstly, the grammar is too coarse for parsing.",
        "The rules in different context always have different distributions.",
        "Secondly, the rules learned from training corpus cannot cover the rules in testing set.",
        "Currently, the state-of-the-art parsing algorithms (Klein and Manning, 2003; Collins 1999) decompose the nary Treebank grammar into sets of head-modifier relationships.",
        "The parsing rules in these algorithms are constructed in the form of finer-grained binary head-modifier dependency relationships.",
        "Fig.2 presents an example of head-modifier based dependency tree mapped from the constituent parse tree in Fig.1.",
        "the scale of issuance of bonds by local enterprise may be expanded",
        "Fig.",
        "2.",
        "Head-modifier Relationships with Aligned Translation",
        "Moreover, there are several reasons for which we adopt the head-modifier structured tree as the main frame of our reordering model.",
        "Firstly, the dependency relationships can reflect some underlying binary long distance dependency relations in the source side.",
        "Thus, binary dependency structure will suffer less from the long distance reordering constraint.",
        "Secondly, in head-modifier relation, we not only can utilize the context of dependency relation in reordering mode l, but also can utilize some well-known and proved helpful context (Johnson, 1998) of constituent base parse tree in reordering model.",
        "Finally, head-modifier relationship is mature and widely adopted method in full parsing.",
        "Based Reor-",
        "Before elaborating the model, we define some notions further easy understanding.",
        "S=<f1, f 2...fn> is the source sentence; T=<e1,e2,...,errr> is the target sentence; AS={as(i) | 1< as(i) < n } where as(i) represents that the ith word in source sentence aligned to the as(i)th word in target sentence; AT={aT(i) | 1< aT (i) < n } where ar(i) represents that the ith word in target sentence aligned to the ar(i)th word in source sentence; D= {( <KD, r(i) )| 0< d(i) <n} is the head-modifier relation set of the words in S where d(i) represents that the ith word in source sentence is the modifier of d(i)th word in source sentence under relationship r(i); O= < ou o2,..., om> is the sequence of the reordering type of every word in target language.",
        "The reordering model probability is P(O| S, T, D, A).",
        "Relationship: in this paper, we not only use the label of the constituent label as Collins (1996), but also use some well-known context in parsing to define the head-modifier relationship r(.",
        "), including the POS of the modifier m, the POS of the head h, the dependency direction d, the parent label of the dependency label /, the grandfather label of the dependency relation p, the POS of adjacent siblings of the modifier s. Thus, the head-modifier relationship can be represented as a 6-tuple <m, h, d, /, p, s>.",
        "In Table 1, there are 7 relationships extracted from the source head-modifier based dependency tree as shown in Fig.2.",
        "Please notice that, in this paper, each source word has a corresponding relation.",
        "Reordering type: there are 4 reordering types for target words with linked word in the source side in our model: R= {rm1, rm2, rm3, rm4}.",
        "The reordering type of target word as(i) is defined as follows:",
        "• rm1: if the position number of the ith word's head is less than i ( d(i) < i ) in source language, while the position number of the word aligned to i is less than",
        "Head-modifier Relation dering Model",
        "r(.)",
        "relationship",
        "r(1)",
        "<VV, - , -, -, -, - >",
        "r(2)",
        "<NN, NN, right, NP, IP, - >",
        "r(3)",
        "<NN,VV, right, IP, CP, - >",
        "r(4)",
        "<VV, DEC, right, CP, NP, - >",
        "r(5)",
        "<NN,VV, left, VP, CP, - >",
        "r(6)",
        "<DEC, NP, right, NP, VP, - >",
        "r(7)",
        "<NN, VV, left, VP, TOP, - >",
        "• rm2: if the position number of the ith word's head is less than i ( d(i) < i ) in source language, while the position number of the word aligned to i is larger than",
        "as(d(i)) (as(i) > as(d(i)) ) in target language.",
        "• rm4: if the position number of the ith word's head is larger than i ( d(i) > i) in source language, while the position number of the word aligned to i is less than as(d(i)) (as(i) < as(d(i)) ) in target language.",
        "Fig.",
        "3.",
        "An example of the reordering types in Fig. 2.",
        "Fig.",
        "3 shows examples of all the reordering types.",
        "In Fig. 3, the reordering type is labeled at the target word aligned to the modifier: for example, the reordering type of rm1 belongs to the target word \"scale\".",
        "Please note that, in general, these four types of reordering can be divided into 2 categories : the target words order of rm2and rrm is identical with source word order while rm1 and rm3 is the swapped order of source.",
        "In practice, there are some special cases that can't be classified into any of the defined reordering types: the head and modifier in source link to the same word in target.",
        "In such cases, rather than define new reordering types, we classify these special cases into these four defined reordering types: if the head is right to the modifier in source, we classify the reordering type into rm2; otherwise, we classify the reordering type into rm4.",
        "Probability estimation: we adopt maximum likelihood (ML) based estimation in this paper.",
        "In ML estimation, in order to avoid the data sparse problem brought by lexicalization, we discard the lexical information in source and target language: P(O | S, T, D, A) where F(. )",
        "is the frequency of the statistic event in training corpus.",
        "For a given set of dependency relationships mapping from constituent tree, the reordering type of ith word is confined to two types: it is whether one of rm1 and rm2 or rm3 and rm4.",
        "Therefore, |O|=2 instead of |O|=4 in (2).",
        "The parameter a is an additive factor to prevent zero probability.",
        "It is computed as:",
        "where c is a constant parameter(c=5 in this paper).",
        "In above, the additive parameter a is an adaptive parameter decreasing with the size of the statistic space.",
        "By doing this, the data sparse problem can be relieved.",
        "Apply the Model to Decoder",
        "Our decoding algorithm is exactly the same as (Kohn, 2004).",
        "In the translation procedure, the decoder keeps on extending new phrases without overlapping, until all source words are translated.",
        "In the procedure, the order of the target words in decoding procedure is fixed.",
        "That is, once a hypothesis is generated, the order of target words cannot be changed in the future.",
        "Taking advantage of this feature, instead of computing a totally new reordering score for a newly generated hypothesis, we merely calculate the reordering score of newly extended part of the hypothesis in decoding.",
        "Thus, in decoding, to compute the reordering score, the reordering types of each target word in the newly extended phrase need to be identified.",
        "The method to identify the reordering types in decoding is proposed in Fig.4.",
        "According to the definition of reordering, the reordering type of the target word is identified by the direction of head-modifier dependency on the source side, the alignment between the source side and target side, and the relative translated order of word pair under the head-modifier relationship.",
        "The direction of dependency and the alignment can be obtained in input sentence and phrase table.",
        "While the relative translation order needs to record during decoding.",
        "A word index is employed to record the order.",
        "The index is constructed in the form of true/false array: the index of the source word is set with true when the word has been translated.",
        "With the help of this index, reordering type of every word in the phrase can be identified.",
        "1: Input : alignment array ^4T; the Start is the start position of the phrase in the source side; head-modifier relation d(.",
        "); source word index C, where C[j]=true indicates that the jth word in source has been translated.",
        "2: Output: reordering type array 0 which reserves the reordering types of each word in the target phrase",
        "Fig.",
        "4.",
        "Identify the Reordering Types of Newly Extended Phrase After all the reordering types in the newly extended phrase are identified, the reordering scores of the phrase can be computed by using equation (3)."
      ]
    },
    {
      "heading": "5. Preprocess the Alignment",
      "text": [
        "In Fig. 4, the word index is to identify the reordering type of the target translated words.",
        "Actually, in order to use the word index without ambiguity, the alignment in the proposed algorithm needs to satisfy some constraints.",
        "Firstly, every word in the source must have alignment word in the target side.",
        "Because, in the decoding procedure, if the head word is not covered by the word index, the algorithm cannot distinguish between the head word will not be translated in the future and the head word is not translated yet.",
        "Furthermore, in decoding, as shown in Fig.4, the index of source would be set with true only when there is word in target linked to it.",
        "Thus, the index of the source word without alignment in target is never set with true.",
        "Fig.",
        "5.",
        "A complicated Example of Alignment in Head-modifier based Reordering Model",
        "Secondly, if the head word has more than one alignment words in target, different alignment possibly result in different reordering type.",
        "For example, in Fig. 5, the reordering type of e2 is different when f2 select to link word e1 and e3 in the source side.",
        "To solve this problem, we modify the alignment to satisfy following conditions: a) each word in source just has only one alignment word in target, and b) each word in target has at most one word aligned in source as its anchor word which decides the reordering type of the target word.",
        "To make the alignment satisfy above constraints, we modify the alignment in corpus.",
        "In order to explain the alignment preprocessing, the following notions are defined: if there is a link between the source wordf and target word ej, let l(ei ,fj) = 1 , otherwise l(ei ,fj) = 0; the source word fjeF1-to-N , iff Ei l(ei,fj) >1, such as the source word f2 in Fig. 5; the source word fjeFNULL, iff Ei l(ei,fj) = 0, such as the source word f4 in Fig. 5; the target word ei e E1-to-N , iff Ej l(ei,fj) > 1, such as the target word e1 in Fig. 5.",
        "In preprocessing, there are 3 types of operation, including DjscardLjnk(fj) , BorrowLjnk(fj) and FjndAnchor(ei ) :",
        "DiscardLink( fj ) : if the word fj in source with more than one words aligned in target, i.e. fje F1-to-N ; We set the target word en with l(en, fj) = 1, where en= argmaxi p(ei | fj) and p(ei | fj) is estimated by ( Koehn e.t.",
        "al, 2003), while set rest of words linked to fj with l (en, fj) = 0.",
        "BorrowLink( fj ) : if the word fj in source without a alignment word in target, i.e. fj eFNULL ; let l(ei,fj)=1 where ej aligned to the word fj , which is the nearest word to fj in the source side; when there are two words nearest to fj with alignment words in the target side at the same time, we select the alignment of the left word firstly .",
        "FindAnchor(ei): for the word e; in target with more than one words aligned in source , i.e. $ e E1-to-N ; we select the word fm aligned to ej as its anchor word to decide the reordering type of ej , where fm= argmaxj p(ei | fj) and p(fj | ei) is estimated by ( Koehn et al., 2003); For the rest of words aligned to ej , we would set their word indexes with true in the update procedure of decoding in the 18th line of Fig.4.",
        "With these operations, the required alignment can be obtained by preprocessing the origin alignment as shown in Fig. 6.",
        "1: Input : set of alignment A between target language e and source language f 2: Output: the 1-to-1 alignment required by the mode l 6: foreach fie FNULL do 7: BorrowLink( fi ) 9: foreach eie E1-to-N do",
        "Fig.",
        "6.",
        "Alignment Pre-Processing algorithm",
        "*lï I II I",
        "Fig.",
        "7.",
        "An Example of Alignment Preprocessing.",
        "An example of the preprocess the alignment in Fig. 5 is shown in Fig. 7 : firstly, Djscar-dLjnk(f2) operation discards the link between f2and e1 in (a); then the link between f4 and e3 is established by operation BorrowLjnk(f4 ) in (b); at last, FjndAnchor(e3) select f2 as the anchor word of e3 in source in (c).",
        "After the preprocessing, the reordering type of e3 can be identified.",
        "Furthermore, in decoding, when the decoder scans over e2, the word index sets the word index of f3 and f4 with true.",
        "In this way, the never-true word indexes in decoding are avoided."
      ]
    },
    {
      "heading": "6. Training the Reordering Model",
      "text": [
        "Before training, we get the required alignment by alignment preprocessing as indicated above.",
        "Then we train the reordering model with this alignment: from the first word to the last word in the target side, the reordering type of each word is identified.",
        "In this procedure, we skip the words without alignment in source.",
        "Finally, all the statistic events required in equation (3) are added to the model.",
        "In our model, there are 20,338 kinds of relations with reordering probabilities which are much smaller than most phrase level reordering mode ls on the training corpus FBIS.",
        "types in model",
        "From Table 1, we can conclude that the reordering type rm2 and rm4 are preferable in reordering which take over nearly 3/4 of total number of reordering type and are identical with word order of the source.",
        "The statistic data indicate that most of the words order doesn't change in our head-modifier reordering view.",
        "This maybe can explain why the models (Wu, 1997; Xiong, 2006; Koehn, et., 2003) with limited capacity of reordering can reach certain performance."
      ]
    },
    {
      "heading": "7. Experiment and Discus sion",
      "text": [
        "We perform Chinese-to-English translation task on NIST MT-05 test set, and use NIST MT-02 as our tuning set.",
        "FBIS corpus is selected as our training corpus, which contains 7.06M Chinese words and 9.15M English words.",
        "We use GI-ZA++(Och and Ney, 2000) to make the corpus aligned.",
        "A 4-gram language model is trained using Xinhua portion of the English Gigaword corpus (181M words).",
        "All models are tuned on",
        "BLEU, and evaluated on both BLEU and NIST score.",
        "To map from the constituent trees to sets of head-modifier relationships, firstly we use the Stanford parser (Klein, 2003) to parse the source of corpus FBIS, then we use the head-finding rules in (Bikel, 2004) to get the head-modifier dependency sets.",
        "In our system, there are 7 groups of features.",
        "They are:"
      ]
    },
    {
      "heading": "1.. Language model score (1 feature)",
      "text": []
    },
    {
      "heading": "2.. word penalty score (1 feature)",
      "text": []
    },
    {
      "heading": "3.. phrase model scores (5 features)",
      "text": []
    },
    {
      "heading": "4.. distortion score (1 feature)",
      "text": []
    },
    {
      "heading": "5.. lexical RM scores (6 features)",
      "text": []
    },
    {
      "heading": "6.. Number of each reordering type (4 features)",
      "text": [
        "7.",
        "Scores of each reordering type (4 features, computed by equation (3))",
        "In these feature groups, the top 5 groups of features are the baseline model, the left two group scores are related with our model.",
        "In decoding, we drop all the OOV words and use default setting in Moses: set the distortion limitation with 6, beam-width with 1/100000, stack size with 200 and max number of phrases for each span with 50.",
        "We take the replicated Moses system as our baseline.",
        "Table 2 shows the results of our model.",
        "In the table, Baseline model is the model including feature group 1, 2, 3 and 4.",
        "Baselinerm model is the Baseline model with feature group 5.",
        "HM mode l is the Baseline model with feature group 6 and 7.",
        "H-Mrm model is the Baselinermmode l with feature group 6 and 7.",
        "From table 2, we can conclude that our reordering model is very effective.",
        "After adding feature group 6 and 7, the performance is improved by 1.41% and 1.48% in bleu score separately.",
        "Our reordering model is more effective than the lexical reordering model in Moses: 1.41% in bleu score is improved by adding our reordering model to Baseline model, while 0.48 is improved by adding the lexical reordering to Baseline model.",
        "Table 3: Performance on NIST-05 with Different Relation Frequency Threshold (bleu4 case-insensitive).",
        "Although our model is lexical free, the data sparse problem affects the performance of the mode l. In the reordering model, nearly half numbers of the relations in our model occur less than three times.",
        "To investigate this, we statistic the frequency of the relationships in our model, and expertise our H-Mfull model with different frequency threshold.",
        "Type of Reordering",
        "Percentage %",
        "rm1",
        "3.69",
        "rm2",
        "27.61",
        "rm3",
        "20.94",
        "rm4",
        "47.75",
        "Model",
        "BLEU%",
        "NIST",
        "Baseline",
        "27.06",
        "7.7898",
        "Baselinerm",
        "27.58",
        "7.8477",
        "H-M",
        "28.47",
        "8.1491",
        "H-Mrm",
        "29.06",
        "8.0875",
        "threshold",
        "KOR",
        "BLEU",
        "NIST",
        ">\\",
        "20,338",
        "29.06",
        "8.0875",
        ">2",
        "13,447",
        "28.83",
        "8.3658",
        "^3",
        "10,885",
        "28.64",
        "8.0350",
        "9,518",
        "28.94",
        "8.1002",
        ">5",
        "8,577",
        "29.18",
        "8.1213",
        "In Table 3, when the frequency of relation is not less than the threshold, the relation is added into the reordering model; KOR is the number of relation type in the reordering model.",
        "Table 3 shows that, in our model, many relations occur only once.",
        "However, these low-frequency relations can improve the performance of the model according to the experimental results.",
        "Although low frequency statistic events always do harm to the parameter estimation in ML, the model can estimate more events in the test corpus with the help of low frequency event.",
        "These two factors affect the experiment results on opposite directions: we consider that is the reason the result don't increase or decrease with the increasing of frequency threshold in the model.",
        "According to the results, the mode l without frequency threshold achieves the highest bleu score.",
        "Then, the performance drops quickly, when the frequency threshold is set with 2.",
        "It is because there are many events can't be estimated by the smaller model.",
        "Although, in the mode l without frequency threshold, there are some probabilities overestimated by these events which occur only once, the size of the mode l affects the performance to a larger extent.",
        "When the frequency threshold increases above 3, the size of model reduces slowly which makes the overestimating problem become the important factor affecting performance.",
        "From these results, we can see the potential ability of our model: if our model suffer less from data spars problem, the performance should be further improved, which is to be verified in the future."
      ]
    },
    {
      "heading": "8. Related Work and Motivation",
      "text": [
        "There are several researches on adding linguistic analysis to MT in a \"soft constraint\" way.",
        "Most of them are based on constituents in parse tree.",
        "Chiang(2005), Marton and Resnik(2008) explored the constituent match/violation in hie-ro; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang(2009) added tree-kernel to",
        "BTG model.",
        "All these studies show promising results.",
        "Making soft constrain is an easy and efficient way in adding linguistic analysis into formal sense SMT model.",
        "In modeling the reordering, most of previous studies are on phrase level.",
        "In Moses, the lexical reordering is modeled on adjacent phrases.",
        "In (Wu, 1996; Xiong, 2006), the reordering is also mode led on adjacent translated phrases.",
        "In hiero, the reordering is modeled on the segments of the unmotivated translation rules.",
        "The tree-to-string models (Yamada et al.",
        "2001; Liu et al.2006) are model on phrases with syntax representations.",
        "All these studies show excellent performance, while there are few studies on word level model in recent years.",
        "It is because, we consider, the alignment in word level model is complex which limits the reordering capacity of word level models.",
        "However, our work exploits a new direction in reordering that, by utilizing the decomposed dependency relations mapped from parse tree as a soft constraint, we proposed a novel head-modifier relation based word level reordering mode l. The word level reordering model is based on a phrase based SMT framework.",
        "Thus, the task to find the proper position of translated words converts to score the reordering of the translated words, which relax the tension between complex alignment and word level reordering in MT."
      ]
    },
    {
      "heading": "9. Conclusion and Future Work",
      "text": [
        "Experimental results show our head-modifier relationship base model is effective to the baseline (enhance by 1.48% bleu score), even with limited size of model and simple parameter estimation.",
        "In the future, we will try more complicated smooth methods or use maximum entropy based reordering model.",
        "We will study the performance with larger distortion constraint, such as the performances of the distortion constraint over 15, or even the performance without distortion model."
      ]
    },
    {
      "heading": "10. Ackno wle dge me nt",
      "text": [
        "The work of this paper is funded by National Natural Science Foundation of China (grant no.",
        "60736014), National High Technology Research and Development Program of China (863",
        "RES-THEME-158)."
      ]
    }
  ]
}
