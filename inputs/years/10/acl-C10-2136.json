{
  "info": {
    "authors": [
      "Jinsong Su",
      "Yang Liu",
      "Haitao Mi",
      "Hongmei Zhao",
      "Yajuan Lü",
      "Qun Liu"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2136",
    "title": "Dependency-Based Bracketing Transduction Grammar for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/C10-2136",
    "year": 2010
  },
  "references": [
    "acl-C04-1090",
    "acl-C08-1127",
    "acl-D07-1056",
    "acl-D09-1073",
    "acl-D09-1127",
    "acl-J07-2003",
    "acl-P00-1056",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P03-2041",
    "acl-P05-1034",
    "acl-P05-1067",
    "acl-P06-1066",
    "acl-P06-1121",
    "acl-P07-1090",
    "acl-P08-1064",
    "acl-P08-1066",
    "acl-P09-1063",
    "acl-P96-1021",
    "acl-W02-1039"
  ],
  "sections": [
    {
      "text": [
        "Jinsong Su, Yang Liu, Haitao Mi, Hongmei Zhao, Yajuan LU, Qun Liu",
        "Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences {suj insong,yliu,htmi,zhaohongmei,lvyajuan,liuqun}@ict.ac.cn",
        "In this paper, we propose a novel dependency-based bracketing transduction grammar for statistical machine translation, which converts a source sentence into a target dependency tree.",
        "Different from conventional bracketing trans-duction grammar models, we encode target dependency information into our lexical rules directly, and then we employ two different maximum entropy models to determine the reordering and combination of partial dependency structures, when we merge two neighboring blocks.",
        "By incorporating dependency language model further, large-scale experiments on Chinese-English task show that our system achieves significant improvements over the baseline system on various test sets even with fewer phrases."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Bracketing transduction grammar (BTG) (Wu, 1995) is an important subclass of synchronous context free grammar, which employs a special synchronous rewriting mechanism to parse parallel sentence of both languages.",
        "Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT).",
        "In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering.",
        "To pursue a better method to predict the order between two neighboring blocks, Xiong et al.",
        "(2006) present an enhanced BTG with a maximum entropy (ME) based reordering model.",
        "Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Se-tiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009).",
        "However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality.",
        "As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected.",
        "Thus, we argue that it is important to model the target-side syntax in BTG-based translation.",
        "Recently, modeling syntactic information on the target side has progressed significantly.",
        "Depending on the type of output, these models can be divided into two categories: the constituent-output systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependency-output systems (Eisner, 2003; Lin, 2004; Ding al., 2008).",
        "Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002).",
        "Typically, Shen et al.",
        "(2008) propose a string-to-dependency model, which integrates the targetside well-formed dependency structure into translation rules.",
        "With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (Chiang, 2007).",
        "So",
        "'A block is a bilingual phrase without maximum length limitation.",
        "we think it will be a promising way to integrate the target-side dependency structure into BTG-based translation.",
        "In this paper, we propose a novel dependency-based BTG (DepBTG) for SMT, which represents translation in the form of dependency tree.",
        "Extended from BTG, our grammars operate on two neighboring blocks with target dependency structure.",
        "We integrate target syntax into bilingual phrases and restrict target phrases to the well-formed structures inspired by (Shen et al., 2008).",
        "Then, we adopt two ME models to predict how to reorder and combine partial structures into a target dependency tree, which gives us access to capturing the target-side syntactic information.",
        "To the best of our knowledge, this is the first effort to combine the translation generation with the modeling of target syntactic structure in BTG-based translation.",
        "The remainder of this paper is structured as follows: In Section 2, we give brief introductions to the bases of our research: BTG and dependency tree.",
        "In Section 3, we introduce DepBTG in detail.",
        "In Section 4, we further illustrate how to create two ME models to predict the reordering and dependency combination between two neighboring blocks.",
        "Section 5 describes the implementation of our decoder.",
        "Section 6 shows our experiments on Chinese-English task.",
        "Finally, we end with a summary and future research in Section 7."
      ]
    },
    {
      "heading": "2. Background 2.1 BTG",
      "text": [
        "BTG is a special case ofsynchronous context free grammar.",
        "There are three rules utilized in BTG:",
        "where the reordering rules (1) and (2) are used to merge two neighboring blocks A and A in a straight or inverted order, respectively.",
        "The lexical rule (3) is used to translate the source phrase x into the target phrase y.",
        "abundant financial Haiti next",
        "UN will provide abundant financial aid to Haiti next week.",
        "In a given sentence, each word depends on a parent word, except for the root word.",
        "The dependency tree for a given sentence reflects the long distance dependency and grammar relations between words.",
        "Figure 1 shows an example of a dependency tree, where a black arrow points from a child word to its parent word.",
        "Compared with constituent tree, dependency tree directly models semantic structure of a sentence in a simpler form.",
        "Thus, it provides a desirable platform for us to utilize the target-side syntactic knowledge."
      ]
    },
    {
      "heading": "3. Dependency-based BTG 3.1 Grammars",
      "text": [
        "In this section, we extend the original BTG into DepBTG.",
        "The rules of DepBTG, which derive from that of BTG, merge blocks with target dependency structure into a larger one.",
        "These rules take the following forms:",
        "where Ad and Ad represent two neighboring blocks with target dependency structure.",
        "Rules (4)~(9) are used to determine the reordering and combination of two dependency structures, when",
        "abundant financial Haiti",
        "Figure 2: Dependency operations on the neighboring dependency structures.",
        "CC = coordinate concatenate, LA = left adjoining, and RA = right adjoining.",
        "we merge two neighboring blocks.",
        "Rule (10) is applied to generate bilingual phrase (x, y) with target dependency structure learned from training corpus.",
        "To distinguish the rules with different functions, the rules (4)~(9) and rule (10) are named as merging rules and lexical rule, respectively.",
        "Specifically, we first merge the neighboring blocks in the straight order using rules (4)~(6) or in the inverted order using rules (7)~(9).",
        "Then, according to different merging rules, we conduct some operations to combine the corresponding dependency structures in the target order: coordinate concatenate (CC), left adjoining (LA) and right adjoining (RA).",
        "To clearly illustrate our operations, we show the process of applying three dependency operations to build larger structures in Figure 2.",
        "Adopting rule (4), the dependency structures \"( ( financial ) aid J' and \"( to ( Haiti ) )\" can be combined into a larger one consisting of two sibling subtrees (see Figure 2(a)).",
        "Adopting rule (5), we can adjoin the left dependency structure \"(abundant )\" to the leftmost sub-root of the right dependency structure \"( (financial ) aid)(to (Haiti ) )\" (see Figure 2(b)).",
        "Adopting rule (6), we can include the right dependency structure \"( ( abundant ) ( financial ) aid )(to (Haiti ) )\" as a child of the rightmost subroot of the left dependency structure \"( provide )\" (see Figure 2(c)).",
        "In a similar way, rules (7)~(9) are applied to deal with two partial structures in the inverted order.",
        "As illustrated in the previous sub section, the rules of DepBTG operate on the blocks with target dependency structure.",
        "Following (Shen et al., 2008), we restrict the target phrases to the well-formed dependency structures.",
        "The main difference is that we use more relaxed constraints to extract more bilingual phrases with rational structure.",
        "Take a sentence S = wt w2...wn for example, we denote the parent word ID of word wi with di, and show the definitions of structures as follows.",
        "Defination 1 A dependency structure diis fixed on head h, where h G if and only if it meets the following conditions Defination 2 A dependency structure di...dj is floating with children C, for a non-empty set C Ç if and only if it meets the following conditions or dk = cr where q and cr represent the IDs of the leftmost and rightmost words in the set C, respectively.",
        "Note that the underline indicates the difference between our definition and that of (Shen et al., 2008).",
        "In our model, we regard the floating structure, which is not complete on its boundary subroots, as an useful structure, since it will become a complete constituent by combining it with other partial structures.",
        "For example, the dependency",
        "aid",
        "to",
        "aid to",
        "/",
        "+",
        "\\",
        "->",
        "/ y",
        "financial",
        "Haiti",
        "financial Haiti",
        "aid to",
        "abundant",
        "+",
        "/ \\",
        "financial Haiti",
        "financial",
        "Figure 3: (a) A fixed structure and (b) (c) two floating structures.",
        "Note that (c) is ill-formed in (Shen et al., 2008).",
        "structures shown in Figure 3 are all well-formed structures.",
        "However, according to the definitions of (Shen et al., 2008), 3(c) is ill-formed because aid does not include its leftmost child word abundant in the structure."
      ]
    },
    {
      "heading": "4. ME Models for Merging Rules 4.1 The Models",
      "text": [
        "A simple way to estimate the probabilities of the merging rules is to adopt maximum likelihood estimation to obtain the conditional probabilities.",
        "However, this method is not applicable to merging rules because the dependency structures become larger and larger during decoding, which are very sparse in the corpus.",
        "Inspired by MEBTG translation (Xiong et al., 2006), which considers phrase reordering as a classification problem, we model the reordering and combination of two neighboring dependency structures based on the ME principle.",
        "Owing to data sparseness and the complexity of multi-class classification, we establish two ME models rather than an unified ME model: one for the reordering between blocks, called reordering model; the other for the dependency operations on the corresponding dependency structures, called operation model.",
        "Thus, according to the ME scheme, we decompose the probability Q of each merging rule into where the functions hii G {0,1} are the features of the ME-based reordering model, 91i are the corresponding weights, and o G {straight, inverted}.",
        "Similarly, the functions h2j G {0,1} and the weights 92j are trained for the ME-based operation model, and d G {CC,LA,RA}.",
        "To train the ME models, we extract examples from a string-to-dependency word-aligned corpus during the process of bilingual phrases extraction (Koehn et al., 2005), and then collect various features for the models.",
        "For the reordering model, we adopt the method of (Xiong et al., 2006) to extract reordering examples.",
        "Due to the limit of space, we skip the details of this method.",
        "For the operation model, given an operation training example consisting of two neighboring dependency structures: the left structure di and the right structure dr, we firstly classify it into different categories by the dependency relation between di and dr :",
        "• if di and dr have the same parent, the category of the example is CC;",
        "• if di depends on the leftmost sub-root of dr, the category of the example is LA;",
        "• if dr depends on the rightmost sub-root of di, the category of the example is RA.",
        "For instance, Figure 4 shows an operation example with RA operation, where the sub-root word week of dr depends on the rightmost sub-root word provide of di.",
        "Then, we collect various features from the following nodes: the rightmost sub-root of di, and its rightmost child node; the leftmost sub-root of dr, and its leftmost child node.",
        "Here, we speculate that these nodes may carry useful information for the dependency combination of the two structures, since they locate nicely at the boundary subtrees of di and dr. For simplicity, we refer to these nodes as the feature nodes of the example.",
        "Let's revisit Figure 4, the feature nodes of the example are marked with dashed ellipses.",
        "The rightmost sub-root word of di is provide, and its rightmost child word is to; The leftmost sub-root word of dris week, and its leftmost child word is next.",
        "Table 1 : Feature categories in the ME-based operation model.",
        "f provide/VV ) abundant/ADJ financial/ADJ Haiti/NR ^next/ADj; d -^df – ",
        "Figure 4: An example with RA category consisting of the neighboring dependency structures di and dr.",
        "The dashed ellipses denote the feature nodes of the example, and each node consists of one word and its corresponding POS tag.",
        "In addition, to keep the number ofoperation examples acceptable, we follow (Xiong et al., 2006) to only extract the smallest one from the examples with the same feature nodes in each sentence.",
        "To capture reordering information, we use the boundary words of bilingual blocks as features, which are proved to be very effective in (Xiong et al., 2006).",
        "To capture dependency operation information, we design two kinds of features on the feature nodes: the Lexical features and Parts-of-speech (POS) features.",
        "With the POS features, the operation ME model will do exact predicating to the best of its ability, and then can back off to approximately predicating if exact predicating fails.",
        "Table 1 shows these feature categories in detail.",
        "Furthermore, we also use some bigram features, since it is generally admitted that the combination of different features can lead to better performance than unigram features.",
        "To better understand our operation features, we continue with the example shown in Figure 4, listing features and instances in Table 2."
      ]
    },
    {
      "heading": "5. Implementation Details 5.1 Decoder",
      "text": [
        "We develop a CKY-style decoder which uses the following features: (1) Phrase translation probabilities in two directions, (2) Lexical translation probabilities in two directions, (3) N-gram LM score, (4) ME-based reordering model score, (5) Number of phrases, (6) Number of target words, (7) ME-based operation model score, (8) Dependency LM scores at word level and POS level separately, and (9) Discount on ill-formed dependency structures.",
        "Here, the former six features are also used in MEBTG translation.",
        "Type",
        "Name",
        "Description",
        "Lexical Features",
        "Wlh{dr)",
        "The leftmost sub-root word of dr",
        "Wrh(di)",
        "The rightmost sub-root word of di",
        "Wllc{dr)",
        "The leftmost child word of Wih{dr)",
        "Wrrc(di)",
        "The rightmost child word of Wrh{di)",
        "POS Features",
        "Plh(dr)",
        "The POS oîWihidr)",
        "Prh{dl)",
        "The POS oîWrh{di)",
        "Pllc{dr)",
        "The POS of Wuc(dr)",
        "Prrcidl)",
        "The POS oïWrrc(di)",
        "Type",
        "Features and Instances",
        "Unigram Features",
        "Wrh(di) = provide Wrrc(di) = to Wih(dr) = week WUc(dr) = next Prh(di)=YV Prrc(dl) = TO Pi/l(dr)=NN PHc(dr)=ADJ",
        "Bigram Features",
        "Wrh(di)-Wih{dr) = provide.week Wrh{di) -Pih{dr) = provide_NN Prh{di)-Wlh{dr) = VV_week PM) .Plh{dr) = VV_NN",
        "Wrh(di)-Wuc(dr) = provide_next Wrh(di)-PUc(dr) = provide_ADJ Prh(di)-Wllc(dr) = VV_next PM) -PUc{dr) = VV_ADJ",
        "Wrrc{di)-Wih{dr) = to_week Wrrc{di)-Pih{dr) = to_NN Prrc(di).Wih(dr) = TCLweek Prrc(di) -Pih(dr) = TCLNN",
        "Following (Shen et al., 2008), we apply different tri-gram dependency LMs at word level and POS level separately to DepBTG translation.",
        "Given a dependency structure, where whis the parent word, wL = wj1 ...w1n and wR = wri ...wrm are child word sequences on the left side and right side respectively, the probability of a tri-gram is computed as follows:",
        "Here P(wL|wh-as-head) can be decomposed into:",
        "where '-as-head' is used to distinguish the head word from child word in the language model.",
        "In like manner, P(wR|wh-as-head) has a similar calculation method.",
        "To preserve the good coverage of bilingual phrases, we keep some bilingual phrases with the special ill-formed dependency structure.",
        "Different from the well-formed structures, where all the children of the sub-roots are complete, these ill-formed structures are not complete on the children of the boundary sub-roots, lacking a well-formed sub structure on the boundary.",
        "We consider them as useful structures with gaps, each of which can be combined with some well-formed structures into a larger well-formed one.",
        "To reduce the search space, we constrain the number of gap to one on each boundary.",
        "During decoding, we directly substitute the gap in a structure with another well-formed structure which has the same direction.",
        "Figure 5: Dependency combination of the ill-formed dependency structure di with the right well-formed dependency structure dr. G denotes gap and the dotted line denotes the substitution of the gap G with dr.",
        "For example, there are two dependency structures in Figure 5: di is an ill-formed structure with a right gap, and dr is a well-formed one.",
        "Instead of investigating three operations to combine these structures, we fill the gap of di with dr, and then compute the corresponding score of the RA operation on the sub structures \"( to )\" and \"((disaster ) area )\" in the ME-based operation model."
      ]
    },
    {
      "heading": "6. Experiment 6.1 Setup",
      "text": [
        "The training corpus comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words).",
        "We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method \"grow-diag-final-and\".",
        "Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009).",
        "From this corpus, we extract bilingual phrases with dependency structure.",
        "Here, the maximum length ofthe source phrase is set to 7.",
        "For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus.",
        "For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the training corpus.",
        "During the process of bilingual phrase extraction, we collect the neighboring blocks without any length limitation to obtain examples for two ME models.",
        "For the reordering model, we obtain about 22.6M examples with monotone order and 4.8M examples with inverted order.",
        "For the operation model, we obtain about 5.9M examples with CC operation, 14.8M examples with LA operation, and 9.7M examples with RA operation.",
        "After collecting various features from the examples, we use the ME training toolkit developed by Zhang (2004) to train ME models with the following parameters: iteration number i=200 and Gaussian prior g=1.0.",
        "The 2002 NIST MT Evaluation test set is used as the development set.",
        "The 2003 and 2005 NIST",
        "MT Evaluation test sets are our test sets.",
        "We perform the MERT training (Och, 2003) to tune the optimal feature weights on the development set.",
        "To run the decoder, we prune the phrase table with b = 100, prune the chart with n = 50, a = 0.1.",
        "See (Xiong et al., 2006) for the meanings of these parameters.",
        "The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002), as calculated by mteval-v11b.pl.",
        "Since (Xiong et al., 2006) has made a deep investigation on the ME-based reordering model, we mainly focus on the study of the ME-based operation model.",
        "To explore the utility of the various features in the operation model, we randomly select about 10K examples from all the operation examples as held-out data, and use the rest examples as training data.",
        "Then, we train the operation models on different feature sets and investigate the performance of models on the held-out data.",
        "Table 3 shows the accuracy rates of the ME operation models using different feature sets.",
        "We find that the bigram feature set provides the most persuasive evidences and achieves best performance than other feature sets.",
        "To investigate the influences of various factors on the system performance, we carried out experiments on the NIST Chinese-English task with the following systems:",
        "• MEBTG + all: an MEBTG translation system, which uses all bilingual phrases.",
        "It is our baseline system;",
        "Table 3: The accuracy rates ofthe ME-based operation models on the held-out data set using different feature sets.",
        "Unigram features include lexical features and POS features, and bigram features are the combinations of different unigram features.",
        "• MEBTG + filter 1 : a baseline system, which uses the bilingual phrases consistent to the well-formed dependency structures by (Shen",
        "• MEBTG + filter2: a baseline system, which uses the bilingual phrases consistent to our well-formed dependency structures;",
        "• MEBTG + filter3 : a baseline system, which uses the bilingual phrases consistent to our well-formed dependency structures and the special ill-formed dependency structures;",
        "• DepBTG + bigram features: a DepBTG system which only uses the bigram features in the ME-based operation model;",
        "• DepBTG + all features: a DepBTG system which uses all features in the ME-based operation model;",
        "• DepBTG + unigram features + dep LMs: a DepBTG system with dependency LMs, where only the unigram features are adopted in the ME-based operation model;",
        "• DepBTG + bigram features + dep LMs: a DepBTG system with dependency LMs, where only the bigram features are adopted in the ME-based operation model;",
        "• DepBTG + all features + dep LMs: a DepBTG system with dependency LMs, where all features are adopted in the ME-based operation model.",
        "Model",
        "Accuracy Rate",
        "lexical features",
        "87.614%",
        "POS features",
        "88.232%",
        "unigram features",
        "90.024%",
        "bigram features",
        "93.907%",
        "all features",
        "93.290%",
        "Experiment results are summarized in Table 4.",
        "Our baseline system extracts 81.4M bilingual phrases and achieves the BLEU scores of 33.41 and 32.65 on two test sets separately.",
        "Adopting the constraint ofthe well-formed structures by (Shen et al., 2008), we extract 27.8M bilingual phrases, which lead to great drops in BLEU score: 1.24 points and 1.39 points on two test sets sep-arately(see Row 3).",
        "Using the constraint of our well-formed structures, the number of extracted bilingual phrases is 33.7M.",
        "We observe the similar results that the performance drops 0.64 points and 0.72 points over the baseline system on two test sets, respectively (see Row 4).",
        "Furthermore, we add some bilingual phrases with the special ill-formed structure into our phrase table, and the number of the bilingual phrases in use is 58.5M accounting up 71.9% of the full phrases.",
        "For two test sets, our system achieves the BLEU scores of 33.29 and 32.71 (see Row 5), which are very close to the scores of baseline system.",
        "Those experimental results demonstrate that phrase coverage has a great effect on the system performance and our definitions of the allowed dependency structures are useful to retain rational bilingual phrases.",
        "Then, by employing the ME-based operation model and two 3-gram dependency LMs, the DepBTG system outperforms the MEBTG system in almost all cases.",
        "The experimental results indicate that the dependency LMs are more effective than the ME-based operation model for DepBTG system.",
        "Especially, using bigram features and dependency LMs, the DepBTG system obtains absolute improvements on two test sets: 0.77 BLEU points on NIST03 test set and 0.93 BLEU points on NIST05 test set (see Row 10), which are both statistically significant at p < 0.05 using the significance tester developed by Zhang et al.",
        "(2004)."
      ]
    },
    {
      "heading": "7. Conclusion and Future Work",
      "text": [
        "In this paper, we propose a novel dependency-based BTG to directly model the syntactic structure of the translation.",
        "Using the bilingual phrases with target dependency structure, our system employs two ME models to generate the translation in line with dependency structure.",
        "Based on the target dependency structure, our system filters 26.4% bilingual phrases (from 81.4M to 59.9M), captures the target-side syntactic knowledge by dependency language models, and achieves significant improvements over the baseline system.",
        "There is some work to be done in the future.",
        "To better utilize the syntactic information, we will put more effort on the study of the dependency LM with deeper syntactic knowledge.",
        "Moreover, we believe that modeling the syntax of both sides is a promising method to further improve BTG-based translation and this will become a study emphasis in our future research.",
        "Finally, inspired by (Tu et al., 2010), we will replace 1-best dependency trees with dependency forests to further increase the phrase coverage.",
        "Acknowledgement",
        "The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60873167, and 863 State Key Project No.2006AA010108.",
        "We thank the anonymous reviewers for their insightful comments.",
        "We are also grateful to Zhaopeng Tu, Shu Cai and Xi-anhua Li for their helpful feedback.",
        "System",
        "Type",
        "#Bp",
        "MT03",
        "MT05",
        "MEBTG",
        "all( baseline )",
        "81.4M",
        "33.41",
        "32.65",
        "filterl",
        "27.8M",
        "32.17(4.",
        "1.24)",
        "31.26(4.",
        "1.39)",
        "filter2",
        "33.7M",
        "32.77(4 0.64)",
        "31.93(4 0.72)",
        "filter3",
        "58.5M",
        "33.29(4 0.12)",
        "32.71(4 0.06)",
        "DepBTG",
        "unigram features",
        "59.9M",
        "33.46(t 0.05)",
        "32.67(4 0.02)",
        "bigram features",
        "59.9M",
        "33.57(t 0.16)",
        "32.89(4 0.24)",
        "all features",
        "59.9M",
        "33.59(t 0.18)",
        "32.86(4 0.21)",
        "unigram features + dep LMs",
        "59.9M",
        "33.90(4 0.49)",
        "33.29(4 0.64)",
        "bigram features + dep LMs",
        "59.9M",
        "34.18(t 0.77)",
        "33.58(4 0.93)",
        "all features + dep LMs",
        "59.9M",
        "34.10(t 0.69)",
        "33.55(4 0.90)"
      ]
    }
  ]
}
