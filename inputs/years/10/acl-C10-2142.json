{
  "info": {
    "authors": [
      "Xuri Tang",
      "Xiaohe Chen",
      "Weiguang Qu",
      "Shiwen Yu"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2142",
    "title": "Semi-Supervised WSD in Selectional Preferences with Semantic Redundancy",
    "url": "https://aclweb.org/anthology/C10-2142",
    "year": 2010
  },
  "references": [
    "acl-C00-1028",
    "acl-E03-1034",
    "acl-J01-3003",
    "acl-J03-4004",
    "acl-J98-2002",
    "acl-P07-1028",
    "acl-P95-1026",
    "acl-W01-0703"
  ],
  "sections": [
    {
      "text": [
        "Semi-Supervised WSD in Selectional Preferences",
        "with Semantic Redundancy",
        "Xuri TANG1,5 , Xiaohe CHEN , Weiguang QU* and Shiwen YU",
        "1.",
        "2.",
        "Jiangsu Research Center of Information Security & Privacy Technology",
        "3.",
        "4.",
        "Institute of Computational Linguistics , Peking University",
        "yusw@pku.edu.cn 5.",
        "College of Foreign Studies, Wuhan Textile University",
        "This paper proposes a semi-supervised approach for WSD in Word-Class based selectional preferences.",
        "The approach exploits syntagmatic and paradigmatic semantic redundancy in the semantic system and uses association computation and minimum description length for the task of WSD.",
        "Experiments on Predicate-Object collocations and Subject-Predicate collocations with polysemous predicates in Chinese show that the proposed approach achieves a precision which is 8% higher than the semantic-association based baseline.",
        "The semi-supervised nature of the approach makes it promising for constructing large scale selectional preference knowledge base."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "This paper addresses word sense disambiguation (WSD) which is required in the construction of selectional preference (SP) knowledge database.",
        "In previous literature of SP, four different types of formalization models are explicitly or implicitly employed.",
        "Two types are distinguished in Li and Abe(1998):",
        "where v stands for verb, n for noun, C for the semantic class of n, r for the grammatical relation between v and n, and P for the preference strength.",
        "Most of the researches(Resnik 1996; Li and Abe 1998; Ciaramita and Johnson 2000; Brockmann and Lapata 2003; Light and Greiff 2002) uses the class model, and a few(Erk 2007) uses the word model.",
        "The other two types of model are given as below:",
        "where Cn , Cv are semantic classes for the noun and verb respectively.",
        "Class-Only model considers solely the semantic classes, while Word-Class model considers both words and semantic classes.",
        "Agirre and Martinez(2001) and Zheng et al(2007) adopted the Class-only Model in research, while in McCarthy and the Word-Class Model is employed.",
        "Among the four models, the Word-Class Model is the type which possesses the most granulated knowledge and is the most potential in applications.",
        "McCarthy and Carroll(2003) reports that the Word-Class Model performs well in unsupervised WSD.",
        "In other NLP tasks such as metaphor recognition, this model may be indispensable.",
        "For instance, to distinguish the predicate verb \"}^z>0 (float)\" in Ex(1a) as Ex.",
        "1",
        "leaf floats b. price floats",
        "a.",
        "literal and Ex(1b) as metaphorical requires different interpretations of the verb.",
        "The present research is concerned with WSD as in the Word-Class model.",
        "Particularly, it aims at disambiguating predicates in subject-predicate (Subj-Pred) and predicate-object (Pred-Obj) constructions.",
        "The motivations behind the research are two folds.",
        "Firstly, semi-supervised and unsupervised WSD in SP are not fully explored.",
        "Merlo and Stevenson(Merlo and Stevenson 2001) employs supervised learning from large annotated corpus, which is difficult to obtain.",
        "One known unsupervised learning approach for WSD in SP is McCarthy and Carroll(2003) which addresses the issue via conditional probability.",
        "The other motivation derives from the fact few research is done on selectional preferences in languages other than English, as is stated in Brockmann and Lapata(2003).",
        "For instance, studies on construction of SP knowledge database in Chinese can only be found in Wu et al(2005), Zhen et al(2007), Jia and Yu(2008) and some others.",
        "The basic idea of the approach proposed for WSD in the paper is that the most acceptable interpretation of senses for a given construction is the pair of senses which encodes the most redundant information in the semantic system of the language.",
        "Two principles, namely Syntagmatic Redundancy Principle and Paradigmatic Redundancy Principle, are proposed in the paper to capture the intuition.",
        "Two corresponding devices are employed to model the two principles: Association for Syntagmatic Redundancy Principle and Minimum Description Length for Paradigmatic Redundancy Principle.",
        "Two experiments are conducted in the paper.",
        "The first is based on semantic association, achieving a 61.98% precision for predicates in experiment is used as baseline as the approach is also used in McCarthy and Carroll(2003) for verb and adjective disambiguation.",
        "In the second experiment, both semantic association and MDL are employed, the precision of WSD amounts to 69.88% and 69.09% for predicates in Subj-Preds and Pred-Objs respectively, indicating that a combination of the two devices are fairly effective in disambiguating word senses for SP.",
        "The rest of the paper is organized as below.",
        "The second part gives further illustration of the rationale for the approach.",
        "The third part describes the procedure and the fourth part discusses the experiment result.",
        "The thesis concludes with some speculations in further researches."
      ]
    },
    {
      "heading": "2. Rationale",
      "text": [
        "collocation C=< Wpred , Warg> , where Wpred is the word of predicate and Warg is the word of argument.",
        "Wpred has M senses, denoted by set",
        "Spred .",
        "Warg has N denoted by SMg .",
        "The possible interpretation of C has M*N possibilities, denoted by where Ç is called a sense collocation.",
        "The task of WSD is to search for a particular sense collocation in SC and assign it to C as its interpretation.",
        "At the initial stage, each sense collocation in SC is considered to have an even number of frequency, namely f (ç) ) = 1/(N X M).",
        "Accordingly, for each",
        "Spred G Spred , f (s'pred) = 1/ M , For eachG S arg, f (s^) = 1/N .",
        "Syntagmatic Redundancy Principle (SRP) can be stated as following: among all possible sense collocations for a word collocation, the most appropriate is the one in which senses exhibit the most redundant information between each other.",
        "The syntagmatic redundancy between words has been noticed very early by linguists and has been applied in WSD.",
        "Firth(1957) argues that there exists \"mutual expectancy\" between words in collocations, and the meaning of word is partially encoded in its juxtaposition.",
        "Lyons(1977:261) comments that Porzig has noticed in 1934 the \"essential meaning relation\" between words of collocations like \"dog barks\" and \"tree fells\" and emphasizes that the meanings of collocationally restricted lexemes such as \"bark\" and \"fell\" can only be explained by taking into account the collocates they occur with.",
        "This notion is also employed in Yarowsky(1995) for WSD, in which the key is the \"one-sense-per-collocation\" statement.",
        "McCarthy and Carroll(2003) also uses this type of redundancy for disambiguation in SP.",
        "SRP can be explained as a statistic correlation between s pred and sarg .",
        "The more co-relevant these two senses are, the more likely the pair is to be accepted as the appropriate interpretation.",
        "This can be described as below:",
        "Ç) = arg max ASSOC(sipred , S^rg) (5) where Assoc(s' ed,si,) is the function for sense association.",
        "Four methods can be considered for association computation: conditional probability (Formula 6 and 7), Lift(Han and Kamber 2006:261) (Formula 8), All-Confidence(Han and Kamber 2006:263) (Formula 9) and cosine (Formula 10).",
        "Note that two versions of conditional probability are considered, as are denoted in Formula 6 and 7.",
        "The first version, Cond-Prob 1, takes argument sense as condition, while the second version Cond-Prob 2 takes predicate sense as condition.",
        "arg I pred '",
        "p(spred, sarg)",
        "Oil _ Conf(s'pred, ^",
        "Vp(spred)* p(sarg)",
        "Paradigmatic Redundancy Principle (PRP) can be stated as following: among all possible sense collocations for a word collocation, the most appropriate is the one which is also implicitly or explicitly expressed by other synonymous, metonymic or metaphorical word collocations.",
        "Ex(2) illustrates the explicit redundancy in synonymous and metaphorical ways, in which the sense collocation \"[Price|f/ffè- ] [QuantityChange|m^]\" is expressed by five word collocations, each with a different predicate : (change), ffsjj(float), MM.",
        "(adjust), ÊÂ(go up and down), $^(alter).",
        "Ex 2.",
        "a. price changes",
        "d.",
        "b.",
        "Student is eased d. labour is eased f. Life is eased",
        "Ex(3) reveals the implicit redundancy in metonymic way, in which the meaning \" A (human) 3z;JL> (is eased)\" is implicitly expressed in all the six collocations, established by semantic relatedness among the arguments \" S/Ö^tft (Maradona)\", \" (student)\", \"Iffi(work)\", \"5?sjj(labour)\", \"fx ^(driving)\", and \"^(life)\".",
        "Ex 3.",
        "a. Maradona is eased c. work is eased e. driving is eased",
        "To apply PRP, WSD in SP is casted as an issue of model selection.",
        "Given a set of word collocations 0 , the process of WSD is to assign to each word collocation one sense collocation from a number of possibilities.",
        "Those assigned sense collocations form a set, or a model for 0 .",
        "The goal of WSD in SP is to select from all those models the one which best interprets 0 .",
        "For this purpose, Miminum Description Length(Barron et al.",
        "1998; Michell selects models by relying on induction bias based on Occam's Razor, which stipulates that the simplest solution is usually the correct one.",
        "One way to interpret MDL in Bays' analysis is as below(Michell 2003:124):",
        "length when model is considered, LD (D I m) is the data description length when model m is used for description.",
        "The model with minimum length is the best model.",
        "b. price floats c. price adjusts",
        "price goes up and down e. price alters",
        "For model description length, we have adopted the method used in (Li and Abe 1998) which considers only the size of the model: s'ze(m) 1 (12) where size(m) is the number of sense collocation contained in model m , and N is the number of word collocation in consideration.",
        "In this study, the set of word collocation with the same predicate word, denoted by 0 , is used as the unit for model description length calculation instead of the whole corpus, so as to reduce computation complexity.",
        "Accordingly, each word collocation in 0 can be assigned one and only one sense collocation in the model m , out of all the potential sense collocations as is explained in section 2.1.",
        "Data description length is calculated on model m and 0 , as is denoted in formulas (13), (14) and (15) below.",
        "The calculation is",
        "if spred s pred if spred * sp",
        "based on the probability of sense collocation Ç'' =< s' , sj >, which in turn is calculated on a modified frequency of the collocation f(çj ).",
        "The frequency is modified by counting the explicit occurrence of the sense collocation itself and the implicit occurrence expressed by other sense collocations in 0 .",
        "This idea is equivalent to enlarge the corpus by 1 fold, thus the overall collocation number is the two times of the original number.",
        "The modified frequency is a sum of two parts, denoted in formula (14).",
        "The first part is f ç ), the frequency of çj.",
        "The second part is the weighted frequency of çj.",
        "The weight is determined by the relatedness of the sense collocation çj and all the other sense collocation çk in the model m. According to this formula, if the sense collocation is found to be more similar to other sense collocations, it should obtain a higher modified frequency, and thus more likely to be the correct one for the word collocation.",
        "The way to calculate the weight is given in formula (15).",
        "If two sense collocations have identical predicate sense, namely spred = skpred, then the weight between the two sense collocations is measured by sarg) , the semantic relatedness between the argument sense sajrg and sarg .",
        "Otherwise, 0 is returned.",
        "There are different ways to measure sense relatedness.",
        "The present study has used semantic similarity based on HowNet(Liu and Li 2002) to calculate the semantic relatedness."
      ]
    },
    {
      "heading": "3. Procedure",
      "text": [
        "Figure 1 maps out the procedure for WSD in SP in the present study.",
        "The procedure is divided into two phases: data collection and disambiguation.",
        "The collocation data are collected from three sources: Sketch Engine, Collocation Dictionary and HowNet Examples.",
        "Two types of collocation data are collected: subject-predicate collocations (Subj-Pred) and predicate-object collocations (Pred-Obj) from Sketch Engine and Collocation Dictionary.",
        "Collocation Retriever reduces HowNet examples into Subj-Preds and Pred-Objs using simple heuristic methods.",
        "As a result, about 70,000 subject-predicate collocations and 106,000 predicate-object collocations are obtained.",
        "Data Combination",
        "Figure 1.",
        "WSD Procedure In disambiguation phase, two devices are employed to filter out unlikely sense collocations: Association-Based Sense Collocation Filter, following SRP, and MDL-Based Sense Collocation Filter, following PRP.",
        "t",
        "Colloc Dict.",
        "Collocation Retriever",
        "Assoc-Based Sense Colloc Filter",
        "MDL-Based Sense Colloc Filter",
        "In this phase, Subj-Preds and Pred-Objs are processed independently but following the same route.",
        "Each phase alone can perform WSD independently.",
        "Accordingly, two experiments are conducted to evaluate the method proposed in this paper.",
        "The first experiment uses association-based filter for word sense disambiguation, which is also used as the baseline.",
        "The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations.",
        "To be particular, the method used by McCarthy and Carroll(2003) is formula (6).",
        "The second experiment is based on the result of the first one so as to observe the improvement obtained by MDL-Based approach.",
        "In the second experiment, unsupervised and semi-supervised WSD are also investigated by including some annotated collocations in the evaluation data.",
        "Two corpora are constructed for evaluation.",
        "One corpus is a set of 1034 subject-predicate constructions.",
        "The other is a set of 1841 predicate-object constructions.",
        "Both are manually annotated by the authors with sense definitions defined in HowNet(Dong 2006).",
        "All together there are 52 highly ambiguous predicates involved in the study."
      ]
    },
    {
      "heading": "4. Experiments and Discussion 4.1 Collocation Retriever",
      "text": [
        "The major task in data collocation is in Collocation Retriever, which retrieves collocations from HowNet examples.",
        "Ex(4) gives a partial entry structure in HowNet, Ex 4.",
        "DEF=[change| in which W_C stands for Chinese Word, DEF for definition, E_C for Examples of Chinese, and the wave \"~\" for the word in question.",
        "From E_C, possible Subj-Preds such as \"ÀJtN(public opinion) ffzft (floats)\", \"fêfe (index) ffzft(floats)\" can be retrieved, in which the sense of \"}^z$J(float)\" is annotated with DEF.",
        "But there are also noises.",
        "A simple heuristic method is applied to automatically filter out unwanted collocations.",
        "The heuristic method",
        "checks whether the collocation retrieved from HowNet share possible sense collocations with collocations in Collocation Dictionary.",
        "If yes, it is accepted as a collocation of the type, otherwise, it is rejected.",
        "Procedures are given below:",
        "(a) Use Subj-Pred collocations and Pred-Obj collocations in Collocation Dictionary to build sense collocation set r „ j and rP d Ob.",
        ";",
        "(b) For each example sentence in E_C, segment it using ICTCLAS to obtain an array of words.",
        "Words before \"~\" forms potential Subj-Pred collocations A Subj-Pr ed and Words after form potential Pred-Obj collocations",
        "BPr ed-Obj .",
        "(c) For each a ga or b G , construct possible sense collocation set ra or it as a Subj-Pred collocation or Pred-Obj collocation.",
        "Evaluation on partial retrieved collocations shows that about 70% of obtained collocations are valid collocations, while about 30% are errors.",
        "Thus manual edition has been applied to rid those invalid collocations.",
        "Association-Based Sense Collocation Filter filters out those sense collocations that are very unlikely to be the right interpretation for a word collocation.",
        "Table 1 gives association computation result for the six senses related to the predicate \" ft (rough)\" in Subj-Pred collocation \" fÉfè- (personality) fR (rough)\".",
        "The 2nd , 3rd, 4th, and 6th are very unlikely interpretations and should be filtered, while the 5th seems to be the most appropriate.",
        "Following the procedure in Figure 1, to filter out those unlikely sense collocations, average association value is used as the filter and those below the average are dropped and those above are chosen for MDL-Based Filter.",
        "In Table 1, the average is 0.0017, and the 1rd and 5th are chosen.",
        "No.",
        "Pred Sense",
        "Arg Sense",
        "Assoc.",
        "Dgr",
        "1",
        "[Behavior|^ih]",
        "[careless|f@.jL>]",
        "0.0019",
        "2",
        "[Behavior|^ih]",
        "[coarse|fia]",
        "0.0002",
        "3",
        "[Behavior|^ih]",
        "[hoarse^IS]",
        "0.0004",
        "4",
        "[Behavior|^ih]",
        "[roughly^!!]",
        "0.0002",
        "5",
        "[Behavior|^[h]",
        "[vulgar|{§]",
        "0.0071",
        "6",
        "[Behavior|^[h]",
        "[widediameter|f@]",
        "0.0002",
        "However, in order to obtain a baseline and to decide which association computation model to use, we have followed the definition in Formula 5 and perform WSD test by choosing the sense collocation with highest association as the correct sense tags.",
        "for used this step solely for WSD, as is defined in Formula 4.",
        "Table 2 gives the experiment results for Subj-Pred and Pred-Obj collocations with all the association computation models denoted in Formula 6-10.",
        "One interesting phenomenon about all the five models is null-invariance.",
        "In selecting models for association computation, null-invariance is an important feature to be considered(Han and Kamber 2006).",
        "A model with null-invariance is not influenced by additional irrelevant data and thus is more stable.",
        "In the experiment, the model Lift is the only one not featured with null-invariance.",
        "The experiments show that Lift is not stable in different collocation types, achieving high precision in Subj-Pred but low precision in",
        "Pred_Obj.",
        "A second interesting phenomenon is collocation directionality exposed by the experiments, which can be observed in the two models of conditional probability: Cond-Prob 1, with argument as condition, and Cond-Prob 2, with predicate as condition.",
        "Directionality in collocation has been noticed earlier in some researches, for example Qu(2008).",
        "Our experiment shows that when using Cond-Prob 1, we are able to get a precision of 61.98% and respectively, while Cond-Prob 2 gets a much lower precision.",
        "This fact can be interpreted that arguments tend to have a stronger selectional preference strength, and the possible selection range is comparatively narrower, while predicates have weaker selectional preference strength and a wider selectional range.",
        "MDL-Based Filter takes as input result from Association-Based Filter using Cond-Prob 1 for association computation and average association as filter.",
        "Table 3 and 4 give the final experiment outcome for Pred-Obj and Subj-Pred constructions and individual predicates.",
        "It can be seen in Table 3 that MDL-Based Filter Several inferences can be made from the experiments.",
        "Firstly, comparison between",
        "Association-Based WSD (Table 2) and MDL",
        "WSD (Table 3) shows that MDL can improve overall performance up to 8%.",
        "As is mentioned earlier, Association-Based WSD is used as baseline in the present study.",
        "Given the fact that the average number of senses for word in question is fairly high, the improvement is considered as significant.",
        "Analysis on the individual predicates in Table 4 gives a clearer picture of WDL-based WSD.",
        "Firstly, it can be seen that MDL is especially effective when the demarcation of word senses is clear-cut.",
        "Predicate words such as \" (quiet)\", \" Et (dirty)\", \" M (difficult)\" in Subj-Preds and \" (beat)\", \"MsÖ(touch)\" and \"tTlT(break)\" in Pred-Objs are successfully disambiguated in Table 4.",
        "These words generally have 2 or 3 senses, and the senses generally differ in terms of abstractness and concreteness, as is indicated in table 5.",
        "This is due to the fact that the arguments in these collocations are clearly delimitated in HowNet and this delimitation is well captured by the modified frequency calculation defined in formula (14).",
        "Via the formula, the concrete sense collocations can",
        "Subj-Pred(%)",
        "Pred-Obj(%)",
        "Cond-Prob 1",
        "61.98",
        "62.54",
        "Cond-Prob 2",
        "55.15",
        "42.4",
        "Lift",
        "63.09",
        "40.84",
        "All Conf",
        "56.16",
        "48.54",
        "Cosine",
        "58.83",
        "55.72",
        "Ave. N.O.S.",
        "Assoc.",
        "WSD (%)",
        "MDL",
        "WSD (%)",
        "Subj-Pred",
        "4.16",
        "61.98",
        "69.09",
        "Pred-Obj",
        "5.03",
        "62.54",
        "69.88",
        "Results for Pred-Obj.",
        "i Results for Subj-Pred.",
        "increase the modified frequency of concrete sense collocations, and the abstract sense collocation can increase the modified frequency of abstract sense collocations, thus leading to the clear demarcation of abstract senses and concrete senses.",
        "The role of semantic relevance can also be clearly noticed in the predicates which have a decreased precision in MDL in Table 4.",
        "Via Paradigmatic Redundancy Principle, the information encoded in one collocation are diffused to other collocations.",
        "Consequently, errors can be diffused.",
        "This explains why the precisions of some predicates such as \" |/L (sink)\", \"ÄA(dumb)\", \"HBf (dark)\" in Subj-Pred and \"Jf(open)\", \"^(harness)\" in Pred-Objs decrease after MDL.",
        "Further analysis shows that this is because MDL has diffused the errors produced by Association Filter.",
        "For instance, at Association Filter phase, the collocation \"ff^f (box) |/L(sink)\" is assigned with the only sense collocation \"[tool^H] [very| ]\" and all other potential sense collocations are filtered.",
        "When MDL is applied, other collocations such as \"ft^(machine) |/L (heavy)\", \"H^(pick) #L(heavy)\", \"Ä^(chaw) #L(heavy)\", \"M^(basket) #L(heavy)\", (box) |/L(heavy)\", \"liCJIffiurniture) t/L(heavy)\", in which the arguments are tightly correlated with that of \"ff^(box) |/L(sink)\", all takes the sense \"[very| ]\", thus leading to the decrease of precision.",
        "The diffusion of senses can also best seen in the comparison between those predicates whose WSD are semi-supervised and those whose WSD are not supervised.",
        "Some predicates have collocations successfully retrieved from HowNet examples in which the word sense is already identified.",
        "These collocations are diffused in MDL filtering and play important roles in improving precision, while some other predicates do not have such resource.",
        "In Table 4, those unsupervised predicates are \"Ä(fall)\", \"^(collapse)\", \"M K(exquisite)\", \")#A(dumb)\", \"Jfl^I(wide)\", \" Jflx (develop)\" in Subj-Preds and \" MJF (spread)\", \"fàM (brush)\", \" fêA(get into)\", \"=ff?",
        "(bring)\", and \"fffil(mar)\" in Pred-Objs.",
        "The other predicates are semi-supervised.",
        "As can be seen in Table 4, most of these unsupervised predicates generally have a precision of 40%-60%, while those semi-supervised predicates enjoy are much higher precision between 50%-100%.",
        "The explanation for the result is straight forward.",
        "When one sense collocation of one word collocation is correctly identified, by way of Paradigmatic Redundancy Principle, the sense collocation which is similar to the correctly identified will have a higher modified frequency and is thus singled out as the best choice.",
        "This feature of MDL has great significance in the process of annotating large scale collocation data.",
        "With only a small number of annotated collocations for each predicate, a fairly high precision can be achieved for all the rest of the data through",
        "Pred.",
        "N. O.",
        "S",
        "Assoc.",
        "WSD",
        "(%)",
        "MDL WSD",
        "(%)",
        "Pred.",
        "N. O. S.",
        "Assoc.",
        "WSD",
        "(%)",
        "MDL WSD",
        "(%)",
        "il(v)",
        "5",
        "69.23",
        "80.77",
        "B)f(a)",
        "2",
        "61.14",
        "92.00",
        "±(v)",
        "14",
        "70.59",
        "70.59",
        "±t(v)",
        "2",
        "72.73",
        "86.36",
        "X(v)",
        "6",
        "56.25",
        "90.62",
        "M(a)",
        "2",
        "47.83",
        "58.7",
        "3",
        "72.22",
        "88.89",
        "fg(a/v)",
        "5",
        "52.17",
        "78.26",
        "«(v)",
        "9",
        "50",
        "60.53",
        "11(a)",
        "3",
        "56.76",
        "81.08",
        "#(v)",
        "8",
        "86.67",
        "93.33",
        "$(a)",
        "5",
        "40",
        "40",
        "*(v)",
        "5",
        "68.75",
        "62.5",
        "2",
        "55.17",
        "41.38",
        "3",
        "73.91",
        "81.16",
        "3tSf(a)",
        "3",
        "75.76",
        "93.94",
        "17",
        "55.93",
        "44.07",
        "#î(a)",
        "4",
        "96.3",
        "66.67",
        "ffig(v)",
        "3",
        "80.36",
        "78.57",
        "3",
        "47.37",
        "42.11",
        "2",
        "66.67",
        "92.31",
        "M(a)",
        "6",
        "88.24",
        "88.24",
        "WJ(v)",
        "2",
        "57.14",
        "80.95",
        "ffi(a)",
        "6",
        "46",
        "60",
        "tfe(v)",
        "6",
        "76.27",
        "79.66",
        "3",
        "44.44",
        "44.44",
        "«S(v)",
        "3",
        "83.33",
        "100",
        "2",
        "38.46",
        "65.38",
        "»(v)",
        "8",
        "63.64",
        "63.64",
        "I^(a)",
        "2",
        "93.33",
        "53.33",
        "»(v)",
        "3",
        "77.14",
        "80",
        "ff#(v)",
        "3",
        "85.19",
        "88.89",
        "ß*3Ö(v)",
        "2",
        "88.24",
        "100",
        "35(a)",
        "10",
        "50",
        "50",
        "UtfP(v)",
        "2",
        "83.87",
        "80.65",
        "2",
        "60.53",
        "63.16",
        "»(v)",
        "9",
        "61.84",
        "68.42",
        "ft(a/v)",
        "9",
        "39.66",
        "53.45",
        "3",
        "40.28",
        "51.39",
        "ffl(a)",
        "6",
        "59.46",
        "51.35",
        "W(v)",
        "4",
        "48.08",
        "53.85",
        "&(v)",
        "6",
        "48.72",
        "74.36",
        "5PF(v)",
        "3",
        "73.49",
        "73.49",
        "3jf8(v)",
        "3",
        "48.15",
        "44.44",
        "«l(v)",
        "2",
        "15.32",
        "40",
        "JSBf(a)",
        "2",
        "88.57",
        "57.14",
        "II(v)",
        "2",
        "84.91",
        "83.02",
        "*(a)",
        "6",
        "68.18",
        "40.91",
        "ft(v)",
        "3",
        "86.54",
        "85.58",
        "*F(v)",
        "8",
        "52.03",
        "65.04",
        "g(v)",
        "4",
        "72.51",
        "72.99",
        "2",
        "95.35",
        "95.35",
        "Pred",
        "Concrete Sense",
        "Abstract Sense(s)",
        "[quiet|f§]",
        "[calm|£l|f], [peaceful^]",
        "[dirty|g£]",
        "[despicable|-Sp.^5], [immoral|^^^]",
        "mm",
        "[difficulté]",
        "[poor|^]",
        "mm",
        "[beat|JT]",
        "[MakeBetter|ftft], [cultivate|itffp]",
        "mm",
        "[touch|$$]",
        "[excite|ÜS:Sj]",
        "[break|î/r»î]",
        "[obstruct||5!±]",
        "MDL."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "The present paper believes that the Word-Class Model gives the fullest description for selectional preference and thus makes efforts to disambiguate predicates in selectional preferences.",
        "From the perspective of semantic system, two principles of semantic redundancy, namely the Syntagmatic Redundancy Principle and Paradigmatic Redundancy Principle, are proposed in the paper and are applied in WSD in SP via Association Computation and Minimum Description Length.",
        "The experiments show that the approach proposed is fairly encouraging in disambiguation of polysemous predicates, especially under semi-supervised conditions when a small portion of data is annotated.",
        "With such a tool, we are able to build large scale selectional preference knowledge database based on Word-Class Models, which can be applied in various tasks, of which metaphor recognition is the particular one we bear in mind.",
        "Acknowledgement",
        "This work is supported by Chinese National Fund of Social Science under Grant 07BYY050 and Chinese National Science",
        "Fund under Grant 60773173 and Chinese National Fund of Social Science under Grant 10CYY021.",
        "We are also grateful to the autonomous reviewers for their valuable advice and suggestions."
      ]
    }
  ]
}
