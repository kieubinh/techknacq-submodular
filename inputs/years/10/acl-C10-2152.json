{
  "info": {
    "authors": [
      "Qiong Wu",
      "Songbo Tan",
      "Xueqi Cheng",
      "Miyi Duan"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2152",
    "title": "MIEA: a Mutual Iterative Enhancement Approach for Cross-Domain Sentiment Classification",
    "url": "https://aclweb.org/anthology/C10-2152",
    "year": 2010
  },
  "references": [
    "acl-P07-1055",
    "acl-P08-1034",
    "acl-P09-2080",
    "acl-W05-0408"
  ],
  "sections": [
    {
      "text": [
        "MIEA: a Mutual Iterative Enhancement Approach for Cross-Domain",
        "Sentiment Classification",
        "Qiong Wu1,2, Songbo Tan, Xueqi Cheng and Miyi Duan",
        "institute of Computing Technology, Chinese Academy of Sciences Graduate University of Chinese Academy of Sciences {wuqiong,tansongbo}@software.ict.ac.cn",
        "Recent years have witnessed a large body of research works on cross-domain sentiment classification problem, where most of the research endeavors were based on a supervised learning strategy which builds models from only the labeled documents or only the labeled sentiment words.",
        "Unfortunately, such kind of supervised learning method usually fails to uncover the full knowledge between documents and sentiment words.",
        "Taking account of this limitation, in this paper, we propose an iterative reinforcement learning approach for cross-domain sentiment classification by simultaneously utilizing documents and words from both source domain and target domain.",
        "Our new method can make full use of the reinforcement between documents and words by fusing four kinds of relationships between documents and words.",
        "Experimental results indicate that our new method can improve the performance of cross-domain sentiment classification dramatically."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Sentiment classification is the task of determining the opinion (e.g., negative or positive) of a given document.",
        "In recent years, it has drawn much attention with the increasing reviewing pages and blogs etc., and it is very important for many applications, such as opinion mining and summarization (e.g., (Ku et al., 2006; McDonald et al., 2007)).",
        "In most cases, a variety of supervised classification methods can perform well in sentiment classification.",
        "This kind of methods requires a condition to guarantee the accuracy of classification: training data should have the same distribution with test data so that test data could share the information got from training data.",
        "So the labeled data in the same domain with test data is considered as the most valuable resources for the sentiment classification.",
        "However, such resources in different domains are very imbalanced.",
        "In some traditional domains or domains of concern, many labeled sentiment data are freely available on the web, but in other domains, labeled sentiment data are scarce and it involves much human labor to manually label reliable sentiment data.",
        "The challenge is how to utilize labeled sentiment data in one domain (that is, source domain) for sentiment classification in another domain (that is, target domain).",
        "This raises an interesting task, cross-domain sentiment classification (or sentiment transfer).",
        "In this work, we focus on one typical kind of sentiment transfer problem, which utilizes only training data from source domain to improve sentiment classification performance for target domain, without any labeled data for the target domain (e.g., (Andreevskaia and Bergler, 2008)).",
        "In recent years, some studies have been conducted to deal with sentiment transfer problems.",
        "However, most of the attempts rely on only the labeled documents (Aue and Gamon, 2005; Tan et al., 2007; Tan et al., 2009; Wu et al., 2009) or the labeled sentiment words (Gamon and Aue, 2005) to improve the performance of sentiment transfer, so this kind of methods fails to uncover the full knowledge between the documents and the sentiment words.",
        "In fact, the opinion of a document can be determined by the interrelated documents as well as by the interrelated words, and this rule is also tenable when determining the opinion of a sentiment word.",
        "This rule is based on the following intuitive observations:",
        "(1) A document strongly linked with other positive (negative) documents could be considered as positive (negative); in the same way, a word strongly linked with other positive (negative) words could be considered as positive (negative).",
        "(2) A document containing many positive (negative) words could be considered as positive (negative); similarly, a word appearing in many positive (negative) documents could be considered as positive (negative).",
        "Inspired by these observations, we aim to take into account all the four kinds of relationships among documents and words (i.e. the relationships between documents, the relationships between words, the relationships between words and documents, and the relationships between documents and words) in both source domain and target domain under a unified framework for sentiment transfer.",
        "In this work, we propose an iterative reinforcement approach to implement the above idea.",
        "The proposed approach makes full use of all the relationships among documents and words from both source domain and target domain to transfer information between domains.",
        "In our approach, the opinion of a document (word) is reinforced by the opinion of all its interrelated documents and words; and the updated opinion of the document (word) will conversely reinforce the opinions of its interrelated documents and words.",
        "That is to say, it is an iterative reinforcement process until it converges to a final result.",
        "The contribution of our work is twofold.",
        "First, we extend the traditional sentiment-transfer methods by utilizing the full knowledge between interrelated documents and words.",
        "Second, we present a reinforcement approach to get the opinions of documents by making use of graphranking algorithm.",
        "The proposed approach is evaluated on three domain-specific sentiment data sets.",
        "The experimental results show that our approach can dramatically improve the accuracy when transferred to another target domain.",
        "And we also conduct extensive experiments to investigate the parameters sensitivity.",
        "The results show that our algorithm is not sensitive to these parameters."
      ]
    },
    {
      "heading": "2. Proposed Methods 2.1 Problem Definition",
      "text": [
        "In this paper, we have two document sets: the test documents DU = {di,...,dnd} where di is the term vector of the ith text document and each diGDU(i = l,...,nd) is unlabeled; the training documents D = {dnd+J,...,dnd+md} where dj represents the term vector of the jth text document and each dj^D(j = nd+1,...,nd+md) should have a label from a category set C = {negative, positive}.",
        "We assume the training dataset D is from the interrelated but different domain with the test dataset DU.",
        "Also, we have two word sets: W = {w;,...,wnw} is the word set of DU and each w,GWU (i = 1,...,nw) is unlabeled; WL = {wnw+i,...,wnw+mw} is the word set of D and each w;- G W(j = nw+1,.. .,nw+mw) has a label from C. Our objective is to maximize the accuracy of assigning a label in C to diGDU (i = 1,.. .,nd) utilizing the training data D and W in another domain.",
        "The proposed algorithm is based on the following presumptions:",
        "(1) W nwU^o>.",
        "(2) The labels of documents appear both in the training data and the test data should be the same.",
        "The proposed approach is inspired by graphranking algorithm whose idea is to give a node high score if it is strongly linked with other high-score nodes.",
        "Graph-ranking algorithm has been successfully used in many fields (e.g. PageRank (Brin et al., 1999), LexRank (Erkan and Radev, 2004)).",
        "We can get the following thoughts based on the ideas of PageRank and HITS (Kleinberg,",
        "(1) If a document is strongly linked with other positive (negative) documents, it tends to be positive (negative); and if a word is strongly linked with other positive (negative) words, it tends to be positive (negative).",
        "(2) If a document contains many positive (negative) words, it tends to be positive (negative); and if a word appears in many positive (negative) documents, it tends to be positive (negative).",
        "Given the data points of documents and words, there are four kinds of relationships in our problem:",
        "• DD-Relationship: It denotes the relationships between documents, usually computed by their content similarity.",
        "• WW-Relationship: It denotes the relationships between words, usually computed by knowledge-based approach or corpus-based approach.",
        "• DW-Relationship: It denotes the relationships between documents and words, usu-",
        "ally computed by the relative importance of a word in a document.",
        "• WD-Relationship: It denotes the relationships between words and documents, usually computed by the relative importance of a document to a word.",
        "Meanwhile, our problem refers to both source domain and target domain, so our approach considers eight relationships altogether: DDO-Relationship (the relationships between DU and D), DDN-Relationship (the relationships between DU), WWO-Relationship (the relationships between WU and W), WWN-Relationship (the relationships between WU and WU), DWO-Relationship (the relationships between DU and W ), DWN-Relationship (the relationships between DU and WU), WDO-Relationship (the relationships between WU and D ), WDN-",
        "Relationship (the relationships between WU and DU).",
        "The first four relationships are used to compute the sentiment scores of the documents, and the others are used to compute the sentiment scores of the words.",
        "The iterative reinforcement approach could make full use of all the relationships in a unified framework.",
        "The framework of the proposed approach is illustrated in Figure 1.",
        "The framework consists of a graph-building phase and an iterative reinforcement phase.",
        "In the graph-building phase, the input includes both the labeled data from source domain and the unlabeled data from target domain.",
        "The proposed approach builds four graphs based on these data to reflect the above relationships respectively.",
        "For source-domain data, we initialize every document and word a score (\"1\" denotes positive, and denotes negative) to represent its degree of sentiment orientation, and we call it sentiment score; for target-domain data, we set the initial sentiment scores to 0.",
        "In the iterative reinforcement phase, our approach iteratively computes the sentiment scores of the documents and words based on the graphs.",
        "When the algorithm converges, all the documents get their sentiment scores.",
        "If its sentiment score is between 0 and 1, the document should be classified as \"positive\".",
        "The closer its sentiment score is near 1, the higher the \"positive\" degree is.",
        "Otherwise, if its sentiment score is between 0 and -1, the document should be classified as \"negative\".",
        "The closer its sentiment score is near -1, the higher the \"negative\" degree is.",
        "The algorithms of sentiment graph building and iterative reinforcement are described in details in the next sections, respectively.",
        "Symbol Definition",
        "In this section, we build four graphs to reflect eight relationships, and the meanings of symbols are shown in Table 1.",
        "In this table, the first column denotes the name of the relationship; the second column denotes the similarity matrix to reflect the corresponding relationship; in consideration of convergence, we normalize the similarity matrix, and the normalized form is listed in the third column; in order to compute sentiment scores, we find the neighbors of a document or a word and the neighbor matrix is listed in the fourth column.",
        "Relationship",
        "Similarity matrix",
        "Normalized form",
        "Neighbor matrix",
        "DDO",
        "UU [ UUj\\ ndxmd",
        "UL",
        "Uni = [Unfij\\ndxK",
        "DDN",
        "UU – [ U ij\\ ndxnd",
        "U U",
        "UnU = [UnUijxK",
        "WWO",
        "[ j\\ nwxmw",
        "VnL = WnLij\\*,K",
        "WWN",
        "r ir 1",
        "y Yy ijlnwxnw",
        "r u",
        "rn = [rnUj\\nwXK",
        "DWO",
        "M=[M ij\\ndxmw",
        "M L",
        "MnL = [MnLij W",
        "DWN",
        "MU=[MUij\\ndxnw",
        "M U",
        "MnU = [MnUj\\ndXK",
        "WDO",
        "nN j\\ nwxmd",
        "N L",
        "NnL = [ NnLj Lk",
        "WDN",
        "NU=[NUij\\nwxnd",
        "N U",
        "NnU = [NnUv\\nwXK",
        "Document-to-Document Graph We build an undirected graph whose nodes denote documents in both DL and DU and edges denote the content similarities between documents.",
        "If the content similarity between two documents is 0, there is no edge between the two nodes.",
        "Otherwise, there is an edge between the two nodes whose weight is the content similarity.",
        "The edges in this graph are divided into two parts: edges between DU and DL; edges between DUitself, so we build the graph in two steps.",
        "(1) Create DU and D Edges",
        "The content similarity between two documents is computed with the cosine measure.",
        "We use an adjacency matrix U to denote the similarity matrix between DU and D. UL=[ULij]ndxmd is defined as follows:",
        "The weight associated with word w is computed with tfwidfw where tfw is the frequency of word w in the document and idfw is the inverse document frequency of word w, i.e. 1+log(N/nw), where N is the total number of documents and nwis the number of documents containing word w in a data set.",
        "In consideration of convergence, we normalize U to UL by making the sum of each row equal to 1:",
        "otherwise",
        "In order to find the neighbors (in another word, the nearest documents) of a document, we sort every row of UUL to UL in descending order.",
        "That is: ULy>ULlt (i = 1,...,nd;j,k = 1,...,md; k^j).",
        "trix UnL = [UnLij ]ndxK to denote the neighbors of DUin source domain, with UnL j corresponding to the jth nearest neighbor of d .",
        "(2) Create DU and DU Edges",
        "Similarly, the edge weight between DU itself is computed by the cosine measure.",
        "We get the similarity matrix UU=[UUij]ndxnd, the normalized similarity matrix UU, and the neighbors of DU in target domain: UnU = [UnUij]ndxK.",
        "Word-to-Word Graph",
        "Similar to the Document-to-Document Graph, we build an undirected graph to reflect the relationship between words in WL and WU, in which each node corresponds to a word and the edge weight between any different words corresponds to their semantic similarity.",
        "The edges in this graph are divided into two parts: edges between WU and WL; edges between WU itself, so we also build the graph in two steps.",
        "(1) Create WU and WL Edges",
        "We compute the semantic similarity using corpus-based approach which computes the similarity between words utilizing information from large corpora.",
        "There are many measures to identify word semantic similarity, such as mutual information (Turney, 2001), latent semantic analysis (Landauer et al., 1998) etc.",
        "In this study, we compute word semantic similarity based on the sliding window measure, that is, two words are semantically similar if they co-occur at least once within a window of maximum Kwin words, where Kwin is the window size.",
        "We use an adjacency matrix VL to denote the similarity matrix between WU and WL.",
        "VL=[VLij]nwxmw is defined as follows:",
        "where N is the total number of words in D ; p(wi, wj) is the probability of the co-occurrence of wi and wj within a window, i.e. num(wi, wL)/N, where num(wi, wj) is the number of the times wi and wj co-occur within the window; p(wi) and p(wj) are the probabilities of the occurrences of wi and wj respectively, i.e. num(wi)/N and num(wj)/N, where num(wi) and num(wj) are the numbers of the times Wi and Wj occur.",
        "We normalize V to VL to make the sum of each row equal to 1.",
        "Then we sort every row of VL to VL in descending order, and we use a matrix VnL = [VnLij ]m/K to denote the neighbors of W in source domain.",
        "(2) Create WU and W Edges",
        "Then we also compute the edge weight between any different nodes which denote words in W by the sliding window measure.",
        "We get the similarity matrix VU=[VUij]nWxnW, the normalized similarity matrix VU, and the neighbors of WU in target domain: VnU = [VnU, ]nWxK.",
        "Document-to-Word Graph We can build a weighted directed bipartite graph from documents in DU and words in WL and WUin the following way: each node in the graph corresponds to a document in DU or a word in WLand WU; if word Wj appears in document d, we create an edge from d to Wj.",
        "The edges in this graph are divided into two parts: edges from DUto WL; edges from DU to WU, so we also build the graph in two steps.",
        "(1) Create DU to WL Edges",
        "The edge weight from a document in DU to a word in WL is proportional to the importance of word Wj in document d. We use an adjacency matrix M to denote the similarity matrix from DU to WL.",
        "ML=[MLij]ndxmW is defined as follows:",
        "where W represents a unique word in di and tfW, idfW are respectively the term frequency in the document and the inverse document frequency.",
        "We normalize M to ML to make the sum of each row equal to 1.",
        "Then we sort every row of ML to M~ L in descending order, and we use a matrix MnL = \\MnLij ]ndxKto denote the neighbors of DU in WL.",
        "(2) Create DU to WU Edges",
        "Similarly, we can also compute the edge weight from a document in DU to a word in WU in the same way.",
        "We get the similarity matrix MU=[MUij]ndxnW, the normalized similarity matrix",
        "m u , and the neig hbors of DU in WU:",
        "MnU = [MnUij]ndxK .",
        "Word-to-Document Graph",
        "In this section, we build a weighted directed bipartite graph from words in WU and documents in DL and DU in which each node in the graph corresponds to a word in WU and a document in DL or DU; if word Wj appears in document di, we create an edge from Wj to di.",
        "The edges in this graph are also divided into two parts: edges from WU to DL; edges from WU to DU.",
        "(1) Create W to DL Edges",
        "Similar to 3.3.4, the edge weight from a word in WU to a document in DL is proportional to the importance of word Wi in document dj.",
        "We use an adjacency matrix N=[Nij]nWxmd to denote the similarity matrix from WU to DL.",
        "We normalize N to nL to make the sum of each row equal to 1.",
        "Then we sort every row of NL to NL in descending order, and we use a matrix NnL = [NnLij ]nwxK to denote the neighbors of WU in DL.",
        "(2) Create WU to DU Edges",
        "We can also compute the edge weight from a word in WU to a document in DU in the same way.",
        "We get the similarity matrix NU=[NU j]nWxnd, the normalized similarity matrix NU , and the neighbors of W in DU: NnU = [NnUv ]nwxK.",
        "Based on the two thoughts introduced in Section 2.2, we fuse the eight relationships abstracted from the four graphs together to iteratively reinforce sentiment scores, and we can obtain the iterative equation as follows:",
        "Wsj = p Yj (V Ljg x WSg ) + p Y, (V Ujh x WSh )",
        "geVnLj.",
        "heVnUj.",
        "(6)",
        "where i. means the ith row of a matrix; Ds = {dsdsnd, dsnd+i,..., ds„d+md} represents the sentiment scores of D and DL; Ws = {WSi,_,WSnW, WSnW+i,^, WSnW+mW,} represents the sentiment scores of W and WL; p and p show the relative contributions to the final sentiment scores from source domain and target domain when calculating DD-Relationship and WW-Relationship, and <p + ß=1; y and ôshow the relative contributions to the final sentiment scores from source domain and target domain when calculating DW-Relationship and WD-Relationship, and y + ô=l.",
        "For simplicity, we merge the relationships from source domain and target domain.",
        "That is, for formula (5), we merge the first two items into one, the last two items into one; for formula (6), we merge its first two items into one, its last two items into one.",
        "Thus, (5) and (6) are transformed into (7) and (8) as follows:",
        "where a and ß show the relative contributions to the final sentiment scores from document sets and word sets, and a+ ß=1.",
        "In consideration of the convergence, Ds and Ws are normalized separately after each iteration as follows to make the sum of positive scores equal to 1, and the sum of negative scores equal to -1:",
        "where DU and DU denote the negative and positive document set of DU respectively; WnUeg and WpUos denote the negative and positive",
        "word set of WU respectively.",
        "Here is the complete algorithm: 1.",
        "Initialize the sentiment score vector dsiof di&D (i = nd+1,..., nd+md) with 1 when di is labeled \"positive\", and with 1 when diis labeled \"negative\", and initialize the sentiment score vector wsi of wi G W (i = nw+1,., nw+mw) with 1 when wi is labeled \"positive\", and with 1 when wi is labeled \"negative\".",
        "And we normalize dsi (i = nd+1,., nd+md) (wsi (i = nw+1,., nw+mw)) to make the sum of positive scores of D (W) equal to 1, and the sum of negative scores of DL (WL) equal to -1.",
        "Also, the initial sentiment scores of DU and WU are set to 0."
      ]
    },
    {
      "heading": "2.. Alternate the following two steps until convergence:",
      "text": [
        "where ds(k ) and ws j) denote the ds.",
        "and ws,- at the kth iteration.",
        "3.",
        "According to dsiGDs (i = 1,...,nd), assign each diGDU (i = 1,...,nd) a label.",
        "If dsifalls in the range [-1,0], assign di the label \"negative\"; if dsi falls in the range [0,1], assign di the label \"positive\"."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "In this section, we evaluate our approach on three different domains and compare it with some state-of-the-art algorithms, and also evaluate the approach's sensitivity to its parameters.",
        "Note that we conduct experiments on Chinese data, but the main idea in the proposed approach is language-independent in essence.",
        "We use three Chinese domain-specific data sets from on-line reviews, which are: Book Reviews(B, www.dangdang.com/), Hotel Reviews (H, www.ctrip.com/) and Notebook Reviews (N, www.360buy.com/).",
        "Each dataset has 4000 labeled reviews (2000 positives and 2000 negatives).",
        "We use ICTCLAS (http://ictclas.org/), a Chinese text POS tool, to segment these Chinese reviews.",
        "Then, utilizing the part-of-speech tagging function provided by ICTCLAS, we take all adjectives, adverbs and adjective-noun phrases as candidate sentiment words.",
        "After removing the repeated words and ambiguous words, we get a list of words in each domain.",
        "For the list of words in each domain, we manually label every word as \"negative\", \"positive\" or \"neutral\", and we take those \"negative\" and \"positive\" words as a sentiment word set.",
        "Note that we use the sentiment word set only for source domain, while using the candidate sentiment words for target domain.",
        "Lastly, the documents are represented by vector space model.",
        "In this model, each document is converted into bag-of-words presentation in the remaining term space.",
        "We compute term weight with the frequency of the term in the document.",
        "We choose one of the three data sets as source-domain data DL, and its corresponding sentiment word set as WL; we choose another data set as target-domain data DU, and its corresponding candidate sentiment words as WU.",
        "In this paper we compare our approach with the following baseline methods:",
        "Proto: This method applies a traditional supervised classifier, prototype classifier (Tan et al., 2005), for the sentiment transfer.",
        "And it only uses source domain documents as training data.",
        "LibSVM: This method applies a state-of-the-art supervised learning algorithm, Support Vector Machine, for the sentiment transfer.",
        "In detail, we use LibSVM (Chang and Lin, 2001) with a linear kernel and set all options as default.",
        "This method only uses source domain documents as training data.",
        "TSVM: This method applies transductive SVM (Joachims, 1999) for the sentiment transfer which is a widely used method for improving the classification accuracy.",
        "In our experiment, we use Joachims's SVM-light package (http://svmlight.joachims.org/) for TSVM.",
        "We use a linear kernel and set all parameters as default.",
        "This method uses both source domain data and target domain data.",
        "In this section, we compare proposed approach with the three baseline methods.",
        "There are three parameters in our algorithm, K, Kwin, a ( ßcan be calculated by 1-a).",
        "We set K to 50, and Kwin to 10 respectively.",
        "With different a, our approach can be considered as utilizing different relative contributions from document sets and word sets.",
        "In order to identify the importance of both document sets and word sets for sentiment transfer, we separately set a to 0, 1, 0.5 to show the accuracy of utilizing only word sets (referred to as WORD), only document sets (referred to as DOC), and both the document and word sets (referred to as ALL).",
        "It is thought that the algorithm achieves the convergence when the changing between the sentiment score dsi computed at two successive iterations for any di^DU (i = 1,...,nd) falls below a given threshold, and we set the threshold 0.00001 in this work.",
        "The parameters will be studied in parameters sensitivity section.",
        "Table 2 shows the accuracy of Prototype, LibSVM, TSVM and our algorithm when training data and test data belong to different domains.",
        "As we can observe from Table 2, our algorithm produces much better performance than supervised baseline methods.",
        "Compared with the traditional classifiers, our approach outperforms them by a wide margin on all the six transfer tasks.",
        "The great improvement compared with the baselines indicates that our approach performs very effectively and robustly.",
        "Table 2 shows the average accuracy of TSVM is higher than both traditional classifiers, since it utilizes the information of both source domain and target domain.",
        "However, the proposed approach outperforms TSVM: the average accuracy of the proposed approach is about 4.3% higher than TSVM.",
        "This is caused by two reasons.",
        "First, TSVM is not dedicated for sentiment-transfer learning.",
        "Second, TSVM requires the ratio between positive and negative examples in the test data to be close to the ratio in the training data, so its performance will be affected if this requirement is not met.",
        "Results of \"DOC\" and \"WORD\" are shown in column 4 and 5 of Table 2.",
        "As we can observe, they produce better performance than all the baselines.",
        "This is caused by two reasons.",
        "First, \"DOC\" and \"WORD\" separately utilize the sentiment information of documents and words.",
        "Second, both \"DOC\" and 'WORD\" involve an iterative reinforcement process to improve their performance.",
        "The great improvement indicates that the iterative reinforcement approach is effective for sentiment transfer.",
        "Traditional Classifier",
        "TSVM",
        "Our Approach",
        "Proto",
        "LibSVM",
        "DOC",
        "WORD",
        "ALL",
        "B->H",
        "0.735",
        "0.747",
        "0.749",
        "0.772",
        "0.734",
        "0.763",
        "B->N",
        "0.651",
        "0.652",
        "0.769",
        "0.714",
        "0.785",
        "0.795",
        "H->B",
        "0.645",
        "0.675",
        "0.614",
        "0.671",
        "0.668",
        "0.703",
        "H->N",
        "0.729",
        "0.669",
        "0.726",
        "0.749",
        "0.727",
        "0.734",
        "N->B",
        "0.612",
        "0.608",
        "0.622",
        "0.638",
        "0.667",
        "0.726",
        "N->H",
        "0.724",
        "0.711",
        "0.772",
        "0.764",
        "0.740",
        "0.792",
        "Average",
        "0.683",
        "0.677",
        "0.709",
        "0.718",
        "0.720",
        "0.752",
        "Besides, Table 2 also shows both document sets and word sets are important for sentiment transfer.",
        "The approach \"ALL\" outperforms the approaches \"DOC\" and \"WORD\" on almost all the six transfer tasks except \"B->H\" and \"H->N\".",
        "The average increase of accuracy over all the six tasks is 3.4% and 3.2% respectively.",
        "The reason is: at every iteration, the classification accuracy of documents and words is improved by each other, and then the accuracy of sentiment transfer is improved by the documents and words that are classified more accurately.",
        "As for \"B->H\" and \"H->N\", the performance of utilizing only document sets is so good that the word sets couldn't improve the performance any more.",
        "The improvement of the approach \"ALL\" convinces us that not a single one of the four relationships can be omitted.",
        "The proposed algorithm has an important parameter, a (ßcan be calculated by 1-a).",
        "In this section, we conduct experiments to show that our algorithm is not sensitive to this parameter.",
        "To investigate the sensitivity of proposed method involved with the parameter a, we set K to 50, and Kwin to 10.",
        "And we change a from 0 to 1, an increase of 0.1 each.",
        "We also evaluate a on the six tasks mentioned in section 3.1, and the results are shown in figure 2.",
        "We can observe from Figure 2 that the accuracy first increases and then decreases when a is increased from 0 to 1.",
        "The accuracy changes gradually when a is near 0 or 1, and it changes less when a is between 0.2 and 0.8.",
        "It is easy to explain this phenomenon.",
        "When a is set to 0, this indicates our algorithm only uses word sets to aid classification, without the information of document sets.",
        "And if a is set to 1, our algorithm only uses document sets to calculate sentiment score, without the help of word sets.",
        "Both cases above don't use all information of four relationships, so their accuracies are worse than to equal the contributions of both document and word sets.",
        "This experiment shows that the proposed algorithm is not sensitive to the parameter a as long as a is not 0 or 1.",
        "We set a to 0.5 in our overallperformance experiment.",
        "Our algorithm is an iterative process that will converge to a local optimum.",
        "We evaluate its convergence on the six tasks mentioned above.",
        "Figure 3 shows the change of accuracy with respect to the number of iterations.",
        "We can observe from figure 3 that the curve rises sharply during the first 6 iterations, and it is very stable after 10 iterations are performed.",
        "This experiment indicates that our algorithm could converge very quickly to get a local optimum.",
        "In this paper, we propose a novel cross-domain sentiment classification approach, which is an iterative reinforcement approach for sentiment transfer by utilizing all the relationships among documents and words from both source domain and target domain to transfer information between domains.",
        "First, we build three graphs to reflect the above relationships respectively.",
        "Then, we assign a score for every unlabelled document to denote its extent to \"negative\" or \"positive\".",
        "We then iteratively calculate the score by making use of the graphs.",
        "Finally, the final score for sentiment classification is achieved when the algorithm converges, so we can label the targetdomain data based on these scores.",
        "0.9",
        "-»-B->H",
        "-«-B->N H->B -*-H->N -n-N->B",
        "-»-N->H",
        "0.8",
        "-1 Jl '_I__»- .",
        "J'~~",
        "y\" – – .",
        "0.7",
        "m -",
        "-----' _-IK-Y H_ *_____",
        "•",
        "__* – ",
        "^ \" ~*",
        "0.6",
        "80.5",
        "U 3",
        "Ö0.4",
        "0.3",
        "0.2",
        "0.1",
        "0",
        "0 0.1",
        "0.2 0.3 0.4 0^5 0.6 0.7 0.8",
        "0.9 1",
        "We conduct experiments on three domain-specific sentiment data sets.",
        "The experimental results show that the proposed approach could dramatically improve the accuracy when transferred to a target domain.",
        "To investigate the parameter sensitivity, we conduct experiments on the same data sets.",
        "It is observed that our approach is not very sensitive to its four parameters, and could converge very quickly to get a local optimum.",
        "In this study, we employ only cosine measure, sliding window measure and vector measure to compute similarity.",
        "These are too general, and perhaps not so suitable for sentiment classification.",
        "In the future, we will try other methods to calculate the similarity.",
        "Furthermore, we experiment our approach on only three domains, and we will apply our approach to many more domains."
      ]
    },
    {
      "heading": "5. Acknowledgments",
      "text": [
        "This work was mainly supported by two funds, i.e., 60933005 & 60803085, and two another projects, i.e., 2007CB311100 & 2007AA01Z441."
      ]
    }
  ]
}
