{
  "info": {
    "authors": [
      "Xiaofeng Yu",
      "Wai Lam"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2160",
    "title": "Jointly Identifying Entities and Extracting Relations in Encyclopedia Text via a Graphical Model Approach",
    "url": "https://aclweb.org/anthology/C10-2160",
    "year": 2010
  },
  "references": [
    "acl-D09-1047",
    "acl-I08-1044",
    "acl-N06-1038",
    "acl-N07-2032",
    "acl-N07-2050",
    "acl-N09-1037",
    "acl-P05-1073",
    "acl-P07-1120",
    "acl-P08-1101",
    "acl-W06-1673"
  ],
  "sections": [
    {
      "text": [
        "Jointly Identifying Entities and Extracting Relations in Encyclopedia Text via A Graphical Model Approach*",
        "Xiaofeng Yu Wai Lam",
        "Information Systems Laboratory Department of Systems Engineering & Engineering Management The Chinese University of Hong Kong {xfyu,wlam}@se.cuhk.edu.hk",
        "In this paper, we investigate the problem of entity identification and relation extraction from encyclopedia articles, and we propose a joint discriminative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simultaneously.",
        "This modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual benefits, as well as a great flexibility to incorporate a large collection of arbitrary, overlapping and non-independent features.",
        "We show the parameter estimation algorithm of this model.",
        "Moreover, we propose a new inference method, namely collective iterative classification (CIC), to find the most likely assignments for both entities and relations.",
        "We evaluate our model on real-world data from Wikipedia for this task, and compare with current state-of-the-art pipeline and joint models, demonstrating the effectiveness and feasibility of our approach."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "We investigate a compound information extraction (IE) problem from encyclopedia articles, which consists of two subtasks – recognizing structured information about entities and extracting the relationships between entities.",
        "The most common approach to this problem is a pipeline architecture: attempting to perform different sub-tasks, namely, named entity recognition and relation extraction between recognized entities in several separate, and independent stages.",
        "Such kind of design is widely adopted in NLP.",
        "The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project No: CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050442 and 2050476).",
        "This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies.",
        "The most common and simplest approach to performing compound NLP tasks is the 1-best pipeline architecture, which only takes the 1-best hypothesis of each stage and pass it to the next one.",
        "Although it is comparatively easy to build and efficient to run, this pipeline approach is highly ineffective and suffers from serious problems such as error propagation (Finkel et al., 2006; Yu, 2007; Yu et al., 2008).",
        "It is not surprising that, the end-to-end performance will be restricted and upper-bounded.",
        "Usually, one can pass N-best lists between different stages in pipeline architectures, and this often gives useful improvements (Hollingshead and Roark, 2007).",
        "However, effectively making use of N-best lists often requires lots of engineering and human effort (Toutanova, 2005).",
        "On the other hand, one can record the complete distribution at each stage in a pipeline, to compute or approximate the complete distribution at the next stage.",
        "Doing this is generally infeasible, and this solution is rarely adopted in practice.",
        "One promising way to tackle the problem of error propagation is to explore joint learning which integrates evidences from multiple sources and captures mutual benefits across multiple components of a pipeline for all relevant subtasks simultaneously (e.g., (Toutanova et al., 2005), (Poon and Domingos, 2007), (Singh et al., 2009)).",
        "Joint learning aims to handle multiple hypotheses and uncertainty information and predict many variables at once such that subtasks can aid each other to boost the performance, and thus usually leads to complex model structure.",
        "However, it is typically intractable to run a joint model and they sometimes can hurt the performance, since they increase the number of paths to propagate errors.",
        "Due to these difficulties, research on building joint approaches is still in the beginning stage.",
        "A significant amount of recent work has shown the power of discriminatively-trained probabilistic graphical models for NLP tasks (Lafferty et al., and Jordan, 2008).",
        "The superiority of graphical model is its ability to represent a large number of random variables as a family of probability distributions that factorize according to an underlying graph, and capture complex dependencies between variables.",
        "And this progress has begun to make the joint learning approach possible.",
        "In this paper we study and formally define the joint problem of entity identification and relation extraction from encyclopedia text, and we propose a joint paradigm in a single coherent framework to perform both subtasks simultaneously.",
        "This framework is based on undirected probabilistic graphical models with arbitrary graphical structure.",
        "We show how the parameters in this model can be estimated efficiently.",
        "More importantly, we propose a new inference method – collective iterative classification (CIC), to find the maximum a posteriori (MAP) assignments for both entities and relations.",
        "We perform extensive experiments on real-world data from Wikipedia for this task, and substantial gains are obtained over state-of-the-art probabilistic pipeline and joint models, illustrating the promise of our approach."
      ]
    },
    {
      "heading": "2. Problem Formulation",
      "text": [
        "This problem involves identifying entities and discovering semantic relationships between entity pairs from English encyclopedic articles.",
        "The basic document is an article, which mainly defines and describes an entity (known as principal entity).",
        "This document mentions some other entities as secondary entities related to the principal entity.",
        "Clearly, our task consists of two subtasks – first, for entity identification, we need to recognize the secondary entities (both the boundaries and types of them) in the document .",
        "Second, after all the secondary entities are identified, our goal for relation extraction is to predict what relation, if any, each secondary entity has to the principal entity.",
        "We assume that there is no relationship between any two secondary entities in one document.",
        "As an illustrative example, Figure 1 shows the task of entity identification and relationship extraction from encyclopedic documents.",
        "Here, Abraham Lincoln is the principal entity.",
        "Our task consists of assigning a set of predefined entity types (e.g., PER, DATE, YEAR, and ORG) to segmentations in encyclopedic documents and assigning a set of predefined relations (e.g., birth_day, birth_year, and member_of) for each identified secondary entity to the principal entity.",
        "Let x be an observation sequence of tokens in encyclopedic text and x = {x1; • • • , xN}.",
        "Let sp be the principal entity (we assume that it is known or can be easily recognized), and let s = {si, ••• ,sL} be a segmentation assignment of observation sequence x.",
        "Each segment si is a triple Si = {ai,ßi,yi}, where ai is a start position, ßiis an end position, and yi is the label assigned to all tokens of this segment.",
        "The segment si satisfies 0 < ai < ßi < |x| and = ßi + 1.",
        "Let rpn be the relation assignment between principal entity sp and secondary entity candidate sn from the segmentation s, and r be the set of relation assignments for sequence x.",
        "Let y = { r, s} be the pair of segmentation s and segment relations r for an observation sequence x.",
        "A valid assignment y must satisfy the condition that the assignments of the segments and the assignments of the relations of segments are maximized simultaneously.",
        "We now formally define this joint optimization problem as follows:",
        "Definition 1 (Joint Optimization of Entity Identification and Relation Extraction ): Given an observation sequence x, the goal of joint optimization of entity identification and relation extraction is to find the assignment y* = {r*, s*} that has the maximum a posteriori (MAP) probability",
        "this paper we only focus on secondary entity identification.",
        "DATE YEAR ORG",
        "Figure 1: An example of entity identification and relation extraction excerpted from our dataset.",
        "The secondary entities are in pink color and labeled.",
        "The semantic relation of each secondary entity to the principal entity Abraham Lincoln (in green color and we assume that it is known or can be easily recognized) is also shown.",
        "where r* and s* denote the most likely relation assignment and segmentation assignment, respectively.",
        "Note that this problem is usually very challenging and offers new opportunities for information extraction, since complex dependencies between segmentations and relations should be exploited."
      ]
    },
    {
      "heading": "3. Our Proposed Model 3.1 Preliminaries",
      "text": [
        "Conditional random fields (CRFs) (Lafferty et al., 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs.",
        "Let G be a factor graph (Kschischang et al., 2001) deining a probability distribution over a set of output variables o conditioned on observation sequences x.",
        "C = |$c(oc, xc)} is a set of factors in G, then the probability distribution over G can be written as where $c is a potential function and Z(x) = Soil cec ^c(oc, xc) is a normalization factor.",
        "We assume the potentials factorize according to a set of features [fk(oc, xc)} as $c(oc, xc) = exP(Sk ®k fk(oc, xc)) so that the family of distributions is an exponential family.",
        "The model parameters are a set of real-valued weights 6 = [9k}, one weight for each feature.",
        "Practical models rely extensively on parameter tying to use the same parameters for several factors.",
        "However, the traditional fashion of CRFs can only deal with single task, they lack the capability to represent more complex interaction between multiple subtasks.",
        "In the following we will describe our joint model in detail for this problem.",
        "3.2 A Joint Model for Entity Identification and Relation Extraction",
        "Following the notations in Section 2.2, let L and M be the number of segments and number of relations for sequence x, respectively.",
        "We deine a joint conditional distribution for segmentation s in observation sequence x and segment relation r in undirected, probabilistic graphical models.",
        "The nature of our modeling enables us to partition the factors C of G into three groups {Cs, Cr, Cv}={{(s}, {fSR}, {</>v}}, namely the segmentation potential (S, the relation potential (fR, and the segmentation-relation joint potential (fv, and each potential is a clique template whose parameters are tied.",
        "The potential function 0s(i, s, x) models segmentation s in x, the potential function (f)R(rpm, rpn, r) (m = n) represent dependencies (e.g., long-distance dependencies, relation transitivity, etc) between any two relations in the relation set r, where rpm is the relation assignment between the principal entity sp and the secondary entity candidate sm from s, and similarly for rpn.",
        "And the joint potential (v(sp,Sj, r) captures rich and complex interactions between segmentation s for secondary entity identiication and relation r between each secondary entity candidate sj to the principal entity sp.",
        "According to the celebrated Hammersley-Clifford theorem (Besag, 1974), the joint conditional distribution P(y|x) = P({r, s}|x) is factorized as a product of potential functions over cliques in the graph G as the form of an exponential family:",
        "where Z(x) = V2y IIOS (SS, x) IiOR (R(rpm",
        ", rpn, r) FTOv (v(sp,sj, r) is the normalization factor of the joint model.",
        "We assume the potential functions (S, (Rand fv factorize according to a set of features and a corresponding set of real-valued weights.",
        "More specifically, (S(i, s, x) =",
        "exp(E != iE K=i ^kgk (i, s, x)).",
        "To effectively capture properties of segmentation, we relax the irst-order Markov assumption to semi-Markov such that each segment feature function gk(•) depends on the current segment si, the previous segment si_1, and the whole observation sequence x, that is, gk(i, s, x) = gk(si_1,si, x) = gk (yi_1,yi,ai,ßi, x).",
        "And transitions within a segment can be non-Markovian.",
        "Similarly, the potential (R(rpm, rpn, r) = exp(EM,n EÜLi ßwqw(rpm, rpn, r)) and (v(sp, sj, r) = exp(XL=i J]vtht(sp, sj, r)), where W and T are number of feature functions, qw (•) and ht( ) are feature functions, ßw and vt are corresponding weights for them.",
        "The potential (R(rpm,rpn, r) allows long-range dependency representation between different relations rpm and rpn.",
        "For example, if the same secondary entity is mentioned more than once in an observation sequence, all mentions probably have the same relation to the principal entity.",
        "Using potential (R(rpm, rpn, r), evidences for the same entity segments to the principal entity are shared among all their occurrences within the document.",
        "The joint factor (v(sp,sj, r) exploits tight dependencies between segmentations and relations.",
        "For example, if a segment is labeled as a location and the principal entity is person, the semantic relation between them can be birthplace or visited, but cannot be employment.",
        "Such dependencies are essential and modeling them often leads to improved performance.",
        "In summary, the probability distribution of the joint model can be rewritten as:",
        "Figure 2: Graphical representation of the probabilistic joint model.",
        "The gray nodes represent sequence tokens {x1, • • • ,xN}.",
        "Each ellipse represents a segment consisting of several consecutive sequence tokens.",
        "The pink nodes represent segmentation assignment {s1 , • • • , sL} of sequence.",
        "The yellow nodes represent relation assignment {rp1, • • • , rpL} between the principal entity sp (in green color) and secondary entity segments.",
        "As illustrated in Figure 2, our model consists of three sub-structures: a semi-Markov chain on the segmentations s conditioned on the observation sequences x, represented by (S ; potential (Rmeasuring dependencies between different relations rpm and rpn; and a fully-connected graph on the principal entity sp and each segment sj for their relations, represented by fv.",
        "While several special cases of CRFs are of particular interest, and we emphasize on the differences and advantages of our model against others.",
        "Linear-chain CRFs (Lafferty et al., 2001) can only perform single sequence labeling, they lack the ability to capture long-distance dependency and represent complex interactions between multiple subtasks.",
        "Skip-chain CRFs (Sutton and Mc-callum, 2004) introduce skip edges to model longdistance dependencies to handle the label consistency problem in single sequence labeling and extraction.",
        "2D CRFs (Zhu et al., 2005) are two-dimensional conditional random ields incorporating the two-dimensional neighborhood dependencies in Web pages, and the graphical representation of this model is a 2D grid.",
        "Hierarchical CRFs (Liao et al., 2007) are a class of CRFs with hierarchical tree structure.",
        "Our probabilistic model for joint entity identiication and relation extraction has distinct graphical structure from 2D and hierarchical CRFs.",
        "And this modeling has several advantages over previous probabilistic graphical models by using semi-Markov chains for efficient segmentation and labeling, by representing long-range dependencies between relations, and by capturing rich and complex interactions between relevant subtasks to exploit mutual benefits."
      ]
    },
    {
      "heading": "4. Learning the Parameters",
      "text": [
        "Given independent and identically distributed (IID) training data D = {x*, y*}^, where x* is the i-th sequence instance, y* = {r*, s*} is the corresponding segmentation and relation assignments.",
        "The objective of learning is to estimate A = {Xk,ßw,vt} which is the vector of model's parameters.",
        "Under the IID assumption, we ignore the summation operator ^NL1 in the log-likelihood during the following derivations.",
        "To reduce over-fitting, we use regularization and a common choice is a spherical Gaussian prior with zero mean and covariance a1.",
        "Then the regularized log-likelihood function L for the data is vtht(sp,sj, r)}, Z (x) = Y, yll *(r, s, x), and 1/2a, 1/2a, 1/2a are regularization parameters.",
        "Taking derivatives of the function L over the parameter Xk yields:",
        "Similarly, the partial derivatives of the log-likelihood with respect to parameters ßw and nutare as follows:",
        "The function L is concave, and can be efficiently maximized by standard techniques such as stochastic gradient and limited memory quasiNewton (L-BFGS) algorithms.",
        "The parameters Xkßw and vt are optimized iteratively until converge."
      ]
    },
    {
      "heading": "5. Finding the Most Likely Assignments",
      "text": [
        "The objective of inference is to find y* = {r*, s*} = argmax{r> s} P(r, s|x) such that both s* and r* are optimized simultaneously.",
        "Unfortunately, exact inference to this problem is generally prohibitive, since it requires enumerating all possible segmentation and corresponding relation assignments.",
        "Consequently, approximate inference becomes an alternative.",
        "We propose a new algorithm: collective iterative classification (CIC) to perform approximate inference to find the maximum a posteriori (MAP) segmentation and relation assignments of our model in an iterative fashion.",
        "The basic idea of CIC is to decode every target hidden variable based on the assigning labels of its sampled variables, where the labels might be dynamically updated throughout the iterative process.",
        "Collective classification refers to the classification of relational objects described as nodes in a graphical structure, as in our model.",
        "The CIC algorithm performs inference in two steps, as shown in Algorithm 1.",
        "The first step, bootstrapping, predicts an initial labeling assignment for a unlabeled sequence x*, given the trained model P(y|x).",
        "The second step is the iterative classification process which re-estimates the labeling assignment of x* several times, picking them in a sample set S based on initial assignment for x*.",
        "Here we exploit the sampling technique (Andrieu et al., 2003).",
        "The advantages of sampling are summarized as follows.",
        "Sampling stochastically enables us to generate a wide range of inference situations, and the samples are likely to be in high probability areas, increasing our chances of finding the maximum, thus leading to more robust and accurate performance.",
        "The CIC algorithm may converge if none of the labeling assignments change during an iteration or a given number of iterations is reached.",
        "Noticeably, this inference algorithm is also used to efficiently compute the marginal probability P(y|x) during parameter estimation (the normalization constant Z(x) can also be calculated via approximation techniques).",
        "As can be seen, this algorithm is simple to design, efficient and scales well w.r.t.",
        "the size of data."
      ]
    },
    {
      "heading": "6. Experiments",
      "text": [
        "Our data comes from Wikipedia, the world's largest free online encyclopedia.",
        "This dataset consists of 1127 paragraphs from 441 pages from the online encyclopedia Wikipedia.",
        "We labeled 7740 entities into 8 categories, yielding 1243 person, 1085 location, 875 organization, 641 date, 1495 year, 38 time, 59 number, and 2304 miscellaneous names.",
        "This dataset also contains 4701 relation instances and 53 labeled relation types.",
        "The 10 most frequent relation types are job-title, visited, birth-place, associate, birth-year, memberjof, birthjday, opus, death-year, and death-day.",
        "Note that this compound IE task involving entity identification and relation extraction is very challenging, and modeling tight interactions between entities and their relations is highly attractive.",
        "Accurate entities enable features that are naturally expected to be useful to boost relation extraction.",
        "And a wide range of rich, overlapping features can be exploited in our model.",
        "These features include contextual features, part-of-speech (POS) tags, morphological features, entity-level dictionary features, clue word features.",
        "Feature conjunctions are also used.",
        "In leveraging relation extraction to improve entity identification, we use a combination of syntactic, entity, keyword, semantic, and Wikipedia characteristic features.",
        "More importantly, our model can incorporate multiple mention features qw (•), which are used to collect Algorithm 1: Collective Iterative Classification Inference_ Input: A unlabeled sequence x* and a trained",
        "model P(y|x) Output: The set of predicted assignment",
        "y* = {ri'Si} // Bootstrapping foreach y* e Y do I y <- argmaxy.",
        "P(y^x*); end",
        "// Iterative Classification repeat",
        "Generate a sample set S based on initial label assignment y* for sequence x*; foreach s * eS do",
        "Assign new label assignment to until all labels have stabilized or a threshold number ofiterations have elapsed ; return y* = {r*, s*} evidences from other occurrences of the same secondary entities for consistent segmentation and relation labeling to the principal entity.",
        "The features ht( ) capture deep dependencies between segmentations and relations, and they are natural and useful to enhance the performance.",
        "We perform fourfold cross-validation on this dataset, and take the average performance.",
        "For performance evaluation, we use the standard measures of Precision (P), Recall (R), and F-measure (the harmonic mean of P and R: Jjg|) for both entity identification and relation extraction.",
        "We conduct holdout methodology for parameter tuning and optimization of our model.",
        "We compare our approach with a series of linear-chain CRFs: CRF+CRF and a joint model DCRF (Sutton et al.",
        ", 2007): dynamic probabilistic models combined with factored approach to multiple sequence labeling.",
        "CRF+CRF perform entity identification and relation extraction separately.",
        "Relation extraction is viewed as a sequence labeling problem in the second CRF.",
        "All these models exploit standard parameter learning and inference algorithms",
        "Table 1: Comparative performance of our model, CRF+CRF, and DCRF models for entity identification.",
        "_ in our experiments.",
        "To avoid over-fitting, penalization techniques on likelihood are performed.",
        "We also use the same set of features for all these models.",
        "Table 1 shows the performance of entity identii-cation and Table 2 shows the overall performance of relation extraction , respectively.",
        "Our model substantially outperforms all baseline models on the overall F-measure for entity identiication, resulting in an relative error reduction of up to 38.97% and 28.83% compared to CRF+CRF and DCRF, respectively.",
        "For relation extraction, the improvements on the F-measure over CRF+CRF and DCRF are 4.68% and 3.75%.",
        "McNemar's paired tests show that all improvements of our model over baseline models are statistically sig-niicant.",
        "These results demonstrate the merits of our approach by capturing tight interactions between entities and relations to explore mutual beneits.",
        "The pipeline model CRF+CRF performs entity identiication and relation extraction independently, and suffers from problems such as error accumulation.",
        "For example, CRF+CRF cannot extract the memberjof relation between the secondary entity Republican and the principal entity George W. Bush, since the organization name Republican is incorrectly labeled as a miscellaneous.",
        "By modeling interactions between two subtasks, enhanced performance is achieved, as illustrated by DCRF.",
        "Unfortunately, training a DCRF model with unobserved nodes (hidden variables) makes this approach difficult to optimize, as we will show below.",
        "The eficiency of different models is summarized in Table 3.",
        "Compared to the pipeline model CRF+CRF, the learning time of our model is only a small constant factor slower.",
        "Notably, our model is over orders of magnitude (approximately 15.7 times) faster than the joint model DCRF.",
        "The DCRF model uses loopy beliefprop-agation (LBP) for approximate learning and inference.",
        "When the graph has large tree-width as in our case, the LBP algorithm in DCRF is inefi-cient, and is slow to converge.",
        "Using L-BFGS and the CIC approximate inference algorithms, both learning and decoding can be carried out ef-iciently.",
        "Table 3: Eficiency comparison of different models on learning time (sec.)",
        "and inference time (sec.",
        ")._",
        "Table 4 compares our CIC inference with two state-of-the-art inference approaches: Gibbs sampling (GS) (Geman and Geman, 1984) and the iterative classification algorithm (ICA) (Neville and Jensen, 2000) for our model.",
        "The CIC inference is shown empirically to help improve classiication accuracy and robustness over these two algorithms.",
        "When probability distributions are very complex or even unknown, the GS algorithm cannot be applied.",
        "ICA iteratively infers the states of variables given the current predicted labeling assignments of neighboring variables as observed information.",
        "Prediction errors on labels may then propagate during the iterations and the algorithm will then have dificulties to generalize correctly.",
        "Entities",
        "CRF+CRF",
        "DCRF",
        "Our model",
        "P",
        "R",
        "Fi",
        "P",
        "R",
        "Fi",
        "P",
        "R",
        "Fi",
        "person",
        "75.33",
        "83.22",
        "79.08",
        "75.96",
        "83.82",
        "79.70",
        "82.91",
        "84.26",
        "83.58",
        "location",
        "77.03",
        "69.45",
        "73.04",
        "77.68",
        "70.13",
        "73.71",
        "82.94",
        "80.52",
        "81.71",
        "organization",
        "53.78",
        "47.76",
        "50.59",
        "54.55",
        "46.98",
        "50.48",
        "61.63",
        "62.61",
        "62.12",
        "date",
        "98.54",
        "97.53",
        "98.03",
        "97.98",
        "95.22",
        "96.58",
        "98.90",
        "96.24",
        "97.55",
        "year",
        "97.14",
        "99.10",
        "98.11",
        "98.12",
        "99.09",
        "98.60",
        "97.36",
        "99.55",
        "98.44",
        "time",
        "60.00",
        "20.33",
        "30.37",
        "50.00",
        "25.33",
        "33.63",
        "100.0",
        "25.00",
        "40.00",
        "number",
        "98.88",
        "60.33",
        "74.94",
        "100.0",
        "66.00",
        "79.52",
        "100.0",
        "65.52",
        "79.17",
        "miscellaneous",
        "77.42",
        "80.56",
        "78.96",
        "79.81",
        "83.14",
        "81.44",
        "82.69",
        "85.16",
        "83.91",
        "Overall",
        "89.55",
        "88.70",
        "89.12",
        "90.98",
        "90.37",
        "90.67",
        "93.35",
        "93.37",
        "93.36",
        "Model",
        "Precision",
        "Recall",
        "F-measure",
        "CRF+CRF",
        "70.40",
        "57.85",
        "63.51",
        "DCRF",
        "69.30",
        "60.22",
        "64.44",
        "Our model",
        "72.57",
        "64.30",
        "68.19",
        "Model",
        "Learning time",
        "Inference time",
        "CRF+CRF",
        "2822.55",
        "6.20",
        "DCRF",
        "105993.00",
        "127.50",
        "Our model",
        "6733.69",
        "62.75",
        "We mention some recently published results related to Wikipedia datasets (Note that it is dificult to compare with them strictly, since these results can be based on different experimental settings).",
        "Culotta et al.",
        "(2006) used a data set with a 70/30 split for training/testing and Nguyen et al.",
        "(2007) used 5930 articles for training and 45 for testing, to perform relatione extraction from Wikipedia.",
        "And the obtained F-measures were 67.91 and 37.76, respectively.",
        "Yu et al.",
        "(2009) proposed an integrated approach incorporating probabilistic graphical models with irst-order logic to perform relation extraction from encyclopedia articles, with a F-measure of 65.66.",
        "All these systems assume that the golden-standard entities are already known and they only perform relation extraction.",
        "However, such assumption is not valid in practice.",
        "Notably, our approach deals with a fairly more challenging problem involving both entity identiication and relation extraction, and it is more applicable to real-world IE tasks."
      ]
    },
    {
      "heading": "7. Related Work",
      "text": [
        "A number of previous researchers have taken steps toward joint models in NLP and information extraction, and we mention some recently proposed, closely related approaches here.",
        "Roth and Yih (2007) considered multiple constraints between variables from tasks such as named entities and relations, and developed a integer linear programming formulation to seek an optimal global assignment to these variables.",
        "Zhang and Clark (2008) employed the generalized per-ceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding.",
        "Toutanova et al.",
        "(2008) presented a model capturing the linguistic intuition that a semantic argument frame is a joint structure, with strong dependencies among the arguments.",
        "Finkel and Manning (2009) proposed a discriminative feature-based constituency parser for joint named entity recognition and parsing.",
        "And Dahlmeier et al.",
        "(2009) proposed a joint model for word sense disambiguation of prepositions and semantic role labeling of prepositional phrases.",
        "However, most of the mentioned approaches are task-speciic (e.g., (Toutanova et al., 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks.",
        "Since we capture rich and complex dependencies between subtasks via potential functions in probabilistic graphical models, our approach is general and can be easily applied to a variety of NLP and IE tasks."
      ]
    },
    {
      "heading": "8. Conclusion and Future Work",
      "text": [
        "In this paper, we investigate the compound IE task of identifying entities and extracting relations between entities in encyclopedia text.",
        "And we propose a uniied framework based on undirected, conditionally-trained probabilistic graphical models to perform all relevant subtasks jointly.",
        "More importantly, we propose a new algorithm: CIC, to enable approximate inference to ind the MAP assignments for both segmentations and relations.",
        "As we shown, our modeling offers several advantages over previous models and provides a natural formalism for this compound task.",
        "Experimental study exhibits that our model signiicantly outperforms state-of-the-art models while also running much faster than the joint models.",
        "In addition, the superiority of the CIC algorithm is also discussed and compared.",
        "We plan to improve the scalability of our approach and apply it to other real-world problems in the future.",
        "Entity",
        "Precision",
        "Recall",
        "F-measure",
        "GS",
        "92.45",
        "92.15",
        "92.30",
        "ICA",
        "92.19",
        "91.98",
        "92.08",
        "CIC",
        "93.35",
        "93.37",
        "93.36",
        "Relation",
        "Precision",
        "Recall",
        "F-measure",
        "GS",
        "71.22",
        "63.29",
        "67.02",
        "ICA",
        "71.58",
        "63.68",
        "67.40",
        "CIC",
        "72.57",
        "64.30",
        "68.19"
      ]
    }
  ]
}
