{
  "info": {
    "authors": [
      "Xiaofeng Yu",
      "Wai Lam"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2161",
    "title": "Accelerated Training of Maximum Margin Markov Models for Sequence Labeling: A Case Study of NP Chunking",
    "url": "https://aclweb.org/anthology/C10-2161",
    "year": 2010
  },
  "references": [
    "acl-H05-1099",
    "acl-J95-4004",
    "acl-N03-1028",
    "acl-W00-0726",
    "acl-W02-1001",
    "acl-W04-3201"
  ],
  "sections": [
    {
      "text": [
        "Accelerated Training of Maximum Margin Markov Models for Sequence Labeling: A Case Study of NP Chunking*",
        "Xiaofeng Yu Wai Lam",
        "Information Systems Laboratory Department of Systems Engineering & Engineering Management The Chinese University of Hong Kong {xfyu,wlam}@se.cuhk.edu.hk",
        "We present the first known empirical results on sequence labeling based on maximum margin Markov networks (MN), which incorporate both kernel methods to efficiently deal with high-dimensional feature spaces, and probabilistic graphical models to capture correlations in structured data.",
        "We provide an efficient algorithm, the stochastic gradient descent (SGD), to speedup the training procedure of MN.",
        "Using official dataset for noun phrase (NP) chunking as a case study, the resulting optimizer converges to the same quality of solution over an order of magnitude faster than the structured sequential minimal optimization (structured SMO).",
        "Our model compares favorably with current state-of-the-art sequence labeling approaches.",
        "More importantly, our model can be easily applied to other sequence labeling tasks."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The problem of annotating or labeling observation sequences arises in many applications across a variety of scientific disciplines, most prominently in natural language processing, speech recognition, information extraction, and bioinformatics.",
        "Recently, the predominant formalism for modeling and predicting label sequences has been based on discriminative graphical models and variants.",
        "Among such models, maximum margin Markov networks (MN) and variants ( Taskar et al.",
        "(2003); Taskar (2004); Taskar et al.",
        "(2005)) have recently gained popularity in the machine learning community.",
        "While the M N framework makes extensive use of many theoretical results",
        "The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project No: CUHK4128/07) and the Direct Grant of the Faculty of Engineering, CUHK (Project Codes: 2050442 and 2050476).",
        "This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies.",
        "available for Markov networks, it largely dispenses with the probabilistic interpretation.",
        "MN thus combines the advantages of both worlds, the possibility to have a concise model of the relationships present in the data via log-linear Markov networks over a set of label variables and the highly accurate predictions based on maximum margin estimation of the model parameters.",
        "Traditionally, MN can be trained using the structured sequential minimal optimization (structured SMO), a coordinate descent method for solving quadratic programming (QP) problems (Taskar et al., 2003).",
        "Clearly, however, the polynomial number of constraints in the QP problem associated with the MN can still be very large, making the structured SMO algorithm slow to converge over the training data.",
        "This currently limits the scalability and applicability of MN to large-scale real world problems.",
        "Stochastic gradient methods (e.g., Lecun et al. (1998); Bottou (2004)), on the other hand, are online and scale sub-linearly with the amount of training data, making them very attractive for large-scale datasets.",
        "In stochastic (or online) gradient descent (SGD), the true gradient is approximated by the gradient of the cost function only evaluated on a single training example.",
        "The parameters are then adjusted by an amount proportional to this approximate gradient.",
        "Therefore, the parameters of the model are updated after each training example.",
        "For large-scale datasets, online gradient descent can be much faster than standard (or batch) gradient descent.",
        "In this paper, we marry the above two techniques and show how SGD can be used to significantly accelerate the training of MN.",
        "And we then apply our model to the well-established sequence labeling task: noun phrase (NP) chunking.",
        "Experimental results show the validity and effectiveness of our approach.",
        "We now summarize the primary contributions of this paper as follows:",
        "• We exploit MN to NP chunking on the standard evaluation dataset, achieving favorable performance against recent top-performing systems.",
        "The MN framework allows arbitrary features of observation sequence, as well as the important benefits of kernels.",
        "To the best of our knowledge, this is the first known empirical study on NP chunking using MN in the NLP community.",
        "• We provide the efficient SGD algorithm to accelerate the training procedure of MN, and experimental results show that it converges over an order of magnitude faster than the structured SMO without sacrificing performance.",
        "• Our model is easily extendable to other sequence labeling tasks, such as part-of-speech tagging and named entity recognition.",
        "Based on the promising results on NP chunking, we believe that our model will significantly further the applicability of margin-based approaches to large-scale sequence labeling tasks."
      ]
    },
    {
      "heading": "2. Maximum Margin Markov Networks for Sequence Labeling",
      "text": [
        "In sequence labeling, the output is a sequence of labels y = (y1,..., yT) which corresponds to an observation sequence x = (x1,..., xT).",
        "Suppose each individual label can take values from set E, then the problem can be considered as a multiclass classification problem with |E|T different classes.",
        "In MN, a pairwise Markov network is defined as a graph G = (Y, E).",
        "Each edge ) G E is associated with a potential function where 0(x, yi, yj) is a pairwise basis function.",
        "All edges in the graph denote the same type of interaction, so that we can define a feature map 0k(x,y) = E(ij)eE0k(x,yi,yj).",
        "The network encodes the following conditional probability distribution (Taskar et al., 2003):",
        "where 0(x, y) = [0102 ... 0|£|((>tmns]T is used to learn a weight vector w. 0k = EI=i 0i(x)I(yi = k), Vk G {1, 2,..., |E|} and (ptrans = [C11C12 ... ctt]T where cij is the number of observed transitions from the ith alphabet to the jth alphabet in E.",
        "Similar to SVMs (Vapnik, 1995), MN tries to find a projection to maximize the margin 7.",
        "On the other hand, MN also attempts to minimize ||w|| to minimize the generalization error.",
        "Suppose Atx(y) = En=1 Atx(yi) = En=11 (yi = (t(x))i) where t((x))i is the true label of the ithsequence xi, and A0x(y) = 0(x, t(x)) – 0(x, y) where t(x) is the true label of the observation sequence x.",
        "We can get a quadratic program (QP) using a standard transformation to eliminate 7 as follows:",
        "min 2l|w|| ; (3) s.t.",
        "wTA0x(y) > Atx(y), Vx g S, Vy G E.",
        "However, the sequence data is often not separable by the defined hyperplane.",
        "In such cases, we can introduce slack variables £x which are guaranteed to be non-negative to allow some constraints.",
        "Thus the complete primal form of the optimization problem can be formulated by:",
        "s.t.",
        "wTA0x(y) > Atx(y) – Ôx, Vx G S, Vy G E.",
        "where C is called the capacity in the support vector literature and presents a way to trade-offthe training error and margin size.",
        "One should note that the number of constraints is ET=1 | Ei |, an extremely large number.",
        "And the corresponding dual formulation can be defined as:",
        "where ax (y) is a dual variable.",
        "As well as loss functions, kernels might have substantial influence on the performance of a classification system.",
        "MN is capable of incorporating many different kinds of kernel functions to reduce computations in the high-dimensional feature space H. This is sometimes referred to as the \"kernel trick\" (Scholkopf and Smola, 2002; Shawe-Taylor and Cristianini, 2004).",
        "A linear kernel can be deined as",
        "For a polynomial kernel, and for a neural kernel, where s, d, and r are coefficients in kernel functions."
      ]
    },
    {
      "heading": "3. Stochastic Gradient Descent",
      "text": [
        "For MN optimization, Taskar et al.",
        "(2003) has proposed a reparametrization of the dual variables to take advantage of the network structure of the labeling sequence problem.",
        "The dual QP is then solved using the structured sequential minimal optimization (structured SMO) analogous to the SMO used for SVMs (Platt, 1998).",
        "However, the resulting number of constraints in the QP make the structured SMO algorithm slow to converge, or even prohibitively expensive for large-scale real world problems.",
        "In this section we will present stochastic gradient descent (SGD) method, and show SGD can significantly speedup the training of MN.",
        "Recall that for MN, the goal is to find a linear hypothesis hw such that hw(x) = argmaxyes wT</>(x, y).",
        "The parameters w are learned by minimizing a regularized loss",
        "m C L(w; {(xi,yi)]f=l,C) = ^(w,£i,y)+ – ||w||.",
        "The function I measures the loss incurred in using w to predict the label of Xj.",
        "Following (Taskar et al., 2003), ^(w, xj} yi) is a variant of the hinge loss, and can be deined as follows:",
        "where e(xi,yi, y) is some non-negative measure of the error incurred in predicting y instead of yias the label of xi.",
        "We assume that e(xi, yi, y) = 0 for all i, so that no loss is incurred for correct prediction, and therefore ^(w, xi,yi) is always non-negative.",
        "This loss function corresponds to the MN approach, which explicitly penalizes training examples for which, for some y = yi, w • (0(xj, yj) - 0(xj, y)) < e(xj, yj, y).",
        "And the function L is convex in w for ^(w, xj, yj).",
        "Therefore, minimization of L can be recast as optimization of the following dual convex problem:",
        "To perform parameter estimation, we need to minimize L(w; {(xj,yj)}T=1,C).",
        "For this purpose we compute its gradient G(w):",
        "In addition to the gradient, second-order methods based on Newton steps also require computation and inversion of the Hessian H(w).",
        "Taking the gradient of Equation 12 wrt.",
        "w yields:",
        "Explicitly computing the full Hessian is time consuming.",
        "Instead we can make use of the differential to efficiently compute the product of the Hessian with a chosen vector v =: dw by forward-mode algorithmic differentiation (Pearlmutter, 1994).",
        "These Hessian-vector products can be computed along with the gradient at only 2-3 times the cost of the gradient computation alone.",
        "We denote G(w) = VwL, and each iteration of the SGD algorithm consists in drawing an example (xi, yi) at random and applying the parameter update rule (Robbins and Monroe, 1951):",
        "where n is the learning rate in the algorithm.",
        "The SGD algorithm has been shown to be fast, reliable, and less prone to reach bad local minima.",
        "In this algorithm, the weights are updated after the presentation of each example, according to the gradient of the loss function (Lecun et al., 1998).",
        "The convergence is very fast when the training examples are redundant since only a few examples are needed to perform.",
        "This algorithm can get a good estimation after considerably few iterations.",
        "The learning rate n is crucial to the speed of SGD algorithm.",
        "Ideally, each parameter weight wi should have its own learning rate ni.",
        "Because of possible correlations between input variables, the learning rate of a unit should be inversely proportional to the square root of the number of inputs to the unit.",
        "If shared weights are used, the learning rate of a weight should be inversely proportional to the square root of the number of connection sharing that weight.",
        "For one-dimensional sequence labeling task, the optimal learning rate yields the fastest convergence in the direction of highest curvature is (Bot-tou, 2004):",
        "and the maximum learning rate is nmax = 2nopt.",
        "The simple SGD update offers lots of engineering opportunities.",
        "In practice, however, at any moment during the training procedure, we can select a small subset of training examples and try various learning rates on the subset, then pick the one that most reduces the cost and use it on the full dataset.",
        "The convergence of stochastic algorithms actually has been studied for a long time in adaptive signal processing.",
        "Given a suitable choice of the learning rate nt, the standard (batch) gradient descent algorithm is known to converge to a local minimum of the cost function.",
        "However, the random noise introduced by SGD disrupts this deterministic picture and the specific study of SGD convergence usually is fairly complex (Benveniste et al., 1987).",
        "It is reported that for the convex case, if several assumptions and conditions are valid, then the SGD algorithm converges almost surely to the optimum w* .",
        "For the general case where the cost function is non-convex and has both local and global minima, if four assumptions and two learning rate assumptions hold, it is guaranteed that the gradient VwL converges almost surely to zero (Bottou, 2004).",
        "We omit the details of the convergence theorem and corresponding proofs due to space limitation.",
        "Unfortunately, many of sophisticated gradient methods are not robust to noise, and scale badly with the number of parameters.",
        "The plain SGD algorithm can be very slow to converge.",
        "Inspired by stochastic meta-descent (SMD) (Schraudolph, 1999), the convergence speed of SGD can be further improved with gradient step size adaptation by using second-order information.",
        "SMD is a highly scalable local optimizer.",
        "It shines when gradients are stochastically approximated.",
        "In SMD, the learning rate n is simultaneously",
        "Input: training set S {(xi,yi),..., (xT,yT)|; factor A; number of iterations N.",
        "Initialize: w0, v0 = 0, no.",
        "Choose a random example (xi; yi) G S Compute the gradient Vt = Gt and Htvt Set vt+i = Avt - nt • (Gt + AHtvt) Update the parameter vector:",
        "adapted via a multiplicative update with /t:",
        "where the vector v (v =: dw) captures the long-term dependencies of parameters.",
        "v can be computed by the simple iterative update:",
        "where the factor 0 < A < 1 governs the time scale over which long-term dependencies are taken into account, and Htvt can be calculated efficiently alongside the gradient by forward-mode algorithmic differentiation via Equation 14.",
        "This Hessian-vector product is computed implicitly and it is the key to SMD's efficiency.",
        "The pseudo-code for the SGD algorithm is shown in Figure 1."
      ]
    },
    {
      "heading": "4. Experiments: A Case Study of NP Chunking",
      "text": [
        "Our data comes from the CoNLL 2000 shared task (Sang and Buchholz, 2000).",
        "The dataset is divided into a standard training set of 8,936 sentences and a testing set of 2,012 sentences.",
        "This data consists of the same partitions of the Wall Street Journal corpus (WSJ) as the widely used data for NP chunking: sections 15-18 as training data (211,727 tokens) and section 20 as test data (47,377 tokens).",
        "And the annotation of the data has been derived from the WSJ corpus.",
        "wt matches [A-Z] wt matches [A-Z] + wt contains dash \"-\" or dash-based \"-based\" wt is capitalized, all-caps, single capital letter, or mixed capitalization wt contains years, year-spans or fractions wt is contained in a lexicon of words with POS p (from the Brill tagger)",
        "Table 1: Input feature template qk(x,t) for NP chunking.",
        "In this table wt is the token (word) at position t, pt is the POS tag at position t, w ranges over all words in the training data, and p ranges over all POS tags.",
        "We follow some top-performing NP chunking systems and perform holdout methodology to design features for our model, resulting in a rich feature set including POS features provided in the official CoNLL 2000 dataset (generated by the Brill tagger (Brill, 1995), with labeling accuracy of around 95-97%), some contextual and morphological features.",
        "Table 1 lists our feature set for NP chunking.",
        "We trained linear-chain conditional random fields (CRFs) (Lafferty et al., 2001) as the baseline.",
        "The well known limited memory quasi-Newton BFGS algorithm (L-BFGS) (Liu and Nocedal, 1989) was applied to learn the parameters for CRFs.",
        "To avoid over-fitting, we penalized the log-likelihood by the commonly used zero-mean Gaussian prior over the parameters.",
        "This gives us a competitive baseline CRF model for NP chunking.",
        "To make fair and accurate comparison, we used the same set of features listed in Table 1 for both MN and CRFs.",
        "All experiments were performed on the Linux platform, with a 3.2GHz Pentium 4 CPu and 4 GB of memory.",
        "Table 2: MN vs. CRFs: Performance and training time comparison for NP chunking on the CoNLL 2000 official dataset.",
        "MN was trained using the structured SMO algorithm.",
        "Table 3: MN vs. CRFs: Performance and training time comparison for NP chunking on the CoNLL 2000 official dataset.",
        "MN was trained using the SGD algorithm.",
        "Similar to other discriminative graphical models such as CRFs, the modeling flexibility of MN permits the feature functions to be complex, arbitrary, nonindependent, and overlapping features, allowing the multiple features described in Table 1 to be directly exploited.",
        "Moreover, MN is capable of incorporating multiple kernel functions (see Section 2) which allow the efficient use of high-dimensional feature spaces during the experiments.",
        "The resulting number of features is 7,835,439, and both MN and CRFs were trained to predict 47,366 tokens with 12,422 noun phrases in the testing set.",
        "For simplicity, we denote a = 0(x, y), and b = c/>(x', y'), and the linear kernel can be rewritten as n(a,b) = (a,b}%.",
        "We performed holdout methodology to find optimal values for coefficients s, d, and r in MN kernel functions.",
        "For polynomial kernels, we varied d from 2 to 4, resulting in quadratic, cubic, and biquadratic kernels, respectively.",
        "Finally, we chose optimized values: s = 1, r = 1 for polynomial kernels, and s = 0.1, r = 0 for neural kernels.",
        "The capacity C for MN was set to 1 in our experiments.",
        "Table 2 shows comparative performance and training time for MN (trained with structured SMO) and CRFs, while Table 3 shows comparative performance and training time for MN (trained with SGD) and CRFs .",
        "For MN, when trained with quadratic kernel and structured SMO, the best F-measure of 94.68 was achieved, leading to an improvement of 0.36 compared to the CRF baseline.",
        "What follows is the linear kernel that obtained 94.40 F-measure.",
        "The cubic and neural kernels obtained close performance, while the biquadratic kernel led to the worst performance.",
        "However, the structured SMO is very computationally intensive, especially for polynomial kernels.",
        "For example, CRFs converged in 352 sec-",
        "Model",
        "Training Method",
        "Kernel Function",
        "Iteration",
        "Training Time(s)",
        "P(%)",
        "R(%)",
        "Fß=i",
        "M N",
        "structured SMO",
        "linear kernel: (a,b)H",
        "100",
        "1176",
        "94.59",
        "94.22",
        "94.40",
        "M N",
        "structured SMO",
        "polynomial(quadratic): ((a,b)H + 1)polynomial(cubic): ((a,b)H + 1)polynomial(biquadratic): ((a,b)H + 1)neural kernel: tanh(0.1 • (a,b)H)",
        "100",
        "30792",
        "94.88",
        "94.49",
        "94.68",
        "M N",
        "structured SMO",
        "100",
        "30889",
        "94.47",
        "94.01",
        "94.24",
        "M N M N",
        "structured SMO structured SMO",
        "100 20",
        "31556 7395",
        "93.90 94.42",
        "93.77 94.02",
        "93.83 94.22",
        "CRFs",
        "L-BFGS",
        "100",
        "352",
        "94.55",
        "94.09",
        "94.32",
        "Model",
        "Training Method",
        "Kernel Function",
        "Iteration",
        "Training Time(s)",
        "P(%)",
        "R(%)",
        "Fß=i",
        "M N",
        "SGD",
        "linear kernel: (a,b)H",
        "100",
        "89",
        "94.58",
        "94.21",
        "94.39",
        "M N M N",
        "SGD SGD",
        "polynomial(quadratic): ((a,b)H + 1)polynomial(cubic): ((a,b)H + 1)polynomial(biquadratic): ((a,b)H + 1)neural kernel: tanh(0.1 • (a,b)H)",
        "100 100",
        "1820 1831",
        "94.89 94.47",
        "94.50 94.01",
        "94.69",
        "94.24",
        "M N M N",
        "SGD SGD",
        "100 20",
        "1857 477",
        "93.91 94.40",
        "93.76 94.01",
        "93.83 94.20",
        "CRFs",
        "L-BFGS",
        "100",
        "352",
        "94.55",
        "94.09",
        "94.32",
        "System",
        "Fß=i",
        "SVMs (polynomial kernel) (Kudo and Matsumoto, 2000)",
        "SVM combination (Kudo and Matsumoto, 2001)",
        "Generalized winnow (Zhang et al., 2002)",
        "Voted perceptron (Collins, 2002)",
        "CRFs (Sha and Pereira, 2003)",
        "Second order CRFs (McDonald et al., 2005)",
        "Chunks from the Charniak Parser (Holling-",
        "shead etal., 2005)",
        "Second order latent-dynamic CRFs + improved A* search based inference (Sun et al., 2008)",
        "93.79",
        "94.39",
        "94.38 94.09 94.38 94.29 94.20",
        "94.34",
        "Our approach",
        "94.69",
        "Figure 2: Convergence speed comparison for structured SMO and SGD algorithms.",
        "The X axis shows number of training iterations, and the Y axis shows objective function value.",
        "(a) The MN model was trained using linear kernel.",
        "(b) The MN model was trained using polynomial(quadratic) kernel.",
        "(c) The MN model was trained using neural kernel.",
        "onds, while MN (polynomial kernels) tookmore than 8.5 hours to finish training.",
        "As can be seen in Table 3, the SGD algorithm significantly accelerated the training procedure of MN without sacrificing performance.",
        "When the linear kernel was used, MN finished training in 89 seconds, more than 13 times faster than the model trained with structured SMO.",
        "And it is even much faster than the CRF model trained with L-BFGS.",
        "More importantly, SGD obtained almost the same performance as structured SMO with all MN kernel functions.",
        "Table 4 gives some representative NP chunking results for previous work and for our best model on the same dataset.",
        "These results showed that our model compares favorably with existing state-of-the-art systems .",
        "Figure 2 compares the convergence speed of structured SMO and SGD algorithms for the MN model.",
        "Linear (Figure 2 (a)), polyno-mial(quadratic) (Figure 2 (b)) and neural kernels (Figure 2 (c)) were used .",
        "We calculated objective function values during effective training iterations.",
        "It can be seen that both structured SMO and SGD algorithms converge to the same objective function value for different kernels, but SGD converges considerably faster than the structured",
        "SMO.",
        "Figure 3 (a) demonstrates the effect of training set size on performance for NP chunking.",
        "We increased the training set size from 1,000 sentences to 8,000 sentences, with an incremental step of 1,000.",
        "And the testing set was fixed to be 2,012 sentences.",
        "The MN models (with different kernels) were trained using the SGD algorithm.",
        "It is particularly interesting to know that the performance boosted for all the models when increasing the training set size.",
        "Using linear and quadratic kernels, MN model significantly and consistently outperforms the CRF model for different training set sizes.",
        "The cubic and neural kernels lead to almost the same performance for MN, which is slightly lower than the CRF baseline.",
        "As illustrated by the curves, MN (trained with quadratic kernel) achieved the best performance and larger training set size leads to better improvement for this model when compared to the CRF model, while MN (trained with biquadratic kernel) obtained the worst performance among all the models.",
        "Accordingly, Figure 3 (b) shows the impact of increasing the training set size on training time for NP chunking.",
        "Increasing training set size leads to an increase in the computational complexity of training procedure for all models.",
        "For the MN model, it is faster when trained with linear kernel than the CRF model.",
        "And the three polynomial kernels (quadratic, cubic and biquadratic) have roughly the same training time.",
        "For CRFs and (MN, neural kernel), the training time is close to each other.",
        "For example, when the training set contains 1,000 sentences, the training time for",
        "CRFs, (MN, linear kernel), (MN, quadratic kernel), (MN, cubic kernel), (MN, biquadratic kernel), and (MN, neural kernel) is 24s, 7s, 72s,",
        "of training",
        "6k 7k sentences ; 4k 5k 6k 7k Number of training sentences",
        "Figure 3: (a) Effect of training set size on performance for NP chunking.",
        "The training set size was increased from 1,000 sentences to 8,000 sentences, with an incremental step of 1,000.",
        "The testing set contains 2,012 sentences.",
        "All the MN models (with different kernels) were trained using the SGD algorithm.",
        "(b) Effect of training set size on training time for NP chunking.",
        "72s, 74s, and 30s.",
        "When trained on 8,000 sentences, the numbers become 336s, 79s, 1679s, 1689s, 1712s, and 411s, respectively."
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "The MN framework and its variants have generated much interest and great progress has been made, as evidenced by their promising results evaluated in handwritten character recognition, collective hypertext classification (Taskar et al., 2003), parsing (Taskar et al., 2004), and XML tag relabeling (Spengler, 2005).",
        "However, all the above mentioned research work used structured SMO algorithm forparameterlearning, whichcan be computationally intensive, especially for very large datasets.",
        "Recently, similar stochastic gradient methods have been applied to train log-linear models such as CRFs (Vishwanathan et al., 2006).",
        "However, the maximum margin loss has a discontinuity in its derivative, making optimization ofsuchmodels somewhat more involved than log-linear ones.",
        "We first exploit SGD method for fast parameter learning of MN and achieve state-of-the-art performance on the NP chunking task in the NLP community.",
        "Several algorithms have been proposed to train max-margin models, including cutting plane SMO (Tsochantaridis et al., 2005), exponentiated gradient (Bartlett et al., 2004; Collins et al., 2008), extragradient (Taskar et al., 2006), and subgradient (Shalev-Shwartz et al., 2007).",
        "Some methods are similar to SGD in that they all process a single training example at a time.",
        "The SGD methods directly optimize the primal problem, and at each update use a single example to approximate the gradient of the primal objective function.",
        "Some of the proposed algorithms, such as exponentiated gradient corresponds to block-coordinate descent in the dual, and uses the exact gradient with respect to the block being updated.",
        "We plan to implement and compare some of these algorithms with SGD for MN."
      ]
    },
    {
      "heading": "6. Conclusion and Future Work",
      "text": [
        "We have presented the first known empirical study on sequence labeling based on MN.",
        "We have also provided the efficient SGD algorithm and shown how it can be applied to significantly speedup the training procedure of MN.",
        "As a case study, we performed extensive experiments on standard dataset for NP chunking, showing the promising and competitiveness of our approach.",
        "Several interesting issues, such as the convergence speed of the SGD algorithm, the effect of training set size on performance for NP chunking, and the effect of training set size on training time, were also investigated in our experiments.",
        "For the future work, we plan to further the scalability and applicability of our approach and evaluate it on other large-scale real world sequence labeling tasks, such as POS tagging and NER."
      ]
    }
  ]
}
