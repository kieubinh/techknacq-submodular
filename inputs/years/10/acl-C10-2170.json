{
  "info": {
    "authors": [
      "Renxian Zhang",
      "Wenjie Li",
      "Qin Lu"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2170",
    "title": "Sentence Ordering with Event-Enriched Semantics and Two-Layered Clustering for Multi-Document News Summarization",
    "url": "https://aclweb.org/anthology/C10-2170",
    "year": 2010
  },
  "references": [
    "acl-C04-1108",
    "acl-J08-1001",
    "acl-J95-2003",
    "acl-P03-1069",
    "acl-P05-1018",
    "acl-P06-1047",
    "acl-P06-1049",
    "acl-P07-2047",
    "acl-W04-1017",
    "acl-W04-3205",
    "acl-W06-2407"
  ],
  "sections": [
    {
      "text": [
        "Renxian Zhang Wenjie Li Qin Lu",
        "{csrzhang,cswjli,csluqin}@comp.polyu.edu.hk",
        "We propose an event-enriched model to alleviate the semantic deficiency problem in the IR-style text processing and apply it to sentence ordering for multi-document news summarization.",
        "The ordering algorithm is built on event and entity coherence, both locally and globally.",
        "To accommodate the event-enriched model, a novel LSA-integrated two-layered clustering approach is adopted.",
        "The experimental result shows clear advantage of our model over event-agonistic models."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "One of the crucial steps in multi-document summarization (MDS) is information ordering, right after content selection and before sentence realization (Jurafsky and Martin, 2009:832834).",
        "Problems with this step are the culprit for much of the dissatisfaction with automatic summaries.",
        "While textual order may guide the ordering in single-document summarization, no such guidance is available for MDS ordering.",
        "A sensible solution is ordering sentences by enhancing coherence since incoherence is the source of disorder.",
        "Recent researches in this direction mostly focus on local coherence by studying lexical cohesion (Conroy et al., 2006) or entity overlap and transition (Barzilay and Lapata, 2008).",
        "But global coherence, i.e., coherence between sentence groups with the whole text in view, is largely unaccounted for and few efforts are made at levels higher than entity or word in measuring sentence coherence.",
        "On the other hand, event as a high-level construct has proved useful in MDS content selection (Filatova and Hatzivassiloglou, 2004;",
        "Li et al., 2006).",
        "But the potential of event in summarization has not been fully gauged and few publications report using event in MDS information ordering.",
        "We will argue that event is instrumental for MDS information ordering, especially multi-document news summarization (MDNS).",
        "Ordering algorithms based on event and entity information outperform those based only on entity information.",
        "After related works are surveyed in section 2, we will discuss in section 3 the problem of semantic deficiency in IR-based text processing, which motivates building event information into sentence representation.",
        "The details of such representation are provided in section 4.",
        "In section 5, we will explicate the ordering algorithms, including layered clustering and cluster-based ordering.",
        "The performance of the event-enriched model will be extensively evaluated in section 6.",
        "Section 7 will conclude the work with directions to future work."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "In MDS, information ordering is often realized on the sentence level and treated as a coherence enhancement task.",
        "A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003).",
        "A different way to capture local coherence in sentence ordering is the Centering Theory (CT, Grosz et al.",
        "1995)-inspired entity-transition approach, advocated by Barzilay and Lapata syntactic roles played by entities and transitions between these syntactic roles underlie the coherence patterns between sentences and in the whole text.",
        "An entity-parsed corpus can be used to train a model that prefers the sentence orderings that comply with the optimal entity transition patterns.",
        "Another important clue to sentence ordering is the sentence positional information in a source document, or \"precedence relation\", which is utilized by Okazaki et al.",
        "(2004) in combination with topical clustering.",
        "Those works are all relevant to the current work because we seek ordering clues from chronological order, lexical cohesion, entity transition, and sentence precedence.",
        "But we also add an important member to the panoply - event.",
        "Despite its intuitive and conceptual appeal, event is not as extensively used in summarization as term or entity.",
        "Filatova and Hatzivassiloglou (2004) use \"atomic events\" as conceptual representations in MDS content selection, followed by Li et al.",
        "(2006) who treat event terms and named entities as graph nodes in their PageRank algorithm.",
        "Yoshioka and Haraguchi (2004) report an event reference-based approach to MDS content selection for Japanese articles.",
        "Although \"sentence reordering\" is a component of their model, it relies merely on textual and chronological order.",
        "Few published works report using event information in MDS sentence ordering.",
        "Our work will represent text content at two levels: event vectors and sentence vectors.",
        "This is close in spirit to Bromberg's (2006) enriched LSA-coherence model, where both sentence and word vectors are used to compute a centroid as the topic of the text."
      ]
    },
    {
      "heading": "3. Semantic Deficiency in IR-Style Text Processing",
      "text": [
        "As automatic summarization traces its root to Information Retrieval (IR), it inherits the vector space model (VSM) of text representation, according to which a sentence is treated as a bag of words or stoplist-filtered terms.",
        "The order or relation among the terms is ignored.",
        "For example,",
        "1a) The storm killed 120,000 people in Jamaica and five in the Dominican Republic before moving west to Mexico.",
        "1b) [Dominican, Mexico, Jamaica, Republic, five, kill, move, people, storm, west]",
        "1c) [Dominican Republic, Mexico, Jamaica, people, storm]",
        "1b) and 1c) are the term-based and entity-based representations of 1a) respectively.",
        "They only indicate what the sentence is about (i.e., some happening, probably a storm, in some place that affects people), but \"aboutness\" is a far cry from informativeness.",
        "For instance, no message about \"people in which place, Mexico or Jamaica, are affected\" or \"what moves to where\" can be gleaned from 1b) although such message is clearly conveyed in 1a).",
        "In other words, the IR-style text representation is semantically deficient.",
        "We argue that a natural text, especially a news article, is not only about somebody or something.",
        "It also tells what happened to somebody or something in a temporal-spatial manner.",
        "A natural approach to meeting the \"what happened\" requirement is to introduce event."
      ]
    },
    {
      "heading": "4. Event-Enriched SentenceRepresentation",
      "text": [
        "In summarization, an event is an activity or episode associated with participants, time, place, and manner.",
        "Conceptually, event bridges sentence and term/entity and partially fills the semantic gap in the sentence representation.",
        "Following (Li et al.",
        "2006), we define an event E as a structured semantic unit consisting of one event term Term(E) and a set of event entities Entity(E).",
        "In the news domain, event terms are typically action verbs or deverbal nouns.",
        "Light verbs such as \"take\", \"give\", etc.",
        "(Tan et al., 2006) are removed.",
        "Event entities include named entities and high-frequency entities.",
        "Named entities denote people, locations, organizations, dates, etc.",
        "High-frequency entities are common nouns or NPs that frequently participate in news events.",
        "Filatova and Hatzivassiloglou (2004) take the top 10 most frequent entities and Li et al.",
        "(2006) take the entities with frequency > 10.",
        "Rather than using a fixed threshold, we reformulate \"high-frequency\" as relative statistics based on (assumed) Gaussian distribution of the entities and consider those with z-score > 1 as candidate event entities.",
        "Event extraction begins with shallow parsing and named entity recognition, analyzing each sentence S into ordered lists of event terms {t1, t2, ...}.",
        "Low-frequency common entities are removed.",
        "If a noun is decided to be an event term, it cannot be (the head noun of) an entity.",
        "The next step is to identify events with event terms and entities.",
        "Filatova and Hatzivassiloglou (2003) treat events as triplets with two event entities sandwiching one connector (event term).",
        "But the number restriction on entities is counterintuitive and is dropped in our method.",
        "We first identify n + 1 Segi segmented by n event terms tj.",
        "Seg0 Seg.",
        "Segj Segn",
        "For each tj, the corresponding event Ej are extracted by taking tj and the event entities in its nearest entity-containing Segp and Segq.",
        "where p = argmaxosiS]_1Entity(Segi) ^0 and q = argminj+lsiSnEntity(Segi) ^0 if such p and q exist.",
        "1d) is the event-extracted result of 1a).",
        "1d) {[killed, [storm, people, Jamaica, Dominican Republic]], [moving, [people, Jamaica, Dominican Republic, west, Mexico]]}",
        "From this representation, it is easy to identify the two events in sentence 1a) led by the event terms \"killed\" and \"moving\".",
        "Unlike the triplets (two named entities and one connector) in (Filatova and Hatzivassiloglou 2003), an event in our model can have an unlimited number of event entities, as is often the real case.",
        "Moreover, we can tell that the \"killing\" involves \"people\", \"storm\", \"Jamaica\", etc.",
        "and the \"moving\" involves \"Jamaica\", \"Dominique Republic\", etc.",
        "The shallow parsing-based approach is admittedly coarse-grade (e.g., \"storm\" is missing from the \"moving\" event), but the extracted event-enriched representations help to alleviate the semantic deficiency problem in IR.",
        "The relations between two events include event term relation and event entity relation.",
        "Two events are similar if their event terms are similar and/or their event entities are similar.",
        "Such similarities are in turn defined on the word level.",
        "For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by",
        "VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu count in all the verb relations except antonymy because considering two antonymous verbs as similar is counterintuitive.",
        "The other four relations - similarity, strength, enablement, before - are all considered in our measurement of verb similarity.",
        "If we denote the normalized score of two verbs on relation i as VOi(V1, V2) with i = 1, 2, 3, 4 corresponding to the above four relations, the term similarity of two events Ht(E1, E2) is defined as in Eq.",
        "2, where s is a small number to suppress zeroes.",
        "s = 0.01 if VOi(Vb V2) = 1 and otherwise s = 0.",
        "E2) = vlTermE), TermE)) = 1 -nf=i(1 -VOi (Term(E1), Term(E2)) + e) (Eq.",
        "2) Entity similarity is measured by the shared entities between two events.",
        "Li et al.",
        "(2006) define entity similarity as the number of shared entities, which may unfairly assign high scores to events with many entities in our model.",
        "So we decide to use the normalized result as shown in Eq.",
        "3, where /«e(E1, E2) denotes the event entity-based similarity between events E1 and E2.",
        "H(E1, E2), the score of event similarity, is a linear combination of //t(E1, E2) and /«e(E1, E2).",
        "In this work, we introduce events as a middle-layer representation between words and sentences under the assumptions that 1) events are widely distributed in a text and that 2) they are natural clusters of salient information in a text.",
        "They guarantee the relevance of event to our task - summaries are condensed collections of salient information in source documents.",
        "In order to confirm them, we scan the whole dataset in our experiment, which consists of 42 200w human extracts and 39 400w human extracts for the DUC 02 multi-document extract task.",
        "Detailed information about the dataset can be found in Section 6.",
        "Table 1 lists the statistics.",
        "200w",
        "400w",
        "200w + 400w",
        "Source Docs",
        "Entity/Sent",
        "8.78",
        "8.48",
        "8.47",
        "6.01",
        "Entity/Word",
        "0.34",
        "0.33",
        "0.33",
        "0.30",
        "Event/Sent",
        "2.43",
        "2.26",
        "2.28",
        "1.42",
        "dimensions of an event vector in an eu-by-event matrix E = as shown in Figure 2.",
        "There are on average 1.42 events per sentence in the source documents, and more than 70% of all the sentences contain events.",
        "The high event density confirms our first assumption about the distribution of events.",
        "For the 200w+400w category consisting of all the human-selected sentences, there are on average 2.28 events per sentence, a 60% increase from the same ratio in the source documents.",
        "The proportion of event-containing sentences reaches 84.6%, 13% higher than that in the source documents.",
        "Such is evidence that events count into the extract-worthiness of sentences, which confirms our second assumption about the relevance of events to summarization.",
        "The data also show higher entity density in the extracts than in the source documents.",
        "As entities are still reliable and domain-independent clues of salient content, we will consider both event and entity in the following ordering algorithm."
      ]
    },
    {
      "heading": "5. MDS Sentence Ordering with Event and Entity Coherence",
      "text": [
        "In this section, we discuss how event can facilitate MDS sentence ordering with layered clustering on the event and sentence levels and then how event and entity information can be integrated in a coherence-based algorithm to order sentences based on sentence clusters.",
        "After sentences are represented as collections of events, we need to vectorize events and sentences to facilitate clustering and cluster-based sentence ordering.",
        "For a document set, event vectorization begins with aggregating all the event terms and entities in a set of event units (eu).",
        "Given m distinct event terms, n distinct named entities, and p distinct high-frequency common entities, the m + n + peu's are a concatenation of the event terms and entities such that eui is an event term for 1 < i < m, a named entity for m + 1 < i < m + n, and a high-frequency entity for m + n + 1 < i < m + n + p.The eu 's define the m + n + p",
        "We further define EntityN(Ej) and Entity^Ej) to be the set of named entities and set of high-frequency entities of Ej.Then,",
        "T.eeEntityN(Ey) Vn(eUi,e)",
        "\\EntityN(E0\\ T.eeEntityH(Ey) Vft(eUj,e)",
        "In Eq.",
        "5, [it(w1, w2) is defined as in Eq.",
        "2.",
        "Both the entity-based vn(w1, w2)and vh(w1, w2) are measured in terms of total equivalence (identity) and partial equivalence.",
        "For named entities, partial equivalence applies to structural subsumption (e.g., \"Britain\" and \"Great Britain\") and hypernymy/holonymy (e.g., \"South Africa\" and \"Zambia\").",
        "For common entities, it applies to synonymy (e.g., \"security\" and \"safety\").",
        "Partial equivalence is considered because of the lexical variations frequently employed in journalist writing.",
        "The named entity scores are doubled because they represent the essential elements of a news story.",
        "Since the events are represented as vectors, sentence vectorization based on events is not as straightforward as on entities or terms.",
        "In this work we propose a novel approach of two-layered clustering for the purpose.",
        "The basic idea is clustering events at the first layer and then using event clusters as a feature to vectorize and cluster sentences at the second are in a hypernymy / holonymy relationship 0 otherwise (Eq.",
        "6) layer.",
        "Hard clustering of events, such as K-means, not only results in binary values in event vectors and data sparseness but also is inappropriate.",
        "For example, if EC1 clusters events all with event terms similar to t* and EC2clusters events all with event entity sets similar to e* (a set), what about event {t*, e*}?",
        "Assigning it to either EC1 or EC2 is problematic as it is partially similar to both.",
        "So we decide to do soft clustering at the first layer.",
        "Event/Word",
        "0.09",
        "0.09",
        "0.09",
        "0.07",
        "Sents with events/Sents",
        "86.9%",
        "85.1%",
        "84.6%",
        "71.3%",
        "eu1",
        "f",
        "elq",
        "eum",
        "emi •",
        "... 4",
        "eum+n",
        "^m+n,l",
        "^m+n,q",
        "eum+n+p",
        "-^m+n+p,l",
        "^m+n+p,q-",
        "A well-studied soft clustering technique is the Expectation-Maximization (EM) algorithm which iteratively estimates the unknown parameters in a probability mixture model.",
        "We assume a Gaussian mixture model for the q event vectors V1, V2,-.., Vq, with hidden variables H{ initial means Mi, priors ni, and covariance matrix Ci.",
        "The E-step is to calculate the hidden variables //[ for each Vt and the M-step re-estimates the new priors ni , means Mi, and covariance matrix Ci.",
        "We iterate the two steps until the log-likelihood converges within a threshold = 10-6.",
        "The performance of the EM algorithm is sensitive to the initial means, which are precomputed by a conventional K-means.",
        "In a preliminary study, we found that the event vectors display pronounced sparseness.",
        "A solution to this problem in an effort to leverage the latent \"event topics\" among eu's is the Latent Semantic Analysis (LSA, Landauer and Dumais, 1997) approach.",
        "We apply LSA-style dimensionality reduction to the eu-by-event matrix E by doing Singular Value Decomposition (SVD).",
        "A problem is with the number h of the largest singular values, which affects the performance of dimensionality reduction.",
        "In this work, we adopt a utility-based metric to find the best h* by maximizing intra-cluster similarity (c£h) and minimizing inter-cluster similarity (!Fh) corresponding to the h-dimensionality reduction",
        "0h is defined as the mean of average cluster similarities measured by cosine distance and Whis the mean of cluster centroid similarities.",
        "Because the EM clustering assigns a probability to every event vector, we also take those probabilities into account when calculating 0hand Wh.",
        "Based on the EM clustering of events, we vectorize a sentence by summing up the probabilities of its constituent event vectors over all event clusters (ECs) and obtaining an EC-by-sentence (Sn) matrix S = [sy].",
        "Sij = ZsrESj p((£V|£Ci)) where £r is Er's vector.",
        "At the sentence layer, hard clustering is sufficient because we need definitive, not probabilistic, membership information for the next step - sentence ordering.",
        "We use K-means for the purpose.",
        "The LSA-style dimensionality reduction is still in order as possible performance gain is expected from the discovery of latent EC \"topics\".",
        "The decision of the best dimensionality is the same as before, except that no probabilities are included.",
        "Our ordering algorithm is based on sentence clusters, which is designed on the observation that human writers and summarizers organize sentences by blocks (paragraphs).",
        "Sentences within a block are conceptually close to each other and adjacent sentences cohere with each other.",
        "Local coherence is thus realized within blocks.",
        "On the other hand, blocks are not randomly ordered.",
        "Two blocks are put next to each other if their contents are close enough to ensure text-level coherence.",
        "So text-level, or global coherence is realized among blocks.",
        "We believe in MDNS, the block-style organization is a sensible strategy taken by human extractors to sort sentences from different sources.",
        "Sentence clusters are simulations of such blocks and our ordering algorithm will be based on local coherence and global coherence described above.",
        "First we have to pinpoint the leading sentence for an extract.",
        "Using the heuristic of time and textual precedence, we first generate a set of possible leading sentences L = as the intersection of the document-leading extract sentence set LDoc and the time-leading sentence set LTime.",
        "Note that |LDoc| = the number of documents, LTime is in fact a sentence collection of time-leading documents, and LDoc D LTime ^ 0.",
        "If L is a singleton, finding the leading sentence SL is trivial.",
        "If not, SL is decided to be the sentence in L most similar to all the other sentences in the extract sentence set P so that it qualifies as a good topic sentence.",
        "Sl = argmaxL.£i IlUep\\{Li}Simß+v(Li,L') (Eq.",
        "9) where Simß+v(S1,S2) is the similarity between S1and S2 in terms of their event similarity //(S1, S2) and entity similarity v(S1, S2).",
        "/«(S1, S2) is an extended version of /«(E1, E2) (Eq.",
        "4) by averaging the /ut(Eh Ej) and //e(Ei, Ej) for all (Ei, Ej) pairs in S1 xS2.",
        "where Event(S) is the set of all events in S. Next, v(Sb S2) is the cosine similarity between their entity vectors S1 and S2 with entity weights constructed according to Eq.",
        "6 and 7.",
        "Then,",
        "After the leading sentence is determined, we identify the leading cluster it belongs to and our local coherence-based ordering starts with this cluster.",
        "We adopt a greedy algorithm, which selects each time from the unordered sentence set a sentence that best coheres with the sentence just selected, called anchor sentence.",
        "Matching each candidate sentence with the anchor sentence only in terms of Simß+V would assume that the sentences are isolated and decontextualized.",
        "But the anchor sentence did not come from nowhere and in order to find its best successor, we should also seek clues from its source context, which is inspired by the \"sentence precedence\" by Okazaki et al.",
        "(2004).",
        "More formally, given an anchor sentence Si at the end of the ordered sentence list, we select the next best sentence Si+1 according to their associative similarity and substitutive similarity, two crucial measures invented by us.",
        "Associative similarity SimASS(Si, Sj) measures how Si and Sj associate with each other in terms of their event and entity coherence, which almost is Simß+v(Si,Sj).",
        "But to better capture the transition between entities and the flow of topic, we also consider a topic-continuity score tc(Si, Sj) according to the Centering Theory.",
        "If the topic continuity is measured in terms of entity change, local coherence can be captured by the centering transitions (CS and CP) in adjacent sentences.",
        "Based on (Taboada and Wiesemann, 2009), we assign 0.2 to the Establish and Continue transitions, 0.1 to Smooth Shift and Retain, and 0 to other centering transitions.",
        "Since tc(Si, Sj) only applies to entities, it is treated as a bonus affiliated to v(Si, Sj).",
        "Substitutive similarity accommodates what we earlier emphasized about the \"source context\" of the extracted sentences by measuring to what degree Si and Sj resemble each other's relevant source context.",
        "More formally, let LC(Si) and RC(Si) be the left and right source contexts of Si respectively, and the substitutive similarity SimSUB(Si, Sj) is defined as follows.",
        "In this work, we simply take LC(Si) and RC(Si) to be the left adjacent sentence and right adjacent sentence of Si in the source document.",
        "Note that tc(Si, Sj) does not apply here.",
        "In view of the chronological order widely accepted in MDS ordering, a time penalty, tp(Si, Sj), is used to discount the score by 0.8 if Si's document date is later than Sj's document date.",
        "Finally, Eq.",
        "14 summarizes our intra-cluster ordering method in a sentence cluster SCk.",
        "After all the sentences in the current sentence cluster are ordered, we move on by considering the similarity of sentence clusters.",
        "Given a processed sentence cluster SCi, the next best sentence cluster SCi+1 is the one that maximizes the cluster similarity SimCLU(SCi, SCj)among the set of all clusters U.",
        "Since clusters are collections of sentences, their similarity is the mean of cross-cluster pairwise sentence similarities, each calculated according to Eq.",
        "14.",
        "Eq.",
        "15 shows how SCi+1 is computed.",
        "This is how we incorporate (block-style) global coherence into MDS sentence ordering.",
        "Starting from the second chosen sentence cluster, we choose the first sentence in the current cluster with reference to the last sentence in the previous processed cluster and apply Eq.",
        "14.",
        "We continue the whole process until all the extract sentences are ordered."
      ]
    },
    {
      "heading": "6. Evaluation",
      "text": [
        "In this section, we report the experimental result on the DUC 02 dataset.",
        "We use the dataset of the DUC 02 summarization track for MDS because it includes an extraction task for which model extracts are provided.",
        "For every document set, 2 model extracts are provided each for the 200w and 400w length categories.",
        "We use 1 randomly chosen model extract per document set per length category as the gold standard.",
        "We intended to use all the 59 document sets on DUC 02 but found that for some categories, both model extracts contain material from sections such as the title, lead, or even byline.",
        "Those extracts are incompatible with our design tailored for news body extracts.",
        "Therefore we have to filter them and retain only those extracts with all units selected from the news body.",
        "As a result, we collect 42 200w extracts and 39 400w extracts as our experimental dataset.",
        "We evaluate the role played by various key elements in our approach, including event, topic continuity, time penalty, and LSA-style dimensionality reduction.",
        "In addition, we produce a random ordering and a baseline ordering according to chronological and textual order only.",
        "Table 2 lists the 9 peer orderings to be evaluated, with their codes.",
        "Baseline (time order + textual order)",
        "Entity only (no LSA) Event only (no LSA)_ Entity + Event - topic continuity (no LSA) Entity + Event - time penalty (no LSA) Entity + Event (event clustering LSA)_",
        "Entity + Event (event + sentence clustering LSA) Table 2.",
        "Peer Orderings",
        "A popular metric used in sequence evaluation is Kendall's x (Lapata, 2006), which measures ordering differences in terms of the number of adjacent sentence inversions necessary to convert a test ordering to the reference ordering.",
        "x = 4m/(n(n - 1)) (Eq.",
        "16) where m is the number of inversions described above and n is the total number of sentences.",
        "The second metric we use is the Average Continuity (AC) developed by Bollegala et al.",
        "(2006), which captures the intuition that the ordering quality can be estimated by the number of correctly arranged continuous sentences.",
        "where k is the maximum number of continuous sentences, s is a small value in case Pn = 1.",
        "Pn, the proportion of continuous sentences of length n in an ordering, is defined as m/(N - n + 1) where m is the number of continuous sentences of length n in both the test and reference orderings and Nis the total number of sentences.",
        "We set k = 4 and s = 0.01.",
        "We empirically determine all the parameters (a) and produce all the peer orderings.",
        "Table 3 lists the result, where we also show the statistical significance between the full model peer ordering \"I\" and all other versions, marked by *",
        "Almost all versions with entity and event information outperform the baseline.",
        "The LSA-style dimensionality reduction proves effective for our task, as the full model (Peer I) ranks first and significantly beats versions without event information, topic continuity, or LSA.",
        "Applying LSA to both event and sentence clustering is better than applying it only to event clustering (Peer H), which produces unstable results and is sometimes outperformed by no-LSA versions (Peer G).",
        "Event (Peer D) proves to be more valuable than entity (Peer C) as the event-only versions outperform the entity-only version in all categories, which is predicable because events",
        "Peer Code",
        "200w",
        "400w",
        "Kendall's z",
        "AC",
        "Kendall's z",
        "AC",
        "A",
        "0.014**",
        "0.009**",
        "-0.019**",
        "0.004**",
        "B",
        "0.387",
        "0.151*",
        "0.259**",
        "0.151*",
        "C",
        "0.369*",
        "0.128*",
        "0.264*",
        "0.156*",
        "D",
        "0.380",
        "0.163",
        "0.270*",
        "0.158*",
        "E",
        "0.375*",
        "0.156*",
        "0.267*",
        "0.157*",
        "F",
        "0.388",
        "0.159*",
        "0.264*",
        "0.157*",
        "G",
        "0.385",
        "0.158*",
        "0.269*",
        "0.162",
        "H",
        "0.384",
        "0.164",
        "0.292*",
        "0.170",
        "I",
        "0.395",
        "0.170",
        "0.350",
        "0.176",
        "1) Thursday's acquittals in the McMartin Pre-School molestation rase outraged perents who said prosecutors botched it, while those on the defense side proclaimed a triumph of justice over hysteria and hype.",
        "2) Originally, there were seven defendants, including Raymond Buckey's sister, Peggy Ann Buckey, and Virginia McMartin, the founder of the school, mother of Mrs. Buckey and grandmother of Raymond Buckey.",
        "3) Seven jurors who spoke with reporters in a joint news conference after acquitting Raymond Buckey and his mother, Peggy McMartin Buckey, on 52 molestation charges Thursday said they felt some children who testified may have been molested _ but not at the family-run McMartin Pre-School.",
        "4) \"The children were never allowed to say in their own words what happened to them,'' said juror John Breese.",
        "5) Ray Buckey and his mother, Peggy McMartin Buckey, were found not guilty Thursday of molesting children at the family-run McMartin Pre-School in Manhattan Beach, a verdict which brought to a close the longest and costliest criminal trial in history .",
        "6) As it becomes apparent that McMartin cases will stretch out for years to come, parents and the former criminal defendants alike are trying to resign themselves to the inevitability that the matter may be one they can never leave behind.",
        "are high-level constructs that incorporate most of the document-level important entities.",
        "When entity is used, extra bonus can be gained from topic continuity concerns from CT (Peer E vs.",
        "Peer G) because the centering transition effectively captures the coherence pattern between adjacent sentences.",
        "The effect of the chronological order seems less clear (Peer F vs. P) as removing it hurts longer extracts rather than short extracts.",
        "Therefore chronological clues are more valuable for arranging more sentences from the same source document.",
        "Our ordering algorithm achieves even better result with long extracts because the importance of order and coherence grows with text length.",
        "Measured by Kendall's x, the full model ordering in the 400w category is significantly better than all other orderings.",
        "For a qualitative evaluation, we select the 200w extract d080ae and list all the sentences in Figure 4.",
        "The event terms are boldfaced and the event entities are underlined.",
        "Limited by space, let's focus on the baseline model versions (3 5 4 2 1 6).",
        "The news extract is about the acquitting of child molesters.",
        "Both the \"acquitting\" and \"molesting\" events are found in 1) and 3) but only the latter qualifies as the topic sentence because it contains important event entities.",
        "Choosing 3) instead of 1) as the leading sentence shows the advantage of our event-enriched model over the baseline.",
        "The same choice is made by the entity-only version because 3) happens to be also entity-intensive.",
        "In order to see the advantage of the full model over the entity-only model, let's consider 2) and 4).",
        "2) is chosen by the entity-only model after 5) because of the heavy entity overlap between 5) and 2).",
        "But semantically, 2) is not as close to 5) as 4) because only 4) contains entities for both the \"acquitting\" (\"juror\") and \"molesting\" (\"children\") events and intuitively, 4) continues the main trial-acquittal event topic but 2) supplies only secondary information.",
        "We examined the sentence clusters before the ordering and found that 3), 5), and 4) are clustered together only by the full model, leading to better coherence, locally and globally."
      ]
    },
    {
      "heading": "7. Conclusion and Future Work",
      "text": [
        "We set out by realizing the semantic deficiency of IR and propose a low-cost approach of building event semantics into sentence representation.",
        "Event extraction relies on shallow parsing and external knowledge sources.",
        "Then we propose a novel approach of two-layered clustering to use event information, coupled with LSA-style dimensionality reduction.",
        "MDS sentence ordering is guided by local and global coherence to simulate the block-style writing and is realized by a greedy algorithm.",
        "The evaluation shows clear advantage of our event-enriched model over baseline and event-agonistic models, quantitatively and qualitatively.",
        "The extraction approach can be refined by deep parsing and rich verb (frame) semantics.",
        "In a follow-up project, we will expand our dataset and experiment with more data and incorporate human evaluation in comparative tasks.",
        "Acknowledgment",
        "The work described in this paper was partially supported by a grant from the HK RGC (Project Number: PolyU5217/07E)."
      ]
    }
  ]
}
