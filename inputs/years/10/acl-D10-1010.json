{
  "info": {
    "authors": [
      "Razvan C. Bunescu",
      "Yunfeng Huang"
    ],
    "book": "EMNLP",
    "id": "acl-D10-1010",
    "title": "Learning the Relative Usefulness of Questions in Community QA",
    "url": "https://aclweb.org/anthology/D10-1010",
    "year": 2010
  },
  "references": [
    "acl-C04-1051",
    "acl-I05-5002",
    "acl-P02-1005",
    "acl-P08-1019",
    "acl-P94-1019",
    "acl-W03-1605",
    "acl-W08-0906"
  ],
  "sections": [
    {
      "text": [
        "Razvan Bunescu",
        "School ofEECS Ohio University Athens, OH 43201, USA",
        "We present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to a new, unanswered reference question.",
        "The ranking model is trained on a collection of question groups manually annotated with a partial order relation reflecting the relative utility of questions inside each group.",
        "Based on a set of meaning and structure aware features, the new ranking model is able to substantially outperform more straightforward, unsupervised similarity measures."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Open domain Question Answering (QA) is one of the most complex and challenging tasks in natural language processing.",
        "In general, a question answering system may need to integrate knowledge coming from a wide variety of linguistic processing tasks such as syntactic parsing, semantic role labeling, named entity recognition, and anaphora resolution (Prager, 2006).",
        "State of the art implementations of these linguistic analysis tasks are still limited in their performance, with errors that compound and propagate into the final performance of the QA system (Moldovan et al., 2002).",
        "Consequently, the performance of open domain QA systems has yet to arrive at a level at which it would become a feasible alternative to the current paradigms for information access based on keyword searches.",
        "Recently, community-driven QA sites such as Yahoo!",
        "Answers and WikiAnswers have established a new approach to question answering that shifts the inherent complexity of open domain QA from the computer system to volunteer contributors.",
        "The computer is no longer required to perform a deep linguistic analysis of questions and generate corresponding answers, and instead acts as a mediator between users submitting questions and volunteers providing the answers.",
        "An important objective in community QA is to minimize the time elapsed between the submission of questions by users and the subsequent posting of answers by volunteer contributors.",
        "One useful strategy for minimizing the response latency is to search the QA repository for similar questions that have already been answered, and provide the corresponding ranked list of answers, if such a question is found.",
        "The success of this approach depends on the definition and implementation of the question-to-question similarity function.",
        "In the simplest solution, the system searches for previously answered questions based on exact string matching with the reference question.",
        "Alternatively, sites such as WikiAnswers allow the users to mark questions they think are rephrasings (\"alternate wordings\", or paraphrases) of existing questions.",
        "These question clusters are then taken into account when performing exact string matching, therefore increasing the likelihood of finding previously answered questions that are semantically equivalent to the reference question.",
        "In order to lessen the amount of work required from the contributors, an alternative approach is to build a system that automatically finds rephrasings of questions, especially since question rephrasing seems to be computationally less demanding than question answering.",
        "According to previous work in this domain, a question is considered a rephrasing of a reference question Q0 if it uses an alternate wording to express an identical information need.",
        "For example, Qo and Qi below are rephrasings of each other, and consequently they are expected to have the same answer.",
        "Q0 What should I feed my turtle?",
        "Q1 What do I feed my pet turtle?",
        "Paraphrasings of a new question cannot always be found in the community QA repository.",
        "We believe that computing a ranked list of existing questions that at least partially address the original information need could also be useful to the user, at least until other users volunteer to give an exact answer to the original, unanswered reference question.",
        "For example, in the absence of any additional information about the reference question Q0, the expected answers to questions Q2 and Q3 below may be seen as partially overlapping in information content with the expected answer for the reference question Q0.",
        "An answer to question Q4, on the other hand, is less likely to benefit the user, even though it has a significant lexical overlap with the reference question.",
        "Q2 What kind of fish should I feed my turtle?",
        "Q3 What do you feed a turtle that is the size of a quarter?",
        "Q4 What kind of food should I feed a turtle dove?",
        "In this paper, we propose a supervised learning approach to the question ranking problem, a generalization of the question paraphrasing problem in which questions are ranked in a partial order based on the relative information overlap between their expected answers and the expected answer of the reference question.",
        "Underlying the question ranking task is the expectation that the user who submits a reference question will find the answers of the highly ranked questions to be more useful than the answers associated with the lower ranked questions.",
        "For the reference question Q0 above, the learned ranking model is expected to produce a partial order in which Q1 is ranked higher than Q2, Q3 and Q4, whereas Q2 and Q3 are ranked higher than Q4."
      ]
    },
    {
      "heading": "2. Partially Ordered Datasets for Question Ranking",
      "text": [
        "In order to enable the evaluation of question ranking approaches, we have previously created a dataset of 60 groups of questions (Bunescu and Huang, 2010b).",
        "Each group consists of a reference question (e.g. Q0 above) that is associated with a partially ordered set of questions (e.g. Q1 to Q4 above).",
        "For each reference questions, its corresponding partially ordered set is created from questions in Yahoo!",
        "Answers and other online repositories that have a high cosine similarity with the reference question.",
        "Out of the 26 top categories in Yahoo!",
        "Answers, the 60 reference questions span a diverse set of categories.",
        "Figure 1 lists the 20 categories covered, where each category is shown with the number of corresponding reference questions between parentheses.",
        "Inside each group, the questions are manually annotated with a partial order relation, according to their utility with respect to the reference question.",
        "We use the notation (Qj >- Qj |Qr) to encode the fact that question Qi is more useful than question Qj with respect to the reference question Qr.",
        "Similarly, (Qi = Qj ) will be used to express the fact that questions Qi and Qj are reformulations of each other (the reformulation relation is independent of the reference question).",
        "The partial ordering among the questions Q0 to Q4 above can therefore be expressed concisely as follows: (Q0 = Q1), (Q1 > Q2 Q0), (Qi y Q3IQ0), (Q2 y Q4IQ0), (Q3 y Q4IQ0).",
        "Note that we do not explicitly annotate the relation (Q1 y Q4|Q0), since it can be inferred based on the transitivity of the more useful than relation:",
        "(Qi y Q2IQ0WQ2 y Q4IQ0) (Qi y Q4IQ0).",
        "Reference question (Qr) Q5 What's a nice summer camp to go to in Florida?",
        "Paraphrasing questions (P) Q6 What camps are good for a vacation during the summer in FL?",
        "Q7 What summer camps in FL do you recommend?",
        "Useful questions (U) Q8 Does anyone know a good art summer camp to go to in FL?",
        "Q9 Are there any good artsy camps for girls in FL?",
        "Q10 What are some summer camps for like singing in Florida?",
        "Q11 What is a good cooking summer camp in FL?",
        "Q12 Do you know of any summer camps in Tampa, FL?",
        "Q13 What is a good summer camp in Sarasota FL for a 12 year old?",
        "Q14 Can you please help me find a surfing summer camp for beginners in Treasure Coast, FL?",
        "Q15 Are there any acting summer camps and/or workshops in the Orlando, FL area?",
        "Q16 Does anyone know any volleyball camps in Miramar, FL?",
        "Q17 Does anyone know about any cool science camps in Miami?",
        "Q18 What's a good summer camp you've ever been to?",
        "Neutral questions (N) Q19 What's a good summer camp in Canada?",
        "Q20 What's the summer like in Florida?",
        "Also note that no relation is specified between Q2and Q3, and similarly no relation can be inferred between these two questions.",
        "This reflects our belief that, in the absence of any additional information regarding the user or the \"turtle\" referenced in Q0, we cannot compare questions Q2 and Q3 in terms of their usefulness with respect to Q0.",
        "Table 1 shows another reference question Q5 from our dataset, together with its annotated group of questions Q6 to Q20.",
        "In order to make the annotation process easier and reproducible, we have divided it into two levels of annotation.",
        "During the first annotation stage, each question group is partitioned manually into 3 subgroups of questions:",
        "• P is the set of paraphrasing questions.",
        "• U is the set of useful questions.",
        "• N is the set of neutral questions.",
        "A question is deemed useful if its expected answer may overlap in information content with the expected answer of the reference question.",
        "The expected answer of a neutral question, on the other hand, should be irrelevant with respect to the reference question.",
        "Let Qr be the reference question, Qp G P a paraphrasing question, Qu G U a useful question, and Qn G N a neutral question.",
        "Then the following relations are assumed to hold among these questions:",
        "1.",
        "(QP y QuIQr): a paraphrasing question is more useful than a useful question.",
        "2.",
        "(Qu y QnIQr): a useful question is more useful than a neutral question.",
        "Note that as long as these relations hold between the 3 types of questions, the names of the subgroups and their definitions are irrelevant with respect to the implied set of more useful than relations, since only the implied ternary relations will be used for training and evaluating question ranking approaches.",
        "We also assume that, by transitivity, the following ternary relations also hold: (Qp y QnIQr), i.e. a paraphrasing question is more useful than a neutral question.",
        "Furthermore, if QP1 , QP2 G P are two paraphrasing questions, this implies (QP1 = QP2 Q).",
        "For the vast majority of questions, the first annotation stage is straightforward and non-controversial.",
        "In the second annotation stage, we perform a finer annotation of relations between questions in the middle group U.",
        "Table 1 shows two such relations (using indentation): (Q8 y Q9IQ5) and (Q8 y Q10IQ5).",
        "Question Q8 would have been a rephrasing of the reference question, were it not for the noun \"art\" modifying the focus noun phrase \"summer camp\".",
        "Therefore, the information content of the answer to Q8 is strictly subsumed in the information content associated with the answer to Q5.",
        "Similarly, in Qg the focus noun phrase is further specialized through the prepositional phrase \"for girls\".",
        "Therefore, (an answer to) Qg is less useful to Q5 than (an answer to) Q8, i.e. (Q8 y QgIQ5).",
        "Furthermore, the focus \"art summer camp\" in Q8 conceptually subsumes the focus \"summer camps for singing\" in Q10, therefore (Q8 y Q10IQ5).",
        "We call this dataset simple since most of the reference questions are shorter than the other questions in their group.",
        "We have also created a complex version of the same dataset, by selecting as the reference question in each group a longer question from the same group.",
        "For example, if Q0 were a reference question, it would be replaced with a more complex question, such as Q2, or Q3.",
        "The annotation is redone to reflect the relative usefulness relations with respect to the new reference questions.",
        "We believe that the new complex dataset is closer to the actual distribution of questions in community QA repositories: unanswered questions tend to be more specific (longer), whereas general questions (shorter) are more likely to have been answered already.",
        "Each dataset is annotated by two annotators, leading to a total of 4 datasets: Simple 1, Simple2, Complex 1, and Complex2.",
        "Table 2 presents the following statistics on the two types of datasets (Simple, Complex) for each annotator (1, 2): the total number of paraphrasings (P), the total number of useful questions (U), the total number of neutral questions (N), the total number of more useful than ordered pairs encoded in the dataset, either explicitly or through transitivity, and the Inter-Annotator Agreement (ITA).",
        "We compute the ITA as the precision (P) and recall (R) with respect to the more useful than ordered pairs encoded in one annotation (Pairs 1) relative to the ordered pairs encoded in the other annotation (Pairs2).",
        "IPairs1 n Pairs2I IPairs1 n Pairs2I",
        "The statistics in Table 2 indicate that the second annotator was in general more conservative in tagging questions as paraphrases or useful questions."
      ]
    },
    {
      "heading": "3. Unsupervised Methods for Question Ranking",
      "text": [
        "An ideal question ranking method would take an arbitrary triplet of questions Qr, Qi and Qj as input, and output an ordering between Qi and Qj with respect to the reference question Qr, i.e. one of (Qi y QjIQr), (Qi = QjIQr), or (Qj y QjQ).",
        "One approach is to design a usefulness function u(Qi, Qr ) that measures how useful question Qi is for the reference question Qr, and define the more useful than (y ) relation as follows:",
        "If we define /(Q) to be the information need associated with question Q, then u(Qi; Qr) could be defined as a measure of the relative overlap between /(Qi) and /(Qr).",
        "Unfortunately, the information need is a concept that, in general, is defined only intensionally and therefore it is difficult to measure.",
        "For lack of an operational definition of the information need, we will approximate u(Qi;Qr) directly as a measure of the similarity between Qi and Qr.",
        "The similarity between two questions can be seen as a special case of text-to-text similarity, consequently one possibility is to use a general text-to-text similarity function such as cosine similarity in the vector space model (Baeza-Yates and Ribeiro-Neto, 1999):",
        "HQiiiiiQr ii",
        "Dataset",
        "P",
        "U",
        "N",
        "Pairs",
        "ITA",
        "Simple 1",
        "164",
        "775",
        "594",
        "11015",
        "P: 76.6",
        "Simple2",
        "134",
        "778",
        "621",
        "10436",
        "R: 81.6",
        "Complex1",
        "103",
        "766",
        "664",
        "10654",
        "P: 71.3",
        "Complex2",
        "89",
        "730",
        "714",
        "9979",
        "R: 81.3",
        "Here, Qi and Qr denote the corresponding tfxidf vectors.",
        "As a measure of question similarity, one major drawback of cosine similarity is that it is oblivious of the meanings of words in each question.",
        "This particular problem is illustrated by the three questions below.",
        "Q22 and Q23 have the same cosine similarity with Q21, they are therefore indistinguishable in terms of their usefulness to the reference question Q21, even though we expect Q22 to be more useful than Q23 (a place that sells hydrangea often sells other types of plants too, possibly including cacti).",
        "Q21 Where can I buy a hydrangea?",
        "Q22 Where can I buy a cactus?",
        "Q23 Where can I buy an iPad?",
        "To alleviate the lexical chasm, we can redefine u(Qi; Qr) to be the similarity measure proposed by (Mihalcea et al., 2006) as follows:",
        "mcs(Qi, Qr)",
        "Since scaling factors are immaterial for ranking, we have ignored the normalization constant contained in the original measure.",
        "For each word w G Qi, maxSim(w, Qr) computes the maximum semantic similarity between w and any word wr G Qr.",
        "The similarity scores are weighted by the corresponding idf's, and normalized.",
        "A similar score is computed for each word w G Qr.",
        "The score computed by maxSim depends on the actual function used to compute the word-to-word semantic similarity.",
        "In this paper, we evaluated four of the knowledge-based measures explored in (Mihalcea et al., 2006): wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang and Conrath, 1997)."
      ]
    },
    {
      "heading": "4. Supervised Learning for Question Ranking",
      "text": [
        "Cosine similarity, henceforth referred as cos, treats questions as bags-of-words.",
        "The meta-measure proposed in (Mihalcea et al., 2006), henceforth called mcs, treats questions as bags-of-concepts.",
        "Both cos and mcs ignore the syntactic relations between the words in a question, and therefore may miss important structural information.",
        "In the next three sections we describe a set of structural features that we believe are relevant for judging question similarity.",
        "These and other types of features will be integrated in an SVM model for ranking, as described later in Section 4.4.",
        "If we consider the question Q24 below as reference, question Q26 will be deemed more useful than Q25when using cos or mcs because of the higher relative lexical and conceptual overlap with Q24.",
        "However, this is contrary to the actual ordering ( Q25 y Q26IQ24), which reflects the fact that Q25, which expects the same answer type as Q24, should be deemed more useful than Q26, which has a different answer type.",
        "Q24 What are some good thriller movies?",
        "Q25 What are some thriller movies with happy ending?",
        "Q26 What are some good songs from a thriller movie?",
        "The analysis above shows the importance of using the answer type when computing the similarity between two questions.",
        "However, instead of relying exclusively on a predefined hierarchy of answer types, we identify the question focus of a question, defined as the set of maximal noun phrases in the question that corefer with the expected answer (Bunescu and Huang, 2010a).",
        "Focus nouns such as movies and songs provide more discriminative information than general answer types such as products.",
        "We use answer types only for questions such as Q27 or Q28 below that lack an explicit question focus.",
        "In such cases, an artificial question focus is created from the answer type (e.g. location for Q27, or method for Q28).",
        "Q27 Where can I buy a good coffee maker?",
        "Q28 How do I make a pizza?",
        "Let fi and fr be the focus words corresponding to questions Qi and Qr.",
        "We introduce a focus feature 0/, and set its value to be equal with the similarity between the focus words:",
        "We use wsim to denote a generic word meaning similarity measure (e.g. wup, res, lin or jcn).",
        "When computing the focus feature, the non-focus word \"movie\" in Q26 will not be compared with the focus word \"movies\" in Q24, and therefore Q26 will have a lower value for this feature than Q25, i.e.",
        "0/(Q26,Q24) < 0/(Q25,Q24).",
        "In addition to the question focus, the main verb of a question can also provide key information in estimating question-to-question similarity.",
        "We define the main verb to be the content verb that is highest in the dependency tree of the question, e.g. buy for Q27, or make for Q28.",
        "If the question does not contain a content verb, the main verb is defined to be the highest verb in the dependency tree, as for example are in Q24 to Q26.",
        "The utility of a question's main verb in judging its similarity to other questions can be seen more clearly in the questions below, where Q2g is the reference:",
        "Q2g How can I transfer music from iTunes to my",
        "Q30 How can I upload music to my iPod?",
        "Q31 How can I play music in iTunes?",
        "The fact that upload, as the main verb of Q30, is more semantically related to transfer is essential in deciding that (Q30 y Q31IQ29), i.e. Q30 is more useful than Q31 to Q2g.",
        "Let viand vr be the main verbs corresponding to questions Qi and Qr.",
        "We introduce a main verb feature 0v as follows:",
        "If Q2g is considered as reference question, it is expected that the main verb feature for question Q30 will have a higher value than the main verb feature for Q31, i.e. 0/(Q31, Q29) < 0/(Q30, Q2g).",
        "The question focus and the main verb are only two of the nodes in the syntactic dependency tree of a question.",
        "In general, all the words in a question are important when judging its semantic similarity with another question.",
        "We therefore propose a more general feature that exploits the dependency structure of the question and, in doing so, it also considers all the words in the question, like cos and mcs.",
        "For any given question we initially ignore the direction of the dependency arcs and change the question dependency tree to be rooted at the focus word, as illustrated in Figure 2 for questions Q5 and Qg.",
        "Interrogative patterns such as \"What is\" or \"Are there any\" are automatically eliminated from the dependency trees.",
        "We define the dependency tree similarity between two questions Qi and Qr to be a function of similarities wsim(vi, vr) computed between aligned nodes vi G Qi and vr G Qr.",
        "The nodes of two dependency trees are aligned through a function MaxMatch(ui .C, ur .C) that takes two sets of children nodes as arguments, one from Qi and one from Qr, and finds the maximum weighted bipartite matching between ui.C and ur.C.",
        "Given two children nodes vi G ui.C and vr G ur.C, the weight of a potential matching between viand vr is defined simply as wsim(vi, vr).",
        "MaxMatch(ui.C, ur.C) is furthermore constrained to match only nodes that have compatible part-of-speech tags (e.g. nouns are matched to nouns, verbs are matched to verbs), and children nodes that have the same head-modifier relationship with their parents (i.e. they are both heads, or they are both dependents of their parents).",
        "Table 3 shows the recursive algorithm used",
        "(Tiice^)",
        "summer",
        "5>.........",
        "to",
        "go",
        "to",
        "Qs",
        "(camp^",
        "(^gööd^)",
        "/ Xartsy",
        "for",
        "Qs",
        "girls",
        "TreeMatch(uj, ur ) [In]: Two dependency tree nodes u4, ur.",
        "[Out]: A set of node pairs M."
      ]
    },
    {
      "heading": "4.. return M",
      "text": [
        "for finding a matching between two question dependency trees rooted at the focus words.",
        "The initial arguments of the algorithm are the two focus words ui = /i and ur = /r.",
        "Thus, the pair (/i,/r) is the first pair of nodes to be added to the matching M in step 1.",
        "In the next step, we compute the maximum weighted matching between the children nodes ui.C and ur.C, and recursively call the matching algorithm on pairs of matched nodes (vi, vr) from M. The algorithm stops when MaxMatch returns an empty matching, which may happen when reaching leaf nodes, or when no pair of children nodes has compatible POS tags, or child-parent dependencies.",
        "Figure 2 shows the results of applying the tree matching algorithm on questions Q5 and Qg.",
        "Matched nodes share the same index and are shown in circles, whereas unmatched nodes are shown in italics.",
        "We introduce a new feature 0t(Qi,Qr) whose value is defined as the dependency tree similarity between questions Qi and Qr.",
        "Once the optimum matching M( Qi, Qr) between dependency trees has been found, 0t(Qi,Qr) is computed as the normalized sum of the similarities between pairs of matched nodes viand vr, as shown in Equations 3 and 4 below.",
        "When computing the similarity between two matched nodes, we factor in the similarities between corresponding pairs of words on the paths /i ~» vi, /r ~» vr between the focus words /i,",
        "/r and the nodes vi, vr, as shown in Equation 5.",
        "This has the effect of reducing the importance of words that are farther away from the focus word in the dependency tree.",
        "If the word similarity function is normalized and defined to return 1 for identical words, the normalizer in Equation 3 becomes equivalent with a/I Qi 11Qr I.",
        "Thus, words that are left unmatched implicitly decrease the dependency tree similarity.",
        "We consider learning a usefulness function u(Qi, Qr) of the following general, linear form:",
        "The vector 0(Qi, Qr) is defined to contain the following generic features:",
        "1.",
        "0/ (Qi, Qr ) = the semantic similarity between focus words, as described in Section 4.1.",
        "2.",
        "0v(Qi,Qr) = the semantic similarity between main verbs, as described in Section 4.2.",
        "3.",
        "0t(Qi ,Qr ) = the semantic similarity between the dependency trees, as described in Section 4.3.",
        "4. cos(Qi, Qr) = the cosine similarity between the two questions, as described in Section 3.",
        "5. mcs(Qi,Qr) = the bag-of-concepts similarity between the two questions, as described in Section 3.",
        "Each of the generic features 0/, 0v, 0t, and mcs corresponds to four actual features, one for each possible choice of the word similarity function wsim (i.e. wup, res, lin or jcn).",
        "An additional pair of features is targeted at questions containing locations:",
        "6.",
        "0l(Qi, Qr) = 1 if both questions contain locations, 0 otherwise.",
        "7.",
        "0d(Qi, Qr) = the normalized geographical distance between the locations in Qi and Qr, 0 if 0l(Qi,Qr ) =0.",
        "Given two location names, we first find their latitude and longitude using Google Maps, and then compute the spherical distance between them using the haversine formula.",
        "The corresponding parameters w will be trained on pairs from one of the partially ordered datasets described in Section 2.",
        "We use the kernel version of the large-margin ranking approach from (Joachims, 2002) which solves the optimization problem in Figure 3 below.",
        "The aim of this formulation is to find a minimize: subject to:",
        "VQr, Qi, Qj g D, (Qi y Qj IQr)",
        "weight vector w such that 1) the number of ranking constraints u(Qi, Qr) > u(Qj, Qr) from the training data D that are violated is minimized, and 2) the ranking function u(Qi, Qr) generalizes well beyond the training data.",
        "The learned w is a linear combination of the feature vectors 0(Qi, Qr), which makes it possible to use kernels."
      ]
    },
    {
      "heading": "5. Experimental Evaluation",
      "text": [
        "We use the four question ranking datasets described in Section 2 to evaluate the three similarity measures cos, mcs, and 0t, as well as the SVM ranking model.",
        "We report one set of results for each of the four word similarity measures wup, res, lin or jcn.",
        "Each question similarity measure is evaluated in terms of its accuracy on the set of ordered pairs, and the performance is averaged between the two annotators for the Simple and Complex datasets.",
        "If (Qi y Qj IQr) is a relation specified in the annotation, we consider the tuple ( Qi, Qj, Qr) correctly classified if and only if u(Qi,Qr) > u(Qj,Qr), where u is the question similarity measure.",
        "We used the SVMligh* implementation of ranking SVMs, with a cubic kernel and the standard parameters.",
        "The SVM ranking model was trained and tested using 10-fold cross-validation, and the overall accuracy was computed by averaging over the 10 folds.",
        "We used the NLTK implementation of the four similarity measures wup, res, lin or jcn.",
        "The idfval-ues for each word were computed from frequency counts over the entire Wikipedia.",
        "For each question, the focus is identified automatically by an SVM tagger trained on a separate corpus of 2,000 questions manually annotated with focus information (Bunescu and Huang, 2010a).",
        "The SVM tagger uses a combination of lexico-syntactic features and a quadratic kernel to achieve a 93.5% accuracy in a 10-fold cross validation evaluation on the 2,000 questions.",
        "The head-modifier dependencies were derived automatically from the syntactic parse tree using the head finding rules from (Collins, 1999).",
        "The syntactic tree is obtained using Spear , a syntactic parser which comes pre-trained on an additional treebank of questions.",
        "The main verb of a question is identified deterministically using a breadth first traversal of the dependency tree.",
        "The overall accuracy results presented in Table 4 show that the SVM ranking model obtains by far the best performance on both datasets, a substantial 10% higher than cos, which is the best performing unsupervised method.",
        "The random baseline - assigning a random similarity value to each pair of questions results in 50% accuracy.",
        "Even though its use of word senses was expected to lead to superior results, mcs does not perform better than cos on this dataset.",
        "Our implementation of mcs did however perform better than cos on the Microsoft paraphrase corpus (Dolan et al., 2004).",
        "One possible reason for this behavior is that mcs seems to be less resilient than cos to differences in question length.",
        "Whereas the Microsoft paraphrase corpus was specifically designed such that \"the length of the shorter of the two sentences, in words, is at least 66% that of the longer\" (Dolan and Brockett, 2005), the question ranking datasets place no constraints on the lengths of the questions.",
        "However, even though by themselves the meaning aware mcs and the structure-and-meaning aware 0t do not outperform the bag-of-words cos, they do help in increasing the performance of the SVM ranking model, as can be inferred from the corresponding columns in Table 5.",
        "The table shows the results of ablation experiments in which all but one type of features are used.",
        "The results indicate that all types of features are useful, with significant contributions being brought especially by cos and the focus related features 0/;t.",
        "The measures investigated in this paper are all compositional and reduce the similarity computations to word level.",
        "The following question patterns illustrate the need to design more complex similarity measures that take into account the context of every word in the question:",
        "P1 Where can I find a job around (City )?",
        "P2 What are some famous people from (City )?",
        "P3 What is the population of ( City ) ?",
        "Below are three instantiations of the first question pattern:",
        "Q32 Where can I find a job around Anaheim, CA?",
        "Q33 Where can I find a job around Los Angeles?",
        "Q34 Where can I find a job around Vista, CA?",
        "If we take Q32 as reference question, the fact that the distance between Los Angeles and Anaheim is smaller than the distance between Vista and Anaheim leads the ranking system to rank Q33 as more useful than Q34 with respect to Q32, which is the expected result.",
        "The preposition \"around\" from the city context in the first pattern is a good indicator that proximity relations are relevant in this case.",
        "When the same three cities are used for instantiating the other two patterns, it can be seen that the proximity relations are no longer as relevant for judging the relative usefulness of questions."
      ]
    },
    {
      "heading": "6. Future Work",
      "text": [
        "We plan to integrate context dependent word similarity measures into a more robust question utility function.",
        "We also plan to make the dependency tree matching more flexible in order to account for paraphrase patterns that may differ in their syntactic structure.",
        "The questions that are posted on community QA sites often contain spelling or grammatical errors.",
        "Consequently, we will work on interfacing the question ranking system with a separate module aimed at fixing orthographic and grammatical errors."
      ]
    },
    {
      "heading": "7. Related Work",
      "text": [
        "The question rephrasing subtask has spawned a diverse set of approaches.",
        "(Hermjakob et al., 2002) derive a set of phrasal patterns for question reformulation by generalizing surface patterns acquired automatically from a large corpus of web documents.",
        "The focus of the work in (Tomuro, 2003) is on deriving reformulation patterns for the interrogative part of a question.",
        "In (Jeon et al., 2005), word translation probabilities are trained on pairs of semantically similar questions that are automatically extracted from an FAQ archive, and then used in a language model that retrieves question reformulations.",
        "(Jijkoun and de Rijke, 2005) describe an FAQ question retrieval system in which weighted combinations of similarity functions corresponding to questions, existing answers, FAQ titles and pages are computed using a vector space model.",
        "(Zhao et al., 2007) exploit the Encarta logs to automatically extract clusters containing question paraphrases and further train a perceptron to recognize question paraphrases inside each cluster based on a combination of lexical, syntactic and semantic similarity features.",
        "More recently, (Bernhard and Gurevych, 2008) evaluated various string similarity measures and vector space based similarity measures on the task of retrieving question paraphrases from the WikiAn-swers repository.",
        "The aim of the question search task presented in (Duan et al., 2008) is to return questions that are semantically equivalent or close to the queried question, and is therefore similar to our question ranking task.",
        "Their approach is evaluated on a dataset in which questions are categorized either as relevant or irrelevant.",
        "Our formulation of question ranking is more general, and in particular subsumes the annotation of binary question categories such as relevant vs. irrelevant, or paraphrases vs. non-paraphrases.",
        "Moreover, we are able to exploit the annotated utility relations as supervision in a learning for ranking approach, whereas (Duan et al., 2008) use the annotated dataset to tune the 3 parameters of a mostly unsupervised approach.",
        "The question ranking task was first formulated in (Bunescu and Huang, 2010b), where an initial version of the dataset was also described.",
        "In this paper, we introduce 4 versions of the dataset, a more general meaning and structure aware similarity measure, and a supervised model for ranking that substantially outperforms the previously proposed utility measures.",
        "Question Dataset",
        "cos",
        "wup",
        "res",
        "lin",
        "jcn",
        "SVM",
        "mes 0t",
        "mes 0t",
        "mes 0t",
        "mes 0t",
        "Simple",
        "73.7",
        "69.1 69.4",
        "71.3 71.8",
        "70.8 69.8",
        "71.9 71.7",
        "82.1",
        "Complex",
        "72.6",
        "64.1 69.6",
        "66.0 71.5",
        "66.9 69.1",
        "69.4 71.0",
        "82.5",
        "Dataset",
        "all",
        "-0/",
        "-0v",
        "-0t",
        "-cos",
        "-mcs",
        "-0/,t",
        "Simple",
        "82.1",
        "79.3",
        "82.0",
        "80.2",
        "81.5",
        "80.3",
        "81.4",
        "78.5",
        "Complex",
        "82.5",
        "81.3",
        "81.3",
        "78.7",
        "81.8",
        "79.2",
        "81.8",
        "77.4"
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "We presented a supervised learning approach to the question ranking task in which previously known questions are ordered based on their relative utility with respect to a new, reference question.",
        "We created four versions of a dataset of 60 groups of questions , each annotated with a partial order relation reflecting the relative utility of questions inside each group.",
        "An SVM ranking model was trained on the dataset and evaluated together with a set of simpler, unsupervised question-to-question similarity models.",
        "Experimental results demonstrate the importance of using structure and meaning aware features when computing the relative usefulness of questions."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the anonymous reviewers for their insightful comments."
      ]
    }
  ]
}
