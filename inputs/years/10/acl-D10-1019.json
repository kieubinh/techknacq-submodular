{
  "info": {
    "authors": [
      "Xian Qian",
      "Qi Zhang",
      "Yaqian Zhou",
      "Xuanjing Huang",
      "Lide Wu"
    ],
    "book": "EMNLP",
    "id": "acl-D10-1019",
    "title": "Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks",
    "url": "https://aclweb.org/anthology/D10-1019",
    "year": 2010
  },
  "references": [
    "acl-D07-1033",
    "acl-D07-1083",
    "acl-D08-1070",
    "acl-I08-4010",
    "acl-I08-4017",
    "acl-N01-1025",
    "acl-P05-1001",
    "acl-P08-1074",
    "acl-P08-1076",
    "acl-P08-1101",
    "acl-P08-1102",
    "acl-W04-3236"
  ],
  "sections": [
    {
      "text": [
        "Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide Wu",
        "Many sequence labeling tasks in NLP require solving a cascade of segmentation and tagging subtasks, such as Chinese POS tagging, named entity recognition, and so on.",
        "Traditional pipeline approaches usually suffer from error propagation.",
        "Joint training/decoding in the crossproduct state space could cause too many parameters and high inference complexity.",
        "In this paper, we present a novel method which integrates graph structures of two subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space.",
        "Experimental evaluations on CoNLL 2000 shallow parsing data set and Fourth SIGHAN Bakeoff CTB POS tagging data set demonstrate the superiority of our method over cross-product, pipeline and candidate reranking approaches."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "There is a typical class of sequence labeling tasks in many natural language processing (NLP) applications, which require solving a cascade of segmentation and tagging subtasks.",
        "For example, many Asian languages such as Japanese and Chinese which do not contain explicitly marked word boundaries, word segmentation is the preliminary step for solving part-of-speech (POS) tagging problem.",
        "Sentences are firstly segmented into words, then each word is assigned with a part-of-speech tag.",
        "Both syntactic parsing and dependency parsing usually start with a textual input that is tokenized, and POS tagged.",
        "The most commonly approach solves cascaded subtasks in a pipeline, which is very simple to implement and allows for a modular approach.",
        "While, the key disadvantage of such method is that errors propagate between stages, significantly affecting the quality of the final results.",
        "To cope with this problem, Shi and Wang (2007) proposed a rerank-ing framework in which N-best segment candidates generated in the first stage are passed to the tagging model, and the final output is the one with the highest overall segmentation and tagging probability score.",
        "The main drawback of this method is that the interaction between tagging and segmentation is restricted by the number of candidate segmentation outputs.",
        "Razvan C. Bunescu (2008) presented an improved pipeline model in which upstream subtask outputs are regarded as hidden variables, together with their probabilities are used as probabilistic features in the downstream subtasks.",
        "One shortcoming of this method is that calculation of marginal probabilities of features may be inefficient and some approximations are required for fast computation.",
        "Another disadvantage of these two methods is that they employ separate training and the segmentation model could not take advantages of tagging information in the training procedure.",
        "On the other hand, joint learning and decoding using crossproduct of segmentation states and tagging states does not suffer from error propagation problem and achieves higher accuracy on both subtasks (Ng and Low, 2004).",
        "However, two problems arises due to the large state space, one is that the amount of parameters increases rapidly, which is apt to overfit on the training corpus, the other is that the inference by dynamic programming could be inefficient.",
        "Sutton (2004) proposed Dynamic Conditional Random Fields (DCRFs) to perform joint training/decoding of subtasks using much fewer parameters than the crossproduct approach.",
        "However, DCRFs do not guarantee non-violation ofhard-constraints that nodes within the same segment get a single consistent tagging label.",
        "Another drawback of DCRFs is that exact inference is generally time consuming, some approximations are required to make it tractable.",
        "Recently, perceptron based learning framework has been well studied for incorporating node level and segment level features together (Kazama and Torisawa, 2007; Zhang and Clark, 2008).",
        "The main shortcoming is that exact inference is intractable for those dynamically generated segment level features, so candidate based searching algorithm is used for approximation.",
        "On the other hand, Jiang (2008) proposed a cascaded linear model which has a two layer structure, the inside-layer model uses node level features to generate candidates with their weights as inputs of the outside layer model which captures non-local features.",
        "As pipeline models, error propagation problem exists for such method.",
        "In this paper, we present a novel graph structure that exploits joint training and decoding in the factorized state space.",
        "Our method does not suffer from error propagation, and guards against violations of those hard-constraints imposed by segmentation subtask.",
        "The motivation is to integrate two Markov chains for segmentation and tagging subtasks intoasinglechain, whichcontains two types of nodes, then standard dynamic programming based exact inference is employed on the hybrid structure.",
        "Experiments are conducted on two different tasks, CoNLL 2000 shallow parsing and SIGHAN 2008 Chinese word segmentation and POS tagging.",
        "Evaluation results of shallow parsing task show the superiority of our proposed method over traditional joint training/decoding approach using crossproduct state space, and achieves the best reported results when no additional resources at hand.",
        "For Chinese word segmentation and POS tagging task, a strong baseline pipeline model is built, experimental results show that the proposed method yields a more substantial improvement over the baseline than candidate reranking approach.",
        "The rest of this paper is organized as follows: In Section 2, we describe our novel graph structure.",
        "In Section 3, we analyze complexity of our proposed method.",
        "Experimental results are shown in Section 4.",
        "We conclude the work in Section 5."
      ]
    },
    {
      "heading": "2. Multi-chain integration using Virtual",
      "text": [
        "We begin with a brief review of the Conditional Random Fields(CRFs).",
        "Let x = x1x2 .. .xi denote the observed sequence, where Xi is the ith node in the sequence, l is sequence length, y = y1y2 ...yi is a label sequence over x that we wish to predict.",
        "CRFs (Lafferty et al., 2001) are undirected graphic models that use Markov network distribution to learn the conditional probability.",
        "For sequence labeling task, linear chain CRFs are very popular, in which a first order Markov assumption is made on the labels:",
        "[fl(x,yi-1 ,yi,i), ■ ■ ■,fm(x,yi-1,yi,i)]T, each element fj(x, yi-\\,yi, i) is a real valued feature function, here we simplify the notation of state feature by writing fj (x, yi,i) = fj (x, yi-i,yi,i), m is the cardinality of feature set {fj }.",
        "w = [w\\,..., wm]Tis a weight vector to be learned from the training set.",
        "Z (x) is the normalization factor over all label sequences for x.",
        "In the traditional joint training/decoding approach for cascaded segmentation and tagging task, each label yi has the form si-ti, which consists of segmentation label si and tagging label ti.",
        "Let s = sis2 .. .sl be the segmentation label sequence over x.",
        "There are several commonly used label sets such as BI, BIO, IOE, BIES, etc.",
        "To facilitate our discussion, in later sections we will use BIES label set, where B,I,E represents Beginning, Inside and End of a multi-node segment respectively, S denotes a single node segment.",
        "Let t = t1t2 .. .tl be the tagging label sequence over x.",
        "For example, in named entity recognition task, tie {PER, LOC, ORG, MISC, O} represents an entity type (person name, location name, organization name, miscellaneous entity",
        "Hendrix 's girlfriend Kathy Etchingham",
        "name and other).",
        "Graphical representation of linear chain CRFs is shown in Figure 1, where tagging label \"P\" is the simplification of \"PER\".",
        "For nodes that are labeled as other, we define si =S, ti =O.",
        "Different from traditional joint approach, our method integrates two linear markov chains for segmentation and tagging subtasks into one that contains two types of nodes.",
        "Specifically, we first regard segmentation and tagging as two independent sequence labeling tasks, corresponding chain structures are built, as shown in the top and middle sub-figures of Figure 2.",
        "Then a chain of twice length of the observed sequence is built, where nodes x1,...,xl on the even positions are original observed nodes, while nodes v1}... ,vl on the odd positions are virtual nodes that have no content information.",
        "For original nodes xi, the state space is the tagging label set, while for virtual nodes, their states are segmentation labels.",
        "The label sequence of the hybrid chain is y = y1.. .y2i = s1t1... siti, where combination of consecutive labels Siti represents the full label for node xi.",
        "Then we let si be connected with si_i and si+1, so that first order Markov assumption is made on segmentation states.",
        "Similarly, ti is connected with ti_1 and ti+1.",
        "Then neighboring tagging and segmentation states are connected as shown in the bottom sub-figure of Figure 2.",
        "Non-violation of hard-constraints that nodes within the same segment get a single consistent tagging label is guaranteed by introducing second order transition features f (ti_1,si, ti, i) that are true if ti_1 = ti and si G {I,E}.",
        "For example, fj(ti_1 ,si,ti,i) is defined as true if ti_1 =PER, si =I and ti =LOC.",
        "In other words, it is true, if a segment is partially tagging as PER, and partially tagged as LOC.",
        "Since such features are always false in the training corpus, their corresponding weights will be very low so that inconsistent label assignments impossibly appear in decoding procedure.",
        "The hybrid graph structure can be regarded as a special case of second order Markov chain.",
        "Compared with traditional joint model that exploits crossproduct state space, our hybrid structure uses factorized states, hence could handle more flexible features.",
        "Any state feature g(x,yi,i) defined in the crossproduct state space can be replaced by a first order transition feature in the factorized space: f (x,si,ti,i).",
        "As for the transition features, we use f(si_1,ti_1,si,i) and f(t_1 ,si,ti,i) instead of g(yi_1, yi, i) in the conventional joint model.",
        "Features in crossproduct state space require that segmentation label and tagging label take on particular values simultaneously, however, sometimes we want to specify requirement on only segmentation or tagging label.",
        "For example, \"Smith\" may be an end of a person name, \"Speaker: John Smith\"; or a single word person name \"Professor Smith will .",
        ".",
        ".",
        "\".",
        "In such case, our observation is that \"Smith\" is likely a (part of) person name, we do not care about its segmentation label.",
        "So we could define state feature f (x, ti, i) = true, if xi is \"Smith\" with tagging label ti=PER.",
        "Further more, we could define features like",
        "f(x, ti_1, ti, i), f(x, si_1, si, i), f(x, ti_1, si, i),etc.",
        "The hybrid structure facilitates us to use varieties of features.",
        "In the remainder of the paper, we use notations f(x, ti_1, si, ti, i) and f (x, si_1,ti_1,si, i) for simplicity.",
        "A hybrid CRFs is a conditional distribution that factorizes according to the hybrid graphical model, and is defined as:",
        "Where w1, w2 are weight vectors.",
        "Luckily, unlike DCRFs, in which graph structure can be very complex, and the crossproduct state space can be very large, in our cascaded labeling task, the segmentation label set is often small, so far as we known, the most complicated segmentation label set has only 6 labels (Huang and Zhao, 2007).",
        "So exact dynamic programming based algorithms can be efficiently performed.",
        "In the training stage, we use second order forward backward algorithm to compute the marginal probabilities p(x, s_1,t_1, si) and p(x,t_1, si,U), and the normalization factor Z(x).",
        "In decoding stage, we use second order Viterbi algorithm to find the best label sequence.",
        "The Viterbi decoding can be used to label a new sequence, and marginal computation is used for parameter estimation."
      ]
    },
    {
      "heading": "3. Complexity Analysis",
      "text": [
        "The time complexity of the hybrid CRFs training and decoding procedures is higher than that of pipeline methods, but lower than traditional crossproduct methods.",
        "Let",
        "• IS I = size of the segmentation label set.",
        "• ITI = size of the tagging label set.",
        "• L = total number of nodes in the training data set.",
        "• U = total number of nodes in the testing data set.",
        "• c = number of joint training iterations.",
        "• cs = number of segmentation training iterations.",
        "• ct = number of tagging training iterations.",
        "• N = number of candidates in candidate rerank-ing approach.",
        "Time requirements for pipeline, cross-product, candidate reranking and hybrid CRFs are summarized in Table 1.",
        "For Hybrid CRFs, original node xi has features {fj(ti_1,si, ti)}, accessing all label subsequences ti_1siti takes ISIIT| time, while virtual node vi has features {fj(si_1,ti_1,si)}, accessing all label subsequences si_1 ti_1si takes I SI IT I time, so the final complexity is (ISI + ITI)ISITIcL.",
        "In real applications, I SI is small, I TI could be very large, we assume that I TI >> I SI , so for each iteration, hybrid CRFs is about I SI times slower than pipeline and I SI times faster than crossproduct method.",
        "When decoding, candidate reranking approach requires more time if candidate number N >",
        "Method",
        "Training",
        "Decoding",
        "Pipeline",
        "(\\S\\cs + \\T\\ct)L",
        "(\\S\\ + \\T\\)U",
        "Cross-Product",
        "(\\S\\\\T\\)cL",
        "(\\S\\\\T\\)U",
        "Reranking",
        "(\\S\\Cs + \\T\\ct)L",
        "(\\S\\ + \\T\\)NU",
        "Hybrid",
        "(\\S\\ + \\T\\)\\S\\\\T\\cL",
        "(\\S\\ + \\T\\)\\S\\\\T\\U",
        "ISI.",
        "Though the space complexity could not be compared directly among some of these methods, hybrid CRFs require less parameters than crossproduct CRFs due to the factorized state space.",
        "This is similar with factorized CRFs (FCRFs) (Sutton et al., 2004)."
      ]
    },
    {
      "heading": "4. Experiments 4.1 Shallow Parsing",
      "text": [
        "Our first experiment is the shallow parsing task.",
        "We use corpus from CoNLL 2000 shared task, which contains 8936 sentences for training and 2012 sentences for testing.",
        "There are 11 tagging labels: noun phrase(NP), verb phrase(VP) , .",
        ".",
        ".",
        "and other (O), the segmentation state space we used is BIES label set, since we find that it yields a little improvement over",
        "BIO set.",
        "We use the standard evaluation metrics, which are precision P (percentage of output phrases that exactly match the reference phrases), recall R (percentage of reference phrases returned by our system), and their harmonic mean, the F1 score F1 = pr+R (which we call F score in what follows).",
        "We compare our approach with traditional crossproduct method.",
        "To find good feature templates, development data are required.",
        "Since CoNLL2000 does not provide development data set, we divide the training data into 10 folds, of which 9 folds for training and 1 fold for developing.",
        "After selecting feature templates by cross validation, we extract features and learn their weights on the whole training data set.",
        "Feature templates are summarized in Table 2, where Wi denotes the ith word, Pi denotes the ith",
        "POS tag.",
        "Notice that in the second row, feature templates of the hybrid CRFs does not contain Wi_2si, Wi+2si, since we find that these two templates degrade performance in cross validation.",
        "However, Wi_2ti, Wi+2ti are useful, which implies that the proper context window size for segmentation is smaller than tagging.",
        "Similarly, for hybrid CRFs, the window size of POS bigram features for segmentation is 5 (from Pi_2 to Pi+2, see the eighth row in the second column); while for tagging, the size is 7 (from Pi_3 to Pi+3, see the ninth row in the second column).",
        "However for crossproduct method, their window sizes must be consistent.",
        "For traditional crossproduct CRFs and our hybrid CRFs, we use fixed gaussian prior a = 1.0 for both methods, we find that this parameter does not signifi-",
        "Cross Product CRFs",
        "Hybrid CRFs",
        "Wi_2yi, wi_1yi, wiyi Wi+1yi, Wi+2yi",
        "Wi_1si, Wisi, Wi+1si",
        "Wi_2ti, Wi_1 ti, Witi, Wi+1ti, Wi+2ti",
        "Wi_1Wiyi, WiWi+1yi",
        "Wi_1Wisi, WiWi+1si",
        "Wi_1Witi, WiWi+1ti",
        "Pi_2yi, Pi_1 yi, Piyi",
        "Pi+1yu pi+2 yi",
        "Pi_1si, Pisi, Pi+1si",
        "Pi_2ti, Pi_1ti, Pi+1ti, Pi+2ti",
        "Pi_2Pi_1yi, Pi_1Piyi, PiPi+1yi,",
        "pi+1pi+2yi",
        "Pi_2Pi_1si, Pi_1Pisi, PiPi+1si, Pi+1Pi+2si",
        "Pi_3Pi_2ti, Pi_2Pi_1ti, Pi_1Piti, PiPi+1ti, Pi+1Pi+2ti, Pi+2Pi+3ti, Pi_1 Pi+1 ti",
        "Pi_2Pi_1Piyi, Pi_1PiPi+1yi, PiPi+1Pi+2yi",
        "Pi_2Pi_1Pisi, Pi_1PiPi+1si, PiPi+1Pi+2si",
        "WiPiti",
        "Wi,si_1,si",
        "Wi_1ti_1ti, Witi_1ti, Pi_1ti_1ti, Piti_1ti",
        "yi_1yi",
        "si_1ti_1si, ti_1siti",
        "Table 3: Results for shallow parsing task, Hybrid CRFs significantly outperform Cross-Product CRFs (McNe-mar's test; p < 0.01) cantly affect the results when it varies between 1 and 10.",
        "LBFGS(Nocedal and Wright, 1999) method is employed for numerical optimization.",
        "Experimental results are shown in Table 3.",
        "Our proposed CRFs achieve a performance gain of 0.43 points in F-score over crossproduct CRFs that use state space while require less training time.",
        "For comparison, we also listed the results of previous top systems, as shown in Table 4.",
        "Our proposed method outperforms other systems when no additional resources at hand.",
        "Though recently semi-supervised learning that incorporates large mounts of unlabeled data has been shown great improvement over traditional supervised methods, such as the last row in Table 4, supervised learning is fundamental.",
        "We believe that combination of our method and semi-supervised learning will achieve further improvement.",
        "Our second experiment is the Chinese word segmentation and POS tagging task.",
        "To facilitate comparison, we focus only on the closed test, which means that the system is trained only with a designated training corpus, any extra knowledge is not allowed, including Chinese and Arabic numbers, letters and so on.",
        "We use the Chinese Treebank (CTB) POS corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008).",
        "The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters.",
        "We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) using the same evaluation metrics as shallow parsing.",
        "We do not compare with crossproduct CRFs due to large amounts of parameters.",
        "For pipeline method, we built our word segmenter based on the work of Huang and Zhao (2007), which uses 6 label representation, 7 feature templates (listed in Table 5, where ci denotes the ithChinese character in the sentence) and CRFs for parameter learning.",
        "We compare our segmentor with other top systems using SIGHAN CTB corpus and evaluation metrics.",
        "Comparison results are shown in Table 6, our segmenter achieved 95.12 F-score, which is ranked 4th of 26 official runs.",
        "Except for the first system which uses extra unlabeled data, differences between rest systems are not signiicant.",
        "Our POS tagging system is based on linear chain CRFs.",
        "Since SIGHAN dose not provide development data, we use the 10 fold cross validation described in the previous experiment to turning feature templates and Gaussian prior.",
        "Feature templates are listed in Table 5, where wi denotes the ith word in",
        "Method",
        "F1",
        "Additional Resources",
        "Cross-Product CRFs",
        "93.88",
        "Hybrid CRFs",
        "94.31",
        "SVM combination",
        "93.91",
        "(Kudo and Mat-",
        "sumoto, 2001)",
        "Voted Perceptrons",
        "93.74",
        "none",
        "(Carreras and Mar-",
        "quez, 2003)",
        "ETL (Milidiu et al.,",
        "92.79",
        "2008)",
        "(Wu et al., 2006)",
        "94.21",
        "Extended features such as token features, afixes",
        "HySOL",
        "94.36",
        "17M words unlabeled",
        "(Suzuki et al., 2007)",
        "data",
        "ASO-semi",
        "94.39",
        "15M words unlabeled",
        "(Ando and Zhang,",
        "data",
        "2005)",
        "(Zhang et al., 2002)",
        "94.17",
        "full parser output",
        "(Suzuki and Isozaki,",
        "95.15",
        "1G words unla-",
        "2008)",
        "beled data",
        "Method",
        "Cross-Product",
        "Hybrid",
        "CRFs",
        "CRFs",
        "Training Time",
        "11.6 hours",
        "6.3 hours",
        "Feature Num-",
        "13 million",
        "10 mil-",
        "ber",
        "lion",
        "Iterations",
        "118",
        "141",
        "F1",
        "93.88",
        "94.31",
        "Segmentation feature templates",
        "POS tagging feature templates",
        "Joint segmentation and POS tagging feature templates the sentence, Cj(Wi),j > 0 denotes the jth Chinese character of word Wi, Cj(Wi),j < 0 denotes the jthlast Chinese character, l(Wi) denotes the word length of Wi.",
        "We compare our POS tagger with other top systems on Bakeoff CTB POS corpus where sentences are perfectly segmented into words, our POS tagger achieved 94.29 accuracy, which is the best of 7 official runs.",
        "Comparison results are shown in Table 7.",
        "For reranking method, we varied candidate numbers n among n G {10, 20, 50,100}.",
        "For hybrid CRFs, we use the same segmentation label set as the segmentor in pipeline.",
        "Feature templates are listed in Table 5.",
        "Experimental results are shown in Figure 3.",
        "The gain of hybrid CRFs over the baseline pipeline model is 0.48 points in F-score, about 3 times higher than 100-best reranking approach which achieves 0.13 points improvement.",
        "Though larger candidate number can achieve higher performance, such improvement becomes trivial for n > 20.",
        "Table 8 shows the comparison between our work and other relevant work.",
        "Notice that, such comparison is indirect due to different data sets and re-",
        "Rank",
        "F1",
        "Description",
        "1/26",
        "95.89*",
        "official best, using extra unlabeled data (Zhao and Kit, 2008)",
        "2/26",
        "95.33",
        "official second",
        "3/26",
        "95.17",
        "official third",
        "4/26",
        "95.12",
        "segmentor in pipeline system",
        "Rank",
        "Accuracy",
        "Description",
        "1/7",
        "94.29",
        "POS tagger in pipeline system",
        "2/7",
        "94.28",
        "official best",
        "3/7",
        "94.01",
        "official second",
        "4/7",
        "93.24",
        "official third",
        "candidate number",
        "Figure 3: Results for Chinese word segmentation and POS tagging task, Hybrid CRFs significantly outperform 100-Best Reranking (McNemar's test; p < 0.01)",
        "Table 8: Comparison of word segmentation and POS tagging, such comparison is indirect due to different data sets and resources.",
        "sources.",
        "One common conclusion is that joint models generally outperform pipeline models."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We introduced a framework to integrate graph structures for segmentation and tagging subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space.",
        "Our approach does not suffer from error propagation, and guards against violations of those hard-constraints imposed by segmentation subtask.",
        "Experiments on shallow parsing and Chinese word segmentation tasks demonstrate our technique."
      ]
    },
    {
      "heading": "6. Acknowledgements",
      "text": [
        "The author wishes to thank the anonymous reviewers for their helpful comments.",
        "This work was partially funded by 973 Program (2010CB327906), The National High Technology Research and Development Program of China (2009AA01A346), Shanghai Leading Academic Discipline Project (B114), Doctoral Fund of Ministry of Education of",
        "Funds for Distinguished Young Scholar of China (61003092), and Shanghai Science and Technology Development Funds (08511500302).",
        "Model",
        "Fl",
        "Pipeline (ours)",
        "90.40",
        "100-Best Reranking (ours)",
        "90.53",
        "Hybrid CRFs (ours)",
        "90.88",
        "Pipeline (Shi and Wang, 2007)",
        "91.67",
        "20-Best Reranking (Shi and Wang,",
        "91.86",
        "2007)",
        "Pipeline (Zhang and Clark, 2008)",
        "90.33",
        "Joint Perceptron (Zhang and Clark,",
        "91.34",
        "2008)",
        "Perceptron Only (Jiang et al., 2008)",
        "92.5",
        "Cascaded Linear (Jiang et al., 2008)",
        "93.4"
      ]
    }
  ]
}
