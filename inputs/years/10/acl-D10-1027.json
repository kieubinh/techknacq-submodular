{
  "info": {
    "authors": [
      "Liang Huang",
      "Haitao Mi"
    ],
    "book": "EMNLP",
    "id": "acl-D10-1027",
    "title": "Efficient Incremental Decoding for Tree-to-String Translation",
    "url": "https://aclweb.org/anthology/D10-1027",
    "year": 2010
  },
  "references": [
    "acl-D08-1089",
    "acl-J07-2003",
    "acl-J97-3002",
    "acl-J99-4005",
    "acl-N07-1051",
    "acl-N07-1063",
    "acl-N10-1128",
    "acl-P03-1021",
    "acl-P06-1077",
    "acl-P06-1098",
    "acl-P07-1019",
    "acl-P07-2045",
    "acl-P08-1023",
    "acl-P08-1025",
    "acl-W06-3601",
    "acl-W07-0405"
  ],
  "sections": [
    {
      "text": [
        "Liang Huang Haitao Mi",
        "information Sciences Institute Key Lab.",
        "of Intelligent Information Processing",
        "Institute of Computing Technology Chinese Academy of Sciences P.O.",
        "Box 2704, Beijing 100190, China htmi@ict.ac.cn",
        "Syntax-based translation models should in principle be efficient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration.",
        "In this paper we borrow from phrase-based decoding the idea to generate a translation incrementally left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in average-case polynomial-time in theory, and lineartime with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice).",
        "Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++)."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Most efforts in statistical machine translation so far are variants of either phrase-based or syntax-based models.",
        "From a theoretical point of view, phrase-based models are neither expressive nor efficient: they typically allow arbitrary permutations and resort to language models to decide the best order.",
        "In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponential-time algorithm (Knight, 1999).",
        "In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004).",
        "However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a \"jump\"",
        "Table 1 : [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based.",
        "In practice means \"approximate search with beams.\"",
        "longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance re-orderings like SVO-to-SOV.",
        "Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguistically-motivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007).",
        "In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based).",
        "But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice.",
        "Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right.",
        "As a result, syntax-based models are often embarassingly slower than their phrase-based counterparts, preventing them from becoming widely useful.",
        "Can we combine the merits of both approaches?",
        "While other authors have explored the possibilities of enhancing phrase-based decoding with syntax-aware reordering (Galley and Manning, 2008), we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion?",
        "in theory",
        "in practice",
        "phrase-based tree-to-string",
        "exponential polynomial",
        "quadratic linear",
        "Watanabe et al.",
        "(2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007).",
        "However, this algorithm even with the beam search still runs in quadratic-time in practice.",
        "Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice.",
        "We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, tree-to-string translation (Liu et al., 2006; Huang et al., 2006), since this is the simplest and fastest among syntax-based approaches.",
        "We develop an incremental dynamic programming algorithm and make the following contributions:",
        "• we show that, unlike previous work, our incremental decoding algorithm runs in average-case polynomial-time in theory for tree-to-string models, and the beam search version runs in linear-time in practice (see Table 1);",
        "• large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++) (Koehn",
        "• furthermore, on the same tree-to-string system, incremental decoding is slightly faster than the standard cube pruning method at the same level of translation quality;",
        "• this is also the first linear-time incremental decoder that performs global reordering.",
        "We will first briefly review phrase-based decoding in this section, which inspires our incremental algorithm in the next section."
      ]
    },
    {
      "heading": "2. Background: Phrase-based Decoding",
      "text": [
        "We will use the following running example from Chinese to English to explain both phrase-based and syntax-based decoding throughout this paper:",
        "o Bùshî i yü 2 Shalong 3 jüxing 4 le 5 huitan 6Bush with Sharon hold -ed meeting",
        "'Bush held talks with Sharon'",
        "Phrase-based decoders generate partial target-language outputs in left-to-right order in the form of hypotheses (Koehn, 2004).",
        "Each hypothesis has a coverage vector capturing the source-language words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment.",
        "This process can be formalized as a deductive system.",
        "For example, the following deduction step grows a hypothesis by the phrase-pair (yü Shäälong, with Sharon) covering Chinese span [1-3]:",
        "where a • in the coverage vector indicates the source word at this position is \"covered\" and where w and w' = w+c+d are the weights of the two hypotheses, respectively, with c being the cost of the phrase-pair, and d being the distortion cost.",
        "To compute d we also need to maintain the ending position of the last phrase (the 3 and 6 in the coverage vector).",
        "To add a bigram model, we split each – LM item above into a series of +LM items; each +LM item has the form (v,a ) where a is the last word of the hypothesis.",
        "Thus a +LM version of (1) might be:",
        "where the score of the resulting +LM item now includes a combination cost due to the bigrams formed when applying the phrase-pair.",
        "The complexity of this dynamic programming algorithm for g-gram decoding is O(2nn\\V|a_) where n is the sentence length and |V| is the English vocabulary size (Huang and Chiang, 2007).",
        "Figure 1: Beam search in phrase-based decoding expands the hypotheses in the current bin (#2) into longer ones.",
        "held x2 with x1 yü jüxing le",
        "To make the exponential algorithm practical, beam search is the standard approximate search method (Koehn, 2004).",
        "Here we group +LM items into n bins, with each bin Bi hosting at most b items that cover exactly i Chinese words (see Figure 1).",
        "The complexity becomes O(nb) because there are a total of O(nb) items in all bins, and to expand each item we need to scan the whole coverage vector, which costs O(n).",
        "This quadratic complexity is still too slow in practice and we often set a small distortion limit of dmax (say, 5) so that no jumps longer than dmax are allowed.",
        "This method reduces the complexity to O(nbdmax) but fails to capture longdistance reorderings (Galley and Manning, 2008)."
      ]
    },
    {
      "heading": "3. Incremental Decoding for Tree-to-String",
      "text": [
        "Translation",
        "We will first briefly review tree-to-string translation paradigm and then develop an incremental decoding algorithm for it inspired by phrase-based decoding.",
        "A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) performs translation in two steps: parsing and decoding.",
        "A parser first parses the source language input into a 1-best tree T, and the decoder then searches for the best derivation (a se(a) Büshi [yü Shäälong ]1 [jüxing le huitan ]2",
        "jj 1-best parser",
        "yü Shalong jüxing le hüitan Shalong jüxing le hüitan hüitai n Shaalong",
        "Figure 3: An example derivation of tree-to-string translation (much simplified from Mi et al.",
        "(2008)).",
        "Shaded regions denote parts of the tree that matches the rule.",
        "quence of translation steps) d* that converts source tree T into a target-language string.",
        "Figure 3 shows how this process works.",
        "The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps.",
        "First, at the root node, we apply rule t1 preserving the top-level word-order which results in two unfinished subtrees, NP@1 and VP@ in (c).",
        "Here X@n denotes a tree node of label X at tree address n (Shieber et al., 1995).",
        "(The root node has address e, and the first child of node n has address n-1, etc.)",
        "Then rule r2 grabs the Büshi subtree and transliterate it into the English word",
        "_•_",
        "\\",
        "_•_",
        "_",
        "\\",
        "_•_",
        "_•_",
        "•_",
        "_",
        "\\",
        "1",
        "2",
        "3",
        "4",
        "5",
        "Table 2: Summary oftime complexities ofvarious algorithms.",
        "b is the beam width, V is the English vocabulary, and c is the number of translation rules per node.",
        "As a special case, phrase-based decoding with distortion limit dmax is O(nbdmax).",
        "*: incremental decoding algorithms.",
        "\"Bush\".",
        "Similarly, rule r3 shown in Figure 2 is applied to the VP subtree, which swaps the two NPs, yielding the situation in (d).",
        "Finally two phrasal rules r4 and r5 translate the two remaining NPs and finish the translation.",
        "In this framework, decoding without language model ( – LM decoding) is simply a linear-time depth-first search with memoization (Huang et al., 2006), since a tree of n words is also of size O(n) and we visit every node only once.",
        "Adding a language model, however, slows it down significantly because we now have to keep track of target-language boundary words, but unlike the phrase-based case in Section 2, here we have to remember both sides the leftmost and the rightmost boundary words: each node is now split into +LM items like (n a * b) where n is a tree node, and a and b are left and right English boundary words.",
        "For example, a bigram +LM item for node VP® might be",
        "This is also the case with other syntax-based models like Hiero or GHKM: language model integration overhead is the most significant factor that causes syntax-based decoding to be slow (Chiang, 2007).",
        "In theory +LM decoding is O(ncjV|4(g-1)), where V denotes English vocabulary (Huang, 2007).",
        "In practice we have to resort to beam search again: at each node we would only allow top-b +LM items.",
        "With beam search, tree-to-string decoding with an integrated language model runs in time O(ncb), where b is the size of the beam at each node, and c is (maximum) number of translation rules matched at each node (Huang, 2007).",
        "See Table 2 for a summary.",
        "Can we borrow the idea of phrase-based decoding, so that we also grow the hypothesis strictly left-to-right, and only need to maintain the rightmost boundary words?",
        "The key intuition is to adapt the coverage-vector idea from phrase-based decoding to tree-to-string decoding.",
        "Basically, a coverage-vector keeps track of which Chinese spans have already been translated and which have not.",
        "Similarly, here we might need a \"tree coverage-vector\" that indicates which subtrees have already been translated and which have not.",
        "But unlike in phrase-based decoding, we can not simply choose any arbitrary uncovered subtree for the next step, since rules already dictate which subtree to visit next.",
        "In other words what we need here is not really a tree coverage vector, but more of a derivation history.",
        "We develop this intuition into an agenda represented as a stack.",
        "Since tree-to-string decoding is a top-down depth-first search, we can simulate this recursion with a stack of active rules, i.e., rules that are not completed yet.",
        "For example we can simulate the derivation in Figure 3 as follows.",
        "At the root node IP®e, we choose rule t1, and push its English-side to the stack, with variables replaced by matched tree nodes, here x1 for NP® and x2 for VP®.",
        "So we have the following stack where the dot.",
        "indicates the next symbol to process in the English word-order.",
        "Since node NP® is the first in the English word-order, we expand it first, and push rule r2 rooted at NP to the stack:",
        "[.",
        "NP® VP® ] [.",
        "Bush].",
        "Since the symbol right after the dot in the top rule is a word, we immediately grab it, and append it to the current hypothesis, which results in the new stack [.",
        "NP® VP® ] [Bush .",
        "].",
        "Now the top rule on the stack has finished (dot is at the end), so we trigger a \"pop\" operation which pops the top rule and advances the dot in the second-to-top rule, denoting that NP®1 is now completed:",
        "[NP®1 .",
        "VP®2].",
        "Figure 4: Simulation of tree-to-string derivation in Figure 3 in the incremental decoding algorithm.",
        "Actions: p, predict; s, scan; c, complete (see Figure 5).",
        "Complete Goal",
        "Figure 5: Deductive system for the incremental tree-to-string decoding algorithm.",
        "Function last) returns the rightmost g – 1 words (for g-gram LM), and match (n, C (r)) tests matching of rule r against the subtree rooted at node n. C(r) and E(r) are the Chinese and English sides of rule r, and function f (n, E(r)) = [x4 i-> n-var(r) replaces each variable Xj on the English side of the rule with the descendant node n-var (i) under n that matches Xj.",
        "stack",
        "hypothesis",
        "[ IP@e / ]",
        "1 <s> .",
        "1± </s>\\",
        "<s>",
        "p",
        "[<s>.",
        "IP®e </s>] [.",
        "NP® VP®]",
        "<s>",
        "p",
        "[<s>.",
        "IP®e </s>] [.",
        "NP® VP®] [.",
        "Bush]",
        "<s>",
        "s",
        "[<s>.",
        "IP® </s>] [.",
        "NP® VP®] [Bush. ]",
        "<s> Bush",
        "c",
        "[<s>.",
        "IP® </s>][NP@1 .",
        "VP®]",
        "<s> Bush",
        "p",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [.",
        "heklNP®2-2-3 withNP®]",
        "<s> Bush",
        "s",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [held .",
        "NP®-- withNP®']",
        "<s> Bush held",
        "p",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [held .",
        "NP®2-2-3 withNP®'1-2] [.",
        "talks]",
        "<s> Bush held",
        "s",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [held .",
        "NP®2-2-3 withNP®2-1-2] [talks . ]",
        "<s> Bush held talks",
        "c",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [heldNP®2-2-3 .",
        "withNP®2-1-2]",
        "<s> Bush held talks",
        "s",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [held NP®2-2-3 with.",
        "NP®2-1-2]",
        "<s> Bush held talks with",
        "p",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [held NP®2-2-3 with.",
        "NP®2-1-2] [.",
        "Sharon]",
        "<s> Bush held talks with",
        "s",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [held NP®2-2-3 with.",
        "NP®2-1-2] [Sharon. ]",
        "<s> Bush held talks with Sharon",
        "c",
        "[<s>.",
        "IP®e </s>] [NP® .",
        "VP®] [heldNP®2-2-3 withNP®2-1-2. ]",
        "<s> Bush held talks with Sharon",
        "c",
        "[<s>.",
        "IP®e </s>] [NP® VP®. ]",
        "<s> Bush held talks with Sharon",
        "c",
        "[ IP®e / ]",
        "1 <s> 1± .",
        "</s>|",
        "<s> Bush held talks with Sharon",
        "s",
        "[ IP®e / ]",
        "1 <s> i± </s>.",
        "I",
        "<s> Bush held talks with Sharon </s>",
        "The next step is to expand VP®, and we use rule r3and push its English-side \"VP – held x2 with x1\" onto the stack, again with variables replaced by matched nodes:",
        "Note that this is a reordering rule, and the stack always follows the English word order because we generate hypothesis incrementally left-to-right.",
        "Figure 4 works out the full example.",
        "We formalize this algorithm in Figure 5.",
        "Each item ( s, p) consists of a stack s and a hypothesis p. Similar to phrase-based dynamic programming, only the last g – 1 words of p are part of the signature for decoding with g-gram LM.",
        "Each stack is a list of dotted rules, i.e., rules with dot positions indicting progress, in the style of Earley (1970).",
        "We call the last (rightmost) rule on the stack the top rule, which is the rule being processed currently.",
        "The symbol after the dot in the top rule is called the next symbol, since it is the symbol to expand or process next.",
        "Depending on the next symbol a, we can perform one of the three actions:",
        "• if a is a node n, we perform a Predict action which expands n using a rule r that can pattern-match the subtree rooted at n; we push r is to the stack, with the dot at the beginning;",
        "• if a is an English word, we perform a Scan action which immediately adds it to the current hypothesis, advancing the dot by one position;",
        "• if the dot is at the end of the top rule, we perform a Complete action which simply pops stack and advance the dot in the new top rule.",
        "Unlike phrase-based models, we show here that incremental decoding runs in average-case polynomial-time for tree-to-string systems.",
        "Lemma 1.",
        "For an input sentence of n words and its parse tree of depth d, the worst-case complexity of our algorithm is f (n,d) = c(cr)d|V|g-1 = O((cr)dng-1), assuming relevant English vocabulary | V | = O(n), and where constants c, r and g are the maximum number ofrules matching each tree node, the maximum arityofa rule, andthe language-model order, respectively.",
        "Proof.",
        "The time complexity depends (in part) on the number of all possible stacks for a tree of depth d. A stack is a list of rules covering a path from the root node to one of the leaf nodes in the following form:",
        "where n1 = e is the root node and ns is a leaf node, with stack depth s < d. Each rule > 1) expands node ni-1, and thus has c choices by the definition of grammar constant c. Furthermore, each rule in the stack is actually a dotted-rule, i.e., it is associated with a dot position ranging from 0 to r, where r is the arity of the rule (length of English side of the rule).",
        "So the total number of stacks is O((cr)d).",
        "Besides the stack, each state also maintains (g – 1) rightmost words of the hypothesis as the language model signature, which amounts to O(|V|g-).",
        "So the total number of states is O((cr)d|V|g-).",
        "Following previous work (Chiang, 2007), we assume a constant number of English translations for each foreign word in the input sentence, so | V| = O(n).",
        "And as mentioned above, for each state, there are c possible expansions, so the overall time complexity is f (n, d) = c(cr)d|V |g- = O((cr)dng-1).",
        "□",
        "We do average-case analysis below because the tree depth (height) for a sentence of n words is a random variable: in the worst-case it can be linear in n (degenerated into a linear-chain), but we assume this adversarial situation does not happen frequently, and the average tree depth is O(log n).",
        "Theorem 1.",
        "Assume for each n, the depth of a parse tree of n words, notated dn, distributes normally with logarithmic mean and variance, i.e., dn ~ N(/xn, ), where /xn = O(log n) and 0^ = O(logn), then the average-case complexity ofthe algorithm is h(n) = O(nk log (cr)+g-) for constant k, thus polynomial in n.",
        "Proof.",
        "From Lemma 1 and the definition of average-case complexity, we have where Ex^D [•] denotes the expectation with respect to the random variable x in distribution D.",
        "Since dn ~ N(/xn, a) is a normal distribution, dn log(cr) ~ N(/t',a ) is also a normal distribution, where // = /tn log(cr) and a' = an log(cr).",
        "Therefore exp(dnlog(cr)) is a log-normal distribution, and by the property of log-normal distribution, its expectation is exp (// + <t'/2).",
        "So we have",
        "Plug it back to Equation (2), and we have the average-case complexity",
        "Since k, c, r and g are constants, the average-case complexity is polynomial in sentence length n. □",
        "The assumption dn ~ N(O(logn),O(logn)) will be empirically verified in Section 5.",
        "Though polynomial complexity is a desirable property in theory, the degree of the polynomial, O(log cr) might still be too high in practice, depending on the translation grammar.",
        "To make it lineartime, we apply the beam search idea from phrase-based again.",
        "And once again, the only question to decide is the choice of \"binning\": how to assign each item to a particular bin, depending on their progress?",
        "While the number of Chinese words covered is a natural progress indicator for phrase-based, it does not work for tree-to-string because, among the three actions, only scanning grows the hypothesis.",
        "The prediction and completion actions do not make real progress in terms of words, though they do make progress on the tree.",
        "So we devise a novel progress indicator natural for tree-to-string translation: the number of tree nodes covered so far.",
        "Initially that number is zero, and in a prediction step which expands node n using rule r, the number increments by |C(r)|, the size of the Chinese-side treelet of r. For example, a prediction step using rule r3 in Figure 2 to expand VP®2 will increase the tree-node count by |C(r3)| = 6, since there are six tree nodes in that rule (not counting leaf nodes or variables).",
        "Scanning and completion do not make progress in this definition since there is no new tree node covered.",
        "In fact, since both of them are deterministic operations, they are treated as \"closure\" operators in the real implementation, which means that after a prediction, we always do as many scanning/completion steps as possible until the symbol after the dot is another node, where we have to wait for the next prediction step.",
        "This method has |T | = O(n) bins where |T | is the size of the parse tree, and each bin holds b items.",
        "Each item can expand to c new items, so the overall complexity of this beam search is O(ncb), which is linear in sentence length."
      ]
    },
    {
      "heading": "4. Related Work",
      "text": [
        "The work of Watanabe et al.",
        "(2006) is closest in spirit to ours: they also design an incremental decoding algorithm, but for the hierarchical phrase-based system (Chiang, 2007) instead.",
        "While we leave detailed comparison and theoretical analysis to a future work, here we point out some obvious differences:",
        "1. due to the difference in the underlying translation models, their algorithm runs in O(nb) time with beam search in practice while ours is linear.",
        "This is because each prediction step now has O(n) choices, since they need to expand nodes like VP[1, 6] as:",
        "where the midpoint i in general has O(n) choices (just like in CKY).",
        "In other words, their grammar constant c becomes O(n).",
        "2. different binning criteria: we use the number of tree nodes covered, while they stick to the original phrase-based idea of number of Chinese words translated;",
        "3. as a result, their framework requires grammar transformation into the binary-branching Greibach Normal Form (which is not always possible) so that the resulting grammar always contain at least one Chinese word in each rule in order for a prediction step to always make progress.",
        "Our framework, by contrast, works with any grammar.",
        "Besides, there are some other efforts less closely related to ours.",
        "As mentioned in Section 1, while we focus on enhancing syntax-based decoding with phrase-based ideas, other authors have explored the reverse, but also interesting, direction of enhancing phrase-based decoding with syntax-aware reordering.",
        "For example Galley and Manning (2008) propose a shift-reduce style method to allow hiearar-chical non-local reorderings in a phrase-based decoder.",
        "While this approach is certainly better than pure phrase-based reordering, it remains quadratic in runtime with beam search.",
        "Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5.",
        "It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding.",
        "We leave this point to future work.",
        "Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010).",
        "In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, – LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system ofMoses.",
        "Furturemore we also compare our incremental decoder with the standard cube pruning approach on the same tree-to-string decoder.",
        "Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively.",
        "We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules.",
        "We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus.",
        "At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern-matching (Mi et al., 2008).",
        "We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set.",
        "We evaluate the translation quality using the BLEU-4 metric, which is calculated by the script mteval-v13a.pl with its default setting which is case-insensitive matching of n-grams.",
        "We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system's BLEU score on development set.",
        "We first verify the assumptions we made in Section 3.3 in order to prove the theorem that tree depth (as a random variable) is normally-distributed with O(log n) mean and variance.",
        "Qualitatively, we verified that for most n, tree depth d(n) does look like a normal distribution.",
        "Quantitatively, Figure 6 shows that average tree height correlates extremely well with 3.5 log n, while tree height variance is bounded by 5.5 logn.",
        "We implemented our incremental decoding algorithm in Python, and test its performance on the development set.",
        "We first compare it with the standard cube pruning approach (also implemented in Python) on the same tree-to-string system.",
        "Fig-",
        "incremental cube pruning",
        "(a) decoding time against sentence length (b) BLEU score against decoding time",
        "Figure 7: Comparison with cube pruning.",
        "The scatter plot in (a) confirms that our incremental decoding scales linearly with sentence length, while cube pruning super-linearly (b = 50 for both).",
        "The comparison in (b) shows that at the same level of translation quality, incremental decoding is slightly faster than cube pruning, especially at smaller beams.",
        "Sentence Length (n)",
        "Figure 6: Mean and variance of tree depth vs. sentence length.",
        "The mean depth clearly scales with 3.5 log n, and the variance is bounded by 5.5 log n.",
        "Figure 8: Comparison of our incremental tree-to-string decoder with Moses in terms of speed.",
        "Moses is shown with various distortion limits (0,6,10, +oo; optimal: 10).",
        "ure 7(a) is a scatter plot of decoding times versus sentence length (using beam b = 50 for both systems), where we confirm that our incremental decoder scales linearly, while cube pruning has a slight tendency of superlinearity.",
        "Figure 7(b) is a side-by-side comparison of decoding speed versus translation quality (in BLEU scores), using various beam sizes for both systems (b=10-70 for cube pruning, and b= 10 – 110 for incremental).",
        "We can see that incremental decoding is slightly faster than cube pruning at the same levels of translation quality, and the difference is more pronounced at smaller beams: for",
        "number of (non-unique) pops from priority queues.",
        "example, at the lowest levels of translation quality (BLEU scores around 29.5), incremental decoding takes only 0.12 seconds, which is about 4 times as fast as cube pruning.",
        "We stress again that cube pruning and incremental decoding are not mutually exclusive, and rather they could potentially be combined to further speed up decoding.",
        "We also compare with the standard phrase-based system of Moses (Koehn et al., 2007), with standard settings except for the ttable limit, which we set to 100.",
        "Figure 8 compares our incremental decoder system/decoder BLEU time",
        "Table 3: Final BLEU score and speed results on the test data (691 sentences), compared with Moses and cube pruning.",
        "Time is in seconds per sentence, including parsing time (0.21s) for the two tree-to-string decoders.",
        "with Moses at various distortion limits (dmax=0, 6, 10, and +oo).",
        "Consistent with the theoretical analysis in Section 2, Moses with no distortion limit (dmax = +oo) scale quadratically, and monotone decoding (dmax = 0) scale linearly.",
        "We use MERT to tune the best weights for each distortion limit, and dmax = 10 performs the best on our dev set.",
        "Table 3 reports the final results in terms of BLEU score and speed on the test set.",
        "Our linear-time incremental decoder with the small beam of size b = 10 achieves a BLEU score of 29.54, comparable to Moses with the optimal distortion limit of 10 (BLEU score 29.41).",
        "But our decoding (including source-language parsing) only takes 0.32 seconds a sentences, which is more than 30 times faster than Moses.",
        "With a larger beam of b = 50 our BLEU score increases to 29.96, which is a half BLEU point better than Moses, but still about 15 times faster."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "We have presented an incremental dynamic programming algorithm for tree-to-string translation which resembles phrase-based based decoding.",
        "This algorithm is the first incremental algorithm that runs in polynomial-time in theory, and linear-time in practice with beam search.",
        "Large-scale experiments on a state-of-the-art tree-to-string decoder confirmed that, with a comparable (or better) translation quality, it can run more than 30 times faster than the phrase-based system of Moses, even though ours is in Python while Moses in C++.",
        "We also showed that it is slightly faster (and scale better) than the popular cube pruning technique.",
        "For future work we would like to apply this algorithm to forest-based translation and hierarchical system by pruning the first-pass – LM forest.",
        "We would also combine cube pruning with our incremental algorithm, and study its performance with higher-order language models."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank David Chiang, Kevin Knight, and Jonanthan Graehl for discussions and the anonymous reviewers for comments.",
        "In particular, we are indebted to the reviewer who pointed out a crucial mistake in Theorem 1 and its proof in the submission.",
        "This research was supported in part by DARPA, under contract HR0011-06-C-0022 under subcontract to BBN Technologies, and under DOI-NBC Grant N10AP20031, and in part by the National Natural Science Foundation of China, Contracts 60736014 and 90920004.",
        "Moses (optimal dmax=10)",
        "29.41",
        "10.8",
        "tree-to-str: cube pruning (b= 10) tree-to-str: cube pruning (b=20)",
        "29.51 29.96",
        "0.65 0.96",
        "tree-to-str: incremental (b= 10) tree-to-str: incremental (b=50)",
        "29.54 29.96",
        "0.32 0.77"
      ]
    }
  ]
}
