{
  "info": {
    "authors": [
      "Gabor Angeli",
      "Percy Liang",
      "Dan Klein"
    ],
    "book": "EMNLP",
    "id": "acl-D10-1049",
    "title": "A Simple Domain-Independent Probabilistic Approach to Generation",
    "url": "https://aclweb.org/anthology/D10-1049",
    "year": 2010
  },
  "references": [
    "acl-D09-1042",
    "acl-N04-1015",
    "acl-P02-1003",
    "acl-P02-1040",
    "acl-P06-1130",
    "acl-P06-1139",
    "acl-P07-1121",
    "acl-P09-1011",
    "acl-W04-0601",
    "acl-W05-1510",
    "acl-W06-1417",
    "acl-W09-0603",
    "acl-W09-0607"
  ],
  "sections": [
    {
      "text": [
        "Gabor Angeli Percy Liang Dan Klein",
        "UC Berkeley UC Berkeley UC Berkeley",
        "Berkeley, CA 94720 Berkeley, CA 94720 Berkeley, CA 94720",
        "We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework.",
        "In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively.",
        "We deployed our system in three different domains – Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-of-the-art domain-specific systems both in terms of BLEU scores and human evaluation."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records.",
        "While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al.",
        "(2003), Green (2006), Turner et al.",
        "(2009), Reiter et al.",
        "(2005), inter alia), it is often difficult to adapt them across different domains.",
        "Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratna-",
        "Mooney (2008), Lu et al.",
        "(2009), etc.)",
        "are typically handled separately.",
        "Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework.",
        "We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output).",
        "We use the model of Liang et al.",
        "(2009) to automatically induce the correspondences between words in the text and the actual database records mentioned.",
        "We break up the full generation process into a sequence of local decisions, training a log-linear classifier for each type of decision.",
        "We use a simple but expressive set of domain-independent features, where each decision is allowed to depend on the entire history of previous decisions, as in the model of Ratnaparkhi (2002).",
        "These long-range contextual dependencies turn out to be critical for accurate generation.",
        "More specifically, our model is defined in terms of three types of decisions.",
        "The first type chooses records from the database (macro content selection) – for example, wind speed, in the case of generating weather forecasts.",
        "The second type chooses a subset of fields from a record (micro content selection) – e.g., the minimum and maximum temperature.",
        "The third type chooses a suitable template to render the content (surface realization) – e.g., winds between [min] and [max] mph; templates are automatically extracted from training data.",
        "We tested our approach in three domains: ROBOCUP, for sportscasting (Chen and Mooney, 2008); SUMTIME, for technical weather forecast generation (Reiter et al., 2005); and WeatherGov, for common weather forecast generation (Liang et al., 2009).",
        "We performed both automatic (BLEU) and human evaluation.",
        "On WeatherGov, we",
        "pass(arg1=purple6, arg2=purple3)",
        "kick(arg1=purple3) badPass(arg1=purple3,arg2=pink9) turnover(arg1=purple3,arg2=pink9)",
        "w: purple3 made a bad pass that was picked off by pink9",
        "(a) RoBQCUP",
        "w: a 20 percent chance of showers after midnight .",
        "increasing clouds with a low around 48 southwest wind between 5 and 10 mph",
        "(b) WeatherGov (c) SumTime",
        "Figure 1: Example scenarios (a scenario is a world state s paired with a text w) for each of the three domains.",
        "Each row in the world state denotes a record.",
        "Our generation task is to map a world state s (input) to a text w (output).",
        "Note that this mapping involves both content selection and surface realization.",
        "achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a twofold improvement over a model similar to that of Liang et al.",
        "(2009).",
        "On Robocup and SumTime, we achieved results comparable to the state-of-the-art.",
        "most importantly, we obtained these results with a general-purpose approach that we believe is simpler than current state-of-the-art systems."
      ]
    },
    {
      "heading": "2. Setup and Domains",
      "text": [
        "Our goal is to generate a text given a world state.",
        "The world state, denoted s, is represented by a set of database records.",
        "Define t to be a set of record types, where each record type t g t is associated with a set of fields Fields(t).",
        "Each record r g s has a record type r.t g t and a field value r.v[f ] for each field f G Fields (t).",
        "The text, denoted w, is represented by a sequence of tokenized words.",
        "We use the term scenario to denote a world state s paired with a text w.",
        "in this paper, we conducted experiments on three domains, which are detailed in the following subsections.",
        "Example scenarios for each domain are detailed in Figure 1.",
        "A world state in the Robocup domain is a set of event records (meaning representations in the terminology of Chen and Mooney (2008)) generated by a robot soccer simulator.",
        "For example, the record pass(arg1=pink1,arg2=pink5) denotes a passing event; records of this type (pass) have two fields: arg1 (the agent) and arg2 (the recipient).",
        "As the game progresses, human commentators talk about some of the events in the game, e.g., purple3 made a bad pass that was picked off by pink9.",
        "We used the dataset created by Chen and Mooney (2008), which contains 1919 scenarios from the 2001-2004 Robocup finals.",
        "Each scenario consists of a single sentence representing a fragment of a commentary on the game, paired with a set of candidate records, which were recorded within five seconds of the commentary.",
        "The records in the Robocup dataset data were aligned by Chen and Mooney (2008).",
        "Each scenario contains on average |s| = 2.4 records and 5.7 words.",
        "See Figure 1(a) for an example of a scenario.",
        "Content selection in this domain is choosing the single record to talk about, and surface realization is talking about it.",
        "Reiter et al.",
        "(2005) developed a generation system and created the SumTime-Meteo corpus, which consists of marine wind weather forecasts used by offshore oil rigs, generated by the output of weather simulators.",
        "More specifically, these forecasts describe various aspects of the wind at different times during the forecast period.",
        "We used the version of the SumTime-Meteo corpus created by Belz (2008).",
        "The dataset consists of 469 scenarios, each containing on average | s| = 2.6 records and 16.2 words.",
        "See Figure 1(c) for an example of a scenario.",
        "This task requires no content selection, only surface realization: The records are given in some fixed order and the task is to generate from each of these records in turn; of course, due to contextual dependencies, these records cannot be generated independently.",
        "In the WeatherGov domain, the world state contains detailed information about a local weather forecast (e.g., temperature, rain chance, etc.).",
        "The text is a short forecast report based on this information.",
        "We used the dataset created by Liang et al.",
        "(2009).",
        "The world state is summarized by records which aggregate measurements over selected time intervals.",
        "The dataset consists of 29,528 scenarios, each containing on average |s| =36 records and 28.7 words.",
        "See Figure 1(b) for an example of a scenario.",
        "While SumTime and WeatherGov are both weather domains, there are significant differences between the two.",
        "SumTime forecasts are intended to be read by trained meteorologists, and thus the text is quite abbreviated.",
        "on the other hand, WeatherGov texts are intended to be read by the general public and thus is more English-like.",
        "Furthermore, SumTime does not require content selection, whereas content selection is a major focus of WeatherGov.",
        "Indeed, on average, only 5 of 36 records are actually mentioned in a WeatherGov scenario.",
        "Also, WeatherGov is more complex: The text is more varied, there are multiple record types, and there are about ten times as many records in each world state.",
        "Generation Process",
        "for i = 1,2,... : choose a record rig s if ri = stop: return choose a field set Fi c Fields(ri.t) choose a template Ti G Templates (ri.t, Fi)",
        "Figure 2: Pseudocode for the generation process.",
        "The generated text w is a deterministic function of the decisions."
      ]
    },
    {
      "heading": "3. The Generation Process",
      "text": [
        "To model the process of generating a text w from a world state s, we decompose the generation process into a sequence of local decisions.",
        "There are two aspects of this decomposition that we need to specify: (i) how the decisions are structured; and (ii) what pieces of information govern the decisions.",
        "The decisions are structured hierarchically into three types of decisions: (i) record decisions, which determine which records in the world state to talk about (macro content selection); (ii) field set decisions, which determine which fields of those records to mention (micro content selection); and (iii) template decisions, which determine the actual words to use to describe the chosen fields (surface realization).",
        "Figure 2 shows the pseudocode for the generation process, while Figure 3 depicts an example of the generation process on a WeatherGov scenario.",
        "Each of these decisions is governed by a set of feature templates (see Figure 4), which are represented as functions of the current decision and past decisions.",
        "The feature weights are learned from training data (see Section 4.3).",
        "We chose a set of generic domain-independent feature templates, described in the sections below.",
        "These features can, in general, depend on the current decision and all previous decisions.",
        "For example, referring to Figure 4, R2 features on the record choice depend on all the previous record decisions, and R5 features depend on the most recent template decision.",
        "This is in contrast with most systems for content selection (Barzilay and Lee, 2004) and surface realization (Belz, 2008), where decisions must decompose locally according to either a graph or tree.",
        "The ability to use global features in this manner is",
        "Decisions skyCoveri: skyCover(time=5pm-6am,mode=50-75) ri = skyCoveri r2 = temperaturei Text mostly cloudy , with a low around 45 .",
        "Specific active (nonzero) features for highlighted decisions",
        "Figure 3: The generation process on an example WeatherGov scenario.",
        "The figure is divided into two parts: The upper part of the figure shows the generation of text from the world state via a sequence of seven decisions (in boxes).",
        "Three of these decisions are highlighted and the features that govern these decisions are shown in the lower part of the figure.",
        "Note that different decisions in the generation process would result in different features being active (nonzero).",
        "Feature Templates",
        "Figure 4: Feature templates that govern the record, field set, and template decisions.",
        "Each line specifies the name, informal description, and formal description of a set of features, obtained by ranging * over possible values (for example, for [ri.t = *], * ranges over all record types T).",
        "Notation: [e] returns 1 if the expression e is true and 0 if it is false.",
        "These feature templates are domain-independent; that is, they are used to create features automatically across domains.",
        "Feature templates marked with t are included in our baseline system (Section 5.2).",
        "one of the principal advantages of our approach.",
        "3.1 Record Decisions",
        "Record decisions are responsible for macro content selection.",
        "Each record decision chooses a record ri from the world state s according to features of the following types:",
        "R1 captures the discourse coherence aspect of content selection; for example, we learn that windSpeed tends to follow windDir (but not always).",
        "R2 captures an unordered notion of coherence – simply which sets of record types are preferable; for example, we learn that rainChance is not generated if sleetChance already was mentioned.",
        "R3 is a coarser version of R2, capturing how likely it is to propose a record of a type that has already been generated.",
        "R4 captures the important aspect of content selection that the records chosen depend on their field values; for example, we learn that snowChance is not chosen unless there is snow.",
        "R5 allows the language model to indicate whether a S ToP record is appropriate; this helps prevent sentences from ending abruptly.",
        "Record",
        "Rlt",
        "list of last k record types",
        "\\ri.t = * and (ri-1.t,..., Ti-k.t) = *] for k G {1,2}",
        "R2",
        "set of previous record types",
        "[r^t = * and {rj.t : j < i} = *]",
        "R3",
        "record type already generated",
        "[rj.t = ri.t for some j < i]",
        "R4",
        "field values",
        "[r;.t = * and ri.v[f\\ = *] for f G FlELDS(r;.t)",
        "R5t",
        "stop under language model (LM)",
        "[ri.t = stop] x logplm(stop | previous two words generated)",
        "Field set",
        "Fit",
        "field set",
        "[Fi = *]",
        "F2",
        "field values",
        "[Fi = * and ri.v[f\\ = *] for f G Fi",
        "Template",
        "W1t",
        "base/coarse generation template",
        "[h(Ti) = *] for h G {Base, Coarse}",
        "W2",
        "field values",
        "[h(Ti) = * and ri.v[f \\ = *] for f G Fi, h G {Base, Coarse}",
        "W3t",
        "first word of template under LM",
        "logpLM (first word in Ti | previous two words)",
        "Field set decisions are responsible for micro content selection, i.e., which fields of a record are mentioned.",
        "Each field set decision chooses a subset of fields Fi from the set of fields FlELDS(ri.t) of the record ri that was just generated.",
        "These decisions are made based on two types of features:",
        "F1 captures which sets of fields are talked about together; for example, we learn that {mean} and {min, max} are preferred field sets for the windSpeed record.",
        "By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al.",
        "(2009) generates a sequence of fields in which a field can only depend on the previous one.",
        "F2 allows the field set to be chosen based on the values of the fields, analogously to R4.",
        "Template decisions perform surface realization.",
        "A template is a sequence of elements, where each element is either a word (e.g., around) or a field (e.g., [min]).",
        "Given the record ri and field set Fi that we are generating from, the goal is to choose a template Ti (Section 4.3.2 describes how we define the set of possible templates).",
        "The features that govern the choice of Ti are as follows:",
        "W1 captures a priori preferences for generation templates given field sets.",
        "There are two ways to control this preference, BASE and COARSE.",
        "BASE(Ti) denotes the template Ti itself, thus allowing us to remember exactly which templates were useful.",
        "To guard against overfitting, we also use COARSE(Ti), which maps Ti to a coarsened version of Ti, in which more words are replaced with their associated fields (see Figure 5 for an example).",
        "W2 captures a dependence on the values of fields in the field set, and is analogous to R4 and F2.",
        "Finally, W3 contributes a language model probability, to ensure smooth transitions between templates.",
        "After Ti has been chosen, each field in the template is replaced with a word given the corresponding field value in the world state.",
        "In particular, a word is chosen from the parameters learned in the model of Liang et al.",
        "(2009).",
        "In the example in Figure 3, the [min] field in T2 has value 44, which is rendered to the word 45 (rounding and other noisy deviations are common in the WEATHERGOV domain)."
      ]
    },
    {
      "heading": "4. Learning a Probabilistic Model",
      "text": [
        "Having described all the features, we now present a conditional probabilistic model over texts w given world states s (Section 4.1).",
        "Section 4.2 describes how to use the model for generation, and Section 4.3 describes how to learn the model.",
        "Recall from Section 3 that the generation process generates ri, Fi, Ti, r2, F2, T2,..., STOP.",
        "To unify notation, denote this sequence of decisions as d =",
        "(db ..., d|d|).Our probability model is defined as follows:",
        "where d<j = ..., dj-i) is the history of decisions and 0 are the model parameters (feature weights).",
        "Note that the text w (the output) is a deterministic function of the decisions d. We use the features described in Section 3 to define a log-linear model for each decision:",
        "where 0 are all the parameters (feature weights), is the feature vector for the j-th decision, and Dj is the domain of the j-th decision (either records, field sets, or templates).",
        "This chaining of log-linear models was used in Ratnaparkhi (1998) for tagging and parsing, and in Ratnaparkhi (2002) for surface realization.",
        "The ability to condition on arbitrary histories is a defining property of these models.",
        "Suppose we have learned a model with parameters 0 (how to obtain 0 is discussed in Section 4.3).",
        "Given a world state s, we would like to use our model to generate an output text w via a decision sequence d.",
        "in our experiments, we choose d by sequentially choosing the best decision in a greedy fashion (until the stop record is generated):",
        "which can be solved by maximize the (conditional) likelihood of the training data:",
        "dj = argmaxp(dj | d<j, s; 0).",
        "Alternatively, instead of choosing the best decision at each point, we can sample from the distribution: dj ~ p(dj | d<j, s; 0), which provides more diverse generated texts at the expense of a slight degradation in quality.",
        "Both greedy search and sampling are very efficient.",
        "Another option is to try to find the Viterbi decision sequence, i.e., the one with the maximum joint probability: d = argmaxd, p(d' | s; 0).",
        "However, this computation is intractable due to features depending arbitrarily on past decisions, making dynamic programming infeasible.",
        "We tried using beam search to approximate this optimization, but we actually found that beam search performed worse than greedy.",
        "Belz (2008) also found that greedy was more effective than viterbi for their model.",
        "Now we turn our attention to learning the parameters 0 of our model.",
        "We are given a set of N scenarios {(s(i), w(i))}N=i as training data.",
        "Note that our model is defined over the decision sequence d which contains information not present in w. in Sections 4.3.1 and 4.3.2, we show how we fill in this missing information to obtain d(i) for each training scenario i.",
        "Assuming this missing information is filled, we end up with a standard supervised learning problem,",
        "Yllog p(d",
        "where A > 0 is a regularization parameter.",
        "The objective function in (4) is optimized using the standard L-BFGS algorithm (Liu and Nocedal, 1989).",
        "As mentioned previously, our training data includes only the world state s and generated text w, not the full sequence of decisions d needed for training.",
        "intuitively, we know what was generated but not why it was generated.",
        "We use the model of Liang et al.",
        "(2009) to impute the decisions d. They introduce a generative model p(a, w|s), where the latent alignment a specifies (1) the sequence of records that were chosen, (2) the sequence of fields that were chosen, and (3) which words in the text were spanned by the chosen records and fields.",
        "The model is learned in an unsupervised manner using EM to produce a observing only w and s.",
        "An example of an alignment is given in the left part of Figure 5.",
        "This information specifies the record decisions and a set of fields for each record.",
        "Because the induced alignments can be noisy, we need to process them to obtain cleaner template decisions.",
        "This is the subject of the next section.",
        "Given an aligned training scenario (Figure 5), we would like to extract two types of templates.",
        "For each record, an aligned training scenario specifies a sequence of fields and the text that is spanned by each field.",
        "We create a template by abstracting fields – that is, replacing the words spanned by a field by the field itself.",
        "We call the resulting template Coarse.",
        "The problem with using this template directly is that fields can be noisy due to errors from the unsupervised model.",
        "Therefore, we also create a Base template which only abstracts a subset of the fields.",
        "In particular, we define a trigger pattern which specifies a simple condition under which a field should be abstracted.",
        "For WeatherGov, we only abstract fields that",
        "Aligned training scenario",
        "skyCover temperature COARSE {[mode]) {with a [time] [min] [mean])",
        "BASE {most cloudy ,) {with a low around [min] .)",
        "Templates extracted",
        "Figure 5: An example of template extraction from an imperfectly aligned training scenario.",
        "Note that these alignments are noisy (e.g., [mean] aligns to a period).",
        "Therefore, for each record (skyCover and temperature in this case), we extract two templates: (1) a Coarse template, which takes the text spanned by the record and abstracts away all fields in the scenario ([mode], [time], [min], and [mean] in the example); and (2) a Base template, which only abstracts away fields whose spanned text matches a simple pattern (e.g., numbers in WeatherGov, corresponding to [min] in the example).",
        "span numbers; for SUMTIME, fields that span numbers and wind directions; and for Robocup, fields that span words starting with purple or pink.",
        "For each record ri, we define Ti so that BASE(Ti) and COARSE(Ti) are the corresponding two extracted templates.",
        "We restrict Fi to the set of abstracted fields in the Coarse template"
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "We now present an empirical evaluation of our system on our three domains – ROBOCUP, SUMTIME, and WeatherGov.",
        "Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al., 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output.",
        "To evaluate macro content selection, we measured the Fi score (the harmonic mean of precision and recall) of the set of records chosen with respect to the human-annotated set of records.",
        "Human Evaluation We conducted a human evaluation using Amazon Mechanical Turk.",
        "For each domain, we chose 100 scenarios randomly from the test set.",
        "We ran each system under consideration on each of these scenarios, and presented each resulting output to 10 evaluators.",
        "Evaluators were given instructions to rank an output on the basis of English fluency and semantic correctness on the following scale:",
        "Score I English Fluency I Semantic Correctness"
      ]
    },
    {
      "heading": "5. Flawless Perfect",
      "text": []
    },
    {
      "heading": "4. Good Near Perfect",
      "text": []
    },
    {
      "heading": "3. Non-native Minor Errors",
      "text": []
    },
    {
      "heading": "2. Disfluent Major Errors",
      "text": []
    },
    {
      "heading": "1. Gibberish Completely Wrong",
      "text": [
        "Evaluators were also given additional domain-specific information: (1) the background of the domain (e.g., that SumTime reports are technical weather reports); (2) general properties of the desired output (e.g., that SumTime texts should mention every record whereas WEATHERGOV texts need not); and (3) peculiarities of the text (e.g., the suffix ly in SumTime should exist as a separate token from its stem, or that pink goalie and pink1 have the same meaning in ROBOCUP).",
        "We evaluated the following systems on our three domains:",
        "• Human is the human-generated output.",
        "• OurSystem uses all the features in Figure 4 and is trained according to Section 4.3.",
        "• Baseline is OurSystem using a subset of the features (those marked with f in Figure 4).",
        "In contrast to OURSYSTEM, the included features only depend on a local context of decisions in a manner similar to the generative model of Liang et al.",
        "(2009) and the pCRU-greedy system of Belz (2008).",
        "Baseline also excludes features that depend on values of the world state.",
        "• The existing state-of-the-art domain-specific system for each domain.",
        "Following the evaluation methodology of Chen and Mooney (2008), we trained our system on three",
        "Records:",
        "skyCoveri",
        "temperaturei",
        "Fields:",
        "mode=50-75",
        "time= 17-30",
        "min=44j [mean=49",
        "Text:",
        "mostly cloudy ,",
        "with a",
        "low around",
        "45 .",
        "Records:",
        "Baseline",
        "OurSystem",
        "Table 1: Robocup results.",
        "WASPER-GEN is described in Chen and Mooney (2008).",
        "The BLEU is reported on systems that use fixed human-annotated records (in other words, we evaluate surface realization given perfect content selection).",
        "Table 2: SumTime results.",
        "The SumTime-Hybrid system is described in (Reiter et al., 2005); pCRU-greedy, in (Belz, 2008).",
        "Records: passi Fields: arg1=purple10 ^^^H arg2=purple9",
        "Text: purplelO passes back to purple9",
        "Fields: Text:",
        "arg1=purple10j arg2=purple9 purplelO kicks to purple9 purplelO passes to|| purple9",
        "WASPER-GEN^BText: purplelO passes to purple9",
        "Figure 6: Outputs of systems on an example Robocup scenario.",
        "There are some minor differences between the outputs.",
        "Recall that OurSystem differs from Baseline mostly in the addition of feature W2, which captures dependencies between field values (e.g., purplelO) and the template chosen (e.g., [arg1] passes to [arg2]).",
        "This allows us to capture value-dependent preferences for different realizations (e.g., passes to over kicks to).",
        "Also, Human uses passes back to, but this word choice requires knowledge of passing records in previous scenarios, which none of the systems have access to.",
        "it would natural, however, to add features that would capture these longer-range dependencies in our framework.",
        "Robocup games and tested on the fourth, averaging over the four train/test splits.",
        "We report the average test accuracy weighted by the number of scenarios in a game.",
        "First, we evaluated macro content selection.",
        "Table 1 shows that OurSystem significantly outperforms BASELINE and WASPER-GEN on Fi.",
        "To compare with Chen and Mooney (2008) on surface realization, we fixed each system's record decisions to the ones given by the annotated data and enforced that all the fields of that record are chosen.",
        "Table 1 shows that OurSystem significantly outperforms Baseline and is comparable to WASPER-GEN on BLEU.",
        "On human evaluation, OURSYSTEM outperforms BASELINE, but WASPER-GEN outperforms OURSYSTEM.",
        "See Figure 6 for example outputs from the various systems.",
        "The SUMTIME task only requires micro content selection and surface realization because the sequence of records to be generated is fixed; only these aspects are evaluated.",
        "Following the methodology of Belz (2008), we used fivefold cross validation.",
        "We found that using the unsupervised model of Liang et al.",
        "(2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically works better with a large amount of data; and (ii) the alignment model does not exploit the temporal structure in the SumTime world state.",
        "Therefore, we used a small set of simple regular expressions to produce aligned training scenarios.",
        "Table 2 shows that OURSYSTEM significantly outperforms Baseline as well as SUMTIME-Hybrid, a hand-crafted system, on BLEU.",
        "Note that OURSYSTEM is domain-independent and has not been specifically tuned to SumTime.",
        "However, OurSystem is outperformed by the state-of-the-art statistical system pCRU-greedy.",
        "Custom Features One of the advantages of our feature-based approach is that it is straightforward to incorporate domain-specific features to capture specific properties of a domain.",
        "To this end, we define the following set of feature templates in place of our generic feature templates from Figure 4:",
        "• F2': Existence of gusts/wind direction/wind speeds",
        "• W1': Change in wind direction (clockwise, counterclockwise, or none)",
        "System",
        "Fi",
        "BLEU*",
        "English Fluency",
        "Semantic Correctness",
        "Baseline",
        "78.7",
        "24.8",
        "4.28 ± 0.78",
        "4.15 ± 1.14",
        "OurSystem",
        "79.9",
        "28.8",
        "4.34 ± 0.69",
        "4.17 ± 1.21",
        "WASPER-GEN",
        "72.0",
        "28.7",
        "4.43 ± 0.76",
        "4.27 ± 1.15",
        "Human",
        " – ",
        " – ",
        "4.43 ± 0.69",
        "4.30 ± 1.07",
        "BLEU",
        "English Fluency",
        "Semantic Correctness",
        "Baseline",
        "32.9",
        "4.23",
        "±",
        "0.71",
        "4.26",
        "±",
        "0.85",
        "OurSystem",
        "55.1",
        "4.25",
        "±",
        "0.69",
        "4.27",
        "±",
        "0.82",
        "OurSystem-Custom",
        "62.3",
        "4.12",
        "±",
        "0.78",
        "4.33",
        "±",
        "0.91",
        "pCRU-greedy",
        "63.6",
        "4.18",
        "0.71",
        "4.49",
        "0.73",
        "SumTime-Hybrid",
        "52.7",
        " – ",
        " – ",
        "Human",
        " – ",
        "4.09 ± 0.83",
        "4.37 ± 0.87",
        "Baseline",
        "OurSystem-Custom pCRU-greedy",
        "Records: windDiri ^ I windDir2",
        "Records: windDir1 11 windDir2 I windDir2",
        "Figure 7: Outputs of systems on an example SumTime scenario.",
        "Two notable differences between OurSystem-Custom and Baseline arise due to OurSystem-Custom's value-dependent features.",
        "For example, OurSystem-Custom can choose whether to include the time field (windDir2) or not (windDiri), depending on the value of the time (F1'), thereby improving content selection.",
        "OurSystem-Custom also improves surface realization, choosing gradually decreasing over Baseline's increasing.",
        "Interestingly, this improvement comes from the joint effort of two features: W2' prefers decreasing over increasing in this case, and W5' adds the modifier gradually.",
        "An important strength of log-linear models is the ability to combine soft preferences from many features.",
        "• W2': Change in wind speed",
        "• W3': Change in wind direction and speed",
        "• W4': Existence of gust min and/or max",
        "• W5': Time elapsed since last record",
        "The resulting system, which we call OurSystem-Custom, obtains a BLEU score which is comparable to pCRU-greedy.",
        "An important aspect of our system that it is flexible and quick to deploy.",
        "According to Belz (2008), SumTime-Hybrid took twelve person-months to build, while pCRU-greedy took one month.",
        "Having developed OurSystem in a domain-independent way, we only needed to do simple reformatting upon receiving the SumTime data.",
        "Furthermore, it took only a few days to develop the custom features above to create OurSystem-Custom, which has BLEU performance comparable to the state-of-the-art pCRU-greedy system.",
        "We also conducted human evaluations on the four systems shown in Table 2.",
        "Note that this evaluation is rather difficult for Mechanical Turkers since SumTime texts are rather technical compared to those in other domains.",
        "interestingly, all systems outperform Human on English fluency; this result corroborates the findings of Belz (2008).",
        "On semantic correctness, all systems perform comparably to Human, except pCRU-greedy, which performs slightly better.",
        "See Figure 7 for a comparison of the outputs generated by the various systems.",
        "Table 3: WeatherGov results.",
        "The BLEU score is on joint content selection and surface realization and is modified to not penalize numeric deviations of at most 5.",
        "We evaluate the WeatherGov corpus on the joint task of content selection and surface realization.",
        "We split our corpus into 25,000 scenarios for training, 1,000 for development, and 3,528 for testing.",
        "In WeatherGov, numeric field values are often rounded or noisily perturbed, so it is difficult to generate precisely matching numbers.",
        "Therefore, we used a modified BLEU score where numbers differing by at most five are treated as equal.",
        "Furthermore, WeatherGov is evaluated on the joint content selection and surface realization task, unlike Robocup, where content selection and surface realization were treated separately, and SumTime, where content selection was not applicable.",
        "Table 3 shows the results.",
        "We see that OurSystem substantially outperforms Baseline, especially on BLEU score and semantic correctness.",
        "This difference shows that taking non-local context into account is very important in this domain.",
        "This result is not surprising, since WeatherGov is the most complicated of the three domains, and this complexity is exactly where non-locality is neces-",
        "Fi",
        "bleu*",
        "English Fluency",
        "Semantic Correctness",
        "Baseline",
        "22.1",
        "22.2",
        "4.07 ± 0.59",
        "3.41 ± 1.16",
        "OurSystem",
        "65.4",
        "51.5",
        "4.12 ± 0.74",
        "4.22 ± 0.89",
        "Human",
        " – ",
        " – ",
        "4.14 ± 0.71",
        "3.85 ± 0.99",
        "Records:",
        "OurSystem",
        "Fields: Text:",
        "Records: Fields: Text:",
        "skyCoveri II temperaturei 11 windDiri 11 windSpeedi mostly cloudy , with a low around 57 .",
        "south wind between 5 and 10 mph .",
        "skyCoveri temperaturei windDiri windSpeedi mostly cloudy ,\\\\with a low around 59 .",
        "south wind between 7 and 15 ||mph .",
        "Figure 8: Outputs of systems on an example WeatherGov scenario.",
        "Most of the gains of OurSystem over Baseline come from improved content selection.",
        "For example, Baseline chooses rainChance because it happens to be the most common first record type in the training data.",
        "However, since OurSystem has features that depend on the value of rainChance (noChance in this case), it has learned to disprefer talking about rain when there is no rain.",
        "Also, OurSystem has additional features on the entire history of chosen records, which enables it to choose a better sequence of records.",
        "sary.",
        "Interestingly, OurSystem even outperforms Human on semantic correctness, perhaps due to generating more straightforward renderings of the world state.",
        "Figure 8 describes example outputs for each system."
      ]
    },
    {
      "heading": "6. Related Work",
      "text": [
        "There has been a fair amount of work both on content selection and surface realization.",
        "in content selection, Barzilay and Lee (2004) use an approach based on local classification with edgewise scores between local decisions.",
        "Our model, on the other hand, can capture higher-order constraints to enforce global coherence.",
        "Liang et al.",
        "(2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model.",
        "Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for).",
        "Even if combined with a language model, generated text was much worse than our baseline.",
        "The prominent approach for surface realization is rendering the text from a grammar.",
        "Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text.",
        "Soricut and Marcu (2006) uses tree structures called WiDL-expressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework.",
        "Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar.",
        "Lu et al.",
        "(2009) uses a conditional random field model over trees.",
        "Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006).",
        "in each of the above cases, the decomposable structure of the tree/grammar enables tractability.",
        "However, we saw that it was important to include features that captured long-range dependencies.",
        "Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization.",
        "One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text.",
        "Koller and Striegnitz (2002) perform surface realization of a flat semantics, which is NP-hard, so they recast the problem as non-projective dependency parsing.",
        "Ratnaparkhi (2002) uses beam search to find an approximate solution.",
        "We found that a greedy approach obtained better results than beam search; Belz (2008) found greedy approaches to be effective as well."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "We have developed a simple yet powerful generation system that combines both content selection and surface realization in a domain independent way.",
        "Despite our approach being domain-independent, we were able to obtain performance comparable to the state-of-the-art across three domains.",
        "Additionally, the feature-based design of our approach makes it easy to incorporate domain-specific knowledge to increase performance even further.",
        "Records:",
        "rainChance2",
        "none",
        "gusti",
        "precipPotentiali",
        "x",
        "Baseline",
        "Fields:",
        "x",
        "^^^^^H ||max=21",
        "x",
        "x ||max=10",
        "Text:",
        "a chance of showers ,\\",
        "with gusts as high as 20",
        "mph .",
        "\\ chance of precipitation is\\\\ 10 \\",
        "%."
      ]
    }
  ]
}
