{
  "info": {
    "authors": [
      "Ioannis P. Klapaftis",
      "Suresh Manandhar"
    ],
    "book": "EMNLP",
    "id": "acl-D10-1073",
    "title": "Word Sense Induction Disambiguation Using Hierarchical Random Graphs",
    "url": "https://aclweb.org/anthology/D10-1073",
    "year": 2010
  },
  "references": [
    "acl-C02-1114",
    "acl-C04-1146",
    "acl-E03-1020",
    "acl-E09-1013",
    "acl-J93-1003",
    "acl-N03-4011",
    "acl-N06-4007",
    "acl-N10-1010",
    "acl-P04-3020",
    "acl-S10-1011",
    "acl-W04-0811",
    "acl-W06-1669",
    "acl-W06-3812",
    "acl-W07-2002",
    "acl-W07-2037",
    "acl-W07-2087",
    "acl-W09-3204"
  ],
  "sections": [
    {
      "text": [
        "Word Sense Induction & Disambiguation Using Hierarchical Random Graphs",
        "Graph-based methods have gained attention in many areas of Natural Language Processing (NLP) including Word Sense Disambiguation (WSD), text summarization, keyword extraction and others.",
        "Most of the work in these areas formulate their problem in a graph-based setting and apply unsupervised graph clustering to obtain a set of clusters.",
        "Recent studies suggest that graphs often exhibit a hierarchical structure that goes beyond simple flat clustering.",
        "This paper presents an unsupervised method for inferring the hierarchical grouping of the senses of a polysemous word.",
        "The inferred hierarchical structures are applied to the problem of word sense disambiguation, where we show that our method performs significantly better than traditional graph-based methods and agglomerative clustering yielding improvements over state-of-the-art WSD systems based on sense induction."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "A number of NLP problems can be cast into a graph-based framework, in which entities are represented as vertices in a graph and relations between them are depicted by weighted or unweighted edges.",
        "For instance, in unsupervised WSD a number of methods (Widdows and Dorow, 2002; Veronis, 2004; Agirre et al., 2006) have constructed word co-occurrence graphs for a target polysemous word and applied graph-clustering to obtain the clusters (senses) of that word.",
        "Similarly in text summarization, Mihalcea (2004) developed a method, in which sentences are represented as vertices in a graph and edges between them are drawn according to their common tokens or words of a given POS category, e.g. nouns.",
        "Graph-based ranking algorithms, such as PageRank (Brin and Page, 1998), were then applied in order to determine the significance of sentences.",
        "In the same vein, graph-based methods have been applied to other problems such as determining semantic similarity of text (Ramage et al., 2009).",
        "Recent studies (Clauset et al., 2006; Clauset et al., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which vertices are divided into groups that are further subdivided into groups of groups, and so on, until we reach the leaves.",
        "This hierarchical structure provides additional information as opposed to flat clustering by explicitly including organisation at all scales of a graph (Clauset et al., 2008).",
        "In this paper, we present an unsupervised method for inferring the hierarchical structure (binary tree) of a graph, in which vertices are the contexts of a polysemous word and edges represent the similarity between contexts.",
        "The method that we use to infer that hierarchical structure is the Hierarchical Random Graphs (HRGs) algorithm due to Clauset et al.",
        "(2008).",
        "The binary tree produced by our method groups the contexts of a polysemous word at different heights of the tree.",
        "Thus, it induces the senses of that word at different levels of sense granularity.",
        "To evaluate our method, we apply it to the problem of noun sense disambiguation showing that inferring the hierarchical structure using HRGs provides additional information from the observed graph leading to improved WSD performance compared to: (1) simple flat clustering, and (2) traditional agglomerative clustering.",
        "Finally, we compare our results with state-of-the-art sense induction systems and show that our method yields improvements.",
        "Figure 1 shows the different stages of the proposed method that we describe in the following sections."
      ]
    },
    {
      "heading": "2. Related work",
      "text": [
        "Typically, graph-based methods, when applied to unsupervised sense disambiguation represent each word wi co-occurring with the target word tw as a vertex.",
        "Two vertices are connected via an edge if they co-occur in one or more contexts of tw.",
        "Once the co-occurrence graph of tw has been constructed, different graph clustering algorithms are applied to induce the senses.",
        "Each cluster (induced sense) consists of a set of words that are semantically related to the particular sense.",
        "Figure 2 shows an example of a graph for the target word paper that appears with two different senses scholarly article and newspaper.",
        "Veronis (2004) has shown that co-occurrence graphs are small-world networks that contain highly dense subgraphs representing the different clusters (senses) of the target word (Veronis, 2004).",
        "To identify these dense regions Veronis's algorithm iteratively finds their hubs, where a hub is a vertex with a very high degree.",
        "The degree of a vertex is defined to be the number of edges incident to that vertex.",
        "The identified hub is then deleted along with its direct neighbours from the graph producing a new cluster.",
        "For example, in Figure 2 the highest degree vertex, news, is the first hub, which would be deleted along with its direct neighbours.",
        "The deleted region corresponds to the newspaper sense of the target word paper.",
        "Veronis (2004) further processed the identified clusters (senses), in order to assign the rest of graph vertices to the identified clusters by utilising the minimum spanning tree of the original graph.",
        "In Agirre et al.",
        "(2006), the algorithm of Veronis (2004) is analysed and assessed on the SensEval-3 dataset (Snyder and Palmer, 2004), after optimising its parameters on the SensEval-2 dataset (Edmonds and Dorow, 2001).",
        "The results show that the WSD F-Score outperforms the Most Frequent Sense (MFS) baseline by approximately 10%, while inducing a large number of clusters (with averages of 60 to 70).",
        "Another graph-based method is presented in (Dorow and Widdows, 2003).",
        "They extract only noun neighbours that appear in conjunctions or disjunctions with the target word.",
        "Additionally, they extract second-order co-occurrences.",
        "Nouns are represented as vertices, while edges between vertices are drawn, if their associated nouns co-occur in conjunctions or disjunctions more than a given number of times.",
        "This co-occurrence frequency is also used to weight the edges.",
        "The resulting graph is then pruned by removing the target word and vertices with a low degree.",
        "Finally, the MCL algorithm (Dongen, 2000) is used to cluster the graph and produce a set of clusters (senses) each one consisting of a set of contextually related words.",
        "parameter-free graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the target word (Klapaftis and Manandhar, 2008).",
        "The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results.",
        "All the described methods for sense induction ap-",
        "Figure 2: Graph of words for the target word paper.",
        "Numbers inside vertices correspond to their degree.",
        "ply flat graph clustering methods to derive the clusters (senses) of a target word.",
        "As a result, they neglect the fact that their constructed graphs often exhibit a hierarchical structure that is useful in several tasks including word sense disambiguation."
      ]
    },
    {
      "heading": "3. Building a graph of contexts",
      "text": [
        "This section describes the process of creating a graph of contexts for a polysemous target word.",
        "Figure 3 provides a running example of the different stages of our method.",
        "In the example, the target word paper appears with the scholarly article sense in the contexts A, B, and with the newspaper sense in the contexts C and D.",
        "Let bc denote the base corpus consisting of the contexts containing the target word tw.",
        "In our work, a context is defined as a paragraph containing the target word.",
        "The aim of this stage is to capture nouns contextually related to tw.",
        "Initially, the target word is removed from bc and part-of-speech tagging is applied to each context.",
        "Following the work in (Veronis, 2004; Agirre et al., 2006) only nouns are kept and lemmatised.",
        "In the next step, the distribution of each noun in the base corpus is compared to the distribution of the same noun in a reference corpus using the log-likelihood ratio (G) (Dunning, 1993).",
        "Nouns with a G below a pre-specified threshold (parameter pi ) are removed from each paragraph of the base corpus.",
        "The upper left part of Figure 3 shows the words kept as a result of this stage.",
        "Graph vertices: To create the graph of vertices, we represent each context ci as a vertex in a graph G. Graph edges: Edges between the vertices of the graph are drawn based on their similarity, defined in Equation 1, where simci (ci,Cj) is the collocational weight of contexts ci, cj and simwd (ci ,Cj ) is their bag-of-words weight.",
        "If the edge weight W(ci,cj ) is above a prespecified threshold (parameter p3), then an edge is drawn between the corresponding vertices in the graph.",
        "Collocational weight: The limited polysemy of collocations can be exploited to compute the similarity between contexts ci and cj.",
        "In our setting, a collocation is a juxtaposition of two nouns within the same context.",
        "Thus, given a context ci, each of its nouns is combined with any other noun yielding a total of collocations for a context with N nouns.",
        "Each collocation, clij is weighted using the log-likelihood ratio (G) (Dunning, 1993) and is filtered out if the G is below a prespecified threshold (parameter p2 ).",
        "At the end of this process, each context ci of tw is associated with a vector of collocations (vi).",
        "The upper right part of Figure 3 shows the collocations associated with each context of our example.",
        "f articled.",
        "f journal >",
        "^publication^",
        "Scholarly article",
        "\\Jf author /C(20)^",
        "/ /guardian^",
        "times",
        "Newspaper",
        "/advertisement",
        "Given two contexts ci and cj, we calculate their collocational weight using the Jaccard coefficient on the collocational vectors, i.e. simcl (ci,cj) = tjttj\\.",
        "The selection of Jaccard is based on the work of Weeds et al.",
        "(2004), who analyzed the variation in a word's distributionally nearest neighbours with respect to a variety of similarity measures.",
        "Their analysis showed that there are three classes of measures, i.e. those selecting distributionally more general neighbours (e.g. cosine), those selecting distributionally less general neighbours (e.g. AMCRM-Precision (Weeds et al., 2004)) and those without a bias towards the distributional generality of a neighbour (e.g. Jaccard).",
        "In our setting, we are interested in calculating the similarity between two contexts without any bias.",
        "We selected Jaccard, since the rest of that class's measures are based on pointwise mutual information that assigns high weights to infrequent events.",
        "Bag-of-words weight: Estimating context similarity using collocations may provide reliable estimates regarding the existence of an edge in the graph, however, it also suffers from data sparsity.",
        "For this reason, we also employ a bag-of-words model.",
        "Specifically, each context ci is associated with a vector githat contains the nouns kept as result of the corpus preprocessing stage.",
        "The upper left part of Figure 3 shows the words associated with each context of our example.",
        "Given two contexts ci and cj, we calculate their bag-of-words weight using the Jaccard coefficient on the word vectors, i.e. simwd (ci, cj ) =",
        "The collocational weight and bag-of-words weight are averaged to derive the edge weight between two contexts as defined in Equation 1.",
        "The resulting graph of our running example is shown on the bottom of Figure 3.",
        "This graph is the input to the hierarchical random graphs method (Clauset et al., 2008) described in the next section."
      ]
    },
    {
      "heading": "4. Hierarchical Random Graphs for sense induction",
      "text": [
        "In this section, we describe the process of inferring the hierarchical structure of the graph of contexts using hierarchical random graphs (Clauset et al., 2008).",
        "A dendrogram is a binary tree with n leaves and n – 1 parents.",
        "Figure 4 shows an example of two dendrograms with 4 leaves and 3 parents.",
        "Given a set of n contexts that we need to arrange hierarchically, let us denote by G = (V, E) the graph of contexts, where V = {v0, vi.. .vn} is the set of vertices, E = {e0, ei... em} is the set of edges and ek = {vi ,vj }.",
        "Given an undirected graph G, each of its n vertices is a leaf in a dendrogram, while the internal nodes of that dendrogram indicate the hierarchical relationships among the leaves.",
        "We denote this organisation by D = {Di, D2,... Dn-i}, where each Dk is an internal node.",
        "Every pair of nodes (vi, vj ) is associated with a unique Dk , which is their lowest common ancestor in the tree.",
        "In this manner D partitions the edges that exist in G.",
        "The primary assumption in the hierarchical random graph model is that edges in G exist independently, but with a probability that is not identically distributed.",
        "In particular, the probability that an edge {vi, vj } exists in G is given by a parameter 9k associated with Dk, the lowest common ancestor of viand vj in D. In this manner, the topological structure D and the vector of probabilities define the HRG given by H(D, 6) (Clauset et al., 2008).",
        ">5",
        "^©2=0.25",
        "P(S,|D0)=1",
        "P(S, ID^o",
        "P(S2|DQ)=0/-",
        "-^P(S2|D1)=l",
        "CT A",
        "B",
        "(A)",
        "^=0.5",
        "0O=O",
        "D",
        "(B)",
        "B 2^ ^",
        "Assuming a uniform prior over all HRGs, the target is to identify the parameters of D and , so that the chosen HRG is statistically similar to G. Let Dk be an internal node of dendrogram D and f (Dk) be the number of edges between the vertices of the subtrees of the subtree rooted at Dk that actually exist in G. For example, in Figure 4(A), f(D2) = 1, because there is one edge in G connecting vertices B and C. Let l(Dk) be the number of leaves in the left subtree of Dk, and r(Dk) be the number of leaves in the right subtree.",
        "For example in Figure 4(A), l(D2) = 2 and r(D2) = 2.",
        "The likelihood of the hierarchical random graph (D, 0) is defined in Equation 2, where A(Dk ) = l(Dk )r(Dk ) – f (Dk ).",
        "The probabilities Ok that maximise the likelihood of a dendrogram D can be easily estimated using the method of MLE i.e Ok = ,,f(Dk\\,.",
        "Substituting this into Equation 2 yields Equation 3.",
        "For numerical reasons, it is more convenient to work with the logarithm of the likelihood which is defined in Equation 4, where h(Ok) = – Ok log Ok – (1 – Ok )log(1 – Ok ).",
        "As can be observed, each term – l(Dk)r(Dk)h(Ok) is maximised when Ok approaches 0 or 1.",
        "This means that high-likelihood dendrograms partition vertices into subtrees, such that the connections among their vertices in the observed graph are either very rare or very common (Clauset et al., 2008).",
        "For example, consider the two dendrograms in Figures 4(A) and 4(B).",
        "We observe that 4(A) is more likely than 4(B), since it provides a better division of the network leaves.",
        "Particularly, the likelihood of 4(A) is L(Di) = (1l • (1 – • (1l • (1 – • (0.25 • (1 – 0.25)) = 0.105, while the likelihood of 4(B) is L(D2) = (0 • (1 – 0)) • (1l • (1 – • (0.5 • (1 – 0.5)) = 0.062.",
        "Finding the values of Ok using the MLE method is straightforward.",
        "However, this is not the case for maximising the likelihood function over the space of all possible dendrograms.",
        "Given a graph with n vertices, i.e. n leaves in each dendrogram, the total number of different dendrograms is super-exponential ((2n – 3)11 « V2(2n)n-le~n) (Clauset et al., 2006).",
        "To deal with this problem, we use a Markov Chain Monte Carlo (MCMC) method that samples dendrograms from the space of dendrogram models with probability proportional to their likelihood.",
        "Each time MCMC samples a dendrogram with a new highest likelihood, that dendrogram is stored.",
        "Hence, our goal is to choose the highest likelihood dendrogram once MCMC has converged.",
        "Following the work in (Clauset et al., 2008), we pick a set of transitions between dendrograms, where a transition is a rearrangement of the subtrees of a dendrogram.",
        "In particular, given a current dendrogram Dcurr, each internal node Dk of Dcurris associated with three subtrees of Dcurr.",
        "For instance, in Figure 5A, the subtrees sti and st2 are derived from the two children of Dk and the third st3 from its sibling.",
        "Given a current dendrogram, Dcurr, the algorithm proceeds as follows:",
        "1.",
        "Choose an internal node, Dk G Dcurr uniformly.",
        "2.",
        "Generate two possible new configurations of the subtrees of Dk (See Figure 5).",
        "3.",
        "Choose one of the configurations uniformly to generate a new dendrogram, Dnext.",
        "4.",
        "Accept or reject Dnext according to Metropolis-Hastings (MH) rule.",
        "5.",
        "If transition is accepted, then Dcurr = Dnext.",
        "6.",
        "GOTO 1.",
        "According to MH rule (Newman and Barkema, 1999), a transition is accepted if log L(Dnext) > logL(Dcurr); otherwise the transition is accepted with probability L{D\"eXt].",
        "These transitions define an ergodic Markov chain, hence its stationary distribution can be reached (Clauset et al., 2008).",
        "Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration, (C) second alternative configuration.",
        "Note that swapping st 1, st2 in (A) results in an equivalent tree.",
        "Hence, this coniguration is excluded.",
        "In our experiments, we noticed that the algorithm converged relatively quickly.",
        "The same behaviour (roughly O (n ) steps) was also noticed in Clauset et al.",
        "(2008), when considering graphs with thousands of vertices."
      ]
    },
    {
      "heading": "5. HRGs for sense disambiguation 5.1 Sense mapping",
      "text": [
        "The output of HRG learning is a dendrogram D with n leaves (contexts) and n – 1 internal nodes.",
        "To perform sense disambiguation, we mapped the internal nodes to gold standard senses using a sense-tagged corpus.",
        "Such a sense-tagged corpus is needed when induced word senses need to be mapped to a gold standard sense inventory.",
        "Instead of using a hard mapping from the dendrogram internal nodes to the Gold Standard (GS) senses, we use a soft probabilistic mapping and calculate P(sk\\Di), i.e the probability of sense sk given node Di.",
        "Let F (Di ) be the set of training contexts grouped by internal node Di.",
        "Let F (sk) be the set of training contexts that are tagged with sense sk.",
        "Then the conditional probability, P (sk \\Di ), is defined in Equation 5.",
        "Table 1 provides a sense-tagged corpus for the running example of Figure 3.",
        "Using this corpus and the tree in Figure 4(A), P(si \\D2) = and P (s2 \\D2 ) = .In Figure 4(A) the rest of the calculated conditional probabilities are given.",
        "For evaluation we compared the proposed method against the current state-of-the-art sense induction systems in the WSD task.",
        "We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007).",
        "In this setting, the base corpus (bc) (Section 3.1) for a target word consists both of the training and testing corpus.",
        "As a result, a testing context cj of tw is a leaf in the generated dendrogram.",
        "The process of disambiguating cj is straightforward exploiting the structural information provided by HRGs.",
        "Let H (cj ) denote the set of parents for context cj.",
        "Then, the weight assigned to sense skis the sum of weighted scores provided by each identiied parent.",
        "This is shown in Equation 6, where 9i is the probability associated with each internal node Di from the hierarchical random graph (see Figure 4(A)).",
        "This probability reflects the discriminating ability of internal nodes.",
        "Finally, the highest weight determines the winning sense for context cj (Equation 7).",
        "In our example (Figure 4(A)), w(si, C) = (0 • 1 + § • 0.25) = 0.16 and w(s2, C) = (1 • 1 + • 0.25) = 1.08.",
        "Hence, s2 is the winning sense.",
        "GS sense",
        "Context ID",
        "Context words",
        "si",
        "A",
        "journal, scholar, observation science, paper",
        "si",
        "B",
        "scholar, scholar, author, publication, paper",
        "s2",
        "D",
        "times, guardian, journalist, paper",
        "We evaluate our method on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task, i.e. supervised evaluation.",
        "Specifically, we use the standard WSD measures of precision and recall in order to produce their harmonic mean (F-Score).",
        "The official scoring software of that task has been used in our evaluation.",
        "Note that the unsupervised measures of that task are not directly applicable to our induced hierarchies, since they focus on assessing flat clustering methods.",
        "The irst aim of our evaluation is to test whether inferring the hierarchical structure of the constructed graphs improves WSD performance.",
        "For that reason our irst baseline, Chinese Whispers Unweighted version (CWU), takes as input the same unweighted graph of contexts as HRGs in order to produce a flat clustering.",
        "The set of produced clusters is then mapped to GS senses using the training dataset and performance is then measured on the testing dataset.",
        "We followed the same sense mapping method as in the SemEval-2007 sense induction task (Agirre and",
        "Soroa, 2007).",
        "Our second baseline, Chinese Whispers Weighted version (CWW), is similar to the previous one, with the difference that the edges of the input graph are weighted using Equation 1.",
        "For clustering the graphs of CWU and CWW we employ, Chinese Whispers (Biemann, 2006).",
        "The second aim of our evaluation is to assess whether the hierarchical structure inferred by HRGs is more informative than the hierarchical structure inferred by traditional Hierarchical Clustering (HAC).",
        "Hence, our third baseline, takes as input a similarity matrix of the graph vertices and performs bottom-up clustering with average-linkage, which has already been used in WSI in (Pantel and Lin, 2003) and was shown to have superior or similar performance to single-linkage and complete-linkage in the related problem of learning a taxonomy of senses (Klapaftis and Manandhar, 2010).",
        "To calculate the similarity matrix of vertices we follow a process similar to the one used in Section 4.2 for calculating the probability of an internal node.",
        "The similarity between two vertices is calculated according to the degree of connectedness among their direct neighbours.",
        "Speciically, we would like to assign high similarity to pairs of vertices, whose neighbours are close to forming a clique.",
        "Given two vertices (contexts) ci and cj , let N (ci, cj ) be the set of their neighbours and K (ci, cj ) be the set of edges between the vertices in N(ci; cj).",
        "The maximum number of edges that could exist between vertices in N(ci; cj) is (|N(<2'Cj^).",
        "Thus, the similarity of ci, cj is set equal to the number of edges that actually exist in that neighbourhood divided by the total number of edges that could exist ( lK(ci,Cj)l ) ( (|N(ci,cj)|) ).",
        "The disambiguation process using the HAC tree is identical to the one presented in Section 5.2 with the only difference that the internal probability, Qi, in Equation 6 does not exist for HAC.",
        "Hence, we replaced it with the factor |H(D.)|, where H(Di) is the set of children of internal node Di.",
        "This factor provides lower weights for nodes high in the tree, since their discriminating ability will possibly be lower.",
        "Table 2 shows the parameter values used in the evaluation.",
        "Figure 6(A) shows the performance of the proposed method against the baselines for p§ = 0.05 and different pi and p2 values.",
        "Figure 6(B) illustrates the results of the same experiment using p§ = 0.09.",
        "In both figures, we observe that HRGs outperform the CWU baseline under all parameter combinations.",
        "In particular, all of the 12 performance differences for p§ = 0.09 are statistically significant using McNemar's test at 95% confidence level, while for p§ = 0.",
        "05 only 2 out of the 12 performance differences were not judged as signiicant from the test.",
        "The picture is the same for p§ = 0.13, where CWU performs signiicantly worse than for p§ =",
        "Parameter",
        "Range",
        "G word threshold",
        "15,25,35,45",
        "G collocation threshold (p2)",
        "10,15,20",
        "Edge similarity threshold (p3)",
        "0.05,0.09,0.13",
        "Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2).",
        "(A) All combinations of p1, p2 andp3 = 0.05.",
        "(B) All combinations of p1, p2 andp3 = 0.09.",
        "0.05 and p§ = 0.09.",
        "Speciically, the largest performance difference between HRGs and CWU is 9.4% at pi = 25, p2 = 10 and p§ = 0.13.",
        "Setting the vertex similarity threshold (p§) equal to 0.13 leads to more sparse and disconnected graphs, which causes Chinese Whispers to produce a large number of clusters.",
        "This leads to sparsity problems and unreliable mapping of clusters to GS senses due to the lack of adequate training data.",
        "In contrast, HRGs suffer less at this high threshold, although their performance when p3<0.13 is better.",
        "This picture does not change for the weighted version of Chinese Whispers (CWW) which performs worse than CWU.",
        "This is because CWW produces a smaller number of clusters than CWU that conflate the target word senses.",
        "It seems that using weighted edges creates a bias towards the MFS, in effect missing rare senses of a target word.",
        "This means that a number of words in the bag-of-words context vectors and collocations in the collocational context vectors (Section 3.2) are associated to more than one sense of the target word and most strongly associated to the MFS.",
        "As a result, increasing the pi threshold to 25 and 35 leads to a higher performance for CWW, since many of these words and collocations are iltered out.",
        "Overall, the comparison of HRGs against the CWU and CWW baselines has shown that inferring the hierarchical structure of observed graphs leads to improved WSD performance as opposed to using flat clustering.",
        "This is because HRGs are able to infer both the hierarchical structure of the graph and include the probabilities, 6k, associated with each internal node.",
        "These probabilities reflect the discriminating ability of each node, offering information missed by flat clustering.",
        "In Figures 6(A) and 6(B) we observe that HRGs perform signiicantly better than HAC.",
        "In particular, all of their performance differences are statistically signiicant for these parameter values.",
        "The largest performance difference is 6.0% at p1 = 45, p2 = 10 and p§ = 0.",
        "05.",
        "However, this picture is not the same when considering a higher context similarity threshold (p§ = 0.13) as Figure 7 shows.",
        "In particular, HRGs and HAC perform similarly for p§ = 0.13, while the majority of performance differences are not statistically signiicant.",
        "The similar behaviour of HRGs and HAC at this threshold is caused both by the worse performance of HRGs and the improved performance of HAC as opposed to lower p§ values.",
        "As it has been mentioned, setting p§ = 0.13 leads to sparse and disconnected graphs.",
        "Additionally, the likelihood function (Equation 3) is maximised when the probability, 6k, of an internal node, Dk, approaches 0 or 1.",
        "This creates a bias towards dendrograms, in which a large number of internal nodes have zero probability.",
        "These dendrograms might be a good-it to the observed graph, but not to the GS.",
        "In contrast, HAC is less affected, because it never considers creating an internal node, when the maximum similarity among any pair of two candidate",
        "Figure 7: Performance of HRGs and HAC for different parameter combinations (Table 2).",
        "All combinations of p1, p2 and p3 > 0.13.",
        "subtrees is zero.",
        "Additionally, our experiments show that HAC is unable to deal with noise when considering sparse graphs (p§ <0.13).",
        "For that reason, the F-Score of HAC increases as the edge similarity threshold decreases.",
        "To further investigate this issue and test whether HAC is able to achieve a higher F-Score than HRGs in higher p§ values, we executed two more experiments for HAC and HRGs increasing p§ to 0.17 and 0.21 respectively.",
        "In the first case we observed that the performance of HAC remained relatively stable compared to p§ = 0.13, while in the second case the performance of HAC decreased as Figure 7 shows.",
        "In both cases, HAC performed signiicantly better than HRGs.",
        "Overall, the comparison of HRGs against HAC has shown that HRGs perform signiicantly better than HAC when considering connected or less sparse graphs (p§ <0.13).",
        "This is due to the fact that HAC creates dendrograms, in which connections within the clusters are dense, while connections between the clusters are sparse, i.e. it only considers assorta-tive structures.",
        "In contrast, HRGs also consider dis-assortative dendrograms, i.e. dendrograms in which vertices are less likely to be connected on small scales than on large ones, as well as mixtures of assortative and disassortative (Clauset et al., 2008).",
        "This is achieved by allowing the probability 9k of a node k to vary arbitrarily throughout the dendrogram.",
        "HAC performs similarly or better than HRGs for largely disconnected and sparse graphs, because HRGs become biased towards disassortative trees which are not a good it to the GS (Figure 7).",
        "Despite that, our evaluation has also shown that the best performance of HAC (F-Score = 86.0% at p1 = 15, p2 = 10, p§ = 0.13) is significantly lower than the best performance of HRGs (F-Score = 87.6% at pi = 35, p2 = 10, p3 = 0.09).",
        "Table 3 compares the best performing parameter combination of our method against state-of-the-art methods.",
        "Table 3 also includes the best performance of our baselines, i.e HAC, CWU and CWW.",
        "Brody & Lapata (2009) presented a sense induction method that is related to Latent Dirichlet Allocation (Blei et al., 2003).",
        "In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009).",
        "A signiicant advantage of their method is the inclusion of more than one layer in the LDA setting, where each layer corresponds to a different feature type e.g. dependency relations, bigrams, etc.",
        "The inclusion of different feature types as separate models in the sense induction process can easily be modeled in our setting, by inferring a different hierarchy of target word instances according to each feature type, and then combining all of them to a consensus tree.",
        "In this work, we have focused on extracting a single hierarchy combining word co-occurrence and bigram features.",
        "Niu et al.",
        "(2007) developed a vector-based method that performs sense induction by grouping the contexts of a target word using three types of features, i.e. POS tags of neighbouring words, word co-occurrences and local collocations.",
        "The sequential information bottleneck algorithm (Slonim et al., 2002) is applied for clustering.",
        "HRGs perform slightly better than the methods of Brody & Lapata (2009) and Niu et al.",
        "(2007), although the differences are not significant (McNemar's test at 95% conidence level).",
        "Klapaftis & Manandhar (2008) developed a graph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn according to the co-occurrence frequency of the corresponding collocations.",
        "The constructed graph is smoothed to identify more edges between vertices and then clustered using Chinese Whispers (Bie-mann, 2006).",
        "This method is related to the basic inputs of our presented method.",
        "Despite that, it is a flat clustering method that ignores the hierarchical structure exhibited by observed graphs.",
        "The previous section has shown that inferring the hierarchical structure of graphs leads to superior WSD performance.",
        "Pedersen (2007) presented SenseClusters, a vector-based method that clusters second order cooccurrence vectors using k-means, where k is automatically determined using the Adapted Gap Statistic (Pedersen and Kulkarni, 2006).",
        "As can be observed, HRGs perform significantly better than the methods of Pedersen (2007) and Klapaftis & Man-andhar (2008) (McNemar's test at 95% confidence level).",
        "Finally, Table 3 shows that the best performing parameter combination of HRGs achieves a significantly higher F-Score than the best performing parameter combination of HAC, CWU and CWW.",
        "Furthermore, HRGs outperform the most frequent sense baseline by 6.7%."
      ]
    },
    {
      "heading": "7. Conclusion & future work",
      "text": [
        "We presented an unsupervised method for inferring the hierarchical grouping of the senses of a polyse-mous word.",
        "Our method creates a graph, in which vertices correspond to contexts of a polysemous target word and edges between them are drawn according to their similarity.",
        "The hierarchical random graphs algorithm (Clauset et al., 2008) was applied to the constructed graph in order to infer its hierarchical structure, i.e. binary tree.",
        "The learned tree provides an induction of the senses of a given word at different levels of sense granularity and was applied to the problem of WSD.",
        "The WSD process mapped the tree's internal nodes to GS senses using a sense tagged corpus, and then tagged new instances by exploiting the structural information provided by the tree.",
        "Our experimental results have shown that our graphs exhibit hierarchical organisation that can be captured by HRGs, in effect providing improved WSD performance compared to flat clustering.",
        "Additionally, our comparison against hierarchical agglomerative clustering with average-linkage has shown that HRGs perform significantly better than HAC when the graphs do not suffer from sparsity (disconnected graphs).",
        "The comparison with state-of-the-art sense induction systems has shown that our method yields improvements.",
        "Our future work focuses on using different feature types, e.g. dependency relations, second-order cooccurrences, named entities and others to construct our undirected graphs and then applying HRGs, in order to measure the impact of each feature type on the induced hierarchical structures within a WSD setting.",
        "Moreover, following the work in (Clauset et al., 2008), we are also working on using MCMC in order to sample more than one dendrogram at equilibrium, and then combine them to a consensus tree.",
        "This consensus tree might be able to express a larger amount of topological features of the initial undirected graph.",
        "Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a finegrained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work is supported by the European Commission via the EU FP7 INDECT project, Grant No.218086, Research area: SEC-2007-1.2-01 Intelligent Urban Environment Observation System.",
        "The authors would like to thank the anonymous reviewers for their useful comments.",
        "System",
        "Performance (%)",
        "HRGs",
        "87.6",
        "(Brody and Lapata, 2009)",
        "87.3",
        "(Niu et al., 2007)",
        "86.8",
        "(Klapaftis and Manandhar, 2008)",
        "86.4",
        "HAC",
        "86.0",
        "CWU",
        "85.1",
        "CWW",
        "84.7",
        "(Pedersen, 2007)",
        "84.5",
        "MFS",
        "80.9"
      ]
    }
  ]
}
