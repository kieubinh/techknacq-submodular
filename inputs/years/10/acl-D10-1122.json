{
  "info": {
    "authors": [
      "Raghavendra Udupa",
      "Shaishav Kumar"
    ],
    "book": "EMNLP",
    "id": "acl-D10-1122",
    "title": "Hashing-Based Approaches to Spelling Correction of Personal Names",
    "url": "https://aclweb.org/anthology/D10-1122",
    "year": 2010
  },
  "references": [
    "acl-C90-2036",
    "acl-D07-1019",
    "acl-D09-1093",
    "acl-D09-1129",
    "acl-H05-1120",
    "acl-P00-1037",
    "acl-P02-1019",
    "acl-P10-1028",
    "acl-W04-3238"
  ],
  "sections": [
    {
      "text": [
        "Hashing-based Approaches to Spelling Correction of Personal Names",
        "Microsoft Research India Bangalore, India",
        "We propose two hashing-based solutions to the problem of fast and effective personal names spelling correction in People Search applications.",
        "The key idea behind our methods is to learn hash functions that map similar names to similar (and compact) binary codewords.",
        "The two methods differ in the data they use for learning the hash functions - the first method uses a set of names in a given language/script whereas the second uses a set of bilingual names.",
        "We show that both methods give excellent retrieval performance in comparison to several baselines on two lists of misspelled personal names.",
        "More over, the method that uses bilingual data for learning hash functions gives the best performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Over the last few years, People Search has emerged as an important search service.",
        "Unlike general Web Search and Enterprise Search where users search for information on a wide range of topics including people, products, news, events, etc., People Search is about people.",
        "Hence, personal names are used predominantly as queries in People Search.",
        "As in general Web Search, a good percentage of queries in People Search is misspelled.",
        "Naturally, spelling correction of misspelled personal names plays a very important role in not only reducing the time and effort needed by users to find people they are searching for but also in ensuring good user experience.",
        "Spelling errors in personal names are of a different nature compared to those in general text.",
        "Long before People Search became widely popular, researchers working on the problem of personal name matching had recognized the human tendency to be inexact in recollecting names from the memory and specifying them.",
        "A study of personal names in hospital databases found that only 39% of the errors in the names were single typographical errors (Friedman and Sideli, 1992).",
        "Further, multiple and long distance typographical errors (Gregzorz Kon-drak for Grzegorz Kondrak), phonetic errors (as in Erik Bryl for Eric Brill), cognitive errors (as in Silvia Cucerzan for Silviu Cucerzan) and word substitutions (as in Rob Moore for Bob Moore) are observed relatively more frequently in personal names compared to general text.",
        "In addition to within-the-word errors, People Search queries are plagued by errors that are not usually seen in general text.",
        "The study by Friedman and Sideli discovered that 36% of the errors were due to addition or deletion of a word (as in Ricardo Baeza for Ricardo Baeza-Yates) (Friedman and Sideli, 1992).",
        "Although word addition and deletion generally do not come under the purview of spelling correction, in People Search they are important and need to be addressed.",
        "Standard approaches to general purpose spelling correction are not well-suited for correcting misspelled personal names.",
        "As pointed out by (Cucerzan and Brill, 2004), these approaches either try to correct individual words (and will fail to correct Him Clijsters to Kim Clijsters) or employ features based on relatively wide context windows which are not available for queries in Web Search and People Search.",
        "Spelling correction techniques meant for general purpose web-queries require large volumes of training data in the form of query logs for learning the error models (Cucerzan and Brill, 2004), (Ahmad and Kondrak, 2005).",
        "However, query logs are not available in some applications (e.g. Email address book search).",
        "Further, unlike general purpose web-queries where word order often matters, in People Search word order is lax (e.g.",
        "I might search for either Kristina Toutanova or Toutanova Kristina).",
        "Therefore, spelling correction techniques that rely crucially on bigram and higher order language models will fail on queries with a different word order than what is observed in the query log.",
        "Unlike general purpose Web Search where it is not reasonable to assume the availability of a high-coverage trusted lexicon, People Search typically employs large authoritative name directories.",
        "For instance, if one is searching for a friend on Face-book, the correct spelling of the friend's name exists in the Facebook people directory (assuming that the friend is a registered user of Facebook at the time of the search).",
        "Similarly, if one is searching for a contact in Enterprise address book, the correct spelling of the contact is part of the address book.",
        "In fact, even in Web Search, broad-coverage name directories are available in the form of Wikipedia, IMDB, etc.",
        "The availability of large authoritative name directories that serve as the source of trusted spellings of names throws open the possibility of correcting misspelled personal names with the help of name matching techniques (Pfeifer et al., 1996), (Christen, 2006), (Navarro et al., 2003).",
        "However, the best of the name matching techniques can at best work with a few thousand names to give acceptable response time and accuracy.",
        "They do not scale up to the needs of People Search applications where the directories can have millions of names.",
        "In this work, we develop hashing-based name similarity search techniques and employ them for spelling correction of personal names.",
        "The motivation for using hashing as a building block of spelling correction is the following: given a query, we want to return the global best match in the name directory that exceeds a similarity threshold.",
        "As matching the query with the names in the directory is a time consuming task especially for large name directories, we solve the search problem in two stages:",
        "• NAMEBUCKETING: For each token of the query, we do an approximate nearest neighbor search of the name tokens of the directory and produce a list of candidates, i.e., tokens that are approximate matches of the query token.",
        "Using the list of candidate tokens, we extract the list of candidate names which contain at least one approximately matching token.",
        "• NAMEJMATCHING: We do a rigorous matching of the query with candidate names.",
        "Clearly, our success in finding the right name suggestion for the query in the NAMEJMATCHING stage depends crucially on our success in getting the right name suggestion in the list of candidates produced by the NAMEBUCKETING stage search.",
        "Therefore, we need a name similarity search technique that can ensure very high recall without producing too many candidates.",
        "Hashing is best suited for this task of fast and approximate name matching.",
        "We hash the query tokens as well as directory tokens into d bit binary codes.",
        "With binary codes, finding approximate matches for a query token is as easy as finding all the database tokens that are at a Hamming distance of r or less from the query token in the binary code representation (Shakhnarovich et al., 2008), (Weiss et al., 2008).",
        "When the binary codes are compact, this search can be done in a fraction of a second on directories containing millions of names on a simple processor.",
        "Our contributions are:",
        "• We develop a novel data-driven technique for learning hash functions for mapping similar names to similar binary codes using a set of names in a given language/script (i.e. monolingual data).",
        "We formulate the problem of learning hash functions as an optmization problem whose relaxation can be solved as a generalized Eigenvalue problem.",
        "(Section 2.1).",
        "• We show that hash functions can also be learnt using bilingual data in the form of name equivalents in two languages.",
        "We formulate the",
        "problem of learning hash functions as an opt-mization problem whose relaxation can be solved using Canonical Correlation Analysis.",
        "(Section 2.2)",
        "• We develop new similarity measures for matching names (Section 3.1).",
        "• We evaluate the two methods systematically and compare our performance against multiple baselines.",
        "(Section 5)."
      ]
    },
    {
      "heading": "2. Learning Hash Functions",
      "text": [
        "In this section, we develop two techniques for learning hash functions using names as training data.",
        "In the first approach, we use monolingual data consisting of names in a language whereas in the second we use bilingual name pairs.",
        "In both techniques, the key idea is the same: we learn hash functions that map similar names in the training data to similar codewords.",
        "Let (s, s') be a pair of names and w (s, s') be their similarity.",
        "We are given a set of name pairs T = {(s, s')} as the training data.",
        "Let 0 (s) G Rdl be the feature representation of s. We want to learn a hash function f that maps each name to a d bit codeword: f : s – { – 1,1}d. We also want the Hamming distance of the codeword of s to the codeword of s' be small when w (s, s') is large.",
        "Further, we want each bit of the codewords to be either 1 or – 1 with equal probablity and the successive bits of the codewords to be uncorrelated.",
        "Thus we arrive at the following optimization problem:",
        "where Id is an identity matrix of size d x d.",
        "Note that the second constraint helps us avoid the trap of mapping all names to the same codeword and thereby making the Hamming error zero while satisfying the first and last constraints.",
        "It can be shown that the above minimization problem is NP-hard even for 1-bit codewords (Weiss et al., 2008).",
        "Further, the optimal solution gives codewords only for the names in the training data.",
        "As we want f to be defined for all s, we address the out-of-sample extension problem by relaxing f as follows:",
        "where A = [ai,..., ad] G RdlXd is a rank d matrix (d < di).",
        "After the linear relaxation (Equation 1), the first constraint simply means that the data be centered, i.e., have zero mean.",
        "We center $ by subtracting the mean of $ from every 0 (s) G $ to get $.",
        "Subsequent to the above relaxation, we get the following optimization problem:",
        "minimize :",
        "where L is the graph Laplacian for the similarity matrix W defined by the pairwise similarities w (s, s').",
        "The minimization problem can be transformed into a generalized Eigenvalue problem and solved efficiently using either Cholesky factorization or QZ algorithm (Golub and Van Loan, 1996):",
        "where A is a d x d diagonal matrix.",
        "Once A has been estimated from the training data, the codeword of a name s can be produced by bina-rizing each coordinate of fR (s):",
        "MGR.",
        "In the reminder of this work, we call the system that uses the hash function learnt from monolingual data as M-HASH.",
        "Let (s, t) be a pair of name s and its transliteration equivalent t in a different language/script.",
        "We are given the set T = {(s, t)} as the training data.",
        "Let 0 (s) G Rdl (and resp.",
        "4 (t) G Rd2 ) be the feature representation of s (and resp.",
        "t).",
        "We want to learn a pair of hash functions f, g that map names to d bit codewords: f : s – { – 1,1}d, g : t – { – 1,1}d. We also want the Hamming distance of the codeword of a name to the codeword of its transliteration be small.",
        "As in Section 2.1, we want each bit of the codewords to be either 1 or – 1 with equal probablity and the successive bits of the codewords to be uncorrelated.",
        "Thus we arrive at the following optimization problem:",
        "where Id is an identity matrix of size d x d.",
        "where A = [ai,..., ad] G RdlXd and B = ..., bd] G Rd2Xd are rank d matrices.",
        "As before, we center $ and * to get $ and * respectively.",
        "Thus, we get the following optimization problem:",
        "where h (a, b; ê, = (at ê - bt (atê - btt.",
        "The minimization problem can be solved as a generalized Eigenvalue problem:",
        "where A is a d x d diagonal matrix.",
        "Further, Equations 13 and 14 find the canonical coefficients of $ and * (Hardoon et al., 2004).",
        "As with monolingual learning, we get the codeword of s by binarizing the coordinates of fR (s):",
        "In the reminder of this work, we call the system that uses the hash function learnt from bilingual data as B-HASH."
      ]
    },
    {
      "heading": "3. Similarity Score",
      "text": [
        "In this section, we develop new techniques for computing the similarity of names at token level as well as a whole.",
        "We will use these techniques in the NAMEJMATCHING stage of our algorithm (Section 4.2.1).",
        "We use a logistic function over multiple distance measures to compute the similarity between name tokens s and s':",
        "While a variety of distance measures can be employed in Equation 16, two obvious choices are the normalized Damerau-Levenshtein edit distance between s and s' and the Hamming distance between the codewords of s and s' (= ||f (s) – f (s') ||).",
        "In our experiments, we found that the continuous relaxation ||fR (s) – fR (s')|| was better than | f (s) – f (s')| and hence we used it with Damerau-Levenshtein edit distance.",
        "We estimated a1 and a2 using a small held out set.",
        "Let Q = s1s2 ... s/ and D = sis2 ... s'j be two multi-token names.",
        "To compute the similarity between Q and D, we first form a weighted bipartite graph with a node for each si and a node for each sj and set edge weight to K sj j.",
        "We then compute the weight (Kmax) of the maximum weighted matching in this graph.",
        "The similarity between Q and D is then computed as"
      ]
    },
    {
      "heading": "4. Spelling Correction using Hashing",
      "text": [
        "In this section, we describe our algorithm for spelling correction using hashing as a building block.",
        "Given a name directory, we break each name into its constituenttokens andformasetofdistinctname tokens.",
        "Using the name tokens and the original names, we build an inverted index which, for each name token, lists all the names that have the token as a constituent.",
        "Further, we hash each name token into a d bit codeword as described in Equation 6 (and resp.",
        "Equation 15) when using the hash function learnt on monolingual data (and resp.",
        "bilingual data) and store in a hash table.",
        "Querying is done in two stages: NAME-BUCKETING and NAMEJMATCHING.",
        "Given a query Q = s1s2 ... s/, we hash each siinto a codeword yi and retrieve all codewords in the hash table that are at a Hamming distance of r or less from yi.",
        "We rank the name tokens thus retrieved using the token level similarity score of Section 3.1 and retain only the top 100.",
        "Using the top tokens, we get all names which contain any of the name tokens as a constituent to form the pool of candidates C for the NAMEJMATCHING stage.",
        "First we find the best match for a query Q in the set of candidates C as follows:",
        "Next we suggest D* as the correction for Q if K (Q, D*) exceeds a certain empirically determined threshold."
      ]
    },
    {
      "heading": "5. Experiments and Results",
      "text": [
        "We now discuss the experiments we conducted to study the retrieval performance of the two hashing-based approaches developed in the previous sections.",
        "Apart from evaluating the systems on test sets using different name directories, we were interested in comparing our systems with several baselines, understanding the effect of some of the choices we made (e.g. training data size, conjugate language) and comparative analysis of retrieval performance on queries of different complexity.",
        "We tested the proposed hashing-based spelling correction algorithms on two test sets:",
        "• DUMBTIONARY: 1231 misspellings of various names from Dumbtionary and a name directory consisting of about 550, 000 names gleaned from the English Wikipedia.",
        "Each of the misspellings had a correct spelling in the name directory.",
        "• INTRANET: 200 misspellings of employees taken from the search logs of the intranet of a large organization and a name directory",
        "consisting of about 150, 000 employee names.",
        "Each of the misspellings had a correct spelling in the name directory.",
        "DUMBTIONARY, the misspellings in INTRANET are more severe as the relatively high edit distance indicates.",
        "Thus, INTRANET represents very hard cases for spelling correction.",
        "name.",
        "names in English (sampled from the list of names in the Internet Movie Database) as training data and for B-HASH we used 14,941 parallel single token names in English-Hindi .",
        "Each name was represented as a feature vector over character bi-grams.",
        "Thus, the name token Klein has the bigrams ei, in, n-} as the features.",
        "We learnt the hash functions from the training data by solving the generalized Eigenvalue problems of Sections 2.1 and 2.2.",
        "For both M-HASH and B-HASH we used the top 32 Eigenvectors to form the hash function resulting in a 32 bit representation for every name token.",
        "We measured the performance of all the systems using Precision@1, the fraction of names for which a correct spelling was suggested at Rank 1.",
        "The baselines are two popular search engines",
        "of NEWS2009 workshop (http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/pages/sharedtask.html).",
        "used phonetic search algorithm (Philips, 2000) and BM25, a very popular Information Retrieval algorithm (Manning et al., 2008).",
        "To use BM25 algorithm for spelling correction, we represented each name as a bag of bigrams and set the parameters K and b to 2 and 0.75 respectively.",
        "Table 2 compares the results of the hashing-based systems with the baselines on DUMBTIONARY.",
        "As the misspellings in DUMBTIONARY are relatively easier to correct, all the systems give reasonably good retrieval results.",
        "Nevertheless, the results of M-HASH and B-HASH are substantially better than the baselines.",
        "M-HASH reduced the error over the best baseline (S1) by 13.04% whereas B-HASH reduced by 46.17% (Table 6).",
        "To get a deeper understanding of the retrieval performance of the various systems, we studied queries of varying complexity of misspelling.",
        "Table 3 compares the results of our systems with S1 for queries that are at various edit distances from the correct names.",
        "We observe that M-HASH and B-HASH are better than S1 in dealing with relatively less severe misspellings.",
        "More interestingly, B-HASH is consistently and significantly better than S1 even when the misspellings are severe.",
        "on DUMBTIONARY.",
        "5.2.2 INTRANET",
        "For INTRANET, search engines could not be used as baselines and therefore we compare our systems with Double Metaphone and BM25 in Table 4.",
        "We observe that both M-HASH and B-HASH give sign-ficantly better retrieval results than the baselines.",
        "M-HASH reduced the error by 36.20% over Double Metaphone whereas B-HASH reduced it by 51.73%.",
        "Relative to BM25, M-HASH reduced the error by 31.87% whereas B-HASH reduced it by 48.44%.",
        "Test Set",
        "Average",
        "Std.",
        "Dev.",
        "DUMBTIONARY",
        "1.39",
        "0.76",
        "INTRANET",
        "2.33",
        "1.60",
        "M-HASH",
        "B-HASH",
        "S1",
        "S2",
        "DM",
        "BM25",
        "87.93",
        "92.53",
        "86.12",
        "79.33",
        "78.95",
        "84.70",
        "Distance",
        "M-HASH",
        "B-HASH",
        "S1",
        "1",
        "96.18",
        "96.55",
        "89.59",
        "2",
        "81.79",
        "87.42",
        "75.76",
        "3",
        "44.07",
        "67.80",
        "59.65",
        "4",
        "21.05",
        "31.58",
        "29.42",
        "5",
        "0.00",
        "37.50",
        "0.00",
        "Table 5 shows the results of our systems for queries that are at various edit distances from the correct names.",
        "We observe that the retrieval results for each category of queries are consistent with the results on DUMBTIONARY.",
        "As before, B-HASH gives signficantly better results than M-HASH.",
        "on INTRANET.",
        "As both M-HASH and B-HASH are data driven systems, the effect of training data size on retrieval performance is important to study.",
        "Table 7 compares the results for systems trained with various amounts of training data on DUMBTIONARY.",
        "B-HASH trained with just 1000 name pairs gives 95.5% of the performance of B-HASH trained with 15000 name pairs.",
        "Similarly, M-HASH trained with 1000 names gives 98.5% of the performance of",
        "M-HASH trained with 30000 name pairs.",
        "This is probably because the spelling mistakes in DUMB-TIONARY are relatively easy to correct.",
        "Table 8 shows the results on INTRANET.",
        "We see that increase in the size of training data brings substantial returns for B-HASH.",
        "In contrast, M-HASH gives the best results at 5000 and does not seem to benefit from additional training data.",
        "In Sections 5.2.1 and 5.2.2, we saw that bilingual data gives substantially better results than monolingual data.",
        "In the experiments with bilingual data, we used English-Hindi data for training B-HASH.",
        "A natural question to ask is what happens when we use someother language, say Hebrew or Russian or Tamil, instead of Hindi.",
        "In other words, does the retrieval performance, on an average, vary substantially with the conjugate language?",
        "English-Hindi, English-Hebrew, English-Russian, and English-Tamil data.",
        "We see that the retrieval results are good despite the differences in the script and language.",
        "Clearly, the source language (English in our experiments) benefits from being paired with any target language.",
        "However, some languages seem to give substantially better results than others when used as the conjugate language.",
        "For instance, Hindi as a conjugate for English seems to be better than Tamil.",
        "At the time of writing this paper, we do not know the reason for this behavior.",
        "We believe that a combination of factors including feature representation, training data, and language-specific confusion matrix need to be studied in greater depth to say anything conclusively about conjugate languages.",
        "M-HASH",
        "B-HASH",
        "DM",
        "BM25",
        "70.65",
        "77.79",
        "54.00",
        "56.92",
        "Size",
        "M-HASH",
        "B-HASH",
        "1000",
        "86.60",
        "88.34",
        "5000",
        "87.36",
        "91.13",
        "10000",
        "86.96",
        "92.53",
        "15000",
        "87.19",
        "92.20",
        "30000",
        "87.93",
        "-",
        "Distance",
        "M-HASH",
        "B-HASH",
        "1",
        "82.76",
        "87.93",
        "2",
        "57.14",
        "72.86",
        "3",
        "34.29",
        "65.71",
        "4",
        "38.46",
        "53.85",
        "5",
        "6.67",
        "26.67",
        "Size",
        "M-HASH",
        "B-HASH",
        "1000",
        "66.04",
        "66.03",
        "5000",
        "70.65",
        "72.67",
        "10000",
        "68.09",
        "75.26",
        "15000",
        "68.60",
        "77.79",
        "30000",
        "65.40",
        "-",
        "Test Set",
        "M-HASH",
        "B-HASH",
        "DUMBTIONARY",
        "13.04",
        "46.17",
        "INTRANET",
        "36.20",
        "51.73",
        "We looked at cases where either M-HASH or B-HASH (or both) failed to suggest the correct spelling.",
        "It turns out that in the DUMBTIONARY test set, for 81 misspelled names, both M-HASH and B-HASH failed to suggest the correct name at rank 1 .",
        "Similarly, in the case of INTRANET test set, both M-HASH and B-HASH failed to suggest the correct name at rank 1 for 47 queries.",
        "This suggests that queries that are difficult for one system are also in general difficult for the other system.",
        "However, B-HASH was able to suggest correct names for some of the queries where M-HASH failed.",
        "In fact, in the",
        "INTRANET test set, whenever B-HASH failed, M-HASH also failed.",
        "And interestingly, in the DUMB-TIONARY test set, the average edit distance of the query and the correct name for the cases where M-HASH failed to get the correct name in top 10 while B-HASH got it at rank 1 was 2.96.",
        "This could be because M-HASH attempts to map names with smaller edit distances to similar codewords.",
        "Table 10 shows some interesting cases we found during error analysis.",
        "For the first query, M-HASH suggested the correct name whereas B-HASH did not.",
        "For the second query, both M-HASH and B-HASH suggested the correct name.",
        "And for the third query, B-HASH suggested the correct name whereas",
        "M-HASH did not.",
        "The average query response time is a measure of the speed of a system and is an important factor in real deployments of a Spelling Correction system.",
        "Ideally, one would like the average query response time to be as small as possible.",
        "However, in practice, average query response time is not only a function of the algorithm's computational complexity but also the computational infrastructure supporting the system.",
        "In our expriments, we used a single threaded implementation of M-HASH and B-HASH on an Intel Xeon processor (2.86 GHz).",
        "Table 11 shows the average query response time.",
        "We note that M-HASH is substantially slower than B-HASH.",
        "This is because the number of collisions in the NAMEJBUCKETING stage is higher for M-",
        "HASH.",
        "We would like to point out that both NAMEJBUCKETING and NAMEJMATCHING stages can be multi-threaded on a multi-core machine and the query response time can be decreased by an order easily.",
        "Further, the memory footprint of the system is very small and the codewords require 4.1 MB for the employees name directory name directory (550,000 names).",
        "Table 11: Average response time in milliseconds (single threadedsystemrunning on2.86GHzIntelXeonProces-sor)."
      ]
    },
    {
      "heading": "6. Related Work",
      "text": [
        "Spelling correction of written text is a well studied problem (Kukich, 1996), (Jurafsky and Martin, 2008).",
        "The first approach to spelling correction made use of a lexicon to correct out-of-lexicon terms by finding the closest in-lexicon word (Dam-erau, 1964).",
        "The similarity between a misspelled word and an in-lexicon word was measured using Edit Distance (Jurafsky and Martin, 2008).",
        "The next class of approaches applied the noisy channel model to correct single word spelling errors (Kernighan et al., 1990), (Brill and Moore, 2000).",
        "A major flaw of single word spelling correction algorithms is they do not make use of the context of the word in correcting the errors.",
        "The next stream of approaches explored ways of exploiting the word's context (Golding and several works have leveraged the Web for improved spelling correction (Chen et al., 2007),(Islam and Inkpen, 2009), (Whitelaw etal., 2009).",
        "Spelling correction algorithms targeted for web-search queries have been developed making use of query logs and click-thru data (Cucerzan and Brill, 2004), (Ahmad and Kondrak, 2005), (Sun et al., 2010).",
        "None of these approaches focus exclusively on correcting name misspellings.",
        "Query",
        "M-HASH",
        "B-HASH",
        "John Tiler",
        "John Tyler",
        "John Tilley",
        "Ddear Dragba",
        "Didear Drogba",
        "Didear Drogba",
        "James Pol",
        "James Poe",
        "James Polk",
        "Conjugate",
        "DUMBTIONARY",
        "INTRANET",
        "Hindi",
        "92.53",
        "77.79",
        "Hebrew",
        "91.30",
        "71.68",
        "Russian",
        "89.42",
        "64.94",
        "Tamil",
        "90.48",
        "69.12",
        "Test Set",
        "MHASH",
        "BHASH",
        "DUMBTIONARY",
        "190",
        "87",
        "INTRANET",
        "148",
        "75",
        "Name matching techniques have been studied in the context of database record deduplication, text mining, and information retrieval (Christen, 2006), (Pfeifer et al., 1996).",
        "Most techniques use one or more measures of phonetic similarity and/or string similarity.",
        "The popular phonetic similarity-based techniques are Soundex, Phonix, and Metaphone (Pfeifer et al., 1996).",
        "Some of the string similarity-based techniques employ Damerau-Levenshtein edit distance, Jaro distance or Winkler distance (Christen, 2006).",
        "Data driven approaches for learning edit distance have also been proposed (Ristad and Yiani-los, 1996).",
        "Most of these techniques either give poor retrieval performance on large name directories or do not scale.",
        "Hashing techniques for similarity search is also a well studied problem (Shakhnarovich et al., 2008).",
        "Locality Sensitive Hashing (LSH) is a theoretically grounded data-oblivious approach for using random projections to define the hash functions for data objects with a single view (Charikar, 2002), (Andoni and Indyk, 2006).",
        "Although LSH guarantees that asymptotically the Hamming distance between the codewords approaches the Euclidean distance between the data objects, it is known to produce long codewords making it practically inefficient.",
        "Recently data-aware approaches that employ Machine Learning techniques to learn hash functions have been proposed and shown to be a lot more effective than LSH on both synthetic and real data.",
        "Semantic Hashing employs Restricted Boltzmann Machine to produce more compact codes than LSH (Salakhutdi-nov and Hinton, 2009).",
        "Spectral Hashing formalizes the requirements for a good code and relates them to the problem of balanced graph partitioning which is known to be NP hard (Weiss et al., 2008).",
        "To give a practical algorithm for hashing, Spectral Hashing assumes that the data are sampled from a multidimensional uniform distribution and solves a relaxed partitioning problem."
      ]
    },
    {
      "heading": "7. Conclusions",
      "text": [
        "We developed two hashing-based techniques for spelling correction of person names in People Search applications.To the best of our knowledge, these are the first techniques that focus exclusively on correcting spelling mistakes in person names.",
        "Our approach has several advantages over other spelling correction techniques.",
        "Firstly, we do not suggest incorrect suggestions for valid queries unlike (Cucerzan and Brill, 2004).",
        "Further, as we suggest spellings from only authoritative name directories, the suggestions are always well formed and coherent.",
        "Secondly, we do not require query logs and other resources that are not easily available unlike (Cucerzan and Brill, 2004), (Ahmad and Kon-drak, 2005).",
        "Neither do we require pairs of misspelled names and their correct spellings for learning the error model unlike (Brill and Moore, 2000) or large-coverage general purpose lexicon for unlike (Cucerzan and Brill, 2004) or pronunciation dictionaries unlike (Toutanova and Moore, 2002).",
        "Thirdly, we correct the query as a whole unlike (Ahmad and Kondrak, 2005) and can handle word order changes unlike (Cucerzan and Brill, 2004).",
        "Fourthly, we do not iteratively process misspelled name unlike (Cucerzan and Brill, 2004).",
        "Fifthly, we handle large name directories efficiently unlike the spectrum of name matching techniques discussed in (Pfeifer et al., 1996).",
        "Finally, our training data requirement is relatively small.",
        "As future work, we would like to explore the possibility of learning hash functions using 1) bilingual and monolingual data together and 2) multiple conjugate languages."
      ]
    }
  ]
}
