{
  "info": {
    "authors": [
      "Arun Ahuja",
      "Doug Downey"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1026",
    "title": "Improved Extraction Assessment through Better Language Models",
    "url": "https://aclweb.org/anthology/N10-1026",
    "year": 2010
  },
  "references": [
    "acl-D09-1098",
    "acl-P06-1101",
    "acl-P06-1102",
    "acl-P07-1088",
    "acl-P96-1041"
  ],
  "sections": [
    {
      "text": [
        "Arun Ahuja, Doug Downey",
        "A variety of information extraction techniques rely on the fact that instances of the same relation are \"distributionally similar,\" in that they tend to appear in similar textual contexts.",
        "We demonstrate that extraction accuracy depends heavily on the accuracy of the language model utilized to estimate distributional similarity.",
        "An unsupervised model selection technique based on this observation is shown to reduce extraction and type-checking error by 26% over previous results, in experiments with Hidden Markov Models.",
        "The results suggest that optimizing statistical language models over unlabeled data is a promising direction for improving weakly supervised and unsupervised information extraction."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Many weakly supervised and unsupervised information extraction techniques assess the correctness of extractions using the distributional hypothesis – the notion that words with similar meanings tend to occur in similar contexts (Harris, 1985).",
        "A candidate extraction of a relation is deemed more likely to be correct when it appears in contexts similar to those of \"seed\" instances of the relation, where the seeds may be specified by hand (Pa§ca et al., 2006), taken from an existing, incomplete knowledge base (Snow et al., 2006; Pantel et al., 2009), or obtained in an unsupervised manner using a generic extractor (Banko et al., 2007).",
        "We refer to this technique as Assessment by Distributional Similarity (ADS).",
        "Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus.",
        "Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. (Chen and Goodman, 1996; Rabiner, 1989; Bell et al., 1990)).",
        "Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance (Downey et al., 2007).",
        "Unla-beled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible.",
        "But how fruitful is such an effort likely to be – to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy?",
        "In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs becomes more accurate.",
        "In a large-scale set of experiments, we quantify how language model perplexity correlates with ADS performance over multiple data sets and SLM techniques.",
        "The experiments show that accuracy over unlabeled data can be used for selecting among SLMs – for an ADS approach utilizing Hidden Markov Models, this results in an average error reduction of 26% over previous results in extraction and type-checking tasks."
      ]
    },
    {
      "heading": "2. Extraction Assessment with Language Models",
      "text": [
        "We begin by formally defining the extraction and typechecking tasks we consider, then discuss statistical language models and their utilization for extraction assessment.",
        "The extraction task we consider is formalized as follows: given a corpus, a target relation R, a list of seed instances Sr, and a list of candidate extractions Ur, the task is to order elements of UR such that correct instances for R are ranked above extraction errors.",
        "Let URi denote the set of the ith arguments of the extractions in Ur, and let SRi be defined similarly for the seed set Sr .",
        "For relations of arity greater than one, we consider the typechecking task, an important subtask of extraction (Downey et al., 2007).",
        "The typechecking task is to rank extractions with arguments that are of the proper type for a relation above type errors.",
        "As an example, the extraction Founded(Bill Gates, Oracle) is type correct, but is not correct for the extraction task.",
        "A Statistical Language Model (SLM) is a probability distribution P(w) over word sequences w = (w1,...,wr).",
        "The most common SLM techniques are n-gram models, which are Markov models in which the probability of a given word is dependent on only the previous n – 1 words.",
        "The accuracy of an n-gram model of a corpus depends on two key factors: the choice of n, and the smoothing technique employed to assign probabilities to word sequences seen infrequently in training.",
        "We experiment with choices of n from 2 to 4, and two popular smoothing approaches, Modified Kneser-Ney (Chen and Goodman, 1996) and Witten-Bell (Bell et al., 1990).",
        "Unsupervised Hidden Markov Models (HMMs) are an alternative SLM approach previously shown to offer accuracy and scalability advantages over n-gram models in ADS (Downey et al., 2007).",
        "An HMM models a sentence w as a sequence of observations wi each generated by a hidden state variable ti.",
        "Here, hidden states take values from {1,..., T}, and each hidden state variable is itself generated by some number k of previous hidden states.",
        "Formally, the joint distribution of a word sequence w given a corresponding state sequence t is:",
        "The distributions on the right side of Equation 1 are learned from the corpus in an unsupervised manner using Expectation-Maximization, such that words distributed similarly in the corpus tend to be generated by similar hidden states (Rabiner, 1989).",
        "The Assessment by Distributional Similarity (ADS) technique is to rank extractions in Ur in decreasing order of distributional similarity to the seeds, as estimated from the corpus.",
        "In our experiments, we utilize an ADS approach previously proposed for HMMs (Downey et al., 2007) and adapt it to also apply to n-gram models, as detailed below.",
        "Define a context of an extraction argument ei to be a string containing the m words preceding and m words following an occurrence of ei in the corpus.",
        "Let Ci = {c1,c2,c\\d\\} be the union of all contexts of extraction arguments ei and seed arguments si for a given relation R. We create a probabilistic context vector for each extraction ei where the j-th dimension of the vector is the probability of the context surrounding given the extraction, P(cj |ei), computed from the language model.",
        "We rank the extractions in Ur according to how similar their arguments' contextual distributions, P(c|ei), are to those of the seed arguments.",
        "Specifically, extractions are ranked according to:",
        "where KL represents KL Divergence, and the outer sum is taken over arguments ei of the extraction e.",
        "For HMMs, we alternatively rank extractions using the HMM state distributions P(t| ei) in place of the probabilistic context vectors P(c|ei).",
        "Our experiments show that state distributions are much more accurate for ADS than are HMM context vectors."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "In this section, we present experiments showing that SLM accuracy correlates strongly with ADS performance.",
        "We also show that SLM performance can be used for model selection, leading to an ADS technique that outperforms previous results.",
        "We experiment with a wide range of n-gram and HMM models.",
        "The n-gram models are trained using the SRILM toolkit (Stolcke, 2002).",
        "Evaluating a",
        "Table 1: Pearson Correlation value for extraction performance (in AUC) and SLM performance (in perplexity).",
        "Extraction accuracy increases as perplexity decreases, with an average correlation coefficient of -0.742.",
        "\"HMM k-T\" denotes an HMM model of order k, with T states.",
        "variety of HMM configurations over a large corpus requires a scalable training architecture.",
        "We constructed a parallel HMM codebase using the Message Passing Interface (MPI), and trained the models on a supercomputing cluster.",
        "All language models were trained on a corpus of 2.8M sentences of Web text (about 60 million tokens).",
        "SLM performance is measured using the standard perplexity metric, and assessment accuracy is measured using area under the precision-recall curve (AUC), a standard metric for ranked lists of extractions.",
        "We evaluated performance on three distinct data sets.",
        "The first two data sets evaluate ADS for unsupervised information extraction, and were taken from (Downey et al., 2007).",
        "The first, Unary, was an extraction task for unary relations (Company, Country, Language, Film) and the second, Binary, was a type-checking task for binary relations (Conquered, Founded, Headquartered, Merged).",
        "The 10 most frequent extractions served as bootstrapped seeds.",
        "The two test sets contained 361 and 265 extractions, respectively.",
        "The third data set, Wikipedia, evaluates ADS on weakly-supervised extraction, using seeds and extractions taken from Wikipedia 'List of' pages (Pantel et al., 2009).",
        "Seed sets of various sizes (5, 10, 15 and 20) were randomly selected from each list, and we present results averaged over 10 random samplings.",
        "Other members of the seed list were added to a test set as correct extractions, and elements from other lists were added as errors.",
        "The data set included",
        "SLM Perplexity",
        "Figure 1: HMM 1-100 Performance.",
        "Information Extraction performance (in AUC) increases as SLM accuracy improves (perplexity decreases).",
        "2264 extractions across 36 unary relations, including Composers and US Internet Companies.",
        "The first question we investigate is whether optimizing individual language models leads to better performance in ADS.",
        "We measured the correlation between SLM perplexity and ADS performance as training proceeds in HMMs, and as n and the smoothing technique vary in the n-gram models.",
        "Table 1 shows that as the SLM becomes more accurate (i.e. as perplexity decreases), ADS performance increases.",
        "The correlation is strong (averaging -0.742) and is consistent across model configurations and data sets.",
        "The low positive correlation for the n-gram models on Wikipedia is likely due to a \"floor effect\"; the models have low performance overall on the difficult Wikipedia data set.",
        "The lowest-perplexity n-gram model (Mod Kneser-Ney smoothing with n=3, KN3) does exhibit the best IE performance, at 0.039 (the average performance of the HMM models is more than twice this, at 0.084).",
        "Figure 1 shows the relationship between SLM and ADS performance in detail for the best-performing HMM configuration.",
        "Different language models can be configured in dif-ferentways: forexample, HMMs require choices for the hyperparameters k and T .",
        "Here, we show that",
        "LM",
        "Unary",
        "Binary",
        "Wikipedia",
        "HMM 1-5",
        "-.911",
        "-.361",
        "-.994",
        "HMM 2-5",
        "-.856",
        ".120",
        "-.930",
        "HMM 3-5",
        "-.823",
        "-.683",
        ".922",
        "HMM 1-10",
        "-.916",
        "-.967",
        "-.905",
        "HMM 2-10",
        "-.877",
        "-.797",
        "-.963",
        "HMM 3-10",
        "-.957",
        "-.669",
        "-.924",
        "HMM 1-25",
        "-.933",
        "-.850",
        "-.959",
        "HMM 1-50",
        "-.942",
        "-.942",
        "-.947",
        "HMM 1-100",
        "-.896",
        "-.877",
        "-.942",
        "N-Gram",
        "-.512",
        "-.999",
        ".024",
        "Figure 2: Model Selection for HMMs.",
        "SLM performance is a good predictor of extraction performance across model configurations.",
        "SLM perplexity can be used to select a high-quality model configuration for ADS using only unlabeled data.",
        "We evaluate on the Unary and Binary data sets, since they have been employed in previous work on our corpora.",
        "Figure 2 shows that for HMMs, ADS performance increases as perplexity decreases across various model configurations (a similar relationship holds for n-gram models).",
        "A model selection technique that picks the HMM model with lowest perplexity (HMM 1-100) results in better ADS performance than previous results.",
        "As shown in Table 2, HMM 1-100 reduces error over the HMM-T model in (Downey et al., 2007) by 26%, on average.",
        "The experiments also reveal an important difference between the HMM and n-gram approaches.",
        "While KN3 is more accurate in SLM than our HMM models, it performs worse in ADS on average.",
        "For example, HMM 1-25 underperforms KN3 in perpexity, at 537.2 versus 227.1, but wins in ADS, 0.880 to 0.853.",
        "We hypothesize that this is because the latent state distributions in the HMMs provide a more informative distributional similarity measure.",
        "Indeed, when we compute distributional similarity for HMMs using probabilistic context vectors as opposed to state distributions, ADS performance for HMM 1-25 decreases to 5.8% below that of KN3."
      ]
    },
    {
      "heading": "4. Conclusions",
      "text": [
        "We presented experiments showing that estimating distributional similarity with more accurate statistical language models results in more accurate extrac-",
        "Table 2: Extraction Performance Results in AUC for Individual Relations.",
        "The lowest-perplexity HMM, 1-100, outperforms the HMM-T model from previous work.",
        "tion assessment.",
        "We note that significantly larger, more powerful language models are possible beyond those evaluated here, which (based on the trajectory observed in Figure 2) may offer significant improvements in assessment accuracy.",
        "Relation",
        "HMM-T",
        "Best HMM",
        "Company",
        ".966",
        ".985",
        "Country",
        ".886",
        ".942",
        "Languages",
        ".936",
        ".914",
        "Film",
        ".803",
        ".801",
        "Unary Avg",
        ".898",
        ".911",
        "Conquered",
        ".917",
        ".923",
        "Founded",
        ".827",
        ".799",
        "Merged",
        ".920",
        ".925",
        "Headquartered",
        ".734",
        ".964",
        "Binary Average",
        ".849",
        ".903"
      ]
    }
  ]
}
