{
  "info": {
    "authors": [
      "Sanaz Jabbari",
      "Mark Hepple",
      "Louise Guthrie"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1037",
    "title": "Evaluation Metrics for the Lexical Substitution Task",
    "url": "https://aclweb.org/anthology/N10-1037",
    "year": 2010
  },
  "references": [
    "acl-W07-2009",
    "acl-W07-2044"
  ],
  "sections": [
    {
      "text": [
        "Sanaz Jabbari Mark Hepple Louise Guthrie",
        "We identify some problems of the evaluation metrics used for the English Lexical Substitution Task of SemEval-2007, and propose alternative metrics that avoid these problems, which we hope will better guide the future development of lexical substitution systems."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The English Lexical Substitution task at SemEval-2007 (here called ELS07) requires systems to find substitutes for target words in a given sentence (McCarthy & Navigli, 2007: M&N).",
        "For example, we might replace the target word match with game in the sentence they lost the match.",
        "System outputs are evaluated against a set of candidate substitutes proposed by human subjects for test items.",
        "Targets are typically sense ambiguous (e.g. match in the above example), and so task performance requires a combination of word sense disambiguation (by exploiting the given sentential context) and (near) synonym generation.",
        "In this paper, we discuss some problems of the evaluation metrics used in ELS07, and then propose some alternative measures that avoid these problems, and which we believe will better serve to guide the development of lexical substitution systems in future work.",
        "The subtasks within ELS07 divide into two groups, in terms of whether they focus on a system's 'best' answer for a test item, or address the broader set of answer candidates a system can produce.",
        "In what follows, we address these two cases in separate sections, and then present some results for applying our new metrics for the second case.",
        "We begin by briefly introducing the test materials that were created for the ELS07 evaluation."
      ]
    },
    {
      "heading": "2. Evaluation Materials",
      "text": [
        "Briefly stated, the ELS07 dataset comprises around 2000 sentences, providing 10 test sentences each for some 201 preselected target words, which were required to be sense ambiguous and have at least one synonym, and which include nouns, verbs, adjectives and adverbs.",
        "Five human annotators were asked to suggest up to three substitutes for the target word of each test sentence, and their collected suggestions serve as the gold standard against which system outputs are compared.",
        "Around 300 sentences were distributed as development data, and the remainder retained for the final evaluation.",
        "To assist defining our metrics, we formally describe this data as follows.",
        "For each sentence tiin the test data (1 < i < N, N the number of test items), let Hi denote the set of human proposed substitutes.",
        "A key aspect of the data is the count of human annotators that proposed each candidate (since a term appears a stronger candidate if proposed by annotators).",
        "For each ti, there is a function freqiwhich returns this count for each term within Hi (and 0 for any other term), and a value maxfreqicorresponding to the maximal count for any term in Hi.",
        "The pairing of Hi and freqi in effect provides a multiset representation of the human answer set.",
        "We use \\S\\i in what follows to denote the multiset cardinality of S according to freqi, i.e. YJa^sfreqi(a).",
        "Some of the ELS07 metrics use a notion of mode answer mi, which exists only for test items that have a single most-frequent human response, i.e. a unique a G Hi such that freqi(a) = maxfreqi.",
        "To adapt an example from M&N, an item with target word happy (adj) might have human answers {glad, merry, sunny, jovial, cheerful} with counts (3,3,2,1,1) respectively.",
        "We will abbreviate this answer set as Hi = {G:3,M:3,S:2,J:1,Ch:1} where it is used later in the paper."
      ]
    },
    {
      "heading": "3. Best Answer Measures",
      "text": [
        "Two of the ELS07 tasks address how well systems are able to find a 'best' substitute for a test item, for which individual test items are scored as follows:",
        "freq i(a) if bgi = mi otherwise",
        "For the first task, a system can return a set of answers Ai (the answer set for item i), but since the score achieved is divided by Ai , returning multiple answers only serves to allow a system to 'hedge its bets' if it is uncertain which candidate is really the best.",
        "The optimal score on a test item is achieved by returning a single answer whose count is maxfreqi, with proportionately lesser credit being received for any answer in Hi with a lesser count.",
        "For the second task, which uses the mode metric, only a single system answer - its 'best guess' bgi - is allowed, and the score is simply 0 or 1 depending on whether the best guess is the mode.",
        "Overall performance is computed by averaging across a broader set of test items (which for the second task includes only items having a mode value).",
        "M&N distinguish two overall performance measures: Recall, which averages over all relevant items, and Precision, which averages only over those items for which the system gave a non-empty response.",
        "We next discuss these measures and make an alternative proposal.",
        "The task for the first measure seems a reasonable one, i.e. assessing the ability of systems to provide a 'best' answer for a test item, but allowing them to offer multiple candidates (to 'hedge their bets').",
        "However, the metric is unsatisfactory in that a system that performs optimally in terms of this task (i.e. which, for every test item, returns a single correct 'most frequent' response) will get a score that is well below 1, because the score is also divided by Hi i, the multiset cardinality of Hi, whose size varies between test items (being a consequence of the number of alternatives suggested by the human annotators), but which is typically larger than the numerator value maxfreqi of an optimal answer (unless Hi is singleton).",
        "This problem is fixed in the following modified metric definition, by dividing instead by maxfreqi, as then a response containing a single optimal answer will score 1.",
        "maxfreqi x\\Ai\\",
        "freqi(bgj) maxfreqi an optimal response Ai = {M} receives score 1, where the original metric gives score 0.3.",
        "Singleton responses containing a correct but non-optimal answer receive proportionately lower credit, e.g. for Ai = {S} we score 0.66 (vs. 0.2 for the original metric).",
        "For a non-singleton answer set including, say, a correct answer and an incorrect one, the credit for the correct answer will be halved, e.g. for Ai = {S,X} we score 0.33.",
        "Regarding the second task, we think it reasonable to have a task where systems may offer only a single 'best guess' response, but argue that the mode metric used has two key failings: it is too brittle in being applicable only to items that have a mode answer, and it loses information valuable to system ranking, in assigning no credit to a response that might be good but not optimal.",
        "We propose instead the besti metric above, which assigns score 1 to a best guess answer with count maxfreqi, but applies to all test items irrespective of whether or not they have a unique mode.",
        "For answers having lesser counts, proportionately less credit is assigned.",
        "This metric is equivalent to the new best metric shown beside it for the case where Ai =1.",
        "For assessing overall performance, we suggest just taking the average of scores across all test items, c.f.",
        "M&N's Recall measure.",
        "Their Precision metric is presumably intended to favour a system that can tell whether it does or does not have any good answers to return.",
        "However, the ability to draw a boundary between good vs. poor candidates will be reflected widely in a system's performance and captured elsewhere (not least by the coverage metrics discussed later) and so, we argue, does not need to be separately assessed in this way.",
        "Furthermore, the fact that a system does not return any answers may have other causes, e.g. that its lexical resources have failed to yield any substitution candidates for a term."
      ]
    },
    {
      "heading": "4. Measures of Coverage",
      "text": [
        "A third task of ELS07 assesses the ability of systems to field a wider set of good substitution candidates for a target, rather than just a 'best' candidate.",
        "This 'out of ten' (oot) task allows systems to offer a set Ai of upto 10 guesses per item i, and is scored as:",
        "Since the score is not divided by the answer set size \\Ai\\, no benefit derives from offering less than 10 candidates.",
        "When systems are asked to field a broader set of candidates, we suggest that evaluation should assess if the response set is good in containing as many correct answers as possible, whilst containing as few incorrect answers as possible.",
        "In general, systems will tackle this problem by combining a means of ranking candidates (drawn from lexical resources) with a means ofdrawing a boundary between good and bad candidates, e.g. threshold setting.",
        "Since the oot metric does not penalise incorrect answers, it does not encourage systems to develop such boundary methods, even though this is important to their ultimate practical utility.",
        "The view of a 'good' answer set described above suggests a comparison of Ai to Hi using versions of 'recall' and 'precision' metrics, that incorporate the 'weighting' of human answers via freqi.",
        "Let us begin by noting the obvious definitions for recall and precision metrics without count-weighting:",
        "Our definitions of these metrics, given below, do include count-weighting, and require some explanation.",
        "The numerator of our recall definition is \\Ai\\inot \\Hi n Ai\\i as \\Ai\\i = \\Hi n Ai\\i (as freqi assigns 0 to any term not in Hi), an observation which also affects the numerator of our P definition.",
        "Regarding the latter's denominator, merely dividing by Ai i would not penalise incorrect terms (as, again, freqi(a) = : for any a G Hi), so this is done directly by adding k\\Ai – Hi\\, where \\Ai – Hi\\ is the number of incorrect answers, and k some penalty factor, which might be k = 1 in the simplest case.",
        "(Note that our weighted R metric is in fact equivalent to the oot definition above.)",
        "As usual, an F-score can be computed as the harmonic mean of these values (i.e. F = 2PR/(P + R)).",
        "For assessing overall performance, we might average P, R and F scores across all test items."
      ]
    },
    {
      "heading": "5. Applying the Coverage measure",
      "text": [
        "Although the 'best guess' task is a valuable indicator of the likely utility of a lexical substitution system within various broader applications, we would argue that the core task for lexical substitution is coverage, i.e. the ability to field a broad set of correct substitution candidates.",
        "This task requires systems both to field and rank promising candidates, and to have a means of drawing a boundary between the good and bad candidates, i.e. a boundary strategy.",
        "In this section, we apply the coverage metrics to the outputs of some lexical substitution systems, and",
        "Table 2: Optimal F-scores (macro-avgd) for coverage, computed over the (oot) ranked outputs of the systems (with penaltyfactor k =1).",
        "compare the indication it provides of relative system performance to that of the oot metric.",
        "We consider three systems described in Jabbari (2010), developed as part of an investigation into the means and benefits of combining models of lexical context: (i) bow: a system using a bag-of-words model to rank candidates, (ii) lm: using a (simple) n-gram language model, and (iii) cmlc: using a model that combines bow and lm models into one.",
        "We also consider the system KU, which uses a very large language model and an advanced treatment ofsmoothing, and which performed well at ELS07 (Yuret, 2007).",
        "Table 1 shows the oot scores for these systems, including a breakdown by part-of-speech, which indicate a performance ranking: bow < lm < cmlc < KU Our first problem is that these systems are developed for the oot task, not coverage, so after ranking their candidates, they do not attempt to draw a boundary between the candidates worth returning and those not.",
        "Instead, we here use the oot outputs to compute an optimal performance for each system, i.e. we find, for the ranked candidates of each question, the cut-off position giving the highest F-score, and then average these scores across questions, which tells us the F-score the system could achieve if it had an optimal boundary strategy.",
        "These scores, shown in Table 2, indicate a ranking of systems in line with that in Table 1, which is not surprising as both will ultimately reflect the quality of candidate ranking achieved by the systems.",
        "Table 3 shows the coverage results achieved by applying a naive boundary strategy to the system outputs.",
        "The strategy is just to always return the top n candidates for each question, for a fixed value n. Again, performance correlates straightforwardly with the underlying quality of ranking.",
        "Comparing tables, we see, for example, that by always returning 6 candidates, the system KU could achieve a coverage of .32 as compared to the .435 optimal score.",
        "Model",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "8",
        "9",
        "10",
        "bow",
        ".067",
        ".114",
        ".151",
        ".173",
        ".191",
        ".201",
        ".212",
        ".219",
        ".222",
        ".225",
        "Im",
        ".119",
        ".192",
        ".228",
        ".246",
        ".256",
        ".267",
        ".271",
        ".272",
        ".271",
        ".271",
        "cmlc",
        ".139",
        ".205",
        ".251",
        ".271",
        ".284",
        ".288",
        ".291",
        ".290",
        ".289",
        ".286",
        "KU",
        ".173",
        ".244",
        ".287",
        ".307",
        ".318",
        ".321",
        ".320",
        ".318",
        ".314",
        ".311",
        "All",
        "By part-of-speech",
        "Model",
        "words",
        "nouns",
        "adj",
        "verb",
        "adv",
        "bow",
        ".326",
        ".343",
        ".334",
        ".205",
        ".461",
        "lm",
        ".393",
        ".372",
        ".442",
        ".252",
        ".562",
        "cmlc",
        ".414",
        ".404",
        ".447",
        ".311",
        ".534",
        "KU",
        ".462",
        ".408",
        ".511",
        ".398",
        ".567",
        "All",
        "By part-of-speech",
        "Model",
        "words",
        "nouns",
        "adj",
        "verb",
        "adv",
        "bow",
        ".298",
        ".315",
        ".302",
        ".189",
        ".422",
        "lm",
        ".371",
        ".35",
        ".408",
        ".24",
        ".539",
        "cmlc",
        ".395",
        ".383",
        ".419",
        ".31",
        ".506",
        "KU",
        ".435",
        ".379",
        ".477",
        ".385",
        ".536"
      ]
    }
  ]
}
