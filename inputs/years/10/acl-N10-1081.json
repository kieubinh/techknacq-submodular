{
  "info": {
    "authors": [
      "Shay B. Cohen",
      "David M. Blei",
      "Noah A. Smith"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1081",
    "title": "Variational Inference for Adaptor Grammars",
    "url": "https://aclweb.org/anthology/N10-1081",
    "year": 2010
  },
  "references": [
    "acl-D07-1072",
    "acl-J93-2004",
    "acl-N07-1018",
    "acl-N09-1009",
    "acl-N09-1012",
    "acl-N09-1019",
    "acl-N09-1036",
    "acl-P04-1061",
    "acl-P07-1094",
    "acl-P08-1046",
    "acl-P09-1012",
    "acl-P96-1024",
    "acl-W08-0704"
  ],
  "sections": [
    {
      "text": [
        "Adaptor grammars extend probabilistic context-free grammars to define prior distributions over trees with \"rich get richer\" dynamics.",
        "Inference for adaptor grammars seeks to find parse trees for raw text.",
        "This paper describes a variational inference algorithm for adaptor grammars, providing an alternative to Markov chain Monte Carlo methods.",
        "To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion.",
        "We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC.",
        "Further, we show a significant speed-up when parallelizing the algorithm.",
        "Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007).",
        "Such methods have been made more flexible with nonparamet-ric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002).",
        "One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns.",
        "Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar.",
        "Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences.",
        "Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005).",
        "MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously slow to converge, especially with complex hidden structures such as syntactic trees.",
        "Johnson (2008b) comments on this, and suggests the use of variational inference as a possible remedy.",
        "Variational inference provides a deterministic alternative to sampling.",
        "It was introduced for Dirich-let process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al.",
        "(2007).",
        "With NP Bayes models, variational methods are based on the stick-breaking representation (Sethu-raman, 1994).",
        "Devising a stick-breaking representation is a central challenge to using variational inference in this setting.",
        "The rest of this paper is organized as follows.",
        "In §2 we describe a stick-breaking representation of adaptor grammars, which enables variational inference (§3) and a well-defined incorporation of recursion into adaptor grammars.",
        "In §4 we give an empirical comparison of the algorithm to MCMC inference and describe a novel application of adaptor grammars to unsupervised dependency parsing."
      ]
    },
    {
      "heading": "2. Adaptor Grammars",
      "text": [
        "We review adaptor grammars and develop a stick-breaking representation of the tree distribution.",
        "Adaptor grammars capture syntactic regularities in sentences by placing a nonparametric prior over the distribution of syntactic trees that underlie them.",
        "The model exhibits \"rich get richer\" dynamics: once a tree is generated, it is more likely to reappear.",
        "Adaptor grammars were developed by Johnson et al.",
        "(2006).",
        "An adaptor grammar is a tuple A = (G, M, a, b, a), which contains: (i) a context-free grammar = (W, N, S) where W is the set of terminals, N is the set of nonterminals, R is a set of production rules, and S G N is the start symbol – we denote by Ra the subset of R with left-hand side A; (ii) a set of adapted nonterminals, M ç N; and (iii) parameters a, b and a, which are described below.",
        "An adaptor grammar assumes the following generative process of trees.",
        "First, the multinomial distributions 6 for a PCFG based on G are drawn from Dirichlet distributions.",
        "Specifically, multinomial 6a ~ Dir(aa) where a is collection of Dirichlet parameters, indexed by A G N.",
        "Trees are then generated top-down starting with S. Any non-adapted nonterminal A G N \\ M is expanded by drawing a rule from Ra.",
        "There are two ways to expand A G M:",
        "1.",
        "With probability (nz – ba)/(na + a a) we expand A to subtree z (a tree rooted at A with a yield in W*), where nz is the number of times the tree z was previously generated and na is the total number of subtrees (tokens) previously generated root being A.",
        "We denote by a the concentration parameters and b the discount parameters, both indexed by A G M. We have a a > 0 and 6a g [0,1].",
        "2.",
        "With probability (a a + kaba)/(na + a a), A is expanded as in a PCFG by a draw from 6a over Ra, where ka is the number of subtrees (types) previously generated with root A.",
        "For the expansion of adapted nonterminals, this process can be explained using the Chinese restaurant process (CRP) metaphor: a \"customer\" (corresponding to a partially generated tree) enters a \"restaurant\" (corresponding to a nonterminal) and selects a \"table\" (corresponding to a subtree) to attach to the partially generated tree.",
        "If she is the first customer at the table, the PCFG (G, 6) produces the new table's associated \"dish\" (a subtree).",
        "When adaptor grammars are defined using the CRP, the PCFG G has to be non-recursive with respect to the adapted nonterminals.",
        "More precisely, for A G N, denote by Reachable(G, A) all the nonterminals that can be reached from A using a partial derivation from G. Then we restrict G such that for all A G M, we have A G Reachable(G, A).",
        "Without this restriction, we might end up in a situation where the generative process is ill-defined: in the CRP terminology, a customer could enter a restaurant and select a table whose dish is still in the process of being selected.",
        "In the more general form of adaptor grammars with arbitrary adaptors, the problem amounts to mutually dependent definitions of distributions which rely on the others to be defined.",
        "We return to this problem in §3.1.",
        "Inference The inference problem is to compute the posterior distribution of parse trees given observed sentences x = (x\\,... ,xn).",
        "Typically, inference with adaptor grammars is done with Gibbs sampling.",
        "Johnson et al.",
        "(2006) use an embedded Metropolis-Hastings sampler (Robert and Casella, 2005) inside a Gibbs sampler.",
        "The proposal distribution is a PCFG, resembling a tree substitution grammar (TSG; Joshi, 2003).",
        "The sampler of Johnson et al.",
        "is based on the representation of the PY process as a distribution over partitions ofintegers.",
        "This representation is not amenable to variational inference.",
        "To develop a variational inference algorithm for adaptor grammars, we require an alternative representation of the model in §2.1.",
        "The CRP-based definition implicitly marginalizes out a random distribution over trees.",
        "For variational inference, we construct that distribution.",
        "We first review the Dirichlet process and its stick-breaking representation.",
        "The Dirichlet process defines a distribution over distributions.",
        "Samples from the Dirichlet process tend to deviate from a base distribution depending on a concentration parameter.",
        "Let G ~ DP(G0, a) be a distribution sampled from the Dirichlet process with base distribution G0 and concentration parameter a.",
        "The distribution G is discrete, which means it puts positive mass on a countable number of atoms drawn from G0.",
        "Repeated draws from G exhibit the \"clustering property,\" which means that they will be assigned to the same value with positive probability.",
        "Thus, they exhibit a partition structure.",
        "Marginalizing out G, the distribution of that partition structure is given by a CRP with parameter a (Pitman, 2002).",
        "The stick-breaking process gives a constructive definition of G (Sethuraman, 1994).",
        "With the stick-breaking process (for the PY process), we first sample \"stick lengths\" n – GEM(a, b) (in the case of Dirichlet process, we have b = 0).",
        "The GEM partitions the interval [0,1] into countably many segments.",
        "First, draw vi – Beta(1 – b, a + ib) for i g {1,...}.",
        "Then, define n = vi nj=1(1 – Vj).",
        "In addition, we also sample infinitely many \"atoms\" independently zi – G0.",
        "Define G as:",
        "where ö(zi, z) is 1 if zi = z and 0 otherwise.",
        "This random variable is drawn from a Pitman-Yor process.",
        "Notice the discreteness of G is laid bare in the stick-breaking construction.",
        "With the stick-breaking representation in hand, we turn to a constructive definition of the distribution over trees given by an adaptor grammar.",
        "Let A1,..., AK be an enumeration of the nonterminals in M which satisfies: i < j => Aj G Reachable(G, Ai).",
        "(That this exists follows from the assumption about the lack of recursiveness of adapted nonterminals.)",
        "Let Yield(z) be the yield of a tree derivation z.",
        "The process that generates observed sentences x = (x1,..., xn) from the adaptor grammar A = (G, M, a, b, a) is as follows:",
        "1.",
        "For each A G N, draw 6a – Dir(aA).",
        "2.",
        "For A from A1 to AK, define G a as follows:",
        "(a) Draw 7Ta | aA,bA – GEM(aA,bA).",
        "i.",
        "Draw A – B1... Bn from Ra.",
        "iii.",
        "While Yield(zA^) has nonterminals:",
        "A.",
        "Choose an unexpanded nonterminal B from yield of ZA,i.",
        "B.",
        "If B g M, expand B according to GB(defined on previous iterations of step 2).",
        "C. If B g N \\ M, expand B with a rule from RB according to Mult(6B).",
        "(a) If S g M, draw zi | G<?",
        "- G<?.",
        "(b) If S / M, draw zi as in 2(b) (omitted for space).",
        "4.",
        "Set xi = Yield(zi) for i {1, .",
        ".",
        ".",
        ", n}.",
        "Here, there are four collections of hidden variables: the PCFG multinomials 6 = {6a | A g N}, the stick length proportions v = {va | A g M} where va = (va^, va^, ...), the adapted nonterminals' subtrees za = {za^ | A g M;i g {1,...}} and the derivations z1:n = z1,..., zn.",
        "The symbol z refers to the collection of {za | A g M}, and z1:nrefers to the derivations of the data x.",
        "Note that the distribution in 2(c) is defined with the GEM distribution, as mentioned earlier.",
        "It is a sample from the Pitman-Yor process (or the Dirich-let process), which is later used in 3(a) to sample trees for an adapted non-terminal."
      ]
    },
    {
      "heading": "3. Variational Inference",
      "text": [
        "Variational inference is a deterministic alternative to MCMC, which casts posterior inference as an optimization problem (Jordan et al., 1999; Wainwright and Jordan, 2008).",
        "The optimized function is a bound on the marginal likelihood of the observations, which is expressed in terms of a so-called \"variational distribution\" over the hidden variables.",
        "When the bound is tightened, that distribution is close to the posterior of interest.",
        "Variational methods tend to converge faster than MCMC, and can be more easily parallelized over multiple processors in a framework such as MapReduce (Dean and Ghe-mawat, 2004).",
        "The variational bound on the likelihood of the data is:",
        "Expectations are taken with respect to the variational distribution q(v, 6, z) and H(q) is its entropy.",
        "Before tightening the bound, we define the functional form of the variational distribution.",
        "We use the mean-field distribution in which all of the hidden variables are independent and governed by individual variational parameters.",
        "(Note that in the true posterior, the hidden variables are highly coupled.)",
        "To account for the infinite collection of random variables, for which we cannot define a variational distribution, we use the truncated stick distribution (Blei and Jordan, 2005).",
        "Hence, we assume that, for all A g M, there is some value Na such that q(vA^A = 1) = 1.",
        "The assigned probability to parse trees in the stick will be 0 for i > Na, so we can ignore za^ for i > Na.",
        "This leads to a factorized variational distribution:",
        "It is natural to define the variational distributions over 6 and v to be Dirichlet distributions with parameters ta and Beta distributions with parameters 7a^, respectively.",
        "The two distributions over trees, q(zA^) and q(zi), are more problematic.",
        "For example, with q(zi | 0), we need to take into account different subtrees that could be generated by the model and use them with the proper probabilities in the variational distribution q(zi | 0).",
        "We follow and extend the idea from Johnson et al.",
        "(2006) and use grammatons for these distributions.",
        "Gramma-tons are \"mini-grammars,\" inspired by the grammar",
        "G.",
        "For two strings in s,t G W*, we use \"t ç s\" to mean that t is a substring of s. In that case, a grammaton is defined as follows:",
        "Definition 1.",
        "Let A = (G, M, a, b, a) be an adaptor grammar with G = (W, N, R, S ).",
        "Let s be a finite string over the alphabet of G and A G N. Let U be the set of nonterminals U = Reachable(G, A) n (N \\ M).",
        "The grammaton G(A, s) is the context-free grammar with the start symbol A and the rules",
        "Using a grammaton, we define the distributions q(zA^ | 0a) and q(zi | 0).",
        "This requires a preprocessing step (described in detail in §3.3) that defines, for each A G M, a list of strings s a = (sa,1, ..., sa,na).",
        "Then, for q(zA,i | 0a) we use the grammaton G(A, SA,i) and for q(zi | 0) we use the grammaton G(A, xi) where xi is the ith observed sentence.",
        "We parametrize the grammaton with weights 0a (or 0) for each rule in the gramma-ton.",
        "This makes the variational distributions over the trees for strings s (and trees for x) globally normalized weighted grammars.",
        "Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al., 2008, and Cohen and Smith, 2009).",
        "In practice we do not have to use rewrite rules for all strings t ç s in the gram-maton.",
        "It suffices to add rewrite rules only for the strings t = sa^ that have some grammaton attached to them, G(A, sa^).",
        "The variational distribution above yields a variational inference algorithm for approximating the posterior by estimating 7a^, ta, 0a and 0 iteratively, given a fixed set of hyperparameters a, b and a.",
        "Let r be a PCFG rule.",
        "Let t | t ç s}.",
        "counts the number of times that rule r is applied in the derivation zk.",
        "Let A – ß denote a rule from G. The quantity f (r, sB^) is computed using the inside-outside (IO) algorithm.",
        "Fig.",
        "1 gives the variational inference updates.",
        "Variational EM We use variational EM to fit the hyperparameters.",
        "Variational EM is an EM algorithm where the E step is replaced by variational inference (Fig.",
        "1).",
        "The M-step optimizes the hyperparameters (a, b and a) with respect to expected sufficient statistics under the variational distribution.",
        "We use Newton-Raphson for each (Boyd and Vanden-berghe, 2004); Fig. 2 gives the objectives.",
        "With recursive grammars, the stick-breaking process representation gives probability mass to events which are ill-defined.",
        "In step 2(iii)(c) of the stick-breaking representation, we assign nonzero probability to an event in which we choose to expand the current tree using a subtree with the same index that we are currently still expanding (see footnote 2).",
        "In",
        "\\BeU J A->Bi",
        "short, with recursive grammars, we can get \"loops\" inside the trees.",
        "(iii) renormalizes:",
        "Our algorithm requires running the IO algorithm for each yield in the variational distribution, for each nonterminal, and for each sentence.",
        "However, IO runs with much smaller grammars coming from the grammatons.",
        "The cost of running the IO algorithm on the yields in the sticks for A G M can be taken into account parsing a string that appears in the corpus with the full grammars.",
        "This leads to an asymp-",
        "argmaxEq[logp'(x, z)] totic complexity of 0(|N|2N + |N|3|xi|2) forthe q ith sentence in the corpus each iteration.",
        "Asymptotically, both sampling and variational EM behave the same.",
        "However, there are different constants that hide in these asymptotic runtimes: the number of iterations that the algorithm takes to converge (for which variational EM generally has an advantage over sampling) and the number ofadditional rewrite rules that rewrite to a string representing a tree (for which MCMC has a relative advantage, because it does not use a fixed set of strings; instead, the size of the grammars it uses grow as sampling proceeds).",
        "In §4, we see that variational EM and sampling methods are similar in the time it takes to complete because of a trade-off between these two constants.",
        "Simple parallelization, however, which is possible only with variational inference, provides significant speed-ups.",
        "For the variational approximation from §3, we need to decide on a set of strings, sa,% (for A G M and i G {1,..., Na}) to define the grammatons in the",
        "We would still like to use recursion in the cases which are not ill-defined.",
        "In the case of recur sive grammars, there is no probreaking representation and thenumerate the nonterminals.",
        "Tstick-breaking process separateabilities for each index in the statoms for each index in the stic",
        "Our variational distributionsany event which is ill-definetioned above.",
        "Optimizing thethis case is equivalent to optimational bound with a model p' (ii) assigns probability 0 toil where p'(x, z) is a probabilityas p'(x, z) = p(x, z)/Eze§p0 otherwise.",
        "For this reason, our variatiolows the use of recursive gramcursive grammars with MCMCatic, since it has no corresponterpretation, enabled by zeroindefined in the variational distrunderlying model such as p', aalgorithm is invalid.",
        "The algorithm in Johnson et sampling from a PCFG containing rewrite rules that rewrite to a whole tree fragment.",
        "This requires a procedure that uses the inside-outside algorithm.",
        "Despite the grammar being bigger (because of the rewrite rules to a string), the asymptotic complexity nonparametric stick.",
        "Any set of strings will give a valid approximation, but to make the variational approximation as accurate as possible, we require that: (i) the strings in the set must be likely to be generated using the adaptor grammar as constituents headed by the relevant nonterminal, and (ii) strings that are more likely to be generated should be associated with a lower index in the stick.",
        "The reason for the second requirement is the exponential decay of coefficients as the index increases.",
        "We show that a simple heuristic leads to an order over the strings generated by the adaptor grammars that yields an accurate variational estimation.",
        "We begin with a weighted context-free grammar Gheurthat has the same rules as in G, only the weight for all of its rules is 1.",
        "We then compute the quantity:",
        "where fi(z; A, s) is a function computing the count of constituents headed by A with yield s in the tree z for the sentence xi.",
        "This quantity can be computed by using the IO algorithm on Gheur.",
        "The term p log |s| is subtracted to avoid preference for shorter constituents, similar to Mochihashi et al.",
        "(2009).",
        "While computing c(A, s) using the IO algorithm, we sort the set of all substrings of s according to their expected counts (aggregated over all strings s).",
        "Then, we use the top Na strings in the sorted list for the grammatons of A.",
        "The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons).",
        "We experiment with two commonly used decoding methods: Viterbi decoding",
        "not achieve better performance and it had an adverse effect on runtime.",
        "For completeness, we give these results in §4.",
        "and minimum Bayes risk decoding (MBR; Goodman, 1996).",
        "To parse a string with Viterbi (or MBR) decoding, we find the tree with highest score for the gramma-ton which is attached to that string.",
        "For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction.",
        "We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem.",
        "We use the standard Brent corpus (Brent and Cartwright, 1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus.",
        "Johnson and Goldwater (2009) test three grammars for this segmentation task.",
        "The first grammar is a character unigram grammar (Gxjnigram).",
        "The second grammar is a grammar that takes into consideration collocations (Gcoiioc) which includes the rules { Sentence – Colloc, Sentence – Colloc Sentence, Colloc – Word+, Word – Char+ }.",
        "The third grammar incorporates more prior knowledge about the syllabic structure of English (Gsyllable).",
        "Gunigram and GSyllable can be found in Johnson and Goldwater (2009).",
        "Once an utterance is parsed, Word constituents denote segments.",
        "The value of p (penalty term for string length) had little effect on our results and was fixed at p = -0.2.",
        "When Na (number of strings used in the variational distributions) is fixed, we use Na = 15,000.",
        "We report results using Viterbi and MBR decoding.",
        "Johnson and Goldwater (2009) experimented with two",
        "Table 1: F performance for word segmentation on the Brent corpus.",
        "Dir.",
        "stands for Dirichlet Process adaptor (b = 0), PY stands for Pitman-Yor adaptor (b optimized), and PY+inc.",
        "stands for Pitman-Yor with iteratively increasing NA for A e M (see footnote 5).",
        "J&G 2009 are the results adapted from Johnson and Goldwater (2009); SA is sample average decoding, and MM is maximum marginal decoding.",
        "Figure 3: F1 performance of GUnigram as influenced by the length of the stick, NWord.",
        "decoding methods, sample average (SA) and maximal marginal decoding (MM), which are closely related to Viterbi and MBR, respectively.",
        "With MM, we marginalize the tree structure, rather than the word segmentation induced, similar to MBR decoding.",
        "With SA, we compute the probability of a whole tree, by averaging its count in the samples, an approximation to finding the tree with highest probability, like Viterbi.",
        "Table 1 gives the results for our experiments.",
        "Notice that the results for the Pitman-Yor process and the Dirichlet process are similar.",
        "When inspecting the learned parameters, we noticed that the discount parameters (b) learned by the variational inference algorithm for the Pitman-Yor process are very close to 0.",
        "In this case, the Pitman-Yor process is reduced to the Dirichlet process.",
        "Similar to Johnson and Goldwater's comparisons, we see superior performance when using minimum Bayes risk over Viterbi decoding.",
        "Further notice that the variational inference algorithm obtains significantly superior performance for simpler grammars than Johnson et al., while performance using the syllable grammar is lower.",
        "The results also suggest that it is better to decide ahead on the set of strings available in the sticks, instead of working gradually and increase the size of the sticks as described in footnote 5.",
        "We believe that the reason is that the variational inference algorithm settles in a trajectory that uses fewer strings, then fails to exploit the strings that are added to the stick later.",
        "Given that selecting Na in advance is advantageous, we may inquire if choosing Na to be too large can lead to degraded performance, because of fragmention of the grammar.",
        "Fig.",
        "3 suggests it is not the case, and performance stays steady after Na reaches a certain value.",
        "One of the advantages of variational approximation over sampling methods is the ability to run for fewer iterations.",
        "For example, with GUnigram convergence typically takes 40 iterations with variational inference, while Johnson and Goldwater (2009) ran their sampler for 2,000 iterations, for which 1,000 were for burning in.",
        "The inside-outside algorithm dominates the iteration's runtime, both for sampling and variational EM.",
        "Each iteration with sampling, however, takes less time, despite the asymptotic analysis in §3.2, because of different implementations and the different number of rules that rewrite to a string.",
        "We now give a comparison of clock time for Gunigram for variational inference and sampling as described in Johnson and Goldwa-ter (2009).",
        "Replicating the experiment in Johnson and Goldwater (first row in Table 1) took 2 hours and 11 minutes.",
        "With the variational approximation, we had the following: (i) the preprocessing (§3.3) step took 114 seconds; (ii) each iteration took approximately 204 seconds, with convergence after 40 iterations, leading to 8,160 seconds of pure variational EM processing; (iii) parsing took another 952 seconds.",
        "The total time is 2 hours and 34 minutes.",
        "grammar",
        "model",
        "Vit.",
        "MBR",
        "SA MM",
        "Dir PY PY+inc",
        "0.49 0.84 0.49 0.84 0.42 0.59",
        "0.57 0.54 0.81 0.75",
        "Dir PY PY+inc",
        "0.40 0.86 0.40 0.86 0.43 0.60",
        "0.75 0.72 0.83 0.86",
        "Dir PY PY+inc",
        "0.77 0.83 0.77 0.83 0.75 0.76",
        "0.84 0.84 0.89 0.88",
        "__^",
        " – -",
        " – – ",
        "/",
        "/",
        "/",
        "At first glance it seems that variational inference is slower than MCMC sampling.",
        "However, note that the cost of the grammar preprocessing step is amortized over all experiments with the specific grammar, and the E-step with variational inference can be parallelized, while sampling requires an update of a global set of parameters after each tree update.",
        "We ran our algorithm on a cluster of 20 1.86GHz CPUs and achieved a significant speed-up: preprocessing took 34 seconds, each variational EM iteration took 43 seconds and parsing took 208 seconds.",
        "The total time was 47 minutes, which is 2.8 times faster than sampling.",
        "We conclude our experiments with preliminary results for unsupervised syntax learning.",
        "This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009).",
        "The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, Gdmv (Smith, 2006).",
        "We note that GDMV is recursive; this is not a problem (§3.1).",
        "We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation.",
        "We follow standard parsing conventions and train on sections 221 and test on section 23 (while using sentences of length 10 or less).",
        "Because of the unsupervised nature of the problem, we report results on the training set, in addition to the test set.",
        "The nonterminals that we adapted correspond to nonterminals that define noun constituents.",
        "We then use the preprocessing step defined in §3.3 with a uniform grammar and take the top 3,000 strings for each nonterminal of a noun constituent.",
        "The results are in Table 4.2.",
        "We report attachment accuracy, the fraction of parent-child relationships that the algorithm classified correctly.",
        "Notice that the results are not very different for Viterbi and MBR decoding, unlike the case with word segmentation.",
        "It seems like the DMV grammar, applied to this task, is more robust to changes in decod-",
        "Table 2: Attachment accuracy for different models for dependency grammar induction.",
        "Bold marks best overall accuracy per evaluation set, and t marks figures that are not significantly worse (binomial sign test, p < 0.05).",
        "ing mechanism.",
        "Adaptor grammars improve performance over classic EM and variational EM with a Dirichlet prior significantly.",
        "We note that adaptor grammars are not limited to a selection of a Dirichlet distribution as a prior for the grammar rules.",
        "Our variational inference algorithm, for example, can be extended to use the logistic normal prior instead of the Dirichlet, shown successful by Cohen and Smith (2009)."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We described a variational inference algorithm for adaptor grammars based on a stick-breaking process representation, which solves a problem with adaptor grammars and recursive PCFGs.",
        "We tested it for a segmentation task, and showed results which are either comparable or an imporvement of state of the art.",
        "We showed that significant speed-ups can be obtained using parallelization of the algorithm.",
        "We also tested the algorithm on a novel task for adaptor grammars, dependency grammar induction.",
        "We showed that an improvement can be obtained using adaptor grammars over non-Bayesian and parametric baselines."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors would like to thank the anonymous reviewers, Jordan Boyd-Graber, Reza Haffari, Mark Johnson, and Chong Wang for their useful feedback and comments.",
        "This work was supported by the following grants: ONR 175-6343 andNSF CAREER 0745520 to Blei; NSF IIS-0836431 and IIS-0915187 to Smith.",
        "non-Bayesian",
        "48.2",
        "48.3",
        "n",
        "ai tr",
        "Dirichlet prior",
        "48.3",
        "48.6",
        "Adaptor grammar",
        "54.0",
        "+53.7",
        "non-Bayesian",
        "45.8",
        "46.1",
        "st",
        "te",
        "Dirichlet prior",
        "45.9",
        "46.1",
        "Adaptor grammar",
        "48.3",
        "50.2"
      ]
    }
  ]
}
