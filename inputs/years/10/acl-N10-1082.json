{
  "info": {
    "authors": [
      "Percy Liang",
      "Michael I. Jordan",
      "Dan Klein"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1082",
    "title": "Type-Based MCMC",
    "url": "https://aclweb.org/anthology/N10-1082",
    "year": 2010
  },
  "references": [
    "acl-D08-1036",
    "acl-J92-4003",
    "acl-N06-1041",
    "acl-N09-1026",
    "acl-N09-1062",
    "acl-P06-1055",
    "acl-P06-1085",
    "acl-P07-1094",
    "acl-P09-2012"
  ],
  "sections": [
    {
      "text": [
        "UC Berkeley",
        "Most existing algorithms for learning latent-variable models – such as EM and existing Gibbs samplers – are token-based, meaning that they update the variables associated with one sentence at a time.",
        "The incremental nature of these methods makes them susceptible to local optima/slow mixing.",
        "In this paper, we introduce a type-based sampler, which updates a block of variables, identified by a type, which spans multiple sentences.",
        "We show improvements on part-of-speech induction, word segmentation, and learning tree-substitution grammars."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "A long-standing challenge in NLP is the unsupervised induction of linguistic structures, for example, grammars from raw sentences or lexicons from phoneme sequences.",
        "A fundamental property of these unsupervised learning problems is multi-modality.",
        "In grammar induction, for example, we could analyze subject-verb-object sequences as either ((subject verb) object) (mode 1) or (subject (verb object)) (mode 2).",
        "Multimodality causes problems for token-based procedures that update variables for one example at a time.",
        "In EM, for example, if the parameters already assign high probability to the ((subject verb) object) analysis, re-analyzing the sentences in E-step only reinforces the analysis, resulting in EM getting stuck in a local optimum.",
        "In (collapsed) Gibbs sampling, if all sentences are already analyzed as ((subject verb) object), sampling a sentence conditioned",
        "Figure 1: Consider a dataset of 3 sentences, each of length 5.",
        "Each variable is labeled with a type (1 or 2).",
        "The unshaded variables are the ones that are updated jointly by a sampler.",
        "The token-based sampler updates the variable for one token at a time (a).",
        "The sentence-based sampler updates all variables in a sentence, thus dealing with intra-sentential dependencies (b).",
        "The type-based sampler updates all variables of a particular type (1 in this example), thus dealing with dependencies due to common parameters (c).",
        "on all others will most likely not change its analysis, resulting in slow mixing.",
        "To combat the problems associated with token-based algorithms, we propose a new sampling algorithm that operates on types.",
        "Our sampler would, for example, be able to change all occurrences of ((subject verb) object) to (subject (verb object)) in one step.",
        "These type-based operations are reminiscent of the type-based grammar operations of early chunk-merge systems (Wolff, 1988; Stolcke and Omohun-dro, 1994), but we work within a sampling framework for increased robustness.",
        "In NLP, perhaps the the most simple and popular sampler is the token-based Gibbs sampler} used in Goldwater et al.",
        "(2006), Goldwater and Griffiths (2007), and many others.",
        "By sampling only one variable at a time, this sampler is prone to slow mixing due to the strong coupling between variables.",
        "A general remedy is to sample blocks of coupled variables.",
        "For example, the sentence-based sampler samples all the variables associated with a sentence at once (e.g., the entire tag sequence).",
        "However, this blocking does not deal with the strong type-based coupling (e.g., all instances of a word should be tagged similarly).",
        "The type-based sampler we will present is designed exactly to tackle this coupling, which we argue is stronger and more important to deal with in unsupervised learning.",
        "Figure 1 depicts the updates made by each of the three samplers.",
        "We tested our sampler on three models: a Bayesian HMM for part-of-speech induction (Gold-water and Griffiths, 2007), a nonparametric Bayesian model for word segmentation (Goldwater et al., 2006), and a nonparametric Bayesian model of tree substitution grammars (Cohn et al., 2009; Post and Gildea, 2009).",
        "Empirically, we find that type-based sampling improves performance and is less sensitive to initialization (Section 5)."
      ]
    },
    {
      "heading": "2. Basic Idea via a Motivating Example",
      "text": [
        "The key technical problem we solve in this paper is finding a block of variables which are both highly coupled and yet tractable to sample jointly.",
        "This section illustrates the main idea behind type-based sampling on a small word segmentation example.",
        "Suppose our dataset x consists of n occurrences of the sequence a b.",
        "Our goal is infer z = (z1,... ,zn), where zj = 0 if the sequence is one word ab, and zj = 1 if the sequence is two, a and b.",
        "We can model this situation with a simple generative model: for each i = 1,..., n, generate one or two words with equal probability.",
        "Each word is drawn independently based on probabilities 0 = (0a, 0b, 0ab) which we endow with a uniform prior 0 ~ Dirichlet(1,1,1).",
        "We marginalize out 0 to get the following standard expression (Goldwater et al., 2009):",
        "iteration",
        "Figure 2: (a) The posterior (1) is sharply bimodal (note the log-scale).",
        "(b) A run of the token-based and type-based samplers.",
        "We initialize both samplers with m = n (n = 1000).",
        "The type-based sampler mixes instantly (in fact, it makes independent draws from the posterior) whereas the token-based sampler requires five passes through the data before finding the high probability region m ~ 0.",
        "ascending factorial?",
        "Figure 2(a) depicts the resulting bimodal posterior.",
        "A token-based sampler chooses one zj to update according to the posterior p(zj | z_j, x).",
        "To illustrate the mixing problem, consider the case where m = n, i.e., all sequences are analyzed as two words.",
        "From (1), we can verify that p(zj = 0 | z_j, x) = O(n).",
        "When n = 1000, this means that there is only a 0.002 probability of setting zj = 0, a very unlikely but necessary first step to take to escape this local optimum.",
        "Indeed, Figure 2(b) shows how the token-based sampler requires five passes over the data to finally escape.",
        "Type-based sampling completely eradicates the local optimum problem in this example.",
        "Let us take a closer look at (1).",
        "Note that p(z | x) only depends on a single integer m, which only takes one of n + 1 values, not on the particular z.",
        "This shows that the zjs are exchangeable.",
        "There are (n) possible values of z satisfying m = j zj, each with the same probability g(m).",
        "Summing, we get:",
        "z:ni=J2i Zi",
        "A sampling strategy falls out naturally: First, sample the number m via (2).",
        "Conditioned on m, choose the particular z uniformly out of the nn possibilities.",
        "Figure 2(b) shows the effectiveness of this type-based sampler.",
        "This simple example exposes the fundamental challenge of multimodality in unsupervised learning.",
        "Both m = 0 and m = n are modes due to the rich-gets-richer property which arises by virtue of all n examples sharing the same parameters 6.",
        "This sharing is a double-edged sword: It provides us with clustering structure but also makes inference hard.",
        "Even though m = n is much worse (by a factor exponential in n) than m = 0, a naive algorithm can easily have trouble escaping m = n."
      ]
    },
    {
      "heading": "3. Setup",
      "text": [
        "We will now present the type-based sampler in full generality.",
        "Our sampler is applicable to any model which is built out of local multinomial choices, where each multinomial has a Dirichlet process prior (a Dirichlet prior if the number of choices is finite).",
        "This includes most probabilistic models in NLP (excluding ones built from log-linear features).",
        "As we develop the sampler, we will provide concrete examples for the Bayesian hidden Markov model (HMM), the Dirichlet process unigram segmentation model (USM) (Goldwater et al., 2006), and the probabilistic tree-substitution grammar (PTSG) (Cohn et al., 2009; Post and Gildea, 2009).",
        "A model is specified by a collection of multinomial parameters 6 = {0r }r^R, where R is an index set.",
        "Each vector 0r specifies a distribution over outcomes: outcome o has probability 0ro.",
        "• HMM: Let K is the number of states.",
        "The set R = {(q,k) : q G {T,E},k = 1,...,K} indexes the K transition distributions {0(Tk)} (each over outcomes {1, .",
        ".",
        ".",
        ", K}) and K emission distributions {0(Ek)} (each over the set of words).",
        "• USM: R = {0}, and do is a distribution over (an infinite number of) words.",
        "• PTSG: R is the set of grammar symbols, and each 0r is a distribution over labeled tree fragments with root label r.",
        "Table 1: Notation used in this paper.",
        "Note that there is a one-to-one mapping between z and (b, x).",
        "The information relevant for evaluating the likelihood is n. We use the following parallel notation: n-s = n(z-s), ns:b = n(zs:b), Ans = n(Azs).",
        "We represent the latent structure z as a set of local choices:",
        "• HMM: z contains elements of the form ( T, i, a, b) , denoting a transition from state a at position i to state b at position i + 1; and (E, i,a, w), denoting an emission of word w from state a at position i.",
        "• USM: z contains elements of the form (i, w), denoting the generation of word w at character position i extending to position i + | w| – 1.",
        "• PTSG: z contains elements of the form (x, t), denoting the generation of tree fragment t rooted at node x.",
        "The choices z are connected to the parameters 6 as follows: p(z | 6) = Yizez 0Z.r,Z.o.",
        "Each choice z G z is identified with some z.r G R and outcome z.o.",
        "Intuitively, choice z was made by drawing drawing z.o from the multinomial distribution 0Z.r.",
        "We place a Dirichlet process prior on 0r (Dirichlet prior for finite outcome spaces): 0r ~ DP(ar, ßr), where ar is a concentration parameter and ßr is a fixed base distribution.",
        "R",
        "index set for parameters",
        "6 =",
        "{0r }reR",
        "multinomial parameters",
        "^ =",
        "{Mr }reR",
        "base distributions (fixed)",
        "S",
        "set of sites",
        "b =",
        "{bs}seS",
        "binary variables (to be sampled)",
        "z",
        "latent structure (set of choices)",
        "z_s",
        "choices not depending on site s",
        "zs:b",
        "choices after setting bs = b",
        "Azs:b",
        "zs:b\\z_s: new choices from bs = b",
        "S c S",
        "sites selected for sampling",
        "m",
        "# sites in S assigned bs = 1",
        "n =",
        "{nro}",
        "counts (sufficient statistics of z)",
        "Let nro(z) = |{z G z : z.r = r, z.o = o}| be the number of draws from 0r resulting in outcome o, and nr.",
        "= Yo nro be the number of times 0r was drawn from.",
        "Let n(z) = {nro(z)} denote the vector of sufficient statistics associated with choices z.",
        "When it is clear from context, we simply write n for n(z).",
        "Using these sufficient statistics, we can write p(z |",
        "6)= llr.o 0noro(Z).",
        "We now marginalize out 6 using Dirichlet-multinomial conjugacy, producing the following expression for the likelihood:",
        "where a(k) = a(a+1) • • • (a+k – 1) is the ascending factorial.",
        "(3) is the distribution that we will use for sampling."
      ]
    },
    {
      "heading": "4. Type-Based Sampling",
      "text": [
        "Having described the setup of the model, we now turn to posterior inference of p(z | x).",
        "We first define a new representation of the latent structure based on binary variables b so that there is a bijection between z and (b, x); z was used to define the model, b will be used for inference.",
        "We will use b to exploit the ideas from Section 2.",
        "Specifically, let b = {bs}seS be a collection of binary variables indexed by a set of sites S.",
        "• HMM: If the HMM has K = 2 states, S is the set of positions in the sequence.",
        "For each s G S, bs is the hidden state at s. The extension to general K is considered at the end of Section 4.4.",
        "• USM: S is the set of non-final positions in the sequence.",
        "For each s G S, bs denotes whether a word boundary exists between positions s and s + 1.",
        "• PTSG: S is the set of internal nodes in the parse tree.",
        "For s G S, bs denotes whether a tree fragment is rooted at node s.",
        "For each site s G S, let zs:o and zs:1 denote the choices associated with the structures obtained by setting the binary variable bs = 0 and bs = 1, respectively.",
        "Define z_s =f zs:0 n zs:1 to be the set of choices that do not depend on the value of bs, and n_s =f n(z_s) be the corresponding counts.",
        "• HMM: z_s includes all but the transitions into and out of the state at s plus the emission at s.",
        "• USM: z_s includes all except the word ending at s and the one starting at s + 1 if there is a boundary (bs = 1 ); except the word covering s if no boundary exists (bs = 0).",
        "• PTSG: z_s includes all except the tree fragment rooted at node s and the one with leaf s if bs = 1; except the single fragment containing s ifbs = 0.",
        "A token-based sampler considers one site s at a time.",
        "Specifically, we evaluate the likelihoods of zs:0 and zs:1 according to (3) and sample bs with probability proportional to the likelihoods.",
        "Intuitively, this can be accomplished by removing choices that depend on bs (resulting in z_s), evaluating the likelihood resulting from setting bs to 0 or 1, and then adding the appropriate choices back in.",
        "More formally, let Azs:b =f zs:b\\z_s be the new choices that would be added if we set bs = b G {0,1}, and let Ans:b d=f n(Azs:b) be the corresponding counts.",
        "With this notation, we can write the posterior as follows:",
        "The form of the conditional (4) follows from the joint (3) via two properties: additivity of counts (ns:b = n_s + Ans:b) and a simple property of ascending factorials (a(fc+<5) = a(k)(a + k)((5)).",
        "In practice, most of the entries of Ans:b are zero.",
        "For the HMM, nrs:ob would be nonzero only for the transitions into the new state (b) at position s (zs_1 – b), transitions out of that state (b – zs+1 ), and emissions from that state (b – xs).",
        "We would like to sample multiple sites jointly as in Section 2, but we cannot choose any arbitrary subset S c S, as the likelihood will in general depend on the exact assignment of bS = {bs}seS, of which",
        "Figure 3: The type-based sampler jointly samples all variables at a set of sites S (in green boxes).",
        "Sites in S are chosen based on types (denoted in red).",
        "(a) HMM: two sites have the same type if they have the same previous and next states and emit the same word; they conflict unless separated by at least one position.",
        "(b) USM: two sites have the same type if they are both of the form ab|c or abc; note that occurrences of the same letters with other segmentations do not match the type.",
        "(c) PTSG: analogous to the USM, only for tree rather than sequences.",
        "there are an exponential number.",
        "To exploit the exchangeability property in Section 2, we need to find sites which look \"the same\" from the model's point of view, that is, the likelihood only depends on bS via m = £ ses bs.",
        "To do this, we need to define two notions, type and conflict.",
        "We say sites s and s' have the same type if the counts added by setting either bs or bs/ are the same, that is, Ans:b = Ans/:b for b G {0,1}.",
        "This motivates the following definition of the type of site s with respect to z:",
        "We say that s and s' have the same type if t(z, s) = t(z, s').",
        "Note that the actual choices added (Azs:band Azs/:b) are in general different as s and s' correspond to different parts of the latent structure, but the model only depends on counts and is indifferent to this.",
        "Figure 3 shows examples of same-type sites for our three models.",
        "However, even if all sites in S have the same type, we still cannot sample bs jointly, since changing one bs might change the type of another site s'; indeed, this dependence is reflected in (5), which shows that types depend on z.",
        "For example, s, s' G S conflict when s' = s + 1 in the HMM or when s and s' are boundaries of one segment (USM) or one tree fragment (PTSG).",
        "Therefore, one additional concept is necessary: We say two sites s and s' conflict if there is some choice that depends on both bsand bs/ ; formally, (z\\z_s) n (z\\z_s/) = 0.",
        "Our key mathematical result is as follows:",
        "Proposition 1 For any set S c S of non-conflicting sites with the same type, for some easily computable g(m), where m =",
        "We will derive g(m) shortly, but first note from (6) that the likelihood for a particular setting of bsdepends on bs only via m as desired.",
        "(7) sums over all (m') settings of bs with m = Yses bs.",
        "The algorithmic consequences of this result is that to sample bs, we can first compute (7) for each m G {0,..., |S|}, sample m according to the normalized distribution, and then choose the actual bsuniformly subject to m.",
        "Let us now derive g(m) by generalizing (4).",
        "Imagine removing all sites S and their dependent choices and adding in choices corresponding to some assignment bs.",
        "Since all sites in S are non-conflicting and of the same type, the count contribution Ans:b is the same for every s G S (i.e., sites in S are exchangeable).",
        "Therefore, the likelihood of the new assignment bs depends only on the new counts:",
        "Using these new counts in place of the ones in (4), we get the following expression:",
        "Thus far, we have shown how to sample bs given a set S c S of non-conflicting sites with the same type.",
        "To complete the description of the type-based",
        "Type-Based Sampler",
        "for each iteration t = 1,..., T : for each pivot site so G S : S – TB(z, s0) (S is the type block centered at s0) decrement n and remove from z based on bSsample m according to (7) sample M c S with |M | = m uniformly at random set bs = I[s G M] for each s G S increment n and add to z accordingly",
        "Figure 4: Pseudocode for the general type-based sampler.",
        "We operate in the binary variable representation b of z.",
        "Each step, we jointly sample |S| variables (of the same type).",
        "sampler, we need to specify how to choose S. Our general strategy is to first choose a pivot site s0 G S uniformly at random and then set S = TB(z, s0) for some function TB.",
        "Call S the type block centered at s0.",
        "The following two criteria on TB are sufficient for a valid sampler: (A) s0 G S, and (B) the type blocks are stable, which means that if we change bsto any b's (resulting in a new z'), the type block centered at s0 with respect to z' does not change (that is, TB(z',s0) = S).",
        "(A) ensures ergodicity; (B), reversibility.",
        "Now we define TB as follows: First set S = {s0}.",
        "Next, loop through all sites s G S with the same type as s0 in some fixed order, adding s to S if it does not conflict with any sites already in S. Figure 4 provides the pseudocode for the full algorithm.",
        "Formally, this sampler cycles over |S| transition kernels, one for each pivot site.",
        "Each kernel (indexed by s0 G S) defines a blocked Gibbs move, i.e. sampling fromp(bTB(z,s0) | • • • ).",
        "Efficient Implementation There are two operations we must perform efficiently: (A) looping through sites with the same type as the pivot site s0, and (B) checking whether such a site s conflicts with any site in S. We can perform (B) in O(1) time by checking if any element of Azs:bs has already been removed; if so, there is a conflict and we skip s. To do (A) efficiently, we maintain a hash table mapping type t to a doubly-linked list of sites with type t. There is an O(1) cost for maintaining this data structure: When we add or remove a site s, we just need to add or remove neighboring sites s' from their respective linked lists, since their types depend on bs.",
        "For example, in the HMM, when we remove site s, we also remove sites s 1 and s+1.",
        "For the USM, we use a simpler solution: maintain a hash table mapping each word w to a list of positions where w occurs.",
        "Suppose site (position) s straddles words a and b.",
        "Then, to perform (A), we retrieve the list of positions where a, b, and ab occur, intersecting the a and b lists to obtain a list of positions where a b occurs.",
        "While this intersection is often much smaller than the pre-intersected lists, we found in practice that the smaller amount of bookkeeping balanced out the extra time spent intersecting.",
        "We used a similar strategy for the PTSG, which significantly reduces the amount of bookkeeping.",
        "Skip Approximation Large type blocks mean larger moves.",
        "However, such a block S is also sampled more frequently – once for every choice of a pivot site s0 G S. However, we found that empirically, bs changes very infrequently.",
        "To eliminate this apparent waste, we use the following approximation of our sampler: do not consider s0 G S as a pivot site if s0 belongs to some block which was already sampled in the current iteration.",
        "This way, each site is considered roughly once per iteration.",
        "Sampling Non-Binary Representations We can sample in models without a natural binary representation (e.g., HMMs with with more than two states) by considering random binary slices.",
        "Specifically, suppose bs G {1,..., K} for each site s G S. We modify Figure 4 as follows: After choosing a pivot site s0 G S, let k = bs0 and choose k' uniformly from {1,..., K}.",
        "Only include sites in one of these two states by redefining the type block to be S = {s G TB(z,s0) : bs G {k,k'}}, and sample bs restricted to these two states by drawing from p(bs | bs G {k, k'}'s', • • • ).",
        "By choosing a random k' each time, we allow b to reach any point in the space, thus achieving ergodicity just by using these binary restrictions."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "We now compare our proposed type-based sampler to various alternatives, evaluating on marginal likelihood (3) and accuracy for our three models:",
        "• HMM: We learned a K = 45 state HMM on the Wall Street Journal (WSJ) portion of the Penn Treebank (49208 sentences, 45 tags) for part-of-speech induction.",
        "We fixed ar to 0.1 and /xr to uniform for all r.",
        "For accuracy, we used the standard metric based on greedy mapping, where each state is mapped to the POS tag that maximizes the number ofcor-rect matches (Haghighi and Klein, 2006).",
        "We did not use a tagging dictionary.",
        "• USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al.",
        "(2006) (9790 sentences) for word segmentation.",
        "We fixed a0 to 0.1.",
        "The base distribution /x0 penalizes the length of words (see Goldwater et al.",
        "(2009) for details).",
        "For accuracy, we used word token F1 .",
        "• PTSG: We learned a PTSG model on sections 2 – 21 of the WSJ treebank.",
        "For accuracy, we used EVALB parsing F1 on section 22.",
        "Note this is a supervised task with latent-variables, whereas the other two are purely unsupervised.",
        "Figure 5(a)-(c) compares the likelihood and accuracy (we use the term accuracy loosely to also include F1 ).",
        "The initial observation is that the type-based sampler (Type) outperforms the token-based sampler (Token) across all three models on both metrics.",
        "We further evaluated the PTSG on parsing.",
        "Our standard treebank PCFG estimated using maximum likelihood obtained 79% F1.",
        "Token obtained an F1of 82.2%, and Type obtained a comparable F1 of 83.2%.",
        "Running the PTSG for longer continued to",
        "of DeNero et al.",
        "(2009) to parse.",
        "improve the likelihood but actually hurt parsing accuracy, suggesting that the PTSG model is overfitting.",
        "To better understand the gains from Type over Token, we consider three other alternative samplers.",
        "First, annealing (Tokenanneal) is a commonly-used technique to improve mixing, where (3) is raised to some inverse temperature.In Figure 5(a) – (c), we see that unlike Type, Tokenanneal does not improve over Token uniformly: it hurts for the HMM, improves slightly for the USM, and makes no difference for the PTSG.",
        "Although annealing does increase mobility of the sampler, this mobility is undirected, whereas type-based sampling increases mobility in purely model-driven directions.",
        "Unlike past work that operated on types (Wolff, 1988; Brown et al., 1992; Stolcke and Omohun-dro, 1994), type-based sampling makes stochastic choices, and moreover, these choices are reversible.",
        "Is this stochasticity important?",
        "To answer this, we consider a variant of Type, Typegreedy: instead of sampling from (7), Typegreedy considers a type block S and sets bs to 0 for all s g S if p(bs = (0,...,0) | ••• ) >p(bs = (1,..., 1) | ••• );else it sets bs to 1 for all s g S. From Figure 5(a) – (c), we see that greediness is disastrous for the HMM, hurts a little for USM, and makes no difference on the PTSG.",
        "These results show that stochasticity can indeed be important.",
        "We consider another block sampler, Sentence, which uses dynamic programming to sample all variables in a sentence (using Metropolis-Hastings to correct for intra-sentential type-level coupling).",
        "For USM, we see that Sentence performs worse than Type and is comparable to Token, suggesting that type-based dependencies are stronger and more important to deal with than intra-sentential dependencies.",
        "We initialized all samplers as follows: For the USM and PTSG, for each site s, we place a boundary (set bs = 1) with probability r?.",
        "For the HMM, we set bsto state 1 with probability r?",
        "and a random state with",
        "Figure 5: (a)-(c): Log-likelihood and accuracy over time.",
        "Type performs the best.",
        "Relative to Type, Typegreedytends to hurt performance.",
        "Token generally works worse.",
        "Relative to Token, Tokenanneal produces mixed results.",
        "Sentence behaves like Token.",
        "(d)-(f): Effect of initialization.",
        "The metrics were applied to the current sample after 15 hours for the HMM and PTSG and 10 minutes for the USM.",
        "Type generally prefers larger r?",
        "and outperform the other samplers.",
        "probability 1 – r?.",
        "Results in Figure 5(a) – (c) were obtained by setting r to maximize likelihood.",
        "Since samplers tend to be sensitive to initialization, it is important to explore the effect of initialization (parametrized by r?",
        "G [0,1]).",
        "Figure 5(d) – (f) shows that Type is consistently the best, whereas other samplers can underperform TYPE by a large margin.",
        "Note that TYPE favors r?",
        "= 1 in general.",
        "This setting maximizes the number of initial types, and thus creates larger type blocks and thus enables larger moves.",
        "Larger type blocks also mean more dependencies that Token is unable to deal with."
      ]
    },
    {
      "heading": "6. Related Work and Discussion",
      "text": [
        "Block sampling, on which our work is built, is a classical idea, but is used restrictively since sampling large blocks is computationally expensive.",
        "Past work for clustering models maintained tractability by using Metropolis-Hastings proposals (Dahl, 2003) or introducing auxiliary variables (Swendsen and Wang, 1987; Liang et al., 2007).",
        "In contrast, our type-based sampler simply identifies tractable blocks based on exchangeability.",
        "Other methods for learning latent-variable models include EM, variational approximations, and uncol-lapsed samplers.",
        "All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction).",
        "However, these methods are at the core all token-based, since they only update variables in a single example at a time.",
        "Blocking variables by type – the key idea of this paper – is a fundamental departure from token-based methods.",
        "Though type-based changes have also been proposed (Brown et al., 1992; Stolcke and Omohundro, 1994), these methods operated greedily, and in Section 5.1, we saw that being greedy led to more brittle results.",
        "By working in a sampling framework, we were able bring type-based changes to fruition.",
        "t _ ---",
        "1",
        "... ,.",
        "-",
        "I – Token",
        "*- tokenanneal",
        "-1",
        "K- typEgreedy",
        "3- Type",
        "-",
        "l – Sentence"
      ]
    }
  ]
}
