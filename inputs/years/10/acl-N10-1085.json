{
  "info": {
    "authors": [
      "Aaron Dunlop",
      "Margaret Mitchell",
      "Brian Roark"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1085",
    "title": "Prenominal Modifier Ordering via Multiple Sequence Alignment",
    "url": "https://aclweb.org/anthology/N10-1085",
    "year": 2010
  },
  "references": [
    "acl-N03-1003",
    "acl-P00-1012",
    "acl-P99-1018",
    "acl-W02-1001",
    "acl-W02-1022",
    "acl-W05-1009",
    "acl-W09-0608"
  ],
  "sections": [
    {
      "text": [
        "Aaron Dunlop Margaret Mitchell Brian Roark",
        "Oregon Health & Science University University of Aberdeen Oregon Health & Science University",
        "Portland, OR Aberdeen, Scotland, U.K. Portland, OR",
        "Producing a fluent ordering for a set of prenominal modifiers in a noun phrase (NP) is a problematic task for natural language generation and machine translation systems.",
        "We present a novel approach to this issue, adapting multiple sequence alignment techniques used in computational biology to the alignment of modifiers.",
        "We describe two training techniques to create such alignments based on raw text, and demonstrate ordering accuracies superior to earlier reported approaches."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Natural language generation and machine translation systems must produce text which not only conforms to a reasonable grammatical model, but which also sounds smooth and natural to a human consumer.",
        "Ordering prenominal modifiers in noun phrases is particularly difficult in these applications, as the rules underlying these orderings are subtle and not well understood.",
        "For example, the phrase \"big red ball\" seems natural, while \"red big ball\" seems more marked, suitable only in specific contexts.",
        "There is some consensus that the order of prenom-inal modifiers in noun phrases is governed in part by semantic constraints, but there is no agreement on the exact constraints necessary to specify consistent orderings for any given set of modifiers.",
        "General principles of modifier ordering based on semantic constraints also fall short on larger domains, where it is not always clear how to map prenominal modifiers to proposed semantic groups.",
        "With the recent advantages of large corpora and powerful computational resources, work on automatically ordering prenominal modifiers has moved away from approaches based on general principles, and towards learning ordering preferences empirically from existing corpora.",
        "Such approaches have several advantages: (1) The predicted orderings are based on prior evidence from 'real-world' texts, ensuring that they are therefore reasonably natural.",
        "(2) Many (if not all) prenominal modifiers can be ordered.",
        "(3) Expanding the training data with more and larger corpora often improves the system without requiring significant manual labor.",
        "In this paper, we introduce a novel approach to prenominal modifier ordering adapted from multiple sequence alignment (MSA) techniques used in computational biology.",
        "MSA is generally applied to DNA, RNA, and protein sequences, aligning three or more biological sequences in order to determine, for example, common ancestry (Durbin et al., 1999; Gusfield, 1997; Carrillo and Lipman, 1988).",
        "MSA techniques have not been widely applied in NLP, but have produced some promising results for building a generation mapping dictionary (Barzilay and Lee, 2002), paraphrasing (Barzilay and Lee, 2003), and phone recognition (White et al., 2006).",
        "We believe that multiple sequence alignment is well-suited for aligning linguistic sequences, and that these alignments can be used to predict prenominal modifier ordering for any given set of modifiers.",
        "Our technique utilizes simple features within the raw text, and does not require any semantic information.",
        "We achieve good performance using this approach, with results competitive with earlier work (Shaw and Hatzivashigher recall and F-measure than that reported in Mitchell (2009) when tested on the same corpus."
      ]
    },
    {
      "heading": "2. Related work",
      "text": [
        "In one of the first attempts at automatically ordering prenominal modifiers, Shaw and Hatzi-vassiloglou (1999) present three empirical methods to order a variety of prenominal modifier types.",
        "Their approach provides ordering decisions for adjectives, gerunds (such as \"running\" in \"running man\"), and past participles (such as \"heated\" in \"heated debate\"), as well as for modifying nouns (such as \"baseball\" in \"baseball field\").",
        "A morphology module transforms plural nouns and comparative/superlative forms into their base forms, increasing the frequency counts for each modifier.",
        "We will briefly recap their three methods, which are categorized as the direct evidence method, the transitivity method, and the clustering method.",
        "Given prenominal modifiers a and b in a training corpus, the direct evidence method compares frequency counts of the ordered sequences <a,b> and <b,a>.",
        "This approach works well, but is limited by data sparsity; groups of two or more modifiers before a noun are relatively infrequent in traditional corpora, and finding the same pair of modifiers together more than once is particularly rare.",
        "To overcome this issue, Shaw and Hatzi-vassiloglou's transitivity and clustering methods make inferences about unseen orderings among prenominal modifiers.",
        "In the transitivity method, given three modifiers a,b,c, where a precedes b and b precedes c, the model concludes that a precedes c. The clustering method calculates a similarity score between modifiers based on where the modifiers occur in relation to the other modifiers in the corpus.",
        "Those modifiers that are most similar are clustered together, and ordering decisions can be made between modifiers in separate clusters.",
        "All three approaches are designed to order pairs of modifiers; it is unclear how to extend these approaches to order groups larger than a pair.",
        "Shaw and Hatzivassiloglou find that NPs with only adjectives as modifiers (including gerunds and past participles) are considerably easier to order than those which contain both adjectives and nouns.",
        "They also find large differences in accuracy across domains; their systems achieve much lower overall accuracy on financial text (the Wall Street Journal (WSJ) corpus (Marcus et al., 1999)) than on medical discharge summaries.",
        "Looking at all modifier pairs, the authors achieve their highest prediction accuracy of 90.7% using the transitivity technique on a medical corpus.",
        "We do not have access to this corpus, but we do have access to the WSJ corpus, which provides a way to compare our methods.",
        "On this corpus, their model produces predictions for 62.5% of all modifier pairs and achieves 83.6% accuracy when it is able to make a prediction.",
        "Random guessing on the remainder yields an overall accuracy of 71.0%.",
        "Malouf (2000) also examines the problem of prenominal modifier ordering.",
        "He too proposes several statistical techniques, achieving results ranging from 78.3% to 91.9% accuracy.",
        "He achieves his best results by combining memory-based learning and positional probability to modifiers from the first 100 million tokens of the BNC.",
        "However, this evaluation is limited to the ordering of prenominal adjectives, which is a considerably simpler task than ordering all types of prenominal modifiers.",
        "Malouf's approaches are also limited to ordering pairs of modifiers.",
        "Mitchell (2009) proposes another approach, grouping modifiers into classes and ordering based on those classes.",
        "A modifier's class is assigned based on its placement before a noun, relative to the other modifiers it appears with.",
        "Classes are composed of those modifiers that tend to be placed closer to the head noun, those modifiers that tend to be placed farther from the head noun, etc., with each class corresponding to a general positional preference.",
        "Unlike earlier work, these classes allow more than one ordering to be proposed for some pairs of modifiers.",
        "Combining corpora of various genres, Mitchell's system achieves a token precision of 89.6% (see Section 4 for discussion and comparison of various evaluation metrics).",
        "However, the model only makes predictions for 74.1% of all modifier pairs in the test data, so recall is quite low (see Tables 4 and 6).",
        "Overall, previous work in noun-phrase ordering has produced impressive accuracies in some domains, but currently available systems tend to adapt poorly to unseen modifiers and do not generalize well to unseen domains."
      ]
    },
    {
      "heading": "3. Methods",
      "text": [
        "Multiple sequence alignment algorithms align sequences of discrete tokens into a series of columns.",
        "They attempt to align identical or easily-substitutable tokens within a column, inserting gaps when such gaps will result in a better alignment (more homogeneous token assignments within each column).",
        "For example, consider the simple alignment shown in Table 1.",
        "The two sequences 'GAACTGAT' and 'AAGT-",
        "GTAT' are aligned to maximize the number of identical items that appear in the same column, substituting tokens (column 3), and inserting gaps (columns 1 and 6).",
        "A full MSA is generally constructed by iteratively aligning each new sequence with an identical or similar sequence already in the MSA (so-called \"progressive alignment\").",
        "The costs of token substitution are often taken from a hand-tuned substitution matrix.",
        "A cost may also be associated with inserting a gap into the existing MSA (a \"gap penalty\").",
        "Once the full MSA has been constructed, a Position Specific Score Matrix (PSSM) can be induced, in which each token (including a special gap token) is assigned a separate alignment cost for each column.",
        "An unseen sequence can then be aligned with the full MSA by Viterbi search.",
        "Predicting sequence ordering within a noun phrase is a natural application for MSA techniques, and it seems reasonable to propose that aligning an unseen set of modifiers with such an MSA model will yield acceptable orderings.",
        "Table 2 illustrates how MSA may be applied to modifiers before a noun.",
        "Given an NP preceded by modifiers hungry, big, and Grizzly, alignment of the modifiers with NPs seen in the training corpus determines the prenominal ordering big hungry Grizzly.",
        "We then align every permutation of the NP and choose the best-scoring alignment.",
        "The vocabulary for a linguistic alignment is large enough to render a hand-tuned substitution matrix impractical, so we instead construct a cost function based on features of the token under consideration and those of the other tokens already aligned in a column.",
        "We know of no prior work on methods for training such an alignment.",
        "We present and compare two training methods, each of which produces competitive ordering accuracies.",
        "Both training methods share the feature-set described in Table 3.",
        "In each case, we train an MSA by aligning each instance in the training data.",
        "In our alignment approach, the features listed in Table 3 are grouped into several classes.",
        "All observed words are a class, all observed stems are a class (Porter, 1980), and so on.",
        "We treat each indicator feature as a separate class, and make the assumption that classes are independent of one another.",
        "This assumption is clearly false, but serves as a reasonable first approximation, similar to the independence assumption in Naïve Bayesian analysis.",
        "After aligning each instance, we estimate the probability of a feature appearing in a column as the simple maximum likelihood estimate given the observed occurrences within its class.",
        "This produces a new PSSM with which to align the next instance.",
        "small",
        "clumsy",
        "black",
        "bear",
        "big",
        "-",
        "black",
        "cow",
        "two-story",
        "-",
        "brown",
        "house",
        "big",
        "clumsy",
        "-",
        "bull",
        "small",
        "fuzzy",
        "brown",
        "duck",
        "large",
        "-",
        "green",
        "house",
        "big",
        "hungry",
        "Grizzly",
        "bear",
        "Our problem differs from alignment of biological sequences in that we have little prior knowledge of the similarity between sequences.",
        "'Similarity' can be defined in many ways; for biological sequences, a simple Levenshtein distance is effective, using a matrix of substitution costs or simple token identity (equivalent to a matrix with cost 0 on the diagonal and 1 everywhere else).",
        "These matrices are constructed and tuned by domain experts, and are used both in choosing alignment order (i.e., which sequence to align next) and during the actual alignment.",
        "When aligning biological sequences, it is customary to first calculate the pairwise distance between each two sequences and then introduce new sequences into the MSA in order of similarity.",
        "In this way, identical sequences may be aligned first, followed by less similar sequences (Durbin et al., 1999).",
        "However, we have no principled method of determining the 'similarity' of two words in an NP.",
        "We have no a priori notion of what the cost of substituting 'two-story' for 'red' should be.",
        "Lacking this prior knowledge, we have no optimal alignment order and we must in effect learn the substitution costs as we construct the MSA.",
        "Therefore, we choose to add instances in the order they occur in the corpus, and to iterate over the entire MSA, reintroducing each sequence.",
        "This allows a word to 'move' from its original column to a column which became more likely as more sequences were aligned.",
        "Each iteration is similar to a step in the EM algorithm: create a model (build up an MSA and PSSM), apply the model to the data (re-align all sequences), and repeat.",
        "Randomly permuting the training corpus did not change our results significantly, so we believe our results are not greatly dependent on the initial sequence order.",
        "Instead of assigning substitution costs, we compute the cost of aligning a word into a particular column, as follows:",
        "C = The set of i feature classes, Ci G C",
        "cnt(i,j,k) = The count of instances of feature j from class i in column k Xi = Laplace smoothing count",
        "for feature class Ci A = The number of aligned instances",
        "{ 1 if word w has feature j from Ci, 0 otherwise",
        "These help define feature positional probabilities for column k:",
        "Identity Features",
        "Word",
        "Token",
        "Stem",
        "Word stem, derived by the Porter Stemmer",
        "Length",
        "'Binned' length indicators: 1, 2, 3, 4, 5-6, 7-8, 9-12, 13-18, >18 characters",
        "Indicator Features",
        "Capitalized",
        "Token begins with a capital",
        "All-caps",
        "Entire token is capitalized",
        "Hyphenated",
        "Token contains a hyphen",
        "Numeric",
        "Entire token is numeric (e.g. 234)",
        "Initial Numeric",
        "Token begins with a numeral (e.g. 123, 2-sided)",
        "Endings",
        "Token ends with -al, -ble, -ed, -er, -est, -ic, -ing, -ive, -ly",
        "That is, the probability of feature j from class i occurring in column k is a simple maximum-likelihood estimate – count the number of times we have already aligned that feature in the column and divide by the number of sequences aligned.",
        "We smooth that probability with simple Laplace smoothing.",
        "We can now calculate the probability of aligning a word w into column k by multiplying the product of the probabilities of aligning each of the word's features.",
        "Taking the negative log to convert that probability into a cost function:",
        "Finally, we define the cost of inserting a new column into the alignment to be equal to the number of columns in the existing alignment, thereby increasingly penalizing each inserted column until additional columns become prohibitively expensive.",
        "The longest NPs aligned were 7 words, and most ML MSAs ended with 12-14 columns.",
        "We experimented with various column insertion costs and values for the smoothing A and found no significant differences in overall performance.",
        "We also trained a discriminative model, using the same feature-set.",
        "Discriminative training does not require division of the features into classes or the independence assumption discussed in Section 3.2.",
        "We again produced a cost vector for each column.",
        "We fixed the alignment length at 8 columns, allowing alignment of the longest instances in our test corpus.",
        "Our training data consists of ordered sequences, but the model we are attempting to learn is a set of column probabilities.",
        "Since we have no gold-standard MSAs, we instead align the ordered NPs with the current model and treat the least cost alignment of the correct ordering as the reference for training.",
        "We trained this model using the averaged per-ceptron algorithm (Collins, 2002).",
        "A percep-tron learns from classifier errors, i.e., when it misorders an NP.",
        "At each training instance, we align all possible permutations of the modifiers with the MSA.",
        "If the least cost alignment does not correspond to the correct ordering of the modifiers, we update the perceptron to penalize features occurring in that alignment and to reward features occurring in the least cost alignment corresponding to the correct ordering, using standard perceptron updates.",
        "Examining every permutation of the NP involves a non-polynomial cost, but the sequences under consideration are quite short (less than 1% of the NPs in our corpus have more than 3 modifiers, and the longest has 6; see Table 7).",
        "So exhaustive search is practical for our problem; if we were to apply MSA to longer sequences, we would need to prune heavily."
      ]
    },
    {
      "heading": "4. Evaluation",
      "text": [
        "We trained and tested on the same corpus used by Mitchell (2009), including identical 10-fold cross-validation splits.",
        "The corpus consists of all NPs extracted from the Penn Treebank, the Brown corpus, and the Switchboard corpus (Marcus et al., 1999; Kucera and Francis, 1967; Godfrey et al., 1992).",
        "The corpus is heavily biased toward WSJ text (74%), with approximately 13% of the NPs from each of the other corpora.",
        "We evaluated our system using several related but distinct metrics, and on both modifier pairs and full NPs.",
        "We define:",
        "T = The set of unique orderings found in the test corpus",
        "P = The set of unique orderings predicted by the system",
        "Type Precision (|P n T|/|P|) measures the probability that a predicted ordering is 'reasonable' (where 'reasonable' is defined as orderings which are found in the test corpus).",
        "We define:",
        "Table 4: Results on the combined WSJ, Switchboard, and Brown corpus; averages and standard deviations over a 10-fold cross validation.",
        "Winning scores are in bold.",
        "Type Recall (|Pn T|/|T|) measures the percentage of 'reasonable' orderings which the system recreates.",
        "Note that these two metrics differ only in notation from those used by Mitchell (2009).",
        "We also define a third metric, Token Accuracy, which measures accuracy on each individual ordering in the test corpus, rather than on unique orderings.",
        "This penalizes producing orderings which are legal, but uncommon.",
        "For example, if {a,b} occurs eight times in the test corpus as <a,b> and two times as <b,a>, we will be limited to a maximum accuracy of 80% (presuming our system correctly predicts the more common ordering).",
        "However, even though suggesting <b,a> is not strictly incorrect, we generally prefer to reward a system that produces more common orderings, an attribute not emphasized by type-based metrics.",
        "Our test corpus does not contain many ambiguous pairings, so our theoretical maximum token accuracy is 99.8%."
      ]
    },
    {
      "heading": "01. ..N = All modifier orderings in the test data",
      "text": [
        "pred(oi) = The predicted ordering for modifiers in oi"
      ]
    },
    {
      "heading": "0. otherwise",
      "text": [
        "Token Accuracy = N",
        "Most earlier work has focused on ordering pairs of modifiers.",
        "The results in Table 4 are directly comparable to those found in Mitchell (2009).",
        "Mitchell's earlier approach does not generate a prediction when the system has insufficient evidence, and allows generation of multiple predictions given conflicting evidence.",
        "In theory, generating multiple predictions could improve recall, but in practice her system appears biased toward under-predicting, favoring precision.",
        "Our approach, in contrast, forces prediction of a single ordering for each test instance, occasionally costing some precision (in particular in cross-domain trials; see Table 5), but consistently balancing recall and precision.",
        "Our measurement of Token Accuracy is comparable to the accuracy measure reported in Shaw and Hatzivassiloglou (1999) and Malouf (2000) (although we evaluate on a different corpus).",
        "Their approaches produce a single ordering for each test instance evaluated, so for each incorrectly ordered modifier pair, there is a corresponding modifier pair in the test data that was not predicted.",
        "Shaw and Hatzivassiloglou found financial text particularly difficult to order, and reported that their performance dropped by 19% when they included nouns as well as adjectives.",
        "Mal-ouf's system surpasses theirs, achieving an accuracy of 91.9%.",
        "However, his corpus was derived from the BNC – he did not attempt to order financial text – and he ordered only adjectives as modifiers.",
        "In contrast, our test corpus consists mainly of WSJ text, and we test on all forms of prenominal modifiers.",
        "We believe this to be a considerably more difficult task, so our peak performance of 88.9% would appear to be – at worst – quite competitive.",
        "Table 5 presents an evaluation of cross-domain generalization, splitting the same corpus by genre – Brown, Switchboard, and WSJ.",
        "In each trial, we train on two genres and test on",
        "Token Accuracy Type Precision Type Recall Type F-measure",
        "Mitchell ML MSA Perceptron MSA",
        "N/A 90.3% (2.2) 67.2% (3.4) 77.1% 85.5% (1.0) 84.6% (1.1) 84.7% (1.1) 84.7% 88.9% (0.7) 88.2% (0.8) 88.1% (0.8) 88.2%",
        "Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation.",
        "To compare directly with Mitchell (2009), we report token precision and recall instead of type.",
        "Our system always proposes one and only one ordering, so token accuracy, precision, and recall are identical.",
        "the third.",
        "Our results mirror those in the previous trials – forcing a prediction costs some precision (vis-a-vis Mitchell's 2009 system), but our recall is dramatically higher, resulting in more balanced performance overall.",
        "We now extend our analysis to ordering entire NPs, a task we feel the MSA approach should be particularly suited to, since (unlike pairwise models) it can model positional probabilities over an entire NP.",
        "To our knowledge, the only previously reported work on this task is Mitchell's (2009).",
        "We train this model on the full NP instead of on modifier pairs; this makes little difference in pairwise accuracy, but improves full-NP ordering considerably.",
        "As seen in Table 6, both MSA models perform quite well, the perceptron-trained MSA again outperforming the maximum likelihood model.",
        "However, we were somewhat disappointed in the performance on longer sequences.",
        "We expected the MSA to encode enough global information to perform accurate full sequence ordering, but found the accuracy drops off dramatically on NPs with more modifiers.",
        "In fact, the accuracy on longer sequences is worse than we would expect by simply extending a pairwise model.",
        "For instance, ordering three modifiers requires three pairwise decisions.",
        "We predict pairwise orderings with 88% accuracy, so we would expect no worse than (.88), or 68% accuracy on such sequences.",
        "However, the pairwise accuracy declines on longer NPs, so it underperforms even that theoretical minimum.",
        "Sparse training data for longer NPs biases the model strongly toward short sequences and transitivity (which our model does not encode) may become important when ordering several modifiers.",
        "Training",
        "Testing",
        "Token",
        "Type",
        "Type",
        "Type",
        "Corpora",
        "Corpus",
        "Accuracy",
        "Precision",
        "Recall",
        "F-measure",
        "Brown+WSJ",
        "Swbd",
        "N/A",
        "94.2%",
        "58.2%",
        "72.0%",
        "Mitchell",
        "Swbd+WSJ",
        "Brown",
        "N/A",
        "87.0%",
        "51.2%",
        "64.5%",
        "Swbd+Brown",
        "WSJ",
        "N/A",
        "82.4%",
        "27.2%",
        "40.9%",
        "Brown+WSJ",
        "Swbd",
        "74.6%",
        "74.7%",
        "75.3%",
        "75.0%",
        "ML MSA",
        "Swbd+WSJ",
        "Brown",
        "75.3%",
        "74.7%",
        "74.9%",
        "74.8%",
        "Swbd+Brown",
        "WSJ",
        "70.2%",
        "71.6%",
        "71.8%",
        "71.7%",
        "Brown+WSJ",
        "Swbd",
        "77.2%",
        "78.2%",
        "77.6%",
        "77.9%",
        "Perceptron MSA",
        "Swbd+WSJ",
        "Brown",
        "76.4%",
        "76.7%",
        "76.4%",
        "76.5%",
        "Swbd+Brown",
        "WSJ",
        "77.9%",
        "77.5%",
        "77.3%",
        "77.4%",
        "Token Accuracy Token Precision Token Recall Token F-measure",
        "Mitchell ML MSA Perceptron MSA",
        "N/A 94.4% 78.6% (1.2) 85.7% 76.9% (1.6) 76.5% (1.4) 76.5% (1.4) 76.50% 86.7% (0.9) 86.7% (0.9) 86.7% (0.9) 86.7%",
        "Modifiers",
        "Frequency",
        "Token",
        "Pairwise",
        "Accuracy",
        "Accuracy",
        "2",
        "89.1%",
        "89.7%",
        "89.7%",
        "3",
        "10.0%",
        "64.5%",
        "84.4%",
        "4",
        "0.9%",
        "37.2%",
        "80.7%"
      ]
    },
    {
      "heading": "5. Ablation Tests",
      "text": [
        "We performed limited ablation testing on the discriminative model, removing features individually and comparing token accuracy (see Table 8).",
        "We found that few of the features provided great benefit individually; the overall system performance remains dominated by the word.",
        "The word and stem features appear to capture essentially the same information; note that performance does not decline when the word or stem features are ablated, but drops drastically when both are omitted.",
        "Performance declines slightly more when ending features are ablated as well as words and stems, so it appears that – as expected – the information captured by ending features overlaps somewhat with lexical identity.",
        "The effects of individual features are all small and none are statistically significant."
      ]
    },
    {
      "heading": "6. Summary and Future Directions",
      "text": [
        "We adapted MSA approaches commonly used in computational biology to linguistic problems and presented two novel methods for training such alignments.",
        "We applied these techniques to the problem of ordering prenominal modifiers in noun phrases, and achieved performance competitive with – and in many cases, superior to – the best results previously reported.",
        "In our current work, we have focused on relatively simple features, which should be adaptable to other languages without expensive resources or much linguistic insight.",
        "We are interested in exploring richer sources of features for ordering information.",
        "We found simple morphological features provided discriminative clues for otherwise ambiguous instances, and believe that richer morphological features might be helpful even in a language as morphologically impoverished as English.",
        "Boleda et al.",
        "(2005) achieved promising preliminary results using morphology for classifying adjectives in Catalan.",
        "Further, we might be able to capture some of the semantic relationships noted by psychological analyses (Ziff, 1960; Martin, 1969) by labeling words which belong to known semantic classes (e.g., colors, size denominators, etc.).",
        "We intend to explore deriving such labels from resources such as WordNet or OntoNotes.",
        "We also plan to continue exploration of MSA training methods.",
        "We see considerable room for refinement in generative MSA models; our maximum likelihood training provides a strong starting point for EM optimization, conditional likelihood, or gradient descent methods.",
        "We are also considering applying maximum entropy approaches to improving the discriminative model.",
        "Finally (and perhaps most importantly), we expect that our model would benefit from additional training data, and plan to train on a larger, automatically-parsed corpus.",
        "Even in its current form, our approach improves the state-of-the-art, and we believe MSA techniques can be a useful tool for ordering prenominal modifiers in NLP tasks."
      ]
    },
    {
      "heading": "7. Acknowledgements",
      "text": [
        "This research was supported in part by NSF Grant #IIS-0811745.",
        "Any opinions, findings, conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF.",
        "Feature(s)",
        "Gain/Loss",
        "Word",
        "0.0",
        "Stem",
        "0.0",
        "Capitalization",
        "-0.1",
        "All-Caps",
        "0.0",
        "Numeric",
        "-0.2",
        "Initial-numeral",
        "0.0",
        "Length",
        "-0.1",
        "Hyphen",
        "0.0",
        "-al",
        "0.0",
        "-ble",
        "-0.4",
        "-ed",
        "-0.4",
        "-er",
        "0.0",
        "-est",
        "-0.1",
        "-ic",
        "+0.1",
        "-ing",
        "0.0",
        "-ive",
        "-0.1",
        "-ly",
        "0.0",
        "Word and stem",
        "-22.9",
        "Word, stem, and endings",
        "-24.2"
      ]
    }
  ]
}
