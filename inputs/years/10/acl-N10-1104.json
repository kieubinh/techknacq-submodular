{
  "info": {
    "authors": [
      "Amr El-Desoky",
      "Ralf Schlüter",
      "Hermann Ney"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1104",
    "title": "A Hybrid Morphologically Decomposed Factored Language Models for Arabic LVCSR",
    "url": "https://aclweb.org/anthology/N10-1104",
    "year": 2010
  },
  "references": [
    "acl-N03-2002",
    "acl-N07-2014",
    "acl-W02-0506"
  ],
  "sections": [
    {
      "text": [
        "In this work, we try a hybrid methodology for language modeling where both morphological decomposition and factored language modeling (FLM) are exploited to deal with the complex morphology of Arabic language.",
        "At the end, we are able to obtain from 3.5% to 7.0% relative reduction in word error rate (WER) with respect to a traditional fullwords system, and from 1.0% to 2.0% relative WER reduction with respect to a non-factored decomposed system."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Arabic language is characterized by a complex morphological structure where different kinds of prefixes and suffixes are appended to the word stems producing a very large number of inflectional forms.",
        "This leads to poor language model (LM) probability estimates, and thus high LM perplexities (PPLs) causing problems in large vocabulary continuous speech recognition (LVCSR).",
        "One successful approach to deal with this problem is to consider LMs including morphologically decomposed words.",
        "Another approach is to use the factored language models (FLMs) which are powerful models that combine multiple sources of information and efficiently integrate them via a complex backoff mechanism (Bilmes and Kirchhoff, 2003).",
        "Morphological decomposition is successfully used for Arabic LMs in several previous works.",
        "Some are based on linguistic knowledge, and others are based on unsupervised methods.",
        "Some of the linguistic methods are based on the Buckwalter Arabic Morphological Analyzer (BAMA) like in (Lamel et al., 2008).",
        "Alternatively, in our previous work (El-Desoky et al., 2009), we use the Morphological Analyzer and Disambiguator for Arabic (MADA) (Habash and Rambow, 2007).",
        "On the other side, most of the unsupervised methods are based on the minimum description length principle (MDL) like in (Creutz et al., 2007).",
        "Another type of models is the FLM, in which words are viewed as vectors of K factors, so that wt := {/t1:K}.",
        "A factor could be any feature of the word such as morphological class, stem, root or even a semantic feature.",
        "An FLM is a model over factors, i.e., p(/t1:K\\/tlLKi, ftK2,/t-^+i), which could be reformed as a product of probabilities of the form p(/h,/n ).",
        "The main idea of the model is to backoff to other factors when some word n-gram is not observed in the training data, thus improving the probability estimates.",
        "In this work we try to combine the strengths of morphological decomposition and factored language modeling.",
        "Therefore, language models with factored morphemes are used.",
        "For this purpose, the LM training data are processed such that fullwords are decomposed into prefix-stem-suffix format with different added features.",
        "We compare our approach with the standard full-word, decomposed word, and factored fullword n-gram approaches."
      ]
    },
    {
      "heading": "2. Factorization and Decomposition",
      "text": [
        "We use MADA 2.0 in order to perform morphological analysis and attach a complete set of morphological tags to Arabic words in context.",
        "From those tags we derive three different features.",
        "Moreover, we derive a fourth feature based on the root of the word generated by \"Sebawai\" (Darwish, 2002).",
        "The list of features is:",
        "• \"W\" (Word): word surface form.",
        "• \"L\" (Lexeme): word lexeme.",
        "• \"M\" (Morph): morphological description.",
        "• \"P\" (Pattern): word after subtracting root.",
        "The LM training corpora are processed so that words are replaced by the factored representation as required by SRILM-FLM extensions (Kirchhoff et al., 2008).",
        "Then, word decomposition is performed based on MADA as described in our previous publication (El-Desoky et al., 2009)."
      ]
    },
    {
      "heading": "3. FLM topologies",
      "text": [
        "In order to obtain a good performance via FLMs, we need to optimize the FLM parameters: the combination of the conditioning factors, backoff path, and smoothing options.",
        "For this purpose, we use a Genetic Algorithm based FLM optimization tool (GA-FLM) developed by Kirchhoff (2006) which seeks to minimize the PPL over some held-out text.",
        "Furthermore, we apply some manual optimization to fine tune the FLM parameters.",
        "For memory limitations, we only use factors up to 2 previous time slots (tri-gram like models).",
        "Finally, we come up with a set of competing FLMs with rather close PPLs.",
        "In Table 1, we record the PPLs measured for some held-out text.",
        "The first column gives the combination of the parent factors.",
        "So that, FLM1 corresponds to the model P(Wt\\Wt-1,Wt-2), which is the FLM equivalent of the standard tri-gram LM (our baseline model), while FLM2 & FLM3 correspond to the model P(Wt\\Wt-1, Mt-1, Lt-1, Pt-1, W-), however FLM4 & FLM5 correspond to the model P(Wt\\Wt-1, Mt-1, Lt-1, Wt-2, Mt-2, Lt-2).",
        "The \"gtmin\" refers to the count threshold that is sufficient to have a language model hit at some node of the the backoff graph (for exact topologies, contact the first author).",
        "From Table 1, comparing PPLs (non-normalized) across factored and non-factored LMs, we see that using more factors other than the normal word could help decreasing the PPL.",
        "This is true for all the used types of vocabulary units.",
        "vocabulary",
        "Table 1: perplexities of the FLMs using vocabularies: (FW: 70k full-words; PD: partially decomposed with 20k ful-words + 50k morphemes; FD: 70k fully decomposed).",
        "In order to select the best FLM topology, we run a simple one pass recognition for a small internal dev corpus derived from GALE data sets, consists of 40 minutes of audio data recorded during January to March 2007.",
        "The acoustic models are within-word tri-phone models trained using 1100h of audio material.",
        "The basic acoustic models are trained based on Maximum Likelihood (ML) method.",
        "Then, a discriminative training based on Minimum Phone Error (MPE) criterion is performed to enhance the models.",
        "A 70k fullwords lexicon is used.",
        "The FLM training data consists of 206 Million running full-words.",
        "A standard bi-gram LM based on fullwords is used to generate N-best lists, then N-best list rescoring is performed using the different FLM topologies shown in Table 1.",
        "We start by N = 1000-best down to 3-best sentences.",
        "Using N = 10 always gives the best results.",
        "The recognition WERs are recorded in Table 2.",
        "The least WER is obtained with FLM4.",
        "We note that the best FLM does not correspond to the least PPL.",
        "This is because a higher \"gt-min\" value causes more backoff in cases of insufficient data leading to better estimates.",
        "Therefore, we select FLM4 for the coming experiments.",
        "FLMx parent factors",
        "FW",
        "PD",
        "FD",
        "1: W1 W2 (baseline)",
        "302.6",
        "284.1",
        "82.7",
        "W1 Ml L1 P1 W2",
        "2: gtmin = 1",
        "306.2",
        "296.9",
        "83.2",
        "3: gtmin = 2-4",
        "290.9",
        "279.1",
        "79.8",
        "W1 M1 L1 W2 M2 L2",
        "4: gtmin = 1",
        "300.2",
        "291.1",
        "83.6",
        "5: gtmin = 2-4",
        "294.5",
        "283.7",
        "81.1",
        "FLMx parent factors",
        "WER [%]",
        "1: W1 W2 (baseline)",
        "20.4",
        "W1 M1 L1 P1 W2",
        "2: gtmin = 1",
        "20.2",
        "3: gtmin = 2-4",
        "20.4",
        "W1 M1 L1 W2 M2 L2",
        "4: gtmin = 1",
        "19.9",
        "5: gtmin = 2-4",
        "20.3"
      ]
    },
    {
      "heading": "4. Experimental Setup",
      "text": [
        "Our recognition system is close to the one described in section 3.",
        "However, we use within and across-word models at different recognition passes.",
        "In addition, we use 70k or 256k lexicon of fullwords or partially decomposed words.",
        "Alternatively, we evaluate the results on the GALE 2007 development and evaluation sets (dev07: 2.5h; eval07: 4h).",
        "Our recognizer works in 3 passes.",
        "In the first pass, within-word acoustic models are used with no adaptation, along with a standard bi-gram LM to generate lattices, followed by a standard tri-gram or 4-gram LM rescoring of lattices.",
        "The second pass does the same, but it uses across-word models with Constrained Maximum Likelihood Linear Regression (CMLLR) adaptation.",
        "Then, a third pass with additional Maximum Likelihood Linear Regression (MLLR) adaptation is performed, using a standard bi-gram LM to generate lattices or N-best lists.",
        "Then, one of the following is performed: 1) lattice rescoring using standard tri-gram or 4-gram LM, 2) N-best list rescoring using FLMs based on full-words, partially or fully decomposed words."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "In this section, we record our recognition results for: 1) systems based on full-words, and 2) systems based on decomposed words.",
        "Also, we introduce additional results for larger lexicon sizes.",
        "In this section, we present the WERs of our recognition systems based on full-words.",
        "Where, during the search, we use a lexicon of 70K full-words.",
        "In the first 2 passes, we use a standard bi-gram LM to generate lattices, followed by a standard tri-gram LM rescoring of lattices.",
        "However, in the third pass, we generate both lattices and N-best lists based on the same bi-gram LM.",
        "The final lattices and N-best lists are rescored using different LMs as shown in Table 3.",
        "In case we perform N-best list rescoring with a FLM, the N-best lists are processed to produce factored representation, followed by partial or full decomposition as previously described in section 2.",
        "It is clear from Table 3 that the least WER is achieved when using N-best list rescoring using a fullwords based FLM.",
        "This gives an absolute improvement of 0.8% (about 4.8% relative) compared to the standard tri-gram lattice rescoring.",
        "On the other hand, we have 0.6% absolute improvement (about 3.7% relative) compared to the standard 4-gram lattice rescoring.",
        "Decomposition does not help in this case.",
        "This is because the original N-best lists are generated in fullwords format, whose decomposition might not lead to better LM scores.",
        "For this reason, we expect that it is better to start with a decomposed LM for lattice and N-best generation.",
        "This section introduces the WERs of our systems based on decomposed words.",
        "We use a similar setup as in section 5.1.",
        "However, we use a lexicon and a bi-gram LM based on a 70k partially decomposed words (20k fullwords + 50k morphemes).",
        "Table 4 presents the results.",
        "As expected, we get the best WER when using N-best list rescoring with a FLM based on partially decomposed words.",
        "An absolute improvement of 0.4% (2.7% relative) is achieved compared to the new baseline.",
        "Compared to the old baseline of Table 3, we get an absolute improvement of 2.2% (13.3% relative).",
        "Now, we increase the size of our lexicon to 256k partially decomposed words (20k fullwords + 236k",
        "LM rescoring (3rd pass)",
        "Dev07 [%]",
        "tri-gram lattice resc.",
        "(baseline)",
        "16.5",
        "4-gram lattice resc.",
        "16.3",
        "N-best FLM resc.",
        ":",
        "+ FW (original N-best)",
        "15.7",
        "+ PD (decomposed N-best)",
        "15.8",
        "+ FD (decomposed N-best)",
        "16.0",
        "Table 3: WERs for 70k fullwords systems.",
        "LM rescoring (3rd pass)",
        "Dev07 [%]",
        "tri-gram lattice resc.",
        "(baseline)",
        "14.7",
        "4-gram lattice resc.",
        "14.5",
        "N-best FLM resc.",
        ":",
        "+ FW (re-joint N-best)",
        "14.6",
        "+ PD (original N-best)",
        "14.3",
        "+ FD (decomposed N-best)",
        "14.4",
        "Table 5: WERs for 256k full-words, and partially decomposed systems (20k fullwords + 236k morphemes).",
        "morphemes).",
        "In addition, we use a standard 4-gram LM for rescoring the bi-gram lattices in the first 2 passes.",
        "To complete our comparisons, we record the WERs using traditional 256k fullwords lexicon, standard bi-gram search, and standard 4-gram LM for lattice rescoring, with no decomposition or factorization.",
        "In Table 5, we see that the improvement persists for the larger lexicon.",
        "Compared to the new baseline, the 256k decomposed system achieves WER reductions of [dev07: 0.3% absolute (2.1% relative); eval07: 0.2% absolute (1.2% relative)] when using N-best list rescoring with a FLM based on partially decomposed words.",
        "Moreover, it improves over the traditional fullwords by [dev07:",
        "1.0% absolute (6.7% relative); eval07: 0.6% absolute (3.6% relative)].",
        "The out-of-vocabulary rates (OOVs) are given in Table 6.",
        "It is worth noting that using fully decomposed lexicons as well as higher order LMs could not improve WERs, this we previously proved in (El-Desoky et al., 2009)."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "We have introduced a hybrid approach to Arabic language modeling.",
        "Our approach combines the strengths of both morphological decomposition and factored language modeling.",
        "Thus, we have used language models with factored morphemes.",
        "We have compared our approach to traditional approaches like: standard fullword n-grams, standard decomposed n-grams, and fullword based factored language models.",
        "Finally, we could achieve some improvements over all the traditional approaches.",
        "Nevertheless, we have only considered the use of factored language models in the rescoring phase."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This material is based upon work supported by the",
        "DARPA under Contract No.",
        "HR0011-06-C-0023.",
        "Any opinions, findings and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.",
        "Dev07",
        "Eval07",
        "System",
        "[%]",
        "[%]",
        "traditional full-words",
        "14.9",
        "16.5",
        "partially decomposed",
        "+ 4-gram lat.",
        "resc.",
        "(baseline)",
        "14.2",
        "16.1",
        "+ N-best FLM resc.",
        ":",
        "+ FW (re-joint N-best)",
        "14.1",
        "-",
        "+ PD (original N-best)",
        "13.9",
        "15.9",
        "+ FD (decomposed N-best)",
        "14.0",
        "-",
        "Corpus",
        "70k vocabularies",
        "256k vocabularies",
        "FW",
        "PD",
        "FD",
        "FW",
        "PD",
        "FD",
        "Dev07",
        "3.65",
        "1.33",
        "0.75",
        "1.36",
        "0.51",
        "0.24",
        "Eval07",
        "4.82",
        "1.94",
        "1.13",
        "1.85",
        "0.64",
        "0.41"
      ]
    }
  ]
}
