{
  "info": {
    "authors": [
      "Rohit Prabhavalkar",
      "Preethi Jyothi",
      "William Hartmann",
      "Jeremy Morris",
      "Eric Fosler-Lussier"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1110",
    "title": "Investigations into the Crandem Approach to Word Recognition",
    "url": "https://aclweb.org/anthology/N10-1110",
    "year": 2010
  },
  "references": [
    "acl-N03-1028"
  ],
  "sections": [
    {
      "text": [
        "Rohit Prabhavalkar, Preethi Jyothi, William Hartmann, Jeremy Morris, and Eric Fosler-Lussier",
        "We suggest improvements to a previously proposed framework for integrating Conditional Random Fields and Hidden Markov Models, dubbed a Crandem system (2009).",
        "The previous authors' work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition.",
        "As an alternative to the log posterior representation used in their system, we explore frame-level representations derived from the CRF feature functions.",
        "We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors.",
        "We report significant gains over the previous Crandem system on the Wall Street Journal word recognition task."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Conditional Random Fields (CRFs) (Lafferty et al., 2001) have recently emerged as a promising new paradigm in the domain of Automatic Speech Recognition (ASR).",
        "Unlike Hidden Markov Models (HMMs), CRFs are direct discriminative models: they predict the probability of a label sequence conditioned on the input.",
        "As a result, CRFs can capture long-range dependencies in the data and avoid the need for restrictive independence assumptions.",
        "Variants of CRFs have been successfully used in phone recognition tasks (Gunawardana et al., 2005; Morris and Fosler-Lussier, 2008; Hifny and Renals, 2009).",
        "While the improvements in the phone recognition task are encouraging, recent efforts have been directed towards extending the CRF paradigm to the word recognition level (Zweig and Nguyen, 2009; Morris and Fosler-Lussier, 2009).",
        "The Crandem system (Morris and Fosler-Lussier, 2009) is one of the promising approaches in this regard.",
        "The Cran-dem system is directly inspired by the techniques of the Tandem system (Hermansky et al., 2000), where phone-label posterior estimates produced by a Multi-Layer Perceptron (MLP) are transformed into a suitable acoustic representation for a standard HMM.",
        "In both systems, the frame-based log posterior vector of P(phone| acoustics) over all phones is decorrelated using the Karhunen-Loeve (KL) transform; unlike MLPs, CRFs take into account the entire label sequence when computing local posteriors.",
        "However, posterior estimates from the CRF tend to be overconfident compared to MLP posteriors (Morris and Fosler-Lussier, 2009).",
        "In this paper, we analyze the interplay between the various steps involved in the Crandem process.",
        "Is the local posterior representation from the CRF the best representation?",
        "Given that the CRF posterior estimates can be overconfident, what transformations to the posteriors are appropriate?",
        "In Section 2 we briefly describe CRFs and the Crandem framework.",
        "We suggest techniques for improving Crandem word recognition performance in Section 3.",
        "Details of experiments and our results are discussed in Sections 4 and 5 respectively.",
        "We conclude with a discussion of future work in Section 6."
      ]
    },
    {
      "heading": "2. CRFs and the Crandem System",
      "text": [
        "Conditional random fields (Lafferty et al., 2001) express the probability of a label sequence Q conditioned on the input data X as a log-linear sum of weighted feature functions, where Sj(•) and fj(•) are known as state feature functions and transition feature functions respectively, and Aj and ßj are the associated weights.",
        "Z(X) is a normalization term that ensures a valid probability distribution.",
        "Given a set of labeled examples, the CRF is trained to maximize the conditional log-likelihood of the training set.",
        "The log-likelihood is concave over the entire parameter space, and can be maximized using standard convex optimization techniques (Lafferty et al., 2001; Sha and Pereira, 2003).",
        "The local posterior probability of a particular label can be computed via a forward-backward style algorithm.",
        "Mathematically, where at(q|X) and ßt(q|X) accumulate contributions associated with possible assignments of labels before and after the current time-step t. The Crandem system utilizes these local posterior values from the CRF analogously to the way in which MLP-posteriors are treated in the Tandem framework (Hermansky et al., 2000), by applying a log transformation to the posteriors.",
        "These transformed outputs are then decorrelated using a KL-transform and then dimensionality-reduced to be used as a replacement for MFCCs in a HMM system.",
        "While the MLP is usually reduced to 39 dimensions, the standard CRF benefits from a higher dimensionality reduction (to 19 dimensions).",
        "The decorrelated outputs are then used as an input representation for a conventional HMM system."
      ]
    },
    {
      "heading": "3. Improving Crandem Recognition Results",
      "text": [
        "Morris and Fosler-Lussier (2009) indicate that the local posterior outputs from the CRF model produces features that are more heavily skewed to the dominant phone class than the MLP system, leading to an increase in word recognition errors.",
        "In order to correct for this, we perform a non-linear transformation on the local CRF posterior representation before applying a KL-transform and subsequent stages.",
        "Specifically, we normalize all of the weights Aj and ßj in Equation 1 by a fixed positive constant n to obtain normalized weights Aj and ßj.",
        "We note that the probability of a label sequence computed using the transformed weights, p'(Q|X), is equivalent to taking the nth-root of the CRF probability computed using the unnormalized weights, with a new normalization term Z'(X) where, p(Q|X) is as defined in Equation 1.",
        "Also observe that the monotonicity of the nth-root function ensures that if p(Q1|X) > p(Q2|X) then p'(Q1|X) > p'(Q2|X).",
        "In other words, the rank order of the n-best phone recognition results are not impacted by this change.",
        "The transformation does, however, increase the entropy between the dominant class from the CRF and its competitors, since p'(Q|X) < p(Q|X).",
        "As we shall discuss in Section 5, this transformation helps improve word recognition performance in the Crandem framework.",
        "Our second set of experiments are based on the following observation regarding the CRF posteriors.",
        "As can be seen from Equation 2, the CRF posteriors involve a global normalization over the entire utterance as opposed to the local normalization of the MLP posteriors in the output softmax layer.",
        "This motivates the use of representations derived from the CRF that are 'local' in some sense.",
        "We therefore propose two alternative representations that are modeled along the lines of the linear outputs from an MLP.",
        "The first uses the sum of the state feature functions, to obtain a vector f state(X, t) for each time step t and input utterance X of length |Q| dimensions, where Q is the set of possible phone labels where q is a particular phone label.",
        "Note that the lack of an exponential term in this representation ensures that the representation is less 'spiky' than the CRF posteriors.",
        "Additionally, the decoupling of the representation from the transition feature functions could potentially allow the system to represent relative ambiguity between multiple phones hypothesized for a given frame.",
        "The second 'local' representation that we experimented with incorporates the CRF transition feature functions as follows.",
        "For each utterance X we perform a Viterbi decoding of the most likely state sequence Qbest = argmax<g{p(Q|X)} hypothesized for the utterance X.",
        "We then augmented the state feature representation with the sum of the transition features corresponding to the phone label hypothesized for the previous frame (qtbes1t) to obtain a vector ftrans(X, t) of length |Q|,",
        "As a final note, following (Morris and Fosler-Lussier, 2009), our CRF systems are trained using the linear outputs of MLPs as its state feature functions and transition biases as the transition feature functions.",
        "Hence, fstate is a linear transformation of the MLP linear outputs down to |Q| dimensions.Both fstate and ftrans can thus be viewed as an implicit mapping performed by the CRF of the input feature function dimensions down to |Q| dimensions.",
        "Note that the CRF implicitly uses information concerning the underlying phone labels unlike dimensionality reduction using KL-transform."
      ]
    },
    {
      "heading": "4. Experimental Setup",
      "text": [
        "To evaluate our proposed techniques, we carried out word recognition experiments on the speaker-independent portion of the Wall Street Journal 5K closed vocabulary task (WSJ0).",
        "Since the corpus is not phonetically transcribed, we first trained a standard HMM recognition system using PLP features and produced phonetic transcriptions by force aligning the training data.",
        "These were used to train an MLP phone classifier with a softmax output layer, using a 9-frame window of PLPs with 4000 hidden layer units to predict one of the 41 phone labels (including silence and short pause).",
        "The linear outputs of the MLP were used to train a baseline Tandem system.",
        "We then trained a CRF using the MLP linear outputs as its state feature functions.",
        "We extract local posteriors as well as the two 'local' representations described in Section 3.",
        "These input representations were then normalized at the utterance level, before applying a KL-transformation to decorrelate them and reduce dimensionality to 39 dimensions.",
        "Finally, each of these representations was used to train a HMM system with intra-word triphones and 16 Gaussians per mixture using the Hidden Markov Model Toolkit (Young et al., 2002)."
      ]
    },
    {
      "heading": "5. Results",
      "text": [
        "Results for each of the experiments described in Section 4 are reported in Table 1 on the 330-sentence standard 5K non-verbalized test set.",
        "The Crandem-baseline represents the system of (Morris and Fosler-Lussier, 2009).",
        "Normalizing the CRF weights of the system by either the weight with largest absolute value (CRF-NormMax) or by 5 (tuned on the development set) leads to significant improvements (p < 0.005) over the Cran-dem baseline.",
        "Similarly, using either the state feature sum (Crandem-state) or the representation augmented with the transition features (Crandem-trans) leads to significant improvements (p < 0.005) over the Crandem baseline.",
        "Note that the performance of these systems is comparable to the Tandem baseline.",
        "To further analyze the results obtained using the state feature sum representations and the Tandem baseline, we compute the mean distance for each phone HMM from every other phone HMM obtained at the end of the GMM-HMM training phase.",
        "The distance between two HMMs is computed as a uniformly weighted sum of the average distances between the GMMs of a one-to-one alignment of states corresponding to the two HMMs.",
        "GMM distances are computed using a 0.5-weighted sum of interdispersions normalized by self-dispersions (Wang et",
        "System",
        "Accuracy (%)",
        "Crandem-baseline",
        "89.4%",
        "Tandem-baseline",
        "91.8%",
        "Crandem-NormMax",
        "91.4%",
        "Crandem-Norm5",
        "92.1%",
        "Crandem-state",
        "91.7%",
        "Crandem-trans",
        "91.0%",
        "al., 2004).",
        "Di tance between monomodal Gau -ian di tribution were computed u ing the Bhat-tacharyya di tance mea ure.",
        "The phone HMM di -tance are normalized u ing the maximum phone di tance for each y tem.",
        "A can be een in Figure 1, the mean di tance obtained from the tate feature um repre entation are con i tently greater than the corre ponding di tance in the Tandem-MLP y -tem, indicating larger eparability of the phone in the feature pace.",
        "Similar trend were een with the tran ition feature um repre entation."
      ]
    },
    {
      "heading": "6. Conclusions and Future Work",
      "text": [
        "In this paper, we report significant improvements over the Crandem baseline.",
        "The weight normalization experiments confirmed the hypothesis that increasing the entropy of the CRF posteriors leads to better word-level recognition.",
        "Our experiments with directly extracting frame-level representations from the CRF reinforce this conclusion.",
        "Although our experiments with the systems using the state feature sum and transition feature augmented representation did not lead to improvements over the Tandem baseline, the increased separability of the phone models trained using these representations is encouraging.",
        "In the future, we intend to examine techniques by which these representations could be used to further improve word recognition results.",
        "Acknowledgement: The authors gratefully acknowledge support by NSF grants IIS-0643901 and IIS-0905420for this work."
      ]
    }
  ]
}
