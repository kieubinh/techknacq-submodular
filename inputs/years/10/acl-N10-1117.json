{
  "info": {
    "authors": [
      "Sebastian Riedel",
      "David A. Smith"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1117",
    "title": "Relaxed Marginal Inference and its Application to Dependency Parsing",
    "url": "https://aclweb.org/anthology/N10-1117",
    "year": 2010
  },
  "references": [
    "acl-D07-1096",
    "acl-D08-1016",
    "acl-E06-1011",
    "acl-N06-1054",
    "acl-P05-1045",
    "acl-W06-1616"
  ],
  "sections": [
    {
      "text": [
        "Sebastian Riedel David A. Smith",
        "Recently, relaxation approaches have been successfully used for MAP inference on NLP problems.",
        "In this work we show how to extend the relaxation approach to marginal inference used in conditional likelihood training, posterior decoding, confidence estimation, and other tasks.",
        "We evaluate our approach for the case of second-order dependency parsing and observe a tenfold increase in parsing speed, with no loss in accuracy, by performing inference over a small subset of the full factor graph.",
        "We also contribute a bound on the error of the marginal probabilities by a sub-graph with respect to the full graph.",
        "Finally, while only evaluated with BP in this paper, our approach is general enough to be applied with any marginal inference method in the inner loop."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In statistical natural language processing (NLP) we are often concerned with finding the marginal probabilities of events in our models or the expectations of features.",
        "When training to optimize conditional likelihood, feature expectations are needed to calculate the gradient.",
        "Marginalization also allows a statistical NLP component to give confidence values for its predictions or to marginalize out latent variables.",
        "Finally, given the marginal probabilities of variables, we can pick the values that maximize these marginal probabilities (perhaps subject to hard constraints) in order to predict a good variable assignment.",
        "'With a loss function that decomposes on the variables, this amounts to Minimum Bayes Risk (MBR) decoding, which is",
        "Traditionally, marginal inference in NLP has been performed via dynamic programming (DP); however, because this requires the model to factor in a way that lends itself to DP algorithms, we have to restrict the class of probabilistic models we consider.",
        "For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing (McDonald and Satta, 2007), we have non-projective languages such as Dutch using second order projective models if we want to apply DP.",
        "Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better non-projective solution (McDonald and Pereira, 2006).",
        "In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference.",
        "One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008).",
        "In both cases we usually work in the framework of graphical models – in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials.",
        "In theory, methods such as belief propagation can take any graph and perform marginal inference.",
        "This means that we gain a great amount of flexibility to represent more global and joint distributions for NLP tasks.",
        "The graphical models of interest, however, are often too large and densely connected for efficient inference in them.",
        "For example, in second order",
        "often very effective.",
        "dependency parsing models, we have O(n) variables and O(n) factors, each of which may have to be inspected several times.",
        "While belief propagation is still tractable here (assuming we follow the approach of Smith and Eisner (2008) to enforce tree constraints), it is still much slower than simpler greedy parsing methods, and the advantage second order models give in accuracy is often not significant enough to offset the lack of speed in practice.",
        "Moreover, if we extend such parsing models to, say, penalizing all pairs of crossing edges or scoring syntax-based alignments, we will need to inspect at least O (n) factors, increasing our efficiency concerns.",
        "When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (Tromble and Eisner, 2006; Riedel and Clarke, 2006).",
        "Here we start with a small subset of the full graph, and run inference for this simpler problem.",
        "Then we search for factors that are \"violated\" in the solution, and add them to the graph.",
        "This is repeated until no more new factors can be added.",
        "Empirically this approach has shown impressive success.",
        "It often dramatically reduces the effective network size, with no loss in accuracy.",
        "How can we extend or generalize MAP relaxation algorithms to the case of marginal inference?",
        "Roughly speaking, we answer it by introducing a notion of factor gain that is defined as the KL divergence between the current distribution with and without the given factor.",
        "This quantity is then used in an algorithm that starts with a sub-model, runs marginal inference in it and then determines the gains of the not-yet-added factors.",
        "In turn, all factors for which the gain exceeds some threshold are added to the current model.",
        "This process is repeated until no more new factors can be found or a maximum number of iterations is reached.",
        "We evaluate this form of relaxed marginal inference for the case of second-order dependency parsing.",
        "We follow Smith and Eisner's tree-aware belief propagation procedure for inference in the inner loop of our algorithm.",
        "This leads to a tenfold increase in parsing speed with no loss in accuracy.",
        "We also contribute a bound on the error on marginal probabilities the sub-graph defines with respect to the full graph.",
        "This bound can be used both for terminating (although not done here) and understanding the dynamics of inference.",
        "Finally, while only evaluated with BP so far, it is general enough to be applied with any marginal inference method in the inner loop.",
        "In the following, we first give a sketch of the graphical model we apply.",
        "Then we briefly discuss marginal inference.",
        "In turn we describe our relaxation algorithm for marginal inference and some of its theoretic guarantees.",
        "Then we present empirical support for the effectiveness of our approach, and conclude."
      ]
    },
    {
      "heading": "2. Graphical Models of Dependency Trees",
      "text": [
        "We give a brief overview of the graphical model we apply in our experiments.",
        "We chose the grandparents and siblings model, together with language specific multiroot and projectivity options as taken from Smith and Eisner (2008).",
        "All our models are defined over a set of binary variables Lij that indicate a dependency between token i and j of the input sentence W.",
        "Following Smith and Eisner (2008), we define a probability distribution over all dependency trees as a collection of edges y for a fixed input sentence W. This distribution is represented by an undirected graphical model, or Markov random field (MRF):",
        "specified by an index set F and a corresponding family of factors *i : Y ^ K+.",
        "Here Z is the partition function = yYii ^ (y).",
        "We will restrict our attention to binary factors that can be represented as *i (y) = eöi<^i((y) with binary functions 0i (y) G {0,1} and weights 6i G K. This leads to as an alternative representation for pf .",
        "Note that when 0i (y) = 1 we will say that *i fires for y.",
        "Note that a factor function *i(y) can depend on any part of the observed input sentence W ; however, for brevity we will suppress this extra argument to",
        "A particular model specifies its preference for set of dependency edges over another by a set of hard and soft constraints.",
        "We use hard constraints to rule out a priori illegal structures, such as trees where a word has two parents, and soft constraints to raise or lower the score of trees that contain particular good or bad substructures.",
        "A hard factor (or constraint) *i evaluates an assignment y with respect to some specified condition and fires only if this condition is violated; in this case it evaluates to 0.",
        "It is therefore ruling out all configurations in which the condition does not hold.",
        "Note that a hard constraint *i corresponds to 6i = – oo in our loglinear representation.",
        "For dependency parsing, we consider two particular hard constraints, each of which touches all edge variables in y: the constraint Tree requires that all edges form a directed spanning tree rooted at the root node 0; the constraint PTree enforces the more stringent condition that all edges form a projective directed tree.",
        "As in (Smith and Eisner, 2008), we used algorithms from edge-factored parsing to compute BP messages for these factors.",
        "In our experiments, we enforced one or the other constraint depending on the projectivity of given treebank data.",
        "A soft factor ^i acts as a soft constraint that prefers some assignments to others.",
        "This is equivalent to saying that its weight 6i is finite.",
        "Note that the weight of a soft factor is usually itself composed as a sum of (sub-)weights Wj for feature functions that have the same input-output behavior as 0i (y) when conditioned on the current sentence.",
        "It is these Wj which are adjusted at training time.",
        "We use three kinds of soft factors from Smith and Eisner (2008).",
        "In the full model, there are: O(n)",
        "LINKi)j- factors that judge dependency edges in isolation; O(n) GRANDi)j)k factors that judge pairs of dependency edges in a grandparent-parent-child chain; and O(n) SIBi)j)k factors that judge pairs of dependency edges that share the same parent."
      ]
    },
    {
      "heading": "3. Marginal Inference",
      "text": [
        "Formally, given our set of factors F and an observed sentence W, marginal inference amounts to calculating the probability ßf that our binary features <\\)iare active.",
        "That is, for each factor ^i",
        "For compactness, we follow the convention ofWain-wright and Jordan (2008) and represent the belief for a variable using the marginal probability of its corresponding unary factor.",
        "Hence, if we want to calculate pf (Lij) we use //îjnk-- in place.",
        "Moreover we will use ßfi = 1 – ßf when we need the probability of the event 0i (y) = 0.",
        "The two most prominent approaches to marginal inference in general graphical models are Markov Chain Monte Carlo (MCMC) and variational methods.",
        "In a nutshell, MCMC iteratively generates a Markov chain that yields pf as its stationary distribution.",
        "Any expectation ßf can then be calculated simply by counting the corresponding statistics in the generated chain.",
        "Generally speaking, variational methods frame marginal inference as an optimization problem.",
        "Either in the sense of minimizing the KL divergence of a much simpler distribution to the actual distribution pf, as in mean field methods.",
        "Or in the sense of maximizing a variational representation of the logpartition function over the set M of valid mean vectors (Wainwright and Jordan, 2008).",
        "Note that the variational representation of the log partition function involves an entropy term that is intractable to calculate in general and therefore usually approximated.",
        "Likewise, the set of constraints that guarantee vectors ß to be valid mean vectors is intractably large and is often simplified.",
        "Because we use belief propagation (BP) as baseline to compare to, and as a subroutine in our proposed algorithm, a brief characterization of it is in order.",
        "BP can be seen as a variational method that uses the Bethe Free Energy as approximation to the entropy, and the set ML of locally consistent mean vectors as an outer bound on M. A mean vector is locally consistent if its beliefs on factors are consistent with the beliefs of the factor neighbors.",
        "BP solves the variational problem by iteratively updating the beliefs of factors and variables based on the current beliefs of their neighbors.",
        "When applied to acyclic graphical models BP yields the exact marginals at convergence.",
        "For general graphs, BP is not guaranteed to converge, and the beliefs it calculates are generally not the true marginals; however, in practice BP often does converge and lead to accurate marginals."
      ]
    },
    {
      "heading": "4. Relaxed Incremental Marginal Inference",
      "text": [
        "Generally the runtime and accuracy of a marginal inference method depends on size, density, tree-width and interaction strength (i.e. the magnitude of its weights) of the Graphical Model.",
        "For example, in Belief Propagation the number of messages we have to send in each iteration scales with the number of factors (and their degrees).",
        "This means that when we add a large number of extra factors to our model, such as the O(n) grandparent and sibling factors for dependency parsing, we have to pay a price in terms of speed, sometimes even accuracy.",
        "However, on close inspection often many of the additional factors we use to model some higher order interactions are somewhat unnecessary or redundant.",
        "To illustrate this, let us look at a second order parsing model with grandparent factors.",
        "Surely determiners are not heads of other determiners, and this should be easy to encourage using LINK features only.",
        "Hence, a grandparent factor that discourages a determiner-determiner-determiner chain seems unnecessary.",
        "This raises two questions: (a) can we get away without most of these factors, and (b) can we efficiently tell which factors should be discarded.",
        "We will see in section 5 that question (a) can be answered affirmatively: with a only fraction of all second order factors we can calculate marginals that are very close to the BP marginals, and when used in MBR decoding, lead to the same trees.",
        "Question (b) can be approached by looking at how a similar problem has been tackled in combinatorial optimization and MAP inference.",
        "Riedel and Clarke (2006) tackled the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated.",
        "If constraints were added, the process is repeated, otherwise we terminate.",
        "To develop such an incremental relaxation approach to marginal inference, we generalize the notion of a violated constraint.",
        "What does it mean for a factor to be violated with respect to the solution of a marginal inference problem?",
        "One answer is to interpret the violation of a constraint as \"adding this constraint will impact our current belief\".",
        "To assess the impact of adding factor *i to a sub-graph F ÇF we can then use the following intuition: if the distribution F' U {i} is very similar to the distribution corresponding to F', it is probably safe to say that the marginals we get from both are close, too.",
        "If we use the KL divergence between the (distributions of) F' U { i} and F' for our interpretation of the above mentioned closeness, we can define a potential gain for adding ^i as follows:",
        "Together with a threshold e on this gain we can now adapt the relaxation approach to marginal inference by simply replacing the question, \"Is *i violated?\"",
        "with the question, \"Is gf (i) > e?\"",
        "We can see the latter question as a generalization of the former if we interpret MAP inference as the zero-temperature limit of marginal inference (Wainwright and Jordan, 2008).",
        "The form of the gain function is chosen to be easily evaluated using the beliefs we have already available for the current sub-graph F'.",
        "It is easy to show (see Appendix) that the following holds:",
        "Proposition 1.",
        "The gain of a factor ^i with respect to the sub-graph F' ÇF is",
        "That is, the gain of a factor ^i depends on two properties of First, the expectation ßf that *i fires under the current model F', and second, its loglinear weight öi.",
        "To get an intuition for this gain, consider the limit lim 1 gf of a factor with positive weight that is expected to be active under F'.",
        "In this case the gain becomes zero, meaning that the more likely fires under the current model, the less useful will it be to add according to our gain.",
        "For lim F 0 g^v the gain also disappears.",
        "Here the confidence of the current model in 0ibeing inactive is so high that any single factor which indicates the opposite cannot make a difference.",
        "Fortunately, the marginal probability ßf ' is usually available after inference, or can be approximated.",
        "This allows us to maintain the same basic algorithm as in the MAP case: in each \"inspection step\" we can use the results of the last run of inference in order to evaluate whether a factor has to be added or not.",
        "Algorithm 1 shows our proposed algorithm, Relaxed Marginal Inference.",
        "We are given an initial factor graph (for example, the first order dependency parsing model), a threshold e on the minimal gain a factor needs to have in order to be added, and a solver S for marginal inference in the partial graphs we generate along the way.",
        "We start by finding the marginals ß for the initial graph.",
        "These marginals are then used in step 4 to find the factors that would, when added in isolation, change the distribution substantially (i.e., by more than e in terms of KL divergence).",
        "We will refer to this step as separation, in line with cutting plane terminology.",
        "The factors are added to the current graph, and we start from the top unless there were no new factors added.",
        "In this case we return the last marginals ß.",
        "Clearly, this algorithm is guaranteed to converge: either we add at least one factor per iteration until we reach the full graph F, or we converge before.",
        "However, it is difficult to make any general statements about the number of iterations it takes until convergence.",
        "Nevertheless, in our experiments we find that algorithm 1 converges to a much smaller graph after a small number of iterations, and hence we are always faster than inference on the full graph.",
        "Finally, note that calculating the gain for all factors in F \\ F' in step 4 (separation) takes time proAlgorithm 1 Relaxed Marginal Inference.",
        "We have seen how to evaluate the potential gain when adding a single factor.",
        "However, this does not tell us how good the current sub-model is with respect to the complete graph.",
        "After all, while all remaining factors individually might not contribute much, in concert they may.",
        "We therefore present a (calculable) bound on the KL divergence of the partial graph from the full graph that can give us confidence in the solutions we return at convergence.",
        "Note that for this bound we still only need feature expectations from the current model.",
        "Moreover, we assume all weights 6i are positive – without loss of generality since we can always replace 0iwith its negation 1 – 0i and then change the sign of Qi (Richardson and Domingos, 2006).",
        "Proposition 2.",
        "Assume non-negative weights, let"
      ]
    },
    {
      "heading": "1.. for the KL divergence between F' and the full",
      "text": [
        "network F we have:",
        "2. for the error we make when estimating 0i's true expectation ßf by ßf we have:",
        "1:",
        "require:",
        "F'' :init.",
        "graph, e: threshold, S:solver, R: max.",
        "it",
        "2:",
        "repeat",
        "Find current marginals using solver S",
        "3:",
        "ß – marginals (F\", S )",
        "Find factors with high gain not yet added",
        "4:",
        "AF – {i GF\\F'|gf' >e}",
        "Add factors to current graph",
        "5:",
        "F' – F' U AF",
        "Check: no more new factors were added or R reached",
        "6:",
        "until AF = 0 or iteration >R",
        "return the marginals for the last graph f",
        "7:",
        "return ß",
        "This says that (1) we get closer to the full distribution and that (2) our marginals closer to the true marginals, if the remaining factors G either have a low total weight ||QG||, or the current belief ßGalready assigns high probability to the features (/>Gbeing active (and hence – (ßG, Qg) is small).",
        "The latter condition is the probabilistic analog to constraints already being satisfied.",
        "Finally, since n can be easily calculated, we plan to investigate its utility as a convergence criterion in future work.",
        "Our approach is inspired by earlier work on relaxation algorithms for performing MAP inference by incrementally tightening relaxations of a graphical model (Anguelov et al., 2004; Riedel, 2008), weighted Finite State Machine (Tromble and Eisner, 2006), Integer Linear Program (Riedel and Clarke, 2006) or Marginal Polytope (Sontag et al., 2008).",
        "However, none of these methods apply to marginal inference.",
        "Sontag and Jaakkola (2007) compute marginal probabilities by using a cutting plane approach that starts with the local polytope and then optimizes some approximation of the log partition function.",
        "Cycle consistency constraints are added if they are violated by the current marginals, and the process is repeated until no more violations appear.",
        "While this approach does tackle marginalization, it is focused on improving its accuracy.",
        "In particular, the optimization problems they solve in each iteration are in fact larger than the problem we want to relax.",
        "Our approach is also related to edge deletion in Bayesian networks (Choi and Darwiche, 2006).",
        "Here edges are removed from a Bayesian network in order to find a close approximation to the full network useful for other inference-related tasks (such as combined marginal and MAP inference).",
        "The core difference to our approach is the fact that they ask which edges to remove from the full graph, instead of which to add to a partial graph.",
        "This requires inference in the full model – the very operation we want to avoid."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "In our experiments we seek to answer the following questions.",
        "First, how fast is our relaxation approach compared to full marginal inference at comparable dependency accuracy?",
        "This requires us to find the best tree in terms of marginal probabilities on the link variables (Smith and Eisner, 2008).",
        "Second, how good is the final relaxed graph as an approximation of the full graph?",
        "Finally, how does incremental relaxation scale with sentence length?",
        "We trained and tested on a subset of languages from the CoNLL Dependency Parsing Shared Tasks (Nivre et al., 2007): Dutch, Danish, Italian, and English.",
        "We apply non-projective second order models for Dutch, Danish and Italian, and a projective second order model for English.",
        "To be able to compare inference on the same model, we trained using BP on the full set of LINK, GRAND, and SIB factors.",
        "Note that our models would rank highly among the shared task submissions, but could surely be further improved.",
        "For example, we do not use any language specific features.",
        "Since our focus in this paper is speeding up marginal inference, we will search for better models in future work.",
        "In our first set of experiments we explore the speed and accuracy of relaxed BP in comparison to full BP.",
        "To this end we first tested BP configurations with at most 5, at most 10, and at most 50 iterations to find the best setup in terms of speed and accuracy.",
        "Smith and Eisner (2008) use 5 iterations but we found that by using 10 iterations accuracy could be slightly improved.",
        "Running at most 50 iterations led to the same accuracy but was significantly slower.",
        "Hence we only report BP results with 10 iterations here.",
        "For relaxed BP we tested along three dimensions: the threshold e on the gain of factors, the maximum number of BP iterations in the inner loop of relaxed BP, and the maximum number of relaxation iterations.",
        "A configuration with maximum relaxation iterations R, threshold e, and maximum BP iterations B will be identified by RelR)£;B.",
        "In all settings we use the LINK factors and the hard factors as initial graph F'.",
        "Table 1 shows the results for several configurations and our four languages in terms of unlabeled dependency accuracy (percentage of correctly identified heads) in comparison to the gold data, and average parsing time in seconds.",
        "Here parsing time includes both time spent for marginal inference and the MBR decoding step after the marginals are available.",
        "We notice that by relaxing BP with no limit on the number of iterations we gain a 4-6 fold increase in parsing speed across all languages when using the threshold e = 0.0001, while accuracy remains as high as for full BP.",
        "This can be achieved with fewer BP iterations (at most 5) in each round of relaxation than full BP needs per sentence (at most 10).",
        "Intuitively this makes sense: since our factor graphs are smaller in each iteration there will be fewer cycles to slow down convergence.",
        "This only has a small impact on overall parsing time for languages other than English, since for most sentences even full BP converges after less than 10 iterations.",
        "We also observe that running just one iteration of our relaxation algorithm (Rel^a000^50) is enough to achieve accurate solutions.",
        "This leads to a twofold speed-up in comparison to running relaxation until convergence (primarily because of fewer calls to the separation routine), and a 7-13 fold speed-up (tenfold on average) when compared to full BP.",
        "How large is the fraction of the full graph needed for accurate marginal probabilities?",
        "And do we really need our relaxation algorithm with repeated inference or could we instead just prune the graph in advance?",
        "Here we try to answer these questions, and will focus on the Danish dataset.",
        "Note that our results for the other languages follow the same pattern.",
        "In table 2, we present the average ratio of the sizes of the partial and the full graph in terms of the second order factors.",
        "We also show the total runtime needed to find the subgraph and run inference in it.",
        "Table 2: Ratio of partial and full graph size (Size), runtime in seconds (Time), avg.",
        "error on marginals (Err.)",
        "and tree accuracy (Acc.)",
        "for Danish.",
        "As a measure of accuracy for marginal probabilities we find the average error in marginal probability for the variables of a sentence.",
        "Note that this measure does not necessarily correspond to the true error of our marginals because BP itself is approximate and may not return the correct marginals.",
        "The first row shows the full BP system, working on 100% of the factor graph.",
        "The next three rows look at relaxed marginal inference.",
        "We notice that with a low threshold e = 0.1 we pick almost no additional factors (0.003%), and this does affect accuracy.",
        "However, by lowering the threshold to 0.0001 and adding about 0.8% of the second order factors, we already match the dependency accuracy of full BP.",
        "On average we are also very close to the BP marginals.",
        "Can we find such small graphs without running extra iterations of inference?",
        "One approach could be to simply cut off factors *i with absolute weights |Qi| that fall under a certain threshold t. In the final rows of the table we test such an approach with t = 0.1, 0.5.",
        "We notice that pruning can reduce the second order factors to 42% while yielding (almost) the same accuracy, and close marginals.",
        "However, it is 5 times slower than our fastest approach.",
        "When reducing",
        "Configuration",
        "Dutch Acc.",
        "Time",
        "Danish Acc.",
        "Time",
        "English Acc.",
        "Time",
        "Italian Acc.",
        "Time",
        "BP",
        "84.9 0.665",
        "88.1 1.44",
        "88.3 2.43",
        "87.4 1.68",
        "Reloo,o.oooi,5",
        "Reloo,0.0001,50",
        "Reli,o.oooi,5o",
        "85.0 0.120 84.9 0.121 84.9 0.060",
        "88.1 0.234",
        "88.2 0.236 88.2 0.110",
        "88.2 0.575",
        "88.3 0.728",
        "88.4 0.352",
        "87.4 0.261 87.4 0.266 87.4 0.132",
        "Configuration",
        "Size",
        "Time",
        "Err.",
        "Acc.",
        "BP",
        "100%",
        "1.44",
        " – ",
        "88.1",
        "Reloo,o.i,5o",
        "« 0%",
        "0.12",
        "0.20",
        "87.5",
        "Reloo,0.000i,50",
        "0.8%",
        "0.24",
        "0.012",
        "88.2",
        "Reli,o.oooi,5o",
        "0.8%",
        "0.11",
        "0.015",
        "88.2",
        "Prunedoj",
        "42%",
        "0.56",
        "0.022",
        "88.0",
        "Pruneda5",
        "22%",
        "0.40",
        "0.098",
        "87.7",
        "Figure 1 : Total runtimes by sentence length.",
        "size further to about 20%, accuracy drops below the values we achieved with our relaxation approach at 0.8% of the second order factors.",
        "Hence simple pruning removes factors that do have a low weight, but are still important to keep.",
        "We have seen how relaxed BP is faster than full BP on average.",
        "But how does its speed scale with sentence length?",
        "To answer this question figure 1 shows a plot of runtime by sentence length for full BP, pruned BP with threshold 0.1, Reloo,o.oooi,50 and Reli,o.oooi,50-",
        "The graph indicates that the advantage of relaxed BP over both full BP and Pruned BP becomes even more significant for longer sentences, in particular when running only one iteration.",
        "This shows that by using our technique, second order parsing becomes more practical, in particular for very long sentences."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "We have presented a novel incremental relaxation algorithm that can be applied to marginal inference.",
        "Instead of adding violated constraints in each iteration, it adds factors that significantly change the distribution of the graph.",
        "This notion is formalized by the introduction of a gain function that calculates the KL divergence between the current network with and without the candidate factor.",
        "We show how this gain can be calculated and provide bounds on the error made by the marginals of the relaxed graph in place of the full one.",
        "Our algorithm led to a tenfold reduction in runtime at comparable accuracy when applied to multilingual dependency parsing with Belief Propagation.",
        "It is five times faster than pruning factors by their absolute weight, and results in smaller graphs with better marginals.",
        "In future work we plan to apply relaxed marginal inference to larger joint inference problems within NLP, and test its effectiveness with other marginal inference algorithms as solvers in the inner loop."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported in part by the Center for Intelligent Information Retrieval and in part by SRI International subcontract #27-001338 and ARFL prime contract #FA8750-09-C-0181.",
        "Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",
        "For Proposition 1 we use the primal form of the KL divergence (Wainwright and Jordan, 2008) where G = Jr\\Jr'.",
        "With G = {i} we get the desired gain.",
        "For Proposition 2, part 1, we first pick a simple upper bound on ZrZ^} by replacing the expectation with e\"\".",
        "Inserting this into the primal form KL divergence leads to the given bound.",
        "For part 2 we represent pF using pFi and reuse our above representation of Z^Z^}.",
        "This gives which can be upper bounded by lower bounding the expectation and upper bounding the log-linear term.",
        "For the latter we use el|9G||1; forthe grst Jensen's inequality gives where the equality follows from linearity of expectations.",
        "This yields pf (y) < pj=i (y) ev and therefore upper bounds on jif and /j-^i.",
        "Basic algebra then gives the desired error interval for ßf in terms of jif ."
      ]
    }
  ]
}
