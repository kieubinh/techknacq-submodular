{
  "info": {
    "authors": [
      "Tetsuji Nakagawa",
      "Kentaro Inui",
      "Sadao Kurohashi"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1120",
    "title": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables",
    "url": "https://aclweb.org/anthology/N10-1120",
    "year": 2010
  },
  "references": [
    "acl-C08-1106",
    "acl-D08-1083",
    "acl-H05-1044",
    "acl-I08-1039",
    "acl-P06-2059",
    "acl-P08-1034",
    "acl-W02-1011",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "text": [
        "Dependency Tree-based Sentiment Classification using CRFs with Hidden",
        "Variables",
        "Tetsuji Nakagawa*, Kentaro Inui*\" and Sadao Kurohashi**",
        "*National Institute of Information and Communications Technology",
        "In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables.",
        "Subjective sentences often contain words which reverse the sentiment polarities of other words.",
        "Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method.",
        "In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable.",
        "The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables.",
        "Sum-product belief propagation is used for inference.",
        "Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Sentiment classification is a useful technique for analyzing subjective information in a large number of texts, and many studies have been conducted (Pang and Lee, 2008).",
        "A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002), which is widely used in topic-based text classification.",
        "In the approach, a subjective sentence is represented as a set of words in the sentence, ignoring word order and head-modifier relation between words.",
        "However, sentiment classification is different from traditional topic-based text classification.",
        "Topic-based text classification is generally a linearly separable problem ((Chakrabarti, 2002), p.168).",
        "For example, when a document contains some domain-specific words, the document will probably belong to the domain.",
        "However, in sentiment classification, sentiment polarities can be reversed.",
        "For example, let us consider the sentence \"The medicine kills cancer cells.\"",
        "While the phrase cancer cells has negative polarity, the word kills reverses the polarity, and the whole sentence has positive polarity.",
        "Thus, in sentiment classification, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently.",
        "Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008).",
        "However, these methods are based on flat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence.",
        "Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data.",
        "Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful.",
        "Wilson et al.",
        "(2005) proposed a method for sentiment classification which utilizes head-modifier relation and machine learning.",
        "However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled.",
        "One issue of the approach to use sentence composition and machine learning is that only the whole sentence is labeled with its polarity in general corpora for sentiment classification, and each component of the sentence is not labeled, though such information is necessary for supervised ma-",
        "Whole Dependency Tree cancer and heart disease.",
        "Polarities of Dependency Subtrees cancer and heart disease.",
        "chine learning to infer the sentence polarity from its components.",
        "In this paper, we propose a dependency tree-based method for Japanese and English sentiment classification using conditional random fields (CRFs) with hidden variables.",
        "In the method, the sentiment polarity of each dependency subtree, which is not observable in training data, is represented by a hidden variable.",
        "The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables.",
        "The rest of this paper is organized as follows: Section 2 describes a dependency tree-based method for sentiment classification using CRFs with hidden variables, and Section 3 shows experimental results on Japanese and English corpora.",
        "Section 4 discusses related work, and Section 5 gives conclusions.",
        "2 Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables",
        "In this study, we handle a task to classify the polarities (positive or negative) of given subjective sentences.",
        "In the rest of this section, we describe a probabilistic model for sentiment classification based on dependency trees, methods for inference and parameter estimation, and features we use.",
        "Let us consider the subjective sentence \"It prevents cancer and heart disease.\"",
        "In the sentence, cancer and heart disease have themselves negative polarities.",
        "However, the polarities are reversed by modifying the word prevents, and the dependency subtree \"prevents cancer and heart disease\" has positive polarity.",
        "As a result, the whole dependency tree \"It prevents cancer and heart disease.\"",
        "has positive polarity (Figure 1).",
        "In such a way, we can consider the sentiment polarity for each dependency subtree of a subjective sentence.",
        "Note that we use phrases as a basic unit instead of words in this study, because phrases are useful as a meaningful unit for sentiment classification.",
        "In this paper, a dependency subtree means the subtree of a dependency tree whose root node is one of the phrases in the sentence.",
        "We use a probabilistic model as shown in Figure 2.",
        "We consider that each phrase in the subjective sentence has a random variable (indicated by a circle in Figure 2).",
        "The random variable represents the polarity of the dependency subtree whose root node is the corresponding phrase.",
        "Two random variables are dependent (indicated by an edge in Figure 2) if their corresponding phrases have head-modifier relation in the dependency tree.",
        "The node denoted as <root> in Figure 2 indicates a virtual phrase which represents the root node of the sentence, and we regard that the random variable of the root node is the polarity of the whole sentence.",
        "In usual annotated corpora for sentiment classification, only each sentence is labeled with its polarity, and each phrase (dependency subtree) is not labeled, so all the random variables except the one for the root node are hidden variables that cannot be observed in labeled data (indicated by gray circles in Figure 2).",
        "With such a probabilistic model, it is possible to utilize properties such that phrases which contain positive (or negative) words tend to have positive (negative) polarities, and two phrases with head-modifier relation tend to have opposite polarities if the head contains a word which reverses sentiment polarity.",
        "Next, we define the probabilistic model as shown in Figure 2 in detail.",
        "Let n denote the number of phrases in a subjective sentence, wi the i-th phrase, and hi the head index of the i-th phrase.",
        "Let si denote the random variable which represents the polarity of the dependency subtree whose root is the i-th phrase (si G {+1, – 1}), and let p denote the polarity of the whole sentence (p G {+1, – 1}).",
        "We regard the 0-th phrase as a virtual phrase which represents the root of the sentence.",
        "w, h, s respectively denote the sequence of Wi, hi, si.",
        "W = Wi ■■■ Wn, h = hi ■■■ hn, S = So ■■■ Sn, P = So.",
        "For the example sentence in Figure 1, w1 =It, w2 =prevents, w3 =cancer, w4 =and heart disease., h1 = 2, h2 = 0, h3 = 2, h4 = 2.",
        "We define the joint probability distribution of the sentiment polarities of dependency subtrees s, given a subjective sentence w and its dependency tree h, using loglinear models:",
        "where A = {X1, ■ ■ ■ ,XK} is the set of parameters of the model.",
        "fk(i, w, h, s) is the feature function of the i-th phrase, and is classified to node feature which considers only the corresponding node, or edge feature which considers both the corresponding node and its head, as follows:",
        "where Kn and Ke respectively represent the sets of indices of node features and edge features.",
        "Let us consider how to infer the sentiment polarity p G {+1, – 1}, given a subjective sentence w and its dependency tree h. The polarity of the root node (s0) is regarded as the polarity of the whole sentence, and p can be calculated as follows:",
        "p=argmax Pa(p'\\w, h), Pa(p\\w, h)= E Pa(s\\w, h).",
        "That is, the polarity of the subjective sentence is obtained as the marginal probability of the root node polarity, by summing the probabilities for all the possible configurations of hidden variables.",
        "However, enumerating all the possible configurations of hidden variables is computationally hard, and we use sum-product belief propagation (MacKay, 2003) for the calculation.",
        "Belief propagation enables us to efficiently calculate marginal probabilities.",
        "In this study, the graphical model to be solved has a tree structure (identical to the syntactic dependency tree) which has no loops, and an exact solution can be obtained using belief propagation.",
        "Dependencies among random variables in Figure 2 are represented by a factor graph in Figure 3.",
        "The factor graph consists of variable nodes Si indicated by circles, and factor (feature) nodes gi indicated by squares.",
        "In the example in Figure 3, gi(1 < i < 4) correspond to the node features in Equation (4), and gi(5 < i < 8) correspond to the edge features.",
        "In belief propagation, marginal distribution is calculated by passing messages (beliefs) among the variables and factors connected by edges in the factor graph (Refer to (MacKay, 2003) for detailed description of belief propagation).",
        "Let us consider how to estimate model parameters A, given L training examples D = {(wl, hl,pl)}f=1.",
        "In this study, we use the maximum a posteriori estimation with Gaussian priors for parameter estimation.",
        "We define the following objective function La, and calculate the parameters A which maximize the value:",
        "A=argmax La, where a is a parameter of Gaussian priors and is set to 1.0 in later experiments.",
        "The partial derivatives of La are as follows:",
        "The model parameters can be calculated with the L-BFGS quasi-Newton method (Liu and Nocedal, 1989) using the objective function and its partial derivatives.",
        "While the partial derivatives contain summation over all the possible configurations of hidden variables, it can be calculated efficiently using belief propagation as explained in Section 2.2.",
        "This parameter estimation method is same to one used for Latent-Dynamic Conditional Random Field (Morency et al., 2007).",
        "Note that the objective function La is not convex, and there is no guarantee for global optimality.",
        "The estimated model parameters depend on the initial values of the parameters, and the setting of the initial values of model parameters will be explained in Section 2.4.",
        "Table 1 shows the features used in this study.",
        "Features (a)-(h) in Table 1 are used as the node features (Equation (4)) for the i-th phrase, and features (A)-(E) are used as the edge features for the i-th and j-th phrases (j=hi).",
        "In Table 1, Si denotes the hidden variable which represents the polarity of the dependency subtree whose root node is the ith phrase, qi denotes the prior polarity of the i-th phrase (explained later), ri denotes the polarity reversal of the i-th phrase (explained later), mi denotes the number of words in the i-th phrase, ui}k, bik, ci,k, fi,k respectively denote the surface form, base form, coarse-grained part-of-speech (POS) tag,",
        "Node Features",
        "Si&qi Si&qi&ri;Edge Features",
        "Si&Sj &rj Si&Sj &rj &qj Si&Sj&bi, 1, ••• , Si&Sj&bi, mi Si&Sj&bj, 1, ••• , Si&Sj&bj, m.",
        "fine-grained POS tag of the k-th word in the i-th phrase.",
        "We used the morphological analysis system JU-MAN and the dependency parser KNP for processing Japanese data, and the POS tagger MX-POST (Ratnaparkhi, 1996) and the dependency parser MaltParser for English data.",
        "KNP outputs phrase-based dependency trees, but MaltParser outputs word-based dependency trees, and we converted the word-based ones to phrase-based ones using simple heuristic rules explained in Appendix A.",
        "The prior polarity of a phrase qi G {+1, 0, – 1} is the innate sentiment polarity of a word contained in the phrase, which can be obtained from sentiment polarity dictionaries.",
        "We used sentiment polarity dictionaries made by Kobayashi et al.",
        "(2007) and Hi-gashiyama et al.",
        "(2008) for Japanese experiments (The resulting dictionary contains 6,974 positive expressions and 8,428 negative expressions), and a dictionary made by Wilson et al.",
        "(2005) for English experiments (The dictionary contains 2,289 positive expressions and 4,143 negative expressions).",
        "When a phrase contains the words registered in the dictionaries, its prior polarity is set to the registered polarity, otherwise the prior polarity is set to 0.",
        "When a phrase contains multiple words in the dictionaries, the registered polarity of the last (nearest to the end of the sentence) word is used.",
        "The polarity reversal of a phrase ri G {0,1} represents whether it reverses the polarities of other phrases (1) of not (0).",
        "We prepared polarity reversing word dictionaries, and the polarity reversal of a phrase is set to 1 if the phrase contains a word in the dictionaries, otherwise set to 0.",
        "We constructed polarity reversing word dictionaries which contain such words as decrease and vanish that reverse sentiment polarity.",
        "A Japanese polarity reversing word dictionary was constructed from an automatically constructed corpus, and the construction procedure is described in Appendix B (The dictionary contains 219 polarity reversing words).",
        "An English polarity reversing word dictionary was constructed from the General Inquirer dictionary in the same way as Choi and Cardie (2008), by collecting words which belong to either NotLw or Decreas categories (The dictionary contains 121 polarity reversing words).",
        "Choi and Cardie (2008) categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate.",
        "The polarity reversal of a phrase ri explained above handles only the content-word negators, and function-word negators are handled in another way, since the scope of a function-word negator is generally limited to the phrase containing it in Japanese, and the number of function-word negators is small.",
        "The prior polarity qi and the polarity reversal ri of a phrase are changed to the following qi and ri, if the phrase contains a function-word negator (in Japanese) or if the phrase is modified by a function-word negator (in English):",
        "In this paper, unless otherwise noted, the word polarity reversal is used to indicate polarity reversing caused by content-word negators, and function-word negators are assumed to be applied to qi and ri in the above way beforehand.",
        "As described in Section 2.3, there is no guarantee of global optimality for estimated parameters, since the objective function is not convex.",
        "In our preliminary experiments, L-BFGS often did not converge and classification accuracy was unstable when the initial values of parameters were randomly set.",
        "Therefore, in later experiments, we set the initial values in the following way.",
        "For the feature (A) in Table 1 in which Si and Sj are equal, we set the initial parameter \\i of the feature to a random number in [0.9,1.1], otherwise we set to a random number in [ – 0.1, 0.1].",
        "By setting suchinitial values, the initial model parameters have a property that two phrases with head-modifier relation tend to have the same polarity, which is intuitively reasonable."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "We conducted experiments of sentiment classification on four Japanese corpora and four English corpora.",
        "We used four corpora for experiments of Japanese sentiment classification: the Automatically Constructed Polarity-tagged corpus (ACP) (Kaji and Kitsuregawa, 2006), the Kyoto University and NTT Blog corpus (KNB) , the NTCIR Japanese opinion corpus (NTC-J) (Seki et al., 2007; Seki et al., 2008), the 50 Topics Evaluative Information corpus (50 Topics) (Nakagawa et al., 2008).",
        "The ACP corpus is an automatically constructed corpus from HTML documents on the Web using lexico-syntactic patterns and layout structures.",
        "The size of the corpus is large (it consists of 650,951 instances), and we used 1/100 of the whole corpus.",
        "The KNB corpus consists of Japanese blogs, and is manually annotated.",
        "The NTC-J corpus consists of Japanese newspaper articles.",
        "There are two NTCIR Japanese opinion corpora available, the NTCIR-6 corpus and the NTCIR-7 corpus; and we combined the two corpora.",
        "The 50 Topics corpus is collected from various pages on the Web, and is manually annotated.",
        "We used four corpora for experiments of English sentiment classification: the Customer Review data (CR), the MPQA Opinion corpus (MPQA), the Movie Review Data (MR) , and the NTCIR English opinion corpus (NTC-E) (Seki et al., 2007; Seki et al., 2008).",
        "The CR corpus consists of review articles about products such as digital cameras and cellular phones.",
        "There are two customer review datasets, the 5 products dataset and the 9 products dataset, and we combined the two datasets.",
        "In the MPQA corpus, sentiment polarities are attached not to sentences but expressions (sub-sentences), and we regarded the expressions as sentences and classified the polarities.",
        "There are two NTCIR English corpora available, the NTCIR-6 corpus and the NTCIR-7 corpus, and we combined the two corpora.",
        "The statistical information of the corpora we used is shown in Table 2.",
        "We randomly split each corpus into 10 portions, and conducted 10-fold cross validation.",
        "Accuracy of sentiment classification was calculated as the number of correctly predicted labels (polarities) divided by the number of test examples.",
        "We compared our method to 6 baseline methods, and this section describes them.",
        "In the following, p0 € {+1, – 1} denotes the major polarity in training data, Hi denotes the set consisting of all the ancestor nodes of the i-th phrase in the dependency tree, and sgn(x) is defined as below:",
        "f+1 (x> 0), sgn(x) = < 0 (x = 0), [ – 1 (x< o).",
        "Voting without Polarity Reversal The polarity of a subjective sentence is decided by voting of each phrase's prior polarity.",
        "In the case of a tie, the major polarity in the training data is adopted.",
        "Voting with Polarity Reversal Same to Voting without Polarity Reversal, except that the polarities of phrases which have odd numbers of reversal phrases in their ancestors are reversed before voting.",
        "Rule The polarity of a subjective sentence is deterministically decided basing on rules, by considering the sentiment polarities of dependency subtrees.",
        "The polarity of the dependency subtree whose root is the i-th phrase is decided by voting the prior polarity of the i-th phrase and the polarities of the dependency subtrees whose root nodes are the modifiers of the i-th phrase.",
        "The polarities of the modifiers are reversed if their head phrase has a reversal word.",
        "The decision rule is applied from leaf nodes in the dependency tree, and the polarity of the root node is decided at the last.",
        "Bag-of-Features with No Dictionaries The polarity of a subjective sentence is classified using Support Vector Machines.",
        "Surface forms, base forms, coarse-grained POS tags and finegrained POS tags of word unigrams and bi-grams in the subjective sentence are used as features.",
        "The second order polynomial kernel is used and the cost parameter C is set to 1 .",
        "0 .",
        "No prior polarity information (dictionary) is used.",
        "Bag-of-Features without Polarity Reversal Same to Bag-of-Features with No Dictionaries, except that the voting result of prior polarities (one of positive, negative or tie) is also used as a feature.",
        "Bag-of-Features with Polarity Reversal Same to Bag-of-Features without Polarity Reversal, except that the polarities of phrases which have (* indicates statistical significance at p < 0.05) odd numbers of reversal phrases in their ancestors are reversed before voting.",
        "Tree-CRF The proposed method based on dependency trees using CRFs, described in Section 2.",
        "The experimental results are shown in Table 3.",
        "The proposed method Tree-CRF obtained the best accuracies for all the four Japanese corpora and the four English corpora, and the differences against the second best methods were statistically significant (p < 0.05) with the paired t-test for the six of the eight corpora.",
        "Tree-CRF performed better for the Japanese corpora than for the English corpora.",
        "For both the Voting methods and the Bag-of-Features methods, the methods with polarity reversal performed better than those without it.",
        "Both BoF-w/ Rev.",
        "and Tree-CRF use supervised machine learning and the same dictionaries (the prior polarity dictionaries and the polarity reversing word dictionaries), but the latter performed better than the former.",
        "Our error analysis showed that BoF-w/ Rev.",
        "was not robust for erroneous words in the prior polarity dictionaries.",
        "BoF-w/ Rev.",
        "uses the voting result of the prior polarities as a feature, and the feature is sensitive to the errors in the dictionary, while Tree-CRF uses several information as well as the prior polarities to decide the polarities of dependency subtrees, and was robust to the dictionary errors.",
        "We investigated the trained model parameters of Tree-CRF, and found that the features (E) in Table 1, in which the head and the modifier have opposite polarities and the head word is such as protect and withdraw, have large positive weights.",
        "Although these words were not included in the polarity reversing word dictionary, the property that these words reverse polarities of other words seems to be learned with the model."
      ]
    },
    {
      "heading": "4. Related Work",
      "text": [
        "Various studies on sentiment classification have been conducted, and there are several methods proposed for handling reversal of polarities.",
        "In this paper, our method was not directly compared with the other methods, since it is difficult to completely implement them or conduct experiments with exactly the same settings.",
        "Language",
        "Corpus",
        "Number of Instances",
        "(Positive / Negative)",
        "ACP",
        "6,510",
        "(2,738 / 3,772)",
        "Japanese",
        "KNB",
        "2,288",
        "(1,423/ 865)",
        "NTC-J",
        "3,485",
        "(1,083/2,402)",
        "50 Topics",
        "5,366",
        "(3,175/2,191)",
        "CR",
        "3,772",
        "(2,406 / 1,366)",
        "English",
        "MPQA",
        "10,624",
        "(3,316/7,308)",
        "MR",
        "10,662",
        "(5,331 / 5,331)",
        "NTC-E",
        "3,812",
        "(1,226/2,586)",
        "Method",
        "Japanese",
        "English",
        "ACP",
        "KNB",
        "NTC-J",
        "50 Topics",
        "CR",
        "MPQA",
        "MR",
        "NTC-E",
        "Voting-w/o Rev.",
        "0.686",
        "0.764",
        "0.665",
        "0.727",
        "0.714",
        "0.804",
        "0.629",
        "0.730",
        "Voting-w/ Rev.",
        "0.732",
        "0.792",
        "0.714",
        "0.765",
        "0.742",
        "0.817",
        "0.631",
        "0.740",
        "Rule",
        "0.734",
        "0.792",
        "0.742",
        "0.764",
        "0.743",
        "0.818",
        "0.629",
        "0.750",
        "BoF-no Dic.",
        "0.798",
        "0.758",
        "0.754",
        "0.761",
        "0.793",
        "0.818",
        "0.757",
        "0.768",
        "BoF-w/o Rev.",
        "0.812",
        "0.823",
        "0.794",
        "0.805",
        "0.802",
        "0.840",
        "0.761",
        "0.793",
        "BoF-w/ Rev.",
        "0.822",
        "0.830",
        "0.804",
        "0.819",
        "0.814",
        "0.841",
        "0.764",
        "0.797",
        "Tree-CRF",
        "0.846*",
        "0.847*",
        "0.826*",
        "0.841*",
        "0.814",
        "0.861*",
        "0.773*",
        "0.804",
        "Choi and Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics.",
        "In their method, the polarity of the whole sentence is determined from the prior polarities of the composing words by predefined rules, and the method differs from ours which uses the probabilistic model to handle interactions between hidden variables.",
        "Syntactic structures were used in the studies of Moilanen and Pulman (2007) and, Jia et al.",
        "(2009), but their methods are based on rules and supervised learning was not used to handle polarity reversal.",
        "As discussed in Section 1, Wilson et al.",
        "(2005) studied a bag-of-features based statistical sentiment classification method incorporating head-modifier relation.",
        "Ikeda et al.",
        "(2008) proposed a machine learning approach to handle sentiment polarity reversal.",
        "For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features.",
        "The method can handle only words with prior polarities, and does not use syntactic dependency structures.",
        "Conditional random fields with hidden variables have been studied so far for other tasks.",
        "Latent-Dynamic Conditional Random Fields (LDCRF) (Morency et al., 2007; Sun et al., 2008) are probabilistic models with hidden variables for sequential labeling, and belief propagation is used for inference.",
        "Out method is similar to the models, but there are several differences.",
        "In our method, only one variable which represents the polarity of the whole sentence is observable, and dependency relation among random variables is not a linear chain but a tree structure which is identical to the syntactic dependency."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, we presented a dependency tree-based method for sentiment classification using conditional random fields with hidden variables.",
        "In this method, the polarity of each dependency subtree of a subjective sentence is represented by a hidden variable.",
        "The values of the hidden variables are calculated in consideration of interactions between variables whose nodes have head-modifier relation in the dependency tree.",
        "The value of the hidden variable of the root node is identified with the polarity of the whole sentence.",
        "Experimental results showed that the proposed method performs better for Japanese and English data than the baseline methods which represents subjective sentences as bag-of-features.",
        "A Rules for Converting Word Sequence to Phrase Sequence",
        "Let v1} • • • ,vN denote an English word sequence, yithe part-of-speech of the i-th word, and zi the head index of the i-th word.",
        "The word sequence was converted to a phrase sequence as follows, by applying",
        "B Construction of Japanese Polarity Reversing Word Dictionary",
        "We constructed a Japanese polarity reversing word dictionary from the Automatically Constructed Polarity-tagged corpus (Kaji and Kitsuregawa, 2006).",
        "First, we collected sentences, each of which contains just one phrase having prior polarity, and the phrase modifies a phrase which modifies the root node.",
        "Among them, we selected sentences in which the prior polarity is not equal to the polarity of the whole sentence.",
        "We extracted all the words in the head phrase, and manually checked them whether they should be put into the dictionary or not.",
        "The rationale behind the procedure is that the prior polarity can be considered to be reversed by a certain word in the head phrase.",
        "rules which combine two adjacent words:",
        "NN = {CD,FW,NN,NNP,NNPS,NNS,SYM,JJ} do if xi and xi+1 are not yet combined A",
        "(xi e LT V xi+i e RT V",
        "Combine the words viand vi+1until No rules are applied"
      ]
    }
  ]
}
