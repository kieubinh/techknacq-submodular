{
  "info": {
    "authors": [
      "Michael Wiegand",
      "Dietrich Klakow"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1121",
    "title": "Convolution Kernels for Opinion Holder Extraction",
    "url": "https://aclweb.org/anthology/N10-1121",
    "year": 2010
  },
  "references": [
    "acl-D09-1143",
    "acl-H05-1044",
    "acl-H05-1045",
    "acl-J08-2003",
    "acl-N06-1037",
    "acl-P02-1034",
    "acl-P03-1054",
    "acl-P05-1045",
    "acl-W06-0301",
    "acl-W06-1651",
    "acl-W08-2126"
  ],
  "sections": [
    {
      "text": [
        "Michael Wiegand and Dietrich Klakow",
        "Spoken Language Systems Saarland University D-66123 Saarbrücken, Germany {Michael.Wiegand jDietrich.Klakowj@lsv.uni - Saarland.de",
        "Opinion holder extraction is one of the important subtasks in sentiment analysis.",
        "The effective detection of an opinion holder depends on the consideration of various cues on various levels of representation, though they are hard to formulate explicitly as features.",
        "In this work, we propose to use convolution kernels for that task which identify meaningful fragments of sequences or trees by themselves.",
        "We not only investigate how different levels of information can be effectively combined in different kernels but also examine how the scope of these kernels should be chosen.",
        "In general relation extraction, the two candidate entities thought to be involved in a relation are commonly chosen to be the boundaries of sequences and trees.",
        "The definition of boundaries in opinion holder extraction, however, is less straightforward since there might be several expressions beside the candidate opinion holder to be eligible for being a boundary."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In recent years, there has been a growing interest in the automatic detection of opinionated content in natural language text.",
        "One of the more important tasks in sentiment analysis is the extraction of opinion holders.",
        "Opinion holder extraction is one of the critical components of an opinion question-answering system (i.e. systems which automatically answer opinion questions, such as \"What does [X] like about [Y]?\").",
        "Such systems need to be able to distinguish which entities in a candidate answer sentence are the sources of opinions (= opinion holder) and which are the targets.",
        "On other NLP tasks, in particular, on relation extraction, there has been much work on convolution kernels, i.e. kernel functions exploiting huge amounts of features without an explicit feature representation.",
        "Previous research on that task has shown that convolution kernels, such as sequence and tree kernels, are quite effective when compared to manual feature engineering (Moschitti, 2008; Bunescu and Mooney, 2005; Nguyen et al., 2009).",
        "In order to effectively use convolution kernels, it is often necessary to choose appropriate substructures of a sentence rather than represent the sentence as a whole structure (Bunescu and Mooney, 2005; Zhang et al., 2006; Moschitti, 2008).",
        "As for tree kernels, for example, one typically chooses the syntactic subtree immediately enclosing two entities potentially expressing a specific relation in a given sentence.",
        "The opinion holder detection task is different from this scenario.",
        "There can be several cues within a sentence to indicate the presence of a genuine opinion holder and these cues need not be member of a particular word group, e.g. they can be opinion words (see Sentences 1-3), communication words, such as maintained in Sentence 2, or other lexical cues, such as according in Sentence 3.",
        "1.",
        "The U.S. commanders consideropinion the prisoners to be un-lawful_combatantsopinion as opposed to prisoners of war.",
        "2.",
        "During the summit, Koizumi maintainedcommunication a clear-cut_collaborative_stanceopinion towards the U.S. and emphasized that the President was objectiveopinion and circumspect.",
        "3.",
        "Accordingc^e to Fernandez, it was the worst_mistakeopinion in the history of the Argentine economy.",
        "Thus, the definition of boundaries of the structures for the convolution kernels is less straightforward in opinion holder extraction.",
        "The aim of this paper is to explore in how far convolution kernels can be beneficial for effective opinion holder detection.",
        "We are not only interested in how far different kernel types contribute to this extraction task but we also contrast the performance of these kernels with a manually designed feature set used as a standard vector kernel.",
        "Finally, we also examine the effectiveness of expanding word sequences or syntactic trees by additional prior knowledge."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Choi et al.",
        "(2005) examine opinion holder extraction using CRFs with various manually defined linguistic features and patterns automatically learnt by the AutoSlog system (Riloff, 1996).",
        "The linguistic features focus on named-entity information and syntactic relations to opinion words.",
        "In this paper, we use very similar settings.",
        "The features presented in Kim and Hovy (2005) and Bloom et al.",
        "(2007) resemble very much Choi et al.",
        "(2005).",
        "Bloom et al.",
        "(2007) also consider communication words to be predictive cues for opinion holders.",
        "Kim and Hovy (2006) and Bethard et al.",
        "(2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al., 2003) for both opinion holder and opinion target extraction.",
        "Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm.",
        "Choi et al.",
        "(2006) is an extension of Choi et al.",
        "(2005) in that opinion holder extraction is learnt jointly with opinion detection.",
        "This requires that opinion expressions and their relations to opinion holders are annotated in the training data.",
        "Semantic roles are also taken as a potential source of information.",
        "In our work, we deliberately work with minimal annotation and, thus, do not consider any labeled opinion expressions and relations to opinion holders in the training data.",
        "We exclusively rely on entities marked as opinion holders.",
        "In many practical situations, the annotation beyond opinion holder labeling is too expensive.",
        "Complex convolution kernels have been successfully applied to various NLP tasks, such as relation extraction (Bunescu and Mooney, 2005; Zhang et al., 2006; Nguyen et al., 2009), question answering (Zhang and Lee, 2003; Moschitti, 2008), and semantic role labeling (Moschitti et al., 2008).",
        "In all these tasks, they offer competitive performance to manually designed feature sets.",
        "Bunescu and Mooney (2005) combine different sequence kernels encoding different contexts of candidate entities in a sentence.",
        "They argue that several kernels encoding different contexts are more effective than just using one kernel with one specific context.",
        "We build on that idea and compare various scopes eligible for opinion holder extraction.",
        "Moschitti (2008) and Nguyen et al.",
        "(2009) suggest that different kinds of information, such as word sequences, part-of-speech tags, syntactic and semantic information should be contained in separate convolution kernels.",
        "We also adhere to this notion.",
        "As labeled data, we use the sentiment annotation of the MPQA 2.0 corpus.",
        "Opinion holders are not explicitly labeled as such.",
        "However sources of private states and subjective speech events (Wiebe et al., 2003) are a fairly good approximation of the task.",
        "Previous work (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006) uses similar approximations."
      ]
    },
    {
      "heading": "4. Method",
      "text": [
        "In this work, we consider all noun phrases (NPs) as possible candidate opinion holders.",
        "Therefore, the set of all data instances is the set of the NPs within the MPQA 2.0 corpus.",
        "Each NP is labeled as to whether it is a genuine opinion holder or not.",
        "Throughout this section, we will use Sentence 2 from Section 1 as an example.",
        "Several levels of representation are important for opinion holder extraction.",
        "Table 1 lists all the different levels that are used in this work.",
        "Generalized sequences employ named-entity tags, an OPINION tag for opinion words and a COMM tag for communication words.",
        "Thus, in a generalized word sequence (WRDgn) a word is replaced by a generalized token whereas in a generalized part-of-speech sequence (POSGn) a part-of-speech tag is replaced.",
        "For augmented constituent trees (CONSTaug), the same sources of information are used.",
        "The difference to generalizing sequences is that instead of replacing words by generalized tokens, we add a node in the syntax tree with a generalized token so that it dominates the pertaining leaf node (see also nodes marked with aug in Figure 2).",
        "All sources used for this type of generalization are known to be predictive for opinion holder classification (Choi et al., 2005; Kim and Hovy, 2005; Choi et al., 2006; Kim and",
        "Hovy, 2006; Bloom et al., 2007).",
        "Note that the grammatical relation paths, i.e. GRAMwrd and GRAMpos, can only be applied in case there is another expression in the focus in addition to the candidate of the data instance itself, e.g. the nearest opinion expression to the candidate.",
        "Section 4.4 explains in detail how this is done.",
        "Predicate-argument structures (PAS) are represented by PropBank trees (Kingsbury and Palmer, 2002).",
        "Support Vector Machines (SVMs) are one of the most robust supervised machine learning techniques in which training data instances x are separated by a hyperplane H(x) = w • x + b = 0 where w G Rnand b G R. One advantage of SVMs is that kernel methods can be applied which map the data to other feature spaces in which they can be separated more easily.",
        "Given a feature function j : O – R, where O is the set of the objects, the kernel trick allows the decision hyperplane to be rewritten as:",
        "where yi is equal to 1 for positive and – 1 for negative examples, ai G R with ai > 0, oiVi G {1,... , l j are the training instances and the product K(oi, o) = (((oi) • (j)(o)) is the kernel function associated with the mapping j.",
        "case there are several tokens making up the candidate.",
        "A sequence kernel (SK) measures the similarity of two sequences by counting the number of common subsequences.",
        "We use the kernel by Taylor and Christianini (2004) which has the advantage that it also considers subsequences of the original sequence with some elements missing.",
        "The extent of these gaps in a sequence is suitably reflected by a weighting function incorporated into the kernel.",
        "Tree kernels (TKs) represent trees by their substructures.",
        "The feature space of these substructures, or fragments, is mapped onto a vector space.",
        "The kernel function computes the similarity of pairs of trees by counting the number of common fragments.",
        "In this work, we evaluate two tree kernels: Subset Tree Kernel (STK) (Collins and Duffy, 2002) and Partial Tree Kernel (PTKb(lsic) (Moschitti, 2006).",
        "In STK, a tree fragment can be any set of nodes and edges of the original tree provided that every node has either all or none of its children.",
        "This constraint makes that kind of kernel well-suited for constituency trees which have been generated by context free grammars since the constraint corresponds to the restriction that no grammatical rule must be broken.",
        "For example, STK enforces that a subtree, such as [VP [VBZ, NP]], cannot be matched with [VP [VBZ]] since the latter VP node only possesses one of the children of the former.",
        "PTKbasic is more flexible since the constraint of STK on nodes is relaxed.",
        "This makes this type of tree kernel less suitable for constituency trees.",
        "We, therefore, apply it only to trees representing predicate-argument structures (PAS) (see Figure 1).",
        "Note that a data instance is represented by a set of those structures rather than a single structure.",
        "Thus, the actual partial tree kernel function we use for this task, PTK, sums over all possible pairs PASl and PASm of two data instances xi and xj: PTK(xi,xj) = PTKbasic (PASi ,PASm ).",
        "PASiExi PASmexj",
        "To summarize, Table 2 lists the different kernel types we use coupled with the suitable levels of representation.",
        "This choice of pairing has already been motivated and empirically proven suitable on other",
        "Table 1 : The different levels of representation.",
        "(b) augmented Figure 1: Predicate-argument structures (P AS).",
        "tasks (Moschitti, 2008; Nguyen et al., 2009).",
        "We argue that using the entire word sequence or syntax tree of the sentence in which a candidate opinion holder is situated to represent a data instance produces too large structures for a convolution kernel.",
        "Since a classifier based on convolution kernels has to derive meaningful features by itself, the larger these structures are, the more likely noise is included in the model.",
        "Previous work in relation extraction has also shown that the usage of more focused substructures, e.g. the smallest subtree containing the two candidate entities of a relation, is more effective (Zhang et al., 2006).",
        "Unfortunately, in our task there is only one explicit entity we know of for each data instance which is the candidate opinion holder.",
        "However, there are several indicative cues within the context of the candidate which might be considered important.",
        "We identify three different cues being the nearest predicate, i.e. full verb or nominalization, opinion word and communication word.",
        "For each of these expressions, we define a scope where the boundaries are the candidate opinion holder and the pertaining cue.",
        "Given these scopes, we can define resulting subsequences/subtrees and combine them.",
        "We further add two background scopes, one being the semantic scope of the candidate opinion holder and the entire sentence.",
        "As semantic scope we consider the subclause in which a candidate opinion holder is situated .",
        "Figure 2 illustrates the different scopes.",
        "Abbreviations are explained in Table 3.",
        "As already mentioned in Section 4.1 for grammatical relation paths, a second expression in addition to the candidate opinion holder is required.",
        "These expressions can be derived from the different scopes, i.e. for PRED it is the nearest predicate to the candidate, for OP it is the nearest opinion word and for COMM it is the nearest communication word.",
        "For the background scopes S EM and SENT, however, there is no second expression in focus.",
        "Therefore, grammatical relation paths cannot be defined for these scopes.",
        "Type",
        "Description",
        "Example",
        "WRD",
        "sequence of words",
        "During the summit, Koizumic^jvj) maintained a clear-cut collaborative stance ...",
        "WRDGN",
        "sequence of generalized words",
        "During the summit, CAND COMM OPINION ...",
        "POS",
        "part-of-speech sequence",
        "IN DET NN PUNC CAND VBD DET JJ JJ NN ...",
        "POSGN",
        "generalized part-of-speech sequence",
        "IN DET NN PUNC CAND COMM OPINION ...",
        "CONST",
        "constituency tree",
        "see Figure 2 without nodes marked aug",
        "CONSTaug",
        "augmented constituency tree",
        "see Figure 2",
        "GRAMWRD",
        "grammatical relation path labels with words",
        "Koizumic^jvj) NSUBJf maintained DOBJJ.",
        "stance",
        "GRAMpos",
        "grammatical relation path labels with part-of-speech tags",
        "CAND NSUBJÎ VBD DOBJJ.",
        "NN",
        "PAS",
        "predicate argument structures",
        "see Figure 1(a)",
        "PASAUG",
        "augmented predicate argument structures",
        "see Figure 1(b)",
        "Type",
        "Description",
        "Levels of Representation",
        "SK",
        "Sequential Kernel",
        "WRD(GN), POS(GN), GRAMWRD, GRAMpos",
        "STK",
        "Subset Tree Kernel",
        "CONST(aug)",
        "PTK",
        "Partial Tree Kernel",
        "PAS",
        "VK",
        "Vector Kernel",
        "not restricted",
        "Table 3 : The different types of scope.",
        "In addition to the different types of convolution kernels, we also define an explicit feature set for a vector kernel (VK).",
        "Many of these features mainly describe properties of the relation between the candidate and the nearest predicate since in our initial experiments the nearest predicate has always been the strongest cue.",
        "Adding these types of features for other cues, e.g. the nearest opinion or communication word, only resulted in a decrease in performance.",
        "Table 4 lists all the features we use.",
        "Note that this manual feature set employs all those sources of information which are also exploited by the convolution kernels.",
        "Some of the information contained in the convolution kernels can, however, only be represented in a more simplified fashion when using a manual feature set.",
        "For example, the first PAS in Figure 1(a) is converted to just the pair of predicate and argument representing the candidate (i.e. REL: maintain JlO: Koizumi).",
        "The entire PAS is not used since it would create too sparse features.",
        "Convolution kernels can cope with fairly complex structures as input since they internally match substructures.",
        "Manual features are less flexible since they do not account for partial matches.",
        "headword/governing category of CAND is CAND capitalized/a person?",
        "is CAND subj\\dobj\\iobj\\pobj of OPINION/COMM?",
        "is CAND preceded by according to?",
        "(Choi et al., 2005) does CAND contain possessive and is followed by OPINION/COMM?",
        "(Choi et al., 2005) is CAND preceded by by which is attached to OPINION/COMM?",
        "(Choi et al., 2005) predicate-argument pairs in which CAND occurs lemma/part-of-speech tag/subcategorization frame/voice of nearest predicate is nearest predicate OPINION/COMM?",
        "does CAND precede/follow nearest predicate?",
        "words between nearest predicate and CAND (bag of words) part-of-speech sequence between nearest predicate and CAND constituency path/grammatical relation path from predicate to CAND",
        "We used 400 documents of the MPQA corpus for fivefold crossvalidation and 133 documents as a development set.",
        "We report statistical significance on the basis of a paired t-test using 0.05 as the significance level.",
        "All experiments were done with the SVM-Light-TK toolkit.",
        "We evaluated on the basis of exact phrase matching.",
        "We set the trade-off parameter j = 5 for all feature sets.",
        "For the manual feature set we used a polynomial kernel of third degree.",
        "These two critical parameters were tuned on the development set.",
        "As far as the sequence and tree kernels are concerned, we used the parameter settings from Moschitti (2008), i.e. X = 0.4 and ß = 0.4.",
        "Kernels were combined using plain summation.",
        "The documents were parsed using the Stanford Parser (Klein and Manning, 2003).",
        "Named-entity information was obtained by the Stanford tagger (Finkel et al., 2005).",
        "Semantic roles were obtained by using the parser by Zhang et al.",
        "(2008).",
        "Opinion expressions were identified using the Subjectivity Lexicon from the MPQA project (Wilson et al., 2005).",
        "Communication words were obtained by using the Appraisal Lexicon (Bloom et al., 2007).",
        "Nominalizations were recognized by looking",
        "relates to the candidate opinion holder.",
        "Type",
        "Description",
        "PRED",
        "scope with the boundaries being the candidate opinion holder and the nearest predicate",
        "OP",
        "scope with the boundaries being the candidate opinion holder and nearest opinion word",
        "COMM",
        "scope with the boundaries being the candidate opinion holder and the nearest communication word",
        "SEM",
        "semantic scope of the candidate opinion holder, i.e. subclause containing the candidate",
        "SENT",
        "entire sentence in which in the opinion holder occurs",
        "Figure 2: Illustration of the different scopes on a CONSTaug; nodes belonging to the candidate opinion holder are marked with CAN D.",
        "up nouns in NOMLEX (Macleod et al., 1998).",
        "(levelOfRepresentation (Table 1), Scope (Table 3), typeOfKernel (Table 2)>, e.g. (CONST, SENT, STK) is a Subset Tree Kernel of a constituency parse having the scope of the entire sentence.",
        "Note that not all combinations of these three parameters are meaningful.",
        "In the following, we will just focus on important and effective combinations.",
        "The kernel composed of manually designed features is denoted by just VK.",
        "The kernel composed of predicate-argument structures is denoted by (PAS, SENT, PTK).",
        "The first line in Table 7 displays the result of the vector kernel using a manually designed feature set.",
        "It should be interpreted as a baseline.",
        "Due to the high class imbalance we will focus on the comparison of F(1)-Score throughout this paper rather than accuracy which is fairly biased on this data set.",
        "The F-Score of this classifier is at 56.16%.",
        "For both sequence and tree kernels we need to find out what the best scope is, whether it is worthwhile to combine different scopes and what different layers of representation can be usefully combined.",
        "The upper part ofTable 5 lists the results ofsimple word kernels using the different scopes.",
        "The performance of the kernels using individual scopes varies greatly.",
        "The best scope is PRED (1), the second best is S EM (2).",
        "The good performance of PRED does not come as a surprise since the sequence is the smallest among the different scopes, so this scope is least affected by data sparseness.",
        "Moreover, this result is consistent with our initial experiments on the manual feature set (see Section 4.5).",
        "Using different combinations of the word sequence kernels shows that PRED and SEM (6) are a good combination, whereas OP, COMM, and SENT (7;8;9) do not positively contribute to the overall performance which is consistent which the individual scope evaluation.",
        "Apparently, these scopes capture less linguistically relevant structure.",
        "The next part ofTable 5 shows the contribution of POS kernels when added to WRD kernels.",
        "Adding the corresponding POS kernel to the WRD kernel with PRED scope (10) results in an improvement by more than 5% in F-Score.",
        "We get another improvement by approx.",
        "3% when the corresponding S EM kernels (11) are added.",
        "This suggests that POS is an effective generalization and that the two scopes PRED and SEM are complementary.",
        "For the GRAMwrd kernel, the PRED scope (12) is again most effective.",
        "We assume that this kernel most likely expresses meaningful syntactic relationships for our task.",
        "Adding the GRAMpos kernel (14) gives another boost by almost 4%.",
        "Generalized sequence kernels are important.",
        "Adding the corresponding WRDgn kernels to the WRD kernel with PRED and SEM scope results in an improvement from 47.77% (1) to 53.00% (15) which is a bit less than the combination of WRD and POS(GN) kernels (16).",
        "However, these types of kernels seem to be complementary since their combination provides an F-Score of 56.06% (17).",
        "This kernel combination already performs on a par with the manually designed vector kernel though less information is taken into consideration.",
        "Finally, the best combination of sequence kernels (18) comprises WRD, WRDgn, POS, and POSgn kernels with PRED and SEM scope combined with a GRAMwrd and a GRAMposkernel with PRED scope.",
        "The performance of 58.70% significantly outperforms the vector kernel.",
        "Table 6 shows the results of the different tree kernels.",
        "The table is divided into two halves.",
        "The left half (A) are plain tree kernels, whereas the right half (B) are the augmented tree kernels.",
        "As far as CONST kernels are concerned, there is a systematic improvement by approximately 2% using tree augmentation.",
        "This proves that further non-syntactic knowledge added to the tree itself results in an improved F-Score.",
        "However, tree augmentation does not have any impact on the PAS kernels.",
        "The overall performance of the tree kernels shows that they are much more expressive than sequence kernels.",
        "For instance, in order to obtain the same performance as of (CONSTaug, PRED, STK) (19B), i.e. a single kernel with an F-Score 56.52, it requires several sequence kernels, hence much more effort.",
        "The performance of the different CONST kernels relative to each other resembles the results of the WRD kernels.",
        "The best scope is PRED (19).",
        "By far the worst performance is obtained by the SENT scope (23).",
        "The combination of PRED and SEM scope achieves an F-Score of 59.67% (25B) which is already slightly better than the best configuration of sequence kernels (18).",
        "The performance of the PAS kernel (28A) with an F-Score of 53.51% is slightly worse than the best single plain CONST kernel (19A).",
        "The PAS kernel and the CONST kernels are complementary, since their best combination (29B) achieves an F-Score of 61.67% which is significantly better than",
        "Table 7: Results of kernel combinations (*: significantly better than best SKs; t: significantly better than best TKs; all convolution kernels are significantly better than VK).",
        "the best combination of CONST kernels (25B) or sequence kernels (18).",
        "Table 7 lists the results of the different kernel type combinations.",
        "If VK is added to the best TKs, the best SKs, or both, a slight increase in F-Score is achieved.",
        "The best performance with an F-Score of 62.61% is obtained by combining all kernels."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "In this paper, we compared convolution kernels for opinion holder extraction.",
        "We showed that, in general, a combination of two scopes, namely the scope immediately encompassing the candidate opinion holder and its nearest predicate and the subclause containing the candidate opinion holder provide best performance.",
        "Tree kernels containing constituency parse information and semantic roles achieve better performance than sequence kernels or vector kernels using a manually designed feature set.",
        "Best performance is achieved if all kernels are combined."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "Michael Wiegand was funded by the German research council DFG through the International Research Training Group \"IRTG\" between Saarland University and University ofEdinburgh.",
        "The authors would like to thank Yi Zhang for processing the MPQA corpus with his semantic-role labeling system, the researchers from the MPQA project for helping to create an opinion holder corpus, and, in particular, Alessandro Moschitti for insightful comments and suggestions.",
        "Combination",
        "Acc.",
        "Prec.",
        "Ree.",
        "Fl",
        "VK",
        "93.63",
        "53.28",
        "59.37",
        "56.16",
        "best SKs",
        "94.21",
        "57.64",
        "59.81",
        "58.70",
        "best TKs",
        "94.16",
        "56.18",
        "68.36",
        "61.67*",
        "VK + best SKs",
        "94.34",
        "58.44",
        "61.27",
        "59.82*",
        "VK + best TKs",
        "94.33",
        "57.41",
        "68.03",
        "62.27*",
        "best SKs + best TKs",
        "94.49",
        "59.22",
        "63.96",
        "61.49*",
        "VK + best SKs + best TKs",
        "94.53",
        "59.10",
        "66.57",
        "62.61*t",
        "ID",
        "Kemel",
        "Acc.",
        "Prec.",
        "Ree.",
        "Fl",
        "1",
        "(WRD, PRED, SK)",
        "93.25",
        "51.08",
        "42.29",
        "46.26",
        "2",
        "(WRD, OP, SK)",
        "92.77",
        "46.38",
        "32.52",
        "38.21",
        "3",
        "(WRD, COMM, SK)",
        "92.42",
        "43.70",
        "35.99",
        "39.46",
        "4",
        "(WRD, SEM, SK)",
        "93.16",
        "50.32",
        "34.65",
        "41.04",
        "5",
        "(WRD, SENT, SK)",
        "90.60",
        "29.90",
        "27.29",
        "28.53",
        "6",
        "(WRD, PRED, SK) + (WRD, SEM, SK)",
        "93.78",
        "56.55",
        "41.36",
        "47.77",
        "7",
        "^jE{pred,op,comm}(WRD, j, SK)",
        "93.55",
        "54.26",
        "39.50",
        "45.71",
        "8",
        "T,jeScoP.s\\senT{WRD,j,SK)",
        "93.82",
        "57.21",
        "40.28",
        "47.26",
        "9",
        "T,jeScopes{WRD,j,SK)",
        "93.63",
        "55.15",
        "39.52",
        "46.03",
        "10",
        "(WRD, PRED, SK) + (POS, PRED, SK)",
        "93.03",
        "49.39",
        "53.53",
        "51.37",
        "11",
        "pred,sem} ( {WRD, i, SK) + (POS, i, SK))",
        "93.86",
        "55.60",
        "53.22",
        "54.38",
        "12",
        "T.ie{pred,sem}(wrd> SK) + (GRAMwrd , PRED, SK)",
        "94.01",
        "58.19",
        "45.88",
        "51.29",
        "13",
        "T.ie{pred,sem}(wrd'i'sk) + T.je{pred,op,comm}(gramwrd,j,SK)",
        "93.83",
        "56.28",
        "45.64",
        "50.40",
        "14",
        "(WRD, i, SK) + (GRAMwrd,PRED, SK) + (GRAMPOs, PRED, SK)",
        "ie{pred,sem}",
        "93.98",
        "56.59",
        "53.92",
        "55.21",
        "15",
        "T,ie{pred,sem} ({WRD, i, SK) + (WRDGN,i, SK))",
        "93.97",
        "57.08",
        "49.46",
        "53.00",
        "16",
        "T.ie{pred,sem} ((WRD,i,SK) + (POSGN,i,SK))",
        "93.97",
        "56.60",
        "52.42",
        "54.42",
        "17",
        "((WRD,i,SK) + (WRDGN,i,SK) + (POS,i,SK) + (POSGN,i, SK))",
        "ie{pred,sem}",
        "93.85",
        "55.16",
        "57.00",
        "56.06",
        "18",
        "Y ((WRD,i,SK) + (WRDGN,i,SK) + (POS,i,SK) + (POSGN,i, SK))",
        "94.21",
        "57.64",
        "59.81",
        "58.70",
        "ie{pred,sem}",
        "+(GRAMWRD,PRED, SK) + (GRAMPOs, PRED, SK)",
        "A",
        "B",
        "i=",
        "CON ST, j = PAS",
        "i = CON STAUG, j = PAS AUG",
        "ID",
        "Kemel",
        "Acc.",
        "Prec.",
        "Ree.",
        "Fl",
        "Acc.",
        "Prec.",
        "Ree.",
        "Fl",
        "19",
        "(i, PRED, STK)",
        "92.89",
        "48.68",
        "62.34",
        "54.67",
        "93.12",
        "49.99",
        "65.04",
        "56.52",
        "20",
        "(i, OP, STK)",
        "93.04",
        "49.49",
        "54.71",
        "51.96",
        "93.27",
        "50.93",
        "59.06",
        "54.68",
        "21",
        "(i, COMM, STK)",
        "92.76",
        "47.79",
        "55.89",
        "51.50",
        "92.96",
        "49.03",
        "58.85",
        "53.47",
        "22",
        "(i, SEM, STK)",
        "93.70",
        "54.40",
        "52.13",
        "53.23",
        "93.90",
        "55.47",
        "56.59",
        "56.03",
        "23",
        "(i, SENT, STK)",
        "92.42",
        "44.34",
        "39.92",
        "41.99",
        "92.50",
        "45.20",
        "42.40",
        "43.74",
        "24",
        "'Ske{PRED,OP,COMM}{i' k> STK)",
        "93.62",
        "53.26",
        "60.05",
        "56.44",
        "93.77",
        "54.06",
        "63.21",
        "58.26",
        "25",
        "!L,ke{PRED,SEM}{i< k> STK)",
        "93.90",
        "55.26",
        "59.50",
        "57.30",
        "94.13",
        "56.57",
        "63.12",
        "59.67",
        "26",
        "!L,keScopes\\SENT{i< k> STK)",
        "94.09",
        "56.65",
        "59.68",
        "58.11",
        "94.21",
        "57.21",
        "62.61",
        "59.80",
        "27",
        "J2keScopes(i^,STK)",
        "94.14",
        "57.41",
        "57.88",
        "57.63",
        "94.29",
        "58.11",
        "61.10",
        "59.56",
        "28",
        "(j, SENT, PTK)",
        "92.11",
        "45.02",
        "69.96",
        "53.51",
        "91.92",
        "44.27",
        "67.39",
        "53.43",
        "29",
        "Y (i, k, STK) + (PAS, SE NT, PTK)",
        "94.05",
        "55.68",
        "66.01",
        "60.40",
        "94.16",
        "56.18",
        "68.36",
        "61.67",
        "ke{PRED,SEM}",
        "30",
        "Y (i, k, STK) + (PAS, SENT, PTK)",
        "94.30",
        "57.95",
        "62.62",
        "60.19",
        "94.36",
        "58.07",
        "64.94",
        "61.31",
        "keScopea\\SENT"
      ]
    }
  ]
}
