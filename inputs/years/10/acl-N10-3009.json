{
  "info": {
    "authors": [
      "Bin Lu"
    ],
    "book": "Proceedings of the NAACL HLT 2010 Student Research Workshop",
    "id": "acl-N10-3009",
    "title": "Identifying Opinion Holders and Targets with Dependency Parser in Chinese News Texts",
    "url": "https://aclweb.org/anthology/N10-3009",
    "year": 2010
  },
  "references": [
    "acl-C08-1103",
    "acl-H05-1045",
    "acl-L08-1087",
    "acl-P02-1053",
    "acl-P97-1023",
    "acl-W02-1011",
    "acl-W03-2102",
    "acl-W06-0301"
  ],
  "sections": [
    {
      "text": [
        "Identifying Opinion Holders and Targets with Dependency Parser in",
        "Chinese News Texts",
        "1",
        "In this paper, we propose to identify opinion holders and targets with dependency parser in Chinese news texts, i.e. to identify opinion holders by means of reporting verbs and to identify opinion targets by considering both opinion holders and opinion-bearing words.",
        "The experiments with NTCIR-7 MOAT's Chinese test data show that our approach provides better performance than the baselines and most systems reported at NTCIR-7."
      ]
    },
    {
      "heading": "Introduction",
      "text": [
        "In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs and product reviews, has drawn much attention in the NLP field (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002; Hu and Liu, 2004).",
        "An opinion expressed in a text involves different components, including opinion expression, opinion holder and target (Wilson and Wiebe, 2003).",
        "Opinion holder is usually an entity that holds an opinion, and opinion target is what the opinion is about (Kim and Hovy, 2006).",
        "Although there have been research on identifying opinion holders and targets in English product reviews and news texts, little work has been reported on similar tasks involving Chinese news texts.",
        "In this study, we investigate how dependency parsing can be used to help the task on opinion holder/target identification in Chinese news texts.",
        "Three possible contributions from this study are: 1) we propose that the existence of reporting verbs is a very important feature for identifying opinion holders in news texts, which has not been clearly indicated; 2) we argue that the identification of opinion targets should not be done alone without considering opinion holders, because opinion holders are much easier to be identified in news texts and the identified holders are quite useful for the identification of the associated targets.",
        "Our approach shows encouraging performance on opinion holder/target identification, and the results are much better than the baseline results and most results reported in NTCIR-7 (Seki et al., 2008).",
        "The paper is organized as follows.",
        "Sec.",
        "2 introduces related work.",
        "Sec.",
        "3 gives the linguistic analysis of opinion holder/target.",
        "The proposed approach is described in Sec. 4, followed by the experiments in Sec. 5.",
        "Lastly we conclude in Sec. 6.",
        "Related Work",
        "Although document-level sentiment analysis (Turney, 2002; Pang et al., 2002) can provide the overall polarity of the whole text, it fails to detect the holders and targets of the sentiment in texts.",
        "For opinion mining of product reviews, opinion holder identification is usually omitted under the assumption that opinion holder is the review writer; and opinion targets are limited to the product discussed and its features (Hu and Liu, 2004).",
        "But in news texts, opinion holders/targets are more diverse: all named entities and noun phrases can be opinion holders; while opinion targets could be noun phrases, verb phrases or even clauses (Kim and Hovy, 2006; Ruppenhofer et al.",
        "2008).",
        "Bethard et al.",
        "(2004) identify opinion propositions and their holders by semantic parsing techniques.",
        "Choi et al.",
        "(2005) and Kim and Hovy (2005) identify only opinion holders on the MPQA corpus (Wilson and Wiebe, 2003).",
        "Kim and Hovy (2006) proposed to map the semantic frames of FrameNet into opinion holder and target for only adjectives and verbs.",
        "Kim et al.",
        "(2008) proposed to use syntactic structures for target identification without considering opinion holders.",
        "Stoyanov and Cardie (2008) define opinion topic and target and treat the task as a co-reference resolution problem.",
        "For the identification of opinion holders/targets in Chinese, there were several reports at NTCIR-7 (Seki et al., 2008).",
        "Xu et al.",
        "(2008) proposed to use some heuristic rules for opinion holder/target identification.",
        "Ku et al.",
        "(2008) treated opinion holder identification as a binary classification problem of determining if a word was a part of an opinion holder.",
        "Dependency structures represent all sentence relationships uniformly as typed dependency relations between pairs of words.",
        "Some major dependency relations for Chinese (Ma et al., 2004) include EÈil (Subject-Verb, SBV), (Verb-Object, VOB), ^^(Attributive-Noun, ATT), %jk (Quantifier, QUN) and ü ^ ft (Independent structure, IS).",
        "Consider the following Chinese sentence:",
        "Russian Foreign Minister Ivanov said that NATO's eastward expansion was \"Towards the wrong direction.\"",
        "Its dependency tree is shown in Figure1.",
        "Its head is the verb M (said), whose subject and object are respectively MB^&ffißLW^i (Russian Foreign Minister Ivanov) and the embedded clause jhföM fàMMM\" MtäMMfäfifä\" (NATO's eastward expansion was \"towards the wrong direction.",
        "\")."
      ]
    },
    {
      "heading": "3. Linguistic Analysis of Opinions",
      "text": [
        "The opinions in news text may be explicitly mentioned or be expressed indirectly by the types of words and the style of language (Wilson and Wiebe, 2003).",
        "Two kinds of lexical clues are exploited here for opinion holder/target identification:",
        "Reporting verbs: verbs indicating speech events;",
        "Opinion-bearing Words: words or phrases containing polarity (i.e. positive, negative or neutral).",
        "In sentence a) above, the reporting verb M (said) indicates a speech event expressing an opinion given by the holder ffijiigjï (Russian Foreign Minister Ivanov).",
        "Meanwhile, the opinionbearing word MM (wrong) shows negative attitude towards the target jbföMlnlfMi'M (NATO's eastward expansion).",
        "Therefore, we assume that a large proportion of holders are governed by such reporting verbs, while targets are usually governed by opinion-bearing words/phrases.",
        "Opinion holders are usually named entities, including, but not limited to, person names (e.g. ff :g|Jp||[|§^||f/economist Ol), organization names (e.g.",
        "^miRJ?",
        "?/UK government), and personal titles (e.g. ffpf-PÜ /the economist).",
        "Opinion holders can also be common noun phrases, such as HHj (companies), M^T^P^.",
        "(two thousand students).",
        "Pronouns can also be opinion holders, e.g. ff (he), ffilfH (they), $)c(I).",
        "Opinion targets are more abstract and diverse, and could be agents, concrete objects, actions, events or even abstract ideas.",
        "In addition to noun phrases, opinion targets could also be verb phrases or embedded clauses."
      ]
    },
    {
      "heading": "4. Identifying Opinion Holders/Targets",
      "text": [
        "In this section, we introduce our approach of identifying opinion holders/targets.",
        "We use the dependency parser in the HIT LTP package (http://ir.hit.edu.cn/) to get the dependency relations of the simplified Chinese sentences converted from the traditional Chinese ones.",
        "The reporting verbs were firstly collected from the",
        "Chinese sample data of NTCIR-6 OAPT (Seki et al., 2007) in which the OPINIONOPR tag was used to mark them.",
        "We then use HowNet, WordNet and Tongyici Cilin to extend the reporting verbs from 68 to 308 words through manual synonym search.",
        "Some frequently used reporting verbs include M(say), ^^(express), |§ Ü(think), etc.",
        "Some of the reporting verbs could also convey opinions, such as fttifF (criticize), ÎÛ M (condemn), |HtJ§ (praise), etc.",
        "For opinion-bearing words/phrases, we use The Lexicon of Chinese Positive Words (Shi and Zhu, 2006) and The Lexicon of Chinese Negative Words (Yang and Zhu, 2006), which consist of 5046 positive items and 3499 negative ones, respectively.",
        "To enhance the robustness of the dependency parser, named entities are first recognized with a traditional Chinese word segmentation tool with access to the very large LIVAC dictionary (http://www.livac.org) collected from Chinese news published in Hong Kong and Taiwan.",
        "The identified named entities, as well as the collected reporting verbs and opinion-bearing words are added to the user dictionary of the HIT LTP package to help parsing.",
        "Before parsing, the parentheses enclosing only English words or numbers are removed in sentences, because the parser cannot properly process the parentheses which may greatly influence the parsing result.",
        "Two hypotheses are used to identify opinion holders in opinionated sentences: 1) the subject of reporting verbs will be the opinion holders; 2) if no reporting verb is found, the author could be the opinion holder.",
        "In addition to the two hypotheses above, the following heuristic rules (HR) are used:",
        "1) Other words having relations with reporting verbs",
        "If the subject of reporting verbs is not found in the sentence, we will find the word having relationship of ATT, VOB or IS with the reporting verbs, because sometimes the parser may wrongly marked the subject as other relations.",
        "2) Colon processing in Headlines",
        "If no reporting verbs are found in news headlines, we just pick up the noun before the colon as the target candidate in the headlines because the author usually replaces the reporting verb with a colon due to length limitation.",
        "E.g. in the headline 0fM -ffin^M'M'X (Morgan:",
        "Tree for Sentence a)",
        "Economic growth has been shut down), the noun ß=ßfM (Morgan) before colon is chosen as the opinion holder.",
        "3) Holder in the previous sentence",
        "If no opinion holder is found in the current clause and one holder candidate is found in the previous clause, we just choose the opinion holder of the previous clause as the holder candidate, because an opinion holder may express several ideas through consecutive sentences or clauses.",
        "Through the procedure of candidate generation, we may find a holder candidate containing only one single word.",
        "But the holder may be a word sequence instead of a single word.",
        "Thus we further expand the holder candidates from the core head word by the following rules:",
        "1) Attributive modifier (ATT)",
        "E.g. in sentence a) mentioned in Sec. 2.2, the subject of the reporting verb M (said) is fêMÊÏ^: (Ivanov), which has the attributive noun %M (Foreign Minister) modified further by an attributive noun mm(Russia).",
        "Therefore, the final extended opinion holder would be jrf-MffißLM^: (Russian Foreign Minister Ivanov).",
        "2) Quantifier modifier and f p/5.",
        "(and/or)",
        "E.g. the quantifier modifier §^^~(some) in the noun phrase SI^SL^fHli (some Asian countries) should be part of the opinion holder.",
        "Sometime, we need to extend the holder across -fP/JK(and/or), e.g. Éfël&fâfflJgff'ffigiMJjffîM (Suharto and two other army generals).",
        "Furthermore, time nouns, numbers and words only containing one Chinese character (except for pronouns) are removed from the candidates, as they are unlikely to be opinion holders.",
        "4.4 Identifying Opinion Targets with Opinion-bearing Words",
        "Here we propose to use automatically identified reporting verbs and opinion holders to help opinion target identification.",
        "The heuristic rules (HR) are as follows.",
        "1) If a candidate of opinion holder is automatically identified with a reporting verb in an opinionated sentence, we will try to find the subject in the embedded clause as the target candidate by the following two steps: a) Find the subject of the object verb of the reporting verb.",
        "E.g. in sentence a) in Sec. 2.2, the opinion target Jtêz/ MfàîÊîM (NATO's eastward expansion) is the subject of the verb M (was) in the embedded clause which is in turn the object of the reporting verb MM (said); b) If no target candidate is found in step a, we try to find after the reporting verb the subject whose parent is an opinion-bearing word as the target candidate.",
        "2) If no target candidate is found in step 1, and no opinion holder is found in the sentence, we find the subject of the sentence as the target candidate, because the author may be the opinion holder and the target could be the subject of the sentence.",
        "3) If still no target candidate is found in step 2, we find the object in the sentence as the target because the object could be the opinion target in case there is no subject and no opinion holder.",
        "Target candidate expansion (EP) is similar to holder candidate expansion described in Sec. 4.3.2.",
        "If an opinion target is in the opinion holder candidates (we call it holder conflict, HC), we remove it from the target candidates, and then try to find another using the above procedure."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "We use the traditional Chinese test data in NTCIR-7 MOAT (Seki et al., 2008) for our experiments.",
        "Out of 4465 sentences, 2174 are annotated as opinionated by the lenient standard, and the opinion holders of some opinionated sentences are marked as POSTAUTHOR denoting the author of the news article.",
        "We use the final list given by the organizers as the gold standard.",
        "Baselines for opinion holder identification: Baseline 1: We just use the subject of reporting verbs as the opinion holder, without sentence preprocessing described in Sec. 4.2 and any heuristic rules introduced in Sec. 4.3.1.",
        "Baseline 2: We also implement the CRF model for detecting opinion holders (Choi et al., 2006) by using CRF++.",
        "The training data is the NTCIR-6 Chinese test data.",
        "The labels used by CRF comprise Holder, Parent of Holder, None (not holder or parent) and the features for each word in our implementation include: basic features (i.e. word, POS-tag, whether the word itself is a reporting verb or not), dependency features (i.e. parent word, POS-tag of its parent, dependency relation with its parent, whether its parent is a reporting verb) and semantic features (i.e. WSD entry in Tongyici Cilin, WSD entry of its parent).",
        "Baseline for opinion target identification:",
        "Baseline 1: we try to find the subject or object of opinion-bearing words as the targets.",
        "If both a subject and an object are found, we just simply choose the subject as the target.",
        "We evaluate performance using 3 measures: exact match (EM), head match (HM), and partial three evaluation metrics: recall (Rec), precision (Pre), and F1.",
        "For opinion holder identification, we consider two cases: 1) all opinionated sentences; 2) only the opinionated sentences whose opinion holders do not contain POST AUTHOR.",
        "The metric ALL Pre reported below is the precision in case 1 which is the same with recall and F1.",
        "The results for holder identification are shown in Table 1, from which we can observe that our proposed approach significantly outperforms the two baseline methods, including the unsupervised",
        "Table 1.",
        "Results for Opinion Holders Unexpectedly, even the unsupervised baseline 1 achieves better performance than baseline 2 (the CRF-based method).",
        "The possible reasons are: 1) the training data is not large enough to cover the cases in the test data, resulting in low recall of the CRF model; 2) the features used by the CRF model could be refined to improve the performance.",
        "ALL Pre",
        "Pre",
        "Rec",
        "F1",
        "Baseline1",
        "EM",
        "52.4",
        "46.8",
        "31.6",
        "37.8",
        "HM",
        "67.1",
        "80.2",
        "54.2",
        "64.7",
        "PM",
        "72.1",
        "89.3",
        "60.4",
        "72.0",
        "Baseline2 (CRF)",
        "EM",
        "45.5",
        "34.7",
        "18.1",
        "23.8",
        "HM",
        "55.2",
        "63.6",
        "33.1",
        "43.6",
        "PM",
        "55.6",
        "64.9",
        "33.8",
        "44.4",
        "Our",
        "EM",
        "69.8",
        "74.4",
        "63.6",
        "68.5",
        "HM",
        "72.5",
        "79.2",
        "67.7",
        "73.0",
        "Approach",
        "PM",
        "75.7",
        "85.1",
        "72.7",
        "78.4",
        "Approach",
        "Here we also evaluate the influences of the following three factors on the performance: sentences preprocessing (SP) in Sec. 4.2, holder expansion (EP) in Sec. 4.3.2 and the heuristic rules (HR) in Sec. 4.3.1.",
        "The results are shown in Figure 2 for different combinations, in which BL refers to baseline 1.",
        "Figure 2.",
        "Influences of Factors on Opinion Holders From Figure 2, we can observe that: 1) All three factors have positive effects on performance compared to baseline 1, and our approach by integrating all factors achieves the best performance; 2) SP improve the performance in terms of all three metrics, showing that SP including named entity recognition and parenthesis removing are useful for holder identification; 3) The major improvement of EP lies in EM, showing that the main contribution of EP is to get the exact opinion holders by expanding the core head noun; 4) SP+EP+HR improves the performance in terms of all three metrics compared with SP+HR, showing the heuristic rules are useful to improve the performance.",
        "The results for opinion target identification are shown in Table 2, from which we can observe that our proposed approach significantly outperforms the baseline method.",
        "Baseline 1",
        "Table 2.",
        "Results for Opinion Targets We also investigate the influences of the following four factors on the performance: sentence preprocessing (SP) in Sec. 4.2, target expansion (EP) in Sec. 4.4, holder conflict (HC), the heuristic rules (HR) proposed in Sec. 4.4.",
        "The F1s for EM, HM and PM are shown in Figure 3, in which BL refers to baseline 1.",
        "Influence of Factors",
        "Figure 3.",
        "Influences of Factors on Opinion Targets From Figure 3, we can observe that: 1) All four factors have positive effects on performance compared to the baseline, and our approach integrating all the factors achieves the best performance; 2) EP significantly improves F1 of EM without much improvement on F1 of HM or PM, showing that EP's major contribution lies in exact match; 3) The major contribution of HC is the improvement of F1s of HM and PM, showing the automatically identified opinion holders are quite helpful for finding opinion targets; 4) SP+EP+HC improves the performance in terms of all three metrics; and our approach further improves the performance by adding HR.",
        "Here we compare our results with those reported at",
        "NTCIR-7 MOAT traditional Chinese test (Seki et al., 2008).",
        "Without considering the errors in the previous step, the highest F1s for opinion holder analysis reported by the four participants were respectively 82.5%, 59.9%, 50.3% and 59.5%, and the highest F1s for target reported by the three participants were respectively 60.6%, 2.1% and 3.6%.",
        "Compared to the results at NTCIR-7, our performances on both opinion holder identification in Table 1 and that on target identification in Table 2 seem quite encouraging even by the EM metrics.",
        "Consider the evaluation for opinion holders/targets was semi-automatic at NTCIR-7.",
        "We should note that although the generated standard had been supplemented by the participants' submissions, some correct answers may still be missing, especially for targets since only three teams participated in the target identification task and the recalls were not high.",
        "Thus the performance reported in Table 1 and 2 may be underestimated.",
        "Pre",
        "Rec",
        "F1",
        "EM",
        "11.1",
        "9.2",
        "10.1",
        "HM",
        "24.0",
        "19.9",
        "21.8",
        "PM",
        "39.4",
        "32.7",
        "35.8",
        "EM",
        "29.3",
        "28.5",
        "28.9",
        "HM",
        "38.4",
        "38.0",
        "38.2",
        "PM",
        "59.3",
        "58.7",
        "59.0",
        "Here we also give an estimate on the percentages of opinionated sentences containing both opinion holders and at least one reporting verb in NTCIR-6 and NTCIR-7's traditional",
        "Chinese test data, which are respectively 94.5% and 83.9%.",
        "The high percentages show that reporting verbs are very common in news report."
      ]
    },
    {
      "heading": "6. Conclusion and Future Work",
      "text": [
        "In this paper, we investigate the problem of identifying opinion holders/targets in opinionated sentences of Chinese news texts based on Chinese dependency parser, reporting verbs and opinion-bearing words.",
        "Our proposed approach shows encouraging performance on opinion holder/target identification with the NTCIR-7's traditional Chinese test data, and outperforms most systems reported at NTCIR-7 and the baseline methods including the CRF-based model.",
        "The proposed approach is highly dependent on dependency parser, and we would like to further investigate machine learning approaches (including the CRF model) by treating dependency structures as one of the linguistic features, which could be more robust to parsing errors.",
        "Opinion targets are more difficult to be identified than opinion holders, and deserve more attention in the NLP field, and we also would extend the targets to verb phrases and embedded clauses in addition to noun phrases.",
        "To explore the effectiveness of our approach with English data such as MPQA is another direction."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We acknowledge the help of our colleagues (Professor Benjamin K. Tsou and Mr. Jiang Tao).",
        "Reference",
        "Steven Bethard, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky.",
        "2004.",
        "Automatic Extraction of Opinion Propositions and their Holders, AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.",
        "Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.",
        "2005.",
        "Identifying sources of opinions with conditional random fields and extraction patterns.",
        "In Proc.",
        "of HLT/EMNLP-05.",
        "Vasileios Hatzivassiloglou and Kathleen McKeown.",
        "1997.",
        "Predicting the Semantic Orientation of Adjectives.",
        "In Proc.",
        "of ACL-97.",
        "174-181.",
        "Minqing Hu and Bing Liu.",
        "2004.",
        "Mining Opinion",
        "Features in Customer Reviews.",
        "In Proc.",
        "of AAAI-04.",
        "Soo-Min Kim and Eduard Hovy.",
        "2005.",
        "Identifying Opinion Holders for Question Answering in Opinion Texts, In Proc.",
        "of AAAI-05 Workshop on Question Answering in Restricted Domains.",
        "Pittsburgh, PA. Soo-Min Kim and Eduard Hovy.",
        "2006.",
        "Extracting opinions, opinion holders, and topics expressed in online news media text, In Proc.",
        "of ACL Workshop on Sentiment and Subjectivity in Text.",
        "Youngho Kim, Seongchan Kim, and Sung-Hyon Myaeng.",
        "2008.",
        "Extracting Topic-related Opinions and their Targets in NTCIR-7, Proc.",
        "of NTCIR-7",
        "Workshop, Tokyo, Japan.",
        "Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan-hua Chen and Hsin-Hsi Chen.",
        "2008.",
        "Sentence-Level Opinion Analysis by CopeOpi in NTCIR-7.",
        "In Proc.",
        "of NTCIR-7 Workshop.",
        "Tokyo, Japan.",
        "Jinshan Ma, Yu Zhang, Ting Liu and Sheng Li.",
        "2004.",
        "A statistical dependency parser of Chinese under small training data.",
        "IJCNLP-04 Workshop: Beyond shallow analyses - Formalisms and statistical modeling for deep analyses.",
        "Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.",
        "2002.",
        "Thumbs up?",
        "Sentiment classification using machine learning techniques.",
        "In Proc.",
        "of EMNLP-02.",
        "Josef Ruppenhofer, Swapna Somasundaran, Janyce Wiebe.",
        "2008.",
        "Finding the Sources and Targets of Subjective Expressions.",
        "In Proc.",
        "of LREC 2008.",
        "Yohei Seki, David Kirk Evans, Lun-Wei Ku, and et al.",
        "2007.",
        "Overview of Opinion Analysis Pilot Task at NTCIR-6.",
        "Proc.",
        "of the NTCIR-6 Workshop.",
        "Yohei Seki, David Kirk Evans, Lun-Wei Ku, and et al.",
        "2008.",
        "Overview of Multilingual Opinion Analysis",
        "Task at NTCIR-7.",
        "Proc.",
        "of the NTCIR-7 Workshop.",
        "Japan.",
        "2008.",
        "12.",
        "Jilin Shi and Yinggui Zhu.",
        "2006.",
        "The Lexicon of Chinese Positive Words ( lSlip?]p?]jft).",
        "Sichuan Lexicon Press.",
        "Veselin Stoyanov and Claire Cardie.",
        "2008.",
        "Topic Identification for Fine-Grained Opinion Analysis.",
        "In Proc.",
        "of COLING-08.",
        "Peter D. Turney.",
        "2002.",
        "Thumbs up or thumbs down?",
        "Semantic orientation applied to unsupervised classification of reviews, In Proc.",
        "of ACL-02.",
        "Theresa Wilson and Janyce Wiebe.",
        "2003.",
        "Annotating opinions in the world press.",
        "Proc.",
        "of the 4th ACL SIGdial Workshop on Discourse and Dialogue (SIGdial-03).",
        "Ruifeng Xu, Kam-Fai Wong and Yunqing Xia.",
        "2008.",
        "Coarse-Fine Opinion Mining - WIA in NTCIR-7 MOAT Task.",
        "Proc.",
        "of the 7th NTCIR Workshop.",
        "Ling Yang and Yinggui Zhu.",
        "2006.",
        "The Lexicon of Chinese Negative Words ( IJIip?]p?",
        "]jft ).",
        "Sichuan Lexicon Press."
      ]
    }
  ]
}
