{
  "info": {
    "authors": [
      "Liang Huang",
      "Kenji Sagae"
    ],
    "book": "ACL",
    "id": "acl-P10-1110",
    "title": "Dynamic Programming for Linear-Time Incremental Parsing",
    "url": "https://aclweb.org/anthology/P10-1110",
    "year": 2010
  },
  "references": [
    "acl-A00-2018",
    "acl-C96-1058",
    "acl-D08-1059",
    "acl-D09-1127",
    "acl-H05-1066",
    "acl-J03-1006",
    "acl-J95-2002",
    "acl-J98-4004",
    "acl-N07-1051",
    "acl-N09-1073",
    "acl-P04-1015",
    "acl-P05-1012",
    "acl-P05-1022",
    "acl-P08-1067",
    "acl-P08-1068",
    "acl-P88-1031",
    "acl-P89-1018",
    "acl-P99-1059",
    "acl-W02-1001",
    "acl-W04-0308",
    "acl-W05-1506"
  ],
  "sections": [
    {
      "text": [
        "USC Institute for Creative Technologies 13274 Fiji Way",
        "Marina del Rey, CA 90292",
        "Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming.",
        "We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging \"equivalent\" stacks based on feature values.",
        "Empirically, our algorithm yields up to a fivefold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.",
        "Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In terms of search strategy, most parsing algorithms in current use for data-driven parsing can be divided into two broad categories: dynamic programming which includes the dominant CKY algorithm, and greedy search which includes most incremental parsing methods such as shift-reduce.",
        "Both have pros and cons: the former performs an exact search (in cubic time) over an exponentially large space, while the latter is much faster (in linear-time) and is psycholinguis-tically motivated (Frazier and Rayner, 1982), but its greedy nature may suffer from severe search errors, as it only explores a tiny fraction ofthe whole space even with a beam.",
        "Can we combine the advantages of both approaches, that is, construct an incremental parser that runs in (almost) linear-time, yet searches over a huge space with dynamic programming?",
        "Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical.",
        "We instead propose a dynamic programming al-ogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice.",
        "The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing (Earley, 1970; Stolcke, 1995) and generalized LR parsing (Tomita, 1991).",
        "However, our formalism is more flexible and our algorithm more practical.",
        "Specifically, we make the following contributions:",
        "• theoretically, we show that for a large class of modern shift-reduce parsers, dynamic programming is in fact possible and runs in polynomial time as long as the feature functions are bounded and monotonic (which almost always holds in practice);",
        "• practically, dynamic programming is up to five times faster (with the same accuracy) as conventional beam-search on top of a state-of-the-art shift-reduce dependency parser;",
        "• as a by-product, dynamic programming can output a forest encoding exponentially many trees, out of which we can draw better and longer k-best lists than beam search can;",
        "• finally, better and faster search also leads to better and faster learning.",
        "Our final parser achieves the best (unlabeled) accuracies that we are aware of in both English and Chinese among dependency parsers trained on the Penn Treebanks.",
        "Being linear-time, it is also much faster than most other parsers, even with a pure Python implementation.",
        "input: \"I saw Al with Joe\"",
        "where I is the step, c is the cost, and the shift cost £ and reduce costs A and p are:",
        "Forconvenience ofpresentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing."
      ]
    },
    {
      "heading": "2. Shift-Reduce Parsing",
      "text": [
        "Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972).",
        "To adapt it to dependency parsing, we split the reduce action into two cases, re^ and re^, depending on which one of the two items becomes the head after reduction.",
        "This procedure is known as \"arc-standard\" (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al.",
        "(2009), which is also the reference parser in our experiments.",
        "More formally, we describe a parser configuration by a state (j, S) where S is a stack of trees so,si;... where so is the top tree, and j is the",
        "Figure 2: A trace of vanilla shift-reduce.",
        "After step (4), the parser branches off into (5a) or (5b).",
        "queue head position (current word qo is Wj).",
        "At each step, we choose one of the three actions:",
        "1. sh: move the head of queue, Wj, onto stack S as a singleton tree;",
        "2. re^: combine the top two trees on the stack, so and si, and replace them with tree si^so.",
        "3. re^: combine the top two trees on the stack, so and si, and replace them with tree si^so.",
        "Note that the shorthand notation t^if denotes a new tree by \"attaching tree i' as the leftmost child of the root of tree t\".",
        "This procedure can be summarized as a deductive system in Figure 1.",
        "States are organized according to step I, which denotes the number of actions accumulated.",
        "The parser runs in linear-time as there are exactly 2n – 1 steps for a sentence of n words.",
        "As an example, consider the sentence \"I saw Al with Joe \" in Figure 2.",
        "At step (4), we face a shift-reduce conflict: either combine \"saw\" and \"Al\" in a re^v action (5a), or shift \"with\" (5b).",
        "To resolve this conflict, there is a cost c associated with each state so that we can pick the best one (or few, with a beam) at each step.",
        "Costs are accumulated in each step: as shown in Figure 1, actions sh, re^, and re^v have their respective costs £, A, and p, which are dot-products of the weights w and features extracted from the state and the action.",
        "We view features as \"abstractions\" or (partial) observations of the current state, which is an important intuition for the development of dynamic programming in Section 3.",
        "Feature templates are functions that draw information from the feature window (see Tab.",
        "1(b)), consisting of the top few trees on the stack and the first few words on the queue.",
        "For example, one such feature templatefioo = so.w o qo.t is a conjunction of two atomic features so.w and qo.t, capturing the root word of the top tree so on the stack, and the part-of-speech tag of the current head word qoon the queue.",
        "See Tab.",
        "1(a) for the list of feature templates used in the full model.",
        "Feature templates are instantiated for a specific state.",
        "For example, at step (4) in Fig. 2, the above template fioo will generate a feature instance (so.w = Al) o (qo.t = IN).",
        "0 : (0, e): 0",
        "I : (j, S ) : c",
        "I + 1: (j + 1, S|Wj)",
        ": c + £",
        "I : (j, S|si|so) :",
        "c",
        "I + 1 : (j, S|si^so)",
        "c + A",
        "I : (j, S|si|so) :",
        "c",
        "I + 1 : (j, S|si^so)",
        "c + p",
        "2n – 1 : (n, s0): c",
        "step",
        "action",
        "stack",
        "queue",
        "0",
        "-",
        "I ...",
        "1",
        "sh",
        "I",
        "saw ...",
        "2",
        "sh",
        "I saw",
        "Al ...",
        "3",
        "re^",
        "saw",
        "Al ...",
        "4",
        "sh",
        "saw Al",
        "with ...",
        "5a",
        "rerv",
        "saw^ Al",
        "with ...",
        "5b",
        "sh",
        "saw Al",
        "with",
        "Joe",
        "More formally, we denote f to be the feature function, such that f(j, S) returns a vector of feature instances for state (j, S).",
        "To decide which action is the best for the current state, we perform a three-way classification based on f (j, S), and to do so, we further conjoin these feature instances with the action, producing action-conjoined instances like (so.w = Al) o (qo.t = IN) o (action = sh).",
        "We denote fSh(j, S), fre^ (j, S), and frerv (j, S) to be the conjoined feature instances, whose dot-products with the weight vector decide the best action (see Eqs.",
        "(1-3) in Fig. 1).",
        "To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel.",
        "At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step.",
        "Our dynamic programming algorithm also runs on top of beam search in practice.",
        "To train the model, we use the averaged percep-tron algorithm (Collins, 2002).",
        "Following Collins and Roark (2004) we also use the \"early-update\" strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.",
        "The intuition behind this strategy is that later mistakes are often caused by previous ones, and are irrelevant when the parser is on the wrong track.",
        "Dynamic programming turns out to be a great fit for early updating (see Section 4.3 for details)."
      ]
    },
    {
      "heading": "3. Dynamic Programming (DP)",
      "text": [
        "The key observation for dynamic programming is to merge \"equivalent states\" in the same beam",
        "qo q1 ..",
        "Table 1: (a) feature templates used in this work, adapted from Huang et al.",
        "(2009).",
        "x.w and x.t denotes the root word and POS tag of tree (or word) x. and x.lc and x.rc denote x's left-and rightmost child.",
        "(b) feature window.",
        "(c) kernel features.",
        "(i.e., same step) if they have the same feature values, because they will have the same costs as shown in the deductive system in Figure 1.",
        "Thus we can define two states (j, S) and (jS') to be equivalent, notated (j, S) ~ (j', S'), iff.",
        "Note that j = j' is also needed because the queue head position j determines which word to shift next.",
        "In practice, however, a small subset of atomic features will be enough to determine the whole feature vector, which we call kernel features f (j, S), defined as the smallest set of atomic templates such that",
        "~f (j,S )= ~f (j',S ') =><j,S >~<j ',S').",
        "For example, the full list of 28 feature templates in Table 1(a) can be determined by just 12 atomic features in Table 1(c), which just look at the root words and tags of the top two trees on stack, as well as the tags of their left-and rightmost children, plus the root tag of the third tree s2, and finally the word and tag of the queue head qo and the equivalence ordering",
        "(a)",
        "Features Templates f (j, S ) qi = Wj+i",
        "(1)",
        "so.w s0.t so.w o s0.t s1.w Sl.t si.w o Sl.t qo.w qo.t qo.w o qo.t",
        "(2)",
        "s0.w o s1 .w so.t o s1.t so.t o qo.t so.w o so.t o s1 .t so.t o s1.w o s1.t so.w o s1.w o s1.t so.w o so.t o s1.w so.w o so.t o s1 o s1.t",
        "(3)",
        "so.t o qo.t o q1.t s1.t o so.t o qo.t so.w o qo.t o q1.t s1.t o so.w o qo.t",
        "(4)",
        "s1.t o s1.lc.t o so.t s1.t o s1.rc.t o so.t s1.t o so.t o so.rc.t s1.t o s1.lc.t o sos1.t o s1.rc.t o so.w s1.t o so.w o so.Ic.t",
        "(5)",
        "s2.t o s1.t o so.t",
        "(c)",
        "Kernel features for DP",
        "ff (j,S) = (j, f2(s2), f1(s1), fo(so))",
        "f2(s2)",
        "s2.t",
        "f1(s1)",
        "s1.w",
        "s1.t",
        "s1.lc.t s1.rc.t",
        "fo(so)",
        "so.w",
        "so.t",
        "so.lc.t so.rc.t",
        "j",
        "qo.w",
        "qo.t",
        "q1.t",
        "I : (i, j, sd...so) ~ I : (i, j, sd...so) iff.",
        "f (j, sd...so) = f (j, sd-^) I : _ : (c, v, _) -< I : _ : (c', v', _) iff.",
        "c < c' or (c = c' and v < v').",
        "axiom (po)",
        "state p:",
        "state q:",
        "where £ = w • fSh(j, sd...so), and 5 = £' + A, with £' = w • fSh(i, sd...so) and A = w • fre^ (j, sd...so).",
        "Figure 3 : Deductive system for shift-reduce parsing with dynamic programming.",
        "The predictor state set n is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995).",
        "The re^v case is similar, replacing so^so with so^so, and A with p = w • frerv (j, sd...so).",
        "Irrelevant information in a deduction step is marked as an underscore (_) which means \"can match anything\".",
        "tag of the next word qi.",
        "Since the queue is static information to the parser (unlike the stack, which changes dynamically), we can use j to replace features from the queue.",
        "So in general we write if the feature window looks at top d + 1 trees on stack, and where fi(si) extracts kernel features from tree sj (0 < i < d).",
        "For example, for the full model in Table 1(a) we have where d = 2, f2(x) = x.t, and fi(x) = fo(x) = (x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).",
        "Now that we have the kernel feature functions, it is intuitive that we might only need to remember the relevant bits of information from only the last (d + 1) trees on stack instead of the whole stack, because they provide all the relevant information for the features, and thus determine the costs.",
        "For shift, this suffices as the stack grows on the right; but for reduce actions the stack shrinks, and in order still to maintain d + 1 trees, we have to know something about the history.",
        "This is exactly why we needed the full stack for vanilla shift-reduce parsing in the first place, and why dynamic programming seems hard here.",
        "To solve this problem we borrow the idea of \"graph-structured stack\" (GSS) from Tomita (1991).",
        "Basically, each state p carries with it a set n(p) of predictor states, each of which can be combined with p in a reduction step.",
        "In a shift step, if state p generates state q (we say \"p predicts q\" in Earley (1970) terms), then p is added onto n(q).",
        "When two equivalent shifted states get merged, their predictor states get combined.",
        "In a reduction step, state q tries to combine with every predictor state p G n(q), and the resulting state r inherits the predictor states set from p, i.e., n(r) = n(p).",
        "Interestingly, when two equivalent reduced states get merged, we can prove (by induction) that their predictor states are identical (proofomitted).",
        "where is the span of the top tree so, and sd..si are merely \"left-contexts\".",
        "It can be combined with some predictor state p spanning [k..i] to form a larger state spanning [k..j], with the resulting top tree being either si^so or si ^so.",
        "This style resembles CKY and Earley parsers.",
        "In fact, the chart in Earley and other agenda-based parsers is indeed a GSS when viewed left-to-right.",
        "In these parsers, when a state is popped up from the agenda, it looks for possible sibling states that can combine with it; GSS, however, explicitly maintains these predictor states so that the newly-popped state does not need to look them up.",
        "We state the main theoretical result with the proof omitted due to space constraints:",
        "Theorem 1.",
        "The deductive system is optimal and runs in worst-case polynomial time as long as the kernel feature function satisfies two properties:",
        "• bounded: f (j, S ) = (j, fd(sd),..., fo(so)) for some constant d, and each |ft(x)| also bounded by a constant for all possible tree x.",
        "• monotonic: ft(x) = ft(y) == ft+i(x) = ft+i(y), for all i and all possible trees x, y.",
        "Intuitively, boundedness means features can only look at a local window and can only extract bounded information on each tree, which is always the case in practice since we can not have infinite models.",
        "Monotonicity, on the other hand, says that features drawn from trees farther away from the top should not be more refined than from those closer to the top.",
        "This is also natural, since the information most relevant to the current decision is always around the stack top.",
        "For example, the kernel feature function in Eq.",
        "5 is bounded and monotonic, since f2 is less refined than fi and fo.",
        "These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic: for example, one cannot refine a grammar by only remembering the grandparent but not the parent symbol.",
        "The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context).",
        "For instance, a context-free rule A – B C would become D A – D B BC for some D if there exists a rule E – aDAß.",
        "This resembles the reduce step in Fig. 3.",
        "The very high-level idea of the proof is that boundedness is crucial for polynomial-time, while monotonicity is used for the optimal substructure property required by the correctness of DP.",
        "Though the DP algorithm runs in polynomial-time, in practice the complexity is still too high, esp.",
        "with a rich feature set like the one in Table 1.",
        "So we apply the same beam search idea from Sec. 2.3, where each step can accommodate only the best b states.",
        "To decide the ordering of states in each beam we borrow the concept ofpre-fix cost from Stolcke (1995), originally developed for weighted Earley parsing.",
        "As shown in Fig. 3, the prefix cost c is the total cost of the best action sequence from the initial state to the end of state p, i.e., it includes both the inside cost v (for Viterbi inside derivation), and the cost of the (best) path leading towards the beginning of state p. We say that a state p with prefix cost c is better than a state p' with prefix cost c', notated p -< p' in Fig. 3, if c < c'.",
        "We can also prove (by contradiction) that optimizing for prefix cost implies optimal inside cost (Nederhof, 2003, Sec. 4).",
        "As shown in Fig. 3, when a state q with costs ( c, v) is combined with a predictor state p with costs (c', v'), the resulting state r will have costs where the inside cost is intuitively the combined inside costs plus an additional combo cost 5 from the combination, while the resulting prefix cost c' + v + 5 is the sum of the prefix cost of the predictor state q, the inside cost of the current state p, and the combo cost.",
        "Note the prefix cost of q is irrelevant.",
        "The combo cost 5 = £ + A consists of shift cost £ of p and reduction cost A of q.",
        "The cost in the non-DP shift-reduce algorithm (Fig.",
        "1) is indeed a prefix cost, and the DP algorithm subsumes the non-DP one as a special case where no two states are equivalent.",
        "As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999).",
        "Here the kernel feature function is",
        "Figure 4: Example of shift-reduce with dynamic programming: simulating an edge-factored model.",
        "GSS is implicit here, and re^ case omitted.",
        "where h(x) returns the head word index of tree x, because all features in this model are based on the head and modifier indices in a dependency link.",
        "This function is obviously bounded and monotonic in our definitions.",
        "The theoretical complexity of this algorithm is O(n) because in a reduction step we have three span indices and three head indices, plus a step index I.",
        "By contrast, the naïve CKY algorithm for this model is O(n) which can be improved to O(n) (Eisner, 1996).",
        "The higher complexity of our algorithm is due to two factors: first, we have to maintain both h and h' in one state, because the current shift-reduce model can not draw features across different states (unlike CKY); and more importantly, we group states by step I in order to achieve incrementality and linear runtime with beam search that is not (easily) possible with CKY or MST."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We first reimplemented the reference shift-reduce parser of Huang et al.",
        "(2009) in Python (henceforth \"non-DP\"), and then extended it to do dynamic programing (henceforth \"DP\").",
        "We evaluate their performances on the standard Penn Treebank (PTB) English dependency parsing task using the standard split: secs 02-21 for training, 22 for development, and 23 for testing.",
        "Both DP and non-DP parsers use the same feature templates in Table 1.",
        "For Secs.",
        "4.1-4.2, we use a baseline model trained with non-DP for both DP and non-DP, so that we can do a side-by-side comparison ofsearch quality; in Sec. 4.3 we will retrain the model with DP and compare it against training with non-DP.",
        "To compare parsing speed between DP and non-DP, we run each parser on the development set, varying the beam width b from 2 to 16 (DP) or 64 (non-DP).",
        "Fig.",
        "5a shows the relationship between search quality (as measured by the average model score per sentence, higher the better) and speed (average parsing time per sentence), where DP with a beam width of b=16 achieves the same search quality with non-DP at b=64, while being 5 times faster.",
        "Fig.",
        "5b shows a similar comparison for dependency accuracy.",
        "We also test with an edge-factored model (Sec.",
        "3.5) using feature templates (1)-(3) in Tab.",
        "1, which is a subset of those in McDonald et al.",
        "(2005b).",
        "As expected, this difference becomes more pronounced (8 times faster in Fig. 5c), since the less expressive feature set makes more states \"equivalent\" and mergeable in DP.",
        "Fig.",
        "5d shows the (almost linear) correlation between dependency accuracy and search quality, confirming that better search yields better parsing.",
        "DP achieves better search quality because it expores an exponentially large search space rather than only b trees allowed by the beam (see Fig. 6a).",
        "As a by-product, DP can output a forest encoding these exponentially many trees, out of which we can draw longer and better (in terms of oracle) k-best lists than those in the beam (see Fig. 6b).",
        "The forest itself has an oracle of 98.15 (as if k – oo), computed a la Huang (2008, Sec. 4.1).",
        "These candidate sets may be used for reranking (Charniak",
        "Another interesting advantage of DP over non-DP is the faster training with perceptron, even when both parsers use the same beam width.",
        "This is due to the use of early updates (see Sec. 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately.",
        "By contrast, in non-DP beam search, states such as p might still",
        "..!......",
        "[......u-x.",
        "DP non-DP (b) parsing accuracy vs. time (full model)",
        "x.....",
        "non-DP _i_L full, non-DP",
        "jk edge-factor, DP",
        "(d) correlation b/w parsing (y) and search (x)",
        "Figure 5: Speed comparisons between DP and non-DP, with beam size b ranging 2~16 for DP and 2^64 for non-DP.",
        "Speed is measured by avg.",
        "parsing time (secs) per sentence on x axis.",
        "With the same level of search quality or parsing accuracy, DP (at b=16) is <~-4.8 times faster than non-DP (at b=64) with the full model in plots (a)-(b), or ~8 times faster with the simplified edge-factored model in plot (c).",
        "Plot (d) shows the (roughly linear) correlation between parsing accuracy and search quality (avg.",
        "model score).",
        "sentence length (a) sizes of search spaces (b) oracle precision on dev",
        "Figure 6: DP searches over a forest of exponentially many trees, which also produces better and longer k-best lists with higher oracles, while non-DP only explores b trees allowed in the beam (b = 16 here).",
        "1010",
        "_",
        "10",
        "-",
        "10",
        "-",
        "10",
        "-",
        "10",
        "~-Â",
        "10",
        "(",
        "+",
        "J",
        "Figure 7: Learning curves (showing precision on dev) of perceptron training for 25 iterations (b=8).",
        "DP takes 18 hours, peaking at the 17th iteration (93.27%) with 12 hours, while non-DP takes 23 hours, peaking at the 18th (93.04%) with 16 hours.",
        "survive in the beam throughout, even though it is no longer possible to rank the best in the beam.",
        "The higher frequency of early updates results in faster iterations of perceptron training.",
        "Table 2 shows the percentage of early updates and the time per iteration during training.",
        "While the number of updates is roughly comparable between DP and non-DP, the rate of early updates is much higher with DP, and the time per iteration is consequently shorter.",
        "Figure 7 shows that training with DP is about 1.2 times faster than non-DP, and achieves +0.2% higher accuracy on the dev set (93.27%).",
        "Besides training with gold POS tags, we also trained on noisy tags, since they are closer to the test setting (automatic tags on sec 23).",
        "In that case, we tag the dev and test sets using an automatic POS tagger (at 97.2% accuracy), and tag the training set using four-way jackknifing similar to Collins (2000), which contributes another +0.1% improvement in accuracy on the test set.",
        "Faster training also enables us to incorporate more features, where we found more lookahead features (q2) results in another +0.3% improvement.",
        "Table 3 presents the final test results of our DP parser on the Penn English Treebank, compared with other state-of-the-art parsers.",
        "Our parser achieves the highest (unlabeled) dependency accuracy among dependency parsers trained on the Treebank, and is also much faster than most other parsers even with a pure Python implementation",
        "Table 2: Perceptron iterations with DP (left) and non-DP (right).",
        "Early updates happen much more often with DP due to equivalent state merging, which leads to faster training (time in minutes).",
        "Table 3: Final test results on English (PTB).",
        "Our parser (in pure Python) has the highest accuracy among dependency parsers trained on the Treebank, and is also much faster than major parsers.",
        "^converted from constituency trees.",
        "C=C/C++, Py=Python, Ja=Java.",
        "Time is in seconds per sentence.",
        "Search spaces: ^linear; others exponential.",
        "(on a 3.2GHz Xeon CPU).",
        "Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower.",
        "Figure 8 shows the parse time in seconds for each test sentence.",
        "The observed time complexity ofour DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers.",
        "Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data.",
        "Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results.",
        "it",
        "update early% time",
        "update early% time",
        "1",
        "31943 98.9 22",
        "31189 87.7 29",
        "5",
        "20236 98.3 38",
        "19027 70.3 47",
        "17",
        "8683 97.1 48",
        "7434 49.5 60",
        "25",
        "5715 97.2 51",
        "4676 41.2 65",
        "word",
        "L",
        "time",
        "comp.",
        "McDonald 05b",
        "90.2",
        "Ja",
        "0.12",
        "O(n)",
        "McDonald 05a",
        "90.9",
        "Ja",
        "0.15",
        "o(n)",
        "Koo 08 base",
        "92.0",
        " – ",
        " – ",
        "O(n)",
        "Zhang 08 single",
        "91.4",
        "C",
        "0.11",
        "o(n)t",
        "this work",
        "92.1",
        "Py",
        "0.04",
        "O(n)",
        "tCharniak 00",
        "92.5",
        "C",
        "0.49",
        "O(n)",
        "ÏPetrov 07",
        "92.4",
        "Ja",
        "0.21",
        "o(n)",
        "Zhang 08 combo",
        "92.1",
        "C",
        " – ",
        "O(n)î",
        "Koo 08 semisup",
        "93.2",
        " – ",
        " – ",
        "O(n)",
        "Cha Berk MST sentence length",
        "Figure 8: Scatter plot of parsing time against sentence length, comparing with Charniak, Berkeley, and the O(n) MST parsers."
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS).",
        "Tomita uses GSS for exhaustive LR parsing, where the GSS is equivalent to a dynamic programming chart in chart parsing (see Footnote 4).",
        "In fact, Tomita's GLR is an instance of techniques for tabular simulation of non-deterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubic-time exhaustive shift-reduce parsing with context-free grammars (Billot and Lang, 1989).",
        "Our work advances this line of research in two aspects.",
        "First, ours is more general than GLR in",
        "report word accuracies, but those can be recovered given non-root and root ones, and the number ofnon-punctuation words.",
        "that it is not restricted to LR (a special case of shift-reduce), and thus does not require building an LR table, which is impractical for modern grammars with a large number of rules or features.",
        "In contrast, we employ the ideas behind GSS more flexibly to merge states based on features values, which can be viewed as constructing an implicit LR table on-the-fly.",
        "Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing.",
        "To the best of our knowledge, our work is the first linear-time incremental parser that performs dynamic programming.",
        "The parser of Roark and Hollingshead (2009) is also almost linear time, but they achieved this by discarding parts of the CKY chart, and thus do achieve incrementality."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search.",
        "This framework is general and applicable to a large-class ofshift-reduce parsers, as long as the feature functions satisfy boundedness and monotonicity.",
        "Empirical results on a state-the-art dependency parser confirm the advantage of DP in many aspects: faster speed, larger search space, higher oracles, and better and faster learning.",
        "Our final parser outperforms all previously reported dependency parsers trained on the Penn Treebanks for both English and Chinese, and is much faster in speed (even with a Python implementation).",
        "For future work we plan to extend it to constituency parsing."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank David Chiang, Yoav Goldberg, Jonathan Graehl, Kevin Knight, and Roger Levy for helpful discussions and the three anonymous reviewers for comments.",
        "Mark-Jan Nederhof inspired the use of prefix cost.",
        "Yue Zhang helped with Chinese datasets, and Wenbin Jiang with feature sets.",
        "This work is supported in part by DARPA GALE Contract No.",
        "HR0011-06-C-0022 under subcontract to BBN Technologies, and by the U.S. Army Research, Development, and Engineering Command (RDECOM).",
        "Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred."
      ]
    }
  ]
}
