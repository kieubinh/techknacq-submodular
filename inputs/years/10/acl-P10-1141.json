{
  "info": {
    "authors": [
      "Georgios Paltoglou",
      "Mike Thelwall"
    ],
    "book": "ACL",
    "id": "acl-P10-1141",
    "title": "A Study of Information Retrieval Weighting Schemes for Sentiment Analysis",
    "url": "https://aclweb.org/anthology/P10-1141",
    "year": 2010
  },
  "references": [
    "acl-H05-1044",
    "acl-N07-1033",
    "acl-N09-1057",
    "acl-P02-1053",
    "acl-P04-1035",
    "acl-P07-1056",
    "acl-P07-1124",
    "acl-W02-1011",
    "acl-W04-3253",
    "acl-W06-2915"
  ],
  "sections": [
    {
      "text": [
        "A study of Information Retrieval weighting schemes for sentiment analysis",
        "Georgios Paltoglou Mike Thelwall",
        "Wolverhampton, United Kingdom Wolverhampton, United Kingdom",
        "Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights.",
        "In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy.",
        "We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing.",
        "The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The increase of user-generated content on the web in the form of reviews, blogs, social networks, tweets, fora, etc.",
        "has resulted in an environment where everyone can publicly express their opinion about events, products or people.",
        "This wealth of information is potentially of vital importance to institutions and companies, providing them with ways to research their consumers, manage their reputations and identify new opportunities.",
        "Wright (2009) claims that \"for many businesses, online opinion has turned into a kind of virtual currency that can make or break a product in the marketplace\".",
        "Sentiment analysis, also known as opinion mining, provides mechanisms and techniques through which this vast amount of information can be processed and harnessed.",
        "Research in the field has mainly, but not exclusively, focused in two sub-problems: detecting whether a segment of text, either a whole document or a sentence, is subjective or objective, i.e. contains an expression of opinion, and detecting the overall polarity of the text, i.e. positive or negative.",
        "Most of the work in sentiment analysis has focused on supervised learning techniques (Sebas-tiani, 2002), although there are some notable exceptions (Turney, 2002; Lin and He, 2009).",
        "Previous research has shown that in general the performance of the former tend to be superior to that of the latter (Mullen and Collier, 2004; Lin and He, 2009).",
        "One of the main issues for supervised approaches has been the representation of documents.",
        "Usually a bag of words representation is adopted, according to which a document is modeled as an unordered collection of the words that it contains.",
        "Early research by Pang et al.",
        "(2002) in sentiment analysis showed that a binary unigram-based representation of documents, according to which a document is modeled only by the presence or absence of words, provides the best baseline classification accuracy in sentiment analysis in comparison to other more intricate representations using bigrams, adjectives, etc.",
        "Later research has focused on extending the document representation with more complex features such as structural or syntactic information (Wilson et al., 2005), favorability measures from diverse sources (Mullen and Collier, 2004), implicit syntactic indicators (Greene and Resnik, 2009), stylistic and syntactic feature selection (Abbasi et al., 2008), \"annotator rationales\" (Zaidan et al., 2007) and others, but no systematic study has been presented exploring the benefits of employing more sophisticated models for assigning weights to word features.",
        "In this paper, we examine whether term weighting functions adopted from Information Retrieval (IR) based on the standard tf.idf formula and adapted to the particular setting of sentiment analysis can help classification accuracy.",
        "We demonstrate that variants of the original tf.idf weighting scheme provide significant increases in classification performance.",
        "The advantages of the approach are that it is intuitive, computationally efficient and doesn't require additional human annotation or external sources.",
        "Experiments conducted on a number of publicly available data sets improve on the previous state-of-the art.",
        "The next section provides an overview of relevant work in sentiment analysis.",
        "In section 3 we provide a brief overview of the original tf.idf weighting scheme along with a number ofvariants and show how they can be applied to a classification scenario.",
        "Section 4 describes the corpora that were used to test the proposed weighting schemes and section 5 discusses the results.",
        "Finally, we conclude and propose future work in section 6."
      ]
    },
    {
      "heading": "2. Prior Work",
      "text": [
        "Sentiment analysis has been a popular research topic in recent years.",
        "Most of the work has focused on analyzing the content of movie or general product reviews, but there are also applications to other domains such as debates (Thomas et al., 2006; Lin et al., 2006), news (Devitt and Ahmad, 2007) and blogs (Ounis et al., 2008; Mishne, 2005).",
        "The book of Pang and Lee (2008) presents a thorough overview of the research in the field.",
        "This section presents the most relevant work.",
        "Pang et al.",
        "(2002) conducted early polarity classification of reviews using supervised approaches.",
        "They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others.",
        "They concluded that sentiment classification is more difficult that standard topic-based classification and that using a SVM classifier with binary unigram-based features produces the best results.",
        "A subsequent innovation was the detection and removal of the objective parts of documents and the application of a polarity classifier on the rest (Pang and Lee, 2004).",
        "This exploited text coherence with adjacent text spans which were assumed to belong to the same subjectivity or objectivity class.",
        "Documents were represented as graphs with sentences as nodes and association scores between them as edges.",
        "Two additional nodes represented the subjective and objective poles.",
        "The weights between the nodes were calculated using three different, heuristic decaying functions.",
        "Finding a partition that minimized a cost function separated the objective from the subjective sentences.",
        "They reported a statistically significant improvement over a Naive Bayes baseline using the whole text but only slight increase compared to using a SVM classifier on the entire document.",
        "Mullen and Collier (2004) used SVMs and expanded the feature set for representing documents with favorability measures from a variety of diverse sources.",
        "They introduced features based on Osgood's Theory of Semantic Differentiation (Osgood, 1967) using WordNet to derive the values of potency, activity and evaluative of adjectives and Turney's semantic orientation (Turney, 2002).",
        "Their results showed that using a hybrid SVM classifier, that uses as features the distance of documents from the separating hyperplane, with all the above features produces the best results.",
        "Whitelaw et al.",
        "(2005) added fine-grained semantic distinctions in the feature set.",
        "Their approach was based on a lexicon created in a semi-supervised fashion and then manually refined It consists of 1329 adjectives and their modifiers categorized under several taxonomies of appraisal attributes based on Martin and White's Appraisal Theory (2005).",
        "They combined the produced appraisal groups with unigram-based document representations as features to a Support Vector Machine classifier (Witten and Frank, 1999), resulting in significant increases in accuracy.",
        "Zaidan et al.",
        "(2007) introduced \"annotator rationales\", i.e. words or phrases that explain the polarity of the document according to human annotators.",
        "By deleting rationale text spans from the original documents they created several contrast documents and constrained the SVM classifier to classify them less confidently than the originals.",
        "Using the largest training set size, their approach significantly increased the accuracy on a standard data set (see section 4).",
        "Prabowo and Thelwall (2009) proposed a hybrid classification process by combining in sequence several ruled-based classifiers with a SVM classifier.",
        "The former were based on the General Inquirer lexicon (Wilson et al., 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words.",
        "Their experiments showed that combining multiple classifiers can result in better effectiveness than any individual classifier, especially when sufficient training data isn't available.",
        "In contrast to machine learning approaches that require labeled corpora for training, Lin and",
        "He (2009) proposed an unsupervised probabilistic modeling framework, based on Latent Dirich-let Allocation (LDA).",
        "The approach assumes that documents are a mixture of topics, i.e. probability distribution of words, according to which each document is generated through an hierarchical process and adds an extra sentiment layer to accommodate the opinionated nature (positive or negative) of the document.",
        "Their best attained performance, using a filtered subjectivity lexicon and removing objective sentences in a manner similar to Pang and Lee (2004), is only slightly lower than that of a fully-supervised approach."
      ]
    },
    {
      "heading": "3. A study of non-binary weights",
      "text": [
        "We use the terms \"features\", \"words\" and \"terms\" interchangeably in this paper, since we mainly focus on unigrams.",
        "The approach nonetheless can easily be extended to higher order n-grams.",
        "Each document D therefore is represented as a bag-of-words feature vector: D = {wi,w2,...,w\\y|} where |V| is the size of the vocabulary (i.e. the number of unique words) and wi, i = 1,..., | V| is the weight of term i in document D.",
        "Despite the significant attention that sentiment analysis has received in recent years, the best accuracy without using complex features (Mullen and Collier, 2004; Whitelaw et al., 2005) or additional human annotations (Zaidan et al., 2007) is achieved by employing a binary weighting scheme (Pang et al., 2002), where Wi = 1, if tfi > 0 and Wi = 0, if tfi = 0, where tfi is the number of times that term i appears in document D (henceforth raw term frequency) and utilizing a SVM classifier.",
        "It is of particular interest that using tfiin the document representation usually results in decreased accuracy, a result that appears to be in contrast with topic classification (Mccallum and Nigam, 1998; Pang et al., 2002).",
        "In this paper, we also utilize SVMs but our study is centered on whether more sophisticated than binary or raw term frequency weighting functions can improve classification accuracy.",
        "We base our approach on the classic tf.idf weighting scheme from Information Retrieval (IR) and adapt it to the domain of sentiment classification.",
        "The classic tf.idf formula assigns weight wi to term i in document D as:",
        "where tfi is the number of times term i occurs in D, idfi is the inverse document frequency of term i, N is the total number of documents and dfi is the number of documents that contain term i.",
        "The utilization of tfi in classification is rather straightforward and intuitive but, as previously discussed, usually results in decreased accuracy in sentiment analysis.",
        "On the other hand, using idf to assign weights to features is less intuitive, since it only provides information about the general distribution of term i amongst documents of all classes, without providing any additional evidence of class preference.",
        "The utilization of idf in information retrieval is based on its ability to distinguish between content-bearing words (words with some semantical meaning) and simple function words, but this behavior is at least ambiguous in classification.",
        "Table 1 : SMART notation for term frequency variants.",
        "maxtitf ) is the maximum frequency of any term in the document and avg-dl is the average number of terms in all the documents.",
        "For ease of reference, we also include the BM25 tf scheme.",
        "The k\\ and b parameters of BM25 are set to their default values of 1.2 and 0.95 respectively (Jones et al., 2000)._",
        "Martineau and Finin (2009) provide a solution to the above issue of idf utilization in a classification scenario by localizing the estimation of idf to the documents of one or the other class and subtracting the two values.",
        "Therefore, the weight of term",
        "Notation",
        "Term frequency",
        "n (natural)",
        "tf",
        "1 (logarithm)",
        "1 + log(tf)",
        "a (augmented)",
        "0.5+ u-^n",
        "maxt(tf)",
        "b (boolean)",
        "r l, tf>o",
        "\\ 0, otherwise",
        "L (log ave)",
        "l+log{tf) l-\\-log(avg-dl)",
        "o (BM25)",
        "fcl((-fe)+fe-^)+*/",
        "Table 2: SMART notation for inverse document frequency variants.",
        "For ease of reference we also include the BM25 idf factor and also present the extensions of the original formulations with their A variants._ i in document D is estimated as:",
        "where Nj is the total number of training documents in class Cj and dfij is the number of training documents in class Cj that contain term i.",
        "The above weighting scheme was appropriately named Delta tf.idf.",
        "The produced results (Martineau and Finin, 2009) show that the approach produces better results than the simple tf or binary weighting scheme.",
        "Nonetheless, the approach doesn't take into consideration a number oftested notions from IR, such as the non-linearity of term frequency to document relevancy (e.g. Robertson et al.",
        "(2004)) according to which, the probability of a document being relevant to a query term is typically sublinear in relation to the number of times a query term appears in the document.",
        "Additionally, their approach doesn't provide any sort of smoothing for the dfij factor and is therefore susceptible to errors in corpora where a term occurs in documents of only one or the other class and therefore dfi,j = 0.",
        "The SMART retrieval system by Salton (1971) is a retrieval system based on the vector space model (Salton and McGill, 1986).",
        "Salton and Buckley (1987) provide a number of variants of the tf.idf weighting approach and present the SMART notation scheme, according to which each weighting function is defined by triples of letters; the first one denotes the term frequency factor, the second one corresponds to the inverse document frequency function and the last one declares the normalization that is being applied.",
        "The upper rows of tables 1, 2 and 3 present the three most commonly used weighting functions for each factor respectively.",
        "For example, a binary document representation would be equivalent to SMART.bnnor more simply bnn, while a simple raw term frequency based would be notated as nnn or nnc with cosine normalization.",
        "_Table 3 : SMART normalization.",
        "Significant research has been done in IR on diverse weighting functions and not all versions of SMART notations are consistent (Manning et al., 2008).",
        "Zobel and Moffat (1998) provide an exhaustive study but in this paper, due to space constraints, we will follow the concise notation presented by Singhal et al.",
        "(1995).",
        "The BM25 weighting scheme (Robertson et al., 1994; Robertson et al., 1996) is a probabilistic model for information retrieval and is one of the most popular and effective algorithms used in information retrieval.",
        "For ease of reference, we incorporate the BM25 tf and idf factors into the SMART annotation scheme (last row of table 1 and 4th row of table 2), therefore the weight wiof term i in document D according to the BM25 scheme is notated as SMART.okn or okn.",
        "Most of the tf weighting functions in SMART and the BM25 model take into consideration the non-linearity of document relevance to term frequency and thus employ tf factors that scale sub-linearly in relation to term frequency.",
        "Additionally, the BM25 tf variant also incorporates a scaling for the length ofthe document, taking into consideration that longer documents will by definition have more term occurences .",
        "Effective weighting functions is a very active research area in information retrieval and it is outside the scope of this paper to provide an in-depth analysis but significant research can be found in Salton and McGill (1986), Robertson et al.",
        "(2004), Manning et al.",
        "(2008) or Armstrong et al.",
        "(2009) for a more recent study.",
        "Notation",
        "Inverse Document Frequency",
        "n (no)",
        "1",
        "t(idf)",
        "p (prob idf)",
        "log d/",
        "k (BM25 idf)",
        "7 N-df+0.5 LUy df+0.5",
        "A(t) (Delta idf)",
        "loaNvdh",
        "l0<JN?,dh",
        "A(t') (Delta smoothed idf)",
        "lnnNi-df2+0.5 luy N2-dh+0.5",
        "A(p) (Delta prob idf)",
        "i(jydfV(N2-df2)",
        "A(p') (Delta smoothed prob idf)",
        "i-ANi-dhydh+o.5",
        "l(Jy (N2-df2)-df1+0.5",
        "A(fc) (Delta BM25 idf)",
        "inr,{Ni-dh+0.5)-df2+0.5 l(Jy (N2-df2+0.5)-dh+0.5",
        "Notation",
        "Normalization",
        "n(none)",
        "1",
        "c (cosine)",
        "l",
        "^w\\+wl+...+w^",
        "We apply the idea of localizing the estimation of idf values to documents of one class but employ more sophisticated term weighting functions adapted from the SMART retrieval system and the BM25 probabilistic model.",
        "The resulting idf weighting functions are presented in the lower part of table 2.",
        "We extend the original SMART annotation scheme by adding Delta (A) variants of the original idf functions and additionally introduce smoothed Delta variants of the idf and the prob idffactors for completeness and comparative reasons, noted by their accented counterparts.",
        "For example, the weight of term i in document D according to the oA(k)n weighting scheme where we employ the BM25 tf weighting function and utilize the difference of class-based BM25 idf values would be calculated as:",
        "where K is defined as k\\ y(l – b) +b avg dlHowever, we used a minor variation of the above formulation for all the final accented weighting functions in which the smoothing factor is added to the product of dfi with Ni (or its variation for A(p') and A(k)), rather than to the dfi alone as the above formulation would imply (see table 2).",
        "The above variation was made for two reasons: firstly, when the dfi's are larger than 1 then the smoothing factor influences the final idf value only in a minor way in the revised formulation, since it is added only after the multiplication of the dfi with Ni (or its variation).",
        "Secondly, when dfi = 0, then the smoothing factor correctly adds only a small mass, avoiding a potential division by zero, where otherwise it would add a much greater mass, because it would be multiplied by Ni.",
        "According to this annotation scheme therefore, the original approach by Martineau and Finin (2009) can be represented as nA(t)n.",
        "We hypothesize that the utilization of sophisticated term weighting functions that have proved effective in information retrieval, thus providing an indication that they appropriately model the distinctive power of terms to documents and the smoothed, localized estimation of idf values will prove beneficial in sentiment classification.",
        "Table 4: Reported accuracies on the Movie Review data set.",
        "Only the best reported accuracy for each approach is presented, measured by 10-fold cross validation.",
        "The list is not exhaustive and because of differences in training/testing data splits the results are not directly comparable.",
        "It is produced here only for reference._",
        "Approach",
        "SVM with unigrams & binary weights (Pang et al., 2002), reported",
        "Hybrid SVM with Turney/Osgood Lemmas (Mullen and Collier, 2004)",
        "SVM with min-cuts (Pang and Lee,",
        "SVM with appraisal groups",
        "SVM with log likehood ratio feature selection (Aue and Gamon, 2005)",
        "SVM with annotator rationales",
        "LDA with filtered lexicon, subjectivity detection (Lin and He, 2009)",
        "Acc.",
        "The approach is straightforward, intuitive, computationally efficient, doesn't require additional human effort and takes into consideration standardized and tested notions from IR.",
        "The results presented in section 5 show that a number of weighting functions solidly outperform other state-of-the-art approaches.",
        "In the next section, we present the corpora that were used to study the effectiveness of different weighting schemes."
      ]
    },
    {
      "heading": "4. Experimental setup",
      "text": [
        "We have experimented with a number of publicly available data sets.",
        "The movie review dataset by Pang et al.",
        "(2002) has been used extensively in the past by a number of researchers (see Table 4), presenting the opportunity to compare the produced results with previous approaches.",
        "The dataset comprises 2,000 movie reviews, equally divided between positive and negative, extracted from the Internet Movie Database archive of the rec.arts.movies.reviews newsgroup.",
        "In order to avoid reviewer bias, only 20 reviews per author were kept, resulting in a total of 312 reviewers.",
        "The best attained accuracies by previous research on the specific data are presented in table 4.",
        "We do not claim that those results are directly comparable to ours, because of potential subtle differences in tokenization, classifier implementations etc, but we present them here for reference.",
        "The Multi-Domain Sentiment data set (MDSD) by Blitzer et al.",
        "(2007) contains Amazon reviews for four different product types: books, electronics, DVDs and kitchen appliances.",
        "Reviews with ratings of 3 or higher, on a 5-scale system, were labeled as positive and reviews with a rating less than 3 as negative.",
        "The data set contains 1,000 positive and 1,000 negative reviews for each product category for a total of8,000 reviews.",
        "Typically, the data set is used for domain adaptation applications but in our setting we only split the reviews between positive and negative .",
        "Lastly, we present results from the BLOGS06 (Macdonald and Ounis, 2006) collection that is comprised of an uncompressed 148GB crawl of approximately 100,000 blogs and their respective RSS feeds.",
        "The collection has been used for 3 consecutive years by the Text REtrieval Conferences (TREC).",
        "Participants of the conference are provided with the task of finding documents (i.e. web pages) expressing an opinion about specific entities X, which may be people, companies, films etc.",
        "The results are given to human assessors who then judge the content of the webpages (i.e. blog post and comments) and assign each webpage a score: \"1\" if the document contains relevant, factual information about the entity but no expression of opinion, \"2\" if the document contains an explicit negative opinion towards the entity and \"4\" is the document contains an explicit positive opinion towards the entity.",
        "We used the produced assessments from all 3 years of the conference in our data set, resulting in 150 different entity searches and, after duplicate removal, 7,930 negative documents (i.e. having an assessment of \"2\") and 9,968 positive documents (i.e. having an assessment of \"4\"), which were used as the \"gold standard\" .",
        "Documents are annotated at the document-level, rather than at the post level, making this data set somewhat noisy.",
        "Additionally, the data set is particularly large compared to the other ones, making classification especially challenging and interesting.",
        "More information about all data sets can be found at table 5.",
        "We have kept the preprocessing of the documents to a minimum.",
        "Thus, we have lower-cased all words and removed all punctuation but we have not removed stop words or applied stemming.",
        "We have also refrained from removing words with low or high occurrence.",
        "Additionally, for the BLOGS06 data set, we have removed all html formatting.",
        "We utilize the implementation of a support vector classifier from the LIBLINEAR library (Fan et al., 2008).",
        "We use a linear kernel and default parameters.",
        "All results are based on leave-one out cross validation accuracy.",
        "The reason for this choice of cross-validation setting, instead of the most standard ten-fold, is that all of the proposed approaches that use some form of idf utilize the training documents for extracting document frequency statistics, therefore more information is available to them in this experimental setting.",
        "Because of the high number of possible combinations between tf and idf variants (6' 9' 2 = 108) and due to space constraints we only present results from a subset of the most representative combinations.",
        "Generally, we'll use the cosine normalized variants of unsmoothed delta weighting schemes, since they perform better than their unTable 5 : Statistics about the data sets used.",
        "normalized counterparts.",
        "We'll avoid using normalization for the smoothed versions, in order to focus our attention on the results of smoothing, rather than normalization."
      ]
    },
    {
      "heading": "5. Results",
      "text": [
        "Results for the Movie Reviews, Multi-Domain Sentiment Dataset and BLOGS06 corpora are reported in figures 1, 2 and 3 respectively.",
        "On the Movie Review data set, the results reconfirm that using binary features (bnc) is better than raw term frequency (nnc) (83.40%) features.",
        "For reference, in this setting the unnor-malized vector using the raw tf approach (nnn) performs similar to the normalized (nnc) (83.40% vs. 83.60%), the former not present in the graph.",
        "Nonetheless, using any scaled tf weighting function (anc or onc) performs as well as the binary approach (87.90% and 87.50% respectively).",
        "Of interest is the fact that although the BM25 tf algorithm has proved much more successful in IR, the same doesn't apply in this setting and its accuracy is similar to the simpler augmented tfapproach.",
        "Incorporating un-localized variants ofidf (middle graph section) produces only small increases in accuracy.",
        "Smoothing also doesn't provide any particular advantage, e.g. btc (88.20%) vs. bt'c (88.45%), since no zero idf values are present.",
        "Again, using more sophisticated tf functions provides an advantage over raw tf, e.g. nt'c attains an accuracy of 86.6% in comparison to at'c's 88.25%, although the simpler at'c is again as effective than the BM25 tf (ot c), which performs at 88%.",
        "The actual idf weighting function is ofsome importance, e.g. ot'c (88%) vs. okc (87.65%) and akc (88%) vs. at'c (88.25%), with simpler idf factors performing similarly, although slightly better than BM25.",
        "Introducing smoothed, localized variants of idf and scaled or binary tf weighting schemes produces significant advantages.",
        "In this setting, smoothing plays a role, e.g. nA(t)c (91.60%) vs. nA(t')n (95.80%) and aA(p)c (92.80%) vs. aA(p')n (96.55%), since we can expect zero class-based estimations of idf values, supporting our initial hypothesis on its importance.",
        "Additionally, using augmented, BM25 or binary tf weights is always better than raw term frequency, providing further support on the advantages of using sublinear tf weighting functions .",
        "In this setting, the best accuracy of96.90% is attained using BM25 tf weights with the BM25 delta idf variant, although binary or augmented tf weights using delta idf perform similarly (96.50% and 96.60% respectively).",
        "The results indicate that the tf and the idf factor themselves aren't of significant importance, as long as the former are scaled and the latter smoothed in some manner.",
        "For example, aA(p')n vs. aA(t')n perform quite similarly.",
        "Data set",
        "^Documents",
        "# Terms",
        "#Unique Terms",
        "Average #Terms per Document",
        "Movie Reviews",
        "2,000",
        "1,336,883",
        "39,399",
        "668",
        "Multi-Domain Sentiment Dataset (MDSD)",
        "8,000",
        "1,741,085",
        "455,943",
        "217",
        "BLOGS06",
        "17,898",
        "51,252,850",
        "367,899",
        "2,832",
        "The results from the Multi-Domain Sentiment data set (figure 2) largely agree with the findings on the Movie Review data set, providing a strong indication that the approach isn't limited to a specific domain.",
        "Binary weights outperform raw term frequency weights and perform similarly with scaled tf's.",
        "Non-localized variants of idf weights do provide a small advantage in this data set although the actual idf variant isn't important, e.g. btc, bt c, and okc all perform similarly.",
        "The utilized tf variant also isn't important, e.g. at c (88.39%) vs. bt c (88.25%).",
        "We focus our attention on the delta idf variants which provide the more interesting results.",
        "The importance of smoothing becomes apparent when comparing the accuracy of aA(p)c and its smoothed variant aA(p')n (92.56% vs. 95.6%).",
        "Apart from that, all smoothed delta idf variants perform very well in this data set, including somewhat surprisingly, nA(t )n which uses raw tf (94.54%).",
        "Considering that the average tf per document is approx.",
        "1 .9 in the Movie Review data set and 1.1 in the MDSD, the results can be attributed to the fact that words tend to typically appear only once per document in the latter, therefore minimizing the difference of the weights attributed by different tf functions.",
        "The best attained accuracy is 96.40% but as the MDSD has mainly been used for domain adaptation applications, there is no clear baseline to compare it with.",
        "Lastly, we present results on the BLOGS06 dataset in figure 3.",
        "As previously noted, this data set is particularly noisy, because it has been annotated at the document-level rather than the postlevel and as a result, the differences aren't as profound as in the previous corpora, although they do follow the same patterns.",
        "Focusing on the delta idf variants, the importance of smoothing becomes apparent, e.g. aA(p)c vs. aA(p')n and nA(t)c vs. nA(t')n. Additionally, because of the fact that documents tend to be more verbose in this data set, the scaled tf variants also perform better than the simple raw tf ones, nA(t )n vs. aA(t )n. Lastly, as previously, the smoothed localized idf variants perform better than their un-smoothed counterparts, e.g. nA(t)n vs. nA(t )n and aA(p)c vs. aA(p')n."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "In this paper, we presented a study of document representations for sentiment analysis using term weighting functions adopted from information retrieval and adapted to classification.",
        "The proposed weighting schemes were tested on a number of publicly available datasets and a number of them repeatedly demonstrated significant increases in accuracy compared to other state-of-the-art approaches.",
        "We demonstrated that for accurate classification it is important to use term weighting functions that scale sublinearly in relation to the number of times a term occurs in a document and that document frequency smoothing is a significant factor.",
        "In the future we plan to test the proposed weighting functions in other domains such as topic classification and additionally extend the approach to accommodate multi-class classification."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported by a European Union grant by the 7th Framework Programme, Theme 3: Science of complex systems for socially intelligent ICT.",
        "It is part of the CyberEmotions Project (Contract 231323)."
      ]
    }
  ]
}
