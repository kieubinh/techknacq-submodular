{
  "info": {
    "authors": [
      "Haitao Mi",
      "Qun Liu"
    ],
    "book": "ACL",
    "id": "acl-P10-1145",
    "title": "Constituency to Dependency Translation with Forests",
    "url": "https://aclweb.org/anthology/P10-1145",
    "year": 2010
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Haitao Mi and Qun Liu",
        "Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O.",
        "Box 2704, Beijing 100190, China {htmi,liuqun}@ict.ac.cn",
        "Tree-to-string systems (and their forest-based extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via target-side syntax.",
        "We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality.",
        "Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules.",
        "This is also the first time that a tree-to-tree model can surpass tree-to-string counterparts."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Linguistically syntax-based statistical machine translation models have made promising progress in recent years.",
        "By incorporating the syntactic annotations of parse trees from both or either side(s) of the bitext, they are believed better than phrase-based counterparts in reorderings.",
        "Depending on the type of input, these models can be broadly divided into two categories (see Table 1): the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar, and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string.",
        "When we also take into account the type of output (tree or string), the tree-based systems can be divided into tree-to-string and tree-to-tree efforts.",
        "Table 1 : A classification and comparison of linguistically syntax-based SMT systems, where gram, denotes grammaticality of the output.",
        "On one hand, tree-to-string systems (Liu et al., 2006; Huang et al., 2006) have gained significant popularity, especially after incorporating packed forests (Mi et al., 2008; Mi and Huang, 2008; Liu et al., 2009; Zhang et al., 2009).",
        "Compared with their string-based counterparts, tree-based systems are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006; Huang et al., 2009), and can have separate grammars for parsing and translation (Huang et al., 2006).",
        "However, they have a major limitation that they do not have a principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output.",
        "On the other hand, string-to-tree systems explicitly model the grammaticality of the output by using target syntactic trees.",
        "Both string-to-constituency system (e.g., (Galley et al., 2006; Marcu et al., 2006)) and string-to-dependency model (Shen et al., 2008) have achieved significant improvements over the state-of-the-art formally syntax-based system Hiero (Chiang, 2007).",
        "However, those systems also have some limitations that they run slowly (in cubic time) (Huang et al., 2006), and do not utilize the useful syntactic information on the source side.",
        "We thus combine the advantages of both tree-to-string and string-to-tree approaches, and propose a novel constituency-to-dependency model, which uses constituency forests on the source side to direct translation, and dependency trees on the target side to guarantee grammaticality of the output.",
        "In contrast to conventional tree-to-tree approaches (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009), which only make use of a single type of trees, our model is able to combine two types of trees, outperforming both phrase-based and tree-to-string systems.",
        "Current tree-to-tree models (Xiong et al., 2007; Zhang et al., 2007; Liu et al., 2009) still have not outperformed the phrase-based system Moses (Koehn et al., 2007) significantly even with the help of forests.",
        "tree on",
        "examples (partial)",
        "fast",
        "gram.",
        "BLEU",
        "source target both",
        "Liu06, HuangOÔ GalleyOÔ, Shen08 Ding05, Liu09",
        "+ +",
        "+ +",
        "+ +",
        "both",
        "our work",
        "+",
        "+",
        "+",
        "Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constituency forests into target dependency trees with a set of features (Section 4).",
        "Medium data experiments (Section 5) show a statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with less translation rules, this is also the first time that a tree-to-tree model can surpass tree-to-string counterparts."
      ]
    },
    {
      "heading": "2. Model",
      "text": [
        "Figure 1 shows a word-aligned source constituency forest Fc and target dependency tree De, our constituency to dependency translation model can be formalized as:",
        "cceFc oeo",
        "= E Ellpw.",
        "CceFc oeO r£o where Cc is a constituency tree in Fc, o is a derivation that translates Cc to De, O is the set of derivation, r is a constituency to dependency translation rule.",
        "'According to the reports of Liu et al.",
        "(2009), their forest-based constituency-to-constituency system achieves a comparable performance against Moses (Koehn et al., 2007), but a significant improvement of +3.6 BLEU points over the 1-best tree-based constituency-to-constituency system.",
        "A constituency forest (in Figure 1 left) is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989).",
        "More formally, following Huang (2008), such a constituency forest is a pair Fc = = ( W, H?",
        "), where V?",
        "is the set of nodes, and the set of hyperedges.",
        "For a given source sentence c\\:m = c\\... cm, each node yf G is in the form ofXij, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, Cj+i... Cj).",
        "Each hy-peredge hß G H?",
        "is a pair (tails(hf), head(h^)), where head(h^) G is the consequent node in the deductive step, and tails (h?)",
        "G (V?",
        ")* is the list of antecedent nodes.",
        "For example, the hyper-edge hi in Figure 1 for deduction (*) is notated:",
        "((NPBo.i, CCi)2> NPB2;3), NP0)3>.",
        "tails(hf0) = {NPBo.i.CCi^NPBa.a}.",
        "The solid line in Figure 1 shows the best parse tree, while the dashed one shows the second best tree.",
        "Note that common sub-derivations like those for the verb VPB3 5 are shared, which allows the forest to represent exponentially many parses in a compact structure.",
        "We also denote IN(yf) to be the set of incoming hyperedges of node v?, which represents the different ways of deriving yf.",
        "Take node IPo,5 in Figure 1 for example, IN (IP 0,5) = {h{,h{}.",
        "There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply So,m where S is the start symbol and m is the sentence length.",
        "A dependency tree for a sentence represents each word and its syntactic dependents through directed arcs, as shown in the following examples.",
        "The main advantage of a dependency tree is that it can explore the long distance dependency.",
        "ziiNPBCCa^NPB",
        "Figure 2: Example of the rule T\\.",
        "The Chinese conjunction yü \"and\" is translated into English preposition \"with\".",
        "We use the lexicon dependency grammar (Hell-wig, 2006) to express a projective dependency tree.",
        "Take the dependency trees above for example, they will be expressed:",
        "where the lexicons in brackets represent the dependencies, while the lexicon out the brackets is the head.",
        "More formally, a dependency tree is also a pair De = Gd = (Vd,Hd).",
        "For a given target sentence ei:n = e\\... en, each node vd G Vd is a word (1 ^ i ^ n), each hyperedge hd G Hd is a directed arc (vf,vd) from node vf to its head node vd.",
        "Following the formalization of the constituency forest scenario, we denote a pair (tails(hd), head(hd)) tobe ahyperedge hd, where head(hd) is the head node, tails(hd) is the node where hd leaves from.",
        "We also denote Li(vd) and Lr(vd) to be the left and right children sequence of node vd from the nearest to the farthest respectively.",
        "Take the node v2 = \"held\" for example:",
        "U{vd) ={Bush}, Lr(vf) = {talk, with}.",
        "Actually, both the constituency forest and the dependency tree can be formalized as a hypergraph G, a pair (V, H).",
        "We use and Gd to distinguish them.",
        "For simplicity, we also use Fc and De to denote a constituency forest and a dependency tree respectively.",
        "Specifically, the size oîtails{hd) of a hyperedge hd in a dependency tree is a constant one."
      ]
    },
    {
      "heading": "3. Rule Extraction",
      "text": [
        "We extract constituency to dependency rules from word-aligned source constituency forest and target dependency tree pairs (Figure 1).",
        "We mainly extend the tree-to-string rule extraction algorithm of Mi and Huang (2008) to our scenario.",
        "In this section, we first formalize the constituency to string translation rule (Section 3.1).",
        "Then we present the restrictions for dependency structures as well formed fragments (Section 3.2).",
        "Finally, we describe our rule extraction algorithm (Section 3.3), fractional counts computation and probabilities estimation (Section 3.4).",
        "More formally, a constituency to dependency translation rule r is a tuple (lhs(r),rhs(r),<fi(r)), where lhs(r) is the source side tree fragment, whose internal nodes are labeled by nonterminal symbols (like NP and VP), and whose frontier nodes are labeled by source language words a (like \"yu\") or variables from a set X = {x\\, x2, ■ ■.",
        "}; rhs(r) is expressed in the target language dependency structure with words ej (like \"with\") and variables from the set X; and <fi(r) is a mapping from X to nonterminals.",
        "Each variable Xi G X occurs exactly once in lhs(r) and exactly once in rhs(r).",
        "For example, the rule r\\ in Figure 2, 4>{n) = {xi 1 ^ NPB, x2 1 ^ NPB, x3 1 ^ VPB}.",
        "Following Shen et al.",
        "(2008), we also restrict rhs(r) to be well formed dependency fragment.",
        "The main difference between us is that we use more flexible restrictions.",
        "Given a dependency",
        "Minimal rules extracted fragment d^j composed by the words from i to j, two kinds of well formed structures are defined as follows:",
        "Fixed on one node vdne, fixed for short, if it meets the following conditions:",
        "• the head of vdne is out of i.e.: Mhd, if tails(hd) = vdne => head(hd) £ e^.",
        "• the heads of other nodes except vdne are in",
        "i.e.: Vfc G and vf / vdme^hd if tails(hd) =vd^ head(hd) G ei:j.",
        "Floating with multi nodes M, floating for short, if it meets the following conditions:",
        "• all nodes in M have a same head node,",
        "i.e.: 3x £ [i,j],^hd if tails(hd) G M head(hd) = v%.",
        "• the heads of other nodes not in M are in",
        "i.e.: Vfc G and vdk $ M,\\Jhd if tails(hd) =vd^ head(hd) G ei:j.",
        "Take the \" (Bush) held ((a) talk))(with (Sharon)) \" for example: partial fixed examples are \" (Bush) held \" and \" held ((a) talk)\"; while the partial floating examples are \" (talk) (with (Sharon)) \" and \" ((a) talk) (with (Sharon)) \".",
        "Please note that the floating structure \" (talk) (with (Sharon)) \" can not be allowed in Shen et al.",
        "(2008)'s model.",
        "The dependency structure \" held ((a))\" is not a well formed structure, since the head of word \"a\" is out of scope of this structure.",
        "The algorithm shown in this Section is mainly extended from the forest-based tree-to-string extraction algorithm (Mi and Huang, 2008).",
        "We extract rules from word-aligned source constituency forest and target dependency tree pairs (see Figure 1) in three steps:",
        "(1) frontier set computation, (2) fragmentation, (3) composition.",
        "The frontier set (Galley et al., 2004) is the potential points to \"cut\" the forest and dependency tree pair into fragments, each of which will form a minimal rule (Galley et al., 2006).",
        "However, not every fragment can be used for rule extraction, since it may or may not respect to the restrictions, such as word alignments and well formed dependency structures.",
        "So we say a fragment is extractable if it respects to all restrictions.",
        "The root node of every extractable tree fragment corresponds to a faithful structure on the target side, in which case there is a \"transla-tional equivalence\" between the subtree rooted at the node and the corresponding target structure.",
        "For example, in Figure 1, every node in the forest is annotated with its corresponding English structure.",
        "The NPo,3 node maps to a non-contiguous structure \"(Bush) U (with (Sharon))\", the VV3,4 node maps to a contiguous but non-faithful structure \"held ((a) *)\".",
        "NPB (huitän) talk CC (yw) – > with P (yw) – > with NPB (Shälong) – > Sharon Algorithm 1 Forest-based constituency to dependency rule extraction.",
        "NPB2;3",
        "NPB4;5",
        "\"Sharon\" 1",
        "\"hold ({äff 1",
        "\"talk\" 1",
        "1",
        "Shälong",
        "1",
        "jüxingle",
        "1",
        "huitän",
        "talk)",
        "( with",
        "( Sharon ) )",
        "Input: Source constituency forest Fc, target dependency tree De, and alignment a Output: Minimal rule set 1Z l: fs < – Frontier(Fc, De,a) > compute frontier set 2: for each yf G fs do 4: while open / 0 do 6: if exps = 0 then > nothing to expand?",
        "7: generate a rule r using fragment hs > generate a rule 9: else > incomplete: further expand 10: v'< – exps.popQ > a non-frontier node ll: for each W G IN(v') do",
        "Following Mi and Huang (2008), given a source target sentence pair (ci:m, ei:ra) with an alignment a, the span of node on source forest is the set of target words aligned to leaf nodes under v?",
        ":",
        "span(v^) = {ei G ei:n | 3cj e yield(v*), (cj,ei) G a}.",
        "where the yield (v^) is all the leaf nodes under t>A For each span(v^), we also denote dep(yf) to be its corresponding dependency structure, which represents the dependency structure of all the words in span(v^).",
        "Take the span(PPi;3) ={with, Sharon} for example, the corresponding dep(PPi;3) is \"with (Sharon)\".",
        "A dep(vf) is faithful structure to node if it meets the following restrictions:",
        "• all words in span(v^) form a continuous substring ei:j,",
        "• every word in span(v^) is only aligned to leaf nodes of v?, i.e.: Ve^ G span(v^),(cj,ei) G a => Cj G yield(yf),",
        "• dep(yf) is a well formed dependency structure.",
        "For example, node VV3;4 has a non-faithful structure (crossed out in Figure 1), since its dep(VV3;4 = \" held ((a) *)\" is not a well formed structure, where the head of word \"a\" lies in the outside of its words covered.",
        "Nodes with faithful structure form the frontier set (shaded nodes in Figure 1) which serve as potential cut points for rule extraction.",
        "Given the frontier set, fragmentation step is to \"cut\" the forest at all frontier nodes and form tree fragments, each of which forms a rule with variables matching the frontier descendant nodes.",
        "For example, the forest in Figure 1 is cut into 10 pieces, each of which corresponds to a minimal rule listed on the right.",
        "Our rule extraction algorithm is formalized in Algorithm 1.",
        "After we compute the frontier set fs (line 1).",
        "We visit each frontier node G fs on the source constituency forest Fc, and keep a queue open of growing fragments rooted at .",
        "We keep expanding incomplete fragments from open, and extract a rule if a complete fragment is found (line 7).",
        "Each fragment hs in open is associated with a list of expansion sites (exps in line 5) being the subset of leaf nodes of the current fragment that are not in the frontier set.",
        "So each fragment along hyperedge h is associated with exps = tails(hf ) \\fs.",
        "A fragment is complete if its expansion sites is empty (line 6), otherwise we pop one expansion node v to grow and spin-off new fragments by following hyperedges off', adding new expansion sites (lines 11-13), until all active fragments are complete and open queue is empty (line 4).",
        "After we get all the minimal rules, we glue them together to form composed rules following Galley et al.",
        "(2006).",
        "For example, the composed rule r\\ in Figure 2 is glued by the following two minimal rules:",
        "where a^CC in r2 is replaced with r3 accordingly.",
        "Following Mi and Huang (2008), we penalize a rule r by the posterior probability of the corresponding constituent tree fragment lhs(r), which can be computed in an Inside-Outside fashion, being the product of the outside probability of its root node, the inside probabilities of its leaf nodes, and the probabilities of hyperedges involved in the fragment.",
        "G leaves(lhs(r)) where root(r) is the root of the rule r, a(v) and ß(v) are the outside and inside probabilities of node v, and leaves(lhs(r)) returns the leaf nodes of a tree fragment lhs(r).",
        "We use fractional counts to compute three conditional probabilities for each rule, which will be used in the next section:"
      ]
    },
    {
      "heading": "4. Decoding",
      "text": [
        "Given a source forest Fc, the decoder searches for the best derivation o* among the set of all possible derivations O, each of which forms a source side constituent tree Tc(o), a target side string e(o), and a target side dependency tree De(o):",
        "&TceFc,oeo where the first two terms are translation and language model probabilities, e(o) is the target string (English sentence) for derivation o, the third and forth items are the dependency language model probabilities on the target side computed with words and POS tags separately, De(o) is the target dependency tree of o, the fifth one is the parsing probability of the source side tree Tc(o) G Fc, the ill(o) is the penalty for the number of ill-formed dependency structures in o, and the last two terms are derivation and translation length penalties, respectively.",
        "The conditional probability P(o | Tc) is decomposes into the product of rule probabilities:",
        "where each P(r) is the product of five probabilities:",
        "where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.4, and the last two are lexical probabilities.",
        "When computing the lexical translation probabilities described in (Koehn et al., 2003), we only take into accout the terminals in a rule.",
        "If there is no terminal, we set the lexical probability to 1.",
        "The decoding algorithm works in a bottom-up search fashion by traversing each node in forest Fc.",
        "We first use pattern-matching algorithm of Mi et al.",
        "(2008) to convert Fc into a translation forest, each hyperedge of which is associated with a constituency to dependency translation rule.",
        "However, pattern-matching failure at a node will cut the derivation path and lead to translation failure.",
        "To tackle this problem, we construct a pseudo translation rule for each parse hyperedge hß e IN(yf) by mapping the CFG rule into a target dependency tree using the head rules of Magerman (1995).",
        "Take the hyperedge hßQ in Figure 1 for example, the corresponding pseudo translation rule is:",
        "c(r)",
        "Er':lfa(r')=lfa(r)",
        "c(r') '",
        "c(r)",
        "5^r' :rhs(r')=rhs(r)",
        "c(r') '",
        "c(r)",
        "since the £3:NPB is the head word of the CFG rule: NP -»■ NPB CC NPB.",
        "After the translation forest is constructed, we traverse each node in translation forest also in bottom-up fashion.",
        "For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4.1).",
        "If all the nodes are visited, we trace back along the 1-best derivation at goal item So,m and build a target side dependency tree.",
        "For A;-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives.",
        "We compute the score of a dependency language model for a dependency tree De in the same way proposed by Shen et al.",
        "(2008).",
        "For each nonterminal node v% = eh in De and its children sequences Li = eh, ei2...eii and Lr = eri,er2...erj, the probability of a trigram is computed as follows:",
        "where the P(Li | e/j§) is decomposed to be:",
        "•p(ei„ I ein_1,ein_2).",
        "We use the suffix \"§\" to distinguish the head word and child words in the dependency language model.",
        "In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree.",
        "We store the POS tag information on the target side for each constituency-to-dependency rule.",
        "So we will also generate a POS taged dependency tree simultaneously at the decoding time.",
        "We calculate this dependency language model by simply replacing each ei in equation 9 with its tag t(ei)."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "Our training corpus consists of 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively.",
        "We first word-align them by GIZA++ (Och and Ney, 2000) with refinement option \"grow-diag-and\" (Koehn et al., 2003), and then parse the Chinese sentences using the parser of Xiong et al.",
        "(2005) into parse forests, which are pruned into relatively small forests with a pruning threshold 3.",
        "We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)'s head rules.",
        "We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately.",
        "Finally, we apply translation rule extraction algorithm described in Section 3.",
        "We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Giga-word corpus.",
        "At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4).",
        "We use the 2002 NIST MT Evaluation test set as our development set and the 2005 NIST MT Evaluation test set as our test set.",
        "We evaluate the translation quality using the BLEU-4 metric (Pap-ineni et al., 2002), which is calculated by the script mteval-vllb.pl with its default setting which is case-insensitive matching of n-grams.",
        "We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system's BLEU score on development set.",
        "Table 2 shows the results on the test set.",
        "Our baseline system is a state-of-the-art forest-based constituency-to-string model (Mi et al., 2008), or forest c2s for short, which translates a source forest into a target string by pattern-matching the constituency-to-string (c2s) rules and the bilingual phrases (s2s).",
        "The baseline system extracts 31.9M c2s rules, 77.9M s2s rules respectively and achieves a BLEU score of 34.17 on the test set.",
        "At first, we investigate the influence of different rule sets on the performance of baseline system.",
        "We first restrict the target side of translation rules to be well-formed structures, and we extract 13.8M constituency-to-dependency (c2d) rules, which is 43% of c2s rules.",
        "We also extract 9.0M string-to-dependency (s2d) rules, which is only 11.6% of s2s rules.",
        "Then we convert c2d and s2d rules to c2s and s2s rules separately by removing the target-dependency structures and feed them into the baseline system.",
        "As shown in the third line in the column of BLEU score, the performance drops 1.7 BLEU points over baseline system due to the poorer rule coverage.",
        "However, when we further use all s2s rules instead of s2d rules in our next experiment, it achieves a BLEU score of 34.03, which is very similar to the baseline system.",
        "Those results suggest that restrictions on c2s rules won't hurt the performance, but restrictions on s2s will hurt the translation quality badly.",
        "So we should utilize all the s2s rules in order to preserve a good coverage of translation rule set.",
        "The last two lines in Table 2 show the results of our new forest-based constituency-to-dependency model (forest c2d for short).",
        "When we only use c2d and s2d rules, our system achieves a BLEU score of 33.25, which is lower than the baseline system in the first line.",
        "But, with the same rule set, our model still outperform the result in the second line.",
        "This suggests that using dependency language model really improves the translation quality by less than 1 BLEU point.",
        "In order to utilize all the s2s rules and increase the rule coverage, we parse the target strings of the s2s rules into dependency fragments, and construct the pseudo s 2d rules (s2s-dep).",
        "Then we use c2d and s2s-dep rules to direct the translation.",
        "With the help of the dependency language model, our new model achieves a significant improvement of +0.7 BLEU points over the forest c2s baseline system (p < 0.05, using the sign-test suggested by",
        "Table 2: Statistics of different types of rules extracted on training corpus and the BLEU scores on the test set.",
        "Collins et al.",
        "(2005)).",
        "For the first time, a tree-to-tree model can surpass tree-to-string counterparts significantly even with fewer rules."
      ]
    },
    {
      "heading": "6. Related Work",
      "text": [
        "The concept of packed forest has been used in machine translation for several years.",
        "For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models.",
        "Mi et al.",
        "(2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation.",
        "Following this direction, Liu et al.",
        "(2009) and Zhang et al.",
        "(2009) apply forest into tree-to-tree (Zhang et al., 2007) and tree-sequence-to-string models(Liu et al., 2007) respectively.",
        "Different from Liu et al.",
        "(2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model.",
        "Shen et al.",
        "(2008) present a string-to-dependency model.",
        "They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations.",
        "This model shows a significant improvement over the state-of-the-art hierarchical phrase-based system (Chiang, 2005).",
        "Compared with this work, we put fewer restrictions on the definition of well-formed dependency structures in order to extract more rules; the other difference is that we can also extract more expressive constituency to dependency rules, since the source side of our rule can encode multilevel reordering and contain more variables being larger than two; furthermore, our rules can be pattern-matched at high level, which is more reasonable than using glue rules in Shen et al.",
        "(2008)'s scenario; finally, the most important one is that our model runs very faster.",
        "System",
        "Rule Set",
        "BLEU",
        "Type",
        "#",
        "forest c2s",
        "c2s s2s",
        "31.9M 77.9M",
        "34.17",
        "c2d s2d",
        "13.8M 9.0M",
        "32.48(11.7)",
        "c2d",
        "s2s",
        "13.8M 77.9M",
        "34.03(10.1)",
        "forest c2d",
        "c2d s2d",
        "13.8M 9.0M",
        "33.25(10.9)",
        "c2d s2s-dep",
        "13.8M 77.9M",
        "34.88(10.7)",
        "Liu et al.",
        "(2009) propose a forest-based constituency-to-constituency model, they put more emphasize on how to utilize parse forest to increase the tree-to-tree rule coverage.",
        "By contrast, we only use 1-best dependency trees on the target side to explore long distance relations and extract translation rules.",
        "Theoretically, we can extract more rules since dependency tree has the best interlingual phrasal cohesion properties (Fox, 2002)."
      ]
    },
    {
      "heading": "7. Conclusion and Future Work",
      "text": [
        "In this paper, we presented a novel forest-based constituency-to-dependency translation model, which combines the advantages of both tree-to-string and string-to-tree systems, runs fast and guarantees grammaticality of the output.",
        "To learn the constituency-to-dependency translation rules, we first identify the frontier set for all the nodes in the constituency forest on the source side.",
        "Then we fragment them and extract minimal rules.",
        "Finally, we glue them together to be composed rules.",
        "At the decoding step, we first parse the input sentence into a constituency forest.",
        "Then we convert it into a translation forest by patter-matching the constituency to string rules.",
        "Finally, we traverse the translation forest in a bottom-up fashion and translate it into a target dependency tree by incorporating string-based and dependency-based language models.",
        "Using all constituency-to-dependency translation rules and bilingual phrases, our model achieves +0.7 points improvement in BLEU score significantly over a state-of-the-art forest-based tree-to-string system.",
        "This is also the first time that a tree-to-tree model can surpass tree-to-string counterparts.",
        "In the future, we will do more experiments on rule coverage to compare the constituency-to-constituency model with our model.",
        "Furthermore, we will replace 1-best dependency trees on the target side with dependency forests to further increase the rule coverage.",
        "Acknowledgement",
        "The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 90920004, and 863 State Key Project No.",
        "2006AA010108.",
        "We thank the anonymous reviewers for their insightful comments.",
        "We are also grateful to Liang Huang for his valuable suggestions."
      ]
    }
  ]
}
