{
  "info": {
    "authors": [
      "Simone Paolo Ponzetto",
      "Roberto Navigli"
    ],
    "book": "ACL",
    "id": "acl-P10-1154",
    "title": "Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems",
    "url": "https://aclweb.org/anthology/P10-1154",
    "year": 2010
  },
  "references": [
    "acl-C08-1021",
    "acl-C92-2082",
    "acl-D08-1080",
    "acl-E06-1002",
    "acl-E09-1005",
    "acl-E09-1068",
    "acl-H05-1053",
    "acl-H93-1061",
    "acl-J03-4004",
    "acl-J06-1005",
    "acl-J96-2004",
    "acl-N07-1025",
    "acl-N09-1004",
    "acl-P06-1100",
    "acl-P85-1037",
    "acl-P98-2181",
    "acl-W01-0703",
    "acl-W02-0817",
    "acl-W06-1663",
    "acl-W07-2006",
    "acl-W07-2054",
    "acl-W07-2068",
    "acl-W08-2231",
    "acl-W99-0501"
  ],
  "sections": [
    {
      "text": [
        "Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems",
        "Simone Paolo Ponzetto Roberto Navigli",
        "Heidelberg University Sapienza Universita di Roma",
        "ponzetto@cl.uni-heidelberg.de navigli@di.uniroma1.it",
        "One of the main obstacles to high-performance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck.",
        "In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia.",
        "We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Knowledge lies at the core of Word Sense Disambiguation (WSD), the task of computationally identifying the meanings of words in context (Navigli, 2009b).",
        "In the recent years, two main approaches have been studied that rely on a fixed sense inventory, i.e., supervised and knowledge-based methods.",
        "In order to achieve high performance, supervised approaches require large training sets where instances (target words in context) are hand-annotated with the most appropriate word senses.",
        "Producing this kind of knowledge is extremely costly: at a throughput of one sense annotation per minute (Edmonds, 2000) and tagging one thousand examples per word, dozens of person-years would be required for enabling a supervised classifier to disambiguate all the words in the English lexicon with high accuracy.",
        "In contrast, knowledge-based approaches exploit the information contained in wide-coverage lexical resources, such as WordNet (Fellbaum, 1998).",
        "However, it has been demonstrated that the amount of lexical and semantic information contained in such resources is typically insufficient for high-performance WSD (Cuadros and Rigau, 2006).",
        "Several methods have been proposed to automatically extend existing resources (cf.",
        "Section 2) and it has been shown that highly-interconnected semantic networks have a great impact on WSD (Navigli and Lapata, 2010).",
        "However, to date, the real potential of knowledge-rich WSD systems has been shown only in the presence of either a large manually-developed extension of WordNet (Navigli and Velardi, 2005) or sophisticated WSD algorithms (Agirre et al., 2009).",
        "The contributions of this paper are two-fold.",
        "First, we relieve the knowledge acquisition bottleneck by developing a methodology to extend WordNet with millions of semantic relations.",
        "The relations are harvested from an encyclopedic resource, namely Wikipedia.",
        "Wikipedia pages are automatically associated with WordNet senses, and topical, semantic associative relations from Wikipedia are transferred to WordNet, thus producing a much richer lexical resource.",
        "Second, two simple knowledge-based algorithms that exploit our extended WordNet are applied to standard WSD datasets.",
        "The results show that the integration of vast amounts of semantic relations in knowledge-based systems yields performance competitive with state-of-the-art supervised approaches on open-text WSD.",
        "In addition, we support previous findings from Agirre et al.",
        "(2009) that in a domain-specific WSD scenario knowledge-based systems perform better than supervised ones, and we show that, given enough knowledge, simple algorithms perform better than more sophisticated ones."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "In the last three decades, a large body of work has been presented that concerns the development of automatic methods for the enrichment of existing resources such as WordNet.",
        "These include proposals to extract semantic information from dictionaries (e.g. Chodorow et al.",
        "(1985) and Rigau et al.",
        "(1998)), approaches using lexico-syntactic patterns (Hearst, 1992; Cimiano et al., 2004; Girju et al., 2006), heuristic methods based on lexical and semantic regularities (Harabagiu et al., 1999), taxonomy-based ontologization (Pen-nacchiotti and Pantel, 2006; Snow et al., 2006).",
        "Other approaches include the extraction ofseman-tic preferences from sense-annotated (Agirre and Martinez, 2001) and raw corpora (McCarthy and Carroll, 2003), as well as the disambiguation of dictionary glosses based on cyclic graph patterns (Navigli, 2009a).",
        "Other works rely on the disambiguation of collocations, either obtained from specialized learner's dictionaries (Navigli and Ve-lardi, 2005) or extracted by means of statistical techniques (Cuadros and Rigau, 2008), e.g. based on the method proposed by Agirre and de Lacalle (2004).",
        "But while most of these methods represent state-of-the-art proposals for enriching lexical and taxonomic resources, none concentrates on augmenting WordNet with associative semantic relations for many domains on a very large scale.",
        "To overcome this limitation, we exploit Wikipedia, a collaboratively generated Web encyclopedia.",
        "The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002).",
        "However, its current status indicates that the project remains a mainly academic attempt.",
        "In contrast, due to its low entrance barrier and vast user base, Wikipedia provides large amounts of information at practically no cost.",
        "Previous work aimed at transforming its content into a knowledge base includes open-domain relation extraction (Wu and Weld, 2007), the acquisition of taxonomic (Ponzetto and Strube, 2007a; Suchanek et al., 2008; Wu and Weld, 2008) and other semantic relations (Nastase and Strube, 2008), as well as lexical reference rules (Shnarch et al., 2009).",
        "Applications using the knowledge contained in Wikipedia include, among others, text categorization (Gabrilovich and Markovitch, 2006), computing semantic similarity of texts (Gabrilovich and Markovitch, 2007; Ponzetto and erence resolution (Ponzetto and Strube, 2007b), multi-document summarization (Nastase, 2008), and text generation (Sauper and Barzilay, 2009).",
        "In our work we follow this line of research and show that knowledge harvested from Wikipedia can be used effectively to improve the performance of a WSD system.",
        "Our proposal builds on previous insights from Bunescu and PaÂ§ca (2006) and Mihalcea (2007) that pages in Wikipedia can be taken as word senses.",
        "Mihalcea (2007) manually maps Wikipedia pages to WordNet senses to perform lexical-sample WSD.",
        "We extend her proposal in three important ways: (1) we fully automatize the mapping between Wikipedia pages and WordNet senses; (2) we use the mappings to enrich an existing resource, i.e. WordNet, rather than annotating text with sense labels; (3) we deploy the knowledge encoded by this mapping to perform unrestricted WSD, rather than apply it to a lexical sample setting.",
        "Knowledge from Wikipedia is injected into a WSD system by means of a mapping to WordNet.",
        "Previous efforts aimed at automatically linking Wikipedia to WordNet include full use of the first WordNet sense heuristic (Suchanek et al., 2008), a graph-based mapping of Wikipedia categories to WordNet synsets (Ponzetto and Nav-igli, 2009), a model based on vector spaces (Ruiz-Casado et al., 2005) and a supervised approach using keyword extraction (Reiter et al., 2008).",
        "These latter methods rely only on text overlap techniques and neither they take advantage of the input from Wikipedia being semi-structured, e.g. hyperlinked, nor they propose a high-performing probabilistic formulation of the mapping problem, a task to which we turn in the next section."
      ]
    },
    {
      "heading": "3. Extending WordNet",
      "text": [
        "Our approach consists of two main phases: first, a mapping is automatically established between Wikipedia pages and WordNet senses; second, the relations connecting Wikipedia pages are transferred to WordNet.",
        "As a result, an extended version of WordNet is produced, that we call Word-Net++.",
        "We present the two resources used in our methodology in Section 3.1.",
        "Sections 3.2 and 3.3 illustrate the two phases of our approach.",
        "WordNet.",
        "Being the most widely used computational lexicon of English in Natural Language Processing, WordNet is an essential resource for WSD.",
        "A concept in WordNet is represented as a synonym set, or synset, i.e. the set of words which share a common meaning.",
        "For instance, the concept of soda drink is expressed as:",
        "where each word's subscripts and superscripts indicate their parts of speech (e.g. n stands for noun) and sense number, respectively.",
        "For each synset, WordNet provides a textual definition, or gloss.",
        "For example, the gloss of the above synset is: \"a sweet drink containing carbonated water and flavoring\".",
        "Wikipedia.",
        "Our second resource, Wikipedia, is a collaborative Web encyclopedia composed of pages.",
        "A Wikipedia page (henceforth, Wikipage) presents the knowledge about a specific concept (e.g.",
        "Soda (soft drink)) or named entity (e.g. Food Standards Agency).",
        "The page typically contains hypertext linked to other relevant Wikipages.",
        "For instance, Soda (soft drink) is linked to Cola, Flavored water, Lemonade, and many others.",
        "The title of a Wikipage (e.g.",
        "Soda (soft drink)) is composed of the lemma of the concept defined (e.g. soda) plus an optional label in parentheses which specifies its meaning in case the lemma is ambiguous (e.g. SOFT DRINK vs.",
        "SODIUM CARBONATE).",
        "Finally, some Wikipages are redirections to other pages, e.g.",
        "SODA (SODIUM CARBONATE) redirects to SODIUM CARBONATE.",
        "During the first phase of our methodology we aim to establish links between Wikipages and WordNet senses.",
        "Formally, given the entire set of pages SensesWiki and WordNet senses SensesWN, we aim to acquire a mapping:",
        "fji .",
        "SensesWiki â ^ SensesWN, such that, for each Wikipage w e SensesWiki:",
        "{ s e SensesWN(w) if a link can be established, e otherwise,",
        "where SensesWN(w) is the set of senses of the lemma of w in WordNet.",
        "For example, if our mapping methodology linked SODA (SOFT DRINK) to the corresponding WordNet sense soda;;, we would have /(Soda (soft drink)) = soda;;.",
        "In order to establish a mapping between the two resources, we first identify different kinds of disambiguation contexts for Wikipages (Section 3.2.1) and WordNet senses (Section 3.2.2).",
        "Next, we intersect these contexts to perform the mapping (see Section 3.2.3).",
        "Given a target Wikipage w which we aim to map to a WordNet sense of w, we use the following information as a disambiguation context:",
        "â¢ Sense labels: e.g. given the page Soda (soft drink), the words soft and drink are added to the disambiguation context.",
        "â¢ Links: the titles' lemmas of the pages linked from the Wikipage w (outgoing links).",
        "For instance, the links in the Wikipage SODA (SOFT drink) include soda, lemonade, sugar, etc.",
        "â¢ Categories: Wikipages are classified according to one or more categories, which represent meta-information used to categorize them.",
        "For instance, the Wikipage SODA (SOFT DRINK) is categorized as SOFT DRINKS.",
        "Since many categories are very specific and do not appear in WordNet (e.g., SWEDISH WRITERS or SCIENTISTS WHO COMMITTED SUICIDE), we use the lemmas of their syntactic heads as disambiguation context (i.e. writer and scientist).",
        "To this end, we use the category heads provided by Ponzetto and Navigli (2009).",
        "Given a Wikipage w, we define its disambiguation context Ctx(w) as the set of words obtained from some or all of the three sources above.",
        "Given a WordNet sense s and its synset S, we use the following information as disambiguation context to provide evidence for a potential link in our mapping",
        "â¢ Synonymy: all synonyms of s in synset S. For instance, given the synset of soda;;, all its synonyms are included in the context (that is, tonic, soda pop, pop, etc.",
        ").",
        "â¢ Hypernymy/Hyponymy: all synonyms in the synsets H such that H is either a hypernym (i.e., a generalization) or a hyponym (i.e., a specialization) of S. For example, given soda;;, we include the words from its hypernym { soft drink; }.",
        "â¢ Sisterhood: words from the sisters of S. A sister synset S' is such that S and S' have a common direct hypernym.",
        "For example, given soda;;, it can be found that bitter lemon; and soda;; are sisters.",
        "Thus the words bitter and lemon are included in the disambiguation context of s.",
        "â¢ Gloss: the set of lemmas of the content words occurring within the gloss of s. For instance, given s = soda;;, defined as \"a sweet drink containing carbonated water and flavoring\", we add to the disambiguation context of s the following lemmas: sweet, drink, contain, carbonated, water, flavoring.",
        "Given a WordNet sense s, we define its disambiguation context Ctx(s) as the set of words obtained from some or all of the four sources above.",
        "In order to link each Wikipedia page to a WordNet sense, we developed a novel algorithm, whose pseudocode is shown in Algorithm 1.",
        "The following steps are performed:",
        "â¢ Initially (lines 1-2), our mapping / is empty, i.e. it links each Wikipage w to e.",
        "â¢ For each Wikipage w whose lemma is monose-mous both in Wikipedia and WordNet (i.e.",
        "|SensesWiki(w)| = |SensesWN(w)| = 1) we map w to its only WordNet sense (lines 3-5).",
        "â¢ Finally, for each remaining Wikipage w for which no mapping was previously found (i.e., / ( w) = e, line 7), we do the following:",
        "- lines 8-10: for each Wikipage d which is a redirection to w, for which a mapping was previously found (i.e. / (d) = e, that is, d is monosemous in both Wikipedia and WordNet) and such that it maps to a sense / (d) in a synset S that also contains a sense of w, we map w to the corresponding sense in S.",
        "- lines 11-14: if a Wikipage w has not been linked yet, we assign the most likely sense to w based on the maximization of the conditional probabilities p(s|w) over the senses Algorithm 1 The mapping algorithm",
        "Input: SensesWiki, SensesWNOutput: a mapping / : Â£ensesWiki â Â£ensesWN 1: for each w e SensesWiki2: /(w) := e 3: for each w e SensesWiki 6: for each w e SensesWiki7: if /(w) = e then 8: for each d e SensesWiki s.t.",
        "d redirects to w 11: for each w e SensesWiki 13: if no tie occurs then",
        "s e SensesWN(w) (no mapping is established if a tie occurs, line 13).",
        "As a result of the execution of the algorithm, the mapping / is returned (line 15).",
        "At the heart of the mapping algorithm lies the calculation of the conditional probability p(s|w) of selecting the WordNet sense s given the Wikipage w. The sense s which maximizes this probability can be obtained as follows:",
        "seSe;isesWN(w) s p(w)",
        "The latter formula is obtained by observing that p(w) does not influence our maximization, as it is a constant independent of s. As a result, the most appropriate sense s is determined by maximizing the joint probability p(s, w) of sense s and page w. We estimate p(s,w) as:",
        "score(s, w)",
        "w eSe;sesWiki(u)",
        "where score(s,w) = |Ctx(s) n Ctx(w)| + 1 (we add 1 as a smoothing factor).",
        "Thus, in our algorithm we determine the best sense s by computing the intersection of the disambiguation contexts of s and w, and normalizing by the scores summed over all senses of w in Wikipedia and WordNet.",
        "We illustrate the execution of our mapping algorithm by way of an example.",
        "Let us focus on the",
        "Wikipage Soda (soft drink).",
        "The word soda is polysemous both in Wikipedia and WordNet, thus lines 3-5 of the algorithm do not concern this Wikipage.",
        "Lines 6-14 aim to find a mapping /(Soda (soft drink)) to an appropriate WordNet sense of the word.",
        "First, we check whether a redirection exists to Soda (soft drink) that was previously disambiguated (lines 8-10).",
        "Next, we construct the disambiguation context for the Wikipage by including words from its label, links and categories (cf.",
        "Section 3.2.1).",
        "The context includes, among others, the following words: soft, drink, cola, sugar.",
        "We now construct the disambiguation context for the two WordNet senses of soda (cf.",
        "Section 3.2.2), namely the sodium carbonate (#1) and the drink (#2) senses.",
        "To do so, we include words from their synsets, hypernyms, hyponyms, sisters, and glosses.",
        "The context for soda; includes: salt, acetate, chlorate, benzoate.",
        "The context for soda;; contains instead: soft, drink, cola, bitter, etc.",
        "The sense with the largest intersection is #2, so the following mapping is established: /(Soda (soft drink)) = soda;;.",
        "The output of the algorithm presented in the previous section is a mapping between Wikipages and WordNet senses (that is, implicitly, synsets).",
        "Our insight is to use this alignment to enable the transfer of semantic relations from Wikipedia to Word-Net.",
        "In fact, given a Wikipage w we can collect all Wikipedia links occurring in that page.",
        "For any such link from w to w', if the two Wikipages are mapped to WordNet senses (i.e., /(w) = e and /( w') = e), we can transfer the corresponding edge (/(w), /(w')) to WordNet.",
        "Note that /(w) and /( w') are noun senses, as Wikipages describe nominal concepts or named entities.",
        "We refer to this extended resource as WordNet++.",
        "For instance, consider the Wikipage Soda (soft drink).",
        "This page contains, among others, a link to the Wikipage Syrup.",
        "Assuming /(Soda (soda drink)) = soda;; and /(Syrup) = syrup;, we can add the corresponding semantic relation (soda;;, syrup^) to WordNet.",
        "Thus, WordNet++ represents an extension of WordNet which includes semantic associative relations between synsets.",
        "These are originally found in Wikipedia and then integrated into WordNet by means of our mapping.",
        "In turn, Word-Net++ represents the English-only subset of a larger multilingual resource, BabelNet (Navigli and Ponzetto, 2010), where lexicalizations of the synsets are harvested for many languages using the so-called Wikipedia inter-language links and applying a machine translation system."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We perform two sets of experiments: we first evaluate the intrinsic quality of our mapping (Section 4.1) and then quantify the impact of WordNet++ for coarse-grained (Section 4.2) and domain-specific WSD (Section 4.3).",
        "Experimental setting.",
        "We first conducted an evaluation of the mapping quality.",
        "To create a gold standard for evaluation, we started from the set of all lemmas contained both in WordNet and Wikipedia: the intersection between the two resources includes 80,295 lemmas which correspond to 105,797 WordNet senses and 199,735 Wikipedia pages.",
        "The average polysemy is 1.3 and 2.5 for WordNet senses and Wikipages, respectively (2.8 and 4.7 when excluding monosemous words).",
        "We selected a random sample of 1,000 Wikipages and asked an annotator with previous experience in lexicographic annotation to provide the correct WordNet sense for each page title (an empty sense label was given if no correct mapping was possible).",
        "505 non-empty mappings were found, i.e. Wikipedia pages with a corresponding WordNet sense.",
        "In order to quantify the quality of the annotations and the difficulty of the task, a second annotator sense tagged a subset of 200 pages from the original sample.",
        "We computed the inter-annotator agreement using the kappa coefficient (Carletta, 1996) and found out that our annotators achieved an agreement coefficient k of 0.9, indicating almost perfect agreement.",
        "Table 1 summarizes the performance of our disambiguation algorithm against the manually annotated dataset.",
        "Evaluation is performed in terms of standard measures of precision (the ratio of correct sense labels to the non-empty labels output by the mapping algorithm), recall (the ratio of correct sense labels to the total of non-empty labels in the gold standard) and Fi-measure ( ).",
        "We also calculate accuracy, which accounts for empty sense labels (that is, calculated on all 1,000 test instances).",
        "As baseline we use the most frequent WordNet sense (MFS), as well as a random sense assignment.",
        "We evaluate the mapping methodology described in Section 3.2 against different disambiguation contexts for the WordNet senses (cf.",
        "Section 3.2.2), i.e. structure-based (including synonymy, hypernymy/hyponymy and sisterhood), gloss-derived evidence, and a combination of the two.",
        "As disambiguation context of a Wikipage (Section 3.2.1) we use all information available, i.e. sense labels, links and categories.",
        "Results and discussion.",
        "The results show that our method improves on the baseline by a large margin and that higher performance can be achieved by using more disambiguation information.",
        "That is, using a richer disambiguation context helps to better choose the most appropriate WordNet sense for a Wikipedia page.",
        "The combination of structural and gloss information attains a slight variation in terms of precision (-0.3% and +0.8% compared to Structure and Gloss respectively), but a significantly high increase in recall (+9.4% and +13.3%).",
        "This implies that the different disambiguation contexts only partially overlap and, when used separately, each produces different mappings with a similar level of precision.",
        "In the joint approach, the harmonic mean of precision and recall, i.e. Fi, is in fact 5 and 8 points higher than when separately using structural and gloss information, respectively.",
        "As for the baselines, the most frequent sense is just 0.6% and 0.4% above the random baseline in terms of Fi and accuracy, respectively.",
        "A x test reveals in fact no statistically significant difference at p < 0.05.",
        "This is related to the random distribution of senses in our dataset and the Wikipedia unbiased coverage of WordNet senses.",
        "So selecting the most frequent sense rather than any other sense for each target page represents a choice as arbitrary as picking a sense at random.",
        "The final mapping contains 81,533 pairs of Wikipages and word senses they map to, covering 55.7% of the noun senses in WordNet.",
        "Using our best performing mapping we are able to extend WordNet with 1,902,859 semantic edges: of these, 97.93% are deemed novel, i.e. no direct edge could previously be found between the synsets.",
        "In addition, we performed a stricter evaluation of the novelty of our relations by checking whether these can still be found indirectly by searching for a connecting path between the two synsets of interest.",
        "Here we found that 91.3%, 87.2% and 78.9% of the relations are novel to WordNet when performing a graph search ofmax-imum depth of 2, 3 and 4, respectively.",
        "Experimental setting.",
        "We extrinsically evaluate the impact of WordNet++ on the Semeval-2007 coarse-grained all-words WSD task (Nav-igli et al., 2007).",
        "Performing experiments in a coarse-grained setting is a natural choice for several reasons: first, it has been argued that the fine granularity of WordNet is one of the main obstacles to accurate WSD (cf. the discussion in Nav-igli (2009b)); second, the meanings of Wikipedia pages are intuitively coarser than those in Word-Net.",
        "For instance, mapping TRAVEL to the first or the second sense in WordNet is an arbitrary choice, as the Wikipage refers to both senses.",
        "Finally, given their different nature, WordNet and Wikipedia do not fully overlap.",
        "Accordingly, we expect the transfer of semantic relations from Wikipedia to WordNet to have sometimes the side effect to penalize some fine-grained senses of a word.",
        "We experiment with two simple knowledge-based algorithms that are set to perform coarsegrained WSD on a sentence-by-sentence basis:",
        "â¢ Simplified Extended Lesk (ExtLesk): The first algorithm is a simplified version of the Lesk",
        "P",
        "R",
        "Fi",
        "A",
        "Structure",
        "82.2",
        "68.1",
        "74.5",
        "81.1",
        "Gloss",
        "81.1",
        "64.2",
        "71.7",
        "78.8",
        "Structure + Gloss",
        "81.9",
        "77.5",
        "79.6",
        "84.4",
        "MFS BL",
        "24.3",
        "47.8",
        "32.2",
        "24.3",
        "Random BL",
        "23.8",
        "46.8",
        "31.6",
        "23.9",
        "algorithm (Lesk, 1986), that performs WSD based on the overlap between the context surrounding the target word to be disambiguated and the definitions of its candidate senses (Kil-garriff and Rosenzweig, 2000).",
        "Given a target word w, this method assigns to w the sense whose gloss has the highest overlap (i.e. most words in common) with the context of w, namely the set of content words co-occurring with it in a predefined window (a sentence in our case).",
        "Due to the limited context provided by the WordNet glosses, we follow Banerjee and Pedersen (2003) and expand the gloss of each sense s to include words from the glosses of those synsets in a semantic relation with s. These include all WordNet synsets which are directly connected to s, either by means of the semantic pointers found in WordNet or through the unlabeled links found in WordNet++.",
        "â¢ Degree Centrality (Degree): The second algorithm is a graph-based approach that relies on the notion of vertex degree (Navigli and Lapata, 2010).",
        "Starting from each sense s of the target word, it performs a depth-first search (DFS) of the WordNet(++) graph and collects all the paths connecting s to senses of other words in context.",
        "As a result, a sentence graph is produced.",
        "A maximum search depth is established to limit the size of this graph.",
        "The sense of the target word with the highest vertex degree is selected.",
        "We follow Navigli and Lapata (2010) and run Degree in a weakly supervised setting where the system attempts no sense assignment if the highest degree score is below a certain (empirically estimated) threshold.",
        "The optimal threshold and maximum search depth are estimated by maximizing Degree's Fi on a development set of 1,000 randomly chosen noun instances from the SemCor corpus (Miller et al., 1993).",
        "Experiments on the development dataset using Degree on WordNet++ revealed a performance far lower than expected.",
        "Error analysis showed that many instances were incorrectly disambiguated, due to the noise from weak semantic links, e.g. the links from SODA (SOFT DRINK) to EUROPE or AUSTRALIA.",
        "Accordingly, in order to improve the disambiguation performance, we developed a filter to rule out weak semantic relations from WordNet++.",
        "Given a WordNet++ edge (/(w),/(w')) where w and w' are both Wikipages and w links to w',",
        "we first collect all words from the category labels of w and w' into two bags of words.",
        "We remove stopwords and lemmatize the remaining words.",
        "We then compute the degree of overlap between the two sets of categories as the number of words in common between the two bags of words, normalized in the [0,1] interval.",
        "We finally retain the link for the DFS if such score is above an empirically determined threshold.",
        "The optimal value for this category overlap threshold was again estimated by maximizing Degree's Fi on the development set.",
        "The final graph used by Degree consists of WordNet, together with 152,944 relations from our semantic relation enrichment method (cf.",
        "Section 3.3).",
        "Results and discussion.",
        "We report our results in terms of precision, recall and Fi measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al., 2007).",
        "We first evaluated ExtLesk and Degree using three different resources: (1) WordNet only; (2) Wikipedia only, i.e. only those relations harvested from the links found within Wikipedia pages; (3) their union, i.e. WordNet++.",
        "In Table 2 we report the results on nouns only.",
        "As common practice, we compare with random sense assignment and the most frequent sense (MFS) from SemCor as baselines.",
        "Enriching WordNet with encyclopedic relations from Wikipedia yields a consistent improvement against using WordNet (+7.1% and +4.9% Fi for ExtLesk and Degree) best results are obtained by using Degree with WordNet++.",
        "The better performance of Wikipedia against WordNet when using ExtLesk (+3.7%) highlights the quality of the relations extracted.",
        "However, no such improvement is found with De-",
        "Resource",
        "Algorithm",
        "Nouns only P R F1",
        "WordNet",
        "ExtLesk Degree",
        "83.6 57.7 68.3 86.3 65.5 74.5",
        "Wikipedia",
        "ExtLesk Degree",
        "82.3 64.1 72.0 96.2 40.1 57.4",
        "WordNet++",
        "ExtLesk Degree",
        "82.7 69.2 75.4 87.3 72.7 79.4",
        "MFS BL Random BL",
        "77.4 77.4 77.4",
        "63.5 63.5 63.5",
        "Table 3: Performance on Semeval-2007 coarsegrained all-words WSD with MFS as a back-off strategy when no sense assignment is attempted.",
        "gree, due to its lower recall.",
        "Interestingly, Degree on WordNet++ beats the MFS baseline, which is notably a difficult competitor for unsupervised and knowledge-lean systems.",
        "We finally compare our two algorithms using WordNet++ with state-of-the-art WSD systems, namely the best unsupervised (Koeling and McCarthy, 2007, SUSSX-FR) and supervised (Chan et al., 2007, NUS-PT) systems participating in the Semeval-2007 coarse-grained all-words task.",
        "We also compare with SSI (Navigli and Velardi, 2005) - a knowledge-based system that participated out of competition - and the unsupervised proposal from Chen et al.",
        "(2009, TreeMatch).",
        "Table 3 shows the results for nouns (1,108) and all words (2,269 words): we use the MFS as a back-off strategy when no sense assignment is attempted.",
        "Degree with WordNet++ achieves the best performance in the literature.",
        "On the noun-only subset of the data, its performance is comparable with SSI and significantly better than the best supervised and unsupervised systems (+3.2% and +4.4% Fi against NUS-PT and SUSSX-FR).",
        "On the entire dataset, it outperforms SUSSX-FR and TreeMatch (+4.7% and +8.1%) and its recall is not statistically different from that of SSI and NUS-PT.",
        "This result is particularly interesting, given that WordNet++ is extended only with relations between nominals, and, in contrast to SSI, it does not rely on a costly annotation effort to engineer the set of semantic relations.",
        "Last but not least, we achieve state-of-the-art performance with a much simpler algorithm that is based on the notion of vertex degree in a graph.",
        "Table 4: Performance on the Sports and Finance sections of the dataset from Koeling et al.",
        "(2005): t indicates results from Agirre et al.",
        "(2009).",
        "The main strength of Wikipedia is to provide wide coverage for many specific domains.",
        "Accordingly, on the Semeval dataset our system achieves the best performance on a domain-specific text, namely d004, a document on computer science where we achieve 82.9% Fi (+6.8% when compared with the best supervised system, namely NUS-PT).",
        "To test whether our performance on the Semeval dataset is an artifact of the data, i.e. d004 coming from Wikipedia itself, we evaluated our system on the Sports and Finance sections of the domain corpora from Koeling et al.",
        "(2005).",
        "In Table 4 we report our results on these datasets and compare them with Personalized PageRank, the state-of-the-art system from Agirre et al.",
        "(2009), as well as Static PageRank and a k-NN supervised WSD system trained on SemCor.",
        "The results we obtain on the two domains with our best configuration (Degree using WordNet++) outperform by a large margin k-NN, thus supporting the findings from Agirre et al.",
        "(2009) that knowledge-based systems exhibit a more robust performance than their supervised alternatives when evaluated across different domains.",
        "In addition, our system achieves better results than Static and Personalized PageRank, indicating that competitive disambiguation performance can still be achieved by a less sophisticated knowledge-based WSD algorithm when provided with a rich amount of high-quality knowledge.",
        "Finally, the results show that WordNet++ enables competitive performance also in a fine-grained domain setting.",
        "Algorithm",
        "Nouns only P/R/Fi",
        "All words P/R/Fi",
        "ExtLesk",
        "81.0",
        "79.1",
        "Degree",
        "85.5",
        "81.7",
        "SUSSX-FR",
        "81.1",
        "77.0",
        "TreeMatch",
        "N/A",
        "73.6",
        "NUS-PT",
        "82.3",
        "82.5",
        "SSI",
        "84.1",
        "83.2",
        "MFS BL",
        "77.4",
        "78.9",
        "Random BL",
        "63.5",
        "62.7",
        "Algorithm",
        "Sports P/R/Fi",
        "Finance P/R/Fi",
        "k-NN",
        "30.3",
        "43.4",
        "Static PR t",
        "20.1",
        "39.6",
        "Personalized PR t",
        "35.6",
        "46.9",
        "ExtLesk",
        "40.1",
        "45.6",
        "Degree",
        "42.0",
        "47.8",
        "MFS BL",
        "19.6",
        "37.1",
        "Random BL",
        "19.5",
        "19.6"
      ]
    },
    {
      "heading": "5. Conclusions",
      "text": [
        "In this paper, we have presented a large-scale method for the automatic enrichment of a computational lexicon with encyclopedic relational knowledge.",
        "Our experiments show that the large amount of knowledge injected into WordNet is of high quality and, more importantly, it enables simple knowledge-based WSD systems to perform as well as the highest-performing supervised ones in a coarse-grained setting and to outperform them on domain-specific text.",
        "Thus, our results go one step beyond previous findings (Cuadros and Rigau, 2006; Agirre et al., 2009; Navigli and La-pata, 2010) and prove that knowledge-rich disambiguation is a competitive alternative to supervised systems, even when relying on a simple algorithm.",
        "We note, however, that the present contribution does not show which knowledge-rich algorithm performs best with WordNet++.",
        "In fact, more sophisticated approaches, such as Personalized PageRank (Agirre and Soroa, 2009), could be still applied to yield even higher performance.",
        "We leave such exploration to future work.",
        "Moreover, while the mapping has been used to enrich WordNet with a large amount of semantic edges, the method can be reversed and applied to the encyclopedic resource itself, that is Wikipedia, to perform disambiguation with the corresponding sense inventory (cf. the task of wikification proposed by Mihalcea and Csomai (2007) and Milne and Witten (2008b)).",
        "In this paper, we focused on English Word Sense Disambiguation.",
        "However, since WordNet++ is part of a multilingual semantic network (Navigli and Ponzetto, 2010), we plan to explore the impact of this knowledge in a multilingual setting."
      ]
    }
  ]
}
