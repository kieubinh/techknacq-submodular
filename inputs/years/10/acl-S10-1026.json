{
  "info": {
    "authors": [
      "Weiwei Guo",
      "Mona Diab"
    ],
    "book": "Workshop on Semantic Evaluations (SemEval)",
    "id": "acl-S10-1026",
    "title": "COLEPL and COLSLM: An Unsupervised WSD Approach to Multilingual Lexical Substitution, Tasks 2 and 3 SemEval 2010",
    "url": "https://aclweb.org/anthology/S10-1026",
    "year": 2010
  },
  "references": [
    "acl-D07-1007",
    "acl-P07-1005"
  ],
  "sections": [
    {
      "text": [
        "COLEUR and COLSLM: A WSD approach to Multilingual Lexical Substitution, Tasks 2 and 3 SemEval 2010",
        "Weiwei Guo and Mona Diab",
        "Center for Computational Learning Systems Columbia University",
        "In this paper, we present a word sense disambiguation (WSD) based system for multilingual lexical substitution.",
        "Our method depends on having a WSD system for English and an automatic word alignment method.",
        "Crucially the approach relies on having parallel corpora.",
        "For Task 2 (Sinha et al., 2009) we apply a supervised WSD system to derive the English word senses.",
        "For Task 3 (Lefever & Hoste, 2009), we apply an unsupervised approach to the training and test data.",
        "Both of our systems that participated in Task 2 achieve a decent ranking among the participating systems.",
        "For Task 3 we achieve the highest ranking on several of the language pairs: French, German and Italian."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In this paper, we present our system that was applied to the cross lingual substitution for two tasks in SEMEVAL 2010, Tasks 2 and 3.",
        "We adopt the same approach for both tasks with some differences in the basic set-up.",
        "Our basic approach relies on applying a word sense disambiguation (WSD) system to the English data that comes from a parallel corpus for English and a language of relevance to the task, language 2 (12).",
        "Then we automatically induce the English word sense correspondences to 12.",
        "Accordingly, for a given test target word, we return its equivalent 12 words assuming that we are able to disambiguate the target word in context."
      ]
    },
    {
      "heading": "2. Our Detailed Approach",
      "text": [
        "We approach the problem of multilingual lexical substitution from a WSD perspective.",
        "We adopt the hypothesis that the different word senses of ambiguous words in one language probably translate to different lexical items in another language.",
        "Hence, our approach relies on two crucial components: a WSD module for the source language (our target test words, in our case these are the English target test words) and an automatic word alignment module to discover the target word sense correspondences with the foreign words in a second language.",
        "Our approach to both tasks is unsupervised since we don't have real training data annotated with the target words and their corresponding translations into 12 at the onset of the problem.",
        "Accordingly, at training time, we rely on automatically tagging large amounts of English data (target word instances) with their relevant senses and finding their 12 correspondences based on automatically induced word alignments.",
        "Each of these English sense and 12 correspondence pairs has an associated translation probability value depending on frequency of co-occurrence.",
        "This information is aggregated in a lookup table over the entire training set.",
        "An entry in the table would have a target word sense type paired with all the observed translation correspondences 12 word types.",
        "Each of the 12 word types has a probability of translation that is calculated as a normalized weighted average of all the instances of this 12 word type with the English sense aggregated across the whole parallel corpus.",
        "This process results in an English word sense translation table (WSTT).",
        "The word senses are derived from Word-Net (Fellbaum, 1998).",
        "We expand the English word sense entry correspondences by adding the translations of the members of target word sense synonym set as listed in WordNet.",
        "For alignment, we specifically use the GIZA++ software for inducing word alignments across the",
        "GIZA++ to the parallel corpus in both directions",
        "English to 12 and 12 to English then take only the",
        "intersection of the two alignment sets, hence fo-129 cusing more on precision of alignment rather than recall.",
        "For each language in Task 3 and Task 2, we use TreeTagger to do the preprocessing for all languages.",
        "The preprocessing includes segmentation, POS tagging and lemmatization.",
        "Since Tree-Tagger is independent of languages, our system does not rely on anything that is language specific; our system can be easily applied to other languages.",
        "We run GIZA++ on the parallel corpus, and obtain the intersection of the alignments in both directions.",
        "Meanwhile, every time a target English word appears in a sentence, we apply our WSD system on it, using the sentence as context.",
        "From this information, we build a WSST from the English sense(s) to their corresponding foreign words.",
        "Moreover, we use WordNet as a means of augmenting the translation correspondences.",
        "We expand the word sense to its synset from WordNet adding the 12 words that corresponded to all the member senses in the synset yielding more translation variability.",
        "At test time, given a test data target word, we apply the same WSD system that is applied to the training corpus to create the WSTT Once the target word instance is disambiguated in context, we look up the corresponding entry in the WSTT and return the ranked list of 12 correspondences.",
        "We present results for best and for oot which vary only in the cut off threshold.",
        "In the BEST condition we return the highest ranked candidate, in the oot condition we return the top 10 (where available).",
        "Given the above mentioned pipeline, Tasks 2 and 3 are very similar.",
        "Their main difference lies in the underlying WSD system applied.",
        "We use a relatively simple monolingual supervised WSD system to create the sense tags on the English data.",
        "We use the SemCor word sense annotated corpus.",
        "SemCor is a subset of the Brown Corpus.",
        "For each of our target English words found disambiguated in the SemCor corpus, we create a sense profile for each of its senses.",
        "A sense profile is a vector of all the content words that occur in the context of this sense in the SemCor corpus.",
        "The dimensions of the vector are word",
        "Table 1 : Precision and Recall results per corpus on Task 2 test set types, as in a bag of words model, and the vector entries are the co-occurrence frequency of the word sense and the word type.",
        "At test time, given a a target English word, we create a bag of word types contextual vector for each instance of the word using the surrounding context.",
        "We compare the created test vector to the SemCor vectors and choose the highest most similar sense and use that for sense disambiguation.",
        "In case of ties, we return more than one sense tag.",
        "We use both naturally occurring parallel data and machine translation data.",
        "The data for our first Task 2 submission, T2-COLEUR, comprises naturally occurring parallel data, namely, the Spanish English portion of the EuroParl data provided by Task 3 organizers.",
        "For the machine translation data, we use translations of the source English data pertaining to the following corpora: the Brown corpus, WSJ, SensEvall, SensEval2 datasets as translated by two machine translation systems: Global Link (GL), Systran (SYS) (Guo & Diab, 2010).",
        "We refer to the translated corpus as the SALAAM corpus.",
        "The intuition for creating SALAAM (an artificial parallel corpus) is to create a balanced translation corpus that is less domain and genre skewed than the EuroParl data.",
        "This latter corpus results in our 2nd system for this task T2-COLSLM.",
        "It is clear that the T2-COLSLM outperforms T2-COLEUR.",
        "Contrary to Task 2, we apply a context based unsupervised WSD module to the English side of the parallel data.",
        "Our unsupervised WSD method, as described in (Guo & Diab, 2009), is a graph based unsupervised WSD method.",
        "Given a sequence of words W = {wi,W2---wn}, each word Wi with several senses {sn,Si2...Sim}.",
        "A graph G = (V,E) is defined such that there exists a vertex v for each sense.",
        "Two senses of two different words may be connected by an edge e, depending on their distance.",
        "That two senses are connected suggests they should have influence on each other, accordingly a maximum allowable distance is set.",
        "They explore 4 different graph based algorithms.We focus on the In-Degree graph based algorithm.",
        "The In-Degree algorithm presents the problem as a weighted graph with senses as nodes and similarity between senses as weights on edges.",
        "The In-Degree of a vertex refers to the number of edges incident on that vertex.",
        "In the weighted graph, the In-Degree for each vertex is calculated by summing the weights on the edges that are incident on it.",
        "After all the In-Degree values for each sense are computed, the sense with maximum value is chosen as the final sense for that word.",
        "In our implementation of the In-Degree algorithm, we use the JCN similarity measure for both Noun-Noun and Verb-Verb similarity calculation.",
        "Corpus",
        "best",
        "oot",
        "P",
        "R",
        "P",
        "R",
        "T2-COLSLM T2-COLEUR",
        "27.59 19.47",
        "25.99 18.15",
        "46.61 44.77",
        "43.91 41.72",
        "We use the training data from EuroParl provided by the task organizers for the 5 different language pairs.",
        "We participate in all the language competitions.",
        "We refer to our system as T3-COLEUR.",
        "As shown in Table 2, our system T3-COLEUR ranks the highest for the French, German and Italian language tasks on both best and oot.",
        "However the overall F-measures are very low.",
        "Our system ranks last for Dutch among 3 systems and it is middle of the pack for the Spanish language task.",
        "In general we note that the results for oot are naturally higher than for BEST since by design it is a more relaxed measure."
      ]
    },
    {
      "heading": "5. Related works",
      "text": [
        "Our work mainly investigates the influence of WSD on providing machine translation candidates.",
        "Carpuat & Wu (2007) and Chan et al.",
        "(2007) show WSD improves MT.",
        "However, in (Carpuat & Wu, 2007) classical WSD is missing by ignoring predefined senses.",
        "They treat translation candidates as sense labels, then find linguistic features in the English side, and cast the disambiguation process as a classification problem.",
        "Of relevance also to our work is that related to the task of English monolingual lexical substitution.",
        "For example some of the approaches that participated in the SemEval 2007 excercise include the following.",
        "Yuret (2007) used a statistical language model based on a large corpus to assign likelihoods to each candidate substitutes for a target word in a sentence.",
        "Martinez et al.",
        "(2007) uses WordNet to find candidate substitutes, produce word sequence including substitutes.",
        "They rank the substitutes by ranking the word sequence including that substitutes using web queries.",
        "In (Giuliano C. et al., 2007), they extract synonyms from dictionaries.",
        "They have 2 ways of ranking of the synonyms: by similarity metric based on LSA and by occurrence in a large 5-gram web corpus.",
        "Dahl et al.",
        "(2007) also extract synonyms from dictionaries.",
        "They present two systems.",
        "The first one scores substitutes based on how frequently the local context match the target word.",
        "The second one incorporates cosine similarity.",
        "Finally, Hassan et al.",
        "(2007) extract candidates from several linguistic resources, and combine many techniques and evidences to compute the scores such as machine translation, most common sense, language model and so on to pick the most suitable lexical substitution candidates."
      ]
    },
    {
      "heading": "6. Conclusions and Future Directions",
      "text": [
        "In this paper we presented a word sense disambiguation based system for multilingual lexical substitution.",
        "The approach relies on having a WSD system for English and an automatic word alignment method.",
        "Crucially the approach relies on having parallel corpora.",
        "For Task 2 we apply a supervised WSD system to derive the English word senses.",
        "For Task 3, we apply an unsupervised approach to the training and test data.",
        "Both of our systems that participated in Task 2 achieve a decent ranking among the participating systems.",
        "For Task 3 we achieve the highest ranking on several of the language pairs: French, German and Italian.",
        "In the future, we would like to investigate the",
        "usage of the Spanish and Italian WordNets for the 131 task.",
        "We would like to also expand our examination to other sources of bilingual data such as comparable corpora.",
        "Finally, we would like to investigate using unsupervised clustering of senses (Word Sense Induction) methods in lieu of the WSD approaches that rely on WordNet.",
        "Language",
        "best",
        "oot",
        "P",
        "R",
        "rank",
        "P",
        "R",
        "rank",
        "Dutch",
        "10.71",
        "10.56",
        "3/3",
        "21.47",
        "21.27",
        "3/3",
        "Spanish",
        "19.78",
        "19.59",
        "3/7",
        "35.84",
        "35.46",
        "5/7",
        "French",
        "21.96",
        "21.73",
        "1/7",
        "49.44",
        "48.96",
        "1/5",
        "German",
        "13.79",
        "13.63",
        "1/3",
        "33.21",
        "32.82",
        "1/3",
        "Italian",
        "15.55",
        "15.4",
        "1/3",
        "40.7",
        "40.34",
        "1/3"
      ]
    }
  ]
}
