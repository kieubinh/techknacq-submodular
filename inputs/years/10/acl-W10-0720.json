{
  "info": {
    "authors": [
      "Jonathan Chang"
    ],
    "book": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk",
    "id": "acl-W10-0720",
    "title": "Not-So-Latent Dirichlet Allocation: Collapsed Gibbs Sampling Using Human Judgments",
    "url": "https://aclweb.org/anthology/W10-0720",
    "year": 2010
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Facebook 1601 S. California Ave. Palo Alto, CA 94304",
        "Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus.",
        "Recent studies have found that while there are suggestive connections between topic models and the way humans interpret data, these two often disagree.",
        "In this paper, we explore this disagreement from the perspective of the learning process rather than the output.",
        "We present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate documents and cluster those annotations.",
        "We use these annotations as a novel approach for constructing a topic model, grounded in human interpretations of documents.",
        "We demonstrate that these topic models have features which distinguish them from traditional topic models."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections (Deerwester et al., 1990; Griffiths and Steyvers, 2002; Blei and Lafferty, 2009).",
        "These models posit a set of latent topics, multinomial distributions over words, and assume that each document can be described as a mixture of these topics.",
        "With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents.",
        "(See Figure 1.)",
        "These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents (Wallach et al., 2009).",
        "However, recent work has explored how these assumptions correspond to humans' understanding of language (Chang et al., 2009; Griffiths and Steyvers, 2006; Mei et al., 2007).",
        "Focusing on the latent space, i.e., the inferred mappings between topics and words and between documents and topics, this work has discovered that although there are some suggestive correspondences between human semantics and topic models, they are often discordant.",
        "In this paper we build on this work to further explore how humans relate to topic models.",
        "But whereas previous work has focused on the results of topic models, here we focus on the process by which these models are learned.",
        "Topic models lend themselves to sequential procedures through which the latent space is inferred; these procedures are in effect programmatic encodings of the modeling assumptions.",
        "By substituting key steps in this program with human judgments, we obtain insights into the semantic model conceived by humans.",
        "Here we present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate a document and cluster that annotation.",
        "This task simulates the sampling step of the collapsed Gibbs sampler (described in the next section), except that the posterior defined by the model has been replaced by human judgments.",
        "The task is quick to complete and is robust against noise.",
        "We report the results of a large-scale human study of this task, and show that humans are indeed able to construct a topic model in this fashion, and that the learned topic model has semantic properties distinct from existing topic models.",
        "We also demonstrate that the judgments can be used to guide computer-learned topic models towards models which are more concordant with human intuitions."
      ]
    },
    {
      "heading": "2. Topic models and inference",
      "text": [
        "Topic models posit that each document is expressed as a mixture of topics.",
        "These topic proportions are drawn once per document, and the topics are shared across the corpus.",
        "In this paper we focus on Latent Dirichlet allocation (LDA) (Blei et al., 2003) a topic model which treats each document's topic assignment as a multinomial random variable drawn from a symmetric Dirichlet prior.",
        "LDA, when applied to a collection of documents, will build a latent space: a collection of topics for the corpus and a collection of topic proportions for each of its documents.",
        "TOPIC 1 \"TECHNOLOGY",
        "TOPIC 3 ENTERTAINMENT",
        "(a) Topics (b) Document Assignments to Topics",
        "Figure 1: The latent space of a topic model consists of topics, which are distributions over words, and a distribution over these topics for each document.",
        "On the left are three topics from a fifty topic LDA model trained on articles from the New York Times.",
        "On the right is a simplex depicting the distribution over topics associated with seven documents.",
        "The line from each document's title shows the document's position in the topic space.",
        "LDA can be described by the following generative process:"
      ]
    },
    {
      "heading": "1.. For each topic k,",
      "text": [
        "(a) Draw topic ßk ~ Dir(rj)"
      ]
    },
    {
      "heading": "2.. For each document d,",
      "text": [
        "(a) Draw topic proportions 6d ~ Dir(a) (b) For each word i.",
        "Draw topic assignment zd^~Mult(0d) ii.",
        "Draw word ~ Mult(ßZdn)",
        "This process is depicted graphically in Figure 2.",
        "The parameters of the model are the number of topics, K, as well as the Dirichlet priors on the topic-word distributions and document-topic distributions, a and rj.",
        "The only observed variables of the model are the words, The remaining variables must be learned.",
        "There are several techniques for performing posterior inference, i.e., inferring the distribution over hidden variables given a collection of documents, including variational inference (Blei et al., 2003) and Gibbs sampling (Griffiths and Steyvers, 2006).",
        "In the sequel, we focus on the latter approach.",
        "Collapsed Gibbs sampling for LDA treats the topic-word and document-topic distributions, 6d and ßk, as nuisance variables to be marginalized out.",
        "The posterior distribution over the remaining latent variables,",
        "Figure 2: A graphical model depiction of latent Dirichlet allocation (LDA).",
        "Plates denote replication.",
        "The shaded circle denotes an observed variable and unshaded circles denote hidden variables.",
        "the topic assignments can be expressed as",
        "r(Ew nw,k + rjw).",
        "where nd k denotes the number of words in document d assigned to topic k and nw k the number of times word w is assigned to topic k. This leads to the sampling equations, where the superscript – d, i indicates that these statistics should exclude the current variable under consideration, zd^.",
        "In essence, the model performs inference by looking at each word in succession, and probabilistically assigning it to a topic according to Equation 1.",
        "Equation 1 is derived through the modeling assumptions and choice of parameters.",
        "By replacing Equation 1 with a different equation or with empirical observations, we may construct new models which reflect different assumptions about the underlying data."
      ]
    },
    {
      "heading": "3. Constructing topics using human judgments",
      "text": [
        "In this section we propose a task which creates a formal setting where humans can create a latent space representation of the corpus.",
        "Our task, tag-and-cluster, replaces the collapsed Gibbs sampling step of Equation 1 with a human judgment.",
        "In essence, we are constructing a gold-standard series of samples from the posterior.",
        "Figure 3 shows how tag-and-cluster is presented to users.",
        "The user is shown a document along with its title; the document is randomly selected from a pool of available documents.",
        "The user is asked to select a word from the document which is discriminative, i.e, a word which would help someone looking for the document find it.",
        "Once the word is selected, the user is then asked to assign the word to the topic which best suits the sense of the word used in the document.",
        "Users are specifically instructed to focus on the meanings of words, not their syntactic usage or orthography.",
        "The user assigns a word to a topic by selecting an entry out of a menu of topics.",
        "Each topic is represented by the five words occuring most frequently in that topic.",
        "The order of the topics presented to the user is determined by the number of words in that document already assigned to each topic.",
        "Once an instance of a word in a document has been assigned, it cannot be reassigned and will be marked in red when subsequent users encounter this document.",
        "In practice, we also prohibit users from selecting infrequently occurring words and stop words."
      ]
    },
    {
      "heading": "4. Experimental results",
      "text": [
        "We conducted our experiments using Amazon Mechanical Turk, which allows workers (our pool of prospective subjects) to perform small jobs for a fee through a Web interface.",
        "No specialized training or knowledge is typically expected of the workers.",
        "Amazon Mechanical Turk has been successfully used in the past to develop gold-standard data for natural language processing (Snow et al., 2008).",
        "We prepare two randomly-chosen, 100-document subsets of English Wikipedia.",
        "For convenience, we denote these two sets of documents as setl and set2.",
        "For each document, we keep only the first 150 words for our experiments.",
        "Because of the encyclopedic nature of the corpus, the first 150 words typically provides a broad overview of the themes in the article.",
        "We also removed from the corpus stop words and words which occur infrequently, leading to a lexicon of 8263 words.",
        "After this pruning setl contained 11614 words and set2 contained 11318 words.",
        "Workers were asked to perform twenty of the tag-gings described in Section 3 for each task; workers were paid $0.25 for each such task.",
        "The number of latent topics, K, is a free parameter.",
        "Here we explore two values of this parameter, K = 10 and K =15, leading to a total of four experiments – two for each set of documents and two for each value of K.",
        "For each experiment we issued 100 HITs, leading to a total of 2000 tags per experiment.",
        "Figure 4 shows the number of HITs performed per person in each experiment.",
        "Between 12 and 30 distinct workers participated in each experiment.",
        "The number of HITs performed per person is heavily skewed, with the most active participants completing an order of magnitude more HITs than other particpants.",
        "Figure 5 shows the amount of time taken per tag, in log seconds.",
        "Each color represents a different experiment.",
        "The bulk of the tags took less than a minute to perform and more than a few seconds.",
        "Learned topics As described in Section 3, the tag-and-cluster task is a way of allowing humans to con-",
        "Show HIT instructions",
        "Please select a word to use as this document's tag.",
        "USS Fayhtih (APA-43)",
        "USS \"Fayette\" (APA-43) was a lhat served with the US Navy during World War H. \"Fayette\" was launched 25 February 1943 by Ingalls Shipbirildhig, Pascagoula, Mississippi, as \"Sea Hawk\"; acquired by the Navy 30 April; placed in ferry commission between 30 April and 14 May; and commissioned in full 14 October 1943, Commander J. C. Leiterin command.",
        "Operational history.",
        "\"Fayette\" embarked marines at Norfolk, Virginia for transportation tt Pearl Harbor, where she arrived 21 December 1943.",
        "After training, she embarked soldiers, and sailed from Honolulu 22 January 1944 for Kwajalein, where she landed her troops on 1 February, one day after the initial assault.",
        "For 4 days, she offloaded combat cargo, and acted as receiving ship for casualties whom she transferred to a hospital ship before sailing 5 February for Funafuti.",
        "After training in landing exercises at Noumea, \"Fayette\" redeployed Marines and soldiers between March and May ,„",
        "You have selected to tag this document ^flffliTflifrfflTM.",
        "Choose the group of tags below which best corresponds to the sense of the tag in this document.",
        "If the tag does not fit in any non-empty set, you may place it into an empty tag cluster if one is available.",
        "navy September politician football comics",
        "actor power enzyme british church mississippi 1960s amcrica Pittsburgh artist actress disabilities fat london fictional correspondent anime",
        "Figure 3: Screenshots of our task.",
        "In the center, the document along with its title is shown.",
        "Words which cannot be selected, e.g., distractors and words previously selected, are shown in red.",
        "Once a word is selected, the user is asked to find a topic in which to place the word.",
        "The user selects a topic by clicking on an entry in a menu of topics, where each topic is expressed by the five words which occur most frequently in that topic.",
        "Figure 4: The number of HITs performed (y-axis) by each participant (x-axis).",
        "Between 12 and 30 people participated in each experiment.",
        "Figure 6: A comparison of the entropy of distributions drawn from a Dirichlet distribution versus the entropy of the topic proportions inferred by workers.",
        "Each column of the boxplot shows the distribution of entropies for 100 draws from a Dirichlet distribution with parameter a.",
        "The two rightmost columns show the distribution of the entropy of the topic proportions inferred by workers on setl and set2.",
        "The a of workers typically falls between 0.2 and 0.5.",
        "1",
        "j",
        "1",
        "JL",
        "s",
        "p",
        "1",
        "B",
        "\\",
        " – ",
        "□",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "-",
        "r",
        "r",
        "Table 1: The five words with the highest probability mass in each topic inferred by humans using the task described in Section 3.",
        "Each subtable shows the results for a particular experimental setup.",
        "Each row is a topic; the most probable words are ordered from left to right.",
        "Figure 7: The log-likelihood achieved by LDA as a function of iteration.",
        "There is one series for each value of a £ {0.05,0.1,0.2,0.5,1.0,2.0} from top to bottom.",
        "railway",
        "lighthouse",
        "(a) setl, K rail",
        "= 10",
        "huddersfield station",
        "president",
        "emperor",
        "(b) set2, K =",
        "politician",
        "10",
        "election",
        "government",
        "school",
        "college",
        "education",
        "history",
        "conference",
        "american",
        "players",
        "swedish",
        "team",
        "zealand",
        "catholic",
        "church",
        "film",
        "music",
        "actor",
        "war",
        "world",
        "navy",
        "road",
        "torpedo",
        "runners",
        "team",
        "championships match",
        "racing",
        "system",
        "pop",
        "microsoft",
        "music",
        "singer",
        "engine",
        "company",
        "power",
        "dwight",
        "engines",
        "september",
        "2007",
        "october",
        "december",
        "1999",
        "university",
        "london",
        "british",
        "college",
        "county",
        "television",
        "dog",
        "name",
        "george",
        "film",
        "food",
        "novel",
        "book",
        "series",
        "superman",
        "people",
        "malay",
        "town",
        "tribes",
        "cliff",
        "november",
        "february",
        "april",
        "august",
        "december",
        "diet",
        "chest",
        "enzyme",
        "hair",
        "therapy",
        "paint",
        "photographs",
        "american",
        "austin",
        "black",
        "british",
        "city",
        "london",
        "english",
        "county",
        "war",
        "history",
        "army",
        "american",
        "battle",
        "school",
        "university",
        "college",
        "church",
        "center",
        "australia",
        "knee",
        "(c) setl, K =",
        "british",
        "15",
        "israel",
        "set",
        "music",
        "pop",
        "(d) set2, K =",
        "records",
        "15",
        "singer",
        "artist",
        "catholic",
        "roman",
        "island",
        "village",
        "columbia",
        "film",
        "paintings",
        "movie",
        "painting",
        "art",
        "john",
        "devon",
        "michael",
        "austin",
        "charles",
        "school",
        "university",
        "english",
        "students",
        "british",
        "school",
        "university",
        "class",
        "community",
        "district",
        "drama",
        "headquarters chess",
        "poet",
        "stories",
        "november",
        "february",
        "2007",
        "2009",
        "2005",
        "family",
        "church",
        "sea",
        "christmas",
        "emperor",
        "lighthouse",
        "period",
        "architects",
        "construction",
        "design",
        "dog",
        "broadcast",
        "television",
        "bbc",
        "breed",
        "railway",
        "rail",
        "huddersfield",
        "ownership",
        "services",
        "champagne",
        "regular",
        "character",
        "characteristic common",
        "cyprus",
        "archdiocese",
        "diocese",
        "king",
        "miss",
        "election",
        "government",
        "parliament",
        "minister",
        "politician",
        "carson",
        "gordon",
        "hugo",
        "ward",
        "whitney",
        "enzyme",
        "diet",
        "protein",
        "hair",
        "oxygen",
        "significant",
        "application",
        "campaign",
        "comic",
        "considered",
        "war",
        "navy",
        "weapons",
        "aircraft",
        "military",
        "born",
        "london",
        "american",
        "england",
        "black",
        "september",
        "october",
        "december",
        "2008",
        "1967",
        "war",
        "defense",
        "history",
        "military",
        "artillery",
        "district",
        "town",
        "marin",
        "america",
        "american",
        "actor",
        "film",
        "actress",
        "band",
        "designer",
        "car",
        "power",
        "system",
        "device",
        "devices",
        "york",
        "michigan",
        "florida",
        "north",
        "photographs",
        "hockey",
        "players",
        "football",
        "therapy",
        "champions",
        "church",
        "catholic",
        "county",
        "2001",
        "agricultural",
        "california",
        "zealand",
        "georgia",
        "india",
        "kolkata",
        "Table 2: The five words with the highest probability mass in each topic inferred by LDA, with a = 0.2.",
        "Each subtable shows the results for a particular experimental setup.",
        "Each row is a topic; the most probable words are ordered from left to right.",
        "Figure 5: The distribution of times taken per HIT.",
        "Each series represents a different experiment.",
        "The bulk of the tags took less than one minute and more than a few seconds.",
        "struct a topic model.",
        "One way of visualizing a learned topic model is by examining its topics.",
        "Table 1 shows the topics constructed by human judgments.",
        "Each subtable shows a different experimental setup and each row shows an individual topic.",
        "The five most frequently occurring words in each topic are shown, ordered from left to right.",
        "Many of the topics inferred by humans have straightforward interpretations.",
        "For example, the {november, february, april, august, december} topic for the setl corpus with K =10 is simply a collection of months.",
        "Similar topics (with years and months combined) can be found in the other experimental configurations.",
        "Other topics also cover specific semantic domains, such as {president, emperor, politican, election, government} or {music, pop, records, singer, artist}.",
        "Several of the topics are combinations of distinct concepts, such as {catholic, church, film, music, actor}, which is often indicative of the number of clusters, K, being too low.",
        "(a) setl, K = 10",
        "(b) set2, K = 10",
        "born",
        "2004",
        "team",
        "award",
        "sydney",
        "september",
        "english",
        "edit",
        "nord",
        "hockey",
        "regiment army",
        "artillery",
        "served",
        "scouting",
        "black",
        "hole",
        "current",
        "england",
        "model",
        "line",
        "station",
        "main",
        "island",
        "railway",
        "training",
        "program",
        "war",
        "election",
        "navy",
        "region",
        "street",
        "located",
        "site",
        "knee",
        "school",
        "university",
        "district",
        "city",
        "college",
        "food",
        "february",
        "conference day",
        "2009",
        "family",
        "word",
        "international",
        "road",
        "japan",
        "pride",
        "greek",
        "knowledge portland",
        "study",
        "publication",
        "time",
        "day",
        "india",
        "bridge",
        "catholic",
        "church",
        "roman",
        "black",
        "time",
        "born",
        "pop",
        "world",
        "released",
        "march",
        "class",
        "series",
        "film",
        "actor",
        "engine",
        "won",
        "video",
        "microsoft",
        "project",
        "hungary",
        "travel",
        "human",
        "office",
        "management defense",
        "film",
        "hair",
        "bank",
        "national",
        "town",
        "school",
        "born",
        "war",
        "world",
        "university",
        "people",
        "name",
        "french",
        "therapy",
        "artist",
        "(c) setl, K",
        "=15",
        "(d) set2, K = 15",
        "time",
        "michael",
        "written",
        "experience",
        "match",
        "family",
        "protein",
        "enzyme",
        "acting",
        "oxygen",
        "line",
        "station",
        "railway",
        "branch",
        "knowledge",
        "england",
        "producer",
        "popular",
        "canadian",
        "sea",
        "film",
        "land",
        "pass",
        "set",
        "battle",
        "system",
        "death",
        "artist",
        "running",
        "car",
        "william",
        "florida",
        "carson",
        "virginia",
        "newfoundland",
        "character",
        "series",
        "dark",
        "main",
        "village",
        "war",
        "regiment",
        "british",
        "army",
        "south",
        "english",
        "word",
        "publication",
        "stream",
        "day",
        "reaction",
        "terminal",
        "copper",
        "running",
        "complex",
        "training",
        "program",
        "hair",
        "students",
        "electrical",
        "born",
        "school",
        "world",
        "college",
        "black",
        "district",
        "town",
        "city",
        "local",
        "kolkata",
        "food",
        "conference",
        "flight",
        "medium",
        "rail",
        "september",
        "edit",
        "music",
        "records",
        "recorded",
        "township",
        "scouting",
        "census",
        "square",
        "county",
        "black",
        "pop",
        "bank",
        "usually",
        "hole",
        "travel",
        "defense",
        "training",
        "management",
        "edges",
        "people",
        "choir",
        "road",
        "diet",
        "related",
        "series",
        "actor",
        "engine",
        "november",
        "award",
        "war",
        "built",
        "navy",
        "british",
        "service",
        "pride",
        "portland",
        "band",
        "northwest",
        "god",
        "center",
        "million",
        "cut",
        "champagne",
        "players",
        "team",
        "knee",
        "2004",
        "sydney",
        "israel",
        "born",
        "television",
        "current",
        "drama",
        "won",
        "catholic",
        "located",
        "site",
        "region",
        "church",
        "school",
        "university",
        "college",
        "election",
        "born",
        "class",
        "february",
        "time",
        "public",
        "king",
        "film",
        "nord",
        "played",
        "league",
        "hockey",
        "Table 2 shows the topics learned by LDA under the same experimental conditions, with the Dirichlet hyperparameter a = 0.2 (we justify this choice in the following section).",
        "These topics are more difficult to interpret than the ones created by humans.",
        "Some topics seem to largely make sense except for some anomalous words, such as {district, town, city, local, kolkata} or {school, university, college, election, born}.",
        "But the small amount of data means that it is difficult for a model which does not leverage prior knowledge to infer meaningful topic.",
        "In contrast, several humans, even working independently, can leverage prior knowledge to construct meaningful topics with little data.",
        "There is another qualitative difference between the topics found by the tag-and-cluster task and LDA.",
        "Whereas LDA must rely on co-occurrence, humans can use ontological information.",
        "Thus, a topic which has ontological meaning, such as a list of months, may rarely be discovered by LDA since the co-occurrence patterns of months do not form a strong pattern.",
        "But users in every experimental configuration constructed this topic, suggesting that the users were consistently leveraging information that would not be available to LDA, even with a larger corpus.",
        "Hyperparameter values A persistent question among practitioners of topic models is how to set or learn the value of the hyperparameter a. a is a Dirichlet parameter which acts as a control on sparsity – smaller values of a lead to sparser document-topic distributions.",
        "By comparing the sparsity patterns of human judgments to those of LDA for different settings of a, we can infer the value of a that would best match human judgments.",
        "Figure 6 shows a boxplot comparison of the entropy of draws from a Dirichlet distribution (the generative process in LDA), versus the observed entropy of the models learned by humans.",
        "The first six columns show the distributions for the Dirichlet draws for various values of a; the last two columns show the observed entropy distributions on the two corpora, setl and set2.",
        "The empirical entropy distributions across the corpora are comparable to those of a Dirichlet distribution with a between approximately 0.2 and 0.5.",
        "Table 3: The five words with the highest probability mass in each topic inferred by LDA on setl with a = 0.2, K = 10, and initialized using human judgments.",
        "Each row is a topic; the most probable words are ordered from left to right.",
        "These settings of a are slightly higher than, but still in line with a common rule-of-thumb of a = 1/K.",
        "Figure 7 shows the log-likelihood, a measure of model fit, achieved by LDA for each value of a.",
        "Higher log-likelihoods indicate better fits.",
        "Commensurate with the rule of thumb, using log-likelihoods to select a would encourage values smaller than human judgments.",
        "However, while the entropy of the Dirichlet draws increases significantly when the number of clusters is increased from K = 10 to K = 15, the entropies assigned by humans does not vary as dramatically.",
        "This suggests that for any given document, humans are likely to pull words from only a small number of topics, regardless of how many topics are available, whereas a model will continue to spread probability mass across all topics even as the number of topics increases.",
        "Improving LDA using human judgments The results of the previous sections suggests that human behavior differs from that of LDA, and that humans conceptualize documents in ways LDA does not.",
        "This motivates using the human judgments to augment the information available to LDA.",
        "To do so, we initialize the topic assignments used by LDA's Gibbs sampler to those made by humans.",
        "We then run the LDA sampler till convergence.",
        "This provides a method to weakly incorporate human knowledge into the model.",
        "Table 3 shows the topics inferred by LDA when initialized with human judgments.",
        "These topics resemble those directly inferred by humans, although as we predicted in the previous sections, the topic consisting of months has largely disappeared.",
        "Other semantically coherent topics, such as {located,",
        "line",
        "station",
        "lighthouse",
        "local",
        "main",
        "school",
        "history",
        "greek",
        "knowledge",
        "university",
        "catholic",
        "church",
        "city",
        "roman",
        "york",
        "team",
        "club",
        "2004",
        "scouting",
        "career",
        "engine",
        "knee",
        "series",
        "medium",
        "reaction",
        "located",
        "south",
        "site",
        "land",
        "region",
        "food",
        "film",
        "conference",
        "north",
        "little",
        "february",
        "class",
        "born",
        "august",
        "2009",
        "pride",
        "portland",
        "time",
        "northwest",
        "june",
        "war",
        "regiment",
        "army",
        "civil",
        "black",
        "Figure 8: The log likelihood achieved by LDA on setl with a = 0.",
        "2, K = 10, and initialized using human judgments (blue).",
        "The red line shows the log likelihood without incorporating human judgments.",
        "LDA with human judgments dominates LDA without human judgments and helps the model converge more quickly.",
        "south, site, land, region}, have appeared in its place.",
        "Figure 8 shows the log-likelihood course of LDA when initialized by human judgments (blue), versus LDA without human judgments (red).",
        "Adding human judgments strictly helps the model converge to a higher likelihood and converge more quickly.",
        "In short, incorporating human judgments shows promise at improving both the interpretability and convergence of LDA."
      ]
    },
    {
      "heading": "5. Discussion",
      "text": [
        "We presented a new method for constructing topic models using human judgments.",
        "Our approach relies on a novel task, tag-and-cluster, which asks users to simultaneously annotate a document with one of its words and to cluster those annotations.",
        "We demonstrate using experiments on Amazon Mechanical Turk that our method constructs topic models quickly and robustly.",
        "We also show that while our topic models bear many similarities to traditionally constructed topic models, our human-learned topic models have unique features such as fixed sparsity and a tendency for topics to be constructed around concepts which models such as LDA typically fail to find.",
        "We also underscore that the collapsed Gibbs sampling framework is expressive enough to use as the basis for human-guided topic model inference.",
        "This may motivate, as future work, the construction of different modeling assumptions which lead to sampling equations which more closely match the empirically observed sampling performed by humans.",
        "In effect, our method constructs a series of samples from the posterior, a gold standard which future topic models can aim to emulate."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The author would like to thank Eytan Bakshy and Professor Jordan Boyd-Graber-Ying for their helpful comments and discussions."
      ]
    }
  ]
}
