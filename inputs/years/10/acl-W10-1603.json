{
  "info": {
    "authors": [
      "Fabio Natanael Kepler",
      "Marcelo Finger"
    ],
    "book": "Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas",
    "id": "acl-W10-1603",
    "title": "Variable-Length Markov Models and Ambiguous Words in Portuguese",
    "url": "https://aclweb.org/anthology/W10-1603",
    "year": 2010
  },
  "references": [
    "acl-A00-1031",
    "acl-H05-1059",
    "acl-N03-1033",
    "acl-P07-1096",
    "acl-P93-1035",
    "acl-W02-1001",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "text": [
        "Variable-Length Markov Models and Ambiguous Words in Portuguese*",
        "Institute of Mathematics and Statistics University of Sao Paulo Sao Paulo, SP, Brazil",
        "kepler@ime.usp.br",
        "mfinger@ime.usp.br",
        "Variable-Length Markov Chains (VLMCs) offer a way of modeling contexts longer than trigrams without suffering from data sparsity and state space complexity.",
        "However, in Historical Portuguese, two words show a high degree of ambiguity: que and a.",
        "The number of errors tagging these words corresponds to a quarter of the total errors made by a VLMC-based tagger.",
        "Moreover, these words seem to show two different types of ambiguity: one depending on non-local context and another on right context.",
        "We searched ways of expanding the VLMC-based tagger with a number of different models and methods in order to tackle these issues.",
        "The methods showed variable degrees of success, with one particular method solving much of the ambiguity of a.",
        "We explore reasons why this happened, and how everything we tried fails to improve the precision of que."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In the Computational Linguistics area, the task of part-of-speech tagging (POS tagging) consists in assigning to words in a text the grammatical class they belong.",
        "Since the same word may belong to more than one class, models for POS tagging have to look at the context where each word occurs to try to solve the ambiguity.",
        "Previous and current work have developed a wide range of models and methods for tagging.",
        "The vast majority uses supervised learning methods, which",
        "TDuring the course of this work Fabio received support from Brazilian funding agencies CAPES and CNPq.",
        "need an already tagged corpus as input in order to train the model, calculating relations, weights, probabilities etc.",
        "Among the various models for tagging, there are Maximum Entropy models (dos Santos et al., 2008; de Almeida Filho, 2002; Ratnaparkhi, 1996), Hidden Markov Models (HMMs) (Brants, 2000), Transformation Based Learning (Brill, 1993), and other succesful approaches (Toutanova et al., 2003; Tsu-ruoka and Tsujii, 2005; Shen et al., 2007).",
        "Current state-of-the-art precision in tagging is achieved by supervised methods.",
        "Although precision is pretty high - less than 3% error rate for English - the disavantage is exactly the need of a tagged corpus, usually built manually.",
        "This is a very restrictive issue for languages with lack of resources such as linguistic especialists, corpora projects etc.",
        "The Portuguese language falls in between resourceful languages, such as English, and languages with restricted resources.",
        "There have been initiatives both in Brazil and in Portugal, which include modern Brazilian Portuguese corpora (ICMC-USP, 2010), European Portuguese corpora (Flo, 2008), and historical Portuguese corpora (IEL-UNICAMP and IME-USP, 2010).",
        "Also, some supervised POS taggers have already been developed for Portuguese (dos Santos et al., 2008; Kepler and Finger, 2006; Aires, 2000) with a good degree of success.",
        "And finally, there has also been increasing effort and interest in Portuguese annotation tools, such as E-",
        "Dictor (de Sousa et al., 2009).",
        "Despite these advances, there is still lack of material and resources for Portuguese, as well as research",
        "!See http://purl.org/edictor.",
        "in unsupervised methods to bootstrap text annotation.",
        "Our work focuses on further improvement of the current state-of-the-art in Portuguese tagging.",
        "For this, we focus on the Tycho Brahe (IEL-UNICAMP and IME-USP, 2010) corpus for testing and benchmarking, because of its great collaboration potential: it is easily accessible; is under continuous development; and has recently started using E-Dictor, which also offers a great collaboration potential.",
        "One popular approach to tagging is to use HMMs of order 2.",
        "Order 2, or trigram, means the tagger considers the previous two words/tags when tagging a word.",
        "This adds context to help disambiguation.",
        "The drawback is that this context may not be sufficient.",
        "Increasing the order does not help, since this incurs in too many model parameters and suffers from the data sparsity problem.",
        "In (Kepler and Finger, 2006), we developed a tagger for Portuguese that uses Markov chains of variable length, that is, orders greater than 2 can be used conditioned on certain tags and sequences of tags.",
        "This approach is better at avoiding the sparsity and complexity problems, while being able to model longer contexts.",
        "However, one interesting conclusion from that work is that, even using longer contexts, some words stay extremely hard to disambiguate.",
        "Apparently, those words rely on flexible contexts not captured by pure VLMCs.",
        "Motivated by this problem, we improve over the previous work, and developed a set of tagger models based on Variable-Length Markov Chains (VLMCs) extended with various other approaches in order to try to tackle the problem.",
        "In the next section we describe the VLMC theory, the results it achieves, and the problems with two common words.",
        "Then, in Section 3, we explain in summary the set of models and approaches we tried to mix with VLMCs, and the different types of results they give.",
        "Conclusions are drawn in Section 4.",
        "Finally, Section 5 describes how this work can be incorporated in other projects, and Section 6 presents ideas for future work."
      ]
    },
    {
      "heading": "2. Variable-Length Markov Chains",
      "text": [
        "The idea is to allow the memory of a Markov chain to have variable length, depending on the observed past values.",
        "(Bühlmann and Wyner, 1999) give a formal description of VLMCs, while here we will explain them in terms of the POS-tagging task.",
        "Consider a Markov chain with a finite, large order k. Let ti be a tag, and ti_k,i_1 be the k tags preceding ti.",
        "Variable length memory can be seen as a cut of irrelevant states from the ti_k)i_1 history.",
        "We call the set of these states the context of ti.",
        "Given a tag ti, its context ti_h;i_1, h < k, is given by the context function c(ti_k,i_i).",
        "A context tree is a tree in which each internal node has at most |T| children, where T is the tagset.",
        "Each value of a context function c( ) is represented as a branch of such tree.",
        "For example, the context given by c(ti_k i_1) is represented as a branch whose sub-branch at the top is determined by ti_1, the next sub-branch by ti_2, and so on, until the leaf, determined by ti_h.",
        "The parameters of a VLMC are the underlying functions c( ) and their probabilities.",
        "To obtain these parameters we use a version of the context algorithm of (Rissanen, 1983).",
        "First, it builds a big context tree, using a training corpus.",
        "For a tag ti, its maximal history ti_k i_1 is placed as a branch in the tree.",
        "Then, the algorithm uses a pruning function considering a local decision criterion.",
        "This pruning cuts off the irrelevant states from the tags' histories.",
        "For each leaf u in the context tree, and branch v that goes from the root to the parent node of u, u is pruned from the tree if where C (vu) is the number of occurrences of the sequence vu in the training corpus, and K is a threshold value, called the cut value of the context tree,",
        "If the probability of a tag does not change much between considering the entire branch together with the leaf (all past history) and considering only the branch (the history without the furthest tag), then the leaf does not need to be considered, and can be removed from the tree.",
        "We want to find the best sequence of tags t1.. .tnfor a given sequence of words w1... wn of size n, that is, arg max TT P(ti|c(ti_fc)i_1))P(wi|ti) .",
        "Probabilities are computed from a tagged training corpus using maximum likelihood estimation from the relative frequencies of words and sequences of tags.",
        "The context tree is built with sequences of tags of maximum length k and then pruned, thus defining the context functions.",
        "For decoding, the Viterbi Algorithm is used (Viterbi, 1967).",
        "We used the tagged texts available by the Ty-cho Brahe Corpus of Historical Portuguese (IEL-UNICAMP and IME-USP, 2010).",
        "The Tycho Brahe project uses 377 POS and inflectional tags, and contains annotated texts written by authors born between 1380 and 1845.",
        "We have selected 19 texts for composing our corpus, which contains 1035593 tagged words and has 262 different tags.",
        "This corpus was then randomly divided into 75% of the sentences for generating a training corpus and 25% for a testing corpus.",
        "The training corpus has 775602 tagged words, while the testing corpus has 259991 tagged words.",
        "The Tycho Brahe project is undergoing rapid development, so as for today there are more texts available which are not present in the corpus we used.",
        "Because of some of the approaches explained below, we also created a new training corpus and a new testing corpus by segmenting contracted words from the original corpus.",
        "Contracted words are words like da, which has the tag P+D-F and is a contraction of the preposition de (P) with the feminine determiner a (D-F).",
        "Using the original corpus, our VLMC implementation, which we will call vlmm tagger (from Variable Length Markov Model), and which better implements under-and overflow control, achieves 96.29% of precision, while the vlmc tagger from (Kepler and Finger, 2006) achieves 95.51%.",
        "Table 1 shows the numbers for both taggers, where P and E means Precision and Error, respectively.",
        "The difference in precision is mainly due to a 21.64% error reduction in known words tagging.",
        "That, combined with 6.82% error reduction in unknown words, results in 17.50% total error reduction.",
        "With the segmented corpus the vlmm tagger achieved 96.54% of precision.",
        "Table 2 shows numbers for the two words that present the most number of errors made by the vlmm tagger.",
        "Note that they are not necessarily the words with the highest error percentage, since there are known words that appear only a couple of times in the testing corpus and may get wrong tags half of this times, for example.",
        "Table2: Results forwords with themostnumberoferrors using the vlmm tagger with the normal corpus.",
        "These two words draw attention because together they correspond to almost 25% of the errors made by the tagger, where most confusion for each of these words is between two different tags:",
        "• The word que is, most of the times, either a relative pronoun - denoted by the tag WPRO and",
        "Tagger",
        "Words",
        "P(%)",
        "Err.",
        "/ Ocurr.",
        "Unknown",
        "69.53",
        "2713 / 8904",
        "vlmc",
        "Known",
        "96.39",
        "9065 / 251087",
        "Total",
        "95.51",
        "11674 / 259991",
        "Unknown",
        "71.60",
        "2528 / 8904",
        "vlmm",
        "Known",
        "97.17",
        "7102 / 251087",
        "Total",
        "96.29",
        "9630 / 259991",
        "Words",
        "P(%)",
        "E (%)",
        "Err.",
        "/ Ocurr.",
        "que",
        "84.7413",
        "15.2586",
        "1687 / 11056",
        "a",
        "90.9452",
        "9.0547",
        "661 / 7300",
        "equivalent to the word which in English -, or a subordinating conjunction - denoted by the tag C and equivalent, in English, to the words that or than;",
        "• The word a is, usually, either a feminine determiner (tag D-F), or a preposition (tag P).",
        "As a baseline, assigning the most common tag toque yields a precision of 55.64%, while a gets a precision of 58.09%.",
        "Also, these words seem to show two different types of ambiguity: one that needs context to the right, and one that needs non-local context.",
        "The VLMM model does not have parameters for these contexts, since it tags from left to right using context immediately to the left.",
        "It seems that a could be better disambiguated by looking at words or tags following it: for example, if followed by a verb, a is much more likely to be a preposition.",
        "For que, it seems that words occuring not immediately before may add important information.",
        "For example, if que follows mais (more than, in English), it is more likely that que has tag C. However, like in the English expression, it is possible to have various different words in between mais and que, as for example: \"mais provâvel que\" (\"more likely than\"); \"mais caro e complexo que\" (\"more expensive and complex than\"); and so on.",
        "Thus, it may yield better results if non-local context could be efficiently modeled.",
        "In order to develop these ideas about que and a and prove them right or wrong, we searched ways of expanding the VLMM tagger with a number of different models and methods that could help solving these two issues.",
        "Those models are described next."
      ]
    },
    {
      "heading": "3. Auxiliary Approaches",
      "text": [
        "The first idea we had was to generalize nodes in the VLMM's context tree, that is, to model a way of abstracting different sequences of tags into the same node.",
        "This could make it possible to have branches in the context tree like ADV * C, that could be used for mais * que.",
        "One way of doing this is to use sequences of tags that form phrases, like noun phrases (NP), prepositional phrases (PP), and verbal phrases (VP), and use them in the context tree in place of the sequences of tags they cover.",
        "The context tree will then have branches like, say, P VP N.",
        "In order to train this mixed model we need a treebank, preferably from the texts in the Tycho Brahe corpus.",
        "However, it does not have a sufficiently large set of parsed texts to allow efficient supervised learning.",
        "Moreover there is not much Portuguese treebanks available, so we were motivated to implement an unsupervised parsed for Portuguese.",
        "Based on the work of (Klein, 2005), we implemented his CCM model, and used it over the Ty-cho Brahe corpus.",
        "The CCM model tries to learn constituents based on the contexts they have in common.",
        "We achieved 60% of f-measure over a set of texts from the Tycho Brahe project that were already parsed.",
        "Using the CCM constituents learned, we extended the vlmm tagger to use this extra information.",
        "It yielded worse results, so we restricted the use of constituents toque (the vlmm+spans-que tagger).",
        "This yielded a precision of 96.56%, with a que precision increase of 3.73% and an a precision reduction of 0.",
        "67%.",
        "A comparison with the plain vlmm tagger over the segmented corpus can be seen in Table 3.",
        "We use the segmented corpus for comparison because the constituents only use segmented tags.",
        "Even after many tries and variations in",
        "Table 3: Comparison of precision using the vlmm tagger (in italics) and the vlmm+spans-que tagger (upcase) with the segmented corpus.",
        "the way the vlmm tagger could use constituents, the result did not improve.",
        "This led us to a new approach, shown in the next section.",
        "Words",
        "P(%)",
        "Err.",
        "/ Ocurr.",
        "que",
        "84.50",
        "85.18",
        "1715 / 11063",
        "1651 / 11063",
        "a",
        "94.52",
        "94.49",
        "745 / 13597 750 / 13597",
        "Total",
        "96.5433",
        "96.5636",
        "9559 / 276541 9503 / 276541",
        "Since induced syntactic structure did not help, a new idea was to, this time, begin with the already parsed and revised texts from the Tycho Brahe, even with they summing only a little more than 300 thousand words.",
        "To ease the problem of sparsity, the trees were flattened and merged in such a way that only NPs, PPs and VPs remained.",
        "Then the bracketed notation was converted to the IOB notation, now forming a chunked corpus.",
        "Chunking, or shallow parsing, divides a sentence into non-overlapping phrases (Manning and Schütze, 1999).",
        "It is used in information extraction and in applications where full parsing is not necessary, offering the advantage of being simpler and faster.",
        "We made a small experiment with the chunked corpus: divided the sentences randomly into 90% and 10% sets, the former for training and the later for testing.",
        "Then we ran the vlmm tagger with these chunked sets, and got a precision in chunking of79%.",
        "A model for chunks processing was mixed into the VLMM model, similar but not equal to the mixed model with CCM.",
        "The chunked corpus uses segmented words, because the parsed texts available in Tycho Brahe only use segmented words.",
        "Thus, we ran the vlmm tagger with the segmented training corpus and the chunked corpus, testing over the segmented test corpus.",
        "The precision yielded with this vlmm+chunks tagger was 96.55%.",
        "Table 4 shows the results for the segmented corpus with the vlmm tagger and the vlmm+chunks tagger.",
        "Interestingly, results did not change much, in spite of the vlmm+chunks tagger achieving a higher precision.",
        "Interestingly, the word a error rate is reduced by around 13% with the help of chunks, while the que error rate increases almost 3%.",
        "Another approach was to follow the intuition about a: that the right context should help solving some ambiguities.",
        "The problem that makes this approach non trivial is that a right tag context is not yet available when tagging a word, due to the natural left-to-right order the tagger follows when tagging a sen-",
        "Words P (%) Err.",
        "/ Ocurr.",
        "Table 4: Comparison of precision using the vlmm tagger (in italics) and the vlmm+chunks tagger (up-case) with the segmented corpus.",
        "tence.",
        "A right context that is available is the context of words to the right, but this presents the problem of sparsity and will probably not yield good results.",
        "Our approach was then to model a right context of tags when the words to the right were not ambiguous, that is, if they could be assigned only one specific tag.",
        "During training, a new context tree is built for the right context, where, for each word in a sentence, a continuous but variable-length sequence of tags from unambiguous words to the right is added as a branch to the right context tree.",
        "That is, if k words to right of a given word are not ambiguous, then the sequence of the k tags these words will have is added to the right tree.",
        "The right context tree is also prunned like the left context tree and the Viterbi algorithm for tagging is adapted to consider these new parameters.",
        "Table 5: Comparison of precision using the vlmm tagger (in italics) and the vlmm+a-right tagger (up-case) with the normal corpus.",
        "After various tests with different options for the right context tree, the result over the original VLMM tagger did not improve.",
        "We then experimented building the right context tree only for the word a, resulting in the vlmm+right-a tagger.",
        "Table 5 shows what happens with the normal corpus.The error rate of a is decreased almost 5% with this bidirectional approach.",
        "que",
        "84.50",
        "84.05",
        "1715 / 11063 1764 / 11063",
        "a",
        "94.52",
        "95.26",
        "745 / 13597 644 / 13597",
        "Total",
        "96.5433",
        "96.5506",
        "9559 / 276541 9539 / 276541",
        "Words",
        "P(%)",
        "Err.",
        "/ Ocurr.",
        "que",
        "84.74",
        "1687 / 11056",
        "84.80",
        "1680 / 11056",
        "a",
        "90.94",
        "661 / 7300",
        "92.15",
        "573 / 7300",
        "Total",
        "96.29",
        "96.33",
        "9630 / 259991 9544 / 259991",
        "The Perceptron algorithm was first applied to POS-tagging by (Collins, 2002).",
        "It is an algorithm for supervised learning that resembles Reinforcement Learning, but is simpler and easier to implement.",
        "(Collins, 2002) describes the algorithm for tri-gram HMM taggers.",
        "Here, we will describe it for the VLMM tagger, adapting the notation and explanation.",
        "Instead of using maximum-likelihood estimation for the model parameters, the perceptron algorithm works as follows.",
        "First, the model parameters are initialized to zero.",
        "Then, the algorithm iterates a given number of times over the sentences of the training corpus.",
        "For each sentence s, formed by a sequence of words ws paired with a sequence of tags ts, the Viterbi decoding is ran over ws, returning zs, the predicted sequence of tags.",
        "Then, for each sequence of tags o of length at most k, k the maximum order of the VLMC, seen c1 times in ts and c2 times in zs, we make ac(o) = ac(o) + c1 – c2.",
        "c(o) is the context function defined in Section 2 applied to the tag sequence o, which returns the maximum subsequence of o found in the context tree.",
        "ac(o) represents the parameters of the model associated to c(o), that is, the branch of the context tree that contains c(o).",
        "The above procedure effectively means that parameters which contributed to errors in zs are penalized, while parameters that were not used to predict zs are promoted.",
        "If ts = zs then no parameter is modified.",
        "See (Collins, 2002) for the proof of convergence.",
        "Implementing the perceptron algorithm into the VLMM tagger resulted in the vlmm+perceptron tagger.",
        "Table 6 shows the results obtained.",
        "Note that no prunning is made to the context tree, because doing so led to worse results.",
        "Training and predicting with a full context tree of height 10 achieved better precision.",
        "The numbers reported were obtained after 25 iterations of perceptron training.",
        "The total precision is lower than the vlmm tagger's precision, but it is interesting to note that the precision for que and a actually increased.",
        "Table 6: Comparison of precision using the vlmm tagger (in italics) and the vlmm+perceptron tagger (upcase) with the normal corpus.",
        "(Shen et al., 2007) developed new algorithms based on the easiest-first strategy (Tsuruoka and Tsujii, 2005) and the perceptron algorithm.",
        "The strategy is to first tag words that show less ambiguity, and then use the tags already available as context for the more difficult words.",
        "That means the order of tagging is not necessarily from left to right.",
        "The inference algorithm works by maintaining hypotheses of tags for spans over a sequence of words, and two queues, one for accepted spans and one for candidate spans.",
        "Beam search is used for keeping only a fixed number of candidate hypotheses for each accepted span.",
        "New words from the queue of candidates are tagged based on their scores, computed by considering every possible tag for the word combined with all the available hypotheses on the left context and on the right context.",
        "The highest scoring word is selected, the top hypotheses are kept, and the two queues are updated.",
        "At each step one word from the queue of candidates is selected and inserted in the queue of accepted spans.",
        "The core idea of Guided Learning (GL) training is to model, besides word, tag, and context parameters, also the order of inference.",
        "This is done by defining scores for hypotheses and for actions of tagging (actions of assigning a hypothesis).",
        "The score of a tagging action if computed by a linear combination of a weight vector and a feature vector of the action, which also dependes on the context hypotheses.",
        "The score of a given span's hypothesis is the sum of the scores of the top hypothesis of the left and right contexts (if available) plus the score of the action that led to this hypothesis.",
        "Words",
        "P(%)",
        "Err.",
        "/ Ocurr.",
        "que",
        "84.74",
        "1687 / 11056",
        "85.15",
        "1641 / 11056",
        "a",
        "90.94",
        "661 / 7300",
        "92.41",
        "554 / 7300",
        "Total",
        "96.29",
        "95.98",
        "9630 / 259991 10464 / 259991",
        "The GL algorithm estimates the values of the weight vector.",
        "The procedure is similar to the inference algorithm.",
        "The top scoring span is selected from the queue of candidate spans and, if its top hipothesis matches the gold standard (the tags from the training corpus), the queues of accepted and candidate spans are updated as in the inference algorithm.",
        "Otherwise, the weight vector is updated in a perceptron style by promoting the features of the gold standard action and demoting the features of the top hypothesis' action.",
        "Then the queue of candidate spans is regenerated based on the accepted spans.",
        "This model uses trigrams for the left and right contexts, and so it could be potentially extended by the use of VLMCs.",
        "It is our aim to develop a tagger combining the VLMM and the GL models.",
        "But as for today, we have not yet finished a succesful implementation of the GL model in C++, in order to combine it with the vlmm tagger's code (current code is crashing during training).",
        "Original GL's code is written in Java, which we had access and were able to run over our training and testing corpora.",
        "Table 7 shows the result over the normal corpus.",
        "The first thing to note is that the GL model does a pretty good job at tagging.",
        "The precision means a 10% error reduction.",
        "However, the most interesting thing happens with our two words, que and a.",
        "The precision of que is not significantly higher.",
        "However, the error rate of a is reduced by half.",
        "Such performance shows that the thought about needing the right context to correctly tag a seems correct.",
        "Table 8 shows the confusion matrix of the most common tags for a."
      ]
    },
    {
      "heading": "4. Conclusions",
      "text": [
        "In almost all extended versions of the vlmm tagger, que and a did not suffer a great increase in precision.",
        "With the approaches that tried to generalize context - by using syntactic structure - and capture longer dependencies for que, the results did not change much.",
        "We could see, however, that the right context does not help disambiguating que at all.",
        "Training the VLMM model with a long context (order 10) helped a little with a, but showed over-",
        "Words P (%) Err.",
        "/ Ocurr.",
        "Table 7: Comparison of precision using the vlmm tagger (in italics) and the guided learning tagger (up-case) with the normal corpus.",
        "Table 8: Confusion matrix for a with the most common tags in the normal corpus (line: reference; column: predicted).",
        "all worse results.",
        "Modeling a right context for a in a simple manner did also help a little, but not significantly.",
        "The model that gave good results for a was the one we still have not finished extending with VLMM.",
        "It looks promising, but a way of better disambiguating que was not found.",
        "A better approach to generalize contexts and to try to capture non-local dependencies is needed.",
        "Some further ideas for future work or work in progress are presented in Section 6."
      ]
    },
    {
      "heading": "5. Oportunities for Collaboration",
      "text": [
        "Tycho Brahe is a corpus project undergoing continuous development.",
        "Since there is already a good amount of resource for supervised tagging, our tagger can be used for boosting new texts annotation.",
        "Furthermore, the project has started using E-Dictor, an integrated annotation tool.",
        "E-Dictor offers a range of easy to use tools for corpora creators: from transcription, philological edition, and text normati-zation, to morphosyntactic annotation.",
        "This last tool needs an integrated POS-tagger to further ease the human task of annotation.",
        "Besides, an increasing number of projects is starting and willing to start using E-Dictor, so the need for an automatic tagger is getting urgent.",
        "We have already been contacted by the E-Dictor developers for further collaboration, and should integrate effors during this year.",
        "que",
        "84.74",
        "1687 / 11056",
        "84.90",
        "1670 / 11056",
        "a",
        "90.94",
        "95.49",
        "661 / 7300",
        "329 / 7300",
        "Total",
        "96.29",
        "96.67",
        "9630 / 259991",
        "8650 / 259991",
        "D-F",
        "P",
        "CL",
        "D-F",
        "<4144>",
        "92",
        "5",
        "P",
        "189",
        "<2528>",
        "2",
        "CL",
        "26",
        "9",
        "<294>",
        "Another project that can benefit from a good POStagger is the Brasiliana Digital Library, from the University of Sao Paulo.",
        "It started last year digitalizing books (and other pieces of literature) about Brazil from the 16th to the 19th century, making them available online.",
        "Many books have been OCRed, and a side project is already studying ways of improving the results.",
        "Since the library is an evolving project, the texts will soon be of reasonable size, and will be able to form another corpus of historical Portuguese A POS-tagger will be of great help in making it a new resource for Computational Linguistics research.",
        "We are already negotiating a project for this with the Brasiliana directors.",
        "There is a tagger for Portuguese embedded in the CoGrOO gramatical corrector for Open Office.",
        "They seem to implement some interesting rules for common use Portuguese that maybe would help some of our disambigation problems.",
        "Besides inspecting the available open source code, we have contacted the current maintainer for further conversation.",
        "A possibility that has appeared is to integrate the vlmm tagger with CoGrOO.",
        "Using different data would be interesting in order to check if the exactly same problems arise, or if other languages show the same kind of problems.",
        "We will try to get in contact with other projects having annotated resources available, and seek for further collaboration.",
        "Currently, we got in touch with people working on another corpus of Portuguese.",
        "Both sides are hoping to form a partnership, with us providing a POS tagger and them the annotated corpora."
      ]
    },
    {
      "heading": "6. Future Work",
      "text": [
        "Short term future work includes implementing Guided Learning in C++ and mixing it with VLMCs.",
        "This looks promising since the current GL implementation uses a fixed trigram for contexts to the left and to the right.",
        "Also, there is a need for fast execution in case our tagger is really integrated into",
        "E-Dictor, so converting GL to C++ seems more natural than implementing the vlmm tagger in Java.",
        "To try to tackle the difficulty in tagging que there are some ideas about using context trees of nonlocal tags.",
        "It seems a potentialy good model could be achieved by mixing such context trees with the Guided Learning approach, making a hypothesis consider non adjacent accepted spans.",
        "This is still a fresh idea, so further investigation on maybe other approaches should be done first.",
        "Further investigation involves analyzing errors made by POS taggers over modern Portuguese and other romance languages like Spanish in order to verify if que and a continue to have the same degree of ambiguity or, in case of Spanish, if there are similar words which show similar issues.",
        "This also involves testing other taggers with our training and testing sets, to check if they get the same errors over que and a as we did."
      ]
    }
  ]
}
