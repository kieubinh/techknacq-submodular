{
  "info": {
    "authors": [
      "Ergun Bicici",
      "S. Serdar Kozat"
    ],
    "book": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR",
    "id": "acl-W10-1740",
    "title": "Adaptive Model Weighting and Transductive Regression for Predicting Best System Combinations",
    "url": "https://aclweb.org/anthology/W10-1740",
    "year": 2010
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Ergun Bicici S. Serdar Kozat",
        "Koç University Koç University",
        "34450 Sariyer, Istanbul, Turkey 34450 Sariyer, Istanbul, Turkey ebicici@ku.edu.tr skozat@ku.edu.tr",
        "We analyze adaptive model weighting techniques for reranking using instance scores obtained by L1 regularized transductive regression.",
        "Competitive statistical machine translation is an on-line learning technique for sequential translation tasks where we try to select the best among competing statistical machine translators.",
        "The competitive predictor assigns a probability per model weighted by the sequential performance.",
        "We define additive, multiplicative, and loss-based weight updates with exponential loss functions for competitive statistical machine translation.",
        "Without any pre-knowledge of the performance of the translation models, we succeed in achieving the performance of the best model in all systems and surpass their performance in most of the language pairs we considered."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "When seen as independent instances, system combination task can be solved with a sequential learning algorithm.",
        "Online learning algorithms enable us to benefit from previous good model choices to estimate the next best model.",
        "We use transductive regression based machine translation model to estimate the scores for each sentence.",
        "We analyze adaptive model weighting techniques for system combination when the competing translators are SMT models.",
        "We use separate model weights weighted by the sequential performance.",
        "We use additive, multiplicative, or loss based weight updates to update model weights.",
        "Without any preknowledge of the performance of the translation models, we are able to achieve the performance of the best model in all systems and we can surpass its performance as well as the regression based machine translation's performance.",
        "The next section reviews the transductive regression approach for machine translation, which we use to obtain instance scores.",
        "In section 3 we present competitive statistical machine translation model for solving sequential translation tasks with competing translation models.",
        "Section 4 presents our results and experiments and the last section gives a summary of our contributions."
      ]
    },
    {
      "heading": "2. Transductive Regression Based Machine Translation",
      "text": [
        "Transduction uses test instances, which can sometimes be accessible at training time, to learn specific models tailored towards the test set.",
        "Transduction has computational advantages since we are not using the full training set and a smaller set of constraints exist to satisfy.",
        "Transductive regression based machine translation (TRegMT) aims to reduce the computational burden of the regression approach by reducing the dimensionality of the training set and the feature set and also improve the translation quality by using trans-duction.",
        "Regression Based Machine Translation:",
        "Let n training instances be represented as (xi, Yi), (xn, yn) G X* xY*, where (xi, yi)corresponds to a pair of source and target language token sequences.",
        "Our goal is to find a mapping f : X * – Y * that can convert a given set of source tokens to a set of target tokens that share the same meaning in the target language.",
        "We use feature mappers $X : X* – FX = RNx and $Y : Y* – FY = Rny to represent the training set.",
        "Then, MX g Rnx xn and MY g Rny xn such that Mx = [$x (xi),---, $x (xn)] and My = [$y (yi),---, $y (Yn)].",
        "The ridge regression solution using L2 regularization is found as:",
        "Two main challenges of the regression based machine translation (RegMT) approach are learning the regression function, g : X* – FY, and solving the pre-image problem, which, given the features of the estimated target string sequence, g(x) = $Y(y), attempts to find y g Y*: f(x) = argminy€Y* ||g(x) – $Y(y)||.",
        "Pre-image calculation involves a search over possible translations minimizing the cost function:",
        "We use n-spectrum weighted word feature mappers (Taylor and Cristianini, 2004) which consider all word sequences up to order n.",
        "Li Regularized Regression for Learning:",
        "HL2 is not a sparse solution as most of the coefficients remain non-zero.",
        "L1 norm behaves both as a feature selection technique and a method for reducing coefficient values.",
        "Equation 3 presents the lasso (least absolute shrinkage and selection operator) (Tibshirani, 1996) solution where the regularization term is defined as || H ||1= ij |.",
        "We use forward stagewise regression (FSR) (Hastie et al., 2006) and quadratic programming (QP) to find HLl.",
        "The details of the TRegMT model can be read in a separate submission to the translation task (Bicici and Yuret, 2010)."
      ]
    },
    {
      "heading": "3. Competitive Statistical Machine Translation",
      "text": [
        "We develop the Competitive Statistical Machine Translation (CSMT) framework for sequential translation tasks when the competing models are statistical machine translators.",
        "CSMT uses the output of different translation models to achieve a translation performance that surpasses the translation performance of all of the component models or achieves the performance of the best.",
        "CSMT uses online learning to update the weights used for estimating the best performing translation model.",
        "Competitive predictor assigns a weight per model estimated by their sequential performance.",
        "At each step, m component translation models are executed in parallel over the input source sentence sequence and the loss lp[n] of model p at observation n is calculated by comparing the desired data y[n] with the output of model p, yp[n].",
        "CSMT model selects a model based on the weights and the performance of the selected model as well as the remaining models to adaptively update the weights given for each model.",
        "This corresponds to learning in full information setting where we have access to the loss for each action (Blum and Mansour, 2007).",
        "CSMT learning involves two main steps: estimation and weight update :",
        "where w[n] = (wi[n\\, - - - ,wm[n\\) for m models, Lp is the cumulative squared loss of model p, L[n] stores cumulative and instance losses, and yc[n] is the competitive model estimated for instance n. The learning problem is finding an adaptive w that minimizes the cumulative squared error with appropriate estimation and update methods.",
        "Related Work: Multistage adaptive filtering (Kozat and Singer, 2002) combines the output of multiple adaptive filters to outperform the best among them where the first stage executes models in parallel and the second stage updates parameters using the performance of the combined prediction, yc[n].",
        "Macherey and Och (2007) investigate different approaches for system combination including candidate selection that maximize a weighted combination of BLEU scores among different system outputs.",
        "Their system uses a fixed weight vector trained on the development set to be multiplied with instance BLEU scores.",
        "We use additive, multiplicative, or loss based updates to estimate model weights.",
        "We measure instance loss with trLoss(y[i], yp[i]), which is a function that returns the translation performance of the output translation of model p with respect to the reference translation at instance i.",
        "1-BLEU (Papineni et al., 2001) is one such function with outputs in the range [0,1].",
        "Cumulative squared loss of the p-th translation model is defined as:",
        "We use exponentially re-weighted prediction to estimate model performances, which uses exponentially re-weighted losses based on the outputs of the m different translation models.",
        "We define the additive exponential weight update as follows:",
        "where n > 0 is the learning rate and the denominator is used for normalization.",
        "The update amount, en p[n] is 1 when lp[n] = 0 and it approaches zero with increasing instance loss.",
        "Perceptrons, gradient descent, and Widrow-Huff learning have additive weight updates.",
        "We define the multiplicative exponential weight update as follows:",
        "where we use the squared instance loss.",
        "Equation 7 is similar to the update of Weighted Majority Algorithm (Littlestone and Warmuth, 1992) where the weights of the models that make a mistake are multiplied by a fixed ß such that 0 < ß < 1.",
        "We use Bayesian Information Criterion (BIC) as a loss based re-weighting technique.",
        "Assuming that instance losses are normally distributed with variance e, BIC score is obtained as (Hastie et al., 2009):",
        "where e is estimated by the average of model sample variances of squared instance loss and dp is the number of parameters used in model p which we assume to be the same for all models; therefore we can discard the second term.",
        "The model with the minimum BIC value becomes the one with the highest posterior probability where the posterior probability of model p can be estimated as (Hastie et al., 2009):",
        "The posterior probabilities become model weights and we basically forget about the previous weights, whose information is presumably contained in the cumulative loss, Lp.",
        "We define multiplicative re-weighting with BIC scores as follows:",
        "Model selection: We use stochastic or deterministic selection to choose the competitive model for each instance.",
        "Deterministic choice randomly selects among the maximum scoring models with minimum translation length whereas stochastic choice draws model p with probability proportional to wp[n].",
        "Randomization with the stochastic model selection decreases expected mistake bounds in the weighted majority algorithm (Littlestone and",
        "Warmuth, 1992; Blum, 1996).",
        "Auer et al.",
        "(2002) show that optimal fixed learning rate for the weighted majority algorithm is found as r?M = \\Jm/L*[n] where L*[n] = mini<i<TO Li[n], which requires prior knowledge of the cumulative losses.",
        "We use n = \\Jm/(0-05n) for constant n."
      ]
    },
    {
      "heading": "4. Experiments and Discussion",
      "text": [
        "We perform experiments on the system combination task for the English-German (ende ), German-English (de-en), English-French (en-fr), English-Spanish (en-es), and English-Czech (en-cz) language pairs using the translation outputs for all the competing systems provided in WMT10.",
        "We experiment in a simulated online learning setting where only the scores obtained from the TRegMT system are used during both tuning and testing.",
        "We do not use reference translations in measuring instance performance in this simulated setting for the results we obtain be in line with system combination challenge's goals.",
        "We use the training set provided in WMT10 to index and select transductive instances from.",
        "The challenge split the test set for the translation task of 2489 sentences into a tuning set of 455 sentences and a test set with the remaining 2034 sentences.",
        "Translation outputs for each system is given in a separate file and the number of system outputs per translation pair varies.",
        "We have tokenized and lowercased each of the system outputs and combined these in a single N-best file per language pair.",
        "We use BLEU (Papineni et al., 2001) and NIST (Dod-dington, 2002) evaluation metrics for measuring the performance of translations automatically.",
        "The problem we are solving is online learning with prior information, which comes from the comparative BLEU scores, LM scores, and TRegMT scores at each step n. The scoring functions are explained below:",
        "1.",
        "TRegMT: Transductive regression based machine translation scores as found by Equation 2.",
        "We use the TRegMT scores obtained by the FSR model.",
        "2.",
        "CBLEU: Comparative BLEU scores we obtain by measuring the average BLEU performance of each translation relative to the other systems' translations in the N-best list.",
        "3.",
        "LM: We calculate 5-gram language model scores for each translation using the language model trained over the target corpus provided in the translation task.",
        "To make things simpler, we use a single prior TRegMT system score linearly combining the three scores mentioned with weights learned on the tuning set.",
        "The overall TRegMT system score for instance n, model i is referred as TRegScorei[n].",
        "Since we do not have access to the reference translations nor to the translation model scores each system obtained for each sentence, we estimate translation model performance by measuring the average BLEU performance of each translation relative to other translations in the N-best list.",
        "Thus, each possible translation in the N-best list is BLEU scored against other translations and the average of these scores is selected as the CBLEU score for the sentence.",
        "Sentence level BLEU score calculation avoids singularities in n-gram precisions by taking the maximum of the match count and 2jiF| for | si | denoting the length of the source sentence si as used in (Macherey and",
        "Och, 2007).",
        "We initialize model weights to 1/m for all models, which are updated after each instance according to the losses based on the TRegMT model.",
        "Table 1 presents the performance of the algorithms on the en-de development set.",
        "We have measured their performances with stochastic (stoc.)",
        "or deterministic (det.)",
        "model selection when using only the weights or mixture weights obtained when instance scores are also considered.",
        "Mixture weights are obtained as: wi[n] = wi[n] TRegScorei[n], for instance n, model i.",
        "Baseline performance obtained with random scores.",
        "TRegMT model obtains a performance ing.",
        "The best model performance among the 12 en-de translation models has 1644 BLEU and 5 2647 NIST scores.",
        "Therefore, by using TRegMT score, we are able to achieve better scores.",
        "Not all of the settings are meaningful.",
        "For instance, stochastic model selection is used for algorithms having multiplicative weight updates.",
        "This is reflected in the Table 1 by low performance on the additive and BIC models.",
        "Similarly, using mixture weights may not result in better scores for algorithms with multiplicative updates, which resulted in decreased",
        "Table 1: Performances of the algorithms on the development set over 100 repetitions.",
        "W: Weights, M: Mixture.",
        "performance in Table 1.",
        "Decreased performance with BIC hints that we may use other techniques for mixture weights.",
        "Table 2 presents reranking results on all of the language pairs we considered with the random, TRegMT, and CSMT models.",
        "Random model score lists the random model performance selected among the competing translations randomly and it can be used as a baseline.",
        "Best model score lists the performance of the best model performance.",
        "CSMT models are named with the weighting model used (Add for additive, Mul for multiplicative, BICW for BIC weighting), model selection technique (S for stochastic, D for deterministic), and mixtures model (W for using only weights, M for using mixture weights) with hyphens in between.",
        "Our challenge submission is given in the last row of Table 2 where we used multiplicative exponential weight updates, deterministic model selection, and only the weights during model selection.",
        "For the challenge results, we initialized the weights to the weights obtained in the development set.",
        "We have presented scores that are better than or close to the best model in bold.",
        "We observe that the additive model performs the best by achieving the performance of the best competing translation model and performing better than the best in most of the language pairs.",
        "For the en-de language pair, additive model score achieves even better than the TRegMT model, which is used for evaluating instance scores."
      ]
    },
    {
      "heading": "5. Contributions",
      "text": [
        "We have analyzed adaptive model weighting techniques for system combination when the competing translators are statistical machine translation models.",
        "We defined additive, multiplicative, and loss-based weight updates with exponential loss functions for the competitive statistical machine translation framework.",
        "Competitive SMT via adaptive weighting of various translators is shown to be a powerful technique for sequential translation tasks.",
        "We have demonstrated its use in the system combination task by using the instance scores obtained by the TRegMT model.",
        "Without any pre-knowledge of the performance of the translation models, we have been able to achieve the performance of the best model in all systems and we are able to surpass its performance as well as TRegMT's performance with the additive model."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The research reported here was supported in part by the Scientific and Technological Research Council of Turkey (TUBITAK).",
        "The first author would like to thank Deniz Yuret for helpful discussions and for guidance and support during the term of this research.",
        "Additive",
        "Multiplicative",
        "BIC",
        "BIC Weighting",
        "Setting",
        "BLEU",
        "NIST",
        "BLEU",
        "NIST",
        "BLEU",
        "NIST",
        "BLEU",
        "NIST",
        "Stoc., W",
        ".1419",
        "5.0016 ±.003",
        ".1528",
        "5.1710 ±.001",
        ".1442",
        "5.0468",
        ".1568 ±.001",
        "5.2052 ±.005",
        "Stoc., M",
        ".1415",
        "5.0001",
        ".1525",
        "5.1601 ±.001",
        ".1459",
        "5.0619 ±.004",
        ".1566 ±.001",
        "5.2030 ±.006",
        "Det., W",
        ".1644",
        "5.3208",
        ".1638",
        "5.2571",
        ".1638",
        "5.2542",
        ".1646",
        "5.2535",
        "Det., M",
        ".1643",
        "5.3173",
        ".1536",
        "5.1756",
        ".1530",
        "5.1871",
        ".1507",
        "5.1973",
        "Model",
        "en BLEU",
        "-de NIST",
        "de BLEU",
        "en NIST",
        "en BLEU",
        "-fr NIST",
        "en BLEU",
        "-es NIST",
        "en BLEU",
        "-cz NIST",
        "Random Best model TRegMT",
        ".1490 .1658 .1689",
        "5.6555 5.9610 5.9638",
        ".2088 .2408 .2357",
        "6.4886 6.9861 6.9254",
        ".2415 .2864 .2947",
        "6.8948 7.5272 7.7107",
        ".2648 .3047 .3049",
        "7.2563 7.7559 7.8156",
        ".1283 .1576 .1657",
        "4.9238 5.4480 5.5632",
        "Add-D-W Add-D-M",
        ".1697 .1698",
        "5.9821 5.9824",
        ".2354 .2353",
        "6.9175 6.9152",
        ".2948 .2949",
        "7.7094 7.7103",
        ".3043 .3044",
        "7.8093 7.8091",
        ".1642 .1642",
        "5.5463 5.5461",
        "Mul-S-W Mul-D-W",
        ".1574 .1618",
        "5.7564 5.8912",
        ".2161 .2408",
        "6.5950 6.9854",
        ".2805 .2847",
        "7.4599 7.5085",
        ".2961",
        ".2785",
        ".7.6870",
        "7.4133",
        ".1572",
        ".1612",
        "5.4394",
        "5.5119",
        "BIC-D-W BIC-D-M",
        ".1614 .1580",
        "5.8852 5.7614",
        ".2408",
        ".2141",
        "6.9853",
        "6.5597",
        ".2842",
        ".2791",
        "7.5022",
        "7.4309",
        ".2785 .2876",
        "7.4132 7.5138",
        ".1623 .1577",
        "5.5236 5.4488",
        "BICW-S-W BICW-S-M BICW-D-W",
        ".1621 .1618 .1648",
        "5.8795 5.8730 5.9298",
        ".2274 .2196 .2355",
        "6.8142 6.6493 6.9112",
        ".2802 .2806 .2807",
        "7.4873 7.4948 7.4648",
        ".2892 .2849 .2785",
        "7.5569 7.4845 7.4134",
        ".1565",
        ".1561 .1534",
        "5.4126",
        "5.4099 5.3458",
        "Challenge",
        ".1567",
        "5.73",
        ".2394",
        "6.9627",
        ".2758",
        "7.4333",
        ".3047",
        "7.7559",
        ".1641",
        "5.5435"
      ]
    }
  ]
}
