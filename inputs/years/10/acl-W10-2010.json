{
  "info": {
    "authors": [
      "Stefan Frank"
    ],
    "book": "Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics",
    "id": "acl-W10-2010",
    "title": "Uncertainty Reduction as a Measure of Cognitive Processing Effort",
    "url": "https://aclweb.org/anthology/W10-2010",
    "year": 2010
  },
  "references": [
    "acl-D09-1034",
    "acl-J93-2004",
    "acl-N01-1021"
  ],
  "sections": [
    {
      "text": [
        "Uncertainty reduction as a measure of cognitive processing effort",
        "Stefan L. Frank",
        "The amount of cognitive effort required to process a word has been argued to depend on the word's effect on the uncertainty about the incoming sentence, as quantified by the entropy over sentence probabilities.",
        "The current paper tests this hypothesis more thoroughly than has been done before by using recurrent neural networks for entropy-reduction estimation.",
        "A comparison between these estimates and word-reading times shows that entropy reduction is positively related to processing effort, confirming the entropy-reduction hypothesis.",
        "This effect is independent from the effect of surprisal."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In the field of computational psycholinguistics, a currently popular approach is to account for reading times on a sentence's words by estimates ofthe amount of information conveyed by these words.",
        "Processing a word that conveys more information is assumed to involve more cognitive effort, which is reflected in the time required to read the word.",
        "In this context, the most common formalization of a word's information content is its sur-prisal (Hale, 2001; Levy, 2008).",
        "If word string w* (short for wi,w2,.. .wt) is the sentence so far and P(wt+i|w* ) the occurrence probability of the next word wt+1, then that word's surprisal is defined as – logP(wt+1|w1 ).",
        "It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and",
        "Levy, 2008).",
        "A lesser known alternative operationalization of a word's information content is based on the uncertainty about the rest of the sentence, quantified by Hale (2003, 2006) as the entropy of the probability distribution over possible sentence structures.",
        "The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time.",
        "However, this entropy-reduction hypothesis has not yet been comprehensively tested, possibly because of the difficulty of computing the required entropies.",
        "Although Hale (2006) shows how sentence entropy can be computed given a PCFG, this computation is not feasible when the grammar is of realistic size.",
        "Here, we empirically investigate the entropy-reduction hypothesis more thoroughly than has been done before, by using recurrent neural networks as language models.",
        "Since these networks do not derive any structure, they provide estimates of sentence entropy rather than sentence-structure entropy.",
        "In practice, these two entropies will generally be similar: If the rest of the sentence is highly uncertain, so is its structure.",
        "Sentence entropy can therefore be viewed as a simplification of structure entropy; one that is less theory dependent since it does not rely on any particular grammar.",
        "The distinction between entropy over sentences and entropy over structures will simply be ignored in the remainder of this paper.",
        "Results show that, indeed, a significant fraction of variance in reading-time data is accounted for by entropy reduction, over and above surprisal."
      ]
    },
    {
      "heading": "2. Entropy and sentence processing 2.1 Sentence entropy",
      "text": [
        "Let W be the set of words in the language and Wthe set of all word strings of length i.",
        "The set of complete sentences, denoted S, contains all word strings of any length (i.e., U°=0 Wexcept that a special end-of-sentence marker < /s > is attached to the end of each string.",
        "A generative language model defines a probability distribution over S. The entropy of this distribution is",
        "Y P(wl)logP(wl)",
        "As words are processed one by one, the sentence probabilities change.",
        "When the first t words (i.e., the string wl G W*) of a sentence have been processed, the entropy of the probability distribution over sentences is",
        "In order to simplify later equations, we define the function h(y|x) = – P(y|x) log P(y|x), such that Eq.",
        "1 becomes",
        "If the first t words of wj do not equal wl (or wjhas fewer than t + 1 words), then P(wj | wl ) = 0 so h(wj |w1 ) = 0.",
        "This means that, for computing H(t), only the words from t + 1 onwards need to be taken into account:",
        "H(t)= h(wl+i|w*1 ).",
        "The reduction in entropy due to processing the next word, wt+1, is",
        "AH (t + 1) = H (t) – H (t + 1).",
        "Note that positive AH corresponds to a decrease in entropy.",
        "According to Hale (2006), the nonnegative reduction in entropy (i.e., max{0, AH}) reflects the cognitive effort involved in processing wt+1 and should therefore be predictive of reading time on that word.",
        "Computing H(t) is computationally feasible only when there are very few sentences in S, or when the language can be described by a small grammar.",
        "To estimate entropy in more realistic situations, an obvious solution is to look only at the next few words instead ofall complete continuations ofw t.",
        "Let Sm be the subset of S containing all (and only) sentences of length m or less, counting also the < /s > at the end of each sentence.",
        "Note that this set includes the 'empty sentence' consisting of only </s >.",
        "The set of length-m word strings that do not end in < /s> is Wm.",
        "Together, these sets form Wm = Wm U Sm, which contains all the relevant strings for defining the entropy over strings up to length m. After processing w1, the entropy over strings up to length t + n is:",
        "It now seems straightforward to define suffix-entropy reduction by analogy with sentence-entropy reduction as expressed in Eq.",
        "2: Simply replace H by Hn to obtain",
        "As indicated by its superscript label, AHnsufquantifies the reduction in uncertainty about the upcoming n-word suffix.",
        "However, this is conceptually different from the original AH of Eq.",
        "2, which is the reduction in uncertainty about the identity of the current sentence.",
        "The difference becomes clear when we view the sentence processor's task as that of selecting the correct element from S. If this set of complete sentences is approximated by W*+n, and the task is to select one element from that set, an alternative definition of suffix-entropy reduction arises:",
        "wj ew +n wlew +n",
        "The label 'sent' indicates that AHnent quantifies the reduction in uncertainty about which sentence forms the current input.",
        "This uncertainty is approximated by marginalizing over all word strings longer than t + n.",
        "It is easy to see that lim AHuf so both approximations of entropy reduction appropriately converge to AH in the limit.",
        "Nevertheless, they formalize different quantities and may well correspond to different cognitive factors.",
        "If it is true that cognitive effort is predicted by the reduction in uncertainty about the identity of the incoming sentence, we should find that word-reading times are predicted more accurately by",
        "AHnsent than by AHnsuf.",
        "so the reduction of entropy over the single next word wt+1 equals the next-word entropy just before processing that word.",
        "Note that AH1ent(t +1) is independent of the word at t + 1, making it a severely impoverished measure of the uncertainty reduction caused by that word.",
        "We would therefore expect reading times to be predicted more accurately by AHnent with n > 1, and possibly even by AH1suf.",
        "Roark et al.",
        "(2009) investigated the relation between H1(t + 1) and reading time on wt+1, and found a significant positive effect: Larger next-word entropy directly after processing wt+1 corresponded to longer reading time on that word.",
        "This is of particular interest because H1 (t + 1) necessarily correlates negatively with entropy reduction AHnent(t + 1): If entropy is large after wt+1 , chances are that it did not reduce much through processing of wt+1 .",
        "Indeed, in our data set, H1(t + 1) and AHnsent(t + 1) correlate between r = – .29 and r = – .26 (for n = 2 to n = 4) which is highly significantly (p œ 0) different from 0.",
        "Roark et al.",
        "'s finding of a positive relation between H1 (t + 1) and reading time on wt+1 therefore seems to disconfirm the entropy-reduction hypothesis."
      ]
    },
    {
      "heading": "3. Method",
      "text": [
        "A set of language models was trained on a corpus of POS tags of sentences.",
        "The advantage of using POS tags rather than words is that their probabilities can be estimated much more accurately and, consequently, more accurate prediction of word-reading time is possible (Demberg and Keller, 2008; Roark et al., 2009).",
        "Subsequent to training, the models were made to generate estimates ofsur-prisal and entropy reductions AHnsuf and AHnsent over a test corpus.",
        "These estimates were then compared to reading times measured over the words of the same test corpus.",
        "This section presents the data sets that were used, language-model details, and the evaluation metric.",
        "The models were trained on the POS tag sequences of the full WSJ corpus (Marcus et al., 1993).",
        "They were evaluated on the POS-tagged Dundee corpus (Kennedy and Pynte, 2005), which has been used in several studies that investigate the relation between word surprisal and reading time and Levy, 2008).",
        "This 2 368-sentence (51501 words) collection of British newspaper editorials comes with eye-tracking data of 10 participants.",
        "POS tags for the Dundee corpus were taken from",
        "Frank (2009).",
        "For each word and each participant, reading time was defined as the total fixation time on that word before any fixation on a later word of the same sentence.",
        "Following Demberg and Keller (2008), data points (i.e., word/participant pairs) were removed if the word was not fixated, was presented as the first or last on a line, contained more than one capital letter or a non-letter (e.g., the apostrophe in a clitic), or was attached to punctuation.",
        "Mainly due to the large number (over 46%) of nonfixations, 62.8% of data points were removed, leaving 191 380 data points (between 16 469 and 21 770 per participant).",
        "Entropy is more time consuming to compute than surprisal, even for n = 1, because it requires estimates of the occurrence probabilities at t + 1 of all word types, rather than just of the actual next word.",
        "Moreover, the number of suffixes rises exponentially as suffix length n grows, and, consequently, so does computation time.",
        "Roark et al.",
        "(2009) used an incremental PCFG parser to obtain H1 but this method rapidly becomes infeasible as n grows.",
        "Low-order Markov models (e.g., a bigram model) are more efficient and can be used for larger n but they do not form particularly accurate language models.",
        "Moreover, Markov models lack cognitive plausibility.",
        "Here, Simple Recurrent Networks (SRNs) (El-man, 1990) are used as language models.",
        "When trained to predict the upcoming input in a word sequence, these networks can generate estimates of",
        "P(wt+1|w1 ) efficiently and relatively accurately.",
        "They thereby allow to approximate entence entropy more clo ely than the incremental par er u ed in previou tudie .",
        "Unlike Markov model , SRN have been claimed to form cognitively re-ali tic entence-proce ing model (Chri tian en and MacDonald, 2009).",
        "Moreover, it ha been hownthatSRN-ba ed urpri al e timate cancor-relate more trongly to reading time than urpri al value e timated by a phra e tructure grammar (Frank, 2009).",
        "The SRN compri ed three layer ofunit : the input layer, the recurrent (hidden) layer, and the output layer.",
        "Each input unit corre pond to one POS tag, making 45 input unit ince there are 45 different POS tag in the WSJ corpu .",
        "The network' output unit repre ent prediction of ub equent input .",
        "The output layer al o ha one unit for each POS tag, plu an extra unit that repre ent < /s>, that i , the ab ence of any further input.",
        "Hence, there were 46 output unit .",
        "The number of recurrent unit wa fairly arbitrarily et to 100.",
        "As is common in these networks, the input layer was fully connected to the recurrent layer, which in turn was fully connected to the output layer.",
        "Also, there were time-delayed connections from the recurrent layer to itself.",
        "In addition, each recurrent and output unit received a bias input.",
        "The vectors of recurrent-and output-layer activations after processing w1 are denoted arec(t) and aout(t), respectively.",
        "At the beginning of each sentence, arec(0) = 0.5.",
        "The input vector a-n, representing POS tag i, consists of zeros except for a single element (corresponding to i) that equals one.",
        "When input i is processed, the recurrent layer's state is updated according to:",
        "where matrices Win and Wrec contain the network's input and recurrent connection weights, respectively; brec is the vector of recurrent-layer biases; and activation function frec(x) is the logistic function f (x) = (1+ e-x)-1 applied elementwise to x.",
        "The new output vector is now given by",
        "aout(t) = fout(Woutarec(t) + bout),",
        "where Wout is the matrix of output connection weights; bout the vectorofoutput-layerbiases; and fout( x) the softmax function",
        "fi,out (x1 j .",
        ".",
        ".",
        "j x46) = 7=^ X~.",
        "This function makes sure that aout sums to one and can therefore be viewed as a probability distribution: The i-th element of aout(t) is the SRN's estimate of the probability that the i-th POS tag will be the input at t + 1, or, in case i corresponds to < /s > , the probability that the sentence ends after t POS tags.",
        "Ten SRNs, differing only in their random initial connection weights and biases, were trained using the standard backpropagation algorithm.",
        "Each string of WSJ POS tags was presented once, with the sentences in random order.",
        "After each POS input, connection weights were updated to minimize the cross-entropy between the network outputs and a 46-element vector that encoded the next input(or marked the end of the sentence) by the corresponding element having a value of one and all others being zero.",
        "Since aout(t) is basically the probability distribution P(wt+1 |w1 ), surprisal and H1 can be read off directly.",
        "To obtain H2, H3, and H4, we use the fact that",
        "Surprisal and entropy estimates were averaged over the ten SRNs.",
        "So, for each POS tag of the Dundee corpus, there was one estimate ofsurprisal and four of entropy (for n = 1 to n = 4).",
        "Since Hn(t) approximates H(t) more closely as n grows, it would be natural to expect a better fit to reading times for larger n. On the other hand, it goes without saying that Hn is only a very rough measure of a reader's actual uncertainty about the upcoming n inputs, no matter how accurate the language model that was used to compute these entropies.",
        "Crucially, the correspondence between Hn and the uncertainty experienced by a reader will grow even weaker with larger n. This is apparent from the fact that, as proven in the Appendix, Hn can be expressed in terms of H1 and Hn- 1 :",
        "suffix length n",
        "Figure 1 : Coefficient of correlation between estimates of surprisal and entropy reduction, as a function of suffix length n.",
        "Q.",
        "Effect size",
        "Figure 2: Cumulative % distribution with 1 degree of freedom, plotting statistical significance (p-value) as a function of effect size.",
        "where E(x) is the expected value of x.",
        "Obviously, the expected value of Hn-1 is less appropriate as an uncertainty measure than is Hn-1 itself.",
        "Hence, Hn can be less accurate than Hn-1 as a quantification of the actual cognitive uncertainty.",
        "For this reason, we may expect larger n to result in worse fit to reading-time data.",
        "Hale (2006) argued for nonnegative entropy reduction max{0, AH}, rather than AH itself, as a measure of processing effort.",
        "For AHsent, the difference between the two is negligible because only about 0.03% of entropy reductions are negative.",
        "As for AHsuf, approximately 42% of values are negative so whether these are left out makes quite a difference.",
        "Since preliminary experiments showed that word-reading times are predicted much more accurately by AHsuf than by max{0, AHsuf}, only AHsuf and AHsent were used here, that is, negative values were included.",
        "Both surprisal and entropy reduction can be taken as measures for the amount of information conveyed by a word, so it is to be expected that they are positively correlated.",
        "However, as shown in Figure 1, this correlation is in fact quite weak, ranging from .14 for AH|uf to .38 for AHfnt.",
        "In contrast, AHnsuf and AHnsent correlate very strongly to each other: The coefficients of correlation range from .73 when n = 1 to .97 for n = 4.",
        "A generalized linear regression model for gamma-distributed data was fitted to the reading times.This model contained several well-known predictors of word-reading time: the number of letters in the word, the word's position in the sentence, whether the next word was fixated, whether the previous word was fixated, log of the word's relative frequency, log of the word's forward and backward transitional probabilities, and surprisal of the part-of-speech.",
        "Next, one set of entropy-reduction estimates was added to the regression.",
        "The effect size is the resulting decrease in the regression model's deviance, which is indicative of the amount of variance in reading time accounted for by those estimates of entropy reduction.",
        "Figure 2 shows how effect size is related to statistical significance: A factor forms a significant (p < .05) predictor of reading time if its effect size is greater than 3.84."
      ]
    },
    {
      "heading": "4. Results and Discussion",
      "text": [
        "Figure 3 shows the effect sizes for both measures of entropy reduction, and their relation to suffix length n. All effects are in the correct direction, that is, larger entropy reduction corresponds to longer reading time.",
        "These results clearly support the entropy-reduction hypothesis: A significant",
        "\\",
        "p = .05",
        "suffix length n"
      ]
    },
    {
      "heading": "2. 3 suffix length n",
      "text": [
        "Figure 4: Effect size of entropy reduction (AHnsent), next-word entropy (H1), or surprisal, over and above the other two predictors.",
        "fraction of variance in reading time is accounted for by the entropy-reduction estimates AHnsent, over and above what is explained by the other factors in the regression analysis, including surprisal.",
        "Moreover, the effect of AHnent is larger than that of AHnsuf, indicating that it is indeed uncertainty about the identity of the current sentence, rather than uncertainty about the upcoming input(s), that matters for cognitive processing effort.",
        "Only at n = 1 was the effect size of AHnent smaller than that of AHnsuf, but it should be kept in mind that AH1sent is independent of the incoming word and is therefore quite impoverished as a measure ofthe effort involved in processing the word.",
        "Moreover, the difference between AH1sent and AH1suf is not significant (p > .4), as determined by the bootstrap method (Efron and Tibshirani, 1986).",
        "In contrast, the differences are significant when n > 1 (all p < .01), in spite of the high correlation between AHnsent and AHnsuf.",
        "Another indication that cognitive processing effort is modeled more accurately by AHnsent than by AHnsuf is that the effect size of AHnsent seems less affected by n. Even though AH, the reduction in entropy over complete sentences, is approximated more closely as suffix length grows, increasing n is strongly detrimental to the effect of AHnsuf: It is no longer significant for n > 2.",
        "Presumably, this can be (partly) attributed to the impoverished relation between formal entropy and psychological uncertainty, as explained in Section 3.3.1.",
        "In any case, the effect of AHnsent is more stable.",
        "Although AHnsuf and AHnsent necessarily converge as n – oo, the two effect sizes seem to diverge up to n = 3: The difference between the effect sizes of AHnent and AHnuf is marginally significantly (p < .07) larger for n = 3 than for n = 2.",
        "It is also of interest that surprisal has a significant effect over and above entropy reduction, in the correct (i.e., positive) direction.",
        "When surprisal estimates are added to a regression model that already contains AHnent, the effect size ranges from 8.7 for n = 1 to 13.9 for n = 4.",
        "This show that there exist independent effects of surprisal and entropy reduction on processing effort.",
        "Be reminded from Section 2.3 that Roark et al.",
        "(2009) found a positive relation between reading time on wt+1 and H1(t + 1), the next-word entropy after processing wt+1.",
        "When that value is added as a predictor in the regression model that already contains surprisal and entropy reduction AHnent, model fit greatly improves.",
        "In fact, as can be seen from comparing Figures 3 and 4, the effect of AHnsent is strengthened by including next-word entropy in the regression model.",
        "Moreover, each of the factors surprisal, entropy reduction, and next-word entropy has a significant effect over and above the other two.",
        "In all cases, these effects were in the positive direction.",
        "This confirms Roark et al.",
        "'s finding and shows that it is in fact compatible with the entropy-reduction hypothesis, in contrast to what was suggested in Section 2.3."
      ]
    },
    {
      "heading": "5. Discussion and conclusion",
      "text": [
        "The current results contribute to a growing body of evidence that the amount of information conveyed by a word in sentence context is indicative of the amount of cognitive effort required for processing, as can be observed from reading time on the word.",
        "Several previous studies have shown that surprisal can serve as a cognitively relevant measure for a word's information content.",
        "In contrast, the relevance of entropy reduction as a cognitive measure has not been investigated this thoroughly before.",
        "Hale (2003; 2006) presents entropy-reduction accounts of particular psycholinguistic phenomena, but does not show that entropy reduction generally correlates with word-reading times.",
        "Roark et al.",
        "(2009) presented data that could be taken as evidence against the entropy-reduction hypothesis, but the current paper showed that the next-word entropy effect, found by Roark et al., is independent of the entropy-reduction effect.",
        "It is tempting to take the independent effects of surprisal and entropy reduction as evidence for two distinct cognitive representations or processes, one related to surprisal, the other to entropy reduction.",
        "However, it is very well possible that these two information measures are merely complementary formalizations of a single, cognitively relevant notion of word information.",
        "Since the quantitative results presented here provide no evidence for either view, a more detailed qualitative analysis is needed.",
        "In addition, the relation between reading time and the two measures of word information may be further clarified by the development of mechanistic sentence-processing models.",
        "Both the sur-prisal and entropy-reduction theories provide only functional-level descriptions (Marr, 1982) of the relation between information content and processing effort, so the question remains which underlying mechanism is responsible for longer reading times on words that convey more information.",
        "That is, we are still without a model that proposes, at Marr's computational level, some specific sentence-processing mechanism that takes longer to process a word that has higher surprisal or leads to greater reduction in sentence entropy.",
        "For surprisal, Levy (2008) makes a first step in that direction by presenting a mechanistic account of why surprisal would predict word-reading time: If the state of the sentence-processing system is viewed as a probability distribution over all possible interpretations ofcomplete sentences, and processing a word comes down to updating this distribution to incorporate the new information, then the word's surprisal equals the Kullback-Leibler divergence from the old distribution to the new.",
        "This divergence is presumed to quantify the amount of work (and, therefore, time) needed to update the distribution.",
        "Likewise, Smith and Levy (2008) explain the surprisal effect in terms ofa reader's optimal preparation to incoming input.",
        "When it comes to entropy reduction, however, no reading-time predicting mechanism has been proposed.",
        "Ideally, of course, there should be a single computational-level model that predicts the effects of both sur-prisal and entropy reduction.",
        "One recent model (Frank, 2010) shows that the reading-time effects of both surprisal and entropy reduction can indeed result from a single processing mechanism.",
        "The model simulates sentence comprehension as the incremental and dynamical update of a non-linguistic representation of the state-of-affairs described by the sentence.",
        "In this framework, surprisal and entropy reduction are defined with respect to a probabilistic model of the world, rather than a model of the language: The amount of information conveyed by a word depends on what is asserted by the sentence-so-far, and not on how the sentence's form matches the statistical patterns of the language.",
        "As it turns out, word-processing times in the sentence-comprehension model correlate positively with both surprisal and entropy reduction.",
        "The model thereby forms a computational-level account of the relation between reading time and both measures of word information.",
        "According to this account, the two information measures do not correspond to two distinct cognitive processes.",
        "Rather, there is one comprehension mechanism that is responsible for the incremental revision of a mental representation.",
        "Surprisal and entropy reduction form two complementary quantifications of the extent of this revision."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The research presented here was supported by grant 277-70-006 ofthe Netherlands Organization for Scientific Research (NWO).",
        "I would like to thank Rens Bod, Reut Tsarfaty, and two anonymous reviewers for their helpful comments.",
        "It is of some interest that Hn can be expressed in terms of H1 and the expected value of Hn-1.",
        "First, note that",
        "wt+iGVV",
        "H1(t)+ E(Hn-1(t +1))."
      ]
    }
  ]
}
