{
  "info": {
    "authors": [
      "Jeffrey Heinz",
      "Cesar Koirala"
    ],
    "book": "Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology",
    "id": "acl-W10-2204",
    "title": "Maximum Likelihood Estimation of Feature-Based Distributions",
    "url": "https://aclweb.org/anthology/W10-2204",
    "year": 2010
  },
  "references": [
    "acl-D08-1113",
    "acl-J96-4003"
  ],
  "sections": [
    {
      "text": [
        "Maximum Likelihood Estimation of Feature-based Distributions",
        "Jeffrey Heinz and Cesar Koirala",
        "Motivated by recent work in phonotac-tic learning (Hayes and Wilson 2008, Albright 2009), this paper shows how to de-tine feature-based probability distributions whose parameters can be provably efficiently estimated.",
        "The main idea is that these distributions are defined as a product of simpler distributions (cf. Ghahra-mani and Jordan 1997).",
        "One advantage of this framework is it draws attention to what is minimally necessary to describe and learn phonological feature interactions in phonotactic patterns.",
        "The \"bottom-up\" approach adopted here is contrasted with the \"top-down\" approach in Hayes and Wilson (2008), and it is argued that the bottom-up approach is more analytically transparent."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The hypothesis that the atomic units of phonology are phonological features, and not segments, is one of the tenets of modern phonology (Jakobson et al., 1952; Chomsky and Halle, 1968).",
        "According to this hypothesis, segments are essentially epiphenomenal and exist only by virtue of being a shorthand description of a collection of more primitive units – the features.",
        "Incorporating this hypothesis into phonological learning models has been the focus of much influential work (Gildea and Jurafsky, 1996; Wilson, 2006; Hayes and Wilson, 2008; Moreton, 2008; Albright, 2009).",
        "This paper makes three contributions.",
        "The first contribution is a framework within which:",
        "1. researchers can choose which statistical independence assumptions to make regarding phonological features;",
        "2. feature systems can be fully integrated into strictly local (McNaughton and Papert, 1971) (i.e. n-gram models (Jurafsky and Martin, 2008)) and strictly piecewise models (Rogers et al., 2009; Heinz and Rogers, 2010) in order to define families of provably well-formed, feature-based probability distributions that are provably efficiently estimable.",
        "The main idea is to define a family of distributions as the normalized product of simpler distributions.",
        "Each simpler distribution can be represented by a Probabilistic Deterministic Finite Acceptor (PDFA), and the product of these PDFAs defines the actual distribution.",
        "When a family of distributions T is defined in this way, T may have many fewer parameters than if T is defined over the product PDFA directly.",
        "This is because the parameters of the distributions are defined in terms of the factors which combine in predictable ways via the product.",
        "Fewer parameters means accurate estimation occurs with less data and, relatedly, the family contains fewer distributions.",
        "This idea is not new.",
        "It is explicit in Factorial Hidden Markov Models (FHMMs) (Ghahra-mani and Jordan, 1997; Saul and Jordan, 1999), and more recently underlies approaches to describing and inferring regular string transductions (Dreyer et al., 2008; Dreyer and Eisner, 2009).",
        "Although HMMs and probabilistic finite-state automata describe the same class of distributions (Vidal et al., 2005a; Vidal et al., 2005b), this paper presents these ideas in formal language-theoretic and automata-theoretic terms because (1) there are no hidden states and is thus simpler than FHMMs, (2) determinstic automata have several desirable properties crucially used here, and (3) PDFAs add probabilities to structure whereas HMMs add structure to probabilities and the authors are more comfortable with the former perspective (for further discussion, see Vidal et al.",
        "(2005a,b)).",
        "The second contribution illustrates the main idea with a feature-based bigram model with a strong statistical independence assumption: no two features interact.",
        "This is shown to capture exactly the intuition that sounds with like features have like distributions.",
        "Also, the assumption of non-interacting features is shown to be too strong because like sounds do not have like distributions in actual phonotactic patterns.",
        "Four kinds of fea-tural interactions are identified and possible solutions are discussed.",
        "Finally, we compare this proposal with Hayes and Wilson (2008).",
        "Essentially, the model here represents a \"bottom-up\" approach whereas theirs is \"top-down.\"",
        "\"Top-down\" models, which consider every set of features as potentially interacting in every allowable context, face the difficult problem of searching a vast space and often resort to heuristic-based methods, which are difficult to analyze.",
        "To illustrate, we suggest that the role played by phonological features in the phono-tactic learner in Hayes and Wilson (2008) is not well-understood.",
        "We demonstrate that classes of all segments but one (i.e. the complement classes of single segments) play a significant role, which diminishes the contribution provided by natural classes themselves (i.e. ones made by phonological features).",
        "In contrast, the proposed model here is analytically transparent.",
        "This paper is organized as follows.",
        "§2 reviews some background.",
        "§3 discusses bigram models and §4 defines feature systems and feature-based distributions.",
        "§5 develops a model with a strong independence assumption and §6 discusses feat-ural interaction.",
        "§7 dicusses Hayes and Wilson (2008) and §8 concludes."
      ]
    },
    {
      "heading": "2. Preliminaries",
      "text": [
        "We start with mostly standard notation.",
        "V{A) is the powerset of A. X denotes a finite set of symbols and a string over S is a finite sequence of these symbols.",
        "S+ and X* denote all strings over this alphabet of nonzero but finite length, and of any finite length, respectively.",
        "A function / with domain A and codomain B is written / : A – > B.",
        "When discussing partial functions, the notation f and I indicate for particular arguments whether the function is undefined and defined, respectively.",
        "A language L is a subset of X*.",
        "A stochastic language V is a probability distribution over X*.",
        "The probability p of word w with respect to V is written Prp(w) = p. Recall that all distributions T> must satisfy ^2we^* Prx>{w) = 1.",
        "If L is language then Prv{L) = J2weL Prv{w).",
        "Since all distributions in this paper are stochastic languages, we use the two terms interchangeably.",
        "A Probabilistic Deterministic Finite-state Automaton (PDFA) is a tuple M = (Q,Y,,qo,ö,F,T) where Q is the state set, X is the alphabet, go is the start state, S is a deterministic transition function, F and T are the final-state and transition probabilities.",
        "In particular, T : Q x X R+ and F : Q R+ such that",
        "PDFAs are typically represented as labeled directed graphs (e.g. A4' in Figure 1).",
        "A PDFA A4 generates a stochastic language T>m- If it exists, the (unique) path for a word w = ao ... ak belonging to X* through a PDFA is a sequence ((q0,a0),(q1,a1),... ,(qk,ak)), where qi+i = ô(qi, ai).",
        "The probability a PDFA assigns to w is obtained by multiplying the transition probabilities with the final probability along w's path if it exists, and zero otherwise.",
        "if d(qo,w)i and 0 otherwise",
        "A stochastic language is regular deterministic iff there is a PDFA which generates it.",
        "The structural components of a PDFA A4 is the deterministic finite-state automata (DFA) given by the states Q, alphabet X, transitions 6, and initial state go of A4.",
        "By the structure of a PDFA, we mean its structural components.",
        "Each PDFA A4 defines a family of distributions given by the possible instantiations of T and F satisfying Equation 1.",
        "These distributions have at most | Q \\ ■ ( | X | + 1) parameters (since for each state there are |S possible transitions plus the possibility of finality.)",
        "These are, for all q £ Q and a £ S, the probabilities T(q, a) and F(q).",
        "To make the connection to probability theory, we sometimes write these as Pr(a I q) and Pr(# | g), respectively.",
        "We define the product of PDFAs in terms of co-emission probabilities (Vidal et al., 2005a).",
        "LetA^i = (Qi,Si,goi,^i,-Fi,Ti) and A42 =",
        "(Q2, S2, q02, ô2, F2, T2} be PDFAs.",
        "The probability that ai is emitted from q\\ £ Q\\ at the same moment a2 is emitted from q2 £ Q2 is CT(a1,a2,q1,q2) = T1(q1,a1)-T2(q2,a2).",
        "Similarly, the probability that a word simultaneously ends at q\\ £ Q\\ and at q2 £ Q2 is CF{q\\,q2) = Fi{qi)-F2{q2).",
        "Definition 1 The normalized co-emission product of PDFAs M\\ and M2 is M = Mi x M2 =",
        "1.",
        "Q, qo, and F are defined in terms of the standard DFA product over the state space Qi x Q2 (Hopcroft et al., 2001).",
        "In other words, the numerators of T and F are defined to be the co-emission probabilities, and division by Z ensures that M defines a well-formed probability distribution.",
        "The normalized co-emission product effectively adopts a statistical independence assumption between the states of M\\ and M2.",
        "If S is a list of PDFAs, we write (g) S for their product (note order of product is irrelevant up to renaming of the states).",
        "The maximum likelihood (ML) estimation of regular deterministic distributions is a solved problem when the structure of the PDFA is known (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, 2010).",
        "Let S* be a finite sample of words drawn from a regular deterministic distribution V. The problem is to estimate parameters T and F of",
        "iZ{(qi,q2)) is less than one whenever either -Fi(gi) or £2 (92 ) are neither zero nor one.",
        "M so that T>m approaches V using the widely-adopted ML criterion (Equation 3).",
        "It is well-known that if V is generated by some PDFA M' with the same structural components as M, then the ML estimate of S with respect to M guarantees that T>m approaches V as the size of S goes to infinity (Vidal et al., 2005a; Vidal et al., 2005b; de la Higuera, 2010).",
        "Finding the ML estimate of a finite sample S with respect to M is simple provided M is deterministic with known structural components.",
        "Informally, the corpus is passed through the PDFA, and the paths of each word through the corpus are tracked to obtain counts, which are then normalized by state.",
        "Let M = (Q,T,,ô, qo,F, T) be the PDFA whose parameters F and T are to be estimated.",
        "For all states q £ Q and symbols a £ S, The ML estimation of the probability of T(q, a) is obtained by dividing the number of times this transition is used in parsing the sample S by the number of times state q is encountered in the parsing of S. Similarly, the ML estimation of F(q) is obtained by calculating the relative frequency of state q being final with state q being encountered in the parsing of S. For both cases, the division is normalizing; i.e. it guarantees that there is a well-formed probability distribution at each state.",
        "Figure 1 illustrates the counts obtained for a machine M with sample S = {abca}.",
        "Figure 1 shows a DFA with counts and the PDFA obtained after normalizing these counts."
      ]
    },
    {
      "heading": "3. Strictly local distributions",
      "text": [
        "In formal language theory, strictly k-local languages occupy the bottom rung of a subregular hierarchy which makes distinctions on the basis of contiguous subsequences (McNaughton and Pa-pert, 1971; Rogers and Pullum, to appear; Rogers et al., 2009).",
        "They are also the categorical counterpart to stochastic languages describable with n-gram models (where n = k) (Garcia et al., 1990; Jurafsky and Martin, 2008).",
        "Since stochastic languages are distributions, we refer to strictly k-local stochastic languages as strictly fc-local distri-",
        "Figure 1 : Ai shows the counts obtained by parsing it with sample S = {abca}.",
        "A4' shows the probabilities obtained after normalizing those counts.",
        "butions (SLD&).",
        "We illustrate with SLD2 (bigram models) for ease of exposition.",
        "For an alphabet X, SL2 distributions have (|S| + l) parameters.",
        "These are, for all a,r £ X U {#}, the probabilities Pr(a | r).",
        "The probability of w = (j\\... an is given in Equation 4.",
        "PDFA representations of SL2 distributions have the following structure: Q = S U {#}, go = #, and for all g £ Q and a £ S, it is the case that S(q,a) = a.",
        "As an example, the DFA in Figure 2 provides the structure of PDFAs which recognize SL2 distributions with X = {a, b, c}.",
        "Plainly, the parameters of the model are given by assigning probabilities to each transition and to the ending at each state.",
        "In fact, for all a £ X and r £ X U {#}, Pr(a I t) is T(r,a) and Pr(# | r) is F(t).",
        "It follows that the probability of a particular path through the model corresponds to Equation 4.",
        "The structure of a SL2 distribution for alphabet X is givenby A^sl2(S).",
        "Additionally, given a finite sample S C X*, the ML estimate of S with respect to the family of distributions describable with .MSL2(E) is given by counting the parse of S through A/JSL2(S) and then normalizing as described in §2.",
        "This is equivalent to the procedure described in Jurafsky and Martin (2008, chap.",
        "4)."
      ]
    },
    {
      "heading": "4. Feature-based distributions",
      "text": [
        "This section first introduces feature systems.",
        "Then it defines feature-based SL2 distributions which make the strong independence assumption that no two features interact.",
        "It explains how to find",
        "Figure 2: *MSL2({a, b, c}) represents the structure of SL2 distributions when S = {a, b, c}.",
        "Table 1 : An example of a feature system with S = {a, b, c} and two features F and G.",
        "the ML estimate of samples with respect to such distributions.",
        "This section closes by identifying kinds of featural interactions in phonotactic patterns, and discusses how such interactions can be addressed within this framework.",
        "Assume the elements of the alphabet share properties, called features.",
        "For concreteness, let each feature be a total function F : S – > Vp, where the codomain Vp is a finite set of values.",
        "A finite vector of features F = (Fi,..., Fn) is called a feature system.",
        "Table 1 provides an example of a feature system with F = (F, G) and values VF = VG = {+,-}.",
        "We extend the domain of all features F £ ¥ to £+, so that F (a i ...an) = F(ai)... F(an).",
        "For example, using the feature system in Table 1, F(abc) = + + - and G(abc) = - + +.",
        "We also extend the domain of F to all languages: F(L) = UwELf(w).",
        "We also extend the notation so thatF(cr) = (Fi(a),..., Fn(a)).",
        "For example, F(c) = { – F, +G) (feature indices are included for readability).",
        "For feature F : S – > Vp, let F^ be the inverse function with domain Vp and codomain P(S).",
        "For example in Table 1, G_1(+) = {b, c}.",
        "F_1is similarly defined, i.e. ¥-\\(-F,+G)) = {c}.",
        "If, for all arguments v, F_1(w) is nonempty then the feature system is exhaustive.",
        "If, for all arguments v such that F_1(î/) is nonempty, it is the case that |F_1(î/)| = 1 then the feature system is distinctive.",
        "E.g. the feature system in Table 1 in not exhaustive since F~ ( { – F, – G)) =0, but it is distinctive since where F_1 is nonempty, it picks out exactly one element of the alphabet.",
        "Generally, phonological feature systems for a particular language are distinctive but not exhaustive.",
        "Any feature system F can be made exhaustive by adding finitely many symbols to the alphabet (since F is finite).",
        "Let X' denote an alphabet obtained by adding to X the fewest symbols which make F exhaustive.",
        "Each feature system also defines a set of indicator functions VF = U/gf(v/ x {/}) witn do\" main X such that (v, f)(a) = 1 iff f(a) = v and 0 otherwise.",
        "In the example in Table 1, VF = {+F, – F, +G, – G} (omitting angle braces for readability).",
        "For all / G F, the set VF/ is the VF restricted to /.",
        "So continuing our example, VFF = {+F,-F}.",
        "We now define feature-based SL2 distributions under the strong independence assumption that no two features interact.",
        "For feature system F = (Fi... F„), there are n PDFAs, one for each feature.",
        "The normalized co-emission product of these PDFAs essentially defines the distribution.",
        "For each Fi, the structure of its PDFA is given by A^sl2(VfJ.",
        "For example, Mf = Ms^Ç^f) and M g = M SL2 {y g ) in figures 3 and 4 illustrate the finite-state representation of feature-based SL2distributions given the feature system in Table 1.The states of each machine make distinctions according to features F and G, respectively.",
        "The parameters of these distributions are given by assigning probabilities to each transition and to the ending at each state (except for Fr(# | #)).",
        "Thus there are 2|VF| + ]TFeF |VFF| + 1 parameters for feature-based SL2 distributions.",
        "For example, the feature system in Table 1 defines a distribution with 2 4 + 2 + 2 + 1 = 17 param-",
        "Figure 3: Mp represents a SL2 distribution with respect to feature F.",
        "Figure 4: M g represents a SL2 distribution with respect to feature G eters, which include Fr(# | +F), Pr(+F | #), Pr(+F I +F), Pr(+F \\ -F),..., the G equivalents, and Fr(# | #).",
        "Let SLD2F be the family of distributions given by all possible parameter settings (i.e. all possible probability assignments for each *MsL2(Vi?",
        ";) in accordance with Equation 1.)",
        "The normalized co-emission product defines the feature-based distribution.",
        "For example, the structure of the product of Mf and M g is shown in Figure 5.",
        "As defined, the normalized co-emission product can result in states and transitions that cannot be interpreted by non-exhaustive feature systems.",
        "An example of this is in Figure 5 since ( – F, – G) is not interprétable by the feature system in Table 1.",
        "We make the system exhaustive by letting X' = X U {d} and setting ¥(d) = (-F, -G).",
        "What is the probability of a given b in the feature-based model?",
        "According to the normalized co-emission product (Defmtion 1), it is",
        "Generally, for an exhuastive distinctive feature system F = (Fi,..., F„), and for all a, r G X, the Pr(a | r) is given by Equation 5.",
        "First, the normalization term is provided.",
        "Let",
        "The probabilities Pr(a | #) and Pr(# | r) are similarly decomposed into featural parameters.",
        "Finally, like SL2 distributions, the probability of a word w G X* is given by Equation 4.",
        "We have thus proved the following.",
        "Theorem 1 The parameters of a feature-based SL2 distribution define a well-formed probability distribution over X*.",
        "Proof It is sufficient to show for all r G S U {#} that 2~^(je£u{#} Pr{a I T) = 1 since in this case, Equation 4 yields a well-formed probability distribution over X*.",
        "This follows directly from the definition of the normalized co-emission product (Definition 1).",
        "□",
        "The normalized co-emission product adopts a statistical independence assumption, which here is between features since each machine represents a single feature.",
        "For example, consider Pr(a \\ b) = Pr((-F,+G) I (+F,+G)).",
        "The probability Pr((-F,+G) I (+F, +G)) cannot be arbitrarily different from the probabilities Pr( – F \\ +F) and Pr(+G \\ +G); it is not an independent parameter.",
        "In fact, because Pr(a \\ b) is computed directly as the normalized product of parameters Pr(-F I +F) and Pr(+G \\ +G), the assumption is that the features F and G do not interact.",
        "In other words, this model describes exactly the state of affairs one expects if there is no statistical interaction between phonological features.",
        "In terms of inference, this means if one sound is observed to occur in some context (at least contexts distinguishable by SL2 models), then similar sounds (i.e. those that share many of its featural values) are expected to occur in this context as well.",
        "The ML estimate of feature-based SL2 distributions is obtained by counting the parse of a sample through each feature machine, and normalizing the results.",
        "This is because the parameters of the distribution are the probabilities on the feature machines, whose product determines the actual distribution.",
        "The following theorem follows immediately from the PDFA representation of feature-based SL2 distributions.",
        "Proof The ML estimate of S with respect to SLD2p returns the parameter values that maximize the likelihood of S within the family SLD2p.",
        "The parameters of V GSLD2p are found on the Theorem 2 Let F = (PiFn) and let V be described by M = ®i<i<nM-sL2(yFi)- Consider a finite sample S drawn from V. Then the ML estimate of S with respect to SI.1)2 is obtained by finding, for each Fi G F, the ML estimate of Fi ( S) with respect to M^iyFi)- states of each MsL2{'^Fi)- By definition, each MsL2{^Fi) describes a probability distribution over Fi(E*), as well as a family of distributions.",
        "Therefore finding the MLE of S with respect to SLD2p means finding the MLE estimate of Fi(S) with respect to each Msl2 (V>; )•",
        "Optimizing the ML estimate of Fi(S) for each Mi = -Msl^V^) means that as \\Fi(S)\\ increases, the estimates Tmî and Fj^n approach the true values Tm{ and Fm»- It follows that as 151 increases, Tm and Fm approach the true values of Tm and Fm and consequently T>m approaches V. □",
        "Feature-based models can have significantly fewer parameters than segment-based models.",
        "Consider binary feature systems, where |VF| = 2|F|.",
        "An exhaustive feature system with 10 binary features describes an alphabet with 1024 symbols.",
        "Segment-based bigram models have ( 1024+1 ) = 1,050,625 parameters, but the feature-based one only has 40 + 40 + 1 = 81 parameters!",
        "Consequently, much less training data is required to accurately estimate the parameters of the model.",
        "Another way of describing this is in terms of expressivity.",
        "For given feature system, feature-based SL2 distributions are a proper subset of SL2 distributions since, as the the PDFA representations make clear, every feature-based distribution can be described by a segmental bigram model, but not vice versa.",
        "The fact that feature-based distributions have potentially far fewer parameters is a reflection of the restrictive nature of the model.",
        "The statistical independence assumption constrains the system in predictable ways.",
        "The next section shows exactly what feature-based generalization looks like under these assumptions."
      ]
    },
    {
      "heading": "5. Examples",
      "text": [
        "This section demonstrates feature-based generalization by comparing it with segment-based generalization, using a small corpus S = {aaab, caca, acab, ebb} and the feature system in Table 1.",
        "Tables 2 and 3 show the results of ML estimation of S with respect to segment-based SL2 distributions (unsmoothed bigram model) and feature-based SL2 distributions, respectively.",
        "Each table shows the Pr(a | r) for all a,r £ {a, b, c, d, #} (where F(d) = (-F, – G)), for ease of comparison.",
        "Observe the sharp divergence between the two models in certain cells.",
        "For example, no words begin with b in the sample.",
        "Hence the segment-based ML estimates of Pr(b | #) is zero.",
        "Conversely, the feature-based ML estimate is nonzero because b, like a, is +F, and b, like c, is +G, and both a and c begin words.",
        "Also, notice nonzero probabilities are assigned to d occuring after a and b.",
        "This is because ¥(d) = { – F, – G) and the following sequences all occur in the corpus: [+F][-F] (ac), [+G][-G] (ca), and [-G][-G] (aa).",
        "On the other hand, zero probabilities are assigned to d ocurring after c and d because there are no cc sequences in the corpus and hence the probability of [-F] occuring after [-F] is zero.",
        "This simple example demonstrates exactly how the model works.",
        "Generalizations are made on the basis of individual features, not individual symbols.",
        "In fact, segments are truly epiphenomenal in this model, as demonstrated by the nonzero prob-abilties assigned to segments outside the original feature system (here, this is d).",
        "To sum up, this model captures exactly the idea that the distribution of segments is conditioned on the distributions of its features.",
        "a",
        "P(cr 1 T)",
        "a",
        "b",
        "c",
        "d",
        "#",
        "a",
        "0.29",
        "0.29",
        "0.29",
        "0.",
        "0.14",
        "b",
        "0.",
        "0.25",
        "0.",
        "0.",
        "0.75",
        "T",
        "c",
        "0.75",
        "0.25",
        "0.",
        "0.",
        "0.",
        "d",
        "0.",
        "0.",
        "0.",
        "0.",
        "0.",
        "#",
        "0.5",
        "0.",
        "0.5",
        "0.",
        "0.",
        "P(cr",
        "\\r)",
        "a",
        "a",
        "b",
        "c",
        "d",
        "#",
        "a",
        "0.22",
        "0.43",
        "0.17",
        "0.09",
        "0.09",
        "b",
        "0.32",
        "0.21",
        "0.09",
        "0.13",
        "0.26",
        "T",
        "c",
        "0.60",
        "0.40",
        "0.",
        "0",
        "0.",
        "d",
        "0.33",
        "0.67",
        "0",
        "0",
        "0",
        "#",
        "0.25",
        "0.25",
        "0.25",
        "0.25",
        "0."
      ]
    },
    {
      "heading": "6. Featural interaction",
      "text": [
        "In many empirical cases of interest, features do interact, which suggests the strong independence assumption is incorrect for modeling phonotactic learning.",
        "There are at least four kinds of featural interaction.",
        "First, different features may be prohibited from occuring simultaneously in certain contexts.",
        "As an example of the first type consider the fact that both velars and nasal sounds occur word-initially in English, but the velar nasal may not.",
        "Second, specific languages may prohibit different features from simultaneously occuring in all contexts.",
        "In English, for example, there are syllabic sounds and obstruents but no syllabic obstruents.",
        "Third, different features may be universally incompatible: e.g. no vowels are both [+high] and [+low].",
        "The last type of interaction is that different features may be prohibited from occuring syntag-matically.",
        "For example, some languages prohibit voiceless sounds from occuring after nasals.",
        "Although the independence assumption is too strong, it is still useful.",
        "First, it allows researchers to quantify the extent to which data can be explained without invoking featural interaction.",
        "For example, following Hayes and Wilson (2008), we may be interested in how well human acceptability judgements collected by Scholes (1966) can be explained if different features do not interact.",
        "After training the feature-based SL2 model on a corpus of word initial onsets adapted from the CMU pronouncing dictionary (Hayes and Wilson, 2008, 395-396) and using a standard phonological feature system (Hayes, 2009, chap.",
        "4), it achieves a correlation (Spearman's r) of 0.751.",
        "In other words, roughly three quarters of the acceptability judgements are explained without relying on featural interaction (or segments).",
        "Secondly, the incorrect predictions of the model are in principle detectable.",
        "For example, recall that English has word-inital velars and nasals, but no word-inital velar nasals.",
        "A one-cell chi-squared test can determine whether the observed number of [#rj] is significantly below the expected number according to the feature-based distribution, which could lead to a new parameter being adopted to describe the interaction of the [dorsal] and [nasal] features word-initially.",
        "The details of these procedures are left for future research and are likely to draw from the rich literature on Bayesian networks (Pearl, 1989; Ghahramani, 1998).",
        "More important, however, is this framework allows researchers to construct the independence assumptions they want into the model in at least two ways.",
        "First, universally incompatible features can be excluded.",
        "For example, suppose [-F] and [-G] in the feature system in Table 1 are anatomically incompatible like [+low] and [+high].",
        "If desired, they can be excluded from the model essentially by zeroing out any probability mass assigned to such combinations and re-normalizing.",
        "Second, models can be defined where multiple features are permitted to interact.",
        "For example, suppose features F and G from Table 1 are embedded in a larger feature system.",
        "The machine in Figure 5 can be defined to be a factor of the model, and now interactions between F and G will be learned, including syntagmatic ones.",
        "The flexibility of the framework and the generality of the normalized co-emission product allow researchers to consider feature-based distributions which allow any two features to interact but which prohibit three-feature interactions, or which allow any three features to interact but which prohibit four-feature interactions, or models where only certain features are permitted to interact but not others (perhaps because they belong to the same node in a feature geometry (Clements, 1985; Clements and Hume, 1995).",
        "This section introduces the Hayes and Wilson (2008) (henceforth HW) phonotactic learner and shows that the contribution features play in generalization is not as clear as previously thought.",
        "HW propose an inductive model which acquires a maxent grammar defined by weighted constraints.",
        "Each constraint is described as a sequence of natural classes using phonological features.",
        "The constraint format also allows reference to word boundaries and at most one complement class.",
        "(The complement class of S Ç S is S/S*.)",
        "For example, the constraint *#[\" -voice,+anterior,+strident] [-approximanf] means that in word-initial C1C2 clusters, if C2 is a nasal or obstruent, then Ci must be [s].",
        "Hayes and Wilson maxent models features & complement classes no features & complement classes features & no complement classes no features & no complement classes",
        "HW report that the model obtains a correlation (Spearman's r) of 0.946 with blick test data from Scholes (1966).",
        "HW and Albright (2009) attribute this high correlation to the model's use of natural classes and phonological features.",
        "HW also report that when the model is run without features, the grammar obtained scores an r value of only 0.885, implying that the gain in correlation is due specifically to the use of phonological features.",
        "However, there are two relevant issues.",
        "The first is the use of complement classes.",
        "If features are not used but complement classes are (in effect only allowing the model to refer to single segments and the complements of single segments, e.g. [t] and [\"t]) then in fact the grammar obtained scores an r value of 0.936, a result comparable to the one reported.",
        "Table 4 shows the r values obtained by the HW learner under different conditions.",
        "Note we replicate the main result of r = 0.946 when using both features and complement classes.",
        "This exercise reveals that phonological features play a smaller role in the HW phonotactic learner than previously thought.",
        "Features are helpful, but not as much as complement classes of single segments (though features with complement classes yields the best result by this measure).",
        "The second issue relates to the first: the question of whether additional parameters are worth the gain in empirical coverage.",
        "Wilson and Obdeyn (2009) provide an excellent discussion of the model comparison literature and provide a rigorous comparative analysis of computational mod-eleling of OCP restrictions.",
        "Here we only raise the questions and leave the answers to future research.",
        "Compare the HW learners in the first two rows in Table 4.",
        "Is the ~ 0.01 gain in r score worth the additional parameters which refer to phono-",
        "'Examination of the output grammar reveals heavy reliance on the complement class [\"s], which is not surprising given the discussion of [sC] clusters in HW.",
        "logically natural classes?",
        "Also, the feature-based SL2 model in §4 only receives an r score of 0.751, much lower than the results in Table 4.",
        "Yet this model has far fewer parameters not only because the maxent models in Table 4 keep track of tri-grams, but also because of its strong independence assumption.",
        "As mentioned, this result is informative because it reveals how much can be explained without featural interaction.",
        "In the context of model comparison, this particular model provides an inductive baseline against which the utility of additional parameters invoking featural interaction ought to be measured."
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "The current proposal explicitly embeds the Jakob-sonian hypothesis that the primitive unit of phonology is the phonological feature into a phonotactic learning model.",
        "While this paper specifically shows how to integrate features into n-gram models to describe feature-based strictly n-local distributions, these techniques can be applied to other regular deterministic distributions, such as strictly fc-piecewise models, which describe long-distance dependencies, like the ones found in consonant and vowel harmony (Heinz, to appear; Heinz and Rogers, 2010).",
        "In contrast to models which assume that all features potentially interact, a baseline model was specifically introduced under the assumption that no two features interact.",
        "In this way, the \"bottom-up\" approach to feature-based generalization shifts the focus of inquiry to the featural interactions necessary (and ultimately sufficient) to describe and learn phonotactic patterns.",
        "The framework introduced here shows how researchers can study feature interaction in phonotactic models in a systematic, transparent way."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank Bill Idsardi, Tim O'Neill, Jim Rogers, Robert Wilder, Colin Wilson and the U. of Delaware's phonology/phonetics group for valuable discussion.",
        "Special thanks to Mark Ellison for helpful comments, to Adam Albright for illuminating remarks on the types of featural interaction in phonotactic patterns, and to Jason Eisner for bringing to our attention FHMMs and other related work."
      ]
    }
  ]
}
