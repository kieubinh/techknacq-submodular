{
  "info": {
    "authors": [
      "Bryan Jurish"
    ],
    "book": "Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology",
    "id": "acl-W10-2209",
    "title": "Comparing Canonicalizations of Historical German Text",
    "url": "https://aclweb.org/anthology/W10-2209",
    "year": 2010
  },
  "references": [
    "acl-A00-2038",
    "acl-A92-1021",
    "acl-J88-1003",
    "acl-J94-3001",
    "acl-J96-1003",
    "acl-J96-4002"
  ],
  "sections": [
    {
      "text": [
        "Berlin-Brandenburg Academy of Sciences Berlin, Germany j uri sh@bbaw.de",
        "Historical text presents numerous challenges for contemporary natural language processing techniques.",
        "In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a static lexicon accessed by orthographic form.",
        "In this paper, we present three methods for associating unknown historical word forms with synchronically active canonical cognates and evaluate their performance on an information retrieval task over a manually annotated corpus of historical German verse."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Historical text presents numerous challenges for contemporary natural language processing techniques.",
        "In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a fixed lexicon accessed by orthographic form, such as document indexing systems (Sokirko, 2003; Cafarella and Cutting, 2004), part-of-speech taggers (DeRose, 1988; Brill, 1992; Schmid, 1994), simple word stemmers (Lovins, 1968; Porter, 1980), or more sophisticated morphological analyzers (Geyken and Hanneforth, 2006; Clematide, 2008).",
        "When adopting historical text into such a system, one of the most crucial tasks is the association of one or more extant equivalents with each word of the input text: synchronically active types which best represent the relevant features of the input word.",
        "Which features are considered \"relevant\" here depends on the application in question: for a lemmatization task only the root lexeme is relevant, whereas syntactic parsing may require additional morphosyntactic features.",
        "For current purposes, extant equivalents are to be understood as canonical cognates, preserving both the root(s) and morphosyntactic features of the associated historical form(s), which should suffice (modulo major grammatical and/or lexical semantic shifts) for most natural language processing tasks.",
        "In this paper, we present three methods for automatic discovery of extant canonical cognates for historical German text, and evaluate their performance on an information retrieval task over a small gold-standard corpus."
      ]
    },
    {
      "heading": "2. Canonicalization Methods",
      "text": [
        "In this section, we present three methods for automatic discovery of extant canonical cognates for historical German input: phonetic conflation (Pho), Levenshtein edit distance (Lev), and a heuristic rewrite transducer (rw).",
        "The various methods are presented individually below, and characterized in terms of the linguistic resources required for their application.",
        "Formally, each canonicalization method R is defined by a characteristic conflation relation a binary relation on the set A* of all strings over the finite grapheme alphabet A. Prototypically, ~# will be a true equivalence relation, inducing a partitioning of A* into equivalence classes or \"conflation sets\" [w]R = {v G A* : v ~r w}.",
        "If we assume despite the lack of consistent orthographic conventions that historical graphemic forms were constructed to reflect phonetic forms, and if the phonetic system of the target language is diachronically more stable than the graphematic system, then the phonetic form of a word should provide a better clue to its extant cognates (if any) than a historical graphemic form alone.",
        "Taken together, these assumptions lead to the canonicalization technique referred to here as phonetic conflation.",
        "In order to map graphemic forms to phonetic forms, we may avail ourselves of previous work in the realm of text-to-speech synthesis, a domain in which the discovery of phonetic forms for arbitrary text is an often-studied problem (Allen et al., 1987; Dutoit, 1997), the so-called \"letter-to-sound\" (LTS) conversion problem.",
        "The phonetic conversion module used here was adapted from the LTS rule-set distributed with the IMS German Festival package (Möhler et al., 2001), and compiled as a finite-state transducer (Jurish, 2008).",
        "In general, the phonetic conflation strategy maps each (historical or extant) input word w G A* to a unique phonetic form pho(w) by means of a computable function pho : A* – >■ V*} conflating those strings which share a common phonetic form:",
        "Although the phonetic conflation technique described in the previous section is capable of successfully identifying a number of common historical graphematic variation patterns such as ey/ei, œ/ô, th/t, and tz/z, it fails to conflate historical forms with any extant equivalent whenever the graphematic variation leads to non-identity of the respective phonetic forms, as determined by the LTS rule-set employed.",
        "In particular, whenever a historical variation would effect a pronunciation difference in synchronic forms, that variation will remain uncaptured by a phonetic conflation technique.",
        "Examples of such phonetically salient variations with respect to the simplified IMS German Festival rule-set include guot/gut \"good\", liecht/licht \"light\", tiuvel/teufel \"devil\", and wolln/wollen \"want\".",
        "In order to accommodate graphematic variation phenomena beyond those for which strict phonetic identity of the variant forms obtains, we may employ an approximate search strategy based on the simple Levenshtein edit distance (Levenshtein, 1966; Navarro, 2001).",
        "Formally, let Lex Ç A* be the lexicon of all extant forms, and let c?Lev : A* x A* – >■ N represent the Levenshtein distance over grapheme strings, then define for every input word w G A* the \"best\" synchronic equivalent lV is a finite phonetic alphabet.",
        "bestLev(w) as the unique extant word v G Lex with minimal edit-distance to the input word:",
        "Ideally, the image of a word w under best Lev will itself be the canonical cognate sought, leading to conflation of all strings which share a common image under best Lev:",
        "The function best Lev (w) : A* – >■ Lex can be computed using a variant of the Dijkstra algorithm (Dijkstra, 1959) even when the lexicon is infinite (as in the case of productive nominal composition in German) whenever the set Lex can be represented by a finite-state acceptor (Mohri, 2002; Al-lauzen and Mohri, 2009; Jurish, 2010).",
        "For current purposes, we used the (infinite) input language of the TAGH morphology transducer (Geyken and Hanneforth, 2006) stripped of proper names, abbreviations, and foreign-language material to approximate Lex.",
        "While the simple edit distance conflation technique from the previous section is quite powerful and requires for its implementation only a lexicon of extant forms, the Levenshtein distance itself appears in many cases too coarse to function as a reliable predictor of etymological relations, since each edit operation (deletion, insertion, or substitution) is assigned a cost independent of the characters operated on and of the immediate context in the strings under consideration.",
        "This operand-independence of the traditional Levenshtein distance results in a number of spurious conflations such as those given in Table 1.",
        "In order to achieve a finer-grained and thus more precise mapping from historical forms to extant canonical cognates while preserving some degree of the robustness provided by the relaxation of the strict identity criterion implicit in the edit-distance conflation technique, a non-deterministic weighted finite-state \"rewrite\" transducer was developed to replace the simple Levenshtein metric.",
        "The rewrite transducer was compiled from a heuristic two-level rule-set (Karttunen et al., 1987; Kaplan and Kay, 1994; Laporte, 1997) whose 306 rules were manually constructed to reflect linguistically plausible patterns of diachronic variation as observed in the lemma-instance pairs automatically extracted from the full 5.5 million word DWB verse corpus (Jurish, 2008).",
        "In particular, phonetic phenomena such as schwa deletion, vowel shift, voicing alternation, and articulator}/ location shift are easily captured by such rules.",
        "Of the 306 heuristic rewrite rules, 131 manipulate consonant-like strings, 115 deal with vowellike strings, and 14 operate directly on syllablelike units.",
        "The remaining 46 rules define expansions for explicitly marked elisions and unrecognized input.",
        "Some examples of rules used by the rewrite transducer are given in Table 2.",
        "Formally, the rewrite transducer Arw defines a pseudo-metric [Arw]] : A* x A* – > Roo on all string pairs (Mohri, 2009).",
        "Assuming the non-negative tropical semiring (Simon, 1987) is used to represent transducer weights, analagous to the transducer representation of the Levenshtein metric (Allauzen and Mohri, 2009), the rewrite pseudo-metric can be used as a drop-in replacement for the Levenshtein distance in Equations (2) and (3), yielding Equations (4) and (5):",
        "The conflation techniques described above were tested on a corpus of historical German verse extracted from the quotation evidence in a single volume of the digital first edition of the dictionary Deutsches Wörterbuch \"DWB\" (Bartz et al., 2004).",
        "The test corpus contained 11,242 tokens of 4157 distinct word types, discounting nonalphabetic types such as punctuation.",
        "Each corpus type was manually assigned one or more extant equivalents based on inspection of its occurrences in the whole 5.5 million word DWB verse corpus in addition to secondary sources.",
        "Only extinct roots, proper names, foreign and other non-lexical material were not explicitly assigned any extant equivalent at all; such types were flagged and treated as their own canonical cognates, i.e. identical to their respective \"extant\" equivalents.",
        "In all other cases, equivalence was determined by direct etymological relation of the root in addition to matching morphosyntactic features.",
        "Problematic types were marked as such and subjected to expert review.",
        "296 test corpus types representing 585 tokens were ambiguously associated with more than one canonical cognate.",
        "In a second annotation pass, these remaining ambiguities were resolved on a per-token basis.",
        "The three conflation strategies from Section 2 were evaluated using the gold-standard test corpus to simulate a document indexing and query scenario.",
        "Formally, let G C A* x A* represent the finite set of all gold-standard pairs (w, w) with w the manually determined canonical cognate for the corpus type w, and let Q = {w : 3(w,w) G G} be the set of all canonical cognates represented in the corpus.",
        "Then define for a binary conflation relation ~R on A* and a query string q G Q the sets relevant (q), retrieved#(g) Ç G of relevant and retrieved gold-standard pairs as:",
        "Type-wise precision and recall can then be defined directly as:",
        "UqeQ retrieved#(g) n relevant (q)",
        "\\JqeQ retrievedß(g) UqeQ retrieved#(g) n relevant (q) UqeQ relevant (q)",
        "lïtpR(q) = retrieved#(g) n relevant (q) represents the set of true positives for a query q, then token-wise precision and recall are defined in terms of the gold-standard frequency function",
        "w",
        "best",
        "Lev M",
        "Extant Equivalent",
        "aug",
        "aus",
        "\"out\"",
        "auge \"eye\"",
        "faszt",
        "fast",
        "\"almost\"",
        "fasst \"grabs\"",
        "ouch",
        "buch",
        "\"book\"",
        "auch \"also\"",
        "ram",
        "rat",
        "\"advice\"",
        "rahm \"cream\"",
        "vol",
        "volk",
        "\"people\"",
        "voll \"full\"",
        "Table 2: Some example heuristics used by the rewrite transducer.",
        "Here, e represents the empty string, # represents a word boundary, and V, C C A are sets of vowel-like and consonant-like characters, respectively.",
        "EçeQ,fleretrievedfl(ç) /gG?)",
        "^qeQ,getPR(q) fciä)",
        "We use the unweighted harmonic precision-recall average F (van Rijsbergen, 1979) as a composite measure for both type-and token-wise evaluation modes:",
        "The elementary canonicalization function for each of the conflation techniques was applied to the entire test corpus to simulate a corpus indexing run.",
        "Running times for the various methods on a 1.8GHz Linux workstation using the gf smxl C library are given in Table 3.",
        "The Levenshtein edit-distance technique is at a clear disadvantage here, roughly 150 times slower than the phonetic technique and 40 times slower than the specialized heuristic rewrite transducer.",
        "This effect is assumedly due to the density of the search space (which is maximal for an unrestricted Levenshtein editor), since the gf smxl greedy fc-best search of a Levenshtein transducer cascade generates at least \\A\\ configurations per character, and a single backtracking step requires an additional 3|„4| heap extractions (Jurish, 2010).",
        "Use of specialized lookup algorithms (Oflazer, 1996) might ameliorate such problems.",
        "Qualitative results for several conflation techniques with respect to the DWB verse test corpus are given in Table 4.",
        "An additional conflation relation \"Id\" using strict identity of grapheme strings (w – id v w = v) was tested to provide a baseline for the methods described in Section 2.",
        "As expected, the strict identity baseline relation was the most precise of all methods tested, achieving 99.9% type-wise and 99.1% token-wise precision.",
        "This is unsurprising, since the Id method yields false positives only when a historical form is indistinguishable from a non-equivalent extant form, as in the case of the mapping wider wieder (\"again\") and the non-equivalent extant form wider (\"against\").",
        "Despite its excellent precision, the baseline method's recall was the lowest of any tested method, which supports the claim that a synchronically-oriented lexicon cannot adequately account for a corpus of historical text.",
        "Type-wise recall was particularly low (70.8%), indicating that diachronic variation was more common in low-frequency types.",
        "Surprisingly, the phonetic and Levenshtein edit-distance methods performed similarly for all measures except token-wise precision, in which Lev incurred 61.6% fewer errors than Pho.",
        "Given their near-identical type-wise precision, this difference can be attributed to a small number of phonetic misconfla-tions involving high-frequency types, such as wider^wieder (\"against\" – \"again\"), statt^stadt, (\"instead\"-\"city\"), and in^ihn (\"in\"-\"him\").",
        "Contrary to expectations, Lev did not yield any recall improvements over Pho, although the union of the two underlying conflation relations",
        "From -»■ To / Left _ Right (Cost)",
        "Example(s)",
        "e^e /(A\\{e})_# ( 5 ) z^s 1 s_ ( 1 )",
        "o – >■ a 1 _u ( 1 )",
        "e^h 1 V_C ( 5 ) l^ll / ( 8 )",
        "aug auge \"eye\" faszt fasst \"grabs\" ouch ^ auch \"also\" ram rahm \"cream\" volvoll \"full\"",
        "Method",
        "Time Throughput",
        "Pho Lev rw",
        "1.82 sec 7322 tok/sec 278.03 sec 48 tok/sec 7.02 sec 1898 tok/sec",
        "(~Pho I Lev = ~Pho U -Lev) achieved a type-wise recall of 84.3% (token-wise recall 91.6%), which suggests that these two methods complement one another when both an LTS module and a high-coverage lexicon of extant types are available.",
        "Of the methods described in Section 2, the heuristic rewrite transducer Arw performed best overall, with a type-wise harmonic mean F of 93.2% and a token-wise F of 95.8%.",
        "While Arwincurred some additional precision errors compared to the naïve graphemic identity method Id, these were not as devastating as those incurred by the phonetic or Levenshtein distance methods, which supports the claim from Section 2.3 that a fine-grained context-sensitive pseudo-metric incorporating linguistic knowledge can more accurately model diachronic processes than an all-purpose metric like the Levenshtein distance.",
        "Recall was highest for the composite phonetic-rewrite relation – pho|rw='~Pho U ~rw, although the precision errors induced by the phonetic component outweighed the comparatively small gain in recall.",
        "The best overall performance is achieved by the heuristic rewrite transducer Arw on its own, yielding a reduction of 60.3% in type-wise recall errors and of 59.5% in token-wise recall errors, while minimizing the number of newly introduced precision errors."
      ]
    },
    {
      "heading": "4. Conclusion & Outlook",
      "text": [
        "We have presented three different methods for associating unknown historical word forms with synchronically active canonical cognates.",
        "The heuristic mapping of unknown forms to extant equivalents by means of linguistically motivated context-sensitive rewrite rules yielded the best results in an information retrieval task on a corpus of historical German verse, reducing type-wise recall errors by over 60% compared to a naïve text-matching strategy.",
        "Depending on the availability of linguistic resources (e.g. phonetization rule-sets, lexica), use of phonetic canonicalization and/or Levenshtein edit distance may provide a more immediately accessible route to improved recall for other languages or applications, at the expense of some additional loss of precision.",
        "We are interested in verifying our results using larger corpora than the small test corpus used here, as well as extending the techniques described here to other languages and domains.",
        "In particular, we are interested in comparing the performance of the domain-specific rewrite transducer used here to other linguistically motivated language-independent metrics such as (Covington, 1996; Kondrak, 2000)."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The work described above was funded by a Deutsche Forschungsgemeinschaft (DFG) grant to the project Deutsches Textarchiv.",
        "Additionally, the author would like to thank Jörg Didakowski, Oliver Duntze, Alexander Geyken, Thomas Han-neforth, Henriette Scharnhorst, Wolfgang Seeker, Kay-Michael Würzner, and this paper's anonymous reviewers for their helpful feedback and comments.",
        "Type-wise %",
        "Token-wise",
        "/o",
        "R",
        "prG",
        "rcG",
        "Fg",
        "Prfc",
        "F/G",
        "Id",
        "99.9",
        "70.8",
        "82.9",
        "99.1",
        "83.7",
        "90.7",
        "Pho",
        "96.7",
        "80.1",
        "87.6",
        "92.7",
        "89.6",
        "91.1",
        "Lev",
        "96.6",
        "78.9",
        "86.9",
        "97.2",
        "87.8",
        "92.2",
        "rw",
        "98.5",
        "88.4",
        "93.2",
        "98.2",
        "93.4",
        "95.8",
        "Pho Lev",
        "94.1",
        "84.3",
        "88.9",
        "91.3",
        "91.6",
        "91.5",
        "Pho rw",
        "96.1",
        "89.8",
        "92.8",
        "92.5",
        "94.5",
        "93.5"
      ]
    }
  ]
}
