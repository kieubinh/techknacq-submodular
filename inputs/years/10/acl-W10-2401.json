{
  "info": {
    "authors": [
      "Haizhou Li",
      "A. Kumaran",
      "Min Zhang",
      "Vladimir Pervouchine"
    ],
    "book": "Proceedings of the 2010 Named Entities Workshop",
    "id": "acl-W10-2401",
    "title": "Report of NEWS 2010 Transliteration Generation Shared Task",
    "url": "https://aclweb.org/anthology/W10-2401",
    "year": 2010
  },
  "references": [
    "acl-C02-1099",
    "acl-D08-1037",
    "acl-I08-8003",
    "acl-J98-4003",
    "acl-N03-1017",
    "acl-P04-1021",
    "acl-P06-1010",
    "acl-P06-1103",
    "acl-P07-1119",
    "acl-P08-1045",
    "acl-W02-0505",
    "acl-W03-1508",
    "acl-W06-1672",
    "acl-W09-3501"
  ],
  "sections": [
    {
      "text": [
        "Haizhou Lit, a Kumaran*, Min Zhang\" and Vladimir Pervouchine\"",
        "\"\"Institute for Infocomm Research, A*STAR, Singapore 138632",
        "{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg",
        "* Multilingual Systems Research, Microsoft Research India",
        "This report documents the Transliteration Generation Shared Task conducted as a part of the Named Entities Workshop (NEWS 2010), an ACL 2010 workshop.",
        "The shared task features machine transliteration of proper names from English to 9 languages and from 3 languages to English.",
        "In total, 12 tasks are provided.",
        "7 teams from 5 different countries participated in the evaluations.",
        "Finally, 33 standard and 8 non-standard runs are submitted, where diverse transliteration methodologies are explored and reported on the evaluation data.",
        "We report the results with 4 performance metrics.",
        "We believe that the shared task has successfully achieved its objective by providing a common benchmarking platform for the research community to evaluate the state-of-the-art technologies that benefit the future research and development."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Names play a significant role in many Natural Language Processing (NLP) and Information Retrieval (IR) systems.",
        "They are important in Cross Lingual Information Retrieval (CLIR) and Machine Translation (MT) as the system performance has been shown to positively correlate with the correct conversion of names between the languages in several studies (Demner-Fushman and traditional source for name equivalence, the bilingual dictionaries – whether handcrafted or statistical – offer only limited support because new names always emerge.",
        "All of the above point to the critical need for robust Machine Transliteration technology and systems.",
        "Much research effort has been made to address the transliteration issue in the research community (Knight and Graehl, 1998; Meng et al., 2001; Li et al., 2004; Zelenko and Aone, 2006; 2009b; Li et al., 2009a).",
        "These previous work fall into three categories, i.e., grapheme-based, phoneme-based and hybrid methods.",
        "Grapheme-based method (Li et al., 2004) treats transliteration as a direct orthographic mapping and only uses orthography-related features while phoneme-based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration.",
        "Hybrid method refers to the combination of several different models or knowledge sources to support the transliteration generation.",
        "The first machine transliteration shared task (Li 2009 at ACL-IJCNLP 2009.",
        "It was the first time to provide common benchmarking data in diverse language pairs for evaluation of state-of-the-art techniques.",
        "NEWS 2010 is a continued effort of NEWS 2009.",
        "It builds on the foundations established in the first transliteration shared task and extends the scope to include new language pairs.",
        "The rest of the report is organised as follows.",
        "Section 2 outlines the machine transliteration task and the corpora used and Section 3 discusses the metrics chosen for evaluation, along with the rationale for choosing them.",
        "Sections 4 and 5 present the participation in the shared task and the results with their analysis, respectively.",
        "Section 6 concludes the report."
      ]
    },
    {
      "heading": "2. Transliteration Shared Task",
      "text": [
        "In this section, we outline the definition and the description of the shared task.",
        "There exists several terms that are used interchangeably in the contemporary research literature for the conversion of names between two languages, such as, transliteration, transcription, and sometimes Romanisation, especially if Latin scripts are used for target strings (Halpern, 2007).",
        "Our aim is not only at capturing the name conversion process from a source to a target language, but also at its practical utility for downstream applications, such as CLIR and MT.",
        "Therefore, we adopted the same definition of transliteration as during the NEWS 2009 workshop (Li et al., 2009a) to narrow down \"transliteration\" to three specific requirements for the task, as follows: \"Transliteration is the conversion of a given name in the source language (a text string in the source writing system or orthography) to a name in the target language (another text string in the target writing system or orthography), such that the target language name is: (i) phonemically equivalent to the source name (ii) conforms to the phonology ofthe target language and (iii) matches the user intuition ofthe equivalent ofthe source language name in the target language, considering the culture and orthographic character usage in the target language.\"",
        "In NEWS 2010, we introduce three back-transliteration tasks.",
        "We define backtransliteration as a process of restoring transliterated words to their original languages.",
        "For example, NEWS 2010 offers the tasks to convert western names written in Chinese and Thai into their original English spellings, or romanized Japanese names into their original Kanji writings.",
        "Following the tradition in NEWS 2009, the shared task at NEWS 2010 is specified as development of machine transliteration systems in one or more of the specified language pairs.",
        "Each language pair of the shared task consists of a source and a target language, implicitly specifying the transliteration direction.",
        "Training and development data in each of the language pairs have been made available to all registered participants for developing a transliteration system for that specific language pair using any approach that they find appropriate.",
        "At the evaluation time, a standard hand-crafted test set consisting of between 1,000 and 3,000 source names (approximately 10% of the training data size) have been released, on which the participants are required to produce a ranked list of transliteration candidates in the target language for each source name.",
        "The system output is tested against a reference set (which may include multiple correct transliterations for some source names), and the performance of a system is captured in multiple metrics (defined in Section 3), each designed to capture a specific performance dimension.",
        "For every language pair every participant is required to submit at least one run (designated as a \"standard\" run) that uses only the data provided by the NEWS workshop organisers in that language pair, and no other data or linguistic resources.",
        "This standard run ensures parity between systems and enables meaningful comparison of performance of various algorithmic approaches in a given language pair.",
        "Participants are allowed to submit more \"standard\" runs, up to 4 in total.",
        "If more than one \"standard\" runs is submitted, it is required to name one of them as a \"primary\" run, which is used to compare results across different systems.",
        "In addition, up to 4 \"non-standard\" runs could be submitted for every language pair using either data beyond that provided by the shared task organisers or linguistic resources in a specific language, or both.",
        "This essentially may enable any participant to demonstrate the limits of performance of their system in a given language pair.",
        "The shared task timelines provide adequate time for development, testing (approximately 1 month after the release of the training data) and the final result submission (7 days after the release of the test data).",
        "We considered two specific constraints in selecting languages for the shared task: language diversity and data availability.",
        "To make the shared task interesting and to attract wider participation, it is important to ensure a reasonable variety among the languages in terms of linguistic diversity, orthography and geography.",
        "Clearly, the ability of procuring and distributing a reasonably large (approximately 10K paired names for training and testing together) hand-crafted corpora consisting primarily of paired names is critical for this process.",
        "At the end of the planning stage and after discussion with the data providers, we have chosen the set of 12 tasks shown in Table 1 (Li et al., 2004; Kumaran and Kellner, 2007; MSRI, 2009; CJKI, 2010).",
        "NEWS 2010 leverages on the success of NEWS 2009 by utilizing the training and dev data of of NEWS 2010.",
        "NEWS 2010 provides totally new test data across all 12 tasks for evaluation.",
        "In addition to the 7 tasks inherited from NEWS 2009, NEWS 2010 is enhanced with 5 new tasks, three new languages (Arabic, Bangla and Thai) and two back-transliteration (Chinese to English and Thai to English).",
        "The names given in the training sets for Chinese, Japanese, Korean and Thai languages are Western names and their respective transliterations; the Japanese Name (in English) – Japanese Kanji data set consists only of native Japanese names; the Arabic data set consists only of native Arabic names.",
        "The Indic data set (Hindi, Tamil, Kannada, Bangla) consists of a mix of Indian and Western names.",
        "For all of the tasks chosen, we have been able to procure paired names data between the source and the target scripts and were able to make them available to the participants.",
        "For some language pairs, such as English-Chinese and English-Thai, there are both transliteration and back-transliteration tasks.",
        "Most of the task are just one-way transliteration, although Indian data sets contained mixture of names of both Indian and Western origins.",
        "The language of origin of the names for each task is indicated in the first column of Table 1.",
        "Finally, it should be noted here that the corpora procured and released for NEWS 2010 represent perhaps the most diverse and largest corpora to be used for any common transliteration tasks today."
      ]
    },
    {
      "heading": "3. Evaluation Metrics and Rationale",
      "text": [
        "The participants have been asked to submit results of up to four standard and four non-standard runs.",
        "One standard run must be named as the primary submission and is used for the performance summary.",
        "Each run contains a ranked list of up to 10 candidate transliterations for each source name.",
        "The submitted results are compared to the ground truth (reference transliterations) using 4 evaluation metrics capturing different aspects of transliteration performance.",
        "We have dropped two MAP metrics used in NEWS 2009 because they don't offer additional information to MAPref.",
        "Since a name may have multiple correct transliterations, all these alternatives are treated equally in the evaluation, that is, any of these alternatives is considered as a correct transliteration, and all candidates matching any of the reference transliterations are accepted as correct ones.",
        "The following notation is further assumed: N : Total number of names (source",
        "words) in the test set ui : Number of reference transliterations",
        "for i-th name in the test set (ui > 1) rijj : j-th reference transliteration for i-th",
        "name in the test set ci}k : k-th candidate transliteration (system output) for i-th name in the test set (1 < k < 10) Ki : Number of candidate transliterations produced by a transliteration system",
        "Also known as Word Error Rate, it measures correctness of the first transliteration candidate in the candidate list produced by a transliteration system.",
        "ACC = 1 means that all top candidates are correct transliterations i.e. they match one of the references, and ACC = 0 means that none of the top candidates are correct.",
        "N 0 otherwise",
        "Th m an F-scor m asur s how diff r nt, on av-rag , th top translit ration candidat is from its clos st r f r nc .",
        "F-scor for ach sourc word is a function of Precision and Recall and equals 1 when the top candidate matches one of the references, and 0 when there are no common characters between the candidate and any of the references.",
        "Precision and Recall are calculated based on the length of the Longest Common Subsequence (LCS) between a candidate and a reference:",
        "Name origin Source script Target script Data Owner",
        "where ED is the edit distance and \\x\\ is the length of x.",
        "For example, the longest common subsequence between \"abcd\" and \"afcde\" is \"acd\" and its length is 3.",
        "The best matching reference, that is, the reference for which the edit distance has the minimum, is taken for calculation.",
        "If the best matching reference is given by then Recall, Precision and F-score for i-th word are calculated as",
        "• The length is computed in distinct Unicode characters.",
        "• No distinction is made on different character types of a language (e.g., vowel vs. consonants vs. combining diereses etc.)",
        "Measures traditional MRR for any right answer produced by the system, from among the candidates.",
        "1/MRR tells approximately the average rank of the correct transliteration.",
        "MRR closer to 1 implies that the correct answer is mostly produced close to the top of the n-best lists.",
        "Measures tightly the precision in the n-best candidates for i-th source name, for which reference transliterations are available.",
        "If all of the references are produced, then the MAP is 1.",
        "Let's denote the number of correct candidates for the i-th source word in k-best list as num(i, k).",
        "MAPre/ is then given by"
      ]
    },
    {
      "heading": "4. Participation in Shared Task",
      "text": [
        "7 teams from 5 countries and regions (Canada, Hong Kong, India, Japan, Thailand) submitted their transliteration results.",
        "Two teams have participated in all or almost all tasks while others participated in 1 to 4 tasks.",
        "Each language pair has attracted on average around 4 teams.",
        "The details are shown in Table 3.",
        "Teams are required to submit at least one standard run for every task they participated in.",
        "In total, we receive 33 standard and 8.",
        "Table 2 shows the number of standard and non-standard runs submitted for each task.",
        "It is clear that the most \"popular\" task is transliteration from English to Hindi attempted by 5 participants.",
        "The next most popular are other Indic scripts (Tamil, Kannada, Bangla) and Thai, attempted by 3 participants.",
        "This is somewhat different from NEWS 2009, where the two most popular tasks were English to Hindi and English to Chinese transliteration."
      ]
    },
    {
      "heading": "0. otherwise",
      "text": [
        "Western",
        "English",
        "Chinese",
        "Institute for Infocomm Research",
        "32K",
        "6K",
        "2K",
        "EnCh",
        "Western",
        "Chinese",
        "English",
        "Institute for Infocomm Research",
        "25K",
        "5K",
        "2K",
        "ChEn",
        "Western",
        "English",
        "Korean Hangul",
        "CJK Institute",
        "5K",
        "2K",
        "2K",
        "EnKo",
        "Western",
        "English",
        "Japanese Katakana",
        "CJK Institute",
        "23K",
        "3K",
        "3K",
        "EnJa",
        "Japanese",
        "English",
        "Japanese Kanji",
        "CJK Institute",
        "7K",
        "3K",
        "3K",
        "JnJk",
        "Arabic",
        "Arabic",
        "English",
        "CJK Institute",
        "25K",
        "2.5K",
        "2.5K",
        "ArAe",
        "Mixed",
        "English",
        "Hindi",
        "Microsoft Research India",
        "10K",
        "2K",
        "2K",
        "EnHi",
        "Mixed",
        "English",
        "Tamil",
        "Microsoft Research India",
        "8K",
        "2K",
        "2K",
        "EnTa",
        "Mixed",
        "English",
        "Kannada",
        "Microsoft Research India",
        "8K",
        "2K",
        "2K",
        "EnKa",
        "Mixed",
        "English",
        "Bangla",
        "Microsoft Research India",
        "10K",
        "2K",
        "2K",
        "EnBa",
        "Western",
        "English",
        "Thai",
        "NECTEC",
        "26K",
        "2K",
        "2K",
        "EnTh",
        "Western",
        "Thai",
        "English",
        "NECTEC",
        "24K",
        "2K",
        "2K",
        "ThEn",
        "Table 2: Number of runs submitted for each task.",
        "Number of participants coincides with the number of standard runs submitted."
      ]
    },
    {
      "heading": "5. Task Results and Analysis 5.1 Standard runs",
      "text": [
        "All the results are presented numerically in Tables 4-15, for all evaluation metrics.",
        "These are the official evaluation results published for this edition of the transliteration shared task.",
        "Among the four submitted system papers, Song et al.",
        "(2010) and Finch and Sumita (2010) adopt the approach of phrase-based statistical machine transliteration (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al., 2003) while Das et al.",
        "(2010) adopts the approach of Conditional Random Fields (CRF) (Lafferty et al., 2001).",
        "Jiampo-jamarn et al.",
        "(2010) further develop DirectTL approach presented at the previous NEWS workshop (Jiampojamarn et al., 2009), achieving very good performance in the NEWS 2010.",
        "An example of a completely languageindependent approach is (Finch and Sumita, 2010).",
        "Other participants used language-independent approach but added language-specific pre-or post-processing (Jiampojamarn et al., 2010; Das et al., 2010; Song et al., 2010), including name origin recognition for English to Hindi task (Jiampojamarn et al., 2010).",
        "Combination of different models via re-ranking of their outputs has been used in most of the systems (Das etal., 2010; Song etal., 2010; Finch and Sumita, 2010).",
        "In fact, one system (Song et al., 2010) is mostly devoted to re-ranking of the system output to achieve significant improvement of the ACC (accuracy in top-1) results compared to the same system in NEWS 2009 workshop (Song, 2009).",
        "Compared the same seven tasks among the training sets, but different test sets), we can see that the performance in the NEWS 2010 drops except the English to Korean task.",
        "This could be due to the fact that NEWS 2010 introduces a entirely new test set, which come from different sources than the train and dev sets, while NEWS 2009 have all train, dev and test sets from the same sources.",
        "English to",
        "Chinese to",
        "English to",
        "Thai to En-",
        "English to",
        "English to",
        "Chinese",
        "English",
        "Thai",
        "glish",
        "Hindi",
        "Tamil",
        "Language pair code",
        "EnCh",
        "ChEn",
        "EnTh",
        "ThEn",
        "EnHi",
        "EnTa",
        "Standard runs",
        "5",
        "2",
        "2",
        "2",
        "7",
        "3",
        "Non-standard runs",
        "0",
        "0",
        "1",
        "1",
        "2",
        "1",
        "English to",
        "English to",
        "English",
        "English to",
        "Arabic to",
        "English to",
        "Kannada",
        "Japanese",
        "to Korean",
        "Japanese",
        "English",
        "Bengali",
        "Katakana",
        "Hangul",
        "Kanji",
        "(Bangla)",
        "Language pair code",
        "EnKa",
        "EnJa",
        "EnKo",
        "JnJk",
        "ArAe",
        "EnBa",
        "Standard runs",
        "3",
        "2",
        "1",
        "1",
        "2",
        "3",
        "Non-standard runs",
        "1",
        "0",
        "0",
        "0",
        "0",
        "2",
        "Team Organisation ID",
        "EnCh ChEn EnTh ThEn EnHi EnTa EnKa EnJa EnKo JnJk ArAe EnBa",
        "1* IIT, Bombay",
        "2 University of Alberta",
        "3",
        "4 City University of Hong Kong",
        "5 NICT 6",
        "7 Jadavpur University",
        "x",
        "xxxxxxxxxxxx",
        "x",
        "x x",
        "xxxxxx xx x x",
        "x x x x",
        "As far as back-transliteration is concerned, we can see that English-to-Thai and Thai-to-English have the similar performance.",
        "However, Chinese-to-English back transliteration performs much worse than English-to-Chinese forward transliteration.",
        "This could be due to the fact that Thai and English are alphabet languages in nature while Chinese is not.",
        "As a result, Chinese have much fewer transliteration units than English and Thai.",
        "In other words, Chinese to English transliteration is a one-to-many mapping while English-to-Chinese is a many-to-one mapping.",
        "The later one has fewer mapping ambiguities.",
        "For the non-standard runs there exist no restrictions on the use of data or other linguistic resources.",
        "The purpose of non-standard runs is to see how best personal name transliteration can be, for a given language pair.",
        "In NEWS 2010, the approaches used in non-standard runs are typical and may be summarised as follows:",
        "• Pronunciation dictionaries to convert words to their phonetic transcription (Jiampojamarn et al., 2010).",
        "• Web search.",
        "First, transliteration candidates are generated.",
        "A Web search is then performed to reaffirm or re-rank the candidacy (Das et al., 2010).",
        "Unfortunately, these additional knowledge used in the non-standard runs is not helpful since all non-standard runs perform worse than their corresponding standard runs.",
        "This would be an interesting issue to look into."
      ]
    },
    {
      "heading": "6. Conclusions and Future Plans",
      "text": [
        "The Transliteration Generation Shared Task in NEWS 2010 shows that the community has a continued interest in this area.",
        "This report summarizes the results of the shared task.",
        "Again, we are pleased to report a comprehensive calibration and baselining of machine transliteration approaches as most state-of-the-art machine transliteration techniques are represented in the shared task.",
        "The most popular techniques such as Phrase-Based Machine Transliteration (Koehn et al., 2003), system combination and re-ranking, are inspired by recent progress in statistical machine translation.",
        "As the standard runs are limited by the use of corpus, most of the systems are implemented under the direct orthographic mapping (DOM) framework (Li et al., 2004).",
        "While the standard runs allow us to conduct meaningful comparison across different algorithms, we recognise that the non-standard runs open up more opportunities for exploiting larger linguistic corpora.",
        "It is also noted that two systems have reported significant performance improvement over their",
        "NEWS 2009 systems.",
        "NEWS 2010 Shared Task represents a successful debut of a community effort in driving machine transliteration techniques forward.",
        "We would like to continue this event in the future conference to promote the machine transliteration research and development."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The organisers of the NEWS 2010 Shared Task would like to thank the Institute for Infocomm Research (Singapore), Microsoft Research India, CJK Institute (Japan) and National Electronics and Computer Technology Center (Thailand) for providing the corpora and technical support.",
        "Without those, the Shared Task would not be possible.",
        "We thank those participants who identified errors in the data and sent us the errata.",
        "We also want to thank the members of programme committee for their invaluable comments that improve the quality of the shared task papers.",
        "Finally, we wish to thank all the participants for their active participation that have made this first machine transliteration shared task a comprehensive one.",
        "Team ID",
        "ACC",
        "F -score",
        "MRR MAPre/",
        "Organisation",
        "Primary runs",
        "4",
        "0.477333",
        "0.740494",
        "0.506209 0.455491",
        "City University of Hong Kong",
        "2",
        "0.363333",
        "0.707435",
        "0.430168 0.347701",
        "Non-primary standard runs",
        "2",
        "0.362667",
        "0.704284",
        "0.428854 0.347500",
        "2",
        "0.360333",
        "0.706765",
        "0.428990 0.345215",
        "2",
        "0.357000",
        "0.702902",
        "0.419415 0.341567",
        "2",
        "0.456456",
        "0.884199",
        "0.559212 0.456456",
        "5",
        "0.445445",
        "0.883841",
        "0.574195 0.445445",
        "NICT",
        "3",
        "0.381381",
        "0.860320",
        "0.403172 0.381381",
        "1",
        "0.158158",
        "0.810309",
        "0.231594 0.158158",
        "IIT, Bombay",
        "7",
        "0.150150",
        "0.714490",
        "0.307674 0.150150",
        "Jadavpur University",
        "Non-primary standard runs",
        "2",
        "0.456456",
        "0.885122",
        "0.558203 0.456456",
        "1",
        "0.142142",
        "0.799092",
        "0.205945 0.142142",
        "IIT, Bombay",
        "Non-standard runs",
        "7",
        "0.254254",
        "0.751766",
        "0.369072 0.254254",
        "Jadavpur University",
        "7",
        "0.170170",
        "0.738777",
        "0.314335 0.170170",
        "Jadavpur University",
        "Table 8: Runs submitted for English to Hindi task.",
        "Team ID",
        "ACC",
        "F-score",
        "MRR MAPre/",
        "Organisation",
        "Primary runs",
        "2",
        "0.390000",
        "0.890692",
        "0.515298 0.390000",
        "5",
        "0.390000",
        "0.886560",
        "0.522088 0.390000",
        "NICT",
        "7",
        "0.013000",
        "0.562917",
        "0.121233 0.013000",
        "Jadavpur University",
        "Non-standard runs",
        "7",
        "0.082000",
        "0.759856",
        "0.142317 0.082000",
        "Jadavpur University",
        "Table 9: Runs submitted for English to Tamil task.",
        "Team ID",
        "ACC",
        "F-score",
        "MRR MAPre/",
        "Organisation",
        "Primary runs",
        "5",
        "0.371000",
        "0.871131",
        "0.506010 0.371000",
        "NICT",
        "2",
        "0.341000",
        "0.867133",
        "0.460189 0.341000",
        "7",
        "0.056000",
        "0.663196",
        "0.111500 0.056000",
        "Jadavpur University",
        "Non-standard runs",
        "7",
        "0.055000",
        "0.662106",
        "0.168750 0.055000",
        "Jadavpur University",
        "Table 10: Runs submitted for English to Kannada task.",
        "Team ID",
        "ACC",
        "F-score",
        "MRR MAPre/",
        "Organisation",
        "Primary runs",
        "2",
        "0.397933",
        "0.791233",
        "0.507828 0.398062",
        "5",
        "0.378295",
        "0.782682",
        "0.510096 0.377778",
        "NICT",
        "Table 11: Runs submitted for English to Japanese Katakana task.",
        "Team ID",
        "ACC",
        "F-score",
        "MRR MAPre/",
        "Organisation",
        "Primary runs",
        "2",
        "0.553604",
        "0.770168",
        "0.672665 0.553835",
        "Team ID",
        "ACC",
        "F-score",
        "MRR MAPre/",
        "Organisation",
        "Primary runs",
        "2",
        "0.463679",
        "0.923826",
        "0.535097 0.265379",
        "5",
        "0.403014",
        "0.891443",
        "0.512337 0.327418",
        "NICT",
        "Team ID",
        "ACC",
        "F-score",
        "MRR MAPre/",
        "Organisation",
        "Primary runs",
        "5",
        "0.411705",
        "0.882858",
        "0.549913 0.411705",
        "NICT",
        "2",
        "0.394551",
        "0.876947",
        "0.511876 0.394551",
        "7",
        "0.232089",
        "0.818470",
        "0.325345 0.232089",
        "Jadavpur University",
        "Non-standard runs",
        "7",
        "0.429869",
        "0.875349",
        "0.526152 0.429869",
        "Jadavpur University",
        "7",
        "0.369324",
        "0.845273",
        "0.450589 0.369324",
        "Jadavpur University"
      ]
    }
  ]
}
