{
  "info": {
    "authors": [
      "Amitava Das",
      "Tanik Saikh",
      "Tapabrata Mondal",
      "Asif Ekbal",
      "Sivaji Bandyopadhyay"
    ],
    "book": "Proceedings of the 2010 Named Entities Workshop",
    "id": "acl-W10-2411",
    "title": "English to Indian Languages Machine Transliteration System at NEWS 2010",
    "url": "https://aclweb.org/anthology/W10-2411",
    "year": 2010
  },
  "references": [
    "acl-C00-1056",
    "acl-I08-1009",
    "acl-J98-4003",
    "acl-P02-1051",
    "acl-P04-1021",
    "acl-P06-2025",
    "acl-W03-1508",
    "acl-W09-3517"
  ],
  "sections": [
    {
      "text": [
        "English to Indian Languages Machine Transliteration System at",
        "NEWS 2010",
        "Amitava Das, Tanik Saikh, Tapabrata Mondal, Asif Ekbal, Sivaji Bandyopadhyay",
        "ekbal@cl.uni-heidelberg.de",
        "This paper reports about our work in the NEWS 2010 Shared Task on Transliteration Generation held as part of ACL 2010.",
        "One standard run and two non-standard runs were submitted for English to Hindi and Bengali transliteration while one standard and one nonstandard run were submitted for Kannada and Tamil.",
        "The transliteration systems are based on Orthographic rules and Phoneme based technology.",
        "The system has been trained on the NEWS 2010 Shared Task on Transliteration Generation datasets.",
        "For the standard run' the system demonstrated mean F-Score values of 0.818 for Bengali, 0.714 for Hindi, 0.663 for Kannada and 0.563 for Tamil.",
        "The reported mean F-Score values of non-standard runs are 0.845 and 0.875 for Bengali non-standard run-1 and 2, 0.752 and 0.739 for Hindi nonstandard run-1 and 2, 0.662 for Kannada nonstandard run-1 and 0.760 for Tamil nonstandard run-1.",
        "Non-Standard Run-2 for Bengali has achieved the highest score among all the submitted runs.",
        "Hindi Non-Standard Run-1 and Run-2 runs are ranked as the 5th and 6thamong all submitted Runs."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Transliteration is the method of translating one source language word into another target language by expressing and preserving the original pronunciation in their source language.",
        "Thus, the central problem in transliteration is predicting the pronunciation of the original word.",
        "Transliteration between two languages that use the same set of alphabets is trivial: the word is left as it is.",
        "However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets.",
        "Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition.",
        "In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khufound in (Das et al., 2009).",
        "One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run:",
        "KSR) and Tamil (Tamil Standard Run: TSR) were submitted.",
        "Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliteration were submitted.",
        "Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tamil",
        "Non-Standard Run-1: TNSR1)."
      ]
    },
    {
      "heading": "2. Machine Transliteration Systems",
      "text": [
        "Five different transliteration models have been proposed in the present report that can generate the transliteration in Indian language from an English word.",
        "The transliteration models are named as Trigram Model (Tri), Joint Source-Channel Model (JSC), Modified Joint Source-Channel Model (MJSC), Improved Modified Joint Source-Channel Model (IMJSC) and International Phonetic Alphabet Based Model (IPA).",
        "Among all the models the first four are categorized as orthographic model and the last one i.e. IPA based model is categorized as phoneme based model.",
        "An English word is divided into Transliteration Units (TUs) with patterns C*V*, where C represents a consonant and V represents a vowel.",
        "The targeted words in Indian languages are divided into TUs with patterns C+M?, where C represents a consonant or a vowel or a conjunct and M represents the vowel modifier or matra.",
        "The TUs are the basic lexical units for machine transliteration.",
        "The system considers the English and Indian languages contextual information in the form of collocated TUs simultaneously to calculate the plausibility of transliteration from each English TU to various Indian languages candidate TUs and chooses the one with maximum probability.",
        "The system learns the mappings automatically from the bilingual NEWS 2010 training set being guided by linguistic features/knowledge.",
        "The output of the mapping process is a decision-list classifier with collocated TUs in the source language and their equivalent TUs in collocation in the target language along with the probability of each decision obtained from the training set.",
        "A Direct example base has been maintained that contains the bilingual training examples that do not result in the equal number of TUs in both the source and target sides during alignment.",
        "The Direct example base is checked first during machine transliteration of the input English word.",
        "If no match is obtained, the system uses direct orthographic mapping by identifying the equivalent TU in Indian languages for each English TU in the input and then placing the target language TUs in order.",
        "The IPA based model has been used for English dictionary words.",
        "Words which are not present in the dictionary are handled by other orthographic models as Trigram, JSC, MJSC and",
        "IMJSC.",
        "The transliteration models are described below in which S and T denotes the source and the target words respectively:"
      ]
    },
    {
      "heading": "3. Orthographic Transliteration models",
      "text": [
        "The orthographic models work on the idea of TUs from both source and target languages.",
        "The orthographic models used in the present system are described below.",
        "For transliteration, P(I), i.e., the probability of transliteration in the target language, is calculated from a English-Indian languages bilingual database If, T is not found in the dictionary, then a very small value is assigned to P(T).",
        "These models have been desribed in details in Ekbal et al.",
        "(2007).",
        "This is basically the Trigram model where the previous and the next source TUs are considered as the context.",
        "This is essentially the Joint Source-Channel model (Hazhiou et al., 2004) where the previous TUs with reference to the current TUs in both the source (s) and the target sides (t) are considered as the context.",
        "In this model, the previous and the next TUs in the source and the previous target TU are considered as the context.",
        "This is the Modified Joint Source-Channel model.",
        "Channel Model (IMJSC)",
        "In this model, the previous two and the next TUs in the source and the previous target TU are considered as the context.",
        "This is the Improved Modified Joint Source-Channel model."
      ]
    },
    {
      "heading": "4. International Phonetic Alphabet (IPA) Model",
      "text": [
        "The NEWS 2010 Shared Task on Transliteration Generation challenge addresses general domain transliteration problem rather than named entity transliteration.",
        "Due to large number of dictionary words as reported in Table 1 in NEWS 2010 data set a phoneme based transliteration algorithm has been devised.",
        "The International Phonetic Alphabet (IPA) is a system of representing phonetic notations based primarily on the Latin alphabet and devised by the International Phonetic Association as a standardized representation of the sounds of spoken language.",
        "The machine-readable Carnegie Mellon Pronouncing Dictionary has been used as an external resource to capture source language IPA structure.",
        "The dictionary contains over 125,000 words and their transcriptions with mappings from words to their pronunciations in the given phoneme set.",
        "The current phoneme set contains 39 distinct phonemes.",
        "As there is no such parallel IPA dictionary available for Indian languages, English IPA structures have been mapped to TUs in Indian languages during training.",
        "An example of such mapping between phonemes and TUs are shown in Table 3, for which the vowels may carry lexical stress as reported in Table 2.",
        "This phone set is based on the ARPAbet symbol set developed for speech recognition uses.",
        "A preprocessing module checks whether a targeted source English word is a valid dictionary word or not.",
        "The dictionary words are then handled by phoneme based transliteration module.",
        "Words and TUs",
        "In the target side we use our TU segregation logic to get phoneme wise transliteration pattern.",
        "We present this problem as a sequence labelling problem, because transliteration pattern changes depending upon the contextual phonemes in source side and TUs in the target side.",
        "We use a standard machine learning based sequence labeller Conditional Random Field (CRF) here.",
        "IPA based model increased the performance for Bengali, Hindi and Tamil languages as reported in Section 6.",
        "The performance has decreased for Kannada."
      ]
    },
    {
      "heading": "5. Ranking",
      "text": [
        "The ranking among the transliterated outputs follow the order reported in Table 4: The ranking decision is based on the experiments as described in (Ekbal et al., 2006) and additionally based on the experiments on NEWS 2010 development dataset.",
        "In BSR, HSR, KSR and TSR the orthographic TU based models such as: IMJSC, MJSC, JSC and Tri have been used only trained by NEWS 2010 dataset.",
        "In BNSR1 and HNSR1 all the orthographic models have been trained with additional census dataset as described in Section 6.",
        "In case of BNSR2, HNSR2, KNSR1 and TNSR1 the output of the IPA based model has been added with highest priority.",
        "As no census data is available for Kannada and Tamil therefore there is only one Non-Standard Run was submitted for these two languages only with the output of IPA based model along with the output of Standard",
        "Run."
      ]
    },
    {
      "heading": "6. Experimental Results",
      "text": [
        "We have trained our transliteration models using the NEWS 2010 datasets obtained from the NEWS 2010 Machine Transliteration Shared datasets are presented in Table 5.",
        "During training, we have split multi-words into collections of single word transliterations.",
        "It was observed that the number of tokens in the source and target sides mismatched in various multi-words and these cases were not considered further.",
        "Following are some examples:",
        "Phoneme",
        "Example",
        "Translation",
        "TUs",
        "AA",
        "odd",
        "AA0-D",
        "AH",
        "hut",
        "HH0-AH-T",
        "D",
        "dee",
        "D-IY1",
        "Train",
        "Dev",
        "Test",
        "Bengali",
        "7.77%",
        "5.14%",
        "6.46%",
        "Hindi",
        "27.82%",
        "15.80%",
        "3.7%",
        "Kannada",
        "27.60%",
        "14.63%",
        "4.4%",
        "Tamil",
        "27.87%",
        "17.31%",
        "3.0%",
        "Word Type",
        "Ranking Order",
        "1",
        "2",
        "3",
        "4",
        "5",
        "Dictionary",
        "IPA",
        "IMJSC",
        "MJSC",
        "JSC",
        "Tri",
        "Non-Dictionary",
        "IMJSC",
        "MJSC",
        "JSC",
        "Tri",
        "-",
        "Representation",
        "Stress level",
        "0",
        "No",
        "1",
        "Primary",
        "2",
        "Secondary",
        "Paris Charles de Gaulle 3f^T Suven Life Scie c&ï5ctf d^q?",
        "Delta Air Lines Ql_6bl_iï",
        "In the training set, some multi-words were partly translated and not transliterated.",
        "Such examples were dropped from the training set.",
        "In the following example the English word \"National\" is being translated in the target as \"TTCfW'.",
        "Australian National University 3ÏÎ+£fà±M TTfRT",
        "There is less number of known examples in the NEWS 2010 test set from training set.",
        "The exact figure is reported in the Table 6.",
        "If the outputs of any two transliteration models are same for any word then only one output are provided for that particular word.",
        "Evaluation results of the final system are shown in Table 7 for Bengali, Table 8 for Hindi, Table 9 for Kannada and Table 10 for Tamil.",
        "Accuracy I",
        "The additional dataset used for the nonstandard runs is mainly the census data consisting of only Indian person names that have been collected from the web .",
        "In the BNSR1 and HNSR1 we have used an English-Bengali/Hindi bilingual census example dataset.",
        "English-Hindi set consist of 961,890 examples and English-Bengali set consist of 582984 examples.",
        "This database contains the frequency of the corresponding English-Bengali/Hindi name pair."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "This paper reports about our works as part of the",
        "NEWS 2010 Shared Task on Transliteration",
        "Generation.",
        "We have used both the orthographic and phoneme based transliteration modules for the present task.",
        "As our all previous efforts was for named entity transliteration.",
        "The Transliteration Generation challenge addresses general domain transliteration problem rather than named entity transliteration.",
        "To handle general transliteration problem we proposed a IPA based methodology.",
        "Accuracy",
        "Parameters",
        "HSR",
        "HNSR1",
        "HNSR2",
        "Accuracy in top-1",
        "0.150",
        "0.254",
        "0.170",
        "Mean F-score",
        "0.714",
        "0.752",
        "0.739",
        "Mean Reciprocal Rank (MRR)",
        "0.308",
        "0.369",
        "0.314",
        "Mean Average Precision",
        "(MAP)ref",
        "0.150",
        "0.254",
        "0.170",
        "Parameters",
        "KSR",
        "KNSR1",
        "Accuracy in top-1",
        "0.056 0.055",
        "Mean F-score",
        "0.663 0.662",
        "Mean Reciprocal Rank (MRR)",
        "0.112 0.169",
        "Mean Average Precision (MAP)ref",
        "0.056 0.055",
        "Table 9: Results on Kannada Test Set",
        "Parameters",
        "Accuracy",
        "TSR",
        "TNSR1",
        "Accuracy in top-1",
        "0.013 0.082",
        "Mean F-score",
        "0.563 0.760",
        "Mean Reciprocal Rank (MRR)",
        "0.121 0.142",
        "Mean Average Precision (MAP)ref",
        "0.013 0.082",
        "Set",
        "Number of examples",
        "Bng Hnd Kn Tm",
        "Training",
        "11938 9975 7990 7974",
        "Development",
        "992 1974 1968 1987",
        "Test",
        "991 1000 1000 1000",
        "Matches with training",
        "Bengali",
        "14.73%",
        "Hindi",
        "0.2%",
        "Kannada",
        "0.0%",
        "Tamil",
        "0.0%",
        "Accuracy",
        "Parameters",
        "BSR",
        "BNSR1",
        "BNSR2",
        "Accuracy in top-1",
        "0.232",
        "0.369",
        "0.430",
        "Mean F-score",
        "0.818",
        "0.845",
        "0.875",
        "Mean Reciprocal Rank (MRR)",
        "0.325",
        "0.451",
        "0.526",
        "Mean Average Precision",
        "(MAP)ref",
        "0.232",
        "0.369",
        "0.430"
      ]
    }
  ]
}
