{
  "info": {
    "authors": [
      "Alexander Clark"
    ],
    "book": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning",
    "id": "acl-W10-2904",
    "title": "Efficient, Correct, Unsupervised Learning for Context-Sensitive Languages",
    "url": "https://aclweb.org/anthology/W10-2904",
    "year": 2010
  },
  "references": [
    "acl-P04-1061",
    "acl-P93-1034",
    "acl-W01-0714",
    "acl-W09-0904"
  ],
  "sections": [
    {
      "text": [
        "Efficient, correct, unsupervised learning of context-sensitive languages",
        "A central problem for NLP is grammar induction: the development of unsupervised learning algorithms for syntax.",
        "In this paper we present a lattice-theoretic representation for natural language syntax, called Distributional Lattice Grammars.",
        "These representations are objective or empiricist, based on a generalisation of distributional learning, and are capable of representing all regular languages, some but not all context-free languages and some non-context-free languages.",
        "We present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efficiency of the algorithm."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Grammar induction, or unsupervised learning of syntax, no longer requires extensive justification and motivation.",
        "Both from engineering and cognitive/linguistic angles, it is a central challenge for computational linguistics.",
        "However good algorithms for this task are thin on the ground.",
        "There are numerous heuristic algorithms, some of which have had significant success in inducing constituent structure (Klein and Manning, 2004).",
        "There are algorithms with theoretical guarantees as to their correctness - such as for example Bayesian algorithms for inducing PCFGs (Johnson, 2008), but such algorithms are inefficient: an exponential search algorithm is hidden in the convergence of the MCMC samplers.",
        "The efficient algorithms that are actually used are heuristic approximations to the true posteriors.",
        "There are algorithms like the Inside-Outside algorithm (Lari and Young, 1990) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that may be, and in practice nearly always is very far from the optimum.",
        "There are naive enumerative algorithms that are correct, but involve exhaustively enumerating all representations below a certain size (Horning, 1969).",
        "There are no correct and efficient algorithms, as there are for parsing, for example.",
        "There is a reason for this: from a formal point of view, the problem is intractably hard for the standard representations in the Chomsky hierarchy.",
        "Abe and Warmuth (1992) showed that training stochastic regular grammars is hard; Angluin and Kharitonov (1995) showed that regular grammars cannot be learned even using queries; these results obviously apply also to PCFGs and CFGs as well as to the more complex representations built by extending CFGs, such as TAGs and so on.",
        "However, these results do not necessarily apply to other representations.",
        "Regular grammars are not learnable, but deterministic finite automata are learnable under various paradigms (Angluin, 1987).",
        "Thus it is possible to learn by changing to representations that have better properties: in particular DFAs are learnable because they are \"objective\"; there is a correspondence between the structure of the language, (the residual languages) and the representational primitives of the formalism (the states) which is expressed by the Myhill-Nerode theorem.",
        "In this paper we study the learnability of a class of representations that we call distributional lattice grammars (DLGs).",
        "Lattice-based formalisms were introduced by Clark et al.",
        "(2008) and Clark (2009) as context sensitive formalisms that are potentially learnable.",
        "Clark et al.",
        "(2008) established a similar learnability result for a limited class of context free languages.",
        "In Clark (2009), the approach was extended to a significantly larger class but without an explicit learning algorithm.",
        "Most of the building blocks are however in place, though we need to make several modifications and extensions to get a clean result.",
        "Most importantly, we need to replace the representation used there, which naively could be exponential, with a lazy, exemplar based model.",
        "In this paper we present a simple algorithm for the inference of these representations and prove its correctness under the following learning paradigm: we assume that as normal there is a supply of positive examples, and additionally that the learner can query whether a string is in the language or not (an oracle for membership queries).",
        "We also prove that the algorithm is efficient in the sense that it will use a polynomial amount of computation and makes a polynomial number of queries at each step.",
        "The contributions of this paper are as follows: after some basic discussion of distributional learning in Section 2, we define in Section 3 an exemplar-based grammatical formalism which we call Distributional Lattice Grammars.",
        "We then give a learning algorithm under a reasonable learning paradigm, together with a self contained proof in elementary terms (not presupposing any extensive knowledge of lattice theory), of the correctness of this algorithm."
      ]
    },
    {
      "heading": "2. Basic definitions",
      "text": [
        "We now define our notation; we have a finite alphabet E; let E* be the set of all strings (the free monoid) over E, with A the empty string.",
        "A (formal) language is a subset of E*.",
        "We can concatenate two languages A and B to get AB = [uv\\u G A,b G B}.",
        "A context or environment, as it is called in structuralist linguistics, is just an ordered pair of strings that we write (l, r) where l and r refer to left and right; l and r can be of any length.",
        "We can combine a context (l, r) with a string u with a wrapping operation that we write 0: so (l, r) 0 u is defined to be lur.",
        "We will sometimes write f for a context (l, r).",
        "There is a special context (A, A): (A, A) 0 w = w. We will extend this to sets of contexts and sets of strings in the natural way.",
        "We will write Sub(w) = (u\\3(l,r) : lur = w} for the set of substrings of a string, and Con(w) = {(l,r)\\3u G E* : lur = w}.",
        "For a given string w we can define the distribution of that string to be the set of all contexts that it can appear in: CL(w) = {(l,r)\\lwr G L}, equivalently {f\\f 0 w G L}.",
        "Clearly (A, A) G CL(w) iff w G L.",
        "Distributional learning (Harris, 1954) as a technical term refers to learning techniques which model directly or indirectly properties of the distribution of strings or words in a corpus or a language.",
        "There are a number of reasons to take distributional learning seriously: first, historically, CFGs and other PSG formalisms were intended to be learnable by distributional means.",
        "Chomsky (2006) says (p. 172, footnote 15):",
        "The concept of \"phrase structure grammar\" was explicitly designed to express the richest system that could reasonably be expected to result from the application of Harris-type procedures to a corpus.",
        "Second, empirically we know they work well at least for lexical induction, (Schütze, 1993; Cur-ran, 2003) and are a component of some implemented unsupervised learning systems (Klein and Manning, 2001).",
        "Linguists use them as one of the key tests for constituent structure (Carnie, 2008), and finally there is some psycholinguistic evidence that children are sensitive to distributional structure, at least in artificial grammar learning tasks (Saffran et al., 1996).",
        "These arguments together suggest that distributional learning has a somewhat privileged status."
      ]
    },
    {
      "heading": "3. Lattice grammars",
      "text": [
        "Clark (2009) presents the theory of lattice based formalisms starting algebraically from the theory of residuated lattices.",
        "Here we will largely ignore this, and start from a straightforward computational treatment.",
        "We start by defining the representation.",
        "Definition 1.",
        "Given a non-empty finite alphabet, E, a distributional lattice grammar (DLG) is a 3-tuple consisting of (K, D, F), where F is a finite subset of E* x E*, such that (A, A) G F, K is a finite subset of E* which contains A and E, and D is a subset of(F 0 KK).",
        "K here can be thought of as a finite set of exemplars, which correspond to substrings or fragments of the language.",
        "F is a set of contexts or features, that we will use to define the distributional properties of these exemplars; finally D is a set of grammatical strings, the data; a finite subset of the language.",
        "F 0 KK using the notation above is {luvr\\u, v G K, (l, r) G F}.",
        "This is the finite part of the language that we examine.",
        "If the language we are modeling is L, then D = L n (F 0 KK).",
        "Since A G K, K ç KK.",
        "We define a concept to be an ordered pair (S, C) where S ç K and C ç F, which satisfies the following two conditions: first C 0 S ç D; that is to say every string in S can be combined with any context in C to give a grammatical string, and secondly they are maximal in that neither K nor F can be increased without violating the first condition.",
        "We define B(K, D, F) to be the set of all such concepts.",
        "We use the B symbol (Begriff) to bring out the links to Formal Concept Analysis (Ganter lattice may contain exponentially many concepts, but it is clearly finite, as the number of concepts is less than min(2|FI, 2|K|).",
        "There is an obvious partial order defined by (Si,Ci) < (S2,C2) iff Si ç S2, Note that Si ç S2 iff C2 ç Ci.",
        "Given a set of strings S we can define a set of contexts S' to be the set of contexts that appear with every element of S.",
        "Dually we can define for a set of contexts C the set of strings C' that occur with all of the elements",
        "The concepts ( S, C) are just the pairs that satisfy S' = C and C' = S; the two maps denoted by ' are called the polar maps.",
        "For any S ç K, can form a concept from any set of strings S ç K by taking (S'', S'); this is a concept as S''' = S'.",
        "We will write this as C (S), and for any C ç F, we will write C(C) = (C',C'').",
        "If S ç T then T' ç S', and S'' ç T''.",
        "For any set of strings S ç K, S ç S''.",
        "One crucial concept here is the concept defined by (A, A) or equivalently by the set K n D which corresponds to all of the elements in the language.",
        "We will denote this concept by L = C({(A,A)})= C(K n D).",
        "We also define a meet operation by",
        "This is the greatest lower bound of the two concepts; this is a concept since if Si'' = Si and this operation is associative and commutative.",
        "We can also define a join operation dually; with these operations B(K, D, D) is a complete lattice.",
        "So far we have only used strings in F 0 K; we now define a concatenation operation as follows.",
        "Since Si and S2 are subsets of K, SiS2 is a subset of KK, but not necessarily of K. (SiS2)' is the set of contexts shared by all elements of Si S2 and (SiS2)'' is the subset of K, not KK, that has all of the contexts of (SiS2)'.",
        "(SiS2)''' might be larger than (SiS2)'.",
        "We can also write this as C ((SiS2)').",
        "Both A and o are monotonic in the sense that if X < Y then X o Z < Y o Z, Z o X < Z o Y and X A Z < Y A Z.",
        "Note that all of these operations can be computed efficiently; using a perfect hash, and a naive algorithm, we can do the polar maps and A operations in time O(\\K\\\\F\\), and the concatenation intime O(\\K \\ \\ F \\ ).",
        "We now define the notion of derivation in this representation.",
        "Given a string w we recursively compute a concept for every substring of w; this concept will approximate the distribution of the string.",
        "We define 0g as a function from E* to B(K, D, F); we define it recursively:",
        "The first step is well defined because all of the strings of length at most 1 are already in K so we can look them up directly.",
        "To clarify the second step, if w = abc then (/>G(abc) = (/>G(a) o (/>c(bc) A (/>G(ab) o (/>g(c); we compute the string from all possible non-trivial splits of the string into a prefix and a suffix.",
        "By using a dynamic programming table that stores the values of 0(u) for all u G Sub(w) we can compute this in time O(\\K \\ \\ F \\ \\w\\); this is just an elementary variant of the CKY algorithm.",
        "We define the language defined by the DLG G to be",
        "That is to say, a string is in the language if we predict that a string has the context (A, A).",
        "We now consider a trivial example: the Dyck language.",
        "Example 1.",
        "Let L be the Dyck language (matched parenthesis language) over S = {a,b}, where a corresponds to open bracket, and b to close bracket.",
        "Define G = (K, D, F) is a DLG.• F = {(A, A), (A,b), (a, A)}.",
        "We will now write down the 5 elements of the lattice:",
        "To compute the concatenation A o B we first compute {a}{b} = {ab}; we then compute {ab}' which is {(A, A)}, and {(A, A)}' = {A, ab}, so A o B = L. Similarly to compute L o L, we first take {A, ab}{A, ab} = {A, ab, abab}.",
        "These all have the context (A, A), so the result is the concept L. Ifwe compute A o A we get {a}{a} which is { aa} which has no contexts so the result is T. We have j(G(A) = L,((G(a) = A,(j)G(b) = B.",
        "Applying the recursive computation we can verify that (jG(w) = L iff w G L and so L(G) = L. We can also see that D = L n (F 0 KK)."
      ]
    },
    {
      "heading": "4. Search",
      "text": [
        "In order to learn these grammars we need to find a suitable set of contexts F, a suitable set of strings K, and then work out which elements of F 0 KK are grammatical.",
        "So given a choice for K and F it is easy to learn these models under a suitable regime: the details of how we collect information about D depend on the learning model.",
        "The question is therefore whether it is easy to find suitable sets, K and F. Because of the way the formalism is designed, it transpires that the search problem is entirely tractable.",
        "In order to analyse the search space, we define two maps between the lattices as K and F are increased.",
        "We are going to augment our notation slightly; we will write B(K, L, F) for B(K, L n (F0KK), F) and similarly (K, L, F) for (K, Ln ( F 0 KK) , F) .",
        "When we use the two polar maps (such as C', S'), though we are dealing with more than one lattice, there is no ambiguity as the maps agree; we will when necessary explicitly restrict the output (e.g. C' n J) to avoid confusion.",
        "Definition 2.",
        "For any language L and any set of contexts F ç G, and any sets of strings J ç K ç S*.",
        "We define a map g from B(J, L, F ) to B(K, L, F) (from the smaller lattice to the larger lattice) as g((S,C)) = (C',C).",
        "We also define a map f from B(K,L,G) to B(K,L,F), (from larger to smaller) as f ((S,C)) = ((C n F)',C n F).",
        "These two maps are defined in opposite directions: this is because of the duality of the lattice.",
        "By defining them in this way, as we will see, we can prove that these two maps have very similar properties.",
        "We can verify that the outputs of these maps are in fact concepts.",
        "We now need to define two monotonicity lemmas: these lemmas are crucial to the success of the formalism.",
        "We show that as we increase K the language defined by the formalism decreases monotonically, and that as we increase F the language increases monotonically.",
        "There is some duplication in the proofs of the two lemmas; we could prove them both from more abstract properties of the maps f, g which are what are called residual maps, but we will do it directly.",
        "Lemma 1.",
        "Given two lattices B(K,L,F) and B(K,L,G) where F ç G; For all X,Y G B(K, L, G) we have that Proof.",
        "The proof is elementary but difficult to read.",
        "We write X = (SX, CX) and similarly for Y.",
        "For part 1 of the lemma: Clearly (S'X n F) ç SX, so (SX n F)' D SX = Sx and the same for Sy.",
        "So (SX n f)'(SY n F)' d SxSy (as subsets of KK).",
        "So ((SX n F)'(SY n F )')' ç (SX SY )' ç (Sx Sy )'''.",
        "Now by definition, f (X ) o f (Y ) is C (Z ) where Z = ((SX n F )'(SY n F )')' n F and f (x o Y) has the set of contexts ((SxSy)''' n F).",
        "Therefore f(X o Y) has a bigger set of contexts than f(X) o f(Y) and is thus a smaller concept.",
        "For part 2: by definition f (X A Y) = (((Sx n Sy )' n F )', (Sx n Sy )' n F ) and f (X ) A f (Y ) = ((SX nF )'n(Sy nF )', ((SX nF )' n(Sy nF )')' nF ) Now SX' n F ç SX' , so (since SX'' = SX) SX ç (SXnF)',andso SxnSy ç (SXnF)'n(SynF)'.",
        "So (Sx n Sy)' D ((SX n F)' n (Sy n F)') which gives the result by comparing the context sets of the two sides of the inequality.",
        "□ Lemma 2.",
        "For any language L, and two sets of contexts F ç G, and any K, ifwe have two DLGs (K, L, F) with map (f : S* – B(K, L, F) and (k, L, G) with map jG : S* – B(K, L, G) then for all w, f ((g(w)) < (f(w).",
        "Proof.",
        "By induction on the length of w; clearly if |w| < 1, f(jG(w)) = (jF(w).",
        "We now take the inductive step; by definition, (suppressing the definition of u, v in the meet)",
        "By the inductive hypothesis we have f (jG(u)) < (jF (u) and similarly for v and so by the monotonicity of A and o:",
        "Since the right hand side is equal to (F (w), the proof is done.",
        "□",
        "It is then immediate that",
        "We now prove the corresponding facts about g.",
        "Lemma 4.",
        "ForanyJ ç K andanyconceptsX, Y in B( J, L, F), we have that Proof.",
        "For the first part: Write X = (SX, CX ) as before.",
        "Note that SX = CX' n J .",
        "SX ç CX' , so Sx Sy ç CX CY, and so (Sx Sy )'' ç (CX CY )'', and ((SXS )'' n J)' D (CX' C ' )'''.",
        "By calculation g(X) o g(Y) = ((CXCY)'', (CXCY)''') On the other hand, g(X o Y) = (((SxSy)'' n J)'', ((SxSy)'' n J)') and so g(X o Y) is smaller as it has a larger set of contexts.",
        "We now state and prove the monotonicity lemma for g.",
        "Lemma 5.",
        "For all J ç K ç S* x S*, and for all strings w; we have that g(j>j(w)) < (K(w).",
        "Proof.",
        "By induction on length of w. Both J and K include the basic elements of S and A.",
        "First suppose |w| < 1, then (j(w) = ((CL(w) n F)' n J,CL(w) n F),and g((j(w)) = ((CL(w) n F)', CL(w) n F) which is equal to (K(w).",
        "Now suppose true for all w of length at most k, and take some w of length k + 1.",
        "By definition of",
        "By the inductive hypothesis and monotonicity ofo and A:",
        "Proof.",
        "Suppose w G L((K, L, F)).",
        "this means that jK(w) < LK.",
        "therefore g(jj(w)) < Lk; which means that (A, A) is in the concept g((j(w)), whichmeans itis in the concept(j(w), and therefore w G L((J, L, F)).",
        "□",
        "Given these two lemmas we can make the following observations.",
        "First, if we have a fixed L and F, then as we increase K, the language will decrease until it reaches a limit, which it will attain after a finite limit.",
        "Lemma 7.",
        "For all L, and finite context sets F, there is a finite K such that for all K2, K C K2, L((K,L,F )) = L((K2,L,F )).",
        "Proof.",
        "We can define the lattice B(S*, L, F).",
        "Define the following equivalence relation between pairs of strings, where (ui,vi) ~ (u2,v2) iff C(ui) = C(u2) andC(vi) = C(v2) andC(uivi) = C(u2v2).",
        "The number of equivalence classes is clearly finite.",
        "If K is sufficiently large that there is a pair of strings (u, v) in K for each equivalence class, then clearly the lattice defined by this K will be isomorphic to B(S*, L, F).",
        "Any superset of K will not change this lattice.",
        "□",
        "Moreover this language is unique for each L, F. We will call this the limit language of L, F, and we",
        "L((S*,L,G)).",
        "Finally, we will show that the limit languages never overgeneralise.",
        "L((S*,L,F)) ç L.",
        "Proof.",
        "Recall that C(w) = ({w}'', {w}') is the real concept.",
        "If G is a limit grammar, we can show that we always have jG(w) > C(w), which will give us the result immediately.",
        "First note that C(u) o C(v) > C(uv), which is immediate by the definition of o.",
        "We proceed, again, by induction on the length of w. For | w| < 1, j(G(w) = C(w).",
        "For the inductive step we have (g(w) = Au^ (g(u) o (g(v); by inductive hypothesis we have that this must be more than Au,v C (u) o C (v) > A„,v C (uv) = C(w) □"
      ]
    },
    {
      "heading": "5. Weak generative power",
      "text": [
        "First we make the following observation: if we consider an infinite variant of this, where we set K = S* and F = S* x S* and D = L, we can prove easily that, allowing infinite \"representations\", for any L, L((K, D, F)) = L. In this infinite data limit, o becomes associative, and the structure of B(K, D, F) becomes a residuated lattice, called the syntactic concept lattice of the language L, B(L).",
        "This lattice is finite iff the language is regular.",
        "The fact that this lattice now has residuation operations suggest interesting links to the theory of categorial grammar.",
        "It is the finite case that interests us.",
        "We will use LDLG to refer to the class of languages that are limit languages in the sense defined above.",
        "Our focus in this paper is not on the language theory: we present the following propositions.",
        "First LDLG properly contains the class of regular languages.",
        "Secondly L",
        "dlg contains some non-context-free languages (Clark, 2009).",
        "Thirdly it does not contain all context-free languages.",
        "A natural question to ask is how to convert a CFG into a DLG.",
        "This is in our view the wrong question, as we are not interested in modeling CFGs but modeling natural languages, but given the status of CFGs as a default model for syntactic structure, it will help to give a few examples, and a general mechanism.",
        "Consider a nonterminal N in a CFG with start symbol S. We can define C(N) = {(1,r)|S 4> INr} and the yield Y (N) = {w|N ^ w}.",
        "Clearly C (N) 0 Y(N) ç L, but these are not necessarily maximal, and thus ( C(N), Y(N)) is not necessarily a concept.",
        "Nonetheless in most cases, we can construct a grammar where the non-terminals will correspond to concepts, in this way.",
        "The basic approach is this: for each nonterminal, we identify a finite set of contexts that will pick out only the set of strings generated from that non-terminal: we find some set of contexts FN typically a subset of C (N) such that Y (N ) = {w|V(1,r) G Fn ,1wr G L}.",
        "We say that we can contextually define this non-terminal if there is such a finite set of contexts FN.",
        "If a CFG in Chomsky normal form is such that every non-terminal can be contextually defined then the language defined by that grammar is in Ldlg.",
        "If we can do that, then the rest is trivial.",
        "We take any set of features F that includes all of these FN ; probably just F = IJN FN ; we then pick a set of strings K that is sufficiently large to rule out all incorrect generalisations, and then define D to be L n (F 0 KK).",
        "Consider the language L = {anbncm|n, m > 0} U {ambncn|n, m > 0}.",
        "L is a classic example of an inherently ambiguous and thus non-deterministic language.",
        "The natural CFG in CNF for L has non-terminals that generate the following for the language L. In this example, it is sufficient to have one context per non-terminal.",
        "This is not in general the case.",
        "Consider L = {anbn|n > 0} U {anb2n|n > 0}.",
        "Here we clearly need to identify sets of strings corresponding to the two parts of this language, but it is easy to see that no one context will suffice.",
        "However, note that the first part is defined by the two contexts (A, A), (a, b) and the second by the two contexts (A, A), (a, bb).",
        "Thus it is sufficient to have a set F that includes these four contexts, as well as similar pairs for the other non-terminals in the grammar, and some contexts to define a and b.",
        "We can see that we will not always be able to do this for every CFG.",
        "One fixable problem is if the CFG has two separate non-terminals, M, N such that C (M ) D C (N ).",
        "If this is the case, then we must have that Y(N) D Y(M), If we pick a set of contexts to define Y(N), then clearly any string in Y(M) will also be picked out by the same contexts.",
        "If this is not the case, then we can clearly try to rectify it by adding a rule N - M which will not change the language defined.",
        "However, we cannot always pick out the nonterminals with a finite set of contexts.",
        "Consider the language L = {anb|n > 0} U {ancm|m > n > 0} defined in Clark et al.",
        "(2008).",
        "Suppose wlog that F contains no context (l, r) such that |1| + |r| > k. Then it is clear that we will not be able to pick out b without also picking out cfc+i, since CL(cfc+i) n F D CL(b) n F. Thus L, which is clearly context-free, is not in LDLG.",
        "Luckily, this example is highly artificial and does not correspond to any phenomena we are aware of in linguistics.",
        "In terms of representing natural languages, we clearly will in many cases need more than one context to pick out syntactically relevant groups of strings.",
        "Using a very simplified example from English, if we want to identify say singular noun phrases, a context like (that is, A) will not be sufficient since as well as noun phrases we will also have some adjective phrases.",
        "However if we include multiple contexts such as (A, is over there) and so on, eventually we will be able to pick out exactly the relevant set of strings.",
        "One of the reasons we need to use a context sensitive representation, is so that we can consider every possible combination of contexts simultaneously: this would require an exponentially large context free grammar."
      ]
    },
    {
      "heading": "6. Learning Model",
      "text": [
        "In order to prove correctness of the learning algorithm we will use a variant of Gold-style inductive inference (Gold, 1967).",
        "Our choice of this rather old-fashioned model requires justification.",
        "There are two problems with learning - the information theoretic problems studied under VC-dimension etc., and the computational complexity issues of constructing a hypothesis from the data.",
        "In our view, the latter problems are the key ones.",
        "Accordingly, we focus entirely on the efficiency issue, and allow ourself a slightly unrealistic model; see (Clark and Lappin, 2009) for arguments that this is a plausible model.",
        "We assume that we have a sequence of positive examples, and that we can query examples for membership.",
        "Given a language L a presentation for L is an infinite sequence of strings wi, w2,... such that {wi|i G N} = L. An algorithm receives a sequence T and an oracle, and must produce a hypothesis H at every step, using only a polynomial number of queries to the membership oracle - polynomial in the total size of the presentation.",
        "It identifies in the limit the language L iff for every presentation T of L there is a N such that for all n > N Hn = HN, and L(HN) = L. We say it identifies in the limit a class of languages L iff it identifies in the limit all L in L. We say that it identifies the class in polynomial update time iff there is a polynomial p, such that at each step the model uses an amount of computation (and thus also a number of queries) that is less than p(n, l), where n is the number of strings and l is the maximum length of a string in the observed data.",
        "We note that this is slightly too weak.",
        "It is possible to produce vacuous enumerative algorithms that can learn anything by only processing a logarithmically small prefix of the string (Pitt, 1989)."
      ]
    },
    {
      "heading": "7. Learning Algorithm",
      "text": [
        "We now define a simple learning algorithm, that establishes learnability under this paradigm.",
        "There is one minor technical detail we need to deal with.",
        "We need to be able to tell when adding a string to a lazy DLG will leave the grammar unchanged.",
        "We use a slightly weaker test.",
        "Given Gi = (K, D, F) we define as before the equivalence relation between pairs of strings of K, where (ui, vi) ~gi (u2,v2) iff Cd(ui) = Cd(u2) and Cd (vi) = Cd (v2) and Cd (uivi) = Cd (u2v2).",
        "Note that CD(u) = {(l, r)|1ur G D}.",
        "but F is unchanged, we say that these two are indistinguishable iff the number of equivalence classes of K x K under ~g1 is equal to the number of equivalence classes of K2 x K2 under ~g2.",
        "This can clearly be computed efficiently using a union-find algorithm, in time polynomial in |K | and |F |.",
        "If they are indistinguishable then they define the same language.",
        "Algorithm 1 presents the basic algorithm.",
        "At various points we compute sets of strings like (F 0 KK) n L; these can be computed using the membership oracle.",
        "First we prove that the program is efficient in the sense that it runs in polynomial update time.",
        "Lemma 9.",
        "There is a polynomial p, such that Algorithm 1, for each wn, runs in time bounded by p(n, l) where l is the maximum length of a string in wi,... wn.",
        "Proof.",
        "First we note that K, K2 and F are always subsets of Sub(E) U S and Con(E), and thus both |K| and |F| are bounded by n1(1 + 1)/2 + |S| + 1.",
        "Computing D is efficient as |F 0 KK | is bounded by |K |1F |.",
        "We can compute jG as mentioned above in time |K | |F |1 ; distinguishability is as observed earlier also polynomial.",
        "□",
        "Before we prove the correctness of the algorithm we make some informal points.",
        "First, we are learning under a rather pessimistic model - the positive examples may be chosen to confuse us, so we cannot make any assumptions.",
        "Accordingly we have to very crudely add all substrings and all",
        "Algorithm 1: DLG learning algorithm Data: Input strings S = {wi, w2 ..., },",
        "membership oracle O Result: A sequence of DLGs Gi, G2,...",
        "if there is some w G E that is not in",
        "L(G) then end else if ( K2, D2, F) not indistinguishable",
        "Output G; contexts, rather than using sensible heuristics to select frequent or likely ones.",
        "Intuitively the algorithm works as follows: ifwe observe a string not in our current hypothesis, then we increase the set of contexts which will increase the language defined.",
        "Since we only see positive examples, we will never explicitly find out that our hypothesis overgenerates, accordingly we always add strings to a tester set K2 and see if this gives us a more refined model.",
        "If this seems like it might give a tighter hypothesis, then we increase K.",
        "In what follows we will say that the hypothesis at step n, Gn = (Kn, Dn, Fn), and the language defined is Ln.",
        "We will assume that the target language is some L G LDLG and wi,... is a presentation of L.",
        "Lemma 10.",
        "Then there is a point n, and a finite set ofcontexts F such that for all N > n, FN = F., and L((S*,L,F)) = L.",
        "Proof.",
        "Since L G Ldlg there is some set of contexts G C Con(L), such that L = L((S*, L, G)).",
        "Any superset of G will define the correct limit language.",
        "Let n be the smallest n such that G is a subset of Con({wi,..., wn}).",
        "Consider Fn.",
        "If Fn defines the correct limit language, then we will never change F as the hypothesis will be a superset of the target.",
        "Otherwise it must define a subset of the correct language.",
        "Then either there is some N > n at which it has converged to the limit language which will cause the first condition in the loop to be satisfied and F will be increased to a superset of G, or F will be increased before it converges, and thus the result holds.",
        "□ Lemma 11.",
        "After F converges according to the previous lemma, there is some n, such that for all N > n, Kn = Kn and L((Kn, L, Fn)) = L.",
        "Proof.",
        "let no be the convergence point of F ; for all n > n0 the hypothesis will be a superset of the target language; therefore the only change that can happen is that K will increase.",
        "By definition of the limit language, it must converge after a finite number of examples.",
        "□ Theorem 1.",
        "For every language L G LDLG, and every presentation ofL, Algorithm 1 will converge to a grammar G such that L(G) = L.",
        "This result is immediate by the two preceding lemmas."
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "We have presented an efficient, correct learning algorithm for an interesting class of languages; this is the first such learning result for a class of languages that is potentially large enough to describe natural language.",
        "The results presented here lack a couple of technical details to be completely convincing.",
        "In particular we would like to show that given a representation of size n, we can learn once we have seen a set of examples that is polynomially bounded by n. This will be challenging, as the size of the K we need to converge can be exponentially large in F. We can construct DFAs where the number of congruence classes of the language is an exponential function of the number of states.",
        "In order to learn languages like this, we will need to use a more efficient algorithm that can learn even with \"insufficient\" K : that is to say when the lattice B(K, L, F) has fewer elements that B(KK, L, F).",
        "This algorithm can be implemented directly and functions as expected on synthetic examples, but would need modification to run efficiently on natural languages.",
        "In particular rather than considering whole contexts of the form (l, r) it would be natural to restrict them just to a narrow window of one or two words or tags on each side.",
        "Rather than using a membership oracle, we could probabilistically cluster the data in the table of counts of strings in F 0 K. In practice we will have a limited amount of data to work with and we can control over-fitting in a principled way by controlling the relative size of K and F.",
        "This formalism represents a process of analogy from stored examples, based on distributional learning - this is very plausible in terms of what we know about cognitive processes, and is compatible with much non-Chomskyan theorizing in linguistics (Blevins and Blevins, 2009).",
        "The class of languages is a good fit to the class of natural languages; it contains, as far as we can tell, all standard examples of context free grammars, and includes non-deterministic and inherently ambiguous grammars.",
        "It is hard to say whether the class is in fact large enough to represent natural languages; but then we don't know that about any formalism, context-free or context-sensitive.",
        "All we can say is that there are no phenomena that we are aware of that don't fit.",
        "Only large scale empirical work can answer this question.",
        "Ideologically these models are empiricist - the structure of the representation is based on the structure of the data: this has to be a good thing for computational modeling.",
        "By minimizing the amount of hidden, unobservable structure, we can improve learnability.",
        "Languages are enormously complex, and it would be simplistic to try to reduce their acquisition to a few pages of mathematics; nonetheless, we feel that the representations and grammar induction algorithms presented in this paper could be a significant piece of the puzzle."
      ]
    }
  ]
}
