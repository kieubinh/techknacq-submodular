{
  "info": {
    "authors": [
      "Daniele Pighin",
      "Alessandro Moschitti"
    ],
    "book": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning",
    "id": "acl-W10-2926",
    "title": "On Reverse Feature Engineering of Syntactic Tree Kernels",
    "url": "https://aclweb.org/anthology/W10-2926",
    "year": 2010
  },
  "references": [
    "acl-D09-1012",
    "acl-D09-1143",
    "acl-H05-1018",
    "acl-J05-1004",
    "acl-J08-2003",
    "acl-N06-1037",
    "acl-P02-1034",
    "acl-P03-1004",
    "acl-P03-1054",
    "acl-P04-1054",
    "acl-P05-1024",
    "acl-P06-1115",
    "acl-P08-1082",
    "acl-P08-2029",
    "acl-W03-1012",
    "acl-W04-3222",
    "acl-W04-3233",
    "acl-W06-2902",
    "acl-W09-1106"
  ],
  "sections": [
    {
      "text": [
        "FBK-irst, DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy",
        "DISI, University of Trento",
        "Via di Sommarive, 14 I-38123 Povo (TN) Italy",
        "moschitti@disi.unitn.it",
        "In this paper, we provide a theoretical framework for feature selection in tree kernel spaces based on gradient-vector components of kernel-based machines.",
        "We show that a huge number of features can be discarded without a significant decrease in accuracy.",
        "Our selection algorithm is as accurate as and much more efficient than those proposed in previous work.",
        "Comparative experiments on three interesting and very diverse classification tasks, i.e.",
        "Question Classification, Relation Extraction and Semantic Role Labeling, support our theoretical findings and demonstrate the algorithm performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Kernel functions are very effective at modeling diverse linguistic phenomena by implicitly representing data in high dimensional spaces, e.g. (Cumby and Roth, 2003; Culotta and Sorensen, 2004; Kudo et al., 2005; Moschitti et al., 2008).",
        "However, the implicit nature of the kernel space causes two major drawbacks: (1) high computational costs for learning and classification, and (2) the impossibility to identify the most important features.",
        "A solution to both problems is the application of feature selection techniques.",
        "In particular, the problem of feature selection in Tree Kernel (TK) spaces has already been addressed by previous work in NLP, e.g. (Kudo and Matsumoto, 2003; Suzuki and Isozaki, 2005).",
        "However, these approaches lack a theoretical characterization of the problem that could support and justify the design of more effective algorithms.",
        "In (Pighin and Moschitti, 2009a) and (Pighin and Moschitti, 2009b) (P&M), we presented a heuristic framework for feature selection in kernel spaces that selects features based on the components of the weight vector, w, optimized by Support Vector Machines (SVMs).",
        "This method appears to be very effective, as the model accuracy does not significantly decrease even when a large number of features are filtered out.",
        "Unfortunately, we could not provide theoretical or intuitive motivations to justify our proposed apporach.",
        "In this paper, we present and empirically validate a theory which aims at filling the above-mentioned gaps.",
        "In particular we provide: (i) a proof of the equation for the exact computation of feature weights induced by TK functions (Collins and Duffy, 2002); (ii) a theoretical characterization of feature selection based on ||w||.",
        "We show that if feature selection does not sensibly reduces ||w||, the margin associated with w does not sensibly decrease as well.",
        "Consequently, the theoretical upperbound to the probability error does not sensibly increases; (iii) a proof that the convolu-tive nature of TK allows for filtering out an exponential number of features with a small ||w|| decrease.",
        "The combination of (ii) with (iii) suggests that an extremely aggressive feature selection can be applied.",
        "We describe a greedy algorithm that exploits these results.",
        "Compared to the one proposed in P&M, the new version of the algorithm has only one parameter (instead of 3), it is more efficient and can be more easily connected with the amount of gradient norm that is lost after feature selection.",
        "In the remainder: Section 2 briefly reviews SVMs and TK functions; Section 3 describes the problem of selecting and projecting features from very high onto lower dimensional spaces, and provides the theoretical foundation to our approach; Section 4 presents a selection of related work; Section 5 describes our approach to tree fragment selection; Section 6 details the outcome of our experiments; finally, in Section 7 we draw our conclusions."
      ]
    },
    {
      "heading": "2. Fragment Weights in TK Spaces",
      "text": [
        "The critical step for feature selection in tree kernel spaces is the computation of the weights of features (free fragments) in the kernel machines' gradient.",
        "The basic parameters are the fragment frequencies which are combined with a decay factor used to downscale the weight of large subtrees (Collins and Duffy, 2002).",
        "In this section, after introducing basic kernel concepts, we describe a theorem that establishes the correct weight of features in the STK space.",
        "Typically, a kernel machine is a linear classifier whose decision function can be expressed as:",
        "where x G 99N is a classifying example and w G 99N and b G 99 are the separating hyperplane's gradient and its bias, respectively.",
        "The gradient is a linear combination of I training points xi G 99N multiplied by their labels yi G { – 1, +1} and their weights ai G 99+.",
        "Different optimizers use different strategies to learn the gradient.",
        "For instance, an SVM learns to maximize the distance between positive and negative examples, i.e. the margin 7.",
        "Applying the so-called kernel trick, it is possible to replace the scalar product with a kernel function defined over pairs of objects, which can more efficiently compute it:",
        "where k(oi, o) = 0(oi) • 0(o), with the advantage that we do not need to provide an explicit mapping (/> : O – 99N of our example objects O in a vector space.",
        "In the next section, we show a kernel directly working on syntactic trees.",
        "Tree Kernel (TK) functions are convolution kernels (Haussler, 1999) defined over pairs of trees.",
        "Different TKs are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002; Kashima and Koyanagi, 2002; Moschitti, 2006).",
        "We will focus on the syntactic tree kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to).",
        "As such, it is especially indicated for tasks involving constituency parsed texts.",
        "Tree kernels compute the number of common substructures between two trees T1 and T2without explicitly considering the whole feature (fragment) space.",
        "Let F = {fi, f2,..., f\\r\\} be the set of tree fragments, i.e. the explicit representation for the components of the fragment space, and xi(n) be an indicator function, equal to 1 if the target fi is rooted at node n and equal to 0 otherwise.",
        "A tree kernel function over T1 and T2 is defined as",
        "ni €NTl U2ENt2",
        "where NTl and NT2 are the sets of nodes in T1 and T2, respectively and",
        "The A function counts the number of common subtrees rooted in ni and n2 and weighs them according to their size.",
        "It can be evaluated as 1. if the productions at ni and n2 are different, then A(ni,n2) = 0; 2. if the productions at ni and n2 are the same, and ni and n2 have only leaf children (i.e. they are pre-terminal symbols) then A(ni, n2) = A; 3. if the productions at ni and n2 are the same, and ni and n2 are not pre-terminals then where 1(ni) is the number of children of ni, cJnis the j-th child of node n and A is a decay factor penalizing larger structures.",
        "Eq.",
        "3 shows that A counts the shared fragments rooted in ni and n2 in the form of scalar product, as evaluated by Eq.",
        "2.",
        "However, when A is used in A as in Eq.",
        "4, it changes the weight of the product Xi(ni)xi(n2).",
        "As A multiplies A in each recursion step, we may be induced to assume that the weight of a fragment is Ad, where d is the depth of the fragment.",
        "On the contrary, we show the actual weight by providing the following:",
        "Theorem 1.",
        "Let T and f be a tree and one of its fragments, respectively, induced by STK.",
        "The weight of f accounted by STK is A 2 , where If (n) is the number of children of n in f and s(f) = |{n G T : If(n) > 0}| is the number ofnodes that have active productions in the fragment, i.e. the size ofthe fragment.",
        "In other words, the exponent of A is the number of fragment nodes that have at least one child (i.e. active productions), divided by 2.",
        "Proof.",
        "The thesis can be proven by induction on the depth d of f. The base case is f of depth 1 .",
        "Fragments of depth 1 are matched by step 2 of A(ni, n2) computation, which assigns a value A = Xi(ni)xi (n2) independently of the number of children (where fi = f).",
        "It follows that the weight of f is Xi(ni) = Xi(n2) = Ai/2.",
        "Suppose that the thesis is valid for depth d and let us consider a fragment f of depth d + 1 , rooted in r. Without loss of generality, we can assume that f is in the set of the fragments rooted in ni and n2, as evaluated by Eq.",
        "4.",
        "It follows that the production rules associated with ni and n2 are identical to the production rule in r. Let us consider M = {i G {1,..,l(m)} : l(c*) > 0}, i.e. the set of child indices of r which have at least a child.",
        "Thus, for j G M, c\\ has a production shared by j and .",
        "Conversely, for j G M, there is no match and A(j, ) = 0.",
        "Therefore, the product in Eq.",
        "4 can be rewritten as AnjeM A(cni, ), where the term 1 in (1 + A( cnl , cn2 )) is not considered since it accounts for those cases where there are no common productions in the children, i.e. c]ni = c^Vj G",
        "M.",
        "We can now substitute A(cnl, cn2) with the weight of the subtree tj of f rooted in cr (and extended until its leaves), which is As(tj) by inductive hypothesis (since t has depth lower than d).",
        "Thus, the weight of f is s(f ) = A njeM As(tj) = Ai+£jGMs(tj), where £jeM s(tj) is the number of nodes in f's subtrees rooted in r's children and having at least one child; by adding 1, i.e. the root of f, we obtain s(f).",
        "Finally, As(f) = Xi(ni)Xi(n2), which satisfies our thesis:",
        "In the light of this result, we can use the definition of a TK function to project a tree t onto a linear space by recognizing that t can be represented as a vector xi = [x(i),..., x(N)] whose attributes are the counts of the occurrences for each fragment, weighed with respect to the decay factor A.",
        "For a normalized STK kernel, the value of the j-th attribute of the example xi is therefore:",
        "where: ti;j is the number of occurrences of the fragment fj, associated with the j-th dimension of the feature space, in the tree ti.",
        "It follows that the components of (see Eq.",
        "1) can be rewritten aiyiti,j A",
        "N i t As(fk) fc=i i,fc"
      ]
    },
    {
      "heading": "3. Projecting Exponentially Large Spaces",
      "text": [
        "In order to provide a theoretical background to our feature selection technique and to develop effective algorithms, we want to relate our approach to statistical learning and, in particular, support vector classification theory.",
        "Since we select features with respect to their weight w(j), we can use the following theorem that establishes a general bound for margin-based classifiers.",
        "Theorem 2.",
        "(Bartlett and Shawe-Taylor, 1998) Let C = {x – w • x : ||w|| < 1, ||Xy < R} be the class of real-valued functions defined in a ball of radius R in 99N.",
        "Then there is a constant k such that Vc G C having a margin 7, i.e. |w • x| > 7, Vx G X (training set), the error of c a probability 1 – 6, where I = |X| and b is the number ofexamples with margin less than 7.",
        "In other words, if X is separated with a margin 7 by a linear classifier, then the error has a bound depending on 7.",
        "Another conclusion is that a feature selection algorithm that wants to preserve the accuracy of the original space should not affect the margin.",
        "Since we would like to exploit the availability of the initial gradient w derived by the application of SVMs, it makes sense to try to quantify the percentage of 7 reduction after feature selection, which we indicate by p. We found out that 7 is linked to the reduction of ||w||, as illustrated by the next lemma.",
        "Lemma 1.",
        "Let X be a set ofpoints in a vector space and w be the gradient vector which separates them with a margin 7.",
        "Ifthe selection decreases ||w || of a p rate, then the resulting hyperplane separates X by a margin larger than 7in = 7 – pR||w ||.",
        "Proof.",
        "Let w = win+wout, where win and wout G 99N are constituted by the components of w that are selected in and out, respectively, and have zero values in the remaining positions.",
        "By hypothesis |w • x| > 7; without loss of generality, we can consider just the case w • x > 7, and write w • Remark 1.",
        "The lemma suggests that, even in case ofvery aggressive feature selection, ifa small percentage p of ||w|| is lost, the margin reduction is small.",
        "Consequently, through Theorem 2, we can conclude that the accuracy ofthe model is by and large preserved.",
        "Remark 2.",
        "We prefer to show the lemma in the more general form, but if we use normalized x and classifiers with ||w|| < 1, then 7in = 7 – ||w||p > – p.",
        "The last result that we present justifies our selection approach as it demonstrates that most of the gradient norm is concentrated in relatively few features, with respect to the huge space induced by tree kernels.",
        "The selection of these few features allows us to preserve most of the norm and the margin.",
        "Lemma 2.",
        "Let w be a linear separator of a set of points X, where each xi G X is an explicit vector representations ofa tree ti in the space induced by STK and let v be the largest s(ti), i.e. the maximum tree size.",
        "Then, if we discard fragments of size greater than r?, ||wout| Proof.",
        "By ties, ||wwout| applying simple norm proper||aiyixouti || = Ee=i ai pout; ||.",
        "To evaluate the latter, we first reorganize the summation in Eq.",
        "5 (with no normalization) such that ||xiy = £fc=i£j:s(fj)=k t2,jAS(fj).",
        "Since a fragment fj can be at maximum rooted in v nodes, then ti;j < v. Therefore, by replacing the number of trees of size k with the upperbound v,, we have x.",
        "\\Jv Y-ß, where we applied geometric series summation.",
        "Now if we assume that our algorithm selects out (i.e. discards) fragments with size",
        "In case of hardRemark 3.",
        "The lemma shows that for an enough large r?",
        "and A < 1/v, ||wout|| can be very small, even though it includes an exponential number of features, i.e. all the subtrees whose size ranges from r to v. Therefore, according to Lemma 1 and Theorem 2, we can discard an exponential number offeatures with a limited loss in accuracy.",
        "Remark 4.",
        "Regarding the proposed norm bound, we observe that v, is a rough overestimation ofthe the real number offragments having size k rooted in the nodes ofthe target tree t. This suggests that we don't really need A < 1/v.",
        "Moreover, in case of soft-margin SVMs, we can bound ai with the value of the trade-off parameter C."
      ]
    },
    {
      "heading": "4. Previous Work",
      "text": [
        "Initial work on feature selection for text, e.g. (Yang and Pedersen, 1997), has shown that it may improve the accuracy or, at least, improve efficiency while preserving accuracy.",
        "Our context for feature selection is different for several important reasons: (i) we focus on structured features with a syntactic nature, which show different behaviour from lexical ones, e.g. they tend to be more sparse; (ii) in the TK space, the a-priori weights are very skewed, and large fragments receive exponentially lower scores than small ones; (iii) there is high redundancy and interdependency between such features; (iv) we want to be able to observe the most relevant features automatically generated by TKs; and (v) the huge number of features makes it impossible to evaluate the weight of each feature individually.",
        "Guyon and Elisseeff (2003) carries out a very informative survey of feature selection techniques.",
        "Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and",
        "=i aiyixout; < £f=i",
        "Gaussian kernels, e.g. (Weston et al., 2001; Neumann et al., 2005).",
        "In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space.",
        "The authors discuss an approximation of their method that allows them to handle high degree polynomial kernels.",
        "Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on %-driven relevance assessment.",
        "With respect to their work, the main differences in the approach that we propose are that we want to exploit the SVM optimizer to select the most relevant features, and to be able to observe the relevant fragments."
      ]
    },
    {
      "heading": "5. Mining Fragments Efficiently",
      "text": [
        "The high-level description of our feature selection technique is as follows: we start by learning an STK model and we greedily explore the support vectors in search for the most relevant fragments.",
        "We store them in an index, and then we decode (or linearize) all the trees in the dataset, i.e. we represent them as vectors in a linear space where only a very small subset of the fragments in the original space are accounted for.",
        "These vectors are then employed for learning and classification in the linear space.",
        "To explore the fragment space defined by a set of support vectors, we adopt the greedy strategy described in Algorithm 5.1.",
        "Its arguments are a model M, and the threshold factor L. The greedy algorithm explores the fragment space in a small to large fashion.",
        "The first step is the generation of the all base fragments F encoded in each tree, i.e. the smallest possible fragments according to the definition of the kernel function.",
        "For STK, such fragments are all those consisting of a node and all its direct children (i.e. production rules of the grammar).",
        "We assess the cumulative relevance of each",
        "Algorithm 5.1: greedyjmodeljminer(M, L)",
        "base fragment according to Eq.",
        "6 and then use the relevance B of the heaviest fragment, i.e. the fragment with the highest relevance in absolute value, as a criterion to set our fragment mining threshold a to B/L.",
        "We then apply the filter( ) operator which discards all the fragments whose cumulative score is less than a.",
        "Then, the update( ) operator stores the ramaining fragments in the index.",
        "The exploration of the kernel space is carried out via the process of fragment expansion, by which each fragment retained at the previous step is incrementally grown to span more levels of the tree and to include more nodes at each level.",
        "These two directions of growth are controlled by the outer and the inner while loops, respectively.",
        "Fragment expansion is realized by the expand(f, n) operator, that grows the fragment f by including the children of n expandable nodes in the fragment.",
        "Expandable nodes are nodes which are leaves in f but that have children in the tree that originated f.",
        "After each expansion, the filter( ) operator is invoked on the set of generated fragments.",
        "If the filtered set is empty, i.e. no fragments more relevant than a have been found during the previous iteration, then the loop is terminated.",
        "unlike previous attempts, this algorithm relies onjust one parameter, i.e. L. As it revolves around the weight of the most relevant fragment, it operates according to the norm-preservation principle described in the previous sections.",
        "In fact, if we call N the number of fragments mined for a given value of L, the norm after feature selection can be",
        "B – BASE_FRAGS (model) B – rel(best(b)) a – B/L",
        "Dprev – FILTER (B, a) UPDATE (Dpr Wp while WD",
        "widthf actor * / - T> for each f e Wprev",
        "f Wnext – Wnext U {f}",
        "update(F ) return (result)",
        "f Ef – expand (f, t ) F – FILTER(Ef ,a)",
        "then < Dnext – F",
        "bounded by f vN < < B^/N .",
        "The choice of B, i.e. the highest relevance of a base fragment, as an upper bound for fragment relevance is motivated as follows.",
        "In Eq.",
        "6, we can identify a term T = a-^/ || that is the same for all the fragments in the tree ij.",
        "For 0 < A < 1, if fj is an expansion of fk, then from our definition of fragment expansion it follows that A 2 <",
        "A 2 .It can also be observed that < .",
        "Indeed, if ti k is a subset of , then it will occur at least as many times as its expansion ti k, possibly occurring as a seed fragment for different expansions in other parts of the tree as well.",
        "Therefore, if E/ is the set of expansions of f, for every two fragments fi;j, fi k coming from the same tree i^, we can conclude that xi < xi Vfjj eE/ik .",
        "In other words, for each tree in the model, base fragments are the most relevant, and we can assume that the relevance of the heaviest fragment is an upper bound for the relevance of any fragment ."
      ]
    },
    {
      "heading": "6. Experiments",
      "text": [
        "We ran a set of thorough experiments to support our claims with empirical evidence.",
        "We show our results on three very different benchmarks: Question Classification (QC) using TREC 10 data (Voorhees, 2001), Relation Extraction (RE) based on the newswire and broadcast news domain of the ACE 2004 English corpus (Dod-dington et al., 2004) and Semantic Role Labeling (SRL) on the CoNLL 2005 shared task data (Carreras and Marquez, 2005).",
        "In the next sections we elaborate on the setup and outcome of each set of experiments.",
        "As a supervised learning framework we used SVM-Light-TK, which extends the SVM-Light optimizer (Joachims, 2000) with support for tree kernel functions.",
        "Unless differently stated, all the classifiers are parametrized for optimal Precision and Recall on a development set, obtained by selecting one example in ten from the training set with the same positive-to-negative example ratio.",
        "The results that we show are obtained on the test sets by using all the available data for training.",
        "For multi-class scenarios, the classifiers are arranged in a one vs.",
        "all configuration, where each sentence is a positive example for one of the classes, and negative for the others.",
        "While binary classifiers are evaluated in terms of Fi measure, for multi-class classifiers we show the final accuracy.",
        "The next paragraphs describe the datasets used for the experiments.",
        "Question Classification (QC) Given a question, the task consists in selecting the most appropriate expected answer type from a given set of possibilities.",
        "We adopted the question taxonomy known as coarse grained, which has been described in (Zhang and Lee, 2003) and (Li and Roth, 2006), consisting of six non overlapping classes: Abbreviations (ABBR), Descriptions (DESC, e.g. definitions or explanations), Entity (ENTY, e.g. animal, body or color), Human (HUM, e.g. group or individual), Location (LoC, e.g. cities or countries) and Numeric (NUM, e.g. amounts or dates).",
        "questions.",
        "For each question, we generate the full parse of the sentence and use it to train our models.",
        "Automatic parses are obtained with the Stanford parser (Klein and Manning, 2003), and we actually have only 5,953 sentences in our data set due to parsing issues.",
        "During preliminary experiments, we observed an uneven distribution of examples in the traditional training/test split (the same used in P&M).",
        "Therefore, we used a random selection to generate an unbiased split, with 5,468 sentences for training and 485 for testing.",
        "The resulting data set is available for download at http://danielepighin.net/cms/research/ QC_dataset.tgz.",
        "Relation Extraction (RE) The corpus consists of 348 documents, and contains seven relation classes defined over pairs of mentions: Physical, Person/Social, Employment/Membership/Subsidiary, Agent-Artifact, PER/ORG Affiliation, GPE Affiliation, and Discourse.",
        "There are 4,400 positive and 38,696 negative examples when the potential relations are generated using all the entity/mention pairs in the same sentence.",
        "Documents are parsed using the Stanford Parser, where the nodes of the entities are enriched with information about the entity type.",
        "Overall, we used the setting and data defined in (Nguyen et al., 2009b).",
        "Semantic Role Labeling (SRL) SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role.",
        "For these experiments we concentrated on this latter task and used exactly the same setup as P&M. We considered all the argument nodes of any of the six PropBank (Palmer et al., 2005) core roles (i.e. A0, ..., A5) from all the available training sections, i.e. 2 through 21, for a total of 179,091 training instances.",
        "Similarly, we collected 9,277 test instances from the annotations of Section 23.",
        "To show the validity of Lemma 1 in practical scenarios, we compare the accuracy of our linearized models against vanilla STK classifiers.",
        "We designed two types of classifiers:",
        "LIN, a linearized STK model, which uses the weights estimated by the learner in the STK space and linearized examples; in other words LIN uses win .",
        "It allows us to measure exactly the loss in accuracy with respect to the reduction of || w ||.",
        "OPT, a linearized STK model that is re-optimized in the linear space, i.e. for which we retrained an SVM using the linearized training examples as input data.",
        "Since the LIN solution is part of the candidate solutions from which OPT is selected, we always expect higher accuracy from it.",
        "Additionally, we compare selection based on gradient w (as detailed in Section 2.4) against to X selection, which evaluates the relevance of features, in a similar way to (Suzuki and Isozaki, 2005).",
        "The relevance of a fragment is calculated as",
        "2 = N(yN - Mx)X = x(N - x)M(N - M) ' where N is the number of support vectors, M is the number of positive vectors (i.e. > 0), and x and y are the fractions of N and M where the fragment is instantiated, respectively.",
        "We specify the selection models by means of Grad for the former and Chi for the latter.",
        "For example, a model called OPT/Grad is a retrained model using the features selected according the highest gradient weights, while LIN/Chi would be a linearized tree kernel model using % for feature selection.",
        "Number of fragments (log)",
        "Figure 1: Percentage of gradient Norm, i.e. 1 – p, according to the number of selected fragments, for different QC classifiers.",
        "Linearized",
        "the LIN/Grad and OPT/Grad models on the QC task.",
        "Each class is identified by its initial (e.g. A=ABBR).",
        "For each class, we considered a value of the threshold factor parameter L so as to retain at least 60% of the gradient norm after feature selection.",
        "The plots in Figure 1 show, for each class, the percentage of the gradient norm (i.e. 1 - p, see Section 3) retained when including a different number of fragments.",
        "This graph empirically validates Lemma 2 since it clearly demonstrates that after 1,000-10,000 features the percentage of the norm reaches a plateau (around 60-65%).",
        "This means that after such threshold, which interestingly generalizes across all classifiers, a huge number of features is needed for a small increase of the norm.",
        "We recall that the maximum reachable norm is around 70% since we apriori filter out fragments of frequency lower than three.",
        "Table 1 shows the Fi of the binary question classifiers learned with STK, LIN/Grad and OPT/Grad models.",
        "It also shows the norm of the gradient before, ||w||, and after, ||wfeature selec-",
        "ha",
        ".....Hüll nfM",
        "<^m/*",
        "jsf",
        "*y*",
        "■V /",
        "ABBR -",
        "-< – ",
        "■",
        "DESC -ENTY ■■",
        "-*......",
        ".....B – ",
        "//",
        "HUM t r\\r^ -",
        ":",
        "f.....x'",
        "NUM <>",
        "Fi",
        "\\\\w \\\\",
        "Frags",
        "Fi",
        "in\\\\",
        "Fi",
        "A",
        "80.00",
        "11.77",
        "566",
        "66.67",
        "7.13",
        "90.91",
        "D",
        "86.26",
        "41.33",
        "5161",
        "81.87",
        "25.10",
        "83.72",
        "E",
        "76.86",
        "51.71",
        "5,702",
        "73.03",
        "31.06",
        "75.56",
        "H",
        "84.92",
        "43.61",
        "5,232",
        "80.47",
        "26.20",
        "77.08",
        "L",
        "81.69",
        "38.73",
        "1,732",
        "78.87",
        "24.27",
        "82.89",
        "N",
        "92.31",
        "37.65",
        "1,015",
        "85.07",
        "24.53",
        "87.07",
        "Number of fragments (log)",
        "tion along with the number of selected fragments, Frags.",
        "Instead of selecting an optimal number of fragments on a validation set, we investigated the 60% value suggested by the previous plot.",
        "Thus, for each category we selected the feature set reaching approximately 60% of \\\\w\\\\.",
        "The table shows that the accuracy of the OPT/Grad model is in line with STK.",
        "In some cases, e.g. ABBR, the projected model is more accurate, i.e. 90.91 vs. 80.00, whereas in others, e.g. HUM, STK performs better, i.e. 84.92 vs. 77.08.",
        "It is interesting to see how the empirical results clearly complement the theoretical findings of the previous sections.",
        "For example, the LOC classifier uses only 1,732 of the ~ 10 features encoded by the corresponding STK model, but since only 40% of the norm of w is lost, classification accuracy is affected only marginally.",
        "As mentioned above, the selected number of features is not optimal for every class.",
        "Figure 2 plots the accuracy of the LIN/Grad and OPT/Grad for different numbers of fragments on two classes .",
        "These show that the former, with more than 60% of the norm, approaches STK whereas the latter requires less fragments.",
        "The plots also show the comparison against the same fragment mining algorithm and learning framework when using %-based selection.",
        "This also provides similar good results, as far as the reduction of \\\\w\\\\ is kept under control, i.e. as far as we select the components of the gradient that mostly affect its norm.",
        "To concretely assess the benefits of our models for QC, Figure 3 plots the accuracy of OPT/Grad and OPT/Chi on the multiclass QC problem wrt the number of fragments employed.",
        "The results for the multi-class classifier are less biased by the binary Precision/Recall classifiers thus they are more stable and clearly show how, after selecting the optimal number of fragments (1,000-10,000 i.e. 60-65% of the norm), the accuracy of the OPT and CHI classifiers stabilize around levels of accuracy which are in line with STK.",
        "Finally, Table 2 shows the best results that we achieved on the three multi-class classification tasks, i.e. QC, RE and SRL, and compares them against the STK .",
        "For all the tasks OPT/Grad produces the best results for all the tests, even though the difference with OPT/Chi is generally not statistically significant.",
        "Out of three tasks, OPT/Grad manages to slightly improve two of them, i.e. QC (84.12 vs. 83.7) and SRL (88.17 vs. 87.56), while STK is more accurate on RE, i.e.",
        "\\_",
        "m''",
        "lit",
        "/",
        "/y",
        "/",
        "Br",
        "Jk--7",
        "LI",
        "N/Gra",
        "d -1",
        "/",
        "Ol I",
        "T/Gra JN/C1 PT/C1 ST",
        "d – * – ii - – – ",
        "4",
        "--",
        "O",
        "ii b – K -",
        "..........................",
        "J",
        "OPT",
        "/Grad -1-",
        "x............................",
        "OPT/Chi",
        "STK -",
        "B",
        "/",
        "/",
        "b------------e",
        "m.",
        "/",
        "/",
        "*",
        "/",
        "/",
        "•",
        "-...................,/",
        "/",
        "/",
        "!",
        "y'",
        "*",
        "LIN/C",
        "ürad -1-",
        " – ",
        "................./",
        "OPT/Grad – * – LIN/Chi ---*--",
        "s",
        "y",
        "/",
        "OPT/Chi – -b – STK -",
        "STK",
        "OPT/Grad",
        "Fi",
        "F1 Frags",
        "QC",
        "83.70",
        "84.12 ~2k",
        "RE",
        "67.53",
        "66.31 ~10k",
        "SRL",
        "87.56",
        "88.17 ~300k",
        "67.53 vs. 66.31.",
        "The results on SRL can be compared against those that we presented in (Pighin and Moschitti, 2009a), where we measured an accuracy of 87.13 exactly on the same benchmark.",
        "As we can see in Table 2, our model improves the classification accuracy of about 1 point, i.e. 88.17.",
        "On the other hand, such comparison is not really fair since the algorithms rely on different parameter sets, and it is almost impossible to find matching configurations for the different versions of the algorithms that would result in exactly the same number of fragments.",
        "In a projected space with approximately 10 or 10 fragments, including a few hundred more features can produce noticeably different accuracy readings.",
        "Generally speaking, the current model can achieve comparable accuracy with P&M while considering a smaller number of fragments.",
        "For example, in (Pighin and Moschitti, 2009b) the best model for the A1 binary classifier of the SRL benchmark was obtained by including 50,000 fragments, achieving an F1 score of 89.04.",
        "With the new algorithm, using approximately half the fragments the accuracy of the linearized A1 classifier is 90.09.",
        "In P&M, the algorithm would only consider expansions of a fragment f where at most m nodes are expanded.",
        "Consequently, the set of mined fragments may include some small structures which can be less relevant than larger ones.",
        "Conversely, the new algorithm (see Alg.",
        "5.1) may include larger but more relevant structures, thus accounting for a larger fraction of the gradient norm with a smaller number of fragments.",
        "Concerning efficiency, the complexity of both mining algorithms is proportional to the number of fragments that they generate.",
        "Therefore, we can conclude that the new implementation is more efficient by considering that we can achieve the same accuracy with less fragments.",
        "As for the complex-",
        "optimized, as we evaluated them by using the same threshold factor parameter L for all the classes.",
        "Better performances could be achieved by selecting an optimal value of L for individual classes when building the multi-class classifier.",
        "ity of decoding, i.e. providing explicit vector representations of the input trees, in P&M, we used a very naive approach, i.e. the generation of all the fragments encoded in the tree and then look up each fragment in the index.",
        "This solution has exponential complexity with the number of nodes in the tree.",
        "Conversely, the new implementation has approximately linear complexity.",
        "The approach is based on the idea of an FST-like index, that we can query with a tree node.",
        "Every time the tree matches one of the fragments, the index increases the count of that fragment for the tree.",
        "The reduction in time complexity is made possible by encoding in the index the sequence of expansion operations that produced each indexed fragment, and by considering only those expansions at decoding time."
      ]
    },
    {
      "heading": "7. Conclusions",
      "text": [
        "Available feature selection frameworks for very high dimensional kernel families, such as tree kernels, suffer from the lack of a theory that could justify the very aggressive selection strategies necessary to cope with the exceptionally high dimensional feature space.",
        "In this paper, we have provided a theoretical foundation in the context of margin classifiers by (i) linking the reduction of the gradient norm to the theoretical error bound and (ii) by proving that the norm is mostly concentrated in a relatively small number of features.",
        "The two properties suggest that we can apply an extremely aggressive feature selection by keeping the same accuracy.",
        "We described a very efficient algorithm to carry out such strategy in the fragment space.",
        "Our experiments empirically support our theoretical findings on three very different NLP tasks."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank Truc-Vien T. Nguyen for providing us with the SVM learning and test files of the Relation Extraction dataset.",
        "Many thanks to the anonymous reviewers for their valuable suggestions.",
        "This research has been partially supported by the EC project, EternalS: \"Trustworthy Eternal Systems via Evolving Software, Data and Knowledge\", project number FP7 247758."
      ]
    }
  ]
}
