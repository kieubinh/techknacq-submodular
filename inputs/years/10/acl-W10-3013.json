{
  "info": {
    "authors": [
      "Shaodian Zhang",
      "Hai Zhao",
      "Guodong Zhou",
      "Bao-Liang Lu"
    ],
    "book": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning",
    "id": "acl-W10-3013",
    "title": "Hedge Detection and Scope Finding by Sequence Labeling with Procedural Feature Selection",
    "url": "https://aclweb.org/anthology/W10-3013",
    "year": 2010
  },
  "references": [
    "acl-H05-1066",
    "acl-P09-2044",
    "acl-W04-3103",
    "acl-W06-1617",
    "acl-W08-0607"
  ],
  "sections": [
    {
      "text": [
        "Hedge Detection and Scope Finding by Sequence Labeling with Normalized Feature Selection*",
        "Shaodian Zhang, Hai Zhao]\" GuodongZhou and Bao-Liang Lu",
        "Center for Brain-Like Computing and Machine Intelligence Dept of Computer Science and Engineering, Shanghai Jiao Tong University MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems",
        "Shanghai Jiao Tong University School of Computer Science and Technology, Soochow University zhangsd.sjtu@gmail.com, zhaohai@cs.sjtu.edu.cn",
        "gdzhou@suda.edu.cn, Abstract",
        "This paper presents a system which adopts a standard sequence labeling technique for hedge detection and scope finding.",
        "For the first task, hedge detection, we formulate it as a hedge labeling problem, while for the second task, we use a two-step labeling strategy, one for hedge cue labeling and the other for scope finding.",
        "In particular, various kinds of syntactic features are systemically exploited and effectively integrated using a large-scale normalized feature selection method.",
        "Evaluation on the CoNLL-2010 shared task shows that our system achieves stable and competitive results for all the closed tasks.",
        "Furthermore, post-deadline experiments show that the performance can be much further improved using a sufficient feature selection."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Hedges are linguistic devices representing speculative parts of articles.",
        "Previous works such as (Hyland, 1996; Marco and Mercer, 2004; Light et al., 2004; Thompson et al., 2008) present research on hedge mainly as a linguistic phenomenon.",
        "Meanwhile, detecting hedges and their scopes automatically are increasingly important tasks in natural language processing and information extraction, especially in biomedical community.",
        "The shared task of CoNLL-2010 described in Farkas et al.",
        "(2010) aims at detecting hedges (task 1) and finding their scopes (task 2) for the literature",
        "* This work is partially supported by the National Natural Science Foundation of China (Grants 60903119, 60773090, 90820018 and 90920004), the National Basic Research Program of China (Grant No.",
        "2009CB320901), and the National High-Tech Research Program of China (Grant No.2008AA02Z315).",
        "t corresponding author",
        "from BioScope corpus (Szarvas et al., 2008) and Wikipedia.",
        "This paper describes a system adopting sequence labeling which performs competitive in the official evaluation, as well as further test.",
        "In addition, a large-scale feature selection procedure is applied in training and development.",
        "Considering that BioScope corpus is annotated by two independent linguists according to a formal guideline (Szarvas, 2008), while Wikipedia weasels are tagged by netizens who are diverse in background and various in evaluation criterion, it is needed to handle them separately.",
        "Our system selects features for Wikipedia and BioScope corpus independently and evaluate them respectively, leading to fine performances for all of them.",
        "The rest of the paper is organized as follows.",
        "The next section presents the technical details of our system of hedge detection and scope finding.",
        "Section 3 gives information of features.",
        "Section 4 shows the evaluation results, including official results and further ones after official outputs collection.",
        "Section 5 concludes the paper."
      ]
    },
    {
      "heading": "2. Methods",
      "text": [
        "Basically, the tasks are formulated as sequence labeling in our approach.",
        "The available label set differs between task 1 and 2.",
        "In addition, it is needed to introduce an indicator in order to find scopes for the multi-hedge sentences properly.",
        "The valid label set of task 1, hedge detection, contains only two labels: \"Hedge\" and which represent that a word is in a hedge cue or not respectively.",
        "Since results of hedge detection in this shared task are evaluated at sentence level, a sentence will be classified as \"uncertain\" in the post-process if it has one or more words labeled \"Hedge\" in it and otherwise \"certain\".",
        "The second task is divided into two steps in our system.",
        "The first step is quite the same as what the system does in task 1: labeling the words as in hedge cues or not.",
        "Then the scope of each hedge will be labeled by taking advantage of the result of the first step.",
        "A scope can be denoted by a beginning word and an ending word to represent the first and the last element.",
        "In scope finding the available label set contains \"Begin\", \"End\", \"Middle\" and representing the first and last word in the scope, in-scope and out-of-scope.",
        "As an example, a sentence with hedge cue and scope labeling is given in Table 1.",
        "Hedge cue \"indicating\" with its scope from \"indicating\" itself to \"transcription\" are labeled.",
        "While evaluating outputs, only \"Be-gin\"s and \"End\"s will be taken into consideration and be treated as the head and tail tokens of the scopes of specific hedge cues.",
        "It seems that the best labeling result of task 1 can be used directly to be the proper intermediate representation of task 2.",
        "However, the complexity of scope finding for multi-hedge sentences forces us to modify the intermediate result of task 2 for the sake of handling the sentences with more than one hedge cue correctly.",
        "Besides, since task 1 is a sentence classification task essentially, while the goal of the first step of task 2 is to label the words as accurately as possible, it is easy to find that the optimal labeling results of task 1 may not be optimal to be the intermediate representations for task 2.",
        "This problem can be solved if sentence-level hedge detection and intermediate representation finding are treated as two separate tasks with independent feature selection procedures.",
        "The details of feature selection will be given in section 3.",
        "Sentences with more than one hedge cue are quite common in both datasets of BioScope corpus and Wikipedia.",
        "By counting hedges in every sentence, we find that about one fourth of the sentences with hedges have more than one hedge cue in all three data sources (Table 2).",
        "In Morante and Daele-mans (2009), three classifiers predict whether each token is Begin, End or None and a postprocessing is needed to associate Begins and Ends with their corresponding hedge cues.",
        "In our approach, in order to decrease ambiguous or illegal outputs e.g. inequivalent numbers of Begins and Ends, a pair of Begin and End without their corresponding hedge cue between them, etc., sentences with more than one hedge cue will be preprocessed by making copies as many as the number of hedges and be handled separately.",
        "The sentence which is selected as a sample has two hedge cues: \"suggesting\" and \"may\", so our system preprocesses the sentence into two single-hedge ones, which is illustrated in Table 3.",
        "Now it comes to the problem of finding scope for single-hedge sentence.",
        "The two copies are labeled separately, getting one scope from \"suggesting\" to \"mitogenesis\" for the hedge cue \"suggesting\" and the other from \"IFN-alpha\" to \"mitogenesis\" for \"may\".",
        "Merging the two results will give the final scope resolution of the sentence.",
        "However, compared with matching Begins and Ends in postprocessing given by Morante and Daelemans (2009), the above method gives rise to out of control of projections of the scopes, i.e. scopes of hedges may partially overlap after copies are merged.",
        "Since scopes should be intact constituents of sentences, namely, subtrees in syntax tree which never partly overlap with each other, results like this are linguistically illegal and should be discarded.",
        "We solve this problem by introducing an instructional feature called \"Indicator\".",
        "For sentences with more than one hedge cue, namely more than one copy while finding scopes, words inside the union of existing (labeled) scopes will be tagged as \"Indicator\" in unhandled copies before every labeling.",
        "For example, after finding scope for the first copy in Table 3 and words from \"suggesting\" to \"mitogenesis\" are put in the scope of cue \"suggesting\", these words should be tagged \"Indicator\" in the second copy, whose result is illustrated in Table 4.",
        "If not in a scope, any word is tagged \"_\" as the indicator.",
        "The \"Indicator\"s tagging from \"suggesting\" to \"mitogenesis\" in Table 4 mean that no other than the situations of a) \"Begin\" is after or at \"suggesting\" and \"End\" is before or at \"mitogenesis\" b) Both \"Begin\" and \"End\" are before \"suggesting\" c) Both next \"Begin\" and next \"End\" are after \"mitogenesis\" can be accepted.",
        "In other words, new labeling should keep the projections of scopes in the result.",
        "Although it is only an instructional indicator and does not have any coerciveness, the evaluation result of experiment shows it effective.",
        "Furthermore .",
        "- - -",
        "inhibition .",
        "can .",
        "be .",
        "blocked .",
        "by .",
        "actinomycin .",
        "D .",
        "„ _ _",
        "indicating .",
        ".",
        "Hedge Begin",
        "a.",
        ".. _ Middle",
        "requirement .",
        ".. _ Middle",
        "for .",
        ".. _ Middle",
        "de .",
        ".. _ Middle",
        "novo .",
        ".. _ Middle",
        "transcription .",
        ".. _ End"
      ]
    },
    {
      "heading": "3. Feature selection",
      "text": [
        "Since hedge and scope finding are quite novel tasks and it is not easy to determine the effective features by experience, a greedy feature selection is conducted.",
        "As it mentioned in section 2, our system divides scope finding into two sub-tasks:",
        "a) Hedge cue labeling b) Scope labeling",
        "The first one is the same as hedge detection task in strategy, but quite distinct in target of feature set, because hedge detection is a task of sentence classification while the first step of scope finding aims at high accuracy of labeling hedge cues.",
        "Therefore, three independent procedures of feature selection are conducted for BioScope corpus dataset.",
        "As Wikipedia is not involved in the task of scope finding, it only needs one final feature set.",
        "About 200 feature templates are initially considered for each task.",
        "We mainly borrow ideas and are enlightened by following sources while initializing feature template sets:",
        "Dataset",
        "# Sentence",
        "# No-hedge",
        "ratio",
        "# One-hedge",
        "ratio",
        "# Multi-hedge",
        "ratio",
        "Biomedical Abstracts",
        "11871",
        "9770",
        "82.3%",
        "1603",
        "13.5%",
        "498",
        "4.2%",
        "Biomedical Fulltexts",
        "2670",
        "2151",
        "80.6%",
        "385",
        "14.4%",
        "134",
        "5.0%",
        "Wikipedia",
        "11111",
        "8627",
        "77.6%",
        "1936",
        "17.4%",
        "548",
        "4.9%",
        "IFN-alpha _",
        "IFN-alpha _",
        "IFN-alpha .",
        "_ _",
        "also _",
        "also _",
        "also .",
        "sensitized _",
        "sensitized _",
        "sensitized .",
        "T _",
        "T _",
        "T.",
        "cells _",
        "cells _",
        "cells .",
        "_ _",
        "to _",
        "to _",
        "to .",
        "IL-2-induced _",
        "IL-2-induced _",
        "IL-2-induced .",
        "proliferation _",
        "proliferation _",
        "proliferation .",
        ".",
        "-",
        "- -",
        "further _",
        "further _",
        "further .",
        "suggesting Hedge",
        "suggesting _",
        "suggesting .",
        ".. Indicator",
        "_ _",
        "that _",
        "that _",
        "that .",
        ".. Indicator",
        "IFN-alpha _",
        "IFN-alpha _",
        "IFN-alpha .",
        ".. Indicator",
        "_ Begin",
        "may _",
        "may Hedge",
        "may .",
        ".. Indicator",
        "Hedge Middle",
        "be _",
        "be _",
        "be .",
        ".. Indicator",
        "_ Middle",
        "involved _",
        "involved _",
        "involved .",
        ".. Indicator",
        "_ Middle",
        "in _",
        "in _",
        "in .",
        ".. Indicator",
        "_ Middle",
        "the _",
        "the _",
        "the .",
        ".. Indicator",
        "_ Middle",
        "regulation _",
        "regulation _",
        "regulation .",
        ".. Indicator",
        "_ Middle",
        "of _",
        "of _",
        "of .",
        ".. Indicator",
        "_ Middle",
        "T-cell _",
        "T-cell _",
        "T-cell .",
        ".. Indicator",
        "_ Middle",
        "mitogenesis _",
        "mitogenesis _",
        "mitogenesis .",
        ".. Indicator",
        "_ End",
        "b) Related works such as named entity recognition (Collins, 1999) and text chunking (Zhang et al., 2001);",
        "A large amount of advanced syntactic features including syntactic connections, paths, families and their concatenations are introduced.",
        "Many ofthese features come from dependency parsing, which aims at building syntactic tree expressed by dependencies between words.",
        "More details about dependency parsing are given in Nivre and Scholz in Zhao et al.",
        "(2009a) is used to construct dependency structures in our system, and some of the notations in this paper adopt those presented in Zhao et al.",
        "(2009c).",
        "Feature templates are from various combinations or integrations of the following basic elements.",
        "Word Property.",
        "This part of features includes word form (form), lemma (lemma), part-of-speech tag (pos), syntactic dependency (dp), syntactic dependency label (dprel).",
        "Syntactic Connection.",
        "This includes syntactic head (h), left(right) farthest(nearest) child (Im, In, rm and rn) and high (low) support verb, noun or preposition.",
        "Here we specify the last one as an example, support verb(noun/preposition).",
        "From a given word to the syntactic root along the syntactic tree, the first verb/noun/preposition that is met is called its low support verb/noun/preposition, and the nearest one to the root(farthest to the given word) is called as its high support verb/noun/preposition.",
        "The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006), and it is extended to nouns and prepositions in Zhao et al.",
        "(2009b).",
        "In addition, a slightly modified syntactic head, pp-head, is introduced, it returns the left most sibling of a given word if the word is headed by a preposition, otherwise it returns the original head.",
        "Path.",
        "There are two basic types of path.",
        "One is the linear path (linePath) in the sequence, the other is the path in the syntactic parsing tree (dp-Path).",
        "For example, m:n\\dpPath represents the dependency path from word m to n. Assuming that the two paths from m and n to the root are pm and pn, m:n\\dpPathShare, m:n\\dpPathPred and m:n\\dpPathArgu represent the common part of pm and pn, part of pm which does not belong to pn and part of pn which does not belong to pm, respectively.",
        "Family.",
        "A children set includes all syntactic children(children) are used in the template notations.",
        "Concatenation of Elements.",
        "For all collected elements according to dpPath, children and so on, we use three strategies to concatenate all those strings to produce the feature value.",
        "The first is seq, which concatenates all collected strings without doing anything.",
        "The second is bag, which removes all duplicated strings and sort the rest.",
        "The third is noDup, which removes all duplicated neighbored strings.",
        "Hedge Cue Dictionary and Scope Indicator.",
        "Hedge cues in the training set are collected and put in a dictionary.",
        "Whether a word in the training or testing set is in the dictionary (dic) is introduced into feature templates.",
        "As the evaluation is non-open, we do not put in any additional hedge cues from other resources.",
        "An indicator (indicator) is given for multi-hedge scope finding, as specified in section 2.At last, in feature set for scope labeling, hedge represents that the word is in a hedge cue.",
        "At last, we take x as current token to be labeled, and xm to denote neighbor words.",
        "m > 0 represents that it is a word goes mth after current word and m < 0 for word – mth before current word.",
        "As optimal feature template subsets cannot be expected to be extracted from so large sets by hand, greedy feature selections according to Zhao et al.",
        "(2009b) are applied.",
        "The normalized feature selection has been proved to be effective in quite a lot of NLP tasks and can often successfully select an optimal or very close to optimal feature set from a large-scale superset.",
        "Although usually it needs 3 to 4 loops denoted by \"While\" in the Algorithm 1 of Zhao et al.",
        "(2009b) to get the best template set, we only complete one before official outputs collection because of time limitation, which to a large extent hinders the performance of the system.",
        "Three template sets are selected for BioScope corpus.",
        "One with the highest accuracy for sentence-level hedge detection (Set B), one with the best performance for word-level hedge cue labeling (Set H) and another one with the maximal F-score for scope finding (Set S).",
        "In addition, one set is discovered for sentence-level hedge detection of Wikipedia (Set W) .",
        "Table 5 lists some selected feature templates which are basic word or hedging properties for the three sets of BioScope corpus and Wikipedia.",
        "From the table we can see it is clear that the combinations of lemma, POS and word form of words in context, which are usually basic and common elements in NLP, are also effective for hedge detection.",
        "And as we expected, the feature that represents whether the word is in the hedge list or not is very useful especially in hedge cue finding, indicating that methods based on a hedge cue lists (Light et al., 2004) or keyword selection (Szarvas, 2008) are quite significant way to accomplish such tasks.",
        "Some a little complicated syntactic features based on dependencies are systemically exploited as features for tasks.",
        "Table 6 enumerates some of the syntactic features which proves to be highly effective.",
        "We noticed that lowSupportNoun, high-SupportNoun and features derived from dpPath is notably useful.",
        "It can be explained by the awareness that hedge labeling and scope finding are to process literatures in the level of semantics where syntactic features are often helpful.",
        "We continue our feature selection procedures for BioScope corpus after official outputs collection and obtain feature template sets that bring better performance.",
        "Table 7 gives some of the features in the optimized sets for BioScope corpus resolution.",
        "One difference between the new sets and the old ones is the former contain more syntactic elements, indicating that exploiting syntactic feature is a correct choice.",
        "Another difference is the new sets assemble more information of words before or after the current word, especially words linearly far away but close in syntax tree.",
        "Appearance of combination of these two factors such as x-i.lm.form seems to provide an evidence of the insufficiency training and development of our system submitted to some extent."
      ]
    },
    {
      "heading": "4. Evaluation results",
      "text": [
        "Two tracks (closed and open challenges) are provided for CoNLL-2010 shared task.",
        "We participated in the closed challenge, select features based",
        "'num in the set of Wikipedia represents the sequential number of word in the sentence x.lowSupportNoun:x | dpPathArgu.dprel.seq x.lowSupportNoun:x\\dpPathArgu.dprel.seq + x.lowSupportProp:x\\dpPathArgu.dprel.seq x.lowSupoortNoun.pos x.pos + x.children.dprel.bag x.rm.dprel + x.form x.pphead.lemma x.form + x.children.dprel.bag x.lowSupportNoun:x – dpTreeRelation x.lowSupportProp.lemma x.form + x.children.dprel.noDup x.highSupportNoun:x| dpTreeRelation + x.form x.lowSupportVerb.form x.lowSupportProp:x\\dpPathShared.dprel.seq x.lowSupportProp:x| dpPathShared.pos.seq x.highSupportNoun.pos x.highSupportNoun:x\\dpTreeRelation x.highSupportNoun:x\\dpPathArgu.dprel.seq + x.highSupportProp:x\\dpPathArgu.dprel.seq xlowSupportProp.lemma x.rm.dprel x.lm.form x.lemma + x.pphead.form x.lowSupportVerb.form x.rm.lemma + x.rm.form x.children.dprel.noDup x.children.dprel.bag x.highSupportNoun:x\\ dpTreeRelation x.lemma + x.pphead.form x.highSupportNoun:x\\ dpTreeRelation + x.form x.lowSupportVerb.form x.lowSupportVerb.lemma x.h.children.dprel.bag x.highSupportVerb.form x.lemma + x.pphead.form x.lm.dprel + x.pos x.lowSupportProp:x\\ dpPathPred.dprel.seq x.rm.lemma x.lowSupportProp:x\\ dpTreeRelation x.lowSupportVerb:x\\ dpPathPred.dprel.seq x.lowSupportVerb:x\\ dpPathPred.pos.seq x.lowSupportVerb:x\\ dpPathShared.pos.seq x.lowSupportProp:x\\ dpPathShared.pos.seq x.lowSupportProp.form x-i .lemma x.lowSupportNoun:x \\ dpPathArgu.dprel.seq x.lowSupportNoun:x\\ dpPathArgu.dprel.seq + x.lowSupportProp:x\\dpPathArgu.dprel.seq x.lowSupportProp:x \\ dpPathShared.dprel.seq x-i .lm.form x.children.dprel.bag x.lemma + x.pphead.form x.highSupportVerb.form x.highSupportNoun:x\\dpTreeRelation + x.form x.lowSupportNoun:x\\ dpTreeRelation + x.form",
        "SetB",
        "x.lemma + xi.lemma + x-i.lemma + x.dic + xi.dic + x-i.dic x.lemma + xi.pos + X-i.pos + x.pos + xi.lemma + x-i.lemma x.form",
        "x.pos + xi.pos + x-i.pos + x2.pos + x-2.pos",
        "x. dic + xi.dic + x-i.dic",
        "xi.",
        "pos",
        "Set H",
        "x.dic + xi.dic + x-i.dic + x2.dic + x-2.dic x.pos + x-i.pos x.dic",
        "x.dic + x.lemma + x.pos + x.form x.pos + xi.pos + x-i.pos + x2.pos + x-2.pos",
        "x-2.form + x-2.lemma x-i.form + x.form x.dic + xi.dic + x-i.dic",
        "SetS",
        "x.dic + xi.dic + x-i.dic + x2.dic + x-2.dic + x3 .dic + x-3.dic x.indicator",
        "x.hedge + xi.hedge + x-i.hedge x.lemma + xi.pos + x-i.pos + x.pos + xi.lemma + x-i.lemma",
        "x. pos + x.hedge + x.dp + x.dprel",
        "xi.",
        "pos",
        "x.pos + xi.pos + x-i.pos + x2.pos + x-2.pos",
        "Set W",
        "x.lemma + xi.lemma + x-i .lemma",
        "+ x.dic + xi.dic + x-i .dic x.lemma + xi.lemma + x-i .lemma",
        "+x2.lemma + x-2.lemma + x.dic + xi.dic + x-i .dic + x2.dic + x-2.dic x.lemma + xi.lemma x.hedge + xi.hedge + x-i.hedge",
        "+ x2.hedge + x-2.hedge + x3.hedge",
        "+ x-3.hedge",
        "x.pos + xi.pos + x-i.pos +x2.pos + x-2.pos + x.dic + xi.dic + x-i.dic + x2.dic + x-2 .dic x.pos + x.dic x.num + x.dic",
        "on the in-domain data and evaluated our system on the in-domain and cross-domain evaluation set.",
        "All the experiments are implemented and run by Maximum Entropy Markov Models (McCallum, 2000).",
        "The official results for tasks are in Table 8, in which three in-domain tests and cue matching result for biomedical texts are listed.",
        "For the first task for BioCorpus, our system gives F-score 0.8363 in in-domain test and for Wikipedia we give F-score 0.5618 in closed evaluation.",
        "For the second task, our system gives results in closed and open test, with F-score 0.4425 and 0.4441 respectively.",
        "We compare the F-score of our system with the best in the final result in Table 9.",
        "We rank pretty high in Wikipedia hedge detection, while other three are quite steady but not prominent.",
        "This is mainly due to two reasons:",
        "1.",
        "Feature selection procedures are not perfectly conducted.",
        "2.",
        "Abstracts and fulltexts in BioScope are mixed to be the training set, which proves quite inappropriate when the evaluation set contains only fulltext literature, since abstract and full-text are quite different in terms of hedging.",
        "Intact feature selection procedures for BioScope corpus are conducted after official outputs collections.",
        "The results of evaluation with completely selected features compared with the incomplete one are given in Table 7.",
        "The system performs a higher score on evaluation data (Table 10), which is more competitive in both tasks on BioScope corpus.",
        "The improvement for task 2 is significant, but the increase of performance of hedge cue detection is less remarkable.",
        "We believe that a larger fulltext training set and a more considerate training plan will help us to do better job in the future work."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We describe the system that uses sequence labeling with normalized feature selection and rich features to detect hedges and find scopes for hedge cues.",
        "Syntactic features which are derived from dependencies are exploited, which prove to be quite favorable.",
        "The evaluation results show that our system is steady in performance and does pretty good hedging and scope finding in both BioScope corpus and Wikipedia, especially when the feature selection procedure is carefully and totally conducted.",
        "The results suggest that sequence labeling and a feature-oriented method are effective in such NLP tasks.",
        "Dataset",
        "F-score",
        "Best",
        "Taskl-closed",
        "0.8363",
        "0.8636",
        "BioScope",
        "Task2-closed",
        "0.4425",
        "0.5732",
        "Cue-matching",
        "0.7853",
        "0.8134",
        "Wikipedia",
        "Taskl-closed",
        "0.5618",
        "0.6017",
        "Dataset",
        "Complete",
        "Incomplete",
        "Task1-closed",
        "0.8522",
        "0.8363",
        "BioScope",
        "Task2-closed",
        "0.5151",
        "0.4425",
        "Cue-matching",
        "0.7990",
        "0.7853",
        "Dataset",
        "TP",
        "FP",
        "FN",
        "precision",
        "recall",
        "F-score",
        "BioScope",
        "Task1-closed",
        "669",
        "141",
        "121",
        "0.8259",
        "0.8468",
        "0.8363",
        "Task2-closed",
        "441",
        "519",
        "592",
        "0.4594",
        "0.4269",
        "0.4425",
        "Cue-matching",
        "788",
        "172",
        "259",
        "0.8208",
        "0.7526",
        "0.7853",
        "Wikipedia",
        "Task1-closed",
        "991",
        "303",
        "1243",
        "0.7658",
        "0.4436",
        "0.5618"
      ]
    }
  ]
}
