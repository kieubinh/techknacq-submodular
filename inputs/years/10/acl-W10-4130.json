{
  "info": {
    "authors": [
      "Yu-Chieh Wu",
      "Jie-Chi Yang",
      "Yue-Shi Lee"
    ],
    "book": "Proceedings of the Joint Conference on Chinese Language Processing",
    "id": "acl-W10-4130",
    "title": "Chinese Word Segmentation with Conditional Support Vector Inspired Markov Models",
    "url": "https://aclweb.org/anthology/W10-4130",
    "year": 2010
  },
  "references": [
    "acl-I05-3025",
    "acl-W02-1815",
    "acl-W03-1719",
    "acl-W06-0127"
  ],
  "sections": [
    {
      "text": [
        "A Chinese Word Segmentation System Based on Structured Support Vector Machine Utilization of Unlabeled Text Corpus",
        "Chongyang Zhang Zhigang Chen Guoping Hu",
        "Anhui Province Anhui Province Anhui Province",
        "Engineering Laboratory Engineering Laboratory Engineering Laboratory",
        "of Speech and Language, of Speech and Language, of Speech and Language,",
        "Technology of China Technology of China Technology of China",
        "cyzhang9 Chenzhigang Applecore",
        "Character-based tagging method has achieved great success in Chinese Word Segmentation (CWS).",
        "This paper proposes a new approach to improve the CWS tagging accuracy by structured support vector machine (SVM) utilization of unlabeled text corpus.",
        "First, character N-grams in unlabeled text corpus are mapped into low-dimensional space by adopting SOM algorithm.",
        "Then new features extracted from these maps and another kind of feature based on entropy for each N-gram are integrated into the structured SVM methods for CWS.",
        "We took part in two tracks of the Word Segmentation for Simplified Chinese Text in bakeoff-2010: Closed track and Open track.",
        "The test corpora cover four domains: Literature, Computer Science, Medicine and Finance.",
        "Our system achieved good performance, especially in the open track on the domain of medicine, our system got the highest score among 18 systems."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In the last decade, many statistics-based methods for automatic Chinese word segmentation (CWS) have been proposed with development of machine learning and statistical method (Huang and Zhao, 2007).",
        "Especially, character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al., 2005).",
        "The character-based tagging method formulates the CWS problem as a task of predicting a tag for each character in the sentence, i.e. every character is considered as one of four different types in 4-tag set: B (begin of word), M (middle of word), E (end of word), and S (single-character word).",
        "Most of these works train tagging models only on limited labeled training sets, without using any unsupervised learning outcomes from unlabeled text.",
        "But in recent years, researchers begin to exploit the value of enormous unlabeled corpus for CWS, such as some statistics information on co-occurrence of subsequences in the whole text has been extracted from unlabeled data and been employed as input features for tagging model training (Zhao and",
        "Kit , 2007).",
        "Word clustering is a common method to utilize unlabeled corpus in language processing research to enhance the generalization ability, such as part-of-speech clustering and semantic clustering (Lee et al., 1999 and B Wang and H Wang 2006).",
        "Character-based tagging method usually employs N-gram features, where an N-gram is an N-character segment of a string.",
        "We believe that there are also semantic or grammatical relationships between most of N-grams and these relationships will be useful in CWS.",
        "Intuitively, assuming the training data contains the bigram \" / M \"(The last two characters of the word \"Israel\" in Chinese), not contain the bigram \" ï / % \"(The last two characters of the word \"Turkey\" in Chinese), if we could cluster the two bigrams together according to unlabeled corpus and employ it as a feature for supervised training of tagging model, then maybe we will know that there should be a word boundary after \"!£/%\" though we only find the existence of word boundary after \" fe / M \" in the training data.",
        "So we investigate how to apply clustering method onto unlabeled data for the purpose of improving CWS accuracy in this paper.",
        "This paper proposes a novel method of using unlabeled data for CWS, which employs Self-Organizing Map (SOM) (Kohonen 1982) to organize Chinese character N-grams on a two-dimensional array, named as \"N-gram cluster map\" (NGCM), in which the character N-grams similar in grammatical structure and semantic meaning are organized in the same or adjacent position.",
        "Two different arrays are built based the N-gram's preceding context and succeeding context respectively because normally N-gram is just part of Chinese word and doesn't share similar preceding and succeeding context in the same time.",
        "Then NGCM-based features are extracted and applied to tagging model of CWS.",
        "Another kind of feature based on entropy for each N-gram is also introduced for improving the performance of CWS.",
        "The rest of this paper is organized as follows: Section 2 describes our system; Section 3 describes structured SVM and the features which are obtained from labeled corpus and also unlabeled corpus; Section 4 shows experimental results on Bakeoff-2010 and Section 5 gives our conclusion.",
        "The architecture of our system for open track is shown in Figure 1.",
        "For improving the cross-domain performance, we train and test with dictionary-based word segmentation outputs.",
        "On large-scale unlabeled corpus we use Self-Organizing Map (SOM) (Kohonen 1982) to organize Chinese character N-grams on a two-dimensional array, named as \"N-gram cluster map\" (NGCM), in which the character N-grams similar in grammatical structure and semantic meaning are organized in the same or adjacent position.",
        "Then new features are extracted from these maps and integrated into the structured SVM methods for CWS.",
        "Training text",
        "Dictionary Based CWS",
        "The large-scale unlabeled corpus",
        "Dictionary Based",
        "Labeled data",
        "Structured SVM test text",
        "Figure 2: closed track system Because the large-scale unlabeled corpus is forbidden to be used on closed track.",
        "We trained the SOM only on the data provided by organizers.",
        "To make up for the deficiency of the sparse data on SOM, we add entropy-based features (ETF) for every N-gram to structured SVM model.",
        "The architecture of our system for close track is shown in Figure 2.",
        "Som",
        "_",
        "Statistic",
        "1"
      ]
    },
    {
      "heading": "3. Learning algorithm",
      "text": [
        "The structured support vector machine can learn to predict structured y, such as trees sequences or sets, from x based on large-margin approach.",
        "We employ a structured SVM that can predict a observation sequences x = (x,...,xT), where y £Z ,£ is the label set for y.",
        "There are two types of features in the structured SVM: transition features (interactions between neighboring labels along the chain), emission features (interactions between attributes of the observation vectors and a specific label).we can represent the input-output pairs via joint feature map (JFM)",
        "Kronecker delta ö , ö observation sequence.",
        "n – 0 is a scaling factor which balances the two types of contributions.",
        "Note that both transition features and emission features can be extended by including higher-order interdependencies of labels (e.g.",
        "Ac (yt) ®Ac(yt+) ®Ac(yt+2) ),by including input features from a window centered at the current position (e.g. replacing (((x') with",
        "The w-parametrized discriminant function F .",
        "X x 7 – R interpreted as measuring the compatibility of x and y is defined as:",
        "So we can maximize this function over the response variable to make a prediction f (x) = arg max F(x, y, w)",
        "Training the parameters can be formulated as the following optimization problem.",
        "s.tVi,Vye Y.",
        "( ^ ^ (x/, y,) - ^ (x,, y^ – À(y, y) - £.",
        "where n is the number of the training set, ^ is a slack variable , C – 0 is a constant controlling the tradeoff between training error minimization and margin maximization,",
        "À(y, y) is the loss function ,usually the number of misclassified tags in the sentence.",
        "For a training sample denoted as x = (x,..., xT ) and y = (y,..., yT ).",
        "We chose first-order interdependencies of labels to be transition features, and dependencies between labels and N-grams (n=1, 2, 3, 4) at current position in observed input sequence to be emission features.",
        "So our JFM is the concatenation of the follow vectors",
        "The emission features of 3-grams and 4-grams are not shown here because of the large number of the dependencies.",
        "The Self-Organizing Map (SOM) (Kohonen 1982), sometimes called Kohonen map, was developed by Teuvo Kohonen in the early 1980s.",
        "Self-organizing semantic maps (Ritter and organized according to word similarities, measured by the similarity of the short contexts of the words.",
        "Our algorithm of building N-gram cluster maps is similar to self-organizing semantic maps.",
        "Because normally N-gram is just part of Chinese word and do not share similar preceding and succeeding context in the same time, so we build two different maps according to the preceding context and the succeeding context of N-gram individually.",
        "In the end we build two NGCMs: NGCMP (NGCM according to preceding context) and NGCMS (NGCM according to succeeding context).",
        "Due to the limitation of our computer and time we only get two 15 x 15 size 2GCMs for open track system from large-scale unlabeled corpus which was obtained easily from websites like Sohu, Netease, Sina and People Daily.",
        "The 2GCMP and 2GCMS we got for the open track task are shown in Figure 4 and Figure 5 respectively.",
        "After checking the results, we find that the 2GCMS have following characters:l) most of the meaningless bigrams that contain characters from more than one word, such as the bigram \" M^c\" in \"...jkM^ië...\" , are organized into the same neurons in the map, 2) most of the first or last bigrams of the country names are organized into a few adjacent neurons, such as 2GCMP.",
        "Two 20xl size 2GCMs are trained for the closed track system only on the data provided by organizers.",
        "The results are not as good as the results of the 15 xl5 size 2GCMs because of the less training data.",
        "The second character described above is no longer apparent as well as the l5 x l5 size 2GCMs, but it still kept the first character.",
        "Then we adopt the position of the neurons which current N-gram mapped in the NGCM as a new feature.",
        "So every feature has D dimensions (D equals to the dimension of the NGCM, every dimension is corresponding to the coordinate value in the NGCM).",
        "In this way, N-gram which is originally represented as a high dimensional vector based on its context is mapped into a very low-dimensional space.",
        "We call it NGCM mapping feature.",
        "So our previous JFM in section 3.2 is concatenated with the following features:",
        "where ç (x) and ç (x) e{0,l,...,l4} denote the NGCM mapping feature from 2GCMS and 2GCMP respectively.",
        "nNGCM(x) denotes the quantization error of current N-gram x on its NGCM.",
        "As an example, the process of import features from NGCMs at y3 is presented in Figure 6.",
        "Entropy-based features",
        "On closed track, the entropy of the preceding and succeeding characters conditional on the N-gram and also the self-information of the N-gram are used as features for the structured SVM methods.",
        "Then our previous JFM in section 3.2 is concatenated with the following features:",
        "xNgram e all the ngrams used in section 3.2",
        "Where P and S denote the set of the preceding and succeeding characters respectively.",
        "The entropy: H(X | N = xNgram) =",
        "-£ p(x | xNgram )log p(x' | xNgram )",
        "The self-information of the N-gram N = xNgram :"
      ]
    },
    {
      "heading": "4. Applications and Experiments",
      "text": [
        "Text Preprocessing",
        "Text is usually mixed up with numerical or alphabetic characters in Chinese natural language, such as office ilSESljEf&i: 9",
        "È \".",
        "These numerical or alphabetic characters are barely segmented in CWS.",
        "Hence, we treat these symbols as a whole \"character\" according to the following two preprocessing steps.",
        "First replace one alphabetic character to four continuous alphabetic characters with El to E4 respectively, five or more alphabetic characters with E5.",
        "Then replace one numerical number to four numerical numbers with Nl to N4 and five or more numerical numbers with N5.",
        "After text preprocessing, the above examples will be",
        "Previous works show that 6-tag set achieved a better CWS performance (Zhao et al., 2006).",
        "Thus, we opt for this tag set.",
        "This 6-tag set adds 'B2' and 'B3' to 4-tag set which stand for the type of the second and the third character in a Chinese word respectively.",
        "For example, the tag sequence for the sentence \"±M$tW<k/WWW¥ ^(Shanghai World Expo / will / last / six months)\" will be \"B B2 B3 M E S B E B E\".",
        "We use svmhmm version 3.l to build our structured SVM models.",
        "The cut-off threshold is set to 2.",
        "The precision parameter is set to 0.l.",
        "The tradeoff between training error minimization and margin maximization is set to l000.",
        "We took part in two tracks of the Word Segmentation for Simplified Chinese Text in bakeoff-20l0: c (Closed track), o (Open track).",
        "The test corpora cover four domains: A (Literature), B (Computer Science), C (Medicine), D (Finance).",
        "Precision(P),Recall(R),F-measure(F),Out-Of-Vocabulary Word Recall(OOV RR) and In-Vocabulary Word Recall(IV RR) are adopted to measure the performance of word segmentation system.",
        "Table l shows the results of our system on the word segmentation task for simplified Chinese text in bakeoff-20l0.",
        "Table 2 shows the comparision between our system results and best results in bakeoff-20l0.",
        "Tabel 2: The comparision between our system results and best results in bakeoff-20l0",
        "It is obvious that our systems are stable and reliable even in the domain of medicine when the F-measure of the best results was decreased.",
        "Our open track system performs better than closed track system, demonstrating the benefit of the dictionary-based word segmentation outputs and the NGCMs which are training on large-scale unlabeled corpus."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "This paper proposes a new approach to improve the CWS tagging accuracy by structured support vector machine (SVM) utilization of unlabeled text corpus.",
        "We use SOM to organize Chinese character N-grams on a two-dimensional array, so that the N-grams similar in grammatical structure and semantic meaning are organized in the same or adjacent position.",
        "Then new features extracted from these maps and another kind of feature based on entropy for each N-gram are integrated into the structured SVM methods for CWS.",
        "Our system achieved good performance, especially in the open track on the domain of medicine, our system got the highest score among l8 systems.",
        "In future work, we will try to organizing all the N-grams on a much larger array, so that every neuron will be labeled by a single N-gram.",
        "The ultimate objective is to reduce the dimension of input features for supervised CWS learning by replacing N-gram features with two-dimensional NGCM mapping features.",
        "A",
        "c",
        "R",
        "0.932",
        "P",
        "0.935",
        "Fl",
        "0.933",
        "OOV RR",
        "0.654",
        "IV RR",
        "0.953",
        "o",
        "0.942",
        "0.943",
        "0.942",
        "0.702",
        "0.959",
        "B",
        "c",
        "0.935",
        "0.934",
        "0.935",
        "0.792",
        "0.96l",
        "o",
        "0.948",
        "0.946",
        "0.947",
        "0.8l2",
        "0.973",
        "C",
        "c",
        "0.937",
        "0.934",
        "0.936",
        "0.76l",
        "0.959",
        "o",
        "0.94l",
        "0.935",
        "0.938",
        "0.787",
        "0.96",
        "D",
        "c",
        "0.955",
        "0.956",
        "0.955",
        "0.848",
        "0.965",
        "o",
        "0.948",
        "0.955",
        "0.95l",
        "0.853",
        "0.957",
        "Fl(Bakeoff-20l0)",
        "Fl(Our system)",
        "A",
        "c",
        "0.946",
        "0.933",
        "o",
        "0.955",
        "0.942",
        "B",
        "c",
        "0.95l",
        "0.935",
        "o",
        "0.95",
        "0.947",
        "C",
        "c",
        "0.939",
        "0.936",
        "o",
        "0.938",
        "0.938",
        "D",
        "c",
        "0.959",
        "0.955",
        "o",
        "0.96",
        "0.95l"
      ]
    }
  ]
}
