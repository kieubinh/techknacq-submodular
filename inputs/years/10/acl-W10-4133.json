{
  "info": {
    "authors": [
      "Kun Wang",
      "Chengqing Zong",
      "Keh-Yih Su"
    ],
    "book": "Proceedings of the Joint Conference on Chinese Language Processing",
    "id": "acl-W10-4133",
    "title": "A Character-Based Joint Model for CIPS-SIGHAN Word Segmentation Bakeoff 2010",
    "url": "https://aclweb.org/anthology/W10-4133",
    "year": 2010
  },
  "references": [
    "acl-I05-3027",
    "acl-P06-2123",
    "acl-P08-1102",
    "acl-P96-1041"
  ],
  "sections": [
    {
      "text": [
        "A Character-Based Joint Model",
        "for CIPS-SIGHAN Word Segmentation Bakeoff 2010",
        "Kun Wang and Chengqing Zong Keh-Yih Su",
        "National Laboratory of Pattern Recognition Behavior Design Corporation Institute of Automation, Chinese Academy of Science",
        "{kunwang,cqzong}@nlpr.ia.ac.cn kysu@bdc.com.tw",
        "This paper presents a Chinese Word Segmentation system for the closed track of CIPS-SIGHAN Word Segmentation Bakeoff 2010.",
        "This system adopts a character-based joint approach, which combines a character-based generative model and a character-based discriminative model.",
        "To further improve the cross-domain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus.",
        "The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The character-based tagging approach (Xue, 2003) has become the dominant technique for Chinese word segmentation (CWS) as it can tolerate out-of-vocabulary (OOV) words.",
        "In the last few years, this method has been widely adopted and further improved in many previous works (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008).",
        "Among various character-based tagging approaches, the character-based joint model (Wang et al., 2010) achieves a good balance between in-vocabulary (IV) words recognition and OOV words identification.",
        "In this work, we adopt the character-based joint model as our basic system, which combines a character-based discriminative model and a character-based generative model.",
        "The generative module holds a robust performance on IV words, while the discriminative module can handle the extra features easily and enhance the OOV words segmentation.",
        "However, the performance of out-of-domain text is still not satisfactory as that of in-domain text, while few previous works have paid attention to this problem.",
        "To further improve the performance of the basic system in out-of-domain text, we use a semi-supervised learning procedure to incorporate the unlabeled corpora of Literature (Unlabeled-A) and Computer (Unlabeled-B).",
        "The final results show that our system performs well on all four testing-sets and achieves comparable segmentation results with other participants."
      ]
    },
    {
      "heading": "2. Our system",
      "text": [
        "The character-based joint model in our system contains two basic components:",
        "> The character-based discriminative model.",
        "> The character-based generative model.",
        "The character-based discriminative model (Xue, 2003) is based on a Maximum Entropy (ME) framework (Ratnaparkhi, 1998) and can be formulated as follows:",
        "Where tk is a member of {Begin, Middle, End, Single} (abbreviated as B, M, E and S from now on) to indicate the corresponding position of character ck in its associated word.",
        "For example, the word \"(Beijing City)\" will be assigned with the corresponding tags as: \" it /B (North) jjC/M (Capital) TpVE (City)\".",
        "This discriminative module can flexibly incorporate extra features and it is implemented with the ME package1 given by Zhang Le.",
        "All training experiments are done with Gaussian prior 1.0 and 200 iterations.",
        "The character-based generative module is a character-tag-pair-based trigram model (Wang et al., 2009) and can be expressed as below:",
        "In our experiments, SRI Language Modeling Toolkit2 (Stolcke, 2002) is used to train the generative trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998).",
        "The character-based joint model combines the above discriminative module and the generative module with log-linear interpolation as follows:",
        "weight for the generative model.",
        "Score(tk) will be directly used during searching the best sequence.",
        "We set an empirical value (a = 0.3) to this model as there is no development-set for various domains.",
        "In this work, the feature templates adopted in the character-based discriminative model are very simple and are listed below:",
        "In the above templates, Cn represents a character and the index n indicates the position.",
        "For example, when we consider the third character \"Jft\" in the sequence \"ASlSs\", template (a) results in the features as following: C-2=;)b, C-1= M, C0=^, C1=3£, C2=^, and template (b) generates the features as: C-2C-1=^M, C.1C0=M^,",
        "C0C1=^S, C1C2=S4x, and template (c) gives the feature dC^ME.",
        "Template (d) is the feature of character type.",
        "Five types classes are defined: dates (\"^\", \"R\", the Chinese character for \"year\", \"month\" and \"day\" respectively) represents class 0; foreign alphabets represent class 1; Arabic and Chinese numbers represent class 2; punctuation represents class 3 and other characters represent class 4.",
        "For example, when we consider the character \", \" in the sequence \"A^> N\"Q\", the feature T(C-2)T(C-1)T(C0)T(C1)T(C2) will be set to \"20341\".",
        "When training the character-based discriminative module, we convert all the binary features into real-value features, and set the real-value of C0 to be 2.0, the value of C-1C0 and C0C1 to be 3.0, and the values of all other features to be 1.0.",
        "This method sounds a little strange because it is equal to duplicate some features for the maximum entropy training.",
        "However, it effectively improves the performance in our previous works.",
        "As the closed track allows the participants to use the character type information, we add some restrictions to our system when constructing the character-tag lattice.",
        "When we consider a character in the sequence, the type information of both the previous and the next character would be taken into account.",
        "The restrictions are list as follows:",
        "• If the previous, the current and the next characters are all English or numbers, we would fix the current tag to be \"M\";",
        "• If the previous and the next characters are both English or numbers, while the current character is a connective symbol such as \"-\", \"/\", \"\\\" etc., we would also fix the current tag to be \"M\";",
        "• Otherwise, all four tags {B, E, M, S} would be given to the current character.",
        "It is shown that in the Computer domain these simple restrictions not only greatly reduce the number of words segmented, but also speed up the system.",
        "Algorithm 1 : Semi-Supervised Learning Given:",
        "• Labeled training corpus: L",
        "• Unlabeled training corpus: U_",
        "Use L to train a segmenter S0;",
        "Use S0 to segment the unlabeled corpus U and then get labeled corpus U0; for i = 1 to K do",
        "Add Ui-1 to L and get a new corpus Use Li to train a new segmenter Si; Use Si to segment the unlabeled corpus U and then get labeled corpus Ui; if convergence criterion meets break",
        "In the last decade, Chinese word segmentation has been improved significantly and gets a high precision rate in performance.",
        "However, the performance for out-of-domain text is still unsatisfactory at the present.",
        "Also, few works have paid attention to the cross-domain problem in Chinese word segmentation task so far.",
        "Self-training and Co-training are two simple semi-supervised learning methods to incorporate unlabeled corpus (Zhu, 2006).",
        "In this work, we use an iterative self-training method to incorporate the unlabeled data.",
        "A segmenter is first trained with the labeled corpus.",
        "Then this segmenter is used to segment the unlabeled data.",
        "Then the predicted data is added to the original training corpus as a new training-set.",
        "The segmenter will be retrained and the procedure repeated.",
        "To simplify the task, we fix the weight a = 0.3 for the generative module of our joint model in the training iterations.",
        "The procedure is shown in Algorithm 1.",
        "The iterations will not be ended until the similarity of two segmentation results Ui-1 and Ui reach a certain level.",
        "Here we used F-score to measure the similarity between Ui-1 and Ui: treat Ui-1 as the benchmark, Ui as a testing-set.",
        "From our observation, this method converges quickly in only 3 or 4 iterations for both Literature and Computer corpora."
      ]
    },
    {
      "heading": "3. Experiments and Discussion 3.1 Results",
      "text": [
        "In this CIPS-SIGHAN bakeoff, we only participate the closed track for simplified-character text.",
        "There are two kinds of training corpora:",
        "• Labeled corpus from News Domain",
        "• Unlabeled corpora from Literature Domain (Unlabeled-A) and Computer Domain (Unlabeled-B).",
        "Also, the testing corpus covers four domains: Literature (Testing-A), Computer (Testing-B), Medicine (Testing-C) and Finance (Testing-D).",
        "As there are only two unlabeled corpora for Domain A and B, we thus adopt different strategies for each testing-set:",
        "• Testing-A: Character-Based Joint Model with semi-supervised learning, training on Labeled corpus and Unlabeled-A;",
        "• Testing-B: Character-Based Joint Model with semi-supervised learning, training on Labeled corpus and Unlabeled-B;",
        "• Testing-C and D: Character-Based Joint Model, training on Labeled corpus;",
        "Table 1 shows that our system achieves F-scores for various testing-sets: 0.937 (A), 0.940 (B), 0.923 (C) and 0.957 (D), which are comparable with other systems.",
        "Among those four testing domains, our system performs unsatisfactorily on Testing-C (Medicine) even the OOV rate of this domain is not the highest.",
        "There are possible reasons for this result: (1) Semi-supervised learning is not conducted for this domain; (2) the statistical property between News and Medicine are significantly different.",
        "Domain",
        "Mark",
        "OOV Rate",
        "R",
        "P",
        "F1",
        "ROOV",
        "Literature",
        "A",
        "0.069",
        "0.937",
        "0.937",
        "0.937",
        "0.652",
        "0.958",
        "Computer",
        "B",
        "0.152",
        "0.941",
        "0.940",
        "0.940",
        "0.757",
        "0.974",
        "Medicine",
        "C",
        "0.110",
        "0.930",
        "0.917",
        "0.923",
        "0.674",
        "0.961",
        "Finance",
        "D",
        "0.087",
        "0.957",
        "0.956",
        "0.957",
        "0.813",
        "0.971",
        "Table 2: Performance of various approaches J: Baseline, the character-based joint model R: Adding restrictions in constructing lattice S: Conduct Semi-Supervised Learning",
        "The aim of restrictions in constructing lattice is to improve the performance of English and numerical expressions, both of which appear frequently in Computer and Finance domain.",
        "Therefore, the improvements gained from these restrictions are significantly in these two domains (as shown in Table 2).",
        "Besides, the adopted semi-supervised learning procedure improves the performance in Domain A and B., but the improvement is not significant.",
        "Semi-supervised learning aims to incorporate large amounts of unlabeled data.",
        "However, the size of unlabeled corpora provided here is too small.",
        "The semi-supervised learning procedure is expected to be more effective if a large amount of unlabeled data is available."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "Our system is based on a character-based joint model, which combines a generative module and a discriminative module.",
        "In addition, we applied a semi-supervised learning method to the baseline approach to incorporate the unlabeled corpus.",
        "Our system achieves comparable performance with other participants.",
        "However, cross-domain performance is still not satisfactory and further study is needed.",
        "Acknowledgement",
        "The research work has been partially funded by",
        "Hi-Tech Research and Development Program (\"863\" Program) of China under Grant No.",
        "2006AA010108-4 as well.",
        "Domain",
        "Model",
        "F1",
        "ROOV",
        "A",
        "J + R + S",
        "0.937",
        "0.652",
        "J + S",
        "0.937",
        "0.646",
        "J + R",
        "0.936",
        "0.646",
        "J",
        "0.936",
        "0.642",
        "B",
        "J + R + S",
        "0.940",
        "0.757",
        "J + S",
        "0.931",
        "0.721",
        "J + R",
        "0.938",
        "0.744",
        "J",
        "0.927",
        "0.699",
        "C",
        "J + R",
        "0.923",
        "0.674",
        "J",
        "0.923",
        "0.674",
        "D",
        "J + R",
        "0.957",
        "0.813",
        "J 0.954 0.786"
      ]
    }
  ]
}
