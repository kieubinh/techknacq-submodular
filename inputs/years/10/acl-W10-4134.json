{
  "info": {
    "authors": [
      "Hua-Ping Zhang",
      "Jian Gao",
      "Qian Mo",
      "He-Yan Huang"
    ],
    "book": "Proceedings of the Joint Conference on Chinese Language Processing",
    "id": "acl-W10-4134",
    "title": "Incorporating New Words Detection with Chinese Word Segmentation",
    "url": "https://aclweb.org/anthology/W10-4134",
    "year": 2010
  },
  "references": [
    "acl-J04-1004"
  ],
  "sections": [
    {
      "text": [
        "Hua-Ping ZHANG Jian GAO Qian MO He-Yan HUANG",
        "Beijing Institute of Technology, Beijing, P.R.C 100081 Beijing Technology and Business University, Beijing, P.R.C 100048 Email: kevinzhang@bit.edu.cn",
        "With development in Chinese words segmentation, in-vocabulary word segmentation and named entity recognition achieves state-of-art performance.",
        "However, new words become bottleneck to Chinese word segmentation.",
        "This paper presents the result from Beijing Institute of Technology (BIT) in the Sixth International Chinese Word Segmentation Bakeoff in 2010.",
        "Firstly, the author reviewed the problem caused by the new words in Chinese texts, then introduced the algorithm of new words detection.",
        "The final section provided the official evaluation result in this bakeoff and gave conclusions."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "With the rapid development of Internet with Chinese language, word segmentation received extensive attention.",
        "In-vocabulary word segmentation and named entity recognition have achieved state-of-art performance.",
        "Chinese words are actually not well defined, and there is not a commonly accepted segmentation lexicon.",
        "It is hard to collect all possible new words, or predict new words occurred in the future.",
        "New words is the bottleneck to Chinese word segmentation.",
        "The problem became more severe with word segmentation on special domain texts, such as computer, medicine and finance.",
        "There are much specialized words which are difficult to be exported to the lexicon.",
        "So new words detection is very important, which would have more substantial impact on the performance of word segmentation than ambiguous segmentation.",
        "In this paper, we presented a method of new words detection, and then detailed the process of Chinese word segmentation incorporating new words detection.",
        "The last section provided the evaluation and gave our conclusions."
      ]
    },
    {
      "heading": "2. Problem with new words",
      "text": [
        "In the process of Chinese word Segmentation, there are many mistakes because of new words.",
        "These new words are Out of vocabulary (OOV), so the system couldn't distinguish them from original texts, and then impacted the results of word segmentation.",
        "We gave an example from Text C in medicine domain to explain and detect the new words.",
        "tmnmitff 12 mb, §§ra^# pad mmw",
        "ABI MMg£#, 0JMttffiHjE# The sentence should be segmented as follows :",
        "amn u ft% mm m",
        "b , mwm pad mm w %",
        "Here, both 'WhJE#\" and are domain words, or new words beyond general segmentation lexicon.",
        "Therefore, new words from domain should be detected and added to segmentation lexicon before word segmentation."
      ]
    },
    {
      "heading": "3. Word segmentation with new words detection",
      "text": [
        "General Lexicon",
        "Domain Lexicon",
        "Generate New Words",
        "As illustrated in Figure 1, Chinese word segmentation with new words detection is a recursive process.",
        "The process is given as follows:",
        "1.",
        "2.",
        "3.",
        "4.",
        "5.",
        "6.",
        "Making Chinese word segmentation with domain lexicon beyond general lexicon.",
        "Frequent string (over twice) finding with postfix tree algorithm, and taking them as new words candidate.",
        "Access Variety statistics [Haodi Feng etc.",
        "2004], and language modeling on word",
        "Exporting new words to domain lexicon.",
        "Recursively, until no more new word detected.",
        "Output final word sequence.",
        "Simple word segmentation is the first step of processing of Chinese language when we deal with a very long Chinese article.",
        "The method of word segmentation is based on HHMM, and Zhang and Liu (2003) have given detailed explanation about this.",
        "During the process of word segmentation in the first, the system records the words which occur frequently.",
        "We can set a threshold value of words' occurrence frequency.",
        "As long as the word occurrence frequency reaches this value, this word could be recorded in the system as frequent string.",
        "With the frequent strings detected, we can do the further analysis.",
        "For every frequent string, we check its left and right adjacent one in the original text segmented respectively.",
        "Through this step, we find the adjacent words which occur next to some frequent string detected.",
        "If the adjacent word also occurs very frequently, or even it occurs at the left or right of the frequent string every time, it's great possibility that the string detected and the adjacent word could merge into one word.",
        "With the detection in above steps, we gain new words from Chinese texts.",
        "Then we import these new words into domain lexicon and our lexicon is updated.",
        "With the lexicon containing new words, we can do the next cycle recursively and revise continually.",
        "Then, we can see this is a recursive structure.",
        "Through the continued process of word segmentation and new words detection, the state of segmentation tends to be steady.",
        "The condition of steady state has several kinds such as no more new words detected or the latest result equal to the previous one.",
        "At this time, we can break the recursion and output the final result.",
        "This is an example.",
        "This sentence is from Text D in finance domain",
        "\"fïSÂ^HjfïiJi^giJ-^, MÈMB t^MJä, IfÊES^. \"",
        "(\"The financial market has been stable and the stock has rebounded in less than one year time after Lehman Brother Corporation went bankrupt.\")",
        "After word segmentation with original lexicon, this altered sentence is:",
        "\"fîltÂlê\" is a new word as a organization name and it is hard to be collected.",
        "Like this kind of word, there are difficulties to add new words to update the lexicon in time.",
        "So it is normal to segment this word \"fîltÂlê\" into three words.",
        "Input Sentence",
        "V",
        "Word Segmentation With general lexicon+domain lexicon",
        "v",
        "r",
        "Frequent String Detection",
        "Through frequent string detection, we gain these three words \"W\", \" S \" and .",
        "With the adjacent analysis, we find the word \"W\" occurs 6 times, \"S\" 3 times and \"Xlê\" 3 times.",
        "The character \"W \" occurs 3 times in the detected word \"^WiÉ\" and 3 times at the left of the word \"S\".",
        "So we can consider the word \"W S\" as a whole word.",
        "Then we can easily find the words \"Xlê\" are always at the right of words \"WS\".",
        "So it's necessary to consider \"WSÄ^\" as a whole word."
      ]
    },
    {
      "heading": "4. Evaluation",
      "text": [
        "The performance of word segmentation is measured by test precision (P), test recall (R), F score (which is defined as 2PR/(P+R)) and the OOV recall rate.",
        "In this competition, our test corpus involved literature, computer, medicine and Finance, totally 425KB.",
        "We take 6 months data of The People's Daily to be the training corpus.",
        "From Table 1, we can see the official evaluation result.",
        "Our system got high Precision Rate and Recall Rate after testing the texts in four domains, especially Recall Rate is all over 95%.",
        "And we also could see that this system detected most new words through several measures of OOV, especially IV RR is all over 97.5%.",
        "This proved that the system could be able to get a nice result through processing professional articles in literature, computer, medicine and finance domains, and we believed it also could do well in other domains.",
        "This also proved that the method of new words detection with Chinese word segmentation was competitive.",
        "5Conclusion",
        "Through this competition, we've found a lot of problems needed to be solved in Chinese word segmentation and tried our best to improve the system.",
        "Finally, we proposed the method of new words detection in Chinese word segmentation.",
        "But we still had some shortage during the evaluation and need to improve in the future.",
        "R",
        "P",
        "F1",
        "OOV R",
        "OOV RR",
        "IV RR",
        "A-Literature",
        "0.965",
        "0.94",
        "0.952",
        "0.069",
        "0.814",
        "0.976",
        "B-Computer",
        "0.951",
        "0.926",
        "0.938",
        "0.152",
        "0.775",
        "0.982",
        "C-Medicine",
        "0.953",
        "0.913",
        "0.933",
        "0.11",
        "0.704",
        "0.984",
        "D-Finance",
        "0.963",
        "0.938",
        "0.95",
        "0.087",
        "0.758",
        "0.982"
      ]
    }
  ]
}
