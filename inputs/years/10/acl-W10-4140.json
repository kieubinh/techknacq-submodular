{
  "info": {
    "authors": [
      "Jiang Guo",
      "Wenjie Su",
      "Yangsen Zhang"
    ],
    "book": "Proceedings of the Joint Conference on Chinese Language Processing",
    "id": "acl-W10-4140",
    "title": "A domain adaption Word Segmenter for Sighan Backoff 2010",
    "url": "https://aclweb.org/anthology/W10-4140",
    "year": 2010
  },
  "references": [],
  "sections": [
    {
      "text": [
        "A domain adaption Word Segmenter",
        "Guo jiang",
        "For Sighan Bakeoff 2010",
        "Su Wenjie Institute of Intelligent Information Processing, Beijing Information Science & Technology University,",
        "Beijing, China, 100192",
        "We present a Chinese word segmentation system which ran on the closed track of the simplified Chinese Word Segmentation task of CIPS-SIGHAN-CLP 2010 bakeoffs.",
        "Our segmenter was built using a HMM.",
        "To fulfill the cross-domain segmentation task, we use semi-supervised machine learning method to get the HMM model.",
        "Finally we get the mean result of four domains: P=0.719, R=0.72"
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The 2010 Sighan Bakeoff included two types of evaluations:",
        "(1) Closed training: In the closed training evaluation, participants can only use data provided by organizers to train their systems specifically, the following data resources and software tools are not permitted to be used in the training:",
        "1) Unspecified corpus;",
        "2) Unspecified dictionary, word list or character list: include the dictionaries of named entity, character lists for specific type of Chinese named entities, idiom dictionaries, semantic lexicons, etc.",
        "3) Human-encoded rule bases;",
        "4) Unspecified software tools, include word segmenters, part-of-speech taggers, or parsers which are trained using unspecified data resources.",
        "The character type information to distinguish the following four character types can be used in training: Chinese characters, English letters, digits and punctuations.",
        "Yangsen Zhang Institute of Intelligent Information Processing, Beijing Information Science & Technology University, (2) Open training: In the open training evaluation, participants can use any language resource, including the training data provided by organizers",
        "We prefer character-based Tagging than dictionary based word segmentation in closed training, for we can only use the provide train corpus and scale of the corpus is not large enough.",
        "If we select dictionary based method we will encounter the out-of-vocabulary problem.",
        "But in character-based Tagging method we can yield a better performance than the dictionary based method for such problem."
      ]
    },
    {
      "heading": "2. Algorithm",
      "text": [
        "Ever before 2002 almost all word segment method is based on dictionary.",
        "In SIGHAN 2003 bakeoff, a character-based Tagging method was proposed and since then the character-based Tagging method became more and more popular.",
        "HMM (Hidden Markov Model) has been used extensively in speech recognition, pos tagging and get good grades.",
        "So we chose HMM as our machine learning method to fulfill our task.",
        "We formally define the elements of an HMM, and explain how the model generates an observation sequence.",
        "An HMM is characterized by the following:",
        "1) N, the number of states in the model.",
        "we denote the individual states as s={s1(s s },and the state at time t as",
        "2) M, the number of distinct observation symbols per state.",
        "we denote the individual symbols as v={v1(v v }",
        "This work was supported by the national natural science foundation of China (60873013) â€ž Beijing natural science foundation (KZ200811232019); The Open Project Program of the Key Laboratory of Computational Linguistics (Peking University), Ministry of Education;Funding Project for Academic Human Resources Development in Institutions of Higher Learning Under the Jurisdiction of Beijing Municipality (PHR201007131)",
        "The state transition probability distribution A= { } where sjjq si], 1<i,j<N.",
        "probability",
        "where Tq P qx Sj",
        "For convenience, we use the compact notation A, B, n) to indicate the complete parameter set of the model.",
        "There are three basic problems for HMM, for problem 1 we use forward-backward algorithm, for problem 2 we use Viterbi algorithm, for problem 3 we use Baum-Welch algorithm.",
        "To application HMM to our task we define the HMM five factors as blow:",
        "We define the whole labels set as Q={B, M, E, S}, B represents word's begin, M represents word's middle, E represents word's end and S represents single word.",
        "We define all Unicode characters as O We define A={alj}, where alj =P[prior token=sl|posterior label =sj] We define B={ bj k }, where bj k =P[current character= v |current5) We define a sentence as a train sample",
        "So n={sentences start with s, s Q}.",
        "Through the design we transform the character-based tagging problem to HMM problem 2.",
        "So we can solve this problem with Viterbi algorithm.",
        "Experiment",
        "We use HMM to establish the Word Segment prototype system and make use of the Labeled supplied by the Chinese Academy of Sciences to train the HMM and get the model parameters which will be used for the next iterative scaling.",
        "After that, we can get a system based on HMM model.",
        "Then, with the help of the gotten system, we process the unlabeled corpus.",
        "Once it is finished, we should add the processed corpus to the labeled corpus and get a larger corpus with which we can retrain the HMM.",
        "All these steps have been done according four test corpuses: literature, computer, medicine, finance.",
        "In the table, R indicates the recall rate, P indicates the precision rate, F1 indicates the macro average, OOV R indicates the out-of-vocabulary (OOV) rate, OOV RR indicates the out-of-vocabulary (OOV) self repair rate, IV RR indicates the out-of-vocabulary (OOV) self repair rate.",
        "In order to more easily view data, we have presented the Graph2.",
        "From the table and graph, we can see that the finance corpus has a better result, the computer corpus don't show a good result for the R, P, F1.",
        "Generally speaking, this result is a reflection for the difference between the dictionary based Tagging method and character-based Tagging method.",
        "After recheck our corpus, we can find that there are more technical terms in the computer corpus than finance corpus.",
        "The explanation for the result is that if the system encounter a technical terms, the character-based Tagging method will have a bad performance.",
        "In such situation, dictionary based Tagging method may have a better performance.",
        "For the OOV R and OOV RR, the system has a not bad performance.",
        "Table I and Graph2 show the detailed experimental data.",
        "The results of four test corpus as follow:",
        "The observation symbol distribution in state j, B={bj k }, where bj k P v at tjq sj The initial state distribution n label =sj]",
        "R",
        "P",
        "F1",
        "OO",
        "V R",
        "OO",
        "V",
        "RR",
        "IV RR",
        "literatur",
        "0.69",
        "0.74",
        "0.71",
        "0.06",
        "0.38",
        "0.71",
        "e",
        "5",
        "4",
        "9",
        "9",
        "1",
        "9",
        "Comput",
        "0.71",
        "0.64",
        "0.67",
        "0.15",
        "0.25",
        "0.79",
        "er",
        "3",
        "1",
        "5",
        "2",
        "7",
        "5",
        "medici",
        "0.73",
        "0.74",
        "0.73",
        "0.11",
        "0.37",
        "0.77",
        "ne",
        "5",
        "8",
        "8",
        "9",
        "finance",
        "0.73",
        "0.75",
        "0.74",
        "0.08",
        "0.23",
        "0.78",
        "6",
        "2",
        "4",
        "7",
        "4"
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "Our system used a HMM and semi-supervised learning for domain adapting.",
        "Our final system achieved a P=0.719, R=0.72.",
        "There exist two ways to improve our system performance one is instead our model of CRF, the other is change another way to use the unlabeled data.",
        "Because the inherent shortage of HMM we could not get a precise model, and the way we use the unlabeled data can import err to labeled data."
      ]
    }
  ]
}
