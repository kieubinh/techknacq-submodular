{
  "info": {
    "authors": [
      "Yuxiang Jia",
      "Shiwen Yu",
      "Zhengyan Chen"
    ],
    "book": "Proceedings of the Joint Conference on Chinese Language Processing",
    "id": "acl-W10-4164",
    "title": "Chinese Word Sense Induction with Basic Clustering Algorithms",
    "url": "https://aclweb.org/anthology/W10-4164",
    "year": 2010
  },
  "references": [
    "acl-W07-2002"
  ],
  "sections": [
    {
      "text": [
        "Yuxiang Jia1,2, Shiwen Yu, Zhengyan Chen",
        "Word Sense Induction (WSI) is an important topic in natural langage processing area.",
        "For the bakeoff task Chinese Word Sense Induction (CWSI), this paper proposes two systems using basic clustering algorithms, k-means and agglomerative clustering.",
        "Experimental results show that k-means achieves a better performance.",
        "Based only on the data provided by the task organizers, the two systems get FScores of 0.7812 and 0.7651 respectively."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Word Sense Induction (WSI) or Word Sense Discrimination is a task of automatically discovering word senses from un-annotated text.",
        "It is distinct from Word Sense Disambiguation (WSD) where the senses are assumed to be known and the aim is to decide the right meaning of the target word in context.",
        "WSD generally requires the use of large-scale manually annotated lexical resources, while WSI can overcome this limitation.",
        "Furthermore, automatically induced word senses can improve performance on many natural language processing tasks such as information retrieval (Uzuner et al., 1999), information extraction (Chai and Biermann, 1999) and machine translation (Vickrey et al., 2005).",
        "WSI is typically treated as a clustering problem.",
        "The input is instances of the ambiguous word with their accompanying contexts and the output is a grouping of these instances into classes corresponding to the induced senses.",
        "In other words, contexts that are grouped together in the same class represent a specific word sense.",
        "The task can be formally defined as a two stage process, feature selection and word clustering.",
        "The first stage determines which context features to consider when comparing similarity between words, while the second stage apply some process that clusters similar words using the selected features.",
        "So the simplest approaches to WSI involve the use of basic word cooccurrence features and application of classical clustering algorithms, more sophisticated techniques improve performance by introducing new context features, novel clustering algorithms, or both.",
        "(Denkowski, 2009) makes a comprehensive survey of techniques for unsupervised word sense induction.",
        "Two tasks on English Word Sense Induction were held on SemEval2007 (Agirre and Soroa, 2007) and SemEval2010 (Manandhar and Kla-paftis, 2010) respectively, which greatly promote the research of English WSI.",
        "However, the study on Chinese Word Sense Induction (CWSI) is inadequate (Zhu, 2009), and Chinese word senses have their own characteristics.",
        "The methods that work well in English may not work well in Chinese.",
        "So, as an exploration, this paper proposes simple approaches utilizing basic features and basic clustering algorithms, such as partitional method k-means and hierarchical agglomerative method.",
        "The rest of this paper is organized as follows.",
        "Section 2 briefly introduces the basic clustering algorithms.",
        "Section 3 describes the feature set.",
        "Section 4 gives experimental details and analysis.",
        "Conclusions and future work are given in Section 5."
      ]
    },
    {
      "heading": "2. Clustering Algorithms",
      "text": [
        "Partitional clustering and hierarchical clustering are the two basic types of clustering algorithms.",
        "Partitional clustering partitions a given dataset into a set of clusters without any explicit structure, while hierarchical clustering creates a hierarchy of clusters.",
        "The k-means algorithm is the most notable partitional clustering method.",
        "It takes a simple two step iterative process, data assignment and relocation of means, to divide the dataset into a specified number of clusters, k.",
        "Hierarchical clustering algorithms are either top-down or bottom-up.",
        "Bottom-up algorithms treat each instance as a singleton cluster at the beginning and then successively merge pairs of clusters until all clusters have been merged into a single cluster.",
        "Bottom-up clustering is also called hierarchical agglomerative clustering, which is more popular than top-down clustering.",
        "We use k-means and agglomerative algorithms for the CWSI task, and compare the performances of the two algorithms.",
        "Estimating the number of the induced clusters, k, is difficult for general clustering problems.",
        "But in CWSI, it is simplified because the sense number of the target word is given beforehand.",
        "CLUTO (Karypis, 2003), a clustering toolkit, is used for implementation.",
        "The similarity between objects is computed using cosine function.",
        "The criterion functions for k-means and agglomerative algorithms are I2 and UPGMA respectively.",
        "Biased agglomerative approach is chosen in stead of the traditional agglomerative approach."
      ]
    },
    {
      "heading": "3. Feature Set",
      "text": [
        "For each target word, instances are extracted from the XML data file.",
        "Then the encoding of the instance file is transformed from UTF-8 to GB2312.",
        "Word segmentation and part-of-speech tagging is finished with the tool ICTCLAS .",
        "Then the following three types of features are extracted:"
      ]
    },
    {
      "heading": "1.. The part-of-speech of the target word",
      "text": [
        "2.",
        "Words before and after the target word within window of size 3 with position information",
        "3.",
        "Unordered single words in all the contextual sentences without the target word, punctuations and symbols of the part-of-speech \"nx\" (Each word is only counted once, which is different from the word frequency in the bag-of-words model)",
        "The target word is not necessarily a segmented word.",
        "Their relations are as follows:",
        "Don't dial my phone.",
        "The target word is \"fr\" (dial) and the segmented word is also \"fr\" (dial).",
        "So they match.",
        "2.",
        "The target word is inside of a segmented word.",
        "The target word is \"r\" (deal), but the segmented word is \"fr3£lË\" (deal with).",
        "Then we split the segmented word and specify the part-of-speech of the target word as \"1\".",
        "3.",
        "The target word is the combination of two segmented words.",
        "E.g. £/v sjj/v \"/w JCitX^-lw/nz \"/w launching the \"Culture Revolution\"",
        "The target word is \"^zft\" (launching), but it is split into two segmented words (start) and \"zft \" (move).",
        "Then we combine the two segmented words and specify the part-of-speech of the target word as \"2\".",
        "4.",
        "The target word is split into two segmented words.",
        "E.g. #J/v fe/v T/u £/j ^M/n blow up northeast wind",
        "The target word is but it is segmented into two words (east) and \";)kM\" (north wind).",
        "In this case, we specify the postion of first segmented word as the position of the target word and the part-of-speech of the target word as \"3\".",
        "If the target word occurs more than once in an instance, we consider the first occurrence."
      ]
    },
    {
      "heading": "4. Experiments 4.1 Data Sets",
      "text": [
        "Two data sets are provided.",
        "The trial set contains 50 target words and 50 examples for each target word.",
        "The test set consists of 100 new target word and 50 examples for each target word.",
        "Both data sets are collected from the internet.",
        "Table 1 shows the distribution of sense numbers of the target words in the two data sets.",
        "We can see that two sense words dominate and three sense words are the second majority.",
        "The word \"fr\" (beat) in the trial set has 21 senses.",
        "_words and segmented words _",
        "As is shown in table 2, the total instance number in the trial set is 2499 because there is a target word has only 49 instances.",
        "About 7.4% of the instances in the trial set and 19.38% of the instances in the test set have mismatched target words and segmented words (with relation types 2, 3 and 4).",
        "The official performance metric for the CWSI task is FScore (Zhao and Karypis, 2005).",
        "Given a particular class Ci of size n and a cluster Sr of size nr, suppose rir examples in the class Ci belong to Sr.",
        "The F value of this class and cluster is defined to be:",
        "where P(Ci, Sr ) = – - is the precision value and R(Ci, Sr ) = – - is the recall value defined for class Ci and cluster Sr.",
        "The FScore of class Ci is the maximum F value attained at any cluster, that is and the FScore of the entire clustering solution is where c is the number of classes and n is the size of the clustering solution.",
        "Another two metrics, Entropy and Purity (Zhao and Karypis, 2001), are also employed in this paper to measure our system performance.",
        "Entropy measures how the various classes of word senses are distributed within each cluster, while Pur ty measures the extent to which each cluster contained word senses from primarily one class.",
        "The entropy of cluster Sr is defined as",
        "The entropy of the entire clustering solution is then defined to be the sum of the individual cluster entropies weighted according to the cluster size.",
        "That is",
        "The purity of a cluster is defined to be P(Sr ) = -max(nr ), which is the fraction of the overall cluster size that the largest class of examples assigned to that cluster represents.",
        "The overall purity of the clustering solution is obtained as a weighted sum of the individual cluster purities and is given by",
        "In general, the larger the values of FScore and Pur ty, the better the clustering solution is.",
        "The smaller the Entropy values, the better the clustering solution is.",
        "The above three metrics are defined to evaluate the result of a single target word.",
        "Macro average metrics are used to evaluate the overall performance of all the target words.",
        "The overall performance on the trial data is shown in table 3.",
        "From the Macro Average Entropy and Macro Average Purity, we can see that k-means works better than agglomerative method.",
        "The detailed results of the k-means system are shown in table 4.",
        "Entropy = Y – E (Sr )",
        "Entropy",
        "Purity",
        "k-means",
        "0.4858",
        "0.8288",
        "agglomerative",
        "0.5328",
        "0.8020",
        "The official results on the test set are shown in table 5.",
        "Our k-means system and agglomerative system rank 5 and 8 respectively among all the 18 systems."
      ]
    },
    {
      "heading": "5. Conclusions and Future Work",
      "text": [
        "This paper tries to build basic systems for Chinese Word Sense Induction (CWSI) task.",
        "Basic clustering algorithms including k-means and agglomerative methods are studied.",
        "No extra language resources are used except the data given by the task organizers.",
        "To improve the performance of CWSI systems, we will introduce new features and study novel clustering algorithms.",
        "We will also investigate the bakeoff data sets to find some more characteristics of Chinese word senses."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The authors are grateful to the organizers of the Word Sense Induction task for their hard work to provide such a good research platform.",
        "The work in this paper is supported by grants from the National Natural Science Foundation of China (No.60773173, No.60970083).",
        "TargetWord",
        "SenseNum",
        "Entropy",
        "Purity",
        "Mil",
        "2",
        "0.855",
        "0.72",
        "UM",
        "2",
        "0.692",
        "0.78",
        "RM",
        "2",
        "0.377",
        "0.92",
        "3",
        "0.207",
        "0.94",
        "2",
        "0.833",
        "0.7",
        "2",
        "0",
        "1",
        "2",
        "0.592",
        "0.82",
        "2",
        "0.245",
        "0.959",
        "2",
        "0.116",
        "0.98",
        "3",
        "0.396",
        "0.82",
        "^a",
        "2",
        "0.201",
        "0.96",
        "2",
        "0.201",
        "0.96",
        "3",
        "0.181",
        "0.9",
        "2",
        "0.122",
        "0.98",
        "2",
        "0.327",
        "0.92",
        "AA",
        "2",
        "0.653",
        "0.82",
        "An",
        "2",
        "0",
        "1",
        "AK",
        "2",
        "0.855",
        "0.72",
        "A¥",
        "2",
        "0.5",
        "0.8",
        "frn",
        "2",
        "0.312",
        "0.92",
        "fr«",
        "2",
        "0.519",
        "0.86",
        "frjp",
        "3",
        "0.534",
        "0.72",
        "frm",
        "2",
        "0.846",
        "0.7",
        "fr",
        "21",
        "0.264",
        "0.48",
        "2",
        "0.521",
        "0.88",
        "3",
        "0",
        "1",
        "2",
        "0.76",
        "0.78",
        "tön",
        "3",
        "0.205",
        "0.92",
        "2",
        "0.854",
        "0.72",
        "Wife",
        "2",
        "0.449",
        "0.9",
        "2",
        "0.467",
        "0.9",
        "2",
        "0.881",
        "0.7",
        "MS",
        "2",
        "0.402",
        "0.92",
        "ëff",
        "2",
        "0.39",
        "0.92",
        "TO",
        "2",
        "0.793",
        "0.76",
        "##n",
        "2",
        "0.904",
        "0.68",
        "2",
        "0.943",
        "0.64",
        "3",
        "0.548",
        "0.74",
        "2",
        "0.583",
        "0.86",
        "2",
        "0.999",
        "0.52",
        "2",
        "0.242",
        "0.96",
        "2",
        "0.75",
        "0.74",
        "3",
        "0.464",
        "0.84",
        "2",
        "0.181",
        "0.96",
        "WÄ",
        "2",
        "0.672",
        "0.78",
        "2",
        "0.471",
        "0.82",
        "3",
        "0.543",
        "0.7",
        "2",
        "0.347",
        "0.9",
        "4",
        "0.508",
        "0.66",
        "2",
        "0.583",
        "0.86",
        "Rank",
        "FScore",
        "Rank",
        "FScore",
        "1",
        "0.7933",
        "6",
        "0.7788",
        "2",
        "0.7895",
        "7",
        "0.7729",
        "3",
        "0.7855",
        "8*",
        "0.7651",
        "4",
        "0.7849",
        "9",
        "0.7598",
        "5*",
        "0.7812",
        "18",
        "0.5789"
      ]
    }
  ]
}
