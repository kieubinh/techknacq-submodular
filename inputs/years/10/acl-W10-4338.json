{
  "info": {
    "authors": [
      "Meritxell González",
      "Silvia Quarteroni",
      "Giuseppe Riccardi",
      "Sebastian Varges"
    ],
    "book": "Proceedings of the SIGDIAL 2010 Conference",
    "id": "acl-W10-4338",
    "title": "Cooperative User Models in Statistical Dialog Simulators",
    "url": "https://aclweb.org/anthology/W10-4338",
    "year": 2010
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Statistical user simulation is a promising methodology to train and evaluate the performance of (spoken) dialog systems.",
        "We work with a modular architecture for data-driven simulation where the \"intentional\" component of user simulation includes a User Model representing user-specific features.",
        "We train a dialog simulator that combines traits of human behavior such as cooperativeness and context with domain-related aspects via the Expectation-Maximization algorithm.",
        "We show that cooperativeness provides a finer representation of the dialog context which directly affects task completion rate."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Data-driven techniques are a promising approach to the development of robust (spoken) dialog systems, particularly when training statistical dialog managers (Varges et al., 2009).",
        "User simulators have been introduced to cope with the scarcity of real user conversations and optimize a number of SDS components (Schatzmann et al., 2006).",
        "In this work, we investigate the combination of aspects of human behavior with contextual aspects of conversation in a joint yet modular data-driven simulation model.",
        "For this, we integrate conversational context representation, centered on a Dialog Act and a Concept Model, with a User Model representing persistent individual features.",
        "Our aim is to evaluate different simulation regimes against real dialogs to identify any impact of user-specific features on dialog performance.",
        "In this paper, Section 2 presents our simulator architecture and Section 3 focuses on our model of cooperativeness.",
        "Our experiments are illustrated in Section 4 and conclusions are summarized in Section 5.",
        "Work partly funded by EU project ADAMACH (022593) and Spanish project OPENMT-2 (TIN2009-14675-C03)."
      ]
    },
    {
      "heading": "2. Simulator Architecture",
      "text": [
        "Data-driven simulation takes place within the rule-based version of the ADASearch system (Varges et al., 2009), which uses a taxonomy of 16 dialog acts and a dozen concepts to deal with three tasks related to tourism in Trentino (Italy): Lodging Enquiry, Lodging Reservation and Event Enquiry.",
        "Simulation in our framework occurs at the intention level, where the simulator and the Dialog Manager (DM) exchange actions, i.e. ordered sequences of dialog acts and a number of concept-value pairs.",
        "In other words, we represent the DM action as as = {da0, ..,dan}, (s is for \"System\") where daj is short for a dialog act defined over zero or more concept-value pairs, daj(co(vo), ..,cm{vm)).",
        "In response to the DM action as, the different modules that compose the User Simulator generate an N-best list of simulated actions Au(as) = {a°u,.., }.",
        "The probability of each possible action being generated after the DM action as is estimated based on the conversation context.",
        "Such a context is represented by a User Model, a Dialog Act Model, a Concept Model and an Error Model (Quarteroni et al., 2010).",
        "The User Model simulates the behavior of an individual user in terms of goals and other behavioral features such as cooperativeness and tendency to hang up.",
        "The Dialog Act Model generates a distribution of M actions Au = {aU,.., af}.",
        "Then, one action au is chosen out of Au.",
        "In order to vary the simulation behavior, the choice of the user action aau is a random sampling according to the distribution of probabilities therein; making the simulation more realistic.",
        "Finally, the Concept Model generates concept values for aau; and the Error Model simulates the noisy ASR-SLU channel by \"distorting\" aau.",
        "These models are derived from the ADASearch",
        "<ecial Interest Group on Discourse and Dialogue, pages 217-220, ©2010 Association for Computational Linguistics",
        "dataset, containing 74 spoken human-computer conversations.",
        "The User Model represents user-specific features, both transient and persistent.",
        "The transient feature we focus on in this work is the user's goal in the dialog (UG), represented as a task name and the list of concepts and values required to fulfill it: an example of UG is {Activity(EventEnquiry), Time_day(2), Time_month(may), Event_type(fair), Loca-tion_name(Povo)}.",
        "Persistent features included in our model so far are: patience, silence (no input) and cooperativeness.",
        "Patience pat is defined as the tendency to abandon the conversation (hang up event), i.e. pat = P(HangUp\\as).",
        "Similarly, NoInput probability noi is used to account for user behavior in noisy environments: noi = P(NoInput\\as).",
        "Finally, cooperativeness coop is a real value representing the ratio of concepts mentioned in as that also appear in aau (see Section 3).",
        "We define three Dialog Act (DA) Models: Obedient (OB), Bigram (BI) and Task-based (TB).",
        "In the Obedient model, total patience and cooperativeness are assumed of the user, who will always respond to each query requiring values for a set of concepts with an answer concerning exactly such concepts.",
        "Formally, the model responds to a DM action as with a single user action aau obtained by consulting a rule table, having probability 1.",
        "In case a request for clarification is issued by the DM, this model returns a clarifying answer.",
        "Any offer from the DM to continue the conversation will be either readily met with a new task request or denied at a fixed probability: Au(as) = {(â«, 1)}.",
        "In the Bigram model, first defined in (Eckert et al., 1997), a transition matrix records the frequencies of transition from DM actions to user actions, including hang up and no input/no match.",
        "Given a DM action as, the model responds with a list of M user actions and their probabilities estimated according to action distribution in the real data:",
        "Au(as) = {(aU,P(aU|as)),.., (af ,P(af |as))}.",
        "The Task-based model, similarly to the \"goal\" model in (Pietquin, 2004), produces an action distribution containing only the actions observed in the dataset of dialogs in the context of a specific task Tk.",
        "The TB model divides the dataset into one partition for each Tk, then creates a task-specific bigram model, by computing V k:",
        "Au(as) = {(aU,P(aU|a„Tfc)),.., (af, P(af |as,Tk))}.",
        "As the partition of the dataset reduces the number of observations, the TB model includes a mechanism to back off to the simpler bigram and unigram models.",
        "The Concept Model takes the action aau selected by the DA Model and attaches values and sampled interpretation confidences to its concepts.",
        "In this work, we adopt a Concept Model which assigns the corresponding User Goal values for the required concepts, which makes the user simulated responses consistent with the user goal.",
        "The Error Model is responsible of simulating the noisy communication channel between user and system; as we simulate the error at SLU level, errors consist of incorrect concept values.",
        "We experiment with a data-driven model where the precision Prc obtained by a concept c in the reference dataset is used to estimate the frequency with which an error in the true value v of c will be introduced: P(c(v)\\c(v)) = 1 – Prc (Quarteroni et al., 2010)."
      ]
    },
    {
      "heading": "3. Modelling Cooperativeness",
      "text": [
        "As in e.g. (Jung et al., 2009), we define cooperativeness at the turn level (coopt) as a function of the number of dialog acts in the DM action as sharing concepts with the dialog acts in the user action au; at the dialog level, coop is the average of turn-level cooperativeness.",
        "We discretize coop into a binary variable reflecting high vs low cooperativeness based on whether or not the dialog cooperativeness exceeds the median value of coop found in a reference corpus; in our ADASearch dataset, the median value found for coop is 0.28; hence, we annotate dialogs as cooperative if they exceed such a threshold, and as uncooperative otherwise.",
        "Using a corpus threshold allows domain-and population-driven tuning of cooperativeness rather than a \"hard\" definition (as in (Jung et al., 2009)).",
        "We then model cooperativeness as two bigram models, reflecting the high vs low value of coop.",
        "In practice, given a DM action as and the coop value (k = high/low) we obtain a list of user actions and their probabilities:",
        "Au(aa,K) = {(aU,P(aHas,n)),.., (af ,P(af |a„«))}.",
        "At this point, the distribution Au(as, k) is linearly interpolated with the distribution of actions Au(as, tp) obtained using the DA model tp (in the Task-based DA model; ^ can have three values, one for each task as explained in Section 2.2):",
        "where AK and are the weights of each feature/model and A^, + Are = 1.",
        "-J high low",
        "For each user action at,, \\K and A estimated using the Baum-Welch Expectation-Maximization algorithm as proposed by (Jelinek and Mercer, 1980).",
        "We use trie distributions of actions obtained from our dataset and we align the set of actions of the two models.",
        "Since we only have two models, we only need to calculate expectation for one of the distributions:",
        "WM i \"i=Oau",
        "where M is the number of actions.",
        "Then, the weights Are and A^, that maximize the data likelihood are calculated as follows:",
        "The resulting combined distribution Au(as) is obtained by factoring the probabilities of each action with the weight estimated for the particular distribution:",
        "To assess the effect of the cooperativeness feature in the final distribution of actions, we set a 5-fold cross-validation experiment with the ADASearch dataset where we average the AK estimated at each turn of the dialog.",
        "We investigated in which context cooperativeness provides more contribution by comparing the AK weights attributed by high vs. low coop models to user action distributions in response to Dialog Manager actions.",
        "Figure 1 shows the values achieved by AK for several DM actions for high vs low coop regimes.",
        "We can see that AK achieves high values in case of uncooperative users in response to DM dialog acts as [ClarificationRequest] and [Info-request].",
        "In contrast, forward-looking actions, such as the ones including [Offer], seem to discard the contribution of the low coop model, but to favor the contribution provided by high coop.",
        "Figure 1 : Estimated AK weights in response to selected DM actions in case of high/low coop"
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We evaluate our simulator models using two methods: first, \"offline\" statistics are used to assess how realistic the action estimations by DA Models are with respect to a dataset of real conversations (Sec.",
        "4.1); then, \"online\" statistics (Sec.",
        "4.2) evaluate end-to-end simulator performance in terms of dialog act distributions, error robustness and task duration and completion rates by comparing real dialogs with fresh simulated dialogs using action sampling in the different simulation models.",
        "In order to compare simulated and real user actions, we evaluate dialog act Precision (Pda) and Recall (Rda) following the methodology in (Schatzmann et al., 2005).",
        "For each DM action as the simulator picks a user action âu from Au(as) and we compare it with the real user choice au.",
        "A simulated dialog act is correct when it appears in the real action au.",
        "The measurements were obtained using 5-fold cross-validation on the ADASearch dataset.",
        "Table 1 : Dialog Act Precision and Recall",
        "Table 1 shows Pda/Rda obtained for the OB, BI and TB models alone and with cooperativeness models (+coop).",
        "First, we see that TB is much better than BI and OB at reproducing real action selection.",
        "This is also visible in both Pda and Rda obtained by selecting a*, the most frequent user action from the As generated by each model.",
        "By definition, a* maximizes the expected Pda and Rda, providing an upper bound for our models; however, to reproduce any possible user behavior, we need to sample au rather than choosing it by frequency.",
        "By now inspecting (+coop) values in Table 1, we see that explicit cooperativeness models match real dialogs more closely.",
        "It points out that partitioning the reference dataset in high vs low coop sets allows better data representation.",
        "There is however no improvement in the a*u case: we explain this by the fact that by \"slicing\" the reference dataset, the cooperative model augments data sparsity, affecting robustness.",
        "Simulation (a* )",
        "Most frequent (a* )",
        "DA Model",
        "Pda",
        "Rda",
        "Pda",
        "Rda",
        "OB",
        "33.8",
        "33.4",
        "33.9",
        "33.5",
        "BI (+coop)",
        "35.6 (35.7)",
        "35.5 (35.8)",
        "49.3 (47.9)",
        "48.8 (47.4)",
        "TB (+coop)",
        "38.2 (39.7)",
        "38.1 (39.4)",
        "51.1 (50.6)",
        "50.6 (50.2)",
        "We now discuss online deployment of our simulation models with different user behaviors and \"fresh\" user goals and data.",
        "To align with the ADASearch dataset, we ran 60 simulated dialogs between the ADASearch DM and each combination of the Task-based and Bigram models and high and low values of coop.",
        "For each set of simulated dialogs, we measured task duration, defined as the average number of turns needed to complete each task, and task completion rate, defined as:",
        "TCR _ number of times a task has been completed",
        "total number of task requests '",
        "Table 2 reports such figures in comparison to the ones obtained for real dialogs from the ADASearch dataset In general, we see that task duration is closer to real dialogs in the Bigram and Task-based models when compared to the Obedient model Moreover, it can easily be observed in both BI and TB models that under high-coop regime (in boldface), the number of turns taken to complete tasks is lower than under low-coop.",
        "Furthermore, in both TB and BI models, TCR is higher when cooperativeness is higher, indicating that cooperative users make dialogs not only shorter but also more efficient."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this work, we address data-driven dialog simulation for the training of statistical dialog managers.",
        "Our simulator supports a modular combination of user-specific features with different models of dialog act and concept-value estimation, in addition to ASR/SLU error simulation.",
        "We investigate the effect of joining a model of user intentions (Dialog Act Model) with a model of individual user traits (User Model).",
        "In particular, we represent the user's cooperativeness as a real-valued feature of the User Model and create two separate simulator behaviors, reproducing high and low cooperativeness.",
        "We explore the impact of combining our cooperativeness model with the Dialog Act model in terms of dialog act accuracy and task success.",
        "We find that 1) an explicit modelling of user cooperativeness contributes to an improved accuracy of dialog act estimation when compared to real conversations; 2) simulated dialogs with high cooperativeness result in higher task completion rates than low-cooperativeness dialogs.",
        "In future work, we will study yet more fine-grained and realistic User Model features.",
        "Model",
        "Lodging Enquiry #turns TCR",
        "Lodging Reserv #turns TCR",
        "Event Enquiry #turns TCR",
        "All TCR",
        "OB",
        "9.2±0.0 78.1",
        "9.7±1.4 82.4",
        "8.1±2.9 66.7",
        "76.6",
        "BI+low BI+high",
        "15.1±4.1 71.4 12.1±2.5 74.6",
        "14.2±3.9 69.4 12.9±3.1 82.9",
        "9.3±1.8 52.2 7.8±1.8 75.0",
        "66.7 77.4",
        "TB+low TB+high",
        "13.6±4.1 75.8 11.6±2.8 80.0",
        "13.4±3.7 83.3 12.6±3.6 83.7",
        "8.4±3.3 64.7 6.5±1.9 57.1",
        "77.2 78.4",
        "Real dialogs",
        "11.1±3.0 71.4",
        "12.7±4.7 69.6",
        "9.3±4.0 85.0",
        "73.4"
      ]
    }
  ]
}
