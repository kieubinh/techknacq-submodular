{
  "info": {
    "authors": [
      "Federico Sangati",
      "Willem Zuidema"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1008",
    "title": "Accurate Parsing with Compact Tree-Substitution Grammars: Double-DOP",
    "url": "https://aclweb.org/anthology/D11-1008",
    "year": 2011
  },
  "references": [
    "acl-C92-3126",
    "acl-C96-2215",
    "acl-D07-1058",
    "acl-E03-1005",
    "acl-E09-1080",
    "acl-J02-1005",
    "acl-J93-2004",
    "acl-N07-1051",
    "acl-P01-1010",
    "acl-P02-1034",
    "acl-P03-1054",
    "acl-P05-1010",
    "acl-P05-1022",
    "acl-P06-1055",
    "acl-P09-2012",
    "acl-P10-1096",
    "acl-P10-1112",
    "acl-W96-0214"
  ],
  "sections": [
    {
      "text": [
        "Federico Sangati and Willem Zuidema",
        "Institute for Logic, Language and Computation University of Amsterdam Science Park 904, 1098 XH Amsterdam, The Netherlands",
        "{f.sangati,zuidema}@uva.nl",
        "We present a novel approach to Data-Oriented Parsing (DOP).",
        "Like other DOP models, our parser utilizes syntactic fragments of arbitrary size from a treebank to analyze new sentences, but, crucially, it uses only those which are encountered at least twice.",
        "This criterion allows us to work with a relatively small but representative set of fragments, which can be employed as the symbolic backbone of several probabilistic generative models.",
        "For parsing we define a transform-backtransform approach that allows us to use standard PCFG technology, making our results easily replicable.",
        "According to standard Parseval metrics, our best model is on par with many state-of-the-art parsers, while offering some complementary benefits: a simple generative probability model, and an explicit representation of the larger units of grammar."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Data-oriented Parsing (DOP) is an approach to wide-coverage parsing based on assigning structures to new sentences using fragments of variable size extracted from a treebank.",
        "It was first proposed by Scha in 1990 and formalized by Bod (1992), and preceded many developments in statistical parsing (e.g., the \"treebank grammars\" of Charniak 1997) and linguistic theory (e.g., the current popularity of \"constructions\", Jackendoff 2002).",
        "A rich literature on DOP has emerged since, yielding state-of-the-art results on the Penn treebank benchmark test (Bod, 2001; Bansal and Klein, 2010) and inspiring developments in related frameworks including tree kernels (Collins and Duffy, 2002), reranking (Charniak and Johnson, 2005) and Bayesian adaptor and fragment grammars (e.g., Johnson et al., 2007; O'Donnell et al., 2009; Cohn et al., 2010).",
        "By formalizing the idea of using large fragments of earlier language experience to analyze new sentences, DOP captures an important property of language cognition that has shaped natural language.",
        "It therefore complements approaches that have focused on properties like lexicalization or incrementality, and might bring supplementary strengths in other NLP tasks.",
        "Early versions of DOP (e.g., Bod et al., 2003) aimed at extracting all subtrees of all trees in the treebank.",
        "The total number of constructions, however, is prohibitively large for non-trivial treebanks: it grows exponentially with the length of the sentences, yielding the astronomically large number of approximately 10 for section 2-21 of the Penn WSJ corpus.",
        "These models thus rely on a big sample of fragments, which inevitably includes a substantial portion of overspecialized constructions.",
        "Later DOP models have used the Goodman transformation (Goodman, 1996, 2003) to obtain a compact representation of all fragments in the treebank (Bod, 2003; Bansal and Klein, 2010).",
        "In this case the grammatical constructions are no longer explicitly represented, and substantial engineering effort is needed to optimally tune the models and make them efficient.",
        "In this paper we present a novel DOP model (Double-DOP) in which we extract a restricted yet representative subset of fragments: those recurring at least twice in the treebank.",
        "The explicit representation of the fragments allows us to derive simple ways of estimating probabilistic models on top of the symbolic grammar.",
        "This and other implementation choices aim at making the methodology transparent and easily replicable.",
        "The accuracy of Double-DOP is well within the range of state-of-the-art parsers currently used in other NLP-tasks, while offering the additional benefits of a simple generative probability model and an explicit representation of grammatical constructions.",
        "The contributions of this paper are summarized as follows: (i) we describe an efficient tree-kernel algorithm which allows us to extract all recurring fragments, reducing the set of potential elementary units from the astronomical 10 to around 10.",
        "(ii) We implement and compare different DOP estimation techniques to induce a probability model (PTSG) on top of the extracted symbolic grammar.",
        "(iii) We present a simple transformation of the extracted fragments into CFG-rules that allows us to use off-the-shelf PCFG parsing and inference.",
        "(iv) We integrate Double-DOP with recent state-splitting approaches (Petrov et al., 2006), yielding an even more accurate parser and a better understanding of the relation between DOP and state-splitting.",
        "The rest of the paper is structured as follows.",
        "In section 2 we describe the symbolic backbone of the grammar formalism that we will use for parsing.",
        "In section 3 we illustrate the probabilistic extension of the grammar, including our transformation of PTSGs to PCFGs that allows us to use a standard PCFG parser, and a different transform that allows us to use a standard implementation of the inside-outside algorithm.",
        "In section 4 we present the experimental setup and the results."
      ]
    },
    {
      "heading": "2. The symbolic backbone",
      "text": [
        "The basic idea behind DOP is to allow arbitrarily large fragments from a treebank to be the elementary units of production of the grammar.",
        "Fragments can be combined through substitution to obtain the phrase-structure tree of a new sentence.",
        "Figure 1 shows an example of a complete syntactic tree obtained by combining three elementary fragments.",
        "As in previous work, two fragments fi and fj can be combined (fi o fj) only if the leftmost substitution site X\\.",
        "in fi has the same label as the root node of fj; in this case the resulting tree will correspond to fi with fj replacing X.",
        "The DOP formalism is discussed in detail in e.g., Bod et al.",
        "(2003).",
        "o DT NNP NNP o JJ NN I",
        "vbd np^ I I I black arm bands",
        "The Free French",
        "DT NNP NNP VBD NP black arm bands",
        "Figure 1: An example of a derivation of a complete syntactic structure (below) obtained combining three elementary fragments (above) by means of the substitution operation o.",
        "Substitution sites are marked with 4.",
        "The first step to build a DOP model is to define its symbolic grammar, i.e. the set of elementary fragments in the model.",
        "In the current work we explicitly extract a subset of fragments from the training treebank.",
        "To limit the fragment set size, we use a simple but heretofore unexplored constraint: we extract only those fragments that occur two or more times in the treebank.",
        "Extracting this particular set of fragments is not trivial, though: a naive approach that filters a complete table of fragments together with their frequencies fails because that set, in a reasonably sized treebank, is astronomically large.",
        "Instead, we use a dynamic programming algorithm based on tree-kernel techniques (Collins and Duffy, 2001; Moschitti, 2006; Sangati et al., 2010).",
        "The algorithm iterates over every pair of trees in",
        "PRP VBP ADJP they are JJ ready",
        "Analysts say",
        "NNP BBZ USAir has great promise",
        "Figgur-^ 2: Left: example of two trees sharing e single maximum fragment, circled in the two srees.",
        "Righe: the chart M which is used in the dynamic algorithm to exaract all maximum fragments shared beeween the two trees.",
        "The highlighted cells in the chart are the ones which contribute to extract the shared fragment.",
        "Tha marked cells are those fo° which the corresponding nodes in the two tree have equivalent labels but differ in their lists erf child no des.",
        "the treebhnk to look eor common fragments.",
        "Figure 2 shows an example of a pair oa frees (a, h) being compares.",
        "The dgorithm builds a chart M with one column tor every indexed non-terminal node ai in a, and one rove for every indexed non-terminal node ßj in ß.",
        "Each ceil M{i, g) identifies a set oa indices corresponding to the largest fragmena id coo m -mon between the two trees starting from ai andßj.",
        "Thisset: if empty if ai and ßj differ in their labels, or they don't have the same list o) child nodes.",
        "Oih-erwife (iff both th0 labels and trie lists of crtildren match) the set is computed recursively as follows:",
        "where ch(af returnf the indices of a's children, and ch(a, c) the index of its cth child.",
        "Alter filling the cliart:, ^^orlth^:^ extracts the set of recurring fragments, and stofr'^s themin a table So keep track of then\" counte.",
        "This is done by converting back each fragment implicitly defined in every cell-set, and filtering out those )hat are properly contained in others.",
        "In a second pass over the treebank, exact counts are obtained for each fragment in thp extracted set.",
        "Parse erees in the training porpue are not necessarily covered entirely by recurring fragments; to ensure coveragf, we also include in the symbolic backbone of our Double-DOP model all PCFGtproductions not included in the set of extracted fragments.",
        "Exp licit gramnws The number oof recurring fragments in our symbotic grammar, extracted from Ohe training sections of she Penn WSJ treebhnk, is around t million, fnd lhut is fignifigantly lower than previous work extracting explicit fragments (e.g., BoP, 2001, used more than 5 million fragments up to depth 04).",
        "When looking at the extracted fragmenis we ask if we could have predicted which fragments occur twice oi\" more.",
        "Figure 3 attempts to tackle this question by reporting oome statistics on the extracted fragmfntt.",
        "The mteprity of fragmente are rather tmall with a limited number of words or subptihition sites in the fixmtier.",
        "YeS, there is a significant portion oa fragments, in the tail o° the distribution, with more than 10 words or substitution gites.",
        "Since the spaœ of all fragments with such characteristics is enormously large, selecting big recurring grPgments using random sampling technique is like finding a needle in a haystack.",
        "Hence, random sampling pro-œsses (like Bod, 2001), will tend to represent frequent recurring constructions such as from NP to NP or whether S or not, together with infrequent overspecialized fragments like from Houston to NP, while missing large generic constructions such as everything you always wanted to know about NP but were afraid to ask.",
        "These large constructions are excluded completely by models that only allow elementary trees up to a certain depth (typically 4 or 5) into the symbolic grammar (Zollmann and Sima'an, 2005; Zuidema, 2007; Borensztajn et al., 2009), or only elementary trees with exactly one lexical anchor (Sangati and Zuidema, 2009).",
        "Depth / Words / Substitution Sites",
        "Figure 3: Distribution of the recurring fragments types according to several features: depth, number of words, and number of substitution sites.",
        "Their corresponding curves peak at 4 (depth), 1 (words), and 4 (substitution sites).",
        "Implicit grammars Goodman (1996, 2003) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees.",
        "This transform, representing larger fragments only implicitly, is used in most recent DOP parsers (e.g., Bod, 2003; Bansal and Klein, 2010).",
        "Bod has promoted the Goodman transform as the solution to the computational challenges of DOP (e.g., Bod, 2003); it's important to realize, however, that the resulting grammars are still very large: WSJ sections 2-21 yield about 2.5 million rules in the basic version of Goodman's transform.",
        "Moreover, the transformed grammars differ from untransformed DOP grammars in that larger fragments are no longer explicitly represented.",
        "Rather, information about their frequency is distributed over many CFG-rules: if a construction occurs n times and contains m context-free productions, Goodman's transform uses the weights of 7nm + m rules to encode this fact.",
        "Thus, the information that the idiomatic fragment (PP (IN \"out\") (PP (IN \"of\") (NP (NN \"town\"))))) occurs 3 times in WSJ sections 2-21, is distributed over 132 rules.",
        "This way, an attractive feature of DOP, viz. the explicit representation of the 'productive units' of language, is lost.",
        "In addition, grammars that implicitly encode all fragments found in a treebank are strongly biased to over-represent big constructions: the great majority of the entire set of fragments belongs in fact to the largest tree in the treebank.",
        "DOP models relying on Goodman's transform, need therefore to counteract this tendency.",
        "Bansal and Klein (2010), for instance, rely on a sophisticated tuning technique to correctly adjust the weights of the rules in the grammar.",
        "In our Double-DOP approach, instead, the number of fragments extracted from each tree varies much less (it ranges between 4 and 1,759).",
        "This comparison is shown in figure 4."
      ]
    },
    {
      "heading": "3. The probabilistic model",
      "text": [
        "Like CFG grammars, our symbolic model produces extremely many parse trees for a given test sentence.",
        "We therefore need to disambiguate between the possible parses by means of a probability model that assigns probabilities to fragments, and defines a proper distribution over the set of possible full parse trees.",
        "For every nonterminal X in the treebank we have:",
        "where FX is the set of fragments in our symbolic grammar rooted in X.",
        "A derivation d = f i, f 2,..., fn of t is a sequence of the fragments that through leftmost substitution produces t. The probability of a derivation is computed as the product of",
        "Depth – i-",
        "7 *\\",
        "Words Substitution Sites",
        "k \\",
        "k X",
        "- '\\%",
        "k \\ i \\ x xx hrx /x >",
        "+^ \\ / \\ x x \\ / XX X",
        "Figure 4: Number of fragments extracted from each tree in sections 2-21 of the WSJ treebank, when considering all-fragments (dotted line) and recurring-fragments (solid line).",
        "Trees on the x-axis are ranked according to the number of fragments.",
        "Note the double logarithmic scale on the y-axis.",
        "the probability of each of its fragments.",
        "In section 3.2 we describe ways of obtaining different probability distributions over the fragments in our grammar.",
        "In the following section we assume a given probabilistic model, and illustrate how to use standard PCFG parsing.",
        "It is possible to define a simple transform of our probabilistic fragment grammar, such that off-the-shelf parsers can be used.",
        "In order to perform the PTSG/PCFG conversion, every fragment in our grammar must be mapped to a CFG rule which will keep the same probability as the original fragment.",
        "The corresponding rule will have as the left hand side the root of the fragment and as the right hand side its yield, i.e., a sequence of terminals and nonterminals (substitution sites).",
        "It might occur that several fragments are mapped to the same CFG rule.",
        "These are interesting cases of syntactic ambiguity as shown in figure 5.",
        "In order to resolve this problem we need to map each ambiguous fragment to two unique CFG rules chained by a unique artificial node, as shown at the bottom of the same figure.",
        "To the first CFG rule in the chain we assign the probability of the fragment, while the second will receive probability 1, so the product gives back the original probability.",
        "The ambiguous and unambiguous PTSG/PCFG mappings need to be stored in a table, in order to convert back the compressed CFG derivations to the original PTSG model after parsing.",
        "Such a transformed PCFG will generate the same derivations as the original PTSG grammar with identical probabilities.",
        "In our experiment we use a standard PCFG parser to produce a list of k-best Viterbi derivations.",
        "These, in turn, will be used to maximize possible objectives as described in section 3.3.",
        "Figure 5: Above: example of 2 ambiguous fragments mapping to the same CFG rule VP – VBD DT NN \"with\" NP.",
        "The first fragment occurs 5 times in the training treebank, (e.g. in the sentence was an executive with a manufacturing concern) while the second fragment occurs 4 times (e.g. in the sentence began this campaign with such high hopes).",
        "Below: the two pairs of CFG rules that are used to map the two fragments to separate CFG derivations.",
        "Relative Frequency Estimate (RFE) The simplest way to assign probabilities to fragments is to make them proportional to their counts in the training set.",
        "When enforcing equation 2, that gives the",
        "Relative Frequency Estimate (RFE):",
        "count (f)",
        "Unlike RFE for PCFGs, however, the RFE for PTSGs has no clear probabilistic interpretation.",
        "In particular, it does not yield the maximum likelihood solution, and when used as an estimator for an allfragments grammar, it is strongly biased since it assigns the great majority of the probability mass to big fragments (Johnson, 2002).",
        "As illustrated in figure 4 this bias is much weaker when restricting the set of fragments with our approach.",
        "Although this does not solve all theoretical issues, it makes RFE a reasonable first choice again.",
        "Equal Weights Estimate (EWE) Various other ways of choosing the weights of a DOP grammar have been worked out.",
        "The best empirical results have been reported by Bod (2003) with the EWE proposed by Goodman (2003).",
        "Goodman defined it for grammars in the Goodman transform, but for explicit grammars it becomes:",
        "where the first sum is over all parse trees t in the treebank (TB), count(f, t) gives the number of times fragment f occurs in t, and |{f ' € t}| is the total number of subtrees of t that were included in the symbolic grammar.",
        "Maximum Likelihood (ML) For reestimation, we can aim at maximizing the likelihood (ML) of the treebank.",
        "For this, it turns out that we can define another transformation of our PTSG, such that we can apply standard Inside-Outside algorithm for PCFGs (Lari and Young, 1990).",
        "The original version of IO is defined over string rewriting PCFGs, and maximizes the likelihood of the training set consisting of plain sentences.",
        "Reestimation shifts probability mass between alternative parse trees for a sentence.",
        "In contrast, our grammars consist of fragments of various size, and our training set consists of parse trees.",
        "Reestimation here shifts probability mass between alternative derivations for a parse tree.",
        "Our transformation approach is illustrated with an example in figure 6.",
        "In step (b) the fragments in the grammar as well as the original parse trees in the treebank are \"flattened\" into bracket notation.",
        "In step (c) each fragment is transformed into a CFG rule in the transformed meta-grammar, whose right-hand side is constituted by the bracket notation of the fragment.",
        "Each substitution site Xi is raised to a meta-nonterminal Xand all other symbols, including parentheses, become meta-terminals.",
        "The left-hand side of the rule is constituted by the original root symbol R of the fragment raised to a meta-nonterminal R.",
        "The resulting PCFG generates trees in bracket notation, and we can run an of-the-shelf inside-outside algorithm by presenting it parse trees from the train corpus in bracket notation.",
        "In the experiments that we report below we used the RFE from section 3, to generate the initial weights for the grammar.",
        "Figure 6: Rule and tree transforms that turn PTSG reestimation into PCFG reestimation; (a) a derivation of the sentence x y through successive substitutions of elementary trees from a PTSG; (b) the same elementary trees and resulting parse tree in bracket notation; (c) an equivalent derivation with the meta-grammar, where the original substitution sites reappear as meta-nonterminals (marked with a prime) and all other symbols as meta-terminals; (d) the yield of the derivation in c.",
        "MPD The easiest objective in parsing, is to select the most probable derivation (MPD), obtained by maximizing equation 3.",
        "MPP A DOP grammar can often generate the same parse tree t through different derivations D(t) = d\\,d2,... dm.",
        "The probability of t is therefore obtained by summing the probabilities of all its possible derivations.",
        "An intuitive objective for a parser is to select, for a given sentence, the parse tree with highest probability according to equation 7, i.e., the most probable parse (MPP): unfortunately, identifying the MPP is computationally intractable (Sima'an, 1996).",
        "However, we can approximate the MPP by deriving a list of k-best derivations, summing up the probabilities of those resulting in the same parse tree, and select the tree with maximum probability.",
        "MCP, MRS Following Goodman (1998), Sima'an (1999, 2003), and others, we also consider other objectives, in particular, the max constituent parse (MCP), and the max rule sum (MRS).",
        "MCP maximizes a weighted average of the expected labeled recall L/Nc and (approximated) labeled precision L/NG under the given posterior distribution, where L is the number of correctly labeled constituents, Nc the number of constituents in the correct tree, and NG the number of constituents in the guessed tree.",
        "Recall is easy to maximize since the estimated Nc is constant.",
        "L/Nc can be in fact maximized in:",
        "where lc ranges over all labeled constituents in t and P(lc) is the marginalized probability of all the derivation trees in the grammar yielding the sentence under consideration which contains lc.",
        "Precision, instead, is harder because the denominator NG depends on the chosen guessed tree.",
        "Goodman (1998) proposes to look at another metric which is strongly correlated with precision, which is the mistake rate (Ng – L)/Nc that we want to minimize.",
        "We combine recall with mistake rate through linear interpolation:",
        "where 10 is obtained from 9 assuming Nc constant, and the optimal level for X has to be evaluated empirically.",
        "Unlike MPP, the MCP can be calculated efficiently using dynamic programming techniques over the parse forest.",
        "However, in line with the aims of this paper to produce an easily reproducible implementation of DOP, we developed an accurate approximation of the MCP using a list of k-best derivations, such as those that can be obtained with an off-the-shelf PCFG parser.",
        "We do so by building a standard CYK chart, where every cell corresponds to a specific span in the test sentence.",
        "We store in each cell the probability of seeing every label in the grammar yielding the corresponding span, by marginalizing the probabilities of all the parse trees in the obtained k-best derivations that contains that label covering the same span.",
        "We then compute the Viterbi-best parse maximizing equation 10.",
        "We implement max rule sum (MRS) in a similar way, but do not only keep track of labels in every cell, but of each CFG rule that span the specific yield (see also Sima'an, 1999, 2003).",
        "We haven't implemented the max rule product (MRP) where posteriors are multiplied instead of added (Petrov and Klein, 2007; Bansal and Klein, 2010)."
      ]
    },
    {
      "heading": "4. Experimental Setup",
      "text": [
        "In order to build and test our Double-DOP model, we employ the Penn WSJ Treebank (Marcus et al., 1993).",
        "We use sections 2-21 for training, section 24 for development and section 23 for testing.",
        "Treebank binarization We start with some preprocessing of the treebank, following standard prac-",
        "JJ|NP NN|NP bands",
        "The Free black arm",
        "Max Const.",
        "Parse « Max Probable Derivation",
        "tice in WSJ parsing.",
        "We remove traces and functional tags.",
        "We apply a left binarization of the training treebank as in Matsuzaki et al.",
        "(2005) and Klein and Manning (2003), setting the horizontal history H=1 and the parent labeling P=1.",
        "This means that when a node has more than 2 children, the ith child (for i > 3) is conditioned on child i – l. Moreover the labels of all non-lexical nodes are enriched with the labels of their parent node.",
        "Figure 7 shows the binarized version of the tree structure in figure 1.",
        "Unknown words We replace words appearing less than 5 times in the training data by one of 50 unknown word categories based on the presence of lexical features as implemented in Petrov (2009).",
        "In some of the experiments we also perform a smoothing over the lexical elements assigning low counts (e = 0.01) to open-class (words, PoS-tags) pairs not encountered in the training corpus.",
        "Fragment extraction We extract the symbolic grammar and fragment frequencies from this preprocessed treebank as explained in section 2.",
        "This is the the most time-consuming step (around 160 CPU hours).",
        "In the extracted grammar we have in total 1,029,342 recurring fragments and 17,768 unseen CFG rules.",
        "We test several probability distributions over the fragments (section 3.2) and various maximization objectives (section 3.3).",
        "Figure 8: Double-DOP results on the development section (< 40) with different maximizing objectives.",
        "Parsing We convert our PTSG into a PCFG (section 3.1) and use Bitpar for parsing.",
        "For approximating MPP and other objectives we marginalize probabilities from the 1,000 best derivations.",
        "We start by presenting in figure 8 the results we obtain on the development set (section 24).",
        "Here we compare the maximizing objectives presented in section 3.3, using RFE to obtain the probability distribution over the fragments.",
        "We conclude that, empirically, MCP for X = 1.15, is the best choice to maximize F1, followed by MRS, MPP, and MPD.",
        "We also compare the various estimators presented in section 3.2, on the same development set, keeping MCP with X = 1.15 as the maximizing objective.",
        "We find that RFE is the best estimator (87.2 F1) followed by EWE (86.8) and ML (86.6).",
        "Our best results with ML are obtained when removing fragments occurring less than 6 times (apart from CFG-rules) and when stopping at the second iteration.",
        "This filtering is done in order to limit the number of big fragments in the grammar.",
        "It is well known that IO for DOP tends to assign most of the probability mass to big fragments, quickly overfit-ting the training data.",
        "It is surprising that EWE and ML perform worse than RFE, in contrast to earlier findings (Bod, 2003).",
        "Fragment frequency threshold",
        "Figure 9: Performance (on the development set) and size of Double-DOP when considering only fragments whose occurring frequency in the training treebank is above a specific threshold (x-axis).",
        "In all cases, all PCFG-rules are included in the grammars.",
        "For instance, at the right-hand side of the plot a grammar is evaluated which included only 6754 fragments with a frequency > 100 as well as 39227 PCFG rules.",
        "We also investigate how a further restriction on the set of extracted fragments influences the performance of our model.",
        "In figure 9 we illustrate the performance of Double-DOP when restricting the grammar to fragments having frequencies greater than 1, 2,..., 100.",
        "We can notice a rather sharp decrease in performance as the grammar becomes more and more compact.",
        "Next, we present some results on various DoubleDOP grammars extracted from the same training treebank after refining it using the Berkeley statesplitting model (Petrov et al., 2006; Petrov and Klein, 2007).",
        "In total we have 6 increasingly refined versions of the treebank, corresponding to the 6 cycles of the Berkeley model.",
        "We observe in figure 10 that our grammar is able to benefit from the state splits for the first four levels of refinement, reaching the maximum score at cycle 4, where we improve over our base model.",
        "For the last two data points, the treebank gets too refined, and using Double-DOP model on top of it, no longer improves accuracy.",
        "We have also compared our best Double-DOP",
        "Berkeley grammar/treebank refinement level",
        "Figure 10: Comparison on section 24 between the performance of Double-DOP (using RFE and MCP with X = 1.15, H=0, P=0) and Berkeley parser on different stages of refinement of the treebank/grammar.",
        "base model and the Berkeley parser on per-category performance.",
        "Here we observe an interesting trend: the Berkeley parser outperforms Double-DOP on very frequent categories, while Double-DOP performs better on infrequent ones.",
        "A detailed comparison is included in table 1.",
        "Finally, in table 2 we present our results on the test set (section 23).",
        "Our best model (according to the best settings on the development set) performs slightly worse than the one by Bansal and Klein (2010) when trained on the original corpus, but outperforms it (and the version of their model with additional refinements) when trained on the refined version, in particular for the exact match score."
      ]
    },
    {
      "heading": "5. Conclusions",
      "text": [
        "We have described Double-DOP, a novel DOP approach for parsing, which uses all constructions recurring at least twice in a treebank.",
        "This methodology is driven by the linguistic intuition that constructions included in the grammar should prove to be reusable in a representative corpus.",
        "The extracted set of fragments is significantly smaller than in previous approaches.",
        "Moreover constructions are explicitly represented, which makes them potentially good candidates as semantic or translation units to be used in other applications.",
        "Despite earlier reported excellent results with DOP parsers, they are almost never used in other",
        "Table 1: Comparison of the performance (per-category F1 score) on the development set between the Berkeley parser and the best Double-DOP model.",
        "NLP tasks: where other successful parsers often feature as components of machine translation, semantic role labeling, question-answering or speech recognition systems, DOP is conspicuously absent in these neighboring fields (but for a possible application of closely related formalisms see, e.g., Yamangil and Shieber, 2010).",
        "The reasons for this are many, but most important are probably the computational inefficiency of many instances of the approach, the lack of downloadable software and the difficulties with replicating some of the key results.",
        "In this paper we have addressed all three obstacles: our efficient algorithm for identifying the recurrent fragments in a treebank runs in polynomial time.",
        "The transformation to PCFGs that we define allows us to use a standard PCFG parser, while retaining the benefit of explicitly representing larger fragments.",
        "A different transform also allows us to run the popular inside-outside algorithm.",
        "Although IO results are slightly worse than with the naive relative frequency estimate, it is important to establish that the standard method for dealing with latent information (i.e., the derivations of a given parse) is not the best choice in this case.",
        "We expect that other re-estimation methods, for instance Vari-",
        "Table 2: Summary of the results of different parsers on the test set (sec 23).",
        "Double-DOP experiments use RFE, MCP with A = 1.15, H=1, P=1; those on statesplitting (Double-DOP-Sp) use Berkeley cycle 4, H=0, P=0.",
        "Results from Petrov and Klein (2007) already include smoothing which is performed similarly to our smoothing technique (see section 4).",
        "(* Results on a development set, with sentences up to length 20.)",
        "ational Bayesian techniques, could be formulated in the same manner.",
        "Finally, the availability of our programs, as well as the third party software that we use, also addresses the replicability issue.",
        "Where some researchers in the field have been skeptical of the DOP approach to parsing, we believe that our independent development of a DOP parser adds credibility to the idea that an approach that uses very many large subtrees, can lead to very accurate parsers."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We gratefully acknowledge funding by the Netherlands Organization for Scientific Research (NWO): FS is funded through a Vici-grant \"Integrating Cognition\" (277.70.006) to Rens Bod, and WZ through a Veni-grant \"Discovering Grammar\" (639.021.612).",
        "We also thank Rens Bod, Gideon Borensztajn, Jos de Bruin, Andreas van Cranen-burgh, Phong Le, Remko Scha, Khalil Sima'an and the anonymous reviewers for very useful comments.",
        "Category",
        "%",
        "F1",
        "F1",
        "label",
        "in gold",
        "Berkeley",
        "Double-DOP",
        "NP",
        "41.42",
        "91.4",
        "89.5",
        "VP",
        "20.46",
        "90.6",
        "88.6",
        "S",
        "13.38",
        "90.7",
        "87.6",
        "PP",
        "12.82",
        "85.5",
        "84.1",
        "SBAR",
        "3.47",
        "86.0",
        "82.1",
        "ADVP",
        "3.36",
        "82.4",
        "81.0",
        "ADJP",
        "2.32",
        "68.0",
        "67.3",
        "QP",
        "0.98",
        "82.8",
        "84.6",
        "WHNP",
        "0.88",
        "94.5",
        "92.0",
        "WHADVP",
        "0.33",
        "92.8",
        "91.9",
        "PRN",
        "0.32",
        "83.0",
        "77.9",
        "NX",
        "0.29",
        "9.50",
        "7.70",
        "SINV",
        "0.28",
        "90.3",
        "88.1",
        "SQ",
        "0.14",
        "82.1",
        "79.3",
        "FRAG",
        "0.10",
        "26.4",
        "34.3",
        "SBARQ",
        "0.09",
        "84.2",
        "88.2",
        "X",
        "0.06",
        "72.0",
        "83.3",
        "NAC",
        "0.06",
        "54.6",
        "88.0",
        "WHPP",
        "0.06",
        "91.7",
        "44.4",
        "CONJP",
        "0.04",
        "55.6",
        "66.7",
        "LST",
        "0.03",
        "61.5",
        "33.3",
        "UCP",
        "0.03",
        "30.8",
        "50.0",
        "INTJ",
        "0.02",
        "44.4",
        "57.1",
        "test (< 40)",
        "test (all)",
        "Parsing Model",
        "F1",
        "EX",
        "F1",
        "EX",
        "PCFG Baseline",
        "PCFG (H=1, P=1)",
        "77.6",
        "17.2",
        "76.5",
        "15.9",
        "PCFG (H=1, P=1) Lex smooth.",
        "78.5",
        "17.2",
        "77.4",
        "16.0",
        "FRAGMENT-BASED PARSERS",
        "Zuidema (2007)*",
        "83.8",
        "26.9",
        "-",
        "-",
        "Cohn et al.",
        "(2010) MRS",
        "85.4",
        "27.2",
        "84.7",
        "25.8",
        "Post and Gildea (2009)",
        "82.6",
        "-",
        "-",
        "-",
        "Bansal and Klein (2010) MCP",
        "88.5",
        "33.0",
        "87.6",
        "30.8",
        "Bansal and Klein (2010) MCP",
        "88.7",
        "33.8",
        "88.1",
        "31.7",
        "+ Additional Refinement",
        "THIS PAPER",
        "Double-DOP",
        "87.7",
        "33.1",
        "86.8",
        "31.0",
        "Double-DOP Lex smooth.",
        "87.9",
        "33.7",
        "87.0",
        "31.5",
        "Double-DOP-Sp",
        "88.8",
        "35.9",
        "88.2",
        "33.8",
        "Double-DOP-Sp Lex smooth.",
        "89.7",
        "38.3",
        "89.1",
        "36.1",
        "REFINEMENT-BASED PARSERS",
        "Collins (1999)",
        "88.6",
        "-",
        "88.2",
        "-",
        "Petrov and Klein (2007)",
        "90.6",
        "39.1",
        "90.1",
        "37.1"
      ]
    }
  ]
}
