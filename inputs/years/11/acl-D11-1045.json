{
  "info": {
    "authors": [
      "Karthik Visweswariah",
      "Rajakrishnan Rajkumar",
      "Ankur Gandhe",
      "Ananthakrishnan Ramanathan",
      "Jiri Navratil"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1045",
    "title": "A Word Reordering Model for Improved Machine Translation",
    "url": "https://aclweb.org/anthology/D11-1045",
    "year": 2011
  },
  "references": [
    "acl-C04-1073",
    "acl-C10-1043",
    "acl-C10-1071",
    "acl-C10-1126",
    "acl-C96-2141",
    "acl-D07-1077",
    "acl-D09-1105",
    "acl-E09-1011",
    "acl-H05-1012",
    "acl-H05-1066",
    "acl-I08-7017",
    "acl-J03-1005",
    "acl-J99-4005",
    "acl-N04-4026",
    "acl-N07-1008",
    "acl-P02-1039",
    "acl-P05-1012",
    "acl-P05-1066",
    "acl-P06-1067",
    "acl-P06-1077",
    "acl-P06-1121",
    "acl-P09-1038",
    "acl-P09-1090",
    "acl-W02-1001",
    "acl-W06-3119"
  ],
  "sections": [
    {
      "text": [
        "Karthik Visweswariah Rajakrishnan Rajkumar Ankur Gandhe",
        "IBM Research India Dept.",
        "of Linguistics IBM Research India",
        "Bangalore, India Ohio State University Bangalore, India",
        "IBM Research India Bangalore, India",
        "Preordering of source side sentences has proved to be useful in improving statistical machine translation.",
        "Most work has used a parser in the source language along with rules to map the source language word order into the target language word order.",
        "The requirement to have a source language parser is a major drawback, which we seek to overcome in this paper.",
        "Instead of using a parser and then using rules to order the source side sentence we learn a model that can directly reorder source side sentences to match target word order using a small parallel corpus with high-quality word alignments.",
        "Our model learns pairwise costs of a word immediately preceding another word.",
        "We use the Lin-Kernighan heuristic to find the best source reordering efficiently during training and testing and show that it suffices to provide good quality reordering.",
        "We show gains in translation performance based on our reordering model for translating from Hindi to English, Urdu to English (with a public dataset), and English to Hindi.",
        "For English to Hindi we show that our technique achieves better performance than a method that uses rules applied to the source side English parse."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Languages differ in the way they order words to produce sentences representing the same meaning.",
        "Machine translation systems need to reorder words in the source sentence to produce fluent output in the target language that preserves the meaning of the source sentence.",
        "Current phrase based machine translation systems can capture short range reorderings via the phrase table.",
        "Even the capturing of these local reordering phenomena is constrained by the amount of training data available.",
        "For example, if adjectives precede nouns in the source language and follow nouns in the target language we still need to see a particular adjective noun pair in the parallel corpus to handle the reordering via the phrase table.",
        "Phrase based systems also rely on the target side language model to produce the right target side order.",
        "This is known to be inadequate (Al-Onaizan and Papineni, 2006), and this inadequacy has spurred various attempts to overcome the problem of handling differing word order in languages.",
        "One approach is through distortion models, that try to model which reorderings are more likely than others.",
        "The simplest models just penalize long jumps in the source sentence when producing the target sentence.",
        "These models have also been generalized (Al-Onaizan and Papineni, 2006; Till-man, 2004) to allow for lexical dependencies on the source.",
        "While these models are simple, and can be integrated with the decoder they are insufficient to capture long-range reordering phenomena especially for language pairs that differ significantly.",
        "The weakness of these simple distortion models has been overcome using syntax of either the source or target sentence (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venu-gopal, 2006).",
        "While these methods have shown to be useful in improving machine translation performance they generally involve joint parsing of the source and target language which is significantly more computationally expensive when compared to phrase based translation systems.",
        "Another approach that overcomes this weakness, is to to reorder the source sentence based on rules applied on the source parse (either hand written or learned from data) both when training and testing (Collins et al., 2005; Gen-zel, 2010; Visweswariah et al., 2010).",
        "In this paper we propose a novel method for dealing with the word order problem that is efficient and does not rely on a source or target side parse being available.",
        "We cast the word ordering problem as a Traveling Salesman Problem (TSP) based on previous work on word-based and phrased-based statistical machine translation (Tillmann and Ney, 2003; Zaslavskiy et al., 2009).",
        "Words are the cities in the TSP and the objective is to learn the distance between words so that the shortest tour corresponds to the ordering of the words in the source sentence in the target language.",
        "We show that the TSP distances for reordering can be learned from a small amount of high-quality word alignment data by means of pairwise word comparisons and an informative feature set involving words and part-of-speech (POS) tags adapted and extended from prior work on dependency parsing (McDonald et al., 2005b).",
        "Obtaining high-quality word alignments that we require for training is fairly easy compared with obtaining a treebank required to obtain parses for use in syntax based methods.",
        "We show experimentally that our reordering model, even when used to reorder sentences for training and testing (rather than being used as an additional score in the decoder) improves machine translation performance for: Hindi – English, English – Hindi, and Urdu – English.",
        "Although Urdu is similar to Hindi from the point of reordering phenomena we include it in our experiments since there are publicly available datasets for Urdu-English.",
        "For English – Hindi we obtained better machine translation performance with our reordering model as compared to a method that uses reordering rules applied to the source side parse.",
        "The rest of the paper is organized as follows.",
        "Section 2 reviews related work and places our work in context.",
        "Section 3 outlines reordering issues due to syntactic differences between Hindi and English.",
        "Section 4 presents our reordering model, Section 5 presents experimental results and Section 6 presents our conclusions and possible future work."
      ]
    },
    {
      "heading": "2. Related work",
      "text": [
        "There have been several studies demonstrating improved machine translation performance by reordering source side sentences based on rules applied to the source side parse during training and decoding.",
        "Much of this work has used hand written rules and several language pairs have been studied e.g German to English (Collins et al., 2005), Chinese to English (Wang et al., 2007), English to Hindi (Ramanathan and Japanese to English (Lee et al., 2010).",
        "There have also been some studies where the rules are learned from the data (Genzel, 2010; Visweswariah et al., 2010; Xia and McCord, 2004).",
        "In addition there has been work (Yamada and Knight, 2002; Zollmann and Venugopal, 2006; Galley et al., 2006; Liu et al., 2006) which uses source and/or target side syntax in a Context Free Grammar framework which results in machine translation decoding being considered as a parsing problem.",
        "In this paper we propose a model that does not require either source or target side syntax while also preserving the efficiency of reordering techniques based on rules applied to the source side parse.",
        "In work that is closely related to ours, (Tromble and Eisner, 2009) formulated word reordering as a Linear Ordering Problem (LOP), an NP-hard permutation problem.",
        "They learned LOP model weights capable of assigning a score to every possible permutation of the source language sentence from an aligned corpus by using a averaged perceptron learning model.",
        "The key difference between our model and the model in (Tromble and Eisner, 2009) is that while they learn costs of a word Wj appearing anywhere before Wj, we learn costs of Wj immediately preceding Wj.",
        "This results in more compact models and (as we show in Section 5) better models.",
        "Our model results in us having to solve a TSP instance.",
        "The relation between the TSP and machine translation decoding has been explored before.",
        "(Knight, 1999) showed that TSP is a subclass of MT decoding and thus established that the latter is NP-hard.",
        "(Zaslavskiy et al., 2009) casts phrase-based decoding as a TSP and they show favorable speed performance trade-offs compared with Moses, an existing state-of-the-art decoder.",
        "In (Tillmann and Ney, 2003), a beam-search algorithm used for TSP is adapted to work with an IBM-4 word-based model and phrase-based model respectively.",
        "As opposed to calculating TSP distances from existing machine translation components ( viz. the translation, distortion and language model probabilities) we learn model weights to reorder source sentences to match target word order using an informative feature set adapted from graph-based dependency parsing (McDonald et al., 2005a)."
      ]
    },
    {
      "heading": "3. Hindi-English reordering issues",
      "text": [
        "This section provides a brief survey of constructions that the two languages in question differ as well as have in common.",
        "(Ramanathan et al., 2009) notes the following divergences:",
        "• English follows SVO order while Hindi follows SOV order",
        "• English uses prepositions while Hindi uses post-positions",
        "• Hindi allows greater word order freedom",
        "• Hindi has a relatively richer case-marking system",
        "In addition to these differences, (Visweswariah et al., 2010) mention the similarity in word order in the case of adjective noun sequences (some books vs. kuch kitab)."
      ]
    },
    {
      "heading": "4. Reordering model",
      "text": [
        "Consider a source sentence w consisting of a sequence of n words wi, w2, ... wn that we would like to reorder into the target language order.",
        "Given a permutation n of the indices 1..n, let the candidate reordering be wni, wn2, wnn.",
        "Thus, n denotes the index of the word in the source sentence that maps to position i in the candidate reordering.",
        "Clearly there are n!",
        "such permutations.",
        "Our reordering model assigns costs to candidate permutations as:",
        "C(n|w) = ^2 c(iTi-i,iTi).",
        "The cost c(m, n) can be thought of as the cost of the word at index m immediately preceding the word with index n in the candidate reordering.",
        "In this paper, we parametrize the costs as:",
        "where 9 is a learned vector of weights and $ is a vector of feature functions.",
        "Given a source sentence w we reorder it according to the permutation n that minimizes the cost C(n|w).",
        "Thus, we would like our cost function C(n | w) to be such that the correct reordering n* has the lowest cost of all possible reorderings n. In Section 4.1 we describe the features $ that we use, and in Section 4.2 we describe how we train the weights 9 to obtain a good reordering model.",
        "Given our model structure, the minimization problem that we need to solve is identical to solving a Asymmetric Traveling Salesman Problem (ATSP) with each word corresponding to a city, and the costs c(m, n) representing the pairwise distances between the cities.",
        "Consider the following example: English input: John eats apples Hindi: John seba(apples) khaataa hai(eats) Desired reordered English: John apples eats The ATSP that we need to solve is represented pictorially in Figure 1 with sample costs.",
        "Note that we have one extra node numbered 0.",
        "We start and end the tour at node 0, and this determines the first word in the reordered sentence.",
        "In this example the minimum cost tour is: Start – John – apple – eats recovering the right reordering for translation into",
        "Hindi.",
        "Solving the ATSP (which is a well known NP hard problem) efficiently is crucial for the efficiency of our reordering model.",
        "To solve the ATSP, we first convert the ATSP to a symmetric TSP and then use the Lin-Kernighan heuristic as implemented in Concorde, a state-of-the-art TSP solver (Applegate et al., 2005).",
        "We also experimented with using the exact TSP solver in Concorde but since it was slower and did not improve performance we preferred using the Lin-Kernighan heuristic.",
        "To convert the ATSP to a symmetric TSP we double the size of the original problem creating a node N for every node N in the original graph.",
        "Following (Hornik and",
        "Hahsler, 2009), we then set new costs as follows: c (A, B) = oo, c (A, B') = c (B ,A) = c(A, B) and C (A, A ) = – oo.",
        "Even with this doubling of the number of nodes, we observed that solving the TSPs with the Lin-Kernighan heuristic is very fast, taking roughly 10 milliseconds per sentence on average.",
        "Overall, this means that our reordering model is as fast as parsing and hence our model is comparable in performance to techniques based on applying rules to the parse tree.",
        "Since we would like to model reordering phenomena which are largely related to analyzing the syntax of the source sentence, we chose to use features based on those that have in the past been used for parsing (McDonald et al., 2005a).",
        "A subset of the features we use was also used for reordering in (Tromble and",
        "Eisner, 2009).",
        "To be able to generalize from relatively small amounts of data, we use features that in addition to depending on the words in the input sentence w depend on the part-of-speech (POS) tags of the words in the input sentence.",
        "All features $(w, i, j) we use are binary features, that fire based on the identities of the words and POS tags at or surrounding positions i and j in the source sentence.",
        "The first set of feature templates we use are given in Table 1.",
        "These features depend only on the identities of the word and POS tag of the two positions i and j and we call",
        "Table 1: Bigram feature templates used to calculate the cost that word at position i immediately precedes word at position j in the target word order.",
        "w4 (pi) denotes the word (POS tag) at position i in the source sentence.",
        "Each of the templates is also conjoined with i-j the signed distance between the two words in the source sentence.",
        "these Bigram features.",
        "The second set of feature templates we use are given in Table 2.",
        "These features, in addition to examining positions i and j examine the surrounding positions.",
        "We instantiate these feature templates separately for the POS tag sequence and for the word sequence.",
        "We call these two feature sets Con-textPOS and ContextWord respectively.",
        "When instantiated with POS tags, the first row of Table 2 looks at all POS tags between positions i and j.",
        "(Tromble and Eisner, 2009) use Bigram and Con-textPOS features, while we extend their feature set with the use of ContextWord features.",
        "Since Hindi is verb final, in Hindi sentences with multiple verb groups it is rare for words with a verb in between to be placed together in the reordering to match English.",
        "Looking at the POS tags of words between positions i and j allows us to penalize such reorderings.",
        "Each of the templates described in Table 1 and Table 2 is also conjoined with i-j the signed distance between the two words in the source sentence.",
        "The values of i-j between 5 and 10, and greater than 10 are quantized (negative values are similarly quantized).",
        "In Section 5.2 we report on experiments showing the relative performance of these different feature in the same order that they occurred in the source sentence.",
        "Obtaining the target ordering in this manner, is certainly not the only possible way and we would like to explore better treatment of this in future work.",
        "We used the single best Margin Infused Relaxed Algorithm (MIRA) ((McDonald et al., 2005b), (Crammer and Singer, 2003)) with the online updates to our parameters being given by",
        "s.t.",
        "In the equation above,",
        "Table 2: Context feature templates used to calculate the cost that word at position i immediately precedes word at position j in the target word order.",
        "Oi denotes the observation at position i in the source sentence and Ob denotes an observation at a position between i and j (i.e i + 1 < b < j – 1).",
        "Each of the templates is instantiated with the observation sequence o taken to be the word sequence w and the POS tag sequence p. Each of the templates is also conjoined with i-j the signed distance between the two positions in the source sentence.",
        "types for the task of reordering Hindi sentences to be in English word order.",
        "To train the weights 9 in our model, we need a collection of sentences, where we have the desired reference reordering n*(x) for each input sentence x.",
        "To obtain these reference reorderings we use word aligned source-target sentence pairs.",
        "The quality and consistency of these reference reorderings will depend on the quality of the word alignments that we use.",
        "Given word aligned source and target sentences, we drop the source words that are not aligned.",
        "Let mi be the mean of the target word positions that the source word at index i is aligned to.",
        "We then sort the source indices in increasing order of mi.",
        "If mi = mj (for example, because wi and wj are aligned to the same set of words) we keep them is the best reordering based on the current parameter value and L is a loss function.",
        "We take the loss of a reordering to be the number of words for which the preceding word is wrong relative to the reference target order.",
        "We also experimented with the averaged percep-tron algorithm (Collins, 2002), but found single best MIRA to work slightly better and hence used MIRA for all our experiments."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "In this section we report on experiments to evaluate our reordering model.",
        "The first method we use for evaluation (monolingual BLEU) is by generating the desired reordering of the source sentence (as described in Section 4.2) and compare the reordered output to this desired reordered sentence using the BLEU metric.",
        "In addition, to these monolingual BLEU results, we also evaluate (in Section 5.5) the reordering by its effect on eventual machine translation performance.",
        "We note that our reordering techniques uses POS information for the input sentence.",
        "The POS taggers used in this paper are Maximum Entropy Markov models trained using manually annotated POS corpora.",
        "For Hindi, we used roughly fifty thousand words with twenty six tags from the corpus described in (Dalal et al., 2007).",
        "For Urdu we used roughly fifty thousand words and forty six tags from the CRULP corpus (Hussain, 2008) and for English we used the Wall Street Journal section of the Penn Treebank.",
        "To train our reordering models we need training data where we have the input source language sentence and the desired reordering in the target language.",
        "As described in Section 4.2 we derive the reference reordered sentence using word alignments.",
        "Table 3 presents our monolingual BLEU results for Hindi to English reordering as the source of the word alignments is varied.",
        "All results in Table 3 are with Bigram and ContextPOS features.",
        "We have word alignments from three sources: A small set of hand aligned sentences, HMM alignments (Vogel et al., 1996) and alignments obtained using a supervised Maximum Entropy aligner (Ittycheriah and Roukos, 2005) trained on the hand alignments.",
        "The F-measure for the HMM alignments were 65% and 78% for the Maximum Entropy model alignments.",
        "We see that the quality of the alignments is an important determiner of reordering performance.",
        "Row 1 shows the BLEU for unreordered (baseline) Hindi compared with the Hindi sentences reordered in English Order.",
        "Using just HMM alignments to train our model we do worse than unreordered Hindi.",
        "Although using the Maximum Entropy alignments is better than using HMM alignments, we do not improve upon a small number of hand alignments by using all the Maximum Entropy alignments.",
        "To improve upon the model trained with only hand alignments we selected a small number of snippets of sentences from our Maximum Entropy alignments.",
        "The goal was to pick parts of sentences where the alignment is reliable enough to use for training.",
        "The heuristic we used in the selection of snippets was to pick maximal snippets of at least 7 consecutive Hindi words with all Hindi words aligned to a consecutive span of English words, with no unaligned English words in the span and no English words aligned to Hindi words outside the span.",
        "Adding snippets selected with this heuristic improves the reordering performance of our model as seen in the last row of Table 3.",
        "In this section we report on experiments to determine the performance of the different classes of features (Bigram, ContextPos and ContextWord) dis-",
        "Table 3: Monolingual BLEU scores for Hindi to English reordering using models trained on different alignment types and tested on a development set of 280 Hindi sentences (5590 tokens).",
        "Table 4: Monolingual BLEU scores for Hindi to English reordering using models trained with different feature sets and tested on a development set of 280 Hindi sentences (5590 tokens).",
        "cussed in Section 4.1.",
        "Table 4 shows monolingual BLEU results for training with different features sets for Hindi to English reordering.",
        "In all cases, we use a set of 6000 sentence pairs which were hand aligned to generate the training data.",
        "It is clear that all three sets offeatures contribute to performance of the reordering model, however the number of Con-textWord features is larger than the number of Bigram and ContextPOS features put together, and it may be desirable to select from this set of features especially when training on large amounts of data.",
        "Table 5 compares our reordering model with a reimplementation of the reordering model proposed in (Tromble and Eisner, 2009).",
        "Both the models use exactly the same features (bigram features and Con-textPOS features) and are trained on the same data.",
        "To generate our training data, for Hindi to English and English to Hindi we use a set of 6000 hand aligned sentences, for Urdu to English we use a set of 8500 hand aligned sentences and for English to Frenchwe use a set of10000 hand aligned sentences (a subset of Europarl and Hansards corpus).",
        "Our",
        "HMM",
        "MaxEnt",
        "Hand",
        "BLEU",
        "-",
        "-",
        "-",
        "35.9",
        "220K",
        "-",
        "-",
        "35.4",
        "-",
        "220K",
        "-",
        "47.0",
        "-",
        "220K",
        "6K",
        "48.4",
        "-",
        "-",
        "6K",
        "49.0",
        "-",
        "Good 17K",
        "6K",
        "51.3",
        "Feature template",
        "Bigram",
        "ContextPOS",
        "ContextWord",
        "BLEU",
        "-",
        "-",
        "-",
        "35.9",
        "X",
        "-",
        "-",
        "43.8",
        "X",
        "X",
        "-",
        "49.0",
        "X",
        "X",
        "X",
        "51.3",
        "Table 5: Monolingual BLEU scores comparing the original source order with desired target reorder without reordering, and reordering using our model (TSP) and the model proposed in (Tromble and Eisner, 2009) (LOP).",
        "test data consisted of 280 sentences for Hindi to English and 400 sentences for all other language pairs generated from hand aligned sentences.",
        "We include English-French here to compare on a fairly similar language pair with local reordering phenomena (the main difference being that in French adjectives generally follow nouns).",
        "We note that our model outperforms the model proposed in (Tromble and Eisner, 2009) in all cases.",
        "To get a feel for the qualitative performance of our reordering algorithm and the kind of phenomena it is able to capture, we analyze the reordering performance in terms of (i) whether the clause restructuring is done correctly - these can be thought of as medium-to-long range reorderings, (ii) whether clause boundaries are respected, and (iii) whether local (short range) reordering is performed correctly.",
        "The following analysis is for Hindi to English reordering with the best model (this is also the model used for Machine Translation experiments reported on in Section 5.5).",
        "• Clause structure: As discussed in Section 3, the canonical clause order in Hindi is SOV, while in English it is SVO.",
        "However, variations on this structure are possible and quite frequent (e.g., clauses with two objects).",
        "To evaluate clause restructuring, we compared sequences of subjects, objects and verbs in the output and reference reorderings.",
        "We had a set of 70 sentences annotated with subject, direct object, indirect object and verb information - these annotations were made on the head word of each phrase, and the comparisons were on sequences of these words alone and not the entire constituent phrase.",
        "52 sentences were reordered by the model to match the order of the corresponding reference.",
        "Eight sentences were ordered correctly but differently from the reference, because the reference was expressed in non-canonical fashion (e.g., in the passive) - note that these cases negatively impact the monolingual BLEU score.",
        "The following example shows a sentence being reordered correctly, where, however, the reference is expressed differently (note the position of the subject \"policy\" (niiti) in the reference and the reordered output) :",
        "Input: abai (now) taka2 (till) aisii3 (this) niiti4(policy) kabhii5 (ever) nahÜ6 (not) rahiiz (has) hais (been) Reordered: taka2 (till) aba1 (now) aisii3 (this) niiti4 (policy) hais (been) kabhii5 (ever) nahÜ6 (not) rahii7 (has) Reference: taka2 (till) aba1 (now) aisii3 (this) kabhii5 (ever) nahÜ6 (not) rahii7 (has) hai8(been) niiti4 (policy) English: Till now this never has been the policy",
        "The remaining ten sentences were reordered incorrectly.",
        "These errors are largely in clauses which deviate from the SVO order in some way - clauses with multiple subjects or objects, clauses with no object, etc.. For example, the following sentence with two subjects and objects corresponding to the verb wearing has not been reordered correctly.",
        "Input: sabhii1 (all) purusha2 (men) safeda3 (white) evama4 (and) mahilaaen5 (women) kesariyaa6 (saffron) vastra7 (clothes) dhaarana8(wear) kiye9 hue1o (-ing) thiin11 (were) Reordered: sabhii1 (all) purusha2 (men) safeda3 (white) evama4 (and) mahilaaen5(women) kesariyaa6 (saffron) vastra7 (clothes) dhaarana8 (wear) thiin11 (were) kiye9 hue10 (Reference: sabhii1 (all) purusha2 (men) thiin11 (were) dhaarana8 (wear) kiye9 hue10 (-",
        "!The numeric subscripts in the examples indicate word positions in the input.",
        "Language pair",
        "Monolingual BLEU",
        "Source",
        "Target",
        "Unreordered",
        "LOP",
        "TSP",
        "Hindi",
        "English",
        "35.9",
        "36.6",
        "49.0",
        "English",
        "Hindi",
        "34.4",
        "48.4",
        "56.7",
        "Urdu",
        "English",
        "35.6",
        "39.5",
        "49.9",
        "English",
        "French",
        "64.4",
        "78.2",
        "81.2",
        "ing) safeda3 (white) evama4 (and) mahilaaen5 (women) kesariyaa6 (saffron) English: All men were wearing white and the women saffron",
        "The model possibly needs more data with patterns that deviate from the standard SOV order to learn to reorder them correctly.",
        "We could also add to the model, features pertaining to subject, object, etc.",
        "• Clause boundaries: Measured on a set of 844 sentences which were marked with clause boundaries, 37 sentences (4.4 %) had reorder-ings that violated these boundaries.",
        "An example of such a clause-boundary violation is below:",
        "Input: main1 (I) sarakaara2 (government) kaa3(of) dhyaana4 (attention) maananiiya5 (honourable) pradhaana6 (prime) mantri7 (minister) dvaaraa8 (by) isa9 (this) sabhaa10(house) me11 (in) kiye12 gaye13 (made) isa14 (this) vaade15 (promise) ki16 ora17 (towards) dilaanaa18 (to bring) chaahuungaa19 (would Reordered: main1 (I) chahuungaa1g (would like) dilaanaa18 (to bring) kii16 ora17 (towards) isa9 (this) vaade15 (promise) kiye12gaye13 (made) dvaaraa8 (by) maananiiya5 (honourable) mantri7 (minister) pradhaana6(prime) dhyaana4 (attention) kaa3 (of) sarakaara2 (government) men11 (in) isa14 (this) sabhaa10 (house) Reference: main1 (I) chahuungaa19 (would like) dilaanaa18 (to bring) dhyaana4 (attention) kaa3 (of) sarakaara2 (government) kii16 ora17(towards) isa9 (this) vaade15 (promise) kiye12gaye13 (made) dvaaraa8 (by) maananiiya5 (honourable) mantri7 (minister) pradhaana6(prime) men11 (in) isa9 (this) sabhaa10 (house) English I would like to bring the attention of the government towards this promise made by the honourable prime minister in this house.",
        "Note how the italicized clause, which is kept together in the reference, is split up incorrectly in the reordered output.",
        "The proportion of such boundary violations is, however, quite low, because Hindi being a verb-final language, most clauses end with a verb and it is probably quite straightforward for the model to keep clauses separate.",
        "A clause boundary detection program should make it possible to eliminate the remaining errors.",
        "• Local reordering: To estimate the short range reordering performance, we consider how often different POS bigrams in the input are reordered correctly.",
        "Here, we expect the model to reorder prepositions correctly, and to avoid any reordering that moves apart nouns and their adjectival pre-modifiers or components of compound nouns (see Section 3).",
        "Table 6 summarizes the reordering performance for these categories for a set of 280 sentences (same as the test set used in Section 5.1).",
        "Each row in Table 6 indicates the total number of correct instances for the pair, i.e., the number of instances of the pair in the reference (column titled Total), the number of instances that already appear in the correct order in the input (column Input), and the number that are ordered correctly by the reordering model (column Reordered).",
        "The first two rows show that adjective-noun and noun-noun (compounds) are in most cases correctly retained in the original order by the model.",
        "The final row shows that while many prepositions have been moved into their correct positions, there are still quite a few mismatches with the reference.",
        "An important reason why this happens is that nouns modified by prepositional phrases can often also be expressed as noun compounds.",
        "For example, vidyuta (electricity) kii (of) aavashyakataaen (requirements) in Hindi can be expressed either as \"requirements of electricity\" or \"electricity requirements\".",
        "The latter expression results in a match with the input (explaining many of the 104 correct orders in the input) and a mismatch with the model's reordering.",
        "The same problem in the training data would also adversely impact the learning of the preposition reordering rule.",
        "We now present experiments in incorporating the reordering model in machine translation systems.",
        "For all results presented here, we reorder the training and test data using the single best reordering based on our reordering model for each sentence.",
        "For each of the language pairs we evaluated, we trained Direct Translation Model 2 (DTM) systems (Ittycheriah and Roukos, 2007) with and without reordering and compared performance on test data.",
        "We note that the DTM system includes features that allow it to model lexicalized reordering phenomena.",
        "The reordering window size was set to +/-8 words for both the baseline and our reordered input.",
        "In our experiments, we left the word alignments fixed, i.e we reordered the existing word alignments rather than realigning the sentences after reordering.",
        "Redoing the word alignments with the reordered data could potentially give further small improvements.",
        "We note that we obtained better baseline performance using DTM systems than the standard Moses/Giza++ pipeline (e.g we obtained a BLEU of 14.9 for English to Hindi with a standard Moses/Giza++ pipeline).",
        "For all of our systems we used a combination of HMM (Vogel et al., 1996) and MaxEnt alignments (Ittycheriah and Roukos, 2005).",
        "For our Hindi-English experiments we use a training set of roughly 250k sentences (5.5M words) consisting of the Darpa-TIDES dataset (Bojar et al., 2010) and an internal dataset from several domains but dominated by news.",
        "Our test set was roughly 1.2K sentences from the news domain with a single reference.",
        "To train our reordering model, we used roughly 6K alignments plus 17K snippets selected from MaxEnt alignments as described in Section 5.1 with bigram, ContextPOS and ContextWord features.",
        "The monolingual reordering BLEU (on the same data reported on in Section 5.3) was 54.0 for Hindi to English and 60.8 for English to Hindi.",
        "For our Urdu-English experiments we used 70k",
        "Table 7: Translation performance without reordering (baseline) compared with performance after preordering with our reordering model.",
        "sentences from the NIST MT-08 training corpus and used the MT-08 eval set for testing.",
        "We note that the MT-08 eval set has four references as compared to one reference for our Hindi-English test set.",
        "This largely explains the improved baseline performance for Urdu-English as compared to Hindi-English.",
        "We present averaged results for the Web and News part of the test sets.",
        "To train the reordering model we used 9K hand alignments and 11K snippets extracted from MaxEnt alignments as described in Section 5.1 with bigram, ContextPOS and ContextWord context feature.",
        "The monolingual reordering BLEU for the reordering model thus obtained (on the same data reported on in Section 5.3) was 52.7.",
        "Table 7 shows that for Hindi to English, English to Hindi and for Urdu to English we see a gain of 1.5 - 2 BLEU points.",
        "For English – Hindi we also experimented with a system that uses rules (learned from the data using the methods described in (Visweswariah et al., 2010)) applied to a parse to reorder source side English sentences.",
        "This system had a BLEU score of 21.2, which is an improvement over the baseline, but our reordering model is better by 1.3 BLEU points.",
        "An added benefit of our reordering model is that the decoder can be run with a smaller search space exploring only a small amount of reordering without losing accuracy but running substantially faster.",
        "Table 8 shows the variation in machine Hindi to English translation performance with varying skip size (this parameter sets the maximum number of words skipped during decoding, lower values are associated with a restricted decoder search space and increased speed).",
        "POS pair",
        "Total",
        "Input",
        "Reordered",
        "adj-noun",
        "234",
        "192",
        "196",
        "noun-noun",
        "46",
        "44",
        "42",
        "prep-noun",
        "436",
        "104",
        "250",
        "Language pair",
        "BLEU",
        "Source",
        "Target",
        "Unreordered",
        "Reordered",
        "Hindi Urdu English",
        "English English Hindi",
        "14.7 23.3 20.7",
        "16.7 24.8 22.5"
      ]
    },
    {
      "heading": "6. Conclusion and future work",
      "text": [
        "In this paper we presented a reordering model to reorder source language data to make it resemble the target language word order without using either a source or target parser.",
        "We showed consistent gains of up to 2 BLEU points in machine translation performance using this model to preorder training and test data.",
        "We show better performance compared to syntax based reordering rules for English to Hindi translation.",
        "Our model used only a part of speech tagger (sometimes trained with fairly small amounts of data) and a small corpus of word alignments.",
        "Considering the fact that treebanks required to build high quality parsers are costly to obtain, we think that our reordering model is a viable alternative to using syntax for reordering.",
        "We also note, that with the preordering based on our reordering model we can achieve the best BLEU scores with a much tighter search space in the decoder.",
        "Even accounting for the cost of finding the best reordering according to our model, this usually results in faster processing than if we did not have the reordering in place.",
        "In future work we plan to explore using more data from automatic alignments, perhaps by considering a joint model for aligning and reordering.",
        "We would also like to explore doing away with the requirement of having a POS tagger, using completely unsupervised methods to class words.",
        "We currently only look at word pairs in calculating the loss function used in MIRA updates.",
        "We would like to investigate the use of other loss functions and their effect on reordering performance.",
        "We also would like to explore whether the use of scores from our reordering model directly in machine translation systems can improve performance relative to using just the single best reordering.",
        "skip",
        "Unreordered",
        "Reordered",
        "2",
        "12.2",
        "16.7",
        "4",
        "13.4",
        "16.7",
        "8",
        "14.7",
        "16.4"
      ]
    }
  ]
}
