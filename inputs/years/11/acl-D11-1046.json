{
  "info": {
    "authors": [
      "Jason Riesa",
      "Ann Irvine",
      "Daniel Marcu"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1046",
    "title": "Feature-Rich Language-Independent Syntax-Based Alignment for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/D11-1046",
    "year": 2011
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Jason Riesa Ann Irvine Daniel Marcu",
        "information Sciences Institute University of Southern California",
        "We present an accurate word alignment algorithm that heavily exploits source and target-language syntax.",
        "Using a discriminative framework and an efficient bottom-up search algorithm, we train a model of hundreds of thousands of syntactic features.",
        "Our new model (1) helps us to very accurately model syntactic transformations between languages; (2) is language-independent; and (3) with automatic feature extraction, assists system developers in obtaining good word-alignment performance off-the-shelf when tackling new language pairs.",
        "We analyze the impact of our features, describe inference under the model, and demonstrate significant alignment and translation quality improvements over already-powerful baselines trained on very large corpora.",
        "We observe translation quality improvements corresponding to 1.0 and 1.3 BLEU for Arabic-English and Chinese-English, respectively."
      ]
    },
    {
      "heading": "Introduction",
      "text": [
        "In recent years, several state-of-the-art statistical machine translation (MT) systems have incorporated both source and target syntax into the grammars that they generate and use to translate.",
        "While some tree-to-tree systems parse source and target sentences separately (Galley et al., 2006; Zollman and Venu-gopal, 2006; Huang and Mi, 2010), others project syntactic parses across word alignments (Li et al., 2009).",
        "In both approaches, as in largely all statistical MT, the quality of the alignments used to generate the rules of the grammar are critical to the success of the system.",
        "However, to date, most word alignment systems have not considered the same degree of syntactic information that MT systems have.",
        "Extending unsupervised models, like the IBM models (Brown et al., 1993), generally requires changing the entire generative story.",
        "The additional complexity would likely make training such models quite expensive.",
        "Already, with ubiquitous tools like GIZA++ (Och and Ney, 2003), training accurate models on large corpora takes upwards of 5 days.",
        "Recent work in discriminative alignment has focused on incorporating features that are unavailable or difficult to incorporate within other models, e.g. (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005b; Blunsom and Cohn, 2006; Lacoste-Julien et al., 2006; Moore et al., 2006).",
        "Even more recently, motivated by the rise of syntax-based translation models, others have sought to inform alignment decisions with syntactic information (Fraser and Marcu, 2007; DeNero and Klein, 2007; May and Knight, 2007; Fossum et al., 2008; Haghighi et al., 2009; Burkett et al., 2010; Pauls and Klein, 2010; Riesa and Marcu, 2010).",
        "Motivated by the wide modeling gap that still remains between syntax-based translation and word-alignment models, in this paper we expand on previous work in discriminative alignment, and move forward in three key areas:",
        "1.",
        "We heavily exploit both source and target syntax in ways that most models can not.",
        "In addition, during training we extract and learn hundreds of thousands of features automatically, learning both the structure and parameters for the model at the same time.",
        "2.",
        "Our model and inference support arbitrary fea-tures,and easily scale to millions of features."
      ]
    },
    {
      "heading": "3.. Having strengthened the synchronicity between",
      "text": [
        "alignment and syntax-based translation models, we advance state-of-the-art performance in terms of both alignment and translation quality over already-powerful baselines on very large corpora.",
        "2A Feature-Rich Syntax-Aware Alignment Model",
        "We follow Riesa and Marcu (2010) for efficient inference with arbitrary features, but do not rely upon hand-crafted syntactic patterns; rather, we extract syntactic features automatically from training data.",
        "We also introduce, in Section 5, an iterative approximate Viterbi inference procedure to deal with the asymmetry of the model.",
        "We show that this boosts both alignment and downstream translation quality even further.",
        "The model itself is a linear combination of features, whose parameters are learned online via a structured perceptron (Collins, 2002).",
        "However, as we describe in Section 3, the features of the model are not known a priori.",
        "In what follows, we describe the search algorithm so that the reader has an understanding of the domain of locality before we begin to describe features and how they are learned.",
        "We formulate the search for the best alignment as bottom-up parsing.",
        "Given a syntactic parse tree on one side of a parallel sentence, we use the structure of the tree to guide the search process.",
        "The key idea is that complex interactions between alignments are less likely to cross constituents, so we search recursively on the tree.",
        "As an illustrative example, we point to the structure of the hypergraph search depicted in Figure 1.",
        "Here we are aligning the sentence pair:",
        "a flag hung from the stage tai shàng guà zhe guöqf",
        "The figure shows the search process for a small example with beam size k.Each black square represents a partial alignment.Each partial alignment at each node is ranked according to its model score.",
        "In this figure, the 1-best hypothesis at the leftmost NP node is constructed by composing the best hypothesis at its child DT and the 2nd-best hypothesis at its",
        "Figure 1 : Approximate search through a hypergraph with beam size k = 5.Each black square represents a partial alignment; larger grey-shaded boxes are links in an alignment.",
        "Each partial alignment at each node is ranked according to its model score.",
        "The root node, S, contains a k-best list of full alignments.",
        "child NN.",
        "At the root node, we have a k-best list of full alignments.",
        "We continue with a procedural description of the algorithm.",
        "We begin by visiting each preterminal node in sequence.",
        "We enumerate and score all one-to-one links as well as the unaligned link (aligned to null).",
        "Next, for a given preterminal node, we use cube pruning (Chiang, 2007) to find the top k one-to-two alignments, given the scores of the one-to-one links.",
        "We perform additional iterations of cube pruning to find top k sets of one-to-wî links.",
        "In theory, we could increase m to the length of the foreign sentence and enumerate top k lists for each English word aligned to between 0 and all foreign words.",
        "However, in practice we set m to limit time spent here, while maintaining acceptable recall.",
        "In our experiments we set m = 2 for both English-Arabic and English-Chinese.",
        "We continue traversing the tree bottom-up.",
        "At each nonterminal node, a k-best list of partial alignments from each of its child nodes are combined into a larger span.",
        "We use cube pruning to do this efficiently.",
        "Nodes in different subtrees are processed independently of one another; i.e., for any node, alignment information at that node's sister is unavailable.",
        "For example, in Figure 1, alignment information at the leftmost NP is unavailable to us while we are constructing partial alignments at the PP.",
        "Search continues recursively up the tree, until we have reached the root node.",
        "The root node again computes the top k alignments from its children, and these comprise our final k-best list of full alignments.",
        "In our experiments we only make use of the 1-best alignment for evaluation and translation.",
        "Previous work has shown that only shallow k-best lists of alignments may be beneficial, and that very deep k-best lists are not especially useful in improving final downstream translation grammar extraction due to rapid degradation in quality (Venugopal et al., 2008; Liu et al., 2009b); though they may have other uses.",
        "3Automatically Exploiting Syntactic Features for Alignment",
        "Up to now, previous work in syntax-based alignment has largely modeled alignments based on features encoding target-side English syntactic and lexical information, but only lexical information on the source side.",
        "However, there is much more data waiting to be exploited, and the flexible model and efficient and modular learning framework of hierarchical discriminative alignment afford us this possibility.",
        "Here, we discuss our target-side features, source-side features, and features that jointly take into account both source-and target-side information.",
        "Most alignment systems currently function without explicit regard to the downstream translation model.",
        "Some notable exceptions are May and Knight (2007) who generate syntactic alignments by realigning word-to-word alignments with a syntactic model;",
        "'Cube pruning is approximate when we have nonlocal combination features, and most of our features are of this type.",
        "and Pauls and Klein (2010) who generate syntactic alignments with a synchronous ITG (Wu, 1997) approach.",
        "We depart from ITG-based models (Cherry and Lin, 2006; Haghighi et al., 2009) because of their complexity (O(n) in the synchronous case), requiring heavy pruning or the computation of outside cost estimates (DeNero and Klein, 2010).",
        "Instead, we use linguistically motivated target-side parse trees to constrain search, as described above.",
        "These trees are output from the Berkeley parser (Petrov and Klein, 2007) and fixed at alignment time.",
        "We use these trees not only as a vehicle for search, but also for features.",
        "A significant motivation for this work is the desire to make the connection, at alignment time, between translation rules used in decoding and the alignments that yield such translation rules.",
        "To do this, we fold the rule extraction process into the alignment search.",
        "At each step in the search process, we can extract translation rules from a given partial alignment and encode them as binary features.",
        "Importantly, the rule extraction process itself is not directly tied to the alignment system, but rather to the downstream translation model.",
        "We can drop in any type of rule extraction we like into the alignment system, though some may generalize better than others to new data in a large corpus.",
        "This is key for supervised training conditions with relatively small amounts of annotated data.",
        "In this work we focus on string-to-tree translation and the translation rule space described in (Galley et al., 2004; Galley et al., 2006).",
        "During training and inference, we are constantly scoring partial alignments.",
        "Every time we have a partial alignment to score, we can extract all potential translation rules implied by that alignment, and encode those rules as features.",
        "In this case, we are doing two important things:",
        "1. informing the alignment search with the rules of the translation model, and",
        "2. modeling actual translation rules - the model parameters give us a way to quantify the relative importance of each rule.",
        "For example, we learn that:",
        "(1 ) Chinese VP and NP tend to be reordered around the",
        "f$ particle when translating to English.",
        "(2) When translating an Arabic NP as part of a VP, we often insert \"is\".",
        "From this process we extract and learn 326,239 lexicalized and non-lexicalized translation rule features in our Arabic-English model; 234,972 in our Chinese-English model.",
        "Those features for which a positive weight is learned tend to generalize well over the training data; negatively weighted features do not, and are generally learned from alignments with mistakes during search.",
        "See Figure 2 for additional examples of rule features learned for Arabic-English alignment.",
        "Negative evidence Nearly 67% of the rule features we learn for Chinese-English, and 55% of the rule features we learn for Arabic-English are negatively weighted.",
        "Early experiments involved only firing indicator rule features when an extracted rule at alignment-time matched in a set of rules extracted offline from our hand-aligned data.",
        "However, coverage from such rules will always be limited; firing every rule as a feature as it is encountered during search gives us many more darts to throw.",
        "Using only rule features extracted from gold data lowers F-measure by close to 5 points.",
        "Source syntactic trees have recently been shown to be helpful in machine translation decoding (Zhang et al., 2008; Liu et al., 2009a; Chiang, 2010), but to our knowledge have not been used in alignment models other than that of Burkett et al.",
        "(2010).",
        "We parse the source side of our data using the Berkeley parser (Petrov and Klein, 2007), and encode information provided by the source syntax as features in the model in two ways: (1) as tree-distance features, and (2) as joint source-target syntax features.",
        "last Chinese words in the examples in Figure 3.",
        "Extracted Rule Feature Weight",
        "Figure 2: Translation rules as features extracted during Arabic-English alignment.",
        "These rules show that we learn to reorder adjectives and nouns inside noun phrases, and that prepositions before sister NPs prefer to be translated monotonically.",
        "For Chinese-English, we learn the opposite.",
        "Drawing on work by Chiang (2010) in stochastically rewriting syntactic constituents across languages in a translation model, we adapt the general idea to alignment modeling.",
        "Chiang calls these features fuzzy syntax features; here, we simply call them coordination features in our adaptation for alignment, so as to avoid the implication that we are rewriting.",
        "This feature family is a set of binary features that may fire at any nonterminal node in the tree during bottom-up search.",
        "A feature fires for each combination of two nonterminal source and target nodes s and t, respectively, that match the following conditions:",
        "1. t is the label of the current target tree node in the bottom-up search.",
        "2. s is the label of the source tree node of maximal depth (i.e. closest to leaf nodes) that spans all links also spanned by t.",
        "Figure 3 shows three examples of this joint feature over source and target trees.",
        "In Figure 3a, the maximal-depth source tree node that spans every link also spanned by the shaded target tree NP",
        "_JNP.",
        "\"nN[2]\"_",
        " – [2] [1]",
        "1.11908",
        "NP",
        "\"nN[2]^\"",
        " – [1] [2]",
        "-0.15417",
        "INrT",
        " – PP-",
        "\"nP[2] \"",
        "- [1] [2]",
        "1.15328",
        "INrT",
        "_JPP-",
        "\"Np[2]",
        "- [2] [1]",
        "-0.65943",
        "feature",
        "weight",
        "NP(NPm VPm ) <-> 12 m",
        "1.01304",
        "feature",
        "weight",
        "VP((VBZ is) NPm ) <-> m",
        "0.67252",
        "(b) Source/target tree feature firing at node IN, returning value ( IN ; PP >.",
        "(a) Source/target tree feature firing at node NP, with value < NP ; NP >.The maximal-depth source tree node that spans every link also spanned by the shaded target tree NP is also labeled NP.",
        "(c) In this figure, depicting an incorrect alignment, the same feature value is fired as for the correct alignment in 3b: ( IN ; PP >.We need more contextual annotation to create more discriminative power.",
        "The value of the feature depends on is also labeled NP.",
        "So, the feature returns a value of (NP; NP>.In Figure 3b, PP is the label of the maximal-depth source tree node that spans every link also spanned by the shaded target tree IN node; the feature fires a value (IN ; PP>.",
        "We might expect this pairing of IN with PP, or of IN with P, but we would expect to learn a penalizing parameter weight for the pairing of, say, IN with NP.",
        "Adding more context Powerful as this feature is, it is not quite discriminating enough; it may return the same feature value for both a correct and incorrect alignment, as shown in Figure 3c.",
        "To overcome this, we introduce additional features annotated with the leftmost and rightmost tags in the current span.",
        "For example, in this figure, we also fire ( IN ; PP(P,NP) >,and learn a negative weight of -0.638 denoting a poor choice of alignment.",
        "We also find it helpful to keep the original unannotated feature as a poor-man's backoff.",
        "Some examples Table 1 shows some of the maximally and minimally-weighted features learned.",
        "As the more highly weighted features show, both models learn to prefer alignments that result in the coordination of similar constituent labels.",
        "For example, the Chinese model learns a very high weight for aligning sets of English words that form prepositional phrases to sets of Chinese",
        "Ara-Eng Model Chi-Eng Model",
        "Table 1: This table shows a sampling of the highest and lowest-weighted coordination features applied when scoring partial alignments at nodes in the tree.",
        "Preterminal tags inside parentheses indicate the POS tags on the left and right edge of a given constituent.",
        "words that also form prepositional phrases.",
        "Inversely, we learn high negative weights for model features that fire for alignments that oblige the firing of features of very dissimilar nonterminal labels, and that often yield asynchronous bracketing.",
        "For example, the Arabic model learns that English words that form prepositional phrases should not align to sets of Arabic words that form entire sentences or verb phrases.",
        "eng",
        "ara",
        "w",
        "eng",
        "chi w",
        "[1]",
        "SBAR",
        "SBAR",
        "6.40",
        "PP",
        "PP 10.3",
        "[2]",
        "S",
        "S(CC,PU)",
        "4.91",
        "NP",
        "NP 9.38",
        "[3]",
        "PP",
        "PP",
        "4.20",
        "SBAR",
        "VP(VV,PU) 6.97",
        "[4]",
        "VP",
        "VP",
        "3.90",
        "NP",
        "NP(DT,NN) 6.67",
        "[5]",
        "SBAR",
        "pp",
        "2.58",
        "PP",
        "PP(PLC) 6.38",
        "[6]",
        "NP",
        "s",
        "-2.80",
        "NP",
        "PP -6.82",
        "[7]",
        "NP",
        "VP",
        "-3.01",
        "S",
        "IP(PU.PU) -7.44",
        "[8]",
        "NP",
        "np(nn.in)",
        "-4.52",
        "PP",
        "IP -7.33",
        "[9]",
        "PP",
        "VP",
        "-5.13",
        "SBAR",
        "VP -7.72",
        "[10]",
        "PP",
        "S",
        "-7.37",
        "NP",
        "IP -7.83",
        "In total, we learn 127,932 syntactic coordination features in our Arabic-English model; 59,239 for Chinese-English.",
        "4Learning",
        "We learn feature weights using a parallelized implementation of online averaged perceptron (Collins, 2002).",
        "We distribute training examples to CPUs in a cluster and essentially run several perceptron learners in parallel.",
        "We communicate and average the weight vectors of each learner according to the Iterative Parameter Mixing strategy described by McDonald et al.",
        "(2010).",
        "Let yi be the correct output for input xi.Here, yi is an alignment; xi is a sentence pair and parse tree.",
        "At each iteration, our perceptron update is:",
        "And we define:",
        "with w our weight vector, h(y) our sparse vector of feature values, Y(xi) all possible outputs for input xi, and F1(yi,y) balanced F-measure.",
        "The loss, £(yi, y), is a measure of how bad it would be to guess y instead of y.",
        "In selecting y,we draw upon the loss-augmented inference literature (Tsochantaridis et al., 2004; Taskar et al., 2005a).",
        "Alignment y is the output candidate maximizing the sum of both the loss and model score.",
        "This guess appears attractive to the model, yet has low F-measure, and so is exactly the sort of output we would like to update away from.",
        "During training, we learn both the parameters and model structure.",
        "Figure 4b shows how the size of the model grows over time.",
        "As described in Sections 2 and 3, we automatically extract and fire features given an alignment configuration and our current position in the tree.",
        "We see a steep initial growth in model size, and then begin to trail off as the number of new unique rules and negative evidence we encounter diminishes.",
        "Model Selection Among models from the first iteration up to convergence, we choose the model parameters from the best performing model as measured by F-measure on a held-out development set of alignments."
      ]
    },
    {
      "heading": "5. Iterative Approximate Viterbi Inference",
      "text": [
        "Though up to now we have described features that fire during bottom-up search on the target-language tree, we can also search bottom-up on the source-language tree.",
        "The syntactic features we have described are generic enough that they will still be extractable and applicable.",
        "Because our model and inference procedure are asymmetric, a search on the source-language tree will generate alignments from a different space, and can provide a unique signal we would not otherwise have.",
        "We can use the Viterbi alignments from each model to inform the other.",
        "In the following we describe a method for simultaneously training both target-tree and source-tree models but with features to enforce agreement, somewhat similar to (Nivre and McDonald, 2008) in integrating two dependency parsing models.",
        "We begin by training two models, one that operates on the target tree, and one that operates on the source tree.",
        "Call the parameters learned from these models w1 and respectively.",
        "Then, performing inference under these models yields alignments a\\ and asx.",
        "In the next iteration we learn parameters w2 and w2,and introduce agreement features.",
        "In this step, during training to find w2, the target-tree model uses asx to fire indicator features.",
        "These fire for any alignment link that was also present in the previous iteration's source-tree alignment, asx .Analogously, when searching for the best w2 ,we use a1 to fire indicator features that fire for any alignment link also present in the previous iteration's target-tree alignment, a[.",
        "This process of using the alignment from the previous iteration's opposing tree continues until convergence, i.e. until we no longer see improvement in our 1-best source-tree and target-tree alignments.",
        "When we use these alignments for downstream translation, we symmetrize with the grow-diag-final heuristic, which continues to work remarkably well in practice.",
        "We also experiment with the intersection of both final alignments.",
        "(a) Learning curves (Arabic-English): F-measure accuracy on heldout development data over time for five different beam settings, k=2, k=4, k=16, k=64 and k=128.",
        "For Arabic-English, improvements are minimal with beams larger than k=128; and for Chinese-English, with beams larger than k=256.",
        "(b) Model size as a function of time for five different beam settings (Arabic-English): We see a steep initial growth, and then begin to trail off as the number of new unique extractable features and negative evidence we encounter diminishes.",
        "Growth rate is higher for models with narrower beams that make more mistakes.",
        "Figure 4: Learning feature-rich alignment models.",
        "Figure 4a shows learning curves on heldout data for five different beam sizes.",
        "Figure 4b shows how the models dynamically grow over time.",
        "In Figure 4b we notice that less accurate models with narrower beams need to add more complexity in an attempt to make up for their many more mistakes.",
        "6Evaluation",
        "From LDC2006E86 and LDC2006E83, we use as training data 2,280 hand-aligned sentence pairs of Arabic-English and 1,102 for Chinese-English.",
        "We measure training convergence using a held-out development set of 100 sentence pairs for each language pair, and evaluate with F-measure on a heldout test set of 184 sentences pairs for Chinese-English and 364 sentence pairs for Arabic-English.",
        "We use instances of the Berkeley parser (Petrov and Klein, 2007) trained on the English Penn Treebank, Chinese Treebank 6, and the Arabic Treebank parts 1-3; for each language, trees are fixed at alignment time using the 1-best output from each parser.",
        "We use Model-4 symmetrized with the grow-diag-final heuristic, trained with GIZA++ as a baseline alignment model.",
        "We train two GIZA++ models on our largest available Chinese-English and Arabic-English parallel corpora.",
        "These consist of 261M and 223M English words, respectively.",
        "The size of these corpora make for quite a powerful unsupervised baseline.",
        "In training our alignment model, we use the syntactic features discussed in Section 3, plus word-based lexical features t(e | f ) and t(f | e) used during initialization, extracted offline directly from the translation-table of GIZA++.",
        "Using these features alone results in an F-measure of 59.1 for Arabic-English, and 55.6 for Chinese-English.",
        "Our automatically extracted syntactic features and iterative inference algorithm get us the rest of the way, bringing performance up to 87.6 and 87.0, respectively.",
        "Table 2 shows the results on our held-out 100-sentence test set.",
        "In an intrinsic evaluation on an alignment task, our F-measure scores are more than 15 points higher than the baseline for both language pairs.",
        "In evaluating downstream translation quality, we build three translation systems each for Arabic-English and Chinese-English: one with alignments from GIZA++, one with alignments from our syntactically-informed discriminative model, and one with alignments from our model with iterative inference (Section 5).",
        "For each of these systems we",
        "Arabic-English Chinese-English FP R FP R",
        "Table 2: F-measure, Precision, Recall for GIZA++ Model-4, and for alignments from this work.",
        "GIZA++ was trained on 223M words for Arabic-English, and 261M words for Chinese-English.",
        "We observe very large gains in accuracy of 15 points for both language pairs.",
        "Iterative inference with source and target-tree alignments yields a large effect on Chinese-English recall, and a modest improvement in Arabic-English.",
        "align our parallel training corpora described in Section 6.1, and compute word-based lexical weighting features (Koehn et al., 2003) based on these alignments.",
        "Because of the number of experiments involved in this research, we needed to accelerate our downstream experimental pipeline.",
        "While we align our full training corpus, we extract translation rules from a subset of our alignment training data; the quality of the translation rules extracted is still a function of the original alignment model.",
        "We train a syntax-based string-to-tree translation model (Galley et al., 2004; Galley et al., 2006) and extract translation rules using alignments produced by each system from 4.25+5.43M words for Arabic-English and 31.8+37.7M words for Chinese-English.",
        "For Arabic-English, we tune our MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout set of 746 parallel sentences with four references each.",
        "For Chinese-English we tune our MT system on a held-out development corpus of 4,089 parallel sentences, and test on a set of 4,060 sentences with four references each.",
        "We tune the translation models for these systems with MIRA (Watanabe et al., 2007; Chiang et al., 2008).",
        "Our tuning and test corpora are drawn from the NIST 2004 and 2006 evaluation data, disjoint from our rule-extraction data.",
        "All systems used two language models; one trained on the combined English sides of our Arabic-English and Chinese-English data (480M words), and one trained on 4 billion words of English data.",
        "MT results are shown in Table 3.",
        "We show a gain ara-eng chi-eng",
        "Alignment model BLEU BLEU",
        "Table 3: IBM BLEU scores using a syntax-based MT system.",
        "We show statistically significant gains in both language pairs over unsupervized GIZA++ Model 4 trained on very large corpora.",
        "An asterisk (*) denotes a statistically significant improvement with p < 0.01 over the number immediately above; a (+) denotes p < 0.05.",
        "of 1.0 and 1.3 BLEU points over GIZA++ Model-4.",
        "Each is statistically significant over the baseline.",
        "In the case of Chinese-English, we see a 1.1 BLEU gain when using iterative inference over the standard model which provides only target-tree alignments.",
        "As measured by a bootstrap resampler, this improvement is statistically significant, with p < 0.01.",
        "For Arabic-English, we see a BLEU gain of 0.7 with target-tree alignments alone, and a total 1.0 BLEU gain over the baseline with iterative inference and our joint-agreement features.",
        "We expect the limited improvement of iterative inference for Arabic-English is due to at least two factors:"
      ]
    },
    {
      "heading": "1.. the relative weakness of our Arabic parser, and",
      "text": [
        "2. as shown in Table 2, our Arabic target-tree alignments are already quite accurate.",
        "7Discussion",
        "We achieve our best downstream BLEU results when using iterative inference with source-tree and target-tree alignments, keeping the intersection.",
        "These alignments have been shown to have recall in a similar neighborhood as our unsupervised baseline, but extremely high precision.",
        "As DeNero and Klein (2010) and others have observed, the relationship between word alignment evaluation metrics and BLEU score remains tenuous at best.",
        "While we are able to induce some of the most accurate alignments we have seen to date, it remains unclear, given our gold hand-aligned data, whether we are optimizing for the right function ultimately for the translation task.",
        "Related metrics, like Rule F-measure (Fossum et al., 2008) and Translation Unit Error Rate (S0gaard and Kuhn, 2009), are still functions of a given gold alignment.",
        "If the gold alignment is not ideally annotated for the translation task, it matters little what our alignment evaluation metric is.",
        "Why do grow-diag-final alignments (for our system) not perform as well?",
        "We believe the answer lies in the fact that these alignments too closely resemble the gold alignments with word-alignment annotation standards that do not handle function words ideally for the translation task.",
        "Indeed, Hermjakob (2009) reports improved BLEU with a hand-modified gold standard.",
        "Interestingly, the places in which our source-tree and target-tree alignments most often disagree is in the alignment of function words with no clear translation in the opposite language.",
        "For example, English the has no translation in Chinese.",
        "Our intersection alignments generally leave the unaligned to Chinese words, whereas in our gold alignments the is generally aligned to the same word as the head of the NP in which it appears.",
        "We see our best translation performance with our",
        "intersection symmetrization does not help GIZA++ because the resulting recall is so low as to severely limit the usefulness of direct translation rule extraction with such alignments (49.7 Recall for Chi-Eng; 47.2 Recall forAra-Eng).",
        "intersection alignments because we believe it largely leaves untranslated words and words without clear translations in the opposite language unaligned; we believe this may be the right thing to do.",
        "Continuing with the the example, our translation model learns to insert words like the where appropriate, and such insertion rules are validated by the language model.",
        "We learn with good coverage accurate high-precision translation rules for content words, and general insertion rules for words like î/îe,instead of learning two unique lexicalized rules for a given content word, one with and one without the.ln this way, we are learning a more general grammar that explains the data.",
        "8Conclusion",
        "In this work we are closing the gap between translation and alignment models in terms of syntactic sophistication.",
        "We have (1) shown how to efficiently extract hundreds of thousands of language-independent syntactic features useful for alignment, (2) given a detailed analysis of the types of linguistic phenomena these varied features generalize, and (3) report significant gains not only on alignment quality but also on downstream machine translation quality (1.0+ BLEU) over very strong baselines across diverse language pairs.",
        "We have also hinted at roadblocks to improved discriminative alignment modeling for translation.",
        "We expect that an accurate discriminative word alignment system, such as the one presented here, in conjunction with better annotation standards for alignment will take us even farther beyond the advancements in translation quality shown here."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The authors would like to thank David Chiang, Steve DeNeefe, Liang Huang, Kevin Knight, Jonathan May, and the anonymous reviewers for their thoughtful comments.",
        "This work was supported in part by NSF IIS-0908532, DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies, and a USC CREATE Fellowship to the first author.",
        "\"Naively leaving all function words unaligned is likely sub-optimal, as many have seem to have direct translations in some contexts; cf. (of, i>) and (of,"
      ]
    }
  ]
}
