{
  "info": {
    "authors": [
      "Markus Dreyer",
      "Jason M. Eisner"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1057",
    "title": "Discovering Morphological Paradigms from Plain Text Using a Dirichlet Process Mixture Model",
    "url": "https://aclweb.org/anthology/D11-1057",
    "year": 2011
  },
  "references": [
    "acl-D08-1113",
    "acl-D09-1011",
    "acl-P06-1085",
    "acl-W01-0712",
    "acl-W02-0606",
    "acl-W06-3209",
    "acl-W98-1239"
  ],
  "sections": [
    {
      "text": [
        "Markus Dreyer* Jason Eisner",
        "SDL Language Weaver Computer Science Dept., Johns Hopkins University Los Angeles, CA 90045, USA Baltimore, MD 21218, USA",
        "We present an inference algorithm that organizes observed words (tokens) into structured inflectional paradigms (types).",
        "It also naturally predicts the spelling of unobserved forms that are missing from these paradigms, and discovers inflectional principles (grammar) that generalize to wholly unobserved words.",
        "Our Bayesian generative model of the data explicitly represents tokens, types, inflections, paradigms, and locally conditioned string edits.",
        "It assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms (string tuples).",
        "Each paradigm is sampled all at once from a graphical model, whose potential functions are weighted finite-state transducers with language-specific parameters to be learned.",
        "These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM, belief propagation, and dynamic programming.",
        "Given 50-100 seed paradigms, adding a 10-million-word corpus reduces prediction error for morphological inflections by up to 10%."
      ]
    },
    {
      "heading": "1. Introduction 1.1 Motivation",
      "text": [
        "Statistical NLP can be difficult for morphologically rich languages.",
        "Morphological transformations on words increase the size of the observed vocabulary, which unfortunately masks important generalizations.",
        "In Polish, for example, each lexical verb has literally 100 inflected forms (Janecki, 2000).",
        "That is, a single lexeme may be realized in a corpus as many different word types, which are differently inflected for person, number, gender, tense, mood, etc.",
        "* This research was done at Johns Hopkins University as part of the first author's dissertation work.",
        "It was supported by the Human Language Technology Center of Excellence and by the National Science Foundation under Grant No.",
        "0347822.",
        "All this makes lexical features even sparser than they would be otherwise.",
        "In machine translation or text generation, it is difficult to learn separately how to translate, or when to generate, each of these many word types.",
        "In text analysis, it is difficult to learn lexical features (as cues to predict topic, syntax, semantics, or the next word), because one must learn a separate feature for each word form, rather than generalizing across inflections.",
        "Our engineering goal is to address these problems by mostly-unsupervised learning of morphology.",
        "Our linguistic goal is to build a generative probabilistic model that directly captures the basic representations and relationships assumed by morphologists.",
        "This model suffices to define a posterior distribution over analyses of any given collection of type and/or token data.",
        "Thus we obtain scientific data interpretation as probabilistic inference (Jaynes, 2003).",
        "Our computational goal is to estimate this posterior distribution.",
        "Our inference algorithm jointly reconstructs token, type, and grammar information about a language's morphology.",
        "This has not previously been attempted.",
        "Tokens: We will tag each word token in a corpus with (1) apart-of-speech (POS) tag, (2) an inflection, and (3) a lexeme.",
        "A token of broken might be tagged as (1) a verb and more specifically as (2) the past participle inflection of (3) the abstract lexeme JjrmA.Reconstructing the latent lexemes and inflections allows the features of other statistical models to consider them.",
        "A parser may care that broken is a past participle; a search engine or question answering system may care that it is a form of JjrmA; and a translation system may care about both facts.",
        "y viap(y | x), and vice-versa.",
        "Dreyer et al.",
        "(2008) define such a family via a log-linear model with latent alignments,",
        "Table 1: Part of a morphological paradigm in German, showing the spellings of some inflections of the lexeme ÂrmA (whose lemma is brechen), organized in a grid.",
        "Types: In carrying out the above, we will reconstruct specific morphological paradigms of the language.",
        "A paradigm is a grid of all the inflected forms of some lexeme, as illustrated in Table 1.",
        "Our reconstructed paradigms will include our predictions of inflected forms that were never observed in the corpus.",
        "This tabular information about the types (rather than the tokens) of the language may be separately useful, for example in translation and other generation tasks, and we will evaluate its accuracy.",
        "Grammar: We estimate parameters f that describe general patterns in the language.",
        "We learn a prior distribution over inflectional paradigms by learning (e.g.) how a verb's suffix or stem vowel tends to change when it is pluralized.",
        "We also learn (e.g.) whether singular or plural forms are more common.",
        "Our basic strategy is Monte Carlo EM, so these parameters tell us how to guess the paradigms (Monte Carlo E step), then these reconstructed paradigms tell us how to reestimate the parameters (M step), and so on iteratively.",
        "We use a few supervised paradigms to initialize the parameters and help reestimate them."
      ]
    },
    {
      "heading": "2. Overview of the Model",
      "text": [
        "We begin by sketching the main ideas of our model, first reviewing components that we introduced in earlier papers.",
        "Sections 5-7 will give more formal details.",
        "Full details and more discussion can be found in the first author's dissertation (Dreyer, 2011).",
        "We begin with a family of joint distributions p(x, y) over string pairs, parameterized by f. For example, to model just the semi-systematic relation between a German lemma and its 3rd-person singular present form, one could train to maximize the likelihood of (x,y) pairs such as (brechen, bricht).",
        "Then, given a lemma x, one could predict its inflected form",
        "Here a ranges over monotonic 1-to-1 character alignments between x and y.",
        "« means \"proportional to\" (p is normalized to sum to 1).",
        "f extracts a vector of local features from the aligned pair by examining trigram windows.",
        "Thus f can reward or penalize specific features – e.g., insertions, deletions, or substitutions in specific contexts, as well as trigram features of x and y separately.",
        "Inference and training are done by dynamic programming on finite-state transducers.",
        "A paradigm such as Table 1 describes how some abstract lexeme (ArmJfù) is expressed in German.",
        "We evaluate whole paradigms as linguistic objects, following word-and-paradigm or realizational morphology (Matthews, 1972; Stump, 2001).",
        "That is, we presume that some language-specific distribution p(n) defines whether a paradigm n is a grammatical – and a priori likely – way for a lexeme to express itself in the language.",
        "Learning p(n) helps us reconstruct paradigms, as described at the end of section 1.2.",
        "Let n = (xi, x2,...).",
        "In Dreyer and Eisner (2009), we showed how to model p(n) as a renormalized product of many pairwise distributions prs(xr,xs), each having the log-linear form of section 2.1:",
        "This is an undirected graphical model (MRF) over string-valued random variables xs; each factor prsevaluates the relationship between some pair of strings.",
        "Note that it is still a log-linear model, and parameters in f can be reused across different rs pairs.",
        "To guess at unknown strings in the paradigm, Dreyer and Eisner (2009) show how to perform approximate inference on such an MRF by loopy belief",
        "singular",
        "plural",
        "present",
        "1st-person",
        "2nd-person",
        "3rd-person",
        "breche",
        "brichst",
        "bricht",
        "brechen",
        "brecht",
        "brechen",
        "past",
        "lst-person",
        "2nd-person",
        "3rd-person",
        "brach",
        "brachst",
        "brach",
        "brachen",
        "bracht",
        "brachen",
        "Figure 1: A distribution over paradigms modeled as an MRF over 7 strings.",
        "Random variables XLem, Xist, etc., are the lemma, the 1st person form, etc.",
        "Suppose two forms are observed (denoted by the \"lock\" icon).",
        "Given these observations, belief propagation estimates the posterior marginals over the other variables (denoted by \"?",
        "\").",
        "propagation, using finite-state operations.",
        "It is not necessary to include all rs pairs.",
        "For example, Fig. 1 illustrates the result of belief propagation on a simple MRF whose factors relate all inflected forms to a common (possibly unobserved) lemma, but not directly to one another.",
        "Our method could be used with any p(n).",
        "To speed up inference (see footnote 7), our present experiments actually use the directed graphical model variant of Fig. 1 – thatis, p(n) = pi(xi) • TJs>1 pis(xs | xi), where x1 denotes the lemma.",
        "Dreyer and Eisner (2009) learned 0 by partially observing some paradigms (type data).",
        "That work, while rather accurate at predicting inflected forms, sometimes erred: it predicted spellings that never occurred in text, even for forms that \"should\" be common.",
        "To fix this, we shall incorporate an unlabeled or POS-tagged corpus (token data) into learning.",
        "We therefore need a model for generating tokens – a probabilistic lexicon that specifies which inflections of which lexemes are common, and how they are spelled.",
        "We do not know our language's probabilistic lexicon, but we assume it was generated as follows:",
        "1.",
        "Choose parameters of the MRF.",
        "This defines p(n): which paradigms are likely a priori.",
        "2.",
        "Choose a distribution over the abstract lexemes.",
        "3.",
        "For each lexeme, choose a distribution over its inflections.",
        "4.",
        "For each lexeme, choose a paradigm that will be used to express the lexeme orthographically.",
        "Details are given later.",
        "Briefly, step 1 samples from a Gaussian prior.",
        "Step 2 samples a distribution from a Dirichlet process.",
        "This chooses a countable number of lexemes to have positive probability in the language, and decides which ones are most common.",
        "Step 3 samples a distribution from a Dirichlet.",
        "For the lexeme think, this might choose to make 1st-person singular more common than for typical verbs.",
        "Step 4 just samples IID from p(n).",
        "In our model, each part of speech generates its own lexicon: verbs are inflected differently from nouns (different parameters and number of inflections).",
        "The size and layout of (e.g.) verb paradigms is language-specific; we currently assume it is given by a linguist, along with a few supervised vE RB paradigms.",
        "At present, we use only a very simple exchangeable model of the corpus.",
        "We assume that each word was independently sampled from the lexicon given its part of speech, with no other attention to context.",
        "For example, a token of brechen may have been chosen by choosing frequent lexeme Anak from the verb lexicon; then choosing 1st-person plural given Jlmak; and finally looking up that inflection's spelling in JjrmA's paradigm.",
        "This final lookup is deterministic since the lexicon has already been generated."
      ]
    },
    {
      "heading": "3. A Sketch of Inference and Learning",
      "text": [
        "Our job in inference is to reconstruct the lexicon that was used and how each token was generated from it (i.e., which lexeme and inflection?).",
        "We use collapsed Gibbs sampling, repeatedly guessing a reanalysis of each token in the context of all others.",
        "Gradually, similar tokens get \"clustered\" into paradigms (section 4).",
        "The state of the sampler is illustrated in Fig. 2.",
        "The bottom half shows the current analyses of the verb tokens.",
        "Each is associated with a particular slot in some paradigm.",
        "We are now trying to reanalyze brechen at position 0.",
        "The dashed arrows show some possible analyses.",
        "briche – breche ?",
        "brichst .",
        "brechst brechen ?",
        "bricht brecht",
        "Figure 2: A state of the Gibbs sampler (note that the assumed generative process runs roughly top-to-bottom).",
        "Each corpus token i has been tagged with part of speech ti, lexeme £i and inflection si.",
        "Token © has been tagged as htak and 3rd sg., which locked the corresponding type spelling in the paradigm to the spelling w1 = bricht; similarly for © and ©.",
        "Now w7 is about to be reanalyzed.",
        "The key intuition is that the current analyses of the other verb tokens imply a posterior distribution over the verb lexicon, shown in the top half of the figure.",
        "First, because of the current analyses of © and ©, the 3rd-person spellings of JjreaA are already constrained to match wi and w3 (the \"lock\" icon).",
        "Second, belief propagation as in Fig. 1 tells us which other inflections of htak (the \"?\"",
        "icon) are plausibly spelled as brechen, and how likely they are to be spelled that way.",
        "Finally, the fact that other tokens are associated with JjrmA suggest that this is a popular lexeme, making it a plausible explanation of 0 as well.",
        "(This is the \"rich get richer\" property of the Chinese restaurant process; see section 6.6.)",
        "Furthermore, certain inflections of JjreaA appear to be especially popular.",
        "In short, given the other analyses, we know which inflected lexemes in the lexicon are likely, and how likely each one is to be spelled as brechen.",
        "This lets us compute the relative probabilities of the possible analyses of token 0, so that the Gibbs sampler can accordingly choose one of these analyses at random.",
        "For a given °, this Gibbs sampler converges to the posterior distribution over analyses of the full corpus.",
        "To improve our estimate, we periodically adjust to maximize or increase the probability of the most recent sample(s).",
        "For example, having tagged w5 =",
        "springt as s5 = 2nd-person plural may strengthen our estimated probability that 2nd-person spellings tend to end in -t. That revision to °, in turn, will influence future moves of the sampler.",
        "If the sampler is run long enough between calls to the optimizer, this is a Monte Carlo EM procedure (see end of section 1.2).",
        "It uses the data to optimize a language-specific prior p(n) over paradigms – an empirical Bayes approach.",
        "(A fully Bayesian approach would resample as part of the Gibbs sampler.)",
        "The lexicon is collapsed out of our sampler, in the sense that we do not represent a single guess about the infinitely many lexeme probabilities and paradigms.",
        "What we store about the lexicon is information about its full posterior distribution: the top half of Fig. 2.",
        "Fig.",
        "2 names its lexemes as and for expository purposes, but of course the sampler cannot reconstruct such labels.",
        "Formally, these labels are collapsed out, and we represent lexemes as anonymous objects.",
        "Tokens © and © are tagged with the same anonymous lexeme (which will correspond to sitting at the same table in a Chinese restaurant process).",
        "For each lexeme £ and inflection s, we maintain pointers to any tokens currently tagged with the slot (£, s).",
        "We also maintain an approximate marginal distribution over the spelling of that slot:",
        "1.",
        "If (£, s) points to at least one token i, then we know (£, s) is spelled as wj (with probability 1).",
        "2.",
        "Otherwise, the spelling of (£, s) is not known.",
        "But if some spellings in £'s paradigm are known, store a truncated distribution that enumerates the 25 most likely spellings for (£, s), according to loopy belief propagation within the paradigm.",
        "3.",
        "Otherwise, we have observed nothing about £: it is currently unused.",
        "All such £ share the same marginal distribution over spellings of (£, s): the marginal of the prior p(n).",
        "Here a 25-best list could not cover all plausible spellings.",
        "Instead we store a probabilistic finite-state language model that approximates this marginal.",
        "springe – sprenge ?",
        "springen sprengen ?",
        "sprangst ^ sprengst ?",
        "springt _ ^",
        "springt sprengt ?",
        "springen _ fcsprengen ?",
        "n ex i",
        "& 4",
        "P ti",
        "VERB",
        "PRON",
        "VERB NOUN VERB",
        "PREP \\VERB \\",
        "ex Ii",
        "AreaA",
        "%f 1st pl.",
        "7 \\",
        "n Si",
        "3rd sg.",
        "3rd pl.",
        "2nd pl.",
        "Vrd.pl>'",
        "pell Wi",
        "bricht",
        "brechen springt",
        "brechen",
        "A hash table based on cases 1 and 2 can now be used to rapidly map any word w to a list of slots of existing lexemes that might plausibly have generated w. To ask whether w might instead be an inflection s of a novel lexeme, we score w using the probabilistic finite-state automata from case 3, one for each s.",
        "The Gibbs sampler randomly chooses one of these analyses.",
        "If it chooses the \"novel lexeme\" option, we create an arbitrary new lexeme object in memory.",
        "The number of explicitly represented lexemes is always finite (at most the number of corpus tokens)."
      ]
    },
    {
      "heading": "4. Interpretation as a Mixture Model",
      "text": [
        "It is common to cluster points in Rn by assuming that they were generated from a mixture ofGaussians, and trying to reconstruct which points were generated from the same Gaussian.",
        "We are similarly clustering word tokens by assuming that they are generated from a mixture of weighted paradigms.",
        "After all, each word token was obtained by randomly sampling a weighted paradigm (i.e., a cluster) and then randomly sampling a word from it.",
        "Just as each Gaussian in a Gaussian mixture is a distribution over all points Rn, each weighted paradigm is a distribution over all spellings X* (but assigns probability > 0 to only a finite subset of X*).",
        "Inference under our model clusters words together by tagging them with the same lexeme.",
        "It tends to group words that are \"similar\" in the sense that the base distribution p(n) predicts that they would tend to co-occur within a paradigm.",
        "Suppose a corpus contains several unlikely but similar tokens, such as discombobulated and discombobulating.",
        "A language might have one probable lexeme from whose paradigm all these words were sampled.",
        "It is much less likely to have several probable lexemes that all coincidentally chose spellings that started with discombobulat-.",
        "Generating discombobulat-only once is cheaper (especially for such a long prefix), so the former explanation has higher probability.",
        "This is like explaining nearby points in Rn as samples from the same Gaussian.",
        "Of course, our model is sensitive to more than shared prefixes, and it does not merely cluster words into a paradigm but assigns them to particular inflectional slots in the paradigm.",
        "fined as at the end of section 2.2.",
        "If not, one could still try belief propagation; or one could approximate by estimating a language model from the spellings associated with slot s by cases 1 and 2.",
        "Our mixture model uses an infinite number of mixture components.",
        "This avoids placing a prior bound on the number of lexemes or paradigms in the language.",
        "We assume that a natural language has an infinite lexicon, although most lexemes have sufficiently low probability that they have not been used in our training corpus or even in human history (yet).",
        "Our specific approach corresponds to a Bayesian technique, the Dirichlet process mixture model.",
        "Appendix A (supplementary material) explains the DPMM and discusses it in our context.",
        "The DPMM would standardly be presented as generating a distribution over countably many Gaussians or paradigms.",
        "Our variant in section 2.3 instead broke this into two steps: it first generated a distribution over countably many lexemes (step 2), and then generated a weighted paradigm for each lexeme (steps 3^).",
        "This construction keeps distinct lexemes separate even if they happen to have identical paradigms (polysemy).",
        "See Appendix A for a full discussion."
      ]
    },
    {
      "heading": "5. Formal Notation 5.1 Value Types",
      "text": [
        "We now describe our probability model in more formal detail.",
        "It considers the following types of mathematical objects.",
        "(We use consistent lowercase letters for values of these types, and consistent fonts for constants of these types.)",
        "A word w, such as broken, is a finite string of any length, over some finite, given alphabet X.",
        "A part-of-speech tag t, such as VERB, is an element of a certain finite set T, which in this paper we assume to be given.",
        "An inflection s, such as past participle, is an element of a finite set St. A token's part-of-speech tag t G T determines its set St of possible inflections.",
        "For tags that do not inflect, \\St\\ = 1.",
        "The sets Stare language-specific, and we assume in this paper that they are given by a linguist rather than learned.",
        "A linguist also specifies features of the inflections: the grid layout in Table 1 shows that 4 of the 12 inflections in SVERB share the \"2nd-person\" feature.",
        "A paradigm for t G T is a mapping n : St – X*, specifying a spelling for each inflection in St. Table 1 shows one verb paradigm.",
        "A lexeme l is an abstract element of some lexical space L. Lexemes have no internal semantic structure: the only question we can ask about a lexeme is whether it is equal to some other lexeme.",
        "There is no upper bound on how many lexemes can be discovered in a text corpus; L is infinite.",
        "Our generative model of the corpus is a joint probability distribution over a collection of random variables.",
        "We describe them in the same order as section 1.2.",
        "Tokens: The corpus is represented by token variables.",
        "In our setting the sequence of words w = wi,..., wn G X* is observed, along with n. We must recover the corresponding part-of-speech tags t = ti,...,tn G T, lexemes l = li,...,ln G L, and inflections ss = si,..., sn, where (Vi)si G Sti.",
        "Types: The lexicon is represented by type variables.",
        "For each of the infinitely many lexemes l G L, and each t G T, the paradigm Trtte is a function St – X*.",
        "For example, Table 1 shows a possible value nVERB j>yrmßl.",
        "The various spellings in the paradigm, such as nVERB^iaÂ(1st-person sing.",
        "pres.",
        ")=breche, are string-valued random variables that are correlated with one another.",
        "Since the lexicon is to be probabilistic (section 2.3), Gt(l) denotes tag t's distribution over lexemes l G L, while Ht>t(s) denotes the tagged lexeme (t,l)'s distribution over inflections s G St.",
        "Grammar: Global properties of the language are captured by grammar variables that cut across lexical entries: our parameters that describe typical inflectional alternations, plus parameters (f)t, at, a't, t (explained below).",
        "Their values control the overall shape of the probabilistic lexicon that is generated."
      ]
    },
    {
      "heading": "6. The Formal Generative Model",
      "text": [
        "We now fully describe the generative process that was sketched in section 2.",
        "Step by step, it randomly chooses an assignment to all the random variables of section 5.2.",
        "Thus, a given assignment's probability – which section 3 s algorithms consult in order to resample or improve the current assignment – is the product of the probabilities of the individual choices, as described in the sections below.",
        "(Appendix B provides a drawing of this as a graphical model.)",
        "First select the grammar variables from a prior.",
        "(We will see below how these variables get used.)",
        "Our experiments used fairly flat priors.",
        "Each weight in or </>t is drawn IID from N(0,10), and each at or oltfrom a Gamma with mode 10 and variance 1000.",
        "For each t G T, let Dt(n) denote the distribution over paradigms that was presented in section 2.2 (where it was called p(n)).",
        "Dt is fully specified by our graphical model for paradigms of part of speech t, together with its parameters as generated above.",
        "This is the linguistic core of our model.",
        "It considers spellings: DVERB describes what verb paradigms typically look like in the language (e.g., Table 1).",
        "Parameters in may be shared across parts of speech t. These \"backoff\" parameters capture general phonotactics of the language, such as prohibited letter bigrams or plausible vowel changes.",
        "For each possible tagged lexeme (t, l), we now draw a paradigm nt,t from Dt.",
        "Most of these lexemes will end up having probability 0 in the language.",
        "We now formalize section 2.3.",
        "For each t G T, the language has a distribution Gt(l) over lexemes.",
        "We draw Gt from a Dirichlet process DP(G, at), where G is the base distribution over L, and at > 0 is a concentration parameter generated above.",
        "If atis small, then Gt will tend to have the property that most of its probability mass falls on relatively few of the lexemes in Lt = {l G L : Gt(l) > 0}.",
        "A closed-class tag is one whose at is especially small.",
        "For G to be a uniform distribution over an infinite lexeme set L, we need L to be uncountable.",
        "However, it turns out that with probability 1, each Lt is countably infinite, and all the Lt are disjoint.",
        "So each lexeme l G L is selected by at most one tag t.",
        "For each tagged lexeme (t, l), the language specifies some distribution Ht,t over its inflections.",
        "First we construct backoff distributions Ht that are independent of I.",
        "For each tag t G T, let Ht be some base distribution over St. As St could be large in some languages, we exploit its grid structure (Table 1) to reduce the number of parameters of Ht.",
        "We take Ht to be a log-linear distribution with parameters </>tthat refer to features of inflections.",
        "E.g., the 2nd-person inflections might be systematically rare.",
        "Now we model each Ht& as an independent draw from a finite-dimensional Dirichlet distribution with mean Ht and concentration parameter at.",
        "E.g., tfwnk might be biased toward Ist-person sing.",
        "present.",
        "6.5 Part-of-Speech Tag Sequence p(t \\ t)",
        "In our current experiments, t is given.",
        "But in general, to discover tags and inflections simultaneously, we can suppose that the tag sequence t (and its length n) are generated by a Markov model, with tag bigram or trigram probabilities specified by some parameters t.",
        "We turn to section 2.4.",
        "A lexeme token depends on its tag: draw li from Gti, so p(£i \\ Gti ) = Gti (£{).",
        "6.7 Inflections p(si \\ Htitii)",
        "An inflection slot depends on its tagged lexeme: we draw Si from HtiA, so p(si \\ Hti,ii) = HtiA(si).",
        "Finally, we generate the word wi through a deterministic spell-out step.",
        "Given the tag, lexeme, and inflection at position i, we generate the word wi simply by looking up its spelling in the appropriate paradigm.",
        "So p(wi \\ 7TtiA (si)) is 1 if Wi = TTti,ei (si), else 0.",
        "Again, a full assignment's probability is the product of all the above factors (see drawing in Appendix B).",
        "But computationally, our sampler's state leaves the Gt unspecified.",
        "So its probability is the integral of p(assignment) over all possible Gt.",
        "As Gt appears only in the factors from headings 6.3 and 6.6, we can just integrate it out of their product, to get a collapsed sub-model that generates p(l \\ t, a) directly:",
        "where it turns out that the factor that generates li is proportional to \\ {j < i : lj = li and tj = ti}\\ if that integer is positive, else proportional to atiG(li).",
        "Metaphorically, each tag t is a Chinese restaurant whose tables are labeled with lexemes.",
        "The tokens are hungry customers.",
        "Each customer i = 1, 2,..., n enters restaurant ti in turn, and li denotes the label of the table she joins.",
        "She picks an occupied table with probability proportional to the number of previous customers already there, or with probability proportional to ati she starts a new table whose label is drawn from G (it is novel with probability 1, since G gives infinitesimal probability to each old label).",
        "Similarly, we integrate out the infinitely many lexeme-specific distributions Ht& from the product of 6.4 and 6.7, replacing it by the collapsed distribution where the factor for si is proportional to {j < i : sj = si and (tj,lj) = (ti,li)}\\ + atiHu(si).",
        "Metaphorically, each table l in Chinese restaurant t has a fixed, finite set of seats corresponding to the inflections s G St. Each seat is really a bench that can hold any number of customers (tokens).",
        "When customer i chooses to sit at table li, she also chooses a seat si at that table (see Fig. 2), choosing either an already occupied seat with probability proportional to the number of customers already in that seat, or else a random seat (sampled from Hti and not necessarily empty) with probability proportional to ati."
      ]
    },
    {
      "heading": "7. Inference and Learning",
      "text": [
        "As section 3 explained, the learner alternates between a Monte Carlo E step that uses Gibbs sampling to sample from the posterior of (s,£,t) given W and the grammar variables, and an M step that adjusts the grammar variables to maximize the probability of the (w, s, £, t) samples given those variables.",
        "As in Gibbs sampling for the DPMM, our sampler's basic move is to reanalyze token i (see section 3).",
        "This corresponds to making customer i invisible and then guessing where she is probably sitting – which restaurant t, table £, and seat s? – given knowledge of wi and the locations of all other customers.",
        "Concretely, the sampler guesses location (ti, £i, si) with probability proportional to the product of",
        "• the probability (from section 6.9) that a new customer in restaurant ti chooses table £i, given the other customers in that restaurant (and ati )",
        "• the probability (from section 6.9) that a new customer at table £i chooses seat si, given the other customers at that table (and </>ti and a't.)",
        "• the probability (from section 3.3's belief propagation) that 7TtiA (si) = Wi (given 6).",
        "We sample only from the (ti,£i, si) candidates for which the last factor is non-negligible.",
        "These are found with the hash tables and FSAs of section 3.3.",
        "Our experiments also consider the semi-supervised case where a few seed paradigms – type data – are fully or partially observed.",
        "Our samples should also be conditioned on these observations.",
        "We assume that our supervised list of observed paradigms was generated by sampling from Gt.",
        "We can modify our setup for this case: certain tables have a host who dictates the spelling of some seats and attracts appropriate customers to the table.",
        "See Appendix C.",
        "Table 2: Whole-word accuracy and edit distance of predicted inflection forms given the lemma.",
        "Edit distance to the correct form is measured in characters.",
        "Best numbers per set of seed paradigms in bold (statistically significant on our large test set under a paired permutation test, p < 0.05).",
        "Appendix E breaks down these results per inflection and gives an error analysis and other statistics.",
        "8 Experiments 8.1 Experimental Design",
        "We evaluated how well our model learns German verbal morphology.",
        "As corpus we used the first 1 million or 10 million words from WaCky (Baroni et al., 2009).",
        "For seed and test paradigms we used verbal inflectional paradigms from the CELEX morphological database (Baayen et al., 1995).",
        "We fully observed the seed paradigms.",
        "For each test paradigm, we observed the lemma type (Appendix C) and evaluated how well the system completed the other 21 forms (see Appendix E.2) in the paradigm.",
        "We simplified inference by fixing the POS tag sequence to the automatic tags delivered with the WaCky corpus.",
        "The result that we evaluated for each variable was the value whose probability, averaged over the entire Monte Carlo EM run, was highest.",
        "For more details, see (Dreyer, 2011).",
        "All results are averaged over 10 different training/test splits of the CELEX data.",
        "Each split sampled 100 paradigms as seed data and used the remaining 5,415 paradigms for evaluation.",
        "From the 100 paradigms, we also sampled 50 to obtain results with smaller seed data.",
        "Type-based Evaluation.",
        "Table 2 shows the results of predicting verb inflections, when running with no corpus, versus with an unannotated corpus of size 10and 10 words.",
        "Just using 50 seed paradigms, but",
        "Corpus size",
        "50 se 0",
        "ed parad 10",
        "ligms 10",
        "100 se 0",
        "ed para 10",
        "digms 10",
        "Accuracy",
        "89.9",
        "90.6",
        "90.9",
        "91.5",
        "92.0",
        "92.2",
        "Edit dist.",
        "0.20",
        "0.19",
        "0.18",
        "0.18",
        "0.17",
        "0.17",
        "Table 3: The inflected verb forms from 5,615 inflectional paradigms, split into 5 token frequency bins.",
        "The frequencies are based on the 10-million word corpus.",
        "no corpus, gives an accuracy of 89.9%.",
        "By adding a corpus of 10 million words we reduce the error rate by 10%, corresponding to a one-point increase in absolute accuracy to 90.9%.",
        "A similar trend can be seen when we use more seed paradigms.",
        "Simply training on 100 seed paradigms, but not using a corpus, results in an accuracy of 91.5%.",
        "Adding a corpus of 10 million words to these 100 paradigms reduces the error rate by 8.3%, increasing the absolute accuracy to 92.2%.",
        "Compared to the large corpus, the smaller corpus of 1 million words goes more than half the way; it results in error reductions of 6.9% (50 seed paradigms) and 5.8% (100 seed paradigms).",
        "Larger unsupervised corpora should help by increasing coverage even more, although Zipf's law implies a diminishing rate of return.",
        "We also tested a baseline that simply inflects each morphological form according to the basic regular German inflection pattern; this reaches an accuracy of only 84.5%.",
        "Token-based Evaluation.",
        "We now split our results into different bins: how well do we predict the spellings of frequently expressed (lexeme, inflection) pairs as opposed to rare ones?",
        "For example, the third person singular indicative of qi/m (geben) is used significantly more often than the second person plural subjunctive of A~uA (aalen); they are in different frequency bins (Table 3).",
        "The more frequent a form is in text, the more likely it is to be irregular (Jurafsky et al., 2000, p. 49).",
        "The results in Table 4 show: Adding a corpus of either 1 or 10 million words increases our prediction accuracy across all frequency bins, often dramatically.",
        "All methods do best on the huge number of",
        "Table 4: Token-based analysis: Whole-word accuracy results split into different frequency bins.",
        "In the last two rows, all predictions are included, weighted by the frequency of the form to predict.",
        "Last row is edit distance.",
        "rare forms (Bin 1), which are mostly regular, and worst on on the 10 most frequent forms of the language (Bin 5).",
        "However, adding a corpus helps most in fixing the errors in bins with more frequent and hence more irregular verbs: in Bins 2-5 we observe improvements of up to almost 8% absolute percentage points.",
        "In Bin 1, the no-corpus baseline is already relatively strong.",
        "Surprisingly, while we always observe gains from using a corpus, the gains from the 10-million-word corpus are sometimes smaller than the gains from the 1-million-word corpus, except in edit distance.",
        "Why?",
        "The larger corpus mostly adds new infrequent types, biasing 0 toward regular morphology at the expense of irregular types.",
        "A solution might be to model irregular classes with separate parameters, using the latent conjugation-class model of Dreyer et al.",
        "(2008).",
        "Note that, by using a corpus, we even improve our prediction accuracy for forms and spellings that are not found in the corpus, i.e., novel words.",
        "This is thanks to improved grammar parameters.",
        "In the token-based analysis above we have already seen that prediction accuracy increases for rare forms (Bin 1).",
        "We add two more analyses that more explicitly show our performance on novel words.",
        "(a) We find all paradigms that consist of novel spellings only, i.e. none of the correct spellings can be found in the corpus.",
        "The whole-word prediction accuracies for the models that use corpus size 0, 1 million, and 10 million words are, respectively, 94.0%, 94.2%, 94.4% using 50 seed paradigms, and 95.1%, 95.3%, 95.2% using 100 seed paradigms.",
        "(b) Another, simpler measure is the prediction accuracy on all forms whose correct spelling cannot be found in the 10-million-word corpus.",
        "Here we measure accuracies of 91.6%, 91.8% and 91.8%, respectively, using 50 seed paradigms.",
        "With 100 seed paradigms, we have 93.0%, 93.4% and 93.1%.",
        "The accuracies for the models that use a corpus are higher, but do not always steadily increase as we increase the corpus size.",
        "Bin",
        "Frequency",
        "# Verb Forms",
        "1",
        "0-9",
        "116,776",
        "2",
        "10-99",
        "4,623",
        "3",
        "100-999",
        "1,048",
        "4",
        "1,000-9,999",
        "95",
        "5",
        "10,000-",
        "10",
        "all",
        "any",
        "122,552",
        "50 seed paradigms",
        "100 seed paradigms",
        "Bin",
        "0",
        "10",
        "10",
        "0",
        "10",
        "10",
        "1",
        "90.5",
        "91.0",
        "91.3",
        "92.1",
        "92.4",
        "92.6",
        "2",
        "78.1",
        "84.5",
        "84.4",
        "80.2",
        "85.5",
        "85.1",
        "3",
        "71.6",
        "79.3",
        "78.1",
        "73.3",
        "80.2",
        "79.1",
        "4",
        "57.4",
        "61.4",
        "61.8",
        "57.4",
        "62.0",
        "59.9",
        "5",
        "20.7",
        "25.0",
        "25.0",
        "20.7",
        "25.0",
        "25.0",
        "all",
        "52.6",
        "57.5",
        "57.8",
        "53.4",
        "58.5",
        "57.8",
        "all (e.d.)",
        "1.18",
        "1.07",
        "1.03",
        "1.16",
        "1.02",
        "1.01",
        "The token-based analysis we have conducted here shows the strength of the corpus-based approach presented in this paper.",
        "While the integrated graphical models over strings (Dreyer and Eisner, 2009) can learn some basic morphology from the seed paradigms, the added corpus plays an important role in correcting its mistakes, especially for the more frequent, irregular verb forms.",
        "For examples of specific errors that the models make, see Appendix E.3.",
        "9 Related Work",
        "Our word-and-paradigm model seamlessly handles nonconcatenative and concatenative morphology alike, whereas most previous work in morphological knowledge discovery has modeled concatenative morphology only, assuming that the orthographic form of a word can be split neatly into stem and affixes – a simplifying asssumption that is convenient but often not entirely appropriate (Kay, 1987) (how should one segment English stopping, hoping, or knives?",
        ").",
        "In concatenative work, Harris (1955) finds morpheme boundaries and segments words accordingly, an approach that was later refined by Hafer and Weiss (1974), Déjean (1998), and many others.",
        "The unsupervised segmentation task is tackled in the annual Morpho Challenge (Kurimo et al., 2010), where ParaMor (Monson et al., 2007) and Morfessor (Creutz and Lagus, 2005) are influential contenders.",
        "The Bayesian methods that Goldwater et al.",
        "(2006b, et seq.)",
        "use to segment between words might also be applied to segment within words, but have no notion of paradigms.",
        "Goldsmith (2001) finds what he calls signatures – sets of affixes that are used with a given set of stems, for example (NULL, -er, -ing, -s).",
        "Chan (2006) learns sets of morphologically related words; he calls these sets paradigms but notes that they are not substructured entities, in contrast to the paradigms we model in this paper.",
        "His models are restricted to concatenative and regular morphology.",
        "Morphology discovery approaches that handle nonconcatenative and irregular phenomena are more closely related to our work; they are rarer.",
        "Yarowsky and Wicentowski (2000) identify inflection-root pairs in large corpora without supervision.",
        "Using similarity as well as distributional clues, they identify even irregular pairs like take/took.",
        "Schone and Jurafsky (2001) and Baroni et al.",
        "(2002) extract whole conflation sets, like \"abuse, abused, abuses, abusive, abusively, ...,\" which may also be irregular.",
        "We advance this work by not only extracting pairs or sets of related observed words, but whole structured inflectional paradigms, in which we can also predict forms that have never been observed.",
        "On the other hand, our present model does not yet use contextual information; we regard this as a future opportunity (see Appendix G).",
        "Naradowsky and Goldwater (2009) add simple spelling rules to the Bayesian model of (Goldwater et al., 2006a), enabling it to handle some systematically nonconcate-native cases.",
        "Our finite-state transducers can handle more diverse morphological phenomena.",
        "10 Conclusions and Future Work",
        "We have formulated a principled framework for simultaneously obtaining morphological annotation, an unbounded morphological lexicon that fills complete structured morphological paradigms with observed and predicted words, and parameters of a non-concatenative generative morphology model.",
        "We ran our sampler over a large corpus (10 million words), inferring everything jointly and reducing the prediction error for morphological inflections by up to 10%.",
        "We observed that adding a corpus increases the absolute prediction accuracy on frequently occurring morphological forms by up to almost 8%.",
        "Future extensions to the model could leverage token context for further improvements (Appendix G).",
        "We believe that a major goal of our field should be to build full-scale explanatory probabilistic models of language.",
        "While we focus here on inflectional morphology and evaluate the results in isolation, we regard the present work as a significant step toward a larger generative model under which Bayesian inference would reconstruct other relationships as well (e.g., inflectional, derivational, and evolutionary) among the words in a family of languages."
      ]
    }
  ]
}
