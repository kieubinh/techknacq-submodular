{
  "info": {
    "authors": [
      "Swapna Gottipati",
      "Jing Jiang"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1074",
    "title": "Linking Entities to a Knowledge Base with Query Expansion",
    "url": "https://aclweb.org/anthology/D11-1074",
    "year": 2011
  },
  "references": [
    "acl-C10-1032",
    "acl-C10-1145",
    "acl-D07-1074",
    "acl-E06-1002",
    "acl-N10-1072"
  ],
  "sections": [
    {
      "text": [
        "swapnag.2 010@smu.edu.sg",
        "jingjiang@smu.edu.sg",
        "In this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion.",
        "We use both local contexts and global world knowledge to expand query language models.",
        "We place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query.",
        "Our experiments on the TAC-KBP 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance.",
        "Compared with the official results from KBP 2010 participants, our system shows competitive performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "When people read news articles, Web pages and other documents online, they may encounter named entities which they are not familiar with and therefore would like to look them up in an encyclopedia.",
        "It would be very useful if these entities could be automatically linked to their corresponding encyclopedic entries.",
        "This task of linking mentions of entities within specific contexts to their corresponding entries in an existing knowledge base is called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang, 2009).",
        "Besides improving an online surfer's browsing experience, entity linking also has potential usage in many other applications such as normalizing entity mentions for information extraction.",
        "The major challenge of entity linking is to resolve name ambiguities.",
        "There are generally two types of ambiguities: (1) Polysemy: This type of ambiguities refers to the case when more than one entity shares the same name.",
        "E.g. George Bush may refer to the 41st President of the U.S., the 43rd President of the U.S., or any other individual who has the same name.",
        "Clearly polysemous names cause difficulties for entity linking.",
        "(2) Synonymy: This type of ambiguities refers to the case when more than one name variation refers to the same entity.",
        "E.g. Metro-Goldwyn-Mayer Inc. is often abbreviated as MGM.",
        "Synonymy affects entity linking when the entity mention in the document uses a name variation not covered in the entity's knowledge base entry.",
        "Intuitively, to disambiguate a polysemous entity name, we should make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations.",
        "Indeed both strategies have been explored in existing literature (Zhang et al., 2010; Dredze et al., 2010; Zheng et al., 2010).",
        "However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data.",
        "In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval.",
        "We use the KL-divergence retrieval model (Zhai and Lafferty, 2001) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web.",
        "We evaluate our retrieval method with query expansion on the 2010 TAC-KBP data set.",
        "We find that our expanded query language models can indeed improve the performance significantly, demonstrating the effectiveness of our principled and yet simple techniques.",
        "Comparison with the official results from KBP participants also shows that our system is competitive.",
        "In particular, when no disambiguation text from the knowledge base is used, our system can achieve an overall 85.2% accuracy and 9.3% relative improvement over the best performance reported in",
        "KBP 2010."
      ]
    },
    {
      "heading": "2. Task Definition and System Overview",
      "text": [
        "Following TAC-KBP (Ji et al., 2010), we define the entity linking task as follows.",
        "First, we assume the existence of a Knowledge Base (KB) of entities.",
        "Each KB entry E represents a unique entity and has three fields: (1) a name string Ne, which can be regarded as the official name of the entity, (2) an entity type TE, which is one of {PER, ORG, GPE, UNKNOWN}, and (3) some disambiguation text De .",
        "Given a query Q which consists of a query name string Nq and a query document Dq where the name occurs, the task is to return a single KB entry to which the query name string refers or Nil if there is no such KB entry.",
        "It is fairly natural to address entity linking by ranking the KB entries given a query.",
        "In this section we present an overview of our system, which consists of two major stages: a candidate selection stage to identify a set of candidate KB entries through name matching, and a ranking stage to link the query entity to the most likely KB entry.",
        "In both stages, we consider the query's local context in the query document and world knowledge obtained from the Web.",
        "It is important to note that the selection stage is based on string matching where the order of the word matters.",
        "It is different from the ranking stage where a probabilistic retrieval model based on bag-of-word representation is used.",
        "Our preliminary experiments demonstrate that without the first candidate selection stage the linking process results in low performance.",
        "The first stage of our system aims to filter out irrelevant KB entries and select only a set of candidates that are potentially the correct match to the query.",
        "Intuitively, we determine whether two entities are the same by comparing their name strings.",
        "We therefore need to compare the query name string Nq with the name string Ne of each KB entry.",
        "However, because of the name ambiguity problem, we cannot expect the correct KB entry to always have exactly the same name string as the query.",
        "To address this problem, we use a set of alternative name strings expanded from Nq and select KB entries whose name strings match at least one of them.",
        "These alternative name strings come from two sources: the query document Dq and the Web.",
        "Symbol",
        "Description",
        "Q",
        "Query",
        "Dq",
        "Query document",
        "Nq",
        "Query name string",
        "E",
        "KB entity node",
        "Ne",
        "KB entity name string",
        "De",
        "KB entity disambiguation text",
        "Sq",
        "Set of alternate query name strings",
        "NqQ",
        "Local alternative name strings",
        "nQ",
        "Global alternative name strings",
        "Eq",
        "Candidate KB entries for Q",
        "Query Language Model",
        "KB entry language model using local context from Dq",
        "KB entry language model using global knowledge",
        "q",
        "KB entry language model using local context and global knowledge",
        "KB entry language model with named entities only",
        "6ne+de",
        "KB entry language model with named entities and disambiguation text",
        "First, we observe that some useful alternative name strings come from the query document.",
        "For example, a PER query name string may contain only a person's last name but the query document contains the person's full name, which is clearly a less ambiguous name string to use.",
        "Similarly, a GPE query name string may contain only the name of a city or town but the query document contains the state or province, which also helps disambiguate the query entity.",
        "Based on this observation, we do the following.",
        "Given query Q, let Sq denote the set of alternative query name strings.",
        "Initially Sq contains only Nq.",
        "We then use an off-the-shelf NER tagger to identify named entities from the query document Dq.",
        "For PER and ORG queries, we select named entities in Dq that contain Nq as a substring.",
        "For GPE queries, we select named entities that are of the type GPE, and we then combine each of them with Nq.",
        "We denote these alternative name strings as {nQ*}^, where l indicates that these name strings come locally from Dq and Kq is the total number of such name strings.",
        "{nQ*} are added to Sq.",
        "Figure 1 and Figure 2 show two example queries together with their Sq.",
        "Sometimes alternative name strings have to come from external knowledge.",
        "For example, one of the queries we have contains the name string \"AMPAS,\" and the query document also uses only this acronym to refer to this entity.",
        "But the full name of the entity, \"Academy of Motion Pictures Arts and Sciences,\" is needed in order to locate the correct KB entry.",
        "To tackle this problem, we leverage Wikipedia to find the most likely official name.",
        "Given query name string Nq, we check whether the following link exists: http://en.wikipedia.org/NQ.",
        "If Nq is an abbreviation, Wikipedia will redirect the link to the Wikipedia page of the corresponding entity with its official name.",
        "So if the link exists, we use the title of the Wikipedia page as another alternative name string for Nq.",
        "We refer to this name string as NQ to indicate that it is a global name variant.",
        "NQ is also added to Sq.",
        "Figure 2 shows such an example.",
        "For each name string N in Sq, we find KB entries whose name strings match N. We take the union of",
        "Query name string (Nq): Mobile Query document (Dq): The site is near Mount Vernon in the Calvert community on the Tombigbee River, some 25 miles (40 kilometers) north of Mobile.",
        "It's on a river route to the Gulf of Mexico and near Mobile's rails and interstates.",
        "Along with tax breaks and $400 million (euro297 million) in financial incentives, Alabama offered a site with a route to a Brazil plant that will provide slabs for processing in Mobile.",
        "Alternative Query Strings (Sq): from local context: Mobile, Mobile Mount Vernon, Mobile Calvert, Mobile River, Mobile Mexico, Mobile Alabama, Mobile Brazil Query name string (Nq): Coppola Query document (Dq): I had no idea of all these semi-obscure connections, felicia!",
        "Alex Greenwald and Claire Oswalt aren't names I'm at all familiar with, but Jason Schwartzman I've heard of.",
        "Isn't he Sophia Coppola's cousin?",
        "I think I once saw a picture of him sometime ago Alternative Query Strings (Sq): from local context: Coppola, Sophia Coppola, Sofia Coppola",
        "from world knowledge(Wikipedia): Sofia Coppola Figure 2: An example PER query from TAC 2010.",
        "these sets of KB entries and refer to it as Eq.",
        "These are the candidate KB entries for query Q.",
        "Given the candidate KB entries Eq, we need to decide which one of them is the correct match.",
        "We adopt the widely-used KL-divergence retrieval model, a statistical language model-based retrieval method proposed by Lafferty and Zhai (2001).",
        "Given a KB entry E and query Q, we score E based on the KL-divergence defined below:",
        "Here 9q and 9E are the query language model and the KB entry language model, respectively.",
        "A language model here is a multinomial distribution over words (i.e. a unigram language model).",
        "V is the vocabulary and w is a single word.",
        "To estimate 9E, we follow the standard maximum likelihood estimation with Dirichlet smooth-",
        "where c(w, E) is the count of w in E, \\E\\ is the number of words in E, 9C is a background language model estimated from the whole KB, and ß is the Dirichlet prior.",
        "Recall that E contains Ne , TEand De .",
        "We consider using either Ne only or both Ne and De to obtain c(w,E) and \\E\\.",
        "We refer to the former estimated 9E as 9Ne and the latter as",
        "To estimate 9q, typically we can use the empirical query word distribution:",
        "where c(w, Nq) is the count of w in Nq and \\Nq\\ is the length of Nq .",
        "We call this model the original query language model.",
        "After ranking the candidate KB entries in Eq using Equation (1), we perform entity linking as follows.",
        "First, using an NER tagger, we determine the entity type of the query name string Nq.",
        "Let Tq denote this entity type.",
        "We then pick the top-ranked KB entry whose score is higher than a threshold t and whose TE is the same as Tq.",
        "The system links the query entity to this KB entry.",
        "If no such entry exists, the system returns Nil."
      ]
    },
    {
      "heading": "3. Query Expansion",
      "text": [
        "We have shown in Section 2.1 that using the original query name string Nq itself may not be enough to obtain the correct KB entry, and additional words from both the query document and external knowledge can be useful.",
        "However, in the KB entry selection stage, these additional words are only used to enlarge the set of candidate KB entries; they have not been used to rank KB entries.",
        "In this section, we discuss how to expand the query language model 9q with these additional words in a principled way in order to rank KB entries based on how likely they match the query entity.",
        "Let us look at the example from Figure 2 again.",
        "During the KB entry ranking stage, if we use 9q estimated from Nq, which contains only the word \"Coppola,\" the retrieval function is unlikely to rank the correct KB entry on the top.",
        "But if we include the contextual word \"Sophia\" from the query document when estimating the query language model, KL-divergence retrieval model is likely to rank the correct KB entry on the top.",
        "This idea of using contextual words to expand the query is very similar to (pseudo) relevance feedback in information retrieval.",
        "We can treat the query document Dq as our only feedback document.",
        "Many different (pseudo) relevance feedback methods have been proposed.",
        "Here we apply the relevance model (Lavrenko and Croft, 2001), which has been shown to be effective and robust in a recent comparative study (Lv and Zhai, 2009).",
        "We first briefly review the relevance model.",
        "Given a set of (pseudo) relevant documents Dr, where for each D g Dr there is a document language model 9D, we can estimate a feedback language model 9q as follows:",
        "For our problem, since we have only a single feedback document Dq, the equation above can be simplified.",
        "In fact, in this case the feedback language model is the same as the document language model of the only feedback document, i.e. 9Dq .",
        "We then linearly interpolate the feedback language model with the original query language model to form an expanded query language model:",
        "where a is a parameter between 0 and 1, to control the amount of feedback.",
        "The larger a is, the less we rely on the local context.",
        "L indicates that the query expansion comes from local context.",
        "This 9q can then replace 9q in Equation (1) to rank KB entries.",
        "Special Treatment of Named Entities",
        "Usually the document language model 9Dq is estimated using the entire text from Dq.",
        "For entity linking, we suspect that named entities surrounding the query name string in Dq are particularly useful for disambiguation and thus should be emphasized over other words.",
        "This can be done by weighting",
        "NE and non-NE words differently.",
        "In the extreme case, we can use only NEs to estimate the document language model 9Dq as follows:",
        "where {Nq} are defined in Section 2.",
        "Positional Model",
        "Another observation is that words closer to the query name string in the query document are likely to be more important than words farther away.",
        "Intuitively, we can use the distance between a word and the query name string to help weigh the word.",
        "Here we apply a recently proposed positional pseudo relevance feedback method (Lv and Zhai, 2010).",
        "The document language model 9Dq now has the following form:",
        "where pj and q are the absolute positions of NQjand Nq in Dq .",
        "The function / is Gaussian function defined as follows:",
        "where variance a controls the spread of the curve.",
        "Similar to the way we incorporate words from Dq into the query language model, we can also construct a feedback language model using the most likely official name of the query entity obtained from Wikipedia.",
        "Specifically, we define",
        "We can then linearly interpolate 9Na with the original query language model 9 to form an expanded query language model 9^ :",
        "Here G indicates that the query expansion comes from global world knowledge.",
        "We can further combine the two kinds of additional words into the query language model as follows:",
        "Note that here we have two parameters a and ß to control the amount of contributions from the local context and from global world knowledge."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "Data Set: We evaluate our system on the TAC-KBP 2010 data set (Ji et al., 2010).",
        "The knowledge base was constructed from Wikipedia with 818,741 entries.",
        "The data set contains 2250 queries and query documents come from news wire and Web pages.",
        "Around 45% of the queries have non-Nil entries in the KB.",
        "Some statistics of the queries are shown in",
        "Table 2.",
        "Tools: In our experiments, to extract named entities within Dq and to determine Tq, we use the Stanford NER tagger.",
        "An example output of the NER tagger is shown below:",
        "<PERSON>Hugh Jackman<PERSON> is Jacked!",
        "!",
        "This piece of text comes from a query document where the query name string is \"Jackman.\"",
        "We can see that the NER tagger can help locate the full name of the person.",
        "We use the Lemur/Indri search engine for retrieval.",
        "It implements the KL-divergence retrieval model as well as many other useful functionalities.",
        "Evaluation Metric: We adopt the Micro-averaged accuracy metric, which is the mean accuracy over all queries.",
        "It was used in TAC-KBP 2010 (Ji et al., 2010) as the official metric to evaluate the performance of entity linking.",
        "This metric is simply defined as the percentage of queries that have been correctly linked.",
        "Entity Type",
        "%Nil",
        "%non-Nil",
        "GPE",
        "32.8%",
        "67.2 %",
        "ORG",
        "59.5%",
        "40.5 %",
        "PER",
        "71.7%",
        "28.3 %",
        "Methods to Compare: Recall that our system consists of a KB entry selection stage and a KB entry ranking stage.",
        "At the selection stage, a set Sq of alternative name strings are used to select candidate KB entries.",
        "We first define a few settings where different alternative name string sets are used to select candidate KB entries:",
        "• Q represents the baseline setting which uses only the original query name string N to select candidate KB entries.",
        "• Q+L represents the setting where alternative name strings obtained from the query document Dq are combined with Nq to select candidate KB entries.",
        "• Q+G represents the setting where the alternative name string obtained from Wikipedia is combined with N to select candidate KB entries.",
        "• Q+L+G represents the setting as we described in Section 2.1, that is, alternative name strings from both Dq and Wikipedia are used together with N to select candidate KB entries.",
        "After selecting candidate KB entries, in the KB entry ranking stage, we have four options for the query language model and two options for the KB entry language model.",
        "For the query language model, we have (1) 9 , the original query language model, (2) 9Q, an expanded query language model using local context from Dq, (3) 9q, an expanded query language model using global world knowledge, and (4) 9Q+G, an expanded query language model using both local context and global world knowledge.",
        "For the KB entry language model, we can choose whether or not to use the KB disambiguation text De and obtain 9Ne and 9Ne+de , respectively.",
        "First, we compare the performance of KB entry selection stage for all four settings on non-Nil queries.",
        "The performance measure recall is defined as",
        "I 1, i/ E that refers to Q, exists in Eq recall = <",
        "I 0, otherwise",
        "The recall statistics in Table 3 shows that, Q+L+G has the highest recall of the KB candidate entries.",
        "Table 3: Comparing the effect of candidate entry selection using different methods - KB entry selection stage recall.",
        "Before examining the effect of query expansion in ranking, we now compare the effect of using different sets of alternative query name strings in the candidate KB entry selection stage.",
        "For this set of experiments, we fix the query language model to 9q and the KB entry language model to 9Ne in the ranking stage.",
        "Table 4 shows the performance of all the settings in terms of micro-averaged accuracy.",
        "The results shown in Tables 4, 5 and 6 are based on the optimum parameter settings.",
        "We can see that in terms of the overall performance, both Q+L and Q+G give better performance than Q with a 7.7% and a 9.9% relative improvement, respectively.",
        "Q+L+G gives the best performance with a 12.8% relative improvement over Q.",
        "If we further zoom into the results, we see that for ORG and PER queries, when no correct KB entry exists (i.e. the Nil case), the performance of Q, Q+L, Q+G and Q+L+G is very close, indicating that the additional alternative query name strings do not help.",
        "It shows that the alternative query name strings are most useful for queries that do have their correct entries in the KB.",
        "We now further analyze the impact of the expanded query language models 9Q, 9'G and 9Q+G.",
        "We first analyze the results without using the KB disambiguation text, i.e. using 9Ne .",
        "Table 5 shows the comparison between 9q and other expanded query language models in terms of micro-averaged accuracy.",
        "The results reveal that the expanded query language models can indeed improve the overall performance (the both Nil and non-Nil case) under all settings.",
        "This shows the effectiveness of using the principled query expansion technique coupled with KL-divergence retrieval model to rank KB entries.",
        "Method",
        "Recall(%)",
        "Q",
        "67.1",
        "Q+L",
        "89.7",
        "Q+G",
        "94.9",
        "Q+L+G",
        "98.2",
        "Table 4: Comparing the performance of using different sets of query name strings for candidate KB entry selection.",
        "9q and 9Ne are used in KB entry ranking.",
        "Table 5: Comparison between the performance of 6q and expanded query language models in terms of micro average accuracy.",
        "9Ne was used in ranking.",
        "On the other hand, again we observe that the effects on the Nil and the non-Nil queries are different.",
        "While in Table 4 the alternative name strings do not affect the performance much for Nil queries, now the expanded query language models actually hurt the performance for Nil queries.",
        "It is not surprising to see this result.",
        "When we expand the query language model, we can possibly introduce noise, especially when we use the external knowledge obtained from Wikipedia, which largely depends on what Wikipedia considers to be the most popular official name of a query name string.",
        "With noisy terms in the expanded query language model we increase the chance to link the query to a KB entry which is not the correct match.",
        "The challenge is that we do not know when additional terms in the expanded query language model are noise and when they are not, because for non-Nil queries we do observe a substantial amount of improvement brought by query expansion, especially with external world knowledge.",
        "We will further investigate this research question in the future.",
        "We now further study the impact of using the KB disambiguation text associated with each entry to estimate the KB entry language model used in the KL-divergence ranking function.",
        "The results are shown in Table 6 for all the methods on 9Ne vs. 9Ne+Deusing the expanded query language models.",
        "We can see that for all methods the impact of using the KB disambiguation text is very minimal and is observed only for GPE and ORG queries.",
        "Table 7 shows an example of the KL-divergence scores for a query, Mobile whose context is previously shown in the Figure 1.",
        "Without the KB disambiguation text both the KB entry Mobile Alabama and the entry Mobile River are given the same score, resulting in inaccurate linking in the 9Ne case.",
        "But with 9Ne+De , Mobile Alabama was scored higher, resulting in an accurate linking.",
        "However, we observe that such cases are very rare in the TAC 2010 query list and thus the overall improvement observed is minimal.",
        "Finally, we compare our performance with the highest scores from TAC-KBP 2010 as shown in the Table 8.",
        "It is important to note that the highest TAC results shown in the table under each setting are not necessarily obtained by the same team.",
        "We can see that our overall performance when KB text is used is competitive compared with the highest TAC score, and is substantially higher than the TAC score when KB text is not used.",
        "Lehmann et al.",
        "(2010) achieved highest TAC scores.",
        "They used a variety of evidence from Wikipedia like disambiguation pages, anchors, expanded acronyms and redirects to build a rich feature set.",
        "But as we discussed, building a rich fea-",
        "Method",
        "All",
        "Nil",
        "Non-Nil",
        "ALL",
        "GPE",
        "ORG",
        "PER",
        "GPE",
        "ORG",
        "PER",
        "GPE",
        "ORG",
        "PER",
        "Q",
        "0.6916",
        "0.5714",
        "0.6533",
        "0.8495",
        "0.8618",
        "0.9888",
        "0.9963",
        "0.4294",
        "0.1612",
        "0.4789",
        "Q+L",
        "0.7449",
        "0.7156",
        "0.6533",
        "0.8655",
        "0.9472",
        "0.9888",
        "0.9944",
        "0.6024",
        "0.1612",
        "0.5399",
        "Q+G",
        "0.7604",
        "0.7009",
        "0.6893",
        "0.8908",
        "0.9431",
        "0.9888",
        "0.9944",
        "0.5825",
        "0.2500",
        "0.6291",
        "Q+L+G",
        "0.7800",
        "0.7583",
        "0.6893",
        "0.8921",
        "0.9431",
        "0.9888",
        "0.9944",
        "0.6680",
        "0.2500",
        "0.6338",
        "Method",
        "QueryModel",
        "All",
        "Nil",
        "Non-Nil",
        "ALL",
        "GPE",
        "ORG",
        "PER",
        "GPE",
        "ORG",
        "PER",
        "GPE",
        "ORG",
        "PER",
        "Q+L",
        "0q",
        "0.7449",
        "0.7156",
        "0.6533",
        "0.8655",
        "0.9472",
        "0.9888",
        "0.9944",
        "0.6024",
        "0.1612",
        "0.5399",
        "°Q",
        "0.7689",
        "0.7850",
        "0.6533",
        "0.8682",
        "0.9309",
        "0.9888",
        "0.9944",
        "0.7137",
        "0.1612",
        "0.5493",
        "Q+G",
        "Sq",
        "0.7604",
        "0.7009",
        "0.6893",
        "0.8908",
        "0.9431",
        "0.9888",
        "0.9944",
        "0.5825",
        "0.2500",
        "0.6291",
        "sGsq",
        "0.8160",
        "0.7423",
        "0.7867",
        "0.9188",
        "0.9106",
        "0.9372",
        "0.9796",
        "0.6600",
        "0.5658",
        "0.7653",
        "Q+L+G",
        "0.7800",
        "0.7583",
        "0.6893",
        "0.8921",
        "0.9431",
        "0.9888",
        "0.9944",
        "0.6680",
        "0.2500",
        "0.6338",
        "0.8516",
        "0.8278",
        "0.7867",
        "0.9401",
        "0.8821",
        "0.9372",
        "0.9814",
        "0.8012",
        "0.5658",
        "0.8357",
        "KB Entry",
        "KB Name",
        "w/o text",
        "w/ text",
        "E0583976 E0183287",
        "Mobile Alabama Mobile River",
        "-6.28514 -6.28514",
        "-6.3839 -6.69372",
        "Table 6: Comparing the performance using KB text and without using KB text for all methods using expanded query models in terms of micro average accuracy on 2250 queries.",
        "9Ne+De represents method using KB text and 9Nerepresents methods without using KB text.",
        "ture set is an expensive task.",
        "Their overall accuracy is 1.5% higher than our model.",
        "Table 8 shows that the performance of ORG entities is lower when compared with the TAC results when we used KB text.",
        "In our analysis, we observed that, even though some entities like AMPAS are linked correctly, the entities like CCC (Consolidated Contractors Company) failed due to ambiguity in the title.",
        "Here, we may benefit by leveraging more global knowledge, i.e, we should expand the NQ with Wikipedia global context entities together with the title to fully benefit from global knowledge.",
        "In particular, when KB text is not used, our system outperforms the highest TAC results for all three types of queries.",
        "From the analysis by Ji et al.",
        "(2010), overall the participating teams generally performed the best on PER queries and the worst on GPE queries.",
        "With our system, we can achieve good performance on GPE queries.",
        "Table 8: Comparison of the best configuration of our system (Q+L+G with 9^+°) with the TAC-KBP 2010 results in terms of micro-averaged accuracy.",
        "9Ne+De represents the method using KB disambiguation text and 9Ne represents the method without using KB disambiguation text.",
        "In all our experiments, we set the Dirichlet prior ß to 2500 following previous studies.",
        "For the threshold t we empirically set it to -12.0 in all the experiments based on preliminary results.",
        "Recall that all the expanded query language models also have a control parameters a.",
        "The local context-based models 9Q and 9Q+g have an additional parameter a which controls the proximity weighing.",
        "The 9Q+g model has another additional parameter ß that controls the balance between local context and world knowledge.",
        "In this subsection, we study the sensitivity of these parameters.",
        "We plot the sensitivity graphs for all the methods that involve a (ß set to 0.5) in Figure 3.",
        "As we can see, all the curves appear to be stable and a=0.4 appears to work well.",
        "Similarly, we set a=0.4 and examine how ß affects micro averaged accuracy.",
        "We plot the sensitivity curve for ß for the Q+L+G setting with 9Q+gin Figure 5.",
        "As we can see, the best performance is achieved when ß=0.5.",
        "This implies that the local context and the global world knowledge are weighed equally for aiding disambiguation and improving the entity linking performance.",
        "Method",
        "KB Text",
        "All",
        "Nil",
        "Non-Nil",
        "ALL",
        "GPE",
        "ORG",
        "PER",
        "GPE",
        "ORG",
        "PER",
        "GPE",
        "ORG",
        "PER",
        "Q",
        "0.6916",
        "0.5714",
        "0.6533",
        "0.8495",
        "0.8618",
        "0.9888",
        "0.9963",
        "0.4294",
        "0.1612",
        "0.4789",
        "8ne+de",
        "0.6888",
        "0.5607",
        "0.6533",
        "0.8495",
        "0.8618",
        "0.9888",
        "0.9963",
        "0.4135",
        "0.1612",
        "0.4789",
        "Q+L",
        "0.7689",
        "0.7850",
        "0.6533",
        "0.8682",
        "0.9309",
        "0.9888",
        "0.9944",
        "0.7137",
        "0.1612",
        "0.5493",
        "SNe+De",
        "0.7707",
        "0.7904",
        "0.6533",
        "0.8682",
        "0.9390",
        "0.9888",
        "0.9944",
        "0.7177",
        "0.1612",
        "0.5493",
        "Q+G",
        "SNe",
        "0.8160",
        "0.7423",
        "0.7867",
        "0.9188",
        "0.9106",
        "0.9372",
        "0.9796",
        "0.6600",
        "0.5658",
        "0.7653",
        "SNe+De",
        "0.8222",
        "0.7450",
        "0.7827",
        "0.9387",
        "0.8902",
        "0.9372",
        "0.9814",
        "0.6740",
        "0.5559",
        "0.8310",
        "Q+L+G",
        "SNe",
        "0.8516",
        "0.8278",
        "0.7867",
        "0.9401",
        "0.8821",
        "0.9372",
        "0.9814",
        "0.8012",
        "0.5658",
        "0.8357",
        "0.8524",
        "0.8291",
        "0.7880",
        "0.9401",
        "0.8740",
        "0.9372",
        "0.9814",
        "0.8072",
        "0.5691",
        "0.8357",
        "Q+L – i – ",
        "Q+G",
        "Q+L+G",
        "- /' / -",
        "-",
        "KB Text Usage",
        "Type",
        "Our System",
        "TAC Highest",
        "All",
        "0.8524",
        "0.8680",
        "GPE",
        "0.8291",
        "0.7957",
        "ORG",
        "0.7880",
        "0.8520",
        "PER",
        "0.9401",
        "0.9601",
        "Ne",
        "All",
        "0.8516",
        "0.7791",
        "GPE",
        "0.8278",
        "0.7076",
        "ORG",
        "0.7867",
        "0.7333",
        "PER",
        "0.9401",
        "0.9001",
        "Furthermore, we systematically test a fixed set of a values from 25 to 125 with an intervals of 25 and examine how a affects micro averaged accuracy.",
        "We set a=0.4 and ß=0.5, which is the best parameter setting as discussed above.",
        "We plot the sensitivity curves for the parameter a for methods that utilize the local context, i.e. 9q and 9Q+G, in Figure 5.",
        "We observe that all the curves are stable and 75 <= a <= 100 appears to work well.",
        "We set a=100 for all our experiments.",
        "Moreover, after 100, the graph becomes stable, which indicates that proximity has less impact on the method from this point on.",
        "This implies that an equal weighing scheme actually would work the same for these experiments.",
        "Part of the reason may be that by using only named entities in the context rather than all words, we have effectively picked the most useful contextual terms.",
        "Therefore, positional feedback models do have exhibit much benefit for our problem."
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "Bunescu and Paşca (2006) and Cucerzan (2007) explored the entity linking task using Vector Space Models for ranking.",
        "They took a classification approach together with the novel idea of exploiting Wikipedia knowledge.",
        "In their pioneering work, they used Wikipedia's category information for entity disambiguation.",
        "They show that using different background knowledge, we can find efficient approaches for disambiguation.",
        "In their work, they took an assumption that every entity has a KB entry and thus the NIL entries are not handled.",
        "Similar to other researchers, Zhang et al.",
        "(2010) took an approach of classification and used a two-stage approach for entity liking.",
        "They proposed a supervised model with SVM ranking to filter out the candidates and deal with disambiguation effectively.",
        "For entity diambiguation they used the contextual comparisons between the Wikipedia article and the KB article.",
        "However, their work ignores the possibilities of acronyms in the entities.",
        "Also, the ambiguous geopolitical names are not handled in their work.",
        "Dredze et al.",
        "(2010) took the approach that large number of entities will be unlinkable, as there is a probability that the relevant KB entry is unavailable.",
        "Their algorithm for learning NIL has shown very good results.",
        "But their proposal for handling the alias name or stage name via multiple lists is not scalable.",
        "Unlike their approach, we use the global knowledge to handle the stage names and thus this gives an optimized solution to handle alias names.",
        "Similarly, for acronyms we use the global knowledge that aids unabbreviating and thus entity disambiguation.",
        "Similar to other approaches, Zheng et al.",
        "(2010) took a learning to rank approach and compared list-wise rank model to the pairwise rank model.",
        "They achieved good results on the list-wise ranking approach.",
        "They handled acronyms and dis-ambiguity through wiki redirect pages and the anchor texts which is similar to others ideas.",
        "Challenges in supervised learning includes careful feature selection.",
        "The features can be selected in ad hoc manner - similarity based or semantic based.",
        "Also machine learning approach induces challenges of handling heterogenous cases.",
        "Unlike their machine learning approach which requires careful feature engineering and heterogenous training data, our method is simple as we use simple similarity measures.",
        "At the same time, we propose a statistical language modeling approach to the linking problem.",
        "Many researchers have proposed efficient ideas in their works.",
        "We integrated some of their ideas like world knowledge with our new techniques to achieve efficient entity linking accuracy."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "In this paper we proposed a novel approach to entity linking based on statistical language model-based information retrieval with query expansion using the local context from the query document as well as world knowledge from the Web.",
        "Our model is a simple unsupervised one that follows principled existing information retrieval techniques.",
        "And yet it performs the entity linking task effectively compared with the best performance achieved in the TAC-KBP 2010 evaluation.",
        "Currently our model does not exploit world knowledge from the Web completely.",
        "World knowledge, especially obtained from Wikipedia, has shown to be useful in previous studies.",
        "As our future work, we plan to explore how to further incorporate such world knowledge into our model in a principled way."
      ]
    }
  ]
}
