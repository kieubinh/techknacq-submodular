{
  "info": {
    "authors": [
      "Yulia Tsvetkov",
      "Shuly Wintner"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1077",
    "title": "Identification of Multi-word Expressions by Combining Multiple Linguistic Information Sources",
    "url": "https://aclweb.org/anthology/D11-1077",
    "year": 2011
  },
  "references": [
    "acl-C10-1002",
    "acl-C10-2144",
    "acl-D07-1110",
    "acl-E06-1043",
    "acl-J03-1002",
    "acl-J90-1003",
    "acl-N03-2027",
    "acl-W02-1801",
    "acl-W03-1809",
    "acl-W07-1101",
    "acl-W07-1104",
    "acl-W10-3712"
  ],
  "sections": [
    {
      "text": [
        "We propose an architecture for expressing various linguistically-motivated features that help identify multi-word expressions in natural language texts.",
        "The architecture combines various linguistically-motivated classification features in a Bayesian Network.",
        "We introduce novel ways for computing many of these features, and manually define linguistically-motivated interrelationships among them, which the Bayesian network models.",
        "Our methodology is almost entirely unsupervised and completely language-independent; it relies on few language resources and is thus suitable for a large number of languages.",
        "Furthermore, unlike much recent work, our approach can identify expressions of various types and syntactic constructions.",
        "We demonstrate a significant improvement in identification accuracy, compared with less sophisticated baselines."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Multi-word Expressions (MWEs) are lexical items that consist of multiple orthographic words (e.g., ad hoc, by and large, New York, kick the bucket).",
        "MWEs are numerous and constitute a significant portion of the lexicon of any natural language (Jackendoff, 1997; Erman and Warren, 2000; Sag et al., 2002).",
        "They are a heterogeneous class of constructions with diverse sets of characteristics, distinguished by their idiosyncratic behavior.",
        "Morphologically, some MWEs allow some of their constituents to freely inflect while restricting (or preventing) the inflection of other constituents.",
        "In some cases MWEs may allow constituents to undergo non-standard morphological inflections that they would not undergo in isolation.",
        "Syntactically, some MWEs behave like words while other are phrases; some occur in one rigid pattern (and a fixed order), while others permit various syntactic transformations.",
        "Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic (Bannard et al., 2003).",
        "Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing applications.",
        "Correct handling of MWEs has been proven beneficial for various applications, including information retrieval, building ontologies, text alignment, and machine translation.",
        "We propose a novel architecture for identifying MWEs of various types and syntactic categories in monolingual corpora.",
        "Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of all types by focusing on the general idiosyncratic properties of MWEs rather than on specific properties of each subclass thereof.",
        "While we only evaluate our methodology on bi-grams, it can in principle be extended to longer MWEs.",
        "The architecture uses Bayesian Networks (BN) to express multiple interdependent linguistically-motivated features.",
        "First, we automatically generate a small (training) set of MWE and non-MWE bi-grams (positive and negative instances, respectively).",
        "We then define a set of linguistically-motivated features that embody observed characteristics of MWEs.",
        "We augment these by features that reflect collocation measures.",
        "Finally, we define dependencies among these features, expressed in the structure of a Bayesian Network model, which we then use for classification.",
        "This is a directed graph, whose nodes express the features used for classification, and whose edges define causal relationships among these features.",
        "In this architecture, learning does not result in a black box, expressed solely as feature weights.",
        "Rather, the structure of the BN allows us to learn the impact of different MWE features on the classification.",
        "The result is a new unsupervised method for identifying MWEs of various types in text corpora.",
        "It combines statistics with a large array of linguistically-motivated features, organized in an architecture that reflects interdependencies among the features.",
        "The contribution of this work is manifold.",
        "First, we show how to generate training material (almost) automatically, so the method is almost completely unsupervised.",
        "The methodology we advocate is thus language-independent, requiring relatively few language resources, and is therefore optimal for medium-density languages (Varga et al., 2005).",
        "Second, we propose several linguistically-motivated features that can be computed from data and that are demonstrably productive for improving the accuracy of MWE identification.",
        "These feature focus on the expression of linguistic idiosyncrasies of various types, a phenomenon typical of MWEs.",
        "We propose novel computational modeling of many of these features; in particular, we account for the morphological idiosyncrasy of MWEs using a histogram of the number of inflected forms, in a technique that draws from image processing.",
        "Third, we advocate the use of Bayesian Networks as a mechanism for expressing manually-crafted dependencies among features; the use of BN significantly improves the classification accuracy.",
        "Finally, we demonstrate the utility of our methodology by applying it to Hebrew.",
        "Our evaluation shows that the use of linguistically-motivated features results in reduction of 23% of the errors compared with a collocation baseline; organizing the knowledge in a BN reduces the error rate by additional 8.7%.",
        "After discussing related work in the next section, we describe in Section 3 the methodology we propose, including a detailed discussion of the features and their implementation.",
        "Section 4 provides a thorough evaluation of the results.",
        "We conclude with suggestions for future research.",
        "!To facilitate readability we use a transliteration of Hebrew using Roman characters; the letters used, in Hebrew lexicographic order, are abgdhwzxTiklmns'pcqrst."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990).",
        "Pecina (2008) compares 55 different association measures in ranking German Adj-N and PP-Verb collocation candidates.",
        "He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure.",
        "Other results (Chang et al., 2002; Villavicencio et al., 2007) suggest that some collocation measures (especially PMI and Log-likelihood) are superior to others for identifying MWEs.",
        "Soon, however, it became clear that mere cooccurrence measurements are not enough to identify MWEs, and their linguistic properties should be exploited as well (Piao et al., 2005).",
        "Hybrid methods that combine word statistics with linguistic information exploit morphological, syntactic and semantic idiosyncrasies to extract idiomatic MWEs.",
        "Ramisch et al.",
        "(2008) evaluate a number of association measures on the task of identifying English Verb-Particle Constructions and German Adjective-Noun pairs.",
        "They show that adding linguistic information (mostly POS and POS-sequence patterns) to the association measure yields a significant improvement in performance over using pure frequency.",
        "Several works address the lexical fixedness or syntactic fixedness of (certain types of) MWEs in order to extract them from texts.",
        "An expression is considered lexically fixed if replacing any of its constituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression.",
        "Syntactically fixed expressions prohibit (or restrict) syntactic variation.",
        "For example, Van de Cruys and Villada Moiron (2007) use lexical fixedness to extract Dutch Verb-Noun idiomatic combinations (VNICs).",
        "Bannard (2007) uses syntactic fixedness to identify English VNICs.",
        "Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson, 2006).",
        "While these approaches are in line with ours, they require lexical semantic resources (e.g., a database that determines semantic similarity among words) and syntactic resources (parsers) that are unavailable for Hebrew (and many other languages).",
        "Our approach only requires morphological processing and a bilingual dictionary, which are more readily-available for several languages.",
        "Note also that these approaches target a specific syntactic construction, whereas ours is adequate for various types of",
        "MWEs.",
        "Several properties of Hebrew MWEs are described by Al-Haj (2010); Al-Haj and Wintner (2010) use them in order to construct an SVM-based classifier that can distinguish between MWE and non-MWE noun-noun constructions in Hebrew.",
        "The features of the SVM reflect several morphological and morpho-syntactic properties of such constructions.",
        "The resulting classifier performs much better than a naive baseline, reducing over one third of the errors.",
        "We rely on some of these insights, as we implement more of the linguistic properties of MWEs.",
        "Again, our methodology is not limited to a particular construction: indeed, we demonstrate that our general methodology, trained on automatically-generated, general training data, performs almost as well as the noun-noun-specific approach of Al-Haj and Wintner (2010) on the very same dataset.",
        "Recently, Tsvetkov and Wintner (2010b) introduced a general methodology for extracting MWEs from bilingual corpora, and applied it to Hebrew.",
        "The results were a highly accurate set of Hebrew MWEs, of various types, along with their English translations.",
        "A major limitation of this work is that it can only be used to identify MWEs in the bilingual corpus, and is thus limited in its scope.",
        "We use this methodology to extract both positive and negative instances for our training set in the current work; but we extrapolate the results much further by extending the method to monolingual corpora, which are typically much larger than bilingual ones.",
        "Bayesian Networks have only scarcely been used for classification in natural language applications.",
        "For example, BN were used for POS tagging of unknown words (Peshkin et al., 2003); dependency parsing (Savova and Peshkin, 2005); and document classification (Lam et al., 1997; Calado et al., 2003; Denoyer and Gallinari, 2004).",
        "Very recently, Ramisch et al.",
        "(2010) have used BN for Portuguese MWE identification.",
        "The features used for classification were of two kinds: (1) various collocation measures; (2) bi-grams aligned together by an automatic word aligner applied to a parallel (Portuguese-English) corpus.",
        "A BN was used to combine the predictions of the various features on the test set, but the structure of the network is not described.",
        "The combined classifier resulted in a much higher accuracy than any of the two methods alone.",
        "However, the BN does not play any special role in this work, and its structure does not reflect any insights or intuitions on the structure of the problem domain or on interdependencies among features.",
        "We, too, acknowledge the importance of combining different types of knowledge in the hard task of MWE identification.",
        "In particular, we also believe that collocation measures are highly important for this task, but cannot completely solve the problem: linguistically-motivated features are mandatory in order to improve the accuracy of the classifier.",
        "In this work we focus on various properties of different types of MWEs, and define general features that may accurately apply to some, but not necessarily all of them.",
        "An architecture of Bayesian Networks is optimal for this task: it enables us to define weighted dependencies among features, such that certain features are more significant for identifying some class of MWEs, whereas others are more prominent in identifying other classes.",
        "As we show below, this architecture results in significant improvements over a more naive combination of features."
      ]
    },
    {
      "heading": "3. Methodology 3.1 Motivation",
      "text": [
        "The task we address is identification of MWEs, of various types and syntactic constructions, in monolingual corpora.",
        "Several properties of MWEs make this task challenging: MWEs exhibit idiosyncrasies on a variety of levels, orthographic, morphological, syntactic and of course semantic (Al-Haj, 2010).",
        "They are also extremely diverse: for example, on the semantic dimension alone, MWEs cover an entire spectrum, ranging from frozen, fixed idioms to free combinations of words (Bannard et al., 2003).",
        "Such a complex task calls for a combination of multiple approaches, and much research indeed suggests \"hybrid\" approaches to MWE identification (Duan et al., 2009; Weller and Fritzinger, 2010; Ramisch et al., 2010; Hazelbeck and Saito, 2010).",
        "We believe that Bayesian Networks provide an optimal architecture for expressing various pieces of knowledge aimed at MWE identification, for the following reasons (Heckerman, 1995):",
        "• In contrast to many other classification methods, BN can learn (and express) causal relationships between features.",
        "This facilitates better understanding of the problem domain.",
        "• BN can encode not only statistical data, but also prior domain knowledge and human intuitions, in the form of interdependencies among features.",
        "We do indeed use this possibility here.",
        "Based on the observations of Al-Haj (2010), we define several linguistically-motivated features that are aimed at capturing some of the unique properties of MWEs.",
        "While many idiosyncratic properties of MWEs have been previously studied, we introduce novel ways to express those properties as computable features informing a classifier.",
        "Note that many of the features we describe below are completely language-independent; others are applicable to a wide range of languages, while few are specific to morphologically-rich languages, and can be exhibited in different ways in different languages.",
        "The methodology we advocate, however, is completely universal.",
        "A common theme for all these features is idiosyn-cracy: they are all aimed at locating some linguistic property on which MWEs may differ from non-MWEs.",
        "Below we detail these properties, along with the features that we define to reflect them.",
        "In all cases, the feature is applied to a candidate MWE, defined here as a bi-gram of tokens (all possible bi-grams are potential candidates).",
        "To compute the features, we use a 46M-token monolingual Hebrew corpus (Itai and Wintner, 2008), which we pre-process as in Tsvetkov and Wintner (2010b).",
        "All statistics are computed from this large corpus.",
        "Likewise, we compute these features on a small training corpus, which we generate automatically (see Section 3.4).",
        "Orthographic variation Sometimes, MWEs are written with dashes instead of inter-token spaces.",
        "We define a binary feature, dash, whose value is 1 iff the dash character appears in some surface form of the candidate MWE.",
        "For example, xd-cddi (one sided) \"unilateral\".",
        "Hapax legomena MWEs sometimes include constituents that have no usage outside the particular expression, and are hence not included in lexicons.",
        "We define a feature, hapax, whose value is a binary vector with 1 in the i-th place iff the i-th word of the candidate is not in the lexicon, and does not occur in other bi-grams at the same location.",
        "For example, hwqws pwqws \"hocus-pocus\".",
        "In order to filter out potential errors, candidates must occur at least 5 times in the corpus in order for this feature to fire.",
        "Frozen form MWE constituents sometimes occur in one fixed, frozen form.",
        "We define a feature, FROZEN, whose value is a binary vector with 1 in the i-th place iff the i-th word of the candidate never inflects in the context of this expression.",
        "Example: bit xwlim (house-of sick-people) \"hospital\"; the noun xwlim must be in the plural in this MWE.",
        "Partial morphological inflection In some cases, MWE constituents undergo a (strict but non-empty) subset of the full inflections that they would undergo in isolation.",
        "We capture this property with a technique that has been proven useful in the area of image processing (Jain, 1989, Section 7.3).",
        "We compute a histogram of the distribution in the corpus of all the possible surface forms of each constituent of an MWE candidate.",
        "Such histograms can compactly represent distributional information on morphological behavior, in the same way that histograms of the distribution of gray levels in a picture are used to represent the picture itself.",
        "Our assumption is that the inflection histograms of non-MWEs are more uniform than the histograms of MWEs, in which some inflections may be more frequent and others may be altogether missing.",
        "Of course, restrictions on the histogram may stem from the part of speech of the expression; such constraints are captured by dependencies in the BN structure.",
        "Since each MWE is idiosyncratic in its own way, we do not expect the histograms of MWEs to have some specific pattern, except non-uniformity.",
        "We therefore sort the columns of each histogram, thereby losing information pertaining to the specific inflections, and retaining only information about the idiosyncrasy of the histogram.",
        "Offline, we compute the average histogram for positive and negative examples: The average histogram of MWEs is shorter and less uniform than the average histogram of non-MWEs.",
        "We define as feature, hist, the Li (Manhattan) distance between the histogram of the candidate and the closest average histogram.",
        "For example, the MWE bit mepv (house-of law) \"court\" occurs in the following inflected forms: bit hmepv \"the court\" (75%); bit mepv \"a court\" (15%); bti hmepv \"the courts\" (8%); and bti mepv \"courts\" (2%).",
        "The histogram for this candidate is thus (75,15, 8, 2).",
        "In contrast, the non-MWE txwm mepv (domain-of law) \"domain of the law\", which is syntactically identical, occurs in nine different inflected forms, and its sorted histogram is (59,14, 7, 7, 5, 2, 2, 2, 2).",
        "Context We hypothesize that MWEs tend to constrain their (semantic) context more strongly than non-MWEs.",
        "We expect words that occur immediately after MWEs to vary less freely than words that immediately follow other expressions.",
        "One motivation for this hypothesis is the observation that MWEs tend to be less polysemous than free combinations of words, thereby limiting the possible semantic context in which they can occur.",
        "We define a feature, context, as follows.",
        "We first compute a histogram of the frequencies of words following each candidate MWE.",
        "We trim the tail of the histogram by removing words whose frequency is lower than 0.1% (the expectation is that non-MWEs would have a much longer tail).",
        "Offline, we compute the same histograms for positive and negative examples and average them as above.",
        "The value of context is 1 iff the histogram of the candidate is closer (in terms of L\\ distance) to the positive average.",
        "For example, the histogram of bit mepv \"court\" includes 15 values, dominated by bit mepv yliwn \"supreme court\" (20%) and bit mepv mxwzi \"district court\" (13%), followed by contexts whose frequency ranges between 5% and 0.6%.",
        "In contrast, the non-MWE txwm mepv \"domain-of law\" has a much shorter histogram, namely (12,11, 6): over 70% of the words following this expression occur less than 0.1% and are hence in the trimmed tail.",
        "Syntactic diversity MWEs can belong to various part of speech categories.",
        "We define as feature, pos, the category of the candidate, with values obtained by selecting frequent tuples of POS tags.",
        "For example, Noun-Noun, PropN-PropN, Noun-Adj, etc.",
        "Translational equivalents Since MWEs are often idiomatic, they tend to be translated in a non-literal way, sometimes to a single word.",
        "We use a dictionary to generate word-by-word translations of candidate MWEs to English, and check the number of occurrences of the English literal translation in a large English corpus.",
        "Due to differences in word order between the two languages, we create two variants for each translation, corresponding to both possible orders.",
        "We expect non-MWEs to have some literal translational equivalent (possibly with frequency that correlates with their frequency in Hebrew), whereas for MWEs we expect no (or few) literal translations.",
        "We define a binary feature, trans, whose value is 1 iff some literal translation of the candidate occurs more than 5 times in the corpus.",
        "For example, the MWE htxtn ym (marry with) \"marry\" is literally translated as with marry, marry with, together marry and marry together, none of which occurs in the corpus.",
        "Collocation As a baseline, statistical association measure, we use a heuristic variant of pointwise mutual information (PMI), promoting also collocations whose constituents are frequent (Tsvetkov and Wint-ner, 2010b).",
        "We define a binary feature, pmi, with values (low and high) reflecting the threshold that maximizes the accuracy of MWE classification in Tsvetkov and Wintner (2010b).",
        "A Bayesian Network (Jensen and Nielsen, 2007) is organized as a graph whose nodes are random variables and whose edges represent interdependencies among those variables.",
        "We use a particular type of BN, known as causal networks, in which directed edges lead to a variable from each of its direct causes.",
        "This facilitates the expression of domain knowledge (and intuitions, beliefs, etc.)",
        "as structural properties of the network.",
        "We use the BN as a classification device: training amounts to computing the joint probability distribution of the training set, whereas classification maximizes the posterior probability of the particular node (variable) being queried.",
        "For MWE identification we define a BN whose nodes correspond to the features described in Section 3.2.",
        "In addition, we define a node MWE for the complete classification task.",
        "Over these nodes we impose the structure depicted graphically in Figure 1.",
        "This structure, which we motivate below, is manually defined: it reflects our understanding of the problem domain and is a result of thorough experimentations.",
        "That said, it can of course be modified in various ways, and in particular, new nodes can be easily added to reflect additional features.",
        "All nodes depend on MWE, as all are affected by whether or not the candidate is a MWE.",
        "The POS of an expression influences its morphological inflection, hence the edges from pos to hist and to frozen.",
        "For example, Hebrew noun-noun constructions allow their constituents to undergo the full inflectional paradigm, but when such a construction is a MWE, inflection is severely constrained (Al-Haj and Wintner, 2010); similarly, when one of the constituents of a MWE is a conjunction, the entire expression is very likely to be frozen.",
        "Hapaxes clearly affect all statistical metrics, hence the edge from hapax to PMI, and also the existence of literal translation, since if a word is not in the lexicon, it does not have a translation, hence the edge from hapax to trans.",
        "Also, we assume that there is a correlation between the frequency (and PMI) of a candidate and whether or not a literal translation of the expression exists, hence the edge from pmi to trans.",
        "The edges from pmi and hist to context are justified by the correlation between the frequency and variability of an expression and the variability of the context in which it occurs.",
        "Once the structure of the network is established, the conditional probabilities of each dependency have to be determined.",
        "We compute the conditional probability tables from our training data (see below) using Weka (Hall et al., 2009), and obtain values for P(X | X\\,..., Xk) for each variable X and all variables Xj, 1 < i < k, such that the graph includes an edge from Xj to X (parents of X).",
        "We then perform inference on the network in order to compute P(Xmwe | Xi,...,Xfc), where Xmwe corresponds to the node MWE, and X1,..., Xk are the variables corresponding to all other nodes in the network.",
        "Using Bayes Rule,",
        "We define the prior, P(Xmwe), to be 0.41: this is the percentage of MWEs in WordNet 1.7 (Fellbaum, 1998).",
        "The conditional probabilities P(Xi , .",
        ".",
        ".",
        ", Xk | Xmwe) are determined by Weka from the conditional probability tables:",
        "where k is the number of nodes in the BN (other than Xmwe) and paj is the set of parents of Xj.",
        "For training we need samples of positive and negative instances of MWEs, each associated with a vector of the values of all features discussed in Section 3.2.",
        "We generate this training material automatically.",
        "We use a small Hebrew-English bilingual corpus (Tsvetkov and Wintner, 2010a).",
        "We word-align the corpus with Giza++ (Och and Ney, 2003), and then apply the (completely unsupervised) algorithm of Tsvetkov and Wintner (2010b), which extracts MWE candidates from the aligned corpus and re-ranks them using statistics computed from a large monolingual corpus.",
        "The core idea behind this method is that MWEs tend to be translated in non-literal ways; in a parallel corpus, words that are 1:1 aligned typically indicate literal translations and are hence unlikely constituents of MWEs.",
        "The result is a set of 134,001 Hebrew bi-gram types (from the bilingual corpus), classified as either 1:1 aligned (implying they are likely not MWEs) or unaligned (in which case they may or may not be MWEs).",
        "In addition, for each bi-gram we have a PMI score; naturally, higher PMI scores are indicative of MWEs.",
        "We thus divide the set into four classes: aligned bi-grams with high PMI score, aligned bi-grams with low PMI score, misaligned with high PMI and misaligned with low PMI.",
        "Aligned bi-grams, independently of their PMI score, are more likely non-MWEs; high-PMI misaligned bi-grams are very likely MWEs; and the status of low-PMI misaligned bi-grams is unclear, and must be further investigated.",
        "This is summarized in",
        "Table 1.",
        "Misaligned Aligned",
        "Low PMI unclear non-MWE",
        "We set the threshold that separates low PMI from high PMI as in Tsvetkov and Wintner (2010b).",
        "The results of this classification is depicted in Table 2.",
        "Misaligned Aligned Total",
        "We assume that all bi-grams in the 'Aligned' column are non-MWEs.",
        "Additionally, we assume that the 2,203 misaligned bi-grams with high PMI scores are likely MWEs.",
        "As for the set of over 61,000 misaligned low-PMI bi-grams, certainly many of them are non-MWEs, but some may be MWEs, and we are interested in including them as positive examples of MWEs with low PMI scores.",
        "We therefore manually annotate a sample of 50 MWEs from this particular set (we had to manually go over a few thousands of bi-grams to select this sample).",
        "This is the only supervision provided in this work.",
        "The remaining question is how to determine the sizes of samples from each of the other three classes.",
        "We use two guidelines: first, we would like the ratio of MWEs to non-MWEs in the training set to be 41 : 59, reflecting the ratio in WordNet (the prior MWE probability).",
        "Second, we would like classification by PMI score only to yield a reasonable baseline; the baseline is defined as the ratio of the sum of high-PMI MWEs plus low-PMI non-MWEs to the size of the training set.",
        "We choose 67%, the PMI baseline reported by Al-Haj and Wintner (2010).",
        "As a result of these two considerations, we end up with training sets whose sizes are depicted in Table 3.",
        "We randomly select from the sample space this many instances for each class.",
        "Since much of the procedure of preparing training data is automatic, the results may be somewhat noisy.",
        "As Bayesian Network are known to be robust to noisy data, we expect the BN to compensate for this problem."
      ]
    },
    {
      "heading": "4. Results and Evaluation",
      "text": [
        "We use the training set described above for training and evaluation: we perform 10-fold cross validation experiments, reporting Precision, Recall, Accuracy and F-measure in three setups: one (SVM) in which we train an SVM classifier with the features described in Section 3.2; one (BN-auto) in which we train a BN but let Weka determine its structure (using the K2 algorithm); and one (BN) in which we train a Bayesian Network whose structure reflects manually-crafted linguistically-motivated knowledge, as depicted in Figure 1.",
        "The results, along with the PMI baseline figures, are listed in Table 4.",
        "MWE",
        "non-MWE",
        "Total",
        "High PMI",
        "300",
        "232",
        "532",
        "Low PMI",
        "50",
        "272",
        "322",
        "Total",
        "350",
        "504",
        "854",
        "Accuracy Prec.",
        "Recall F-score",
        "Table 4: 10-fold cross validation evaluation results",
        "The linguistically-motivated features defined in Section 3.2 are clearly helpful in the classification task: the accuracy of the SVM, informed by these features, is close to 75%, reducing the error rate of the PMI baseline by 23%.",
        "The contribution of the Bayesian Network is also highly significant, reducing almost 7% more errors (8.7% of the errors made by the SVM classifier), or a total of almost 30% error-rate reduction with respect to the baseline.",
        "Interestingly, a BN whose structure does not reflect prior knowledge, but is rather learned automatically, performs poorly.",
        "It is the combination of linguistically-motivated features with feature interdependencies reflecting domain knowledge that contribute to the best performance.",
        "As a further demonstration of the utility of our approach, we evaluate the algorithm on an additional test set that was used for evaluation in the past (Tsvetkov and Wintner, 2010b; Al-Haj and Wintner, 2010).",
        "This is a small annotated corpus, NN, of Hebrew noun-noun constructions.",
        "The corpus consists of 413 high-frequency bi-grams of the same syntactic construction; of those, 178 are tagged as MWEs (in this case, noun compounds) and 235 as non-MWEs.",
        "This corpus consolidates the annotation of three annotators: only instances on which all three agreed were included.",
        "Since it includes both positive and negative instances, this corpus facilitates a robust evaluation of precision and recall.",
        "We train a Bayesian Network on the training set described in Section 3.4 and use it to classify the set NN.",
        "We compare the results of this classifier with a PMI baseline (using the same threshold as above), and also with the classification results reported by Al-Haj and Wintner (2010) (AW); the latter reflects 10-fold cross-validation evaluation using the entire set, so it should be considered an upper bound for any classifier that uses a general training corpus.",
        "The results are depicted in Table 5.",
        "They clearly demonstrate that the linguistically-motivated features we define provide a significant improvement in classification accuracy over the baseline PMI measure.",
        "Note that our F-score, 0.77, is very close to the best result of 0.79 obtained by Al-Haj and Wint-ner (2010) as the average of 10-fold cross validation runs, using only high-frequency noun-noun constructions for training.",
        "We interpret this result as a further proof of the robustness of our architecture.",
        "Accuracy Precision Recall F-score",
        "Finally, we have used the trained BN to classify the entire set of bi-grams present in the (Hebrew side of the) parallel corpus described in Tsvetkov and Wintner (2010a).",
        "Of the 134,000 candidates, only 4,000 are classified as MWEs.",
        "We sort this list of potential MWEs by the probability assigned by the BN to the positive value of the variable Xmwe.",
        "The resulting sorted list is dominated by high-PMI bi-grams, especially proper names, all of which are indeed MWEs.",
        "The first non-MWE (false positive) occurs in the 50th place on the list; it is crpt niqwla \"France Nicolas\", which is obviously a subsequence of the larger MWE, neia crpt niqwla srqwzi \"French president Nicolas Sarkozy\".",
        "Similar sub-sequences are also present, but only five are in the top-100.",
        "Such false positives can be reduced when longer MWEs are extracted, as it can be assumed that a subsequence of a longer MWE does not have to be identified.",
        "Other false positives in the top-100 include some highly frequent expressions, but over 85 of the top-100 are clearly MWEs.",
        "While more careful evaluation is required in order to estimate the rate of true positives in this list, we trust that the vast majority of the positive results are indeed MWEs."
      ]
    },
    {
      "heading": "5. Conclusions and future work",
      "text": [
        "We presented a novel architecture for identifying MWEs in text corpora.",
        "The main insights we emphasize are sophisticated computational encoding of linguistic knowledge that focuses on the idiosyncratic behavior of such expressions.",
        "This is reflected in two ways in our work: by defining computable features that reflect different facets of irregularities; and by framing the features as part of a larger Bayesian Network that accounts for interdependencies among them.",
        "We also introduce a method for automatically generating a training set for this task, which renders the classification almost entirely unsupervised.",
        "The result is a nearly-unsupervised, language-independent classification method that can identify MWEs of various lengths, types and con-sanctions.",
        "Evaluation on Hebrew shows significant improvement in the accuracy of the classifier compared with the state of the art.",
        "PMI",
        "66.98%",
        "0.73",
        "0.67",
        "0.67",
        "BN-auto",
        "71.19%",
        "0.71",
        "0.71",
        "0.71",
        "SVM",
        "74.59%",
        "0.75",
        "0.75",
        "0.75",
        "BN",
        "76.82%",
        "0.77",
        "0.77",
        "0.77",
        "The modular architecture of BN facilitates easy exploration with more features.",
        "We are currently investigating the contribution of various other sources of information to the classification task.",
        "For example, Hebrew lacks large-scale lexical semantic resources.",
        "However, it is possible to literally translate a MWE candidate to English and rely on the English WordNet for generating synonyms of the literal translation.",
        "Such \"literal synonyms\" can then be back-translated to Hebrew.",
        "The assumption is that if a back-translated expression has a high PMI, the original candidate is very likely not a MWE.",
        "While such a feature may contribute little on its own, incorporating it in a well-structured BN may improve performance.",
        "While our methodology is applicable to MWEs of any length, we have so far only evaluated it on bi-grams.",
        "In the future, we intend to extend the evaluation to longer n-grams.",
        "We also plan to apply the methodology to languages other than Hebrew."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research was supported by THE ISRAEL SCIENCE FOUNDATION (grants No.",
        "137/06, 1269/07).",
        "We are grateful to Gennadi Lembersky for his continuous help."
      ]
    }
  ]
}
