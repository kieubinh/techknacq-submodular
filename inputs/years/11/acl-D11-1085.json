{
  "info": {
    "authors": [
      "Zhifei Li",
      "Ziyuan Wang",
      "Jason M. Eisner",
      "Sanjeev P. Khudanpur",
      "Brian Roark"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1085",
    "title": "Minimum Imputed-Risk: Unsupervised Discriminative Training for Machine Translation",
    "url": "https://aclweb.org/anthology/D11-1085",
    "year": 2011
  },
  "references": [
    "acl-C10-2075",
    "acl-D07-1080",
    "acl-D08-1065",
    "acl-D08-1076",
    "acl-D09-1005",
    "acl-J07-2003",
    "acl-N03-1017",
    "acl-N06-1045",
    "acl-N07-1018",
    "acl-N09-1025",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P06-1096",
    "acl-P06-1121",
    "acl-P06-2101",
    "acl-P08-1024",
    "acl-P08-1115",
    "acl-P09-1067",
    "acl-W02-1001",
    "acl-W05-1506",
    "acl-W09-0424"
  ],
  "sections": [
    {
      "text": [
        "Minimum Imputed Risk: Unsupervised Discriminative Training for",
        "Machine Translation",
        "Zhifei Li*",
        "Ziyuan Wang, Sanjeev Khudanpur",
        "Johns Hopkins University Baltimore, MD 21218, USA",
        "Discriminative training for machine translation has been well studied in the recent past.",
        "A limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training.",
        "We present an unsupervised discriminative training framework to incorporate the usually plentiful target-language monolingual data by using a rough \"reverse\" translation system.",
        "Intuitively, our method strives to ensure that probabilistic \"round-trip\" translation from a target-language sentence to the source-language and back will have low expected loss.",
        "Theoretically, this may be justified as (discrimina-tively) minimizing an imputed empirical risk.",
        "Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Missing data is a common problem in statistics when fitting the parameters 9 of a model.",
        "A common strategy is to attempt to impute, or \"fill in,\" the missing data (Little and Rubin, 1987), as typified by the EM algorithm.",
        "In this paper we develop imputation techniques when 9 is to be trained discriminatively.",
        "We focus on machine translation (MT) as our example application.",
        "A Chinese-to-English machine translation system is given a Chinese sentence x and",
        "* Zhifei Li is currently working at Google Research, and this work was done while he was a PHD student at Johns Hopkins University.",
        "Jason Eisner Brian Roark",
        "asked to predict its English translation y.",
        "This system employs statistical models pe (y | x) whose parameters 9 are discriminatively trained using bilingual sentence pairs (x,y).",
        "But bilingual data for such supervised training may be relatively scarce for a particular language pair (e.g., Urdu-English), especially for some topics (e.g., technical manuals) or genres (e.g., blogs).",
        "So systems seek to exploit additional monolingual data, i.e., a corpus of English sentences y with no corresponding source-language sentences x, to improve estimation of 9.",
        "This is our missing data scenario.",
        "Discriminative training of the parameters 9 of pe (y | x) using monolingual English data is a curious idea, since there is no Chinese input x to translate.",
        "We propose an unsupervised training approach, called minimum imputed risk training, which is conceptually straightforward: First guess x (probabilistically) from the observed y using a reverse English-to-Chinese translation model P(f>(x | y).",
        "Then train the discriminative Chinese-to-English model pe(y | x) to do a good job at translating this imputed x back to y, as measured by a given performance metric.",
        "Intuitively, our method strives to ensure that probabilistic \"round-trip\" translation from a target-language sentence to the source-language and back again will have low expected loss.",
        "Our approach can be applied in an application scenario where we have (1) enough out-of-domain bilingual data to build two baseline translation systems, with parameters 9 for the forward direction, and 0 for the reverse direction; (2) a small amount of in-domain bilingual development data to discrim-inatively tune a small number of parameters in 0; and (3) a large amount of in-domain English monolingual data.",
        "The novelty here is to exploit (3) to discrimina-tively tune the parameters 9 of all translation model components, pe(y|x) and pe(y), not merely train a generative language model pe(y), as is the norm.",
        "Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems – learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs – with our unsupervised discriminative training using only y.",
        "One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009).",
        "We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance.",
        "2 Supervised Discriminative Training via Minimization of Empirical Risk",
        "Let us first review discriminative training in the supervised setting – as used in MERT (Och, 2003) and subsequent work.",
        "One wishes to tune the parameters 9 of some complex translation system 6e (x).",
        "The function 6e, which translates Chinese x to English y = öe (x) need not be probabilistic.",
        "For example, 9 may be the parameters of a scoring function used by 6, along with pruning and decoding heuristics, for extracting a high-scoring translation of x.",
        "The goal of discriminative training is to minimize the expected loss of 6e(•), under a given task-specific loss function L(y', y) that measures how bad it would be to output y' when the correct output is y.",
        "For an MT system that is judged by the bleu metric (Papineni et al., 2001), for instance, L(y', y) may be the negated bleu score of y' w.r.t.",
        "y.",
        "To be precise, the goal is to find 9 with low Bayes risk, where p(x, y) is the joint distribution of the input-output pairs.",
        "The true p( x, y) is, of course, not known and, in practice, one typically minimizes empirical risk by replacing p( x, y) above with the empirical distribution p(x, y) given by a supervised training set {(xi, yi),i = 1,..., N}.",
        "Therefore,",
        "The search for 9* typically requires the use of numerical methods and some regularization."
      ]
    },
    {
      "heading": "3. Unsupervised Discriminative Training with Missing Inputs",
      "text": [
        "We now turn to the unsupervised case, where we have training examples {yi} but not their corresponding inputs {xi}.",
        "We cannot compute the summand L(6e(xi),yi) for such i in (2), since 6e(xi) requires to know xi.",
        "So we propose to replace where 1 •) is a \"reverse prediction model\" that attempts to impute the missing x» data.",
        "We call the resulting variant of (2) the minimization of imputed empirical risk, and say that is the estimate with the minimum imputed risk.",
        "The minimum imputed risk objective of (4) could be evaluated by brute force as follows.",
        "1.",
        "For each unsupervised example y», use the reverse prediction model p^(-1 y») to impute possible reverse translations x» = {x»1, x»2,...}, and add each (x»j ,y») pair, weighted by P4>(x»j i y») < 1, to an imputed training set.",
        "2.",
        "Perform the supervised training of (2) on the imputed and weighted training data.",
        "The second step means that we must use 60 to forward-translate each imputed x»j, evaluate the loss of the translations yj against the corresponding true translation yi, and choose the 9 that minimizes the weighted sum of these losses (i.e., the empirical risk when the empirical distribution p(x,y) is derived from the imputed training set).",
        "Specific to our MT task, this tries to ensure that probabilistic \"round-trip\" translation, from the target-language sentence yi to the source-language and back again, will have a low expected loss.",
        "The trouble with this method is that the reverse model generates a weighted lattice or hyper-graph x» encoding exponentially many translations of y», and it is computationally infeasible to forward-translate each x»j g x».",
        "We therefore investigate several approximations to (4) in Section 3.4.",
        "A crucial ingredient in (4) is the reverse prediction model ( | ) that attempts to impute the missing x».",
        "We will train this model in advance, doing the best job we can from available data, including any out-of-domain bilingual data as well as any in-domain monolingual data x.",
        "In the MT setting, 60 and may have similar parameterization.",
        "One translates Chinese to English; the other translates English to Chinese.",
        "Yet the setup is not quite symmetric.",
        "Whereas 60is a translation system that aims to produce a single, low-loss translation, the reverse version is rather a probabilistic model.",
        "It is supposed to give an accurate probability distribution over possible values x»j of the missing input sentence x».",
        "All of these values are taken into account in (4), regardless of the loss that they would incur if they were evaluated for translation quality relative to the missing x».",
        "Thus, 0 does not need to be trained to minimize the risk itself (so there is no circularity).",
        "Ideally, it should be trained to match the underlying conditional distribution of x given y, by achieving a low conditional cross-entropy",
        "In practice, 0 is trained by (empirically) minimizing -jSjE^l l°gMxj yj) + 2^ II0II2 on some bilingual data, with the regularization coefficient <rtuned on held out data.",
        "It may be tolerable for to impute mediocre translations x»j.",
        "All that is necessary is that the (forward) translations generated from the imputed x»j \"simulate\" the competing hypotheses that we would see when translating the correct Chinese input x».",
        "3.3 The Forward Translation System 50 and The Loss Function L(50(x»), y»)",
        "The minimum empirical risk objective of (2) is quite general and various popular supervised training methods (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, 2006) can be formalized in this framework by choosing different functions for öe and L(öe(x), y).",
        "The generality of (2) extends to our minimum imputed risk objective of (4).",
        "Below, we specify the öe and L(öe(xi),iji) we considered in our investigation.",
        "A simple translation rule would define",
        "If this öe (x) is used together with a loss function L(öe(xi),yi) that is the negated bleu score, our minimum imputed risk objective of (4) is equivalent to MERT (Och, 2003) on the imputed training data.",
        "However, this would not yield a differentiable objective function.",
        "Infinitesimal changes to 9 could result in discrete changes to the winning output string öe(x) in (6), and hence to the loss L(öe(x), yi).",
        "Och (2003) developed a specialized line search to perform the optimization, which is not scalable when the number of model parameters 9 is large.",
        "Instead of using the argmax of (6), we assume during training that öe (x) is itself random, i.e. the MT system randomly outputs a translation y with probability pe (y | x).",
        "As a result, we will modify our objective function of (4) to take yet another expectation over the unknown y.",
        "Specifically, we will replace L(öe (x), yi) in (4) with",
        "Now, the minimum imputed empirical risk objective of (4) becomes",
        "If the loss function L(y, yi) is a negated bleu, this is equivalent to performing minimum-risk training described by (Smith and Eisner, 2006; Li and Eisner, 2009) on the imputed data}",
        "as CRFs (Lafferty et al., 2001).",
        "The objective function in (8) is now differentiable, since each coefficient pe (y | x) is a differentiable function of 9, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm (Liu et al., 1989) in our experiments.",
        "We perform experiments with the syntax-based MT system Joshua (Li et al., 2009a), which implements dynamic programming algorithms for second-order expectation semirings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8).",
        "As mentioned at the end of Section 3.1, it is computationally infeasible to forward-translate each of the imputed reverse translations xij.",
        "We propose four approximations that are computationally feasible.",
        "Each may be regarded as a different approximation of p^(x | yi) in equations (4) or (8).",
        "k-best.",
        "For each yi, add to the imputed training set only the k most probable translations {xi1,... xik} according to (x | yi).",
        "(These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).)",
        "Rescale their probabilities to sum to 1.",
        "Sampling.",
        "For each yi, add to the training set k independent samples {xi1;... xik} from the distribution p^(x | yi), each with weight 1/k.",
        "(These can be sampled from Xi using standard algorithms (Johnson et al., 2007).)",
        "This method is known in the literature as multiple imputation (Rubin, 1987).",
        "Lattice.",
        "Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming.",
        "Although xi does contain exponentially many translations, it may use a \"packed\" representation in which these translations share structure.",
        "This representation may furthermore enable sharing work in forward-translation, so as to efficiently translate the entire set Xi and obtain a distribution over translations y.",
        "Finally, the expected loss under that distribution, as required by equation (3), may also be efficiently computable.",
        "All this turns out to be possible if (a) the posterior distribution p^(x | yi) is represented by an un- ambiguous weighted finite-state automaton Xi, (b) the forward translation system öe is structured in a certain way as a weighted synchronous context-free grammar, and (c) the loss function decomposes in a certain way.",
        "We omit the details of the construction as beyond the scope of this paper.",
        "In our experimental setting described below, (b) is true (using Joshua), and (c) is true (since we use a loss function presented by Tromble et al.",
        "(2008) that is an approximation to bleu and is decomposable).",
        "While (a) is not true in our setting because Xi is a hypergraph (which is ambiguous), Li et al.",
        "(2009b) show how to approximate a hypergraph representation of p^(x | yi) by an unambiguous WFSA.",
        "One could then apply the construction to this WFSA, obtaining an approximation to (3).",
        "Rule-level Composition.",
        "Intuitively, the reason why the structure-sharing in the hypergraph Xi (generated by the reverse system) cannot be exploited during forward translating is that when the forward Hiero system translates a string xi G Xi, it must parse it into recursive phrases.",
        "But the structure-sharing within the hypergraph of Xi has already parsed xi into recursive phrases, in a way determined by the reverse Hiero system; each translation phrase (or rule) corresponding to a hy-peredge.",
        "To exploit structure-sharing, we can use a forward translation system that decomposes according to that existing parse of xi.",
        "We can do that by considering only forward translations that respect the hypergraph structure of Xi.",
        "The simplest way to do this is to require complete isomorphism of the SCFG trees used for the reverse and forward translations.",
        "In other words, this does round-trip imputation (i.e., from y to x, and then to y') at the rule level.",
        "This is essentially the approach taken by Li et al.",
        "(2010).",
        "We have not yet specified the form of pe.",
        "Following much work in MT, we begin with a linear model where f (x, y) is a feature vector indexed by k. Our deterministic test-time translation system öe simply outputs the highest-scoring y for fixed x.",
        "At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model)",
        "The scaling factor 7 controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y.",
        "For large 7, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable.",
        "In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y.",
        "A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntax-based system (Galley et al., 2006; Chiang, 2007).",
        "We change our model to assign scores not to an ( x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)-(10), and finally define pe (y|x) by marginalizing out d, where D( x, y) represents the set of derivations that yield x and y."
      ]
    },
    {
      "heading": "4. Minimum Imputed Risk vs. EM",
      "text": [
        "The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approach.",
        "So it is instructive to compare EM with minimum imputed risk.",
        "One can estimate 9 by maximizing the log-likelihood of the data {(xi; yi), i = 1,..., N} as",
        "If the xi's are missing, EM tries to iteratively maximize the marginal probability:",
        "The E-step of each iteration comprises computing Ex Pet (x | yi)log pe the expected log-likelihood of the complete data, where pet (x | yi) is the conditional part of pet (x,yi) under the current iterate 9t, and the M-step comprises maximizing it:",
        "0t+i = argmax j^Y^Pet (x | yi) log Pe (x,yi).",
        "Notice that if we replace pet (x|yi) with (x | yi) in the equation above, and admit negated log-likelihood as a loss function, then the EM update (14) becomes identical to (4).",
        "In other words, the minimum imputed risk approach of Section 3.1 differs from EM in (i) using an externally-provided and static , instead of refining it at each iteration based on the current pet, and (ii) using a specific loss function, namely negated log-likelihood.",
        "So why not simply use the maximum-likelihood (EM) training procedure for MT?",
        "One reason is that it is not discriminative: the loss function (e.g. negated bleu) is ignored during training.",
        "A second reason is that training good joint models pe(x,y) is computationally expensive.",
        "Contemporary MT makes heavy use of log-linear probability models, which allow the system designer to inject phrase tables, linguistic intuitions, or prior knowledge through a careful choice of features.",
        "Computing the objective function of (14) in closed form is difficult if pe is an arbitrary log-linear model, because the joint probability pe (xi; yi) is then defined as a ratio whose denominator Ze involves a sum over all possible sentence pairs (x, y) of any length.",
        "By contrast, our discriminative framework will only require us to work with conditional models.",
        "While conditional probabilities such as p^(x | y) and pe(y | x) are also ratios, computing their denominators only requires us to sum over a packed forest of possible translations of a given y or x.",
        "In summary, EM would impute missing data using pe (x | y) and predict outputs using pe (y | x), both being conditional forms of the same joint model pe(x, y).",
        "Our minimum imputed risk training method is similar, but it instead uses a pair of separately parameterized, separately trained models p^(x | y) and pe (y | x).",
        "By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train 0, which should improve both efficiency and accuracy at test time."
      ]
    },
    {
      "heading": "5. Experimental Results",
      "text": [
        "We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007).",
        "We train both reverse and forward baseline systems.",
        "The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), which comprises 40,000 pairs of transcribed utterances in the travel domain.",
        "We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp.",
        "Chinese) side of the bitext.",
        "We use a standard training pipeline and pruning settings recommended by (Chiang, 2007).",
        "For the NIST task, the TM is trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method implemented in Joshua.",
        "We also used a 5-gram language model, trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the bitext's English side.",
        "We use two classes of features fk for discriminative training of peas defined in (9).",
        "We include ten features that are standard in Hi-ero (Chiang, 2007).",
        "In particular, these include one baseline language model feature, three baseline translation models, one word penalty feature, three features to count how many rules with an arity of zero/one/two are used in a derivation, and two features to count how many times the unary and binary glue rules in Hiero are used in a derivation.",
        "In this paper, we do not attempt to discrimina-tively tune a separate parameter for each bilingual rule in the Hiero grammar.",
        "Instead, we train several hundred features that generalize across these rules.",
        "For each bilingual rule, we extract bigram features over the target-side symbols (including nonterminals and terminals).",
        "For example, if a bilingual rule's target-side is \"on the X1 issue of X2\" where X1 and X2 are non-terminals (with a position index), we extract the bigram features on the, the X, X issue, issue of, and ofX.",
        "(Note that the position index of a non-terminal is ignored in the feature.)",
        "Moreover, for the terminal symbols, we will use their dominant POS tags (instead of the symbol itself).",
        "For example, the feature the X becomes DT X.",
        "We use 541 such bigram features for IWSLT task (and 1023 such features for NIST task) that fire frequently.",
        "In addition to the 40,000 sentence pairs used to train the baseline generative models (which are used to compute the features fk), we use three bilingual data sets listed in Table 1, also from IWSLT, for discriminative training: one to train the reverse model (which uses only the 10 standard Hiero features as described in Section 5.2.1), one to train the forward model 6e (which uses both classes of features described in Section 5.2, i.e., 551 features in total), and one for test.",
        "Note that the reverse model 0 is always trained using the supervised data of Dev_0, while the forward model 0 may be trained in a supervised or semi-supervised manner, as we will show below.",
        "In all three data sets, each Chinese sentence xi has 16 English reference translations, so each yi is actually a set of 16 translations.",
        "When we impute data from yi (in the semi-supervised scenario), we",
        "Data set Purpose",
        "Table 1: IWSLT Data sets used for discriminative training/test.",
        "Dev_0 is used for discriminatively training of the reverse model 0, Dev_0 is for the forward model, and EvaL0 is for testing.",
        "The star * for Dev_0 emphasizes that some of its Chinese side will not be used in the training (see Table 2 for details).",
        "actually impute 16 different values of xi, by using to separately reverse translate each sentence in yi.",
        "This effectively adds 16 pairs of the form (xi; yi) to the training set (see section 3.4), where each xi is a different input sentence (imputed) in each case, but yi is always the original set of 16 references.",
        "sentences) to tune the component parameters in both the forward and reverse baseline systems.",
        "Additionally, we use the English side of MT04 (having 1788 sentences) to perform semi-supervised tuning of the forward model.",
        "The test sets are MT05 and MT06 (having 1082 and 1099 sentences, respectively).",
        "In all the data sets, each source sentence has four reference translations.",
        "We compare two training scenarios: supervised and semi-supervised.",
        "The supervised system (\"Sup\") carries out discriminative training on a bilingual data set.",
        "The semi-supervised system (\"+Unsup\") additionally uses some monolingual English text for discriminative training (where we impute one Chinese translation per English sentence).",
        "Tables 2 and 3 report the results for the two tasks under two training scenarios.",
        "Clearly, adding unsupervised data improves over the supervised case, by at least 1.3 bleu points in IWSLT and 0.5 bleu in",
        "NIST.",
        "Below, we will present more results on the IWSLT data set to help us understand the behavior of the",
        "Training scenario",
        "Table 2: BLEU scores for semi-supervised training for IWSLT task.",
        "The supervised system (\"Sup\") is trained on a subset of Dev_0 containing 200 Chinese sentences and 200x 16 English translations.",
        "\"+Unsup\" means that we include additional (monolingual) English sentences from Dev_0 for semi-supervised training; for each English sentence, we impute the 1-best Chinese translation.",
        "A star * indicates a result that is signicantly better than the \"Sup\" baseline (paired permutation test, p < 0.05).",
        "Table 3: BLEU scores for semi-supervised training for NIST task.",
        "The \"Sup\" system is trained on MT03, while the \"+Unsup\" systemis trained with additional 1788 English sentences from MT04.",
        "(Note that while MT04 has 1788x 4 English sentences as it has four sets of references, we only use one such set, for computational efficiency of discriminative training.)",
        "A star * indicates a result that is signicantly better than the \"Sup\" baseline (paired permutation test, p < 0.05).",
        "methods proposed in this paper.",
        "A critical component of our unsupervised method is the reverse translation model p^(x | y).",
        "We wonder how the performance of our unsupervised method changes when the quality of the reverse system varies.",
        "To study this question, we used two different reverse translation systems, one with a language model trained on the Chinese side of the bitext (\"WLM\"), and the other one without using such pervised case) shows that the imputed Chinese translations have a far lower bleu score without the language model, and that this costs us about 1 English",
        "Table 4: BLEU scores for unsupervised training with/without using a language model in the reverse system.",
        "A data size of 101 means that we use only the English sentences from a subset of Dev_0 containing 101 Chinese sentences and 101 x 16 English translations; for each English sentence we impute the 1-best Chinese translation.",
        "\"WLM\" means a Chinese language model is used in the reverse system, while \"NLM\" means no Chinese language model is used.",
        "In addition to reporting the bleu score on Eval_0, we also report \"Imputed-CN bleu\", the bleu score of the imputed Chinese sentences against their corresponding Chinese reference sentences.",
        "b le u point in the forward translations.",
        "Still, even with the worse imputation (in the case of \"NLM\"), our forward translations improve as we add more monolingual data.",
        "In all the experiments so far, we used the reverse translation system to impute only a single Chinese translation for each English monolingual sentence.",
        "This is the 1 best approximation of section 3.4.",
        "Table 5 shows (in the fully unsupervised case) that the performance does not change much as k increases.",
        "This may be because that the 5-best sentences are likely to be quite similar to one another (May and Knight, 2006).",
        "Imputing a longer k-best list, a sample, or a lattice for xi (see section 3.4) might achieve more diversity in the training inputs, which might make the system more robust."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "In this paper, we present an unsupervised discriminative training method that works with missing inputs.",
        "The key idea in our method is to use a reverse model to impute the missing input from the observed output.",
        "The training will then forward translate the imputed input, and choose the parameters of the forward model such that the imputed risk (i.e.,",
        "Test bleu",
        "Data size",
        "Imputed-CN bleu",
        "Test-EN bleu",
        "47.6",
        "WLM",
        "NLM",
        "WLM NLM",
        "49.0",
        "101",
        "11.8",
        "3.0",
        "48.5 46.7",
        "48.9",
        "202",
        "11.7",
        "3.2",
        "48.9 47.6",
        "49.7*",
        "303",
        "13.4",
        "3.5",
        "48.8 47.9",
        "Training scenario",
        "Test bleu MT05 MT06",
        "Sup, (919,919x4)",
        "32.4 30.6",
        "+Unsup, 1788 Eng sentences",
        "33.0* 31.1*",
        "Table 5: bleu scores for unsupervised training with different k-best sizes.",
        "We use 101 x 16 monolingual English sentences, and for each English sentence we impute the k-best Chinese translations using the reverse system.",
        "the expected loss of the forward translations with respect to the observed output) is minimized.",
        "This matches the intuition that the probabilistic \"round-trip\" translation from the target-language sentence to the source-language and back should have low expected loss.",
        "We applied our method to two Chinese to English machine translation tasks (i.e. IWSLT and NIST).",
        "We showed that augmenting supervised data with unsupervised data improved performance over the supervised case (for both tasks).",
        "Our discriminative model used only a small amount of training data and relatively few features.",
        "In future work, we plan to test our method in settings where there are large amounts of monolingual training data (enabling many discriminative features).",
        "Also, our experiments here were performed on a language pair (i.e., Chinese to English) that has quite rich bilingual resources in the domain of the test data.",
        "In future work, we plan to consider low-resource test domains and language pairs like Urdu-English, where bilingual data for novel domains is sparse."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was partially supported by NSF Grants No IIS-0963898 and No IIS-0964102 and the DARPA GALE Program.",
        "The authors thank Markus Dreyer, Damianos Karakos and Jason Smith for insightful discussions.",
        "Training scenario",
        "Test BLEU",
        "Unsup, k=1",
        "48.5",
        "Unsup, k=2",
        "48.4",
        "Unsup, k=3",
        "48.9",
        "Unsup, k=4",
        "48.5",
        "Unsup, k=5",
        "48.4"
      ]
    }
  ]
}
