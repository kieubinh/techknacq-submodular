{
  "info": {
    "authors": [
      "Dipak L. Chaudhari",
      "Om P. Damani",
      "Srivatsan Laxman"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1098",
    "title": "Lexical Co-occurrence, Statistical Significance, and Word Association",
    "url": "https://aclweb.org/anthology/D11-1098",
    "year": 2011
  },
  "references": [
    "acl-D07-1061",
    "acl-D09-1066",
    "acl-J06-1003",
    "acl-J93-1003",
    "acl-N09-1003",
    "acl-P06-1127",
    "acl-P06-2084",
    "acl-P89-1010",
    "acl-W09-3206"
  ],
  "sections": [
    {
      "text": [
        "Dipak L. Chaudhari Om P. Damani Srivatsan Laxman",
        "Computer Science and Engg.",
        "Computer Science and Engg.",
        "Microsoft Research India",
        "IIT Bombay IIT Bombay Bangalore",
        "Lexical co-occurrence is an important cue for detecting word associations.",
        "We propose a new measure of word association based on a new notion of statistical significance for lexical co-occurrences.",
        "Existing measures typically rely on global unigram frequencies to determine expected co-occurrence counts.",
        "Instead, we focus only on documents that contain both terms (of a candidate word-pair) and ask if the distribution of the observed spans of the word-pair resembles that under a random null model.",
        "This would imply that the words in the pair are not related strongly enough for one word to influence placement of the other.",
        "However, if the words are found to occur closer together than explainable by the null model, then we hypothesize a more direct association between the words.",
        "Through extensive empirical evaluation on most of the publicly available benchmark data sets, we show the advantages of our measure over existing co-occurrence measures."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Lexical co-occurrence is an important indicator of word association and this has motivated several co-occurrence measures for word association like PMI (Church and Hanks, 1989), LLR (Dunning, 1993), Dice (Dice, 1945), and CWCD (Washtell and Markert, 2009).",
        "In this paper, we present a new measure of word association based on a new notion of statistical significance for lexical co-occurrences.",
        "In general, a lexical co-occurrence could refer to a pair of words that co-occur in a large number of documents; or it could refer to a pair of words that, although co-occur only in a small number of documents, occur close to each other within those documents.",
        "We formalize these ideas and construct a significance test that allows us to detect different kinds of co-occurrences within a single unified framework (a feature which is absent in current measures for co-occurrence).",
        "Another distinguishing feature of our measure is that it is based solely on the cooccurrence counts in the documents containing both words of the pair, unlike all existing measures which also take global unigram frequencies in account.",
        "We need a null hypothesis that can account for an observed co-occurrence as a pure chance event and this in-turn requires a corpus generation model.",
        "Documents in a corpus can be assumed to be generated independent of each other.",
        "Existing cooccurrence measures further assume that each document is drawn from a multinomial distribution based on global unigram frequencies.",
        "The main concern with such a null model is the overbearing influence of the unigram frequencies on the detection of word associations.",
        "For example, the association between anomochilidae (dwarf pipe snakes) and snake could go undetected in our wikipedia corpus, since less than 0.1% of the pages containing snake also contained anomochilidae.",
        "Also, under current models, the expected span of a word pair is very sensitive to the associated unigram frequencies: the expected span of a word pair composed of low frequency un-igrams is much larger than that with high frequency unigrams.",
        "This is contrary to how word associations appear in language, where semantic relationships manifest with small inter-word distances irrespective of the underlying unigram distributions.",
        "Based on these considerations we employ a null model that represents each document as a bag of words .",
        "A random permutation of the associated bag of words gives a linear representation for the document.",
        "Under this null model, the locations of an unrelated pair of words will likely be randomly distributed in the documents in which they co-occur.",
        "If the observed span distribution of a word-pair resembles that under the (random permutation) null model, then the relation between the words is not strong enough for one word to influence the placement of the other.",
        "However, if the words are found to occur closer together than explainable by our null model, then we hypothesize a more direct association between the words.",
        "Therefore, this null model detects biases in span distributions of word-pairs while being agnostic to variations in global unigram frequencies.",
        "In this paper, we propose a new measure of word association based on the statistical significance of the observed span distribution of a word-pair.",
        "We perform extensive experiments on all the publicly available benchmark data sets and compare our measure against other popular co-occurrence measures.",
        "Our experiments demonstrate the advantages of our measure over all the competing measures.",
        "The ranked list of word associations output by our measure has the best correlation with the corresponding gold-standard in three (out of seven) data sets in our experiments, while remaining in the top three in other four datasets.",
        "While different measures perform best on different data sets, our measure outperforms other measures by being consistently either the best measure or very close to the best measure on all the data sets.",
        "The average deviation of our measure's correlation with the gold-standard from the best measure's correlation with the gold-standard (average taken across all the datasets) is 0.02, which is the least average deviation among all the measures, the next best deviations being 0.04 and 0.06.",
        "The paper is organized as follows.",
        "We present our notion of statistical significance of span distribution in Section 2.",
        "Algorithm for computing the proposed word association measure is described in Section 3.",
        "We discuss related work in Section 4.",
        "Performance evaluation is presented in Section 5 followed with conclusions in Section 6."
      ]
    },
    {
      "heading": "2. Lexically significant co-occurrences",
      "text": [
        "Evidence for significant lexical co-occurrences can be gathered at two levels in the data - document-level and corpus-level.",
        "First, at the document level, we may find that for a given word-pair, a surprisingly high proportion of its occurrences within a document have smaller spans than they would have by random chance.",
        "Second, at the corpus-level, we may find a pair of words appearing closer-than-random in multiple documents in the corpus.",
        "We now describe how to combine both kinds of evidence to decide whether the nearby occurrences of a word-pair are statistically significant or not.",
        "Let the frequency f of a word-pair a in a document D, be the maximum number of non-overlapped occurrences of a in D. A set of occurrences of a word-pair is said to be non-overlapped if the words corresponding to one occurrence from the set do not appear in-between the words corresponding to any other occurrence from the set.",
        "Let fx denote the maximum number of non-overlapped occurrences of a in D with span less than a given threshold x.",
        "We refer to /x as the span-constrained frequency of a in D. Note that /x cannot exceed f.",
        "To assess the statistical significance of the word-pair a we ask if the span-constrained frequency /x (of a) is more than what we would expect in a document of size I containing f 'random' occurrences of a.",
        "Our intuition is that if two words are associated in some way, they will often appear close to each other in the document and so the distribution of the spans will typically exhibit a bias toward values less than a suitably chosen threshold x.",
        "Deinition 1 Consider the null hypothesis that the linear representation ofa document is generated by choosing a random permutation ofthe bag ofwords associated with the document.",
        "Let £ be the length of the document and f denote the frequency ofa word-pair in the document.",
        "For a given a span threshold x, we define irx (fx, f, £) as the probability under the null that the word-pair will appear in the document with a span-constrained frequency of at least fx.",
        "Observe that 7rx(0,f, £) = 1 for any x > 0; also, for x > I we have 7rx(f, f, £) = 1 (i.e. all f occurrences will always have span less than x for x > £).",
        "However, for typical values of x (i.e. for x <C £) the probability 7rx(/x, f,£) decreases with increasing /x.",
        "For example, consider a document of length 400 with 4 non-overlapped occurrences of a.",
        "The probabilities of observing at least 4, 3, 2, 1 and 0 occurrences of a within a span of 20 words are 0.007, 0.09, 0.41, 0.83, and 1.0 respectively.",
        "Since 7T2o(3, 4, 400) = 0.09, even if 3 of the 4 occurrences of a have span less than 20 words, there is 9% chance that the occurrences were a consequence of a random event.",
        "As a result, if we desired a confidence-level of at least 95%, we would have to declare observed co-occurrences of a as insignificant.",
        "Given an e (0 < e < 1) and a span threshold x (> 0) the document D is said to support the hypothesis \"a is an e-significant word-pair within the document\" if we have [ttx(fx,f,£) < e].",
        "We refer to e as the document-level evidence ofthe lexical co-occurrence of a.",
        "We now describe how to aggregate evidence for lexical significance by considering the occurrence of a across multiple documents in the corpus.",
        "Let (Di,..., DK } denote the set of K documents (from out of the entire corpus) that contain at least one occurrence of a.",
        "Let £i be the length of Dj, f be the frequency of a in Di, and, fix be the span-constrained frequency of a in Di.",
        "Define indicator variables zi, i = 1,..., K as:",
        "As discussed previously, zi indicates whether \"a is an e-significant word-pair within the document",
        "Di.\"",
        "Note that we view fix as the only random quantity here, with x fixed by the user, and £i and fifixed given the document Di and word-pair a.",
        "Let Z = K=1 zi; Z models the number of documents (out of K) that support the hypothesis \"a is an e-significant word-pair.\"",
        "The expected value of Z is given by",
        "where ge>x (fi; £i) is given by Definition 2 below.",
        "Deinition 2 Given a document oflength £ in which a word-pair has a frequency off, and given a span threshold x, we define ge,x(f, £) as the smallest r for which the inequality [nx(r, f, £) < e] holds.",
        "Note that ge,x(f, £) is well-defined since 7rx(r, f, £) is non-increasing with respect to r. For the example given earlier, g0.2)20(4,400) = 3 and g0.05,20(4, 400) = 4.",
        "Since each document in the corpus is assumed to be generated independently, zi's are independent random variables and we can bound the deviation of the observed value ofZ from its expectation using Hoeffding's Inequality - for any t > 0, we have",
        "Recall that Z models the number of documents supporting the hypothesis \"a is an e-significant word-pair.\").",
        "Thus, the upper-bound ö (= exp( – 2Kt)), 0 < ö < 1 denotes the upper-bound on the probability that just due to random chance, more than (E(Z) + Kt) documents out of K will support the hypothesis \"a is an e-significant word-pair.\"",
        "We call ö the corpus-level evidence of the lexical cooccurrence a.",
        "For example, in our corpus, the word-pair (canyon, landscape) occurs in K = 416 documents.",
        "For e = 0.1, we have e-significant occurrences in Z = 33 documents (out of 416) , while E(Z) = 14.34.",
        "Suppose we want to be 99% sure that the occurrences of (canyon, landscape) in the 33 documents were a consequence of non-random phenomena.",
        "Let ö = 1 – 0.99 = 0.01.",
        "By setting",
        "Table 1: Examples of word-pairs from Florida dataset having statistically significant co-occurrences in the wikipedia corpus for different (e,6) combinations under a span constraint of 20 words.",
        "t = y/ln 6/(-2K) = 0.07, we get E(Z) + Kt = 43.46.",
        "Only if Z was 44 or more, there would be less than 1% chance of that being a random phenomena.",
        "Thus, we cannot be 99% sure that the observed cooccurrences in the 33 documents are non-random.",
        "Hence, our test declares (canyon, landscape) as insignificant at e = 0.1,6 = 0.01.",
        "We now summarize our significance test in the definition below.",
        "Definition 3 (Significant lexical co-occurrence) Consider a word-pair a and a set of K documents containing at least one occurrence each of a.",
        "Fix a span threshold of x (> 0), a document-level evidence of e (0 < e < 1) and a corpus-level evidence of 6 (0 < 6 < 1).",
        "Let Z denote the number of documents (out of K) that support the hypothesis \"a is e-significant within the document.\"",
        "The word-pair a is said to be (e, 6)-significant if we have [Z > E(Z) + Kt], where t = ./log 6/(-2K) and E(Z) is given by Eq.",
        "(2).",
        "The ratio [Z/(E(Z) + Kt)] is called the Co-occurrence Significance Ratio (CSR) for a.",
        "The significance test of Definition 3 gathers both document-level and corpus-level evidence from data in calibrated amounts.",
        "Prescribing e fixes the strength of the document-level hypothesis in our test, while, 6, controls the extent of corpus-level evidence we need to declare a word-pair as significant.",
        "A small 6 demands that there must be multiple documents in the corpus, each of which, individually have some evidence of relatedness for the pair of words.",
        "By running the significance test with different values of e and 6, the CSR test can be used to detect different types of lexically significant co-occurrences.",
        "For example, the strongest lexical co-occurrences would have both strong document-level evidence (low e) as well as high corpus-level evidence (low 6).",
        "Informally, these would represent pairs of words that appear multiple times with small spans within a document, in many documents, and in-practice, we find that multi-word expressions or pairs of words separated by stop words tend to dominate this type.",
        "On the other hand, a higher e would represent word-pairs that appear relatively farther apart within a document, or a higher 6 would represent word-pairs that appear together in relatively fewer documents.",
        "Note that to detect co-occurrences that exclusively correspond to (say) low e and high 6, we would have to run the test with low e and high 6, and then remove word-pairs that were also found significant at low e and low 6.",
        "In Table 1, we present some examples of different types of co-occurrences.",
        "The table lists word-pairs that were found to be statistically significant for different choices of (e, 6).",
        "Note that a word-pair is reported under (e = 0.1, 6 = 0.4) or (e = 0.4, 6 = 0.1) only if it was not also found significant under other two parameter settings.",
        "The strongest correlations are the word-pairs corresponding to (e = 0.1,6 = 0.1) e.g., algae-green, rat-dirty and worm-insect.",
        "Different sets of weaker co-occurrences are detected depending on whether we relaxed 6 or e. For example, algae-mold is significant at a higher 6, while algae-pool is significant for higher e.",
        "The semantic notion of word association is an abstract concept and different kinds of associations (with potentially different statistical characterizations) may be preferred by human judges in different situations.",
        "While in Section 5, we discuss in detail various datasets used, the evaluation methodology, and the performance of CSR across datasets, we wish to point out here that in 3 out of 5 cross-validation runs for wordsim dataset, the best performing CSR parameters were x = 50w, e = 0.1 and 6 = 0.9, while in 3 out of 5 runs for Minnesota dataset, the best performing CSR parameters were x = 20w, e = 0.3 and 6 = 0.5.",
        "This gives us some indication that different kinds of word associations were preferred in different data sets.",
        "word-1",
        "word-2",
        "(0.1, 0.1)",
        "(0.1, 0.4)",
        "(0.4, 0.1)",
        "algae",
        "green",
        "mold",
        "pool",
        "amuse",
        "entertain",
        "clown",
        "amaze",
        "damn",
        "hell",
        "mad",
        "bad",
        "rat",
        "dirty",
        "ugly",
        "disease",
        "sedative",
        "drug",
        "narcotic",
        "calm",
        "topping",
        "chocolate",
        "flavour",
        "caramel",
        "umbrella",
        "rain",
        "dry",
        "shade",
        "unknown",
        "known",
        "dark",
        "secret",
        "worm",
        "insect",
        "dirt",
        "fishing",
        "wrap",
        "cover",
        "seal",
        "bandage"
      ]
    },
    {
      "heading": "3. Computing Co-occurrence Significance Ratio(CSR)",
      "text": [
        "There are three main steps for computing CSR and the pseudocodes for these are listed in Procedures 1, 2 & 3.",
        "Of these, the first two can be run offline since they do not depend on the text corpus.",
        "They need to be run only once, after which CSR can be computed for any word-pair on any given corpus of documents.",
        "We describe these steps in the subsections below.",
        "The first step is to compute a histogram for the span-constrained frequency, /x, of a word-pair whose frequency is f in a document of length £, given a chosen span threshold of x (under our null model).",
        "Definition 4 Given a document of length £ and a span threshold of x, we define histfte>x('fx) as the number ofways to embed f non-overlapped occurrences ofa word-pair in the document such that exactly fx occurrences have span less than x.",
        "Procedure 1 ComputeHist(f, £, x) - Offline",
        "Input f - number of non-overlapped occurrences; £ - document length;",
        "x - span threshold Computes histf,£,x [•] as per Definition 4 3: return 6: return",
        "Procedure 1 lists the pseudocode for computing the histogram histf,£,x.",
        "The main steps involve selecting a start and end position for embedding the very first occurrence (lines 7-8) and then recursively calling ComputeHist(, •, •) (line 9).",
        "The i-loop selects a start position for the first occurrence of the word-pair, and the j-loop selects the end position.",
        "The recursion step now computes the number of ways to embed the remaining (f - 1) non-overlapped occurrences in the remaining (I – j) positions.",
        "Once we have histf – 1,£ – j, we check whether the occurrence introduced at positions (i, j) will contribute to the fx count.",
        "If (j – i) < x, whenever there are k span-constrained occurrences in positions (j + 1) to I, there will be (k + 1) span-constrained occurrences in positions 1 to I.",
        "Thus, we increment histf,£[k + 1] by the quantity histf [k] (lines 10-12).",
        "However, if (j – i) > x, there is no contribution to the span-constrained frequency from the ( i, j) occurrence, and so we increment histf,£[k] by the quantity histf – 1,£ – j[k] (lines 10-11, 13-14).",
        "Finally, we note that in our implementation we use memorization to avoid redundant recursive calls.",
        "Procedure 2 ComputePiDist(f, £, x) - Offline",
        "Input f - number of non-overlapped occurrences; £ - document length;",
        "x - span threshold Computes Distribution nx [f, £, •] as per Definition 1 and ge>x [f, £] as",
        "The second offline step is computation of the nx(,f,t) distribution.",
        "We store the number of ways of embedding f non-overlapped occurrences of a word-pair in a document of length £ in the array N[f,£].",
        "Similarly, the array Nx[fx, f,£] stores the number of ways of embedding f non-overlapped occurrences of the word-pair in a document of length £, such that at least fx of the f occurrences have span less than x.",
        "To compute N[f, £, x] and Nx[fx, f, £], we need the histogram histf,£,x[ ] which is the output of Procedure 1.",
        "Procedure 2 lists the pseudocode for computing nx(fx, f, £) from N(f, £) and Nx(fx, f,£) given histf/from Procedure 1 (For the sake of readability the pseudocode does not describe some optimizations that we used in our implementation).",
        "The Procedure 1 is exponential in f and £ but it does not depend on the data corpus.",
        "Hence, we can run the Procedures 1 and 2 off-line, and publish the nx[] and gex[] tables for various x, /x, f and £.",
        "Using these tables, anyone wishing to compute CSR needs to only run Procedure 3.",
        "Procedure 3 ComputeCSR(a, e,6,x)",
        "Input a - word-pair; e - document-level evidence; S - corpus-level evidence; x - span threshold; Corpus of documents Computes CSR(a) - Co-occurrence Significance Ratio (CSR) for a as per Definition 3",
        "1: D 4 – {Di,..., Dk} // Set of documents from the corpus that each contain at least one occurrence of a.",
        "6: fi = Frequency of a in Di 7: ff = Span-constrained frequency of a in Di",
        "Procedure 3 implements the significance test given in Definition 3 and requires that the nx [] and gex [] tables have already been computed offline.",
        "The first step is to determine the subset D of documents containing the given word-pair (line 1).",
        "Then we compute t based on 6 and K (the size of D) (line 2).",
        "Next we determine how many of the K documents support the hypothesis \"a is e-significant within the document\" (lines 3-12).",
        "The expected number of documents supporting the hypothesis is accumulated in ZE (line 13).",
        "CSR is then computed as the ratio of Z to (ZE + Kt) (line 14).",
        "The computation of Co-occurrence Significance Ratio (CSR) as given in Definition 3 might appear more complex than the simple formulae for other co-occurrence measures given in Table 2.",
        "However, bulk of the complexity in calculating CSR lies in the one-time (data independent) off-line computation of the nx[] and ge,x[] tables.",
        "Once these tables are published, the cost of comparing CSR for a given word pair is comparable to the cost of computing any other (spanned) measure in Table 2.",
        "The main data-dependent computations for a spanned measure are in determining span-constrained frequencies; all other steps are simple arithmetic operations or memory lookups.",
        "To illustrate this, Procedure 4 gives details of computing PMI.",
        "The comparison of Procedures 3 and 4 shows their almost parallel structures.",
        "The main overhead in these procedures is incurred in line 7, where span-constrained frequencies in a given document are computed.",
        "Procedure 4 ComputePMI(a, b)",
        "Input (x, y) - word pair; Computes PMI (Table 2) for (x,y).",
        "1: let D = {D1t..., Dk } // set of documents containing at least one occurrence of a.",
        "2: N = total number of words in corpus 3: (fx,fy) = unigram frequencies of x, y in corpus 7: fi = span-constrained frequency of a in Di8: f = f + fi"
      ]
    },
    {
      "heading": "4. Related Work",
      "text": [
        "Existing word association measures can be divided into three broad categories: (i) Co-occurrence measures that rely on co-occurrence frequencies of both words in a corpus in addition to the individual unigram frequencies (Table 2), (ii) Distributional similarity-based measures that characterize a word by the distribution of other words around it (Agirre et al., 2009; Bollegala et al., 2007; Chen et al., 2006; Wandmacher et al., 2008), and (iii) Knowledge-based measures that use knowledge-sources like thesauri, semantic networks, or taxonomies (Milne",
        "Gabrilovich and Markovitch, 2007; Yeh et al., 2009; Strube and Ponzetto, 2006; Finkelstein et al., 2002; Liberman and Markovitch, 2009).",
        "In this paper, we focus on comparison with other co-occurrence measures.",
        "These measures are used in several domains like ecology, psychology, medicine, and language processing.",
        "Table 2 lists several measures chosen from all these domains.",
        "Except Ochiai (Ochiai, 1957; Janson and Vegelius, 1981) and the recently introduced",
        "N Total number of tokens in the corpus f (x), f (y) unigram frequencies of x, y in the corpus f(x,y) Span-constrained (x, y) word pair frequency in corpus M Harmonic mean of the spans of f (x, y) occurrences",
        "CWCD (Washtell and Markert, 2009) all other measures are well-known in the NLP community (Pecina and Schlesinger, 2006).",
        "Our results show that Ochiai and Chi-Square have almost identical performance, differing only in 3rd decimal digits.",
        "Rankings produced by Chi-square is almost monotonic with respect to the rankings produced by Ochiai.",
        "This is because, for most word pairs (x, y), [f (x) « N], [f (y) « N], [f (x,y) « f (x)], and [f (x, y) « f (y)].",
        "Therefore three of the four terms in the Chi-square summation become zero and the fourth term approximates to the square of Ochiai.",
        "Similarly Jaccard and Dice coincide.",
        "While presenting our experimental results, we report these pairs of measures together.",
        "Data Set",
        "No.",
        "of Respon-",
        "Filtered"
      ]
    },
    {
      "heading": "5. Performance Evaluation",
      "text": [
        "Two main aspects of word association studied in literature are: a) semantic relatedness, and b) free association.",
        "Semantic relatedness encompasses many different relationships between words, like synonymy, meronymy, antonymy, and functional association (Budanitsky and Hirst, 2006).",
        "Free association refers to the first response-words that come to mind when presented with a stimulus.",
        "(ESSLLI, 2008).",
        "We experiment with all the publicly available datasets that come with gold standard judgement of these aspects, except the very small ones with less than 80 word-pairs.",
        "Details of the datasets used in our experiments are listed in Table 3.",
        "Each data set comes with a goldstandard of human judgments - a ranked list of association scores for the word-pairs in the data set.",
        "The wordsim dataset was prepared by asking the subjects to estimate the relatedness of the word pairs on a",
        "Goodenough (Rubenstein and Goodenough,",
        "Method",
        "Formula",
        "CSR (this work)",
        "Z/(E(Z) + Kt)",
        "CWCD (Washtell and Markert, 2009)",
        "f(x,y) 1/max(p(x),p(y)) p(x) M",
        "Dice (Dice, 1945)",
        "2f(x,y)",
        "f (x) + f (y)",
        "LLR (Dunning, 1993)",
        "''P v(x' v')loa p(x',y')/ > ' y / y p(x')p(y')",
        "x G {x, – x} y' G {y, – y}",
        "Jaccard (Jaccard, 1912)",
        "f(x,y)f (x ) + f (y)-f (x,y)",
        "Ochiai (Janson and Veg-elius, 1981)",
        "f(x,y)",
        "Vf (x)f(y)",
        "Pearson's x test",
        "Y> (f(x',y')-Ef(x',y'))Ef (x' ,y')",
        "x G {x, – x} y' G {y, – y}",
        "PMI (Church and Hanks, 1989)",
        "log p(x)p(y)",
        "SCI (Washtell and Markert, 2009)",
        "p(x,y)p(x)V p(y)",
        "T-test",
        "f(x,y)-Ef(x,y) ^f (x,y)(-^)",
        "dents",
        "Pairs",
        "Word Pairs",
        "Semantic",
        "wordsim",
        "16",
        "353",
        "351",
        "relatedness",
        "(Finkelstein et al., 2002)",
        "Edinburg (Kiss et al.,",
        "100",
        "325,588",
        "83,713",
        "1973)",
        "Florida (Nelson et",
        "5,019",
        "65,523",
        "59,852",
        "al., 1980)",
        "Free-",
        "Goldfarb-Halpern",
        "316",
        "410",
        "384",
        "Association",
        "(Goldfarb and",
        "Halpern, 1984)",
        "Kent (Kent and",
        "1,000",
        "14,576",
        "14,086",
        "Rosanoff, 1910)",
        "Minnesota (Russell",
        "1,007",
        "10,447",
        "9,649",
        "and Jenkins, 1954)",
        "White-Abrams",
        "440",
        "745",
        "652",
        "(White and Abrams,",
        "2004)",
        "Table 4: Comparison of the average Spearman coefficients obtained across five cross-validation runs by different measures.",
        "The best performing measure for each dataset is shown in bold.",
        "All standard deviations for Edinburg and Florida were less than 0.01, for Kent and Minnesota were between 0.01 and 0.02, for White-Abrams were between 0.05 and 0.08, for Goldfarb-Halpern between 0.05 and 0.15 and for wordsim were between 0.02 and 0.15.",
        "Number of word-pairs in each dataset is shown in brackets against its name.",
        "Table 5: Comparison of deviations from the best performing measure on each data set.",
        "Number of word-pairs in each dataset is shown in brackets against its name.",
        "Figures in brackets against the deviation values denote the ranks of the measures in the corresponding data sets.",
        "scale from 0 to 10 (Finkelstein et al., 2002).",
        "The methodology for collecting free association data is explained at (ESSLLI, 2008): The degree of free association between a stimulus (S) and response (R) is the percentage of respondents who respond R as the first response when presented with stimulus S.",
        "These datasets are of varying size, and they were constructed at different point in time, in different geographies.",
        "This allows us to compare different measures comprehensively under varying range of circumstances.",
        "To the best of our knowledge, no previous work has reported such a detailed comparison of co-occurrence measures.",
        "We use the Wikipedia (Wikipedia, April 2008) corpus with 2.7 million articles (total of 1.24 Giga-words).",
        "We did no preprocessing - no lemmatization or function-word removal.",
        "When counting document size (in words), punctuations were ignored.",
        "Documents larger than 1500 words were partitioned such that each part was at most 1500 words.",
        "We indexed the corpus using Lucene search engine library and used Lucene APIs to obtain various statistics and documents containing given word-pairs.",
        "Each measure listed in Table 2 produces a ranked list of association scores for the word-pairs in a data set.",
        "We evaluate each measure by the Spearman's rank correlation between the ranking produced by the measure and the gold-standard ranking.",
        "The span threshold (or window-width) x is a user-defined parameter in all measures.",
        "In addition, CSR has the parameters e and 6.",
        "For any measure, the ranking of word-pairs will likely change with chang-",
        "Edinburg (83,713)",
        "Florida (59,852)",
        "Kent (14,086)",
        "Minnesota",
        "(9,649)",
        "White-",
        "Abrams",
        "(652)",
        "Goldfarb-Halpern",
        "(384)",
        "wordsim",
        "(351)",
        "CSR",
        "0.25",
        "0.30",
        "0.42",
        "0.31",
        "0.34",
        "0.10",
        "0.63",
        "CWCD",
        "0.23",
        "0.23",
        "0.40",
        "0.30",
        "0.21",
        "0.19",
        "0.54",
        "Dice (Jaccard)",
        "0.20",
        "0.27",
        "0.43",
        "0.32",
        "0.21",
        "0.09",
        "0.59",
        "LLR",
        "0.20",
        "0.26",
        "0.40",
        "0.29",
        "0.18",
        "0.03",
        "0.51",
        "Ochiai (x)",
        "0.24",
        "0.30",
        "0.43",
        "0.31",
        "0.29",
        "0.08",
        "0.62",
        "PMI",
        "0.22",
        "0.25",
        "0.36",
        "0.26",
        "0.22",
        "0.11",
        "0.69",
        "SCI",
        "0.24",
        "0.27",
        "0.38",
        "0.27",
        "0.23",
        "0.06",
        "0.37",
        "TTest",
        "0.17",
        "0.23",
        "0.37",
        "0.26",
        "0.17",
        "-0.02",
        "0.45",
        "Edinburg",
        "(83,713)",
        "Florida",
        "(59,852)",
        "Kent (14,086)",
        "Minnesota",
        "(9,649)",
        "White-",
        "Abrams",
        "(652)",
        "Goldfarb-",
        "Halpern",
        "(384)",
        "wordsim",
        "(351)",
        "Worst Rank",
        "Avg.",
        "Deviation",
        "Worst Deviation",
        "CSR",
        "0.00 (1)",
        "0.00 (1)",
        "0.01 (3)",
        "0.01 (2)",
        "0.00 (1)",
        "0.09 (3)",
        "0.06 (2)",
        "3",
        "0.02",
        "0.09",
        "CWCD",
        "0.02 (4)",
        "0.07 (7)",
        "0.03 (4)",
        "0.02 (4)",
        "0.13 (5)",
        "0.00 (1)",
        "0.15 (5)",
        "7",
        "0.06",
        "0.15",
        "Dice (Jaccard)",
        "0.05 (6)",
        "0.03 (3)",
        "0.00 (1)",
        "0.00 (1)",
        "0.13 (5)",
        "0.10(4)",
        "0.10(4)",
        "6",
        "0.06",
        "0.13",
        "LLR",
        "0.05 (6)",
        "0.04 (5)",
        "0.03 (4)",
        "0.03 (5)",
        "0.16 (7)",
        "0.16(7)",
        "0.18 (6)",
        "7",
        "0.09",
        "0.18",
        "Ochiai (x)",
        "0.01 (2)",
        "0.00 (1)",
        "0.00 (1)",
        "0.01 (2)",
        "0.05 (2)",
        "0.11 (5)",
        "0.07 (3)",
        "5",
        "0.04",
        "0.11",
        "PMI",
        "0.03 (5)",
        "0.05 (6)",
        "0.07 (8)",
        "0.06 (7)",
        "0.12 (4)",
        "0.08 (2)",
        "0.00 (1)",
        "8",
        "0.06",
        "0.12",
        "SCI",
        "0.01 (2)",
        "0.03 (3)",
        "0.05 (6)",
        "0.05 (6)",
        "0.11 (3)",
        "0.13 (6)",
        "0.32 (8)",
        "8",
        "0.10",
        "0.32",
        "TTest",
        "0.08 (8)",
        "0.07 (7)",
        "0.06 (7)",
        "0.06 (7)",
        "0.17 (8)",
        "0.21 (8)",
        "0.24 (7)",
        "8",
        "0.13",
        "0.24",
        "Table 6: Comparison of co-occurrence based measures with knowledge-based and distributional similarity based measures.",
        "These other measures have not been applied to the free association datasets shown in Table 3.",
        "Data for missing entries is not available.",
        "Note that sim and rel are subsets of wordsim dataset.",
        "Number of word-pairs in each dataset is shown in brackets against its name.",
        "ing parameter values.",
        "Hence we follow the standard methodology of fixing parameters through cross validation.",
        "Specifically, we partition the data into five folds, four of which are used for training and one holdout fold is used for testing.",
        "For each measure, the parameter values that achieve best correlation with human judgments on 4 training folds are used to predict on the 1 holdout testing fold.",
        "This experiment is repeated 5 times for different training and test folds.",
        "The average rank correlation obtained by each measure over 5 cross-validation runs is reported for each dataset.",
        "We varied e and 6 between 0.01 and 0.90 and x between 5 and 50 words.",
        "For each measure and for each data set, the average correlation over the 5 cross-validation runs is reported in Table 4.",
        "The corresponding standard deviations are mentioned in the table's caption.",
        "The best performing measure in each case is highlighted in bold.",
        "While different measures performed best on different data sets, the results in Table 4 shows that CSR performs consistently well across all data sets.",
        "In all data sets the correlation for CSR was always either the best or close to the best.",
        "As expected, our results are statistically more significant for the larger data sets, compared to the smaller ones.",
        "The standard deviations of the results are small for two largest data sets (less than 0.01 for Edinburg and Florida), gradually increasing (less than 0.02 for Kent and Minnesota), and becoming high (upto .15) for the three smallest datasets.",
        "Although, among all measures, CSR has the best average correlation over all datasets, taking average of correlations across widely different dataset is not a meaningful way to decide on which measure to use.",
        "Ideally one would like to access an oracle to learn which measure will perform best on a particular unseen application dataset.",
        "Short of such an oracle, if one were to pick a fixed measure a-priori, then one would like to know how much worse off one is compared to the best measure for that dataset.",
        "To compare different measures from this perspective, we compute the deviation of the correlation for each measure from the correlation of the best measure for each data set.",
        "These deviations are reported in Table 5, along with the corresponding ranks.",
        "The average deviation of CSR over all the data sets is 0.02, which is the least among all the measures, the next two being 0.04 and 0.06.",
        "CSR also has the least worst-deviation among all measures.",
        "Also, CSR is never ranked worse than 3 in any of the data sets.",
        "This is also the smallest worst-rank among all measures.",
        "Based on these results, we infer that CSR is overall the best performing co-occurrence based word association measure.",
        "While the focus of our work is on the cooccurrence measures, for completeness, we present all the known results for knowledge and distributional similarity-based measures on the datasets under consideration in Table 6.",
        "Note that in (Agirre et al., 2009), the wordsim data set was partitioned into two sets, namely sim and rel, and in Esslli shared task (ESSLLI, 2008), a 272 word pair subset of the Edinburgh dataset was chosen.",
        "To facilitate comparison, in addition to CSR, we also present results for PMI and Ochiai (Chi-Square) which are the best performing co-occurrence measures on wordsim, and Esslli datasets.",
        "For co-occurrence-based measures, we used 5-fold cross validation, which is inapplicable for parameterless measures.",
        "Results show that co-occurrence-based measures compare well with other resource-heavy measures.",
        "wordsim",
        "Method",
        "Resource",
        "wordsim",
        "sim",
        "rel",
        "Esslli",
        "(353)",
        "(203)",
        "(252)",
        "(272)",
        "PMI",
        "Wikipedia",
        "0.69",
        "0.72",
        "0.68",
        "0.32",
        "Ochiai (x )",
        "Wikipedia",
        "0.62",
        "0.68",
        "0.62",
        "0.44",
        "Significance Ratio (CSR)",
        "Wikipedia",
        "0.63",
        "0.70",
        "0.64",
        "0.43",
        "Latent Semantic Analysis (Wandmacher et al., 2008)",
        "Newspaper corpus",
        "-",
        "-",
        "-",
        "0.38",
        "Graph Traversal (WN30g) (Agirre et al., 2009))",
        "Wordnet",
        "0.66",
        "0.72",
        "0.56",
        "-",
        "Bag of Words based Distributional Similarity (BoW) (Agirre et al., 2009))",
        "Web corpus",
        "0.65",
        "0.70",
        "0.62",
        "-",
        "Context Window based Distributional Similarity (CW) (Agirre et al., 2009))",
        "Web corpus",
        "0.60",
        "0.77",
        "0.46",
        "-",
        "Hyperlink Graph (Milne and Witten, 2008)",
        "Wikipedia hyperlinks graph",
        "0.69",
        "-",
        "-",
        "-",
        "Random Graph Walk (Hughes and Ramage, 2007)",
        "WordNet",
        "0.55",
        "-",
        "-",
        "-",
        "Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007)",
        "Wikipedia concepts",
        "0.75",
        "-",
        "-",
        "-",
        "(reimplemented in (Yeh et al., 2009))",
        "(0.71)",
        "Normalized Path-length (lch) (Strube and Ponzetto, 2006)",
        "Wikipedia category tree",
        "0.55",
        "-",
        "-",
        "-",
        "Thesarus based (Jarmasz, 2003)",
        "Roget's thesaurus",
        "0.55",
        "-",
        "-",
        "-",
        "Latent Semantic Analysis (Finkelstein et al., 2002)",
        "Web corpus",
        "0.56",
        "-",
        "-",
        "-"
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "In this paper, we introduced a new measure called CSR for word-association based on statistical significance of lexical co-occurrences.",
        "Our measure, while being agnostic to global unigram frequencies, detects skews in span distributions of word-pairs in documents containing both words.",
        "We carried out extensive evaluation on several benchmark datasets.",
        "Our experiments demonstrate the advantages of our measure over all the competing measures."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported in part by the Ministry of Human Resources Development, Government of India and by the Tata Research Development and Design Center (TRDDC).",
        "We thank Mr. Justin Washtell (University of Leeds) for providing us with various datasets."
      ]
    }
  ]
}
