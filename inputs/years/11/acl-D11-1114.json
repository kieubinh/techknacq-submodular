{
  "info": {
    "authors": [
      "Shay B. Cohen",
      "Carlos Gómez-Rodríguez",
      "Giorgio Satta"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1114",
    "title": "Exact Inference for Generative Probabilistic Non-Projective Dependency Parsing",
    "url": "https://aclweb.org/anthology/D11-1114",
    "year": 2011
  },
  "references": [
    "acl-C96-2215",
    "acl-D07-1014",
    "acl-D07-1015",
    "acl-D09-1005",
    "acl-E09-1034",
    "acl-E09-1055",
    "acl-H05-1036",
    "acl-H05-1066",
    "acl-J03-1006",
    "acl-J08-4003",
    "acl-J99-4004",
    "acl-P05-1013",
    "acl-P09-1040",
    "acl-P10-1110",
    "acl-P10-1151",
    "acl-P11-1068",
    "acl-P89-1018",
    "acl-P98-1106",
    "acl-P99-1059",
    "acl-W04-0308",
    "acl-W06-2920",
    "acl-W06-2922",
    "acl-W07-2216"
  ],
  "sections": [
    {
      "text": [
        "Shay B. Cohen Carlos Gomez-Rodriguez",
        "Departamento de Computation Universidade da Coruna, Spain",
        "cgomezr@udc.es",
        "We describe a generative model for non-projective dependency parsing based on a simplified version of a transition system that has recently appeared in the literature.",
        "We then develop a dynamic programming parsing algorithm for our model, and derive an inside-outside algorithm that can be used for unsupervised learning of non-projective dependency trees."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Giorgio Satta",
        "of Information Engineering University of Padua, Italy",
        "In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al.",
        "(2011) and present a polynomial dynamic programming algorithm for non-projective transition-based parsing.",
        "Our algorithm is coupled with a simplified version of the transition system from At-tardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks.",
        "Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi's system allows reduction of elements at non-adjacent positions in the stack.",
        "We also present a generative probabilistic model for transition-based parsing.",
        "The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework.",
        "Dynamic programming algorithms for non-projective parsing have been proposed by Kahane et al.",
        "(1998), Gomez-Rodriguez et al.",
        "(2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the 'gap degree' of the parsed structures.",
        "To the best of our knowledge, this paper is the first to programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011).",
        "Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself.",
        "In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for ma chine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009).",
        "Dependency grammars have receivedattention in the statistical parsing crecent years.",
        "These grammatical fofer a good balance between structurity and processing efficiency.",
        "Most nnon-projectivity is supported, these fomodel crossing syntactic relations thatlanguages with relatively free word ord",
        "Recent work has reduced non-projeto the identification of a maximum spangraph (McDonald et al., 2005; Koo et aDonald and Satta, 2007; Smith and S",
        "An alternative to this approach is to ubased parsing (Yamada and Matsumotoand Nilsson, 2005; Attardi, 2006;Gomez-Rodriguez and Nivre, 2010), wan incremental processing of a stringthat scores transitions between parsertioned on the parse history.",
        "This papthe latter approach.",
        "The above work on transition-basefocused on greedy algorithms set inframework (Nivre, 2008).",
        "More recen introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree.",
        "The rest of this paper is organized as follows.",
        "In §2 and §3 we outline the transition-based model we use, together with a probabilistic generative interpretation.",
        "In §4 we give the tabular algorithm for parsing, and in §5 we discuss statistical inference using expectation maximization.",
        "We then discuss some other aspects of the work in §6 and conclude in §7."
      ]
    },
    {
      "heading": "2. Transition-based Dependency Parsing",
      "text": [
        "In this section we briefly introduce the basic definitions for transition-based dependency parsing.",
        "For a more detailed presentation of this subject, we refer the reader to Nivre (2008).",
        "We then define a specific transition-based model for non-projective dependency parsing that we investigate in this paper.",
        "Assume an input alphabet E with a special symbol $ G E, which we use as the root of our parse structures.",
        "Throughout this paper we denote the input string as w = a0 • • • an-1, n > 1, where a0 = $ and ai G E \\ {$} for each i with 1 < i < n – 1.",
        "A dependency tree for w is a directed tree Gw = (Vw, Aw), where Vw = {0,..., n – 1} is the set of nodes, and Aw ç Vw x Vw is the set of arcs.",
        "The root of Gw is the node 0.",
        "The intended meaning is that each node in Vw encodes the position of a token in w. Furthermore, each arc in Aw encodes a dependency relation between two tokens.",
        "We write i – j to denote a directed arc (i, j) G Aw, where node i is the head and node j is the dependent.",
        "A transition system (for dependency parsing) is a tuple S = (C, T, I, Ct), where C is a set of configurations, defined below, T is a finite set of transitions, which are partial functions t: C – C, I is a total initialization function mapping each input string to a unique initial configuration, and Ct ç C is a set of terminal configurations.",
        "A configuration is defined relative to some input string w, and is a triple (a,ß,A), where a and ß are disjoint lists called stack and buffer, respectively, and A ç Vw x Vw is a set of arcs.",
        "Elements of a and ß are nodes from Vw and, in the case of the stack, a special symbol <: that we will use as initial stack symbol.",
        "If t is a transition and c1, c2 are configurations such that t(ci) = c2, we write ci ht c2, or simply c1 h c2 if t is understood from the context.",
        "Given an input string w, a parser based on S incrementally processes w from left to right, starting in the initial configuration I(w).",
        "At each step, the parser nondeterministically applies one transition, or else it stops if it has reached some terminal configuration.",
        "The dependency graph defined by the arc set associated with a terminal configuration is then returned as one possible analysis for w.",
        "Formally, a computation of S is a sequence 7 = c0,..., cm, m > 1, of configurations such that, for every i with 1 < i < m, ci-1 hti ci for some ti G T. In other words, each configuration in a computation is obtained as the value of the preceding configuration under some transition.",
        "A computation is called complete whenever co = I(w) for some input string w, and cm G Ct.",
        "We can view a transition-based dependency parser as a device mapping strings into graphs (dependency trees).",
        "Without any restriction on transition functions in T, these functions might have an infinite domain, and could thus encode even non-recursively enumerable languages.",
        "However, in standard practice for natural language parsing, transitions are always specified by some finite mean.",
        "In particular, the definition of each transition depends on some finite window at the top of the stack and some finite window at the beginning of the buffer in each configuration.",
        "In this case, we can view a transition-based dependency parser as a notational variant of a push-down transducer (Hopcroft et al., 2000), whose computations output sequences that directly encode dependency trees.",
        "These transducers are nondeterministic, meaning that several transitions can be applied to some configurations.",
        "The transition systems we investigate in this paper follow these principles.",
        "We close this subsection with some additional notation.",
        "We denote the stack with its topmost element to the right and the buffer with its first element to the left.",
        "We indicate concatenation in the stack and buffer by a vertical bar.",
        "For example, for k G Vw, a\\k denotes some stack with topmost element k and k\\ß denotes some buffer with first element k. For 0 < i < n – 1, ßi denotes the buffer [i, i + 1,..., n – 1]; for i > n, ßi denotes [] (the empty buffer).",
        "We now turn to give a description of our transition system for non-projective parsing.",
        "While a projective dependency tree satisfies the requirement that, for every arc in the tree, there is a directed path between its headword and each of the words between the two endpoints of the arc, a non-projective dependency tree may violate this condition.",
        "Even though some natural languages exhibit syntactic phenomena which require non-projective expressive power, most often such a resource is used in a limited way.",
        "This idea is demonstrated by Attardi (2006), who proposes a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency.",
        "The author defines this distance as the degree of the transition, with transitions of degree one being able to handle only projective dependencies.",
        "This formulation permits parsing a subset of the non-projective trees, where this subset depends on the degree of the transitions.",
        "The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three.",
        "For instance, on training data for Czech containing 28,934 non-projective relations, 27,181 can be handled by degree two transitions, and 1,668 additional dependencies can be handled by degree three transitions.",
        "Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006).",
        "We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two.",
        "Our results are based on such a restriction.",
        "It is not difficult to extend our algorithms (§4) to higher degree transitions, but this comes at the expense of higher complexity.",
        "See §6 for more discussion on this issue.",
        "Let w = a0 • • • an-1 be an input string over E defined as in §2.1, with a0 = $.",
        "Our transition system for non-projective dependency parsing is",
        "Table 1: The number of non-projective relations of various degrees for several treebanks (training sets), as reported by the parser of Attardi (2006).",
        "Deg.",
        "stands for 'degree.'",
        "The parser did not detect non-projective relations of degree higher than 4.",
        "where C is the same set of configurations defined in §2.1.",
        "The initialization function I(np) maps each string w to the initial configuration ([<:], ß0, 0).",
        "The set of terminal configurations Ct(np) contains all configurations of the form ([<:, 0], [], A), for any set of arcs A.",
        "The set of transition functions is defined as where each transition is specified below.",
        "We let variables i, j, k, l range over V«, and variable a is a list of stack elements from Vw U {<:}:",
        "ra2 : (a\\i\\j\\k,ß,A) h (a\\i\\j,ß,A U {i – k}).",
        "Each of the above transitions is undefined on configurations that do not match the forms specified above.",
        "As an example, transition la2 is not defined for a configuration (a, ß, A) with \\a\\ < 2, and transition shb is not defined for a configuration (a, k\\ß, A) with b = ak, or for a configuration (a, [], A).",
        "Transition shb removes the first node from the buffer, in case this node represents symbol b G E, and pushes it into the stack.",
        "These transitions are called shift transitions.",
        "The remaining four transitions are called reduce transitions, i.e., transitions that consume nodes from the stack.",
        "Notice that in the transition system at hand all the reduce transitions decrease the size of the stack by one element.",
        "Transition lai creates a new arc with the topmost node on the stack as the head and the second-topmost node as the dependent, and removes the latter from the stack.",
        "Transition rai is symmetric with respect to lai.",
        "Transitions lai and rai have degree one, as already explained.",
        "When restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of Nivre (2004).",
        "Transition la2 and transition ra2 are very similar to lai and rai, respectively, but with the difference that they create a new arc between the topmost node in the stack and a node which is two positions below the topmost node.",
        "Hence, these transitions have degree two, and are the key components in parsing of non-projective dependencies.",
        "Language",
        "Deg.",
        "2",
        "Deg.",
        "3",
        "Deg.",
        "4",
        "Arabic",
        "180",
        "21",
        "7",
        "Bulgarian",
        "961",
        "41",
        "10",
        "Czech",
        "27181",
        "1668",
        "85",
        "Danish",
        "876",
        "136",
        "53",
        "Dutch",
        "9072",
        "2119",
        "171",
        "German",
        "15827",
        "2274",
        "466",
        "Japanese",
        "1484",
        "143",
        "9",
        "Portuguese",
        "3104",
        "424",
        "37",
        "Slovene",
        "601",
        "48",
        "13",
        "Spanish",
        "66",
        "7",
        "0",
        "Swedish",
        "1566",
        "226",
        "79",
        "Turkish",
        "579",
        "185",
        "8",
        "We turn next to describe the equivalence between our system and the system in Attardi (2006).",
        "The transition-based parser presented by Attardi pushes back into the buffer elements that are in the top position of the stack.",
        "However, a careful analysis shows that only the first position in the buffer can be affected by this operation, in the sense that elements that are pushed back from the stack are never found in buffer positions other than the first.",
        "This means that we can consider the first element of the buffer as an additional stack element, always sitting on the top of the topmost stack symbol.",
        "More formally, we can define a function mc : C – C that maps configurations in the original algorithm to those in our variant as follows:",
        "By applying this mapping to the source and target configuration of each transition in the original system, it is easy to check that c1 h c2 in that parser if and only if mc(c1) h mc(c2) in our variant.",
        "We extend this and define an isomorphism between computations in both systems, such that a computation c0, .",
        ".",
        ".",
        ", cm in the original parser is mapped to a computation mc(c0), .",
        ".",
        ".",
        ", mc(cm) in the variant, with both generating the same dependency graph A.",
        "This proves that our notational variant is in fact equivalent to Attardi's parser.",
        "A relevant property of the set of dependency structures that can be processed by Attardi's parser, even when restricted to transitions of degree two, is that the number of discontinuities present in each of their subtrees, defined as the gap degree by Bod-irsky et al.",
        "(2005), is not bounded.",
        "For example, the dependency graph in Figure 1 has gap degree n – 1 , and it can be parsed by the algorithm for any arbitrary n > 1 by applying 2n shb transitions to push all the nodes into the stack, followed by (2n – 2) ra2 transitions to create the crossing arcs, and finally one rai transition to create the dependency 1 – 2.",
        "As mentioned in §1, the computational complexity of the dynamic programming algorithm that will be described in later sections does not depend on the gap degree, contrary to the non-projective dependency chart parsers presented by Gomez-Rodriguez et al.",
        "(2009) and by Kuhlmann and Satta (2009), whose running time is exponential in the maximum gap degree allowed by the grammar."
      ]
    },
    {
      "heading": "3. A Generative Probabilistic Model",
      "text": [
        "In this section we introduce a generative probabilistic model based on the transition system of §2.2.",
        "In formal language theory, there is a standard way of giving a probabilistic interpretation to a non-deterministic parser whose computations are based on sequences of elementary operations such as transitions.",
        "The idea is to define conditional probability distributions over instances of the transition functions, and to 'combine' these probabilities to assign probabilities to computations and strings.",
        "One difficulty we have to face with when dealing with transition systems is that the notion of computation, defined in §2.1, depends on the input string, because of the buffer component appearing in each configuration.",
        "This is a pitfall to generative modeling, where we are interested in a system whose computations lead to the generation of any string.",
        "To overcome this problem, we observe that each computation, defined as a sequence of stacks and buffers (the configurations) can equivalently be expressed as a sequence of stacks and transitions.",
        "More precisely, consider a computation 7 = co,..., cm, m > 1.",
        "Let be the stack associated with cj, for each i with 0 < i < m. Let also Ca be the set of all stacks associated with configurations in C. We can make explicit the transitions that have been used in the computation by rewriting 7 in the form co htl C1 • • • cm_1 htm cm.",
        "In this way, 7 generates a string that is composed by all symbols that are pushed into the stack by transitions shb, in the left to right order.",
        "We can now associate a probability to (our representation of) sequence 7 by setting",
        "To assign probabilities to complete computations we should further multiply p(y) by factors ps(a0) and Pe(cm), where ps and pe are start and end probability distributions, respectively, both defined over Ca.",
        "Note however that, as defined in §2.2, all initial configurations are associated with stack [<:] and all final configurations are associated with stack [<:, 0], thus ps and pe are deterministic.",
        "Note that the Markov chain represented in Eq.",
        "1 is homogeneous, i.e., the probabilities of the transition operations do not depend on the time step.",
        "As a second step we observe that, according to the definition of transition system, each t G T has an infinite domain.",
        "A commonly adopted solution is to introduce a special function, called history function and denoted by H, defined over the set Ca and taking values over some finite set.",
        "For each t G T and a, a' G Ca, we then impose the condition whenever H (a) = H (a').",
        "Since H is finitely valued, and since T is a finite set, the above condition guarantees that there will only be a finite number of parameters p(t \\ a) in our model.",
        "So far we have presented a general discussion of how to turn a transition-based parser into a generative probabilistic model, and have avoided further specification of the history function.",
        "We now turn our attention to the non-projective transition system of §2.2.",
        "To actually transform that system into a parametrized probabilistic model, and to develop an associated efficient inference procedure as well, we need to balance between the amount of information we put into the history function and the computational complexity which is required for inference.",
        "We start the discussion with a naive model using a history function defined by a fixed size window over the topmost portion of the stack.",
        "More precisely, each transition is conditioned on the lexical form of the three symbols at the top of the stack a, indicated as 63,62,61^ G E below, with b1 referring to the topmost symbol.",
        "The parameters of the model are defined as follows.",
        "p(la2 \\ 63,62,61) = 0^ , p(ra2 \\ 63,62,61) = c^bi .",
        "The parameters above are subject to the following normalization conditions, for every choice of 63,62,61 G E:",
        "%,b2,bi + Z_> %,b2,bi = .",
        "This naive model presents two practical problems.",
        "The first problem relates to the efficiency of an inference algorithm, which has a quite high computational complexity, as it will be discussed in §5.",
        "A second problem arises in the probabilistic setting.",
        "Using this model would require estimating many parameters which are based on trigrams.",
        "This leads to higher sample complexity to avoid sparse counts: we would need more samples to accurately estimate the model.",
        "We therefore consider a more elaborated model, which tackles both of the above problems.",
        "Again, let 63, 62, 61 G E indicate the lexical form of the three symbols at the top of the stack.",
        "We define the distributions p(t \\ a) as follows:",
        "The parameters above are subject to the following normalization conditions, for every b3, b2, bi G S :",
        "minimum stack length in",
        "buffer'\\gize .",
        "''stack size",
        "Intuitively, parameter denotes the probability that we perform a reduce transition instead of a shift transition, given that we have seen lexical form b at the top of the stack.",
        "Similarly, parameter or^^bi denotes the probability that we perform a reduce transition of degree 2 (see §2.2) instead of a reduce transition of degree 1, given that we have seen lexical forms bi and b2 at the top of the stack.",
        "We observe that the above model has a number of parameters \\S\\ +4 • \\S\\ + 2 • \\S\\ (not all independent).",
        "This should be contrasted with the naive model, that has a number of parameters 4 • \\S\\ + \\S\\."
      ]
    },
    {
      "heading": "4. Tabular parsing",
      "text": [
        "We present here a dynamic programming algorithm for simulating the computations of the system from §2-3.",
        "Given an input string w, our algorithm produces a compact representation of the set r (w), defined as the set of all possible computations of the model when processing w. In combination with the appropriate semirings, this method can provide for instance the highest probability computation in r(w), or else the probability of w, defined as the sum of all probabilities of computations in r(w).",
        "We follow a standard approach in the literature on dynamic programming simulation of stack-based automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989).",
        "More recently, this approach has also been applied by Huang and Sagae (2010) and by",
        "Kuhlmann et al.",
        "(2011) to the simulation of projective transition-based parsers.",
        "The basic idea in this approach is to decompose computations of the parser into smaller parts, group them into equivalence classes and recombine to obtain larger parts of computations.",
        "Let w = ao • • • ara_i, Vw and S(np) be defined as in §2.",
        "We use a structure called item, defined as where 0 < i < j < n and hi, h2, h3 G Vw must satisfy hi < i and i < h2 < h3 < j.",
        "The intended interpretation of an item can be stated as follows; see also Figure 2.",
        "• For each i with 1 < i < m, the stack associated with configuration has the list a at the bottom and satisfies \\aj\\ > \\a\\ + 2.",
        "Some comments on the above conditions are in order here.",
        "Let ti, • • • ,tm be the sequence of transitions in T(np) associated with computation 7.",
        "Then we have ti = shai, since \\ai\\ > \\a\\ + 2.",
        "Thus we conclude that ai = a + 2.",
        "The most important consequence of the definition of item is that each transition ti with 2 < i < m does not depend on the content of the a portion of the stack ai.",
        "To see this, consider transition ci_i htici.",
        "If ti = shai, the content of a is irrelevant at",
        "• There exists a computation 7 of S(np) on w having the form c0,..., cm, m > 1, with c0 = (a\\hi,ßi,A) and Cm = (a\\h,2\\h,3, ßj,Ä) for some stack a and some arc sets A and A';",
        "p(shb \\ bi)",
        "= %",
        "Vb G S ,",
        "p(lai \\ b2,bi)",
        "= obi •",
        "olai",
        "°b2,bi '",
        "p(rai \\ b2,bi)",
        "= obi •",
        "orai",
        "b2,bi '",
        "p(la2 \\ b3,b2,bi)",
        "= obi •",
        "obd2b • oba2b",
        "b2,bi b3,b2,",
        "p(ra2 \\ b3,b2,bi)",
        "= obi •",
        "Ord2 ora2",
        "ob2 bi • ob3 b2",
        "this step, since in our model shai is conditioned only on the topmost stack symbol of ai_i, and we have ai_i > a + 2.",
        "Consider now the case of ti = la2.",
        "From \\ai\\ > \\a\\ +2 we have that \\ai_i\\ > \\a\\ + 3.",
        "Again, the content of a is irrelevant at this step, since in our model la2 is conditioned only on the three topmost stack symbols of ai_i.",
        "A similar argument applies to the cases of ti G {ra2, la1; ra1}.",
        "From the above, we conclude that if we apply the transitions ti , .",
        ".",
        ".",
        ", tm to stacks of the form a hi , the resulting computations have all identical probabilities, independently of the choice of a.",
        "Each computation satisfying the two conditions above will be called an I-computation associated with item [hi,i,h2h3,j].",
        "Notice that an I-computation has the overall effect of replacing node hi sitting above a stack a with nodes h2 and h3.",
        "This is the key property in the development of our algorithm below.",
        "We specify our dynamic programming algorithm as a deduction system (Shieber et al., 1995).",
        "The deduction system starts with axiom [<:, 0, <:0,1], corresponding to an initial stack [<:] and to the shift of a0 = $ from the buffer into the stack.",
        "The set r(w) is non-empty if and only if item [<:, 0, <:0, n] can be derived using the inference rules specified below.",
        "Each inference rule is annotated with the type of transition it simulates, along with the arc constructed by the transition itself, if any.",
        "[hi,i,h2h3,k] [h3,k,h4h5,j] .",
        ", .",
        "The above deduction system infers items in a bottom-up fashion.",
        "This means that longer computations over substrings of w are built by combining shorter ones.",
        "In particular, the inference rule shajasserts the existence of I-computations consisting of a single shaj transition.",
        "Such computations are represented by the consequent item [h3, j, h3j, j + 1], indicating that the index of the shifted word aj is added to the stack by pushing it on top of h3.",
        "The remaining four rules implement the reduce transitions of the model.",
        "We have already observed in §2.2 that all available reduce transitions shorten the size of the stack by one unit.",
        "This allows us to combine pairs of I-computations with a reduce transition, resulting in a computation that is again an I-computation.",
        "More precisely, if we concatenate an I-computation asserted by an item [hi, i, h2h3, k] with an I-computation asserted by an item [h3,k,h4h5, j], we obtain a computation that has the overall effect of increasing the size of the stack by 2, replacing the topmost stack element hi with stack elements h2, h4 and h5.",
        "If we now apply any of the reduce transitions from the inventory of the model, we will remove one of these three nodes from the stack, and the overall result will be again an I-computation, which can then be asserted by a certain item.",
        "For example, if we apply the reduce transition la1, the consequent item is [ha, i, h2h5, j], since an lai transition removes the second topmost element from the stack (h4).",
        "The other reduce transitions remove a different element, and thus their rules produce different consequent items.",
        "The above argument shows the soundness of the deduction system, i.e., an item I = [hl;i,h2h3, j] is only generated if there exists an I-computation 7 = co,..., cm with co = (a\\hi, ßi, A) and cm = (a\\h2\\h3, ßj, A').",
        "To prove completeness, we must show the converse result, i.e., that the existence of an I-computation 7 implies that item I is inferred.",
        "We first do this under the assumption that the inference rule for the shift transitions do not have an antecedent, i.e., items [hi; j, hij, j + 1] are considered as axioms.",
        "We proceed by using strong induction on the length m of the computation 7.",
        "For m = 1, 7 consists of a single transition shaj, and the corresponding item I = [hi; j, hi j, j + 1] is constructed as an axiom.",
        "For m > 1, let 7 be as specified above.",
        "The transition that produced cm must have been a reduce transition, otherwise 7 would not be an I-computation.",
        "Let ck be the rightmost configuration in c0,..., cm-i whose stack size is a + 2.",
        "Then it can be shown that the computations 71 = co,..., cfc and 72 = cfc,..., cm_i are again I-computations.",
        "Since 7i and 72 have strictly fewer transitions than 7, by the induction hypothesis, the system constructs items [hi, i, h2h3, k] and [h3,k,h4h5, j], where h2 and h3 are the stack elements at the top of ck.",
        "Applying to these items the inference rule corresponding to the reduce transition at hand, we can construct item I.",
        "When the inference rule for the shift transition has an antecedent [hi, i, h2h3, j], as indicated above, we have the overall effect that I-computations consisting of a single transition shifting aj on the top of h3 are simulated only in case there exists a computation starting with configuration ([<:], ß0) and reaching a configuration of the form (a\\h2\\h3, ßj ).",
        "This acts as a filter on the search space of the algorithm, but does not invalidate the completeness property.",
        "However, in this case the proof is considerably more involved, and we do not report it here.",
        "An important property of the deduction system above, which will be used in the next section, is that the system is unambiguous, that is, each I-computation is constructed by the system in a unique way.",
        "This can be seen by observing that, in the sketch of the completeness proof reported above, there always is an unique choice of ck that decomposes I-computation 7 into I-computations 7i and 72.",
        "In fact, if we choose a configuration ck/ other than ck with stack size \\a\\ + 2, the computation 72 = cfc/,..., cm_i will contain ck as an intermediate configuration, which violates the definition of I-computation because of an intervening stack having size not larger than the size of the stack associated with the initial configuration.",
        "As a final remark, we observe that we can keep track of all inference rules that have been applied in the computation of each item by the above algorithm, by encoding each application of a rule as a reference to the pair of items that were taken as antecedent in the inference.",
        "In this way, we obtain a parse forest structure that can be viewed as a hypergraph or as a non-recursive context-free grammar, similar to the case of parsing based on context-free grammars.",
        "See for instance Klein and Manning (2001) or Nederhof (2003).",
        "Such a parse forest encodes all valid computations in r(w), as desired.",
        "The algorithm runs in O(n) time.",
        "Using methods similar to those specified in Eisner and Satta (1999), we can reduce the running time to O(n).",
        "However, we do not further pursue this idea here, and proceed with the discussion of exact inference, found in the next section."
      ]
    },
    {
      "heading": "5. Inference",
      "text": [
        "We turn next to specify exact inference with our model, for computing feature expectations.",
        "Such inference enables, for example, the derivation of an expectation-maximization algorithm for unsupervised parsing.",
        "Here, a feature is a function over computations, providing the count of a pattern related to a parameter.",
        "We denote by bi (7), for instance, the number of occurrences of transition la2 within 7 with topmost stack symbols having word forms b3, b2, bi G S, with bi associated with the topmost stack symbol.",
        "Feature expectations are computed by using an inside-outside algorithm for the items in the tabular algorithm.",
        "More specifically, given a string w, we associate each item [hi, i, h2h3, j] defined as in § 4 with two quantities:",
        "I([hi,i,h2h3, j]) and O([hi,i,h2h3, j]) are called the inside and the outside probabilities, respectively, of item [hi , i, h2h3, j].",
        "The tabular algorithm of §4 can be used to compute the inside probabilities.",
        "Using the gradient transformation (Eisner et al., 2005), a technique for deriving outside probabilities from a set of inference rules, we can also compute O([hi,i,h2h3, j]).",
        "The use of the gradient transformation is valid in our case because the tabular algorithm is unambiguous (see §4).",
        "Using the inside and outside probabilities, we can now efficiently compute feature expectations for our",
        "Figure 3: Decomposition of the feature expectation £p(7|w) [f,^ bl (7)] into a finite summation.",
        "Quantity p(w) above is the sum over all probabilities of computations in r(w).",
        "model.",
        "Figure 3 shows how to express the expectation of feature bi (7) by means of a finite summation.",
        "Using Eq.",
        "5 and 6 and the relation",
        "• ^ I([hi,i,h2h3,k]) • I([h3,k,h4h5, j]) .",
        "Very similar expressions can be derived for the expectations for features /ba)b2 ,bi (^ ./b^ (), and /br2aibi(7).",
        "As for feature /J*(7), b G S, the above approach leads to",
        "b • o([h,i,hi,i + 1]).",
        "ord ord2 ol22",
        "As mentioned above, these expectations can be used, for example, to derive an EM algorithm for our model.",
        "The EM algorithm in our case is not completely straightforward because of the way we parametrize the model.",
        "We give now the re-estimation steps for such an EM algorithm.",
        "We assume that all expectations below are taken with respect to a set of parameters 6 from iteration s – 1 of the algorithm, and we are required to update these 6.",
        "To simplify notation, let us assume that there is only one string w in the training corpus.",
        "For each bi G S, we define:",
        "Zb2,bi =E EP(7|w) /b2b2,bl()+ /bb2,bi() .",
        "We then have, for every b G S :",
        "Furthermore, we have:",
        "The rest of the parameter updates can easily be derived using the above updates because of the sum-to-1 constraints in Eq.",
        "2 – 4."
      ]
    },
    {
      "heading": "6. Discussion",
      "text": [
        "We note that our model inherits spurious ambiguity from Attardi's model.",
        "More specifically, we can have different derivations, corresponding to different system computations, that result in identical dependency graphs and strings.",
        "While running our tabular algorithm with the Viterbi semiring efficiently computes the highest probability computation in r(w), spurious ambiguity means that finding the highest probability dependency tree is NP-hard.",
        "This latter result can be shown using proof techniques similar to those developed by Sima'an (1996).",
        "We leave it for future work how to eliminate spurious ambiguity from the model.",
        "While in the previous sections we have described a tabular method for the transition system of Attardi (2006) restricted to transitions of degree up to two, it is possible to generalize the model to include higher-degree transitions.",
        "In the general formulation of Attardi parser, transitions of degree d create links involving nodes located d positions beneath the topmost position in the stack:",
        "with co = (a|si| ••• |sD-i,ßi,A) and cm = (a|ei| • • • |eD,ßj, A').",
        "The deduction steps corresponding to reduce transitions in this general system have the general form",
        "To define a transition system that supports transitions up to degree D, we use a set of items of the form [si • • • sD-i, i, ei • • • eD, j], corresponding (in the sense of §4) to computations of the form ^•••jcm, m > 1, where the values of p and c differ for each transition: to obtain the inference rule corresponding to a \\ad transition, we make p = D + 1 and c = D + 1 – d; and to obtain the rule for a rad transition, we make p = D + 1 – d and c = D + 1.",
        "Note that the parser runs in time O(n3D+), where D stands for the maximum transition degree, so each unit increase in the transition degree adds a cubic factor to the parser's polynomial time complexity.",
        "This is in contrast to a previous tabular formulation of the Attardi parser by Gomez-Rodrîguez et al.",
        "(2011), which ran in exponential time.",
        "The model for the transition system we give in this paper is generative.",
        "It is not hard to naturally extend this model to the discriminative setting.",
        "In this case, we would condition the model on the input string to get a conditional distribution over derivations.",
        "It is perhaps more natural in this setting to use arbitrary weights for the parameter values, since the computation of a normalization constant (the probability of a string) is required in any case.",
        "Arbitrary weights in the generative setting could be more problematic, because it would require computing a normalization constant corresponding to a sum over all strings and derivations."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "We presented in this paper a generative probabilistic model for non-projective parsing, together with the description of an efficient tabular algorithm for parsing and doing statistical inference with the model."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors thank Marco Kuhlmann for helpful comments on an early draft of the paper.",
        "The authors also thank Giuseppe Attardi for the help received to extract the parsing statistics.",
        "The second author has been partially supported by Ministerio de Ciencia e Innovacion and FEDER (TIN2010-18552-C03-02)."
      ]
    }
  ]
}
