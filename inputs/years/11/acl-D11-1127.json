{
  "info": {
    "authors": [
      "Gonzalo Iglesias",
      "Cyril Allauzen",
      "William Byrne",
      "Adrià Gispert",
      "de",
      "Michael D. Riley"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1127",
    "title": "Hierarchical Phrase-based Translation Representations",
    "url": "https://aclweb.org/anthology/D11-1127",
    "year": 2011
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Hierarchical Phrase-Based Translation Representations",
        "Gonzalo Iglesias* Cyril Allauzen* William Byrne* Adrià de Gispert* Michael Riley*",
        "This paper compares several translation representations for a synchronous context-free grammar parse including CFGs/hypergraphs, finite-state automata (FSA), and pushdown automata (PDA).",
        "The representation choice is shown to determine the form and complexity of target LM intersection and shortest-path algorithms that follow.",
        "Intersection, shortest path, FSA expansion and RTN replacement algorithms are presented for PDAs.",
        "Chinese-to-English translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs.",
        "For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Hierarchical phrase-based translation, using a synchronous context-free translation grammar (SCFG) together with an n-gram target language model (LM), is a popular approach in machine translation (Chiang, 2007).",
        "Given a SCFG G and an n-gram language model M, this paper focuses on how to decode with them, i.e. how to apply them to the source text to generate a target translation.",
        "Decoding has three basic steps, which we first describe in terms of the formal languages and relations involved, with data representations and algorithms to follow.",
        "1.",
        "Translating the source sentence s with G to give target translations: T = {s} o Q. a (weighted) context-free language resulting from the composition of a finite language and the algebraic relation Q for SCFG G.",
        "2.",
        "Applying the language model to these target translations: C = TnM, a (weighted) context-free language resulting from the intersection of a context-free language and the regular language M for M.",
        "3.",
        "Searching for the translation and language model combination with the highest-probablity path: £ = argmaxie£_L",
        "Of course, decoding requires explicit data representations and algorithms for combining and searching them.",
        "In common to the approaches we will consider here, s is applied to G by using the CYK algorithm in Step 1 and M is represented by a finite automaton in Step 2.",
        "The choice of the representation of T in many ways determines the remaining decoder representations and algorithms needed.",
        "Since {s} is a finite language and we assume throughout that G does not allow unbounded insertions.",
        "T and are, in fact, regular languages.",
        "As such.",
        "and have finite automaton representations T/ and In this case, weighted finite-state intersection and single-source shortest path algorithms (using negative log probabilities) can be used to solve Steps 2 and 3 (Mohri, 2009).",
        "This is the approach taken in (Iglesias et al., 2009a; de Gispert et al.. 2010).",
        "Instead T and C can be represented by hy-pergraphs and (or very similarly context-free rules, and-or trees, or deductive systems).",
        "In this case, hypergraph intersection with a finite automaton and hypergraph shortest path algorithms can be used to solve Steps 2 and 3 (Huang, 2008).",
        "This is the approach taken by Chiang (2007).",
        "In this paper, we will consider another representation for context-free languages T and C as well, pushdown automata (PDA) Tp and familiar from formal language theory (Aho and Ullman, 1972).",
        "We will describe PDA intersection with a finite automaton and PDA shortest-path algorithms in Section 2 that can be used to solve Steps 2 and 3.",
        "It cannot be overemphasized that the CFG, hypergraph and PDA representations of T are used for their compactness rather than for expressing non-regular languages.",
        "As presented so far, the search performed in Step 3 is admissible (or exact) - the true shortest path is found.",
        "However, the search space in MT can be quite large.",
        "Many systems employ aggressive pruning during the shortest-path computation with little theoretical or empirical guarantees of correctness.",
        "Further, such pruning can greatly complicate any complexity analysis of the underlying representations and algorithms.",
        "In this paper, we will exclude any inadmissible pruning in the shortest-path algorithm itself.",
        "This allows us in Section 3 to compare the computational complexity of using these different representations.",
        "We show that the PDA representation is particularly suited for decoding with large SCFGs and compact LMs.",
        "We present Chinese-English translation results under the FSA and PDA translation representations.",
        "We describe a two-pass translation strategy which we have developed to allow use of the PDA representation in large-scale translation.",
        "In the first pass, translation is done using a lattice-generating version of the shortest path algorithm.",
        "The full translation grammar is used but with a compact, entropy-pruned version (Stolcke, 1998) of the full language model.",
        "This first-step uses admissible pruning and lattice generation under the compact language model.",
        "In the second pass, the original, unpruned LM is simply applied to the lattices produced in the first pass.",
        "We find that entropy-pruning and first-pass translation can be done so as to introduce very few search errors in the overall process; we can identify search errors in this experiment by comparison to exact translation under the full translation grammar and language model using the FSA representation.",
        "We then investigate a translation grammar which is large enough that exact translation under the FSA representation is not possible.",
        "We find that translation is possible using the two-pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar.",
        "There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation.",
        "The challenge is to find algorithms that can be made to work with large translation grammars and large language models.",
        "Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits left-to-right nature of the language models.",
        "Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison.",
        "Using a relatively simple phrase-based translation grammar, Iglesias et al.",
        "(2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suffered significant search errors.",
        "For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al.",
        "(2009a) and de Gispert et al.",
        "(2010).",
        "Relaxation techniques have also recently been shown to finding exact solutions in parsing (Koo et al., 2010) and in SMT with tree-to-string translation grammars and trigram language models (Rush and Collins, 2011), much smaller models compared to the work presented in this paper.",
        "Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al., 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel.",
        "This is an approach that is widely-used in automatic speech recognition (Ljolje et al.. 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with F S As and PDAs."
      ]
    },
    {
      "heading": "2. Pushdown Automata",
      "text": [
        "In this section, we formally define pushdown automata and give intersection, shortest-path and related algorithms that will be needed later.",
        "Informally, pushdown automata are finite automata that have been augmented with a stack.",
        "Typ-",
        "Figure 1: PDA Examples: (a) Non-regular PDA accepting {anbn\\n G N}.",
        "(b) Regular (but not bounded-stack) PDA accepting a*b*.",
        "(c) Bounded-stack PDA accepting a*b* and (d) its expansion as an FSA.",
        "ically this is done by adding a stack alphabet and labeling each transition with a stack operation (a stack symbol to be pushed onto, popped or read from the stack) in additon to the usual input label (Aho and Ullman, 1972; Berstel, 1979) and weight (Kuich and Salomaa, 1986; Petre and Salomaa, 2009).",
        "Our equivalent representation allows a transition to be labeled by a stack operation or a regular input symbol but not both.",
        "Stack operations are represented by pairs of open and close parentheses (pushing a symbol on and popping it from the stack).",
        "The advantage of this representation is that is identical to the finite automaton representation except that certain symbols (the parentheses) have special semantics.",
        "As such, several finite-state algorithms either immediately generalize to this PDA representation or do so with minimal changes.",
        "The algorithms described in this section have been implemented in the PDT extension (Allauzen and Riley, 2011) of the OpenFst library (Allauzen et al., 2007).",
        "A (restricted) Dyck language consist of \"well-formed\" or \"balanced\" strings over a finite number of pairs of parentheses.",
        "Thus the string ([()()]{}[])() is m the Dyck language over 3 pairs of parentheses.",
        "More formally, let A and A be two finite alphabets such that there exists a bijection / from A to A.",
        "Intuitively, / maps an open parenthesis to its corresponding close parenthesis.",
        "Let a denote /(a) if a G A and f~(a) if a G A.",
        "The Dyck language Da over the alphabet A = A U A is then the language defined by the following context-free grammar: S – > e, S – > SS and S – > aSä for all a G A.",
        "We define the mapping ca : A* – >• A* as follow.",
        "ca{x) is the string obtained by iteratively deleting from x all factors of the form aa with a G A.",
        "Observe that Da = c~A (e).",
        "Let A and B be two finite alphabets such that B ç A, we define the mapping vb '■ A* – >• B* by rB{x\\ ...xn)=yi...yn with ^ = Xi if Xi G B and Vi = e otherwise.",
        "A weighted pushdown automaton (PDA) T over the tropical semiring (R U {oo}, min, +, oo, 0) is a 9-tuple (E, II, LÎ, Q, E, L_F, p) where E is the finite input alphabet, IT and II are the finite open and close parenthesis alphabets, Q is a finite set of states.",
        "I eQ the initial state, F ç Q the set of final states.",
        "£CQx(EuflU {e}) x (R U {oo}) x Q a finite set of transitions, and p : F – >• RU{oo}the final weight function.",
        "Let e = (p[e\\, i[e], w[e], n[e\\) denote a transition in E.",
        "A path 7T is a sequence of transitions tt = e\\... ensuch that n[ei] =p[ei+i] for 1 < i < n. We then define p[tt] =p[ei], n[7r] =n[en], i[7r] =i[ei] •••i[en], and w[tt] =w[e\\] + ... + w[en].",
        "A path 7T is accepting if p[ir] = I and n[7r] G F. A path 7T is balanced if rjj(i[7r]) G Du-A balanced path 7T accepts the string x G E* if it is a balanced accepting path such that r-z(i[jr]) = x.",
        "The weight associated by T to a string x G E* is T(x) = min^p^) w[tt] + p{n[n}) where P{x) denotes the set of balanced paths accepting x.",
        "A weighted language is recognizable by a weighted pushdown automaton iff it is context-free.",
        "We define the size of T as |T| = |Q| + |£|.",
        "A PDA T has a bounded stack if there exists K G N such that for any sub-path 7r of any balanced path inT: |cn(?",
        "\"fj(î[vr]))| < K. If Thas abounded stack then it represents a regular language.",
        "Figure 1 shows non-regular, regular and bounded-stack PDAs.",
        "A weightedfinite automaton (FSA) can be viewed as a PDA where the open and close parentheses alphabets are empty, see (Mohri, 2009) for a standalone definition.",
        "Given a bounded-stack PDA T, the expansion of T is the FSA T' equivalent to T defined as follows.",
        "A state in T' is a pair (q, z) where q is a state in T and 2 G II*.",
        "A transition a, w, q') in T results in a transition ((g, z),a', w, (q', z)) in T' only when: (a) a G S U {e}, = z and a' = a, (b) a<EH, z' = za and a' = e, or (c) a G n, z' is such that z = z'a and a' = e. The initial state of T' is /' = (I, e).",
        "A state (q, z) in T' is final if q is final in T and 2 = e (p'((q, e)) = p(<?)).",
        "The set of states of T\" is the set of pairs (g, z) that can be reached from an initial state by transitions defined as above.",
        "The condition that T has a bounded stack ensures that this set is finite (since it implies that for any (q, z), \\z\\ < K).",
        "The complexity of the algorithm is linear in 0{\\T'\\) = 0(elTl).",
        "Figure Id show the result of the algorithm when applied to the PDA of Figure lc.",
        "The class of weighted pushdown automata is closed under intersection with weighted finite automata (Bar-Hillel et al., 1964; Nederhof and Satta, 2003).",
        "Considering a pair T2) where one element is an FSA and the other element a PDA, then there exists a PDA Ti nT2, the intersection of T\\ and T2, such that for all x G X*: (T1nT2)(x) = T1(x)+T2(x).",
        "We assume in the following that T2 is an FSA.",
        "We also assume that T2 has no input-e transitions.",
        "When T2 has input-e transitions, an epsilon filter (Mohri.",
        "2009; Allauzen et al., 2011) generalized to handle parentheses can be used.",
        "A state in T = T\\C\\T2 is a pair (qi,q2) where q\\ is a state of T\\ and q2 a state of T2.",
        "The initial state is I=(Ii,I2).",
        "Given a transition e\\ = (qi,a, Wi,q'{) in T\\, transitions out of {q\\, q2) in T are obtained using the following rules.",
        "If a G X, then e\\ can be matched with a transition (q2,a,w2,q2) in T2 resulting a transition ((qi,q2),a,wi+w2,(q'1,q'2)) in T.",
        "If a = e, then e\\ is matched with staying in q2resulting in a transition ((qi, q2), e, W\\, {q^q^).",
        "Finally, if a G TI, e\\ is also matched with staying in q2, resulting in a transition {(qi, q2), a, Wi,(q[,q2)) in T.",
        "A state (qi,q2) in T is final when both q\\ and q2are final, and then p((qu q2)) = p\\{qi)+p2{q2).",
        "ShortestDistance(T) 1 for each q € Q and a € II do"
      ]
    },
    {
      "heading": "3. GETDlSTANCE(T, I)",
      "text": [
        "4 return d[f, I]",
        "GetDistance(T, s) 1 for each q € Q do"
      ]
    },
    {
      "heading": "7. Dequeue(S s )",
      "text": [
        "8 for each e <E E[q] do"
      ]
    },
    {
      "heading": "9. ifi[e] GSUjf) then",
      "text": []
    },
    {
      "heading": "13. elseifi [e] G II then",
      "text": [
        "14 if d[n[e], n[e]] is undefined then"
      ]
    },
    {
      "heading": "15. GetDistance(T, n[e\\)",
      "text": [
        "Figure 2: PDA shortest distance algorithm.",
        "We assume that F = {/} and p(/) = 0 to simplify the presentation.",
        "The complexity of the algorithm is in O ( | T\\ \\ \\ T2 \\ ).",
        "A shortest path in a PDA T is a balanced accepting path with minimal weight and the shortest distance in T is the weight of such a path.",
        "We show that when T has a bounded stack, shortest distance and shortest path can be computed in 0(\\T\\ log |T|) time (assuming T has no negative weights) and 0{\\T\\) space.",
        "Given a state s in T with at least one incoming open parenthesis transition, we denote by Cs the set of states that can be reached from s by a balanced path.",
        "If s has several incoming open parenthesis transitions, a naive implementation might lead to the states in Cs to be visited up to exponentially many times.",
        "The basic idea of the algorithm is to memo-ize the shortest distance from s to states in Cs.",
        "The pseudo-code is given in Figure 2.",
        "GetDi stance (T, s) starts a new instance of the shortest-distance algorithm from s using the queue Ss, initially containing s. While the queue is not empty, a state is dequeued and its outgoing transitions examined (line 5-9).",
        "Transitions labeled by non-parenthesis are treated as in Mohri (2009) (line 9-10).",
        "When the considered transition e is labeled by a close parenthesis, it is remembered that it balances all incoming open parentheses in s labeled by i[e] by adding e to B[s, i[e\\] (line 11-12).",
        "Finally, when e is labeled with an open parenthesis, if its destination has not already been visited, a new instance is started from n[e] (line 14-15).",
        "The destination states of all transitions balancing e are then relaxed (line 16-18).",
        "The space complexity of the algorithm is quadratic for two reasons.",
        "First, the number of non-infinity d[q,s] is \\Q\\.",
        "Second, the space required for storing B is at most in 0{\\E\\) since for each open parenthesis transition e, the size of £>[n[e],i[e]]\\ is 0(|£7|) in the worst case.",
        "This last observation also implies that the cumulated number of transitions examined at line 16 is in 0(N\\Q\\ \\E\\) in the worst case, where N denotes the maximal number of times a state is inserted in the queue for a given call of GetDistance.",
        "Assuming the cost of a queue operation is T(n) for a queue containing n elements, the worst-case time complexity of the algorithm can then be expressed as 0(N\\T\\ r(|T|)).",
        "When T contains no negative weights, using a shortest-first queue discipline leads to a time complexity in 0(\\T\\ log |T|).",
        "When all the Cs's are acyclic, using a topological order queue discipline leads to a 0(|T|) time complexity.",
        "In effect, we are solving a fc-sources shortest-path problem with k single-source solutions.",
        "A potentially better approach might be to solve the k-sources or A;-pairs problem directly (Hershberger et al., 2003).",
        "When T has been obtained by converting an RTN or an hypergraph into a PDA (Section 2.5), the polynomial dependency in |T| becomes a linear dependency both for the time and space complexities.",
        "Indeed, for each q in T, there exists a unique s such that d[q, s] is non-infinity.",
        "Moreover, for each close parenthesis transistion e, there exists a unique open parenthesis transition ê such that e G £>[n[e'], i[e'}}.",
        "When each component of the RTN is acyclic, the complexity of the algorithm is hence in 0(|T|) in time and space.",
        "The algorithm can be modified to compute the shortest path by keeping track of parent pointers.",
        "A recursive transition network (RTN) can be specified by (N, E, (Tu)uen, S) where N is an alphabet of nonterminals, E is the input alphabet, {Tv)v^n is a family of FSAs with input alphabet SUl, and S G N is the root nonterminal.",
        "A string x G E* is accepted by R if there exists an accepting path tt in Ts such that recursively replacing any transition with input label v G N by an accepting path in Tv leads to a path tt* with input x.",
        "The weight associated by R is the minimum over all such 7T* of w[7r*]+ps(n[7r*}).",
        "Given an RTN R, the replacement of R is the PDA T equivalent to R defined by the 9-tuple (E, n, ÎÎ, Q, E, I, F, a, p) with U = Q = [j^N Qv, I = Is,F = Fs,p = ps, and E = \\JveN \\JeeEv Eewhere Ee = {e} if i[e\\_ g N and Ee = {{'p [e], n [e], w [e], Iß ), (/, n [e], pß ( /), n [e] ) | / G Fß } with ß = i[e]EN otherwise.",
        "The complexity of the construction is in 0(|T|).",
        "If \\FV\\ = 1, then |T| = 0(£,eW \\TV\\) = 0{\\R\\).",
        "Creating a superfinal state for each Tv would lead to a T whose size is always linear in the size of R."
      ]
    },
    {
      "heading": "3. Hierarchical Phrase-Based Translation Representation",
      "text": [
        "In this section, we compare several different representations for the target translations T of the source sentence s by synchronous CFG G prior to language model M application.",
        "As discussed in the introduction, T is a context-free language.",
        "For example, suppose it corresponds to:",
        "S^abXdg, S^acXfg, and X – >bc.",
        "Figure 3 shows several alternative representations of T: Figure 3a shows the hypergraph representation of this grammar; there is a 1:1 correspondence between each production in the CFG and each hyperedge in the hypergraph.",
        "Figure 3b shows the RTN representation of this grammar with a 1:1 correspondence between each production in the CFG and each path in the RTN; this is the translation representation pro-",
        "Figure 3 : Alternative translation representations duced by the HiFST decoder (Iglesias et al., 2009a; de Gispert et al., 2010).",
        "Figure 3c shows the pushdown automaton representation generated from the RTN with the replacement algorithm of Section 2.5.",
        "Since s is a finite language and G does not allow unbounded insertion, Tp has a bounded stack and T is, in fact, a regular language.",
        "Figure 3d shows the finite-state automaton representation of T generated by the PDA using the expansion algorithm of Section 2.2.",
        "The HiFST decoder converts its RTN translation representation immediately into the finite-state representation using an algorithm equivalent to converting the RTN into a PDA followed by PDA expansion.",
        "As shown in Figure 4, an advantage of the RTN.",
        "PDA, and FSA representations is that they can benefit from FSA epsilon removal, determinization and minimization algorithms applied to their components (for RTNs and PDAs) or their entirety (for FSAs).",
        "For the complexity discussion below, however, we disregard these optimizations.",
        "Instead we focus on the complexity of each MT step described in the introduction:",
        "1.",
        "SCFG Translation: Assuming that the parsing of the input is performed by a CYK parse, then the CFG, hypergraph, RTN and PDA représentations can be generated in 0(|s||G|) time and space (Aho and Ullman, 1972).",
        "The FSA representation can require an additional 0(e'sl lGl) time and space since the PDA expansion can be exponential.",
        "2.",
        "Intersection: The intersection of a CFG T^ with a finite automaton M can be performed by the classical Bar-Hillel algorithm (Bar-Hillel et al., 1964) with time and space complexity 0(\\Th\\\\M\\) The PDA intersection algorithm from Section 2.3 has time and space complexity 0(\\Tp\\\\M\\).",
        "Finally, the FSA intersection algorithm has time and space complexity 0{\\Tf\\\\M\\) (Mohri, 2009).",
        "3.",
        "Shortest Path: The shortest path algorithm on the hypergraph, RTN, and FSA representations requires linear time and space (given the underlying acyclicity) (Huang, 2008; Mohri, 2009).",
        "As presented in Section 2.4, the PDA representation can require time cubic and space quadratic in \\M\\.",
        "Table 1 summarizes the complexity results.",
        "Note the PDA representation is equivalent in time and superior in space to the CFG/hypergraph representation, in general, and it can be superior in both space",
        "'The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity 0(|T)j||M|); the modifications were introduced presumably to benefit the subsequent pruning method employed (but see Huang et al.",
        "(2005)).",
        "Representation Time Complexity Space Complexity",
        "Table 1 : Complexity using various target translation representations.",
        "and time to the FSA representation depending on the relative SCFG and LM sizes.",
        "The FSA representation favors smaller target translation sets and larger language models.",
        "Should a better complexity PDA shortest path algorithm be found, this conclusion could change.",
        "In practice, the PDA and FSA representations benefit hugely from the optimizations mentioned above, these optimizations improve the time and space usage by one order of magnitude."
      ]
    },
    {
      "heading": "4. Experimental Framework",
      "text": [
        "We use two hierarchical phrase-based SMT decoders.",
        "The first one is a lattice-based decoder implemented with weighted finite-state transducers (de Gispert et al., 2010) and described in Section 3.",
        "The second decoder is a modified version using PDAs as described in Section 2.",
        "In order to distinguish both decoders we call them HiFST and HiPDT, respectively.",
        "The principal difference between the two decoders is where the finite-state expansion step is done.",
        "In HiFST, the RTN representation is immediately expanded to an FSA.",
        "In HiPDT, this expansion is delayed as late as possible - in the output of the shortest path algorithm.",
        "Another possible configuration is to expand after the LM intersection step but before the shortest path algorithm; in practice this is quite similar to HiFST.",
        "In the following sections we report experiments in Chinese-to-English translation.",
        "For translation model training, we use a subset of the GALE 2008 evaluation parallel text; this is 2.1M sentences and approximately 45M words per language.",
        "We report translation results on a development set tune-nw (1,755 sentences) and a test set test-nw (1,671 sentences).",
        "These contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT06.",
        "In tuning the sys-",
        "Table 2: Number of ngrams (in millions) in the 1st pass 4-gram language models obtained with different 0 values (top row).",
        "tems, standard MERT (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set.",
        "The parallel corpus is aligned using MTTK (Deng and Byrne, 2008) in both source-to-target and target-to-source directions.",
        "We then follow standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009b) to extract hierarchical phrases from the union of the directional word alignments.",
        "We call a translation grammar the set of rules extracted from this process.",
        "We extract two translation grammars:",
        "• A restricted grammar where we apply the following additional constraint: rules are only considered if they have a forward translation probability p > 0.01.",
        "We call this G\\.",
        "As will be discussed later, the interest of this grammar is that decoding under it can be exact, that is.",
        "without any pruning in search.",
        "• An unrestricted one without the previous constraint.",
        "We call this G2.",
        "This is a superset of the previous grammar, and exact search under it is not feasible for HiFST: pruning is required in search.",
        "The initial English language model is a Kneser-Ney 4-gram estimated over the target side of the parallel text and the AFP and Xinhua portions of monolingual data from the English Gigaword Fourth Edition (LDC2009T13).",
        "This is a total of 1.3B words.",
        "We will call this language model Mi.",
        "For large language model rescoring we also use the LM M2 obtained by interpolating Mi with a zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram estimated using 6.6B words of English newswire text.",
        "We next describe how we build translation systems using entropy-pruned language models.",
        "1.",
        "We build a baseline HiFST system that uses Mi and a hierarchical grammar G, parameters being optimized with MERT under BLEU.",
        "0",
        "7.5 x l(Ty",
        "7.5 x 1(TS",
        "7.5 x 1(T",
        "207.5",
        "20.2",
        "4.1",
        "0.9",
        "2.",
        "We then use entropy-based pruning of the language model (Stolcke, 1998) under a relative perplexity threshold of 9 to reduce the size of Mi.",
        "We will call the resulting language model as Mf.",
        "Table 2 shows the number of n-grams (in millions) obtained for different 9 values.",
        "3.",
        "We translate with Mf using the same parameters obtained in MERT in step 1, except for the word penalty, tuned over the lattices under BLEU performance.",
        "This produces a translation lattice in the topmost cell that contains hypotheses with exact scores under the translation grammar and Mf.",
        "4.",
        "Translation lattices in the topmost cell are pruned with a likelihood-based beam width ß.",
        "5.",
        "We remove the Mf scores from the pruned translation lattices and reapply Mi, moving the word penalty back to the original value obtained in MERT.",
        "These operations can be carried out efficiently via standard FSA operations.",
        "6.",
        "Additionally, we can rescore the translation lattices obtained in steps 1 or 5 with the larger language model M2.",
        "Again, this can be done via standard FSA operations.",
        "Note that if ß = oo or if 9 = 0, the translation lattices obtained in step 1 should be identical to the ones of step 5.",
        "While the goal is to increase 9 to reduce the size of the language model used at Step 3, ß will have to increase accordingly so as to avoid pruning away desirable hypotheses in Step 4.",
        "If ß defines a sufficiently wide beam to contain the hypotheses which would be favoured by Mi, faster decoding with Mf would be possible without incurring search errors Mi.",
        "This is investigated next."
      ]
    },
    {
      "heading": "5. Entropy-Pruned LM in Rescoring",
      "text": [
        "In Table 3 we show translation performance under grammar G\\ for different values of 9.",
        "Performance is reported after first-pass decoding with Mf (see step 3 in Section 4), after rescoring with Mi (see step 5) and after rescoring with M2 (see step 6).",
        "The baseline (experiment number 1) uses 9 = 0 (that is.",
        "Mi) for decoding.",
        "Under translation grammar G\\, HiFST is able to generate an FSA with the entire space of possible candidate hypotheses.",
        "Therefore, any degradation in performance is only due to the Mf involved in decoding and the ß applied prior to rescoring.",
        "As shown in row number 2, for 9 < 1CT the system provides the same performance to the baseline when ß > 8, while decoding time is reduced by roughly 40%.",
        "This is because Mf is 10% of the size of the original language model Mi, as shown in Table 2.",
        "As Mf is further reduced by increasing 9 (see rows number 3 and 4), decoding time is also reduced.",
        "However, the beam width ß required in order to recover the good hypotheses in rescoring increases, reaching 12 for experiment 3 and 15 for experiment 4.",
        "Regarding rescoring with the larger M2 (step 6 in Section 4), the system is also able to match the baseline performance as long as ß is wide enough, given the particular Mf used in first-pass decoding.",
        "Interestingly, results show that a similar ß value is needed when rescoring either with Mi or M2.",
        "The usage of entropy-pruned language models increments speed at the risk of search errors.",
        "For instance, comparing the outputs of systems 1 and 2 with ß = 10 in Table 3 we find 45 different 1-best hypotheses, even though the BLEU score is identical.",
        "In other words, we have 45 cases in which system 2 is not able to recover the baseline output because the lst-pass likelihood beam ß is not wide enough.",
        "Similarly, system 3 fails in 101 cases (ß = 12) and system 4 fails in 95 cases.",
        "Interestingly, some of these sentences would require unpractically huge beams.",
        "This might be due to the Kneser-Ney smoothing, which interacts badly with entropy pruning (Chelba et al., 2010)."
      ]
    },
    {
      "heading": "6. Hiero with PDAs and FSAs",
      "text": [
        "In this section we contrast HiFST with HiPDT under the same translation grammar and entropy-pruned language models.",
        "Under the constrained grammar G\\ their performance is identical as both decoders can generate the entire search space which can then be rescored with Mi or M2 as shown in the previous section.",
        "Therefore, we now focus on the unconstrained grammar G2, where exact search is not feasible for HiFST.",
        "In order to evaluate this problem, we run both decoders over tune-nw, restricting memory usage to 10 gigabytes.",
        "If this limit is reached in decod-",
        "Table 3: Results (lowercase IBM BLEU scores) under G\\ with various Mf as obtained with several values of 0.",
        "Performance in subsequent rescoring with M\\ and M2 after likelihood-based pruning of the translation lattices for various ß is also reported.",
        "Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.",
        "Table 4: Percentage of success in producing the 1-best translation under C?2 with various Mf when applying a hard memory limitation of 10 GB, as measured over tune-nw (1755 sentences).",
        "If decoder fails, we report what step was being done when the limit was reached.",
        "HiFST could be expanding into an FSA or composing the FSA with Mf; HiPDT could be PDA composing with Mf or PDA expanding into an FSA.",
        "Table 5: HiPDT performance on grammar C?2 with 0 = 7.5 x 10 .",
        "Exact search with HiFST is not possible under these conditions: pruning during search would be required.",
        "ing, the process is killed.",
        "We report what internal decoding operation caused the system to crash.",
        "For HiFST, these include expansion into an FSA (Expand) and subsequent intersection with the language model (Compose).",
        "For HiPDT, these include PDA intersection with the language model (Compose) and subsequent expansion into an FSA (Expand), using algorithms described in Section 2.",
        "Table 4 shows the number of times each decoder succeeds in finding a hypothesis given the memory limit, and the operations being carried out when they fail to do so, when decoding with various Mf.",
        "With 0 = 7.5 x 10\" (row 2), HiFST can only decode 218 sentences, while HiPDT succeeds in 703 cases.",
        "The differences between both decoders increase as the Mf is more reduced, and for 9 = 7.5 x 10~ (row 4), HiPDT is able to perform exact search over all but three sentences.",
        "Table 5 shows performance using the latter configuration (Table 4, row 4).",
        "After large language model rescoring, HiPDT improves 0.5 BLEU over baseline with G\\ (Table 3, row 1)."
      ]
    },
    {
      "heading": "7. Discussion and Conclusion",
      "text": [
        "HiFST fails to decode mainly because the expansion into an FST leads to far too big search spaces (e.g. fails 938 times under 9 = 7.5 x 10\").",
        "If it succeeds in expanding the search space into an FST, the decoder still has to compose with the language model, which is also critical in terms of memory usage (fails 536 times).",
        "In contrast, HiPDT creates a PDA, which is a more compact representation of the search space and allows efficient intersection with the language model before expansion into an FST.",
        "Therefore, the memory usage is considerably lower.",
        "Nevertheless, the complexity of the language model is critical for the PDA intersection and very specially the PDA expansion into an FST (fails 403 times for 0 = 7.5 x 10\").",
        "HiFST (Gi + Mf )",
        "+Mi",
        "+M2",
        "#",
        "0",
        "tune-nw",
        "test-nw",
        "time",
        "ß",
        "tune-nw",
        "test-nw",
        "tune-nw",
        "test-nw",
        "1",
        "0(Mi)",
        "34.3",
        "34.5",
        "0.68",
        "-",
        "-",
        "-",
        "34.8",
        "35.6",
        "2",
        "7.5 x l(Ty",
        "32.0",
        "32.8",
        "0.38",
        "10",
        "34.3",
        "34.5",
        "34.8",
        "35.6",
        "9 8",
        "34.9",
        "35.5",
        "3",
        "7.5 x 1(TS",
        "29.5",
        "30.0",
        "0.28",
        "12",
        "34.2",
        "34.5",
        "34.7",
        "35.6",
        "9",
        "34.3",
        "34.4",
        "34.8",
        "35.2",
        "8",
        "34.2",
        "35.1",
        "4",
        "7.5 x 1(TY",
        "26.0",
        "26.4",
        "0.20",
        "75",
        "34.2",
        "34.5",
        "34.7",
        "35.6",
        "72",
        "34.4",
        "35.5",
        "Exact search for G2 + Mf with memory usage under 10 GB",
        "#",
        "0",
        "HiFST",
        "HiPDT",
        "Success",
        "Failure",
        "Success",
        "Failure",
        "Expand",
        "Compose",
        "Compose",
        "Expand",
        "2",
        "7.5 x 10~y",
        "12",
        "51",
        "37",
        "40",
        "8",
        "52",
        "3",
        "7.5 x 10~",
        "16",
        "53",
        "31",
        "76",
        "1",
        "23",
        "4",
        "7.5 x 10~Y",
        "18",
        "53",
        "29",
        "99.8",
        "0",
        "0.2",
        "HiPDT (G2 +",
        "Mf)",
        "+Mi",
        "+M2",
        "0",
        "tune-nw",
        "test-nw",
        "ß",
        "tune-nw",
        "test-nw",
        "tune-nw",
        "test-nw",
        "7.5 x 10-",
        "25.7",
        "26.3",
        "15",
        "34.6",
        "34.8",
        "35.2",
        "36.1",
        "With the algorithms presented in this paper, decoding with PDAs is possible for any translation grammar as long as an entropy pruned LM is used.",
        "While this allows exact decoding, it comes at the cost of making decisions based on less complex LMs, although this has been shown to be an adequate strategy when applying compact CFG rule-sets.",
        "On the other hand, HiFST cannot decode under large translation grammars, thus requiring pruning during lattice construction, but it can apply an un-pruned LM in this process.",
        "We find that with carefully designed pruning strategies, HiFST can match the performance of HiPDT reported in Table 5.",
        "But without pruning in search, expansion directly into an FST would lead to an explosion in terms of memory usage.",
        "Of course, without memory constraints both strategies would reach the same performance.",
        "Overall, these results suggest that HiPDT is more robust than HiFST when using complex hierarchical grammars.",
        "Conversely, FSTs might be more efficient for search spaces described by more constrained hierarchical grammars.",
        "This suggests that a hybrid solution could be effective: we could use PDAs or FSTs e.g. depending on the number of states of the FST representing the expanded search space, or other conditions."
      ]
    },
    {
      "heading": "8. Acknowledgments",
      "text": [
        "The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement number 247762, and was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No.HROOll-06-C-0022, and a Google Faculty Research Award, May 2010."
      ]
    }
  ]
}
