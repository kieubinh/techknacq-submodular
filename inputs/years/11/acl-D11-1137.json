{
  "info": {
    "authors": [
      "Katsuhiko Hayashi",
      "Taro Watanabe",
      "Masayuki Asahara",
      "Yuji Matsumoto"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1137",
    "title": "Third-order Variational Reranking on Packed-Shared Dependency Forests",
    "url": "https://aclweb.org/anthology/D11-1137",
    "year": 2011
  },
  "references": [
    "acl-C10-1123",
    "acl-C96-1058",
    "acl-D07-1101",
    "acl-D08-1022",
    "acl-D09-1058",
    "acl-E06-1011",
    "acl-J07-2003",
    "acl-N10-1115",
    "acl-P03-1021",
    "acl-P05-1012",
    "acl-P05-1022",
    "acl-P08-1067",
    "acl-P08-1068",
    "acl-P10-1001",
    "acl-P10-1110",
    "acl-W05-1506",
    "acl-W09-3839"
  ],
  "sections": [
    {
      "text": [
        "Katsuhiko Hayashit, Taro Watanabe*, Masayuki Asaharat, Yuji Matsumotot",
        "^Nara Insutitute of Science and Technology Ikoma, Nara, 630-0192, Japan ^National Institute of Information and Communications Technology Sorakugun, Kyoto, 619-0289, Japan",
        "{katsuhiko-h,masayu-a,matsu}@is.naist.jp taro.watanabe@nict.go.jp",
        "We propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of Eisner's generative model.",
        "In our framework, we define two kinds of generative model for reranking.",
        "One is learned from training data offline and the other from a forest generated by a baseline parser on the fly.",
        "The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model.",
        "In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms.",
        "Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniakand Johnson, 2005; Huang, 2008).",
        "Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system.",
        "In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking.",
        "Sangati et al.",
        "(2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model.",
        "They use a variant of Eisner's generative model C (Eisner, 1996b;",
        "Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisner's second-order generative model.",
        "Their reranking model showed large improvements in dependency parsing accuracy.",
        "They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selection among the few best candidates.",
        "In this paper, we propose a forest generative reranking algorithm, opposed to Sangati et al.",
        "(2009)'s approach which reranks only k-best candidates.",
        "Forests usually encode better candidates more compactly than k-best lists (Huang, 2008).",
        "Moreover, our reranking uses not only a generative model obtained from training data, but also a sentence specific generative model learned from a forest.",
        "In the reranking stage, we use linearly combined model of these models.",
        "We call this variational rerank-ing model.",
        "The model proposed in this paper is factored in the third-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Viterbi search.",
        "To solve this problem, we also propose a new search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (Koo and Collins, 2010).",
        "This algorithm enables us an exact 1 best reranking without any approximation.",
        "We summarize our contributions in this paper as follows.",
        "• To extend k-best to forest generative reranking.",
        "• We introduce variational reranking which is a combination approach of generative reranking and variational decoding (Li et al., 2009).",
        "• To obtain 1 best tree in the reranking stage, we",
        "propose an exact 1-best search algorithm with the third-order model.",
        "In experiments on English Penn Treebank data, we show that our proposed methods bring significant improvement to dependency parsing.",
        "Moreover, our variational reranking framework achieves consistent improvement, compared to conventional approaches, such as simple k-best and forest-based generative reranking algorithms."
      ]
    },
    {
      "heading": "2. Dependency Parsing",
      "text": [
        "Given an input sentence x g X, the task of statistical dependency parsing is to predict output dependencies y for x.",
        "The task is usually modeled within a discriminative framework, defined by the following equation:",
        "y = argmax s(x,y) yey",
        "where Y is the output space, A is a parameter vector, and F() is a set of feature functions.",
        "We denote a set of candidates as G(x).",
        "By using G(x), the conditional probability p(y|x) is typically derived as follows:",
        "where s(x,y) is the score function shown in Eq.1 and y is a scaling factor to adjust the sharpness of the distribution and Z(x) is a normarization factor.",
        "We propose to encode many hypotheses in a compact representation called dependency forest.",
        "While there may be exponentially many dependency trees, the forest represents them in polynomial space.",
        "A dependency forest (or tree) can be defined as a hypergraph data strucure HG (Tu et al., 2010).",
        "Figure 1 shows an example of a hypergraph for a dependency tree.",
        "A shaded hyperedge e is defined as the following form:",
        "e : {(Ii,2, girl3)5, withes), saw^g).",
        "a34:D telescope6 s :N",
        "The node sawi s is a head node of e. The nodes, Ii 2, girl3 5 and with5 s, are tail nodes of e. The hyper-edge e is an incoming edge for sawi s and outgoing edge for each of Ii 2, girl3 5 and with5 s.",
        "More formally, HG(x) of a forest is a pair (V, E), where V is a set of nodes and E is a set of hyperedges.",
        "Given a length m sentence x = (w1... wm), each node v G V is in the form of Wi j (= (wj... wJ+1)) which denotes that a word w dominates the substring from positions i to j.",
        "In our implementation, each word is paired with POStag tag(w).",
        "We denote the root node of dependency tree y as top.",
        "Each hyperedge e G E is a pair (tails(e), head(e)), where head(e) g V is the head and tails(e) g V + are its dependants.",
        "For notational brevity of algorithmic description, we do not distinguish left and right tails in the definition, but, our implementation implicitly distinguishes left tails tailsl(e) and right tails tailsR(e).",
        "We define the set of incoming edges of a node v as IE(v) and the set of outgoing edges of a node v as OE(v)."
      ]
    },
    {
      "heading": "3. Forest Reranking",
      "text": [
        "Given a node v in a dependency tree y, the left and right children are generated as two separate Markov sequences, each conditioned on ancestral and sibling information (context).",
        "Like a variation of Eisner's generative model C (Eisner, 1996b; Eisner, 1996a),",
        "Table 1: An event list of tri-sibling model whose event space is v\\h, sib, tsib, dir, extracted from hyperedge e in Figure 1.",
        "EOC is an end symbol of sequence.",
        "event space 11 saw NONE NONE L EOC | saw I NONE L girl | saw NONE NONE R with | saw girl NONE R",
        "EOC | saw with girl R",
        "the probability of our model q is defined as follows:",
        "where \\tailsL(e) \\ and \\tailsR(e) \\ are the number of left and right children of v, vl and vr are the left and right child of position l and r in each side.",
        "C(V) is a context event space of v. We explain the context event space later in more detail.",
        "The probability of the entire dependency tree y is recursively computed by q(y(top)) where y(top) denotes a top node of y.",
        "The probability q(v\\C(v)) is dependent on a context space C(v) for a node v. We define two kinds of context spaces.",
        "First, we define a tri-sibling model whose context space consists of the head node, sibling node, tri-sibling node and direction of a node v:",
        "where h, sib and tsib are head, sibling and tri-sibling node of v, and dir is a direction of v from h. Table 1 shows an example of an event list of the tri-sibling model, which is extracted from hyperedge e in Figure 1.",
        "EOC indicates the end of the left or right child sequence.",
        "This is factored in a tri-sibling structure shown in the left side of Figure 2.",
        "Eq.4 is further decomposed into a product of the form consisting of three terms:",
        "where tag(v) and wrd(v) are the POS-tag and word of v and dist(v, h) is the distance between positions of v and h. The values of dist(v, h) are partitioned into 4 categories: 1,2,3 – 6, 7 – oo.",
        "Second, following Sangati et al.",
        "(2009), we define a grandsibling model whose context space consists of the head node, sibling node, grandparent node and direction of a node v.",
        "where g is a grandparent node of v. Analogous to Eq.5, Eq.6 is decomposed into three terms:",
        "where notations are the same as those in Eq.5 with the exception of tri-sibling tsib and grandparent g. This model is factored in a grandsibling structure shown in the right side of Figure 2.",
        "The direct estimation of tri-sibling and grandsib-ling models from a corpus suffers from serious data sparseness issues.",
        "To overcome this, Eisner (1996a) proposed a back-off strategy which reduces the conditioning of a model.",
        "We show the reductions list for each term of two models in Table 2.",
        "The usage of reductions list is identical to Eisner (1996a) and readers may refer to it for further details.",
        "The final prediction is performed using a loglinear interpolated model.",
        "It interpolates the baseline discriminative model and two (tri-sibling and grandsibling) generative models.",
        "+ log p(y\\x)ebase q1(v\\h, sib, tsib, dir) (5) where 6 are parameters to adjust the weight of each = qi(dist(v,h),wrd(v),tag(v)\\h,sib,tsib,dir) terminprediction.",
        "Theseparametersaretunedusing xql(wrd(v)|tag(v),h, sib, tsib, dir) son why we chose MERT is that it effectively tunes xq1(dist(v, h) \\wrd(v), tag(v), h, sib, tsib, dir) dense parameters with a line search algorithm.",
        "Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word, POS-tag for a node.",
        "d indicates the direction.",
        "The first reduction on the list keeps all or most of the original condition; later reductions throw away more and more of this information.",
        "Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure.",
        "Our baseline discriminative model uses first-and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006).",
        "Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge.",
        "On the other hand, the grandsib-ling model has non-local features because the grandparent is not factored in one hyperedge.",
        "We summarize the order of each model in Table 3.",
        "Our reranking models are generative versions ofKoo and Collins (2010)'s third-order factorization model.",
        "Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm.",
        "One solution to resolve the intractability is an approximate k-best Viterbi search.",
        "For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features.",
        "Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram language model (Chiang, 2007).",
        "It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005).",
        "In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programming-based third-order parsing algorithm, which enumerates all grandparents with an additional loop.",
        "Our hypergraph based search algorithm for Eq.8 share the same spirit to their third-order parsing algorithm since the grandsibling model is similar to their model 1 in that it is factored in grandsibling structure.",
        "Algorithm 1 shows the search algorithm.",
        "This is almost the same bottom-up 1-best Viterbi algorithm except an additional loop in line 4.",
        "Line 4 references outgoing edge é of node h from a set of outgoing edges OE(h).",
        "tails(e) contains a node v, the sibling node sib and tri-sibling node tsib of v, moreover, the head of é (head(e')) is the grandparent for v and sib.",
        "Thus, in line 5, we can capture tri-sibling and grandsibling information and compute the current inside estimate of Eq.8.",
        "In our actual implementation, each score of components in Eq.8 is represented as a cost.",
        "This is written as a shortest path search algorithm with a tropical (real) semiring framework (Mohri, 2002; Huang, 2006).",
        "Therefore, © denotes the min operater and <g) denotes the + operater.",
        "The function f is defined as follows:",
        "where d(vi,e) denotes the current estimate of the best cost for a pair of node viand a hyperedge e.",
        "sums the best cost of a pair of a sub span node and hyperedge e. Each ctsib and cgsib in line 5 and 7 indicates the cost of tri-sibling and grandsibling Algorithm 1 Exact DP-Search Algorithm(HG(x))",
        "tri-sibling",
        "grandsibling",
        "1-st term",
        "2-nd term",
        "3-rd term",
        "1-st term",
        "2-nd term",
        "3-rd term",
        "wt(h),wt(sib),wt(tsib),d",
        "wt(h),t(sib),d",
        "wt(v),t(h),t(sib),d",
        "wt(h),wt(sib),wt(g),d",
        "wt(h) ,t(sib) ,d",
        "wt(v),t(h),t(sib),d",
        "wt(h),wt(sib),t(tsib),d",
        "t(h),t(sib),d",
        "t(v),t(h),t(sib),d",
        "wt(h) ,wt(sib) ,t(g) ,d",
        "t(h),t(sib),d",
        "t(v),t(h),t(sib),d",
        "t(h) ,wt(sib) ,t(tsib) ,d wt(h),t(sib),t(tsib),d",
        " – ",
        " – ",
        "t(h),wt(sib),t(g),d wt(h),t(sib),t(g),d",
        " – ",
        " – ",
        "t(h),t(sib),t(tsib),d",
        " – ",
        " – ",
        "t(h),t(sib),t(g),d",
        " – ",
        " – ",
        "first-order",
        "McDonald et al.",
        "(2005)",
        "second-order (sibling)",
        "Eisner (1996a) McDonald et al.",
        "(2005)",
        "third-order (tri-sibling)",
        "tri-sibling model Model 2 (Koo and Collins, 2010)",
        "third-order (grandsibling)",
        "grandsibling model (Sangati et al., 2009) Model 1 (Koo and Collins, 2010)",
        "1: for h e V in bottom-up topological order do 2: for e e IE(h) do 3: // tails(e) is {vi, ..",
        "., v|e|}.",
        "4: for e e OE(h) do",
        "- top then",
        "model.",
        "we indicates the cost of hyperedge e computed from a baseline discriminative model.",
        "Lines 6-7 denote the calculation of the best cost for a top node.",
        "We do not compute the cost of the grandsib-ling model when h is top node because top node has no outgoing edges.",
        "Our baseline k-best second-order parser is implemented using Huang and Chiang (2005)'s algorithm 2 whose time complexity is O(m + mk log k).",
        "Koo and Collins (2010)'s third-order parser has O(m) time complexity and is theoretically slower than our baseline k-best parser for a long sentence.",
        "Our search algorithm is based on the third-order parsing algorithm, but, the search space is previously shrank by a baseline parser's k-best approximation and a forest pruning algorithm presented in the next section.",
        "Therefore, the time efficiency of our reranking is unimpaired.",
        "Charniak and Johnson (2005) and Huang (2008) proposed forest pruning algorithms to reduce the size of a forest.",
        "Huang (2008)'s pruning algorithm uses a 1-best Viterbi inside/outside algorithm to compute an inside probability ß(v) and an outside probability a(v), while Charniak and Johnson (2005) use the usual inside/outside algorithm.",
        "In our experiments, we use Charniak and Johnson (2005)'s forest pruning criterion because the variational model needs traditional inside/outside probabilities for its ML estimation.",
        "We prune away all hyperedges that have score < p for a threshold p."
      ]
    },
    {
      "heading": "4. Variational Reranking Model",
      "text": [
        "In place of a maximum a posteriori (MAP) decision based on Eq.2, the minimum Bayes risk (MBR) decision rule (Titov and Henderson, 2006) is commonly used and defined as following equation:",
        "where loss(y, y') represents a loss function.",
        "As an alternative to the MBR decision rule, Li et al.",
        "(2009) proposed a variational decision rule that rescores candidates with an approximate distribution q* e Q.",
        "where q* minimizes the KL divergence KL(p\\\\ q)",
        "argmin KL(p\\\\q) argmax ^ p log q",
        "Following Huang (2008), we also prune away nodes with all incoming and outgoing hyperedges pruned.",
        "where each p and q represents p(y\\x) and q(y).",
        "For SMT systems, q* is modeled by n-gram language model over output strings.",
        "While the decoding based on q* is an approximation of intractable MAP de-coding, it works as a rescoring function for candidates generated from a baseline model.",
        "Here, we propose to apply the variational decision rule to dependency parsing.",
        "For dependency parsing, we can choose to model q* as the tri-sibling and grandsib-ling generative models in section 3.",
        "variational approximate framework can be applied to other tasks collapsing spurious ambiguity, such as latent-variable parsing (Matsuzaki et al., 2005).",
        "Algorithm 2 DP-ML Estimation(HG(x))",
        "1: run inside and outside algorithm on HG(x) 2: for v e V do 3: for e e IE(v) do 5: for u e tails(e) do 16: for u e tails(e) do 19: MLE estimate qf , q| using formula Eq.14",
        "q*(v|C(v)) is estimated from a forest using a maximum likelihood estimation (MLE).",
        "The count of events is no longer an integer count, but an expected count under p, which is formulated as follows:",
        "where Ce(y) is the number of event e in y.",
        "The estimation of Eq.14 can be efficiently performed on a hypergraph data structure HG(x) of a forest.",
        "Algorithm 2 shows the estimation algorithm.",
        "First, it runs the inside/outside algorithm on HG(x).",
        "We denote inside weight for a node v as ß(v) and outside weight as a(v).",
        "For each hyperedge e, we denote ctsib as the posterior weight for computing expected count Ci of events in the tri-sibling model q*.",
        "Lines 16-18 compute C1 for all events occuring in a hyperedge e.",
        "The expected count C2 needed for the estimation of grandsibling model q* is extracted in lines 7-15.",
        "C2 for a grandsibling model must be extracted over two hyperedges e and e' because it needs grandparent information.",
        "Lines 8-12 show the algorithm to compute the posterior weight Cgsib of e and e', which",
        "Figure 3: The relationship between tha data size (the number of hyperedges) and oracle scores on development data: Forests encode candidates with high accuracy scores more compactly than k-best lists.",
        "is similar to that to compute the posterior weight of rules of tree substitution grammars used in tree-based MT systems (Mi and Huang, 2008).",
        "Lines 13-15 compute expected counts C2 of events occur-ing over two hyperedges e and e'.",
        "Finally, line 19 estimates q* and q* using the form in Eq.14.",
        "Li et al.",
        "(2009) assumes n-gram locality of the forest to efficiently train the model, namely, the baseline n-gram model has larger n than that of variational n-gram model.",
        "In our case, grandsibling locality is not embedded in the forest generated from the baseline parser.",
        "Therefore, we need to reference incoming hyperedges of tail nodes in line 7.",
        "y* of Eq.12 may be locally appropriate but globally inadequate because q* only approximates P. Therefore, we log-linearly combine q* with a global generative model estimated from the training data and the baseline discriminative model.",
        "Algorithm1 is also applicable to the decoding of Eq.15.",
        "Note that this framework is a combination of variational decoding and generative reranking.",
        "We call this framework variational reranking.",
        "IUU",
        "99 98",
        "I I I / p=0.001",
        "i i i",
        "\"kbest\" – i – \"forest\" – x – _",
        "97",
        "^ ~~ k=100",
        "96",
        "- J k=20",
        "-",
        "95",
        "- /",
        "-",
        "94",
        "7",
        "-",
        "93 92",
        "i i i",
        "i i i",
        "Table 4: The statistics of forests and 20-best lists on development data: this shows the average number of hyper-edges and nodes per sentence and oracle scores."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "Experiments are performed on English Penn Treebank data.",
        "We split WSJ part of the Treebank into sections 02-21 for training, sections 22 for development, sections 23 for testing.",
        "We use Yamada and Matsumoto (2003)'s head rules to convert phrase structure to dependency structure.",
        "We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the second-order Eisner algorithms.",
        "We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing.",
        "We set the scaling factor 7 = 1.0.",
        "We also train a generative reranking model from the training data.",
        "To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a).",
        "Parameters 6 are trained using MERT (Och, 2003) and for each sentence in the development data, 300-best dependency trees are extracted from its forest.",
        "Our variational reranking does not need much time to train the model because the training is performed over not the training data (39832 sentences) but the development data (1700 sentences).",
        "After MERT was performed until the convergence, the variational reranking finally achieved a 94.5 accuracy score on development data.",
        "Figure 3 shows the relationship between the size of data structure (the number of hyperedges) and accuracy scores on development data.",
        "Obviously, forests can encode a large number of potential candidates more compactly than k-best lists.",
        "This means that for reranking, there is more possibility of selecting good candidates in forests than k-best lists.",
        "Table 4 shows the statistics of forests and 20-best lists on development data.",
        "This setting, threshold p = 10-3 for pruning, is also used for testing.",
        "Forests, which have an average of 180.67 hyper-edges per sentence, achieve oracle score of 98.76, which is about 1.0% higher than the 96.78 oracle score of 20-best lists with 255.04 hyperedges per sentence.",
        "Though the size of forests is smaller than that of k-best lists, the oracle scores of forests are much higher than those of k-best lists.",
        "First, we compare the performance of variational decoding with that of MBR decoding.",
        "The results are shown in Table 5.",
        "Variational decoding outperforms MBR decodings.",
        "However, compared with baseline, the gains of variational and MBR decoding are small.",
        "Second, we also compare the performance of variational reranking with k-best and forest generative reranking algorithms.",
        "Table 6 shows that our variational reranking framework achieves the highest accuracy scores.",
        "Being different from the decoding framework, reranking achieves significant improvements.",
        "This result is intuitively reasonable because the rerank-ing model obtained from training data has the ability to select a globally consistent candidate, while the variational approximate model obtained from a forest only supports selecting a locally consistent candidate.",
        "On the other hand, the fact that variational reranking achieves the best results clearly indicates that the combination of sentence specific generative model and that obtained from training data is successful in selecting both locally and globally appropriate candidate from a forest.",
        "Table 7 shows the parsing time (on 2.66GHz Quad-Core Xeon) of the baseline k-best, generative reranking and variational reranking parsers (java implemented).",
        "The variational reranking parser contains the following procedures."
      ]
    },
    {
      "heading": "1.. k best forest creation (baseline)",
      "text": []
    },
    {
      "heading": "2.. Estimation of variational model",
      "text": []
    },
    {
      "heading": "3.. Forest pruning",
      "text": []
    },
    {
      "heading": "4.. Search with the third-order model",
      "text": [
        "Our reranking parser incurred little overhead to the",
        "forest",
        "20-best",
        "pruning threshold ave. num of hyperedges ave. num of nodes oracle scores",
        "p = 10-3180.67 135.74 98.76",
        "255.04 491.42 96.78",
        "Table 5: The comparison of the decoding frameworks: MBR decoding seeks a candidate which has the highest accuracy scores over a forest (Kumar et al., 2009).",
        "Variational decoding is performed based on Eq.8.",
        "Table 7: The parsing time (CPU second per sentence) and accuracy score of the baseline k-best, generative reranking and variational reranking parsers",
        "Table 8: The comparison of tri-sibling and grandsibling models: the performance of the grandsibling model outperforms that of the tri-sibling model.",
        "baseline parser in terms of runtime.",
        "This means that our reranking parser can parse sentences at reasonable times.",
        "From results in section 5.2, our variational rerank-ing model achieves higher accuracy scores than the others.",
        "To analyze the factors that improve accuracy scores, we further investigate whether variational reranking is performed better with the tri-sibling or grandsibling model.",
        "Table 8 indicates that grandsibling model achieves a larger gain than that of tri-sibling model.",
        "Table 9 shows the examples whose accuracy scores improved by the grandsib-ling model.",
        "For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model.",
        "On the other hand, many errors remain still in",
        "Table 6: The comparison of the reranking frameworks: Generative means k-best or forest reranking algorithm based on a generative model estimated from a corpus.",
        "Variational reranking is performed based on Eq.15.",
        "Table 10: Comparison of our best result (using 16-best forests) with other best-performing Systems on the whole section 23_ our results.",
        "In our experiments, 48% of sentences which contain errors have Prepositional word errors.",
        "In fact, well-known PP-Attachment is a problem to be solved for natural language parsers.",
        "Other remaining errors are caused by symbols such as .,:\"\"().",
        "45% sentences contain such a dependency mistake.",
        "Adding features to solve these problems may potentially improve our parser more.",
        "Table 10 shows the comparison of the performance of variational reranking (16-best forests) with that of other systems.",
        "Our method outperforms supervised parsers with second-order features, and achieves comparable results compared to a parser with third-order features (Koo and Collins, 2010).",
        "We can not directly compare our method with semi-supervised parsers such as Koo et al.",
        "(2008)'s semi-sup and Suzuki et al.",
        "(2009), because ours does not use additional unlabeled data for training.",
        "The model trained from unlabeled data can be easily incorporated into our reranking framework.",
        "We plan to investigate semi-supervised learning in future work.",
        "Eval",
        "Decoding\"\"^-^^.^",
        "Unlabeled",
        "baseline MBR (8-best forest) Variational (8-best forest)",
        "91.9 91.99 92.17",
        "Eval",
        "Reranking~^~^^_^",
        "Unlabeled",
        "Generative (8-best) Generative (8-best forest) Variational (8-best forest)",
        "92.66 92.72 92.87",
        "Parser",
        "English",
        "McDonald et al.",
        "(2005)",
        "90.9",
        "McDonald and Pereira (2006)",
        "91.5",
        "Koo et al.",
        "(2008) standard",
        "92.02",
        "Huang and Sagae (2010)",
        "92.1",
        "Koo and Collins (2010) model1",
        "93.04",
        "Koo and Collins (2010) model2",
        "92.93",
        "this work",
        "92.89",
        "Koo et al.",
        "(2008) semi-sup",
        "93.16",
        "Suzuki et al.",
        "(2009)",
        "93.79",
        "k",
        "baseline",
        "generative",
        "variational",
        "2",
        "0.09 (91.9)",
        "+0.03 (92.67)",
        "+0.05 (92.76)",
        "4",
        "0.1 (91.9)",
        "+0.05 (92.68)",
        "+0.09 (92.81)",
        "8",
        "0.13 (91.9)",
        "+0.06 (92.72)",
        "+0.11 (92.87)",
        "16",
        "0.18 (91.9)",
        "+0.07 (92.75)",
        "+0.12(92.89)",
        "32",
        "0.29 (91.9)",
        "+0.07 (92.73)",
        "+0.13 (92.89)",
        "64",
        "0.54 (91.9)",
        "+0.08 (92.72)",
        "+0.15 (92.87)",
        "Eval Model^-\\^",
        "Unlabeled",
        "tri-sibling grandsibling",
        "92.63 92.74",
        "Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational reranking parsers.",
        "The underlined portions show the effect of the grandsibling model."
      ]
    },
    {
      "heading": "6. Related Work",
      "text": [
        "Collins (2000) and Charniak and Johnson (2005) proposed a reranking algorithm for constituent parsers.",
        "Huang (2008) extended it to a forest rerank-ing algorithm with non-local features.",
        "Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm.",
        "Sangati et al.",
        "(2009) proposed a k-best generative reranking algorithm for dependency parsing.",
        "In this paper, we use a similar generative model, but combined with a variational model learned on the fly.",
        "Moreover, our framework is applicable to forests, not k-best lists.",
        "Koo and Collins (2010) presented third-order dependency parsing algorithm.",
        "Their model 1 is defined by an enclosing grandsibling for each sibling or grandchild part used in Carreras (2007).",
        "Our grandsibling model is similar to the model 1, but ours is defined by a generative model.",
        "The decoding in the reranking stage is also similar to the parsing algorithm of their model 1.",
        "In order to capture grandsibling factors, our decoding calculates inside probablities for not the current head node but each pair of the node and its outgoing edges.",
        "Titov and Henderson (2006) reported that the MBR approach could be applied to a projective dependency parser.",
        "In the field of SMT, for an approximation of MAP decoding, Li et al.",
        "(2009) proposed variational decoding and Kumar et al.",
        "(2009) presented hypergraph MBR decoding.",
        "Our variational model is inspired by the study of Li et al.",
        "(2009) and we apply it to a dependency parser in order to select better candidates with third-order information.",
        "We also propose an efficient algorithm to estimate the non-local third-order model structure.",
        "7 Conclusions",
        "In this paper, we propose a novel forest reranking algorithm for dependency parsing.",
        "Our reranking algorithm is a combination approach of generative reranking and variational decoding.",
        "The search algorithm in the reranking stage can be performed using dynamic programming algorithm.",
        "Our variational reranking is aimed at selecting a candidate from a forest, which is correct both in local and global.",
        "Our experimental results show more significant improvements than conventional approaches, such as k-best and forest generative reranking.",
        "In the future, we plan to investigate more appropriate generative models for reranking.",
        "PP-Attachment is one of the most difficult problems for a natural language parser.",
        "We plan to examine to model such a complex structure (granduncle) (Goldberg and Elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser.",
        "As we mentioned in Section 5.4, we also plan to incorporate semi-supervised learning into our framework, which may potentially improve our reranking performance."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank Graham Neubig and Masashi Shimbo for their helpful comments and to the anonymous reviewers for their effort of reviewing our paper and giving valuable comments.",
        "This work was supported in part by Grant-in-Aid for Japan Society for the Promotion of Science (JSPS) Research Fellowship for Young Scientists.",
        "sent (No.148)",
        "A quick turnaround is crucial to Quantum because its cash requirements remain heavy .",
        "correct",
        "3 3 4 0 4 5 6 4 11 11 12 8 12 4",
        "baseline",
        "33404564 11 11 88 12 4",
        "proposed",
        "3 3 4 0 4 5 6 4 11 11 12 8 12 4",
        "sent (No.283)",
        "Many called it simply a contrast in styles.",
        "correct",
        "202662672",
        "baseline",
        "202262672",
        "proposed",
        "202662672"
      ]
    }
  ]
}
