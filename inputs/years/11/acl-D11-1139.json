{
  "info": {
    "authors": [
      "Andre Martins",
      "Noah A. Smith",
      "Mario Figueiredo",
      "Pedro Aguiar"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1139",
    "title": "Structured Sparsity in Structured Prediction",
    "url": "https://aclweb.org/anthology/D11-1139",
    "year": 2011
  },
  "references": [
    "acl-D08-1091",
    "acl-H05-1066",
    "acl-N04-1039",
    "acl-P02-1062",
    "acl-P07-1104",
    "acl-P09-1054",
    "acl-P10-1052",
    "acl-P11-1137",
    "acl-W00-0726",
    "acl-W02-1001",
    "acl-W02-2024",
    "acl-W03-0419",
    "acl-W03-1018",
    "acl-W06-2920"
  ],
  "sections": [
    {
      "text": [
        "André F. T. Martins*f Noah A. Smith* Pedro M. Q. Aguiar* Mario A. T. Figueiredof",
        "* School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA *Instituto de Sistemas e Robotica, Instituto Superior Tecnico, Lisboa, Portugal ^Instituto de Telecomunicacoes, Instituto Superior Tecnico, Lisboa, Portugal {afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.pt",
        "Linear models have enjoyed great success in structured prediction in NLP.",
        "While a lot of progress has been made on efficient training with several loss functions, the problem of endowing learners with a mechanism for feature selection is still unsolved.",
        "Common approaches employ ad hoc filtering or L1-regularization; both ignore the structure of the feature space, preventing practicioners from encoding structural prior knowledge.",
        "We fill this gap by adopting regularizers that promote structured sparsity, along with efficient algorithms to handle them.",
        "Experiments on three tasks (chunking, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation.",
        "State-of-the-art models usually involve linear combinations of features and are trained discrim-inatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured per-ceptron (Collins, 2002a).",
        "In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance.",
        "In this paper, we are concerned with model selection: which features should be used to define the prediction score?",
        "The fact that models with few features (\"sparse\" models) are desirable for several reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003).",
        "Our focus is on methods which embed this selection into the learning problem via the regularization term.",
        "We depart from previous approaches in that we seek to make decisions jointly about all candidate features, and we want to promote sparsity patterns that go beyond the mere cardinality of the set of features.",
        "For example, we want to be able to select entire feature templates (rather than features individually), or to make the inclusion of some features depend on the inclusion of other features.",
        "We achieve the goal stated above by employing regularizers which promote structured sparsity.",
        "Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space.",
        "Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011).",
        "Eisenstein et al.",
        "(2011) employed structured sparsity in computational sociolinguistics.",
        "However, none of these works have addressed structured prediction.",
        "Here, we combine these two levels of structure: structure in the output space, and structure in the feature space.",
        "The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering.",
        "We obtain models that are interpretable, accurate, and often much more compact than L2-regularized ones.",
        "Compared with Li-regularized models, ours are often more accurate and yield faster runtime."
      ]
    },
    {
      "heading": "2. Structured Prediction",
      "text": [
        "We address structured prediction problems, which involve an input set X (e.g., sentences) and an output set Y, assumed large and structured (e.g., tags or parse trees).",
        "We assume that each x G X has a set of candidate outputs Y (x) ç Y.",
        "We consider linear models, in which predictions are made according to where 0(x, y) G RD is a vector of features, and 0 G RD is the vector of corresponding weights.",
        "Let D = {(xj; yi)}i=1 be a training sample.",
        "We assume a cost function is defined such that c(y, y) is the cost of predicting y when the true output is y ; our goal is to learn 0 with small expected cost on unseen data.",
        "To achieve this goal, linear models are usually trained by solving a problem of the form where Q is a regularizer and L is a loss function.",
        "Examples of losses are: the negative conditional logcomputational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following:",
        "• Li regularization (Kazama and Tsujii, 2003;",
        "The latter is known as \"Lasso,\" as popularized by Tibshirani (1996) in the context of sparse regression.",
        "In the two cases above, A and t are nonnegative coefficients controlling the intensity of the regularization.",
        "q^ usually leads to easier optimization and robust performance; q^ encourages sparser models, where only a few features receive nonzero weights; see Gao et al.",
        "(2007) for an empirical comparison.",
        "More recently, Petrov and Klein (2008b) applied Li regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a).",
        "Elastic nets interpolate between Li and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al.",
        "(2010) to regularize CRFs.",
        "Neither of the regularizers just described \"looks\" at the structure of the feature space, since they all treat each dimension independently – we call them unstructured regularizers, as opposed to the structured ones that we next describe.",
        "Empirical comparison among these loss functions We are interested in regularizers that share with Q^ can be found in the literature (see, e.g., Martins et al., the ability to promote sparsity, so that they can be 2010, who also consider interpolations of the losses used for selecting features.",
        "In addition, we want to above).",
        "In practice, it has been observed that the endow the feature space RD with additional strucchoice of loss has far less impact than the model de ture, so that features are not penalized individually sign and choice of features.",
        "Hence, in this paper, (as in the Li-case) but collectively, encouraging enwe focus our attention on the regularization term in tire groups of features to be discarded.",
        "The choice of",
        "Eq.",
        "2.",
        "We specifically address ways in which this groups will allow encoding prior knowledge regardterm can be used to help design the model by pro ing the kind of sparsity patterns that are intended in moting structured sparsity.",
        "While this has been a the model.",
        "This can be achieved with group-Lasso topic of intense research in signal processing and regularization, which we next describe.",
        "To capture the structure of the feature space, we group our D features into M groups Gi;..., GM, where each Gm ç {1,..., D}.",
        "Ahead, we discuss meaningful ways of choosing group decompositions; for now, let us assume a sensible choice is obvious to the model designer.",
        "Denote by 0m = (Qd)deGm the subvector of those weights that correspond to the features in the m-th group, and let di;..., dM be nonnegative scalars (one per group).",
        "We consider the following group-Lasso regularizers:",
        "These regularizers were first proposed by Bakin (1999) and Yuan and Lin (2006) in the context of regression.",
        "If di = ... = dM, Q^L becomes the \"Linorm of the L2 norms.\"",
        "Interestingly, this is also a norm, called the mixed L2 i-norm.",
        "These regularizers subsume the Li and L2 cases, which correspond to trivial choices of groups:",
        "• If each group is a singleton, i.e., M = D and",
        "Gd = {#d}, and di = ... = dM = t, we recover Li-regularization (cf. Eqs.",
        "7-8).",
        "• If there is a single group spanning all the features, i.e., M = 1 and Gi = {1,..., D}, then the right hand side of Eq.",
        "8 becomes di „0„2.",
        "This is equivalent to L2 regularization.",
        "We next present some non-trivial examples concerning different topologies of G = {Gi;..., GM}.",
        "Non-overlapping groups.",
        "Let us first consider the case where G is a partition of the feature space: the groups cover all the features ( m Gm = {1,..., D}), and they do not overlap (Ga n Gb = 0, Va = b).",
        "Then, Q^L is termed a non-overlapping group-Lasso regularizer.",
        "It encourages sparsity patterns in which entire groups are discarded.",
        "A judicious choice of groups can lead to very compact models and pinpoint relevant groups of features.",
        "The following examples lie in this category:",
        "• The two cases above (Li and L2 regularization).",
        "• Label-based groups.",
        "In multi-label classification, where Y = { 1 , .",
        ".",
        ".",
        ", L} , features are typically designed as conjunctions of input features with label indicators, i.e., they take the form 0(x, y) = ?/>(x) <g> ey, where ?/>(x) G RDx , ey G RL has all entries zero except the y-th entry, which is 1, and <g) denotes the Kronecker product.",
        "Hence 0(x, y) can be reshaped as a DX -by-L matrix, and we can let each group correspond to a row.",
        "In this case, all groups have the same size and we typically set di = .",
        ".",
        ".",
        "= dM.",
        "A similar design can be made for sequence labeling problems, by considering a similar grouping for the unigram features.",
        "• Template-based groups.",
        "In NLP, features are commonly designed via templates.",
        "For example, a template such as w0 A p0 A pi denotes the word in the current position (w0) conjoined with its part-of-speech (p0) and that of the previous word (p-i).",
        "This template encloses many features corresponding to different instantiantions of w0, p0, and p-i.",
        "In §5, we learn feature templates from the data, by associating each group to a feature template, and letting that group contain all features that are instantiations of this template.",
        "Since groups have different sizes, it is a good idea to let dm increase with the group size, so that larger groups pay a larger penalty for being included.",
        "Tree-structured groups.",
        "More generally, we may let the groups in G overlap but be nested, i.e., we may want them to form a hierarchy (two distinct groups either have empty intersection or one is contained in the other).",
        "This induces a partial order on G (the set inclusion relation 5), endowing it with the structure of a partially ordered set (poset).",
        "A convenient graphical representation of the poset (G, 5) is its Hasse diagram.",
        "Each group is a node in the diagram, and an arc is drawn from group Gato group Gb if Gb C Ga and there is no b' s.t.",
        "Gb C Gb' C Ga.",
        "When the groups are nested, this diagram is a forest (a union of directed trees).",
        "The corresponding regularizer enforces sparsity patterns where a group of features is only selected if all its ancestors are also selected.",
        "Hence, entire subtrees in the diagram can be pruned away.",
        "Examples are:",
        "• The elastic net.",
        "The diagram of G has a root node for Gi = { 1 , .",
        ".",
        ".",
        ", D} and D leaf nodes, one per each singleton group (see Fig. 1).",
        "• The sparse group-Lasso.",
        "This regularizer was proposed by Friedman et al.",
        "(2010):",
        "where the total number of groups is M = M' + D, and the components 0i , .",
        ".",
        ".",
        ", 0M' are non-overlapping.",
        "This regularizer promotes sparsity at both group and feature levels (i.e., it eliminates entire groups and sparsifies within each group).",
        "Graph-structured groups.",
        "In general, the groups in G may overlap without being nested.",
        "In this case, the Hasse diagram of G is a directed acyclic graph (DAG).",
        "As in the tree-structured case, a group of features is only selected if all its ancestors are also selected.",
        "Based on this property, Jenatton et al.",
        "(2009) suggested a way of reverse engineering the groups from the desired sparsity pattern.",
        "We next describe a strategy for coarse-to-fine feature template selection that directly builds on that idea.",
        "Suppose that we are given M feature templates T = {Ti,..., TM} which are partially ordered according to some criterion, such that if Ta ^ Tb we would like to include Tb in our model only if Tais also included.",
        "This criterion could be a measure of coarseness: we may want to let coarser part-of-speech features precede finer lexical features, e.g., p0 A pi r< w0 A wi, or conjoined features come after their elementary parts, e.g., p0 ^ p0 A pi.",
        "The order does not need to be total, so some templates may not be comparable (e.g., we may want p0 A p_iand p0 A pi not to be comparable).",
        "To achieve the sparsity pattern encoded in (T, we choose G = (Gi,..., Gm) as follows: let I(T0) be the set of features that are instantiations of template Ta; then define G„ = (Jb:a^b I(Tb), for a = 1,..., M. It is easy to see that (G, 5) and (T, ^) are isomorph posets (their Hasse diagrams have the same shape; see Fig. 1).",
        "The result is a \"coarse-to-fine\" regular-izer, which prefers to select feature templates that are coarser before zooming into finer features.",
        "The prior knowledge encoded in the group-Lasso regularizer (Eq.",
        "8) comes with a Bayesian interpretation, as we next describe.",
        "In a probabilistic model (e.g. in the CRF case, where L = LCRF), the optimization problem in Eq.",
        "2 can be seen as maximum a posteriori estimation of 0, where the regularization term Q(0) corresponds to the negative log of a prior distribution (call it p(0)).",
        "It is well-known that L2-regularization corresponds to choosing independent zero-mean Gaussian priors, 9d – N(0, A-i), and that Li-regularization results from adopting zero-mean Laplacian priors, p($d) a exp(T |0d|).",
        "Figueiredo (2002) provided an alternative interpretation of Li regularization in terms of a two-level hierarchical Bayes model, which happens to generalize to the non-overlapping group-Lasso case, where Q = Q^L.",
        "As in the L2-case, we also assume that each parameter receives a zero-mean Gaussian prior, but now with a group-specific variance Tm, i.e., 0m ~ N(0,TmI) for m = 1,...,M. This reflects the fact that some groups should have their feature weights shrunk more towards zero than others.",
        "The variances Tm > 0 are not pre-specified but rather generated by a one-sided exponential hyperprior p(Tm|dm) a exp( – dmTm/2).",
        "It can be shown that after marginalizing out Tm, we obtain",
        "Hence, the non-overlapping group-Lasso corresponds to the following two-level hierachical Bayes model: independently for each m = 1 , .",
        ".",
        ".",
        ", M,",
        "Before introducing our learning algorithm for handling group-Lasso regularization, we need to define the concept of a Q-proximity operator.",
        "This is the function proxn : RD – >• RD defined as follows:",
        "Group Lasso (nonoverlapping)",
        "Sparse Group Lasso Overlapping Groups (not tree-structured)",
        "Figure 1: Hasse diagrams of several group-based regularizers.",
        "For all tree-structured cases, we use the same plate notation that is traditionally used in probabilistic graphical models.",
        "The rightmost diagram represents a coarse-to-fine regularizer: each node is a template involving contiguous sequences of words (w) and POS tags (p); the symbol order 0 ^ p ^ w induces a template order (Ta ^ Tbiff at each position i [Ta]j ^ [Tb]i).",
        "Digits below each node are the group indices where each template belongs.",
        "Proximity operators generalize Euclidean projections and have many interesting properties; see Bach et al.",
        "(2011) for an overview.",
        "By requiring zero to be a subgradient of the objective function in Eq.",
        "12, we obtain the following closed expression (called soft-thresholding) for the Q^ proximity operator:",
        "ed - t if ed >t Od + t if Od < -t.",
        "For the non-overlapping group Lasso case, the proximity operator is given by otherwise.",
        "which can be seen as a generalization of Eq.",
        "13: if the L2-norm of the m-th group is less than dm, the entire group is discarded; otherwise it is scaled so that its L2-norm decreases by an amount of dm.",
        "When groups overlap, the proximity operator lacks a closed form.",
        "When G is tree-structured, it can still be efficiently computed by a recursive procedure (Jenatton et al., 2010).",
        "When G is not tree-structured, no specialized procedure is known, and a convex optimizer is necessary to solve Eq.",
        "12."
      ]
    },
    {
      "heading": "4. Online Prox-Grad Algorithm",
      "text": [
        "We now turn our attention to efficient ways of handling group-Lasso regularizers.",
        "Several fast and scalable algorithms having been proposed for training Li-regularized CRFs, based on quasi-Newton optimization (Andrew and Gao, 2007), coordinate descent (Sokolovska et al., 2010; Lavergne et al., 2010), and stochastic gradients (Carpenter, 2008;",
        "Langford et al., 2009; Tsuruoka et al., 2009).",
        "The algorithm that we use in this paper (Alg.",
        "1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al.",
        "(2011) for multiple kernel learning.",
        "Alg.",
        "1 addresses the learning problem in Eq.",
        "2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer.",
        "Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings.",
        "The reason we have chosen the algorithm of Martins et al.",
        "(2011) is that it effectively handles overlapping groups, without the need of evaluating prox0(which, as seen in §3.3, can be costly if G is not tree-structured).",
        "To do so, it decomposes Q as for some J > 1, and nonnegative a1,...,aj ; each Q j-proximal operator is assumed easy to compute.",
        "Such a decomposition always exists: if G does not have overlapping groups, take J = 1.",
        "Otherwise, find J < M disjoint sets G1,---, GJ such that \\JJ=1 Gj = G and the groups on each Gj are non-overlapping.",
        "The proximal steps are then applied sequentially, one per each Qj.",
        "Overall, Alg.",
        "1 satisfies the following important requirements:",
        "• Computational efficiency.",
        "Each gradient step at round t is linear in the number of features that fire for that instance and independent of the total number of features D. Each proximal step is linear in the number of groups M, and does not need be to performed every round (as we will see later).",
        "Algorithm 1 Online Sparse Prox-Grad Algorithm",
        "• Memory efficiency.",
        "Only a small active set of features (those that have nonzero weights) need to be maintained.",
        "Entire groups of features can be deleted after each proximal step.",
        "Furthermore, only the features which correspond to nonzero entries in the gradient vector need to be inserted in the active set; for some losses (Lsvm and LSP) many irrelevant features are never instantianted.",
        "• Convergence.",
        "With high probability, Alg.",
        "1 produces an e-accurate solution after T < O(1/e) rounds, for a suitable choice of stepsizes and holding vjt constant, vjt = Vj (Martins et al., 2011).",
        "This result can be generalized to any sequence (vjt)T=i such tiutf: vj = T £T=i vjt.",
        "We next describe several algorithmic ingredients that make Alg.",
        "1 effective in sparse modeling.",
        "Budget-Driven Shrinkage.",
        "Alg.",
        "1 requires the choice of a \"gravity sequence.\"",
        "We follow Langford et al.",
        "(2009) and set (vjt)J=1 to zero for all t which is not a multiple of some prespecified integer K ; this way, proximal steps need only be performed each K rounds, yielding a significant speed-up when the number of groups M is large.",
        "A direct adoption of the method of Langford et al.",
        "(2009) would set vjt = Kvj for those rounds; however, we have observed that such a strategy makes the number of groups vary substantially in early epochs.",
        "We use a different strategy: for each Gj, we specify a budget of Bj > 0 groups (this may take into consideration practical limitations, such as the available memory).",
        "If t is a multiple of K, we set vjt as follows:",
        "1.",
        "If Gj does not have more than Bj nonzero groups, set vjt = 0 and do nothing.",
        "2.",
        "Otherwise, sort the groups in Gj by decreasing order of their L2-norms.",
        "Check the L2-norms of the Bj-th and Bj+1-th entries in the list and set vjt as the mean of these two divided by r]t.",
        "3.",
        "Apply a rjtVjtQj-proximal step using Eq.",
        "14.",
        "At the end of this step, no more than Bj groups will remain nonzero.",
        "If the average of the gravity steps converge, limTTfYlT=1 vjt – > vj, then the limit points Vj implicitly define the regularizer, via Q = ^J=1 Vj Qj .",
        "Hence, we have shifted the control of the amount of regularization to the budget constants Bj, which unlike the vj have a clear meaning and can be chosen under practical considerations.",
        "Space and Time Efficiency.",
        "The proximal steps in Alg.",
        "1 have a scaling effect on each group, which affects all features belonging to that group (see Eq.",
        "14).",
        "We want to avoid explicitly updating each feature in the active set, which could be time consuming.",
        "We mention two strategies that can be used for the non-overlapping group Lasso case.",
        "• The first strategy is suitable when M is large and only a few groups (<C M) have features that fire in each round; this is the case, e.g., of label-based groups (see §3.1).",
        "It consists of making lazy updates (Carpenter, 2008), i.e., to delay the update of all features in a group until at least one of them fires; then apply a cumulative penalty.",
        "The amount of the penalty can be computed if one assigns a timestamp to each group.",
        "• The second strategy is suitable when M is small and some groups are very populated; this is the typical case of template-based groups (§3.1).",
        "Two operations need to be performed: updating each feature weight (in the gradient steps), and scaling entire groups (in the proximal steps).",
        "We adapt a trick due to Shalev-Shwartz et al.",
        "(2007): represent the weight vector of the m-th group, 0m, by a",
        "triple (£m, Cm, Pm) G R|Gm| x R+ x R+, such that 0m = Cm£m and \\\\0m \\\\ = Pm.",
        "This representation allows performing the two operations above in constant time, and it keeps track of the group L2-norms, necessary in the proximal updates.",
        "For sufficient amounts of regularization, our algorithm has a low memory footprint.",
        "Only features that, at some point, intervene in the gradient computed in line 5 need to be instantiated; and all features that receive zero weights after some proximal step can be deleted from the model (cf.",
        "Fig.",
        "2).",
        "Sparseptron and Debiasing.",
        "Although Alg.",
        "1 allows to simultaneously select features and learn the model parameters, it has been observed in the sparse modeling literature that Lasso-like regularizers usually have a strong bias which may harm predictive performance.",
        "A post-processing stage is usually taken (called debiasing), in which the model is refitted without any regularization and using only the selected features (Wright et al., 2009).",
        "If a final de-biasing stage is to be performed, Alg.",
        "1 only needs to worry about feature selection, hence it is appealing to choose a loss function that makes this procedure as simple as possible.",
        "Examining the input of Alg.",
        "1, we see that both a gravity and a stepsize sequence need to be specified.",
        "The former can be taken care of by using budget-driven shrinkage, as described above.",
        "The stepsize sequence can be set as r/t = Wa/\\t/N 1, which ensures convergence, however no requires tuning.",
        "Fortunately, for the structured perceptron loss LSP (Eq.",
        "5), Alg.",
        "1 is independent of r0, up to a scaling of 0, which does not affect predictions (see Eq.",
        "1).",
        "We call the instantiation of Alg.",
        "1 with a group-Lasso regularizer and the loss LSP the sparseptron.",
        "Overall, we propose the following two-stage approach:",
        "1.",
        "Run the sparsepton for a few epochs and discard the features with zero weights.",
        "2.",
        "Refit the model without any regularization and using the loss L which one wants to optimize."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "We present experiments in three structured prediction tasks for several group choices.",
        "Text Chunking.",
        "We use the English dataset provided in the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of 8,936 training and 2,012 testing sentences (sections 15-18 and 20 of the WSJ.)",
        "The input observations are the token words and their POS tags; we want to predict the sequences of IOB tags representing phrase chunks.",
        "We built 96 contextual feature templates as follows:",
        "• Up to 5-grams of POS tags, in windows of 5 tokens on the left and 5 tokens on the right;",
        "• Up to 3-grams of words, in windows of 3 tokens on the left and 3 tokens on the right;",
        "• Up to 2-grams of word shapes, in windows of 2 tokens on the left and 2 tokens on the right.",
        "Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types – e.g., Confidence and 2, 664, 098 are respectively mapped to Aa and 0,0 + ,0 + (Collins, 2002b).",
        "We defined unigram features by conjoining these templates with each of the 22 output labels.",
        "An additional template was defined to account for label bigrams – features in this template do not look at the input string, but only at consecutive pairs of labels.We evaluate the ability of group-Lasso regularization to perform feature template selection.",
        "To do that, we ran 5 epochs of the sparseptron algorithm with template-based groups and budget-driven shrinkage (budgets of 10, 20, 30, 40, and 50 templates were tried).",
        "For each group Gm, we set dm = log2 | Gm |, which is the average number of bits necessary to encode a feature in that group, if all features were equiprobable.",
        "We set K = 1000 (the number of instances between consecutive proximal steps).",
        "Then, we refit the model with 10 iterations of the max-loss 1-best MIRA algorithm (Crammer et al., 2006).",
        "Table 1 compares the F1 scores and",
        "# Epochs",
        "Figure 2: Memory footprints of the MIRA and sparseptron algorithms in text chunking.",
        "The oscillation in the first 5 epochs (bottom line) comes from the proximal steps each K = 1000 rounds.",
        "The features are then frozen and 10 epochs of unregularized MIRA follow.",
        "Overall, the sparseptron requires < 7.5% of the memory as the MIRA baseline.",
        "the model sizes obtained with the several budgets against those obtained by running 15 iterations of MIRA with the original set of features.",
        "Note that the total number of iterations is the same; yet, the group-Lasso approach has a much smaller memory footprint (see Fig. 2) and yields much more compact models.",
        "The small memory footprint comes from the fact that Alg.",
        "1 may entertain a large number of features without ever instantiating all of them.",
        "The predictive power is comparable (although some choices of budget yield slightly better scores for the group-Lasso approach).",
        "Named Entity Recognition.",
        "We experiment with the Spanish, Dutch, and English datasets provided in the CoNLL 2002/2003 shared tasks (Sang, 2002; Sang and De Meulder, 2003).",
        "For Spanish, we use the POS tags provided by Car-",
        "(described next), we used L2 regularized MIRA and tuned the regularization constant with cross-validation.",
        "reras (http://www.lsi.upc.es/~nlp/tools/ nerc/nerc.html); for English, we ignore the syntactic chunk tags provided with the dataset.",
        "Hence, all datasets have the same sort of input observations (words and POS) and all have 9 output labels.",
        "We use the feature templates described above plus some additional ones (yielding a total of 452 templates):",
        "• For prefix/suffix sizes of 1, 2, 3, up to 3-grams of word prefixes/suffixes, in windows of size 3;",
        "• Up to 5-grams of case, punctuation, and digit indicators, in windows of size 5.",
        "As before, an additional feature template was defined to account for label bigrams.",
        "We do feature template selection (same setting as before) for budget sizes of 100, 200, and 300.",
        "We compare with both MIRA (using all the features) and the sparseptron with a standard Lasso regularizer Q^, for several values of C = 1/ (tN).",
        "Table 2 shows the results.",
        "We observe that template-based group-Lasso wins both in terms of accuracy and compactness.",
        "Note also that the ability to discard feature templates (rather than individual features) yields faster test runtime than models regularized with the standard Lasso: fewer templates will need to be instantiated, with a speed-up in score computation.",
        "Multilingual Dependency Parsing.",
        "We trained non-projective dependency parsers for 6 languages using the CoNLL-X shared task datasets (Buchholz and Marsi, 2006): Arabic, Danish, Dutch, Japanese, Slovene, and Spanish.",
        "We chose the languages with the smallest datasets, because regularization is more important when data is scarce.",
        "The output to be predicted from each input sentence is the set of dependency links, which jointly define a spanning tree.",
        "Table 1: Results for",
        "MIRA",
        "Group Lasso B =10",
        "B = 20",
        "B = 30",
        "B = 40",
        "B = 50",
        "text chunking.",
        "Fi (%)",
        "93.10",
        "92.99",
        "93.28",
        "93.59",
        "93.42",
        "93.40",
        "model size (# features)",
        "5,300,396",
        "71,075",
        "158,844",
        "389,065",
        "662,018",
        "891,378",
        "MIRA",
        "Lasso C = 0.1",
        "C = 0.5",
        "C = 1",
        "Group-Lasso B = 100",
        "B = 200",
        "B = 300",
        "Spa.",
        "dev/test",
        "70.38/74.09",
        "69.19/71.9",
        "70.75/72.38",
        "71.7/74.03",
        "71.79/73.62",
        "72.08/75.05",
        "71.48/73.3",
        "8,598,246",
        "68,565",
        "1,017,769",
        "1,555,683",
        "83,036",
        "354,872",
        "600,646",
        "Dut.",
        "dev/test",
        "69.15/71.54",
        "64.07/66.35",
        "66.82/69.42",
        "70.43/71.89",
        "69.48/72.83",
        "71.03/73.33",
        "71.2/72.59",
        "5,727,004",
        "164,960",
        "565,704",
        "953,668",
        "128,320",
        "447,193",
        "889,660",
        "Eng.",
        "dev/test",
        "83.95/79.81",
        "80.92/76.95",
        "82.58/78.84",
        "83.38/79.35",
        "85.62/80.26",
        "85.86/81.47",
        "85.03/80.91",
        "8,376,901",
        "232,865",
        "870,587",
        "1,114,016",
        "255,165",
        "953,178",
        "1,719,229",
        "✓",
        "✓",
        "1",
        "1",
        "1",
        "---MIRA",
        "/",
        "1",
        "-Sparceptron + MIRA (B=30) -",
        "1____",
        "NM",
        "Japanese",
        " – B – Group-Lasso",
        "Group-Lasso (C2F) Lasso",
        "Number of Features Slovene",
        "Figure 3: Comparison between non-overlapping group-Lasso, coarse-to-fine group-Lasso (C2F), and a filter-based method based on information gain for selecting feature templates in multilingual dependency parsing.",
        "The x-axis is the total number of features at different regularization levels, and the y-axis is the unlabeled attachment score.",
        "The plots illustrate how accurate the parsers are as a function of the model sparsity achieved, for each method.",
        "The standard Lasso (which does not select templates, but individual features) is also shown for comparison.",
        "We use arc-factored models, for which exact inference is tractable (McDonald et al., 2005).",
        "We defined M = 684 feature templates for each candidate arc by conjoining the words, shapes, lemmas, and POS of the head and the modifier, as well as the contextual POS, and the distance and direction of attachment.",
        "We followed the same two-stage approach as before, and compared with a baseline which selects feature templates by ranking them according to the information gain criterion.",
        "This baseline assigns a score to each template Tm which reflects an empirical estimate of the mutual information between Tm and the binary variable A that indicates the presence/absence of a dependency link:",
        "where ( / , a) is the joint probability of feature / firing and an arc being active (a = 1) or innactive (a = 0), and P(/) and P (a) are the corresponding marginals.",
        "All probabilities are estimated from the empirical counts of events observed in the data.",
        "The results are plotted in Fig. 3, for budget sizes but one language (Spanish is the exception), non-overlapping group-Lasso regularization is more effective at selecting feature templates than the information gain criterion, and slightly better than coarse-to-fine group-Lasso.",
        "For completeness, we also display the results obtained with a standard Lasso regularizer.",
        "Table 3 shows what kind of feature templates were most selected for each language.",
        "Some interesting patterns can be observed: morphologically-rich languages with small datasets (such as Turkish and Slovene) seem to avoid lexical features, arguably due to potential for overfitting; in Japanese, contextual POS appear to be specially relevant.",
        "It should be noted, however, that some of these patterns may be properties of the datasets rather than of the languages themselves."
      ]
    },
    {
      "heading": "6. Related Work",
      "text": [
        "A variant of the online proximal gradient algorithm used in this paper was proposed by Martins et al.",
        "84",
        "a-\"\"",
        "83.5",
        "83",
        "82.5",
        "82",
        "Table 3: Variation of feature templates that were selected accross languages.",
        "Each line groups together similar templates, involving lexical, contextual POS, word shape information, as well as attachment direction and length.",
        "Empty cells denote that very few or none of the templates in that category was selected; + denotes that some were selected; ++ denotes that most or all were selected.",
        "(2011), along with a theoretical analysis.",
        "The focus there, however, was multiple kernel learning, hence overlapping groups were not considered in their experiments.",
        "Budget-driven shrinkage and the sparseptron are novel techniques, at the best of our knowledge.",
        "Apart from Martins et al.",
        "(2011), the only work we are aware of which combines structured sparsity with structured prediction is Schmidt and Murphy (2010); however, their goal is to predict the structure of graphical models, while we are mostly interested in the structure of the feature space.",
        "Schmidt and Murphy (2010) used to generative models, while our approach emphasizes discriminative learning.",
        "Mixed norm regularization has been used for a while in statistics as a means to promote structured sparsity.",
        "Group Lasso is due to Bakin (1999) and Yuan and Lin (2006), after which a string of variants and algorithms appeared (Bach, 2008; Zhao et al., 2009; Jenatton et al., 2009; Friedman et al., 2010; Obozinski et al., 2010).",
        "The flat (non-overlapping) case has tight links with learning formalisms such as multiple kernel learning (Lanckriet et al., 2004) and multi-task learning (Caruana, 1997).",
        "The tree-structured case has been addressed by Kim and Xing (2010), Liu and Ye (2010) and Mairal et al.",
        "(2010), along with Loo,i and L2)1 regularization.",
        "Graph-structured groups are discussed in Jenatton et al.",
        "(2010), along with a DAG representation.",
        "In NLP, mixed norms have been used recently by Graca et al.",
        "(2009) in posterior regularization, and by Eisenstein et al.",
        "(2011) in a multi-task regression problem."
      ]
    },
    {
      "heading": "7. Conclusions",
      "text": [
        "In this paper, we have explored two levels of structure in NLP problems: structure on the outputs, and structure on the feature space.",
        "We have shown how the latter can be useful in model design, through the use of regularizers which promote structured sparsity.",
        "We propose an online algorithm with minimal memory requirements for exploring large feature spaces.",
        "Our algorithm, which specializes into the sparseptron, yields a mechanism for selecting entire groups of features.",
        "We apply sparseptron for selecting feature templates in three structured prediction tasks, with advantages over filter-based methods, L1, and L2 regularization in terms of performance, compactness, and model interpretability."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank all reviewers for their comments, Eric Xing for helpful discussions, and Slav Petrov for his comments on a draft version ofthis paper.",
        "A. M. was supported by a FCT/ICTI grant through the CMU-Portugal Program, and also by Priberam.",
        "This work was partially supported by the FET programme (EU FP7), under the SIMBAD project (contract 213250).",
        "N. S. was supported by NSF CAREER IIS-1054319.",
        "Ara.",
        "Dan.",
        "Jap.",
        "Slo.",
        "Spa.",
        "Tur.",
        "Bilexical",
        "++",
        "+",
        "+",
        "Lex.",
        " – POS",
        "+",
        "+",
        "POS – Lex.",
        "++",
        "+",
        "+",
        "+",
        "+",
        "POS - POS",
        "++",
        "+",
        "Middle POS",
        "++",
        "++",
        "++",
        "++",
        "++",
        "++",
        "Shape",
        "++",
        "++",
        "++",
        "++",
        "Direction",
        "+",
        "+",
        "+",
        "+",
        "+",
        "Distance",
        "++",
        "+",
        "+",
        "+",
        "+",
        "+"
      ]
    }
  ]
}
