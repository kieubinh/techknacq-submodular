{
  "info": {
    "authors": [
      "Mitesh M. Khapra",
      "Salil Joshi",
      "Arindam Chatterjee",
      "Pushpak Bhattacharyya"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-1057",
    "title": "Together We Can: Bilingual Bootstrapping for WSD",
    "url": "https://aclweb.org/anthology/P11-1057",
    "year": 2011
  },
  "references": [
    "acl-C10-1063",
    "acl-C96-1005",
    "acl-D09-1048",
    "acl-H05-1052",
    "acl-J04-1001",
    "acl-P04-1036",
    "acl-P95-1026",
    "acl-P96-1006"
  ],
  "sections": [
    {
      "text": [
        "Mitesh M. Khapra Salil Joshi Arindam Chatterjee Pushpak Bhattacharyya",
        "Department Of Computer Science and Engineering, IIT Bombay, Powai, Mumbai, 400076.",
        "{miteshk, salil j , arindam, pb}@cse.iitb.ac.in",
        "Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L\\) can benefit from the annotation work done in a resource rich language (L2) via parameter projection.",
        "However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible.",
        "Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data.",
        "We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of Li is used to annotate the untagged data of L2 and vice versa using parameter projection.",
        "The untagged instances of L\\ and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated.",
        "Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (Li) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world.",
        "The problem is even more hard-hitting for multilingual regions (e.g., India which has more than 20 constitutionally recognized languages).",
        "To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies.",
        "Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words.",
        "The above situation highlights the need for high accuracy resource conscious approaches to all-words multilingual WSD.",
        "Recent work by Khapra et al.",
        "(2010) in this direction has shown that it is possible to perform cost effective WSD in a target language (L2) without compromising much on accuracy by leveraging on the annotation work done in another language (L\\).",
        "This is achieved with the help of a novel synset-aligned multilingual dictionary which facilitates the projection of parameters learned from the Wordnet and annotated corpus of L\\ to L2.",
        "This approach thus obviates the need for collecting large amounts of annotated corpora in multiple languages by relying on sufficient annotated corpus in one resource rich language.",
        "However, in many situations such a pivot resource rich language itself may not be available.",
        "Instead, we might have two or more languages having a small amount of annotated corpus and a large amount of untagged corpus.",
        "Addressing such situations is the main focus of this work.",
        "Specifically, we address the following question:",
        "In the absence of a pivot resource rich language is it possible for two resource deprived languages to mutually benefit from each other's annotated data?",
        "While addressing the above question we assume that even though it is hard to obtain large amounts of annotated data in multiple languages, it should be fairly easy to obtain a large amount of untagged data in these languages.",
        "We leverage on such untagged data by employing a bootstrapping strategy.",
        "The idea is to train an initial model using a small amount of annotated data in both the languages and iteratively expand this seed data by including untagged instances which get tagged with a high confidence in successive iterations.",
        "Instead of using monolingual bootstrapping, we use bilingual bootstrapping via parameter projection.",
        "In other words, the parameters learned from the annotated data of L\\ (and L2 respectively) are projected to L2 (and L\\ respectively) and the projected model is used to tag the untagged instances of L2 (and L\\ respectively).",
        "Such a bilingual bootstrapping strategy when tested on two domains, viz., Tourism and Health using Hindi (L\\) and Marathi (L2) as the language pair, consistently does better than a baseline strategy which uses only seed data for training without performing any bootstrapping.",
        "Further, it consistently performs better than monolingual bootstrapping.",
        "A simple and intuitive explanation for this is as follows.",
        "In monolingual bootstrapping a language can benefit only from its own seed data and hence can tag only those instances with high confidence which it has already seen.",
        "On the other hand, in bilingual bootstrapping a language can benefit from the seed data available in the other language which was not previously seen in its self corpus.",
        "This is very similar to the process of co-training (Blum and Mitchell, 1998) wherein the annotated data in the two languages can be seen as two different views of the same data.",
        "Hence, the classifier trained on one view can be improved by adding those untagged instances which are tagged with a high confidence by the classifier trained on the other view.",
        "The remainder of this paper is organized as follows.",
        "In section 2 we present related work.",
        "Section 3 describes the Synset aligned multilingual dictionary which facilitates parameter projection.",
        "Section 4 discusses the work of Khapra et al.",
        "(2009) on parameter projection.",
        "In section 5 we discuss bilingual bootstrapping which is the main focus of our work followed by a brief discussion on monolingual bootstrapping.",
        "Section 6 describes the experimental setup.",
        "In section 7 we present the results followed by discussion in section 8.",
        "Section 9 concludes the paper."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Bootstrapping for Word Sense Disambiguation was first discussed in (Yarowsky, 1995).",
        "Starting with a very small number of seed collocations an initial decision list is created.",
        "This decisions list is then applied to untagged data and the instances which get tagged with a high confidence are added to the seed data.",
        "This algorithm thus proceeds iteratively increasing the seed size in successive iterations.",
        "This monolingual bootstrapping method showed promise when tested on a limited set of target words but was not tried for all-words WSD.",
        "The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort.",
        "Recent work in this direction by Khapra et al.",
        "(2009) aims at reducing the annotation effort in multiple languages by leveraging on existing resources in a pivot language.",
        "They showed that it is possible to project the parameters learned from the annotation work of one language to another language provided aligned Wordnets for the two languages are available.",
        "However, they do not address situations where two resource deprived languages have aligned Wordnets but neither has sufficient annotated data.",
        "In such cases bilingual bootstrapping can be used so that the two languages can mutually benefit from each other's small annotated data.",
        "Li and Li (2004) proposed a bilingual bootstrapping approach for the more specific task of Word Translation Disambiguation (WTD) as opposed to the more general task of WSD.",
        "This approach does not need parallel corpora (just like our approach) and relies only on in-domain corpora from two languages.",
        "However, their work was evaluated only on a handful of target words (9 nouns) for WTD as opposed to the broader task of WSD.",
        "Our work instead focuses on improving the performance of all words WSD for two resource deprived languages using bilingual bootstrapping.",
        "At the heart of our work lies parameter projection facilitated by a synset aligned multilingual dictionary described in the next section."
      ]
    },
    {
      "heading": "3. Synset Aligned Multilingual Dictionary",
      "text": [
        "A novel and effective method of storage and use of dictionary in a multilingual setting was proposed by Mohanty et al.",
        "(2008).",
        "For the purpose of current discussion, we will refer to this multilingual dictionary framework as MultiDict.",
        "One important departure in this framework from the traditional dictionary is that synsets are linked, and after that the words inside the synsets are linked.",
        "The basic mapping is thus between synsets and thereafter between the words.",
        "Table 1 : Multilingual Dictionary Framework",
        "Table 1 shows the structure of MultiDict, with one example row standing for the concept of boy.",
        "The first column is the pivot describing a concept with a unique ID.",
        "The subsequent columns show the words expressing the concept in respective languages (in the example table, English, Hindi and Marathi).",
        "After the synsets are linked, cross linkages are set up manually from the words of a synset to the words of a linked synset of the pivot language.",
        "For example, for the Marathi word 'JFRTT (mulgaa), \"a youthful male person\", the correct lexical substitute from the corresponding Hindi synset is tf&tfrl (ladkaa).",
        "The average number of such links per synset per language pair is approximately 3.",
        "However, since our work takes place in a semi-supervised setting, we do not assume the presence of these manual cross linkages between synset members.",
        "Instead, in the above example, we assume that all the words in the Hindi synset are equally probable translations of every word in the corresponding Marathi synset.",
        "Such cross-linkages between synset members facilitate parameter projection as explained in the next section."
      ]
    },
    {
      "heading": "4. Parameter Projection",
      "text": [
        "Khapra et al.",
        "(2009) proposed that the various parameters essential for domain-specific Word Sense Disambiguation can be broadly classified into two categories:",
        "Wordnet-dependent parameters:",
        "• belongingness-to-dominant-concept",
        "• conceptual distance",
        "• semantic distance",
        "Corpus-dependent parameters:",
        "• sense distributions",
        "• corpus co-occurrence",
        "They proposed a scoring function (Equation (1)) which combines these parameters to identify the correct sense of a word in a context:",
        "% G Candidate Synsets J = Set of disambiguated words 6i = BelongingnessToDominantConcept(Si) Vi = P(Si\\word) Wij = CorpusCooccurrence(Si, Sj) * 1/WNConceptualDistance(Si, Sj) * 1/WNSemanticGraphDistance(Si, Sj)",
        "The first component OiVi of Equation (1) captures influence of the corpus specific sense of a word in a domain.",
        "The other component Wij *V,*Vj captures the influence of interaction of the candidate sense with the senses of context words weighted by factors of co-occurrence, conceptual distance and semantic distance.",
        "Wordnet-dependent parameters depend on the structure of the Wordnet whereas the Corpus-dependent parameters depend on various statistics learned from a sense marked corpora.",
        "Both the tasks of (a) constructing a Wordnet from scratch and (b) collecting sense marked corpora for multiple languages are tedious and expensive.",
        "Khapra et al.",
        "(2009) observed that by projecting relations from the Wordnet of a language and by projecting corpus statistics from the sense marked corpora of the language to those of the target language, the effort required in constructing semantic graphs for multiple Wordnets and collecting sense marked corpora for multiple languages can be avoided or reduced.",
        "At the heart of their work lies the MultiDict described in previous section which facilitates parameter projection in the following manner:",
        "Concepts",
        "LI",
        "L2",
        "L3",
        "(English)",
        "(Hindi)",
        "(Marathi)",
        "04321:",
        "{male",
        "{H-ä + l",
        "{^HJII",
        "a youth-",
        "child,",
        "(ladkaa),",
        "(mulgaa),",
        "ful male",
        "boy}",
        "person",
        "(baalak),",
        "(porgaa),",
        "TÏT (por)}",
        "(bachchaa)}",
        "1.",
        "By linking with the synsets of a pivot resource rich language (Hindi, in our case), the cost of building Wordnets of other languages is partly reduced (semantic relations are inherited).",
        "The Wordnet parameters of Hindi Wordnet now become projectable to other languages.",
        "2.",
        "For calculating corpus specific sense distributions, P(Sense Si\\Word W), we need the counts, #(Si,W).",
        "By using cross linked words in the synsets, these counts become projectable to the target language (Marathi, in our case) as they can be approximated by the counts of the cross linked Hindi words calculated from the Hindi sense marked corpus as follows:",
        "H=(Si, crossJLinkedJiindi-Word) S-; itiSji cross JLinkedJiindi-Word)",
        "The rationale behind the above approximation is the observation that within a domain the counts of cross-linked words will remain the same across languages.",
        "This parameter projection strategy as explained above lies at the heart of our work and allows us to perform bilingual bootstrapping by projecting the models learned from one language to another."
      ]
    },
    {
      "heading": "5. Bilingual Bootstrapping",
      "text": [
        "We now come to the main contribution of our work, i.e., bilingual bootstrapping.",
        "As shown in Algorithm 1, we start with a small amount of seed data (LD\\ and LD2) in the two languages.",
        "Using this data we learn the parameters described in the previous section.",
        "We collectively refer to the parameters learned",
        "Algorithm 1 Bilingual Bootstrapping LD\\ := Seed Labeled Data from L\\ LD2 := Seed Labeled Data from L2 UD\\ := Unlabeled Data from L\\ IJD2 := Unlabeled Data from L2",
        "9\\ := model trained using LD\\ 62 := model trained using LD2",
        "for all ui G UDi do s := sense assigned by 9\\ to u\\ if confidence(s) > e then",
        "UDX := UDX - m end if end for for all U2 G UD2 do s := sense assigned by 62 to U2 if confidence(s) > e then LD2 := LD2 + u2UD2 := UD2 - u2end if end for until convergence from the seed data as models 61 and 62 for Li and L2 respectively.",
        "The parameter projection strategy described in the previous section is then applied to 61 and 62 to obtain the projected models 62 and Q\\ respectively.",
        "These projected models are then applied to the untagged data of Li and L2 and the instances which get labeled with a high confidence are added to the labeled data of the respective languages.",
        "This process is repeated till we reach convergence, i.e., till it is no longer possible to move any data from UDi (and UD2) to LDi (and LD2 respectively).",
        "We compare our algorithm with monolingual bootstrapping where the self models 61 and 62 are directly used to annotate the unlabeled instances in Li and L2 respectively instead of using the projected models Q\\ and 62.",
        "The process of monolingual boot-",
        "Algorithm 2 Monolingual Bootstrapping LD\\ := Seed Labeled Data from L\\ LD2 := Seed Labeled Data from L2UD\\ := Unlabeled Data from L\\ IJD2 := Unlabeled Data from L2 9\\ := model trained using LD\\ 62 := model trained using LD2 for all ui G UDi do s := sense assigned by 9\\ to u\\ if confidence(s) > e then LDi := LDx +ui UDX := UDX - m end if end for for all U2 G IJD2 do s := sense assigned by 62 to U2 if confidence(s) > e then LL>2 := LD2 + u2[/L'a := UD2 - u2end if end for until convergence strapping is shown in Algorithm 2.",
        "6 Experimental Setup",
        "We used the publicly available dataset described in Khapra et al.",
        "(2010) for all our experiments.",
        "The data was collected from two domains, viz., Tourism and Health.",
        "The data for Tourism domain was collected by manually translating English documents downloaded from Indian Tourism websites into Hindi and Marathi.",
        "Similarly, English documents for Health domain were obtained from two doctors and were manually translated into Hindi and Marathi.",
        "The entire data was then manually annotated by three lexicographers adept in Hindi and Marathi.",
        "The various statistics pertaining to the total number of words, number of words per POS category and average degree of polysemy are described in Tables 2 to 5.",
        "Although Tables 2 and 3 also report the numPolysemous words Monosemous words",
        "Table 3 : Polysemous and Monosemous words per category in each domain for Marathi",
        "Category",
        "Tourism",
        "Health",
        "Tourism",
        "Health",
        "Noun",
        "62336",
        "24089",
        "35811",
        "18923",
        "Verb",
        "6386",
        "1401",
        "3667",
        "5109",
        "Adjective",
        "18949",
        "8773",
        "28998",
        "12138",
        "Adverb",
        "4860",
        "2527",
        "13699",
        "7152",
        "All",
        "92531",
        "36790",
        "82175",
        "43322",
        "Category",
        "Tourism",
        "Health",
        "Tourism",
        "Health",
        "Noun",
        "45589",
        "17482",
        "27386",
        "11383",
        "Verb",
        "7879",
        "3120",
        "2672",
        "1500",
        "Adjective",
        "13107",
        "4788",
        "16725",
        "6032",
        "Adverb",
        "4036",
        "1727",
        "5023",
        "1874",
        "All",
        "70611",
        "27117",
        "51806",
        "20789",
        "Avg.",
        "degree of Wordnet polysemy",
        "for polysemous words",
        "Category",
        "Tourism Health",
        "Noun",
        "3.02",
        "3.17",
        "Verb",
        "5.05",
        "6.58",
        "Adjective",
        "2.66",
        "2.75",
        "Adverb",
        "2.52",
        "2.57",
        "All",
        "3.09",
        "3.23",
        "Avg.",
        "degree of Wordnet polysemy",
        "for polysemous words",
        "Category",
        "Tourism Health",
        "Noun",
        "3.06",
        "3.18",
        "Verb",
        "4.96",
        "5.18",
        "Adjective",
        "2.60",
        "2.72",
        "Adverb",
        "2.44",
        "2.45",
        "All",
        "3.14",
        "3.29",
        "Seed Size v/s F-score Seed Size (words)",
        "Figure 1 : Comparison of BiBoot, Mono-Boot, OnlySeed and WFS on Hindi Health data ber of monosemous words, we would like to clearly state that we do not consider monosemous words while evaluating the performance of our algorithms (as monosemous words do not need any disambiguation).",
        "We did a 4-fold cross validation of our algorithm using the above described corpora.",
        "Note that even though the corpora were parallel we did not use this property in any way in our experiments or algorithm.",
        "In fact, the documents in the two languages were randomly split into 4 folds without ensuring that the parallel documents remain in the same folds for the two languages.",
        "We experimented with different seed sizes varying from 0 to 5000 in steps of 250.",
        "The seed annotated data and untagged instances for bootstrapping are extracted from 3 folds of the data and the final evaluation is done on the held-out data in the 4th fold.",
        "We ran both the bootstrapping algorithms (i.e., monolingual bootstrapping and bilingual bootstrapping) for 10 iterations but, we observed that after 1-2 iterations the algorithms converge.",
        "In each iteration only those words for which P(assignedsense\\word) > 0.6 get moved to the labeled data.",
        "Ideally, this threshold (0.6) should have been selected using a development set.",
        "However, since our work focuses on resource scarce languages we did not want to incur the additional cost of using a development set.",
        "Hence, we used a fixed threshold of 0.6 so that in each iteration only those words get moved to the labeled data for which the assigned sense is clearly a majority sense (P > 0.6).",
        "-",
        "- ß'",
        "-",
        "1",
        "OnlySeed – i – ",
        "WFS",
        "BiBoot *",
        "MonoBoot a",
        "Language-Domain Algorithm F-score(%) No.",
        "of tagged words needed to achieve this F-score % Reduction in annotation cost",
        "Hindi-Health",
        "Biboot OnlySeed",
        "Marathi-Health",
        "Hindi-Tourism",
        "Marathi-Tourism Biboot 61.90 OnlySeed 61.68"
      ]
    },
    {
      "heading": "7. Results",
      "text": [
        "The results of our experiments are summarized in Figures 1 to 4.",
        "The a>axis represents the amount of seed data used and the y-axis represents the F-scores obtained.",
        "The different curves in each graph are as follows:",
        "b. MonoBoot.",
        "This curve represents the F-score obtained after 10 iterations by using monolingual bootstrapping with different amounts of seed data.",
        "c. OnlySeed.",
        "This curve represents the F-score obtained by training on the seed data alone without using any bootstrapping.",
        "d. WFS.",
        "This curve represents the F-score obtained by simply selecting the first sense from Wordnet, a typically reported baseline."
      ]
    },
    {
      "heading": "8. Discussions",
      "text": [
        "In this section we discuss the important observations made from Figures 1 to 4.",
        "For small seed sizes, the F-score of bilingual bootstrapping is consistently better than the F-score obtained by training only on the seed data without using any bootstrapping.",
        "This is true for both the languages in both the domains.",
        "Further, bilingual bootstrapping also does better than monolingual bootstrapping for small seed sizes.",
        "As explained earlier, this better performance can be attributed to the fact that in monolingual bootstrapping the algorithm can tag only those instances with high confidence which it has already seen in the training data.",
        "Hence, in successive iterations, very little new information becomes available to the algorithm.",
        "This is clearly evident from the fact that the curve of monolingual bootstrapping (MonoBoot) is always close to the curve of OnlySeed.",
        "The benefit of bilingual bootstrapping is clearly felt for small seed sizes.",
        "However, as the seed size increases the performance of the 3 algorithms, viz., MonoBoot, BiBoot and OnlySeed is more or less the same.",
        "This is intuitive, because, as the seed size increases the algorithm is able to see more and more tagged instances in its self corpora and hence does not need any assistance from the other language.",
        "In other words, the annotated data in Li is not able to add any new information to the training process of L*2 and vice versa.",
        "The performance boost obtained at small seed sizes suggests that bilingual bootstrapping helps to reduce the overall annotation costs for both the languages.",
        "To further illustrate this, we take some sample points from the graph and compare the number of tagged words needed by BiBoot and OnlySeed to reach the same (or nearly the same) F-score.",
        "We present this comparison in Table 6.",
        "The rows for Hindi-Health and Marathi-Health in Table 6 show that when BiBoot is employed we need 1250 tagged words in Hindi and 1750 tagged words in Marathi to attain F-scores of 57.70% and 64.97% respectively.",
        "On the other hand, in the absence of bilingual bootstrapping, (i.e., using Only-Seed) we need 2250 tagged words each in Hindi and Marathi to achieve similar F-scores.",
        "BiBoot thus gives a reduction of 33.33% in the overall annotation cost ( {1250 + 1750} v/s {2250 + 2250}) while achieving similar F-scores.",
        "Similarly, the results for Hindi-Tourism and Marathi-Tourism show that BiBoot gives a reduction of 43.75% in the overall annotation cost while achieving similar F-scores.",
        "Further, since the results of MonoBoot are almost the same as OnlySeed, the above numbers indicate that BiBoot provides a reduction in cost when compared to MonoBoot also.",
        "8.4 Contribution of monosemous words in the performance of BiBoot",
        "As mentioned earlier, monosemous words in the test set are not considered while evaluating the performance of our algorithm but, we add monosemous words to the seed data.",
        "However, we do not count monosemous words while calculating the seed size as there is no manual annotation cost associated with monosemous words (they can be tagged automatically by fetching their singleton sense id from the wordnet).",
        "We observed that the monosemous words of L\\ help in boosting the performance of L2 and vice versa.",
        "This is because for a given monosemous word in L2 (or L\\ respectively) the corresponding cross-linked word in L\\ (or L2 respectively) need not necessarily be monosemous.",
        "In such cases, the cross-linked polysemous word in L2 (or L\\ respectively) benefits from the projected statistics of a monosemous word in L\\ (or L2 respectively).",
        "This explains why BiBoot gives an F-score of 35-52% even at zero seed size even though the F-score of OnlySeed is only 2-5% (see Figures 1 to 4)."
      ]
    },
    {
      "heading": "9. Conclusion",
      "text": [
        "We presented a bilingual bootstrapping algorithm for Word Sense Disambiguation which allows two resource deprived languages to mutually benefit from each other's data via parameter projection.",
        "The algorithm consistently performs better than monolingual bootstrapping.",
        "It also performs better than using only monolingual seed data without using any bootstrapping.",
        "The benefit of bilingual bootstrapping is felt prominently when the seed size in the two languages is very small thus highlighting the usefulness of this algorithm in highly resource constrained scenarios."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We acknowledge the support of Microsoft Research India in the form of an International Travel Grant, which enabled one of the authors (Mitesh M. Khapra) to attend this conference."
      ]
    }
  ]
}
