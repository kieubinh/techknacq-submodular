{
  "info": {
    "authors": [
      "Hongning Wang",
      "Duo Zhang",
      "ChengXiang Zhai"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-1153",
    "title": "Structural Topic Model for Latent Topical Structure Analysis",
    "url": "https://aclweb.org/anthology/P11-1153",
    "year": 2011
  },
  "references": [
    "acl-D07-1031",
    "acl-H05-1042",
    "acl-J06-4002",
    "acl-N03-1030",
    "acl-N09-1042",
    "acl-P03-1071",
    "acl-P05-1046",
    "acl-W00-0405"
  ],
  "sections": [
    {
      "text": [
        "Hongning Wang, Duo Zhang, ChengXiang Zhai",
        "Topic models have been successfully applied to many document analysis tasks to discover topics embedded in text.",
        "However, existing topic models generally cannot capture the latent topical structures in documents.",
        "Since languages are intrinsically cohesive and coherent, modeling and discovering latent topical transition structures within documents would be beneficial for many text analysis tasks.",
        "In this work, we propose a new topic model, Structural Topic Model, which simultaneously discovers topics and reveals the latent topical structures in text through explicitly modeling topical transitions with a latent first-order Markov chain.",
        "Experiment results show that the proposed Structural Topic Model can effectively discover topical structures in text, and the identified structures significantly improve the performance of tasks such as sentence annotation and sentence ordering."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "A great amount of effort has recently been made in applying statistical topic models (Hofmann, 1999; Blei et al., 2003) to explore word co-occurrence patterns, i.e. topics, embedded in documents.",
        "Topic models have become important building blocks of many interesting applications (see e.g., (Blei and Jordan, 2003; Blei and Lafferty, 2007; Mei et al., 2007; Lu and Zhai, 2008)).",
        "In general, topic models can discover word clustering patterns in documents and project each document to a latent topic space formed by such word clusters.",
        "However, the topical structure in a document, i.e., the internal dependency between the topics, is generally not captured due to the exchangeability assumption (Blei et al., 2003), i.e., the document generation probabilities are invariant to content permutation.",
        "In reality, natural language text rarely consists of isolated, unrelated sentences, but rather collocated, structured and coherent groups of sentences (Hovy, 1993).",
        "Ignoring such latent topical structures inside the documents means wasting valuable clues about topics and thus would lead to non-optimal topic modeling.",
        "Taking apartment rental advertisements as an example, when people write advertisements for their apartments, it's natural to first introduce \"size\" and \"address\" of the apartment, and then \"rent\" and \"contact\".",
        "Few people would talk about \"restriction\" first.",
        "If this kind of topical structures are captured by a topic model, it would not only improve the topic mining results, but, more importantly, also help many other document analysis tasks, such as sentence annotation and sentence ordering.",
        "Nevertheless, very few existing topic models attempted to model such structural dependency among topics.",
        "The Aspect HMM model introduced in (Blei and Moreno, 2001) combines pLSA (Hofmann, 1999) with HMM (Rabiner, 1989) to perform document segmentation over text streams.",
        "However, Aspect HMM separately estimates the topics in the training set and depends on heuristics to infer the transitional relations between topics.",
        "The Hidden Topic Markov Model (HTMM) proposed by (Gruber et al., 2007) extends the traditional topic models by assuming words in each sentence share the same topic assignment, and topics transit between adjacent sentences.",
        "However, the transitional structures among topics, i.e., how likely one topic would follow another topic, are not captured in this model.",
        "In this paper, we propose a new topic model, named Structural Topic Model (strTM) to model and analyze both latent topics and topical structures in text documents.",
        "To do so, strTM assumes: 1) words in a document are either drawn from a content topic or a functional (i.e., background) topic; 2) words in the same sentence share the same content topic; and 3) content topics in the adjacent sentences follow a topic transition that satisfies the first order Markov property.",
        "The first assumption distinguishes the semantics of the occurrence of each word in the document, the second requirement confines the unrealistic \"bag-of-word\" assumption into a tighter unit, and the third assumption exploits the connection between adjacent sentences.",
        "To evaluate the usefulness of the identified topical structures by strTM, we applied strTM to the tasks of sentence annotation and sentence ordering, where correctly modeling the document structure is crucial.",
        "On the corpus of 8,031 apartment advertisements from craiglist (Grenager et al., 2005) and 1,991 movie reviews from IMDB (Zhuang et al., 2006), strTM achieved encouraging improvement in both tasks compared with the baseline methods that don't explicitly model the topical structure.",
        "The results confirm the necessity of modeling the latent topical structures inside documents, and also demonstrate the advantages of the proposed strTM over existing topic models."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Topic models have been successfully applied to many problems, e.g., sentiment analysis (Mei et al., 2007), document summarization (Lu and Zhai, 2008) and image annotation (Blei and Jordan, 2003).",
        "However, in most existing work, the dependency among the topics is loosely governed by the prior topic distribution, e.g., Dirichlet distribution.",
        "Some work has attempted to capture the interrelationship among the latent topics.",
        "Correlated Topic Model (Blei and Lafferty, 2007) replaces Dirichlet prior with logistic Normal prior for topic distribution in each document in order to capture the correlation between the topics.",
        "HMM-LDA (Griffiths et al., 2005) distinguishes the short-range syntactic dependencies from long-range semantic dependencies among the words in each document.",
        "But in",
        "HMM-LDA, only the latent variables for the syntactic classes are treated as a locally dependent sequence, while latent topics are treated the same as in other topic models.",
        "Chen et al.",
        "introduced the generalized Mallows model to constrain the latent topic assignments (Chen et al., 2009).",
        "In their model, they assume there exists a canonical order among the topics in the collection of related documents and the same topics are forced not to appear in disconnected portions of the topic sequence in one document (sampling without replacement).",
        "Our method relaxes this assumption by only postulating transitional dependency between topics in the adjacent sentences (sampling with replacement) and thus potentially allows a topic to appear multiple times in disconnected segments.",
        "As discussed in the previous section, HTMM (Gruber et al., 2007) is the most similar model to ours.",
        "HTMM models the document structure by assuming words in the same sentence share the same topic assignment and successive sentences are more likely to share the same topic.",
        "However, HTMM only loosely models the transition between topics as a binary relation: the same as the previous sentence's assignment or draw a new one with a certain probability.",
        "This simplified coarse modeling of dependency could not fully capture the complex structure across different documents.",
        "In contrast, our strTM model explicitly captures the regular topic transitions by postulating the first order Markov property over the topics.",
        "Another line of related work is discourse analysis in natural language processing: discourse segmentation (Sun et al., 2007; Galley et al., 2003) splits a document into a linear sequence of multi-paragraph passages, where lexical cohesion is used to link together the textual units; discourse parsing (Soricut and Marcu, 2003; Marcu, 1998) tries to uncover a more sophisticated hierarchical coherence structure from text to represent the entire discourse.",
        "One work in this line that shares a similar goal as ours is the content models (Barzilay and Lee, 2004), where an HMM is defined over text spans to perform information ordering and extractive summarization.",
        "A deficiency of the content models is that the identification of clusters of text spans is done separately from transition modeling.",
        "Our strTM addresses this deficiency by defining a generative process to simultaneously capture the topics and the transitional relationship among topics: allowing topic modeling and transition modeling to reinforce each other in a principled framework."
      ]
    },
    {
      "heading": "3. Structural Topic Model",
      "text": [
        "In this section, we formally define the Structural Topic Model (strTM) and discuss how it captures the latent topics and topical structures within the documents simultaneously.",
        "From the theory of linguistic analysis (Kamp, 1981), we know that document exhibits internal structures, where structural segments encapsulate semantic units that are closely related.",
        "In strTM, we treat a sentence as the basic structure unit, and assume all the words in a sentence share the same topical aspect.",
        "Besides, two adjacent segments are assumed to be highly related (capturing cohesion in text); specifically, in strTM we pose a strong transitional dependency assumption among the topics: the choice of topic for each sentence directly depends on the previous sentence's topic assignment, i.e., first order Markov property.",
        "Moveover, taking the insights from HMM-LDA that not all the words are content conveying (some of them may just be a result of syntactic requirement), we introduce a dummy functional topic zB for every sentence in the document.",
        "We use this functional topic to capture the document-independent word distribution, i.e., corpus background (Zhai et al., 2004).",
        "As a result, in strTM, every sentence is treated as a mixture of content and functional topics.",
        "Formally, we assume a corpus consists of D documents with a vocabulary of size V, and there are k content topics embedded in the corpus.",
        "In a given document d, there are m sentences and each sentence i has Ni words.",
        "We assume the topic transition probability p(z\\zr) is drawn from a Multinomial distribution Mul(az'), and the word emission probability under each topic p(w\\z) is drawn from a Multinomial distribution Mul( ßz ).",
        "To get a unified description of the generation process, we add another dummy topic T-START in strTM, which is the initial topic with position for every document but does not emit any words.",
        "In addition, since our functional topic is assumed to occur in all the sentences, we don't need to model its transition with other content topics.",
        "We use a Binomial variable n to control the proportion between content and functional topics in each sentence.",
        "Therefore, there are k+1 topic transitions, one for T-START and others for k content topics; and k emission probabilities for the content topics, with an additional one for the functional topic zB (in total k+1 emission probability distributions).",
        "Conditioned on the model parameters 6 = (a,ß,n), the generative process of a document in strTM can be described as follows:"
      ]
    },
    {
      "heading": "1.. For each sentence s i in document d:",
      "text": [
        "(a) Draw topic zi from Multinomial distribution conditioned on the previous sentence si-i's topic assignment zi-\\:",
        "(b) Draw each word wij in sentence si from the mixture of content topic zi and functional topic zB :",
        "The joint probability of sentences and topics in one document defined by strTM is thus given by:",
        "where the topic to sentence emission probability is defined as:",
        "This process is graphically illustrated in Figure 1.",
        "From the definition of strTM, we can see that the document structure is characterized by a document-specific topic chain, and forcing the words in one sentence to share the same content topic ensures semantic cohesion of the mined topics.",
        "Although we do not directly model the topic mixture for each document as the traditional topic models do, the word co-occurrence patterns within the same document are captured by topic propagation through the transitions.",
        "This can be easily understood when we write down the posterior probability of the topic assignment for a particular sentence:",
        "In the E-Step of EM algorithm, we need to collect the expected count of a sequential topic pair (z, z) and a topic-word pair (z, w) to update the model parameters a and ß in the M-Step.",
        "In strTM, E[c(z, z')\\ can be easily calculated by forward-backward algorithm.",
        "But we have to go one step further to fetch the required sufficient statistics for E[c(z,w)], because our emission probabilities are defined over sentences.",
        "Through forward-backward algorithm, we can get the posterior probability p(si,z\\d, 6).",
        "In strTM, words in one sentence are independently drawn from either a specific content topic z or functional topic zB according to the mixture weight n. Therefore, we can accumulate the expected count of (z, w) over all the sentences by:",
        "The first part of Eq(3) describes the recursive influence on the choice of topic for the it/î sentence from its preceding sentences, while the second part captures how the succeeding sentences affect the current topic assignment.",
        "Intuitively, when we need to decide a sentence's topic, we will look \"backward\" and \"forward\" over all the sentences in the document to determine a \"suitable\" one.",
        "In addition, because of the first order Markov property, the local topical dependency gets more emphasis, i.e., they are interacting directly through the transition probabilities p(zi\\zi-i) andp(zi+i\\zi).",
        "And suchinteraction on sentences farther away would get damped by the multiplication of such probabilities.",
        "This result is reasonable, especially in a long document, since neighboring sentences are more likely to cover similar topics than two sentences far apart."
      ]
    },
    {
      "heading": "4. Posterior Inference and Parameter Estimation",
      "text": [
        "The chain structure in strTM enables us to perform exact inference: posterior distribution can be efficiently calculated by the forward-backward algorithm, the optimal topic sequence can be inferred using the Viterbi algorithm, and parameter estimation can be solved by the Expectation Maximization (EM) algorithm.",
        "More technical details can be found in (Rabiner, 1989).",
        "In this section, we only discuss strTM-specific procedures.",
        "where c(w, s) indicates the frequency of word w in sentence s.",
        "Eq(4) can be easily explained as follows.",
        "Since we already observe topic z and sentence s co-occur with probability p(s, z d, 6), each word w in s should share the same probability of being observed with content topic z.",
        "Thus the expected count of c(z, w) in this sentence would be p(s, z d, 6)c(w, s).",
        "However, since each sentence is also associated with the functional topic zB, the word w may also be drawn from zB.",
        "By applying the Bayes' rule, we can properly reallocate the expected count of c(z, w) by Eq(4).",
        "The same strategy can be applied to obtain E[c(zB, w)].",
        "As discussed in (Johnson, 2007), to avoid the problem that EM algorithm tends to assign a uniform word/state distribution to each hidden state, which deviates from the heavily skewed word/state distributions empirically observed, we can apply a Bayesian estimation approach for strTM.",
        "Thus we introduce prior distributions over the topic transition Mul(azi) and emission probabilities Mul(ßz), and use the Variational Bayesian (VB) (Jordan et al., 1999) estimator to obtain a model with more skewed word/state distributions.",
        "Since both the topic transition and emission probabilities are Multinomial distributions in strTM, the conjugate Dirichlet distribution is the natural choice for imposing a prior on them (Diaconis and Ylvisaker, 1979).",
        "Thus, we further assume:",
        "where we use exchangeable Dirichlet distributions to control the sparsity of az and ßz.",
        "As rj and 7 approach zero, the prior strongly favors the models in which each hidden state emits as few words/states as possible.",
        "In our experiments, we empirically tuned r and 7 on different training corpus to optimize log-likelihood.",
        "The resulting VB estimation only requires a minor modification to the M-Step in the original EM algorithm:",
        "labels (We didn't use the additional opinion annotations in this data set.)",
        ", e.g., VP (vision effects), MS (music and sound effects), etc., also on the sentences, but the annotations in the review data set is much sparser than that in the Ads data set (see in Table 1).",
        "The sentence-level annotations make it possible to quantitatively evaluate the discovered topic structures.",
        "We performed simple preprocessing on these two data sets: 1) removed a standard list of stop words, terms occurring in less than 2 documents; 2) discarded the documents with less than 2 sentences; 3) aggregated sentence-level annotations into document-level labels (binary vector) for each document.",
        "Table 1 gives a brief summary on these two data sets after the processing.",
        "where $(x) is the exponential of the first derivative of the log-gamma function.",
        "The optimal setting of n for the proportion of content topics in the documents is empirically tuned by cross-validation over the training corpus to maximize the log-likelihood."
      ]
    },
    {
      "heading": "5. Experimental Results",
      "text": [
        "In this section, we demonstrate the effectiveness of strTM in identifying latent topical structures from documents, and quantitatively evaluate how the mined topic transitions can help the tasks of sentence annotation and sentence ordering.",
        "We used two different data sets for evaluation: apartment advertisements (Ads) from (Grenager et al., 2005) and movie reviews (Review) from (Zhuang et al., 2006).",
        "The Ads data consists of 8,767 advertisements for apartment rentals crawled from Craigslist website.",
        "302 of them have been labeled with 11 fields, including size, feature, address, etc., on the sentence level.",
        "The review data contains 2,000 movie reviews discussing 11 different movies from IMDB.",
        "These reviews are manually labeled with 12 movie feature *Only in 302 labeled ads Table 1: Summary of evaluation data set",
        "Topic Transition Modeling",
        "First, we qualitatively demonstrate the topical structure identified by strTM from Ads data.",
        "We trained strTM with 11 content topics in Ads data set, used word distribution under each class (estimated by maximum likelihood estimator on document-level labels) as priors to initialize the emission probability Mul(ßz) in Eq(6), and treated document-level labels as the prior for transition from T-START in each document, so that the mined topics can be aligned with the predefined class labels.",
        "Figure 2 shows the identified topics and the transitions among them.",
        "To get a clearer view, we discarded the transitions below a threshold of 0.1 and removed all the isolated nodes.",
        "From Figure 2, we can find some interesting topical structures.",
        "For example, people usually start with \"size\", \"features\" and \"address\", and end with \"contact\" information when they post an apartset.",
        "Ads",
        "Review",
        "Document Size",
        "8,031",
        "1,991",
        "Vocabulary Size",
        "21,993",
        "14,507",
        "Avg Stn/Doc",
        "8.0",
        "13.9",
        "Avg Labeled Stn/Doc",
        "7.1*",
        "5.1",
        "Avg Token/Stn",
        "14.1",
        "20.0",
        "neighborhood",
        "NUM bedroom bath room large",
        "http photos",
        "click pictures",
        "water garbage included",
        "paid utilities parking kitchen room laundry storage close shopping transportation location",
        "TELEPHONE appointment information contact email deposit month lease rent year",
        "pets kitchen",
        "cat negotiate smoking",
        "ment ads.",
        "Also, we can discover a strong transition from \"size\" to \"features\".",
        "This intuitively makes sense because people usually write \"it's a two bedrooms apartment\" first, and then describe other \"features\" about the apartment.",
        "The mined topics are also quite meaningful.",
        "For example, \"restrictions\" are usually put over pets and smoking, and parking and laundry are always the major \"features\" of an apartment.",
        "To further quantitatively evaluate the estimated topic transitions, we used Kullback-Leibler (KL) divergency between the estimated transition matrix and the \"ground-truth\" transition matrix as the metric.",
        "Each element of the \"ground-truth\" transition matrix was calculated by Eq(9), where c(z, z) denotes how many sentences annotated by z' immediately precede one annotated by z. ö is a smoothing factor, and we fixed it to 0.01 in the experiment.",
        "truth over all the topics:",
        "The KL divergency between two transition matrices is defined in Eq(10).",
        "Because we have a k x k transition matrix (Tstart is not included), we calculated the average KL divergency against the groundwhere p(z\\z') is the ground-truth transition probability estimated by Eq(9), and p(z\\z') is the transition probability given by the model.",
        "We used pLSA (Hofmann, 1999), latent permutation model (lPerm) (Chen et al., 2009) and HTMM (Gruber et al., 2007) as the baseline methods for the comparison.",
        "Because none of these three methods can generate a topic transition matrix directly, we extended them a little bit to achieve this goal.",
        "For pLSA, we used the document-level labels as priors for the topic distribution in each document, so that the estimated topics can be aligned with the predefined class labels.",
        "After the topics were estimated, for each sentence we selected the topic that had the highest posterior probability to generate the sentence as its class label.",
        "For lPerm and HTMM, we used Kuhn-Munkres algorithm (Lovasz and Plummer, 1986) to find the optimal topic-to-class alignment based on the sentence-level annotations.",
        "After the sentences were annotated with class labels, we estimated the topic transition matrices for all of these three methods by Eq(9).",
        "Since only a small portion of sentences are annotated in the Review data set, very few neighboring sentences are annotated at the same time, which introduces many noisy transitions.",
        "As a result, we only performed the comparison on the Ads data set.",
        "The \"ground-truth\" transition matrix was estimated based on all the 302 annotated ads.",
        "In Table 2, the p-value was calculated based on t-test of the KL divergency between each topic's transition probability against strTM.",
        "From the results, we can see that avgKL of strTM is smaller than the other three baseline methods, which means the estimated transitional relation by strTM is much closer to the ground-truth transition.",
        "This demonstrates that strTM captures the topical structure well, compared with other baseline methods.",
        "In this section, we demonstrate how the identified topical structure can benefit the task of sentence annotation.",
        "Sentence annotation is one step beyond the traditional document classification task: in sentence annotation, we want to predict the class label for each sentence in the document, and this will be helpful for other problems, including extractive summarization and passage retrieval.",
        "However, the lack of detailed annotations on sentences greatly limits the effectiveness of the supervised classification methods, which have been proved successful on document classifications.",
        "In this experiment, we propose to use strTM to address this annotation task.",
        "One advantage of strTM is that it captures the topic transitions on the sentence level within documents, which provides a regularization over the adjacent predictions.",
        "To examine the effectiveness of such structural regularization, we compared strTM with four baseline methods: pLSA, lPerm, HTMM and Naive Bayes model.",
        "The sentence labeling approaches for strTM, pLSA, lPerm and HTMM have been discussed in the previous section.",
        "As for Naive Bayes model, we used EM algorithm with both labeled and unlabeled data for the training purpose (we used the same unigram features as in topics models).",
        "We set weights for the unlabeled data to be 10-3 in Naive Bayes with EM.",
        "The comparison was performed on both data sets.",
        "We set the size of topics in each topic model equal to the number of classes in each data set accordingly.",
        "To tackle the situation where some sentences in the document are not strictly associated with any classes, we introduced an additional NULL content topic in all the topic models.",
        "During the training phase, none of the methods used the sentence-level annotations in the documents, so that we treated the whole corpus as the training and testing set.",
        "To evaluate the prediction performance, we calculated accuracy, recall and precision based on the correct predictions over the sentences, and averaged over all the classes as the criterion.",
        "Annotation performance on the two data sets is shown in Table 3 and Table 4.",
        "We can see that strTM outperformed all the other baseline methods on most of the metrics: strTM has the best accuracy and recall on both of the two data sets.",
        "The improvement confirms our hypothesis that besides solely depending on the local word patterns to perform predictions, adjacent sentences provide a structural regularization in strTM (see Eq(3)).",
        "Compared with lPerm, which postulates a strong constrain over the topic assignment (sampling without replacement), strTM performed much better on both of these two data sets.",
        "This validates the benefit of modeling local transitional relation compared with the global ordering.",
        "Besides, strTM achieved over 46% accuracy improvement compared with the second best HTMM in the review data set.",
        "This result shows the advantage of explicitly modeling the topic transitions between neighbor sentences instead of using a binary relation to do so as in HTMM.",
        "pLSA+prior",
        "lPerm",
        "HTMM",
        "strTM",
        "avgKL",
        "0.743",
        "1.101",
        "0.572",
        "0.372",
        "p-value",
        "0.023",
        "1e-4",
        "0.007",
        "-",
        "Model",
        "Accuracy",
        "Recall",
        "Precison",
        "pLSA+prior",
        "0.432",
        "0.649",
        "0.457",
        "lPerm",
        "0.610",
        "0.514",
        "0.471",
        "HTMM",
        "0.606",
        "0.588",
        "0.443",
        "NB+EM",
        "0.528",
        "0.337",
        "0.612",
        "strTM",
        "0.747",
        "0.674",
        "0.620",
        "le 3: Sentence annotation performance on Ads",
        "Model",
        "Accuracy",
        "Recall",
        "Precison",
        "pLSA+prior",
        "0.342",
        "0.278",
        "0.250",
        "lPerm",
        "0.286",
        "0.205",
        "0.184",
        "HTMM",
        "0.369",
        "0.131",
        "0.149",
        "NB+EM",
        "0.341",
        "0.354",
        "0.431",
        "strTM",
        "0.541",
        "0.398",
        "0.323",
        "To further testify how the identified topical structure can help the sentence annotation task, we first randomly removed 100 annotated ads from the training corpus and used them as the testing set.",
        "Then, we used the ground-truth topic transition matrix estimated from the training data to order those 100 ads according to their fitness scores under the ground-truth topic transition matrix, which is defined in Eq(11).",
        "We tested the prediction accuracy of different models over two different partitions, top 50 and bottom 50, according to this order.",
        "where ti is the class label for ith sentence in document d, d is the number of sentences in document d, and f>(ti\\ti-\\) is the transition probability estimated by Eq(9).",
        "The results are shown in Table 5.",
        "From this table, we can find that when the testing documents follow the regular patterns as in the training data, i.e., top 50 group, strTM performs significantly better than the other methods; when the testing documents don't share such structure, i.e., bottom 50 group, strTM's performance drops.",
        "This comparison confirms that when a testing document shares similar topic structure as the training data, the topical transitions captured by strTM can help the sentence annotation task a lot.",
        "In contrast, because pLSA and Naive Bayes don't depend on the document's structure, their performance does not change much over these two partitions.",
        "In this experiment, we illustrate how the learned topical structure can help us better arrange sentences in a document.",
        "Sentence ordering, or text planning, is essential to many text synthesis applications, including multi-document summarization (Goldstein et al., 2000) and concept-to-text generation (Barzilay and",
        "Lapata, 2005).",
        "In strTM, we evaluate all the possible orderings of the sentences in a given document and selected the optimal one which gives the highest generation probability:",
        "where a(m) is a permutation of 1 to m, and a[i] is the ith element in this permutation.",
        "To quantitatively evaluate the ordering result, we treated the original sentence order (OSO) as the perfect order and used Kendall's t(a) (Lapata, 2006) as the evaluation metric to compute the divergency between the optimum ordering given by the model and OSO.",
        "Kendall's t(a) is widely used in information retrieval domain to measure the correlation between two ranked lists and it indicates how much an ordering differs from OSO, which ranges from 1 (perfect matching) to 1 (totally mismatching).",
        "Since only the HTMM and lPerm take the order of sentences in the document into consideration, we used them as the baselines in this experiment.",
        "We ranked OSO together with candidate permutations according to the corresponding model's generation probability.",
        "However, when the size of documents becomes larger, it's infeasible to permutate all the orderings, therefore we randomly permutated 200 possible orderings of sentences as candidates when there were more than 200 possible candidates.",
        "The 2bedroom lbath in very nice complex!",
        "Pool, 2bedroom lbath in very nice complex!",
        "Pool, car-carport, laundry facilities!!",
        "Call Don (650)207- port, laundry facilities!!",
        "Great location!!",
        "Also 5769 to see!",
        "Great location!!",
        "Also available, available, 2bed.2bath for $1275 in same complex.",
        "2bed.2bath for $1275 in same complex.",
        "Call Don (650)207-5769 to see!",
        "Top 50",
        "p-value",
        "Bot 50",
        "p-value",
        "pLSA+prior",
        "0.496",
        "4e-12",
        "0.542",
        "0.004",
        "lPerm",
        "0.669",
        "0.003",
        "0.505",
        "8e-4",
        "HTMM",
        "0.683",
        "0.004",
        "0.579",
        "0.003",
        "NB + EM",
        "0.492",
        "1e-12",
        "0.539",
        "0.002",
        "strTM",
        "0.752",
        "-",
        "0.644",
        "-",
        "2 bedrooms l bath + a famyly room in a cul-de- 2 bedrooms l bath + a famyly room in a cul-desac location.",
        "Please drive by and call Marilyn for ==^> sac location.",
        "Address: 517 Price Way, Vallejo.",
        "No appointment 650-652-5806.",
        "Address: 517 Price Pets Please!",
        "Please drive by and call Marilyn for",
        "Way, Vallejo.",
        "No Pets Please!",
        "appointment 650-652-5806.",
        "experiment was performed on both data sets with 80% data for training and the other 20% for testing.",
        "We calculated the t(a) of all these models for each document in the two data sets and visualized the distribution of t(a) in each data set with histogram in Figure 3.",
        "From the results, we could observe that strTM's t(a) is more skewed towards the positive range (with mean 0.619 in Ads data set and 0.398 in review data set) than lPerm's results (with mean 0.566 in Ads data set and 0.08 in review data set) and HTMM's results (with mean 0.332 in Ads data set and 0.286 in review data set).",
        "This indicates that strTM better captures the internal structure within the documents.",
        "(b) Review",
        "We see that all methods performed better on the Ads data set than the review data set, suggesting that the topical structures are more coherent in the Ads data set than the review data.",
        "Indeed, in the Ads data, strTM perfectly recovered 52.9% of the original sentence order.",
        "When examining some mismatched results, we found that some of them were due to an \"outlier\" order given by the original document (in comparison to the \"regular\" patterns in the set).",
        "In Table 6, we show two such examples where we see the learned structure \"suggested\" to move the contact information to the end, which intuitively gives us a more regular organization of the ads.",
        "It's hard to say that in this case, the system's ordering is inferior to that of the original; indeed, the system order is arguably more natural than the original order."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "In this paper, we proposed a new structural topic model (strTM) to identify the latent topical structure in documents.",
        "Different from the traditional topic models, in which exchangeability assumption precludes them to capture the structure of a document, strTM captures the topical structure explicitly by introducing transitions among the topics.",
        "Experiment results show that both the identified topics and topical structure are intuitive and meaningful, and they are helpful for improving the performance of tasks such as sentence annotation and sentence ordering, where correctly recognizing the document structure is crucial.",
        "Besides, strTM is shown to outperform not only the baseline topic models that fail to model the dependency between the topics, but also the semi-supervised Naive Bayes model for the sentence annotation task.",
        "Our work can be extended by incorporating richer features, such as named entity and co-reference, to enhance the model's capability of structure finding.",
        "Besides, advanced NLP techniques for document analysis, e.g., shallow parsing, may also be used to further improve structure finding."
      ]
    },
    {
      "heading": "7. Acknowledgments",
      "text": [
        "We thank the anonymous reviewers for their useful comments.",
        "This material is based upon work supported by the National Science Foundation under Grant Numbers IIS-0713581 and CNS-0834709, and NASA grant NNX08AC35A."
      ]
    }
  ]
}
