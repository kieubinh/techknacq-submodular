{
  "info": {
    "authors": [
      "Nikhil Garg",
      "James B. Henderson"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-2003",
    "title": "Temporal Restricted Boltzmann Machines for Dependency Parsing",
    "url": "https://aclweb.org/anthology/P11-2003",
    "year": 2011
  },
  "references": [
    "acl-D07-1097",
    "acl-D07-1099",
    "acl-H05-1066",
    "acl-N10-1091",
    "acl-P07-1080",
    "acl-P08-1108",
    "acl-W04-2407",
    "acl-W06-2933",
    "acl-W08-2121",
    "acl-W08-2122",
    "acl-W08-2123",
    "acl-W09-1201",
    "acl-W09-1210"
  ],
  "sections": [
    {
      "text": [
        "James Henderson",
        "We propose a generative model based on Temporal Restricted Boltzmann Machines for transition based dependency parsing.",
        "The parse tree is built incrementally using a shift-reduce parse and an RBM is used to model each decision step.",
        "The RBM at the current time step induces latent features with the help of temporal connections to the relevant previous steps which provide context information.",
        "Our parser achieves labeled and unlabeled attachment scores of 88.72% and 91.65% respectively, which compare well with similar previous models and the state-of-the-art."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "There has been significant interest recently in machine learning methods that induce generative models with high-dimensional hidden representations, including neural networks (Bengio et al., 2003; Col-lobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006).",
        "In this paper, we investigate how these models can be applied to dependency parsing.",
        "We focus on Shift-Reduce transition-based parsing proposed by Nivre et al.",
        "(2004).",
        "In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree.",
        "To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b).",
        "In a related earlier work, Ratnaparkhi (1999) proposed a maximum entropy model for transition-based constituency parsing.",
        "Of these approaches, only ISBNs induce high-dimensional latent representations to encode parse history, but suffer from either very approximate or slow inference procedures.",
        "We propose to address the problem of inference in a high-dimensional latent space by using an undirected graphical model, Restricted Boltzmann Machines (RBMs), to model the individual parsing decisions.",
        "Unlike the Sigmoid Belief Networks (SBNs) used in ISBNs, RBMs have tractable inference procedures for both forward and backward reasoning, which allows us to efficiently infer both the probability of the decision given the latent variables and vice versa.",
        "The key structural difference between the two models is that the directed connections between latent and decision vectors in SBNs become undirected in RBMs.",
        "A complete parsing model consists of a sequence of RBMs interlinked via directed edges, which gives us a form of Temporal Restricted Boltzmann Machines (TRBM) (Taylor et al., 2007), but with the incrementally specified model structure required by parsing.",
        "In this paper, we analyze and contrast ISBNs with TRBMs and show that the latter provide an accurate and theoretically sound model for parsing with high-dimensional latent variables."
      ]
    },
    {
      "heading": "2. An ISBN Parsing Model",
      "text": [
        "Our TRBM parser uses the same history-based probability model as the ISBN parser of Titov and Henderson (2007b): P(tree) = TltP^lv,v*_1), where each",
        "Figure 1: An ISBN network.",
        "Shaded nodes represent decision variables and 'H' represents a vector of latent variables.",
        "W^H denotes the weight matrix for directed connection of type c between two latent vectors.",
        "v* is a parser decision of the type Left-Arc, Right-Arc, Reduce or Shift.",
        "These decisions are further decomposed into sub-decisions, as for example P{Left-Arc\\yl,v*_1)P(Label|Ze/î-^«;, v, ...,v*_1) The TRBMs and ISBNs model these probabilities.",
        "In the ISBN model shown in Figure 1, the decisions are shown as boxes and the sub-decisions as shaded circles.",
        "At each decision step, the ISBN model also includes a vector of latent variables, denoted by 'H', which act as latent features of the parse history.",
        "As explained in (Titov and Henderson, 2007b), the temporal connections between latent variables are constructed to take into account the structural locality in the partial dependency structure.",
        "The model parameters are learned by back-propagating likelihood gradients.",
        "Because decision probabilities are conditioned on the history, once a decision is made the corresponding variable becomes observed, or visible.",
        "In an ISBN, the directed edges to these visible variables and the large numbers of heavily interconnected latent variables make exact inference of decision probabilities intractable.",
        "Titov and Henderson (2007a) proposed two approximation procedures for inference.",
        "The first was a feed forward approximation where latent variables were allowed to depend only on their parent variables, and hence did not take into account the current or future observations.",
        "Due to this limitation, the authors proposed to make latent variables conditionally dependent also on a set of explicit features derived from the parsing history, specifically, the base features denned in (Nivre et al., 2006b).",
        "As shown in our experiments, this addition results in a big improvement for the parsing task.",
        "The second approximate inference procedure, called the incremental mean field approximation, extended the feed-forward approximation by updating the current time step's latent variables after each sub-decision.",
        "Although this approximation is more accurate than the feed-forward one, there is no analytical way to maximize likelihood w.r.t.",
        "the means of the latent variables, which requires an iterative numerical method and thus makes inference very slow, restricting the model to only shorter sentences."
      ]
    },
    {
      "heading": "3. Temporal Restricted Boltzmann Machines",
      "text": [
        "In the proposed TRBM model, RBMs provide an analytical way to do exact inference within each time step.",
        "Although information passing between time steps is still approximated, TRBM inference is more accurate than the ISBN approximations.",
        "An RBM is an undirected graphical model with a set of binary visible variables v, a set of binary latent variables h, and a weight matrix W for bipartite connections between v and h. The probability of an RBM configuration is given by: p{\\, h) = (l/Z)e_E(v'h) where Z is the partition function and E is the energy function denned as:",
        "where and bj are biases for corresponding visible and latent variables respectively, and Wij is the symmetric weight between Viand hj.",
        "Given the visible variables, the latent variables are conditionally independent of each other, and vice versa:",
        "where a{x) = 1/(1 + e~x) (the logistic sigmoid).",
        "RBM based models have been successfully used in image and video processing, such as Deep Belief Networks (DBNs) for recognition of handwritten digits (Hinton et al., 2006) and TRBMs for modeling motion capture data (Taylor et al., 2007).",
        "Despite their success, RBMs have seen limited use in the NLP community.",
        "Previous work includes RBMs for topic modeling in text documents (Salakhutdinov and Hinton, 2009), and Temporal Factored RBM for language modeling (Mnih and Hinton, 2007).",
        "TRBMs (Taylor et al., 2007) can be used to model sequences where the decision at each step requires some context information from the past.",
        "Figure 2",
        "-W",
        "' UV",
        "HH",
        ",w",
        "' HV",
        "HH",
        ",w",
        "' HV",
        "• • •",
        "• • •",
        "• • •",
        ",w",
        "O O O",
        "Figure 2: Proposed TRBM Model.",
        "Edges with no arrows represent undirected RBM connections.",
        "The directed temporal connections between time steps contribute a bias to the latent layer inference in the current step.",
        "shows our proposed TRBM model with latent to latent connections between time steps.",
        "Each step has an RBM with weights Wrbm composed of smaller weight matrices corresponding to different sub-decisions.",
        "For instance, for the action Left-Arc, Wrbm consists of RBM weights between the latent vector and the sub-decisions: \"Left-Arc\" and \"Label\".",
        "Similarly, for the action Shift, the sub-decisions are \"Shift\", \"Part-of-Speech\" and \"Word\".",
        "The probability distribution of a TRBM is:",
        "where \\\\ denotes the set of visible vectors from time steps 1 to T i.e. v to vT.",
        "The notation for latent vectors h is similar.",
        "h^c) denotes the latent vector in the past time step that is connected to the current latent vector through a connection of type c. To simplify notation, we will denote the past connections {h^, ...,h(c)} by history.",
        "The conditional distribution of the RBM at each time step is given by:",
        "+ TJ3{b3 + TJc>lw^Hih^)h)) where v\\ and denote the ith visible and jth latent variable respectively at time step t. h\\ denotes a latent variable in the past time step, and wHHi denotes the weight of the corresponding connection.",
        "Section 3.1 describes an RBM where visible variables can take binary values.",
        "In our model, similar to (Salakhutdinov et al., 2007), we have multivalued visible variables which we represent as one-hot binary vectors and model via a softmax distribution:",
        "Si expioi + Y^jMjWij)",
        "Latent variable inference is similar to equation 1 with an additional bias due to the temporal connections.",
        "Here, ß denotes the mean of the corresponding latent variable.",
        "To keep inference tractable, we do not do any backward reasoning across directed connections to update //c).",
        "Thus, the inference procedure for latent variables takes into account both the parse history and the current observation, but no future observations.",
        "The limited set of possible values for the visible layer makes it possible to marginalize out latent variables in linear time to compute the exact likelihood.",
        "Let v*(fc) denote a vector with v\\ = 1 and f *(j^fc) = 0.",
        "The conditional probability of a sub-decision is:",
        "where Z = ^ievisibiee^j&^l + eb^).",
        "We actually perform this calculation once for each sub-decision, ignoring the future sub-decisions in that time step.",
        "This is a slight approximation, but avoids having to compute the partition function over all possible combinations of values for all sub-decisions.",
        "The complete probability of a derivation is:",
        "The gradient of an RBM is given by:",
        "where ()d denotes the expectation under distribution d. In general, computing the exact gradient is intractable and previous work proposed a Contrastive Divergence (CD) based learning procedure that approximates the above gradient using only one step reconstruction (Hinton, 2002).",
        "Fortunately, our model has only a limited set of possible visible values, which allows us to use a better approximation by taking the derivative of equation 5 :",
        "'in cases where computing the partition function is still not feasible (for instance, because of a large vocabulary), sampling methods could be used.",
        "However, we did not find this to be necessary.",
        "w",
        "RBM",
        "HH",
        "W",
        "RBM",
        "HH",
        "W",
        "RBM",
        "• • •",
        "• • •",
        "• • •",
        "W",
        "RBM",
        "• • •",
        "d log piy{k^history)",
        "Further, the weights on the temporal connections are learned by back-propagating the likelihood gradients through the directed links between steps.",
        "The back-proped gradient from future time steps is also used to train the current RBM weights.",
        "This back-propagation is similar to the Recurrent TRBM model of Sutskever et al.",
        "(2008).",
        "However, unlike their model, we do not use CD at each step to compute gradients.",
        "We use the same beam-search decoding strategy as used in (Titov and Henderson, 2007b).",
        "Given a derivation prefix, its partial parse tree and associated TRBM, the decoder adds a step to the TRBM for calculating the probabilities of hypothesized next decisions using equation 5.",
        "If the decoder selects a decision for addition to the candidate list, then the current step's latent variable means are inferred using equation 4, given that the chosen decision is now visible.",
        "These means are then stored with the new candidate for use in subsequent TRBM calculations."
      ]
    },
    {
      "heading": "4. Experiments & Results",
      "text": [
        "We used syntactic dependencies from the English section of the CoNLL 2009 shared task dataset (Hajic et al., 2009).",
        "Standard splits of training, development and test sets were used.",
        "To handle word sparsity, we replaced all the (POS, word) pairs with frequency less than 20 in the training set with (POS, UNKNOWN), giving us only 4530 tag-word pairs.",
        "Since our model can work only with projective trees, we used MaltParser (Nivre et al., 2006a) to projec-tivize/deprojectivize the training input/test output.",
        "Table 1 lists the labeled (LAS) and unlabeled (UAS) attachment scores.",
        "Row a shows that a simple ISBN model without features, using feed forward inference procedure, does not work well.",
        "As explained in section 2, this is expected since in the absence of explicit features, the latent variables in a given layer do not take into account the observations in the previous layers.",
        "The huge improvement in performance on adding the features (row b) shows that the feed forward inference procedure for ISBNs relies heavily on these feature connections to compensate for the lack of backward inference.",
        "The TRBM model avoids this problem as the inference procedure takes into account the current observation, which makes the latent variables much more informed.",
        "However, as row c shows, the TRBM model without features falls a bit short of the ISBN performance, indicating that features are indeed a powerful substitute for backward inference in sequential latent variable models.",
        "TRBM models would still be preferred in cases where such feature engineering is difficult or expensive, or where the objective is to compute the latent features themselves.",
        "For a fair comparison, we add the same set of features to the TRBM model (row d) and the performance improves by about 2% to reach the same level (non-significantly better) as ISBN with features.",
        "The improved inference in TRBM does however come at the cost of increased training and testing time.",
        "Keeping the same likelihood convergence criteria, we could train the ISBN in about 2 days and TRBM in about 5 days on a 3.3 GHz Xeon processor.",
        "With the same beam search parameters, the test time was about 1.5 hours for ISBN and about 4.5 hours for TRBM.",
        "Although more code optimization is possible, this trend is likely to remain.",
        "We also tried a Contrastive Divergence based training procedure for TRBM instead of equation 7, but that resulted in about an absolute 10% lower LAS.",
        "Further, we also tried a very simple model without latent variables where temporal connections are between decision variables themselves.",
        "This model gave an LAS of only 60.46%, which indicates that without latent variables, it is very difficult to capture the parse history.",
        "Model",
        "LAS",
        "UAS",
        "a.",
        "ISBN w/o features",
        "38.38",
        "54.52",
        "b.",
        "ISBN w/ features",
        "88.65",
        "91.44",
        "c.",
        "TRBM w/o features",
        "86.01",
        "89.78",
        "d.",
        "TRBM w/ features",
        "88.72",
        "91.65",
        "e.",
        "MST (McDonald et al., 2005)",
        "87.07",
        "89.95",
        "/•",
        "Malt^ (Hall et al., 2007)",
        "85.96",
        "88.64",
        "9-",
        "MSTMalt (Nivre md McDonald, 2008)",
        "87.45",
        "90.22",
        "h.",
        "CoNLL 2008 #1 (Johansson and Nugues, 2008)",
        "90.13",
        "92.45",
        "i.",
        "enSemble^oo<% (SurdeanuandManning,2010)",
        "88.83",
        "91.47",
        "3-",
        "CoNLL 2009 #1 (Bohnet, 2009)",
        "89.88",
        "unknown",
        "Table 1 : LAS and UAS for different models.",
        "For comparison, we also include the performance numbers for some state-of-the-art dependency parsing systems.",
        "Surdeanu and Manning (2010) compare different parsing models using CoNLL 2008 shared task dataset (Surdeanu et al., 2008), which is the same as our dataset.",
        "Rows e – i show the performance numbers of some systems as mentioned in their paper.",
        "Row j shows the best syntactic model in CoNLL 2009 shared task.",
        "The TRBM model has only 1.4% lower LAS and 0.8% lower UAS compared to the best performing model.",
        "We analyzed the latent layers in our models to see if they captured semantic patterns.",
        "A latent layer is a vector of 100 latent variables.",
        "Every Shift operation gives a latent representation for the corresponding word.",
        "We took all the verbs in the development setand partitioned their representations into 50 clusters using the k-means algorithm.",
        "Table 2 shows some partitions for the TRBM model.",
        "The partitions look semantically meaningful but to get a quantitative analysis, we computed pairwise semantic similarity between all word pairs in a given cluster and aggregated this number over all the clusters.",
        "The semantic similarity was calculated using two different similarity measures on the wordnet corpus (Miller et al., 1990): path and lin.",
        "path similarity is a score between 0 and 1, equal to the inverse of the shortest path length between the two word senses, lin similarity (Lin, 1998) is a score between 0 and 1 based on the Information Content of the two word senses and of the Least Common Subsumer.",
        "Table 3 shows the similarity scores.",
        "We observe that TRBM latent representations give a slightly better clustering than ISBN models.",
        "Again, this is because of the fact that the inference procedure in TRBMs takes into account the current observation.",
        "However, at the same time, the similarity numbers for ISBN with features",
        "Table 2: K-means clustering of words according to their TRBM latent representations.",
        "Duplicate words in the same cluster are not shown.",
        "Table 3 : Wordnet similarity scores for clusters given by different models.",
        "are not very low, which shows that features are a powerful way to compensate for the lack of backward inference.",
        "This is in agreement with their good performance on the parsing task."
      ]
    },
    {
      "heading": "5. Conclusions & Future Work",
      "text": [
        "We have presented a Temporal Restricted Boltzmann Machines based model for dependency parsing.",
        "The model shows how undirected graphical models can be used to generate latent representations of local parsing actions, which can then be used as features for later decisions.",
        "The TRBM model for dependency parsing could be extended to a Deep Belief Network by adding one more latent layer on top of the existing one (Hinton et al., 2006).",
        "Furthermore, as done for unlabeled images (Hinton et al., 2006), one could learn high-dimensional features from unlabeled text, which could then be used to aid parsing.",
        "Parser latent representations could also help other tasks such as Semantic Role Labeling (Henderson et al., 2008).",
        "A free distribution of our implementation is available at http : / /cui .",
        "unige .",
        "ch/ ~garg."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was partly funded by Swiss NSF grant 200021_125137 and European Community FP7 grant 216594 (CLASSiC, www.classic-project.org).",
        "Cluster 1",
        "Cluster 2",
        "Cluster 3",
        "Cluster 4",
        "says",
        "needed",
        "pressing",
        "renewing",
        "contends",
        "expected",
        "bridging",
        "cause",
        "adds",
        "encouraged",
        "curing",
        "repeat",
        "insists",
        "allowed",
        "skirting",
        "broken",
        "remarked",
        "thought",
        "tightening",
        "extended",
        "Model",
        "path",
        "lin",
        "ISBN w/o features",
        "0.228",
        "0.381",
        "ISBN w/features",
        "0.366",
        "0.466",
        "TRBM w/o features",
        "0.386",
        "0.487",
        "TRBM w/ features",
        "0.390",
        "0.489"
      ]
    }
  ]
}
