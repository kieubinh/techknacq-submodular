{
  "info": {
    "authors": [
      "Coşkun Mermer",
      "Murat Saraçlar"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-2032",
    "title": "Bayesian Word Alignment for Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/P11-2032",
    "year": 2011
  },
  "references": [
    "acl-C08-1128",
    "acl-C96-2141",
    "acl-D08-1033",
    "acl-D09-1075",
    "acl-J03-1002",
    "acl-J07-2003",
    "acl-J93-2003",
    "acl-N03-1017",
    "acl-N06-1014",
    "acl-N07-1018",
    "acl-N10-1068",
    "acl-P02-1040",
    "acl-P04-1066",
    "acl-P06-2124",
    "acl-P07-1094",
    "acl-P07-2045",
    "acl-P09-1088",
    "acl-W09-1804"
  ],
  "sections": [
    {
      "text": [
        "Co§kun Mermer1,2 Murat Saraclar",
        "BILGEM Electrical and Electronics Eng.",
        "TUBITAK Bogazici University",
        "Gebze 41470 Kocaeli, Turkey Bebek 34342 Istanbul, Turkey",
        "coskun@uekae.tubitak.gov.tr murat.saraclar@boun.edu.tr",
        "In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM).",
        "We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution.",
        "We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points.",
        "We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction (Koehn etal., 2003; Chiang, 2007; Galley et al., 2006).",
        "State-of-the-art word alignment models, such as IBM Models (Brown et al., 1993), HMM (Vogel et al., 1996), and the jointly-trained symmetric HMM (Liang et al., 2006), contain a large number of parameters (e.g., word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables.",
        "The most common method of inference in such models is expectation-maximization (EM) (Dempster et al., 1977) or an approximation to EM when exact EM is intractable.",
        "However, being a maximization (e.g., maximum likelihood (ML) or maximum a posteriori (MAP)) technique, EM is generally prone to local optima and overfitting.",
        "In essence, the alignment distribution obtained via EM takes into account only the most likely point in the parameter space, but does not consider contributions from other points.",
        "Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported.",
        "Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirich-let priors, but they find the MAP solution.",
        "Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008).",
        "Word alignment learning problem was addressed jointly with segmentation learning in Xu et al.",
        "(2009) .",
        "The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling.",
        "However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al., 2008) or by local maximization (Nguyen et al., 2010).",
        "On the other hand,",
        "Chung and Gildea (2009) apply a sparse Dirichlet prior on the multinomial parameters to prevent over-fitting.",
        "They use variational Bayes for inference, but they do not investigate the effect of Bayesian inference to word alignment in isolation.",
        "Recently, Zhao and Gildea (2010) proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters and their inference method is actually stochastic EM (also known as Monte Carlo EM), a ML technique in which sampling is used to approximate the expected counts in the E-step.",
        "Even though they report substantial reductions in alignment error rate, the translation BLEU scores do not improve.",
        "Our approach in this paper is fully Bayesian in which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive, sparse prior.",
        "We develop a Gibbs sampler for alignments under IBM Model 1, which is relevant for the state-of-the-art SMT systems since: (1) Model 1 is used in bootstrapping the parameter settings for EM training of higherorder alignment models, and (2) many state-of-the-art SMT systems use Model 1 translation probabilities as features in their log-linear model.",
        "We evaluate the inferred alignments in terms of the end-to-end translation performance, where we show the results with a variety of input data to illustrate the general applicability of the proposed technique.",
        "To our knowledge, this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance."
      ]
    },
    {
      "heading": "2. Bayesian Inference with IBM Model 1",
      "text": [
        "Given a sentence-aligned parallel corpus (E, F), let ej (fj) denote the i-th (j-th) source (target) word in e (f), which in turn consists of I (J) words and denotes the s-th sentence in E (F).",
        "Each source sentence is also hypothesized to have an additional imaginary \"null\" word eo.",
        "Also let VE (VF) denote the size of the observed source (target) vocabulary.",
        "In Model 1 (Brown et al., 1993), each target word fj is associated with a hidden alignment variable ej whose value ranges over the word positions in the corresponding source sentence.",
        "The set of alignments for a sentence (corpus) is denoted by a (A).",
        "The model parameters consist of a VE x VF table T of word translation probabilities such that te,f = P (f |e).",
        "The joint distribution of the Model-1 variables is given by the following generative model :",
        "In the proposed Bayesian setting, we treat T as a random variable with a prior P(T).",
        "To find a suitable prior for T, we rewrite (2) as:",
        "where in (3) the count variable nej denotes the number of times the source word type e is aligned to the target word type f in the sentence-pair s, and in (4) Nej = s nej.",
        "Since the distribution over (te,f } in (4) is in the exponential family, specifically being a multinomial distribution, we choose the conjugate prior, in this case the Dirichlet distribution, for computational convenience.",
        "For each source word type e, we assume the prior distribution forte = te1 • • • teyF, which is itself a distribution over the target vocabulary, to be a Dirichlet distribution (with its own set of hyperparameters 0e = öe>1 • • • 9eyF) independent from the priors of other source word types:",
        "te ~ Dirichlet(te; 0e) fj |a, e, T ~ Multinomial(fj; te )",
        "We choose symmetric Dirichlet priors identically for all source words e with 6ej = 6 = 0.0001 to obtain a sparse Dirichlet prior.",
        "A sparse prior favors",
        "=nri(te,f )Nefii t",
        "distributions that peak at a single target word and penalizes flatter translation distributions, even for rare words.",
        "This choice addresses the well-known problem in the IBM Models, and more severely in Model 1, in which rare words act as \"garbage collectors\" (Och and Ney, 2003) and get assigned excessively large number of word alignments.",
        "Then we obtain the joint distribution of all (observed + hidden) variables as:",
        "where 0 = 01 • • • 0Ve .",
        "To infer the posterior distribution of the alignments, we use Gibbs sampling (Geman and Ge-man, 1984).",
        "One possible method is to derive the Gibbs sampler from P(E, F, A, T; 0) obtained in (5) and sample the unknowns A and T in turn, resulting in an explicit Gibbs sampler.",
        "In this work, we marginalize out T by:",
        "and obtain a collapsed Gibbs sampler, which samples only the alignment variables.",
        "Using P (E, F, A; 0) obtained in (6), the Gibbs sampling formula for the individual alignments is derived as: where the superscript – j denotes the exclusion of the current value of aj.",
        "The algorithm is given in Table 1.",
        "Initialization of A in Step 1 can be arbitrary, but for faster convergence special initializations have been used, e.g., using the output of EM (Chiang et al., 2010).",
        "Once the Gibbs sampler is deemed to have converged after B burn-in iterations, we collect M samples of A with L iterations in-between to estimate P(A|E, F).",
        "To obtain the Viterbi alignments, which are required for phrase extraction (Koehn et al., 2003), we select for each aj the most frequent value in the M collected samples.",
        "Input: E, F; Output: K samples of A"
      ]
    },
    {
      "heading": "1. Initialize A",
      "text": []
    },
    {
      "heading": "3. for each sentence-pair s in (E, F) do",
      "text": []
    },
    {
      "heading": "7. Sample a new value for a j",
      "text": []
    },
    {
      "heading": "3. Experimental Setup",
      "text": [
        "For Turkish^English experiments, we used the 20K-sentence travel domain BTEC dataset (Kikui et al., 2006) from the yearly IWSLT evaluationsfor training, the CSTAR 2003 test set for development, and the IWSLT 2004 test set for testing.",
        "For Czech^English, we used the 95K-sentence news commentary parallel corpus from the WMT shared task for training, news2008 set for development, news2009 set for testing, and the 438M-word English and 81.7M-word Czech monolingual news corpora for additional language model (LM) training.",
        "For Arabic^English, we used the 65K-sentence LDC2004T18 (news from 2001-2004) for training, the AFP portion of LDC2004T17 (news from 1998, single reference) for development and testing (about 875 sentences each), and the 298M-word English and 215M-word Arabic AFP and Xinhua subsets of the respective Gigaword corpora (LDC2007T07 and",
        "LDC2007T40) for additional LM training.",
        "All language models are 4-gram in the travel domain experiments and 5-gram in the news domain experiments.",
        "For each language pair, we trained standard phrase-based SMT systems in both directions (including alignment symmetrization and log-linear model tuning) using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and ZMERT (Zaidan, 2009) tools and evaluated using BLEU (Papineni et al., 2002).",
        "To obtain word alignments, we used the accompanying Perl code for Bayesian inference and",
        "GIZA++ (Och and Ney, 2003) for EM.",
        "For each translation task, we report two EM estimates, obtained after 5 and 80 iterations (EM-5 and EM-80), respectively; and three Gibbs sampling estimates, two of which were initialized with those two EM Viterbi alignments (GS-5 and GS-80) and a third was initialized naively (GS-N).",
        "Sampling settings were B = 400 for T^E, 4000 for C^E and 8000 for A^E; M = 100, and L = 10.",
        "For reference, we also report the results with IBM Model 4 alignments (M4) trained in the standard bootstrapping regimen of 1H34."
      ]
    },
    {
      "heading": "4. Results",
      "text": [
        "Table 2 compares the BLEU scores of Bayesian inference and EM estimation.",
        "In all translation tasks, Bayesian inference outperforms EM.",
        "The improvement range is from 2.59 (in Turkish-to-English) up to 2.99 (in English-to-Turkish) BLEU points in travel domain and from 0.16 (in English-to-Czech) up to 0.85 (in English-to-Arabic) BLEU points in news domain.",
        "Compared to the state-of-the-art IBM Model 4, the Bayesian Model 1 is better in all travel domain tasks and is comparable or better in the news domain.",
        "Fertility of a source word is defined as the number of target words aligned to it.",
        "Table 3 shows the distribution of fertilities in alignments obtained from different methods.",
        "Compared to EM estimation, including Model 4, the proposed Bayesian inference dramatically reduces \"questionable\" high-fertility (4 < fertility < 7) alignments and almost entirely elim-",
        "Table3: Distributionofinferredalignmentfertilities.",
        "The four blocks of rows from top to bottom correspond to (in order) the total number of source tokens, source tokens with fertilities in the range 4-7, source tokens with fertilities higher than 7, and the maximum observed fertility.",
        "The first language listed is the source in alignment (Section 2).",
        "inates \"excessive\" alignments (fertility > 8).",
        "The number of distinct word-pairs induced by an alignment has been recently proposed as an objective function for word alignment (Bodrumlu et al., 2009).",
        "Small dictionary sizes are preferred over large ones.",
        "Table 4 shows that the proposed inference method substantially reduces the alignment dictionary size, in most cases by more than 50%."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs, data sizes and domains.",
        "As a result of this increase, Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM",
        "Method",
        "TE",
        "ET",
        "CE",
        "EC",
        "AE",
        "EA",
        "EM-5",
        "38.91",
        "26.52",
        "14.62",
        "10.07",
        "15.50",
        "15.17",
        "EM-80",
        "39.19",
        "26.47",
        "14.95",
        "10.69",
        "15.66",
        "15.02",
        "GS-N",
        "41.14",
        "27.55",
        "14.99",
        "10.85",
        "14.64",
        "15.89",
        "GS-5",
        "40.63",
        "27.24",
        "15.45",
        "10.57",
        "16.41",
        "15.82",
        "GS-80",
        "41.78",
        "29.51",
        "15.01",
        "10.68",
        "15.92",
        "16.02",
        "M4",
        "39.94",
        "27.47",
        "15.47",
        "11.15",
        "16.46",
        "15.43",
        "Method",
        "TE",
        "ET",
        "CE",
        "EC",
        "AE",
        "EA",
        "All",
        "140K",
        "183K",
        "1.63M",
        "1.78M",
        "1.49M",
        "1.82M",
        "EM-80",
        "5.07K",
        "2.91K",
        "52.9K",
        "45.0K",
        "69.1K",
        "29.4K",
        "M4",
        "5.35K",
        "3.10K",
        "36.8K",
        "36.6K",
        "55.6K",
        "36.5K",
        "GS-80",
        "755",
        "419",
        "14.0K",
        "10.9K",
        "47.6K",
        "18.7K",
        "EM-80",
        "426",
        "227",
        "10.5K",
        "18.6K",
        "21.4K",
        "24.2K",
        "M4",
        "81",
        "163",
        "2.57K",
        "10.6K",
        "9.85K",
        "21.8K",
        "GS-80",
        "1",
        "1",
        "39",
        "110",
        "689",
        "525",
        "EM-80",
        "24",
        "24",
        "28",
        "30",
        "44",
        "46",
        "M4",
        "9",
        "9",
        "9",
        "9",
        "9",
        "9",
        "GS-80",
        "8",
        "8",
        "13",
        "18",
        "20",
        "19",
        "Method",
        "TE",
        "ET",
        "CE",
        "EC",
        "AE",
        "EA",
        "EM-80",
        "52.5K",
        "38.5K",
        "440K",
        "461K",
        "383K",
        "388K",
        "M4",
        "57.6K",
        "40.5K",
        "439K",
        "441K",
        "422K",
        "405K",
        "GS-80",
        "23.5K",
        "25.4K",
        "180K",
        "209K",
        "158K",
        "176K",
        "Model 4.",
        "The proposed method learns a compact, sparse translation distribution, overcoming the well-known \"garbage collection\" problem of rare words in EM-estimated current models."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Murat Saraclar is supported by the TUBA-GEB^award."
      ]
    }
  ]
}
