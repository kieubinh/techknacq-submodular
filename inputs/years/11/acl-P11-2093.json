{
  "info": {
    "authors": [
      "Graham Neubig",
      "Yosuke Nakata",
      "Shinsuke Mori"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-2093",
    "title": "Pointwise Prediction for Robust, Adaptable Japanese Morphological Analysis",
    "url": "https://aclweb.org/anthology/P11-2093",
    "year": 2011
  },
  "references": [
    "acl-C00-1004",
    "acl-C04-1067",
    "acl-C08-1113",
    "acl-C94-1032",
    "acl-D08-1112",
    "acl-I08-7018",
    "acl-P02-1064",
    "acl-P07-1007",
    "acl-P09-1058",
    "acl-P10-1037",
    "acl-W04-3230",
    "acl-W04-3236",
    "acl-W07-1516",
    "acl-W10-0104"
  ],
  "sections": [
    {
      "text": [
        "Graham Neubig, Yosuke Nakata, Shinsuke Mori",
        "Graduate School of Informatics, Kyoto University Yoshida Honmachi, Sakyo-ku, Kyoto, Japan",
        "We present a pointwise approach to Japanese morphological analysis (MA) that ignores structure information during learning and tagging.",
        "Despite the lack of structure, it is able to outperform the current state-of-the-art structured approach for Japanese MA, and achieves accuracy similar to that of structured predictors using the same feature set.",
        "We also find that the method is both robust to out-of-domain data, and can be easily adapted through the use of a combination of partial annotation and active learning."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Japanese morphological analysis (MA) takes an unsegmented string of Japanese text as input, and outputs a string of morphemes annotated with parts of speech (POSs).",
        "As MA is the first step in Japanese NLP, its accuracy directly affects the accuracy of NLP systems as a whole.",
        "In addition, with the proliferation of text in various domains, there is increasing need for methods that are both robust and adaptable to out-of-domain data (Escudero et al., 2000).",
        "Previous approaches have used structured predictors such as hidden Markov models (HMMs) or conditional random fields (CRFs), which consider the interactions between neighboring words and parts of speech (Nagata, 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004).",
        "However, while structure does provide valuable information, Liang et al.",
        "(2008) have shown that gains provided by structured prediction can be largely recovered by using a richer feature set.",
        "This approach has also been called \"pointwise\" prediction, as it makes a single independent decision at each point (Neubig and Mori, 2010).",
        "While Liang et al.",
        "(2008) focus on the speed benefits of pointwise prediction, we demonstrate that it also allows for more robust and adaptable MA.",
        "We find experimental evidence that pointwise MA can exceed the accuracy of a state-of-the-art structured approach (Kudo et al., 2004) on in-domain data, and is significantly more robust to out-of-domain data.",
        "We also show that pointwise MA can be adapted to new domains with minimal effort through the combination of active learning and partial annotation (Tsuboi et al., 2008), where only informative parts of a particular sentence are annotated.",
        "In a realistic domain adaptation scenario, we find that a combination of pointwise prediction, partial annotation, and active learning allows for easy adaptation."
      ]
    },
    {
      "heading": "2. Japanese Morphological Analysis",
      "text": [
        "Japanese MA takes an unsegmented string of characters x[ as input, segments it into morphemes wJ, and annotates each morpheme with a part of speech tJ.",
        "This can be formulated as a two-step process of first segmenting words, then estimating POSs (Ng and Low, 2004), or as a single joint process of finding a morpheme/POS string from unsegmented text (Kudo et al., 2004; Nakagawa, 2004; Kruengkrai et al., 2009).",
        "In this section we describe an existing joint sequence-based method for Japanese MA, as well as our proposed two-step pointwise method.",
        "Japanese MA has traditionally used sequence based models, finding a maximal POS sequence for en-",
        "(a) Joint Word/POS Pairs",
        "ho-sekinomigaku max",
        "Boundary Tags POS Tags",
        "ho-sekinomigaku ho-seki wo miga ku",
        "Figure 1: Joint MA (a) performs maximization over the entire sequence, while two-step MA (b) maximizes the 4 boundary and 4 POS tags independently.",
        "Table 1: Features for the joint model using tags t and words W. c( ) is a mapping function onto character types (kanji, katakana, etc.",
        ").",
        "tire sentences as in Figure 1 (a).",
        "The CRF-based method presented by Kudo et al.",
        "(2004) is generally accepted as the state-of-the-art in this paradigm.",
        "CRFs are trained over segmentation lattices, which allows for the handling of variable length sequences that occur due to multiple segmentations.",
        "The model is able to take into account arbitrary features, as well as the context between neighboring tags.",
        "We follow Kudo et al.",
        "(2004) in defining our feature set, as summarized in Table 1.",
        "Lexical features were trained for the top 5000 most frequent words in the corpus.",
        "It should be noted that these are word-based features, and information about transitions between POS tags is included.",
        "When creating training data, the use of word-based features indicates that word boundaries must be annotated, while the use of POS transition information further indicates that all of these words must be annotated with POSs.",
        "Table 2: Features for the two-step model.",
        "x\\ and Xr indicate the characters to the left and right of the word boundary or word Wj in question.",
        "ls, rs, and is represent the left, right, and inside dictionary features, while djk indicates that tag k exists in the dictionary for word j.",
        "In our research, we take a two-step approach, first segmenting character sequence x{ into the word sequence wf with the highest probability, then tagging each word with parts of speech tf.",
        "This approach is shown in Figure 1 (b).",
        "We follow Sassano (2002) in formulating word segmentation as a binary classification problem, estimating boundary tags b\\ – .",
        "Tag bi = 1 indicates that a word boundary exists between characters xi and xi+1, while bi = 0 indicates that a word boundary does not exist.",
        "POS estimation can also be formulated as a multi-class classification problem, where we choose one tag tj for each word Wj.",
        "These two classification problems can be solved by tools in the standard machine learning toolbox such as logistic regression (LR), support vector machines (SVMs), or conditional random fields (CRFs).",
        "We use information about the surrounding characters (character and character-type n-grams), as well as the presence or absence of words in the dictionary as features (Table 2).",
        "Specifically dictionary features for word segmentation ls and rs are active if a string of length s included in the dictionary is present directly to the left or right of the present word boundary, and is is active if the present word boundary is included in a dictionary word of length s. Dictionary feature djk for POS estimation indicates whether the current word Wj occurs as a dictionary entry with tag tk.",
        "Previous work using this two-stage approach has used sequence-based prediction methods, such as maximum entropy Markov models (MEMMs) or",
        "Type",
        "Feature Strings",
        "Character n-gram",
        "Xl , Xy , X\\ – 1 Xl , X\\Xy ,",
        "Xy Xy + 1 , Xl_1 Xl Xy , Xl Xy Xy + \\",
        "Char.",
        "Type n-gram",
        "C(Xl), C(Xy )",
        "C(Xl – lXl), C(Xl Xy ), C(Xy Xy+1 )",
        "c(Xl – 2Xl – iXl), C(Xl – iXlXy) C(XlXyXy+i), C(XyXy+1Xy+2)",
        "WS Only",
        "POS Only",
        "Wj, C(Wj), djk",
        "ho-seki",
        "wo",
        "miga",
        "ku",
        "M",
        "<",
        "N",
        "p",
        "V",
        "Suf",
        "Type",
        "Feature Strings",
        "Unigram",
        "tj, tj Wj, C(Wj ), Ij C(Wj )",
        "Bigram",
        "tj – \\tj, tj – \\tjWj – \\, tj-ltjWj, tj – 1tjWj – \\Wj",
        "CRFs (Ng and Low, 2004; Peng et al., 2004).",
        "However, as Liang et al.",
        "(2008) note, and we confirm, sequence-based predictors are often not necessary when an appropriately rich feature set is used.",
        "One important difference between our formulation and that of Liang et al.",
        "(2008) and all other previous methods is that we rely only on features that are directly calculable from the surface string, without using estimated information such as word boundaries or neighboring POS tags.",
        "This allows for training from sentences that are partially annotated as described in the following section."
      ]
    },
    {
      "heading": "3. Domain Adaptation for Morphological Analysis",
      "text": [
        "NLP is now being used in domains such as medical text and legal documents, and it is necessary that MA be easily adaptable to these areas.",
        "In a domain adaptation situation, we have at our disposal both annotated general domain data, and unannotated target domain data.",
        "We would like to annotate the target domain data efficiently to achieve a maximal gain in accuracy for a minimal amount of work.",
        "Active learning has been used as a way to pick data that is useful to annotate in this scenario for several applications (Chan and Ng, 2007; Rai et al., 2010) so we adopt an active-learning-based approach here.",
        "When adapting sequence-based prediction methods, most active learning approaches have focused on picking full sentences that are valuable to annotate (Ringger et al., 2007; Settles and Craven, 2008).",
        "However, even within sentences, there are generally a few points of interest surrounded by large segments that are well covered by already annotated data.",
        "Partial annotation provides a solution to this problem (Tsuboi et al., 2008; Sassano and Kurohashi, 2010).",
        "In partial annotation, data that will not contribute to the improvement of the classifier is left untagged.",
        "For example, if there is a single difficult word in a long sentence, only the word boundaries and POS of the difficult word will be tagged.",
        "\"Difficult\" words can be selected using active learning approaches, choosing words with the lowest classifier accuracy to annotate.",
        "In addition, corpora that are tagged with word boundaries but not POS tags are often available; this is another type of partial annotation.",
        "When using sequence-based prediction, learning on partially annotated data is not straightforward, as the data that must be used to train context-based transition probabilities may be left unannotated.",
        "In contrast, in the pointwise prediction framework, training using this data is both simple and efficient; unannotated points are simply ignored.",
        "A method for learning CRFs from partially annotated data has been presented by Tsuboi et al.",
        "(2008).",
        "However, when using partial annotation, CRFs' already slow training time becomes slower still, as they must be trained over every sequence that has at least one annotated point.",
        "Training time is important in an active learning situation, as an annotator must wait while the model is being re-trained."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "In order to test the effectiveness of pointwise MA, we did an experiment measuring accuracy both on in-domain data, and in a domain-adaptation situation.",
        "We used the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), specifying the whitepaper, news, and books sections as our general domain corpus, and the web text section as our target domain corpus (Table 3).",
        "As a representative of joint sequence-based MA described in 2.1, we used MeCab (Kudo, 2006), an open source implementation of Kudo et al.",
        "(2004)'s CRF-based method (we will call this joint).",
        "For the pointwise two-step method, we trained logistic regression models with the LIBLINEAR toolkit (Fan et al., 2008) using the features described in Section 2.2 (2-LR).",
        "In addition, we trained a CRF-based model with the CRFSuite toolkit (Okazaki, 2007) using the same features and set-up (for both word",
        "Table 4: Word/POS F-measure for each method when trained and tested on general (gen) or target (tar) domain corpora.",
        "segmentation and POS tagging) to examine the contribution of context information (2-CRF).",
        "To create the dictionary, we added all of the words in the corpus, but left out a small portion of singletons to prevent overfitting on the training data.",
        "As an evaluation measure, we follow Nagata (1994) and measure, so that both word boundaries and POS tags must be correct for a word to be considered correct.",
        "In our first experiment we compared the accuracy of the three methods on both the in-domain and out-of-domain test sets (Table 4).",
        "It can be seen that 2-LR outperforms JOINT, and achieves similar but slightly inferior results to 2-CRF.",
        "The reason for accuracy gains over JOINT lies largely in the fact that while JOINT is more reliant on the dictionary, and thus tends to mis-segment unknown words, the two-step methods are significantly more robust.",
        "The small difference between 2-LR and 2-CRF indicates that given a significantly rich feature set, context-based features provide little advantage, although the advantage is larger on out-of-domain data.",
        "In addition, training of 2-LR is significantly faster than 2-CRF.",
        "2-LR took 16m44s to train, while 2-CRF took 51m19s to train on a 3.33GHz Intel Xeon CPU.",
        "Our second experiment focused on the domain adaptability of each method.",
        "Using the target domain training corpus as a pool of unannotated data, we performed active learning-based domain adaptation using two techniques.",
        "• Sentence-based annotation (sent), where sentences with the lowest total POS and word",
        "boundary probabilities were annotated first.",
        "• Word-based partial annotation (PART), where the word or word boundary with the smallest probability margin between the first and second candidates was chosen.",
        "This can only be used with the pointwise 2-LR approach .",
        "For both methods, 100 words (or for SENT until the end of the sentence in which the 100th word is reached) are annotated, then the classifier is retrained and new probability scores are generated.",
        "Each set of 100 words is a single iteration, and 100 iterations were performed for each method.",
        "From the results in Figure 2, it can be seen that the combination of PART and 2-LR allows for significantly faster adaptation than other approaches, achieving accuracy gains in 15 iterations that are achieved in 100 iterations with SENT, and surpassing 2-CRF after 15 iterations.",
        "Finally, it can be seen that JOINT improves at a pace similar to PART, likely due to the fact that its pre-adaptation accuracy is lower than the other methods.",
        "It can be seen from Table 4 that even after adaptation with the full corpus, it will still lag behind the two-step methods."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "This paper proposed a pointwise approach to Japanese morphological analysis.",
        "It showed that despite the lack of structure, it was able to achieve results that meet or exceed structured prediction methods.",
        "We also demonstrated that it is both robust and adaptable to out-of-domain text through the use of partial annotation and active learning.",
        "Future work in this area will include examination of performance on other tasks and languages.",
        "Train",
        "Test",
        "JOINT",
        "2-CRF",
        "2-LR",
        "GEN",
        "GEN",
        "97.31%",
        "98.08%",
        "98.03%",
        "GEN",
        "TAR",
        "94.57%",
        "95.39%",
        "95.13%",
        "GEN+TAR",
        "TAR",
        "96.45%",
        "96.91%",
        "96.82%",
        "-\"-2step",
        "-*-2step",
        "-lr/part -lr/sent -crf/sent ent",
        "v 2step a joint/s"
      ]
    }
  ]
}
