{
  "info": {
    "authors": [
      "Yun Huang",
      "Min Zhang",
      "Chew Lim Tan"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-2094",
    "title": "Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars",
    "url": "https://aclweb.org/anthology/P11-2094",
    "year": 2011
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Nonparametric Bayesian Machine Transliteration with Synchronous",
        "Adaptor Grammars",
        "Yun Huang1,2 Min Zhang Chew Lim Tan",
        "huangyun@comp.nus.edu.sg mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg",
        "Human Language Department Department of Computer Science",
        "Institute for Infocomm Research National University of Singapore",
        "1 Fusionopolis Way, Singapore 13 Computing Drive, Singapore",
        "Machine transliteration is defined as automatic phonetic translation of names across languages.",
        "In this paper, we propose synchronous adaptor grammar, a novel nonpara-metric Bayesian learning approach, for machine transliteration.",
        "This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages.",
        "The proposed model outperforms the state-of-the-art EM-based model in the English to Chinese transliteration task."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Proper names are one source of OOV words in many NLP tasks, such as machine translation and cross-lingual information retrieval.",
        "They are often translated through transliteration, i.e. translation by preserving how words sound in both languages.",
        "In general, machine transliteration is often modelled as monotonic machine translation (Rama and Gali, 2009; Finch and Sumita, 2009; Finch and Sumita, 2010), the joint source-channel models (Li et al., 2004; Yang et al., 2009), or the sequential labeling problems (Reddy and Waxmonsky, 2009; Abdul Hamid and Darwish, 2010).",
        "Syllable equivalents acquisition is a critical phase for all these models.",
        "Traditional learning approaches aim to maximize the likelihood of training data by the Expectation-Maximization (EM) algorithm.",
        "However, the EM algorithm may over-fit the training data by memorizing the whole training instances.",
        "To avoid this problem, some approaches restrict that a single character in one language could be aligned to many characters of the other, but not vice versa introduced to obtain many-to-many alignments by combining two directional one-to-many alignments (Rama and Gali, 2009).",
        "Compared to maximum likelihood approaches, Bayesian models provide a systemic way to encode knowledges and infer compact structures.",
        "They have been successfully applied to many machine learning tasks (Liu and Gildea, 2009; Zhang et al., 2008; Blunsom et al., 2009).",
        "Among these models, Adaptor Grammars (AGs) provide a framework for defining nonparametric Bayesian models based on PCFGs (Johnson et al., 2007).",
        "They introduce additional stochastic processes (named adaptors) allowing the expansion of an adapted symbol to depend on the expansion history.",
        "Since many existing models could be viewed as special kinds of PCFG, adaptor grammars give general Bayesian extension to them.",
        "AGs have been used in various NLP tasks such as topic modeling (Johnson, 2010), perspective modeling (Hardisty et al., 2010), morphology analysis and word segmentation (Johnson and Goldwater, 2009; Johnson, 2008).",
        "In this paper, we extend AGs to Synchronous Adaptor Grammars (SAGs), and describe the inference algorithm based on the Pitman-Yor process (Pitman and Yor, 1997).",
        "We also describe how transliteration could be modelled under this formalism.",
        "It should be emphasized that the proposed method is language independent and heuristic-free.",
        "Experiments show the proposed approach outperforms the strong EM-based baseline in the English to Chinese transliteration task."
      ]
    },
    {
      "heading": "2. Synchronous Adaptor Grammars 2.1 Model",
      "text": [
        "A Pitman-Yor Synchronous Adaptor Grammar (PYSAG) is a tuple Q – {Qs,J\\fa,o,,b,cx), where Gs = (N,Ts,%,lZ, 5,0) is a Synchronous Context-Free Grammar (SCFG) (Chiang, 2007), TV is a set of nonterminal symbols, Tsl% are source/target terminal symbols, 1Z is a set of rewrite rules, 5 g TV is the start symbol, 0 is the distribution of rule probabilities, TVa ç TV is the set of adapted nonterminals, a g [0,1], b > 0 are vectors of discount and concentration parameters both indexed by adapted nonterminals, and a are Dirich-let prior parameters.",
        "Algorithm 1 Generative Process draw 6a ~ Dii{aA) for all A G TV for each yield pair (s / t) do > Sample from root",
        "function Sample(A) if A g TV0 then return Sample S AG(A)",
        "return SampleSCFG(A) function SampleSCFG(A) > For A G TV from the start symbol 5 (line 3) for each yield pair.",
        "For nonterminals that are not adapted, the grammar expands it just as the original synchronous grammar (function SampleSCFG).",
        "For each adapted nonterminal A g TVa, the grammar maintains a cache Ca to store previously generated subtrees under A.",
        "Let Zi be the subtree index in Ca, denoting the synchronous subtree generated at the ith expansion of A.",
        "At some particular time, assuming n subtrees rooted at A have been generated with m different types in the cache of A, each of which has been generated for m,..., nm times respectively.",
        "Then the grammar either generates the {n + l)th synchronous subtree as SCFG (line 17) or chooses an existing subtree (line 22), according to the conditional probability P(z\\zi<n).",
        "The above generative process demonstrates \"rich get richer\" dynamics, i.e. previous sampled subtrees under adapted nonterminals would more likely be sampled again in following procedures.",
        "This is suitable for many learning tasks since they prefer sparse solutions to avoid the over-fitting problems.",
        "If we integrate out the adaptors, the joint probability of a particular sequence of indexes z with cached counts (rii,..., nm) under the Pitman-Yor process is draw rule r = {ß / 7) - Multi(0A) treetß ^SAMPLE(B)fornonterminalB G ßöj return BuiLDTREE(r, tBl, tB2, • • •) 14: function Sample S AG(A) 15: draw cache index zn+1 > For A G N'a tree t <- SampleSCFG(A)",
        "InsertToCache(Ca, t).",
        "else > Old entry",
        "tree t < – FindInCache(Ca, zn+1) return t",
        "The generative process of a synchronous tree set T is described in Algorithm 1.",
        "First, rule probabilities are sampled for each nonterminal A g TV (line 1) according to the Dirichlet distribution.",
        "Then synchronous trees are generated in the top-down fashion",
        "Given synchronous tree set T, the joint probability under the PYSAG is where fA is the vector containing the number of times that rules r g TZa are used in the T, and B is the Beta function.",
        "Directly drawing samples from Equation (2) is intractable, so we extend the component-wise Metropolis-Hastings algorithm (Johnson et al., 2007) to the synchronous case.",
        "In detail, we draw sample T[ from some proposal distribution Q(Ti\\yi,T_i), then accept the new sampled syn-",
        "Sample(S') chronous tree T( with probability",
        "In theory, Q could be any distribution if it never assigns zero probability.",
        "For efficiency reason, we choose the probabilistic SCFG as the proposal distribution.",
        "We pre-parse the training instances before inference and save the structure of synchronous parsing forests.",
        "During the inference, we only change rule probabilities in parsing forests without changing the forest structures.",
        "The probability of rule r G TZa in Q is estimated by relative frequency 0r = – i – , where TZa is the set of rules rooted at A, and [fr]-i is the number of times that rule r is used in the tree set T_j.",
        "We use the sampling algorithm described in (Blunsom and Osborne, 2008) to draw a synchronous tree from the parsing forest according to the proposal Q.",
        "Following (Johnson and Goldwater, 2009), we put an uninformative Beta(l, 1) prior on a and a \"vague\" Gamma(10, 0.1) prior on b to model the uncertainty of hyperparameters."
      ]
    },
    {
      "heading": "3. Machine Transliteration 3.1 Grammars",
      "text": [
        "For machine transliteration, we design the following grammar to learn syllable mappings :",
        "We implement the CKY-like bottom up parsing algorithm described in (Wu, 1997).",
        "The complexity is 0(\\s\\\\t\\).",
        "where the adapted nonterminal Syl is designed to capture the syllable equivalents between two languages, and the nonterminal NEC, SEC and TEC capture the character pairs with no empty character, empty source and empty target respectively.",
        "Note that this grammar restricts the leftmost characters on both sides must be aligned one-by-one.",
        "Since our goal is to learn the syllable equivalents, we are not interested in the subtree tree inside the syllables.",
        "We refer this grammar as syllable grammar.",
        "The above grammar could capture inner-syllable dependencies.",
        "However, the selection of the target characters also depend on the context.",
        "For example, the following three instances are found in the training set:",
        "where the same English syllable (a a) are transliterated to (J^[ao]), (^[ai]) and <H[a]) respectively, depending on the following syllables.",
        "To model these contextual dependencies, we propose the hierarchical SAG.",
        "The two-layer word grammar is obtained by adding following rules:",
        "We might further add a new adapted nonterminal Col to learn the word collocations.",
        "The following rules appear in the collocation grammar.",
        "Name (Col/ Col)+ Col (Word / Word) + Word (Syl/Syl)+",
        "Figure 1 gives one synchronous parsing trees under the collocation grammar of the example (m a x/S[mai] j£[ke] |fr[si]).",
        "After sampling, we need a translation model to transliterate new source string to target string.",
        "Following (Li et al., 2004), we use the n-gram translation model to estimate the joint distribution P{s,t) = n£=i ^(Pfcbî-1), where Vk is the kthsyllable pair of the string pair (s / t).",
        "The first step is to construct joint segmentation lattice for each training instance.",
        "We first generate a merged grammar G' using collected subtrees under adapted nonterminals, then use synchronous parsing",
        "Name -",
        "■* (Syl/Syl)+",
        "Syl -",
        "(NECs / NECs)",
        "Syl -",
        "(NECs SECs / NECs",
        "SECs)",
        "Syl -",
        "(NECs TECs / NECs",
        "TECs)",
        "NECs -",
        "(NEC / NEC)+",
        "SECs -",
        "■» (SEC / SEC)+",
        "TECs -",
        "(TEC / TEC)+",
        "NEC -",
        "■» (si 1 tj)",
        "SEC -",
        "- (e/tj)",
        "TEC -",
        "■» (si 1 £)",
        "Figure 1 : An example of parse tree.",
        "to obtain probabilities in the segmentation lattice.",
        "Specifically, we \"flatten\" the collected subtrees under Syl, i.e. removing internal nodes, to construct new synchronous rules.",
        "For example, we could get two rules from the tree in Figure 1 :",
        "Syl (x/M) If multiple subtrees are flattened to the same synchronous rule, we sum up the counts of these subtrees.",
        "For rules with non-adapted nonterminal as parent, we assign the probability as the same of the sampled rule probability, i.e. let 6'r = 6r.",
        "For the adapted nonterminal Syl, there are two kinds of rules: (1) the rules in the original probabilistic SCFG, and (2) the rules flattened from subtrees.",
        "We assign the rule probability as if r is original SCFG rule if r is flatten from subtree where a and b are the parameters associated with Syl, m is the number of types of different rules flatten from subtrees, nr is the count of rule r, and n is the total number of flatten rules.",
        "One may verify that the rule probabilities are well normalized.",
        "Based on this merged grammar G', we parse the training string pairs, then encode the parsed forest into the lattice.",
        "Figure 2 show a lattice example for the string pair (a a 1 t o / Nïa] &[er] JÊ[tuo]).",
        "The transition probabilities in the lattice are the \"inside\" probabilities of corresponding Syl node in the parsing forest.",
        "After building the segmentation lattice, we train 3-order language model from the lattice using the SRILM.",
        "In decoding, given a new source string, we use the Viterbi algorithm with beam search (Li et al., 2004) to find the best transliteration candidate."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We conduct experiments on the English-Chinese data in the ACL Named Entities Workshop (NEWS 2009).",
        "Table 1 gives some statistics of the data.",
        "For evaluation, we report the word accuracy and mean F-score metrics defined in (Li et al., 2009).",
        "Table 1 : Transliteration data statistics",
        "In the inference step, we first run sampler through the whole training corpus for 10 iterations, then collect adapted subtree statistics for every 10 iterations, and finally stop after 20 collections.",
        "After each iteration, we resample each of hyperparameters from the posterior distribution of hyperparameters using a slice sampler (Neal, 2003).",
        "We implement the joint source-channel model (Li et al., 2004) as the baseline system, in which the orthographic syllable alignment is automatically derived by the Expectation-Maximization (EM) algorithm.",
        "NECs 1",
        "TECs 1",
        "NECs 1",
        "SECs 1",
        "1",
        "NEC i",
        "1",
        "TEC i",
        "1",
        "NEC 1",
        "1",
        "SEC 1",
        "1",
        "m/S",
        "1",
        "a/e",
        "1",
        "x/3£",
        "Train",
        "Dev",
        "Test",
        "# Entry",
        "31,961",
        "2,896",
        "2,896",
        "# En Char",
        "218,073",
        "19,755",
        "19,864",
        "# Ch Char",
        "101,205",
        "9,160",
        "9,246",
        "# Ch Type",
        "370",
        "275",
        "283",
        "Since EM tends to memorize the training instance as a whole, Li et al.",
        "(2004) restrict the Chinese side to be single character in syllable equivalents.",
        "Our method can be viewed as the Bayesian extension of the EM-based baseline.",
        "Since PYSAGs could learn accurate and compact transliteration units, we do not need the restriction any more.",
        "Table 2: Transliteration results, in the format of word accuracy / mean F-score.",
        "\"Syl\",\"Word\" and \"Col\" denote the syllable, word and collocation grammar respectively.",
        "1.",
        "The best results of our model are 67.1%/87.2% on development set and corresponding 67.0%/86.7% on test set, achieved by word grammars.",
        "The results on test set outperform the EM-based baseline system on both word accuracy and mean F-score.",
        "2.",
        "Comparing grammars of different layers, we find that the word grammars perform consistently better than the syllable grammars.",
        "These support the assumption that the context information are helpful to identify syllable equivalents.",
        "However, the collocation grammars do not further improve performance.",
        "We guess the reason is that the instances in transliteration are very short, so two-layer grammars are good enough while the collocations become very sparse, which results in unreliable probability estimation.",
        "Table 3 shows some examples of learned syllable mappings in the final sampled tree of the syllable grammar.",
        "We can see that the PYSAGs could find good syllable mappings from the raw name pairs without any heuristic or restriction.",
        "In this point of view, the proposed method is language independent.",
        "Specifically, we are interested in the English token \"x\", which is the only one that has two corre-",
        "Table 3: Examples of learned syllable mappings.",
        "Chinese Pinyin are given in the square bracket.",
        "The counts of syllable mappings in the final sampled tree are also given.",
        "sponding Chinese characters (\"5rLÄT[ke si]\").",
        "Table 3 demonstrates that nearly all these correct mappings are discovered by PYSAGs.",
        "Note that these kinds of mapping can not be learned if we restrict the Chinese side to be only one character (the heuristic used in (Li et al., 2004)).",
        "We will conduct experiments on other language pairs in the future."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "This paper proposes synchronous adaptor grammars, a nonparametric Bayesian model, for machine transliteration.",
        "Based on the sampling, the PYSAGs could automatically discover syllable equivalents without any heuristic or restriction.",
        "In this point of view, the proposed model is language independent.",
        "The joint source-channel model is then used for training and decoding.",
        "Experimental results on the English-Chinese transliteration task show that the proposed method outperforms the strong EM-based baseline system.",
        "We also compare grammars in different layers and find that the two-layer grammars are suitable for the transliteration task.",
        "We plan to carry out more transliteration experiments on other language pairs in the future."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the anonymous reviewers for their helpful comments and suggestions.",
        "We also thank Zhixiang Ren, Zhenghua Li, and Jun Sun for insightful discussions.",
        "Special thanks to Professor Mark Johnson for his open-source codes.",
        "Available from http://web.science.mq.edu.",
        "au/~mj ohnson/Software.htm",
        "s/9r[si]/1669",
        "t/#[te]/728",
        "man/ji[man]/703",
        "d/tl[de]/579",
        "cyï£[ke]/564",
        "de/tl[de]/564",
        "ro/^[luo]/531",
        "son/||[sen]/442",
        "k/j£[ke]/408",
        "ma/S,[ma]/390",
        "co/f4[ke]/387",
        "ll/^K[er]/383",
        "la/fe[la]/382",
        "tt/#[te]/380",
        "l/^K[er]/367",
        "ton/fi[dun]/360",
        "ri/S[li]/342",
        "ra/fe[la]/339",
        "ca/-R[ka]/333",
        "m/#t[mu]/323",
        "li/flj[li]/314",
        "ber/{â[bo]/311",
        "ley/flj[li]/310",
        "na/^[na]/302",
        "x/3£ff[ke si]/40",
        "x/3£[ke]/3",
        "x/Äf[si]/l",
        "Grammar",
        "Dev (%)",
        "Test (%)",
        "Baseline",
        "67.8/86.9",
        "66.6/85.7",
        "Syl Word Col",
        "66.6/87.0 67.1/87.2 67.2/87.1",
        "66.6/86.6 67.0/86.7",
        "66.9/86.7"
      ]
    }
  ]
}
