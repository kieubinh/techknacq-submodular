{
  "info": {
    "authors": [
      "Jun Suzuki",
      "Hideki Isozaki",
      "Masaaki Nagata"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-2112",
    "title": "Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning",
    "url": "https://aclweb.org/anthology/P11-2112",
    "year": 2011
  },
  "references": [
    "acl-D09-1058",
    "acl-D09-1060",
    "acl-D10-1004",
    "acl-J93-2004",
    "acl-P05-1001",
    "acl-P08-1068",
    "acl-P08-1076",
    "acl-P09-1054",
    "acl-P09-1116",
    "acl-P10-1001",
    "acl-P10-1040",
    "acl-W03-0419",
    "acl-W09-1119"
  ],
  "sections": [
    {
      "text": [
        "Learning Condensed Feature Representations from Large Unsupervised",
        "Data Sets for Supervised Learning",
        "Jun Suzuki, Hideki Isozaki, and Masaaki Nagata",
        "NTT Communication Science Laboratories, NTT Corp. 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan",
        "This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning.",
        "We use unsupervised data to generate informative 'condensed feature representations' from the original feature set used in supervised NLP systems.",
        "The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-of-the-art performance provided by the recently developed high-performance semi-supervised learning technique.",
        "Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In the last decade, supervised learning has become a standard way to train the models of many natural language processing (NLP) systems.",
        "One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data.",
        "Specifically, an approach that involves incorporating 'clustering-based word representations (CWR)' induced from unsupervised data as additional features of supervised learning has demonstrated substantial performance gains over state-of-the-art supervised learning systems in typical NLP tasks, such as named entity recognition (Lin and Wu, 2009; Turian et al., 2010) and dependency parsing (Koo et al., 2008).",
        "We refer to this approach as the iCWR approach, The iCWR approach has become popular for enhancement because of its simplicity and generality.",
        "The goal of this paper is to provide yet another simple and general framework, like the iCWR approach, to enhance existing state-of-the-art supervised NLP systems.",
        "The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, C is the CWR feature set, and H is the new feature set generated by our method.",
        "Then, with the iCWR approach, C is induced independently from F, and used in addition to F in supervised learning, i.e., FUC.",
        "In contrast, in our method H is directly induced from F with the help of an existing model already trained by supervised learning with F, and used in place of F in supervised learning.",
        "The largest contribution of our method is that it offers an architecture that can drastically reduce the number of features, i.e., from 10M features in F to less than 1K features in H by constructing 'condensed feature representations (COFER)', which is a new and very unique property that cannot be matched by previous semi-supervised learning methods including the iCWR approach.",
        "One noteworthy feature of our method is that there is no need to handle sparse and high-dimensional feature spaces often used in many supervised NLP systems, which is one of the main causes of the data sparseness problem often encountered when we learn the model with a supervised leaning algorithm.",
        "As a result, NLP systems that are both compact and highperformance can be built by retraining the model with the obtained condensed feature set H."
      ]
    },
    {
      "heading": "2. Condensed Feature Representations",
      "text": [
        "Let us first define the condensed feature set H. In this paper, we call the feature set generally used in supervised learning, F, the original feature set.",
        "Let N and M represent the numbers of features in F and H, respectively.",
        "We assume M < , and generally .",
        "A condensed feature is charac-",
        "[ Each condensed feature is represented as a set of features in the original feature set F.",
        "f The potencies are also utilized as 1 [ an (M+1)-th condensed feature J",
        "terized as a set of features in F, that is, hm = Smwhere Sm ç F. We assume that each original feature fn G F maps, at most, to one condensed feature hm.",
        "This assumption prevents two condensed features from containing the same original feature, and some original features from not being mapped to any condensed feature.",
        "Namely, Sm D S m = 0 for all m",
        "m=i Sm ÇF hold.",
        "and m, where m = m', and \\J",
        "The value of each condensed feature is calculated by summing the values of the original features assigned to it.",
        "Formally, let X and Y represent the sets of all possible inputs and outputs of a target task, respectively.",
        "Let x G X be an input, and y G Y (x) be an output, where Y (x) ç Y represents the set of possible outputs given x.",
        "We write the nth feature function of the original features, whose value is determined by x and y, as fn(x, y), where n G {1,...,N}.",
        "Similarly, we write the m-th feature function of the condensed features as hm(x, y), where m g{1, ..., M}.",
        "We state that the value of hm(x, y) is calculated as follows:",
        "flm(x, y)=Ef„eSm fn(x, y)."
      ]
    },
    {
      "heading": "3. Learning COFERs",
      "text": [
        "The remaining part of our method consists of the way to map the original features into the condensed features.",
        "For this purpose, we define the feature potency, which is evaluated by employing an existing supervised model with unsupervised data sets.",
        "Figure 1 shows a brief sketch of the process to construct the condensed features described in this section.",
        "We assume that we have a model trained by supervised learning, which we call the ' base supervised model', and the original feature set F that is used in the base supervised model.",
        "We consider a case where the base supervised model is a (log-)linear model, and use the following equation to select the best output y given x:",
        "where Wn is a model parameter (or weight) of fn.",
        "Linear models are currently the most widely-used models and are employed in many NLP systems.",
        "To simplify the explanation, we define function r(x, y), where r(x, y) returns 1 if y = y is obtained from the base supervised model given x, and 0 otherwise.",
        "Let r(x) represent the average of r(x, y) in x (see Figure 2 for details).",
        "We also define V+ (fn) and V (fn) as shown in Figure 2 where V represents the unsupervised data set.",
        "V1+(fn) measures the positive correlation with the best output y given by the base supervised model since this is the summation of all the (weighted) feature values used in the estimation of the one best output y over all x in the unsupervised data V. Similarly, V (fn) measures the negative correlation with y.",
        "Next, we define VD(fn) as the feature potency of fn: VD(fn) =",
        "VD(fn) - VD(fn).",
        "An intuitive explanation of VD(fn) is as follows; if \\ Vv (fn) \\ is large, the distribution of fn has either a large positive or negative correlation with the best output y given by the base supervised model.",
        "This implies that fn is an informative and potent feature in the model.",
        "Then, the distribution of fn has very small (or no) correlation to determine y if \\VD(fn)\\ is zero or near zero.",
        "In this case, fn can be evaluated as an uninformative feature in the model.",
        "From this perspective, we treat VD (fn) as a measure of feature potency in terms of the base supervised model.",
        "The essence of this idea, evaluating features against each other on a certain model, is widely used in the context of semi-supervised learning, i.e., (Ando and Zhang, 2005; Suzuki and Isozaki, 2008; Druck and McCallum, 2010).",
        "Our method is rough and a much simpler framework for implementing this fundamental idea of semi-supervised learning developed for NLP tasks.",
        "We create a simple framework to achieve improved flexibility, extendability, and applicability.",
        "In fact, we apply the framework by incorporating a feature merging and elimination architecture to obtain effective condensed feature sets for supervised learning.",
        "To discount low potency values, we redefine feature potency as VD(fn) instead of VD(fn) as follows:",
        "where Rn and An are defined in Figure 2.",
        "Note that Vd(fn) = V+(fn) - Vv(fn) = Rn - An.",
        "The difference from V (fn) is that we cast it in the log-domain and introduce a non-negative constant C. The introduction of C is inspired by the L1-regularization technique used in supervised learning algorithms such as (Duchi and Singer, 2009; Tsu-ruoka et al., 2009).",
        "C controls how much we discount V (fn) toward zero, and is given by the user.",
        "We define VD (fn) as VD (fn) = \\SVD (fn)] if VD (fn) > 0 and VD(fn) = WD (fn)\\ otherwise, where ö is a positive user-specified constant.",
        "Note that VD (fn) always becomes an integer, that is, VD (fn) G N where N = {..., -2, -1,0,1,2,...}.",
        "This calculation can be seen as mapping each feature into a discrete (integer) space with respect to V' (fn).",
        "ö controls the range of V' (fn) mapping into the same integer.",
        "Suppose we have M different quantized feature potency values in VD(fn) for all n, which we rewrite as {um}M=1.",
        "Then, we define Sm as a set of fnwhose quantized feature potency value is um.",
        "As described in Section 2, we define the m-th condensed feature hm(x, y) as the summation of all the original features fn assigned to Sm.",
        "That is, hm(x, y) = Y.f„eSm fn(x, y).",
        "This feature fusion process is intuitive since it is acceptable if features with the same (similar) feature potency are given the same weight by supervised learning since they have the same potency with regard to determining y. ö determines the number of condensed features to be made; the number of condensed features becomes large if ö is large.",
        "Obviously, the upper bound of the number of condensed features is the number of original features.",
        "To exclude possibly unnecessary original features from the condensed features, we discard feature fn for all n if un = 0.",
        "This is reasonable since, as described in Section 3.1, a feature has small (or no) effect in achieving the best output decision in the base supervised model if its potency is near 0.",
        "C introduced in Section 3.2 mainly influences how many original features are discarded.",
        "Additionally, we also utilize the 'quantized' feature potency values themselves as a new feature.",
        "The reason behind is that they are also very informative for supervised learning.",
        "Their use is important to further boost the performance gain offered by our method.",
        "For this purpose, we define (f)(x, y) as 4>((x, y) = MM=1(um/ö)hm(x, y).",
        "We then use (f)(x, y) as the (M + 1)-th feature of our condensed feature set.",
        "As a result, the condensed feature set obtained with our method is represented as H = {Iii(x, y),..., hM(x, y)A(x,y)}.",
        "Note that the calculation cost of (f)(x, y) is negligible.",
        "We can calculate the linear discriminant function g(x,y) as: g(x,y) = £M=i wmhm(x,y) +",
        "(wm + wM+1um/ö).",
        "We emphasize that once {wm}M=ii are determined by supervised learning, we can calculate wm' in a preliminary step before the test phase.",
        "Thus, our method also takes the form of a linear model.",
        "The number of features for our method is essentially M even if we add f .",
        "We modify our method to better suit structured prediction problems in terms of calculation cost.",
        "For a structured prediction problem, it is usual to decompose or factorize output structure y into a set of local substructures z to reduce the calculation cost and to cope with the sparsity of the output space Y.",
        "This factorization can be accomplished by restricting features that are extracted only from the information within decomposed local substructure z and given input x.",
        "We write z G y when the local substructure z is a part of output y, assuming that output y is constructed by a set of local substructures.",
        "Then formally, the nth feature is written as fn(x,z), and fn(x, y) = £zey fn(x,z) holds.",
        "Similarly, we introduce r(x, z), where r(x, z) = 1 if z G y, and r(x, z) = 0 otherwise, namely z G y.",
        "We define Z (x) as the set of all local substructures possibly generated for all y in Y(x).",
        "Z(x) can be enumerated easily, unless we use typical first-or second-order factorization models by the restriction of efficient decoding algorithms, which is the typical case for many NLP tasks such as named entity recognition and dependency parsing.",
        "Finally, we replace all Y(x) with Z (x), and use fn(x, z) and r(x, z) instead of fn(x, y) and r(x, y), respectively, in Rn and An.",
        "When we use these substitutions, there is no need to incorporate an efficient algorithm such as dynamic programming into our method.",
        "This means that our feature potency estimation can be applied to the structured prediction problem at low cost.",
        "Our feature potency estimation described in Section 3.1 to 3.3 is highly suitable for implementation in the MapReduce framework (Dean and Ghemawat, 2008), which is a modern distributed parallel computing framework.",
        "This is because Rn and An can be calculated by the summation of a data-wise calculation (map phase), and V^,(fn) can be calculated independently by each feature (reduce phase).",
        "We emphasize that our feature potency estimation can be performed in a 'single' map-reduce process."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We conducted experiments on two different NLP tasks, namely NER and dependency parsing.",
        "To facilitate comparisons with the performance of previous methods, we adopted the experimental settings used to examine high-performance semi-supervised NLP systems; i.e., NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al., 2008; Chen et al., 2009; Suzuki et al., 2009).",
        "For the supervised datasets, we used CoNLL'03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III (PTB) corpus (Marcus et al., 1994) for dependency parsing.",
        "We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al., 2009).",
        "We mainly compare the effectiveness of COFER with that of CWR derived by the Brown algorithm.",
        "The iCWR approach yields the state-of-the-art results with both dependency parsing data derived from PTB-III (Koo et al., 2008), and the CoNLL'03 shared task data (Turian et al., 2010).",
        "By comparing COFER with iCWR we can clarify its effectiveness in terms of providing better features for supervised learning.",
        "We use the term active features to refer to features whose corresponding model parameter is non-zero after supervised learning.",
        "It is well-known that we can discard non-active features from the trained model without any loss after finishing supervised learning.",
        "Finally, we compared the performance in terms of the number of active features in the model given by supervised learning.",
        "We note here that the number of active features for COFER is the number of features hm if w'm = 0, which is not wm = 0 for a fair comparison.",
        "Unlike COFER, iCWR does not have any architecture to winnow the original feature set used in supervised learning.",
        "For a fair comparison, we prepared L1 regularized supervised learning algorithms, which try to reduce the non-zero parameters in a model.",
        "Specifically, we utilized L1 regularized CRF (L1CRF) optimized by OWL-QN (Andrew and Gao, 2007) for NER, and the online structured output learning version of FOBOS (Duchi and Singer, 2009; Tsuruoka et al., 2009) with L1-regularization (ostL1FOBOS) for dependency parsing.",
        "In addition, we also examined L2 regularized CRF (Lafferty et al., 2001) optimized by L-BFGS (Liu and Nocedal, 1989) (L2CRF) for NER, and the online structured output learning version of the Passive-Aggressive algorithm (ostPA) (Crammer et al., 2006) for dependency parsing to illustrate the baseline performance regardless of the active feature number.",
        "We utilized baseline supervised learning models as the base supervised models of COFER.",
        "In addition, we also report the results when we treat iCWR as COFER's base supervised models (iCWR+COFER).",
        "This is a very natural and straightforward approach to combining these two.",
        "We generally handle several different types offea-tures such as words, part-of-speech tags, word surface forms, and their combinations.",
        "Suppose we have K different feature types, which are often defined by feature templates, i.e., (Suzuki and Isozaki, 2008; Lin and Wu, 2009).",
        "In our experiments, we restrict the merging of features during the condensed feature construction process if and only if the features are the same feature type.",
        "As a result, COFER essentially consists of K different condensed feature sets.",
        "The numbers of feature types K were 79 and 30 for our NER and dependency parsing experiments, respectively.",
        "We note that this kind of feature partition by their types is widely used in the context of semi-supervised learning (Ando and Zhang, 2005; Suzuki and Isozaki, 2008).",
        "Figure 3 displays the performance on the development set with respect to the number of active features in the trained models given by each supervised learning algorithm.",
        "In both NER and dependency parsing experiments, COFER significantly outperformed iCWR.",
        "Moreover, COFER was surprisingly robust in relation to the number of active features in the model.",
        "These results reveal that COFER provides effective feature sets for certain NLP tasks.",
        "We summarize the noteworthy results in Figure 3, and also the performance of recent top-line systems for NER and dependency parsing in Table 1.",
        "Overall, COFER matches the results of top-line semi-",
        "Table 1: Comparison with previous top-line systems on test data.",
        "(#.USD: unsupervised data size.",
        "#.AF: the size of active features in the trained model.)",
        "supervised learning systems even though it uses far fewer active features.",
        "In addition, the combination of iCWR+COFER significantly outperformed the current best results by achieving a 0.12 point gain from 90.90 to 91.02 for dependency parsing, with only 5.94M and 5.77M features, respectively."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "This paper introduced the idea of condensed feature representations (COFER) as a simple and general framework that can enhance the performance of existing supervised NLP systems.",
        "We also proposed a method that efficiently constructs condensed feature sets through discrete feature potency estimation over unsupervised data.",
        "We demonstrated that COFER based on our feature potency estimation can offer informative dense and low-dimensional feature spaces for supervised learning, which is theoretically preferable to the sparse and high-dimensional feature spaces often used in many NLP tasks.",
        "Existing NLP systems can be made more compact with higher performance by retraining their models with our condensed features.",
        "NER system",
        "dev.",
        "test",
        "#.USD",
        "#.AF",
        "Sup.L1CRF",
        "90.40",
        "85.08",
        "0",
        "0.57M",
        "iCWR: L1CRF",
        "93.33",
        "89.99",
        "3,720M",
        "0.62M",
        "COFER: L1CRF (S = 1e + 00)",
        "93.42",
        "88.81",
        "3,720M",
        "359",
        "(S =1e + 04)",
        "93.60",
        "89.22",
        "3,720M",
        "2.46M",
        "iCWR+COFER: (S = 1e + 00)",
        "94.39",
        "90.72",
        "3,720M",
        "344",
        "L1CRF (S = 1e + 04)",
        "94.91",
        "91.02",
        "3,720M",
        "5.94M",
        "(Ando and Zhang, 2005)",
        "93.15",
        "89.31",
        "27M",
        "N/A",
        "(Suzuki and Isozaki, 2008)",
        "94.48",
        "89.92",
        "1,000M",
        "N/A",
        "(Ratinov and Roth, 2009)",
        "93.50",
        "90.57",
        "N/A",
        "N/A",
        "(Turianetal.,2010)",
        "93.95",
        "90.36",
        "37M",
        "N/A",
        "(Lin and Wu, 2009)",
        "N/A",
        "90.90",
        "700,000M",
        "N/A",
        "iCWR+COFER: L2CRF iCWR+COFER L1CRF COFER L2CRF COFER: L1 CRF",
        "iCWR+COFER: ostPA -6-iCWR+COFER: ostLlFOBOS COFER: ostPA COFER: ostLlFOBOS",
        "iCWR: L2CRF *iCWR L1CRF • Sup.L2CRF -S-Sup.Ll CRF",
        "iCWR: ostPA – iCWR: ostLlFOBOS Sup.ostPA ■©■Sup.ostLlFOBOS",
        "Dependency parser",
        "dev.",
        "test",
        "#.USD",
        "#.AF",
        "ostL1FOBOS",
        "93.15",
        "92.82",
        "0",
        "6.80M",
        "iCWR: ostL1FOBOS",
        "93.69",
        "93.49",
        "3,720M",
        "9.67M",
        "COFER:ostL1FOBOS (S = 1e + 03)",
        "(S = 1e + 05)",
        "93.53 93.91",
        "93.23 93.71",
        "3,720M 3,720M",
        "20.7K 3.23M",
        "iCWR+COFER: (S = 1e + 03) ostL1FOBOS (S = 1e + 05)",
        "93.93 94.33",
        "93.55 94.22",
        "3,720M 3,720M",
        "12.5K 5.77M",
        "(Koo and Collins, 2010) (Martins etal., 2010)",
        "93.49 N/A",
        "93.04 93.26",
        "0 0",
        "N/A 55.25M",
        "(Koo et al., 2008)",
        "(Chen et al., 2009)",
        "(Suzuki et al., 2009)",
        "93.30 N/A 94.13",
        "93.16 93.16 93.79",
        "43M 43M 3,720M",
        "N/A N/A N/A"
      ]
    }
  ]
}
