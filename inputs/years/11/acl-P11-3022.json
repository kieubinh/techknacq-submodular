{
  "info": {
    "authors": [
      "Hajime Senuma"
    ],
    "book": "Proceedings of the ACL 2011 Student Session",
    "id": "acl-P11-3022",
    "title": "K-means Clustering with Feature Hashing",
    "url": "https://aclweb.org/anthology/P11-3022",
    "year": 2011
  },
  "references": [
    "acl-W08-0804"
  ],
  "sections": [
    {
      "text": [
        "One of the major problems of K-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional.",
        "We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors.",
        "Our analysis gives theoretical motivation and justification for applying feature hashing to K-means, by showing how much will the objective of K-means be (additively) distorted.",
        "Furthermore, to empirically verify our method, we experimented on a document clustering task."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In natural language processing (NLP) and text mining, clustering methods are crucial for various tasks such as document clustering.",
        "Among them, K-means (MacQueen, 1967; Lloyd, 1982) is \"the most important flat clustering algorithm\" (Manning et al., 2008) both for its simplicity and performance.",
        "One of the major problems of K-means is that it has K centroids which are dense vectors where K is the number of clusters.",
        "Thus, it is infeasible to store them in memory and slow to compute if the dimension of inputs is huge, as is often the case with NLP and text mining tasks.",
        "A well-known heuristic is truncating after the most significant features (Manning et al., 2008), but it is difficult to analyze its effect and to determine which features are significant.",
        "Recently, Weinberger et al.",
        "(2009) introduced feature hashing, a simple yet effective and analyzable dimension-reduction technique for large-scale multitask learning.",
        "The idea is to combine features which have the same hash value.",
        "For example, given a hash function h and a vector x, if h(1012) = h(41234) = 42, we make a new vector y by setting y42 = xioi2 + X41234 (or equally possibly",
        "x1012-x41234, -x1012 +x41234, or -x1012-x41234).",
        "This trick greatly reduces the size of dense vectors, since the maximum index value becomes equivalent to the maximum hash value ofh.",
        "Furthermore, unlike random projection (Achlioptas, 2003; Boutsidis et al., 2010), feature hashing retains sparsity of sparse input vectors.",
        "An additional useful trait for NLP tasks is that it can save much memory by eliminating an alphabet storage (see the preliminaries for detail).",
        "The authors also justified their method by showing that with feature hashing, dot-product is unbiased, and the length of each vector is well-preserved with high probability under some conditions.",
        "Plausibly this technique is useful also for clustering methods such as -means.",
        "In this paper, to motivate applying feature hashing to -means, we show the residual sum of squares, the objective of -means, is well-preserved under feature hashing.",
        "We also demonstrate an experiment on document clustering and see the feature size can be shrunk into 3.5% of the original in this case."
      ]
    },
    {
      "heading": "2. Preliminaries",
      "text": [
        "In this paper, || • || denotes the Euclidean norm, and (•, •) does the dot product.",
        "öitj is the Kronecker's delta, that is, öitj = 1 if i = j and 0 otherwise.",
        "Although we do not describe the famous algorithm of K-means (MacQueen, 1967; Lloyd, 1982) here, we remind the reader of its overall objective for later analysis.",
        "If we want to group input vectors into K clusters, K-means can surely output clusters uj1, ...uoK and their corresponding vectors ^1,/i.K such that they locally minimize the residual sum of squares (RSS) which is defined as",
        "||x -^||2.",
        "In the algorithm, /Ltk is made into the mean of the vectors in a cluster uok.",
        "Hence comes the name K-means.",
        "Note that RSS can be regarded as a metric since the sum of each metric (in this case, squared Euclidean distance) becomes also a metric by constructing a 1-norm product metric.",
        "Suppose one wants to embed a metric space (X, d) into another one (Xd') by a mapping 0.",
        "Its additive distortion is the infimum of e which, for any observed x, y G X, satisfies the following condition:",
        "d(x, y) - e < d'(0(x), 0(y)) < d(x, y) + e.",
        "According to an account by John Langford , a co-author of papers on feature hashing (Shi et al., 2009; Weinberger et al., 2009), hashing tricks for dimension-reduction were implemented in various machine learning libraries including Vowpal Wabbit, which he realesed in 2007.",
        "Ganchev and Dredze (2008) named their hashing trick random feature mixing and empirically supported it by experimenting on NLP tasks.",
        "It is similar to feature hashing except lacking of a binary hash function.",
        "The paper also showed that hashing tricks are useful to eliminate alphabet storage.",
        "Shi et al.",
        "(2009) suggested hash kernel, that is, dot product on a hashed space.",
        "They conducted thorough research both theoretically and experimentally, extending this technique to classification of graphs and multi-class classification.",
        "Although they tested K-means in an experiment, it was used for classification but not for clustering.",
        "Weinberger et al.",
        "(2009) introduced a technique feature hashing (a function itself is called the hashed feature map), which incorporates a binary hash function into hashing tricks in order to guarantee the hash kernel is unbiased.",
        "They also showed applications to various real-world applications such as multitask learning and collaborative filtering.",
        "Though their proof for exponential tail bounds in the original paper was refuted later, they reproved it under some extra conditions in the latest version.",
        "Below is the definition.",
        "Definition 2.1.",
        "Let S be a set of hashable features, h be a hash function h : S – {1,m}, and £ be £ : S – {±1}.",
        "The hashed feature map RIsI – Rm is a function such that the i-th element",
        "#°(x)= £ £(j)xj.",
        "If h and £ are clear from the context, we simply write as ( .",
        "As well, a kernel function is defined on a hashed feature map.",
        "Definition 2.2.",
        "The hash kernel (•, is defined as",
        "(x, x% = (0(x),0(x')).",
        "They also proved the following theorem, which we use in our analysis.",
        "Theorem 2.3.",
        "The hash kernel is unbiased, that is,",
        "http://hunch.net/-jl/projects/hash_ reps/index.html",
        "Then, DRSS is defined as follows:",
        "In this kind of hashing tricks, an index of inputs do not have to be an integer but can be any hash-able value, including a string.",
        "Ganchev and Dredze (2008) argued this property is useful particularly for implementing NLP applications, since we do not anymore need an alphabet, a dictionary which maps features to parameters.",
        "Let us explain in detail.",
        "In NLP, features can be often expediently expressed with strings.",
        "For instance, a feature 'the current word ends with -ing' can be expressed as a string cur:end:ing (here we suppose : is a control character).",
        "Since indices of dense vectors (which may be implemented with arrays) must be integers, traditionally we need a dictionary to map these strings to integers, which may waste much memory.",
        "Feature hashing removes this memory waste by converting strings to integers with on-the-fly computation."
      ]
    },
    {
      "heading": "3. Method",
      "text": [
        "For dimension-reduction to K-means, we propose a new method hashed K-means.",
        "Suppose you have N input vectors x1,xN.",
        "Given a hashed feature map ( , hashed K-means runs K-means on 0(x1),(f)(xN ) instead of the original ones."
      ]
    },
    {
      "heading": "4. Analysis",
      "text": [
        "In this section, we show clusters obtained by the hashed K-means are also good clusters in the original space with high probability.",
        "While Weinberger et al.",
        "(2009) proved a theorem on (multiplicative) distortion for Euclidean distance under some tight conditions, we illustrate (additive) distortion for RSS.",
        "Since K-means is a process which monotonically decreases RSS in each step, if RSS is not distorted so much by feature hashing, we can expect results to be reliable to some extent.",
        "Let us define the difference of the residual sum of squares (DRSS).",
        "Definition 4.1.",
        "Let w1, ...wK be clusters, /LtKbe their corresponding centroids in the original space, ff be a hashed feature map, and /xp,»K be their corresponding centroids in the hashed space.",
        "||x - »k |||.",
        "fc=1 xecok",
        "Before analysis, we define a notation for the (Euclidean) length under a hashed space:",
        "Definition 4.2.",
        "The hash length || • ||p is defined as",
        "= \\/(f(x),f(x)) = \\/ (x, x)i.",
        "Note that it is clear from Theorem 2.3 that = 0.",
        "In order to show distortion, we want to use Cheby-shev's inequality.",
        "To this end, it is vital to know the expectation and variance of the sum of squared hash lengths.",
        "Because the variance of the sum of random variables derives from each covariance between pairs of variables, first we show the covariance between the squared hash length of two vectors.",
        "Lemma 4.3.",
        "The covariance between the squared hash length of two vectors x, y G Rn is",
        "V(x y) = Y1 xixj yiyj.",
        "This lemma can be proven by the same technique described in the Appendix A of Weinberger et al.",
        "(2009).",
        "Now we see the following lemma.",
        "Proof.",
        "This is an application of Chebyshev's inequality.",
        "Namely, for any e > 0,",
        "e2.",
        "Since the expectation of a sum is the sum of expectations we readily know the zero expectation:",
        "Ep[X ] = 0.",
        "Since adding constants to the inputs ofcovariance does not change its result, from Lemma 4.3, for any",
        "Because the variance of the sum of random variables is the sum of the covariances between every pair of them,",
        "Finally, we see the following theorem for additive distortion.",
        "Theorem 4.5.",
        "Let * be the sum of V(x, y) for any observed pair ofx, y, each ofwhich expresses the difference between an example and its corresponding centroid.",
        "Then, for any e, Thus, if m > 7-1*e-2 where 0 < 7 <= 1, with probability at least 1 - 7, RSS is additively distorted by e. Proof.",
        "Note that a hashed feature map f (h>?)",
        "is linear, since ff(x) = Mx with a matrix M such that Mi;j = £(i)5h(i),j.",
        "By this liearlity, =",
        "K|_1Exe,fc f(x) = f(|wfc*e*fc x) = ff (/tk).",
        "Reapplying linearlity to this result, we have",
        "Figure : The change of F5-measure along with the hash size features, function words should be ignored not only because they give no information for clustering but also because their high frequencies magnify distortion."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "To empirically verify our method, from 20 Newsgroups, a dataset for document classification or clustering , we chose 6 classes and randomly drew 00 documents for each class.",
        "We used unigrams and bigrams as features and ran our method for various hash sizes m (Figure ).",
        "The number of unigrams is 33,0 7 and bigrams 09,395, so the feature size in the original space is 42,4 2.",
        "To measure performance, we used the F5 measure (Manning et al., 2008).",
        "The scheme counts correctness pairwisely.",
        "For example, if a document pair in an output cluster is actually in the same class, it is counted as true positive.",
        "In contrast, if it is actually in the different class, it is counted as false positive.",
        "Following this manner, a contingency table can be made as follows:",
        "The existence of * in the theorem suggests that to use feature hashing, we should remove useless features which have high values from data in advance.",
        "For example, if frequencies of words are used as",
        "Now, Fß measure can be defined as where the precision P = TP/(TP + FP) and the recall R = TP/(TP + FN).",
        "Same cluster Diff.",
        "clusters",
        "Same class Diff.",
        "classes",
        "TP FN FP TN",
        "In short, F5 measure strongly favors precision to recall.",
        "Manning et al.",
        "(2008) stated that in some cases separating similar documents is more unfavorable than putting dissimilar documents together, and in such cases the Fß measure (where ß > 1) is a good evaluation criterion.",
        "At the first look, it seems odd that performance can be higher than the original where m is low.",
        "A possible hypothesis is that since K-means only locally minimizes RSS but in general there are many local minima which are far from the global optimal point, therefore distortion can be sometimes useful to escape from a bad local minimum and reach a better one.",
        "As a rule, however, large distortion kills clustering performance as shown in the figure.",
        "Although clustering is heavily case-dependent, in this experiment, the resulting clusters are still reliable where the hash size is 3.5% of the original feature space size (around 5,000)."
      ]
    },
    {
      "heading": "6. Future Work",
      "text": [
        "Arthur and Vassilvitskii (2007) proposed K-means++, an improved version of K-means which guarantees its RSS is upper-bounded.",
        "Combining their method and the feature hashing as shown in our paper will produce a new efficient method (possibly it can be named hashed K-means++).",
        "We will analyze and experiment with this method in the future."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "In this paper, we argued that applying feature hashing to K-means is beneficial for memory-efficiency.",
        "Our analysis theoretically motivated this combination.",
        "We supported our argument and analysis by an experiment on document clustering, showing we could safely shrink memory-usage into 3.5% of the original in our case.",
        "In the future, we will analyze the technique on other learning methods such as K-means++ and experiment on various real-data NLP tasks."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We are indebted to our supervisors, Jun'ichi Tsujii and Takuya Matsuzaki.",
        "We are also grateful to the anonymous reviewers for their helpful and thoughtful comments."
      ]
    }
  ]
}
