{
  "info": {
    "authors": [
      "Ani Nenkova",
      "Sameer Maskey",
      "Yang Liu"
    ],
    "book": "Tutorial Abstracts of ACL 2011",
    "id": "acl-P11-5003",
    "title": "Automatic Summarization",
    "url": "https://aclweb.org/anthology/P11-5003",
    "year": 2011
  },
  "references": [
    "acl-A00-2025",
    "acl-C08-1124",
    "acl-J02-4003",
    "acl-N06-2050",
    "acl-P09-1062"
  ],
  "sections": [
    {
      "text": [
        "Motivation: where does summarization",
        "help?",
        "Single document summarization Simulate the work of intelligence analyst Judge if a document is relevant to a topic of interest",
        "?Summaries as short as 17% of the full text length speed up decision making twice, with no significant degradation in accuracy.?",
        "?Query-focused summaries enable users to find more relevant documents more accurately, with less need to consult the full text of the document.?",
        "[Mani et al., 2002]",
        "Motivation: multi-document summarization helps in compiling and presenting Reduce search time, especially when the goal of the user is to find as much information as possible about a given topic Writing better reports, finding more relevant information, quicker Cluster similar articles and provide a multi-document summary of the similarities Single document summary of the information unique to an article [Roussinov and Chen, 2001; Mana-Lopez et al., 2004; McKeown et al., 2005 ]",
        "Benefits from speech summarization",
        "Voicemail Shorter time spent on listening (call centers) Meetings Easier to find main points Broadcast News Summary of story from mulitiple channels Lectures Useful for reviewing of course materials",
        "[He et al., 2000; Tucker and Whittaker, 2008; Murray et al., 2009]",
        "Computing informativeness Topic models (unsupervised) Figure out what the topic of the input Frequency, Lexical chains, TF*IDF LSA, content models (EM, Bayesian) Select informative sentences based on the topic Graph models (unsupervised) Sentence centrality Supervised approaches Ask people which sentences should be in a summary Use any imaginable feature to learn to predict human choices",
        "Frequency as document topic proxy",
        "10 incarnations of an intuition Simple intuition, look only at the document(s) Words that repeatedly appear in the document are likely to be related to the topic of the document Sentences that repeatedly appear in different input",
        "documents represent themes in the input But what appears in other documents is also helpful in determining the topic Background corpus probabilities/weights for word",
        "What is an article about?",
        "Word probability/frequency Proposed by Luhn in 1958 [Luhn 1958] Frequent content words would be indicative of the topic of the article In multi-document summarization, words or facts repeated in the input are more likely to appear in human summaries [Nenkova et al., 2006]",
        "HOW: Main steps in sentence selection according to word probabilities Step 1 Estimate word weights (probabilities)",
        "Is this a reasonable approach: yes, people seem to be doing something similar",
        "Simple test Compute word probability table from the input Get a batch of summaries written by H(umans) and S(ystems) Compute the likelihood of the summaries given the word probability table Results Human summaries have higher likelihood"
      ]
    },
    {
      "heading": "HSSSSSSSSSSHSSSHSSHHSHHHHH HIGH LIKELIHOODLOW",
      "text": [
        "Obvious shortcomings of the pure frequency approaches",
        "Does not take account of related words suspects -- trail Gadhafi ?",
        "Libya Does not take into account evidence from other documents Function words: prepositions, articles, etc.",
        "Domain words: ?cell?",
        "in cell biology articles Does not take into account many other aspects",
        "Two easy fixes Lexical chains [Barzilay and Elhadad, 1999, Silber and McCoy, 2002, Gurevych and Nahnsen, 2005] Exploits existing lexical resources (WordNet) TF*IDF weights [most summarizers] Incorporates evidence from a background corpus",
        "Lexical chains and WordNet relations Lexical chains Word sense disambiguation is performed Then topically related words represent a topic Synonyms, hyponyms, hypernyms Importance is determined by frequency of the words in a topic rather than a single word One sentence per topic is selected Concepts based on WordNet [Schiffman et al., 2002, Ye et al., 2007] No word sense disambiguation is performed {war, campaign, warfare, effort, cause, operation} {concern, carrier, worry, fear, scare}",
        "TF*IDF weights for words Combining evidence for document topics from the input and from a background corpus",
        "Topic words (topic signatures) Which words in the input are most descriptive?",
        "Instead of assigning probabilities or weights to all words, divide words into two classes: descriptive or not For iterative sentence selection approach, the binary distinction is key to the advantage over frequency and TF*IDF Systems based on topic words have proven to be the most successful in official summarization evaluations",
        "Example input and associated topic words Input for summarization: articles relevant to the following user need Title: Human Toll of Tropical Storms Narrative: What has been the human toll in death or injury of tropical storms in recent years?",
        "Where and when have each of the storms caused human casualties?",
        "What are the approximate total number of casualties attributed to each of the storms?",
        "ahmed, allison, andrew, bahamas, bangladesh, bn, caribbean, carolina, caused, cent, coast, coastal, croix, cyclone, damage, destroyed, devastated, disaster, dollars, drowned, flood, flooded, flooding, floods, florida, gulf, ham, hit, homeless, homes, hugo, hurricane, insurance, insurers, island, islands, lloyd, losses, louisiana, manila, miles, nicaragua, north, port, pounds, rain, rains, rebuild, rebuilding, relief, remnants, residents, roared, salt, st, storm, storms, supplies, tourists, trees, tropical, typhoon, virgin, volunteers, weather, west, winds, yesterday."
      ]
    },
    {
      "heading": "Topic Words",
      "text": [
        "Formalizing the problem of identifying topic words",
        "Given t: a word that appears in the input T: cluster of articles on a given topic (input) NT: articles not on topic T (background corpus) Decide if t is a topic word or not Words that have (almost) the same probability in T and NT are not topic words",
        "Computing probabilities View a text as a sequence of Bernoulli trails A word is either our term of interest t or not The likelihood of observing term t which occurs with probability p in a text consisting of N words is given by Estimate the probability of t in three ways Input + background corpus combines Input only",
        "Testing which hypothesis is more likely: log-likelihood ratio test has a known statistical distribution: chi-square At a given significance level, we can decide if a word is descriptive of the input or not.",
        "This feature is used in the best performing systems for multi-document summarization of news [Lin and Hovy, 2000; Conroy et al., 2006] Likelihood of the data given H1 Likelihood of the data given H2",
        "The background corpus takes more central stage",
        "Learn topics from the background corpus topic ~ themes often discusses in the background topic representation ~ word probability tables Usually one time training step To summarize an input Select sentences from the input that correspond to the most prominent topics",
        "Latent semantic analysis (LSA) [Gong and Liu, 2001, Hachey et al., 2006, Steinberger et al., 2007] Discover topics from the background corpus with n unique words and d documents Represent the background corpus as nxd matrix A Rows correspond to words Aij=number of times word I appears in document j Use standard change of coordinate system and dimensionality reduction techniques In the new space each row corresponds to the most important topics in the corpus Select the best sentence to cover each topic",
        "Notes on LSA and other approaches The original article that introduced LSA for single document summarization of news did not find significant difference with TF*IDF For multi-document summarization of news LSA approaches have not outperformed topic words or extensions of frequency approaches Other topic/content models have been much more influential",
        "Cluster sentences from these documents Implicit topics Obtain a word probability table for each topic Counts only from the cluster representing the topic Select sentences from the input with highest probability for main topics",
        "Text structure can be learnt",
        "Topic = cluster of similar sentences from the background corpus",
        "Sentences cluster from earthquake articles Topic ?earthquake location?",
        "The Athens seismological institute said the temblor's epicenter was located 380 kilometers (238 miles) south of the capital.",
        "Seismologists in Pakistan's Northwest Frontier Province said the temblor's epicenter was about 250 kilometers (155 miles) north of the provincial capital Peshawar.",
        "The temblor was centered 60 kilometers (35 miles) north-west of the provincial capital of Kunming, about 2,200 kilometers (1,300 miles) southwest of Beijing, a bureau seismologist said.",
        "Content model [Barzilay and Lee, 2004, Pascale et al., 2003] Hidden Markov Model (HMM)-based States - clusters of related sentences ?topics?",
        "Transition prob.",
        "- sentence precedence in corpus Emission prob.",
        "- bigram language model",
        "Learning the content model",
        "Many articles from the same domain Cluster sentences: each cluster represents a topic from the domain Word probability tables for each topic Transitions between clusters can be computed from sentence adjacencies in the original articles Probabilities of going from one topic to another Iterate between clustering and transition probability estimation to obtain domain model",
        "To select a summary",
        "Find main topics in the domain using a small collection of summary-input pairs Find the most likely topic for each sentence in the input Select the best sentence per main topic",
        "Historical note Some early approaches to multi-document summarization relied on clustering the sentences in the input alone [McKeown et al., 1999,",
        "Example cluster Choose one sentence to represent the cluster 1.",
        "PAL was devastated by a pilots' strike in June and by the region's currency crisis.",
        "2.",
        "In June, PAL was embroiled in a crippling three-week pilots' strike.",
        "3.",
        "Tan wants to retain the 200 pilots because they stood by",
        "him when the majority of PAL's pilots staged a devastating strike in June."
      ]
    },
    {
      "heading": "One for general English",
      "text": [
        "One for each of the inputs to be summarized One for each document in any input To select a summary S with L words from document collection D given as input The goal is to select the summary, not a sentence.",
        "Greedy selection vs. global will be discussed in detail later",
        "Intriguing side note",
        "In the full Bayesian topic models, word probabilities for all words is more important than binary distinctions of topic and non-topic word Haghighi and Vanderwende report that a system that chooses the summary with highest expected number of topic words performs as SumBasic",
        "Advantages of the graph model Combines word frequency and sentence clustering Gives a formal model for computing importance: random walks Normalize weights of edges to sum to 1 They now represent probabilities of transitioning from one node to another",
        "Random walks for summarization Represent the input text as graph Start traversing from node to node following the transition probabilities occasionally hopping to a new node What is the probability that you are in any particular node after doing this process for a certain time?",
        "Standard solution (stationary distribution) This probability is the weight of the sentence",
        "For extractive summarization, the task can be represented as binary classification A sentence is in the summary or not Use statistical classifiers to determine the score of a sentence: how likely it's included in the summary Feature representation for each sentence Classification models trained from annotated data Select the sentences with highest scores (greedy for now, see other selection methods later)",
        "So that is it with supervised methods?",
        "It seems it is a straightforward classification problem What are the issues with this method?",
        "How to get good quality labeled training data How to improve learning Some recent research has explored a few directions Discriminative training, regression, sampling, co",
        "Improving supervised methods: different training approaches",
        "What are the problems with standard training methods?",
        "Classifiers learn to determine a sentence's label (in summary or not) Sentence-level accuracy is different from summarization evaluation criterion (e.g., summary-level ROUGE scores) Training criterion is not optimal Sentences?",
        "labels used in training may be too strict (binary classes)",
        "Improving supervised methods: MERT discriminative training Discriminative training based on MERT [Aker et al., 2010] In training, generate multiple summary candidates (using A* search algorithm) Adjust model parameters (feature weights) iteratively to optimize ROUGE scores Note: MERT has been used for machine translation discriminative training",
        "Improving supervised methods: ranking approaches Ranking approaches [Lin et al. 2010] Pairwise training Not classify each sentence individually Input to learner is a pair of sentences Use Rank SVM to learn the order of two sentences Direct optimization Learns how to correctly order/rank summary candidates (a set of sentences) Use AdaRank [Xu and Li 2007] to combine weak rankers",
        "Improving supervised methods: regression model Use regression model [Xie and Liu, 2010] In training, a sentence's label is not +1 and 1 Each one is labeled with numerical values to represent their importance Keep +1 for summary sentence For non-summary sentences (-1), use their similarity to the summary as labels Train a regression model to better discriminate sentence candidates",
        "Improving supervised methods: sampling Problems -- in binary classification setup for summarization, the two classes are imbalanced",
        "Summary sentences are minority class.",
        "Imbalanced data can hurt classifier training How can we address this?",
        "Sampling to make distribution more balanced to train classifiers Has been studied a lot in machine learning 66 Improving supervised methods: sampling Upsampling: increase minority samples Replicate existing minority samples Generate synthetic examples (e.g., by some kind of interpolation) Downsampling: reduce majority samples Often randomly select from existing majority samples",
        "Improving supervised methods: sampling Sampling for summarization [Xie and Liu, 2010] Different from traditional upsampling and downsampling Upsampling select non-summary sentences that are like summary sentences based on cosine similarity or ROUGE scores change their label to positive Downsampling: select those that are different from summary sentences These also address some human annotation disagreement The instances whose labels are changed are often the ones that humans have problems with",
        "are expressed by only one person Do humans agree on summary sentence selection?",
        "Human agreement on word/sentence/fact selection",
        "A lot of research has been done on semi-supervised learning for various tasks Co-training and active learning have been used in summarization",
        "summary sentences and the sentences in the lecture slides are high",
        "In summarization methods we try to find 1.",
        "Most significant sentences 2.",
        "Remove redundant ones 3.",
        "Keep the summary under given length Can we combine all 3 steps in one?",
        "Optimize all 3 parameters at once",
        "Greedy solution is an approximate algorithm which may not be optimal Choose the most relevant + least redundant sentence if the total length does not exceed the summary length Maximal Marginal Relevance is one such greedy algorithm proposed by [Carbonell et al., 1998]",
        "Relevance with query or centroid We can compute relevance of text snippet with respect to query or centroid Centroid as defined in [Radev, 2004] based on the content words of a document TF*IDF vector of all documents in corpus Select words above a threshold : remaining vector is a centroid vector",
        "and Favre, 2009; Gillick et al., 2009; McDonald, 2007] Greedy algorithm is an approximate solution Use exact solution algorithm with ILP (scaling issues though) ILP is constrained optimization problem Cost and constraints are linear in a set of integer variables Many solvers on the web Define the constraints based on relevance and redundancy for summarization Sentence based ILP N-gram based ILP",
        "of n-gram i in summary and its weight is wi",
        "function than one shown before",
        "given the constraint |S |=< K (budget)",
        "there there are a variety of ways of doing itme010 Broadcast news transcripts and summary (in red) try to use electrical appliances before p.m. and after p.m. and turn off computers, copiers and lights when they're not being used set your thermostat at 68 degrees when you're home, 55 degrees when you're away energy officials are offering tips to conserve electricity, they say, to delay holiday lighting until after at night the area shares power across many states meanwhile, a cold snap in the pacific northwest is putting an added strain on power supplies coupled with another unit, it can provide enough power for about 2 million people it had been shut down for maintenance a unit at diablo canyon nuclear plant is expected to resume production today california's strained power grid is getting a boost today which might help increasingly taxed power supplies",
        "Speech vs. text summarization: similarities When high quality transcripts are available Not much different from text summarization Many similar approaches have been used Some also incorporate acoustic information For genres like broadcast news, style is also similar to text domains",
        "Speech vs. text summarization: differences Challenges in speech summarization Speech recognition errors can be very high Sentences are not as well formed as in most text domains: disfluencies, ungrammatical There are not clearly defined sentences Information density is also low (off-topic discussions, chit chat, etc.)",
        "Multiple participants",
        "What should be extraction units in speech summarization?",
        "Text domain Typically use sentences (based on punctuation marks) Speech domain Sentence information is not available Sentences are not as clearly defined",
        "Utterance from previous example: there there are a variety of ways of doing it uh let me just mention something that i don't want to pursue today which is there are technical ways of doing it",
        "information, acoustic info (pause, pitch, energy)",
        "Using acoustic information in summarization Are acoustic features useful when combining it with lexical information?",
        "Results vary depending on the tasks and domains Often lexical features are ranked higher But acoustic features also contribute to overall system performance Some studies showed little impact when adding speech information to textual features [Penn and Zhu, 2008]",
        "Using acoustic information in summarization",
        "Can we use acoustic information only for speech summarization?",
        "Transcripts may not be available Another way to investigate contribution of acoustic information Studies showed using just acoustic information can achieve similar performance to using lexical information [Maskey and Hirschberg, 2005; Xie et al., 2009; Zhu et al., 2009] Caveat: in some experiments, lexical information is used",
        "(e.g., define the summarization units)",
        "Speech recognition errors Some studies evaluated effect of recognition errors on summarization by varying word error rate [Christensen et al., 2003; Penn and Zhu, 2008; Lin et al., 2009] Degradation is not much when word error rate is not too low (similar to spoken document retrieval) Reason: better recognition accuracy in summary sentences than overall",
        "What can we do about ASR errors?",
        "Deliver summary using original speech Can avoid showing recognition errors in the delivered text summary But still need to correctly identify summary sentences/segments Use recognition confidence measure and multiple candidates to help better summarize",
        "Address problems due to ASR errors Redefine summarization task: select sentences that are most informative, at the same time have high recognition accuracy Important words tend to have high recognition accuracy Use ASR confidence measure or n-gram language model scores in summarization Unsupervised methods [Zechner, 2002; Kikuchi et al., 2003; Maskey, 2008] Use as a feature in supervised methods",
        "Address problems due to ASR errors",
        "Disfluencies and summarization Disfluencies (filler words, repetitions, revisions, restart, etc) are frequent in conversational speech Example from meeting transcript: so so does i-just remind me of what what you were going to do with the what what what what's y-you just described what you've been doing Existence may hurt summarization systems, also affect human readability of the summaries",
        "that is between 25% and 50% different depending on which of two available human extracts is used for evaluation Recall more important/informative than precision?",
        "More problems?"
      ]
    },
    {
      "heading": "Granularity",
      "text": [
        "We need help.",
        "Fires have spread in the nearby forest and threaten several villages in this remote area.",
        "SCU: label, weight, contributors Label London was where Pinochet was arrested Weight=3 S1 Pinochet arrested in London on Oct 16 at a Spanish judge's request for atrocities against Spaniards in Chile.",
        "S2 Former Chilean dictator Augusto Pinochet has been arrested in London at the request of the Spanish government.",
        "S3 Britain caused international controversy and Chilean turmoil by arresting former Chilean dictator Pinochet in London.",
        "Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well",
        "Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well",
        "Ideally informative summary Does not include an SCU from a lower tier unless all SCUs from higher tiers are included as well",
        "Distance between two distributions as average KL divergence from their mean distribution JS divergence between input and"
      ]
    },
    {
      "heading": "References",
      "text": []
    },
    {
      "heading": "References",
      "text": []
    },
    {
      "heading": "References",
      "text": []
    },
    {
      "heading": "References",
      "text": []
    }
  ]
}
