{
  "info": {
    "authors": [
      "Gregory Druck",
      "Kuzman Ganchev",
      "João Graça"
    ],
    "book": "Tutorial Abstracts of ACL 2011",
    "id": "acl-P11-5005",
    "title": "Rich Prior Knowledge in Learning for Natural Language Processing",
    "url": "https://aclweb.org/anthology/P11-5005",
    "year": 2011
  },
  "references": [
    "acl-D09-1009",
    "acl-D09-1014",
    "acl-D10-1025",
    "acl-D10-1120",
    "acl-N10-1009",
    "acl-P07-1036",
    "acl-P08-1099",
    "acl-P09-1041",
    "acl-P09-1042"
  ],
  "sections": [
    {
      "text": [
        "This approach does not scale to every task and domain of interest.",
        "However, we already know a lot about most problems of interest.",
        "labeled features: information about the label distribution when word w is present",
        "the word ACM should be labeled either journal or booktitle most of the time ?",
        "non-Markov (long-range) dependencies: ?",
        "each reference has at most one segment of each type W. H. Enright.",
        "Improving the efficiency of matrix operations in the numerical solution of stiff ordinary differential equations.",
        "ACM Trans.",
        "Math.",
        "Softw., 4(2), 127-136, June 1978. extraction from research papers:",
        "linguistic knowledge: each sentence should have a verb ?",
        "posterior sparsity: the total number of different POS tags assigned to each word type should be small Tags A career with the European institutions must become more attractive.",
        "Too many young, new...",
        "Bijectivity: alignment should be mostly one-to-one ?",
        "Symmetry: source?target and target?source alignments should agree A career with the European institutions must become more attractive.",
        "Uma carreira nas institui?",
        "?es europeias t?m de se tornar mais atractiva.",
        "This Tutorial In general, how can we leverage such knowledge and an unannotated corpus during learning?",
        "Notation & Models input variables (documents, sentences): structured output variables (parses, sequences): unstructured output variables (labels): input / output variables for entire corpus: probabilistic model parameters: generative models: discriminative models: model feature function:",
        "model: Maximum Entropy Classifier (Logistic Regression) ?",
        "setting: lightly supervised; no labeled data ?",
        "prior knowledge: ?",
        "labeled features: information about the label distribution when word w is present ?",
        "label is often hockey or baseball when game is present",
        "approach: Encode prior knowledge with a prior on parameters.",
        "limitation: Our prior knowledge is not about parameters!",
        "Parameters are difficult to interpret; hard to get desired effect.",
        "?",
        "Example #1: often (not always) game ?",
        "{hockey,baseball} ?",
        "Example #2: alignment should be mostly one-to-one natural: ?",
        "should be small (or sparse)??",
        "( informative prior ) possible: ?",
        "should be close to ?",
        "?i ?",
        "?i",
        "A Language for Encoding Prior Knowledge Our prior knowledge is about distributions over latent output variables.",
        "(output variables are interpretable) Specifically, we know some properties of this distribution: ?",
        "Example #1: often (not always) game?",
        "{hockey,baseball} Formulation: know about the expectations of some functions under distribution over latent output variables",
        "returns a vector with mth value = number of target words in sentence x that align with source word m",
        "vector with expected distribution over labels for documents that contain w ( is the count of w)",
        "Choosing parameters Model Family: conditional exponential models Objective: maximize observed data likelihood Note: Frameworks also suitable for generative models (no labeled data necessary)",
        "A language for prior information The expectations of user-defined constraint features are close to some value"
      ]
    },
    {
      "heading": "Running Example:",
      "text": [
        "Want to ensure that 25% of unlabeled documents are about politics ?",
        "constraint features ?",
        "preferred expected value ?",
        "Expectation w.r.t.",
        "unlabeled data"
      ]
    },
    {
      "heading": "Visual Example: Constraint Driven Learning",
      "text": [
        "where are ?imagined?",
        "labels and",
        "Usually easy if decompose as the model: and ?",
        "Otherwise: Sample (e.g. K. Bellare, G. Druck, and A. McCallum, 2009)",
        "treatment of the labeled data (XL,YL) from treatment of the unlabeled data X. and produce some value ?",
        "(X,Y), which is never observed directly.",
        "Instead, we observe some noisy version b ?",
        "?(X,Y).",
        "The measured values b are distributed according to some noise model pN(b|?(X,Y)).",
        "Liang et al. [2009] note that the optimization is convex for log-concave noise and use box noise in their experiments, giving b uniform probability in some range near ?(X,Y).",
        "In the Bayesian setting, the model parameters ?",
        "as well as the observed measurement values b are random variables.",
        "Liang et al. [2009] use the mode of p(?|XL,YL,X,b) as a point estimate for ?",
        ":",
        "Y p(?,Y,b|X,XL,YL).",
        "Liang et al. [2009] focus on computing p(?,Y,b|X,XL,YL).",
        "They define their model for this quantity as follows:",
        "where the Y and X are particular instantiations of the random variables in the entire unlabeled corpusX.",
        "Equation 4.7 is a product of three terms: a prior on ?, the model probability p?",
        "(Y|X), and a noise model pN(b|?).",
        "The noise model is the probability that we observe a value, b, of the measurement features ?, given that its actual value was ?(X,Y).",
        "The idea is that we model errors in the estimation of the posterior probabilities as noise in the measurement process.",
        "Liang et al. [2009] use a uniform distribution over ?",
        "(X,Y) ?",
        "?, which they call ?box noise?.",
        "Under this model, observing b farther than ?",
        "from ?",
        "(X,Y) has zero probability.",
        "In log space, the exact MAP objective, becomes:",
        "P. Liang, M. Jordan, D. Klein (2009) Objective: mode of given observations A Bayesian View: Measurements",
        "Figure 4.1: The model used by Liang et l. [2009], using our notation.",
        "We have separated treatment of the labeled data (XL,YL) from treatment of the unlabeled data X. and produce some value ?",
        "(X,Y), which is never observed directly.",
        "Instead, we observe some noisy version b ?",
        "?(X,Y).",
        "The measured values b are distributed according to some noise model pN(b|?(X,Y)).",
        "Liang et al. [2009] note that the optimization is convex for log-concave noise and use box noise in their experiments, giving b uniform probability in some range near ?(X,Y).",
        "In the Bayesian setting, the model parameters ?",
        "as well as the observed measurement values b are random variables.",
        "Liang et al. [2009] use the mode of p(?|XL,YL,X,b) as a point estimate for ?",
        ":",
        "Y p(?,Y,b|X,XL,YL).",
        "Liang et al. [2009] focus on computing p(?,Y,b|X,XL,YL).",
        "They define their model for this quantity as follows:",
        "where the Y and X are particular instantiations of the random variables in the entire unlabeled corpusX.",
        "Equation 4.7 is a product of three terms: a prior on ?, the model probability p?",
        "(Y|X), and a noise model pN(b|?).",
        "The noise model is the probability that we observe a value, b, of the measurement features ?, given that its actual value was ?(X,Y).",
        "The idea is that we model errors in the estimation of the posterior probabilities as noise in the measurement process.",
        "Liang et al. [2009] use a uniform distribution over ?",
        "(X,Y) ?",
        "?, which they call ?box noise?.",
        "Under this model, observing b farther than ?",
        "from ?",
        "(X,Y) has zero probability.",
        "In log space, the exact MAP objective, becomes:",
        "What's wrong with this picture?",
        "Objective: mode of given observations Example: Exactly 25% of articles are ?politics?",
        "What is the probability exactly 25% of the articles are labeled ``politics''?",
        "How do we optimize this with respect to ?",
        "What's wrong with this picture?",
        "Example: Compute prob: 25% of docs are ?politics?.",
        "Naively: in this case we can use a DP, but if there are many constraints, that doesn't work.",
        "Easier: What is the expected number of ?politics?",
        "articles?",
        "difficult to compute expectations of arbitrary functions but... Usually: decomposes as a sum e.g. 25% of articles are ?politics?",
        "Idea: approximate",
        "this can be hard because and the usual dynamic programs (inside outside, forward backward) can't compute this.",
        "C ov( ?",
        ", f) = E [ ??",
        "f ] ?",
        "E [ ? ]",
        "?",
        "E [ f ]",
        "E.g. if inference is a hypergraph problem.",
        "often we can still provide some light supervision ?",
        "prior knowledge: labeled features ?",
        "formally: have an estimate of the distribution over labels for documents that contain word w:",
        "Detached single family house.",
        "3 bedrooms 1 1/2 baths.",
        "Almost 1000 square feet in living area.",
        "1 car garage.",
        "New pergo floor and tile kitchen floor.",
        "New interior/exterior paint.",
        "Close to shopping mall and bus stop.",
        "Near 101/280.",
        "Available July 1, 2004.",
        "If you are interested, email for more details.",
        "Cousot, P. and Cousot, R. 1978.",
        "Static determination of dynamic properties of recursive procedures.",
        "In Proceedings of the IFIP Conference on Programming Concepts, E. Neuhold,",
        "expectation: label distribution when q is true model: Linear Chain CRF note: Semiring trick makes GE",
        "constraint features: vector with a 1 in the lth position if y is the lth label and predicate q is true (i.e. w is present at i) ?",
        "q (x, yi , i) = 1 (y i = l)q(x, i)",
        "a run into town.",
        "of the mile run.",
        "run gold.",
        "run errands.",
        "run for mayor.",
        "is Gold, middle is EM, and bottom is PR.",
        "since then it does not have to pay the cost of assigning a parent with a new tag to cover each noun that does not come with a determiner.",
        "Table 4 contrasts the most frequent types of errors EM, SDP, and PR make on several test sets where PR does well.",
        "The ?acc?",
        "column is accuracy and the ?errs?",
        "column is the absolute number of errors of the key type.",
        "Accuracy for the key ?parent POS truth/guess?",
        "child POS?",
        "is computed as a function of the true relation.",
        "So, if the key is pt /p g ?",
        "c , then accuracy is:",
        "In the following subsections we provide some analysis of the results from Table 4."
      ]
    },
    {
      "heading": "7.1 English Corrections",
      "text": [
        "Considering English first, there are several notable differences between EM and PR errors.",
        "Similar to the example for Spanish, the direction of the noun-determiner relation is corrected by PR.",
        "This is reflected by the VB/DT?",
        "NN key, the NN/VBZ?",
        "DT key, the NN/IN?",
        "DT key, the IN/DT?",
        "NN key, the NN/VBD?",
        "DT key, the NN/VBP?",
        "DT key, and the NN/VB?",
        "DT key, which for EM and SDP have accuracy 0.",
        "PR corrects these errors.",
        "A second correction PR makes is reflected in the VB/TO?",
        "VB key.",
        "One explanation for the reason PR is able to correctly identify VBs as the parents of other VBs instead of mistakenly making TO the parent of VBs is that ?VB CC VB?",
        "is a frequently occurring sequence.",
        "For example, ?build and hold?",
        "and ?panic and bail?",
        "are two instances of the ?VB CC VB?",
        "pattern from the test corpus.",
        "Presented with such scenarios, where there is no TO present to be the parent of VB, PR chooses the first VB as the parent of the second.",
        "It maintains this preference for making the first VB a parent of the second when encountered with ?VB TO VB?",
        "sequences, such as ?used to eliminate?, because it would have to pay an additional penalty to make TO the parent of the second VB.",
        "In this manner, PR corrects the VB/TO?",
        "VB key error of EM and SDP."
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.",
        "Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages.",
        "During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules.",
        "We also automatically refine the syntactic categories given in our coarsely tagged input.",
        "Across six languages our approach outperforms state-of-the-art unsupervised methods by a significant margin.1"
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure.",
        "These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing.",
        "In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010).",
        "dependent relationships between coarse (i.e., unsplit) syntactic categories.",
        "An explanation of the ruleset is provided in Section 5.",
        "In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules.",
        "As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-of-speech tags) and a set of universal rules defined over these categories, such as those in Table 1.",
        "These rules incorporate the definitional properties of syntactic categories in terms of their interdependencies and thus are universal across languages.",
        "They can potentially help disambiguate structural ambiguities that are difficult to learn from data alone ?",
        "for example, our rules prefer analyses in which verbs are dependents of auxiliaries, even though analyz-ing auxiliaries as dependents of verbs is also consistent with the data.",
        "Leveraging these universal rules has the potential to improve parsing performance for a large number of human languages; this is particularly relevant to the processing of low-resource Small set of universal rules = 1 if edge in rule set",
        "inference algorithms implemented (MaxEnt, CRF) ?",
        "primarily need to write methods for E-step (projection): ?",
        "restriction: constraints must factor with model compute constraint features and expectations compute scores under q for E-step compute objective function for E-step compute gradient for E-step"
      ]
    },
    {
      "heading": "GE Implementation Advice",
      "text": [
        "?",
        "computing covariance (required for gradient): ?",
        "trick: compute cov.",
        "of composite constraint feature ?",
        "example: penalty: ?",
        "result: only need to store vectors of size in computation, rather than covariance matrix ?",
        "trick: efficient gradient computation in hypergraphs ?",
        "use semiring algorithms of [Li & Eisner 09] ?",
        "result: same time complexity as supervised (w. both)",
        "For a more up-to-date bibliography as well as additional information about these methods, point your browser to: http://sideinfo.wikkii.com/"
      ]
    },
    {
      "heading": "1 Constraint-Driven Learning",
      "text": [
        "Constraint driven learning (CoDL) was first introduced in Chang et al. [2007], and has been used also in Chang et al. [2008].",
        "A further paper on the topic is in submission [Chang et al., 2010]."
      ]
    },
    {
      "heading": "2 Generalized Expectation",
      "text": [
        "Generalized Expectation (GE) constraints were first introduced by Mann and McCallum [2007] 1 and were used to incorporate prior knowledge about the label distribution into semi-supervised classification.",
        "GE constraints have also been used to leverage ?labeled features?",
        "in document classification [Druck et al., 2008] and information extraction [Mann and McCallum, 2008, Druck et al., 2009b, Bellare and McCallum, 2009], and to incorporate linguistic prior knowledge into dependency grammar induction [Druck et al., 2009a]."
      ]
    },
    {
      "heading": "3 Posterior Regularization",
      "text": [
        "The most clearly written overview of Posterior Regularization (PR) is Ganchev et al. [2010].",
        "PR was first introduced in Graca et al. [2008], and has been applied to dependency grammar induction [Ganchev et al., 2009, Gillenwater et al., 2009, 2011, Naseem et al., 2010], part of speech induction [Grac?a et al., 2009a], multi-view learning [Ganchev et al., 2008], word alignment [Graca et al., 2008, Ganchev et al., 2009, Grac?a et al., 2009b], and cross-lingual semantic alignment [Platt et al., 2010].",
        "The framework was independently discovered by Bellare et al. [2009] as an approximation to GE constraints, under the name Alternating Projections, and used under that name also by Singh et al. [2010] and Druck and McCallum [2010] for information extraction.",
        "The framework was also independently discovered by Liang et al. [2009] as an approximation to 1In Mann and McCallum [2007] the method was called Expectation Regularization.",
        "a Bayesian model motivated by modeling prior information as measurements, and applied to information extraction."
      ]
    },
    {
      "heading": "4 Closely related frameworks",
      "text": [
        "Quadrianto et al. [2009] introduce a distribution matching framework very closely related to GE constraints, with the idea that the model should predict the same feature expectations on labeled and undlabeled data for a set of features, formalized as a kernel.",
        "Carlson et al. [2010] introduce a framework for semi-supervised learning based on constraints, and trained with an iterative update algorithm very similar to CoDL, but introducing only confident constraints as the algorithm progresses.",
        "Gupta and Sarawagi [2011] introduce a framework for agreement that is closely related to the PR-based work in Ganchev et al. [2008], with a slightly different objective and a different training algorithm."
      ]
    }
  ]
}
