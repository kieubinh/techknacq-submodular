{
  "info": {
    "authors": [
      "James Blythe",
      "Jerry R. Hobbs",
      "Pedro Domingos",
      "Rohit J. Kate",
      "Raymond J. Mooney"
    ],
    "book": "Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011)",
    "id": "acl-W11-0107",
    "title": "Implementing Weighted Abduction in Markov Logic",
    "url": "https://aclweb.org/anthology/W11-0107",
    "year": 2011
  },
  "references": [],
  "sections": [
    {
      "text": [
        "James Blythe Jerry R. Hobbs",
        "USC ISI USC ISI",
        "Abduction is a method for finding the best explanation for observations.",
        "Arguably the most advanced approach to abduction, especially for natural language processing, is weighted abduction, which uses logical formulas with costs to guide inference.",
        "But it has no clear probabilistic semantics.",
        "In this paper we propose an approach that implements weighted abduction in Markov logic, which uses weighted first-order formulas to represent probabilistic knowledge, pointing toward a sound probabilistic semantics for weighted abduction.",
        "Application to a series of challenge problems shows the power and coverage of our approach."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Abduction is inference to the best explanation.",
        "Typically, one uses it to find the best hypothesis explaining a set of observations, e.g., in diagnosis and plan recognition.",
        "In natural language processing the content of an utterance can be viewed as a set of observations, and the best explanation then constitutes the interpretation of the utterance.",
        "Hobbs et al.",
        "[7] described a variety of abduction called \"weighted abduction\" for interpreting natural language discourse.",
        "The key idea was that the best interpretation of a text is the best explanation or proof of the logical form of the text, allowing for assumptions.",
        "What counted as \"best\" was defined in terms of a cost function which favored proofs with the fewest number of assumptions and the most salient and plausible axioms, and in which the pervasive redundancy implicit in natural language discourse was exploited.",
        "It was argued in that paper that such interpretation problems as coreference and syntactic ambiguity resolution, determining the specific meanings of vague predicates and lexical ambiguity resolution, metonymy resolution, metaphor interpretation, and the recognition of discourse structure could be seen to \"fall out\" of the best abductive proof.",
        "Specifically, weighted abduction has the following features:",
        "1.",
        "In a goal expression consisting of an existentially quantified conjunction of positive literals, each literal is given a cost that represents the utility of proving that literal as opposed to assuming it.",
        "That is, a low cost on a literal will make it more likely for it to be assumed, whereas a high cost will result in a greater effort to find a proof.",
        "the DARPA, AFRL, ONR, ARO, or the US government.",
        "2.",
        "Costs are passed back across the implication in Horn clauses according to weights on the conjuncts in the antecedents.",
        "Specifically, if a consequent costs $c and the weight on a conjunct in the antecedent is v, then the cost on that conjunct will be $vc.",
        "Note that if the weights add up to less than one, backchaining on the rule will be favored, as the cost of the antecedent will be less than the cost of the consequent.",
        "If the weights add up to more than one, backchaining will be disfavored unless a proof can be found for one or more of the conjuncts in the antecedent, thereby providing partial evidence for the consequent.",
        "3.",
        "Two literals can be factored or unified, where the result is given the minimum cost of the two, providing no contradiction would result.",
        "This is a frequent mechanism for coreference resolution.",
        "In practice, only a shallow or heuristic check for contradiction is done.",
        "4.",
        "The lowest-cost proof is the best interpretation, or the best abductive proof of the goal expression.",
        "However, there are two significant problems with weighted abduction as it was originally presented.",
        "First, it required a large knowledge base of commonsense knowledge.",
        "This was not available when weighted abduction was first described, but since that time there have been substantial efforts to build up knowledge bases for various purposes, and at least two of these have been used with promising results in an abductive setting â€“ Extended WordNet [6] for question-answering and FrameNet [11] for textual inference.",
        "The second problem with weighted abduction was that the weights and costs did not have a probabilistic semantics.",
        "This, for example, hampers automatic learning of weights from data or existing resources.",
        "That is the issue we address in the present paper.",
        "In the last decade and a half, a number of formalisms for adding uncertain reasoning to predicate logic have been developed that are well-founded in probability theory.",
        "Among the most widely investigated is Markov logic [14, 4].",
        "In this paper we show how weighted abduction can be implemented in Markov logic.",
        "This demonstrates that Markov logic networks can be used as a powerful mechanism for interpreting natural language discourse, and at the same time provides weighted abduction with something like a probabilistic semantics.",
        "In Section 2 we briefly describe Markov logic and Markov logic networks.",
        "Section 3 then describes how weighted abduction can be implemented in Markov logic.",
        "In Section 4 we describe experiments in which fourteen published examples of the use of weighted abduction in natural language understanding are implemented in Markov logic networks, with good results.",
        "Section 5 on current and future directions briefly describes an ongoing experiment in which we are attempting to scale up to apply this procedure to the textual inference problem with a knowledge base derived from FrameNet with tens of thousands of axioms."
      ]
    },
    {
      "heading": "2. Markov Logic Networks and Related Work",
      "text": [
        "Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order logic and probabilistic graphical models.",
        "A traditional first-order knowledge base can be seen as a set of hard constraints on the set of possible worlds: if a world violates even one formula, its probability is zero.",
        "In order to soften these constraints, Markov logic attaches a weight to each first-order logic formula in the knowledge base.",
        "Such a set of weighted first-order logic formulae is called a Markov logic network (MLN).",
        "A formula's weight reflects how strong a constraint it imposes on the set of possible worlds: the higher the weight, the lower the probability of a world that violates it; however, that probability need not be zero.",
        "An MLN with all infinite weights reduces to a traditional first-order knowledge base with only hard constraints.",
        "Formally, an MLN L is a set of formula-weight pairs (Fi, wj).",
        "Given a set of constants, it defines a joint probability distribution over a set of boolean variables X = (X\\, X2...) corresponding to the possible groundings (using the given constants) of the literals present in the first-order formulae:",
        "where ui(x) is the number of true groundings of Fi in x and Z is a normalization term obtained by summing P(X = x) over all values ofX.",
        "Semantically, an MLN can be viewed as a set of templates for constructing Markov networks [12], the undirected counterparts of Bayesian networks.",
        "An MLN and a set of constants produce a Markov network in which each ground literal is a node and every pair of ground literals that appear together in some grounding of some formula are connected by an edge.",
        "Different sets of constants produce different Markov networks; however, there are certain regularities in their structure and parameters.",
        "For example, all groundings of the same formula have the same weight.",
        "Probabilistic inference for an MLN (such as finding the most probable truth assignment for a given set of ground literals, or finding the probability that a particular formula holds) can be performed by first producing the ground Markov network and then using well known inference techniques for Markov networks, like Gibbs sampling.",
        "Given a knowledge base as a set of first-order logic formulae, and a database of training examples each consisting of a set of true ground literals, it is also possible to learn appropriate weights for the MLN formulae which maximize the probability of the training data.",
        "An open-source software package for MLNs, called Alchemy , is also available with many built-in algorithms for performing inference and learning.",
        "Much of the early work on abduction was done in a purely logical framework (e.g., [13, 3, 9, 10].",
        "Typically the choice between alternative explanations is made on the basis of parsimony; the shortest proofs with the fewest assumptions are favored.",
        "However, a significant limitation of these purely logical approaches is that they are unable to reason under uncertainty or estimate the likelihood of alternative explanations.",
        "A probabilistic form of abduction is needed in order to account for uncertainty in the background knowledge and to handle noisy and incomplete observations.",
        "In Bayesian networks [12] background knowledge with its uncertainties is encoded in a directed graph.",
        "Then, given a set of observations, probabilistic inference over the graph structure is done to compute the posterior probability of alternative explanations.",
        "However, Bayesian networks are based on propositional logic and cannot handle structured representations, hence preventing their use in situations, characteristic of natural language processing, that involve an unbounded number of entities with a variety of relations between them.",
        "In recent years there have been a number of proposals attempting to combine the probabilistic nature of Bayesian networks with structured first-order representations.",
        "It is impossible here to review this literature here.",
        "A a good review of much of it can be found in [5], and in [14] there are detailed comparisonss of various models to MLNs.",
        "Charniak and Shimony [2] define a variant of weighted abduction, called \"cost-based abduction\" in which weights are attached to terms rather than to rules or to antecedents in rules.",
        "Thus, the term Pi has the same cost whatever rule it is used in.",
        "The cost of an assignment to the variables in the domain is the sum of the costs of the variables that are true in the assignment.",
        "Charniak and Shimony provide a probabilistic semantics for their approach by showing how to construct a Bayesian network from a domain such that a most probable explanation solution to the Bayes net corresponds to a lowest-cost solution to the abduction problem.",
        "However, in natural language applications the utility of proving a proposition can vary by context; weighted abduction accomodates this, whereas cost-based abduction",
        ".cs.washington.edu"
      ]
    },
    {
      "heading": "3. Weighted Abduction and MLNs",
      "text": [
        "Kate and Mooney [8] show how logical abduction can be implemented in Markov logic networks.",
        "They use forward inference in MLNs to perform abduction by adding clauses with reverse implications.",
        "Universally quantified variables from the left hand side of rules are converted to existentially quantified variables in the reversed clause.",
        "For example, suppose we have the following rule saying that mosquito bites transmit malaria:",
        "mosquito(x) A iufected(x, Malaria) A bite(x, y) D iufected(y, Malaria) This would be translated into the soft rule",
        "[w] iufected(y, Malaria) D 3x[mosquito(x) A iufected(x, Malaria) A bite(x, y)] Where there is more than one possible explanation, they include a closure axiom saying that one of the explanations must hold.",
        "Since blood transfusions also cause malaria, they have the hard rule iufected(y, Malaria) D 3x[mosquito(x) A iufected(x, Malaria) A bite(x, y)] V 3x[iufected(x, Malaria) A trausfuse(Blood, x, y)].",
        "Kate and Mooney also add a soft mutual exclusivity clause that states that no more than one of the possible explanations is true.",
        "In translating between weighted abduction and Markov logic, we need similarly to specify the axioms in Markov logic that correspond to a Horn clause axiom in weighted abduction.",
        "In addition, we need to describe the relation between the numbers in weighted abduction and the weights on the Markov logic axioms.",
        "Hobbs et al.",
        "[7] give only broad, informal guidelines about how the numbers correspond to probabilities.",
        "In this development, we elaborate on how the numbers can be defined more precisely within these guidelines in a way that links with the weights in Markov logic, thereby pointing to a probabilistic semantics for the weighted abduction numbers.",
        "There are two sorts of numbers in weighted abduction â€“ the weights on conjuncts in the antecedents of Horn clause axioms, and the costs on conjuncts in goal expressions, which are existentially quantified conjunctions of positive literals.",
        "We deal first with the weights, then with the costs.",
        "The space of events over which probabilities are taken is the set of proof graphs constituting the best interpretations of a set of texts in a corpus.",
        "Thus, by the probability of p(x) given q(x), we mean the probability that p(x) will occur in a proof graph in which q(x) occurs.",
        "The translation from weighted abduction axioms to Markov logic axioms can be broken into two steps.",
        "First we consider the \"or\" node case, determining the relative costs of axioms that have the same consequent.",
        "Then we look at the \"and\" node case, determining how the weights should be distributed across the conjuncts in the antecedent of a Horn clause, given the total weight for the antecedent.",
        "Weights on Antecedents in Axioms.",
        "First consider a set of Horn clause axioms all with the same consequent, where we collapse the antecedent into a single literal, and for simplicity allow x to stand for all the universally quantified variables in the antecedent, and assume the consequent to have only those variables.",
        "That is, we convert all axioms of the form",
        "To convert this into Markov logic, we first introduce the hard constraint",
        "Ai(x) D q(x).",
        "In addition, given a goal of proving q(x), in weighted abduction we will want to backchain on at least (and usually at most) one of these axioms or we will want simply to assume q(x).",
        "Thus, we can introduce another hard constraint with the disjunction of these antecedents as well as a literal AssumeQ(x) that means q(x) is assumed rather than proved.",
        "q(x) D A1(x) V A2(x) V ... V An(x) V AssumeQ(x).",
        "Then we need to introduce soft constraints to indicate that each of these disjuncts is a possible explanation, or proof, ofq(x), with an associated probability, orweight.",
        "[wi] q(x) D Ai(x), .",
        ".",
        ".",
        "The probability that AssumeQ(x) is true is the conditional probability P0 that none of the antecedents is true given that q(x) is true.",
        "In weighted abduction, when the antecedent weight is greater than one, we prefer assuming the consequent to assuming the antecedent.",
        "When the antecedent weight is less than one we prefer to assume the antecedent.",
        "If the probability that an antecedent Aj,(x) is the explanation of q(x) is greater than P0, it should be given a weighted abduction weight vi less than 1, making it more likely to be chosen.",
        "Correspondingly, if it is less than Po, it should be given a weight vi greater than 1, making it less likely to be chosen.",
        "In general, the weighted abduction weights should be in reverse order of the conditional probabilities Pi that Ai(x) is the explanation ofq(x).",
        "If we assign the weights vi in weighted abduction to be Vj = logPo then this is consistent with informal guidelines in [7] on the meaning of these weights.",
        "We use the logs of the probabilities rather than the probabilities themselves to moderate the effect of one axiom being very much more probable than any of the others.",
        "Kate and Mooney [8], in their translation of logical abduction into Markov logic, also include soft constraints stipulating that the different possible explanations Ai(x) are normally mutually exclusive.",
        "We do not do that here, but we get a kind of soft mutual exclusivity constraint by virtue of the axioms below that levy a cost for any literal that is taken to be true.",
        "In general, more parimonious explanations will be favored.",
        "Nevertheless, in most cases a single explanation will suffice.",
        "When this is true, the probability of Ai(x) holding when q(x) holds is ^.",
        "Then a reasonable approximation for the relation between the weighted abduction weights viand the Markov logic weights wi is Wj = -VjlogPo",
        "Weights on Conjuncts in Antecedents.",
        "Next consider how cost is spread across the conjuncts in the antecedent of a Horn clause in weighted abduction.",
        "Here we use u's to represent the weighted abduction weights on the conjuncts.",
        "The u's should somehow represent the semantic contribution of each conjunct to the conclusion.",
        "That is, given that the conjunct is true, what is the probability that it is part of an explanation of the consequent?",
        "Conjuncts with a higher such probability should be given higher weights u; they play a more significant role in explaining A(x).",
        "Let Pi be the conditional probability of the consequent given the ith conjunct in the antecedent.",
        "and let Z be a normalization factor.",
        "z = En=i Pi",
        "Let v be the weight of the entire antecedent as determined above.",
        "Then it is consistent with the guidelines in [7] to define the weights on the conjuncts as follows:",
        "The weights ui will sum to v and each will correspond to the semantic contribution of its conjunct to the consequent.",
        "In Markov logic, weights apply only to axioms as a whole, not parts of axioms.",
        "Thus, the single axiom above must be decomposed into one axiom for each conjunct and the dependencies must be written as [wj] pj(x) D A(x), ...",
        "The relation between the weighted abduction weights ui and the Markov logic weights wi can be approximated by",
        "Costs on Goals.",
        "The other numbers in weighted abduction are the costs associated with the conjuncts in the goal expression.",
        "In weighted abduction these costs function as utilities.",
        "Some parts of the goal expression are more important to interpret correctly than others; we should try harder to prove these parts, rather than simply assuming them.",
        "In language it is important to recognize the referential anchor of an utterance in shared knowledge.",
        "Thus, those parts of a sentence most likely to provide this anchor have the highest utility.",
        "If we simply assume them, we lose their connection with what is already known.",
        "Those parts of a sentence most likely to be new information will have a lower cost, because we usually would not be able to prove them in any case.",
        "Consider the two sentences",
        "The smart man is tall.",
        "The tall man is smart.",
        "The logical form for each of them will be",
        "In weighted abduction, an interpretation of the sentence is a proof of the logical form, allowing assumptions.",
        "In the first sentence we want to prove smart(x) to anchor the sentence referentially.",
        "Then tall(x) is new information; it will have to be assumed.",
        "We will want to have a high cost on smart (x) to force the proof procedure to find this referential anchor.",
        "The cost on tall(x) will be low, to allow it to be assumed without expending too much effort in trying to locate that fact in shared knowledge.",
        "In the second sentence, the case is the reverse.",
        "Let's focus on the first sentence and assume we know that educated people are smart and big people are tall, and furthermore that John is educated and Bill is big.",
        "In weighted abduction, the best interpretation will be that the smart man is John, because he is educated, and we pay the cost for assuming he is tall.",
        "The interpretation we want to avoid is one that says x is Bill; he is tall because he is big, and we pay the cost of assuming he is smart.",
        "Weighted abduction with its differential costs on conjuncts in the goal expression favors the first and disfavors the second.",
        "In weighted abduction, only assumptions cost; literals that are proved cost nothing.",
        "When the above axioms are translated into Markov logic, it would be natural to capture the differential costs by attaching a negative weight to smart(x) to model the cost associated with assuming it.",
        "However, this weight would apply to any assignment in which smart(J ) is true, regardless of whether it was assumed, derived from an assumed fact, or derived from a known fact.",
        "A potential solution might be to attach the negative weight to AssumeSmart(x).",
        "But the first axiom above allows us to bypass the negative weight on smart(x).",
        "We can hypothesize that x is Bill, pay a low cost on AssumeEducated(B), derive smart(B), and get the wrong assignment.",
        "Thus it is not enough to attach a negative weight to high-cost conjuncts in the goal expression.",
        "This negative weight would have to be passed back through the whole knowledge base, making the complexity of setting the weights at problem time in the MLN knowledge base equal to the complexity of running the inference problem.",
        "An alternative solution, which avoids this problem when the forward inferences are exact, is to use a set of predicates that express knowing a fact without any assumptions.",
        "In the current example, we would add Ksmart(x) for knowing that an entity is smart.",
        "The facts asserted in the data base are now Keducated(J) and Kbig(B).",
        "For each hard axiom involving non-K predicates, we have a corresponding axiom that expresses the relation between the K-predicates, and we have a soft axiom allowing us to cross the border between the K predicates and their non-K counterparts.",
        "Keducated(x) D Ksmart(x)., .",
        ".",
        ".",
        "[w] Ksmart(x) D smart(x), .",
        ".",
        ".",
        "Here the positive weight w attached is chosen to counteract the negative weight we would attach to smart(x) to reflect the high cost of assuming it.",
        "This removes the weight associated with assuming smart(x) regardless of the inference path that leads to knowing smart(x) (KSmart(x))).",
        "Further, this translation takes linear time in the size of the goal expression to compute, since we do not need to know the equivalent weighted abduction cost assigned to the possible antecedents of smart(x).",
        "If the initial facts do not include KEducated(B) and instead educated(B) must be assumed, then the negative weight associated with smart(B) is still present.",
        "In this solution, there is no danger that the inference process can bypass the cost of assuming smart(B), since it is attached to the required predicate and can only be removed by inferring KSmart(B).",
        "Finally, there is a tendency in Markov logic networks for assignments of high probability for propositions for which there is no evidence one way or the other.",
        "To suppress this, we associate a small negative weight with every predicate.",
        "In practice, it has turned out that a weight of â€“ 1 effectively suppresses this behavior."
      ]
    },
    {
      "heading": "4. Experimental Results",
      "text": [
        "We have tested our approach on a set of fourteen challenge problems from [7] and subsequent papers, designed to exercise the principal features of weighted abduction and show its utility for solving natural language interpretation problems.",
        "The knowledge bases used for each of these problems are sparse, consisting of only the axioms required for solving the problems plus a few distractors.",
        "An example of a relatively simple problem is #5 in the table below, resolving \"he\" in the text",
        "I saw my doctor last week.",
        "He told me to get more exercise.",
        "where we are given a knowledge base that says a doctor is a person and a male person is a \"he\".",
        "Solving the problem requires assuming the doctor is male.",
        "The logical form fragment to prove is (3 x)he(x), where we know doctor(D).",
        "A problem of intermediate difficulty (#7) is resolving the three lexical ambiguities in the sentence",
        "The plane taxied to the terminal.",
        "where we are given a knowledge base saying that airplanes and wood smoothers are planes, planes moving on the ground and people taking taxis are both described as \"taxiing\", and computer terminals and airport terminals are both terminals.",
        "An example of a difficult problem is #12, finding the coherence relation, thereby resolving the pronoun \"they\", between the sentences",
        "The police prohibited the women from demonstrating.",
        "They feared violence.",
        "The axioms specify relations between fearing, not wanting, and prohibiting, as well as the defeasible transitivity of causality and the fact that a causal relation between sentences makes the discourse coherent.",
        "The weights in the axioms were mostly distributed evenly across the conjuncts in the antecedents and summed to 1.2.",
        "For each of these problems, we compare the performance of the method described here with a manually constructed gold standard and also with a method based on Kate and Mooney's (KM) approach.",
        "In this method, weights were assigned to the reversed clauses based on the negative log of the sum of weights in the original clause.",
        "This approach does not capture different weights for different antecedents of the same rule, and so has less fidelity to weighted abduction than our approach.",
        "In each case, we used Alchemy's probabilistic inference to determine the most probable explanation (MPE) [12].",
        "In some of the problems the system should make more than one assumption, so there are 22 assumptions in total over all 14 problems in the gold standard.",
        "Using our method, 18 of the assumptions were found, while 15 were found using the KM method.",
        "Table 1 shows the number of correct assumptions found and the running time for the two approaches for each problem.",
        "Our method in particular provides good coverage, with a recall of over 80% of the assumptions made in the gold standard.",
        "It has a shorter running time overall, approximately 5.3 seconds versus 8.7 seconds for the reversal method.",
        "This is largely due to one problem in the test set, problem #9, where the running time for the KM method is relatively high because the technique finds a less sparse network, leading to larger cliques.",
        "There were two problems in the test set that neither approach could solve.",
        "One of these contains predicates that have a large number of arguments, leading to large clique sizes."
      ]
    },
    {
      "heading": "5. Current and Future Directions",
      "text": [
        "In other work [11] we are experimenting with using weighted abduction with a knowledge base with tens of thousands of axioms derived from FrameNet for solving problems in recognizing textual entailment (RTE2) from the Pascal dataset [1].",
        "For a direct comparison between standard weighted abduction and the Markov logic approach described here, we are also experimenting with using the latter on the same task with the same knowledge base.",
        "For each text-hypothesis pair, the sentences are parsed and a logical form is produced.",
        "The output for the first sentence forms the specific knowledge the system has while the output for the second sentence is used as the target to be explained.",
        "If the cost of the best explanation is below a threshold we take the target sentence to be true given the initial information.",
        "It is a major challenge to scale our approach to handle all the problems from the RTE2 development and test sets.",
        "We are not yet able to address the most complex of these using inference in Markov logic networks.",
        "However, we have devised a number of preprocessing steps to reduce the complexity of the resultant network, which significantly increase the number of problems that are tractable.",
        "The FrameNet knowledge base contains a large number of axioms with general coverage.",
        "For any individual entailment problem, most of them are irrelevant and can be removed after a simple graphical analysis.",
        "We are able to remove more irrelevant axioms and predicates with an iterative approach that in",
        "Table 1: Performance on each problem in our test set, comparing two encodings of weighted abduction into Markov logic networks and a gold standard.",
        "each iteration both drops axioms that are shown to be irrelevant and simplifies remaining axioms in such a way as not to change the probability of entailment.",
        "We also simplify predications by removing unnecessary arguments.",
        "The most natural way to convert FrameNet frames to axioms is to treat a frame as a predicate whose arguments are the frame elements for all of its roles.",
        "After converting to Markov logic, this results in rules having large numbers of existentially quantified variables in the consequent.",
        "This can lead to a combinatorial explosion in the number of possible ground rules.",
        "Many of the variables in the frame predicate are for general use and can be pruned in the particular entailment.",
        "Our approach essentially creates abstractions of the original predicates that preserve all the information that is relevant to the current problem but greatly reduces the number of ground instances to consider.",
        "Before implementing these preprocessing steps, only two or three problems could be run to completion on a Macbook Pro with 8 gigabytes of RAM.",
        "After making them, 28 of the initial 100 problems could be run to completion.",
        "Work on this effort continues."
      ]
    },
    {
      "heading": "6. Summary",
      "text": [
        "Weighted abduction is a logical reasoning framework that has been successfully applied to solve a number of interesting and important problems in computational natural-language semantics ranging from word sense disambiguation to coreference resolution.",
        "However, its method for representing and combining assumption costs to determine the most preferred explanation is ad hoc and without a firm theoretical foundation.",
        "Markov Logic is a recently developed formalism for combining first-order logic with probabilistic graphical models that has a well-defined formal semantics in terms of specifying a probability distribution over possible worlds.",
        "This paper has presented a method for mapping weighted abduction to Markov logic, thereby providing a sound probabilistic semantics for the approach and also allowing it to exploit the growing toolbox of inference and learning algorithms available for Markov logic.",
        "Com-plementarily, it has also demonstrated how Markov logic can thereby be applied to help solve important problems in computational semantics.",
        "Our Method",
        "KM Method",
        "Gold",
        "Problem",
        "score",
        "seconds",
        "score",
        "seconds",
        "standard",
        "1",
        "3",
        "300",
        "3",
        "16",
        "3",
        "2",
        "1",
        "250",
        "1",
        "265",
        "1",
        "3",
        "1",
        "234",
        "1",
        "266",
        "1",
        "4",
        "2",
        "234",
        "2",
        "203",
        "2",
        "5",
        "1",
        "218",
        "1",
        "218",
        "1",
        "6",
        "1",
        "218",
        "0",
        "265",
        "1",
        "7",
        "3",
        "300",
        "3",
        "218",
        "3",
        "8",
        "1",
        "200",
        "1",
        "250",
        "1",
        "9",
        "2",
        "421",
        "0",
        "5000",
        "2",
        "10",
        "1",
        "2500",
        "1",
        "1500",
        "3",
        "11",
        "0",
        "0",
        "1",
        "12",
        "0",
        "0",
        "1",
        "13",
        "1",
        "250",
        "1",
        "250",
        "1",
        "14",
        "1",
        "219",
        "1",
        "219",
        "1",
        "Total",
        "18",
        "5344",
        "15",
        "8670",
        "22"
      ]
    }
  ]
}
