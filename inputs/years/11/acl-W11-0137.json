{
  "info": {
    "authors": [
      "Rachel Cotterill"
    ],
    "book": "Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011)",
    "id": "acl-W11-0137",
    "title": "Question Classification for Email",
    "url": "https://aclweb.org/anthology/W11-0137",
    "year": 2011
  },
  "references": [
    "acl-C02-1150",
    "acl-D09-1057"
  ],
  "sections": [
    {
      "text": [
        "Question Classification for Email*",
        "Rachel Cotterill University of Sheffield",
        "Question classifiers are used within Question Answering to predict the expected answer type for a given question.",
        "This paper describes the first steps towards applying a similar methodology to identifying question classes in dialogue contexts, beginning with a study of questions drawn from the Enron email corpus.",
        "Human-annotated data is used as a gold standard for assessing the output from an existing, open-source question classiier (QA-SYS).",
        "Problem areas are identiied and potential solutions discussed."
      ]
    },
    {
      "heading": "1. Introduction and Motivation",
      "text": [
        "In information retrieval, question classification is an important first stage of the question answering task.",
        "A question classification module typically takes a question as input and returns the class of answer which is anticipated.",
        "In an IR context, this enables candidate answers to be identified within a set of documents, and further methods can then be applied to ind the most likely candidate.",
        "The present work is motivated by a desire to identify questions and their answers in the context of written dialogue such as email, with the goal of improving inbox management and search.",
        "Reconstruction of meaning in a single email may often be impossible without reference to earlier messages in the thread, and automated systems are not yet equipped to deal with this distribution of meaning, as text mining techniques developed from document-based corpora such as newswire do not translate naturally into the dialogue-based world of email.",
        "Take the following hypothetical exchange:",
        "Can you let me know the name of your lawyer?",
        "Thanks.",
        "John",
        "Ally McBeal.",
        " â€“ Jane",
        "This is an extreme example, but it serves to illustrate the \"separate document problem\" in email processing.",
        "Context is critical to pragmatic analysis, but with email and related media the context (and consequently, a single piece of information) may be spread across more than one document.",
        "In this case the second message in isolation gives no information concerning \"Ally McBeal\" as we do not have any context to put her in.",
        "However, by considering the question and answer pair together, we can discover that she is a lawyer (or, at the very least, that Jane believes or claims that to be the case; a philosophical distinction best left aside for the time being).",
        "It is anticipated that questions in a dialogue context will exhibit a somewhat different range of types to typical IR questions, but that some will indeed be seeking the kind of factual information for which QA classifiers are currently designed.",
        "If this subset of fact-seeking questions can be reliably identified",
        "*The author would like to thank GCHQ for supporting this research.",
        "by an automated process, then existing question classiiers could be used to identify the expected answer type.",
        "Candidate answers could then be sought within the text of replies to the message in which the question is asked.",
        "This paper briefly describes the gold standard data (Section 2), compares human annotation to the output of Ng & Kan's (2010) QA-SYS question classifier (Section 3), and proposes some future directions for research (Section 4)."
      ]
    },
    {
      "heading": "2. The Data",
      "text": [
        "In order to investigate question types in email, a suitable set of questions was required.",
        "To this end questions were automatically extracted from CMU's deduplicated copy of the Enron corpus (Klimt & Yang 2004).",
        "Of the 39,530 unique question strings identified in Enron outboxes, a random sample of 1147 were manually examined and annotated with the expected question type.",
        "A number of taxonomies have been proposed for classifying answer types, of which Li & Roth's (2002) two-tier hierarchy is a reasonably comprehensive and widely-adopted example.",
        "Their coarse classes are Abbreviation (ABBR), Description (DESC), Entity (ENTY), Human (HUM), Location (LOC), and Numeric (NUM), and they then define a set of 50 subclasses.",
        "Table 1 shows how Li & Roth's taxonomy was mapped to the category labels adopted for the current work.",
        "Table 1: The new dialogue taxonomy, with mappings to Li & Roth where applicable, and percentage distribution in the Enron sample",
        "Cotterill 2010",
        "Li & Roth 2002",
        "%",
        "Person(s)",
        "HUM{individual,title}",
        "2.53",
        "Group or Organisation",
        "HUM {group}",
        "0.17",
        "Descriptive text",
        "HUM {description}",
        "11.51",
        "DESC {manner, definition, description}",
        "Reason",
        "DESC {reason}",
        "1.57",
        "Date or Time",
        "NUM{date, period}",
        "3.57",
        "Numeric",
        "NUM{weight, volume/size, ordinal, percentage, count, speed, money, temperature, distance, other}",
        "1.92",
        "Phone",
        "NUM{code}",
        "0.40",
        "URL",
        "0.17",
        "Email",
        "0.17",
        "Place",
        "LOC{country, state, city, mountain, other}",
        "0.96",
        "Animal",
        "ENTY{animal}",
        "0.00",
        "Physical Object",
        "ENTY {instrument, plant, body part, vehicle, food, product, substance}",
        "0.30",
        "Concept",
        "ENTY{language, religion, letter, color, creative/artwork, disease/medical, currency}",
        "0.40",
        "Event or Activity",
        "ENTY{event, sport, technique/method}",
        "0.87",
        "Other",
        "ENTY{symbol, term, word, other}",
        "0.00",
        "ABBR{abbreviation, expression}",
        "Yes/No",
        "41.33",
        "Action Request",
        "8.98",
        "Rhetorical",
        "5.23",
        "Multiple",
        "3.23",
        "Non-Question",
        "16.74",
        "A number of extra categories were added to account for the nature of the data, as identiied by preliminary experiments.",
        "Examples of questions falling into some of the new categories are presented in",
        "Table 2.",
        "It is important to observe that a massive 75.5% of questions in the Enron sample do not fall into any of the categories deined by Li & Roth.",
        "Assuming that this is a fair representation of the distribution across the Enron corpus (if not email as a whole) then we are clearly justified in stating that some further work will be required before question classiication can be meaningfully applied to the email task.",
        "The most common category is Yes/No, giving a \"most common class\" baseline of 41.3%.",
        "That is to say, a classiication system which classiied every question as a Yes/No question would expect to see accuracy in this region, and any results must be considered in this context.",
        "The most common of the IR-derived categories is Description, representing 11.51% of questions overall, or 46.2% of those falling into IR categories.",
        "This compares to 26.6% reported across the equivalent categories in Li & Roth's analysis of TREC questions.",
        "Full details of the Enron question dataset will be published in due course."
      ]
    },
    {
      "heading": "3. Performance of QA-SYS",
      "text": [
        "QANUS (Ng & Kan 2010) is an open-source question answering framework which uses the Li & Roth categories in its question classiication module.",
        "The framework is designed to be extensible, which makes it a good candidate for further work.",
        "However, the results presented in this section deal only with the output of the QA-SYS default question processing module as supplied with QANUS v26Jan2010.",
        "The question classiication component of QA-SYS is an instance of the Stanford classiier, a supervised learning module trained on a dataset of information retrieval questions.",
        "Ng & Kan do not report their question classiication accuracy, providing igures only for the \"factoid accuracy\" of the end-to-end question answering system, which makes it dificult to compare their results to the present study.",
        "However Huang, Thint & Cellikyilmaz (2009) publish results for a maximum entropy classiier trained on similar IR data, reporting an encouragingly high accuracy of 89.0%.",
        "QA-SYS question classification was used to provide an automatic classification for each of the questions extracted from the Enron dataset.",
        "In order to assess the performance of the system, the results were compared to the hand-annotated examples.",
        "QA-SYS output agreed with human annotation in only 13.4% of cases overall - much lower than the \"most common class\" baseline deined above.",
        "However, this igure is artiicially low as QA-SYS supplies a result in all circumstances, without any associated level of conidence.",
        "The system will therefore provide an incorrect result in cases where it does not have an appropriate category (even when faced with a nonsense string).",
        "This may be acceptable behaviour within information retrieval, particularly for participating in competitions when there is a high expectation of the question falling into one of Li & Roth's categories, but for dialogue questions it produces a number of undesirable effects.",
        "Any competent end-to-end system would need (at a minimum) to ilter out nonsense strings, and direct questions to appropriate classiiers based on language (therefore removing the need to attempt an intelligent classiication of texts in multiple languages).",
        "Considering the proportion of questions in our sample which fell into the new categories of our extended taxonomy, the framework should also be extended to include a number of classiiers to handle these data types speciically.",
        "\"Are you guys still thinking of maybe joining us skiing?\"",
        "Yes/No",
        "\"Did you know Moller was going to be on TV or were you just channel surfing?\"",
        "\"Do you stock those wheels and tires or would I have to order them?\"",
        "Multiple choice",
        "\"Will it ever end???\"",
        "Rhetorical",
        "\"Would you please handle this?\"",
        "\"Also, could you check for reservations at the Georgian hotel in Santa Monica?\"",
        "Action Request",
        "We are therefore justiied in considering what might happen if a pre-classiier fed to QA-SYS only those questions whichitmay stand some chance ofcategorising correctly.",
        "Including only those questions falling into categories onwhichQA-SYS has been trained, outputagrees withhumanannotation in 55.0% of cases.",
        "Table 3 presents a small number of examples where the QA-SYS annotation agreed with human assessment.",
        "It may also be instructive to consider the recall and precision on a per-category basis, as there is a strong variation between the success rates for different QA-SYS categories.",
        "Table 4 gives the igures for those classes with at least 10 examples in the current dataset, and which QA-SYS claims to address.",
        "This shows that some categories with the highest recall (e.g.",
        "Person, Reason) suffer from low precision, but examination of the full confusion matrix shows that the incorrect categorisation is largely accounted for by the categories for which QA-SYS is not trained (particularly Yes/No questions).",
        "If reliable classiiers could be trained to ilter out these question types at an earlier stage, the validity of QA-SYS results would be signiicantly improved.",
        "However, there are some features of QA-SYS question classiication which cannot be resolved by simply adding additional categories to the classiier framework.",
        "Most notably, the system exhibits a high degree of case sensitivity.",
        "For example, the two strings \"What do you think?\"",
        "and \"what do you think?\"",
        "are both present in the Enron corpus.",
        "To a human eye the lack of capitalisation is unlikely to affect the meaning, but QA-SYS categorises these two sentences differently: the former as DESC:desc, the latter as ENTY:term.",
        "A further example of case-sensitivity is found in the response of QA-SYS to questions written entirely in uppercase.",
        "Of the eleven examples in the dataset which contain only uppercase letters, all are classified as ABBR:exp.",
        "The 'uppercase' feature seems to overwhelm any other considerations (such as question word) which may be present.",
        "For instance \"WHAT?\"",
        "is classified as ABBR:exp, whereas \"What?\"",
        "and \"what?\"",
        "are (correctly) classified as DESC:desc.",
        "Certain words also have a signiicant impact on the classiication, regardless of the syntax of the question.",
        "For example, a question containing the word 'percent' is likely to be classiied as NUM:perc, a question containing the word 'week' is likely to be classified as NUM:date, and a question containing the word 'state' is likely to be classifed as some subtype of LOCATION.",
        "Other lexical effects were surprising by their absence.",
        "For instance, of 111 questions (in the entire Enron question-set) beginning \"What time... \" only eleven are classified as requiring the NUM:date response.",
        "\"Remind me when your wedding date is?\"",
        "NUM:date",
        "DateTime",
        "\"Also, who is following up on the VA license?\"",
        "HUM:ind",
        "Person",
        "\"What is our strategy/plan in agricultural commodities training?\"",
        "DESC:desc",
        "Description",
        "Category",
        "Recall",
        "Precision",
        "F-measure",
        "Description",
        "62.8",
        "28.6",
        "39.3",
        "DateTime",
        "53.7",
        "28.6",
        "37.3",
        "Numeric",
        "40.9",
        "15.5",
        "22.5",
        "Reason",
        "61.1",
        "10.2",
        "17.5",
        "Person",
        "65.5",
        "6.7",
        "12.2",
        "Place",
        "45.5",
        "5.2",
        "9.3",
        "Event",
        "10.0",
        "2.9",
        "4.6",
        "Another small but important set of questions, which are barely represented in the current dataset, are compound questions.",
        "These are cases, such as the examples in Table 5, in which more than one answer is expected.",
        "In all of these examples, the category generated by QA-SYS can hardly be called incorrect, however it is not the whole story.",
        "Presently QA-SYS does not allow for multiple answer types.",
        "This is worthy of further study."
      ]
    },
    {
      "heading": "4. Future Work",
      "text": [
        "The present work should be extended using a larger dataset to train additional classiiers for the answer types which are beyond the scope of IR classiiers such as QA-SYS.",
        "A larger dataset will also enable further analysis, for example to identify any common features of questions which prove particularly hard to categorise.",
        "Speciic work to identify further examples in the very small categories (including a representative sample of compound questions) would also be beneicial.",
        "The next step is to extend the QANUS framework with additional classiiers trained on Enron data, and this work should be thoroughly tested to ensure it is not over-itted to Enron.",
        "There is a wealth of public dialogue data on the web, available from textual media such as web forums and Twitter, which may be reasonably expected to have some characteristics in common with email and which could be used for testing the classiiers.",
        "Recent work on email has considered the task of highlighting messages within an inbox which require action (e.g Bennett & Carbonell 2005, achieving 81.7% accuracy).",
        "This is an interesting result for us as the set of actions intersects with the set of questions: some questions have the pragmatic force of an action request.",
        "It would be interesting to examine the size of this intersection.",
        "\"How many kids are in the class and who is the instructor?\"",
        "NUM:count",
        "\"do you want to get together on friday or saturday and where?\"",
        "LOC:other",
        "\"How (and when) do you plan to get there?\"",
        "DESC:manner"
      ]
    }
  ]
}
