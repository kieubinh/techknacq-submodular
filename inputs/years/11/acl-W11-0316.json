{
  "info": {
    "authors": [
      "Zhiyuan Liu",
      "Xinxiong Chen",
      "Yabin Zheng",
      "Maosong Sun"
    ],
    "book": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning",
    "id": "acl-W11-0316",
    "title": "Automatic Keyphrase Extraction by Bridging Vocabulary Gap",
    "url": "https://aclweb.org/anthology/W11-0316",
    "year": 2011
  },
  "references": [
    "acl-C08-1122",
    "acl-C10-1148",
    "acl-D09-1027",
    "acl-D09-1051",
    "acl-J03-1002",
    "acl-J10-3010",
    "acl-J93-2003",
    "acl-P00-1041",
    "acl-P03-1003",
    "acl-P07-1059",
    "acl-P10-1085",
    "acl-W00-0405",
    "acl-W03-1028",
    "acl-W04-3219",
    "acl-W04-3252"
  ],
  "sections": [
    {
      "text": [
        "Automatic Keyphrase Extraction by Bridging Vocabulary Gap *",
        "Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, Maosong Sun",
        "State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Science and Technology Tsinghua University, Beijing 100084, China",
        "{lzy.thu, cxx.thu, yabin.zheng}@gmail.com, sms@tsinghua.edu.cn",
        "Keyphrase extraction aims to select a set of terms from a document as a short summary of the document.",
        "Most methods extract keyphrases according to their statistical properties in the given document.",
        "Appropriate keyphrases, however, are not always statistically significant or even do not appear in the given document.",
        "This makes a large vocabulary gap between a document and its keyphrases.",
        "In this paper, we consider that a document and its keyphrases both describe the same object but are written in two different languages.",
        "By regarding keyphrase extraction as a problem of translating from the language of documents to the language of keyphrases, we use word alignment models in statistical machine translation to learn translation probabilities between the words in documents and the words in keyphrases.",
        "According to the translation model, we suggest keyphrases given a new document.",
        "The suggested keyphrases are not necessarily statistically frequent in the document, which indicates that our method is more flexible and reliable.",
        "Experiments on news articles demonstrate that our method outperforms existing unsupervised methods on precision, recall and F-measure."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Information on the Web is emerging with the development of Internet.",
        "It is becoming more and more important to effectively search and manage information.",
        "Keyphrases, as a brief summary of a document, provide a solution to help organize and",
        "*Zhiyuan Liu and Xinxiong Chen have equal contribution to this work.",
        "retrieve documents, which have been widely used in digital libraries and information retrieval (Turney, 2000; Nguyen and Kan, 2007).",
        "Due to the explosion of information, it is ineffective for professional human indexers to manually annotate documents with keyphrases.",
        "How to automatically extract keyphrases from documents becomes an important research problem, which is usually referred to as keyphrase extraction.",
        "Most methods for keyphrase extraction try to extract keyphrases according to their statistical properties.",
        "These methods are susceptible to low performance because many appropriate keyphrases may not be statistically frequent or even not appear in the document, especially for short documents.",
        "We name the phenomenon as the vocabulary gap between documents and keyphrases.",
        "For example, a research paper talking about \"machine transliteration\" may less or even not mention the phrase \"machine translation\".",
        "However, since \"machine transliteration\" is a sub-field of \"machine translation\", the phrase \"machine translation\" is also reasonable to be suggested as a keyphrase to indicate the topics of this paper.",
        "Let us take another example: in a news article talking about \"iPad\" and \"iPhone\", the word \"Apple\" may rarely ever come up.",
        "However, it is known that both \"iPad\" and \"iPhone\" are the products of \"Apple\", and the word \"Apple\" may thus be a proper keyphrase of this article.",
        "We can see that, the essential challenge of keyphrase extraction is the vocabulary gap between documents and keyphrases.",
        "Therefore, the task of keyphrase extraction is how to capture the semantic relations between the words in documents and in keyphrases so as to bridge the vocabulary gap.",
        "In this paper, we provide a new perspective to documents and their keyphrases: each document and its keyphrases are descriptions to the same object, but the document is written using one language, while keyphrases are written using another language.",
        "Therefore, keyphrase extraction can be regarded as a translation problem from the language of documents into the language of keyphrases.",
        "Based on the idea of translation, we use word alignment models (WAM) (Brown et al., 1993) in statistical machine translation (SMT) (Koehn, 2010) and propose a unified framework for keyphrase extraction: (1) From a collection of translation pairs of two languages, WAM learns translation probabilities between the words in the two languages.",
        "(2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents.",
        "As a promising approach to solve the problem of vocabulary gap, SMT has been widely exploited in many applications such as information retrieval (Berger and Lafferty, 1999; Karimzade-hgan and Zhai, 2010), image and video annotation (Duygulu et al., 2002), question answering (Berger et al., 2000; Echihabi and Marcu, 2003; Murdock and Croft, 2004; Soricut and Brill, 2006; Xue et al., 2008), query expansion and rewriting (Riezler et al., 2007; Riezler et al., 2008; Riezler and Liu, 2010), summarization (Banko et al., 2000), collocation extraction (Liu et al., 2009b; Liu et al., 2010b) and paraphrasing (Quirk et al., 2004; Zhao et al., 2010).",
        "Although SMT is a widely adopted solution to vocabulary gap, for various applications using SMT, the crucial and non-trivial problem is to find appropriate and enough translation pairs for",
        "SMT.",
        "The most straightforward translation pairs for keyphrase extraction is document-keyphrase pairs.",
        "In practice, however, it is time-consuming to annotate a large collection of documents with keyphrases for sufficient WAM training.",
        "In order to solve the problem, we use titles and summaries to build translation pairs with documents.",
        "Titles and summaries are usually accompanying with the corresponding documents.",
        "In some special cases, titles or summaries may be unavailable.",
        "We are also able to extract one or more important sentences from the corresponding documents to construct sufficient translation pairs."
      ]
    },
    {
      "heading": "2. State of the Art",
      "text": [
        "Some researchers (Frank et al., 1999; Witten et al., 1999; Turney, 2000) regarded keyphrase extraction as a binary classification problem (is-keyphrase or non-keyphrase) and learned models for classification using training data.",
        "These supervised methods need manually annotated training set, which is time-consuming.",
        "In this paper, we focus on unsupervised methods for keyphrase extraction.",
        "The most simple unsupervised method for keyphrase extraction is using TFIDF (Salton and Buckley, 1988) to rank the candidate keyphrases and select the top-ranked ones as keyphrases.",
        "TFIDF ranks candidate keyphrases only according to their statistical frequencies, which thus fails to suggest keyphrases with low frequencies.",
        "Starting with TextRank (Mihalcea and Tarau, 2004), graph-based ranking methods are becoming the state-of-the-art methods for keyphrase extraction (Liu et al., 2009a; Liu et al., 2010a).",
        "Given a document, TextRank first builds a word graph, in which the links between words indicate their semantic relatedness, which are estimated by the word co-occurrences in the document.",
        "By executing PageRank (Page et al., 1998) on the graph, we obtain the PageRank score for each word to rank candidate keyphrases.",
        "In TextRank, a low-frequency word will benefit from its high-frequency neighbor words and thus be ranked higher as compared to using TFIDF.",
        "This alleviates the problem of vocabulary gap to some extent.",
        "TextRank, however, still tends to extract high-frequency words as keyphrases because these words have more opportunities to get linked with other words and obtain higher PageRank scores.",
        "Moreover, TextRank usually constructs a word graph simply according to word co-occurrences as an approximation of the semantic relations between words.",
        "This will introduce much noise because of connecting semantically unrelated words and highly influence extraction performance.",
        "Some methods have been proposed to improve TextRank, of which ExpandRank (Wan and Xiao, 2008b; Wan and Xiao, 2008a) uses a small number, namely k, of neighbor documents to provide more information of word relatedness for the construction of word graphs.",
        "Compared to TextRank, ExpandRank performs better when facing the vocabulary gap by borrowing the information on document level.",
        "However, the finding of neighbor documents are usually arbitrary.",
        "This process may introduce much noise and result in topic drift when the document and its so-called neighbor documents are not exactly talking about the same topics.",
        "Another potential approach to alleviate vocabulary gap is latent topic models (Landauer et al., 1998; Hofmann, 1999; Blei et al., 2003), of which latent Dirichlet allocation (LDA) (Blei et al., 2003) is most popular.",
        "Latent topic models learn topics from a collection of documents.",
        "Using a topic model, we can represent both documents and words as the distributions over latent topics.",
        "The semantic relatedness between a word and a document can be computed using the cosine similarities of their topic distributions.",
        "The similarity scores can be used as the ranking criterion for keyphrase extraction (Heinrich, 2005; Blei and Lafferty, 2009).",
        "On one hand, latent topic models use topics instead of statistical properties of words for ranking, which abates the vocabulary gap problem on topic level.",
        "On the other hand, the learned topics are usually very coarse, and topic models tend to suggest general words for a given document.",
        "Therefore, the method usually fails to capture the specific topics of the document.",
        "In contract to the above-mentioned methods, our method addresses vocabulary gap on word level, which prevents from topic drift and works out better performance.",
        "In experiments, we will show our method can better solve the problem of vocabulary gap by comparing with TFIDF, TextRank, ExpandRank and LDA."
      ]
    },
    {
      "heading": "3. Keyphrase Extraction by Bridging Vocabulary Gap Using WAM",
      "text": [
        "First, we give a formal definition of keyphrase extraction: given a collection of documents D, for each document d G D, keyphrase extraction aims to rank candidate keyphrases according to their likelihood given the document d, i.e., Pr(p\\d) for all p G P, where P is the candidate keyphrase set.",
        "Then we select top-Md ones as keyphrases, where Md can be fixed or automatically determined by the system.",
        "The document d can be regarded as a sequence of words wd = {wi}Nd, where Nd is the length of d.",
        "In Fig. 1, we demonstrate the framework of keyphrase extraction using WAM.",
        "We divide the algorithm into three steps: preparing translation pairs, training translation models and extracting keyphrases for a given document.",
        "We will introduce the three steps in details from Section 3.1 to Section 3.3.",
        "Input: A large collection of documents D for keyphrase extraction.",
        "Step 1: Prepare Translation Pairs.",
        "For each d G D, we may prepare two types of translation pairs:",
        "• Title-based Pairs.",
        "Use the title td of each document d and prepare translation pairs, denote as (D, T).",
        "• Summary-based Pairs.",
        "Use the summary sd of each document d and prepare translation pairs, denote as (D , S).",
        "Step 2: Train Translation Model.",
        "Given translation pairs, e.g., (D, T), train word-word translation model Pr(d,t) (t\\w) using WAM, where w is the word in document language and t is the word in title language.",
        "Step 3: Keyphrase Extraction.",
        "For a document d, extract keyphrases according to a trained translation model, e.g., Pr(D T)(t\\w).",
        "1.",
        "Measure the importance score Pr(w\\d) of each word w in document d."
      ]
    },
    {
      "heading": "2.. Compute the ranking score of candidate keyphrase",
      "text": [
        "3.",
        "Select top-Md ranked candidate keyphrases according to Pr(p\\d) as the keyphrases of document d.",
        "Training dataset for WAM consists of a number of translation pairs written in two languages.",
        "In keyphrase extraction task, we have to construct sufficient translation pairs to capture the semantic relations between documents and keyphrases.",
        "Here we propose to construct two types of translation pairs: title-based pairs and summary-based pairs.",
        "Title is usually a short summary of the given document.",
        "In most cases, documents such as research papers and news articles have corresponding titles.",
        "Therefore, we can use title to construct translation pairs for a document.",
        "WAM assumes each translation pair should be of comparable length.",
        "However, a document is usually much longer than title.",
        "It will hurt the performance if we fill the length-unbalanced pairs for WAM training.",
        "We propose two methods to address the problem: sampling method and split method.",
        "In sampling method, we perform word sampling for each document to make it comparable to the length of its title.",
        "Suppose the lengths of a document and its title are Nd and Nt , respectively.",
        "For document d, we first build a bag of words bd = {(wi,ei)}]W=d1, where Wd is the number of unique words in d, and ei is the weights of word wi in d.",
        "In this paper, we use TFIDF scores as the weights of words.",
        "Using bd, we sample words for Nt times with replacement according to the weights of words, and finally form a new bag with Nt words to represent document d. In the sampling result, we keep the most important words in document d. We can thus construct a document-title pair with balanced length.",
        "In split method, we split each document into sentences which are of comparable length to its title.",
        "For each sentence, we compute its semantic similarity with the title.",
        "There are various methods to measure semantic similarities.",
        "In this paper, we use vector space model to represent sentences and titles, and use cosine scores to compute similarities.",
        "If the similarity is smaller than a threshold ô, we will discard the sentence; otherwise, we will regard the sentence and title as a translation pair.",
        "Sampling method and split method have their own characteristics.",
        "Compared to split method, sampling method loses the order information of words in documents.",
        "While split method generates much more translation pairs, which leads to longer training time of WAM.",
        "In experiment section, we will investigate the performance of the two methods.",
        "For most research articles, authors usually provide abstracts to summarize the articles.",
        "Many news articles also have short summaries.",
        "Suppose each document itself has a short summary, we can use the summary and document to construct translation pairs using either sampling method or split method.",
        "Because each summary usually consists of multiple sentences, split method for constructing summary-based pairs has to split both the document and summary into sentences, and the sentence pairs with similarity scores above the threshold are filled in training dataset for WAM.",
        "Without loss of generality, we take title-based pairs as the example to introduce the training process of translation models, and suppose documents are written in one language and titles are written in another language.",
        "In this paper, we use IBM Model-1 (Brown et al., 1993) for WAM training.",
        "IBM Model-1 is a widely used word alignment algorithm which does not require linguistic knowledge for two languages .",
        "In IBM Model-1, for each translation pair (wd , wt ), the relationship of the document language wd = {w}L= 0 and the title language wt = {ti }L= 0is connected via a hidden variable a = { ai}iL=d1describing an alignment mapping from words of documents to words of titles,",
        "For example, aj = i indicates word wj in wd at position j is aligned to word ti in wt at position i.",
        "The alignment a also contains empty-word alignments a j = 0 which align words of documents to an empty word.",
        "IBM Model-1 can be trained using Expectation-Maximization (EM) algorithm (Dempster et al., 1977) in an unsupervised fashion.",
        "Using IBM Model-1, we can obtain the translation probabilities of two language-vocabularies, i.e., Pr(t\\ w) and Pr(w\\ t), where w is a word in document vocabulary and t is a word in title vocabulary.",
        "IBM Model-1 will produce one-to-many alignments from one language to another language, and the trained model is thus asymmetric.",
        "Hence, we can train two different translation models by assigning translation pairs in two directions, i.e., (document – title) and (title – document).",
        "We denote the former model as Prd2t and the latter as Prt2d.",
        "We define Pr{D ,T)(t\\w) in Eq.",
        "(1) as the harmonic mean of the two models:",
        "where X is the harmonic factor to combine the two models.",
        "When X = 1.0 or X = 0.0, it simply uses model Prd2t or Prt2d, correspondingly.",
        "Using the translation probabilities Pr(t \\w) we can bridge the vocabulary gap between documents and keyphrases.",
        "Given a document d, we rank candidate keyphrases by computing their likelihood Pr(p\\d).",
        "Each candidate keyphrase p may be composed of multiple words.",
        "As shown in (Hulth, 2003), most keyphrases are noun phrases.",
        "Following (Mihalcea and Tarau, 2004; Wan and Xiao, 2008b), we simply select noun phrases from the given document as candidate keyphrases with the help of POS tags.",
        "For each word t, we compute its likelihood given d, Pr(t\\ d) = LwedPr(t\\w)Pr(w\\d), where Pr(w\\d) is the weight of the word w in d, which is measured using normalized TFIDF scores.",
        "Pr(t\\ w) is the translation probabilities obtained from WAM training.",
        "Using the scores of all words in candidate keyphrases, we compute the ranking score of each candidate keyphrase by summing up the scores of each word in the candidate keyphrase, i.e., Pr(p\\d) = LtepPr(t\\d).",
        "In all, the ranking scores of candidate keyphrases is formalized in Eq.",
        "(1) of Fig. 1.",
        "According to the ranking scores, we can suggest top-Md ranked candidates as the keyphrases, where Md is the number of suggested keyphrases to the document d pre-specified by users or systems.",
        "We can also consider the number of words in the candidate keyphrase as a normalization factor to Eq.",
        "(1), which will be our future work.",
        "We also investigate the influence of parameters to WAM with title-based pairs prepared using split method, which achieves the best performance as shown in Fig. 2.",
        "The parameters include: harmonic factor X (described in Eq.",
        "3) and threshold factor ô .",
        "Harmonic factor X controls the weights of the translation models trained in two directions, i.e., Prd2t(t\\ w) and Prt2d(t\\ w) as shown in Eq.",
        "(3).",
        "As described in Section 3.1.1, using threshold factor ô we filter out the pairs with similarities lower than ô.",
        "In Fig. 3, we show the precision-recall curves of WAM for keyphrase extraction when harmonic factor X ranges from 0.0 to 1.0 stepped by 0.2.",
        "From the figure, we observe that the translation model Prd2t(t\\w) (i.e., when X = 1.0) performs better than",
        "Method",
        "Precision",
        "Recall",
        "TFIDF",
        "0.187",
        "0.256",
        "TextRank",
        "0.217",
        "0.301",
        "LDA",
        "0.181",
        "0.253",
        "ExpandRank",
        "0.228",
        "0.316",
        "Title-Sa",
        "0.299",
        "0.424",
        "Title-Sp",
        "0.300",
        "0.425",
        "Summ-Sa",
        "0.258",
        "0.361",
        "Summ-Sp",
        "0.273",
        "0.384",
        "TFIDF – Er – TextRank",
        "LDA – e – ExpandRank Title-Sa Title-Sp * Summ-Sa -Summ-Sp ▼",
        "\\",
        "-",
        "\\",
        "■ •",
        "Prt2d(f\\w) (i.e., when X = 0.0).",
        "This indicates that it is sufficient to simply train a translation model in one direction (i.e., Prd2t(f\\w)) for keyphrase extraction.",
        "In Fig. 4, we show the precision-recall curves of WAM for keyphrase extraction when threshold factor ô ranges from 0.01 to 0.90.",
        "In title-based pairs using split method, the total number of pairs without filtering any pairs (i.e., ô = 0) is 347,188.",
        "When ô = 0.01, 0.10 and 0.90, the numbers of retained translation pairs are 165,023, 148 , 605 and 41 , 203, respectively.",
        "From Fig. 4, we find that more translation pairs result in better performance.",
        "However, more translation pairs also indicate more training time of WAM.",
        "Fortunately, we can see that the performance does not drop much when discarding more translation pairs with low similarities.",
        "Even when ô = 0.9, our method can still achieve performance with precision p = 0.277, recall r = 0.391 and F-measure f = 0.312 when Md = 2.",
        "Meanwhile, we reduce the training efforts by about 50% as compared to ô = 0.01.",
        "In all, based on the above analysis on two parameters, we demonstrate the effectiveness and robustness of our method for keyphrase extraction.",
        "Suppose in some special cases, the titles or summaries are unavailable, how can we construct translation pairs?",
        "Inspired by extraction-based document summarization (Goldstein et al., 2000; Mihalcea and Tarau, 2004), we can extract one or more important sentences from the given document to construct translation pairs.",
        "Unsupervised sentence extraction for document summarization is a well-studied task in natural language processing.",
        "As shown in Table 2, we only perform two simple sentence extraction methods to demonstrate the effectiveness: (1) Select the first sentence of a document (denoted as \"First\"); and (2) Compute the cosine similarities between each sentence and the whole document represented as two bags-of-words (denoted as \"Importance\").",
        "It is interesting to find that the method of using the first sentence performs similar to using titles.",
        "This profits from the characteristic of news articles which tend to give a good summary for the whole article using the first sentence.",
        "Although the second method drops much on performance as compared to using titles, it still outperforms than other existing methods.",
        "Moreover, the second method will improve much if we use more effective measures to identify the most important sentence.",
        "Table 2: Precision, recall and F-measure of keyphrase extraction when Md = 2 by extracting one sentence to construct translation pairs."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "To perform experiments, we crawled a collection of 13 ,702 Chinese news articles from www.163.",
        "com, one of the most popular news websites in China.",
        "The news articles are composed of various topics including science, technology, politics, sports, arts, society and military.",
        "All news articles are manually annotated with keyphrases by website editors, and all these keyphrases come from the corresponding documents.",
        "Each news article is also provided with a title and a short summary.",
        "In this dataset, there are 72 900 unique words in documents, and 12 ,405 unique words in keyphrases.",
        "The average lengths of documents, titles and summaries are 971.7 words, 11.6 words, and 45.8 words, respectively.",
        "The average number of keyphrases for each document is 2.4.",
        "In experiments, we use the annotated titles and summaries to construct translation pairs.",
        "In experiments, we select GIZA++ (Och and Ney, 2003) to train IBM Model-1 using translation pairs.",
        "GIZA++, widely used in various applications of statistical machine translation, implements IBM Models 1-5 and an HMM word alignment model.",
        "To evaluate methods, we use the annotated keyphrases by www.163.com as the standard keyphrases.",
        "If one suggested keyphrase exactly matches one of the standard keyphrases, it is a correct keyphrase.",
        "We use precision p =",
        "ccorrect /cmethod, recaïï r = ccorrect /cstandard and F-",
        "measure f = 2pr/(p + r) for evaluation, where ccorrect is the number of keyphrases correctly suggested by the given method, cmethod is the number of suggested keyphrases, and cstandard is the number of standard keyphrases.",
        "The following experiment results are obtained by 5-fold cross validation.",
        "4.1 Evaluation on Keyphrase Extraction 4.1.1 Performance Comparison and Analysis",
        "We use four representative unsupervised methods as baselines for comparison: TFIDF, TextRank (Mi-halcea and Tarau, 2004), ExpandRank (Wan and Xiao, 2008b) and LDA (Blei et al., 2003).",
        "We denote our method as WAM for short.",
        "In Fig. 2, we demonstrate the precision-recall curves of various methods for keyphrase extraction including TFIDF, TextRank, ExpandRank, LDA and WAM with title-based pairs prepared using sampling method (Title-Sa) and split method (Title-Sp), and WAM with summary-based pairs prepared using sampling method (Summ-Sa) and split method (Summ-Sp).",
        "For WAM, we set the harmonic factor X = 1.0 and threshold ô = 0.1, which is the optimal setting as shown in the later analysis on parameter influence.",
        "For TextRank, LDA and ExpandRank, we report their best results after parameter tuning, e.g., the number of topics for LDA is set to 400, and the number of neighbor documents for ExpandRank is set to 5 .",
        "The points on a precision-recall curve represent different numbers of suggested keyphrases from Md = 1 (bottom right) to Md = 10 (upper left), respectively.",
        "The closer the curve is to the upper right, the better the overall performance of the method is.",
        "In Table 1, we further demonstrate the precision, recall and F-measure scores of various methods when Md = 2 .",
        "In Table 1, we also show the statistical variances after ±.",
        "From Fig. 2 and Table 1, we have the following observations:",
        "First, our method outperforms all baselines.",
        "It indicates that the translation perspective is valid for keyphrase extraction.",
        "When facing vocabulary gap, TFIDF and TextRank have no solutions, ExpandRank adopts the external information on document level which may introduce noise, and LDA adopts the external information on topic level which may be too coarse.",
        "In contrast to these baselines, WAM aims to bridge the vocabulary gap on word level, which avoids topic drift effectively.",
        "Therefore, our method can better solve the problem of vocabulary gap in keyphrase extraction.",
        "Second, WAM with title-based pairs performs better than summary-based pairs consistently, no matter prepared using sampling method or split method.",
        "This indicates the titles are closer to the keyphrase language as compared to summaries.",
        "This is also consistent with the intuition that titles are more important than summaries.",
        "Meanwhile, we can save training efforts using title-based pairs.",
        "Last but not least, split method achieves better or comparable performance as compared to sampling method on both title-based pairs and summary-based pairs.",
        "The reasons are: (1) the split method generates more translation pairs for adequate training than sampling method; and (2) split method also keeps the context of words, which helps to obtain better word alignment, unlike bag-of-words in sampling method.",
        "In Section 4.1, we evaluate our method on keyphrase extraction by suggesting keyphrases from documents.",
        "In fact, our method is also able to suggest keyphrases that have not appeared in the content of given document.",
        "The ability is important especially when the length of each document is short, which itself may not contain appropriate keyphrases.",
        "We name the new task keyphrase generation.",
        "To evaluate these methods on keyphrase generation, we perform keyphrase generation for the titles of documents, which are usually much shorter than their corresponding documents.",
        "The experiment setting is as follows: the training phase is the same to the previous experiment, but in the test phase we suggest keyphrases only using the titles.",
        "LDA and ExpandRank, similar to our method, are also able to select candidate keyphrases beyond the titles.",
        "We still use the annotated keyphrases of the corresponding documents as standard answers.",
        "In this case, about 59% standard keyphrases do not appear in titles.",
        "................",
        "8 = 0.01 – B – 8 = 0.05 – ■ – 8 = 0.10 8 = 0.30 – I 8 = 0.50 – 8 = 0.70 – 8 = 090 – – ",
        "\\",
        "........v~~",
        "1",
        "l = 0.0 – B – l = 0.2 – ■ – ",
        "l = 0.4 – e – l = 0.6",
        "..................._.",
        "l = 0.Ö l = 1.0",
        "■",
        "V",
        "Method",
        "Precision",
        "Recall",
        "F-measure",
        "First Importance",
        "0.290 0.260",
        "0.410 0.367",
        "0.327±0.013 0.293±0.010",
        "In Table 3 we show the evaluation results of various methods for keyphrase generation when Md = 2.",
        "For WAM, we only show the results using title-based pairs prepared with split method.",
        "From the table, we have three observations: (1) WAM outperforms other methods on keyphrase generation.",
        "Moreover, there are about 10% correctly suggested keyphrases by WAM do not appear in titles, which indicates the effectiveness of WAM for keyphrase generation.",
        "(2) The performance of TFIDF and TextRank is much lower as compared to Table 1, because the titles are so short that they do not provide enough candidate keyphrases and even the statistical information to rank candidate keyphrases.",
        "(3) LDA, ExpandRank and WAM roughly keep comparable performance as in Table 1 (The performance of ExpandRank drops a bit).",
        "This indicates the three methods are able to perform keyphrase generation, and verifies again the effectiveness of our method.",
        "To demonstrate the features of our method for keyphrase generation, in Table 4 we list top-5 keyphrases suggested by LDA, ExpandRank and WAM for a news article entitled Israeli Military Claims Iran Can Produce Nuclear Bombs and Considering Military Action against Iran (We translate the original Chinese title and keyphrases into English for comprehension.).",
        "We have the following observations: (1) LDA suggests general words like \"negotiation\" and \"sanction\" as keyphrases because the coarse-granularity of topics.",
        "(2) ExpandRank suggests some irrelevant words like \"Lebanon\" as keyphrases, which are introduced by neighbor documents talking about other affairs related to Israel.",
        "(3) Our method can generate appropriate keyphrases with less topic-drift.",
        "Moreover, our method can find good keyphrases like \"nuclear weapon\" which even do not appear in the title.",
        "LDA: Iran, U.S.A., negotiation, Israel, sanction ExpandRank: Iran, Israel, Lebanon, U.S.A., Israeli Military WAM: Iran, military action, Israeli Military, Israel, nuclear weapon"
      ]
    },
    {
      "heading": "5. Conclusion and Future Work",
      "text": [
        "In this paper, we provide a new perspective to keyphrase extraction: regarding a document and its keyphrases as descriptions to the same object written in two languages.",
        "We use IBM Model-1 to bridge the vocabulary gap between the two languages for keyphrase generation.",
        "We explore various methods to construct translation pairs.",
        "Experiments show that our method can capture the semantic relations between words in documents and keyphrases.",
        "Our method is also language-independent, which can be performed on documents in any languages.",
        "We will explore the following two future work: (1) Explore our method on other types of articles and on other languages.",
        "(2) Explore more complicated methods to extract important sentences for constructing translation pairs."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is supported by the National Natural Science Foundation of China (NSFC) under Grant No.",
        "60873174.",
        "The authors would like to thank Peng Li and Xiance Si for their suggestions.",
        "Method",
        "Precision Recall F-measure",
        "TFIDF TextRank LDA ExpandRank",
        "0.105 0.141 0.115±0.004 0.107 0.144 0.118±0.005 0.180 0.256 0.204±0.008 0.194 0.268 0.216±0.012",
        "WAM",
        "0.296 0.420 0.334±0.009"
      ]
    }
  ]
}
