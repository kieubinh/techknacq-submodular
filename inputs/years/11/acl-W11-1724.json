{
  "info": {
    "authors": [
      "Ruifeng Xu",
      "Jun Xu",
      "Wang XiaoLong"
    ],
    "book": "Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011)",
    "id": "acl-W11-1724",
    "title": "Instance Level Transfer Learning for Cross Lingual Opinion Analysis",
    "url": "https://aclweb.org/anthology/W11-1724",
    "year": 2011
  },
  "references": [
    "acl-D08-1014",
    "acl-D08-1058",
    "acl-N06-1026",
    "acl-P07-1056",
    "acl-P07-1123",
    "acl-P09-1027"
  ],
  "sections": [
    {
      "text": [
        "Ruifeng Xu, Jun Xu and Xiaolong Wang",
        "Key Laboratory of Network Oriented Intelligent Computation Department of Computer Science and Technology Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China",
        "{xuruifeng,xujun}@hitsz.edu.cn, wangxl@insun.hit.edu.cn",
        "This paper presents two instance-level transfer learning based algorithms for cross lingual opinion analysis by transferring useful translated opinion examples from other languages as the supplementary training data for improving the opinion classifier in target language.",
        "Starting from the union of small training data in target language and large translated examples in other languages, the Transfer AdaBoost algorithm is applied to iteratively reduce the influence of low quality translated examples.",
        "Alternatively, starting only from the training data in target language, the Transfer Self-training algorithm is designed to iteratively select high quality translated examples to enrich the training data set.",
        "These two algorithms are applied to sentence-and document-level cross lingual opinion analysis tasks, respectively.",
        "The evaluations show that these algorithms effectively improve the opinion analysis by exploiting small target language training data and large cross lingual training data."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In recent years, with the popularity of Web 2.0, massive amount of personal opinions including comments, reviews and recommendations in different languages have been shared on the Internet.",
        "Accordingly, automated opinion analysis has attracted growing attentions.",
        "Opinion analysis, also known as sentiment analysis, sentiment classification, and opinion mining, aims to identify opinions in text and classify their sentiment polarity (Pang and Lee, 2008).",
        "Many sentiment resources such as sentiment lexicons (e.g., SentiWordNet (Esuli and Sebastiani, 2006))and opinion corpora (e.g., MPQA (Blitzer et al., 2007)) have been developed on different languages in which most of them are for English.",
        "The lack of reliably sentiment resources is one of the core issues in opinion analysis for other languages.",
        "Meanwhile, the manually annotation is costly, thus the amount of available annotated opinion corpora are still insufficient for supporting supervised learning, even for English.",
        "These facts motivate to \"borrow\" the opinion resources in one language (source language, SL) to another language (target language, TL) for improving the opinion analysis on the target language.",
        "Cross lingual opinion analysis (CLOA) techniques are investigated to improve opinion analysis in TL through leveraging the opinion-related resources, such as dictionaries and annotated corpus in SL.",
        "Some CLOA works used bilingual dictionaries (Mihalcea et al., 2007), or aligned corpus (Kim and Hovy, 2006) to align the expressions between source and target languages.",
        "These works are puzzled by the limited aligned opinion resources.",
        "Alternatively, some works used machine translation system to do the opinion expression alignment.",
        "Banea et al.",
        "(2008) proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language.",
        "Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively.",
        "These works directly used all of the translation of annotated corpus in source language as the training data for target language without considering the following two problems: (1) the machine translation errors propagate to following CLOA procedure; (2) The annotated corpora from different languages are collected from different domains and different writing styles which lead the training and testing data having different feature spaces and distributions.",
        "Therefore, the performances of these supervised learning algorithms are affected.",
        "To address these problems, we propose two instance level transfer learning based algorithms to estimate the confidence of translated SL examples and to transfer the promising ones as the supplementary TL training data.",
        "We firstly apply Transfer AdaBoost (TrAdaBoost) (Dai et al., 2007) to improve the overall performance with the union of target and translated source language training corpus.",
        "A boosting-like strategy is used to down-weight the wrongly classified translated examples during iterative training procedure.",
        "This method aims to reduce the negative affection of low quality translated examples.",
        "Secondly, we propose a new Transfer Self-training algorithm (TrStr).",
        "This algorithm trains the classifier by using only the target language training data at the beginning.",
        "By automatically labeling and selecting the translated examples which is correct classified with higher confidence, the classifier is iteratively trained by appending new selected training examples.",
        "The training procedure is terminated until no new promising examples can be selected.",
        "Different from TrAdaBoost, TrStr aims to select high quality translated examples for classifier training.",
        "These algorithms are evaluated on sentence-and document-level CLOA tasks, respectively.",
        "The evaluations on simplified Chinese (SC) opinion analysis by using small SC training data and large traditional Chinese (TC) and English (EN) training data, respectively, show that the proposed transfer learning based algorithms effectively improve the CLOA.",
        "Noted that, these algorithms are applicable to different language pairs.",
        "The rest of this paper is organized as follows.",
        "Section 2 describes the transfer learning based approaches for opinion analysis.",
        "Evaluations and discussions are presented in Section 3.",
        "Finally,",
        "Section 4 gives the conclusions and future work."
      ]
    },
    {
      "heading": "2. CLOA via Transfer Learning",
      "text": [
        "Given a large translated SL opinion training data, the objective of this study is to transfer more high quality training examples for improving the TL opinion analysis rather than use the whole translated training data.",
        "Here, we propose to investigate the instance level transfer learning based approaches.",
        "In the case of transfer learning, the set of translated training SL examples is denoted by Ts = {(xi,yi)}rn=l, and the TL training data is denoted by Tt={(xi, yi)while the size of T is much smaller than that of Ts, i.e., \\m\\ ^ \\n\\.",
        "The idea of transfer learning is to use Tt as the indicator to estimate the quality of translated examples.",
        "By appending selected high quality translated examples as supplement training data, the performance of opinion analysis on TL is expected to be enhanced.",
        "TrAdaBoost is an extension of the AdaBoost algorithm (Freund and Schapir, 1996).",
        "It uses boosting technique to adjust the sample weight automatically (Dai et al., 2007).",
        "TrAdaBoost joins both the source and target language training data during learning phase with different re-weighting strategy.",
        "The base classifier is trained on the union of the weighted source and target examples, while the training error rate is measured on the TL training data only.",
        "In each iteration, for a SL training example, if it is wrongly classified by prior base classifier, it tends to be a useless examples or conflict with the TL training data.",
        "Thus, the corresponding weight will be reduced to decrease its negative impact.",
        "On the contrary, if a TL training example is wrongly classified, the corresponding weight will be increased to boost it.",
        "The ensemble classifier is obtained after several iterations.",
        "In this study, we apply TrAdaBoost algorithm with small revision to fit the CLOA task, as described in Algorithm 1.",
        "Noted that, our revised algorithm can handle multi-category problem which is different with original TrAdaBoost algorithm for binary classification problem only.",
        "More details and theoretical analysis of TrAdaBoost are given in Dai et al.",
        "'s work (Dai et al., 2007).",
        "Algorithm 1 CLOA with TrAdaBoost.",
        "Input: Ts, translated opinion training data in SL, n = \\Ts\\; Tt, training data in TL , m = \\Tt\\; L, base classifier; K, iteration number.",
        "1: Initialize the distribution of training samples:",
        "3: Get a hypothesis hk by training L with the combined training set Ts U Tt using distribution Dk: hk = L(Ts U Tt,Dk).",
        "4: Calculate the training error of hk on Tt:",
        "K = k – 1, break.",
        "Dkti)/ßk Zk",
        "9: if hk(xi) = yi then 10: Update the distribution:",
        "Output: argmax^kk/2]t[hk(x) = y]log(1/ßk) /* I [•] is an indicator function, which equals 1 if the inner expression is true and 0 otherwise.",
        "*/",
        "Different from TrAdaBoost which focuses on the filtering of low quality translated examples, we propose a new Transfer Self-training algorithm (TrStr) to iteratively train the classifier through transferring high quality translated SL training data to enrich the TL training data.",
        "The flow of this algorithm is given in Algorithm 2.",
        "The algorithm starts from training a classifier on Tt.",
        "This classifier is then applied to Ts, the translated SL training data.",
        "For each category in Ts (subjective/objective or positive/negative in our different experiments), top p correctly classified translated examples are selected.",
        "These translated examples are regarded as high quality ones and thus they are appended in the TL training data.",
        "Next, the classifier is retrained on the enriched training data.",
        "The updated classifier is applied to SL examples again to select more high quality examples.",
        "Such Algorithm 2 CLOA with Transfer Self-training.",
        "Input: Ts, translated opinion training data in SL, n = \\Ts\\; Tt, training data in TL , m = \\Tt\\; L, base classifier; K, iteration number.",
        "1: To = Tt, k = 1.",
        "2: Get a hypothesis hk by training a base classifier",
        "L with the training set Tk_1.",
        "3: for each instance (xi, yi) e Ts do 4: Use hk to label (xi, yi) .",
        "5: if ht(xi) = yi then 6: Add (xi,yi)to T 7: end if 8: end for",
        "9: Choose p instances per class with top confidence from T and denote the set as Tp.",
        "10: Tk = Tk_1 U ^ Ts = Ts – Tp.11: k = k + 1.",
        "12: Iterate K times over steps 2 to 11 or repeat until",
        "Tp = 0.",
        "Output: Final classifier by using the enriched training set Tk.",
        "procedure terminates until the increments are less than a specified threshold or the maximum number of iterations is exceeded.",
        "The final classifier is obtained by training on the union of target data and selected high quality translated SL training data."
      ]
    },
    {
      "heading": "3. Evaluation and Discussion",
      "text": [
        "The proposed approaches are evaluated on sentence-and document-level opinion analysis tasks in the bilingual case, respectively.",
        "In our experiments, the TL is simplified Chinese (SC) and the SL for the two experiments are English (EN)/traditional Chinese (TC) and EN, respectively.",
        "In the sentence-level opinionated sentence recognition experiment, the dataset is from the NTCIR-7 Multilingual Opinion Analysis Tasks (MOAT) (Se-ki et al., 2008) corpora.",
        "The information of this dataset is given in Table 1.",
        "Two experiments are performed.",
        "The first one is denoted by SenOR : TC – > SC, which uses TCs as source language training dataset, while the second one",
        "Zk is a normalization constant and Ti+m D^+\\(i) = 1.",
        "is SenOR : EN – SC, which uses ENS\\ SCSis shrunk to different scale as the target language training corpus by random.",
        "The opinion analysis results are evaluated with simplified Chinese testing dataset SCt under lenient and strict evaluation standard , respectively, as described in (Seki et al., 2008).",
        "Note I Lang.",
        "I Data I Total I »»Hifet^oh^ Lenient Strict open source SVM package -LIBSVM(Chang and Lin, 2001) with all default parameters.",
        "In the opinionated sentence recognition experiment, we use the presences of following linguistic features to represent each sentence example including opinion word, opinion operator, opinion indicator, the unigram and bigram of Chinese words.",
        "It is developed with the reference of (Xu et al., 2008).",
        "In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009).",
        "Here, document frequency is used for feature selection.",
        "Meanwhile, term frequency weighting is chosen for document representation.",
        "In order to investigate the effectiveness of the two proposed transfer learning approaches, they are compared with following baseline methods: (1) NoTr(T), which applies SVM with only TL training data; (2) NoTr(S),which applies SVM classifier with only the translated SL training data; (3) NoTr(S&T), which applies SVM with the union of TL and SL training data.",
        "Accuracy (Acc), precision (P), recall (R) and F-measure (F1) are used as evaluation metrics.",
        "All the performances are the average of 10 experiments.",
        "Here, the number of iterations in TrAdaBoost is set to 10 in order to avoid over-discarding SL examples.",
        "The achieved performance of the opinionated sentence recognition task under lenient and strict evaluation are given in Table 2 respectively, in which only 1/16 target train data is used as Tt.",
        "It is shown that NoTr(T) achieves a acceptable accuracy, but the recall and F1 for \"subjective\" category are obviously low.",
        "For the two sub-tasks, i.e. SenOR : TC – SC and SenOR : EN – SC tasks, the accuracies achieved by NoTr(S&T) are always between that of NoTr(T) and NoTr(S).",
        "The reason is that some translated examples from source language may likely conflict with the target language training data.",
        "It is shown that the direct use of all of the translated training data is infeasible.",
        "It is also shown that our approaches achieve better",
        "In the document-level review polarity classification experiment,, we used the dataset adopted in (Wan, 2009).",
        "Its English subset is collected by Blitzer et al.",
        "(2007), which contains a collection of 8,000 product reviews about four types of products: books, DVDs, electronics and kitchen appliances.",
        "For each type of products, there are 1,000 positive reviews and 1,000 negative ones, respectively.",
        "The Chinese subset has 451 positive reviews and 435 negative reviews of electronics products such as mp3 players, mobile phones etc.",
        "In our experiments, the Chinese subset is further split into two parts randomly: TL training dataset and test set.",
        "The cross lingual review polarity classification task is then denoted by DocSC: EN – SC.",
        "In this study, Google Translate is choose for providing machine translation results.",
        "This study focus on the approaches improving the opinion analysis by using cross lingual examples, while the classifier improving on target language is not our major target.",
        "Therefore, in the experiments, a Support Vector Machines (SVM) with linear kernel is used as the base classifier.",
        "We use the performance on both tasks while few TL training data is used.",
        "In which, TrStr performances the best on SenOR : TC – SC task while TrAdaBoost outperforms other methods on SenOR : EN – SC task.",
        "The proposed transfer learning approaches enhanced the accuracies achieved by NoTr(S&T) for 4.2-8.6% under lenient evaluation and 5.3-11.8% under strict evaluation, respectively.",
        "Table 4: The Results of Chinese Review Polarity Classification Task (Features:Unigrams+Bigrams; m=20).",
        "Table 3 and Table 4 give the achieved results of different methods on the task DocSC : EN – SC by using 20 Chinese annotated reviews as Tt.",
        "It is shown that transfer learning approaches outperform other methods, in which TrStr performs better than TrAdaBoost when unigram+bigram features are used.",
        "Compared to NoTr(T&S), the accuracies are increased about 1.8-6.2% relatively.",
        "Overall, the transfer learning approaches are shown are beneficial to TL polarity classification.",
        "Figure 1: Performances with Different Size of SCs on Opinionated Sentence Recognition Task under Lenient Evaluation",
        "In order to estimate the influence of different size of TL training data, we conduct a set of experiments on both tasks.",
        "Fig 1 and Fig 2 show the influence",
        "Method",
        "Sub-task",
        "Lenient Evaluation",
        "Strict Evaluation",
        "Acc",
        "subjective",
        "objective",
        "Acc",
        "subjective",
        "objective",
        "P",
        "R",
        "F1",
        "P",
        "R",
        "F1",
        "P",
        "R",
        "F1",
        "P",
        "R",
        "F1",
        "NoTr(T)",
        ".6254",
        ".534",
        ".3468",
        ".355",
        ".6824",
        ".7985",
        ".7115",
        ".6922",
        ".5259",
        ".3900",
        ".3791",
        ".7725",
        ".8264",
        ".7776",
        "NoTr(S) NoTr(S&T) TrAdaBoost TrStr",
        "u",
        "î U",
        ".6059 .6101 .6533 .6625",
        ".4911 .4943 .5335 .5448",
        ".7828 .7495 .7751 .7309",
        ".6035 .5957 .6314 .6238",
        ".7861 .7711 .8063 .7884",
        ".4960 .5236 .5777 .6199",
        ".6082 .6235 .6720 .6934",
        ".6448 .6531 .7184 .7304",
        ".4576 .4632 .5273 .5414",
        ".8352 .8051 .8473 .8182",
        ".5912 .588 .6494 .6511",
        ".8845 .8714 .9077 .896",
        ".5603 .5856 .6611 .6914",
        ".6860 .7004 .7643 .7801",
        "NoTr(S) NoTr(S&T) TrAdaBoost TrStr",
        "U",
        "S",
        "î",
        "z; w",
        ".6590 .6411 .6723",
        ".6686",
        ".5707 .5292 .5988 .5691",
        ".4446 .5759 .4371 .5746",
        ".4998 .5515 .5018 .5678",
        ".6966 .7212 .7019 .7360",
        ".7922 .6817 .8184 .7271",
        ".7413 .7009 .7549 .7292",
        ".7390 .7105 .7630",
        ".7484",
        ".5872 .5254 .6485 .589",
        ".5100 .608 .5019 .6276",
        ".5459 .5637 .5614 .6026",
        ".7944 .8129 .8002 .8315",
        ".8408 .7560 .8789 .8021",
        ".8169 .7834 .8371 .8147",
        "Method",
        "Acc",
        "positive",
        "negative",
        "P",
        "R",
        "F1",
        "P",
        "R",
        "F1",
        "NoTr(T)",
        ".7542",
        ".7447",
        ".8272",
        ".7747",
        ".8001",
        ".6799",
        ".7235",
        "NoTr(S)",
        ".7122",
        ".6788",
        ".8248",
        ".7447",
        ".7663",
        ".5954",
        ".6701",
        "NoTr(S&T)",
        ".7531",
        ".714",
        ".8613",
        ".7801",
        ".8187",
        ".6415",
        ".7179",
        "TrAdaBoost",
        ".7704",
        ".8423",
        ".6594",
        ".7376",
        ".7285",
        ".8781",
        ".7954",
        "TrStr",
        ".7998",
        ".8411",
        ".7338",
        ".7818",
        ".7727",
        ".8638",
        ".8144",
        "Method",
        "Acc",
        "positive",
        "negative",
        "P",
        "R",
        "F1",
        "P",
        "R",
        "F1",
        "NoTr(T)",
        ".7518",
        ".7399",
        ".8294",
        ".7741",
        ".7983",
        ".6726",
        ".7185",
        "NoTr(S)",
        ".7415",
        ".7143",
        ".8204",
        ".7637",
        ".7799",
        ".6598",
        ".7148",
        "NoTr(S&T)",
        ".7840",
        ".7507",
        ".8674",
        ".8035",
        ".8385",
        ".6982",
        ".7592",
        "TrAdaBoost",
        ".7984",
        ".8416",
        ".7297",
        ".7792",
        ".7707",
        ".8652",
        ".8138",
        "TrStr",
        ".8022",
        ".8423",
        ".7393",
        ".7843",
        ".7778",
        ".8634",
        ".8164",
        "„ nn r",
        "' 1",
        "i",
        "• <''.M ■",
        "i 1",
        "i",
        "1 1",
        "1",
        "il II",
        "i",
        "1 1",
        "1",
        "il II",
        "i",
        "1",
        "Jill",
        "1 1",
        "i",
        "Figure 2: Performances with Different Size of SCs on Opinionated Sentence Recognition Task under Strict Evaluation",
        "* – Transfer Self-training +■■■ TrAdaBoost xNoTr(S&T) •-- NoTr(TJ",
        "(a) Unigrams",
        "Transfer Self-training TrAdaBoost NoTr(S&T) NoTr(T)",
        "(b) Unigrams+Bigrams",
        "on the opinionated sentence recognition task under lenient and strict evaluation respectively.",
        "Fig 3 shows the influence on task DocSC : EN – SC.",
        "Fig 3(a) shows the results use unigram features and Fig 3(b) uses both unigrams and bigrams.",
        "It is observed that TrAdaBoost and TrStr achieve better performances than the baseline NoTr(S&T) in most cases.",
        "More specifically, TrStr performs the best when few TL training data is used.",
        "When more TL training data is used, the performance improvements by transfer learning approaches become small.",
        "The reason is that less target training data is helpful to transfer useful knowledge in translated examples.",
        "If too much TL training data is used, the weights of SL instances may decrease exponentially after several iterations, and thus more source training data is not obviously helpful."
      ]
    },
    {
      "heading": "4. Conclusions and Future Work",
      "text": [
        "To address the problems in CLOA caused by inaccurate translations and different domain/category distributions between training data in different languages, two transfer learning based algorithms are investigated to transfer promising translated SL training data for improving the TL opinion analysis.",
        "In this study, Transfer AdaBoost and Transfer Self-Training algorithms are investigated to reduce the influences of low quality translated examples and to select high quality translated examples, respectively.",
        "The evaluations on sentence-and document-level opinion analysis tasks show that the proposed algorithms improve opinion analysis by using the union of few TL training data and selected cross lingual training data.",
        "One of our future directions is to develop other transfer leaning algorithms for CLOA task.",
        "Another future direction is to employ other moderate weighting scheme on source training dataset to reduce the over-discarding of training examples from source language.",
        "NoTr(T)",
        "1 1 TrAdaBoost",
        "1 1 NoTr(TSS)",
        "1 1 Transfer Self-training"
      ]
    }
  ]
}
