{
  "info": {
    "authors": [
      "Jie Cai",
      "Eva Mujdricza-Maydt",
      "Michael Strube"
    ],
    "book": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task",
    "id": "acl-W11-1907",
    "title": "Unrestricted Coreference Resolution via Global Hypergraph Partitioning",
    "url": "https://aclweb.org/anthology/W11-1907",
    "year": 2011
  },
  "references": [
    "acl-C10-1017",
    "acl-L08-1328",
    "acl-N03-1033",
    "acl-P05-1022",
    "acl-P05-1045",
    "acl-P09-1074",
    "acl-W00-0730",
    "acl-W11-1901"
  ],
  "sections": [
    {
      "text": [
        "Jie Cai and Eva Müjdricza-Maydt and Michael Strube",
        "Natural Language Processing Group Heidelberg Institute for Theoretical Studies gGmbH Heidelberg, Germany",
        "(j ie.cai|eva.mujdriczamaydt|michael.strube)@h-its.org",
        "We present our end-to-end coreference resolution system, COPA, which implements a global decision via hypergraph partitioning.",
        "In constrast to almost all previous approaches, we do not rely on separate classification and clustering steps, but perform coreference resolution globally in one step.",
        "COPA represents each document as a hypergraph and partitions it with a spectral clustering algorithm.",
        "Various types of relational features can be easily incorporated in this framwork.",
        "COPA has participated in the open setting of the CoNLL shared task on modeling unrestricted coreference."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Coreference resolution is the task of grouping mentions of entities into sets so that all mentions in one set refer to the same entity.",
        "Most recent approaches to coreference resolution divide this task into two steps: (1) a classification step which determines whether a pair of mentions is coreferent or which outputs a confidence value, and (2) a clustering step which groups mentions into entities based on the output of step 1.",
        "In this paper we present an end-to-end corefer-ence resolution system, COPA, which avoids the division into two steps and instead performs a global decision in one step.",
        "The system presents a document as a hypergraph, where the vertices denote mentions and the edges denote relational features between mentions.",
        "Coreference resolution is then performed globally in one step by partitioning the hypergraph into subhypergraphs so that all mentions in one subhypergraph refer to the same entity (Cai and Strube, 2010).",
        "COPA assigns edge weights by applying simple descriptive statistics on the tranin-ing data.",
        "Since COPA does not need to learn an explicit model, we used only 30% of the CoNLL shared task training data.",
        "We did this not for efficiency reasons, only for convenience.",
        "While COPA has been developed originally to perform coreference resolution on MUC and ACE data (Cai and Strube, 2010), the move to the OntoNotes data (Weischedel et al., 2011) required mainly to update the mention detector and the feature set.",
        "Since several off-the-shelf preprocessing components are used, COPA participated in the open setting of the CoNLL shared task on modeling unrestricted coreference (Pradhan et al., 2011).",
        "We did not make extensive use of information beyond information from the closed class setting."
      ]
    },
    {
      "heading": "2. Preprocessing",
      "text": [
        "COPA is implemented on top of the BART-toolkit (Versley et al., 2008).",
        "Documents are transformed into the MMAX2-format (Muller and Strube, 2006) which allows for easy visualization and (linguistic) debugging.",
        "Each document is stored in several XML-files representing different layers of annotations.",
        "These annotations are created by a pipeline of preprocessing components.",
        "We use the Stanford MaxentTagger (Toutanova et al., 2003) for part-of-speech tagging, and the Stanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities.",
        "In order to derive syntactic information, we use the Charniak/Johnson rerank-ing parser (Charniak and Johnson, 2005) combined with a constituent-to-dependency conversion Tool (http ://nlp.cs.lth.se/software/ treebank_converter).",
        "The preprocessing models are not trained on CoNLL data, so we only participated in the open task.",
        "We have implemented an in-house mention detector, which makes use of the parsing output, the part-of-speech tags, as well as the chunks from the Yam-cha Chunker (Kudoh and Matsumoto, 2000).",
        "For the OntoNotes data, the mention detector annotates the biggest noun phrase spans."
      ]
    },
    {
      "heading": "3. COPA: Coreference Partitioner",
      "text": [
        "The COPA system consists of modules which derive hyperedges from features and assign edge weights indicating a positive correlation with the coreference relation, and resolution modules which create a hypergraph representation for the testing data and perform partitioning to produce subhypergraphs, each of which represents an entity.",
        "COPA needs training data only for computing the hyperedge weights.",
        "Hyperedges represent features.",
        "Each hyperedge corresponds to a feature instance modeling a simple relation between two or more mentions.",
        "This leads to initially overlapping sets of mentions.",
        "Hyperedges are assigned weights which are calculated on the training data as the percentage of the initial edges being in fact coreferent.",
        "Due to the simple strategy of assigning edge weights, only a reasonable size of training data is needed.",
        "Unlike pairwise models, COPA processes a document globally in one step, taking care of the preference information among all the mentions simultaneously and clustering them into sets directly.",
        "A document is represented as a single hypergraph with multiple edges.",
        "The hypergraph resolver partitions the hypergraph into several sub-hypergraphs, each corresponding to one set of coreferent mentions.",
        "A single document is represented in a hypergraph with basic relational features.",
        "Each hyperedge in a graph corresponds to an instance of one of those features with the weight assigned by the HyperEdge- Learner.",
        "Instead of connecting nodes with the target relation as usually done in graph models, COPA builds the graph directly out of low dimensional features without assuming a distance metric.",
        "In order to partition the hypergraph we adopt a spectral clustering algorithm (Agarwal et al., 2005).",
        "All experimental results are obtained using symmetric Laplacians (Lsym) (von Luxburg, 2007).",
        "We apply the recursive variant of spectral clustering, recursive 2-way partitioning (R2 partitioner) (Cai and Strube, 2010).",
        "This method does not need any information about the number of target sets (the number k of clusters).",
        "Instead a stopping criterion a* has to be provided which is adjusted on development data.",
        "Since edge weights are assigned using simple descriptive statistics, the time HGResolver needs for building the graph Laplacian matrix is not substantial.",
        "For eigensolving, we use an open source library provided by the Colt projectwhich implements a Householder-QL algorithm to solve the eigenvalue decomposition.",
        "When applied to the symmetric graph Laplacian, the complexity of the eigensolving is given by O(n), where n is the number of mentions in a hypergraph.",
        "Since there are only a few hundred mentions per document in our data, this complexity is not an issue.",
        "Spectral clustering gets problematic when applied to millions ofdata points."
      ]
    },
    {
      "heading": "4. Features",
      "text": [
        "In our system, features are represented as types of hyperedges.",
        "Any realized edge is an instance of the corresponding edge type.",
        "All instances derived from the same type have the same weight, but they may get reweighed by the distance feature (see Cai and Strube (2010)).",
        "We use three types offeatures:",
        "negative: prevent edges between mentions; positive: generate strong edges between mentions;",
        "weak: add edges to an existing graph without introducing new vertices;",
        "In the following subsections we describe the features used in our experiments.",
        "Some of the features described in Cai and Strube (2010) had to be changed to cope with the OntoNotes data.",
        "We also introduced a few more features (in particular in order to deal with the dialogue section in the data).",
        "Negative features describe pairwise relations which are most likely not coreferent.",
        "While we implemented this information as weak positive features in Cai and Strube (2010), here we apply these features before graph construction as global variables.",
        "When two mentions are connected by a negative relation, no edges will be built between them in the graph.",
        "For instance, no edges are allowed between the mention HillaryClinton and the mention he due to incompatible gender.",
        "(1) NGender, (2) NJNumber: Two mentions do not agree in gender or number.",
        "(3) NJSemanticClass: Two mentions do not agree in semantic class (only the Object, Date and Person top categories derived from WordNet (Fellbaum, 1998) are used).",
        "(4) NJMod: Two mentions have the same syntactic heads, and the anaphor has a pre-modifier which does not occur in the antecedent and does not contradict the antecedent.",
        "(5) NLDSPrn: Two first person pronouns in direct speeches assigned to different speakers.",
        "(6) NXontraSubjObj: Two mentions are in the subject and object positions of the same verb, and the anaphor is a non-possesive pronoun.",
        "The majority of well studied coreference features (e.g. Stoyanov et al.",
        "(2009)) are actually positive coreference indicators.",
        "In our system, the mentions which participate in positive relations are included in the graph representation.",
        "(7) StrMatchJNpron & (8) StrMatch_Pron: After discarding stop words, if the strings of mentions completely match and are not pronouns, they are put into edges of the StrMatchNpron type.",
        "When the matched mentions are pronouns, they are put into the StrMatchPron type edges.",
        "(9) Alias: After discarding stop words, if mentions are aliases of each other (i.e. proper names with partial match, full names and acronyms, etc.",
        ").",
        "(10) HeadMatch: If the syntactic heads of mentions match.",
        "(11) Nprn_Prn: If the antecedent is not a pronoun and the anaphor is a pronoun.",
        "This feature is restricted to a sentence distance of 2.",
        "Though it is not highly weighted, it is crucial for integrating pronouns into the graph.",
        "(12) Speaker12Prn: If the speaker of the second person pronoun is talking to the speaker of the first person pronoun.",
        "The mentions contain only first or second person pronouns.",
        "(13) DSPrn: If one of the mentions is the subject of a speak verb, and other mentions are first person pronouns within the corresponding direct speech.",
        "(14) ReflexivePrn: If the anaphor is a reflexive pronoun, and the antecedent is subject of the sentence.",
        "(15) PossPrn: If the anaphor is a possesive pronoun, and the antecedent is the subject of the sentence or the subclause.",
        "(16) GPEIsA: If the antecedent is a Named Entity of GPE entity type (i.e. one of the ACE entity type (NIST, 2004)), and the anaphor is a definite expression of the same type.",
        "(17) OrgIsA: If the antecedent is a Named Entity of Organization entity type, and the anaphor is a definite expression of the same type.",
        "Weak features are weak coreference indicators.",
        "Using them as positive features would introduce too much noise to the graph (i.e. a graph with too many singletons).",
        "We apply weak features only to mentions already integrated in the graph, so that weak information provides it with a richer structure.",
        "(18) W_Speak: If mentions occur with a word meaning to say in a window size of two words.",
        "(19) WWSubject: If mentions are subjects.",
        "(20) W_Synonym: If mentions are synonymous as indicated by WordNet."
      ]
    },
    {
      "heading": "5. Results",
      "text": [
        "We submitted COPA s results to the open setting in the CoNLL shared task on modeling unrestricted coreference.",
        "We used only 30% of the training data (randomly selected) and the 20 features described in Section 4.",
        "The stopping criterion a* (see Section 3) is tuned on development data to optimize the final coreference scores.",
        "A value of 0.06 is chosen for testing.",
        "COPA s results on development set (which consists of 202 files) and on testing set are displayed in Table 1 and Table 2 respectively.",
        "The Overall numbers in both tables are the average scores of MUC, BCUBED and CEAF(E).",
        "Table 2: COPA s results on CoNLL testing set"
      ]
    },
    {
      "heading": "6. Mention Detection Errors",
      "text": [
        "As described in Section 2, our mention detection is based on automatically extracted information, such as syntactic parses and basic noun phrase chunks.",
        "Since there is no minimum span information provided in the OntoNotes data (in constrast to the previous standard corpus, ACE), exact mention boundary detection is required.",
        "A lot of the spurious mentions in our system are generated due to mismatches of ending or starting punctuations, and the OntoNotes annotation is also not consistent in this regard.",
        "Our current mention detector does not extract verb phrases.",
        "Therefore it misses all the Event mentions in the OntoNotes corpus.",
        "We are planning to include idiomatic expression identification into our mention detector, which will help to avoid detecting a lot of spurious mentions, such as God in the phrase for God's sake."
      ]
    },
    {
      "heading": "7. COPA Errors",
      "text": [
        "Besides the fact that the current COPA is not resolving any event coreferences, our in-house mention detector performs weakly in extracting date mentions too.",
        "As a result, the system outputs several spurious coreference sets, for instance a set containing the September from the mention 15th September.",
        "A large amount of the recall loss in our system is due to the lack of the world knowledge.",
        "For example, COPA does not resolve the mention the Europe station correctly into the entity Radio Free Europe, for it has no knowledge that the entity is a station.",
        "Some more difficult coreference phenomena in OntoNotes data might require a reasoning mechanism.",
        "To be able to connect the mention the victim with the mention the groom s brother, the event of the brother being killed needs to be intepreted by the system.",
        "We also observed from the experiments that the resolution of the it mentions are quite inaccurate.",
        "Although our mention detector takes care of discarding pleonastic it s, there are still a lot of them left which introduce wrong coreference sets.",
        "Since the it s do not contain enough information by themselves, more features exploring their local syntax are necessary."
      ]
    },
    {
      "heading": "8. Conclusions",
      "text": [
        "In this paper we described a coreference resolution system, COPA, which implements a global decision in one step via hypergraph partitioning.",
        "COPA s hypergraph-based strategy is a general preference model, where the preference for one mention depends on information on all other mentions.",
        "The system implements three types of relational features – negative, positive and weak features, and assigns the edge weights according to the statitics from the training data.",
        "Since the weights are robust with respect to the amount of training data we used only 30% of the training data.",
        "Acknowledgements.",
        "This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany.",
        "The first author has been supported by a HITS PhD.",
        "scholarship.",
        "Metric",
        "R",
        "P",
        "F1",
        "MUC",
        "52.69",
        "57.94",
        "55.19",
        "BCUBED",
        "64.26",
        "73.39",
        "68.52",
        "CEAF (M )",
        "54.44",
        "54.44",
        "54.44",
        "CEAF (E )",
        "45.73",
        "40.92",
        "43.19",
        "BLANC",
        "69.78",
        "75.26",
        "72.13",
        "Overall",
        "55.63",
        "1: COPA s results on CoNLL developme",
        "Metric",
        "R",
        "P",
        "F1",
        "MUC",
        "56.73",
        "58.90",
        "57.80",
        "BCUBED",
        "64.60",
        "71.03",
        "67.66",
        "CEAF (M )",
        "53.37",
        "53.37",
        "53.37",
        "CEAF (E )",
        "42.71",
        "40.68",
        "41.67",
        "BLANC",
        "69.77",
        "73.96",
        "71.62",
        "Overall",
        "55.71"
      ]
    }
  ]
}
