{
  "info": {
    "authors": [
      "Gonz&#XE1",
      "JosÉ lez-Brenes",
      "Jack Mostow"
    ],
    "book": "Proceedings of the SIGDIAL 2011 Conference",
    "id": "acl-W11-2003",
    "title": "Which System Differences Matter? Using L1/L2 Regularization to Compare Dialogue Systems",
    "url": "https://aclweb.org/anthology/W11-2003",
    "year": 2011
  },
  "references": [
    "acl-H92-1005",
    "acl-J06-2004",
    "acl-W01-0902",
    "acl-W07-0305"
  ],
  "sections": [
    {
      "text": [
        "Which System Differences Matter?",
        "Using t\\/t2 Regularization to Compare Dialogue Systems",
        "Jose P. Gonzalez-Brenes and Jack Mostow",
        "We investigate how to jointly explain the performance and behavioral differences of two spoken dialogue systems.",
        "The Join Evaluation and Differences Identification (JEDI), finds differences between systems relevant to performance by formulating the problem as a multi-task feature selection question.",
        "JEDI provides evidence on the usefulness of a recent method, £i/£p-regularized regression (Obozinski et al., 2007).",
        "We evaluate against manually annotated success criteria from real users interacting with five different spoken user interfaces that give bus schedule information."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "This paper addresses the problem of how to determine which differences between two versions of a system affect their behavior.",
        "Researchers in Spoken Dialogue Systems (SDSs) can be perplexed as to which of the differences between alternative systems affect performance metrics (Bacchiani et al., 2008).",
        "For example, when testing on real users at different periods of time, the variance of the performance metrics might be higher than the difference between systems, causing (i) significantly different scores in identical systems deployed at different times, and (ii) the same score on different systems (Gonzalez-Brenes et al., 2009).",
        "We approach the problem of finding which system differences matter by describing dialogues as feature vectors constructed from the logs of dialogs generated by the SDSs interacting with real users.",
        "Hence, we aim to identify features that jointly characterize the system differences and the performance of the",
        "SDS being evaluated.",
        "These features should be able to (i) predict a performance metric and (ii) distinguish between the two SDS being evaluated.",
        "The main contribution of this paper is a novel algorithm for detecting differences between two systems that can explain performance.",
        "Additionally, we provide details on how to implement state-of-the-art multi-task learning for SDSs.",
        "The rest of this manuscript is organized as follows.",
        "Section 2 reviews multi-task feature selection.",
        "Section 3 describes two algorithms to find which system differences matter.",
        "Section 4 describes the specific SDS used to illustrate our algorithms.",
        "Section 5 presents some experimental results.",
        "Section 6 reviews related prior work.",
        "Section 7 presents some concluding remarks and future work.",
        "Appendix A provides implementation details of the multi-task learning approach we used."
      ]
    },
    {
      "heading": "2. Feature Selection",
      "text": [
        "In this section we describe how we use regression to perform feature selection.",
        "Feature selection methods construct and select subsets of features in order to build a good predictor.",
        "We focus our attention on feature selection methods that use complexity (regu-larization) penalties, because of their recent theoretical and experimental success (Yuan and Lin, 2006; Park and Hastie, 2007).",
        "We provide a more rigorous description of how to implement this formulation as an optimization problem in Appendix A.",
        "We use labels to encode the output we want to predict.",
        "For example, if our performance metric is binary, we label successful dialogues with a +1, and unsuccesful dialogues with a – 1.",
        "Given a training set consisting of labeled dialogues, we want to learn a model that assigns a label to unseen dialogues.",
        "We follow an approach called empirical risk minimization (Obozinski et al., 2007), that aims to minimize the error of fitting the training data, while penalizing the complexity of the model:",
        "Minimize | Model loss | + À | Complexity (1)",
        "Here the hyper-parameter À controls the trade-off between a better fit to the training data (with a higher risk of over-fitting it), and a simpler model, with fewer features selected (and less predictive power).",
        "We now review the two components of risk minimization, model loss and complexity penalty.",
        "We model probabilistically the loss of our model against the real-life phenomenon studied.",
        "Given a dialogue x, with correct label l, its loss using a model ß is:",
        "Here y is the predicted value of the event y.",
        "Since l is the true label, P(y = 1|x; reality) = 1.",
        "To get the overall loss of the model, we aggregate over the prediction loss of each of the dialogues in the training set by summing their individual loss calculated with Equation 2.",
        "Let X = {x(1), x(2),... x(n)} be the n dialogues in the training set.",
        "Then the overall loss of model ß is:",
        "Since we use discrete labels, we use a logistic function to model their probability.",
        "Let x1,... xkbe the k features extracted from dialogue x.",
        "Then the logistic regression model is:",
        "Here ß1...ßk are the parameters of the model, and Z simply normalizes P to ensure that P is a valid probability function (the range of P should be 0 to",
        "Multi-task learning solves related regression problems at the same time using a shared representation.",
        "We now describe the risk-minimization formulation for multi-task learning.",
        "Let ym be the value of the performance metric.",
        "Let ys be the label of the system that generated the dialogue.",
        "The individual dialogue loss of using models ßm and ßs is:",
        "We consider a feature xj to be selected into the model if its regression coefficient ßj is non-zero.",
        "Complexity penalties encourage selecting only a few features.",
        "We review several commonly used penalties (Zou and Hastie, 2005):",
        "• i2 Penalty.",
        "Under some circumstances ^2",
        "penalties perform better than other types of",
        "• i\\ Penalty.",
        "An ^1 penalty induces sparsity by setting many parameters of the model ß to exactly zero (Tibshirani, 1996).",
        "• ^i /£2 Penalty.",
        "Yuan and Lin (2006) proposed a group penalty for penalizing groups of features simultaneously.",
        "Previous work has shown that grouping features between tasks encourages features to be used either by all tasks or by none (Turlach et al., 2005; Obozinski et al., 2007; Lounici et al., 2009; Puniyani et al., 2010).",
        "Our penalty is:"
      ]
    },
    {
      "heading": "3. Finding Features that Predict",
      "text": [
        "Performance and System Differences",
        "We find system differences that are predictive of SDS performance, relying on:",
        "• Describing dialogues as feature vectors.",
        "The behavior of the systems must be describable by features extracted from the logs of the systems.",
        "A discussion of feature engineering for dialogue systems is found in (Gonzalez-Brenes",
        "and Mostow, 2011).",
        "• Finding system differences.",
        "The features of a classifier that distinguishes between SDSs, can be used to identify their differences (Gonzalez-Brenes et al., 2009).",
        "When comparing two SDSs, we label the baseline system with – 1, and the alternate version with +1.",
        "• Modeling performance.",
        "Although our approach does not depend on a specific performance metric, in this paper we use dialogue success, a binary indicator that triggers that the user's query was answered by the SDS.",
        "Task completion is cheaper to compute than dialogue success, as it does not require a manual human labeled reference, but we consider that dialogue success is a more accurate metric.",
        "Task completion is used in commercial applications (Bacchiani et al., 2008), and has been extensively studied in the literature (Walker et al., 2001; Walker et al., 2002; Hajdinjak and Mi-helic, 2006; Levin and Pieraccini, 2006; Möller et al., 2007; Moller et al., 2008; Schmitt et al., 2010).",
        "We encode success of dialogues by manually annotating them with a binary variable that distinguishes if the user query is ful-",
        "illed by the SDS.",
        "We now present two algorithms to ind what differences matter between systems.",
        "We introduce Serial EvaluatioN Analysis (SERENA) as a scaffold for the Join Evaluation and Differences Identification (JEDI) algorithm.",
        "The input to SERENA is a collection of log iles created by two different SDSs and two functions that represent the correct label for the regression tasks.",
        "In our case these functions should return binary labels (+1, – 1): one task distinguishes between successful and unsuccessful dialogues, and the other task distinguishes a baseline from an alternative SDS version.",
        "SERENA's objective is to select features from one task, and use them to predict the other task.",
        "For example, SERENA selects features that predict differences between versions, and uses them to predict performance.",
        "Algorithm 1 provides the pseudo-code for SERENA.",
        "Line 1 builds the training set X from parsing the logs of the SDSs.",
        "Lines 2 and 3 create the output Algorithm 1 SERENA algorithm",
        "Require: Logsi, LogS2 are the collections of SDS logs of two systems.",
        "task1, task2 are functions that return the value of a performance metric, and which system is being evaluated ( – 1 if is the baseline, +1 otherwise).",
        "1: X – extrac^features(Log1, Log2)",
        "4: // Select features that explain both tasks:",
        "variables y for the regression tasks.",
        "Line 6 returns the most predictive features using l1 regularization as described in Section 2.",
        "Line 8 builds a new training set, removing the features that were not selected in line 6.",
        "Line 9 builds the inal coeficients by itting a ^2-regularized model using a constant Ac.",
        "We calculate the coefficients using an l2 penalty, because it has a better fit to the data (Zou and Hastie, 2005).",
        "Moreover, by using the same penalty, we control for the idiosyncrasies different penalties have in parameter learning.",
        "In the experiments described in Section 5, all of our experiments are reported itting a l2 regularized models.",
        "SERENA is not conmutative with regards to the order of the tasks: selecting the features that predict performance and using them to predict system differences is not the same as the reverse.",
        "More importantly, SERENA only searches in one of the tasks at a time.",
        "We are interested in inding the features that explain both tasks simultaneously.",
        "In the next subsection we describe JEDI which makes use of recent advances in multi-task feature selection in order to ind the features for both tasks at the same time.",
        "Algorithm 2 provides the pseudo-code for JEDI.",
        "JEDI uses multi-task regression to ind the features that affect performance and system differences",
        "task i( Logs i) taski(LogS2) task2(LogSi) task2(LogS2)",
        "Require: Logs1, Logs2 are the collections of SDS logs of two systems.",
        "task1, task2 are functions that return the value of a performance metric, and which system is being evaluated ( – 1 if is the baseline, +1 otherwise).",
        "1: X – extrac^features(Log1, Log2) Algorithm 2 JEDI algorithm // Select features that explain both tasks:",
        "jointly.",
        "To ensure a sparse feature set across the tasks, it applies a l1 /l2 penalty.",
        "In this way, a feature is either selected to predict performance and system differences, or it is not selected at all.",
        "In the next section we review a concrete example of a SDS in which we apply SERENA and JEDI."
      ]
    },
    {
      "heading": "4. Case Study",
      "text": [
        "We use data collected from Let's Go!, an experi-mentary SDS available for research purposes that provides bus planning information to the Pittsburgh East End community (Raux et al., 2006).",
        "Figure 1 shows a sample dialogue.",
        "We assembled three disjoint datasets to have dialogues just before and after what we considered important architectural differences of Let's Go's implementation.",
        "By going through the change log, we realize that the magnitude of the differences varies widely.",
        "Table 1 describes the date the systems were deployed, the description of the change in the SDS, and the number of dialogues in each set.",
        "From the 86 dialogues of \"Set A\", half are a random sample of the logs before the acoustic model was retrained (Log1 ), and the other half is a sample after the acoustic model was retrained (Log2).",
        "In the case of the 84 dialogues of \"Set B\", half of the logs System: Welcome to the CMU Let's Go!",
        "Bus Information System.",
        "How may I help you?",
        "User: I'd like to go to Forbes and Murray.",
        "System: To Forbes and Murray.",
        "Where are you leaving from?",
        "User: Carnegie Mellon.",
        "System: From Carnegie Mellon.",
        "There are ive buses running between Carnegie Mellon and Forbes and Murray: the 50 , the 59U, the 6 A, the 6 B, and the 6 C.",
        "What else can I do for you?",
        "User: When is the next one?",
        "System: The next 6 C going to Forbes and Murray will leave Carnegie",
        "Mellon at 5: 3 PM.",
        "use an older version of the Olympus SDS infrastructure (Bohus et al., 2007), and the other half uses Olympus II.",
        "Since each system was deployed in a different period of time, we want to corroborate that we are modeling the differences among systems, and not seasonal.",
        "Hence, for control conditions, we also chose a data set that contained no major change to the system or to other conditions (Set C).",
        "Sets were built by randomly sampling from the collection of logs.",
        "They have the same number of dialogues from each SDS version (baseline/alternate).",
        "Each dialogue was manually annotated to indicate whether the user's query was fulilled, and we removed from our analysis the two dialogues that were only partially fulilled.",
        "The number of successful dialogues is different from the number of unsuccessful dialogues.",
        "We created a script to extract features from the log files of Let's Go!.",
        "The script has an explicit list of features to extract from the event logs, such as the words that were identiied by the Automatic Speech Recognizer.",
        "Although this script is dependent on our speciic log format, it should be a simple programming task to adapt it to a different dialogue system, provided its logs are comprehensive enough.",
        "The ßti ß*2 – regressions/^ (X,yt // Get feature weights: X' – X; where xk|Vxk G X', ßk =0",
        "Table 1: Dataset Description",
        "Set Size Description Table 2: Features Dialogue Properties # of re-prompted turns # of turns",
        "Mean Dialogue length Turn Properties",
        "Occurrences of word w script performs the standard transformation of centering feature values as z-scores with mean zero and standard deviation one.",
        "Table 2 summarizes the properties we are interested to model.",
        "Dialogue properties are the features that summarize the behavior of the whole dialogue, and turn properties work at a iner-grain.",
        "We encode turn properties into features in the following way:",
        "• Global average.",
        "Turn properties are averaged over the entire dialogue.",
        "• Beginning window.",
        "Turn properties are averaged across an initial window.",
        "Based on preliminary experiments, we deined the window as the irst 5 turns.",
        "• State.",
        "We relied on the fact that SDSs are often engineered as inite state automata (Bohus et al., 2007).",
        "Properties are averaged across the states that belong to a speciic dialogue state (for example, asking departure place).",
        "Because we are interested in early identiication of differences, we restricted state features to be inside the beginning window."
      ]
    },
    {
      "heading": "5. Evaluation",
      "text": [
        "We assess the performance of our algorithms by evaluating the classiication accuracy using the features selected.",
        "To facilitate assessment of SDS, we only consider models that select up to 15 features.",
        "Figure 2 reports mean classiication accuracy using ive-fold cross-validation.",
        "Its irst column describes how well the features selected perform on detecting system differences, and the second column describes how well they predict task success as a performance metric.",
        "We compare JEDI and SERENA against the following approaches:",
        "# of parse errors # of unrecognized words # of words # of repeated words # of unique words Turn length Words per minute",
        "Failed prompts (number and percentage) Mean Utterance Length Barge-in (in seconds) Machine-user pause (in seconds) User-machine pause (in seconds) Amplitude (power) statistics",
        "• Majority classifier baseline.",
        "A classifier that always selects the majority class (datasets B and C are not balanced in the number of successful dialogues).",
        "• Same Task Classifier We report the classification accuracy of the model trained and tested on the same task.",
        "Features are selected using an l1 penalty, and the coefficients are estimated with ^2-regularized logistic regression.",
        "For example, in the column of the left, SERENA uses the most predictive features of system differences to predict success, while the same task classiier uses them to predict system differences.",
        "The same task classiier does not answer \"which system differences matter\", it is just an interesting benchmark.",
        "We used a one-sample t-test to check for statistically signiicant differences against the classii-cation accuracy of the majority classiier baseline.",
        "We used a paired-sample t-test to check for sig-niicant differences in classiication accuracy between classiiers.",
        "Paired samples have the same A hyper-parameter, which was described in the risk-",
        "Baseline",
        "8/05",
        "10/05",
        "New acoustic model",
        "12/05",
        "2/05",
        "Baseline",
        "8/06",
        "10/06",
        "New SDS architecture",
        "6/07",
        "7/07",
        "Baseline",
        "10/07",
        "11/07",
        "No change",
        "11/07",
        "12/07",
        "System Differences Dialogue Success",
        "Table 3: Features selected in Dataset A Feature Suc.",
        "Diff.",
        "JEDI",
        "Majority O SERENA",
        "minimization formulation explained in Section 2.",
        "This hyper-parameter is related to the number of features selected - as A increases, the number of features selected decreases.",
        "We use 5% as the significance level at which to reject the null hypothesis.",
        "When checking for statistical differences, we tested on the range of As computed.",
        "First we investigate the performance of the simpler algorithm SERENA.",
        "For Dataset A, SERENA does not yield signiicant differences over the majority classiier baseline.",
        "For Dataset B, SERENA is signiicantly better than the majority classiier in predicting system differences, but is signiicantly worse for predicting success.",
        "This means that the order in which we choose the tasks in SERENA affects its performance.",
        "SERENA performs signiicantly worse in the Control Set C. We conclude that SER-",
        "ENA is not very reliable in predicting which system differences matter.",
        "We now discuss how well JEDI is able to ill-in for the deiciencies of SERENA.",
        "As an \"upper-bound\", we will compare it to a classiier trained and tested in the same task.",
        "This classiier signiicantly dominates over the majority baseline, even for the the Control Set C, where there were no changes in the SDS.",
        "This suggests that the classiier might be picking up on seasonal differences.",
        "For Set A, JEDI performs signiicantly better than the majority classi-ier and than SERENA.",
        "For Set B, there are no sig-niicant differences between the upper-bound clas-siier and JEDI when predicting for changes in the SDS.",
        "Again, JEDI dominates over SERENA and the majority baseline.",
        "For the Control Set C, JEDI is not statistically different from the majority baseline.",
        "This is the expected behavior, since the difference in performance cannot be explained by the differences between the SDS.",
        "We hypothesize that the classii-cation accuracy of JEDI could be used as a distance function between SDS: The closer the accuracy of distinguishing SDS is to 50%, the more similar the SDSs are.",
        "Conversely, when JEDI is able to classify system differences closer to 100%, it is because the SDSs are more different.",
        "Tables 3 and 4 describe the features selected for Sets A and B respectively.",
        "The numbers indicate in how many folds the feature was selected by JEDI and by classiiers trained to predict Success and SDS differences using ive-fold cross validation.",
        "The A used is selected to contain the closest to ive features (ties are resolved randomly).",
        "We only report features that appeared in at least three folds.",
        "In Dataset A we see that time of day is selected to predict dialogue success.",
        "Anecdotally, we have noticed that many users during weekend nights appear to be intoxicated when calling the system.",
        "JEDI does not select \"is weekend night\" as a feature, because it has little predictive power to detect system differences.",
        "In Dataset A, JEDI selects a speech recognition feature (the token \"Forbes.St\" was recognized), and an end-pointing feature.",
        "Since in Dataset A, the difference between systems correspond to a different acoustic model, these features make sense intuitively.",
        "In Dataset B, JEDI detected that the features most predictive with system differences and success are percentage of failed prompts and the length of the turn.",
        "The models for both systems make sense after the fact.",
        "However, neither model was known beforehand, nor did we know which of many features considered would turn out to be informative.",
        "Anecdotally, the documentation of the history of changes of Let's Go!",
        "is maintained manually.",
        "Sometimes, because of human error, this history is incomplete.",
        "The ability of JEDI to identify system differences has been able to help completing the history of changes (Gonzalez-Brenes et al., 2009).",
        "System-user pause",
        "5",
        "5",
        "Weekend night?",
        "3",
        "% of failed prompts",
        "4",
        "\"Forbes_St.\"",
        "word",
        "53",
        "User's max.",
        "power",
        "5",
        "Table 4: Features selected in Dataset B",
        "Feature",
        "Suc.",
        "Diff.",
        "JEDI",
        "% of failed prompts",
        "5",
        "4",
        "User's power std.dev.",
        "5",
        "Weekend night?",
        "3",
        "Unrecognized word",
        "5",
        "Words/min.",
        "4",
        "User-system pause",
        "5",
        "Turn length",
        "55"
      ]
    },
    {
      "heading": "6. Relation to Prior Work",
      "text": [
        "The scientiic literature offers several performance metrics to assess SDS performance (Polifroni et al., 1992; Danieli and Gerbino, 1995; Bacchiani et al., 2008; Suendermann et al., 2010).",
        "SDS are evaluated using different objective and subjective metrics.",
        "Examples of objective metrics are the mean number of turns in the dialogue, and dialogue success.",
        "Subjective evaluations study measure satisfaction through controlled user studies.",
        "Ai et al.",
        "(2007) studied the differences in using assessment metrics with real users and paid users.",
        "PARADISE, a notable example of a SDS subjective evaluation, inds linear predictors of a satisfaction score using automatic and hand-labeled features or only automatic features (Hastie et al., 2002).",
        "Satisfaction scores are calibrated using surveys in controlled experiments (Moller et al., 2007; Moller et al., 2008).",
        "Alternatively, Eckert et al.",
        "(1998) proposed simulated users to evaluate SDSs.",
        "Their performance metric has to be tuned with a subjective evaluation as well, in which they refer to the PARADISE methodology.",
        "Our approach does not require user surveys to be calibrated.",
        "Moreover, it would be feasible to adapt JEDI to regress to PARADISE, or other performance metrics.",
        "Our work extends previous studies that deine performance metrics, in proposing an algorithm that inds how system differences are related to performance."
      ]
    },
    {
      "heading": "7. Conclusions and Future Work",
      "text": [
        "We have presented JEDI, a novel algorithm that inds features describing system differences relevant to a success metric.",
        "This is a novel, automated \"glass box\" assessment in the sense of linking changes in overall performance to speciic behavioral changes.",
        "JEDI is an application of feature selection using regularized regression.",
        "We have presented empirical evidence suggesting that JEDI's use of multi-task feature selection performs better than single-task feature selection.",
        "Future work could extend JEDI to quantify the variability in performance explained by the differences found.",
        "Common techniques in econometrics, such as the Seemingly Unrelated Regressions (SUR) formulation (Zellner, 1962), may prove useful for this.",
        "In our approach we used a single binary evaluation criterion.",
        "By using a different loss function, JEDI can be extended to allow continuous-valued metrics.",
        "Moreover, previous work has argued that evaluating SDSs should not be based on just a single criterion (Paek, 2001).",
        "JEDI's multi-task formulation can be extended to include more than one performance criterion at the same time, and may prove helpful to understand trade-offs among different evaluation criteria.",
        "A Implementation Details of Feature Selection",
        "In this appendix we review how to set-up multi-task feature selection as an optimization problem.",
        "A.1 ^1-Regularized Regression for Single-Task Feature Selection",
        "We first review using regression with ^1 regularization for single-task feature selection.",
        "Given a training set represented by X, denoting a n x k matrix, where n is the number of dialogues, and k is the number of features extracted for each dialogue, we want to ind the coeficients of the parameter vector ß, that can predict the output variables described in the vector y of length n.",
        "For this, we ind the parameter vector that minimizes the loss function J, penalized by a regularization term (Tibshirani, 1996):",
        "In the case of binary classiication, outputs are binary (any given y = ±1).",
        "A commonly used loss function J is the Logistic Loss:",
        "The ^oo-norm is defined as ||ß||too = max(ß1,ß2,... ,ßk).",
        "The regularization term ||ß||tl in Equation 3 controls model complexity: The higher the value of the hyper-parameter A, the smaller number of features selected.",
        "Conversely, the smaller the value of A, the better the it to the training data, with higher risk of over-itting it.",
        "Thus, Equation 3 jointly performs feature selection and parameter estimation; it induces sparsity by setting many coefficients of ß to zero (Tibshirani, 1996).",
        "Features with non-zero coeficients are considered the features selected.",
        "A.2 ^1-Regularized Regression for Multi-Task Feature Selection ^1 regularization can be used to learn a classifier for each of T prediction task independently.",
        "In our case we are interested in only two prediction tasks: version and success.",
        "We will index tasks with superscript t, and we define X* as the n x k training data for task t, used to predict the output variable yt.",
        "Learning each model separately yields the following optimization problem (Obozinski et al., 2007):",
        "Solving this problem leads to individual sparsity in each task (each ß has many zeros), but the model does not enforce a common subset of features for all of the related output variables simultaneously (Turlach et al., 2005).",
        "In the next subsection we study how to achieve global sparsity across tasks.",
        "A.3 £1 /^-Regularized Regression for Multi-task Feature Selection",
        "Although ^1-regularization is very successful at selecting individual features, it does not perform adequately when a group of features should enter or leave the model simultaneously (Yuan and Lin, 2006).",
        "Group LASSO (Yuan and Lin, 2006), which relies on ^1/^p regularization to overcome this limitation, by allowing groups of feature entering or leaving the model simultaneously.",
        "^1/^p regularization has been studied for multi-task learning by grouping each of the k features across the T learning",
        "Lounici et al., 2009; Puniyani et al., 2010).",
        "Let us define B as a n x T matrix, whose tth column is the parameter vector for the task t. For example, since we have two tasks B = [ß , ß ].",
        "Let ßg denote the gth row of B.",
        "In the context of multi-task learning, the ^1/^p-norm of a matrix B is deined as (Obozinski et al., 2007; Puniyani et al.,",
        "Multi-task feature selection with ^1/^p regularization is formulated as (Obozinski et al., 2007;",
        "When T = 1 , the multi-task problem of Equation 7 reduces to the single-task problem of Equation 5.",
        "A.4 Optimization procedure",
        "Puniyani et al.",
        "(2010) describe that inding the parameter coefficients B of Equation 7 can be achieved more easily by transforming the problem into an equivalent single-task multivariate regression.",
        "We follow their procedure to create yg, ßg and Xg :",
        "1.",
        "Concatenate the vectors yt's into a single vector yg of length n x T. In our case, since we have only two tasks (T = 2), we get the vector",
        "yg = [£2].",
        "through Grant R305A080628 to Carnegie Mellon",
        "University.",
        "The opinions expressed are those of the authors and do not necessarily represent the views of the Institute or U.S. Department of Education.",
        "We thank the educators, students, and LISTENers who helped generate, collect, and analyze our data, and the reviewers for their helpful comments.",
        "The irst author was partially supported by the Costa Rican Ministry of Science and Technology (MICIT)."
      ]
    }
  ]
}
