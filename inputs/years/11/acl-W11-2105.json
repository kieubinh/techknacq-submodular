{
  "info": {
    "authors": [
      "Boxing Chen",
      "Roland Kuhn"
    ],
    "book": "Proceedings of the Sixth Workshop on Statistical Machine Translation",
    "id": "acl-W11-2105",
    "title": "AMBER: A Modified BLEU, Enhanced Ranking Metric",
    "url": "https://aclweb.org/anthology/W11-2105",
    "year": 2011
  },
  "references": [
    "acl-D08-1064",
    "acl-D10-1092",
    "acl-E06-1032",
    "acl-N10-1080",
    "acl-P02-1040",
    "acl-P04-1077",
    "acl-P08-1007",
    "acl-P09-1034",
    "acl-W05-0909",
    "acl-W08-0309",
    "acl-W09-0441",
    "acl-W10-1749",
    "acl-W10-1751",
    "acl-W10-1753",
    "acl-W10-1754"
  ],
  "sections": [
    {
      "text": [
        "Boxing Chen and Roland Kuhn",
        "National Research Council of Canada, Gatineau, Québec, Canada First.Last@nrc.gc.ca",
        "This paper proposes a new automatic machine translation evaluation metric: AMBER, which is based on the metric BLEU but incorporates recall, extra penalties, and some text processing variants.",
        "There is very little linguistic information in AMBER.",
        "We evaluate its system-level correlation and sentence-level consistency scores with human rankings from the WMT shared evaluation task; AMBER achieves state-of-the-art performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Automatic evaluation metrics for machine translation (MT) quality play a critical role in the development of statistical MT systems.",
        "Several metrics have been proposed in recent years.",
        "Metrics such as BLEU (Papineni et al., 2002), NIST (Dodding-ton, 2002), WER, PER, and TER (Snover et al., 2006) do not use any linguistic information - they only apply surface matching.",
        "METEOR (Banerjee and Lavie, 2005), METEOR-NEXT (Denkowski and Lavie 2010), TER-Plus (Snover et al., 2009), MaxSim (Chan and Ng, 2008), and TESLA (Liu et al., 2010) exploit some limited linguistic resources, such as synonym dictionaries, part-of-speech tagging or paraphrasing tables.",
        "More sophisticated metrics such as RTE (Padó et al., 2009) and DCU-LFG (He et al., 2010) use higher level syntactic or semantic analysis to score translations.",
        "Though several of these metrics have shown better correlation with human judgment than BLEU, BLEU is still the de facto standard evaluation metric.",
        "This is probably due to the following facts:",
        "1.",
        "BLEU is language independent (except for word segmentation decisions).",
        "2.",
        "BLEU can be computed quickly.",
        "This is important when choosing a metric to tune an MT system.",
        "3.",
        "BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010).",
        "When we developed our own metric, we decided to make it a modified version of BLEU whose rankings of translations would (ideally) correlate even more highly with human rankings.",
        "Thus, our metric is called AMBER: \"A Modified Bleu, Enhanced Ranking\" metric.",
        "Some of the AMBER variants use an information source with a mild linguistic flavour - morphological knowledge about suffixes, roots and prefixes - but otherwise, the metric is based entirely on surface comparisons."
      ]
    },
    {
      "heading": "2. AMBER",
      "text": [
        "Like BLEU, AMBER is composed of two parts: a score and a penalty.",
        "AMBER = score X penalty (1)",
        "To address weaknesses of BLEU described in the literature (Callison-Burch et al., 2006; Lavie and Denkowski, 2009), we use more sophisticated formulae to compute the score and penalty.",
        "First, we enrich the score part with geometric average of n-gram precisions (AvgP), F-measure derived from the arithmetic averages of precision and recall (Fmean), and arithmetic average of F-measure of precision and recall for each n-gram (AvgF).",
        "Let us define n-gram precision and recall as follows:",
        "# ngrams(T n R) where T = translation, R = reference.",
        "Then the geometric average of n-gram precisions AvgP, which is also the score part of the BLEU metric, is defined as:",
        "The arithmetic averages for n-gram precision and recall are:",
        "The F-measure that is derived from P(N) and R(M), (Fmean), is given by:",
        "The arithmetic average of F-measure of precision and recall for each n-gram (AvgF) is given by:",
        "The score is the weighted average of the three values: AvgP, Fmean, and AvgF.",
        "The free parameters N, M, a , 61 and 62 were manually tuned on a dev set.",
        "Instead of the original brevity penalty, we experimented with a product of various penalties:",
        "penalty = Y\\ penw where wi is the weight of each penalty peni.",
        "Strict brevity penalty (SBP): (Chiang et al., 2008) proposed this penalty.",
        "Let ti be the translation of input sentence i, and let ri be its reference (or if there is more than one, the reference whose length in words I ri I is closest to length I ti I).",
        "Set SBP = exp",
        "Zimin{I ti I,I r I}",
        "Strict redundancy penalty (SRP): long sentences are preferred by recall.",
        "Since we rely on both recall and precision to compute the score, it is necessary to punish the sentences that are too long.",
        "SRP = exp",
        "Zmax{I ti I,I ri I}",
        "Character-based strict brevity penalty (CSBP) and Character-based strict redundancy penalty (CSRP) are defined similarly.",
        "The only difference with the above two penalties is that here, length is measured in characters.",
        "Chunk penalty (CKP): the same penalty as in",
        "# matches(word ) Y and ß are free parameters.",
        "We do not compute the word alignment between the translation and reference; therefore, the number of chunks is computed as #chunks =#matches(bigram)-#matches(word) .",
        "For example, in the following two-sentence translation (references not shown), let \"m\" stand for a matched word, \"x\" stand for zero, one or more unmatched words:",
        "S1: m1 m2 x m3 m4 m5 x m6S2: m7 x m8 m9 x m10 m11 m12 x m13If we consider only unigrams and bigrams, there are 13 matched words and 6 matched bigrams (m1 m2, m3 m4, m4 m5, m8 m9, m10 m11, m11 m12), so there are 13-6=7 chunks (m1 m2, m3 m4 m5, m6, m7, m8m9, mio mn mn, mu).",
        "Continuity penalty (CTP): if all matched words are continuous, then #ngrams(T n R) 1.",
        "CTP = expl # ngramsij n R)",
        "Short word difference penalty (SWDP): a good translation should have roughly the same number of stop words as the reference.",
        "To make AMBER more portable across all Indo-European languages, we use short words (those with fewer than 4 characters) to approximate the stop words.",
        "# unigram(r )",
        "where a and b are the number of short words in the translation and reference respectively.",
        "Long word difference penalty (LWDP): is defined similarly to SWDP.",
        "where c and d are the number of long words (those longer than 3 characters) in the translation and reference respectively.",
        "Normalized Spearman's correlation penalty (NSCP): we adopt this from (Isozaki et al., 2010).",
        "This penalty evaluates similarity in word order between the translation and reference.",
        "We first determine word correspondences between the translation and reference; then, we rank words by their position in the sentences.",
        "Finally, we compute Spearman's correlation between the ranks of the n words common to the translation and reference.",
        "where di indicates the distance between the ranks of the i-th element.",
        "For example:",
        "T: Bob reading book likes",
        "R: Bob likes reading book The rank vector of the reference is [1, 2, 3, 4], while the translation rank vector is [1, 3, 4, 2].",
        "The Spearman's correlation score between these two",
        "In order to avoid negative values, we normalized the correlation score, obtaining the penalty NSCP: NSCP = (1 + p) / 2 (17)",
        "Normalized Kendall's correlation penalty (NKCP): this is adopted from (Birch and Osborne, 2010) and (Isozaki et al., 2010).",
        "In the previous example, where the rank vector of the"
      ]
    },
    {
      "heading": "2. x # increasing pairs 1",
      "text": [
        "# all pairs",
        "Therefore, Kendall's correlation for the translation \"Bob reading book likes\" is 2x4/6 – 1=0.33.",
        "Again, to avoid negative values, we normalized the coefficient score, obtaining the penalty NKCP: NKCP = (1 + t) / 2 (19)",
        "The original BLEU metric weights all n-grams equally; however, different n-grams have different amounts of information.",
        "We experimented with applying tf-idf to weight each n-gram according to its information value.",
        "In the original BLEU metric, there is only one matching strategy: n-gram matching.",
        "In AMBER, we provide four matching strategies (the best AMBER variant used three of these):",
        "1.",
        "N-gram matching: involved in computing precision and recall.",
        "2.",
        "Fixed-gap n-gram: the size of the gap between words \"word1 [] word2\" is fixed; involved in computing precision only.",
        "3.",
        "Flexible-gap n-gram: the size of the gap between words \"word1 * word2\" is flexible; involved in computing precision only.",
        "4.",
        "Skip n-gram: as used ROUGE (Lin, 2004); involved in computing precision only.",
        "The AMBER score can be computed with different types of preprocessing.",
        "When using more than one type, we computed the final score as an average over runs, one run per type (our default AMBER variant used three of the preprocessing types):",
        "Final _ AMBER = – I AMBER(t )",
        "We provide 8 types of possible text input: 0.",
        "Original - true-cased and untokenized.",
        "1.",
        "Normalized - tokenized and lower-cased.",
        "(All variants 2-7 below also tokenized and lower-cased.)",
        "2.",
        "\"Stemmed\" - each word only keeps its first 4 letters.",
        "3.",
        "\"Suffixed\" - each word only keeps its last 4 letters.",
        "4.",
        "Split type 1 - each longer-than-4-letter word is segmented into two sub-words, with one being the first 4 letters and the other the last 2 letters.",
        "If the word has 5 letters, the 4th letter appears twice: e.g., \"gangs\" becomes \"gang\" + \"gs\".",
        "If the word has more than 6 letters, the middle part is thrown away",
        "5.",
        "Split type 2 - each word is segmented into fixed-length (4-letter) sub-word sequences, starting from the left.",
        "6.",
        "Split type 3 - each word is segmented into prefix, root, and suffix.",
        "The list of English prefixes, roots, and suffixes used to split the word is from the Internet; it is used to split words from all languages.",
        "Linguistic knowledge is applied here (but not in any other aspect of AMBER).",
        "7.",
        "Long words only - small words (those with fewer than 4 letters) are removed."
      ]
    },
    {
      "heading": "3. Experiments",
      "text": [
        "We evaluated AMBER on WMT data, using WMT 2008 all-to-English submissions as the dev set.",
        "Test sets include WMT 2009 all-to-English, WMT 2010 all-to-English and 2010 English-to-all submissions.",
        "Table 1 summarizes the dev and test set statistics.",
        "Before evaluation, we manually tuned all free parameters on the dev set to maximize the systemlevel correlation with human judgments and decided on the following default settings for",
        "1.",
        "The parameters in the formula score( N ) = 01 x AvgP( N ) and 6>2= 0.5."
      ]
    },
    {
      "heading": "2.. All penalties are applied; the manually set",
      "text": [
        "penalty weights are shown in Table 2."
      ]
    },
    {
      "heading": "3.. We took the average of runs over input text",
      "text": [
        "types 1, 4, and 6 (i.e. normalized text, split type 1 and split type 3).",
        "7=0.1.",
        "5.",
        "By default, tf-idf is not applied."
      ]
    },
    {
      "heading": "6.. We used three matching strategies: n-gram,",
      "text": [
        "fixed-gap n-gram, and flexible-gap n-gram; they are equally weighted.",
        "Table 2: Weight of each penalty",
        "We used Spearman's rank correlation coefficient to measure the correlation of AMBER with the human judgments of translation at the system level.",
        "The human judgment score we used is based on the \"Rank\" only, i.e., how often the translations of the system were rated as better than the translations from other systems (Callison-Burch et al., 2008).",
        "Thus, AMBER and the other metrics were evaluated on how well their rankings correlated with the human ones.",
        "For the sentence level, we use consistency rate, i.e., how consistent the ranking of sentence pairs is with the human judgments.",
        "Name of penalty",
        "Weight value",
        "SBP",
        "0.30",
        "SRP",
        "0.10",
        "CSBP",
        "0.15",
        "CSRP",
        "0.05",
        "SWDP",
        "0.10",
        "LWDP",
        "0.20",
        "CKP",
        "1.00",
        "CTP",
        "0.80",
        "NSCP",
        "0.50",
        "NKCP",
        "2.00",
        "Set",
        "Dev",
        "Test1",
        "Test2",
        "Test3",
        "Year",
        "2008",
        "2009",
        "2010",
        "2010",
        "Lang.",
        "xx-en",
        "xx-en",
        "xx-en",
        "en-xx",
        "#system",
        "43",
        "39",
        "53",
        "32",
        "#sent-pair",
        "7,861",
        "13,912",
        "14,212",
        "13,165",
        "Table 1: statistics of the dev and test sets.",
        "All test results shown in this section are averaged over all three tests described in 3.1.",
        "First, we compare AMBER with two of the most widely used metrics: original IBM BLEU and METEOR v1.0.",
        "Table 3 gives the results; it shows both the version of AMBER with basic preprocessing, AMBER(1) (with tokenization and lowercasing) and the default version used as baseline for most of our experiments (AMBER(1,4,6)).",
        "Both versions of AMBER perform better than BLEU and METEOR on both system and sentence levels.",
        "Table 3: Results of AMBER vs BLEU and METEOR",
        "Second, as shown in Table 4, we evaluated the impact of different types of preprocessing, and some combinations of preprocessing (we do one run of evaluation for each type and average the results).",
        "From this table, we can see that splitting words into sub-words improves both system-and sentence-level correlation.",
        "Recall that input 6 preprocessing splits words according to a list of English prefixes, roots, and suffixes: AMBER(4,6) is the best variant.",
        "Although test 3 results, for target languages other than English, are not broken out separately in this table, they are as follows: input 1 yielded 0.8345 system-level correlation and 0.5848 sentence-level consistency, but input 6 respectively.",
        "Thus, surprisingly, splitting non-English words up according to English morphology helps performance, perhaps because French, Spanish, German, and even Czech share some word roots with English.",
        "However, as indicated by the underlined results, if one wishes to avoid the use of any linguistic information, AMBER(4) performs almost as well as AMBER(4,6).",
        "The default setting, AMBER(1,4,6), doesn't perform quite as well as AMBER(4,6) or AMBER(4), but is quite reasonable.",
        "Varying the preprocessing seems to have more impact than varying the other parameters we experimented with.",
        "In Table 5, \"none+tf-idf ' means we do one run without tf-idf and one run for \"tf-idf only\", and then average the scores.",
        "Here, applying tf-idf seems to benefit performance slightly.",
        "Table 4: Varying AMBER preprocessing (best linguistic = bold, best non-ling.",
        "= underline)",
        "Table 6 shows what happens if you disable one penalty at a time (leaving the weights of the other penalties at their original values).",
        "The biggest system-level performance degradation occurs when LWDP is dropped, so this seems to be the most useful penalty.",
        "On the other hand, dropping CKP, CSRP, and SRP may actually improve performance.",
        "Firm conclusions would require retuning of weights each time a penalty is dropped; this is future work.",
        "Input",
        "Dev",
        "3 tests average",
        "A tests",
        "0",
        "sys",
        "0.84",
        "0.79",
        "N/A",
        "(baseline)",
        "sent",
        "0.59",
        "0.58",
        "N/A",
        "1",
        "sys",
        "0.83",
        "0.83",
        "+0.04",
        "sent",
        "0.61",
        "0.58",
        "+0.00",
        "2",
        "sys",
        "0.83",
        "0.84",
        "+0.05",
        "sent",
        "0.61",
        "0.59",
        "+0.01",
        "3",
        "sys",
        "0.83",
        "0.84",
        "+0.05",
        "sent",
        "0.61",
        "0.58",
        "+0.00",
        "4",
        "sys",
        "0.84",
        "0.87",
        "+0.08",
        "sent",
        "0.62",
        "0.60",
        "+0.01",
        "5",
        "sys",
        "0.82",
        "0.86",
        "+0.07",
        "sent",
        "0.61",
        "0.56",
        "+0.01",
        "6",
        "sys",
        "0.83",
        "0.88",
        "+0.09",
        "sent",
        "0.62",
        "0.60",
        "+0.02",
        "7",
        "sys",
        "0.34",
        "0.56",
        "-0.23",
        "sent",
        "0.58",
        "0.53",
        "-0.05",
        "1,4",
        "sys",
        "0.84",
        "0.85",
        "+0.07",
        "sent",
        "0.62",
        "0.60",
        "+0.01",
        "4,6",
        "sys",
        "0.83",
        "0.88",
        "+0.09",
        "sent",
        "0.62",
        "0.60",
        "+0.02",
        "1,4,6",
        "sys",
        "0.84",
        "0.86",
        "+0.07",
        "sent",
        "0.62",
        "0.60",
        "+0.02",
        "Metric",
        "Dev",
        "3 tests average",
        "A tests",
        "BLEU_ibm",
        "sys",
        "0.68",
        "0.72",
        "N/A",
        "(baseline)",
        "sent",
        "0.37",
        "0.40",
        "N/A",
        "METEOR",
        "sys",
        "0.80",
        "0.80",
        "+0.08",
        "v1.0",
        "sent",
        "0.58",
        "0.56",
        "+0.17",
        "AMBER(1)",
        "sys",
        "0.83",
        "0.83",
        "+0.11",
        "(basic preproc.)",
        "sent",
        "0.61",
        "0.58",
        "+0.19",
        "AMBER(1,4,6)",
        "sys",
        "0.84",
        "0.86",
        "+0.14",
        "(default)",
        "sent",
        "0.62",
        "0.60",
        "+0.20",
        "tf-idf",
        "Dev",
        "3 tests average",
        "A tests",
        "none",
        "sys",
        "0.84",
        "0.86",
        "N/A",
        "(baseline)",
        "sent",
        "0.62",
        "0.60",
        "N/A",
        "tf-idf",
        "sys",
        "0.81",
        "0.88",
        "+0.02",
        "only",
        "sent",
        "0.62",
        "0.61",
        "+0.01",
        "none+tf-",
        "sys",
        "0.82",
        "0.87",
        "+0.01",
        "idf",
        "sent",
        "0.62",
        "0.61",
        "+0.01",
        "Table 6: Dropping penalties from AMBER(1,4,6) biggest drops on test in bold Table 7: Varying matching strategy for AMBER(1,4,6)",
        "Finally, we evaluated the effect of the matching strategy.",
        "According to the results shown in Table 7, our default strategy, which uses three of the four types of matching (n-grams, fixed-gap n-grams, and flexible-gap n-grams) is close to optimal; the use of skip n-grams (either by itself or in combination) may hurt performance at both system and sentence levels."
      ]
    },
    {
      "heading": "4. Conclusion",
      "text": [
        "This paper describes AMBER, a new machine translation metric that is a modification of the widely used BLEU metric.",
        "We used more sophisticated formulae to compute the score, we developed several new penalties to match the human judgment, we tried different preprocessing types, we tried tf-idf, and we tried four n-gram matching strategies.",
        "The choice of preprocessing type seemed to have the biggest impact on performance.",
        "AMBER(4,6) had the best performance of any variant we tried.",
        "However, it has the disadvantage of using some light linguistic knowledge about English morphology (which, oddly, seems to be helpful for other languages too).",
        "A purist may prefer AMBER(1,4) or AMBER(4), which use no linguistic information and still match human judgment much more closely than either BLEU or",
        "METEOR.",
        "These variants of AMBER share",
        "BLEU's virtues: they are language-independent and can be computed quickly.",
        "Of course, AMBER could incorporate more linguistic information: e.g., we could use linguistically defined stop word lists in the SWDP and LWDP penalties, or use synonyms or paraphrasing in the n-gram matching.",
        "AMBER can be thought of as a weighted combination of dozens of computationally cheap features based on word surface forms for evaluating MT quality.",
        "This paper has shown that combining such features can be a very effective strategy for attaining better correlation with human judgment.",
        "Here, the weights on the features were manually tuned; in future work, we plan to learn weights on features automatically.",
        "We also plan to redesign AMBER so that it becomes a metric that is highly suitable for tuning SMT systems.",
        "Penalties",
        "Dev",
        "3 tests average",
        "A tests",
        "All",
        "sys",
        "0.84",
        "0.86",
        "N/A",
        "(baseline)",
        "sent",
        "0.62",
        "0.60",
        "N/A",
        "-SBP",
        "sys",
        "0.82",
        "0.84",
        "-0.02",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "-SRP",
        "sys",
        "0.83",
        "0.88",
        "+0.01",
        "sent",
        "0.62",
        "0.60",
        "+0.00",
        "-CSBP",
        "sys",
        "0.84",
        "0.85",
        "-0.01",
        "sent",
        "0.62",
        "0.60",
        "+0.00",
        "-CSRP",
        "sys",
        "0.83",
        "0.87",
        "+0.01",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "-SWDP",
        "sys",
        "0.84",
        "0.86",
        "-0.00",
        "sent",
        "0.62",
        "0.60",
        "+0.00",
        "-LWDP",
        "sys",
        "0.83",
        "0.83",
        "-0.03",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "-CTP",
        "sys",
        "0.82",
        "0.84",
        "-0.02",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "-CKP",
        "sys",
        "0.83",
        "0.87",
        "+0.01",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "-NSCP",
        "sys",
        "0.83",
        "0.86",
        "-0.00",
        "sent",
        "0.62",
        "0.60",
        "+0.00",
        "-NKCP",
        "sys",
        "0.82",
        "0.85",
        "-0.01",
        "sent",
        "0.62",
        "0.60",
        "+0.00",
        "Matching",
        "Dev",
        "3 tests avg",
        "A tests",
        "n-gram + fxd-",
        "sys",
        "0.84",
        "0.86",
        "N/A",
        "gap+ flx-gap",
        "sent",
        "0.62",
        "0.60",
        "N/A",
        "(default)",
        "n-gram",
        "sys",
        "0.84",
        "0.86",
        "-0.00",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "fxd-gap+",
        "sys",
        "0.84",
        "0.86",
        "-0.00",
        "n-gram",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "flx-gap+",
        "sys",
        "0.83",
        "0.86",
        "-0.00",
        "n-gram",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "skip+",
        "sys",
        "0.83",
        "0.85",
        "-0.01",
        "n-gram",
        "sent",
        "0.62",
        "0.60",
        "-0.00",
        "All four",
        "sys",
        "0.83",
        "0.86",
        "-0.01",
        "matchings",
        "sent",
        "0.62",
        "0.60",
        "0.00"
      ]
    }
  ]
}
