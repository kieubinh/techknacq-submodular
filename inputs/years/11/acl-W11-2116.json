{
  "info": {
    "authors": [
      "Jesús González-Rubio",
      "Francisco Casacuberta"
    ],
    "book": "Proceedings of the Sixth Workshop on Statistical Machine Translation",
    "id": "acl-W11-2116",
    "title": "The UPV-PRHLT combination system for WMT 2011",
    "url": "https://aclweb.org/anthology/W11-2116",
    "year": 2011
  },
  "references": [
    "acl-C10-1036",
    "acl-D08-1065",
    "acl-N04-1022",
    "acl-N07-1029",
    "acl-N10-1141",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P09-1019",
    "acl-P11-1127",
    "acl-W10-1743"
  ],
  "sections": [
    {
      "text": [
        "Jesus Gonzalez-Rubio and Francisco Casacuberta",
        "Instituto Tecnologico de Informatica Departamento de Sistemas Informaticos y Computacion Universität Politecnica de Valencia",
        "{jegonzalez|fcn}@dsic.upv.es"
      ]
    },
    {
      "heading": "2. Minimum Bayes risk Decoding",
      "text": [
        "This paper presents the submissions of the pattern recognition and human language technology (PRHLT) group to the system combination task of the sixth workshop on statistical machine translation (WMT 2011).",
        "Each submissions is generated by a multi-system minimum Bayes risk (MBR) technique.",
        "Our technique uses the MBR decision rule and a linear combination of the component systems' probability distributions to search for the minimum risk translation among all the sentences in the target language."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The UPV-PHRLT approach to machine translation (MT) system combination is based on the minimum Bayes risk system combination (MBRSC) algorithm (Gonzlez-Rubio et al., 2011).",
        "A multisystem MBR technique that computes consensus translations over multiple component systems.",
        "MBRSC operates directly on the outputs of the component models.",
        "We perform an MBR decoding using a linear combination of the component models' probability distributions.",
        "Instead of re-ranking the translations provided by the component systems, we search for the hypothesis with the minimum expected translation error among all the possible finite-length strings in the target language.",
        "By using a loss function based on BLEU (Papineni et al., 2002), we avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007).",
        "MBRSC assumes only that each translation model can produce expectations of n-gram counts; the latent derivation structures of the component systems can differ arbitrary.",
        "This flexibility allows us to combine a great variety of MT systems.",
        "SMT can be described as a mapping of a word sequence f in a source language to a word sequence e in a target language; this mapping is produced by the MT decoder D(f ).",
        "If the reference translation e is known, the decoder performance can be measured by the loss function L(e, D(f )).",
        "Given such a loss function L(e, e') between an automatic translation e' and a reference e, and an underlying probability model P (e|f ), MBR decoding has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004):",
        "where R(e') denotes the Bayes risk of candidate translation e' under loss function L, and E represents the space of translations.",
        "If the loss function between any two hypotheses can be bounded: L(e, e') < Lmax, the MBR decoder can be rewritten in term of a similarity function S(e, e') = Lmax – L(e, e').",
        "In this case, instead of minimizing the Bayes risk, we maximize the Bayes gain G(e'):",
        "MBR decoding can use different spaces for hypothesis selection and gain computation (arg max and sum in Eq.",
        "(4)).",
        "Therefore, the MBR decoder can be more generally written as follows:",
        "e'eEh eeEe",
        "where Eh refers to the hypotheses space form where the translations are chosen and Ee refers to the evidences space that is used to compute the Bayes gain.",
        "We will investigate the expansion of the hypotheses space while keeping the evidences space as provided by the decoder."
      ]
    },
    {
      "heading": "3. MBR System Combination",
      "text": [
        "MBRSC is a multi-system generalization of MBR decoding.",
        "It uses the MBR decision rule on a linear combination of the probability distributions of the component systems.",
        "Unlike existing MBR decoding methods that re-rank translation outputs, MBRSC search for the minimum risk hypotheses on the complete set of finite-length hypotheses over the output vocabulary.",
        "We assume the component systems to be statistically independent and define the Bayes gain as a linear combination of the Bayes gains of the components.",
        "Each system provides its own space of evidences Dn(f ) and its posterior distribution over translations Pn(e|f ).",
        "Given a sentence f in the source language, MBRSC is written as follows:",
        "e'eEh n=i",
        "where N is the total number of component systems, Eh represents the hypotheses space where the search is performed, Gn(e') is the Bayes gain of hypothesis e' given by the nth component system and an is a scaling factor introduced to take into account the differences in quality of the component models.",
        "It is worth mentioning that by using a linear combination instead of a mixture model, we avoid the problem of component systems not sharing the same search space (Duan et al., 2010).",
        "We are interested in performing MBRSC under BLEU.",
        "Therefore, we rewrite the gain function G(•) using single evidence (or reference) BLEU (Papwhere r is the length of the evidence, c the length of the hypothesis, mk the number of n-gram matches of size k, and ck the count of n-grams of size k in the hypothesis.",
        "The evidences space Dn(f ) may contain a huge number of hypotheses which often make impractical to compute Eq.",
        "(9) directly.",
        "To avoid this problem, Tromble et al.",
        "(2008) propose linear BLEU, an approximation to the BLEU score to efficiently perform MBR decoding on the lattices provided by the component systems.",
        "However, we want to explore a hypotheses space not restricted to the evidences provided by the systems.",
        "In Eq.",
        "(9), we have one hypothesis e' that is to be compared to a set of evidences e G Dn(f ) which follow a probability distribution Pn(e|f).",
        "Instead of computing the expected BLEU score by calculating the BLEU score with respect to each of the evidences, our approach will be to use the expected n-gram counts and sentence length of the evidences to compute a single-reference BLEU score.",
        "We replace the reference statistics (r and mn in Eq.",
        "(10)) by the expected statistics (r' and given the posterior distribution Pn(e| f) over the evidences:",
        "where A4 (e') is the set of n-grams of size k in the hypothesis, Ce' (ng) is the count of the n-gram ng in the hypothesis and C'(ng) is the expected count of ng in the evidences.",
        "To compute the n-gram matchings m'k, the count of each n-gram is truncated, if necessary, to not exceed the expected count for that n-gram in the evidences.",
        "We have replaced a summation over a possibly exponential number of items (e' G Dn(f ) in Eq.",
        "(9)) with a summation over a polynomial number of n-grams that occur in the evidences.",
        "Both, the expected length of the evidences r' and their expected n-gram counts m'k can be precomputed efficiently from N-best lists and translation lattices (Kumar et al., 2009; DeNero et al., 2010).",
        "The scaling factors in Eq.",
        "(8) denote the \"quality\" of each system with respect to the rest of them, i.e. the relative importance of each system in the Bayes gain computation.",
        "This scaling factors must be carefully tuned to obtain good translations.",
        "We compute the scaling factor of each system as the number of times the hypothesis of the system is the best TER-scoring translation in the tuning corpora.",
        "Previous works show that this measure obtains the best translation results among other heuristic measures (Gonzalez-Rubio et al., 2010) and even as good results as more complex methods such as MERT (Och, 2003).",
        "A normalization is performed to transform these counts into the range [0.0,1.0].",
        "After the normalization, a weight value of o.o is assigned to the lowest-scoring system, i.e. the lowest-scoring system is discarded and not taken into account in the computation of the Bayes gain.",
        "In most MBR algorithms, the hypotheses space is equal to the evidences space.",
        "However, we are interested in extend the hypotheses space by including new sentences created using fragments of the hypotheses in the evidences spaces of the component models.",
        "We perform the search (argmax operation in Eq.",
        "(8)) using the approximate median string (AMS) algorithm (Martinez et al., 2000).",
        "AMS algorithm perform a hill-climbing search on a hypotheses space equal to the free monoid X* of the vocabulary of the evidences X = Voc(Ee ).",
        "Algorithm 1 MBRSC decoding algorithm.",
        "Require: Initial hypothesis e Require: Vocabulary the evidences X",
        "2: repeat",
        "The AMS algorithm is shown in Algorithm 1.",
        "AMS starts with an initial hypothesis e that is modified using edit operations until there is no improvement in the Bayes gain (Lines 3-16).",
        "On each position j of the current solution ecur, we apply all the possible single edit operations: substitution of the jth word of ecur by each word a in the vocabulary (Lines 5-9), deletion of the jth word of ecur (Line 10) and insertion of each word a in the vocabulary in the jth position of ecur (Lines 11-15).",
        "If the Bayes gain of any of the new edited hypotheses is higher than the Bayes gain of the current hypothesis (Line 17), we repeat the loop with this new hypotheses e, in other case, we return the current hypothesis.",
        "AMS algorithm takes as input an initial hypothesis e and the combined vocabulary of the evidences spaces X.",
        "Its output is a possibly new hypothesis whose Bayes gain is assured to be higher or equal than the Bayes gain of the initial hypothesis.",
        "is polynomial in the number of edges in the lattice.",
        "Table 1: BLEU scores (case-sensitive) on the shared translation task development and test corpora of the best and worst single systems and MBRSC.",
        "For each translation direction, we show the number of systems being combined.",
        "Best translation results are in bold.",
        "puting the gain of a hypothesis, and usually only a moderate number of iterations (< 10) is needed to converge (Martinez et al., 2000)."
      ]
    },
    {
      "heading": "4. Results",
      "text": [
        "Experiments were conducted on all the 8 translation directions of the shared translation task Czech-English (cz – en), German-English (de – en), Spanish-English (es – en) and French-English (fr – en) and also on the raw and clean versions of the Haitian creole-English featured translation task (ht – en).",
        "All the experiments were carried out with the true-cased, detokenized version of the tuning and test corpora, following the WMT 2011 submission guidelines.",
        "development and test corpora in comparison with the score of the best and worst individual systems.",
        "In most of the translation directions, MBRSC improved the results of the best individual system, e.g. +2.7/+3.0 BLEU point in es – en.",
        "However, in en – cz and en – fr, MBRSC performs worse than the best individual system.",
        "One thing we noticed is that for these translation directions, the translations from one provided single system (online-B) were much better in terms of BLEU than those ofall other systems (in the former case by more than 14% relative in development).",
        "In our experience, MBRSC requires \"comparably good\" systems to be able to achieve significant improvements (particularly if using heuristic scaling factors).",
        "On the other hand, we would have achieved improvements over all remaining systems leaving out online-B.",
        "Regarding the ht – en featured translation task, MBRSC is not able to improve the results of the best individual system in any case.",
        "As in the en – cz and en – fr translation directions, one of the systems (bm-i2r) perform much much better than all other systems.",
        "We can notice the surprisingly low score of one of the systems (umd-hu) in the clean task.",
        "The translations of this system are all equal (\"N / A\") so we suppose that some error occurred during the translation or submission processes.",
        "Table 2: BLEU scores (case-sensitive) on the featured translation task development corpora of the best and worst single systems and MBRSC.",
        "Best translation results are in bold."
      ]
    },
    {
      "heading": "5. summary",
      "text": [
        "The UPV-PRHLT submissions for WMT 2011 system combination task were described in this paper.",
        "The combination was based on a multi-system MBR technique that uses the MBR decision rule and a linear combination of the component systems' probability distributions to search for the minimum risk translation among all the finite-length strings in the output vocabulary.",
        "We introduced expected BLEU, an approximation to the BLEU score that allows to efficiently apply MBR in these conditions.",
        "In most of the translation directions we were able to obtain BLEU gains over the best individual systems.",
        "cz – en en – cz",
        "de – en en – de",
        "es – en en – es",
        "fr – en en – fr",
        "#systems",
        "12 14",
        "25 34",
        "15 22",
        "23 21",
        "Worst 1 Best MBRSC",
        "15.6 8.8 25.9 16.9",
        "26.7 15.9",
        "12.8 4.5 22.2 16.3 22.2 17.1",
        "15.1 20.3 27.8 32.7 30.5 33.3",
        "15.8 13.9 28.6 35.5 30.2 34.7",
        "^ Worst $ Best MBRSC",
        "13.3 9.1 27.2 18.6 27.9 17.7",
        "12.9 5.1 21.9 16.7 22.1 16.5",
        "14.7 20.7 27.4 32.5 30.4 32.9",
        "16.1 13.0 28.1 33.5 29.6 32.7",
        "ht-",
        "en",
        "raw",
        "clean",
        "#systems",
        "8",
        "16",
        "worst",
        "15.4",
        "2.9",
        "best",
        "29.6",
        "33.1",
        "MBRSC",
        "28.6",
        "32.2"
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This paper is based upon work supported by the EC (FEDER/FSE) and the Spanish MEC/MICINN under the MIPRCV \"Consolider Ingenio 2010\" program (CSD2007-00018), the iTrans2 (TIN2009-14511) project and the UPV under grant 20091027.",
        "Also supported by the Spanish MITyC under the erudito.com (TSI-020110-2009-439) project and by the Generalitat Valenciana under grant Prome-teo/2009/014."
      ]
    }
  ]
}
