{
  "info": {
    "authors": [
      "Kevin Gimpel",
      "Noah A. Smith"
    ],
    "book": "Proceedings of the Sixth Workshop on Statistical Machine Translation",
    "id": "acl-W11-2165",
    "title": "Generative Models of Monolingual and Bilingual Gappy Patterns",
    "url": "https://aclweb.org/anthology/W11-2165",
    "year": 2011
  },
  "references": [
    "acl-D07-1080",
    "acl-D08-1024",
    "acl-H05-1095",
    "acl-J03-1002",
    "acl-J07-2003",
    "acl-J92-4003",
    "acl-N03-1017",
    "acl-N10-1140",
    "acl-P03-1021",
    "acl-P05-1033",
    "acl-P06-1096",
    "acl-P07-1019",
    "acl-P07-2045",
    "acl-P11-1129",
    "acl-P11-1131",
    "acl-W02-1021",
    "acl-W08-0336",
    "acl-W10-1703"
  ],
  "sections": [
    {
      "text": [
        "Kevin Gimpel Noah A. Smith",
        "Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA",
        "A growing body of machine translation research aims to exploit lexical patterns (e.g., n-grams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011).",
        "Typically, these \"gappy patterns\" are discovered using heuristics based on word alignments or local statistics such as mutual information.",
        "In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps.",
        "We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus.",
        "We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Beginning with the success of phrase-based translation models (Koehn et al., 2003), a trend arose of modeling larger and increasingly complex structural units in translation.",
        "One thread of work has focused on the use of lexical patterns with gaps.",
        "Simard et al.",
        "(2005) proposed using phrase pairs with gaps in a phrase-based translation model, providing a heuristic method to extract gappy phrase pairs from word-aligned parallel corpora.",
        "The widely-used hierarchical phrase-based translation framework was introduced by Chiang (2005) and also relies on a simple heuristic for phrase pair extraction.",
        "On the monolingual side, researchers have taken inspiration from trigger-based language modeling for speech recognition (Rosenfeld, 1996).",
        "Recently Xiong et al.",
        "(2011) used monolingual trigger pairs to improve handling of long-distance dependencies in machine translation output.",
        "All of this previous work used heuristics or local statistical tests to extract patterns from corpora.",
        "In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps.",
        "We exploit non-parametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text.",
        "We first inspect these patterns manually and discuss the categories of phenomena that they capture.",
        "We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007).",
        "We present experiments for Spanish-English and Chinese-English translation, reporting encouraging preliminary results."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "There is a rich history of trigger-based language modeling in the speech recognition community, typically involving the use of statistical tests to discover useful trigger-word pairs (Rosenfeld, 1996; Jelinek, 1997).",
        "Xiong et al.",
        "(2011) used Rosenfeld's mutual information procedure to discover trigger pairs and added a single feature to a phrase-based MT system that scores new words based on all potential triggers from previous parts of the derivation.",
        "We are not aware of prior work that uses generative modeling and Bayesian nonparametrics to discover these same types of patterns automatically; doing so allows us to discover larger patterns with more words and gaps if they are warranted by the data.",
        "In addition to the gappy phrase-based (Simard et al., 2005) and hierarchical phrase-based (Chiang, 2005) models mentioned earlier, other researchers have explored the use of bilingual gappy structures for machine translation.",
        "Crego and Yvon (2009) and",
        "nato must either say \" yes \" or \" no \" to the baltic states .",
        "%((*) = must %(%) = \" __ \" __ \" __ \" = baltic states = either_or 7i(#) = yes __ no = .",
        "Figure 1: A sentence from the news commentary corpus, along with color assignments for the words and the n function for each color.",
        "Galley and Manning (2010) proposed ways of incorporating phrase pairs with gaps into standard left-to-right decohing algorithms familiar to phra^-based and N-gram-based MT; both used heuristics to extract phrase paire.",
        "BansUlee al.",
        "\"2011) presented a model and training procedure for word alignment that uses phrase pairs with gaps.",
        "They use a semi-Markov model with an enlarged dynamic programming state in order to represent alignment between gappy phrases.",
        "Their model permits up to one gap per phrase while our models permit an arbitrary number."
      ]
    },
    {
      "heading": "3. Monolingual Pattern Models",
      "text": [
        "We first present a model that generates a sentence as a set of lexical items that we will refer to as gappy patterns, or simply patterns.",
        "A pattern is defined as a sequence containing elements of two types: words and gaps.",
        "All patterns must obey the regular expression w+ (_w+)*, where w is a word and _ is a gap.",
        "That is, patterns must begin and end with words and may not contain consecutive gaps.",
        "We assume that we have an n-word sentence wi:„.",
        "We represent patterns in a sentence by associating each word with a color.",
        "To do so, we introduce a vector of color assignment variables c1:n, with one for each word.",
        "We represent a color Cj as a set in terms of the ci variables: Cj = {i : ci = j}.",
        "Each color corresponds to a pattern that is obtained by concatenating its words from left to right in the sentence, inserting gaps when necessary.",
        "We denote the pattern for a color Cj by n(Cj); Figure 1 shows examples of the correspondence between colors and patterns.",
        "The generative story for a single sentence follows:"
      ]
    },
    {
      "heading": "1.. Sample the number of words: n ~ Poisson(ß)",
      "text": [
        "2.",
        "Sample the number of unique colors in the sentence given n: m ~ Uniform(1, n)",
        "3.",
        "For each word index i = 1... n, sample the color of word i: ci ~ Uniform(1, m).",
        "If any of the m colors has no words, repeat this step.",
        "4.",
        "For each color j = 1... m, sample from a multinomial distribution over patterns: wCj ~ Mult(ß).",
        "If the words wCj are not consistent with the color assignments, i.e., wrong number of words or gaps, gaps not in the correct locations, repeat this step.",
        "Thus, the probability of generating number of words n, words w1:n, color assignments c1:n, and number of colors m is where Z is a normalization constant required by the potential repetition of sampling in the final two steps of the generative story.",
        "Without Z, the model would be deficient as we would waste probability mass on internally inconsistent color assignments.",
        "The core of the model is a single multinomial distribution ) over patterns.",
        "We use a Dirich-let process (DP) prior for this multinomial so that we can model an unbounded set of patterns: ß ~ DP(a, Po), where a is the concentration parameter and P0 is the base distribution.",
        "The base distribution includes a Poisson(v) over the number of words in the pattern, a uniform distribution (over word types in the vocabulary) for each word, a uniform distribution over the number of gaps given the number of words, and a uniform distribution over the arrangement of gaps given the numbers of gaps and words.",
        "Inference We use collapsed Gibbs sampling for inference.",
        "Our goal is to obtain samples from the posterior distribution p({c(i),m(i)}S=1 | {w(i)}S=1, v, a), where S is the total number of sentences in the corpus and ß is marginalized out.",
        "nato must either say \" yes \" or \" no \" to the baltic states .",
        "During each iteration of Gibbs sampling, we proceed through the corpus and sample a new value for each ci variable conditioned on the values of all others in the corpus.",
        "The m variables are determined by the ci variables and therefore do not need to be sampled directly.",
        "When sampling ci, we first remove ci from the corpus (and its color if the color only contained i).",
        "Where the remaining colors in the sentence are numbered from 1 to m, there are m + 1 possibilities for ci : m for each of the existing colors and one for choosing a new color.",
        "Since choosing a new color corresponds to creating a new instance of the pattern the probability of choosing a new color m + 1 is proportional where #n is the count of pattern n in the rest of the sentence and all other sentences in the corpus, and # is the total count of all patterns in this same set.",
        "The probability of choosing the existing color j (for 1 < j < m) is proportional to where the denominator encodes the fact that the move will cause an instance of the pattern for the color Cj to be removed from the corpus as the new pattern for Cj U { i} is added.",
        "We note that, even though these two types of moves will result in different numbers of colors (m) in the sentence, we do not have to include a term for this in the sampler because we use a uniform distribution for m and therefore all (valid) numbers of colors have the same probability.",
        "The normalization constant Z in Equation 1 does not affect inference because our sampler is designed to only consider valid (i.e., internally consistent) settings for the c(i)and m(i) variables.",
        "This model makes few assumptions, using uniform distributions whenever possible.",
        "This simplifies inference and causes the resulting lexicon to be influenced primarily by the \"rich-get-richer\" effect of the DP prior.",
        "Despite its simplicity, we will show later that this model discovers patterns that capture a variety of linguistic phenomena.",
        "la otan tiene que decir \" si \" o \" no \" a los paises bâlticos .",
        "Figure 2: A Spanish-English sentence pair with the intersection of automatic word alignments in each direction.",
        "Some source word s accept the colors of target words aligned to them while others (light gray) do not.",
        "Bilingual patterns for a few colors are shown."
      ]
    },
    {
      "heading": "4. Bilingual Pattern Models",
      "text": [
        "We now present a generative model for a sentence pair that will enable us to discover bilingual patterns.",
        "In this section we present one example of extending the previous model \"o b e bilingual, but we note that many other extension s are p ossibk; indeed, flexibility is one of the key advantages of working within the framework of probabilistic modeling.",
        "We assume that we are given sentence pairs and one-to-one word alignments.",
        "That is, in addition to an n-word target sentence w1:n, we assume we have an n'-word source sentence w'1:n, and word alignments a1:n/ where ai = jiff w' is aligned to Wj and ai = 0 if wi is aligned to null.",
        "To model bilingual patterns, we distinguish source colors from target colors.",
        "A target-language word can only be colored with a target color, but a source word can be colored with either a source color or with the target color of the target word it is aligned to (if any).",
        "We have m target colors as before and now add m' source colors.",
        "We introduce additional random variables in the form of a binary vector g of length n' that indicates, for each source word, whether or not it accepts the color of its aligned target word.",
        "We introduce an additional parameter 7 for the probability that a source word will accept the color of its aligned word.",
        "We fix its value to 0.5 and do not learn it during inference.",
        "Figure 2 shows an example Spanish-English sentence pair with automatic word alignments and color assignments.",
        "The bilingual patterns for a few target colors are shown.",
        "The generative story for a sentence pair follows:",
        "1.",
        "Sample the numbers of words in the source and target sentences: n', n ~ Poisson(ß)",
        "3.",
        "Sample the alignment vector from any distribution that ensures links are 1-to-1: a1:n, ~ p(a)",
        "4.",
        "For each target word index i = 1 .",
        ".",
        ".",
        "n, sample the color of target word i from a uniform distribution over all target colors: ci ~ Uniform(1, m).",
        "While any of the m colors has no words, repeat this step.",
        "1.",
        "Decide whether to use a source color or to use the target color of the aligned target word: gi ~ 6.",
        "If any source color has no words, repeat Step 5.",
        "1.",
        "Sample from a multinomial over bilingual patterns: wCj ~ Mult(ß).",
        "While the words wCjare not consistent with the color assignments, repeat this step.",
        "The distribution pY(gi | ai) is defined below:",
        "where 7 determines how frequently source tokens will be added to target patterns.",
        "The probability of generating target words w1:n, source words w'1:n,, alignments a1:n,, target color assignments ci:n, source color assignments c'i:n,, color propagation variables gi:n,, number of target colors m, and number of source colors m' is where Z again serves as a normalization constant to prevent the model from leaking probability mass on internally inconsistent configurations.",
        "There are now two multinomial distributions over patterns with parameter vectors ß and ß'.",
        "They both use DP priors with identical concentration parameters a and differing base distributions P0 and P0.",
        "The base distribution for source patterns, P0, takes the same form as the base distribution for the model described in §3.",
        "For target patterns with aligned source words, P0 generates the target part of the pattern like the base distribution in §3 and then generates the number of aligned source words to each target word with a Poisson(1) distribution; the number of aligned source words can only be 0 or 1 when all word links are 1-to-1.",
        "If it is 1, the base distribution generates the aligned source word by sampling uniformly from among all source types.",
        "While there are connections between this model and work on performing translation using phrase pairs with gaps, the patterns we discover are not guaranteed to be bilingual translation units.",
        "Rather, they typically contain additional target-side words that have no explicit correlate on the source side.",
        "They can be used to assist an existing translation model by helping to choose the best phrase translation for each source phrase.",
        "To define a generative model for phrase pairs with gaps, changes would have to be made to the bilingual model we presented.",
        "Inference As before, we use collapsed Gibbs sampling for inference.",
        "Our goal is to obtain samples from the posterior p({(c, c',g, m, m')(i)}|=1 | {(w, w', a)(i)}f=i).",
        "We go through each sentence pair and sample new color assignment variables for each word.",
        "For an aligned word pair (wi, Wj ), we sample a new value for the tuple (gi,ci,cj).",
        "The possible values for cj include all target colors, including a new target color.",
        "The possible values for gi are 0, in which case c'i can be any of the source colors, including a new source color, and 1, for which c'i must be cj.",
        "For an unaligned target word Wj, Cj can be any target color, including a new one, and for an unaligned source word wi, ci can be any source color, including a new one.",
        "The full equations for sampling can be easily derived using the equations from §3."
      ]
    },
    {
      "heading": "5. Evaluation",
      "text": [
        "We conducted evaluation to determine (1) what types of phenomena are captured by the most probable patterns discovered by our models, and (2) whether including the patterns as features can improve translation quality.",
        "Since inference is computationally expensive, we used the 126K-sentence English news commentary corpus provided for the WMT shared tasks (Callison-Burch et al., 2010).",
        "We ran Gibbs sampling for 600 iterations through the data, discarding the first 300 samples for burn-in and computing statistics of the patterns using the remaining 300 samples.",
        "Each iteration took approximately 3 minutes on a single 2.2GHz CPU.",
        "When looking primarily at the most frequent patterns, we found that this list did not vary much when only using half of the data instead.",
        "We set v = 3 and a = 100; we found these hyperparameters to have only minor effects on the results.",
        "Since many frequent patterns include the period (.",
        "), we found it useful to constrain the model to treat this token differently: we modify the base distribution so that it assigns zero probability to patterns that contain a period along with other words and we force each occurrence of a period to be alone in its own pattern during initialization.",
        "We do not need to change the inference procedure at all; with the modified base distribution and with no patterns including a period with other words, the probability of creat-",
        "Table 1: Top-ranked gappy patterns from samples according to patterns without gaps are omitted.",
        "The special string \"_\" represents a gap that can be filled by any nonempty sequence of words.",
        "ing a new illegal pattern during inference is always zero (Eq.",
        "3).",
        "We also perform inference on a transformed version of the corpus in which every word is replaced with its hard word class obtained from Brown clustering (Brown et al., 1992).",
        "One property of Brown clusters is that each function word effectively receives its own class, as each ends up in a cluster in which it occupies >95% of the token counts of all types in the cluster.",
        "We call clusters that satisfy this property singleton clusters.",
        "To obtain Brown clusters for the source and target languages, we used code from Liang (2005).We used the data from the news commentary corpus along with the first 500K sentences of the additional monolingual newswire data also provided for the WMT shared tasks.",
        "We used 300 clusters, ignoring words that appeared only once in this corpus.",
        "We did not use the hierarchical information from the clusters but merely converted each cluster name into a unique integer, using one additional integer for unknown words.",
        "We used the same values for v and a as above but ran Gibbs sampling for 1,300 iterations, again using the last 300 for collecting statistics on patterns.",
        "Judging by the number of color assignments changed on each iteration, the sampler takes longer to converge when run on word clusters than on words.",
        "As above, we constrain the singleton word cluster corresponding to the period to be alone during both initialization and inference.",
        "\" _ \"",
        "as_as",
        "\"",
        "- -",
        "the_of_in",
        "why_?",
        "(_)",
        "the_is",
        ", _ the _ of",
        "the _ of",
        "not only_but",
        "from_to",
        ", , ,",
        "it is that",
        "the_between_and",
        "the_(_)",
        "of _ \" _ \"",
        "such as_,",
        "both_and",
        "not_, but",
        "either_or",
        "the_of_and",
        "in_,_in",
        "but_is",
        "more_than",
        "the_of _,",
        "\" _ \" _ the",
        "what _ ?",
        "has_been",
        ", _ \" _ \"",
        "between_and",
        "in_,_,",
        "the _ \" _ \"",
        "the_of_'s",
        "an_of",
        "Table 3: Top-ranked gappy patterns according to Pattern Ranking Statistics Several choices exist for ranking patterns.",
        "The simplest is to take the pattern count from the posterior samples, averaged over all sampling iterations after burn-in.",
        "We refer to this criterion as the marginal probability:",
        "where #n is the average count of the pattern across the posterior samples and # is the count of all patterns.",
        "The top-ranked gappy patterns under this criterion are shown in Table 1.",
        "While many of these patterns match our intuitions, there are also several that are highly-ranked simply because their constituent words are frequent.",
        "Alternatively, we can rank patterns by the conditional probability of the pattern given the words that comprise it:",
        "where w(n) returns the sequence of words in the pattern n and is the number of occurrences of this sequence of words in the corpus that are compatible with pattern n. The ranking of patterns under this criterion is shown in Table 2.",
        "This method favors precision but also causes very rare patterns to be highly ranked.",
        "To address this, we also consider a product-of-experts model by simply multiplying together the two probabilities, resulting in the ranking shown in Table 3.",
        "This ranking is similar to that in Table 1 but penalizes patterns that are only ranked highly because they consist of common words.",
        "Table 4 shows a manual grouping of these highly-ranked patterns into several categories.",
        "We show both lexical and Brown cluster patterns.",
        "It is common in both types of patterns to find long-distance dependencies involving punctuation near the top of the ranking.",
        "Among agreement patterns, the lexical model finds relationships between pronouns and their associated possessive adjectives while the cluster model finds more general patterns involving classes of nouns.",
        "Cluster patterns are more likely to capture topicality within a sentence, while the finer granularity of the lexical model is required to identify constructions like those shown (verbs triggering particular prepositions).",
        "There are also many probable patterns without gaps, shown at the bottom of Table 4.",
        "From these patterns we can see that our models can also be used to find collocations, but we note that these are discovered in the context of the gappy patterns.",
        "That is, due to the use of latent variables in our models (the color assignments), there is a natural trading-off effect whereby the gappy patterns encourage particular non-gappy patterns to be used, and vice versa.",
        "We use the news commentary corpus for each language and take the intersection of GIZA++ (Och and Ney, 2003) word alignments in each direction, thereby ensuring that they are 1-to-1 alignments.",
        "We ran Gibbs sampling for 300 iterations, averaging pattern counts from the last 200.",
        "We set a = 100, A = 3, and 7 = 0.5.",
        "We ran the model in 3 conditions: source words, target words; source clusters, target clusters; and source clusters, target words.",
        "We",
        "academy_sciences",
        "regulators_supervisors",
        "beijing_shanghai",
        "sine_non",
        "booms_busts",
        "stalin_mao",
        "council_advisers",
        "treasury secretary_geithner",
        "dominicans_haitian",
        "sooner_later",
        "flemish_walloons",
        "first_foremost",
        "gref_program",
        "played_role",
        "heat_droughts",
        "down_road",
        "humanitarian_displaced",
        "freedom_expression",
        "karnofsky_hassenfeld",
        "at_disposal",
        "kazakhstan_kyrgyzstan",
        "take_granted",
        "portugal_greece",
        "- -",
        "whether_or",
        "france_germany",
        "around_world",
        "he_his",
        "has_been",
        "allow_to",
        "both_and",
        "how ?",
        "for_first time",
        "not only_but",
        "the_(_)",
        "china_india",
        "\" _ \"",
        "on_basis",
        "what_do",
        "more_than",
        "less_than",
        "we_our",
        "either_or",
        "on_other hand",
        "over_past",
        "why_?",
        "at_level",
        "prevent_from",
        "neither_nor",
        "it is_that",
        "in_way",
        "what _ ?",
        "not_, but",
        "one_another",
        "rule_law",
        "play_role",
        "political_economic",
        "Non-Gappy Lexical Patterns",
        "Non-Gappy Brown Cluster Patterns as {well, soon, quickly, seriously, slowly} as {rather, please} than the united {states, nations, airlines} {don, didn, doesn, isn, wasn} 't as well their own the united states prime minister have been climate change rather than the bush administration based on developing countries",
        "Table 4: Gappy patterns manually divided into categories of long-distance dependencies.",
        "Patterns were ranked according to p(n)p(n|w(n)) and manually selected from the top 300 to exemplify categories.",
        "Lower pane shows top ranked non-gappy patterns.",
        "Clusters are shown as enough words to cover 95% of the token counts of the cluster, up to a maximum of 5.",
        "again ensured that the period and its word class remained isolated in their own patterns for each condition.",
        "We note that no source-side word order information is contained within these bilingual patterns; aligned source words can be in any order in the source sentence and the pattern will still match.",
        "The most probable patterns included many monolingual source-only and target-only patterns that are similar to those shown in Table 4.",
        "There were also many phrase pairs with gaps like those that are commonly extracted by heuristics (Galley and Manning, 2010).",
        "Additionally we noted examples of source words triggering more target-side information than merely one word.",
        "There were several examples of patterns that encouraged inclusion of the subject in English when translating from Spanish, as Spanish often drops the subject when it is clear from context, e.g., \"we are(estamos)\".",
        "Also, one probable pattern for German-English was \"the _ of the(des)\" (des is aligned to the final the).",
        "The German determiner des is in the genitive case, so this pattern helps to encourage its object to also be in the genitive case when translated.",
        "Rank",
        "Gappy Lexical Patterns",
        "Rank",
        "Gappy Brown Cluster Patterns",
        "1",
        "-- --",
        "2",
        "{what, why, whom, whatever} {?, !}",
        "2",
        "(_)",
        "6",
        "{--, -,-}_{--,-,-}",
        "6",
        "\" _ \"",
        "28",
        "{according, compared, subscribe, thanks, referring} to_,",
        "9",
        "why_?",
        "178",
        "{-, -, -} {even, especially, particularly, mostly, mainly}_{-, -, -}",
        "63",
        "according to_,",
        "239",
        "{obama, bush, clinton, mccain, brown}_\"_\"",
        "26",
        "he_his",
        "8",
        "{people, things, americans, journalists, europeans}_their",
        "31",
        "we_our",
        "12",
        "we_{our, my}",
        "46",
        "his_his",
        "21",
        "{children, women, others, men, students}_their",
        "86",
        "china_its",
        "23",
        "{china, europe, america, russia, iran} 's_its",
        "90",
        "his_he",
        "43",
        "{obama, bush, clinton, mccain, brown}_his",
        "99",
        "you_your",
        "46",
        "{our, my}_{our, my}",
        "136",
        "leaders_their",
        "149",
        "{people, things, americans, journalists, europeans}_they",
        "140",
        "we_ourselves",
        "172",
        "{president, bill, sen., king, senator} {obama, bush, clinton, mccain, brown}_his",
        "165",
        "these_are",
        "180",
        "{all, both, either}_{countries, companies, banks, groups, issues}",
        "4",
        "both_and",
        "5",
        "{more, less}_{more, less}",
        "5",
        "not only_but",
        "9",
        "if_,_{will, would, could, should, might}",
        "8",
        "either_or",
        "19",
        "{deal, plan, vote, decision, talks} {against, between, involving}_and",
        "10",
        "neither_nor",
        "40",
        "a_{against, between, involving}_and",
        "13",
        "whether_or",
        "45",
        "{better, different, further, higher, lower}_than",
        "19",
        "less_than",
        "50",
        "{much, far, slightly, significantly, substantially}_than",
        "23",
        "not_, but",
        "56",
        "{yet, instead, perhaps, thus, neither}_but",
        "54",
        "if_then",
        "68",
        "not {only, necessarily}_{also, hardly}",
        "109",
        "between_and",
        "98",
        "as {much, far, slightly, significantly, substantially}_as",
        "192",
        "relationship between_and",
        "131",
        "is_{more, less}_than",
        "25",
        "france_germany",
        "1",
        "(UNK)_(UNK)",
        "29",
        "china_india",
        "15",
        "{china, europe, ... }'s_{system, crisis, program, recession, situation}",
        "36",
        "political_economic",
        "30",
        "{health, security, defense, safety, intelligence}_{health,... }",
        "43",
        "rich_poor",
        "47",
        "{china, europe, ... }_{china, europe,... }_{china, europe,... }",
        "50",
        "oil_gas",
        "62",
        "{power, growth, interest, development}_{10, 1, 20, 30, 2} {percent, %, p.m., a.m.}",
        "62",
        "billions_dollars",
        "72",
        "in {iraq, washington, london, 2008, 2009} _ {iraq, washington, london, 2008, 2009}",
        "96",
        "economic_social",
        "73",
        "the {end, cost, head, rules, average} of_{prices, markets, services, problems, costs}",
        "106",
        "the us_europe",
        "113",
        "{china, europe, ... }'s_{economy, election, elections, population, investigation}",
        "181",
        "public_private",
        "119",
        "{prices, markets,... }_{oil, energy, tax, food, investment}_{oil, energy,... }",
        "14",
        "around_world",
        "14",
        "for_{first, second, third, final, whole} {time, period, term, class, avenue}",
        "18",
        "on_basis",
        "17",
        "in_{last, next, 20th} {year, week, month, season, summer}",
        "38",
        "at_time",
        "51",
        "at_{end, cost, head, rules, average} of",
        "42",
        "in_region",
        "71",
        "at_{group, rate, leader, level, manager}",
        "80",
        "in_manner",
        "112",
        "for_{times, points, games, goals, reasons}",
        "85",
        "at_expense",
        "126",
        "{over, around, across, behind, above}_{country, company, region, nation, virus}",
        "112",
        "during_period",
        "190",
        "{one, none} of_{best, top, largest, main, biggest}",
        "33",
        "prevent_from",
        "84",
        "enable_to",
        "114",
        "provide_for",
        "123",
        "impose_on",
        "177",
        "turn_into",
        "We consider the Spanish-to-English (ES^EN) translation task from the ACL-2010 Workshop on Statistical Machine Translation (Callison-Burch et al., 2010).",
        "We trained a Moses system (Koehn et al., 2007) following the baseline training instructions for the shared task.",
        "In particular, we performed word alignment in each direction using GIZA++ (Och and Ney, 2003), used the \"grow-diag-final-and\" heuristic for symmetrization, and extracted phrase pairs up to a maximum length of seven.",
        "After filtering sentence pairs with one sentence longer than 50 words, we ended up with 1.45M sentence pairs of Europarl data and 91K sentence pairs of news commentary data.",
        "Language models (N = 5) were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).",
        "Language models were trained on the target side of the parallel corpus as well as the first 5 million additional sentences from the extra English monolingual newswire data provided for the shared tasks.",
        "We used news-test2008 for tuning and news-test2 00 9 for testing.",
        "We also consider Chinese-English (ZH^EN) and followed a similar training procedure as above.",
        "We used 303K sentence pairs from the FBIS corpus (LDC2003E14) and segmented the Chinese data using the Stanford Chinese segmenter in \"CTB\" mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words.",
        "A trigram language model was estimated using modified Kneser-Ney smoothing from the English side of the parallel corpus concatenated with 200M words of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times).",
        "We used",
        "NIST MT03 for tuning and NIST MT05 for testing.",
        "For evaluation, we used case-insensitive IBM",
        "BLEU (Papineni et al., 2001).",
        "Unlike n-gram language models, our models have latent structure (the color assignments), making it difficult to compute the probability of a translation during decoding.",
        "We leave this problem for future work and instead simply add a feature for each of the most probable patterns discovered by our models.",
        "Each feature counts the number of occurrences of its pattern in the translation.",
        "We wish to add thousands of features to our model, but the standard training algorithm - minimum error rate training (MERT; Och, 2003) - cannot handle large numbers of features.",
        "So, we leverage recent work on feature-rich training for MT using online discriminative learning algorithms.",
        "Our training procedure is shown as Algorithm 1.",
        "We find it convenient to notationally distinguish feature weights for the standard Moses features (A) from weights for our pattern features We use h(e) to denote the feature vector for translation 6.",
        "The function Bi (t) returns the sentence BLEU score for translation t given reference ei (i.e., treating the sentence pair as a corpus).",
        "MERT is run to convergence on the tuning set to obtain weights for the standard Moses features (line 1).",
        "Phrase lattices (Ueffing et al., 2002) are generated for all source sentences in the tuning set using the trained weights AM (line 2).",
        "The lattices are used within a modified version of the margin-infused relaxed algorithm (MIRA; Crammer et al., 2006) for structured max-margin learning (lines 515).",
        "A k-best list is extracted from the current lattice (line 7), then the translations on the k-best list with the highest and lowest sentence-level BLEU scores are found (lines 8 and 9).",
        "The step size is then computed using the standard MIRA formula (lines 1011) and the update is made (line 12).",
        "The returned weights are averaged over all updates.",
        "This training procedure is inspired by several",
        "Input: input sentences F = {fi}NL1, references E = {eJüi, initial weights Ao, size of k-best list k, MIRA max step size C, num.",
        "iterations T Output: learned weights: AM, (A*, 6*) 2 – generateLattices (F, AM); 5 for iter – 1 to T do 6 for i – 1 to N do Algorithm 1: Train others that have been shown to be effective for",
        "Watanabe et al., 2007; Chiang et al., 2008).",
        "Though not shown in the algorithm, in practice we store the BLEU-best translation on each k-best list from all previous iterations and use it as e+ if it has a higher BLEU score than any on the k-best list on the current iteration.",
        "At decoding time, we follow a procedure similar to training: we generate lattices for each source sentence using Moses with its standard set of features and using weights AM.",
        "We rescore the lattices using A* and use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to incorporate the gappy pattern features with weights 6*.",
        "Cube pruning is necessary because the pattern features may match anywhere in the translation; thus they are non-local in the phrase lattice and require approximate inference.",
        "Before adding pattern features, we evaluate our training algorithm by comparing it to MERT using the same standard Moses features.",
        "As the initial weights Ao, we used the default Moses feature",
        "T = 15.",
        "For the n-best list size used during cube pruning during both training and decoding, we used n = 100.",
        "There are several Moses parameters that affect the scope of the search during decoding and therefore the size of the phrase lattices.",
        "We used default values for these except for the stack size parameter, for which we used 100.",
        "The resulting lattices encode up to 10 derivations for ES^EN and 10 derivations for ZH^EN.",
        "Table 5 shows test set %BLEU for each language pair and training algorithm.",
        "Our procedure performs comparably to MERT.",
        "Therefore we use it as our baseline for subsequent experiments since it can handle a large number of feature weights; this allows us to observe the contribution of the additional gappy pattern features more clearly.",
        "We chose monolingual and bilingual pattern features using the posterior samples obtained via the inference procedures described above.",
        "We ranked patterns using the product-of-experts formula, removed patterns consisting of only a single token, and added the top 10K patterns from the lexical model and the top 15K patterns from the Brown cluster model.",
        "For simplicity of implementation, we skipped over patterns with 3 or more gaps and patterns with 2 gaps and more than 3 total words; this procedure skipped fewer than 1% of the top patterns.",
        "For results with bilingual pattern features, we added 15K pattern features (5K word-word, 5K cluster-cluster, and 5K cluster-word).",
        "The first set of results is shown in Table 6.",
        "The first row is the same as in Table 5, the second row adds monolingual pattern features, the third adds bilingual pattern features, and the final row includes both sets.",
        "While gains are modest overall,",
        "ES^EN",
        "ZH^EN",
        "MERT",
        "25.64",
        "32.47",
        "Alg.",
        "1",
        "25.85",
        "32.33",
        "said that_the of_million however,_the ,_likely agence france_presse reported_the said that_and rate_percent",
        "%BLEU.",
        "Table 7: Comparing ways of ranking patterns from posterior samples.",
        "Scores are on MT05 for ZH^EN translation.",
        "the pattern features show an encouraging improvement of 0.48 BLEU for ZH^EN.",
        "This is similar to the improvement reported by Xiong et al.",
        "(2011) (+0.4 BLEU when adding their trigger pair language model).",
        "While bilingual patterns give an improvement of 0.35 BLEU, using both monolingual and bilingual features in the same model does not provide additional improvement over monolingual features alone.",
        "For ES^EN, the pattern features have only small effects on BLEU; we suspect that the decreased BLEU score for the full feature set is due to over-fitting.",
        "It is unclear why the results differ for the two language pairs.",
        "one possibility is the use of only a single reference translation when tuning and testing with ES^EN while four references were used for ZH^EN.",
        "Another possibility is that our pattern features are correcting some of the mid-to longrange reorderings that are known to be problematic for phrase-based modeling of ZH^EN translation.",
        "ES^EN exhibits less long-range reordering and therefore may not benefit as much from our patterns.",
        "Table 7 shows additional ZH^EN results when varying the method of ranking patterns.",
        "When using both sets of features, the \"Ranking\" column contains the criterion for ranking monolingual patterns; bilingual patterns are always ranked using",
        "the_{media, school, university, election, bank}_ {made, established, given, taken, reached} {said, stressed, stated, indicated, noted} that_in {meeting, report, conference, reports}_{1, july, june, march, april} {news, press, spokesman, reporter}_{1, july, june, march, april} the_{enterprises, companies, students, customers, others}_",
        "Table 8: Selected features from the 15 most highly-weighted lexical and cluster pattern features in the best",
        "ZH EN model.",
        "The results show that ranking monolingual patterns using the product-of-experts method results in the highest BLEU scores, validating our intuitions from observing Tables 1-3.",
        "Table 8 shows the most highly-weighted pattern features for the best ZH^EN model."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "We have presented generative models for monolingual and bilingual gappy patterns.",
        "A qualitative analysis shows that the models discover patterns that match our intuitions in capturing linguistic phenomena.",
        "our experimental results show promise for the ability of these patterns to improve translation for certain language pairs.",
        "A key advantage of generative models is the ability to rapidly develop and experiment with variations, especially when using Gibbs sampling for inference.",
        "In order to encourage modifications and extensions to these models we have made our source code available at www.ark.cs.cmu.edu/MT."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors thank Chris Dyer, Qin Gao, Alon Lavie, Nathan Schneider, Stephan Vogel, and the anonymous reviewers for helpful comments.",
        "This research was supported in part by the NSF through grant IIS-0844507, the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533, and Sandia National Laboratories (fellowship to K. Gimpel).",
        "ES^EN",
        "ZH^EN",
        "Baseline",
        "25.85",
        "32.33",
        "Monopats",
        "25.84",
        "32.81",
        "biPats",
        "25.92",
        "32.68",
        "Monopats + biPats",
        "25.59",
        "32.80",
        "Ranking",
        "%BLEU",
        "Baseline",
        "N/A",
        "32.33",
        "Monopats",
        "32.65",
        "Monopats",
        "32.53",
        "Monopats",
        "32.81",
        "BI Pats",
        "32.68",
        "Monopats + biPats",
        "32.78",
        "Monopats + biPats",
        "32.80"
      ]
    }
  ]
}
