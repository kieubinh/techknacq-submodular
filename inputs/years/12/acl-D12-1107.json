{
  "info": {
    "authors": [
      "Kenneth Heafield",
      "Philipp Koehn",
      "Alon Lavie"
    ],
    "book": "EMNLP",
    "id": "acl-D12-1107",
    "title": "Language Model Rest Costs and Space-Efficient Storage",
    "url": "https://aclweb.org/anthology/D12-1107",
    "year": 2012
  },
  "references": [
    "acl-D07-1090",
    "acl-D10-1026",
    "acl-J07-2003",
    "acl-N04-1033",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P07-1019",
    "acl-P07-1065",
    "acl-P07-2045",
    "acl-P10-4002",
    "acl-W06-3113",
    "acl-W08-0402",
    "acl-W09-0424",
    "acl-W11-2103",
    "acl-W11-2123"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments.",
        "We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant.",
        "Common practice uses lower-order entries in an N gram model to score the first few words of a fragment; this violates assumptions made by common smoothing strategies, including Kneser-Ney.",
        "Instead, we use a unigram model to score the first word, a bigram for the second, etc.",
        "This improves search at the expense of memory.",
        "Conversely, we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores, at the expense of less accurate estimates for sentence fragments.",
        "These changes can be stacked, achieving better estimates with unchanged memory usage.",
        "In order to interpret changes in search accuracy, we adjust the pop limit so that accuracy is unchanged and report the change in CPU time.",
        "In a German-English Moses system with target-side syntax, improved estimates yielded a 63% reduction in CPU time; for a Hiero-style version, the reduction is 21%.",
        "The compressed language model uses 26% less RAM while equivalent search quality takes 27% more CPU.",
        "Source code is released as part of KenLM."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Language model storage is typically evaluated in terms of speed, space, and accuracy.",
        "We introduce a fourth dimension, rest cost quality, that captures how well the model scores sentence fragments for purposes of approximate search.",
        "Rest cost quality is distinct from accuracy in the sense that the score of a complete sentence is held constant.",
        "We first show how to improve rest cost quality over standard practice by using additional space.",
        "Then, conversely, we show how to compress the language model by making a pessimistic rest cost assumption1.",
        "Language models are designed to assign probability to sentences.",
        "However, approximate search algorithms use estimates for sentence fragments.",
        "If the language model has order N (an N gram model), then the first N ?",
        "1 words of the fragment have incomplete context and the last N ?",
        "1 words have not been completely used as context.",
        "Our baseline is common practice (Koehn et al. 2007; Dyer et al. 2010; Li et al. 2009) that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words.",
        "Formally, the baseline estimate for sentence",
        "where each wn is a word and pN is an N gram language model.",
        "The problem with the baseline estimate lies in lower order entries pN (wn|w",
        "including the modified version (Chen and Goodman, 1998), assumes that a lower-order entry will only be used because a longer match could not be found2.",
        "Formally, these entries actually eval",
        "of scoring sentence fragments, additional context is simply indeterminate, and the assumption may not hold.",
        "As an example, we built 5-gram and unigram language models with Kneser-Ney smoothing on the same data.",
        "Sentence fragments frequently begin with ?the?.",
        "Using a lower-order entry from the 5 gram model, log10 p5(the) = ?2.49417.",
        "The unigram model does not condition on backing off, assigning log10 p1(the) = ?1.28504.",
        "Intuitively, the 5-gram model is surprised, by more than an order of magnitude, to see ?the?",
        "without matching words that precede it.",
        "To remedy the situation, we train N language models on the same data.",
        "Each model pn is an n-gram model (it has order n).",
        "We then use pn to score the nth word of a sentence fragment.",
        "Thus, a unigram model scores the first word of a sentence fragment, a bigram model scores the second word, and so on until either the n-gram is not present in the model or the first N?1 words have been scored.",
        "Storing probabilities from these models requires one additional value per n-gram in the model, except for N grams where this probability is already stored.",
        "Conversely, we can lower memory consumption relative to the baseline at the expense of poorer rest costs.",
        "Baseline models store two entries per n-gram: probability and backoff.",
        "We will show that the probability and backoff values in a language model can be collapsed into a single value for each n-gram without changing sentence probability.",
        "This transformation saves memory by halving the number of values stored per entry, but it makes rest cost estimates worse.",
        "Specifically, the rest cost pessimistically assumes that the model will back off to uni-grams immediately following the sentence fragment.",
        "The two modifications can be used independently or simultaneously.",
        "To measure the impact of their different rest costs, we experiment with cube pruning (Chiang, 2007) in syntactic machine transla2Other smoothing techniques, including Witten-Bell (Witten and Bell, 1991), do not make this assumption.",
        "tion.",
        "Cube pruning's goal is to find high-scoring sentence fragments for the root non-terminal in the parse tree.",
        "It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal.",
        "Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit.",
        "Increasing the pop limit therefore makes search more accurate but costs more time.",
        "By moderating the pop limit, improved accuracy can be interpreted as a reduction in CPU time and vice-versa."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007).",
        "Most relevant is their use of a class-based language model for the first of two decoding passes.",
        "This first pass is cheaper because translation alternatives are likely to fall into the same class.",
        "Entries are scored with the maximum probability over class members (thereby making them no longer normalized).",
        "Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options.",
        "The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal.",
        "Zens and Ney (2008) present rest costs for phrase-based translation.",
        "These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future.",
        "Our rest costs examine words internal to the sentence fragment, namely the first and last few words.",
        "We also differ by focusing on syntactic translation.",
        "A wide variety of work has been done on language model compression.",
        "While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures.",
        "Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach.",
        "In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost",
        "purposes.",
        "Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N grams which all have backoff 1).",
        "This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al. 2007).",
        "Whether to use one smoothing technique or the other then becomes largely an issue of training costs and quality after quantization."
      ]
    },
    {
      "heading": "3 Contribution",
      "text": []
    },
    {
      "heading": "3.1 Better Rest Costs",
      "text": [
        "As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N gram language model.",
        "However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off.",
        "Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has:",
        "the training data3.",
        "This adjustment is also performed for modified Kneser-Ney smoothing.",
        "The intuition is based on the fact that the language model will base its probability on the longest possible match.",
        "If an N gram was seen in the training data, the model will match it fully and use the smoothed count.",
        "Otherwise, the full N gram was not seen in the training data and the model resorts to a shorter n-gram match.",
        "Probability of this shorter match is based on how often the n-gram is seen in different contexts.",
        "Thus, these shorter n-gram probabilities are not representative of cases where context is short simply because additional context is unknown at the time of scoring.",
        "In some cases, we are able to determine that the model will back off and therefore the lower-order probability makes the appropriate assumption.",
        "Specifically, if vwn1 does not appear in the model for any word v, then computing p(wn|vw",
        "rion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al. 2011).",
        "Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost.",
        "Specifically, there are N language models, one of each order from 1 to N .",
        "The models are trained on the same corpus with the same smoothing parameters to the extent that they apply.",
        "We then compile these into one data structure where each n-gram record has three values:",
        "1.",
        "Probability pn from the n-gram language model 2.",
        "Probability pN from the N gram language model 3.",
        "Backoff b from the N gram language model",
        "For N -grams, the two probabilities are the same and backoff is always 1, so only one value is stored.",
        "Without pruning, the n-gram model contains the same n-grams as the N gram model.",
        "With pruning, the two sets may be different, so we query the n-gram model in the normal way to score every n-gram in the N gram model.",
        "The idea is that pn is the average conditional probability that will be encountered once additional context becomes known.",
        "We also tried more complicated estimates by additionally interpolating upper bound, lower bound, and pN with weights trained on cube pruning logs; none of these improved results in any meaningful way.",
        "Formalizing the above, let wk1 be a sentence fragment.",
        "Choose the largest s so that vws1 appears in the model for some v; equivalently ws1 is the left state described in Li and Khudanpur (2008).",
        "The 4Usually, this happens because wn1 does not appear, though it can also happen that wn1 appears but all vw",
        "while our improved estimate is",
        "The difference between these equations is that pn is used for words in the left state i.e. 1 ?",
        "n ?",
        "s. We have also abused notation by using pN to denote both probabilities stored explicitly in the model and the model's backoff-smoothed probabilities when not present.",
        "It is not necessary to store backoffs for pn because s was chosen such that all queried n-grams appear in the model.",
        "This modification to the language model improves rest costs (and therefore quality or CPU time) at the expense of using more memory to store pn.",
        "In the next section, we do the opposite: make rest costs worse to reduce storage size."
      ]
    },
    {
      "heading": "3.2 Less Memory",
      "text": [
        "Many language model smoothing strategies, including modified Kneser-Ney smoothing, use the backoff algorithm shown in Figure 1.",
        "Given an n-gram wn1 , the backoff algorithm bases probability on as much context as possible.",
        "Equivalently, it finds the minimum f so that wnf is in the model then uses p(wn|w",
        "f ) as a basis.",
        "Backoff penalties b are charged because a longer match was not found, forming the product",
        "dependent of wn, though which backoff penalties are charged depends on f and therefore wn.",
        "backoff?",
        "1 for f = 1?",
        "n do if wnf is in the model then return p(wn|wn?1f ) ?",
        "backoff else if wn?1f is in the model then backoff?",
        "backoff ?",
        "b(wn?1f )",
        "ability because even unknown words are treated as a unigram.",
        "for f = 1?",
        "n do if wnf is in the model then",
        "computed at model building time.",
        "In order to save memory, we propose to account for backoff in a different way, defining q",
        "where again wnf is the longest matching entry in the model.",
        "The idea is that q is a term in the telescoping series that scores a sentence fragment, shown in equation (1) or (2).",
        "The numerator pessimistically charges all backoff penalties, as if the next word wn+1 will only match a unigram.",
        "When wn+1 is scored, the denominator of q(wn+1|wn1 ) cancels out backoff terms that were wrongly charged.",
        "Once these terms are canceled, all that is left is p, the correct backoff penalties, and terms on the edge of the series.",
        "Proposition 1.",
        "The terms of q telescope.",
        "Formally, let wk1 be a sentence fragment and f take the minimum value so that wkf is in the model.",
        "Then,",
        "Proof.",
        "By induction on k. When k = 1, f = 1 since the word w1 is either in the vocabulary or mapped to <unk> and treated like a unigram.",
        "where f has the lowest value such that wkf is in the model.",
        "Applying the inductive hypothesis to expand",
        "where e has the lowest value such that wk?1e is in the model.",
        "The backoff terms cancel to yield",
        "Finally, the conditional probability folds as desired",
        "We note that entries ending in </s> have backoff 1, so it follows from Proposition 1 that sentence-level scores are unchanged.",
        "Proposition 1 characterizes q as a pessimistic rest cost on sentence fragments that scores sentences in exactly the same way as the baseline using p and b.",
        "To save memory, we simply store q in lieu of p and b.",
        "Compared with the baseline, this halves number of values from two to one float per n-gram, except N grams that already have one value.",
        "The impact of this reduction is substantial, as seen in Section 4.3.",
        "Run-time scoring is also simplified as shown in Figure 2 since the language model locates the longest match wnf then returns the value",
        "tion or additional lookup.",
        "Baseline language models either retrieve backoffs values with additional lookups (Stolcke, 2002; Federico et al. 2008) or modify the decoder to annotate sentence fragments with backoff information (Heafield, 2011); we have effectively moved this step to preprocessing.",
        "The disadvantage is that q is not a proper probability and it produces worse rest costs than does the baseline.",
        "Language models are actually applied at two points in syntactic machine translation: scoring lexical items in grammar rules and during cube pruning.",
        "Grammar scoring is an offline and embarrassingly parallel process where memory is not as tight (since the phrase table is streamed) and fewer queries are made, so slow non-lossy compression and even network-based sharding can be used.",
        "We therefore use an ordinary language model for grammar scoring and only apply the compressed model during cube pruning.",
        "Grammar scoring impacts grammar pruning (by selecting only top-scoring grammar rules) and the order in which rules are tried during cube pruning."
      ]
    },
    {
      "heading": "3.3 Combined Scheme",
      "text": [
        "Our two language model modifications can be trivially combined by using lower-order probabilities on the left of a fragment and by charging all backoff penalties on the right of a fragment.",
        "The net result is a language model that uses the same memory as the baseline but has better rest cost estimates."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al. 2007) for the WMT 2011 German-English translation task (Callison-Burch et al. 2011).",
        "Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007).",
        "Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side.",
        "The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year.",
        "Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings.",
        "In all scenarios, the primary language model has order 5.",
        "For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model.",
        "Feature weights were trained with MERT (Och, 2003) on the baseline using a pop limit of 1000 and 100-best output.",
        "Since final feature values are unchanged, we did not rerun MERT in each condition.",
        "Measurements were collected by running the decoder on the 3003-sentence test set."
      ]
    },
    {
      "heading": "4.1 Rest Costs as Prediction",
      "text": [
        "Scoring the first few words of a sentence fragment is a prediction task.",
        "The goal is to predict what the probability will be when more context becomes known.",
        "In order to measure performance on this task, we ran the decoder on the hierarchical system with a pop limit of 1000.",
        "Every time more context became known, we logged5 the prediction error (estimated log probability minus updated log probabil",
        "variance (of the error) for the lower-order rest cost and the baseline.",
        "Error is the estimated log probability minus the final probability.",
        "Statistics were computed separately for the first word of a fragment (n = 1), the second word (n = 2), etc.",
        "The lower-order estimates are better across the board, reducing error in cube pruning.",
        "All numbers are in log base ten, as is standard for ARPA-format language models.",
        "Statistics were only collected for words with incomplete context.",
        "ity) for both lower-order rest costs and the baseline.",
        "Table 1 shows the results.",
        "Cube pruning uses relative scores, so bias matters less, though positive bias will favor rules with more arity.",
        "Variance matters the most because lower variance means cube pruning's relative rankings are more accurate.",
        "Our lower-order rest costs are better across the board in terms of absolute bias, mean squared error, and variance."
      ]
    },
    {
      "heading": "4.2 Pop Limit Trade-Offs",
      "text": [
        "The cube pruning pop limit is a trade-off between search accuracy and CPU time.",
        "Here, we measure how our rest costs improve (or degrade) that trade-off.",
        "Search accuracy is measured by the average model score of single-best translations.",
        "Model scores are scale-invariant and include a large constant factor; higher is better.",
        "We also measure overall performance with uncased BLEU (Papineni et al. 2002).",
        "CPU time is the sum of user and system time used by Moses divided by the number of sentences (3003).",
        "Timing includes time to load, though files were forced into the disk cache in advance.",
        "Our test machine has 64 GB of RAM and 32 cores.",
        "Results are shown in Figures 3 and 4.",
        "Lower-order rest costs perform better in both systems, reaching plateau model scores and BLEU with less CPU time.",
        "The gain is much larger for tar",
        "get syntax, where a pop limit of 50 outperforms the baseline with pop limit 700.",
        "CPU time per sentence is reduced to 23.5 seconds from 64.0 seconds, a 63.3% reduction.",
        "The combined setting, using the same memory as the baseline, shows a similar 62.1% reduction in CPU time.",
        "We attribute this difference to improved grammar rule scoring that impacts pruning and sorting.",
        "In the target syntax model, the grammar is not saturated (i.e. less pruning will still improve scores) but we nonetheless prune for tractability reasons.",
        "The lower-order rest costs are particularly useful for grammar pruning because lexical items are typically less than five words long (and frequently only word).",
        "The hierarchical grammar is nearly saturated with respect to grammar pruning, so improvement there is due mostly to better search.",
        "In the hierarchical system, peak BLEU 22.34 is achieved under the lower-order condition with pop limits 50 and 200, while other scenarios are still climbing to the plateau.",
        "With a pop limit of 1000, the baseline's average model score is -101.3867.",
        "Better average models scores are obtained from the lower-order model with pop limit 690 using 79% of baseline CPU, the combined model with pop limit 900 using 97% CPU, and the pessimistic model with pop limit 1350 using 127% CPU.",
        "Pessimistic compression does worsen search, requiring 27% more CPU in the hierarchical system to achieve the same quality.",
        "This is worthwhile to fit large-scale language models in memory, especially if the alternative is a remote language model."
      ]
    },
    {
      "heading": "4.3 Memory Usage",
      "text": [
        "Our rest costs add a value (for lower-order probabilities) or remove a value (pessimistic compression) for each n-gram except those of highest order",
        "and removes another, so it uses the same memory as the baseline.",
        "The memory footprint of adding or removing a value depends on the number of such n-grams, the underlying data structure, and the extent of quantization.",
        "Our test language model has 135 million n-grams for n < 5 and 56 million 5-grams.",
        "Memory usage was measured for KenLM data structures (Heafield, 2011) and minimal perfect hashing (Guthrie and Hepple, 2010).",
        "For minimal perfect hashing, we assume the Compress, Hash and Displace algorithm (Belazzougui et al. 2008) with 8-bit signatures and 8-bit quantization.",
        "Table 2 shows the results.",
        "Storage size of the smallest model is reduced by 26%, bringing higher-quality smoothed models in line with stupid backoff models that also store one value per n-gram.",
        "excluding operating system overhead.",
        "Change is the cost of adding an additional value to store lower-order probabilities.",
        "Equivalently, it is the savings from pessimistic compression."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "Our techniques reach plateau-level BLEU scores with less time or less memory.",
        "Efficiently storing lower-order probabilities and using them as rest costs improves both cube pruning (21% CPU reduction in a hierarchical system) and model filtering (net 63% CPU time reduction with target syntax) at the expense of 13-26% more RAM for the language model.",
        "This model filtering improvement is surprising both in the impact relative to changing the pop limit and simplicity of implementation, since it can be done offline.",
        "Compressing the language model to halve the number of values per n-gram (except N - grams) results in a 13-26% reduction in RAM with 26% over the smallest model, costing 27% more CPU and leaving overall sentence scores unchanged.",
        "This compression technique is likely to have more general application outside of machine translation, especially where only sentence-level scores are required.",
        "Source code is being released6 under the LGPL as part of KenLM (Heafield, 2011)."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was supported by the National Science Foundation under grants DGE-0750271, IIS0713402, and IIS-0915327; by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme), and by the DARPA GALE program.",
        "Benchmarks were run on Trestles at the San Diego Supercomputer Center under allocation TG-CCR110017.",
        "Trestles is part of the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI-1053575."
      ]
    }
  ]
}
