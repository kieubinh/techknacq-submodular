{
  "info": {
    "authors": [
      "Mohit Bansal",
      "John DeNero",
      "Dekang Lin"
    ],
    "book": "NAACL",
    "id": "acl-N12-1095",
    "title": "Unsupervised Translation Sense Clustering",
    "url": "https://aclweb.org/anthology/N12-1095",
    "year": 2012
  },
  "references": [
    "acl-C04-1192",
    "acl-C10-1124",
    "acl-C96-2141",
    "acl-C98-1012",
    "acl-D07-1043",
    "acl-D11-1095",
    "acl-E09-1010",
    "acl-J92-4003",
    "acl-J93-2003",
    "acl-N03-1015",
    "acl-P02-1033",
    "acl-P03-1058",
    "acl-P08-1086",
    "acl-P09-1116",
    "acl-W09-0210"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We propose an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense.",
        "Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the softK-Means algorithm.",
        "In addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation.",
        "By comparing our induced clusters to reference clusters generated from WordNet, we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora.",
        "Finally, we describe a method for annotating clusters with usage examples."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The ability to learn a bilingual lexicon from a parallel corpus was an early and influential area of success for statistical modeling techniques in natural language processing.",
        "Probabilistic word alignment models can induce bilexical distributions over target-language translations of source-language words (Brown et al., 1993).",
        "However, word-to-word correspondences do not capture the full structure of a bilingual lexicon.",
        "Consider the example bilingual dictionary entry in Figure 1; in addition to enumerating the translations of a word, the dictionary author has grouped those translations into three sense clusters.",
        "Inducing such a clustering would prove useful in generating bilingual dictionaries automatically or building tools to assist bilingual lexicographers.",
        "?Author was a summer intern with Google Research while conducting this research project.",
        "Colocar [co?lo?car?",
        "], va. 1.",
        "To arrange, to put in due place or order.",
        "2.",
        "To place, to put in any place, rank condition or office, to provide a place or em",
        "English translations of the polysemous Spanish word colocar into three clusters that correspond to different word senses (Vela?zquez de la Cadena et al., 1965).",
        "This paper formalizes the task of clustering a set of translations by sense, as might appear in a published bilingual dictionary, and proposes an unsupervised method for inducing such clusters.",
        "We also show how to add usage examples for the translation sense clusters, hence providing complete structure to a bilingual dictionary.",
        "The input to this task is a set of source words and a set of target translations for each source word.",
        "Our proposed method clusters these translations in two steps.",
        "First, we induce a global clustering of the entire target vocabulary using the soft K-Means algorithm, which identifies groups of words that appear in similar contexts (in a monolingual corpus) and are translated in similar ways (in a parallel corpus).",
        "Second, we derive clusters over the translations of each source word by projecting the global clusters.",
        "We evaluate these clusters by comparing them to reference clusters with the overlapping BCubed metric (Amigo et al., 2009).",
        "We propose a clustering criterion that allows us to derive reference clusters from the synonym groups of WordNet R?",
        "(Miller, 1995).1 Our experiments using Spanish-English and Japanese-English datasets demonstrate that the automatically generated clusters produced by our method are substantially more similar to the",
        "Sense cluster WordNet sense description Usage example collocate group or chunk together in a certain order or place side by side colocar juntas todas los libros collocate all the books invest, place, put make an investment capitales para colocar capital to invest locate, place assign a location to colocar el nu?mero de serie locate the serial number place, position, put put into a certain place or abstract",
        "{collocate, invest, locate, place, position, put}.",
        "Only the sense clusters are outputs of the translation sense clustering task; the additional columns are presented for clarity.",
        "WordNet-based reference clusters than naive baselines.",
        "Moreover, we show that bilingual features collected from parallel corpora improve clustering accuracy over monolingual distributional similarity features alone.",
        "Finally, we present a method for annotating clusters with usage examples, which enrich our automatically generated bilingual dictionary entries."
      ]
    },
    {
      "heading": "2 Task Description",
      "text": [
        "We consider a three-step pipeline for generating structured bilingual dictionary entries automatically.",
        "(1) The first step is to identify a set of high-quality target-side translations for source lexical items.",
        "In our experiments, we ask bilingual human annotators to create these translation sets.2 We restrict our present study to word-level translations, disallowing multi-word phrases, in order to leverage existing lexical resources for evaluation.",
        "(2) The second step is to cluster translations of each word according to common word senses.",
        "This clustering task is the primary focus of the paper, and we formalize it in this section.",
        "(3) The final step annotates clusters with usage examples to enrich the structure of the output.",
        "Section 7 describes a method of identifying cluster-specific usage examples.",
        "In the task of translation sense clustering, the second step, we assume a fixed set of source lexical items of interest S, each with a single part of",
        "input.",
        "speech3, and for each s ?",
        "S a set Ts of target translations.",
        "Moreover, we assume that each target word t ?",
        "Ts has a set of senses in common with s. These senses may also be shared among different target words.",
        "That is, each target word may have multiple senses and each sense may be expressed by multiple words.",
        "Given a translation set Ts, we define a clusterG ?",
        "Ts to be a correct sense cluster if it is both coherent and complete.",
        "?",
        "A sense cluster G is coherent if and only if there exists some sense B shared by all of the target words in G. ?",
        "A sense clusterG is complete if and only if, for every sense B shared by all words in G, there is no other word in Ts but not in G that also shares that sense.",
        "The full set of correct clusters for a set of translations consists of all sense clusters that are both coherent and complete.",
        "The example translation set for the Spanish word colocar in Figure 2 is shown with four correct sense clusters.",
        "For descriptive purposes, these clusters are annotated by WordNet senses and bilingual usage examples.",
        "However, the task we have defined does not require the WordNet sense or usage example to be identified: we must only produce the correct sense clusters within a set of translations.",
        "In fact, a cluster may correspond to more than one sense.",
        "Our definition of correct sense clusters has several appealing properties.",
        "First, we do not attempt to enumerate all senses of the source word.",
        "Sense 3A noun and verb that share the same word form would constitute two different source lexical items.",
        "language-level synsets (C) to source-specific synsets (B) and then filters the set of synsets for redundant subsets to produce the complete set of source-specific synsets that are both coherent and complete (G).",
        "distinctions are only made when they affect cross-lingual lexical choice.",
        "If a source word has many fine-grained senses but translates in the same way regardless of the sense intended, then there is only one correct sense cluster for that translation.",
        "Second, no correct sense cluster can be a superset of another, because the subset would violate the completeness condition.",
        "This criterion encourages larger clusters that are easier to interpret, as their unifying senses can be identified as the intersection of senses of the translations in the cluster.",
        "Third, the correct clusters need not form a partition of the input translations.",
        "It is common in published bilingual dictionaries for a translation to appear in multiple sense clusters.",
        "In our example, the polysemous English verbs place and put appear in multiple clusters."
      ]
    },
    {
      "heading": "3 Generating Reference Clusters",
      "text": [
        "To construct a reference set for the translation sense clustering task, we first collected English translations of Spanish and Japanese nouns, verbs, and adverbs.",
        "Translation sets were curated by human annotators to keep only high-quality single-word translations.",
        "Rather than gathering reference clusters via an additional annotation effort, we leverage WordNet, a large database of English lexical semantics (Miller, 1995).",
        "WordNet groups words into sets of cogni-Synsets collocate collocate, lump, chunk invest, put, commit, place invest, clothe, adorn invest, vest, enthrone ?",
        "locate, turn up situate, locate locate, place, site ?",
        "put, set, place, pose, position, lay rate, rank, range, order, grade, place locate, place, site invest, put, commit, place ?",
        "position put, set, place, pose, position, lay put, set, place, pose, position, lay put frame, redact, cast, put, couch invest, put, commit, place ?",
        "Spanish source word colocar.",
        "We show the target translation words to be clustered, their WordNet synsets (with words not in the translation set grayed out), and the final set of correct sense clusters.",
        "tive synonyms called synsets, each expressing a distinct concept.",
        "We use WordNet version 2.1, which has wide coverage of nouns, verbs, and adverbs, but sparser coverage of adjectives and prepositions.4 Reference clusters for the set of translations Ts of some source word s are generated algorithmically from WordNet synsets via the Cluster Projection (CP) algorithm defined in Figure 3.",
        "An input to the CP algorithm is the translation set Ts of some source word s. Also, each translation t ?",
        "Ts belongs to some set of synsets Dt, where each synset C ?",
        "Dt contains target-language words that may or may not be translations of s. First, the CP algorithm constructs a source-specific synset B for each C, which contains only translations of s. Second, it identifies all correct sense clusters G that are both coherent and complete with respect to the source-specific senses B.",
        "A sense cluster must correspond to some synset B ?",
        "B to be coherent, and it must 4WordNet version 2.1 is almost identical to version 3.0, for Unix-like systems, as described in http://wordnetcode.princeton.edu/3.0/CHANGES.",
        "The latest version 3.1 is not yet available for download.",
        "not have a proper superset in B to be complete.5 Figure 4 illustrates the CP algorithm for the translations of the Spanish source word colocar that appear in our input dataset."
      ]
    },
    {
      "heading": "4 Clustering with K-Means",
      "text": [
        "In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora.",
        "Our method is language independent."
      ]
    },
    {
      "heading": "4.1 Distributed SoftK-Means Clustering",
      "text": [
        "As a first step, we cluster all words in the target-language vocabulary in a way that relates words that have similar distributional features.",
        "Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008).",
        "We use a distributed implementation of the ?soft?",
        "K-Means clustering algorithm described in Lin and Wu (2009).",
        "Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows:",
        "1.",
        "Select K elements as the initial centroids for K clusters.",
        "repeat 2.",
        "Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space.",
        "3.",
        "Recompute each cluster's centroid by averaging the feature vectors of the elements in that cluster.",
        "until convergence"
      ]
    },
    {
      "heading": "4.2 Monolingual Features",
      "text": [
        "Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word.",
        "In our setup, the 5One possible shortcoming of our approach to constructing reference sets for translation sense clustering is that a cluster may correspond to a sense that is not shared by the original source word used to generate the translation set.",
        "All translations must share some sense with the source word, but they may not share all senses with the source word.",
        "It is possible that two translations are synonymous in a sense that is not shared by the source.",
        "However, we did not observe this problem in practice.",
        "context of a word w consists of the words immediately to the left and right of w. The context feature vector of w is constructed by first aggregating the frequency counts of each word f in the context of each w. We then compute point-wise mutual information (PMI) features from the frequency counts:",
        "where w is a word, f is a neighboring word, and c(?)",
        "is the count of a word or word pair in the corpus.6 A feature vector for w contains a PMI feature for each word type f (with relative position left or right) for all words that appears a sufficient number of times as a neighbor of w. The similarity of two feature vectors is the cosine of the angle between the vectors.",
        "We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total-count threshold for the feature vectors, and a threshold for PMI scores."
      ]
    },
    {
      "heading": "4.3 Bilingual Features",
      "text": [
        "In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments).",
        "We have two types of bilingual features: unigram features capture source-side reverse-translations ofw, while bigram features capture both the reverse-translations and source-side neighboring context words to the left and right.",
        "Features are expressed again as PMI computed from frequency counts of aligned phrase pairs in a parallel corpus.",
        "For example, one unigram feature for place would be the PMI computed from the number of times that place was in the target side of a phrase pair whose source side was the unigram lugar.",
        "Similarly, a bigram feature for place would be the PMI computed from the number of times that place was in the target side of a phrase pair whose source side was the bigram lugar de.",
        "These features characterize the way in which a word is translated, an indication of its meaning.",
        "6PMI is typically defined in terms of probabilities, but has proven effective previously when defined in terms of counts."
      ]
    },
    {
      "heading": "4.4 Predicting Translation Clusters",
      "text": [
        "As a result of softK-Means clustering, each word in the target-language vocabulary is assigned to a list of up to M clusters.",
        "To predict the sense clusters for a set of translations of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt)."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "To our knowledge, the translation sense clustering task has not been explored previously.",
        "However, much prior work has explored the related task of monolingual word and phrase clustering.",
        "Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vla-chos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs.",
        "Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora.",
        "However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word.",
        "The underlying notion of ?sense?",
        "is shared across these tasks, but the way in which we use and evaluate induced senses is novel."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively recover the reference sense clusters derived from WordNet."
      ]
    },
    {
      "heading": "6.1 Datasets",
      "text": [
        "We conduct experiments using two bilingual datasets: Spanish-to-English (S?E) and Japanese-to-English (J?E).",
        "Table 1 shows, for each dataset, the number of source words and the total number of target words in their translation sets.",
        "The datasets",
        "are limited in size because we solicited human annotators to filter the set of translations for each source word.",
        "The S?E dataset has 52 source-words with a part-of-speech-tag distribution of 38 nouns, 10 verbs and 4 adverbs.",
        "The J?E dataset has 369 source-words with 319 nouns, 38 verbs and 12 adverbs.",
        "We included only these parts of speech because WordNet version 2.1 has adequate coverage for them.",
        "Most source words have 3 to 5 translations each.",
        "Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text.",
        "Bilingual features were computed from 0.78 (S?E) and 1.04 (J?E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010).",
        "Word alignments were induced from the HMM-based alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993).",
        "Both models were trained using 2 iterations of the expectation maximization algorithm.",
        "Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrase-based machine translation (Koehn et al., 2003)."
      ]
    },
    {
      "heading": "6.2 Clustering Evaluation Metrics",
      "text": [
        "The quality of text clustering algorithms can be evaluated using a wide set of metrics.",
        "For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsber-gen (1974)).",
        "For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003).",
        "Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003).",
        "Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clus",
        "tering evaluation by estimating precision and recall for each item in the distribution.",
        "Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties.",
        "They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families.",
        "Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints.",
        "After defining each constraint below, we briefly describe its relevance to the translation sense clustering task.",
        "Homogeneity: In a cluster, we should not mix items belonging to different categories.",
        "Relevance: All words in a proposed cluster should share some common WordNet sense.",
        "Completeness: Items belonging to the same category should be grouped in the same cluster.",
        "Relevance: All words that share some common WordNet sense should appear in the same cluster.",
        "Rag Bag: Introducing disorder into a disordered cluster is less harmful than introducing disorder into a clean cluster.",
        "Relevance: We prefer to maximize the number of error-free clusters, because these are most easily interpreted and therefore most useful.",
        "Cluster Size vs.",
        "Quantity: A small error in a big cluster is preferable to a large number of small errors in small clusters.",
        "Relevance: We prefer to minimize the total number of erroneous clusters in a dictionary.",
        "Amigo et al. (2009) also show that BCubed extends cleanly to settings with overlapping clusters, where an element can simultaneously belong to more than one cluster.",
        "For these reasons, we focus on BCubed for cluster similarity evaluation.",
        "The BCubed metric for scoring overlapping clusters is computed from the pairwise precision and recall between pairs of items:",
        "where e and e?",
        "are two items, L(e) is the set of reference clusters for e and C(e) is the set of predicted 7An evaluation using purity and inverse purity (extended to overlapping clusters) has been omitted for space, but leads to the same conclusions as the evaluation using BCubed.",
        "clusters for e (i.e., clusters to which e belongs).",
        "Note that P(e, e?)",
        "is defined only when e and e?",
        "share some predicted cluster, and R(e, e?)",
        "when e and e?",
        "share some reference cluster.",
        "The BCubed precision associated to one item is its averaged pairwise precision over other items sharing some of its predicted clusters, and likewise for recall8; and the overall BCubed precision (or recall) is the averaged precision (or recall) of all items:"
      ]
    },
    {
      "heading": "6.3 Results",
      "text": [
        "Figure 5 shows the F?-score for various ?",
        "values:",
        "This graph gives us a trade-off between precision and recall (?",
        "= 0 is exact precision and ?",
        "?",
        "?",
        "tends to exact recall).9 Each curve in Figure 5 represents a particular clustering method.",
        "We include three naive baselines: ewnc: Each word in its own cluster aw1c: All words in one cluster Random: Each target word is assigned M random cluster id's in the range 1 to K, then translation sets are clustered with the CP algorithm.",
        "The curves for K-Means clustering include one condition with monolingual features alone and two curves that include bilingual features as well.10 The bilingual curves correspond to two different feature sets: the first includes only unigram features (t1), while the second includes both unigram and bigram features (t1t2).",
        "Each point on an F?",
        "curve in Figure 5 (including the baseline curves) represents a maximum over two",
        "where we first compute PB3 and RB3 for each source-word, then compute the average PB3 and RB3 over all source-words, and finally compute the F-score using these averaged PB3 and RB3.",
        "10All bilingual K-Means experiments include monolingual features also.",
        "K-Means with only bilingual features does not produce accurate clusters.",
        "to clustering with additional bilingual features.",
        "count K and clusters per word M .",
        "parameters: K, the number of clusters created in the whole corpus andM , the number of clusters allowed per word (in M best soft K-Means).",
        "As both the random baseline and proposed clustering methods can be tuned to favor precision or recall, we show the best result from each technique across this spectrum of F?",
        "metrics.",
        "We vary ?",
        "to highlight different potential objectives of translation sense clustering.",
        "An application that focuses on synonym discovery would favor recall, while an application portraying highly granular sense distinctions would favor precision.",
        "Clustering accuracy improves over the baselines with monolingual features alone, and it improves further with the addition of bilingual features, for a wide range of ?",
        "values.",
        "Our unsupervised approach with bilingual features achieves up to 6-8% absolute improvement over the random baseline, and is particularly effective for recall-weighted metrics.11 As an example, in a S?E experiment with a K-Means setting ofK = 4096 : M = 3, the overall F1.5 score 11It is not surprising that a naive baseline like random clustering can achieve a high precision: BCubed counts each word itself as correctly clustered, and so even trivial techniques that create many singleton clusters will have high precision.",
        "High recall (without very low precision) is harder to achieve, because it requires positing larger clusters, and it is for recall-focused objectives that our technique substantially outperforms the random baseline.",
        "increases from 80.58% to 86.12% upon adding bilingual features.",
        "Table 2 shows two examples from that experiment for which bilingual features improve the output clusters.",
        "The parameter values we use in our experiments are K ?",
        "{23, 24, .",
        ".",
        ".",
        ", 212} and M ?",
        "{1, 2, 3, 4, 5}.",
        "To provide additional detail, Figure 6 shows the BCubed precision and recall for each induced clustering, as the values of K and M vary, for Japanese-English.12 Each point in this scatter plot represents a clustering methodology and a particular value for K and M .",
        "Soft K-Means with bilingual features provides the strongest performance across a broad range of cluster parameters."
      ]
    },
    {
      "heading": "6.4 Evaluation Details",
      "text": [
        "Certain special cases needed to be addressed in order to complete this evaluation.",
        "Target words not in WordNet: Words that did not have any synset in WordNet were each assigned to a singleton reference cluster.13 The S?E dataset has only 2 out of 225 target types missing in WordNet and the J?E dataset has only 55 out of 1351 target 12Spanish-English precision-recall results are omitted due to space constraints, but depict similar trends.",
        "13Note that certain words with WordNet synsets also end up in their own singleton cluster because all other words in their cluster are not in the translation set.",
        "types missing.",
        "Target words not clustered by K-Means: The K-Means algorithm applies various thresholds during different parts of the process.",
        "As a result, there are some target word types that are not assigned any cluster at the end of the algorithm.",
        "For example, in the J?E experiment with K = 4096 and with bilingual (t1 only) features, only 49 out of 1351 target-types are not assigned any cluster by K-Means.",
        "These unclustered words were each assigned to a singleton cluster in post-processing."
      ]
    },
    {
      "heading": "7 Identifying Usage Examples",
      "text": [
        "We now briefly consider the task of automatically extracting usage examples for each predicted cluster.",
        "We identify these examples among the extracted phrase pairs of a parallel corpus.",
        "Let Ps be the set of source phrases containing source word s, and letAt be the set of source phrases that align to target phrases containing target word t. For a source word s and target sense cluster G, we identify source phrases that contain s and translate to all words in G. That is, we collect the set of phrases Ps ?",
        "?",
        "t?GAt.",
        "We use the same parallel corpus as we used to compute bilingual features.",
        "For example, if we consider the cluster [place, position, put] for the Spanish word colocar, then we find Spanish phrases that contain colocar and also align to English phrases containing place, position, and put somewhere in the parallel corpus.",
        "Sample usage examples extracted by this approach appear in Figure 7.",
        "We have not performed a quantitative evaluation of these extracted examples, although qualitatively we have found that the technique surfaces useful phrases.",
        "We look forward to future research that further explores this important subtask of automatically generating bilingual dictionaries."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We presented the task of translation sense clustering, a critical second step to follow translation extraction in a pipeline for generating well-structured bilingual dictionaries automatically.",
        "We introduced a method of projecting language-level clusters into clusters for specific translation sets using the CP algorithm.",
        "We used this technique both for constructing reference clusters, via WordNet synsets, and constructing pre-debajo [\"below\",\"beneath\"] ?",
        "debajo de la superficie (below the surface) [\"below\",\"under\"] ?",
        "debajo de la l?nea (below the line) [\"underneath\"] ?",
        "debajo de la piel (under the skin) ??",
        "[\"break\"] ?",
        "????",
        "??",
        "?",
        "??",
        "??",
        "??",
        "?",
        "?",
        "??",
        "??",
        ".",
        "(I worked hard and I deserve a good break.)",
        "[\"recreation\"] ?",
        "??",
        "?",
        "??",
        "?",
        "??",
        "??",
        "(Traditional healing and recreation activities) [\"rest\"] ?",
        "???",
        "?",
        "??",
        "??",
        "??",
        "?",
        "??",
        "??",
        ".",
        "(Bed rest is the only treatment required.)",
        "their English sense clusters.",
        "Our approach extracts multiple examples per cluster, but we show only one.",
        "We also show the translation of the examples back into English produced by Google Translate.",
        "dicted clusters from the output of a vocabulary-level clustering algorithm.",
        "Our experiments demonstrated that the soft K-Means clustering algorithm, trained using distributional features from very large monolingual and bilingual corpora, recovered a substantial portion of the structure of reference clusters, as measured by the BCubed clustering metric.",
        "The addition of bilingual features improved clustering results over monolingual features alone; these features could prove useful for other clustering tasks as well.",
        "Finally, we annotated our clusters with usage examples.",
        "In future work, we hope to combine our clustering method with a system for automatically generating translation sets.",
        "In doing so, we will develop a system that can automatically induce high-quality, human-readable bilingual dictionaries from large corpora using unsupervised learning methods."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank Jakob Uszkoreit, Adam Pauls, and the anonymous reviewers for their helpful suggestions."
      ]
    }
  ]
}
