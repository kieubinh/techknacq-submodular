{
  "info": {
    "authors": [
      "Ariya Rastrow",
      "Mark Dredze",
      "Sanjeev Khudanpur"
    ],
    "book": "ACL",
    "id": "acl-P12-1019",
    "title": "Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining",
    "url": "https://aclweb.org/anthology/P12-1019",
    "year": 2012
  },
  "references": [
    "acl-D07-1111",
    "acl-D09-1116",
    "acl-D10-1002",
    "acl-D10-1069",
    "acl-N10-1115",
    "acl-P05-1063",
    "acl-P06-2089",
    "acl-P10-1110",
    "acl-W11-0328"
  ],
  "sections": [
    {
      "text": [
        "wi the search considers the current state and next state.",
        "first search lookahead procedure to select the best action at each step, which considers future decisions up to depth d5.",
        "An example for d = 1 is shown in Figure 2.",
        "Using d = 1 for the lookahead search strategy, we modify the kernel features since the decision forwi is affected by the state pii+1.",
        "The kernel features in position i should be f?",
        "(pii) ?",
        "f?",
        "(pii+1):",
        "While we have fast decoding algorithms for the parsing and tagging, the simpler underlying models can lead to worse performance.",
        "Using more complex models with higher accuracy is impractical because they are slow.",
        "Instead, we seek to improve the accuracy of our fast tools.",
        "To achieve this goal we use up-training, in which a more complex model is used to improve the accuracy of a simpler model.",
        "We are given two models, M1 and M2, as well as a large collection of unlabeled text.",
        "Model M1 is slow but very accurate while M2 is fast but obtains lower accuracy.",
        "Up-training applies M1 to tag the unlabeled data, which is then used as training data for M2.",
        "Like self-training, a model is retrained on automatic output, but here the output comes form a more accurate model.",
        "Petrov et al. (2010) used up-training as a domain adaptation technique: a constituent parser ?",
        "which is more robust to domain changes ?",
        "was used to label a new domain, and a fast dependency parser 5 Tsuruoka et al. (2011) shows that the lookahead search improves the performance of the local ?history-based?",
        "models for different NLP tasks was trained on the automatically labeled data.",
        "We use a similar idea where our goal is to recover the accuracy lost from using simpler models.",
        "Note that while up-training uses two models, it differs from co-training since we care about improving only one model (M2).",
        "Additionally, the models can vary in different ways.",
        "For example, they could be the same algorithm with different pruning methods, which can lead to faster but less accurate models.",
        "We apply up-training to improve the accuracy of both our fast POS tagger and dependency parser.",
        "We parse a large corpus of text with a very accurate but very slow constituent parser and use the resulting data to up-train our tools.",
        "We will demonstrate empirically that up-training improves these fast models to yield better WER results."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "The idea of efficiently processing a hypothesis set is similar to ?lattice-parsing?, in which a parser consider an entire lattice at once (Hall, 2005; Cheppalier et al., 1999).",
        "These methods typically constrain the parsing space using heuristics, which are often model specific.",
        "In other words, they search in the joint space of word sequences present in the lattice and their syntactic analyses; they are not guaranteed to produce a syntactic analysis for all hypotheses.",
        "In contrast, substructure sharing is a general purpose method that we have applied to two different algorithms.",
        "The output is identical to processing each hypothesis separately and output is generated for each hypothesis.",
        "Hall (Hall, 2005) uses a lattice parsing strategy which aims to compute the marginal probabilities of all word sequences in the lattice by summing over syntactic analyses of each word sequence.",
        "The parser sums over multiple parses of a word sequence implicitly.",
        "The lattice parser therefore, is itself a language model.",
        "In contrast, our tools are completely separated from the ASR system, which allows the system to create whatever features are needed.",
        "This independence means our tools are useful for other tasks, such as machine translation.",
        "These differences make substructure sharing a more attractive option for efficient algorithms.",
        "While Huang and Sagae (2010) use the notion of ?equivalent states?, they do so for dynamic programming in a shift-reduce parser to broaden the search space.",
        "In contrast, we use the idea to identify sub",
        "structures across inputs, where our goal is efficient parsing in general.",
        "Additionally, we extend the definition of equivalent states to general transition based structured prediction models, and demonstrate applications beyond parsing as well as the novel setting of hypothesis set parsing."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "Our ASR system is based on the 2007 IBM Speech transcription system for the GALE Distillation Go/No-go Evaluation (Chen et al., 2006) with state of the art discriminative acoustic models.",
        "See Table 2 for a data summary.",
        "We use a modified Kneser-Ney (KN) backoff 4-gram baseline LM.",
        "Word-lattices for discriminative training and rescor-ing come from this baseline ASR system.",
        "The long-span discriminative LM's baseline feature weight (?0) is tuned on dev data and hill climbing (Rastrow et al., 2011a) is used for training and rescoring.",
        "The dependency parser and POS tagger are trained on supervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of Huang et al.",
        "(2010), a state of the art broadcast news (BN) parser, with phrase structures converted to labeled dependencies by the Stanford converter.",
        "While accurate, the parser has a huge grammar (32GB) from using products of latent variable grammars and requires O(l3) time to parse a sentence of length l. Therefore, we could not use the constituent parser for ASR rescoring since utterances can be very long, although the shorter up-training text data was not a problem.7 We evaluate both unlabeled (UAS) and labeled dependency accuracy (LAS)."
      ]
    },
    {
      "heading": "6.1 Results",
      "text": [
        "Before we demonstrate the speed of our models, we show that up-training can produce accurate and fast models.",
        "Figure 3 shows improvements to parser accuracy through up-training for different amount of (randomly selected) data, where the last column indicates constituent parser score (91.4% UAS).",
        "We use the POS tagger to generate tags for dependency training to match the test setting.",
        "While there is a large difference between the constituent and dependency parser without up-training (91.4%",
        "varying amounts of data (number of words.)",
        "The first column is the dependency parser with supervised training only and the last column is the constituent parser (after converting to dependency trees.)",
        "vs. 86.2% UAS), up-training can cut the difference by 44% to 88.5%, and improvements saturate around 40m words (about 2m sentences.",
        ")8 The dependency parser remains much smaller and faster; the up-trained dependency model is 700MB with 6m features compared with 32GB for constituency model.",
        "Up-training improves the POS tagger's accuracy from 95.9% to 97%, when trained on the POS tags produced by the constituent parser, which has a tagging accuracy of 97.2% on BN.",
        "We train the syntactic discriminative LM, with headword and POS tag features, using the faster parser and tagger and then rescore the ASR hypotheses.",
        "Table 3 shows the decoding speedups as well as the WER reductions compared to the baseline LM.",
        "Note that up-training improvements lead to WER reductions.",
        "Detailed speedups on substructure sharing are shown in Table 4; the POS tagger achieves a 5.3 times speedup, and the parser a 5.7 speedup without changing the output.",
        "We also observed speedups during training (not shown due to space.)",
        "The above results are for the already fast hill climbing decoding, but substructure sharing can also be used for N best list rescoring.",
        "Figure 4 (logarithmic scale) illustrates the time for the parser and tagger to processN best lists of varying size, with more substantial speedups for larger lists.",
        "For example, for N=100 (a typical setting) the parsing time re-8Better performance is due to the exact CKY-style ?",
        "compared with best-first and beam?",
        "search and that the constituent parser uses the product of huge self-trained grammars."
      ]
    },
    {
      "heading": "Usage Data Size",
      "text": [
        "Acoustic model training Hub4 acoustic train 153k uttr, 400 hrs Baseline LM training: modified KN 4-gram TDT4 closed captions+EARS BN03 closed caption 193m words Disc.",
        "LM training: long-span w/hill climbing Hub4 (length <50) 115k uttr, 2.6m words Baseline feature (?0) tuning dev04f BN data 2.5 hrs Supervised training: dep.",
        "parser, POS tagger Ontonotes BN treebank+ WSJ Penn treebank 1.3m words, 59k sent.",
        "Supervised training: constituent parser Ontonotes BN treebank + WSJ Penn treebank 1.3m words, 59k sent.",
        "Up-training: dependency parser, POS tagger TDT4 closed captions+EARS BN03 closed caption 193m words available Evaluation: up-training BN treebank test (following Huang et al. (2010)) 20k words, 1.1k sent.",
        "Evaluation: ASR transcription rt04 BN evaluation 4 hrs, 45k words",
        "ing.",
        "Substructure sharing yields a 5.3 times speedup.",
        "The times for with and without up-training are nearly identical, so we include only one set for clarity.",
        "Time spent is dominated by the parser, so the faster parser accounts for much of the overall speedup.",
        "Timing information includes neighborhood generation and LM rescoring, so it is more than the sum of the times in Table 4. duces from about 20,000 seconds to 2,700 seconds, about 7.4 times as fast."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "The computational complexity of accurate syntactic processing can make structured language models impractical for applications such as ASR that require scoring hundreds of hypotheses per input.",
        "We have",
        "to process hypotheses during hill climbing rescoring.",
        "presented substructure sharing, a general framework that greatly improves the speed of syntactic tools that process candidate hypotheses.",
        "Furthermore, we achieve improved performance through up-training.",
        "The result is a large speedup in rescoring time, even on top of the already fast hill climbing framework, and reductions in WER from up-training.",
        "Our results make long-span syntactic LMs practical for real-time ASR, and can potentially impact machine translation decoding as well."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Thanks to Kenji Sagae for sharing his shift-reduce dependency parser and the anonymous reviewers for helpful comments."
      ]
    }
  ]
}
