{
  "info": {
    "authors": [
      "Qiming Diao",
      "Jing Jiang",
      "Feida Zhu",
      "Ee-Peng Lim"
    ],
    "book": "ACL",
    "id": "acl-P12-1056",
    "title": "Finding Bursty Topics from Microblogs",
    "url": "https://aclweb.org/anthology/P12-1056",
    "year": 2012
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Microblogs such as Twitter reflect the general public's reactions to major events.",
        "Bursty topics from microblogs reveal what events have attracted the most online attention.",
        "Although bursty event detection from text streams has been studied before, previous work may not be suitable for microblogs because compared with other text streams such as news articles and scientific publications, microblog posts are particularly diverse and noisy.",
        "To find topics that have bursty patterns on microblogs, we propose a topic model that simultaneously captures two observations: (1) posts published around the same time are more likely to have the same topic, and (2) posts published by the same user are more likely to have the same topic.",
        "The former helps find event-driven posts while the latter helps identify and filter out ?personal?",
        "posts.",
        "Our experiments on a large Twitter dataset show that there are more meaningful and unique bursty topics in the top-ranked results returned by our model than an LDA baseline and two degenerate variations of our model.",
        "We also show some case studies that demonstrate the importance of considering both the temporal information and users?",
        "personal interests for bursty topic detection from microblogs."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "With the fast growth of Web 2.0, a vast amount of user-generated content has accumulated on the social Web.",
        "In particular, microblogging sites such as Twitter allow users to easily publish short instant posts about any topic to be shared with the general public.",
        "The textual content coupled with the temporal patterns of these microblog posts provides important insight into the general public's interest.",
        "A sudden increase of topically similar posts usually indicates a burst of interest in some event that has happened offline (such as a product launch or a natural disaster) or online (such as the spread of a viral video).",
        "Finding bursty topics from microblogs therefore can help us identify the most popular events that have drawn the public's attention.",
        "In this paper, we study the problem of finding bursty topics from a stream of microblog posts generated by different users.",
        "We focus on retrospective detection, where the text stream within a certain period is analyzed in its entirety.",
        "Retrospective bursty event detection from text streams is not new (Kleinberg, 2002; Fung et al., 2005; Wang et al., 2007), but finding bursty topics from microblog steams has not been well studied.",
        "In his seminal work, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream in order to identify bursts.",
        "This model has been widely used.",
        "However, this model assumes that documents in the stream are all about a given topic.",
        "In contrast, discovering interesting topics that have drawn bursts of interest from a stream of topically diverse microblog posts is itself a challenge.",
        "To discover topics, we can certainly apply standard topic models such as LDA (Blei et al., 2003), but with standard LDA temporal information is lost during topic discovery.",
        "For microblogs, where posts are short and often event-driven, temporal information can sometimes be critical in determining the topic of a post.",
        "For example, typically a post containing the",
        "word ?jobs?",
        "is likely to be about employment, but right after October 5, 2011, a post containing ?jobs?",
        "is more likely to be related to Steve Jobs?",
        "death.",
        "Essentially, we expect that on microblogs, posts published around the same time have a higher probability to belong to the same topic.",
        "To capture this intuition, one solution is to assume that posts published within the same short time window follow the same topic distribution.",
        "Wang et al.",
        "(2007) proposed a PLSA-based topic model that exploits this idea to find correlated bursty patterns across multiple text streams.",
        "However, their model is not immediately applicable for our problem.",
        "First, their model assumes multiple text streams where word distributions for the same topic are different on different streams.",
        "More importantly, their model was applied to news articles and scientific publications, where most documents follow the global topical trends.",
        "On microblogs, besides talking about global popular events, users also often talk about their daily lives and personal interests.",
        "In order to detect global bursty events from microblog posts, it is important to filter out these ?personal?",
        "posts.",
        "In this paper, we propose a topic model designed for finding bursty topics from microblogs.",
        "Our model is based on the following two assumptions: (1) If a post is about a global event, it is likely to follow a global topic distribution that is time-dependent.",
        "(2) If a post is about a personal topic, it is likely to follow a personal topic distribution that is more or less stable over time.",
        "Separation of ?global?",
        "and ?personal?",
        "posts is done in an unsupervised manner through hidden variables.",
        "Finally, we apply a state machine to detect bursts from the discovered topics.",
        "We evaluate our model on a large Twitter dataset.",
        "We find that compared with bursty topics discovered by standard LDA and by two degenerate variations of our model, bursty topics discovered by our model are more accurate and less redundant within the top-ranked results.",
        "We also use some example bursty topics to explain the advantages of our model."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "To find bursty patterns from data streams, Kleinberg (2002) proposed a state machine to model the arrival times of documents in a stream.",
        "Different states generate time gaps according to exponential density functions with different expected values, and bursty intervals can be discovered from the underlying state sequence.",
        "A similar approach by Ihler et al. (2006) models a sequence of count data using Poisson distributions.",
        "To apply these methods to find bursty topics, the data stream used must represent a single topic.",
        "Fung et al. (2005) proposed a method that identifies both topics and bursts from document streams.",
        "The method first finds individual words that have bursty patterns.",
        "It then finds groups of words that tend to share bursty periods and co-occur in the same documents to form topics.",
        "Weng and Lee (2011) proposed a similar method that first characterizes the temporal patterns of individual words using wavelets and then groups words into topics.",
        "A major problem with these methods is that the word clustering step can be expensive when the number of bursty words is large.",
        "We find that the method by Fung et al. (2005) cannot be applied to our dataset because their word clustering algorithm does not scale up.",
        "Weng and Lee (2011) applied word clustering to only the top bursty words within a single day, and subsequently their topics mostly consist of two or three words.",
        "In contrast, our method is scalable and each detected bursty topic is directly associated with a word distribution and a set of tweets (see Table 3), which makes it easier to interpret the topic.",
        "Topic models provide a principled and elegant way to discover hidden topics from large document collections.",
        "Standard topic models do not consider temporal information.",
        "A number of temporal topic models have been proposed to consider topic changes over time.",
        "Some of these models focus on the change of topic composition, i.e. word distributions, which is not relevant to bursty topic detection (Blei and Lafferty, 2006; Nallapati et al., 2007; Wang et al., 2008).",
        "Some other work looks at the temporal evolution of topics, but the focus is not on bursty patterns (Wang and McCallum, 2006; Ahmed and Xing, 2008; Masada et al., 2009; Ahmed and Xing, 2010; Hong et al., 2011).",
        "The model proposed by Wang et al. (2007) is the most relevant to ours.",
        "But as we have pointed out in Section 1, they do not need to handle the separation of ?personal?",
        "documents from event-driven documents.",
        "As we will show later in our experiments, for microblogs it is critical to model users?",
        "personal interests in addition to global topical trends.",
        "To capture users?",
        "interests, Rosen-Zvi et al. (2004) expand topic distributions from document-level to user-level in order to capture users?",
        "specific interests.",
        "But on microblogs, posts are short and noisy, so Zhao et al. (2011) further assume that each post is assigned a single topic and some words can be background words.",
        "However, these studies do not aim to detect bursty patterns.",
        "Our work is novel in that it combines users?",
        "interests and temporal information to detect bursty topics."
      ]
    },
    {
      "heading": "3 Method",
      "text": []
    },
    {
      "heading": "3.1 Preliminaries",
      "text": [
        "We first introduce the notation used in this paper and formally formulate our problem.",
        "We assume that we have a stream of D microblog posts, denoted as d1, d2, .",
        ".",
        ".",
        ", dD.",
        "Each post di is generated by a user ui, where ui is an index between 1 and U , and U is the total number of users.",
        "Each di is also associated with a discrete timestamp ti, where ti is an index between 1 and T , and T is the total number of time points we consider.",
        "Each di contains a bag of words, denoted as {wi,1, wi,2, .",
        ".",
        ".",
        ", wi,Ni}, where wi,j is an index between 1 and V , and V is the vocabulary size.",
        "Ni is the number of words in di.",
        "We define a bursty topic b as a word distribution coupled with a bursty interval, denoted as (?b, tbs, tbe), where ?b is a multinomial distribution over the vocabulary, and tbs and tbe (1 ?",
        "tbs ?",
        "tbe ?",
        "T ) are the start and the end timestamps of the bursty interval, respectively.",
        "Our task is to find meaningful bursty topics from the input text stream.",
        "Our method consists of a topic discovery step and a burst detection step.",
        "At the topic discovery step, we propose a topic model that considers both users?",
        "topical interests and the global topic trends.",
        "Burst detection is done through a standard state machine method."
      ]
    },
    {
      "heading": "3.2 Our Topic Model",
      "text": [
        "We assume that there are C (latent) topics in the text stream, where each topic c has a word distribution ?c.",
        "Note that not every topic has a bursty interval.",
        "On the other hand, a topic may have multiple bursty intervals and hence leads to multiple bursty topics.",
        "We also assume a background word distribution ?B that captures common words.",
        "All posts are assumed to be generated from some mixture of these C + 1 underlying topics.",
        "In standard LDA, a document contains a mixture of topics, represented by a topic distribution, and each word has a hidden topic label.",
        "While this is a reasonable assumption for long documents, for short microblog posts, a single post is most likely to be about a single topic.",
        "We therefore associate a single hidden variable with each post to indicate its topic.",
        "Similar idea of assigning a single topic to a short sequence of words has been used before (Gruber et al., 2007; Zhao et al., 2011).",
        "As we will see very soon, this treatment also allows us to model topic distributions at time window level and user level.",
        "As we have discussed in Section 1, an important observation we have is that when everything else is equal, a pair of posts published around the same time is more likely to be about the same topic than a random pair of posts.",
        "To model this observation, we assume that there is a global topic distribution ?t for each time point t. Presumably ?t has a high probability for a topic that is popular in the microblog-sphere at time t. Unlike news articles from traditional media, which are mostly about current affairs, an important property of microblog posts is that many posts are about users?",
        "personal encounters and interests rather than global events.",
        "Since our focus is to find popular global events, we need to separate out these ?personal?",
        "posts.",
        "To do this, an intuitive idea is to compare a post with its publisher's general topical interests observed over a long time.",
        "If a post does not match the user's long term interests, it is more likely related to a global event.",
        "We therefore introduce a time-independent topic distribution ?u for each user to capture her long term topical interests.",
        "We assume the following generation process for all the posts in the stream.",
        "When user u publishes a post at time point t, she first decides whether to write about a global trendy topic or a personal topic.",
        "If she chooses the former, she then selects a topic according to ?t.",
        "Otherwise, she selects a topic according to her own topic distribution ?u.",
        "With the chosen topic, words in the post are generated from the word distribution for that topic or from the background word distribution that captures white noise.",
        "1.",
        "Draw ?B ?",
        "Dirichlet(?",
        "), ?",
        "?",
        "Beta(?",
        "), ?",
        "?",
        "Beta(?)",
        "2.",
        "For each time point t = 1, .",
        ".",
        ".",
        ", T (a) draw ?t ?",
        "Dirichlet(?)",
        "3.",
        "For each user u = 1, .",
        ".",
        ".",
        ", U (a) draw ?u ?",
        "Dirichlet(?)",
        "4.",
        "For each topic c = 1, .",
        ".",
        ".",
        ", C, (a) draw ?c ?",
        "Dirichlet(?)",
        "5.",
        "For each post i = 1, .",
        ".",
        ".",
        ", D, (a) draw yi ?",
        "Bernoulli(?)",
        "(b) draw zi ?",
        "Multinomial(?ui) if yi = 0 or zi ?",
        "Multinomial(?ti) if yi = 1 (c) for each word j = 1, .",
        ".",
        ".",
        ", Ni i. draw xi,j ?",
        "Bernoulli(?)",
        "ii.",
        "draw wi,j ?",
        "Multinomial(?B) if",
        "We use ?",
        "to denote the probability of choosing to talk about a global topic rather than a personal topic.",
        "Formally, the generation process is summarized in Figure 2.",
        "The model is also depicted in Figure 1(a).",
        "There are two degenerate variations of our model that we also consider in our experiments.",
        "The first one is depicted in Figure 1(b).",
        "In this model, we only consider the time-dependent topic distributions that capture the global topical trends.",
        "This model can be seen as a direct application of the model by Wang et al. (2007).",
        "The second one is depicted in Figure 1(c).",
        "In this model, we only consider the users?",
        "personal interests but not the global topical trends, and therefore temporal information is not used.",
        "We refer to our complete model as TimeUserLDA, the model in Figure 1(b) as TimeLDA and the model in Figure 1(c) asUserLDA.",
        "We also consider a standard LDA model in our experiments, where each word is associated with a hidden topic.",
        "Learning We use collapsed Gibbs sampling to obtain samples of the hidden variable assignment and to estimate the model parameters from these samples.",
        "Due to space limit, we only show the derived Gibbs sampling formulas as follows.",
        "First, for the i-th post, we know its publisher ui and timestamp ti.",
        "We can jointly sample yi and zi based on the values of all other hidden variables.",
        "Let us use y to denote the set of all hidden variables y and y?i to denote all y except yi.",
        "We use similar symbols for other variables.",
        "We then have",
        "where l = ui when p = 0 and l = ti when p =",
        "1.",
        "Here every M is a counter.",
        "Mpi(0) is the number of posts generated by personal interests, while Mpi(1) is the number of posts coming from global topical",
        "(c) is the number of posts by user ui and assigned to topic c, and Mui(?)",
        "is the total number of posts by ui.",
        "M ti(c) is the number of posts assigned to topic c at time point ti, and M ti(?)",
        "is the total number of posts at ti.",
        "E(v) is the number of times word v occurs in the i-th post and is labeled as a topic word, while E(?)",
        "is the total number of topic words in the i-th post.",
        "Here, topic words refer to words whose latent variable x equals 1.",
        "M c(v) is the number of times word v is assigned to topic c, and M c(?)",
        "is the total number of words assigned to topic c. All the counters M mentioned above are calculated with the i-th post excluded.",
        "We sample xi,j for each word wi,j in the i-th post",
        "(1) are counters to record the numbers of words assigned to the background model and any",
        "is the number of times word wi,j occurs as a background word.",
        "M zi(wi,j) counts the number of times word wi,j is assigned to topic zi, and M zi(?)",
        "is the total number of words assigned to topic zi.",
        "Again, all counters are calculated with the current word wi,j excluded."
      ]
    },
    {
      "heading": "3.3 Burst Detection",
      "text": [
        "Just like standard LDA, our topic model itself finds a set of topics represented by ?c but does not directly generate bursty topics.",
        "To identify bursty topics, we use the following mechanism, which is based on the idea by Kleinberg (2002) and Ihler et al. (2006).",
        "In our experiments, when we compare different models, we also use the same burst detection mechanism for other models.",
        "We assume that after topic modeling, for each discovered topic c, we can obtain a series of counts (mc1,mc2, .",
        ".",
        ".",
        ",mcT ) representing the intensity of the topic at different time points.",
        "For LDA, these are the numbers of words assigned to topic c. For TimeUserLDA, these are the numbers of posts which are in topic c and generated by the global topic distribution ?ti , i.e whose hidden variable yi is 1.",
        "For other models, these are the numbers of posts in topic c. We assume that these counts are generated by two Poisson distributions corresponding to a bursty state and a normal state, respectively.",
        "Let ?0 denote the expected count for the normal state and ?1 for the bursty state.",
        "Let vt denote the state for time point t, where vt = 0 indicates the normal state and vt = 1 indicates the bursty state.",
        "The probability of observing a count of mct is as follows:",
        "where l is either 0 or 1.",
        "The state sequence (v0, v1, .",
        ".",
        ".",
        ", vT ) is a Markov chain with the following transition probabilities:",
        "remove redundant bursty topics.",
        "where l is either 0 or 1.",
        "count over time.",
        "We set ?1 = 3?0.",
        "For transition probabilities, we empirically set ?0 = 0.9 and ?1 = 0.6 for all topics.",
        "We can use dynamic programming to uncover the underlying state sequence for a series of counts.",
        "Finally, a burst is marked by a consecutive subsequence of bursty states."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Data Set",
      "text": [
        "We use a Twitter data set to evaluate our models.",
        "The original data set contains 151,055 Twitter users based in Singapore and their tweets.",
        "These Twitter users were obtained by starting from a set of seed Singapore users who are active online and tracing",
        "Nov 29 vote, big, awards, (1) why didnt 2ne1 win this time!",
        "Mnet Asian bang, mama, win, (2) 2ne1.",
        "you deserved that urgh!",
        "Music Awards 2ne1, award, won (3) watching mama.",
        "whoohoo (MAMA) Oct 5 ?",
        "Oct 8 steve, jobs, apple, (1) breaking: apple says steve jobs has passed away!",
        "Steve Jobs iphone, rip, world, (2) google founders: steve jobs was an inspiration!",
        "death changed, 4s, siri (3) apple 4 life thankyousteve Nov 1 ?",
        "Nov 3 reservior, bedok, adlyn, (1) this adelyn totally disgust me.",
        "slap her mum?",
        "girl slapping slap, found, body, queen of cine?",
        "joke please can.",
        "mom mom, singapore, steven (2) she slapped her mum and boasted about it on fb (3) adelyn lives in woodlands , later she slap me how?",
        "Nov 5 reservior, bedok, adlyn, (1) bedok = bodies either drowned or killed.",
        "suicide near slap, found, body, (2) another body found, in bedok reservoir?",
        "bedok reservoir mom, singapore, steven (3) so many bodies found at bedok reservoir.",
        "alamak.",
        "Oct 23 man, arsenal, united, (1) damn you man city!",
        "we will get you next time!",
        "football game liverpool, chelsea, city, (2) wtf 90min goal!",
        "goal, game, match (3) 6-1 to city.",
        "unbelievable.",
        "their follower/followee links by two hops.",
        "Because this data set is huge, we randomly sampled 2892 users from this data set and extracted their tweets between September 1 and November 30, 2011 (91 days in total).",
        "We use one day as our time window.",
        "Therefore our timestamps range from 1 to 91.",
        "We then removed stop words and words containing non-standard characters.",
        "Tweets containing less than 3 words were also discarded.",
        "After preprocessing, we obtained the final data set with 3,967,927 tweets and 24,280,638 tokens."
      ]
    },
    {
      "heading": "4.2 Ground Truth Generation",
      "text": [
        "To compare our model with other alternative models, we perform both quantitative and qualitative evaluation.",
        "As we have explained in Section 3, each model gives us time series data for a number of topics, and by applying a Poisson-based state machine, we can obtain a set of bursty topics.",
        "For each method, we rank the obtained bursty topics by the number of tweets (or words in the case of the LDA model) assigned to the topics and take the top-30 bursty topics from each model.",
        "In the case of the LDA model, only 23 bursty topics were detected.",
        "We merged these topics and asked two human judges to judge their quality by assigning a score of either 0 or 1.",
        "The judges are graduate students living in Singapore and not involved in this project.",
        "The judges were given the bursty period and 100 randomly selected tweets for the given topic within that period for each bursty topic.",
        "They can consult external resources to help make judgment.",
        "A bursty topic was scored 1 if the 100 tweets coherently describe a bursty event based on the human judge's understanding.",
        "The inter-annotator agreement score is 0.649 using Cohen's kappa, showing substantial agreement.",
        "For ground truth, we consider a bursty topic to be correct if both human judges have scored it 1.",
        "Since some models gave redundant bursty topics, we also asked one of the judges to identify unique bursty",
        "topics from the ground truth bursty topics."
      ]
    },
    {
      "heading": "4.3 Evaluation",
      "text": [
        "In this section, we show the quantitative evaluation of the four models we consider, namely, LDA, TimeLDA, UserLDA and TimeUserLDA.",
        "For each model, we set the number of topics C to 80, ?",
        "to 50C and ?",
        "to 0.01 after some preliminary experiments.",
        "Each model was run for 500 iterations of Gibbs sampling.",
        "We take 40 samples with a gap of 5 iterations in the last 200 iterations to help us assign values to all the hidden variables.",
        "Table 1 shows the comparison between these models in terms of the precision of the top-K results.",
        "As we can see, our model outperforms all other models for K <= 20.",
        "For K = 30, the UserLDA model performs the best followed by our model.",
        "As we have pointed out, some of the bursty topics are redundant, i.e. they are about the same bursty event.",
        "We therefore also calculated precision at K for unique topics, where for redundant topics the one ranked the highest is scored 1 and the other ones are scored 0.",
        "The comparison of the performance is shown in Table 2.",
        "As we can see, in this case, our model outperforms other models with all K. We will further discuss redundant bursty topics in the next section."
      ]
    },
    {
      "heading": "4.4 Sample Results and Discussions",
      "text": [
        "In this section, we show some sample results from our experiments and discuss some case studies that illustrate the advantages of our model.",
        "First, we show the top-5 bursty topics discovered by the TimeUserLDA model in Table 3.",
        "As we can see, all these bursty topics are meaningful.",
        "Some of these events are global major events such as Steve Jobs?",
        "death, while some others are related to online events such as the scandal of a girl boasting about slapping her mother on Facebook.",
        "For comparison, we also show the top-5 bursty topics discovered by other models in Table 4.",
        "As we can see, some of them are not meaningful events while some of them are redundant.",
        "Next, we show two case studies to demonstrate the effectiveness of our model.",
        "Effectiveness of Temporal Models: Both TimeLDA and TimeUserLDA tend to group posts published on the same day into the same topic.",
        "We find that this can help separate bursty topics from general ones.",
        "An example is the topic on the Circle Line.",
        "The Circle Line is one of the subway lines of Singapore's mass transit system.",
        "There were a few incidents of delays or breakdowns during the period between September and November, 2011.",
        "We show the time series data of the topic related to the Circle Line of UserLDA, TimeLDA and TimeUserLDA in Figure 3.",
        "As we can see, the UserLDA model detects a much large volume of tweets related to this topic.",
        "A close inspection tells us that the topic under UserLDA is actually related to the subway systems in Singapore in general, which include a few other subway lines, and the Circle Line topic is merged with this general topic.",
        "On the other hand, TimeLDA and TimeUserLDA are both able to separate the Circle Line topic from the general subway topic because the Circle Line has several bursts.",
        "What is shown in Figure 3 for TimeLDA and TimeUserLDA is only the topic on the Circle Line, therefore the volume is much smaller.",
        "We can see that TimeLDA and TimeUserLDA show clearer bursty patterns than UserLDA for this topic.",
        "The bursts around day 20, day 44 and day 85 are all real events based on our ground truth.",
        "Effectiveness of User Models: We have stated that it is important to filter out users?",
        "?personal?",
        "posts in order to find meaningful global events.",
        "We find that our results also support this hypothesis.",
        "Let us look at the example of the topic on the Mnet Asian Music Awards, which is a major music award show that is held by Mnet Media annually.",
        "In 2011, this event took place in Singapore on November 29.",
        "Because Korean pop music is very popular in Singapore, many Twitter users often tweet about Korean pop music bands and singers in general.",
        "All our topic models give multiple topics related to Korean pop music, and many of them have a burst on November 29, 2011.",
        "Under the TimeLDA and UserLDA models, this leads to several redundant bursty topics for the MAMA event ranked within the top-30.",
        "For TimeUserLDA, however, although the MAMA event is also ranked the top, there is no redundant one within the top-30 results.",
        "We find that this is because with TimeUserLDA, we can remove tweets that are considered personal and therefore do not contribute to bursty topic ranking.",
        "We show the topic intensity of a topic about a Korean pop singer in",
        "of the topic on Steve Jobs?",
        "death under each model.",
        "We can see that because this topic is related to Korean pop music, it has a burst on day 90 (November 29).",
        "But if we consider the relative intensity of this burst compared with Steve Jobs?",
        "death, under TimeLDA and UserLDA, this topic is still strong but under TimeUserLDA its intensity can almost be ignored.",
        "This is why with TimeLDA and UserLDA this topic leads to a redundant burst within the top30 results but with TimeUserLDA the burst is not ranked high."
      ]
    },
    {
      "heading": "5 Conclusions",
      "text": [
        "In this paper, we studied the problem of finding bursty topics from the text streams on microblogs.",
        "Because existing work on burst detection from text streams may not be suitable for microblogs, we proposed a new topic model that considers both the temporal information of microblog posts and users?",
        "personal interests.",
        "We then applied a Poisson-based state machine to identify bursty periods from the topics discovered by our model.",
        "We compared our model with standard LDA as well as two degenerate variations of our model on a real Twitter dataset.",
        "Our quantitative evaluation showed that our model could more accurately detect unique bursty topics among the top ranked results.",
        "We also used two case studies to illustrate the effectiveness of the temporal factor and the user factor of our model.",
        "Our method currently can only detect bursty topics in a retrospective and offline manner.",
        "A more interesting and useful task is to detect realtime bursts in an online fashion.",
        "This is one of the directions we plan to study in the future.",
        "Another limitation of the current method is that the number of topics is predetermined.",
        "We also plan to look into methods that allow appearance and disappearance of topics along the timeline, such as the model by Ahmed and Xing (2010)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.",
        "We thank the reviewers for their valuable comments."
      ]
    }
  ]
}
