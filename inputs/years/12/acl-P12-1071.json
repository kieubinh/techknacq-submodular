{
  "info": {
    "authors": [
      "Zhenghua Li",
      "Ting Liu",
      "Wanxiang Che"
    ],
    "book": "ACL",
    "id": "acl-P12-1071",
    "title": "Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars",
    "url": "https://aclweb.org/anthology/P12-1071",
    "year": 2012
  },
  "references": [
    "acl-A00-2018",
    "acl-D07-1003",
    "acl-D08-1017",
    "acl-D08-1059",
    "acl-D08-1092",
    "acl-D09-1060",
    "acl-D09-1086",
    "acl-D09-1127",
    "acl-D11-1038",
    "acl-D11-1044",
    "acl-D11-1109",
    "acl-E06-1011",
    "acl-P05-1012",
    "acl-P05-1022",
    "acl-P08-1068",
    "acl-P08-1101",
    "acl-P08-1108",
    "acl-P09-1006",
    "acl-P09-1059",
    "acl-P10-1001",
    "acl-P10-1003",
    "acl-P11-1070",
    "acl-P11-1156",
    "acl-P11-2033",
    "acl-P99-1065",
    "acl-W00-1205",
    "acl-W02-1001",
    "acl-W06-3104",
    "acl-W09-1201",
    "acl-W09-1205",
    "acl-W09-1207",
    "acl-W09-1210",
    "acl-W10-2906"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for parsing.",
        "Several types of transformation patterns (TP) are designed to capture the systematic annotation inconsistencies among different treebanks.",
        "Based on such TPs, we design quasi-synchronous grammar features to augment the baseline parsing models.",
        "Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target treebanks (Penn Chinese Treebank 5.1 and 6.0) using the Chinese Dependency Treebank as the source treebank.",
        "The improvements are respectively 1.37% and 1.10% with automatic part-of-speech tags.",
        "Moreover, an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The scale of available labeled data significantly affects the performance of statistical data-driven models.",
        "As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem.",
        "However, the heavy cost of treebanking typically limits one single treebank in both scale and genre.",
        "At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1",
        "Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010).",
        "The existence of multiple monolingual treebanks opens another door for this issue.",
        "For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications.",
        "In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006).",
        "The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which we leave as future work.",
        "Despite the divergence of annotation philosophy, these treebanks contain rich human knowledge on the Chinese syntax, thereby having a great deal of common ground.",
        "Therefore, exploiting multiple treebanks is very attractive for boosting parsing accuracy.",
        "Figure 1 gives an example with different an",
        "trates that the two treebanks annotate coordination constructions differently.",
        "In CTB5, the last noun is the head, whereas the first noun is the head in CDT.",
        "One natural idea for multiple treebank exploitation is treebank conversion.",
        "First, the annotations in the source treebank are converted into the style of the target treebank.",
        "Then, both the converted treebank and the target treebank are combined.",
        "Finally, the combined treebank are used to train a better parser.",
        "However, the inconsistencies among different treebanks are normally nontrivial, which makes rule-based conversion infeasible.",
        "For example, a number of inconsistencies between CTB5 and CDT are lexicon-sensitive, that is, they adopt different annotations for some particular lexicons (or word senses).",
        "Niu et al. (2009) use sophisticated strategies to reduce the noises of the converted treebank after automatic treebank conversion.",
        "The present paper proposes a simple and effective framework for this problem.",
        "The proposed framework avoids directly addressing the difficult annotation transformation problem, but focuses on mod-eling the annotation inconsistencies using transformation patterns (TP).",
        "The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees.",
        "We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6).",
        "Results show that our approach can significantly boost state-of-the-art parsing accuracy.",
        "Moreover, an indirect comparison indicates that our ap2CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b).",
        "Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules.",
        "proach also outperforms the treebank conversion approach of Niu et al. (2009)."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "The present work is primarily inspired by Jiang et al.",
        "(2009) and Smith and Eisner (2009).",
        "Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People's Daily).",
        "Their framework is similar to ours.",
        "However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing.",
        "Smith and Eisner (2009) propose effective QG features for parser adaptation and projection.",
        "The first part of their work is closely connected with our work, but with a few important differences.",
        "First, they conduct simulated experiments on one treebank by manually creating a few trivial annotation inconsistencies based on two heuristic rules.",
        "They then focus on better adapting a parser to a new annotation style with few sentences of the target style.",
        "In contrast, we experiment with two real large-scale treebanks, and boost the state-of-the-art parsing accuracy using QG features.",
        "Second, we explore much richer QG features to fully exploit the knowledge of the source treebank.",
        "These features are tailored to the dependency parsing problem.",
        "In summary, the present work makes substantial progress in modeling structural annotation inconsistencies with QG features for parsing.",
        "Previous work on treebank conversion primarily focuses on converting one grammar formalism of a treebank into another and then conducting a study on the converted treebank (Collins et al., 1999; Xia et al., 2008).",
        "The work by Niu et al. (2009) is, to our knowledge, the only study to date that combines the converted treebank with the existing target treebank.",
        "They automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a statistical constituency parser trained on CTB5.",
        "Their experiments show that the combined treebank can significantly improve the performance of constituency parsers.",
        "However, their method requires several sophisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors.",
        "Instead of using the noisy converted treebank as additional training data, our approach allows the QG",
        "enhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust.",
        "Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two mainstream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008).",
        "However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies."
      ]
    },
    {
      "heading": "3 Dependency Parsing",
      "text": [
        "Given an input sentence x = w0w1...wn and its POS tag sequence t = t0t1...tn, the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h,m, l) : 0 ?",
        "h ?",
        "n, 0 < m ?",
        "n, l ?",
        "L}, where (h,m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and L is the label set.",
        "We omit the label l because we focus on unlabeled dependency parsing in the present paper.",
        "The artificial node w0, which always points to the root of the sentence, is used to simplify the formalizations.",
        "In the current research, we adopt the graph-based parsing models for their state-of-the-art performance in a variety of languages.3 Graph-based models view the problem as finding the highest scoring tree from a directed graph.",
        "To guarantee the efficiency of the decoding algorithms, the score of a dependency tree is factored into the scores of some small parts (subtrees).",
        "where p is a scoring part which contains one or more dependencies of d, and fbs(.)",
        "denotes the basic parsing features, as opposed to the QG features.",
        "Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices.",
        "We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features.",
        "3Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications.",
        "dependency sibling grandparent",
        "?",
        "The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3) parsing time.",
        "?",
        "The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3) parsing time.",
        "?",
        "The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010).",
        "The time complexity of the decoding algorithm is O(n4).4 For the O2 model, the score function is rewritten as:",
        "where fdep(.",
        "), fsib(.)",
        "and fgrd(.)",
        "correspond to the features for the three kinds of scoring parts.",
        "We adopt the standard features following Li et al. (2011).",
        "For the O1 and O2sib models, the above formula is modified by deactivating the extra parts."
      ]
    },
    {
      "heading": "4 Dependency Parsing with QG Features",
      "text": [
        "Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages.",
        "Given a source sentence x?",
        "and its syntactic tree d?, a QG defines a monolingual grammar that generates translations of x?, which can be denoted by p(x,d,a|x?,d?",
        "), where x and d refer to a translation and its parse, and a is a cross-language alignment.",
        "Under a QG, any portion of d can be aligned to any 4We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decoding procedure (Koo and Collins, 2010).",
        "Consistent: 55.4% Reverse: 8.6%Sibling: 10.0%Grand: 11.7% Reverse-grand: 1.4%( ', , )dep d h m?",
        "?",
        "target.",
        "A TP comprises two syntactic structures, one in the source side and the other in the target side, and denotes the process by which the left-side subtree is transformed into the right-side structure.",
        "Functions ?dep(.",
        "), ?sib(.",
        "), and ?grd(.)",
        "return the specific TP type for a candidate scoring part according to the source tree d?.",
        "portion of d?, and the construction of d can be inspired by arbitrary substructures of d?.",
        "To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011).",
        "In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks.",
        "The key idea is to let the parse tree of one style inspire the parsing process of another style.",
        "Different from a MT process, our problem considers one single sentence (x = x?",
        "), and the alignment a is trivial.",
        "Figure 3 shows the framework of our approach.",
        "First, we train a statistical parser on the source treebank, which is called the source parser.",
        "The source parser is then used to parse the whole target treebank.",
        "At this point, the target treebank contains two sets of annotations, one conforming to the source style, and the other conforming to the target style.",
        "During both the training and test phases, the target parser are inspired by the source annotations, and the score of a target dependency tree becomes",
        "The first part corresponds to the baseline model, whereas the second part is affected by the source tree d?",
        "and can be rewritten as Scoreqg(x, t,d?,d) = wqg ?",
        "fqg(x, t,d?,d) where fqg(.)",
        "denotes the QG features.",
        "We expect the QG features to encourage or penalize certain scoring parts in the target side according to the source tree d?.",
        "Taking Figure 1 as an example, suppose that the upper structure is the target.",
        "The target parser can raise the score of the candidate dependence ?and?",
        "?",
        "?industry?, because the depen",
        "dency also appears in the source structure, and evidence in the training data shows that both annotation styles handle conjunctions in the same manner.",
        "Similarly, the parser may add weight to ?trade?",
        "?",
        "?industry?, considering that the reverse arc is in the source structure.",
        "Therefore, the QG-enhanced model must learn the systematic consistencies and inconsistencies from the training data.",
        "To model such consistency or inconsistency sys-tematicness, we propose the use of TPs for encoding the structural correspondence between the source and target styles.",
        "Figure 4 presents the three kinds of TPs used in our model, which correspond to the three scoring parts of our parsing models.",
        "Dependency TPs shown in the first row consider how one dependency in the target side is transformed in the source annotations.",
        "We only consider the five cases shown in the figure.",
        "The percentages in the lower boxes refer to the proportion of the corresponding pattern, which are counted from the training data of the target treebank with source annotations T+S .",
        "We can see that the noisy source structures and the gold-standard target structures have 55.4% common dependencies.",
        "If the source structure does not belong to any of the listed five cases, ?dep(d?, h,m) returns ?else?",
        "(12.9%).",
        "We could consider more complex structures, such as h being the grand grand father of m, but statistics show that more complex transformations become very scarce in the training data.",
        "For the reason that dependency TPs can only model how one dependency in the target structure is transformed, we consider more complex transformations for the other two kinds of scoring parts of the target parser, i.e., the sibling and grand TPs shown in the bottom two rows.",
        "We only use high-frequency TPs of a proportion larger than 1.0%, aggregate others as ?else?, which leaves us with 21 sibling TPs and 22 grand TPs.",
        "Based on these TPs, we propose the QG features for enhancing the baseline parsing models, which are shown in Table 2.",
        "The type of the TP is conjoined with the related words and POS tags, such that the QG-enhanced parsing models can make more elaborate decisions based on the context.",
        "Then, the score contributed by the QG features can be redefined as",
        "which resembles the baseline model and can be naturally handled by the decoding algorithms."
      ]
    },
    {
      "heading": "5 Experiments and Analysis",
      "text": [
        "We use the CDT as the source treebank (Liu et al., 2006).",
        "CDT consists of 60,000 sentences from the People's Daily in 1990s.",
        "For the target treebank, we use two widely used versions of Penn Chinese Treebank, i.e., CTB5 and CTB6, which consist of Xinhua newswire, Hong Kong news and articles from Sinarama news magazine (Xue et al., 2005).",
        "To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5.",
        "CTB6 is used as the Chinese data set in the CoNLL 2009 shared task (Hajic?",
        "et al, 2009).",
        "Therefore, we adopt the same setting.",
        "CDT and CTB5/6 adopt different POS tag sets, and converting from one tag set to another is difficult (Niu et al., 2009).5 To overcome this problem, we use the People's Daily corpus (PD),6 a large-scale corpus annotated with word segmentation and POS tags, to train a statistical POS tagger.",
        "The tagger produces a universal layer of POS tags for both the source and target treebanks.",
        "Based on the common tags, the source parser projects the source annotations into the target treebanks.",
        "PD comprises approximately 300 thousand sentences of with approximately 7 million words from the first half of 1998 of People's Daily.",
        "Table 3 summarizes the data sets used in the present work.",
        "CTB5X is the same with CTB5 but follows the data split of Niu et al. (2009).",
        "We use",
        "?dep(d?, h,m) ?",
        "th ?",
        "tm ?sib(d?, h, s,m) ?",
        "th ?",
        "ts ?",
        "tm ?grd(d?, g, h,m) ?",
        "tg ?",
        "th ?",
        "tm ?dep(d?, h,m) ?",
        "wh ?",
        "tm ?sib(d?, h, s,m) ?",
        "wh ?",
        "ts ?",
        "tm ?grd(d?, g, h,m) ?",
        "wg ?",
        "th ?",
        "tm ?dep(d?, h,m) ?",
        "th ?",
        "wm ?sib(d?, h, s,m) ?",
        "th ?",
        "ws ?",
        "tm ?grd(d?, g, h,m) ?",
        "tg ?",
        "wh ?",
        "tm ?dep(d?, h,m) ?",
        "wh ?",
        "wm ?sib(d?, h, s,m) ?",
        "th ?",
        "ts ?",
        "wm ?grd(d?, g, h,m) ?",
        "tg ?",
        "th ?",
        "wm ?sib(d?, h, s,m) ?",
        "ts ?",
        "tm ?grd(d?, g, h,m) ?",
        "tg ?",
        "tm Table 2: QG features used to enhance the baseline parsing models.",
        "dir(h,m) denotes the direction of the dependency (h,m), whereas dist(h,m) is the distance |h ?m|.",
        "?dir(h,m) ?",
        "dist(h,m) indicates that the features listed in the corresponding column are also conjoined with dir(h,m) ?",
        "dist(h,m) to form new features.",
        "We adopt unlabeled attachment score (UAS) as the primary evaluation metric.",
        "We also use Root accuracy (RA) and complete match rate (CM) to give more insights.",
        "All metrics exclude punctuation.",
        "We adopt Dan Bikel's randomized parsing evaluation comparator for significance test (Noreen, 1989).7 For all models used in current work (POS tagging and parsing), we adopt averaged perceptron to train the feature weights (Collins, 2002).",
        "We train each model for 10 iterations and select the parameters that perform best on the development set."
      ]
    },
    {
      "heading": "5.1 Preliminaries",
      "text": [
        "This subsection describes how we project the source annotations into the target treebanks.",
        "First, we train a statistical POS tagger on the training set of PD, which we name TaggerPD .8 The tagging accuracy on the test set of PD is 98.30%.",
        "We then use TaggerPD to produce POS tags for all the treebanks (CDT, CTB5, and CTB6).",
        "Based on the common POS tags, we train a second-order source parser (O2) on CDT, denoted by ParserCDT .",
        "The UAS on CDT-test is 84.45%.",
        "We then use ParserCDT to parse CTB5 and CTB6.",
        "test with gold-standard POS tags.",
        "Li11 refers to the second-order graph-based model of Li et al. (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011).",
        "At this point, both CTB5 and CTB6 contain dependency structures conforming to the style of CDT."
      ]
    },
    {
      "heading": "5.2 CTB5 as the Target Treebank",
      "text": [
        "Table 4 shows the results when the gold-standard POS tags of CTB5 are adopted by the parsing models.",
        "We aim to analyze the efficacy of QG features under the ideal scenario wherein the parsing models suffer from no error propagation of POS tagging.",
        "We determine that our baseline O2 model achieves comparable accuracy with the state-of-the-art parsers.",
        "We also find that QG features can boost the parsing accuracy by a large margin when the baseline parser is weak (O1).",
        "The improvement shrinks for stronger baselines (O2sib and O2).",
        "This phenomenon is understandable.",
        "When gold-standard POS tags are available, the baseline features are very reliable and the QG features becomes less helpful for more complex models.",
        "The p-values in parentheses present the statistical significance of the improvements.",
        "We then turn to the more realistic scenario wherein the gold-standard POS tags of the target treebank are unavailable.",
        "We train a POS tagger on the training set of CTB5 to produce the automatic",
        "with automatic POS tags.",
        "POS tags for the development and test sets of CTB5.",
        "The tagging accuracy is 93.88% on the test set.",
        "The automatic POS tags of the training set are produced using 10-fold cross-validation.9 Table 5 shows the results.",
        "We find that QG features result in a surprisingly large improvement over the O1 baseline and can also boost the state-of-the-art parsing accuracy by a large margin.",
        "Li et al.",
        "(2011) show that a joint POS tagging and dependency parsing model can significantly improve parsing accuracy over a pipeline model.",
        "Our QG-enhanced parser outperforms their best joint model by 0.25%.",
        "Moreover, the QG features can be used to enhance a joint model and achieve higher accuracy, which we leave as future work."
      ]
    },
    {
      "heading": "5.3 Analysis Using Parser-O2 with AUTO-POS",
      "text": [
        "We then try to gain more insights into the effect of the QG features through detailed analysis.",
        "We select the state-of-the-art O2 parser and focus on the realistic scenario with automatic POS tags.",
        "Table 6 compares the efficacy of different feature sets.",
        "The first major row analyzes the efficacy of 9We could use the POS tags produced by TaggerPD in Section 5.1, which however would make it difficult to compare our results with previous ones.",
        "Moreover, inferior results may be gained due to the differences between CTB5 and PD in word segmentation standards and text sources.",
        "the basic features fbs(.)",
        "and the QG features fqg(.).",
        "When using the few QG features in Table 2, the accuracy is very close to that when using the basic features.",
        "Moreover, using both features generates a large improvement.",
        "The second major row compares the efficacy of the three kinds of QG features corresponding to the three types of scoring parts.",
        "We can see that the three feature sets are similarly effective and yield comparable accuracies.",
        "Combining these features generate an additional improvement of approximately 0.2%.",
        "These results again demonstrate that all the proposed QG features are effective.",
        "Figure 5 describes how the performance varies when the scale of CTB5 and CDT changes.",
        "In the left subfigure, the parsers are trained on part of the CTB5-train, and ?16?",
        "indicates the use of all the training instances.",
        "Meanwhile, the source parser ParserCDT is trained on the whole CDT-train.",
        "We can see that QG features render larger improvement when the target treebank is of smaller scale, which is quite reasonable.",
        "More importantly, the curves indicate that a QG-enhanced parser trained on a target treebank of 16,000 sentences may achieve comparable accuracy with a baseline parser trained on a treebank that is double the size (32,000), which is very encouraging.",
        "In the right subfigure, the target treebank is trained on the whole CTB5-train, whereas the source parser is trained on part of the CDT-train, and ?55.5?",
        "indicates the use of all.",
        "The curve clearly demonstrates that the QG features are more helpful when the source treebank gets larger, which can be explained as follows.",
        "A larger source treebank can teach a source parser of higher accuracy; then, the better source parser can parse the target treebank more reliably; and finally, the target parser can better learn the annotation divergences based on QG features.",
        "These results demonstrate the effectiveness and stability of our approach.",
        "Table 7 presents the detailed effect of the QG features on different dependency patterns.",
        "A pattern ?VV ?",
        "NN?",
        "refers to a right-directed dependency with the head tagged as ?VV?",
        "and the modifier tagged as ?NN?.",
        "whereas ???",
        "means left-directed.",
        "The ?w/o QG?",
        "column shows the number of the corresponding dependency pattern that appears in the gold-standard trees but misses in the results of the baseline parser, whereas the signed figures in the ?+QG?",
        "column are the changes made by the QG",
        "test when the scale of CDT and CTB5 varies (thousands in sentence number)."
      ]
    },
    {
      "heading": "5.4 CTB6 as the Target Treebank",
      "text": [
        "We use CTB6 as the target treebank to further verify the efficacy of our approach.",
        "Compared with CTB5, CTB6 is of larger scale and is converted into dependency structures according to finer-grained head-finding rules (Hajic?",
        "et al, 2009).",
        "We directly adopt the same transformation patterns and features tuned on CTB5.",
        "Table 8 shows results.",
        "The improvements are similar to those on CTB5, demonstrating that our approach is effective and robust.",
        "We list the top three systems of the CoNLL 2009 shared task in Table 8, showing that our approach also advances the state-of-the-art parsing accuracy on this data set.10 10We reproduce their UASs using the data released by the organizer: http://ufal.mff.cuni.cz/conll2009-st/results/ results.php.",
        "The parsing accuracies of the top systems may be underestimated since the accuracy of the provided POS tags in CoNLL 2009 is only 92.38% on the test set, while the POS tagger used in our experiments reaches 94.08%.",
        "Models without QG with QG",
        "set of CTB5X.",
        "Niu et al. (2009) use the maximum entropy inspired generative parser (GP) of Charniak (2000) as their constituent parser."
      ]
    },
    {
      "heading": "5.5 Comparison with Treebank Conversion",
      "text": [
        "As discussed in Section 2, Niu et al. (2009) automatically convert the dependency-structure CDT to the phrase-structure annotation style of CTB5X and use the converted treebank as additional labeled data.",
        "We convert their phrase-structure results on CTB5Xtest into dependency structures using the same head-finding rules.",
        "To compare with their results, we run our baseline and QG-enhanced O2 parsers on CTB5X.",
        "Table 9 presents the results.11 The indirect comparison indicates that our approach can achieve larger improvement than their treebank conversion based method."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "The current paper proposes a simple and effective framework for exploiting multiple large-scale treebanks of different annotation styles.",
        "We design rich TPs to model the annotation inconsistencies and consequently propose QG features based on these TPs.",
        "Extensive experiments show that our approach can effectively utilize the syntactic knowledge from another treebank and significantly improve the state-of-the-art parsing accuracy.",
        "11We thank the authors for sharing their results.",
        "Niu et al. (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing.",
        "They find a less improvement on F score with RP than with GP (0.9% vs. 1.1%).",
        "We refer to their Table 5 and 6 for details."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported by National Natural Science Foundation of China (NSFC) via grant 61133012, the National ?863?",
        "Major Projects via grant 2011AA01A207, and the National ?863?",
        "Leading Technology Research Project via grant 2012AA011102."
      ]
    }
  ]
}
