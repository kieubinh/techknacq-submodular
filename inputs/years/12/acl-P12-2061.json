{
  "info": {
    "authors": [
      "Isao Goto",
      "Masao Utiyama",
      "Eiichiro Sumita"
    ],
    "book": "ACL",
    "id": "acl-P12-2061",
    "title": "Post-ordering by Parsing for Japanese-English Statistical Machine Translation",
    "url": "https://aclweb.org/anthology/P12-2061",
    "year": 2012
  },
  "references": [
    "acl-C04-1073",
    "acl-C10-1051",
    "acl-D09-1105",
    "acl-D10-1092",
    "acl-D11-1018",
    "acl-J07-2003",
    "acl-J08-1002",
    "acl-J97-3002",
    "acl-N04-1035",
    "acl-N07-1051",
    "acl-N09-1029",
    "acl-N10-1127",
    "acl-P05-1066",
    "acl-P05-1067",
    "acl-P06-1077",
    "acl-P07-2045",
    "acl-P09-1063"
  ],
  "sections": [
    {
      "heading": "Eiichiro Sumita Abstract",
      "text": [
        "Reordering is a difficult task in translating between widely different languages such as Japanese and English.",
        "We employ the post-ordering framework proposed by (Sudoh et al., 2011b) for Japanese to English translation and improve upon the reordering method.",
        "The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT, while our method reorders the sequence by: 1) parsing the sequence to obtain syntax structures similar to a source language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English.",
        "Many reordering methods have been proposed in statistical machine translation (SMT) research.",
        "Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly.",
        "These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009).",
        "Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, 2011; Wu et al., 2011).",
        "First, these methods reorder the source language sentence into the target language word order.",
        "Then, they translate the reordered source word sequence using SMT methods.",
        "Type-3: Post-ordering (Sudoh et al., 2011b; Ma-tusov et al., 2005).",
        "First, these methods translate the source sentence almost monotonously into a sequence of the target language words.",
        "Then, they reorder the translated word sequence into the target language word order.",
        "This paper employs the post-ordering framework for Japanese-English translation based on the discussions given in Section 2, and improves upon the reordering method.",
        "Our method uses syntactic structures, which are essential for improving the target word order in translating long sentences between Japanese (a Subject-Object-Verb (SOV) language) and English (an SVO language).",
        "Before explaining our method, we explain the pre-ordering method for English to Japanese used in the post-ordering framework.",
        "In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011).",
        "The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g., phrases and clauses).",
        "This method first changes the English word order into a word order similar to Japanese word order using the head finalization rule.",
        "Then, it translates (almost monotonously) the pre-ordered",
        "English words into Japanese.",
        "There are two key reasons why this pre-ordering method works for estimating Japanese word order.",
        "The first reason is that Japanese is a typical head-final language.",
        "That is, a syntactic head word comes after nonhead (dependent) words.",
        "Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads.",
        "Consequently, the parsed English input sentences can be pre-ordered into a Japanese-like word order using the head finalization rule.",
        "Pre-ordering using the head finalization rule naturally cannot be applied to Japanese-English translation, because English is not a head-final language.",
        "If we want to pre-order Japanese sentences into an English-like word order, we therefore have to build complex rules (Sudoh et al., 2011b)."
      ]
    },
    {
      "heading": "2 Post-ordering for Japanese to English",
      "text": [
        "Sudoh et al. (2011b) proposed a post-ordering method for Japanese-English translation.",
        "The translation flow for the post-ordering method is shown in",
        "glish words in a Japanese-like structure.",
        "It can be constructed by applying the head-finalization rule (Isozaki et al., 2010b) to an English sentence parsed by Enju.",
        "Therefore, if good rules are applied to this HFE sentence, the underlying English sentence can be recovered.",
        "This is the key observation of the post-ordering method.",
        "The process of post-ordering translation consists of two steps.",
        "First, the Japanese input sentence is translated into HFE almost monotonously.",
        "Then, the word order of HFE is changed into an English word order.",
        "Training for the post-ordering method is conducted by first converting the English sentences in a Japanese-English parallel corpus into HFE sentences using the head-finalization rule.",
        "Next, a monotone phrase-based Japanese-HFE SMT model is built using the Japanese-HFE parallel corpus Japanese: kare wa kinou hon wo katta",
        "whose HFE was converted from English.",
        "Finally, an HFE-to-English word reordering model is built using the HFE-English parallel corpus."
      ]
    },
    {
      "heading": "3 Post-ordering Models",
      "text": []
    },
    {
      "heading": "3.1 SMT Model",
      "text": [
        "Sudoh et al. (2011b) have proposed using phrase-based SMT for converting HFE sentences into English sentences.",
        "The advantage of their method is that they can use off-the-shelf SMT techniques for post-ordering."
      ]
    },
    {
      "heading": "3.2 Parsing Model",
      "text": [
        "Our proposed model is called the parsing model.",
        "The translation process for the parsing model is shown in Figure 2.",
        "In this method, we first parse the HFE sentence into a binary tree.",
        "We then swap the nodes annotated with ?",
        "SW?",
        "suffixes in this binary tree in order to produce an English sentence.",
        "The structures of the HFE sentences, which are used for training our parsing model, can be obtained from the corresponding English sentences as follows.1 First, each English sentence in the training Japanese-English parallel corpus is parsed into a binary tree by applying Enju.",
        "Then, for each node in this English binary tree, the two children of each node are swapped if its first child is the head node (See (Isozaki et al., 2010b) for details of the head",
        "final rules).",
        "At the same time, these swapped nodes are annotated with ?",
        "SW?.",
        "When the two nodes are not swapped, they are annotated with ?",
        "ST?",
        "(indicating ?Straight?).",
        "A node with only one child is not annotated with either ?",
        "ST?",
        "or ?",
        "SW?.",
        "The result is an HFE sentence in a binary tree annotated with ?",
        "SW?",
        "and ?",
        "ST?",
        "suffixes.",
        "Observe that the HFE sentences can be regarded as binary trees annotated with syntax tags augmented with swap/straight suffixes.",
        "Therefore, the structures of these binary trees can be learnable by using an off-the-shelf grammar learning algorithm.",
        "The learned parsing model can be regarded as an ITG model (Wu, 1997) between the HFE and English sentences.",
        "2 In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures.",
        "The HFE sentences can be parsed by using the learned parsing model.",
        "Then the parsed structures can be converted into their corresponding English structures by swapping the ?",
        "SW?",
        "nodes.",
        "Note that this parsing model jointly learns how to parse and swap the HFE sentences."
      ]
    },
    {
      "heading": "4 Detailed Explanation of Our Method",
      "text": [
        "This section explains the proposed method, which is based on the post-ordering framework using the parsing model."
      ]
    },
    {
      "heading": "4.1 Translation Method",
      "text": [
        "First, we produce N-best HFE sentences using Japanese-to-HFE monotone phrase-based SMT.",
        "Next, we produce K-best parse trees for each HFE sentence by parsing, and produce English sentences by swapping any nodes annotated with ?",
        "SW?.",
        "Then we score the English sentences and select the English sentence with the highest score.",
        "For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence.",
        "2There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010).",
        "These methods are not post-ordering methods."
      ]
    },
    {
      "heading": "4.2 HFE and Articles",
      "text": [
        "This section describes the details of HFE sentences.",
        "In HFE sentences: 1) Heads are final except for coordination.",
        "2) Pseudo-particles are inserted after verb arguments: va0 (subject of sentence head), va1 (subject of verb), and va2 (object of verb).",
        "3) Articles (a, an, the) are dropped.",
        "In our method of HFE construction, unlike that used by (Sudoh et al., 2011b), plural nouns are left as-is instead of converted to the singular.",
        "Applying our parsing model to an HFE sentence produces an English sentence that does not have articles, but does have pseudo-particles.",
        "We removed the pseudo-particles from the reordered sentences before calculating the probabilities used for the scores of the reordered sentences.",
        "A reordered sentence without pseudo-particles is represented by E. A language model P (E) was trained from English sentences whose articles were dropped.",
        "In order to output a genuine English sentence E?",
        "fromE, articles must be inserted intoE.",
        "A language model trained using genuine English sentences is used for this purpose.",
        "We try to insert one of the articles {a, an, the} or no article for each word in E. Then we calculate the maximum probability word sequence through dynamic programming for obtaining E?."
      ]
    },
    {
      "heading": "5 Experiment",
      "text": []
    },
    {
      "heading": "5.1 Setup",
      "text": [
        "We used patent sentence data for the Japanese to English translation subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010).",
        "There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8.",
        "XML entities included in the data were decoded to UTF-8 characters before use.",
        "We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data.",
        "Mecab 3 v0.98 was used for the Japanese morphological analysis.",
        "The translation model was trained using sentences of 64 words or less from the training corpus as (Sudoh et al., 2011b).",
        "We used 5-gram language models using SRILM (Stolcke et al., 2011).",
        "We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to",
        "parse HFE.",
        "The parsing model was trained using 0.5 million sentences randomly selected from training sentences of 40 words or less.",
        "We used the phrase-based SMT system Moses (Koehn et al., 2007) to calculate the SMT score and to produce HFE sentences.",
        "The distortion limit was set to 0.",
        "We used 10-best Moses outputs and 10-best parsing results of Berkeley parser."
      ]
    },
    {
      "heading": "5.2 Compared Methods",
      "text": [
        "We used the following 5 comparison methods:",
        "ordering based on hierarchical phrase-based SMT (PO-HPBMT).",
        "We used Moses for these 5 systems.",
        "For PO-PBMT, a distortion limit 0 was used for the Japanese-to-HFE translation and a distortion limit 20 was used for the HFE-to-English translation.",
        "The PO-HPBMT method changes the post-ordering method of PO-PBMT from a phrase-based SMT to a hierarchical phrase-based SMT.",
        "We used a max-chart-span 15 for the hierarchical phrase-based SMT.",
        "We used distortion limits of 12 or 20 for PBMT and a max-chart-span 15 for HPBMT.",
        "The parameters for SMT were tuned by MERT using the first half of the development data with HFE converted from English."
      ]
    },
    {
      "heading": "5.3 Results and Discussion",
      "text": [
        "We evaluated translation quality based on the case-insensitive automatic evaluation scores of RIBES",
        "From the results, the proposed method achieved the best scores for both RIBES and BLEU for NTCIR-9 and NTCIR-8 test data.",
        "Since RIBES is sensitive to global word order and BLEU is sensitive to local word order, the effectiveness of the proposed method for both global and local reordering can be demonstrated through these comparisons.",
        "In order to investigate the effects of our post-ordering method in detail, we conducted an ?HFE-to-English reordering?",
        "experiment, which shows the main contribution of our post-ordering method in the framework of post-ordering SMT as compared with (Sudoh et al., 2011b).",
        "In this experiment, we changed the word order of the oracle-HFE sentences made from reference sentences into English, this is the same way as Table 4 in (Sudoh et al., 2011b).",
        "The results are shown in Table 2.",
        "This results show that our post-ordering method is more effective than PO-PBMT and PO-HPBMT.",
        "Since RIBES is based on the rank order correlation coefficient, these results show that the proposed method correctly recovered the word order of the English sentences.",
        "These high scores also indicate that the parsing results for high quality HFE are fairly trustworthy.",
        "In these experiments, we did not compare our method to pre-ordering methods.",
        "However, some groups used pre-ordering methods in the NTCIR-9 Japanese to English translation subtask.",
        "The NTT-UT (Sudoh et al., 2011a) and NAIST (Kondo et al., 2011) groups used pre-ordering methods, but could not produce RIBES and BLEU scores that both were better than those of the baseline results.",
        "In contrast, our method was able to do so."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "This paper has described a new post-ordering method.",
        "The proposed method parses sentences that consist of target language words in a source language word order, and does reordering by transferring the syntactic structures similar to the source language syntactic structures into the target language syntactic structures."
      ]
    }
  ]
}
