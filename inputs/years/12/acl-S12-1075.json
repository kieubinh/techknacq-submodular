{
  "info": {
    "authors": [
      "Jian Xu",
      "Qin Lu",
      "Zhengzhong Liu"
    ],
    "book": "SemEval",
    "id": "acl-S12-1075",
    "title": "PolyUCOMP: Combining Semantic Vectors with Skip bigrams for Semantic Textual Similarity",
    "url": "https://aclweb.org/anthology/S12-1075",
    "year": 2012
  },
  "references": [
    "acl-N04-3012",
    "acl-P04-1077",
    "acl-S12-1051",
    "acl-W99-0625"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents the work of the Hong"
      ]
    },
    {
      "heading": "Kong Polytechnic University (PolyUCOMP)",
      "text": [
        "team which has participated in the Semantic Textual Similarity task of SemEval-2012.",
        "The PolyUCOMP system combines semantic vectors with skip bigrams to determine sentence similarity.",
        "The semantic vector is used to compute similarities between sentence pairs using the lexical database WordNet and the Wikipedia corpus.",
        "The use of skip bigram is to introduce the order of words in measuring sentence similarity."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Sentence similarity computation plays an important role in text summarization, classification, question answering and social network applications (Lin and Pantel, 2001; Erkan and Radev, 2004; Ko et al., 2004; Ou et al., 2011).",
        "The SemEval 2012 competition includes a task targeted at Semantic Textual Similarity (STS) between sentence pairs (Eneko et al., 2012).",
        "Given a set of sentence pairs, participants are required to assign to each sentence pair a similarity score.",
        "Because a sentence has only a limited amount of content words, it is not easy to determine sentence similarities because of the sparseness issue.",
        "Hatzivassiloglou et al. (1999) proposed to use linguistic features as indicators of text similarity to address the problem of sparse representation of sentences.",
        "Mihalcea et al. (2006) measured sentence similarity using component words in sentences.",
        "Li et al. (2006) proposed to incorporate the semantic vector and word order to calculate sentence similarity.",
        "In our approach to the STS task, semantic vector is used and the semantic relatedness between words is derived from two sources: WordNet and Wikipedia.",
        "Because WordNet is limited in its coverage, Wikipedia is used as a candidate for determining word similarity.",
        "Word order, however, is not considered in semantic vector.",
        "As semantic information are coded in sentences according to its order of writing, and in our systems, content words may not be adjacent to each other, we proposed to use skip bigrams to represent the structure of sentences.",
        "Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a).",
        "Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams.",
        "It is reasonable to assume that similar sentences should have more overlapping skip bigrams, and the gaps in their shared skip bigrams should also be similar.",
        "The rest of this paper is organized as followed.",
        "Section 2 describes sentence similarity using semantic vectors and the order-sensitive skip bigrams.",
        "Section 3 gives the performance evaluation.",
        "Section 4 is the conclusion."
      ]
    },
    {
      "heading": "2 Similarity between Sentences",
      "text": [
        "Words are used to represent a sentence in the vector space model.",
        "Semantic vectors are constructed for sentence representations with each entry corresponding to a word.",
        "Since the semantic vector does not consider word order, we further proposed to use skip bigrams to represent sentence structure.",
        "Moreover, these skip bigrams are",
        "weighted by a decaying factor based on the so called skip distance in the sentence."
      ]
    },
    {
      "heading": "2.1 Sentence similarity using Semantic Vector",
      "text": [
        "Given a sentence pair, S1 and S2, for example, S1: Chairman Michael Powell and FCC colleagues at the Wednesday hearing.",
        "S2: FCC chief Michael Powell presides over hearing Monday.",
        "The term set of the vector space is first formed by taking only the content words in both sentences, T={chairman, chief, colleagues, fcc, hearing, michael, monday, powell, presides, wednesday } Each entry of the semantic vector corresponds to a word in the joint word set (Li et al., 2006).",
        "Then, the vector for each sentence is formed in two steps: For a word both in the term set T and in the sentence, the value for this word entry is set to 1.",
        "If a word is not in the sentence, the most similar word in the sentence will then be identified, and the corresponding path similarity value will be assigned to this entry.",
        "Let T be the term set with a sorted list of content words, T=(t1, t2,?, tn).",
        "Without loss of generality, let a sentence S=(w1 w2?wm) where wj is a content word and wj is a word in T. Let the vector space of the sentence S be VSs = (v1, v2, ?, vn).",
        "Then the value of vi is assigned as follows, where the similarity function SIM(ti, wj) is calculated according to the path measure (Pedersen et al., 2004) using the WordNet, formally defined as, ),(1),( jiji wtdistwtSIM ?",
        "where dist(ti, wj) is the shortest path from ti, to wj by counting nodes in the WordNet taxonomy.",
        "Based on this, the semantic vectors for the two example sentences will be,",
        "Based on the two semantic vectors, the cosine metric is used to measure sentence similarity.",
        "In the WordNet, the entry chairman in the joint set is most similar to the word chief in sentence S2.",
        "In practice, however, this entry might be closer to the word presides than to the word chief.",
        "Therefore, we try to obtain the semantic relatedness using the Wikipedia for sentence T and find that the entry chairman is closest to the word presides.",
        "The Wik-ipedia-based word relatedness utilizes the hyper-link structure (Milne & Witten, 2008).",
        "It first identifies the candidate articles, a and b, that discuss ti and wj respectively in this case and then compute relatedness between these articles,",
        "where A and B are sets of articles that link to a and b. W is the set of all articles in the Wikipedia.",
        "Finally, two articles that represent ti and wj are selected and their relatedness score is assigned to SIM(ti, wj)."
      ]
    },
    {
      "heading": "2.2 Sentence Similarity by Skip bigrams",
      "text": [
        "Skip bigrams are pairs of words in a sentence order with arbitrary gaps.",
        "They contain the order-sensitive information between two words.",
        "The skip bigrams of a sentence are extracted as features which will be stacked in a vector space.",
        "Each skip bigram is weighted by a decaying factor with its skip distances in the sentence.",
        "To illustrate this, consider the following sentences S and T:",
        "where w denotes a word.",
        "It can be used more than once in a sentence.",
        "Each sentence above has a C(5, 2) 1 = 10 skip bigrams.",
        "The sentence S has the following skip bigrams: ?w1w2?, ?w1w1?, ?w1w3?, ?w1w4?, ?w2w1?, ?w2w3?",
        ", ?w2w4?",
        ", ?w1w3?, ?w1w4?, ?w3w4?",
        "The sentence T has the following skip bigrams: ?",
        "2w1?, ?w2w4?, ?w2w5?, ?w2w4?, ?w1w4?, ?w1w5?",
        ", ?w1w4?",
        ", ?w4w5?, ?w4w4?, ?w5w4?",
        "In the sentence S, we have two repeated skip bigrams ?w1w4?",
        "and ?w1w3?.",
        "In the sentence T, we have ?w2w4?",
        "and ?w1w4?",
        "repeated twice.",
        "In this case, the weight of the recurring skip bigrams will be increased.",
        "Hereafter, vectors for S and T will be",
        "1 Combination: C(5,2)=5!/(2!*3!",
        ")=10.",
        "formulated with each entry corresponding to a distinctive skip bigram.",
        "Now, the question remains how to weight the skip bigrams.",
        "Given?",
        "as a finite word set, let S=w1w2?w|S |be a sentence, wi?",
        "?and 1?i?|S|.",
        "A skip bigram of S, denoted by u, is defined by an index set I=(i1, i2) of S (1?i1<i2?|S |and u=S[I]).",
        "The skip distance of S[I] , denoted by du (I), is the skip distance of the first word and the second word of u, calculated by i2-i1+1.",
        "For example, if S is the sentence of w1w2w1w3w4 and u = w1w4, then there are two index sets, I1=[3,5] and I2=[1,5] such that u=S[3,5] and u=S[1,5], and the skip distances of S[3,5] and S[1,5] are 3 and 5.",
        "The weight of a skip bigram u for a sentence S with all its possible occurrences, denoted by ( )u S?",
        ", is defined as:",
        "where ?",
        "is the decay factor which penalizes the longer skip distance of a skip bigram.",
        "By doing so, for the sentence S, the complete word set is ?={w1, w2, w3,w4}.",
        "The weights for the skip bigrams are listed in Table 1:",
        "In Table 1, if ?",
        "is set to 0.25, the weight of the skip bigram w1w2 in S is 0.25 2=0.0625, and w1w3 is 0.254 +0.252=0.064.",
        "Similarly, the skip bigrams and weights in the sentence T can be obtained.",
        "With the skip bigram-based vectors, cosine metric is then used to compute similarity between S and T."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": [
        "In the STS task, three training datasets are available: MSR-Paraphrase, MSR-Video and SMTeuroparl (Eneko et al., 2012).",
        "The number of sentence pairs for three dataset is 750, 750 and 734.",
        "In the following experiments, Let SWN, SWIKI and SSKIP denote similarity measures of the vector space representation using WordNet, Wikipedia and skip bigrams, respectively.",
        "The three similarity measures are linearly combined as SCOMB: SKIPWIKIWNCOMB SSSS ????????",
        ")1( ????",
        "where ?",
        "and ?",
        "are weight factors for SWN and SWIKI in the range [0,1].",
        "If ?",
        "is set to 1, only the WordNet-based similarity measure is used; if ?",
        "is 0, the Wikipedia and skip bigram measures are used.",
        "Because each dataset has a different representation for sentences, the parameter configurations for them are different.",
        "For the word similarity using the lexical resource WordNet, the path measure is used in experiments.",
        "To get word relatedness from the English Wikipedia, the Wikipedia Miner tool2 is used.",
        "When computing sentence similarity based on the skip bigrams, the decaying factor (DF) must be specified beforehand.",
        "Hence, parameter configurations for the three datasets are listed in Table 2:",
        "In the testing phase, five testing dataset are provided.",
        "In addition to three test datasets drawn from the publicly available datasets used in the training phase, two surprise datasets are given.",
        "They are SMTnews and OnWN (Eneko et al., 2012).",
        "SMTnews has 399 pairs of sentences and OnWN contains 750 sentence pairs.",
        "The parameter configurations for these two surprise datasets are the same as those for the dataset MSR-Paraphrase.",
        "The official scoring is based on Pearson correlation.",
        "If the system gives the similarity scores close to the reference answers, the system will attain a high correlation value.",
        "Besides, three other evaluation metrics (ALL, ALLnrm, Mean) based on the Pearson correlation are used (Eneko et al., 2012).",
        "Among the 89 submitted systems, the results of our system are given in Table 3:",
        "Using the ALL metric, our system ranks 31, but for ALLnrm and Mean metrics, our system ranking is decreased to 59 and 51.",
        "In terms of ALL metric, our system achieves a medium performance, implying that our system correlates well with human assessments.",
        "In terms of ALLnrm and Mean metrics, our system performance degrades a lot, implying that our system is not well correlated with the reference answer when each dataset is normalized into the aggregated dataset using the least square error or the weighted mean across the datasets.",
        "To see how well each of the individual vector space models performed on the evaluation sets, we experiment on the five datasets using vectors based on WordNet, Wikipedia (Wiki), SkipBigram and",
        "representations, each dataset obtains the best performance.",
        "The WordNet-based approach gives a better performance than Wikipedia-based approach in MSRvid dataset.",
        "The two approaches, however, give similar performance in other four datasets.",
        "This is because the sentences in the MSRvid dataset are too short with limited amount of content words.",
        "It is difficult to capture the meaning of a sentence without distinguishing words in consecutive positions.",
        "This is why the order-sensitive SkipBigram approach gives better performance than the other two approaches.",
        "For example, A woman is playing a game with a man.",
        "A man is playing piano.",
        "Using the semantic vectors, we will get high similarity scores, but the two sentences are dissimilar.",
        "If the skip bigram approach is used, the similarity score between sentences will be 0, which correlates with human judgment.",
        "In parameter configurations for the MSRvid dataset, higher weight (1-0.123-0.01=0.867) is also given to skip bigrams.",
        "It is interesting to note that the decaying factor for this dataset is 1.4 and is not in the range from 0 to 1 inclusive.",
        "This is because higher decaying factor helps to capture semantic meaning between words that span afar.",
        "For example, A man is playing a flute.",
        "A man is playing a bamboo flute.",
        "In this sentence pair, the second sentence is entailed by the first one.",
        "The similarity can be captured by assigned larger decay factor to weigh the skip bigram ?playing flute?",
        "in two sentences.",
        "Hence, if the value of the decay factor is greater than 1, the two sentences will become much more similar.",
        "After careful investigation, these two sentences are similar to a large extent.",
        "In this sense, a higher decaying factor would help capture the meaning between sentence pairs.",
        "This is quite different from the other four datasets which focus on shared skip bigrams with smaller decaying factor."
      ]
    },
    {
      "heading": "4 Conclusions and Future Work",
      "text": [
        "In the Semantic Textual Similarity task of SemEval-2012, we proposed to combine the semantic vector with the order-sensitive skip bigrams to capture the meaning between sentences.",
        "First, a semantic vector is derived from either the WordNet or Wikipedia.",
        "The WordNet simulates the common human knowledge about word concepts.",
        "However, WordNet is limited in its word coverage.",
        "To remedy this, Wikipedia is used to obtain the semantic relatedness between words.",
        "Second, the proposed approach also considers the impact of word order in sentence similarity by using skip bigrams.",
        "Finally, the overall sentence similarity is defined as a linear combination of the three similarity metrics.",
        "However, our system is limited in its approaches.",
        "In future work, we would like to apply machine learning approach in determining sentence similarity."
      ]
    }
  ]
}
