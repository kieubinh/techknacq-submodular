{
  "info": {
    "authors": [
      "Demetrios Glinos"
    ],
    "book": "SemEval",
    "id": "acl-S12-1079",
    "title": "ATA-Sem: Chunk-based Determination of Semantic Text Similarity",
    "url": "https://aclweb.org/anthology/S12-1079",
    "year": 2012
  },
  "references": [],
  "sections": [
    {
      "text": [
        "standard training data using the CRF++ toolkit using the same feature set described above.",
        "4 Rule-based Chunk Method The rule-based chunk similarity method employs a breadth-first search method for selecting candidate chunks for further comparison.",
        "The algorithm operates by selecting the first chunk of the sentence with the larger number of chunks.",
        "It then marches down each chunk of the shorter sentence looking for an exact term match or head term synonym match.",
        "If a match is found, a chunk-level score value of 3 is assigned, and the next chunk in the longer sentence is considered.",
        "If a match is not found, then a new search is performed, this time searching for a hypernym match.",
        "If a match is found in this second pass, a chunk score of 2 is assigned, and the next index chunk is considered.",
        "If not, then a third and final pass is performed searching for a related term match.",
        "If a match is found after this third pass, a chunk score of 1 is assigned; otherwise, the chunks are deemed dissimilar and receive a chunk score of zero.",
        "We describe this algorithm as ?breadth-first?",
        "because it has the effect of conducting up to three passes across all of the chunks of the target (short-er) sentence, looking for successively ?looser?",
        "matches.",
        "For these purposes, we consider a hyper-nym match to be looser than an exact or synynym match, and a common-ancestor (related) term match to be looser than a hypernym match.",
        "The chunk-level matching scores are accumulated in the above manner, just as for the corpus-based method.",
        "However, in this case, the results are used directly by the rule-based scoring algo-rithm.",
        "The scoring algorithm treats predicate and argument chunks separately and generates raw scores for each.",
        "It then combines them to compute the final similarity score.",
        "The predicate raw score is the accumulated score for all VP chunk compari-sons, divided by three times the number of such comparisons.",
        "This results in a predicate raw score that is in the range [0,1], since the maximum chunk-level matching score is three.",
        "The argument raw score is produced in the same manner and multiplied by 5.0, producing a value in the range [0,5].",
        "Where both predicate and argument raw scores exist, the total similarity score for the sentence pair is computed as the product of the two raw scores.",
        "This formulation has the benefit of permitting the degree of similarity for each score type to affect the overall score.",
        "For example, consider ?Sarah bought the book,?",
        "and ?Sarah read the book.?",
        "Here, the difference in predicate (?bought?",
        "versus ?read?)",
        "will temper the otherwise exact match on the arguments.",
        "Similarly, for ?Sarah bought the book,?",
        "and ?Sarah bought the fish,?",
        "the inexact match on arguments will soften the perfect predicate score.",
        "Table 3 illustrates how the basic rule-based algorithm works.",
        "The table shows the associated chunks from each sentence, their chunk type for scoring purposes, and their chunk-level matching score values.",
        "Thus, for example, ?The Korean Air deal?",
        "and ?the final agreement?",
        "have a matching score value of 2 because ?agreement?",
        "is a hyper-nym of ?deal?.",
        "Moreover, because there is no mention of ?Bob Saling?",
        "in the first sentence, the corresponding matching value is zero.",
        "Based on the chunk-level scores in the table, the similarity score is calculated as follows.",
        "The raw predicate score for the two predicate chunk pairs is 6 (3 for each, from the table), divided by the max-S1: Boeing said the final agreement is expected to be signed during the next few weeks.",
        "S2: The Korean Air deal is expected to be finalized ?in the next several weeks,?",
        "Boeing spokesman Bob Saling said.",
        "S2 chunk phrase S1 chunk phrase Chunk type Matching score The Korean Air deal the final agreement argument 2 is expected to be finalized is expected to be signed predicate 3 in the next several weeks during the next several weeks argument 3 Boeing spokesman Boeing argument 3 Bob Saling argument 0 said said predicate 3 Table 3.",
        "Chunk-level matching scores for rule-based scoring example from training data.",
        "imum possible score, which is also 6, yielding a value of 1.000.",
        "The argument raw score is the sum of the scores for the argument pairs, 8 in this case, divided by the maximum possible (12 for the four argument chunks), scaled by 5, yielding a value of 3.333.",
        "The final score is their product, 3.333, which compares favorably with the gold standard score value of 3.000 for this sentence pair.",
        "If there are no predicate chunk comparisons for the sentence pair, the rule-based scoring algorithm uses the raw argument score without modification.",
        "Similarly, where there are no argument chunk comparisons, the rule uses the raw predicate score multiplied by 5.0 to scale it to cover the range [0,5].",
        "By being robust against zero values in this manner, the algorithm is able to handle comparisons of sentence fragments such as ?Tunisia?, in the event it is the entirety of the input ?sentence?.",
        "Additionally, the final score that is reported is the minimum of the combined score described above and an upper limit value that is initialized at 5.0, but which can be reduced as each chunk-level comparison is performed.",
        "The upper limit value is reduced to 4.0 if there is a qualifier mismatch (e.g., ?uncooked pizza?",
        "v.",
        "?pizza?).",
        "It is reduced to 3.0 if there is a number mismatch, for example, ?Two men are playing chess?",
        "versus ?Three men are playing chess.?",
        "5 Results and Discussion Table 4 shows the results for both algorithms against the STS test suite.",
        "The ?Corpus-based?",
        "and ?Rule-based?",
        "columns reports results for the two chunk-based similarity algorithm.",
        "The five lowest rows represent the five individual data sets in the suite.",
        "The values in the table represent Pearson correlation values, which range from 1 to +1, where the closer a value is to 1, the stronger the positive correlation.",
        "The three upper rows represent the three metrics that were used to compute global results across all of the data sets.",
        "?All?",
        "refers to the computation of a Pearson value where the five gold standards and corresponding results were concatenated.",
        "The ?Allnrm?",
        "row reports correlation values obtained by scaling and translating system outputs in a manner that maintains the individual data set correlation values, yet minimizes the combined data set error.",
        "Finally, the ?Mean?",
        "reports the weighted average of the individual data set correlation values, where the weights used were the numbers of sentence pairs in each data set.",
        "There were 750 sentence pairs in each of the MSRpar, MSRvid, and OnWN data sets, but only 459 in the SMT-eur data set and 399 in the SMT-news data set, for a total of 3108 sentence pairs.",
        "The characteristics of the different data sets and greater detail on the global scoring metrics are discussed further in the STS task description paper (Agirre, et al., 2012).",
        "Category Corpus-based Rule-based Improve ment (%) All .4976 .5306 6.63% Allnrm .7160 .7646 6.79% Mean .3215 .5069 57.67% MSRpar .2312 .4536 96.20% MSRvid .6595 .7079 7.33% SMT-eur .1504 .3996 165.68% On-WN .2735 .5149 88.26% SMT-news .1426 .3379 136.98% Table 4.",
        "Results against STS test suite.1 As Table 4 shows, the rule-based method outperformed the corpus-based method for all individual data sets and for all combined measures.",
        "The percentage improvement is noted in the rightmost column in the figure.",
        "We believe the results for the rule-based method are sufficient to show that chunk-based methods may have a role to play in text similarity determi-nations, particularly in high volume applications where high throughput is essential.",
        "Chunking is computationally cheap to perform.",
        "It is also robust against sentence fragments and against incomplete or ungrammatical sentence constructions, as may be found in emails, text messages, and blog posts.",
        "However, chunk-based methods may be restricted to such applications since, on an absolute scale, performance was in the bottom one-third of all systems that reported results against the STS data suite.",
        "Nevertheless, we recognize that our investigations into chunk-based methods were limited in both time and scope.",
        "As a result, we do not believe we have yet encountered the upper limit on performance for chunk-based text similarity systems.",
        "1 The results for the corpus-based chunk method are reported under the name ?demetrios_glinos/task6-ATA-CHNK?",
        "on the official STS results page, http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=results-update."
      ]
    },
    {
      "heading": "References Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012). Alberto Barr?n-Cede?o, Andreas Eiselt, and Paolo Ros-so. 2009. Monolingual Text Similarity Measures: A Comparison of Models over Wikipedia Articles Revisions. In Proceedings of the ICON-2009: 7th International Conference on Natural Language Processing, pp. 29-38. Moses S. Charikar. 2002. Similarity Estimation Techniques from Rounding Algorithms. In STOC '02 Proceedings of the thirty-fourth annual ACM symposium on theory of computing. Taku-ku. 2012. CRF++: Yet Another CRF toolkit. http://crfpp.googlecode.com/svn/trunk/doc/index.html. David K. Evans, Kathleen McKeown, and Judith L. Klavans. 2005. Similarity-based Multilingual Multi-Document Summarization. IEEE Transactions on Information Theory. Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press. Ryan Moulton. 2010. Simple Simhashing: Clustering in linear time. [Internet]. Version 1. Ryan Moulton's Articles. Available from: http://moultano.wordpress. com/article/simple-simhashing-3kbzhsxyg4467-6/. NLM. 2012. U.S. National Library of Medicine, Lexical Systems Group, Lexical Tools. http://lexsrv3.nlm. nih.gov/LexSysGroup/Projects/lvg/current/web/index.html.",
      "text": []
    }
  ]
}
