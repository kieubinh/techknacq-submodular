{
  "info": {
    "authors": [
      "MariÃ«lle Leijten",
      "Lieve Macken",
      "Veronique Hoste",
      "Eric Van Horenbeeck",
      "Luuk Van Waes"
    ],
    "book": "Proceedings of the Second Workshop on Computational Linguistics and Writing (CLW 2012): Linguistic and Cognitive Aspects of Document Creation and Document Engineering",
    "id": "acl-W12-0301",
    "title": "From Character to Word Level: Enabling the Linguistic Analyses of Inputlog Process Data",
    "url": "https://aclweb.org/anthology/W12-0301",
    "year": 2012
  },
  "references": [],
  "sections": [
    {
      "text": [
        "and the within-word typing errors are excluded from further analyses.",
        "Although the set-up of the Inputlog extension is largely language-independent, the NLP tools used are language-dependent.",
        "As proof-of-concept, we provide evidence from English and Dutch (See Figure 3).",
        "Figure 3 Flow of the linguistic analyses.",
        "4.3 Step 3 ?",
        "enriching process data with linguistic information As standard NLP tools are trained on clean data, these tools are not suited for processing input containing spelling errors.",
        "Therefore, we only enrich the final product data and the deleted fragments with different kinds of linguistic annotations.",
        "As part-of-speech taggers typically use the surrounding local context to determine the proper part-of-speech tag for a given word (typically a window of two to three words and/or tags is used), the deletions in context are extracted from the S-notation to be processed by the part-of-speech tagger.",
        "The deleted fragments in context consist of the whole text string without the insertions and are only used to optimize the results of the linguistic annotation.",
        "(4 - deleted fragments in context) Volgend?jaar?organiseert?{#}VWEC?een?{boeiend?}congres?[over?'][met?als?thema]{over}?'Corporate?Communication{'}.[.][?Wat?levert?het?op?'.]?Blijf?[ons?volgen?op]{op?de?hoogte?via|}?www.vwec2012.be.|?",
        "For the shallow linguistic analysis, we used the LT3 shallow parsing tools suite consisting of: ?",
        "a part-of-speech tagger (LeTsTAG), ?",
        "a lemmatizer (LeTsLEMM), and ?",
        "a chunker (LeTsCHUNK).",
        "The LT3 tools are platform-independent and hence run on Windows.",
        "Part of speech tags The English PoS tagger uses the Penn Treebank tag set, which contains 45 distinct tags.",
        "The Dutch part-of-speech tagger uses the CGN tag set codes (Van Eynde, Zavrel, & Daelemans, 2000), which is characterized by a high level of granularity.",
        "Apart from the word class, the CGN tag set codes a wide range of morpho-syntactic features as attributes to the word class.",
        "In total, 316 distinct tags are discerned.",
        "Lemmata During lemmatization, for each orthographic token, the base form (lemma) is generated.",
        "For verbs, the base form is the infinitive; for most other words, this base form is the stem, i.e., the word form without inflectional affixes.",
        "The lemmatizers make use of the predicted PoS codes to disambiguate ambiguous word forms, e.g., Dutch ?landen?",
        "can be an infinitive (base form ?landen?)",
        "or plural form of a noun (base form ?land?).",
        "The lemmatizers were trained on the English and Dutch parts of the Celex lexical database respectively (Baayen, Piepenbrock, & van Rijn, 1993).",
        "Chunks During text chunking syntactically related consecutive words are combined into non-overlapping, non-recursive chunks on the basis of a fairly superficial analysis.",
        "The chunks are represented by means of IOB-tags.",
        "In the IOB-tagging scheme, each token belongs to one of the following three types: I (inside), O (outside) and B (begin); the Ben I-tags are followed by the chunk type, e.g., B-VP, I-VP.",
        "We adapted the IOB-tagging scheme and added end tag (E) to explicitly mark the end of a chunk.",
        "Accuracy sores of part-of-speech taggers and lemmatizers typically fluctuate around 97% to 98%; accuracy scores of 95% to 96% are obtained for chunking.",
        "After annotation, the final writing product, deleted fragments, and word-level corrections are aligned and the indices are restored.",
        "Figures 4 and 5 show how we enriched the logged process data with different kinds of linguistic information: lemmata, part-of-speech tags, and chunk boundaries.",
        "We further added some word-level annotations on the final writing product and the deletions,",
        "lemmata.",
        "The Wikipedia frequency lists can thus group different word forms belonging to one lemma.",
        "The current version of the Dutch frequency list has been compiled on the basis of nearly 100 million tokens coming from 395,673 Wikipedia pages, which is almost half of the Dutch Wikipedia dump of December 2011.",
        "Frequencies are presented as absolute frequencies.",
        "4.4 Step 4 - combining process data with linguistic information In a final step we combine the process data with the linguistic information.",
        "Based on the time information provided by Inputlog, researchers can calculate various measures, e.g., length of a pause within, before and after lemmata, part-of-speech tags, and at chunk boundaries.",
        "As an example Table 1 shows the mean pausing time before and after the adjectives and nouns in the tweet.",
        "Of course, this is a very small-scale example, but it shows the possibilities of exploring writing process data from a linguistic perspective.",
        "mean pause before mean pause after mean pause within ADJ 1880 671 148 NOUN 728 1455 232 B (begin) 1412 1174 164 E (end) 685 1353 148 I (inside) 730 1034 144 Table 1.",
        "Example of process data and linguistic information In this example the mean pausing time before adjectives is twice as long as before nouns.",
        "The pausing time after such a segment shows the opposite proportion.",
        "Also pauses in the beginning of chunks are more than twice as long as in the middle of a chunk.",
        "5 Future research In this paper we presented how writing process data can be enriched with linguistic information.",
        "The annotated output facilitates the linguistic analysis of the logged data and provides a valuable basis for more linguistically-oriented writing process research.",
        "We hope that this perspective will further enrich writing process research."
      ]
    },
    {
      "heading": "5.1 Additional annotations and analyses In a first phase we only focused on English and Dutch, but the method can be easily applied to other languages as well provided that the linguistic tools are available for a Windows platform. For the moment, the linguistic annotations are limited to part-of-speech tags, lemmata, chunk information, syllabification, and word frequency information, but can be extended, e.g., by n-gram frequencies to capture collocations. By aggregating the logged process data from the character level (keystroke) to the word level, general statistics (e.g., total number of deleted or inserted words, pause length before nouns preceded by an adjective or not) can be generated easily from the output of Inputlog as well. 5.2 Technical flow of Inputlog & linguistic tools At this point Inputlog is a standalone program that needs to be installed on the same local machine that is used to produce the texts. This makes sense as long as the heaviest part of the work is the logging of a writing process. However, extending the scope from a character based analysis device to a system that supplements fine-grained production and process information to various NLP tools is a compelling reason to rethink the overall architecture of the software. It is not feasible to install the necessary linguistic software with its accompanying databases on every device. By decoupling the capturing part from the analytics a research group will have a better view on the use of its hard-and software resources while also allowing to solve potential copyright issues. Inputlog is now pragmatically Windows-based, but with the new architecture any tool on any OS will be capable to exchange data and results. It will be possible to add an NLP module that receives Inputlog data through a communication layer. A workflow procedure then presents the data in order to the different NLP packages and collects the final output. Because all data traffic is done with XML files, cooperation between software with different creeds becomes conceivable. Finally, the module has an administration utility handling the necessary user authentication and permits.",
      "text": []
    }
  ]
}
