{
  "info": {
    "authors": [
      "Christine Howes",
      "Matthew Purver",
      "Rose McCabe",
      "Patrick G. T. Healey",
      "Mary Lavelle"
    ],
    "book": "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    "id": "acl-W12-1610",
    "title": "Predicting Adherence to Treatment for Schizophrenia from Dialogue Transcripts",
    "url": "https://aclweb.org/anthology/W12-1610",
    "year": 2012
  },
  "references": [],
  "sections": [
    {
      "text": [
        "sults as reported.",
        "3Bi- and tri-gram features were not extracted from this data because of the small amount of data available which we felt would result in models that suffered from overfitting (note that the same concern holds for the unigram features).",
        "4Classifiers were trained on 80% and tested on 20% of the sample, with this was repeated 5 times over each possible 80/20 combination so as to test the whole dataset.",
        "random distribution and the majority class distribution are shown marked with *.",
        "Results show good performance for all experiments when including lexical features, with all factors being predictable with around 90% accuracy with the exception of PEQ communication at just below 80%.",
        "However, using high-level features alone gives negligible performance, except for a small benefit on the PANSS negative and positive symptom measures, though contrary to our hypotheses the most important high-level features were OCR-PerWord by the doctor (negative) and WhWords by an other participant (positive).",
        "Examination of the most predictive unigrams shows that sets selected for different outcome measures are different: for example, the 54 features selected for adherence and the 73 selected for PEQ overall have only 1 word in common (?mates?).",
        "Adherence-related words include words related to conditions, treatment and medication (?schizophrenic?, ?sickness?, ?symptoms?, ?worse?, ?pains?, ?flashbacks?, ?sodium?, ?chemical?, ?monthly?",
        "); PEQ-related words include those related to personal life (?sundays?, ?thursdays?, ?television?, ?sofa?, ?wine?, ?personally?, ?played?",
        "), and filled pauses (?eerrmm?, ?uhhm?)",
        "?",
        "although more investigation is required to draw any firm conclusions from these.",
        "Table 3 shows the full lists for adherence and PEQ overall."
      ]
    },
    {
      "heading": "5 Discussion and Conclusions",
      "text": [
        "The results show that although we can weakly predict symptoms at levels above chance using only high-level dialogue factors, we cannot do so for adherence, or satisfaction measures.",
        "Despite the link between patient other initiated repair and adherence, this is also not an effective predictor for our machine learning approach because of the scarcity of the phenomenon, and the fact that many of the consultations for which the patients subsequently exhibited good adherence behaviour do not feature a single patient clarification, which may be linked to psychiatrist clarity rather than lack of effort or engagement on the patient's part.",
        "The high accuracies with lexical features show that some aspects of the consultations do enable accurate prediction of adherence, PEQ measures and symptoms.",
        "However, as the features which allow us to achieve such good results rely on specific words used, it is unclear how generalisable or interpretable such results are.",
        "The lexical features chosen do generalise over our dataset (in which individual patients appear only once), and exclude doctor talk, so cannot be simply picking out unique unigram signatures relating to individual patients or doctors; however, given the small size of the dataset used for this initial investigation with its constrained domain, genre and topics, and the use of the whole dataset to select predictive words, it is unclear whether these results will scale up to a larger dataset.",
        "We therefore suspect that more general, higher-level dialogue features such as specific interaction phenomena (repair, question-answering) and/or more general models of topic may be required.",
        "While unigrams are too low-level to be explanatory and may not generalise, the dialogue features discussed are too high-level to be useful; we are therefore examining mid-level phenomena and models to capture the predictability while remaining general and providing more interpretable features and results.",
        "Although the word lists offer clues as to the relevance of specific words for the overall predictability, we would not like to leave it at that.",
        "Further experiments are therefore underway to investigate whether we can find a level of appropriate explanatory power and maximal predictivity using an interim level of analysis, for example with n-gram and part-of-speech-based models, topic models based on word distributions, and turn-taking phenomena.",
        "Additional experiments also look at the turn-level data to see if the patient led clarification factor can be directly extracted from the transcripts.",
        "Adherence PEQ overall air grass schizophrenic 20th electric onto sometime anyone grave sensation ages energy overweight son balanced guitar sickness angry environment oxygen standing bleach h simply anxiety experiencing packed stomach build hahaha sodium background facilities percent suddenly building lager stable bladder friendly personally sundays busy laying stock booked helps picture suppose challenge lifting symptoms boy ignore played table chemical lucky talks broken immediately programs team complaining mates teach bus increased progress television cup monthly terminology certificate irritated provide thursdays dates mouse throat dead kick public troubles en nowhere virtually deep later quid uhhm fill pains was drunk lee radio upsetting finished possibly wave earn loose realised walks fish pr weve eeerrrr low reply watchers flashbacks recent worse eerrmm march sat wine removed writing eerrrmm mates shaky ri moments sofa Table 3: Most predictive unigram features"
      ]
    }
  ]
}
