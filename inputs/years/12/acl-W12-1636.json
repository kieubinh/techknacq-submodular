{
  "info": {
    "authors": [
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ],
    "book": "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
    "id": "acl-W12-1636",
    "title": "Contingency and Comparison Relation Labeling and Structure Prediction in Chinese Sentences",
    "url": "https://aclweb.org/anthology/W12-1636",
    "year": 2012
  },
  "references": [
    "acl-D09-1036",
    "acl-L08-1093",
    "acl-P09-1077",
    "acl-P09-2004",
    "acl-P10-1073",
    "acl-P11-1100",
    "acl-W00-1205",
    "acl-W05-0312"
  ],
  "sections": [
    {
      "text": [
        "C5.01, and the support vector machine, SVMlight2, are applied.",
        "The linguistic features are the crucial part in the learning-based approaches.",
        "Various features from different linguistic levels are evaluated in the experiments as shown below.",
        "Word: The bags of words in each clause.",
        "The Stanford Chinese word segmenter3 is applied to all the sentences to tokenize the Chinese words.",
        "In addition, the first word and the last word in each clause are extracted as distinguished features.",
        "POS: The bags of parts of speech (POS) of the words in each clause are also taken as features.",
        "All the sentences in the dataset are sent to the Stanford parser4 that parses a sentence from a surface form into a syntactic tree, labels POS for each word, and generates all the dependencies among the words.",
        "In addition, the POS tags of the first word and the last word in each clause are extracted as distinguished features.",
        "Length: Several length features are considered, including the number of clauses in the sentence and the number of words for each clause in the sentence.",
        "Connective: In English, some words/phrases called connectives are used as discourse markers.",
        "For example, the phrase ?due to?",
        "is a typical connective that indicates a Contingency relation, and the word ?however?",
        "is a connective that indicates a Comparison relation.",
        "Similar to the connectives in English, various words and word pair patterns are usually used as discourse markers in Chinese.",
        "A dictionary that contains several types of discourse markers is used.",
        "The statistics of the connective dictionary and samples are listed in Table 2.",
        "An intra-sentential phrase pair indicates a relation which occurs only inside a sentence.",
        "In other words, a relation occurs when the two phrases of an intra-sentential pair exist in the same sentence no matter whether they are in the same clause or not.",
        "In contrast, an inter-sentential connective indicates a relation that can occur across neighboring sentences.",
        "Some connectives belong to both intra-sentential and inter-sentential types.",
        "Each connective in each clause is detected and marked with its corresponding type.",
        "For example, the phrase ?",
        "?",
        "Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word ?76.75% 58.94% 72.36% 56.54% +POS 77.15% 61.72% 72.28% 60.53% +Length 77.15% 61.72% 72.60% 61.09% +Connective ?81.63% 71.11% ?78.05% 69.17% +Dependency 81.14% 70.79% 77.80% 68.79% +Structure 81.30% 70.78% ?77.48% 69.08% +Polarity 81.30% 70.78% 77.64% 69.09% Table 3: Performance of the two-way classification.",
        "Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word ?76.50% 34.72% 73.58% 31.54% +POS 76.99% 36.77% 72.52% 34.44% +Length 76.99% 36.77% 72.36% 34.54% +Connective 79.84% 44.08% ?77.89% 45.26% +Dependency 79.92% 44.47% ?77.07% 44.42% +Structure 79.92% 44.47% 77.15% 44.69% +Polarity 79.92% 44.47% 77.40% 44.80% Table 4: Performance of the four-way classification.",
        "Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word 73.66% 3.00% 70.00% 3.62% +POS 73.66% 3.00% 69.84% 4.29% +Length 73.66% 3.00% 70.00% 5.08% +Connective 74.80% 4.90% 74.39% 7.66% +Dependency 74.72% 4.61% 72.60% 5.60% +Structure 74.72% 4.61% 73.01% 5.49% +Polarity 74.72% 4.61% 72.76% 5.23% Table 5: Performance of the 49-way classification.",
        "Task Explicit Implicit Accuracy F-score Accuracy F-score 2-way 77.97% 69.26% 88.98% 50.64% 4-way 76.06% 42.54% 88.98% 31.39% 49-way 71.33% 4.88% 89.41% 1.92% Table 6: Performances for explicit cases and implicit cases.",
        "to predict a given sentence with four classes: existence of Contingency relations only, existence of Comparison relations only, existence of Both relations, and Nil.",
        "The experimental results of the four-way classification task are shown in Table 4.",
        "Consistent with the results of the two-way classification task, the addition of Connective to the SVM yields a significant improvement at p=0.005.",
        "The performance between the decision tree and the SVM is still similar, but the SVM achieves a slightly better F-score of 45.26% in comparison with the best F-score of 44.47% achieved by the decision tree.",
        "We further extend our model to predict the full relation structure of a given sentence as shown in Figure 1 and Figure 4.",
        "This is a 49-way classification task because there are 49 types of the full relation structures in the dataset.",
        "Not only as many as 49-ways, 72.6% of instances belong to the Nil relation, which yields an unbalanced classification problem.",
        "The experimental results are shown in Table 5.",
        "In the most challenging case, the SVM achieves a better F-score of 7.66% in comparison with the F-score of 4.90% achieved by the decision tree.",
        "Connective is still the most helpful feature.",
        "Comparing the F-scores of the SVM in the three tasks with the F-scores of the decision tree, it shows that the SVM performs better for predicting finer classes.",
        "4.2 Explicit versus Implicit We compare the performances between the explicit instances and the implicit instances for the three tasks with the decision tree model trained on all features.",
        "The results are shown in Table 6.",
        "The higher accuracies and the lower F-scores of the implicit cases are due to the fact that the classifier tends to predict the sentences as Nil when no connective is found, and most implicit samples are Nil.",
        "For example, the relation of Contingency in implicit sample (S6) should be inferred from the meaning of ??",
        "?brought?.",
        "(S6) ??????????????????????????",
        "(?The unique geographical environment, it really brought the infinite wealth to this hundred-year port.?)",
        "In addition, some informal/spoken phrases are useful clues for predicting the relations, but they are not present in our connective dictionary.",
        "For example, the phrase ?",
        "?",
        "?if?",
        "implies a Contingency relation in (S7).",
        "This issue can be addressed by using a larger connective dictionary that contains informal and spoken phrases.",
        "(S7) ???????????????????????",
        "(?If you want to backpacking, how about an organized tour??)",
        "We regard an instance as explicit if there is at least one connective in the sentence.",
        "However, many explicit instances are still not easy to label",
        "even with the connectives.",
        "As a result, predicting explicit samples is much more challenging than the task of recognizing explicit discourse relations in English.",
        "One reason is the ambiguous usage of connectives as shown in (S2).",
        "The following sentence depicts another issue.",
        "The word ??",
        "?however?",
        "in (S8) is a connective used as a marker of an inter-sentential relation.",
        "That is, the entire sentence is one of the arguments of an inter-sentential Comparison relation, but it does not contain any intra-sentential relation inside the sentence itself.",
        "(S8) ????????????????????????",
        "(?However, Fu Wu Kang, who speaks fluent Chinese, openly criticizes this opinion.?)",
        "The fact that connectives possess multiple senses is one of the important reasons for their misclassification.",
        "This issue can be addressed by employing contextual information such as the neighboring sentences.",
        "4.3 Number of Clauses We compare the performance among the 2-clause instances, the 3-clause instances, and the 4-clause instances for the three tasks with the decision tree model trained on all the features.",
        "The accuracies (A) and F-scores (F) are reported in Table 7.",
        "Comparing the two-way classification and the four-way classification tasks, the performance of the longer instances decreases a little in relation labeling.",
        "Although sentence complexity increases with length, a longer sentence provides more information at the same time.",
        "In the 49-way classification, the model should predict the sentence structure and the relation tags from the 49 candidate classes.",
        "The performances are greatly decreased because the feasible classes are substantially increased along with the number of clauses.",
        "4.4 Contingency versus Comparison The confusion matrix of the decision tree model trained on all features for the four-way classification is shown in Table 8.",
        "Each row represents the samples in an actual class, while each column of the matrix represents the samples in a predicted class.",
        "The precision (P), recall (R), Task 2-Clause 3-Clause 4-Clause A (%) F (%) A (%) F (%) A (%) F (%) 2-way 81.80 66.39 78.52 70.32 79.41 69.32 4-way 79.84 49.98 75.62 42.64 80.88 46.73 49-way 80.23 29.62 70.02 9.56 69.85 2.25 Table 7: Performances of clauses of different lengths.",
        "Actual Class Predicted Class Performance Cont.",
        "Comp.",
        "Both Nil P (%) R (%) F (%) Cont.",
        "61 3 0 131 81.33 31.28 45.19 Comp.",
        "3 40 0 83 74.07 31.75 44.44 Both 2 4 0 5 0 0 0 Nil 9 7 0 882 80.11 98.22 88.24 Table 8: Confusion matrix of the best model in the 4-way classification.",
        "Feature instance Category Usages The first token in the third clause is the word?",
        "?but; however?",
        "Word 100% The first token in the second clause is the word ?",
        "?but; however?",
        "Word 99% The first token in the third clause is a single connective of Contingency Connective 98% The first token in the first clause is the word ??",
        "?because; due to?",
        "Word 96% There is at least one word ??",
        "?in order to avoid?",
        "in the entire sentence Word 95% The first token in the second clause is the word ?",
        "?moreover; while; but?",
        "Word 94% The first token in the third clause is a single connective of Comparison Connective 93% The second clause contains a single connective of Contingency Connective 92% The first token in the second clause is a single connective of Contingency Connective 91% The first clause contains a single connective of Contingency Connective 90% Table 9: Instances of the top ten useful features for the decision tree model and F-score (F) for each class are provided on the right side of the table.",
        "The class Both is too small to train the model, thus our model does not correctly predict the samples in the Both class.",
        "The confusion matrix shows that the confusions between the classes Contingency and Comparison are very rare.",
        "The major issue is to distinguish Contingency and Comparison from the largest class, Nil.",
        "The lower recall of the Contingency and Comparison relations also show that our model tends to predict the instances as the largest class.",
        "4.5 Features The top ten useful feature instances reported by the decision tree model in the 49-way classification are shown in Table 9.",
        "Word and Connective provide useful information for the classification.",
        "Moreover,"
      ]
    },
    {
      "heading": "References Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-ming Gao, and Kuang-Yu Chen. 2000. Sinica Treebank: Design Criteria, Annotation Guidelines, and On-line Interface. In Proceedings of 2nd Chinese Language Processing Workshop (Held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, ACL-2000), pages 29-37. Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese Discourse Relation Recognition. In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 1442-1446. Aravind Joshi and Bonnie L. Webber. 2004. The Penn Discourse Treebank. In Proceedings of the Language and Resources and Evaluation Conference, Lisbon. Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing Implicit Discourse Relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), Singapore. Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically Evaluating Text Coherence Using Discourse Relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 997-1006. Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13-1, Singapore. Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic Sense Prediction for Implicit Discourse Relations in Text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP 2009), Singapore. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC). WenTing Wang, Jian Su, and Chew Lim Tan. 2010. Kernel Based Discourse Relation Recognition with Temporal Ordering Information. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July.",
      "text": [
        "Nianwen Xue.",
        "2005.",
        "Annotating Discourse Connectives in the Chinese Treebank.",
        "In Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 84-91."
      ]
    }
  ]
}
