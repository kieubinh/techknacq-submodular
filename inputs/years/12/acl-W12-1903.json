{
  "info": {
    "authors": [
      "Valentin I. Spitkovsky",
      "Hiyan Alshawi",
      "Daniel Jurafsky"
    ],
    "book": "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure",
    "id": "acl-W12-1903",
    "title": "Capitalization Cues Improve Dependency Grammar Induction",
    "url": "https://aclweb.org/anthology/W12-1903",
    "year": 2012
  },
  "references": [
    "acl-D07-1096",
    "acl-D10-1120",
    "acl-D11-1005",
    "acl-D11-1006",
    "acl-D11-1117",
    "acl-D11-1118",
    "acl-D12-1063",
    "acl-J93-2004",
    "acl-N09-1009",
    "acl-N10-1116",
    "acl-P07-1049",
    "acl-P10-1130",
    "acl-P10-1152",
    "acl-P11-1108",
    "acl-P11-2120",
    "acl-P92-1017",
    "acl-W06-2920",
    "acl-W11-0303",
    "acl-W11-1109"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We show that orthographic cues can be helpful for unsupervised parsing.",
        "In the Penn Treebank, transitions between upper-and lower-case tokens tend to align with the boundaries of base (English) noun phrases.",
        "Such signals can be used as partial bracketing constraints to train a grammar inducer: in our experiments, directed dependency accuracy increased by 2.2% (average over 14 languages having case information).",
        "Combining capitalization with punctuation-induced constraints in inference further improved parsing performance, attaining state-of-the-art levels for many languages."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Dependency grammar induction and related problems of unsupervised syntactic structure discovery are attracting increasing attention (Rasooli and Faili, 2012; Marec?ek and Zabokrtsky?, 2011, inter alia).",
        "Since sentence structure is underdetermined by raw text, there have been efforts to simplify the task, via (i) pooling features of syntax across languages (Cohen et al., 2011; McDonald et al., 2011; Cohen and Smith, 2009); as well as (ii) identifying universal rules (Naseem et al., 2010) ?",
        "such as verbocentricity (Gimpel and Smith, 2011) ?",
        "that need not be learned at all.",
        "Unfortunately most of these techniques do not apply to plain text, because they require knowing, for example, which words are verbs.",
        "As standard practice shifts away from relying on gold part-of-speech (POS) tags (Seginer, 2007; Pon-vert et al., 2010; S?gaard, 2011b; Spitkovsky et al., 2011c, inter alia), lighter cues to inducing linguistic structure become more important.",
        "Examples of useful POS-agnostic clues include punctuation boundaries (Ponvert et al., 2011; Spitkovsky et al., 2011b; Briscoe, 1994) and various kinds of bracketing constraints (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b; Pereira and Schabes, 1992).",
        "We propose adding capitalization to this growing list of sources of partial bracketings.",
        "Our intuition stems from English, where (maximal) spans of capitalized words ?",
        "such as Apple II, World War I, Mayor William H. Hudnut III, International Business Machines Corp. and Alexandria, Va ?",
        "tend to demarcate proper nouns.",
        "Consider a motivating example (all of our examples are from WSJ) without punctuation, in which all (eight) capitalized word clumps and uncased numerals match base noun phrase constituent boundaries: [NP Jay Stevens] of [NP Dean Witter] actually cut his per-share earnings estimate to [NP $9] from [NP $9.50] for [NP 1989] and to [NP $9.50] from [NP $10.35] in [NP 1990] because he decided sales would be even weaker than he had expected.",
        "and another (whose first word happens to be a leaf), where capitalization complements punctuation cues: [NP Jurors] in [NP U.S. District Court] in [NP Miami] cleared [NP Harold Hershhenson], a former executive vice president; [NP John Pagones], a former vice president; and [NP Stephen Vadas] and [NP Dean Ciporkin], who had been engineers with [NP Cordis].",
        "Could such chunks help bootstrap grammar induction and/or improve the accuracy of already-trained unsupervised parsers?",
        "In answering these questions, we will focus predominantly on sentence-internal capitalization.",
        "But we will also show that first words ?",
        "those capitalized by convention ?",
        "and uncased segments ?",
        "whose characters are not even drawn from an alphabet ?",
        "could play a useful role as well.",
        "2 English Capitalization from a Treebank We began our study by consulting the 51,558 parsed sentences of the WSJ corpus (Marcus et al., 1993): 30,691 (59.5%) of them contain non-trivially capitalized fragments ?",
        "maximal (non-empty and not",
        "sentence-initial) consecutive sequences of words that each differs from its own lower-cased form.",
        "Nearly all ?",
        "59,388 (96.2%) ?",
        "of the 61,731 fragments are dominated by noun phrases; slightly less than half ?",
        "27,005 (43.8%) ?",
        "perfectly align with constituent boundaries in the treebank; and about as many ?",
        "27,230 (44.1%) are multi-token.",
        "Table 1 shows the top POS sequences comprising fragments."
      ]
    },
    {
      "heading": "3 Analytical Experiments with Gold Trees",
      "text": [
        "We gauged the suitability of capitalization-induced fragments for guiding dependency grammar induction by assessing accuracy, in WSJ,1 of parsing constraints derived from their end-points.",
        "Following the suite of increasingly-restrictive constraints on how dependencies may interact with fragments, introduced by Spitkovsky et al. (2011b, ?2.2), we tested several such heuristics.",
        "The most lenient constraint, thread, only asks that no dependency path from the root to a leaf enter the fragment twice; tear requires any incoming arcs to come from the same side of the fragment; sprawl demands that there be exactly one incoming arc; loose further constrains any outgoing arcs to be from the fragment's head; and strict ?",
        "the most stringent constraint ?",
        "bans external dependents.",
        "Since only strict is binding for single words, we experimented also with strict?",
        ": applying strict solely to multi-token fragments (ignoring singletons).",
        "In sum, we explored six ways in which dependency parse trees can be constrained by fragments whose endpoints could be defined by capitalization (or in other various ways, e.g., semantic an",
        "%-correctness of their derived constraints (for English).",
        "notations, punctuation or HTML tags in web pages).",
        "For example, in the sentence about Cordis, the strict hypothesis would be wrong about five of the eight fragments: Jurors attaches in; Court takes the second in; Hershhenson and Pagones derive their titles, president; and (at least in our reference) Vadas attaches and, Ciporkin and who.",
        "Based on this, we would consider strict to be 37.5%-accurate.",
        "But loose ?",
        "and the rest of the more relaxed constraints ?",
        "would get perfect scores.",
        "(And strict?",
        "would retract the mistake about Jurors but also the correct guesses about Miami and Cordis, scoring only 20%.)",
        "Table 2 (capital) shows scores averaged over the entire treebank.",
        "Columns markup (Spitkovsky et al., 2010b) and punct (Spitkovsky et al., 2011b) indicate that capitalization yields across-the-board more accurate constraints (for English) compared with fragments derived from punctuation or markup (i.e., anchor text, bold, italics and underline tags in HTML), for which such constraints were originally intended."
      ]
    },
    {
      "heading": "4 Pilot Experiments on Supervised Parsing",
      "text": [
        "To further test the potential of capitalization-induced constraints, we applied them in the Viterbi-decoding phase of a simple (unlexicalized) supervised dependency parser ?",
        "an instance of DBM-1 (Spitkovsky et al., 2012, ?2.1), trained on WSJ sentences with up punct.",
        ": thread tear sprawl loose",
        "14 held-out sets from 2006/7 CoNLL shared tasks, and ordered by number of multi-token fragments in training data.",
        "to 45 words (excluding Section 23).",
        "Table 3 shows evaluation results on held-out data (all sentences), using ?add-one?",
        "smoothing.",
        "All constraints other than strict improve accuracy by about a half-a-point, from 71.8 to 72.4%, suggesting that capitalization is informative of certain regularities not captured by DBM grammars; moreover, it still continues to be useful when punctuation-based constraints are also enforced, boosting accuracy from 74.5 to 74.9%."
      ]
    },
    {
      "heading": "5 Multi-Lingual Grammar Induction",
      "text": [
        "So far, we showed only that capitalization information can be helpful in parsing a very specific genre of English.",
        "Next, we tested its ability to generally aid dependency grammar induction, focusing on situations when other bracketing cues are unavailable.",
        "We experimented with 14 languages from 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), excluding Arabic, Chinese and Japanese (which lack case), as well as Basque and Spanish (which are preprocessed in a way that loses relevant capitalization information).",
        "For all remaining languages we trained only on simple sentences ?",
        "those lacking sentence-internal punctuation ?",
        "from the relevant training sets (for blind evaluation).",
        "Restricting our attention to a subset of the available training data serves a dual purpose.",
        "First, it allows us to estimate capitalization's impact where no other (known or obvious) cues could also be used.",
        "Otherwise, unconstrained baselines would not yield the strongest possible alternative, and hence not the most interesting comparison.",
        "Second, to the extent that presence of punctuation may correlate with sentence complexity (Frank, 2000), there are benefits to ?starting small?",
        "(Elman, 1993): e.g., relegating full data to later stages helps training (Spitkovsky et al., 2010a; Cohn et al., 2011; Tu and Honavar, 2011).",
        "Our base systems induced DBM-1, starting from uniformly-at-random chosen parse trees (Cohen and Smith, 2010) of each sentence, followed by inside-outside re-estimation (Baker, 1979) with ?add-one?",
        "smoothing.2 Capitalization-constrained systems differed from controls in exactly one way: each learner got a slight nudge towards more promising structures by choosing initial seed trees satisfying an appropriate constraint (but otherwise still uniformly).",
        "Table 4 contains the stats for all 14 training sets, ordered by number of multi-token fragments.",
        "Final accuracies on respective (disjoint, full) evaluation sets are improved by all constraints other than strict, with the highest average performance resulting from sprawl: 45.0% directed dependency accuracy,3 on average.",
        "This increase of about two points over the base system's 42.8% is driven primarily by improvements in two languages (Greek and Italian)."
      ]
    },
    {
      "heading": "6 Capitalizing on Punctuation in Inference",
      "text": [
        "Until now we avoided using punctuation in grammar induction, except to filter data.",
        "Yet our pilot experiments indicated that both kinds of information are helpful in the decoding stage of a supervised system.",
        "We took trained models obtained using the sprawl nudge (from ?5) and proceeded to again apply constraints in inference (as in ?4).",
        "Capitalization alone increased parsing accuracy only slightly, from 45.0 to 45.1%, on average.",
        "Using punctuation constraints instead led to more improved performance: 46.5%.",
        "Combining both types of constraints again resulted in slightly higher accuracies: 46.7%.",
        "Table 5 breaks down our last average performance number by language and shows the combined approach to be competitive with state-of-the-art.",
        "We suspect that further improvements could be attained by also incorporating both constraints in training and with full data."
      ]
    },
    {
      "heading": "7 Discussion and A Few Post-Hoc Analyses",
      "text": [
        "Our discussion, thus far, has been English-centric.",
        "Nevertheless, languages differ in how they use capitalization (and even the rules governing a given language tend to change over time ?",
        "generally towards having fewer capitalized terms).",
        "For instance, adjectives derived from proper nouns are not capitalized in French, German, Polish, Spanish or Swedish, unlike in English (see Table 1: JJ).",
        "And while English forces capitalization of the first-person pronoun in the nominative case, I (see Table 1: PRP), in Danish it is the plural second-person pronoun (also I) that is capitalized; further, formal pronouns (and their case-forms) are capitalized in German (Sie and Ihre, Ihres...), Italian, Slovenian, Russian and Bulgarian.",
        "In contrast to pronouns, single-word proper nouns ?",
        "including personal names ?",
        "are capitalized in nearly all European languages.",
        "Such shortest bracketings are not particularly useful for constraining sets of possible parse trees in grammar induction, however, compared to multi-word expressions; from this perspective, German appears less helpful than most cased languages, because of noun compounding, despite prescribing capitalization of all nouns.",
        "Another problem with longer word-strings in many languages is that, e.g., in French (as in English) lower-case prepositions may be mixed in with contiguous groups of proper nouns: even in surnames,",
        "and punctuation-induced constraints in inference, tested against the 14 held-out sets from 2006/7 CoNLL shared tasks, and state-of-the-art results (all sentence lengths) for systems that: (i) are also POS-agnostic and monolingual, including SCAJ (Spitkovsky et al., 2011a, Tables 5?6) and SAJ (Spitkovsky et al., 2011b); and (ii) rely on gold POS-tag identities to (a) discourage noun roots (Marec?ek and Zabokrtsky?, 2011, MZ), (b) encourage verbs (Rasooli and Faili, 2012, RF), or (c) transfer delexicalized parsers (S?gaard, 2011a, S) from resource-rich languages with parallel translations (McDonald et al., 2011, MPH).",
        "the German particle von is not capitalized, although the Dutch van is, unless preceded by a given name or initial ?",
        "hence Van Gogh, yet Vincent van Gogh."
      ]
    },
    {
      "heading": "7.1 Constraint Accuracies Across Languages",
      "text": [
        "Since even related languages (e.g., Flemish, Dutch, German and English) can have quite different conventions regarding capitalization, one would not expect the same simple strategy to be uniformly useful ?",
        "or useful in the same way ?",
        "across disparate languages.",
        "To get a better sense of how universal our constraints may be, we tabulated their accuracies for the full training sets of the CoNLL data, after all grammar induction experiments had been executed.",
        "Table 6 shows that the less-strict capitalization-induced constraints all fall within narrow (yet high) bands of accuracies of just a few percentage points: 99?100% in the case of thread, 98?100% for tear, 95?99% for sprawl and 94?99% for loose.",
        "By contrast, the ranges for punctuation-induced constraints are all at least 10%.",
        "We do not see anything partic",
        "ularly special about Greek or Italian in these summaries that could explain their substantial improvements (18 and 11%, respectively ?",
        "see Table 4), though Italian does appear to mesh best with the sprawl constraint (not by much, closely followed by Swedish).",
        "And English ?",
        "the language from which we drew our inspiration ?",
        "barely improved with capitalization-induced constraints (see Table 4) and caused the lowest accuracies of thread and strict.",
        "These outcomes are not entirely surprising: some best-and worst-performing results are due to noise, since learning via non-convex optimization can be chaotic: e.g., in the case of Greek, applying 113 constraints to initial parse trees could have a significant impact on the first grammar estimated in training ?",
        "and consequently also on a learner's final, converged model instance.",
        "We expect the averages (i.e., means and medians) ?",
        "computed over many data sets ?",
        "to be more stable and meaningful than the outliers."
      ]
    },
    {
      "heading": "7.2 Immediate Impact from Capitalization",
      "text": [
        "Next, we considered two settings that are less affected by training noise: grammar inducers immediately after an initial step of constrained Viterbi EM and supervised DBM parsers (trained on sentences with up to 45 words), for various languages in the CoNLL sets.",
        "Table 7 shows effects of capitalization to be exceedingly mild, both if applied alone and in tandem with punctuation.",
        "Exploring better ways of incorporating this informative resource ?",
        "perhaps as soft features, rather than as hard constraints ?",
        "and in combination with punctuation-and markup-induced bracketings could be a fruitful direction."
      ]
    },
    {
      "heading": "7.3 Odds and Ends",
      "text": [
        "Our earlier analysis excluded sentence-initial words because their capitalization is, in a way, trivial.",
        "But for completeness, we also tested constraints derived from this source, separately (see Table 2: initials).",
        "As expected, the new constraints scored worse (despite many automatically-correct single-word fragments) except for strict, whose binding constraints over singletons drove up accuracy.",
        "It turns out, most first words in WSJ are leaves ?",
        "possibly due to a dearth of imperatives (or just English's determiners).",
        "We broadened our investigation of the ?first leaf?",
        "and supervised performance with induced constraints, on 2006/7 CoNLL evaluation sets (sentences under 145 tokens).",
        "phenomenon and found that in 16 of the 19 CoNLL languages first words are more likely to be leaves than other words without dependents on the left;4 last words, by contrast, are more likely to take dependents than expected.",
        "These propensities may be related to the functional tendency of languages to place old information before new (Ward and Birner, 2001) and could also help bias grammar induction.",
        "Lastly, capitalization points to yet another class of words: those with identical upper-and lower-case forms.",
        "Their constraints too tend to be accurate (see",
        "ticularly interesting.",
        "In WSJ, caseless multi-token fragments are almost exclusively percentages (e.g., the two tokens of 10%), fractions (e.g., 1 1/4) or both.",
        "Such boundaries could be useful in dealing with financial data, as well as for breaking up text in languages without capitalization (e.g., Arabic, Chinese"
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "Orthography provides valuable syntactic cues.",
        "We showed that bounding boxes signaled by capitalization changes can help guide grammar induction and boost unsupervised parsing performance.",
        "As with punctuation-delimited segments and tags from web markup, it is profitable to assume only that a single word derives the rest, in such text fragments, without further restricting relations to external words ?",
        "possibly a useful feature for supervised parsing models.",
        "Our results should be regarded with some caution, however, since improvements due to capitalization in grammar induction experiments came mainly from two languages, Greek and Italian.",
        "Further research is clearly needed to understand the ways that capitalization can continue to improve parsing."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Funded, in part, by Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract FA8750-09-C-0181.",
        "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, AFRL, or the US government.",
        "We also thank Ryan McDonald and the anonymous reviewers for helpful comments on draft versions of this paper."
      ]
    }
  ]
}
