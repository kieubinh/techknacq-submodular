{
  "info": {
    "authors": [
      "David Marecek",
      "ZdenÄ›k Zabokrtsky"
    ],
    "book": "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure",
    "id": "acl-W12-1911",
    "title": "Unsupervised Dependency Parsing using Reducibility and Fertility features",
    "url": "https://aclweb.org/anthology/W12-1911",
    "year": 2012
  },
  "references": [
    "acl-A00-1031",
    "acl-D11-1117",
    "acl-N09-1012",
    "acl-P04-1061"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes a system for unsupervised dependency parsing based on Gibbs sampling algorithm.",
        "The novel approach introduces a fertility model and reducibility model, which assumes that dependent words can be removed from a sentence without violating its syntactic correctness."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "One of the traditional linguistic criteria for recognizing dependency relations (including their head-dependent orientation) is that stepwise deletion of dependent elements within a sentence preserves its syntactic correctness (Lopatkova?",
        "et al, 2005; Ku?bler et al., 2009; Gerdes and Kahane, 2011).1 If a word can be removed from a sentence without damaging it, then it is likely to be dependent on some other (still present) word.",
        "Our approach allows to utilize information from very large corpora.",
        "While the computationally demanding sampling procedure can be applied only on limited data, the unrepeated precomputation of",
        "rious fact that there are many language phenomena precluding the ideal (word by word) sentence reducibility (e.g. in the case of prepositional groups, or in the case of subjects in English finite clauses).",
        "However, we borrow only the very core of the reducibility idea.",
        "statistics for reducibility estimates can easily exploit much larger data."
      ]
    },
    {
      "heading": "2 Precomputing PoS tag reducibility scores",
      "text": [
        "We call a word (or a sequence of words) in a sentence reducible, if the sentence after removing the word remains grammatically correct.",
        "Although we cannot automatically recognize grammaticality of such newly created sentence, we can search for it in a large corpus.",
        "If we find it, we assume the word was reducible in the original sentence.",
        "It is obvious that the number of such reducible sequences of words found in a corpus is relatively low.",
        "However, it is sufficient for determining reducibility scores at least for individual types of words (part-of-speech tags).2 Assume a PoS n-gram g = [t1, .",
        ".",
        ".",
        ", tn].",
        "We go through the corpus and search for all its occurrences.",
        "For each such occurrence, we remove the respective words from the current sentence and check in the corpus whether the rest of the sentence occurs at least once elsewhere in the corpus.3 If so, then such occurrence of PoS n-gram is reducible, otherwise it is not.",
        "We denote the number of such reducible occurrences of PoS n-gram g by r(g).",
        "The number of all its occurrences is c(g).",
        "The relative reducibility 2Although we search for reducible sequences of word forms in the corpus, we compute reducibility scores for sequences of part-of-speech tags.",
        "This requires to have the corpus morphologically disambiguated.",
        "3We do not take into account sentences with less then 10 words, because they could be nominal (without any verb) and might influence the reducibility scores of verbs.",
        "English n-grams.",
        "R(g) of a PoS n-gram g is then computed as",
        "where the normalization constant N , which expresses relative reducibility over all the PoS n-grams (denoted by G), causes the scores are concentrated around the value 1.",
        "(2) Smoothing constants ?1 and ?2, which prevent reducibility scores from being equal to zero, are set as follows: 4",
        "Table 1 shows reducibility scores of the most frequent English PoS n-grams.",
        "If we consider only unigrams, we can see that the scores for verbs are often among the lowest.",
        "Verbs are followed by prepositions and nouns, and the scores for adjectives and adverbs are very high for all three examined languages.",
        "That is desired, because the reducible unigrams will more likely become leaves in dependency trees."
      ]
    },
    {
      "heading": "3 Dependency Models",
      "text": [
        "We introduce a new generative model that is different from the widely used Dependency Model with 4This setting causes that even if a given PoS n-gram is not reducible anywhere in the corpus, its reducibility score is 1/(c(g) + 1).",
        "Valence (DMV).5 Our generative model introduces fertility of a node.",
        "For a given head, we first generate the number of its left and right children (fertility model) and then we fill these positions by generating its individual dependents (edge model).",
        "If a zero fertility is generated, the head becomes a leaf.",
        "Besides the fertility model and the edge model, we use two more models (subtree model and distance model), which force the generated trees to have more desired shape."
      ]
    },
    {
      "heading": "3.1 Fertility model",
      "text": [
        "We express a fertility of a node by a pair of numbers: the number of its left dependents and the number of its right dependents.6 Fertility is conditioned by part-of-speech tag of the node and it is computed following the Chinese restaurant process.",
        "The formula for computing probability of fertility fi of a word on the position i in the corpus is as follows:",
        "where ti is part-of-speech tag of the word on the position i, c?i(?ti, fi?)",
        "stands for the count of words with PoS tag ti and fertility fi in the history, and P0 is a prior probability for the given fertility which depends on the total number of node dependents denoted by |fi |(the sum of numbers of left and right dependents):",
        "This prior probability has a nice property: for a given number of nodes, the product of fertility probabilities over all the nodes is equal for all possible dependency trees.",
        "This ensures balance of this model during inference.",
        "5In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al., 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated.",
        "Given a certain head, all its dependents in left direction are generated first, then the STOP sign in that direction, then all its right dependents and then STOP in the other direction.",
        "This process continues recursively for all generated dependents.",
        "Besides the basic fertility model, we introduce also an extended fertility model, which uses frequency of a given word form for generating number of children.",
        "We assume that the most frequent words are mostly function words (e.g. determiners, prepositions, auxiliary verbs, conjunctions).",
        "Such words tend to have a stable number of children, for example (i) some function words are exclusively leaves, (ii) prepositions have just one child, and (iii) attachment of auxiliary verbs depends on the annotation style, but number of their children is also not very variable.",
        "The higher the frequency of a word form, the higher probability mass is concentrated on one specific number of children and the lower Dirichlet hyperparameter ?",
        "in Equation 4 is needed.",
        "The extended fertility is described by equation",
        "where F (wi) is a frequency of the word wi, which is computed as a number of words wi in our corpus divided by number of all words."
      ]
    },
    {
      "heading": "3.2 Edge model",
      "text": [
        "After the fertility (number of left and right dependents) is generated, the individual slots are filled using the edge model.",
        "A part-of-speech tag of each dependent is conditioned by part-of-speech tag of the head and the edge direction (position of the dependent related to the head).8 Similarly as for the fertility model, we employ Chinese restaurant process to assign probabilities of individual dependent.",
        "where ti and tj are the part-of-speech tags of the head and the generated dependent respectively; dj is a direction of edge between the words i and j, which can have two values: left and right.",
        "c?i(?ti, tj , dj?)",
        "stands for the count of edges ti ?",
        "tj with the direction dj in the history, |T |is a number of unique tags in the corpus and ?",
        "is a Dirichlet hyperparameter.",
        "8For the edge model purposes, the PoS tag of the technical root is set to ?<root>?",
        "and it is in the zero-th position in the sentence, so the head word of the sentence is always its right dependent."
      ]
    },
    {
      "heading": "3.3 Distance model",
      "text": [
        "Distance model is an auxiliary model that prevents the resulting trees from being too flat.9 This simple model says that shorter edges are more probable than longer ones.",
        "We define probability of a distance between a word and its parent as its inverse value,10 which is then normalized by the normalization constant d.",
        "The hyperparameter ?",
        "determines the weight of this model."
      ]
    },
    {
      "heading": "3.4 Subtree model",
      "text": [
        "The subtree model uses the reducibility measure.",
        "It plays an important role since it forces the reducible words to be leaves and reducible n-grams to be sub-trees.",
        "Words with low reducibility are forced towards the root of the tree.",
        "We define desc(i) as a sequence of tags [tl, .",
        ".",
        ".",
        ", tr] that corresponds to all the descendants of the word wi including wi, i.e. the whole subtree of wi.",
        "The probability of such subtree is proportional to its reducibility R(desc(i)).",
        "The hyperparameter ?",
        "determines the weight of the model; s is a normalization constant."
      ]
    },
    {
      "heading": "3.5 Probability of the whole treebank",
      "text": [
        "We want to maximize the probability of the whole generated treebank, which is computed as follows:",
        "where pi(i) denotes the parent of the word on the position i.",
        "We multiply the probabilities of fertility, edge, distance from parent, and subtree over all 9Ideally, the distance model would not be needed, but experiments showed that it helps to infer better trees.",
        "10Distance between any word and the technical root of the dependency tree was set to 10.",
        "Since each technical root has only one dependent, this value does not affect the model.",
        "words (nodes) in the corpus.",
        "The extended fertility model P ?f can be substituted by its basic variant Pf ."
      ]
    },
    {
      "heading": "4 Sampling algorithm",
      "text": [
        "For stochastic searching for the most probable dependency trees, we employ Gibbs sampling, a standard Markov Chain Monte Carlo technique (Gilks et al., 1996).",
        "In each iteration, we loop over all words in the corpus in a random order and change the dependencies in their neighborhood (a small change described in Section 4.2).",
        "In the end, ?average?",
        "trees based on the whole sampling are built."
      ]
    },
    {
      "heading": "4.1 Initialization",
      "text": [
        "Before the sampling starts, we initialize the projective trees randomly in a way that for each sentence, we choose randomly one word as the head and attach all other words to it.11"
      ]
    },
    {
      "heading": "4.2 Small Change Operator",
      "text": [
        "We use the bracketing notation for illustrating the small change operator.",
        "Each projective dependency tree consisting of n words can be expressed by n pairs of brackets.",
        "Each bracket pair belongs to one node and delimits its descendants from the rest of the sentence.",
        "Furthermore, each bracketed segment contains just one word that is not embedded deeper; this node is the segment head.",
        "An example of this notation is in Figure 1.",
        "The small change is then very simple.",
        "We remove one pair of brackets and add another, so that the conditions defined above are not violated.",
        "An example of such change is in Figure 2.",
        "From the perspective of dependency structures, the small change can be described as follows: 11More elaborated methods for generating random trees converges to similar results.",
        "Therefore we conclude that the choice of the initialization mechanism is not so important here.",
        "TTThe dogwadosinootpoTTre doki.",
        "(dooT)dd TTThe dogwadosinootpoTTre doki.",
        "(dooT)ddTTThe dogwadosinootpoTTre doki.",
        "(dooT)ddTTThe dogwadosinootpoTTre doki.",
        "(dooT)ddTTd d TTThe dogwadosinootpoTTre doki.",
        "(dooT)ddTTThe dogwadosinootpoTTre doki.",
        "(dooT)ddT Td d",
        "tree.",
        "The bracket (in the park) is removed and there are five possibilities how to replace it.",
        "1.",
        "Pick a random non-root word w (the word in in our example) and find its parent p (the word was).",
        "2.",
        "Find all other children of w and p (the words dog, park, and .)",
        "and denote this set by C. 3.",
        "Choose the new head out of w and p. Mark the new head as g and the second candidate as d. Attach d to g. 4.",
        "Select a neighborhood D adjacent to the word d as a continuous subset of C and attach all words from D to d. D may be also empty.",
        "5.",
        "Attach the remaining words from C that were not in D to the new head g."
      ]
    },
    {
      "heading": "4.3 Building ?average? trees",
      "text": [
        "The ?burn-in?",
        "period is set to 10 iterations.",
        "After this period, we begin to count how many times an edge occurs at a particular location in the corpus.",
        "This counts are collected over the whole corpus with the collection-rate 0.01.12 When the sampling is finished, the final dependency trees are built using such edges that belonged to the most frequent ones during the sampling.",
        "We employ the maximum spanning tree (MST) algorithm (Chu and Liu, 1965) to find them.13 Tree pro-jectivity is not guaranteed by the MST algorithm."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We evaluated our parser on 10 treebanks included in the WILS shared-task data.",
        "Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011), the tuning experiments were performed on English only.",
        "We used 12After each small change is made, the edges from the whole corpus are collected with a probability 0.01.",
        "13The weights of edges needed in MST algorithm correspond to the number of times they were present during the sampling.",
        "English development data for checking functionality of the individual models and for optimizing hyperparameter values.",
        "The best configuration of the parser achieved on English was then used for parsing all other languages.",
        "This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manually annotated treebanks."
      ]
    },
    {
      "heading": "5.1 Data",
      "text": [
        "For each experiment, we need two kinds of data: a smaller treebank, which is used for sampling and for evaluation, and a large corpus, from which we compute n-gram reducibility scores.",
        "The induction of dependency trees and evaluation is done only on WILS testing data.",
        "For obtaining reducibility scores, we used Wikipedia articles downloaded by Majlis?",
        "and Z?abokrtsky?",
        "(2012).",
        "Their statistics across languages are showed in Table 2.",
        "To make them useful, the necessary preprocessing steps must have been done.",
        "The texts were first automatically segmented and tokenized14 and then they were part-of-speech tagged by TnT tagger (Brants, 2000), which was trained on the respective WILS training data.",
        "The quality of such tagging is not very high, since we do not use any lexicons15 or pretrained models.",
        "However, it is sufficient for obtaining good reducibility scores."
      ]
    },
    {
      "heading": "5.2 Setting the hyperparameters",
      "text": [
        "The applicability of individual models and their parameters were tested on English development data 14The segmentation to sentences and tokenization was performed using the TectoMT framework (Popel and Z?abokrtsky?, 2010).",
        "15Using lexicons or another pretrained models for tagging means using other sources of human annotated data, which is not allowed if we want to compare our results with others.",
        "and full part-of-speech tags.",
        "The four hyperparameters ?",
        "(fertility model), ?",
        "(edge model), ?",
        "(distance model), and ?",
        "(subtree model), were set by a grid search algorithm, which found the following optimal values: ?e = 0.01, ?",
        "= 1, ?",
        "= 1.5, ?",
        "= 1"
      ]
    },
    {
      "heading": "5.3 Results",
      "text": [
        "The best setting from the experiments on English is now used for evaluating our parser across all WILS treebanks.",
        "Besides using the standard part-of-speech tags (POS), the experiments were done also on coarse tags (CPOS) and universal tags (UPOS).",
        "From results presented in Table 3, it is obvious that our systems outperforms all the given baselines considering the average scores across all the testing languages.",
        "However, it is important to note that we used an additional source of information, namely large unannotated corpora for computing reducibility scores, while the baseline system (Gillenwater et al., 2011) use only the WILS datasets."
      ]
    },
    {
      "heading": "6 Conclusions and Future Work",
      "text": [
        "We have described a novel unsupervised dependency parsing system employing new features, such as reducibility or fertility.",
        "The reducibility scores are extracted from a large corpus, and the computationally demanding inference is then done on smaller data.",
        "In future work, we would like to estimate the hyperparameters automatically, for example by the Metropolis-Hastings technique (Gilks et al., 1996).",
        "Furthermore, we would like to add lexicalized models and automatically induced word classes instead of the PoS tags designed by humans.",
        "Finally, we would like to move towards deeper syntactic structures, where the tree would be formed only by content words and the function words would be treated in a different way."
      ]
    }
  ]
}
