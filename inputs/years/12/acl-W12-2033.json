{
  "info": {
    "authors": [
      "Keisuke Sakaguchi",
      "Yuta Hayashibe",
      "Shuhei Kondo",
      "Lis Kanashiro",
      "Tomoya Mizumoto",
      "Mamoru Komachi",
      "Yuji Matsumoto"
    ],
    "book": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP",
    "id": "acl-W12-2033",
    "title": "NAIST at the HOO 2012 Shared Task",
    "url": "https://aclweb.org/anthology/W12-2033",
    "year": 2012
  },
  "references": [
    "acl-C08-1022",
    "acl-J06-4003",
    "acl-J96-1002",
    "acl-N10-1019",
    "acl-P07-1031",
    "acl-P07-1033",
    "acl-P10-2065",
    "acl-P11-1093",
    "acl-P98-1013",
    "acl-W07-0712",
    "acl-W10-4236"
  ],
  "sections": [
    {
      "text": [
        "The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 281?288, Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics NAIST at the HOO 2012 Shared Task"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "This paper describes the Nara Institute of Science and Technology (NAIST) error correction system in the Helping Our Own (HOO) 2012 Shared Task.",
        "Our system targets preposition and determiner errors with spelling correction as a preprocessing step.",
        "The result shows that spelling correction improves the Detection, Correction, and Recognition F-scores for preposition errors.",
        "With regard to preposition error correction, F-scores were not improved when using the training set with correction of all but preposition errors.",
        "As for determiner error correction, there was an improvement when the constituent parser was trained with a concatenation of treebank and modified treebank where all the articles appearing as the first word of an NP were removed.",
        "Our system ranked third in preposition and fourth in determiner error corrections."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Researchers in natural language processing have focused recently on automatic grammatical error detection and correction for English as a Second Language (ESL) learners?",
        "writing.",
        "There have been a lot of papers on these challenging tasks, and remarkably, an independent session for grammatical error correction took place in the ACL-2011.",
        "The Helping Our Own (HOO) shared task (Dale and Kilgarriff, 2010) is proposed for improving the quality of ESL learners?",
        "writing, and a pilot run with six teams was held in 2011.",
        "The HOO 2012 shared task focuses on the correction of preposition and determiner errors.",
        "There has been a lot of work on correcting preposition and determiner errors, where discriminative models such as Maximum Entropy and Averaged Perceptron (De Felice and Pulman, 2008; Rozovskaya and Roth, 2011) and/or probablistic language models (Gamon, 2010) are generally used.",
        "In addition, it is pointed out that spelling and punctuation errors often disturb grammatical error correction.",
        "In fact, some teams reported in the HOO 2011 that they corrected spelling and punctuation errors before correcting grammatical errors (Dahlmeier et al., 2011).",
        "Our strategy for HOO 2012 follows the above procedure.",
        "In other words, we correct spelling errors at the beginning, and then train classifiers for correcting preposition and determiner errors.",
        "The result shows our system achieved 24.42% (third-ranked) in F-score for preposition error correction, 29.81% (fourth-ranked) for determiners, and 27.12% (fourth-ranked) for their combined.",
        "In this report, we describe our system architecture and the experimental results.",
        "Sections 2 to 4 describe the system for correcting spelling, preposition, and determiner errors.",
        "Section 5 shows the experimental design and results."
      ]
    },
    {
      "heading": "2 System Architecture for Spelling Correction",
      "text": [
        "Spelling errors in second language learners?",
        "writing often disturb part-of-speech (POS) tagging and dependency parsing, becoming an obstacle for grammatical error detection and correction tasks.",
        "For example, POS tagging for learners?",
        "writing fails be",
        "e.g.",
        "I think it is *verey/very *convent/convenient for the group.",
        "without spelling error correction: ...",
        "(?it?, ?PRP?",
        "), (?is?, ?VBZ?",
        "), (?verey?, ?PRP?",
        "), (?convent?, ?NN?",
        "), ... with spelling error correction : ...",
        "(?It?, ?PRP?",
        "), (?is?, ?VBZ?",
        "), (?very?, ?RB?",
        "), (?convenient?, ?JJ?",
        "), ...",
        "cause of misspelled words (Figure 1).1 To reduce errors derived from misspelled words, we conduct spelling error correction as a preprocessing task.",
        "The procedure of spelling error correction we use is as follows.",
        "First of all, we look for misspelled words and suggest candidates by GNU Aspell2, an open-source spelling checker.",
        "The candidates are ranked by the probability of 5-gram language model built from Google N-gram (Web 1T 5-gram Version 1)3 (Brants and Franz, 2006) with IRST LM Toolkit (Federico and Cettolo, 2007).4 Finally, according to the rank, we changed the misspelled word into the 1-best candidate word.",
        "In a preliminary experiment, where we use the original CLC FCE dataset,5 our spelling error correction obtains 52.4% of precision, 72.2% of recall, and 60.7% of F-score.",
        "We apply the spelling error correction to the training and test sets provided, and use both spelling-error and spelling-error-free sets for comparison."
      ]
    },
    {
      "heading": "3 System Architecture for Preposition",
      "text": []
    },
    {
      "heading": "Error Correction",
      "text": [
        "There are so many prepositions in English.",
        "Because it is difficult to perform multi-class classification, we focus on twelve prepositions: of, in, for, to, by, with, at, on, from, as, about, since, which account for roughly 91% of preposition usage (Chodorow et al., 2010).",
        "The errors are classified into three categories according to their ways of correction.",
        "First, replacement error indicates that learners use a wrong preposition.",
        "For instance, with in Example (1) is a",
        "and tagged with a label ?S?.",
        "replacement error.",
        "I went there withby bus.",
        "(1) Second, insertion error points out they incorrectly inserted a preposition, such as ?about?",
        "in Example (2).6 We discussed aboutNONE the topic.",
        "(2) Third, deletion error means they fail to write obligatory prepositions.",
        "For example, ?NONE?",
        "in Example (3) is an deletion error.",
        "This is the place to relax NONEin.",
        "(3) Replacement and insertion error correction can be regarded as a multi-class classification task at each preposition occurrence.",
        "However, deletion errors differ from the other two types of errors in that they may occur at any place in a sentence.",
        "Therefore, we build two models, a combined model for replacement and insertion errors and a model for deletion errors, taking the difference into account.",
        "For the model of replacement and insertion errors, we simultaneously perform error detection and correction with a single model.",
        "For the model of deletion errors, we only check whether direct objects of verbs need prepositions, because it is time consuming to check all the gaps between words.",
        "Still, it covers most deletion errors.7 We merge the outputs of the two models to get the final output.",
        "We used two types of training sets extracted from the original CLC-FCE dataset.",
        "One is the ?gold?",
        "set, where training sentences are corrected except for preposition errors.",
        "In the gold set, spelling errors are also corrected to the gold data in the corpus.",
        "The other is the ?original?",
        "set, which includes the 6?NONE?",
        "means there are no words.",
        "72,407 out of 5,324 preposition errors in CLC-FCE are between verbs and nouns.",
        "We performed sentence splitting using the implementation of Kiss and Strunk (2006) in NLTK 2.0.1rc2.",
        "We conducted dependency parsing by Stanford parser 1.6.9.8 We used the features described in (Tetreault et al., 2010) as shown in Table 1 with Maximum Entropy (ME) modeling (Berger et al., 1996) as a multi-class classifier.",
        "We used the implementation of Maximum Entropy Modeling Toolkit9 with its default parameters.",
        "For web n-gram calculation, we used Google N-gram with a search system for giga-scale n-gram corpus, called SSGNC 0.4.6.10"
      ]
    },
    {
      "heading": "4 System Architecture for Determiner",
      "text": []
    },
    {
      "heading": "Error Correction",
      "text": [
        "We focused on article error correction in the determiner error correction subtask, because the errors related to articles significantly outnumber the errors unrelated to them.",
        "Though more than twenty types of determiners are involved in determiner error corrections of the HOO training set, over 90% of errors",
        "are related to three articles a, an and the.",
        "We defined article error correction as a multi-class classification problem with three classes, a, the and null article, and assumed that target articles are placed at the left boundary of a noun phrase (NP).",
        "The indefinite article an was normalized to a in training and testing, and restored to an later in an example-based post-processing step.",
        "If the system output was a and the word immediately after a appeared more frequently with an than with a in the training corpus, a was restored to an.",
        "If the word appeared equally frequently with a and an or didn't appear in the training corpus, a was restored to an if the word's first character was one of a, e, i, o, u.",
        "Each input sentence was parsed using the Berkeley Parser11 with two models, ?normal?",
        "and ?mixed?.",
        "The ?normal?",
        "model was trained on a treebank of normal English sentences.",
        "In preliminary experiments, the ?normal?",
        "model sometimes misjudged the span of NPs in ESL writers?",
        "sentences due to missing articles.",
        "So we trained the ?mixed?",
        "model on a concatenation of the normal treebank and a modified treebank in which all the articles appearing as the first word of an NP were removed.",
        "By",
        "augmenting the training data for the parser model with sentences lacking articles, the span of NPs that lack an article might have better chance of being correctly recognized.",
        "In addition, dependency information was extracted from the parse using the Stanford parser 1.6.9.",
        "For each NP in the parse, we extracted a feature vector representation.",
        "We used the feature templates shown in Table 2, which are inspired by (De Felice, 2008) and adapted to the CFG representation.",
        "For the parser models, we trained the ?normal?",
        "model on the WSJ part of Penn Treebank sections 02-21 with the NP annotation by Vadas and Curran (2007).",
        "The ?mixed?",
        "model was trained on the concatenation of the WSJ part and its modified version.",
        "For the classification model, we used the written part of the British National Corpus (BNC) in addition to the CLC FCE Dataset, because the amount of in-domain data was limited.",
        "In examples taken from the CLC FCE Dataset, the true labels after the correction were used.",
        "In examples taken from the BNC, the article of each NP was used as the label.",
        "We trained a linear classifier using opal12 with the PA-I algorithm.",
        "We also used the feature augmentation",
        "and each contains 180.1 word tokens on average.",
        "We defined eight distinct configurations based on our subsystem parameters (Table 3).",
        "The official task evaluation uses three metrics (Detection, Recognition, and Correction), and three measures Precision, Recall, and F-score were computed13 for",
        "systems.",
        "In terms of the effect of pre-processing, spelling correction improved the F-score of Detection, Correction, and Recognition for preposition errors after revision, whereas there were fluctuations in other conditions.",
        "This may be because there were a few spelling errors corrected in the test set.14 Another reason why no stable improvement was found in determiner error correction is because spelling correction often produces nouns that affect the determiner error detection and correction more sensitively than prepositions.",
        "For example, a misspelled word *freewho / free who was corrected as freezer.",
        "This type of error may have increased false positives.",
        "The example *National Filharmony / the National Philharmony was corrected as National Fleming, where the proper noun Fleming does not need a determiner and this type of error increased false negatives.",
        "As for preposition error correction, the classifier performed better when it was trained with the ?original?",
        "set rather than the error-corrected (all but preposition errors) ?gold?",
        "set.",
        "The reason for this is that the gold set is trained with the test set that contains correcttext.org/hoo2012/eval.html 14There was one spelling correction per document in average.",
        "several types of errors which the original CLC-FCE dataset alo contains.",
        "Therefore, the ?original?",
        "classifier is more optimised and suitable for the test set than the ?gold?",
        "one.",
        "For determiner error correction, the ?mixed?",
        "model improved precision and F-score in the additional experiments."
      ]
    },
    {
      "heading": "5.1 Error Analysis of Preposition Correction",
      "text": [
        "We briefly analyze some errors in our proposed model according to the three categories of errors.",
        "First, most replacement errors require deep understanding of context.",
        "For instance, for in Example (4) must be changed to to.",
        "However, modifications of is also often used, so it is hard to decide either to or of is suitable based on the values of N-gram frequencies.",
        "Its great news to hear you have been given extra money and that you will spend it in modifications forto the cinema.",
        "(4) Second, most insertion errors need a grammatical judgement rather than a semantic one.",
        "For instance, ?in?",
        "in Example (5) must be changed to ?NONE.?",
        "Their love had always been kept inNONE se",
        "?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.",
        "nize ?keep?",
        "takes an object and a complement; in Example (5) ?love?",
        "is the object and ?secret?",
        "is the complement of ?keep?",
        "while the former is left-extraposed.",
        "A rule-based approach may be better suited for these cases than a machine learning approach.",
        "Third, most deletion errors involve discrimination between transitive and intransitive.",
        "For instance, ?NONE?",
        "in Example (6) must be changed to ?for?, because ?wait?",
        "is intransitive.",
        "I?ll wait NONEfor your next letter.",
        "(6) To deal with these errors, we may use rich knowledge about verbs such as VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998) in order to judge whether a verb is transitive or intransitive."
      ]
    },
    {
      "heading": "5.2 Error Analysis of Determiner Correction",
      "text": [
        "We conducted additional experiments for determiner errors and report the results here because the submitted system contained a bug.",
        "In the submitted system, while the test data were parsed by the ?mixed?",
        "model, the training data and the test data were parsed by the default grammar provided with Berkeley Parser.",
        "Moreover, though there were about 5.5 million sentences in the BNC corpus, only about 2.7 million of them had been extracted.",
        "Though these errors seem to have improved the performance, it is difficult to specify which errors had positive effects.",
        "Table 10 shows the result of additional experiments.",
        "Unlike the submitted system, the ?mixed?",
        "model contributed toward a higher precision and F-score.",
        "Though the two parser models parsed the sentences differently, the difference in the syntactic analysis of test sentences did not always led to different output by the downstream classifiers.",
        "On the contrary, the classifiers often returned different outputs even for an identically parsed sentence.",
        "In fact, the major source of the performance gap between the two models was the number of the wrong outputs rather than the number of correct ones.",
        "While the ?mixed?",
        "model without spelling correction returned 146 outputs, of which 83 were spurious, the ?normal?",
        "model without spelling correction produced 209 outputs, of which 143 were spurious.",
        "This may suggest the difference of the two models can be attributed to the difference in the syntactic analysis of the training data.",
        "One of the most frequent types of errors common to the two models were those caused by misspelled words.",
        "For example, when your letter was misspelled to be *yours letter, it was regarded as an",
        "NP without a determiner resulting in a false positive such as *a yours letter.",
        "Among the other types of errors, several seemed to be caused by the information from the context window.",
        "For instance, the system output for It was last month and ... was it was *the last month and ....",
        "It is likely that the word last triggered the misinsertion here.",
        "Such kind of errors might be avoided by conjunctive features of context information and the head word.",
        "Last but not least, compound errors were also frequent and probably the most difficult to solve.",
        "For example, it is quite difficult to correct *for a month to per month if we are dealing with determiner errors and preposition errors separately.",
        "A more sophisticated approach such as joint modeling seems necessary to correct this kind of errors."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "This report described the architecture of our preposition and determiner error correction system.",
        "The experimental result showed that spelling correction advances the performance of Detection, Correction and Recognition for preposition errors.",
        "In terms of preposition error correction, F-scores were not improved when the error-corrected dataset was used.",
        "As to determiner error correction, there was an improvement when the constituent parser was trained on a concatenation of treebank and modified treebank where all the articles appearing as the first word of an NP were removed."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was partly supported by the National Institute of Information and Communications Technology Japan."
      ]
    }
  ]
}
